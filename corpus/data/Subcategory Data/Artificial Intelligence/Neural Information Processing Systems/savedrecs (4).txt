PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Wainwright, MJ; Jordan, MI		Thrun, S; Saul, K; Scholkopf, B		Wainwright, MJ; Jordan, MI			Semidefinite relaxations for approximate inference on graphs with cycles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present a new method for calculating approximate marginals for probability distributions defined by graphs with cycles, based on a Gaussian entropy bound combined with a semidefinite outer bound on the marginal polytope. This combination leads to a log-determinant maximization problem that can be solved by efficient interior point methods [8]. As with the Bethe approximation and its generalizations [12], the optimizing arguments of this problem can be taken as approximations to the exact marginals. In contrast to Bethe/Kikuchi approaches, our variational problem is strictly convex and so has a unique global optimum. An additional desirable feature is that the value of the optimal solution is guaranteed to provide an upper bound on the log partition function. In experimental trials, the performance of the log-determinant relaxation is comparable to or better than the sum-product algorithm, and by a substantial margin for certain problem classes. Finally, the zero-temperature limit of our log-determinant relaxation recovers a class of well-known semidefinite relaxations for integer programming [e.g., 3].	Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Wainwright, MJ (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013	Wainwright, Martin J./0000-0002-8760-2236				COWELL R, 1999, STAT ENG INFORMATION; Deza M., 1997, GEOMETRY CUTS METRIC; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Jordan M. I., 1999, LEARNING GRAPHICAL M; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Rockafellar G., 1970, CONVEX ANAL; Vandenberghe L, 1998, SIAM J MATRIX ANAL A, V19, P499, DOI 10.1137/S0895479896303430; WAINWRIGHT M, 2003, UCBCSD31226; Wainwright M. J., 2003, 649 UC BERK DEP STAT; WAINWRIGHT MJ, 2002, UNCERTAINTY ARTIFICI, V18, P536; Yedidia J, 2002, TR200122 MITS EL RES; [No title captured]	12	9	9	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						369	376						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500047
C	Edelman, S; Hiles, BP; Yang, HJ; Intrator, N		Dietterich, TG; Becker, S; Ghahramani, Z		Edelman, S; Hiles, BP; Yang, HJ; Intrator, N			Probabilistic principles in unsupervised learning of visual structure: human data and a model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				REPRESENTATION; SHAPE	To find out how the representations of structured visual objects depend on the co-occurrence statistics of their constituents, we exposed subjects to a set of composite images with tight control exerted over (1) the conditional probabilities of the constituent fragments, and (2) the value of Barlow's criterion of "suspicious coincidence" (the ratio of joint probability to the product of marginals). We then compared the part verification response times for various probe/target combinations before and after the exposure. For composite probes, the speedup was much larger for targets that contained pairs of fragments perfectly predictive of each other, compared to those that did not. This effect was modulated by the significance of their co-occurrence as estimated by Barlow's criterion. For lone-fragment probes, the speedup in all conditions was generally lower than for composites. These results shed light on the brain's strategies for unsupervised acquisition of structural information in vision.	Cornell Univ, Dept Psychol, Ithaca, NY 14853 USA	Cornell University	Edelman, S (corresponding author), Cornell Univ, Dept Psychol, Ithaca, NY 14853 USA.	se37@cornell.edu; bph7@cornell.edu; hy56@cornell.edu; Nathan.Intrator@brown.edu						Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; Berns GS, 1997, SCIENCE, V276, P1272, DOI 10.1126/science.276.5316.1272; BIENENSTOCK E, 1997, NEURAL INFORMATION P, V9; Edelman S, 2000, SPATIAL VISION, V13, P255, DOI 10.1163/156856800741072; Edelman S, 2001, ADV NEUR IN, V13, P10; EDELMAN S, 2000, IN PRESS TRENDS COGN, V6; Fiser J, 2001, PSYCHOL SCI, V12, P499, DOI 10.1111/1467-9280.00392; FLANNAGAN MJ, 1986, J EXP PSYCHOL LEARN, V12, P241, DOI 10.1037/0278-7393.12.2.241; KIRKHAM NZ, 2002, IN PRESS COGNITION; PALMER SE, 1977, COGNITIVE PSYCHOL, V9, P441, DOI 10.1016/0010-0285(77)90016-0; Pomerleau D. A., 1993, P ADV NEUR INF PROC, P279; Saffran JR, 1996, SCIENCE, V274, P1926, DOI 10.1126/science.274.5294.1926; SAS Institute, 1999, US GUID VERS 8; STAINVAS I, 2000, P ICPR, V2, P809; ZEMEL RS, 1995, NEURAL COMPUT, V7, P549, DOI 10.1162/neco.1995.7.3.549	16	9	9	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						19	26						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100003
C	Jaakkola, T; Siegelmann, H		Dietterich, TG; Becker, S; Ghahramani, Z		Jaakkola, T; Siegelmann, H			Active information retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In classical large information retrieval systems, the system responds to a user initiated query with a list of results ranked by relevance. The users may further refine their query as needed. This process may result in a lengthy correspondence without conclusion. We propose an alternative active learning approach, where the system responds to the initial user's query by successively probing the user for distinctions at multiple levels of abstraction. The system's initiated queries are optimized for speedy recovery and the user is permitted to respond with multiple selections or may reject the query. The information is in each case unambiguously incorporated by the system and the subsequent queries are adjusted to minimize the need for further exchange. The system's initiated queries are subject to resource constraints pertaining to the amount of information that can be presented to the user per iteration.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Jaakkola, T (corresponding author), MIT, AI Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.							Atkinson A. C., 1992, OXFORD STAT SCI SERI; CUTTING DR, 1996, P 15 ANN INT ACM SIG; Heckerman D., 1995, LEARNING BAYESIAN NE, V20; LIPSON H, 2000, NEURAL COMPUTATION, V12; ROCCHIO JJ, SMART SYSTEM EXPT AU, V313; SALTON G, 1990, J AM SOC INFORM SCI, V41, P288, DOI 10.1002/(SICI)1097-4571(199006)41:4<288::AID-ASI8>3.0.CO;2-H; YEDIDIA JS, 2001, NEURAL INFORMATION P, V13	7	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						777	784						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100097
C	Schuurmans, D; Patrascu, R		Dietterich, TG; Becker, S; Ghahramani, Z		Schuurmans, D; Patrascu, R			Direct value-approximation for factored MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a simple approach for computing reasonable policies for factored Markov decision processes (MDPs), when the optimal value function can be approximated by a compact linear form. Our method is based on solving a single linear program that approximates the best linear fit to the optimal value function. By applying an efficient constraint generation procedure we obtain an iterative solution method that tackles concise linear programs. This direct linear programming approach experimentally yields a significant reduction in computation time over approximate value- and policy-iteration methods (sometimes reducing several hours to a few seconds). However, the quality of the solutions produced by linear programming is weaker-usually about twice the approximation error for the same approximating class. Nevertheless, the speed advantage allows one to use larger approximation classes to achieve similar error in reasonable time.	Univ Waterloo, Dept Comp Sci, Waterloo, ON N2L 3G1, Canada	University of Waterloo	Schuurmans, D (corresponding author), Univ Waterloo, Dept Comp Sci, Waterloo, ON N2L 3G1, Canada.							Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; Boutilier C., 2000, ARTIFICIAL INTELLIGE; BOYAN J, 1999, P ICML; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; GUESTRIN C, 2001, P IJCAI; KOLLER D, 2000, P UAI; KOLLER D, 1999, P IJCAI; Martin R. K., 1999, LARGE SCALE LINEAR I; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; SALLANS B, 2000, P NIPS; STAUBIN R, 2000, P NIPS; VANROY B, 1998, THESIS MIT; WILLIAMS RJ, 1993, TIGHT PERFORMANCE BO	14	9	9	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1579	1586						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100196
C	Teh, YW; Welling, M		Dietterich, TG; Becker, S; Ghahramani, Z		Teh, YW; Welling, M			The unified propagation and scaling algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In this paper we will show that a restricted class of constrained minimum divergence problems, named generalized inference problems, can be solved by approximating the KL divergence with a Bethe free energy. The algorithm we derive is closely related to both loopy belief propagation and iterative scaling. This unified propagation and scaling algorithm reduces to a convergent alternative to loopy belief propagation when no constraints are present. Experiments show the viability of our algorithm.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada	University of Toronto	Teh, YW (corresponding author), Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd, Toronto, ON M5S 3G4, Canada.		Teh, Yee Whye/C-3400-2008					DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; Deming WE, 1940, ANN MATH STAT, V11, P427, DOI 10.1214/aoms/1177731829; MURPHY K, 1999, P C UNC ART INT, V15; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Welling M., 2013, P 17 C UNC ART INT; YEDIDIA JS, 2000, ADV NEURAL INFORMATI, V13; YUILLE AL, 2002, CCCP ALGORITHMS MINI	7	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						953	960						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100119
C	Torralba, A		Dietterich, TG; Becker, S; Ghahramani, Z		Torralba, A			Contextual modulation of target saliency	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SCENE; REPRESENTATION; ATTENTION; OBJECTS; MODEL	The most popular algorithms for object detection require the use of exhaustive spatial and scale search procedures. In such approaches, an object is defined by means of local features. In this paper we show that including contextual information in object detection procedures provides an efficient way of cutting down the need for exhaustive search. We present results with real images showing that the proposed scheme is able to accurately predict likely object classes, locations and sizes.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Torralba, A (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.	torralba@ai.mit.edu	Zheng, Zhenzhu/F-1081-2011					BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; Carson C, 1997, IEEE WORKSHOP ON CONTENT-BASED ACCESS OF IMAGE AND VIDEO LIBRARIES, PROCEEDINGS, P42, DOI 10.1109/IVL.1997.629719; GERSHNFELD N, 1999, NATURE MATH MODELING; GORKANI MM, 1994, P INT C PAT REC JER, V1, P459; HEISLE B, 2001, P 2001 IEEE COMP SOC; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; RAO RPN, 1996, NIPS 8; Schiele B, 2000, INT J COMPUT VISION, V36, P31, DOI 10.1023/A:1008120406972; STRAT TM, 1991, IEEE T PATTERN ANAL, V13, P1050, DOI 10.1109/34.99238; Szummer M., 1998, IEEE INT WORKSH CONT; TORRALBA A, 2002, IEEE P INT C COMP VI; TREISMAN AM, 1980, COGNITIVE PSYCHOL, V12, P97, DOI 10.1016/0010-0285(80)90005-5; Viola P., 2001, P 2001 IEEE COMP SOC; WOLFE JM, 1994, PSYCHON B REV, V1, P202, DOI 10.3758/BF03200774	16	9	9	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1303	1310						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100162
C	Arleo, A; Smeraldi, F; Hug, S; Gerstner, W		Leen, TK; Dietterich, TG; Tresp, V		Arleo, A; Smeraldi, F; Hug, S; Gerstner, W			Place cells and spatial navigation based on 2D visual feature extraction, path integration, and reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				HEAD-DIRECTION CELLS; HIPPOCAMPAL; MODEL; RATS	We model hippocampal place cells and head-direction cells by combining allothetic (visual) and idiothetic (proprioceptive) stimuli. Visual input, provided by a video camera on a miniature robot, is preprocessed by a set of Gabor filters on 31 nodes of a log-polar retinotopic graph. Unsupervised Hebbian learning is employed to incrementally build a population of localized overlapping place fields. Place cells serve as basis functions for reinforcement learning. Experimental results for goal-oriented navigation of a mobile robot are presented.	Swiss Fed Inst Technol, Ctr Neuromimet Syst, MANTRA, Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Arleo, A (corresponding author), Swiss Fed Inst Technol, Ctr Neuromimet Syst, MANTRA, Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.	angelo.arleo@epfl.ch	Arleo, Angelo/N-9255-2019	Arleo, Angelo/0000-0002-1990-1971				[Anonymous], 1989, LEARNING DELAYED REW; Arleo A, 2000, BIOL CYBERN, V83, P287, DOI 10.1007/s004220000171; Arleo A, 2000, FROM ANIM ANIMAT, P236; DAUGMAN JG, 1980, VISION RES, V20, P847, DOI 10.1016/0042-6989(80)90065-6; DAYAN P, 1991, ADV NEURAL INFORMATI, V3, P464; Foster DJ, 2000, HIPPOCAMPUS, V10, P1, DOI 10.1002/(SICI)1098-1063(2000)10:1<1::AID-HIPO1>3.0.CO;2-1; Franz MO, 1998, AUTON ROBOT, V5, P111, DOI 10.1023/A:1008821210922; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; KNIERIM JJ, 1995, J NEUROSCI, V15, P1648, DOI 10.1523/JNEUROSCI.15-03-01648.1995; McCallum RA, 1996, IEEE T SYST MAN CY B, V26, P464, DOI 10.1109/3477.499796; MORRIS RGM, 1982, NATURE, V297, P681, DOI 10.1038/297681a0; O'Keefe J, 1978, HIPPOCAMPUS COGNITIV; OKeefe J, 1996, NATURE, V381, P425, DOI 10.1038/381425a0; Salinas E, 1994, J Comput Neurosci, V1, P89, DOI 10.1007/BF00962720; SMERALDI F, 2000, P S SWED SOC AUT IM, P87; Smeraldi F., 1999, 11 SCAND C IM AN, P39; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TAUBE JS, 1990, J NEUROSCI, V10, P420; WILSON MA, 1993, SCIENCE, V261, P1055, DOI 10.1126/science.8351520	20	9	9	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						89	95						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800013
C	Edelman, S; Intrator, N		Leen, TK; Dietterich, TG; Tresp, V		Edelman, S; Intrator, N			A productive, systematic framework for the representation of visual structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				CORTEX; FEATURES; OBJECTS; RECOGNITION; MODELS	We describe a unified framework for the understanding of structure representation in primate vision. A model derived from this framework is shown to be effectively systematic in that it has the ability to interpret and associate together objects that are related through a rearrangement of common "middle-scale" parts, represented as image fragments. The model addresses the same concerns as previous work on compositional representation through the use of what+where receptive fields and attentional gain modulation. It does not require prior exposure to the individual parts, and avoids the need for abstract symbolic binding.	Cornell Univ, Dept Psychol, Ithaca, NY 14853 USA	Cornell University	Edelman, S (corresponding author), Cornell Univ, Dept Psychol, 232 Uris Hall, Ithaca, NY 14853 USA.							Bar M, 1998, PSYCHOL SCI, V9, P464, DOI 10.1111/1467-9280.00086; BIENENSTOCK E, 1997, NIPS, V9; BURL MC, 1998, LNCS SERIES, V1406, P628; Conner CE, 1997, J NEUROSCI, V17, P3201; DENEVE S, 1998, NIPS, V11; Edelman S, 2000, SPATIAL VISION, V13, P255, DOI 10.1163/156856800741072; Edelman S., 1999, REPRESENTATION RECOG; EDELMAN S, 1998, 500 CSRP U SUSS; Edelman S, 1997, TRENDS COGN SCI, V1, P296, DOI 10.1016/S1364-6613(97)01090-5; FUJITA I, 1992, NATURE, V360, P343, DOI 10.1038/360343a0; Hadley RF, 1997, MIND LANG, V12, P137, DOI 10.1111/1468-0017.00040; Heeger DJ, 1996, P NATL ACAD SCI USA, V93, P623, DOI 10.1073/pnas.93.2.623; HUMMEL JE, 2000, COGNITIVE DYNAMICS C, pCH7; KOBATAKE E, 1994, J NEUROPHYSIOL, V71, P856, DOI 10.1152/jn.1994.71.3.856; Love BC, 1999, COGNITIVE PSYCHOL, V38, P291, DOI 10.1006/cogp.1998.0697; NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3; POMERLEAU D, 1993, NIPS, V5, P279; Rao SC, 1997, SCIENCE, V276, P821, DOI 10.1126/science.276.5313.821; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Salinas E, 1997, J NEUROPHYSIOL, V77, P3267, DOI 10.1152/jn.1997.77.6.3267; STAINVAS I, 2000, IMPROVING RECOGNITIO; TREISMAN A, 1992, AM PSYCHOL, V47, P862, DOI 10.1037/0003-066X.47.7.862; Treisman AM, 1998, CURR OPIN NEUROBIOL, V8, P218, DOI 10.1016/S0959-4388(98)80143-8	23	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						10	16						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800002
C	Kjems, U; Hansen, LK; Strother, SC		Leen, TK; Dietterich, TG; Tresp, V		Kjems, U; Hansen, LK; Strother, SC			Generalizable singular value decomposition for ill-posed datasets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We demonstrate that statistical analysis of ill-posed data sets is subject to a bias, which can be observed when projecting independent test set examples onto a basis defined by the training examples. Because the training examples in an ill-posed data set do not fully span the signal space the observed training set variances in each basis vector will be too high compared to the average variance of the test set projections onto the same basis vectors. On basis of this understanding we introduce the Generalizable Singular Value Decomposition (GenSVD) as a means to reduce this bias by re-estimation of the singular values obtained in a conventional Singular Value Decomposition, allowing for a generalization performance increase of a subsequent statistical model. We demonstrate that the algorithm succesfully corrects bias in a data set from a functional PET activation study of the human brain.	Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark	Technical University of Denmark	Kjems, U (corresponding author), Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark.		Strother, Stephen/ABF-3371-2021; Strother, Stephen C/D-6752-2011; Hansen, Lars/E-3174-2013	Strother, Stephen/0000-0002-3198-217X; Strother, Stephen C/0000-0002-3198-217X; 				BISHOP C, 1999, ADV NEURAL INFORMATI, V11; Hansen LK, 1999, NEUROIMAGE, V9, P534, DOI 10.1006/nimg.1998.0425; HANSEN LK, 1996, P IEEE INT C NEUR NE, V1, P25; Jackson J.E., 1991, WILEY SERIES PROBABI; LAUTRUP B, 1995, P WORKSH SUP BRAIN R, P137; ROWEIS S, 1998, ADV NEURAL INFORMATI, V10	6	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						549	+						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800078
C	Schwartz, O; Simoncelli, EP		Leen, TK; Dietterich, TG; Tresp, V		Schwartz, O; Simoncelli, EP			Natural sound statistics and divisive normalization in the auditory system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NEURONS; CORTEX; SUPPRESSION; INTENSITY; RESPONSES; SCENES; NERVE; CELLS; CAT	We explore the statistical properties of natural sound stimuli preprocessed with a bank of linear filters. The responses of such filters exhibit a striking form of statistical dependency, in which the response variance of each filter grows with the response amplitude of filters tuned for nearby frequencies. These dependencies may be substantially reduced using an operation known as divisive normalization, in which the response of each filter is divided by a weighted sum of the rectified responses of other filters. The weights may be chosen to maximize the independence of the normalized responses for an ensemble of natural sounds. We demonstrate that the resulting model accounts for nonlinearities in the response characteristics of the auditory nerve, by comparing model simulations to electrophysiological recordings. In previous work (NIPS, 1998) we demonstrated that an analogous model derived from the statistics of natural images accounts for non-linear properties of neurons in primary visual cortex. Thus, divisive normalization appears to be a generic mechanism for eliminating a type of statistical dependency that is prevalent in natural signals of different modalities.	NYU, Ctr Neural Sci, New York, NY 10003 USA	New York University	Schwartz, O (corresponding author), NYU, Ctr Neural Sci, New York, NY 10003 USA.	odelia@cns.nyu.edu; eero.simoncelli@nyu.edu		Simoncelli, Eero/0000-0002-1206-527X				Attias H, 1997, ADV NEUR IN, V9, P27; Attias H, 1998, ADV NEUR IN, V10, P103; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Bell AJ, 1996, NETWORK-COMP NEURAL, V7, P261, DOI 10.1088/0954-898X/7/2/005; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Carandini M, 1997, J NEUROSCI, V17, P8621; Dan Y, 1996, J NEUROSCI, V16, P3351; GEISLER DC, 1998, SOUND SYNAPSE PHYSL; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; JAVEL E, 1978, J ACOUST SOC AM, V63, P1093, DOI 10.1121/1.381817; LYON RF, 1990, LECT NOTES BIOMATH, V87, P395; Nelken I, 1999, NATURE, V397, P154, DOI 10.1038/16456; Olshausen BA, 1996, NETWORK-COMP NEURAL, V7, P333, DOI 10.1088/0954-898X/7/2/014; ROSE JE, 1971, J NEUROPHYSIOL, V34, P685, DOI 10.1152/jn.1971.34.4.685; RUDERMAN DL, 1994, PHYS REV LETT, V73, P814, DOI 10.1103/PhysRevLett.73.814; Simoncelli EP, 1999, ADV NEUR IN, V11, P153; SIMONCELLI EP, 1999, P SPIE, V3813; Slaney M, 1993, 35 APPL; VINJE WE, 2000, SCIENCE          FEB, P287; WAINWRIGHT MJ, 2001, IN PRESS STAT THEORI; Wang KS, 1994, IEEE T SPEECH AUDI P, V2, P421, DOI 10.1109/89.294356; Zhao HB, 1999, NATURE, V399, P359, DOI 10.1038/20686	23	9	9	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						166	172						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800024
C	Shelton, CR		Leen, TK; Dietterich, TG; Tresp, V		Shelton, CR			Balancing multiple sources of reward in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					For many problems which would be natural for reinforcement learning, the reward signal is not a single scalar value but has multiple scalar components. Examples of such problems include agents with multiple goals and agents with multiple users. Creating a single reward value by combining the multiple components can throw away vital information and can lead to incorrect solutions. We describe the multiple reward source problem and discuss the problems with applying traditional reinforcement learning. We then present an new algorithm for finding a solution and results on simulated environments.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Shelton, CR (corresponding author), MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA.		Shelton, Christian/GQJ-1146-2022	Shelton, Christian/0000-0001-6698-7838				HU J, 1998, P 15 INT C MACH LEAR, P242; ISBELL CL, 2001, UNPUB AUTONOMOUS AGE; Karlsson J., 1997, LEARNING SOLVE MULTI; KEARNS M, 2000, P 16 C UNC ART INT; LITTMAN ML, 1994, P 11 INT C MACH LEAR, P157; Owen G., 1995, GAME THEORY; Singh S, 2000, P 16 C UNC ART INT; SINGH SP, 1992, NIPS, V4	8	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1082	1088						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800152
C	Xie, XH; Hahnloser, R; Seung, HS		Leen, TK; Dietterich, TG; Tresp, V		Xie, XH; Hahnloser, R; Seung, HS			Learning winner-take-all competition between groups of neurons in lateral inhibitory networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					It has long been known that lateral inhibition in neural networks can lead to a winner-take-all competition, so that only a single neuron is active at a steady state. Here we show how to organize lateral inhibition so that groups of neurons compete to be active. Given a collection of potentially overlapping groups, the inhibitory connectivity is set by a formula that can be interpreted as arising from a simple learning rule. Our analysis demonstrates that such inhibition generally results in winner-take-all competition between the given groups, with the exception of some degenerate cases. In a broader context, the network serves as a particular illustration of the general distinction between permitted and forbidden sets, which was introduced recently. From this viewpoint, the computational function of our network is to store and retrieve memories as permitted sets of coactive neurons.	MIT, E25 210, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Xie, XH (corresponding author), MIT, E25 210, 77 Massachusetts Ave, Cambridge, MA 02139 USA.			Hahnloser, Richard/0000-0002-4039-7773; Xie, Xiaohui/0000-0002-5479-6345				AMARI S.- I., 1977, COMPETITION COOPERAT, P119; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Feng J, 1996, J PHYS A-MATH GEN, V29, P5019, DOI 10.1088/0305-4470/29/16/023; HAHNLOSER R, 2000, NATURE, V3, P609; Hahnloser RLT, 1998, NEURAL NETWORKS, V11, P691, DOI 10.1016/S0893-6080(98)00012-4; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; Kohonen T., 1989, SELF ORG ASSOCIATIVE, V3rd; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0	9	9	9	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						350	356						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800050
C	Chklovskii, DB; Stevens, CF		Solla, SA; Leen, TK; Muller, KR		Chklovskii, DB; Stevens, CF			Wiring optimization in the brain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				COMPONENT PLACEMENT; CORTICAL MAPS; ORGANIZATION	The complexity of cortical circuits may be characterized by the number of synapses per neuron. We study the dependence of complexity on the fraction of the cortical volume that is made up of "wire" (that is, of axons and dendrites), and find that complexity is maximized when wire takes up about 60% of the cortical volume. This prediction is in good agreement with experimental observations. A consequence of our arguments is that any rearrangement of neurons that takes more wire would sacrifice computational power.	Salk Inst Biol Studies, Sloan Ctr Theoret Neurobiol, La Jolla, CA 92037 USA	Salk Institute	Chklovskii, DB (corresponding author), Salk Inst Biol Studies, Sloan Ctr Theoret Neurobiol, La Jolla, CA 92037 USA.	mitya@salk.edu; stevens@salk.edu						ALLMAN JM, 1974, BRAIN RES, V76, P247, DOI 10.1016/0006-8993(74)90458-2; BEKKERS JM, 1990, PROG BRAIN RES, V83, P37; Braitenberg VB, 1998, CORTEX STAT GEOMETRY; Cajal S. R., 1995, HISTOLOGY NERVOUS SY; CHERNIAK C, 1994, J NEUROSCI, V14, P2418; CHERNIAK C, 1995, TRENDS NEUROSCI, V18, P522, DOI 10.1016/0166-2236(95)98373-7; CHERNIAK C, 1992, BIOL CYBERN, V66, P503, DOI 10.1007/BF00204115; COWEY A, 1979, Q J EXP PSYCHOL, V31, P1, DOI 10.1080/14640747908400703; DURBIN R, 1990, NATURE, V343, P644, DOI 10.1038/343644a0; kandel, 1977, NERVOUS SYSTEM, V1; MITCHISON G, 1991, P ROY SOC B-BIOL SCI, V245, P151, DOI 10.1098/rspb.1991.0102; MITCHISON G, 1992, TRENDS NEUROSCI, V15, P122, DOI 10.1016/0166-2236(92)90352-9; RUSHTON WAH, 1951, J PHYSIOL-LONDON, V115, P101, DOI 10.1113/jphysiol.1951.sp004655; YOUNG MP, 1992, NATURE, V358, P152, DOI 10.1038/358152a0	14	9	9	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						103	107						5	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700015
C	Liu, SC		Solla, SA; Leen, TK; Muller, KR		Liu, SC			A winner-take-all circuit with controllable soft max property	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SYSTEM	I describe a silicon network consisting of a group of excitatory neurons and a global inhibitory neuron. The output of the inhibitory neuron is normalized with respect to the input strengths. This output models the normalization property of the wide-field direction-selective cells in the fly visual system. This normalizing property is also useful in any system where we wish the output signal to code only the strength of the inputs, and not be dependent on the number of inputs. The circuitry in each neuron is equivalent to that in Lazzaro's winner-take-all (WTA) circuit with one additional transistor and a voltage reference. Just as in Lazzaro's circuit, the outputs of the excitatory neurons code the neuron with the largest input. The difference here is that multiple winners can be chosen. By varying the voltage reference of the neuron, the network can transition between a soft-max behavior and a hard WTA behavior. I show results from a fabricated chip of 20 neurons in a 1.2 mum CMOS technology.	Swiss Fed Inst Technol, UNIZ, Inst Neuroinformat, CH-8057 Zurich, Switzerland	ETH Zurich	Liu, SC (corresponding author), Swiss Fed Inst Technol, UNIZ, Inst Neuroinformat, Winterthurstr 190, CH-8057 Zurich, Switzerland.	shih@ini.phys.ethz.ch						Amari S., 1982, COMPETITION COOPERAT; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4; HAHNLOSER R, 1998, THESIS SWISS FEDERAL; HANHLOSER R, 1988, NEURAL NETWORKS, V11, P691; Indiveri G, 1998, ANALOG CIRCUITS SIG, P367; Lazzaro J., 1988, ADV NEURAL INFORMATI, V1, P703; Morris TG, 1998, IEEE T CIRCUITS-II, V45, P1564, DOI 10.1109/82.746669; REICHARDT W, 1983, BIOL CYBERN, V46, P1, DOI 10.1007/BF00595226	9	9	9	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						717	723						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700102
C	Tang, AC; Pearlmutter, BA; Hely, TA; Zibulevsky, M; Weisend, MP		Solla, SA; Leen, TK; Muller, KR		Tang, AC; Pearlmutter, BA; Hely, TA; Zibulevsky, M; Weisend, MP			An MEG study of response latency and variability in the human visual system during a visual-motor integration task	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SPIKE TRAINS; NEOCORTICAL NEURONS; BLIND SEPARATION; PRECISION; MONKEYS; CORTEX; INPUT	Human reaction times during sensory-motor tasks vary considerably. To begin to understand how this variability arises, we examined neuronal populational response time variability at early versus late visual processing stages. The conventional view is that precise temporal information is gradually lost as information is passed through a layered network of mean-rate "units." We tested in hu mans whether neuronal populations at different processing stages behave like mean-rate "units". A blind source separation algorithm was applied to MEG signals from sensory-motor integration tasks. Response time latency and variability for multiple visual sources were estimated by detecting single-trial stimulus-locked events for each source. In two subjects tested on four visual reaction time tasks, we reliably identified sources belonging to early and late visual processing stages. The standard deviation of response latency was smaller for early rather than late processing stages. This supports the hypothesis that human populational response time variability increases from early to late visual processing stages.	Univ New Mexico, Dept Psychol, Albuquerque, NM 87131 USA	University of New Mexico	Tang, AC (corresponding author), Univ New Mexico, Dept Psychol, Albuquerque, NM 87131 USA.	akaysha@unm.edu; bap@cs.unm.edu; timhely@santafe.edu; michael@cs.unm.edu; mweisend@unm.edu	Pearlmutter, Barak A./AAL-8999-2020; Pearlmutter, Barak A/M-8791-2014	Pearlmutter, Barak A/0000-0003-0521-4553				ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629; BAIR W, 1996, NEURAL COMPUT, V8, P1184; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Belouchrani A., 1993, PROC INT C DIGITAL S, P346; Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411; Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250; deCharms RC, 1996, NATURE, V381, P610, DOI 10.1038/381610a0; Gur M, 1997, J NEUROSCI, V17, P2914; HYVARINEN A, 1997, NEURAL COMPUTATION, V9; Jung TP, 1999, ADV NEUR IN, V11, P118; JUNG TP, 1999, IN PRESS PSYCHOPHYSI; Lewine J.D., 1995, FUNCT BRAIN IMAG, P369; MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778; Makeig S, 1997, P NATL ACAD SCI USA, V94, P10979, DOI 10.1073/pnas.94.20.10979; Marsalek PR, 1997, P NATL ACAD SCI USA, V94, P735, DOI 10.1073/pnas.94.2.735; Reich DS, 1997, J NEUROPHYSIOL, V77, P2836, DOI 10.1152/jn.1997.77.5.2836; Tang AC, 1997, CEREB CORTEX, V7, P502, DOI 10.1093/cercor/7.6.502; TANG AC, 1999, IN PRESS NEUROCOMPUT; vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805; VIGARIO R, 1998, ADV NEURAL INFORMATI, V10	20	9	9	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						185	191						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700027
C	Deneve, S; Pouget, A; Latham, PE		Kearns, MS; Solla, SA; Cohn, DA		Deneve, S; Pouget, A; Latham, PE			Divisive normalization, line attractor networks and ideal observers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				RESPONSES	Gain control by divisive inhibition, a.k.a. divisive normalization. has been proposed to be a general mechanism throughout the visual cortex. We explore in this study the statistical properties of tl lis normalization in the presence of noise. Using simulations, we show that divisive normalization is a close approximation to a maximum likelihood estimator, which, in the context of population coding, is the same as an ideal observer. We also demonstrate analytically that this is a general property of a large class of nonlinear recurrent networks with line attractors. Our work suggests that divisive normalization plays a critical role in noise filtering, and that every cortical layer may be an ideal observer of the activity in the preceding layer.	Georgetown Univ, Georgetown Inst Computat & Cognit Sci, Washington, DC 20007 USA	Georgetown University	Deneve, S (corresponding author), Georgetown Univ, Georgetown Inst Computat & Cognit Sci, Washington, DC 20007 USA.							HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; ITTI L, 1998, ADV NEURAL INFORMATI, V11; Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809; Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1	4	9	9	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						104	110						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700015
C	Etienne-Cummings, R; Gruev, V; Ghani, MA		Kearns, MS; Solla, SA; Cohn, DA		Etienne-Cummings, R; Gruev, V; Ghani, MA			VLSI implementation of motion centroid localization for autonomous navigation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					A circuit for fast, compact and low-power focal-plane motion centroid localization is presented. This chip, which uses mixed signal CMOS components to implement photodetection, edge detection, ON-set detection and centroid localization, models the retina and superior colliculus. The centroid localization circuit uses time-windowed asynchronously triggered row and column address events and two linear resistive grids to provide the analog coordinates of the motion centroid. This VLSI chip is used to realize fast lightweight autonavigating vehicles. The obstacle avoiding line-following algorithm is discussed.	Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA	Johns Hopkins University	Etienne-Cummings, R (corresponding author), Johns Hopkins Univ, Dept Elect & Comp Engn, Baltimore, MD 21218 USA.		Etienne-Cummings, Ralph/A-3227-2010					[Anonymous], 1989, ANALOG VLSI IMPLEMEN; BARLOW H, 1982, SENSES PHYSL RETINA; BOAHEN K, 1996, ISCAS 96 ATLANTA GA; DEWEERTH SP, 1992, INT J COMPUT VISION, V8, P191, DOI 10.1007/BF00055151; ETIENNECUMMINGS R, 1998, NEUROMORPHIC SYSTEMS; HORIUCHI T, 1996, ADV NEURAL INFORMATI, V9; KOCH C, 1995, VISION CHIPS IMPLEME; MANSFIELD P, 1996, LASER FOCUS WORLD, V30, pS21; SPARKS D, 1990, P COLD SPRING HARB S, V55	9	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						685	691						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700097
C	McGovern, A; Moss, E		Kearns, MS; Solla, SA; Cohn, DA		McGovern, A; Moss, E			Scheduling straight-line code using reinforcement learning and rollouts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The execution order of a block of computer instructions can make a difference in its running time by a factor of two or more. In order to achieve the best possible speed, compilers use heuristic schedulers appropriate to each specific architecture implementation. However, these heuristic schedulers are time-consuming and expensive to build. In this paper, we present results using both rollouts and reinforcement learning to construct heuristics for scheduling basic blocks. The rollout scheduler outperformed a commercial scheduler, and the reinforcement learning scheduler performed almost as well as the commercial scheduler.	Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	McGovern, A (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.	amy@cs.umass.edu; moss@cs.umass.edu						BERTSEKAS DP, 1997, P 35 ALL C COMM CO 3; BERTSKAS DP, 1997, J HEURISTICS; *DEC, 1992, DEC CHIP 21064 AA MI; MOSS JEB, 1997, P ADV NEUR INF PROC, P10; SCHEEFF D, 1997, APPLYING REINFORCEME; Sites R. L., 1992, ALPHA ARCHITECTURE R; SRIVASTAVA A, 1994, P SIGPLAN 94 C PROGR, P196; STEFANOVIC D, 1997, CHARACTER INSTRUCTIO; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TESAURO G, 1996, ADV NEURAL INFORMATI	11	9	9	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						903	909						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700127
C	Sharma, RK; Leen, TK; Pavel, M		Kearns, MS; Solla, SA; Cohn, DA		Sharma, RK; Leen, TK; Pavel, M			Probabilistic image sensor fusion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We present a probabilistic method for fusion of images produced hy multiple sensors. The approach is based on an image formation model in which the sensor images are noisy, locally linear functions of an underlying, true scene. A Bayesian framework then provides for maximum likelihood or maximum a posteriori estimates of the true scene from the sensor images. Maximum likelihood estimates of the parameters of the image formation model involve (local) second order image statistics, and thus are related to local principal component analysis. We demonstrate the efficacy of the method on images from visible-band and infrared sensors.	Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, Portland, OR 97291 USA		Sharma, RK (corresponding author), Oregon Grad Inst Sci & Technol, Dept Elect & Comp Engn, POB 91000, Portland, OR 97291 USA.	ravi@ece.ogi.edu; tleen@cse.ogi.edu; pavel@ece.ogi.edu						Basilevsky A.T., 1994, STAT FACTOR ANAL REL; BURT PJ, 1993, INT C COMP VIS, P173, DOI DOI 10.1109/ICCV.1993.378222; Clark J.J., 1990, DATA FUSION SENSORY; KERR JR, 1995, P SOC PHOTO-OPT INS, V2463, P38, DOI 10.1117/12.212753; Klein L., 1993, SENSOR DATA FUSION C; Li HH, 1996, OPT ENG, V35, P391, DOI 10.1117/1.600908; Pavel M, 1997, P SOC PHOTO-OPT INS, V3088, P169, DOI 10.1117/12.277231; PAVEL M, 1992, P SOC INFORMATION DI, P475; ROBERTS B, 1994, P SOC PHOTO-OPT INS, V2220, P246, DOI 10.1117/12.179609; TIPPING M, 1997, NCRG97010 AST U; Toet A., 1990, Machine Vision and Applications, V3, P1, DOI 10.1007/BF01211447	11	9	11	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						824	830						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700116
C	Sutton, RS; Singh, S; Precup, D; Ravindran, B		Kearns, MS; Solla, SA; Cohn, DA		Sutton, RS; Singh, S; Precup, D; Ravindran, B			Improved switching among temporally abstract actions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In robotics and other control applications it is commonplace to have a preexisting set of controllers for solving subtasks, perhaps hand-crafted or previously learned or planned, and still face a difficult problem of how to choose and switch among the controllers to solve an overall task as well as possible. In this paper we present a Framework based on Markov decision processes and semi-Markov decision processes for phrasing this problem, a basic theorem regarding the improvement in performance that can be obtained by switching flexibly between given controllers, and example applications of the theorem. In particular, we show how an agent can plan with these high-level controllers and then use the results of such planning to find an even better plan, by modifying the existing controllers, with negligible additional cost and no re-planning. In one of our examples, the complexity of the problem is reduced from 24 billion state-action pairs to less than a million state-controller pairs.	AT&T Labs Res, Florham Park, NJ 07932 USA	AT&T	Sutton, RS (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Park, NJ 07932 USA.			Ravindran, Balaraman/0000-0002-5364-7639				BRADTKE SJ, 1995, NIPS, V7, P393; DITTERICH TG, 1998, P 15 INT C MACH LEAR; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Kaelbling LP., 1993, P 10 INT C MACHINE L, P951; Mahadevan S., 1997, P 14 INT C MACH LEAR, P202; Mcgovern A., 1997, G HOPPER CELEBRATION, P13; PARR R, 1998, NIPS, V10; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; SCHWARTZ W, 1985, SOC WORK GROUPS, V8, P7; Singh SP., 1992, P AAAI, P202; Sutton R. S., 1998, ICML 1998; SUTTON RS, 1998, 9874 TR U MASS DEP C; SUTTON RS, 1995, P 12 INT C MACH LEAR, P531	13	9	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1066	1072						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700150
C	Tanaka, T		Kearns, MS; Solla, SA; Cohn, DA		Tanaka, T			A theory of mean field approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BOLTZMANN MACHINES; SPIN-GLASS; MODEL	I present a theory of mean field approximation based on information geometry. This theory includes in a consistent way the naive mean field approximation, as well as the TAP approach and the linear response theorem in statistical physics, giving clear information-theoretic interpretations to them.	Tokyo Metropolitan Univ, Dept Elect & Informat Engn, Hachioji, Tokyo 1920397, Japan	Tokyo Metropolitan University	Tanaka, T (corresponding author), Tokyo Metropolitan Univ, Dept Elect & Informat Engn, 1-1 Minami Osawa, Hachioji, Tokyo 1920397, Japan.		Tanaka, Toshiyuki/C-2749-2011					ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; AMARI S, 1992, IEEE T NEURAL NETWOR, V3, P260, DOI 10.1109/72.125867; Amari S., 1985, LECT NOTES STAT, V28; GALLAND CC, 1993, NETWORK-COMP NEURAL, V4, P355, DOI 10.1088/0954-898X/4/3/007; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; Kappen HJ, 1998, ADV NEUR IN, V10, P280; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Peterson C., 1987, Complex Systems, V1, P995; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302; Tanaka T, 1998, ICONIP'98: THE FIFTH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING JOINTLY WITH JNNS'98: THE 1998 ANNUAL CONFERENCE OF THE JAPANESE NEURAL NETWORK SOCIETY - PROCEEDINGS, VOLS 1-3, P554; TANAKA T, INFORMATION GEOMETRY; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992	14	9	10	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						351	357						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700050
C	Zemel, RS; Dayan, P		Kearns, MS; Solla, SA; Cohn, DA		Zemel, RS; Dayan, P			Distributional population codes and multiple motion models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				MOVEMENT DIRECTION; TRANSPARENT MOTION; VISUAL-MOTION; RESPONSES	Most theoretical and empirical studies of population codes make the assumption that underlying neuronal activities is a unique and unambiguous value of an encoded quantity. However, population activities can contain additional information about such things as multiple values of or uncertainty about the quantity. We have previously suggested a method to recover extra information by treating the activities of the population of cells as coding for a complete distribution over the coded quantity rather than just a single value. We now show how this approach bears on psychophysical and neurophysiological studies of population codes for motion direction in tasks involving transparent motion stimuli. We show that, unlike standard approaches, it is able to recover multiple motions from population responses, and also that its output is consistent with both correct and erroneous human performance on psychophysical tasks.	Univ Arizona, Tucson, AZ 85721 USA	University of Arizona	Zemel, RS (corresponding author), Univ Arizona, Tucson, AZ 85721 USA.							ANDERSON CH, 1995, 19 INT WORKSH COND M; Basso MA, 1998, J NEUROSCI, V18, P7519; Bastian A, 1998, NEUROREPORT, V9, P315, DOI 10.1097/00001756-199801260-00025; BRITTEN KH, 1992, J NEUROSCI, V12, P4745; Grunewald A, 1996, ADV NEUR IN, V8, P837; HOL K, 1997, SOC NEUR ABSTR, V23, P179; MATHER G, 1983, Q J EXP PSYCHOL-A, V35, P513, DOI 10.1080/14640748308402485; RAUBER HJ, 1997, SOC NEUR ABSTR, V23, P179; Recanzone GH, 1997, J NEUROPHYSIOL, V78, P2904, DOI 10.1152/jn.1997.78.6.2904; Shadlen MN, 1996, J NEUROSCI, V16, P1486; SNIPPE HP, 1996, NEURAL COMPUT, V8, P29; vanWezel RJA, 1996, VISION RES, V36, P2805, DOI 10.1016/0042-6989(95)00324-X; WILLIAMS D, 1991, VISION RES, V31, P275, DOI 10.1016/0042-6989(91)90118-O; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818	14	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						174	180						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700025
C	De Bonet, JS; Viola, P		Jordan, MI; Kearns, MJ; Solla, SA		De Bonet, JS; Viola, P			A non-parametric multi-scale statistical model for natural images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The observed distribution of natural images is far from uniform. On the contrary, real images have complex and important structure that can be exploited for image processing, recognition and analysis. There have been many proposed approaches to the principled statistical modeling of images, but each has been limited in either the complexity of the models or the complexity of the images. We present a non-parametric multi-scale statistical model for images that can be used for recognition, image de-noising, and in a "generative mode" to synthesize high quality textures.	MIT, Artificial Intelligence Lab, Learning & Vis Grp, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	De Bonet, JS (corresponding author), MIT, Artificial Intelligence Lab, Learning & Vis Grp, 545 Technol Sq, Cambridge, MA 02139 USA.	jsd@ai.mit.edu; viola@ai.mit.edu							0	9	9	0	3	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						773	779						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700109
C	Hofmann, T; Buhmann, JM		Jordan, MI; Kearns, MJ; Solla, SA		Hofmann, T; Buhmann, JM			Active data clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Active data clustering is a novel technique for clustering of proximity data which utilizes principles from sequential experiment design in order to interleave data generation and data analysis. The proposed active data sampling strategy is based on the expected value of information, a concept rooting in statistical decision theory. This is considered to be an important step towards the analysis of large-scale data sets, because it offers a way to overcome the inherent data sparseness of proximity data. We present applications to unsupervised texture segmentation in computer vision and information retrieval in document databases.	MIT, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Hofmann, T (corresponding author), MIT, Ctr Biol & Computat Learning, 77 Massachusetts Ave, Cambridge, MA 02139 USA.		Buhmann, Joachim/AAU-4760-2020						0	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						528	534						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700075
C	Leen, TK; Schottky, B; Saad, D		Jordan, MI; Kearns, MJ; Solla, SA		Leen, TK; Schottky, B; Saad, D			Two approaches to optimal annealing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We employ both master equation and order parameter approaches to analyze the asymptotic dynamics of on-line learning with different learning rate annealing schedules. We examine the relations between the results obtained by the two approaches and obtain new results on the optimal decay coefficients and their dependence on the number of hidden nodes in a two layer architecture.	Oregon Grad Inst Sci & Technol, Dept Comp Engn & Sci, Portland, OR 97291 USA		Leen, TK (corresponding author), Oregon Grad Inst Sci & Technol, Dept Comp Engn & Sci, POB 91000, Portland, OR 97291 USA.								0	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						301	307						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700043
C	Meila, M; Jordan, MI		Jordan, MI; Kearns, MJ; Solla, SA		Meila, M; Jordan, MI			Estimating dependency structure as a hidden variable	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper introduces a probability model, the mixture of trees that can account for sparse, dynamically changing dependence relationships. We present a family of efficient algorithms that use EM and the Minimum Spanning Tree algorithm to find the ML and MAP mixture of trees for a variety of priors, including the Dirichlet and the MDL priors.	MIT, Ctr Biol & Computat Learning, Cambridge, MA 02142 USA	Massachusetts Institute of Technology (MIT)	Meila, M (corresponding author), MIT, Ctr Biol & Computat Learning, 45 Carleton St E25-201, Cambridge, MA 02142 USA.		Jordan, Michael I/C-5253-2013						0	9	9	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						584	590						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700083
C	Munos, R; Bourgine, P		Jordan, MI; Kearns, MJ; Solla, SA		Munos, R; Bourgine, P			Reinforcement learning for continuous stochastic control problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper is concerned with the problem of Reinforcement Learning (RL) for continuous state space and time stochastic control problems. We state the Hamilton-Jacobi-Bellman equation satisfied by the value function and use a Finite-Difference method for designing a convergent approximation scheme. Then we propose a RL algorithm based on this scheme and prove its convergence to the optimal solution.	LISC, CEMAGREF, F-92185 Antony, France	INRAE; UDICE-French Research Universities; Universite Paris Saclay	Munos, R (corresponding author), LISC, CEMAGREF, Parc Tourvoie,BP 121, F-92185 Antony, France.	Remi.Munos@cemagref.fr; Bourgine@poly.polytechnique.fr							0	9	9	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1029	1035						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700145
C	Vasconcelos, N; Lippman, A		Jordan, MI; Kearns, MJ; Solla, SA		Vasconcelos, N; Lippman, A			Multiresolution tangent distance for affine-invariant classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The ability to rely on similarity metrics invariant to image transformations is an important issue for image classification tasks such as face or character recognition. We analyze an invariant metric that has performed well for the latter - the tangent distance - and study its limitations when applied to regular images, showing that the most significant among these (convergence to local minima) can be drastically reduced by computing the distance in a multiresolution setting. This leads to the multiresolution tangent distance, which exhibits significantly higher invariance to image transformations, and can be easily combined with robust estimation procedures.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Vasconcelos, N (corresponding author), MIT, Media Lab, 20 Ames St,E15-320M, Cambridge, MA 02139 USA.			Vasconcelos, Nuno/0000-0002-9024-4302					0	9	9	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						843	849						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700119
C	Cohn, DA		Mozer, MC; Jordan, MI; Petsche, T		Cohn, DA			Minimizing statistical bias with queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				I describe a querying criterion that attempts to minimize the error of a learner by minimizing its estimated squared bias. I describe experiments with locally-weighted regression on two simple problems, and observe that this ''bias-only'' approach outperforms the more common ''variance-only'' exploration approach, even in the presence of noise.			Cohn, DA (corresponding author), HARLEQUIN INC,ADAPT SYST GRP,1 CAMBRIDGE CTR,CAMBRIDGE,MA 02142, USA.								0	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						417	423						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00059
C	Orr, GB; Leen, TK		Mozer, MC; Jordan, MI; Petsche, T		Orr, GB; Leen, TK			Using curvature information for fast stochastic search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present an algorithm for fast stochastic gradient descent that uses a nonlinear adaptive momentum scheme tea optimize the late time convergence rate. The algorithm makes effective use of curvature information, requires only O(n) storage and computation, and delivers convergence rates close to the theoretical optimum. We demonstrate the technique on linear and large nonlinear back-prop networks.			Orr, GB (corresponding author), WILLAMETTE UNIV,DEPT COMP SCI,900 STATE ST,SALEM,OR 97301, USA.								0	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						606	612						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00086
C	Pollack, JB; Blair, AD		Mozer, MC; Jordan, MI; Petsche, T		Pollack, JB; Blair, AD			Why did TD-Gammon work?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Although TD-Gammon is one of the major successes in machine learning, it has not led to similar impressive breakthroughs in temporal difference learning for other applications or even other games, We were able to replicate some of the success of TD-Gammon, developing a competitive evaluation function on a 4000 parameter feed-forward neural network, without using back-propagation, reinforcement or temporal difference learning methods. Instead we apply simple hill-climbing in a relative fitness environment. These results and further analysis suggest that the surprising success of Tesauro's program had more to do with the co-evolutionary structure of the learning task and the dynamics of the backgammon game itself.			Pollack, JB (corresponding author), BRANDEIS UNIV,DEPT COMP SCI,WALTHAM,MA 02254, USA.								0	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						10	16						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00002
C	Kershaw, D; Robinson, T; Hochberg, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Kershaw, D; Robinson, T; Hochberg, M			Context-dependent classes in a hybrid recurrent network-HMM speech recognition system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CAMBRIDGE,DEPT ENGN,CAMBRIDGE CB2 1PZ,ENGLAND	University of Cambridge									0	9	10	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						750	756						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00106
C	Krogh, A; Riis, SK		Touretzky, DS; Mozer, MC; Hasselmo, ME		Krogh, A; Riis, SK			Prediction of beta sheets in proteins	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SANGER CTR,HINXTON CB10 1RQ,CAMBS,ENGLAND	Wellcome Trust Sanger Institute			Krogh, Anders/M-1541-2014						0	9	11	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						917	923						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00129
C	Pappu, S; Gold, S; Rangarajan, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Pappu, S; Gold, S; Rangarajan, A			A framework for non-rigid matching and correspondence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						YALE UNIV,DEPT DIAGNOST RADIOL,NEW HAVEN,CT 06520; YALE UNIV,DEPT COMP SCI,NEW HAVEN,CT 06520; YALE UNIV,YALE NEUROENGN & NEUROSCI CTR,NEW HAVEN,CT 06520	Yale University; Yale University; Yale University			Rangarajan, Anand/A-8652-2009	Rangarajan, Anand/0000-0001-8695-8436					0	9	9	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						795	801						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00112
C	Waterhouse, SR; Robinson, AJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Waterhouse, SR; Robinson, AJ			Constructive algorithms for hierarchical mixtures of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CAMBRIDGE,DEPT ENGN,CAMBRIDGE CB2 1PZ,ENGLAND	University of Cambridge									0	9	9	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						584	590						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00083
C	DEMERS, D; KREUTZDELGADO, K		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DEMERS, D; KREUTZDELGADO, K			LEARNING GLOBAL DIRECT INVERSE KINEMATICS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	9	9	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						589	594						6	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00072
C	Sun, XM; Panda, R; Feris, R; Saenko, K		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Sun, Ximeng; Panda, Rameswar; Feris, Rogerio; Saenko, Kate			AdaShare: Learning What To Share For Efficient Deep Multi-Task Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Multi-task learning is an open and challenging problem in computer vision. The typical way of conducting multi-task learning with deep neural networks is either through handcrafted schemes that share all initial layers and branch out at an adhoc point, or through separate task-specific networks with an additional feature sharing/fusion mechanism. Unlike existing methods, we propose an adaptive sharing approach, called AdaShare, that decides what to share across which tasks to achieve the best recognition accuracy, while taking resource efficiency into account. Specifically, our main idea is to learn the sharing pattern through a task-specific policy that selectively chooses which layers to execute for a given task in the multi-task network. We efficiently optimize the task-specific policy jointly with the network weights, using standard back-propagation. Experiments on several challenging and diverse benchmark datasets with a variable number of tasks well demonstrate the efficacy of our approach over state-of-the-art methods. Project page: https://cs-people.bu.edu/sunxm/AdaShare/project.html.	[Sun, Ximeng; Saenko, Kate] Boston Univ, Boston, MA 02215 USA; [Panda, Rameswar; Feris, Rogerio; Saenko, Kate] IBM Res, MIT IBM Watson AI Lab, Cambridge, MA USA	Boston University; International Business Machines (IBM)	Sun, XM (corresponding author), Boston Univ, Boston, MA 02215 USA.	sunxm@bu.edu; rpanda@ibm.com; rsferis@us.ibm.com; saenko@bu.edu	Panda, Rameswar/AAY-9834-2020	Panda, Rameswar/0000-0003-4359-2475; Saenko, Kate/0000-0002-7564-7218	DARPA [FA8750-19-C-1001]; NSF; IBM	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); IBM(International Business Machines (IBM))	This work is supported by DARPA Contract No. FA8750-19-C-1001, NSF and IBM. It reflects the opinions and conclusions of its authors, but not necessarily the funding agents.	Ahn Chanho, 2019, P IEEE INT C COMP VI; [Anonymous], 2012, INT C MACH LEARN; [Anonymous], 2017, P INT C MACH LEARN; Bengio Emmanuel, 2015, CORR; Bengio Y., 2009, P 26 ANN INT C MACH; Bengio Yoshua, 2013, ARXIV; Bilen Hakan, 2016, ADV NEURAL INFORM PR; Bragman Felix JS, 2019, P IEEE INT C COMP VI; Caruana R., 1997, MACHINE LEARNING J; Chen J., 2018, ARXIV180807658; Chen L.-C, 2017, IEEE T PATTERN ANAL; Conneau Alexis, 2017, EUROPEAN ASS COMPUTA; Cordts M., 2016, P IEEE C COMP VIS PA; Dvornik N., 2017, P IEEE INT C COMP VI; Eigen D., 2015, P IEEE INT C COMP VI; Eigen D, 2014, DEPTH MAP PREDICTION; Elsken T, 2019, J MACH LEARN RES, V20; Gao Yuan, 2019, P IEEE C COMP VIS PA; Guo Yunhui, 2019, P IEEE C COMP VIS PA; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hu P., 2020, ARXIV200703815; Hu Peiyun, 2020, P IEEE C COMP VIS PA; Huang GL, 2017, IEEE ICC; Huang Junshi, 2015, P IEEE INT C COMP VI; Huang Y., 2019, AAAI C ART INT; Jacob L., 2009, ADV NEURAL INFORM PR; Jang Eric, 2017, P 5 INT C LEARN REPR; Jin XX, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901353; Jou B., 2016, P 24 ACM INT C MULT; Kang Zhuoliang, 2011, P INT C MACH LEARN; Kingma D.P, P 3 INT C LEARNING R; Kokkinos I., 2017, P IEEE C COMP VIS PA; Kumar Abhishek, 2012, P INT C MACH LEARN; Liang Jason, 2018, P GEN EV COMP C; Liu Hanxiao, 2019, INT C LEARNING REPRE; Liu Shaohui, 2019, P IEEE C COMP VIS PA, P4; Lu Yongxi, 2017, P IEEE C COMP VIS PA; Maddison C.J, 2014, ADV NEURAL INFORM PR; Maddison Chris J, 2016, ARXIV161100712; Maninis Kevis-Kokitsi, 2019, P IEEE C COMP VIS PA; Meyerson Elliot, 2017, ARXIV171100108; Mirhoseini A., 2017, INT C LEARN REPR; Misra I., 2016, P IEEE C COMP VIS PA; Peng Xingchao, 2019, P IEEE INT C COMP VI; Ranjan R., 2016, ARXIV160301249; Rosenbaum Clemens, 2017, ARXIV171101239; Ruder S., 2017, PREPRINT; Ruder Sebastian, 2019, AAAI C ART INT; Rusu A. A., 2016, ARXIV160604671; Sanders MW, 2018, PROCEEDINGS OF THE EIGHTH ACM CONFERENCE ON DATA AND APPLICATION SECURITY AND PRIVACY (CODASPY'18), P2, DOI 10.1145/3176258.3176307; Silberman Pushmeet Kohli Nathan, 2012, EUR C COMP VIS; Standley T., 2020, 37 INT C MACH LEAR F, P9057; Stanley Kenneth O, 2002, EVOLUTIONARY COMPUTA; Strezoski Gjorgji, 2019, ARXIV190312117; Tao Andrew, 2020, ARXIV200510821; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Vandenhende Simon, 2019, ARXIV190402920; Veit A., 2016, ADV NEURAL INFORM PR; Veit Andreas, 2018, EUR C COMP VIS; Wang Xin, 2018, P EUR C COMP VIS ECC; Wu Bichen, 2019, P IEEE C COMP VIS PA; Wu Zuxuan, 2018, P IEEE C COMP VIS PA; Wu Zuxuan, 2019, ADV NEURAL INFORM PR; Xie S, 2019, INT C LEARN REPR; Xie Saining, 2017, P IEEE C COMP VIS PA; Xue Y., 2007, J MACHINE LEARNING R; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zamir Amir R, 2018, P IEEE C COMP VIS PA; Zhou J., 2011, ADV NEURAL INFORM PR; Zoph Barret, 2017, P 5 INT C LEARNING R; Zoph Barret, 2018, P IEEE C COMP VIS PA	71	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000046
C	Allen-Zhu, Z; Li, YZ; Song, Z		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Allen-Zhu, Zeyuan; Li, Yuanzhi; Song, Zhao			On the Convergence Rate of Training Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODEL	How can local-search methods such as stochastic gradient descent (SGD) avoid bad local minima in training multi-layer neural networks? Why can they fit random labels even given non-convex and non-smooth architectures? Most existing theory only covers networks with one hidden layer, so can we go deeper? In this paper, we focus on recurrent neural networks (RNNs) which are multi-layer networks widely used in natural language processing. They are harder to analyze than feedforward neural networks, because the same recurrent unit is repeatedly applied across the entire time horizon of length L, which is analogous to feedforward networks of depth L. We show when the number of neurons is sufficiently large, meaning polynomial in the training data size and in L, then SGD is capable of minimizing the regression loss in the linear convergence rate. This gives theoretical evidence of how RNNs can memorize data. More importantly, in this paper we build general toolkits to analyze multi-layer networks with ReLU activations. For instance, we prove why ReLU activations can prevent exponential gradient explosion or vanishing, and build a perturbation theory to analyze first-order approximation of multi-layer networks.	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA; [Li, Yuanzhi] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Song, Zhao] UT Austin, Austin, TX USA	Carnegie Mellon University; University of Texas System; University of Texas Austin	Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; yuanzhil@andrew.cmu.edu; zhaos@utexas.edu	Li, Yuan/GXV-1310-2022					Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Allen-Zhu Zeyuan, 2019, NEURIPS; Arora S., 2018, ARXIV181002281; Arora S, 2014, PR MACH LEARN RES, V32; Bartlett P., 2018, INT C MACH LEARN, P520; Brutzkus A., 2018, ICLR; Brutzkus A, 2017, PR MACH LEARN RES, V70; CAO Y., 2019, ARXIV190201384; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2014, ARXIV14123555; Du S. S., 2018, ICML; Du Simon S., 2018, ARXIV E PRINTS; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Ge Rong, 2017, ICLR; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Graves A., 2014, ARXIV14105401; Hahnloser RLT, 1998, NEURAL NETWORKS, V11, P691, DOI 10.1016/S0893-6080(98)00012-4; HARDT M., 2017, P 5 INT C LEARN REPR; Hardt M, 2018, J MACH LEARN RES, V19; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Janzamin M., 2015, ARXIV150608473; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Laurent B, 2000, ANN STAT, V28, P1302; Lee Jaehoon, 2017, ARXIV171100165; Li YT, 2017, ADV NEUR IN, V30; Li Y, 2018, PROC EUR TEST SYMP; LOJASIEWICZ S., 1963, EQUATIONS DERIVEES P, P87; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov T, 2011, INT CONF ACOUST SPEE, P5528; Oseledets, 2018, INT C LEARN REPR, P1; Panigrahy R., 2018, ITCS; Pennington J, 2017, PR MACH LEARN RES, V70; Poole B, 2016, ADV NEUR IN, V29; Safran I, 2018, PR MACH LEARN RES, V80; Sak H, 2014, INTERSPEECH, P338; Salehinejad H, 2017, RECENT ADV RECURRENT, DOI DOI 10.48550/ARXIV.1801.01078; Salinas E, 1996, P NATL ACAD SCI USA, V93, P11956, DOI 10.1073/pnas.93.21.11956; Sedghi Hanie, 2015, ICLR; SIEGELMANN HT, 1991, APPL MATH LETT, V4, P77, DOI 10.1016/0893-9659(91)90080-F; Soltanolkotabi M., 2017, ARXIV PREPRINT ARXIV; Spielman DA, 2004, J ACM, V51, P385, DOI 10.1145/990308.990310; Sundermeyer M, 2012, 13TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2012 (INTERSPEECH 2012), VOLS 1-3, P194; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Tian YD, 2017, PR MACH LEARN RES, V70; Xiao LC, 2018, PR MACH LEARN RES, V80; Yang Greg, 2018, ICLR OPEN REV; Zeng HQ, 2017, PROC INT CONF RECON; Zhong K., 2017, ARXIV171103440; Zhong K, 2017, PR MACH LEARN RES, V70	59	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306065
C	Barp, A; Briol, FX; Duncan, AB; Girolami, M; Mackey, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Barp, Alessandro; Briol, Francois-Xavier; Duncan, Andrew B.; Girolami, Mark; Mackey, Lester			Minimum Stein Discrepancy Estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				METRICS	When maximum likelihood estimation is infeasible, one often turns to score matching, contrastive divergence, or minimum probability flow to obtain tractable parameter estimates. We provide a unifying perspective of these techniques as minimum Stein discrepancy estimators, and use this lens to design new diffusion kernel Stein discrepancy (DKSD) and diffusion score matching (DSM) estimators with complementary strengths. We establish the consistency, asymptotic normality, and robustness of DKSD and DSM estimators, then derive stochastic Riemannian gradient descent algorithms for their efficient optimisation. The main strength of our methodology is its flexibility, which allows us to design estimators with desirable properties for specific models at hand by carefully selecting a Stein discrepancy. We illustrate this advantage for several challenging problems for score matching, such as non-smooth, heavy-tailed or light-tailed densities.	[Barp, Alessandro; Duncan, Andrew B.] Imperial Coll London, Dept Math, London, England; [Briol, Francois-Xavier] UCL, Dept Stat Sci, London, England; [Girolami, Mark] Univ Cambridge, Dept Engn, Cambridge, England; [Mackey, Lester] Microsoft Res, Cambridge, MA USA	Imperial College London; University of London; University College London; University of Cambridge; Microsoft	Barp, A (corresponding author), Imperial Coll London, Dept Math, London, England.	a.barp16@imperial.ac.uk; f.briol@ucl.ac.uk; a.duncan@imperial.ac.uk; mag92@eng.cam.ac.uk; imackey@microsoft.com		Girolami, Mark/0000-0003-3008-253X	Roth scholarship from the Department of Mathematics at Imperial College London; EPSRC [EP/L016710/1, EP/R018413/1, EP/J016934/3, EP/K034154/1, EP/P020720/1]; Lloyds Register Foundation Programme on Data-Centric Engineering; UKRI Strategic Priorities Fund under the EPSRC [EP/T001569/1]; Alan Turing Institute under the EPSRC [EP/N510129/1]	Roth scholarship from the Department of Mathematics at Imperial College London; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Lloyds Register Foundation Programme on Data-Centric Engineering; UKRI Strategic Priorities Fund under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	AB was supported by a Roth scholarship from the Department of Mathematics at Imperial College London. FXB was supported by the EPSRC grants [EP/L016710/1, EP/R018413/1]. AD and MG were supported by the Lloyds Register Foundation Programme on Data-Centric Engineering, the UKRI Strategic Priorities Fund under the EPSRC Grant [EP/T001569/1] and the Alan Turing Institute under the EPSRC grant [EP/N510129/1]. MG was supported by the EPSRC grants [EP/J016934/3, EP/K034154/1, EP/P020720/1, EP/R018413/1].	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S.I., 2016, INFORM GEOMETRY ITS, V194; [Anonymous], 2011, STAT INFERENCE MINIM; Araya M., 2015, P ADV NEUR INF PROC, P2053; Barbour A., 2005, LECT NOTES SERIES; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Bernton E, 2019, J R STAT SOC B, V81, P235, DOI 10.1111/rssb.12312; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Briol F.-X., 2019, ARXIV190605944; Casella G, 2001, STAT INFERENCE, V2nd; Ceylan C., 2018, ARXIV180603664; Chen W. Y., 2019, INT C MACH LEARN PML, P1011; Chen WY, 2018, PR MACH LEARN RES, V80; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Dawid AP, 2016, SCAND J STAT, V43, P123, DOI 10.1111/sjos.12168; Dawid AP, 2014, METRON, V72, P169, DOI 10.1007/s40300-014-0039-y; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; GABAY D, 1982, J OPTIMIZ THEORY APP, V37, P177, DOI 10.1007/BF00934767; Genevay A, 2018, PR MACH LEARN RES, V84; GEYER CJ, 1994, J ROY STAT SOC B MET, V56, P261; Gorham J., 2016, ANN APPL PROBABILITY; Gorham J, 2017, PR MACH LEARN RES, V70; Gorham J, 2015, ADV NEUR IN, V28; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Gutmann MU, 2012, J MACH LEARN RES, V13, P307; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; Hoeffding W., 1961, STRONG LAW LARGE NU; Huber PJ, 2009, WILEY SERIES PROBABI; Huggins JH, 2018, ADV NEUR IN, V31; Hyvarinen A, 1999, NEURAL COMPUT, V11, P1739, DOI 10.1162/089976699300016214; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Hyvdrinen A., 2006, J MACHINE LEARNING R, V6, P695; Jitkrittum W., 2017, ADV NEURAL INFORM PR, P262; Karakida R., 2016, ARTIFICIAL NEURAL NE; Kingma D. P., 2010, ADV NEURAL INF PROCE, P1126; Koster U, 2010, NEURAL COMPUT, V22, P2308, DOI 10.1162/NECO_a_00010; Kotz S., 2001, LAPLACE DISTRIBUTION; Li Y., 2018, INT C LEARN REPR; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu C., 2017, RIEMANNIAN STEIN VAR; Liu Q, 2016, PR MACH LEARN RES, V48; Liu Qiang, 2016, ADV NEURAL INFORM PR; Liu Qiang, 2017, ARXIV170700797; Liu S., 2018, ARXIV180507454; Lyu S., 2009, P 25 C UNC ART INT, P359; Ma C., 2017, ADV APPROXIMATE BAYE; Mardia K., 2016, ARXIV160408470; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Micheli M., 2013, ARXIV13085739; Newey W.K., 1994, HDB ECONOMETRICS, VIV, DOI DOI 10.1016/S1573-4412(05)80005-4; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Pardo L., 2005, STAT INFERENCE BASED, V170; Pigola S., 2014, EN MAT, V26, P1; Ranganath R, 2016, ADV NEUR IN, V29; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; Scheichl R., 2018, ADV NEURAL INFORM PR, P9169; Sohl-Dickstein J., 2011, P 28 INT C INT C MAC, P905; Sriperumbudur Bharath, 2017, J MACHINE LEARNING R, V18, P1830; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Stein C., 1972, PROC 6 BERKELEY S MA, VII, p583?602; Swersky K., 2011, PROC 28 INT C MACH L, P1201; Teh Yee Whye, 2012, P 29 INT C INT C MAC, P419; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Welling M., 2003, ADV NEURAL INFORM PR, P1383; Wenliang L., 2018, ARXIV181108357; Yeo IK, 2001, STAT PROBABIL LETT, V51, P63, DOI 10.1016/S0167-7152(00)00143-7; [No title captured]	70	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904060
C	Chen, MS; Jiang, HM; Liao, WJ; Zhao, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Minshuo; Jiang, Haoming; Liao, Wenjing; Zhao, Tuo			Efficient Approximation of Deep ReLU Networks for Functions on Low Dimensional Manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NEURAL-NETWORKS; BOUNDS	Deep neural networks have revolutionized many real world applications, due to their flexibility in data fitting and accurate predictions for unseen data. A line of research reveals that neural networks can approximate certain classes of functions with an arbitrary accuracy, while the size of the network scales exponentially with respect to the data dimension. Empirical results, however, suggest that networks of moderate size already yield appealing performance. To explain such a gap, a common belief is that many data sets exhibit low dimensional structures, and can be modeled as samples near a low dimensional manifold. In this paper, we prove that neural networks can efficiently approximate functions supported on low dimensional manifolds. The network size scales exponentially in the approximation error, with an exponent depending on the intrinsic dimension of the data and the smoothness of the function. Our result shows that exploiting low dimensional data structures can greatly enhance the efficiency in function approximation by neural networks. We also implement a sub-network that assigns input data to their corresponding local neighborhoods, which may be of independent interest.	[Chen, Minshuo; Jiang, Haoming; Liao, Wenjing; Zhao, Tuo] Georgia Inst Technol, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Chen, MS (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	mchen393@gatech.edu; jianghm@gatech.edu; wliao60@gatech.edu; tourzhao@gatech.edu			NSF [DMS 1818751, III 1717916]	NSF(National Science Foundation (NSF))	This work is supported by NSF grants DMS 1818751 and III 1717916. The authors would like to thank Ryan Tibshirani for his helpful discussions and insightful comments.	Aamari E, 2019, ELECTRON J STAT, V13, P1359, DOI 10.1214/19-EJS1551; [Anonymous], 2016, INT C MACH LEARN; [Anonymous], 2018, BRIEF BIOINFORM, DOI DOI 10.1093/bib/bbx044; ARORA S., 2018, ARXIV180206509; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Bengio Y., 2011, DEEP SPARSE RECTIFIE; Bickel P. J., 2007, COMPLEX DATASETS INV, V54, P177; Chui C. K., 2016, ARXIV160707110; CHUI CK, 1992, J APPROX THEORY, V70, P131, DOI 10.1016/0021-9045(92)90081-X; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004; Conway JH, 1987, SPHERE PACKINGS LATT; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; DEVORE RA, 1989, MANUSCRIPTA MATH, V63, P469, DOI 10.1007/BF01171759; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Goodfellow I., 2016, DEEP LEARNING; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, 2013 IEEE INT C AC S; Hanin B., 2017, ARXIV170802691; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Hu J., 2018, P IEEE C COMP VIS PA; IRIE B., 1988, IEEE INT C NEUR NETW, V1; Jiang F, 2017, STROKE VASC NEUROL, V2, P230, DOI 10.1136/svn-2017-000101; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Lee J. M., 2006, RIEMANNIAN MANIFOLDS, DOI 10.1007/b98852; LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5; LI H., 2018, ADV NEURAL INFORM PR; Lillicrap T., 2017, 2017 IEEE INT C ROB; Long J., 2015, P 2015 IEEE C COMP V, P3431, DOI [10.1109/CVPR.2015.7298965, DOI 10.1109/CVPR.2015.7298965]; LU Z., 2017, ADV NEURAL INFORM PR; Mhaskar HN, 1996, NEURAL COMPUT, V8, P164, DOI 10.1162/neco.1996.8.1.164; Niyogi P, 2008, DISCRETE COMPUT GEOM, V39, P419, DOI 10.1007/s00454-008-9053-2; PANAYOTOV V., 2015, 2015 IEEE INT C AC S; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Shaham U, 2018, APPL COMPUT HARMON A, V44, P537, DOI 10.1016/j.acha.2016.04.003; Simonyan K., 2015, ICLR; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tu L., 2010, UNIVERSITEXT; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738; Zhang Chiyuan, 2016, ARXIV161103530	42	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308022
C	Chen, RTQ; Behrmann, J; Duvenaud, D; Jacobsen, JH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Ricky T. Q.; Behrmann, Jens; Duvenaud, David; Jacobsen, Joern-Henrik			Residual Flows for Invertible Generative Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Flow-based generative models parameterize probability distributions through an invertible transformation and can be trained by maximum likelihood. Invertible residual networks provide a flexible family of transformations where only Lipschitz conditions rather than strict architectural constraints are needed for enforcing invertibility. However, prior work trained invertible residual networks for density estimation by relying on biased log-density estimates whose bias increased with the network's expressiveness. We give a tractable unbiased estimate of the log density using a "Russian roulette" estimator, and reduce the memory required during training by using an alternative infinite series for the gradient. Furthermore, we improve invertible residual blocks by proposing the use of activation functions that avoid derivative saturation and generalizing the Lipschitz condition to induced mixed norms. The resulting approach, called Residual Flows, achieves state-of-the-art performance on density estimation amongst flow-based models, and outperforms networks that use coupling blocks at joint generative and discriminative modeling.	[Chen, Ricky T. Q.; Duvenaud, David; Jacobsen, Joern-Henrik] Univ Toronto, Toronto, ON, Canada; [Behrmann, Jens] Univ Bremen, Bremen, Germany; [Chen, Ricky T. Q.; Duvenaud, David; Jacobsen, Joern-Henrik] Vector Inst, Toronto, ON, Canada	University of Toronto; University of Bremen	Chen, RTQ (corresponding author), Univ Toronto, Toronto, ON, Canada.; Chen, RTQ (corresponding author), Vector Inst, Toronto, ON, Canada.	rtqichen@cs.toronto.edu; jensb@uni-bremen.de; duvenaud@cs.toronto.edu; j.jacobsen@vectorinstitute.ai	Chen, Ricky Tian Qi/AAS-3168-2021		German Science Foundation [RTG 2224]	German Science Foundation(German Research Foundation (DFG))	Jens Behrmann gratefully acknowledges the financial support from the German Science Foundation for RTG 2224 "pi<SUP>3</SUP>: Parameter Identification - Analysis, Algorithms, Applications"	Adams Ryan P, 2018, ARXIV180203451; Almeida L. B., 1987, IEEE First International Conference on Neural Networks, P609; Beatson A, 2019, PR MACH LEARN RES, V97; Behrmann J, 2019, PR MACH LEARN RES, V97; Bouchard-Cote Alexandre, 2018, TOPICS PROBABILITY A, V1; Boutsidis C., 2008, 14 ACM SGKDD INT C K, P61; Chang B, 2018, AAAI CONF ARTIF INTE, P2811; Chen Ricky T. Q., 2018, ADV NEURAL INFORM PR; DECO G, 1995, NEURAL NETWORKS, V8, P525, DOI 10.1016/0893-6080(94)00108-X; Dinh L, 2017, 5 INT C LEARN REPR I; Dinh Laurent, 2014, ARXIV14108516; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gouk H., 2018, ARXIV180404368; Grathwohl W., 2019, P INT C LEARN REPR; Han Insu, 2018, C NEUR INF PROC SYST; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Ho Jonathan, 2019, ICML; Horn R.A., 2012, MATRIX ANAL, DOI [DOI 10.1017/CBO9780511810817, 10.1017/CBO9780511810817]; HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866; Jacobsen Joern-Henrik, 2019, ICLR; Johnston Nathaniel, 2016, QETLAB MATLAB TOOLBO; Kahn Herman, 1955, USE DIFFERENT MONTE; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kirk D.B., 1990, SIGGRAPH, P63, DOI DOI 10.1145/97880.97886; Le, 2017, ARXIV PREPRINT ARXIV; Liao R., 2018, ARXIV180306396; Loshchilov Ilya, 2019, INT C LEARN REPR; McLeish D, 2011, MONTE CARLO METHODS, V17, P301, DOI 10.1515/MCMA.2011.013; Nalisnick E., 2019, INT C LEARN REPR, P6; Ostrovski G, 2018, PR MACH LEARN RES, V80; Petersen K. B., 2012, MATRIX COOKBOOK; PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229; Polyak Boris T., 1992, ACCELERATION STOCHAS; Ramesh Aditya, 2018, C NEUR INF PROC SYST; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rhee CH, 2015, OPER RES, V63, P1026, DOI 10.1287/opre.2015.1404; Rhee Chang-han, 2012, P WINT SIM C, P17; SKILLING J, 1989, FUND THEOR, V36, P455; Tallec C., 2017, ARXIV170508209; Theis Lucas, 2015, ARXIV151101844; TROPP J. A., 2004, THESIS; van den Oord A, 2016, PR MACH LEARN RES, V48; Zhang Guojun, 2019, INT C LEARN REPR, P8	48	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901053
C	Fort, S; Jastrzebski, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fort, Stanislav; Jastrzebski, Stanislaw			Large Scale Structure of Neural Network Loss Landscapes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					There are many surprising and perhaps counter-intuitive properties of optimization of deep neural networks. We propose and experimentally verify a unified phenomenological model of the loss landscape that incorporates many of them. High dimensionality plays a key role in our model. Our core idea is to model the loss landscape as a set of high dimensional wedges that together form a large-scale, inter-connected structure and towards which optimization is drawn. We first show that hyperparameter choices such as learning rate, network width and L-2 regularization, affect the path optimizer takes through the landscape in similar ways, influencing the large scale curvature of the regions the optimizer explores. Finally, we predict and demonstrate new counter-intuitive properties of the loss-landscape. We show an existence of low loss subspaces connecting a set (not only a pair) of solutions, and verify it experimentally. Finally, we analyze recently popular ensembling techniques for deep networks in the light of our model.	[Fort, Stanislav] Google Res, Zurich, Switzerland; [Jastrzebski, Stanislaw] NYU, New York, NY USA	Google Incorporated; New York University	Fort, S (corresponding author), Google Res, Zurich, Switzerland.		Jastrzębski, Stanisław/AAX-4621-2020	Jastrzębski, Stanisław/0000-0003-4138-1818				CHOROMANSKA A, 2015, J MACHINE LEARNING R, V38, P192; Draxler Felix, 2018, P 35 INT C MACH LEAR; Fort Stanislav, 2018, GOLDILOCKS ZONE BETT; Frankle J, 2019, P INT C LEARN REPR; Garipov T, 2018, ADV NEUR IN, V31; Goodfellow I. J., 2015, INT C LEARNING REPRE; Izmailov P., 2018, AVERAGING WEIGHTS LE; Jastrzebski S., 2017, 3 FACTORS INFLUENCIN; Jastrzebski S., 2018, ARXIV180705031; Keskar N.S., 2017, ICLR; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Li Chunyuan, 2018, INT C LEARN REPR; Li H, 2018, ADV NEUR IN, V31; Liu FQ, 2017, 2017 IEEE 28TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), DOI 10.1109/PIMRC.2017.8292517; Nguyen Q, 2017, PR MACH LEARN RES, V70; Sagun L, 2016, EIGENVALUES HESSIAN; Smith Samuel L., 2018, INT C LEARN REPR; Xing C., 2018, ARXIV180208770	18	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306068
C	Foster, DJ; Krishnamurthy, A; Luo, HP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Foster, Dylan J.; Krishnamurthy, Akshay; Luo, Haipeng			Model selection for contextual bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce the problem of model selection for contextual bandits, where a learner must adapt to the complexity of the optimal policy while balancing exploration and exploitation. Our main result is a new model selection guarantee for linear contextual bandits. We work in the stochastic realizable setting with a sequence of nested linear policy classes of dimension d(1) < d(2) < ... , where the m*-th class contains the optimal policy, and we design an algorithm that achieves (O) over tilde (T(2/3)d(m*)(1/3)) regret with no prior knowledge of the optimal dimension din*. The algorithm also achieves regret (O) over tilde (T-3/4 + root Td(m*)), which is optimal for d(m*) >= root T. This is the first model selection result for contextual bandits with non-vacuous regret for all values of d(m*), and to the best of our knowledge is the first positive result of this type for any online learning setting with partial information. The core of the algorithm is a new estimator for the gap in the best loss achievable by two linear policy classes, which we show admits a convergence rate faster than the rate required to learn the parameters for either class.	[Foster, Dylan J.] MIT, Cambridge, MA 02139 USA; [Krishnamurthy, Akshay] Microsoft Res NYC, New York, NY USA; [Luo, Haipeng] Univ Southern Calif, Los Angeles, CA 90007 USA	Massachusetts Institute of Technology (MIT); University of Southern California	Foster, DJ (corresponding author), MIT, Cambridge, MA 02139 USA.	dylanf@mit.edu; akshay@cs.umass.edu; haipengl@usc.edu			NSF [IIS-1755781]	NSF(National Science Foundation (NSF))	We thank Ruihao Zhu for working with us at the early stages of this project, and for many helpful discussions. AK is supported by NSF IIS-1763618. HL is supported by NSF IIS-1755781.	AbbasiYadkori  Y., 2011, ADV NEURAL INFORM PR; Agarwal A., 2017, C LEARN THEOR; Agarwal Alekh, 2012, INT C ART INT STAT; Agarwal Alekh, 2014, INT C MACH LEARN; Allen-Zhu Z., 2018, INT C MACH LEARN; Allenberg Chamy, 2006, INT C ALG LEARN THEO; Auer P., 2002, SIAM J COMPUTING; Auer P., 2018, EUR WORKSH REINF LEA, V14; Bartlett Peter L, 2002, MACHINE LEARNING; Bastani H., 2017, ARXIV170409011; Ben-David Shai, 1995, J COMPUTER SYSTEM SC; Beygelzimer Alina, 2011, INT C ART INT STAT; Brown Lawrence D, 2007, ANN STAT; Chatterji Niladri S, 2019, ARXIV190510040; Chen Y., 2019, C LEARN THEOR; Cheung Wang Chi, 2019, INT C ART INT STAT; Chu Wei, 2011, INT C ART INT STAT; Cutkosky Ashok, 2017, C LEARN THEOR; Daniely Amit, 2015, J MACHINE LEARNING R; de la Pena V, 1998, DECOUPLING DEPENDENC; Devroye Luc, 1996, PROBABILISTIC THEORY; Dicker Lee H, 2014, BIOMETRIKA; Foster D. J., 2017, ADV NEURAL INFORM PR; Foster Dylan J., 2018, INT C MACH LEARN; Freund Y., 1997, J COMPUTER SYSTEM SC; Gaillard Pierre, 2014, C LEARN THEOR; Haussler David, 1995, J COMBINATORIAL TH A; Kannan Sampath, 2018, ADV NEURAL INFORM PR; Koltchinskii Vladimir, 2001, IEEE T INFORM THEORY; Kong Weihao, 2018, ADV NEURAL INFORM PR; Koolen Wouter M, 2015, C LEARN THEOR; Krishnamurthy A., 2016, ADV NEURAL INFORM PR; Krishnamurthy A, 2018, PR MACH LEARN RES, V80; Krishnamurthy Akshay, 2019, C LEARN THEOR; Langford John, 2008, ADV NEURAL INFORM PR; Lattimore T., 2015, ADV NEURAL INFORM PR; Li L., 2017, ARXIV170300048; Locatelli Andrea, 2018, C LEARN THEOR; Lugosi Gdibor, 1999, ANN STAT; Luo Haipeng, 2018, C LEARN THEOR; Luo Haipeng, 2015, C LEARN THEOR; Lykouris Thodoris, 2018, C LEARN THEOR; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; Massart Pascal, 1998, MINIMUM CONTRAST EST; McMahan Brendan, 2013, ADV NEURAL INFORM PR; Neu G., 2015, ADV NEURAL INFORM PR; Orabona Francesco, 2014, ADV NEURAL INFORM PR; Raghavan M., 2018, C LEARN THEOR; Russo Daniel, 2013, ADV NEURAL INFORM PR; Shawe-Taylor John, 1998, IEEE T INFORM THEORY; Vapnik V., 1992, ADV NEURAL INFORM PR; Vapnik V., 1982, ESTIMATION DEPENDENC; Vershynin R., 2012, INTRO NONASYMPTOTIC; Wang Lie, 2008, ANN STAT	55	8	8	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906040
C	Gama, F; Bruna, J; Ribeiro, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gama, Fernando; Bruna, Joan; Ribeiro, Alejandro			Stability of Graph Scattering Transforms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Scattering transforms are non-trainable deep convolutional architectures that exploit the multi-scale resolution of a wavelet filter bank to obtain an appropriate representation of data. More importantly, they are proven invariant to translations, and stable to perturbations that are close to translations. This stability property provides the scattering transform with a robustness to small changes in the metric domain of the data. When considering network data, regular convolutions do not hold since the data domain presents an irregular structure given by the network topology. In this work, we extend scattering transforms to network data by using multiresolution graph wavelets, whose computation can be obtained by means of graph convolutions. Furthermore, we prove that the resulting graph scattering transforms are stable to metric perturbations of the underlying network. This renders graph scattering transforms robust to changes on the network topology, making it particularly useful for cases of transfer learning, topology estimation or time-varying graphs.	[Gama, Fernando; Ribeiro, Alejandro] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA; [Bruna, Joan] NYU, Courant Inst Math Sci, New York, NY 10012 USA	University of Pennsylvania; New York University	Gama, F (corresponding author), Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA.	fgama@seas.upenn.edu; bruna@cims.nyu.edu; aribeiro@seas.upenn.edu		Gama, Fernando/0000-0001-6117-8193	NSF [CCF 1717120, RI-1816753]; ARO [W911NF1710438]; ARL DCIST CRA [W911NF-17-2-0181]; ISTC-WAS; Intel DevCloud; Alfred P. Sloan Foundation; NSF CAREER [CIF 1845360]; Samsung Electronics	NSF(National Science Foundation (NSF)); ARO; ARL DCIST CRA; ISTC-WAS; Intel DevCloud; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Samsung Electronics(Samsung)	Fernando Gama and Alejandro Ribeiro are supported by NSF CCF 1717120, ARO W911NF1710438, ARL DCIST CRA W911NF-17-2-0181, ISTC-WAS and Intel DevCloud.; Joan Bruna is partially supported by the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CAREER CIF 1845360, and Samsung Electronics.	Anderson BDO, 1979, INFORM SYSTEM SCI SE; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J., 2014, C TRACK P; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P53, DOI 10.1016/j.acha.2006.04.004; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Gama F, 2019, IEEE T SIGNAL PROCES, V67, P1034, DOI 10.1109/TSP.2018.2887403; Gama Fernando, 2019, 7 INT C LEARNING REP; Gao F., 2019, 36 INT C MACH LEARN, P1; Goodfellow I., 2016, DEEP LEARNING; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Haykin S., 1996, INFORM SYSTEM SCI SE; Horn R. A., 1986, MATRIX ANAL; Isufi E., 2019, PROC IEEE 13 INT C S, P1; Isufi E., 2019, 27 EUR SIGN PROC C; Kay S. M., 1993, SIGNAL PROCESSING SE; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413; McAuley J., 2012, 26 C NEUR INF PROC S; Murphy Kevin P., 2012, ADAPTIVE COMPUTATION; NADLER B., 2005, NIPS, P1; Oppenheim A. V, 2010, DISCRETE TIME SIGNAL, VThird; Perlmutter M., 2019, ARXIV181206968V3STAA; Rao C., 1973, WILEY SERIES PROBABI; Sandryhaila A, 2014, IEEE T SIGNAL PROCES, V62, P3042, DOI 10.1109/TSP.2014.2321121; Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Segarra S, 2017, IEEE T SIGNAL INF PR, V3, P467, DOI 10.1109/TSIPN.2017.2731051; Segarra S, 2017, IEEE T SIGNAL PROCES, V65, P4117, DOI 10.1109/TSP.2017.2703660; Segarra S, 2015, IEEE T SIGNAL PROCES, V63, P5464, DOI 10.1109/TSP.2015.2451111; Shuman DI, 2015, IEEE T SIGNAL PROCES, V63, P4223, DOI 10.1109/TSP.2015.2424203; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Tolstaya E., 2019, C ROB LEARN 2019; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Zou DM, 2020, APPL COMPUT HARMON A, V49, P1046, DOI 10.1016/j.acha.2019.06.003	39	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308010
C	Gao, RQ; Cai, TL; Li, HC; Wang, LW; Hsieh, CJ; Lee, JD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gao, Ruiqi; Cai, Tianle; Li, Haochuan; Wang, Liwei; Hsieh, Cho-Jui; Lee, Jason D.			Convergence of Adversarial Training in Overparametrized Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural networks are vulnerable to adversarial examples, i.e. inputs that are imperceptibly perturbed from natural data and yet incorrectly classified by the network. Adversarial training [31], a heuristic form of robust optimization that alternates between minimization and maximization steps, has proven to be among the most successful methods to train networks to be robust against a pre-defined family of perturbations. This paper provides a partial answer to the success of adversarial training, by showing that it converges to a network where the surrogate loss with respect to the the attack algorithm is within epsilon of the optimal robust loss. Then we show that the optimal robust loss is also close to zero, hence adversarial training finds a robust classifier. The analysis technique leverages recent work on the analysis of neural networks via Neural Tangent Kernel (NTK), combined with motivation from online-learning when the maximization is solved by a heuristic, and the expressiveness of the NTK kernel in the l(infinity)-norm. In addition, we also prove that robust interpolation requires more model capacity, supporting the evidence that adversarial training requires wider networks.	[Gao, Ruiqi; Cai, Tianle] Peking Univ, Sch Math Sci, Beijing, Peoples R China; [Li, Haochuan] MIT, Dept EECS, Cambridge, MA 02139 USA; [Wang, Liwei] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China; [Hsieh, Cho-Jui] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA; [Lee, Jason D.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA	Peking University; Massachusetts Institute of Technology (MIT); Peking University; University of California System; University of California Los Angeles; Princeton University	Gao, RQ (corresponding author), Peking Univ, Sch Math Sci, Beijing, Peoples R China.				elite undergraduate training program of School of Mathematical Sciences in Peking University; Natioanl Key R&D Program of China [2018YFB 1402600]; BJNSF [L172037]; ARO under MURI Award [W91 1NF-1 1-1-0303]; Sloan Research Fellowship; NSF CCF [1900145]	elite undergraduate training program of School of Mathematical Sciences in Peking University; Natioanl Key R&D Program of China; BJNSF; ARO under MURI Award; Sloan Research Fellowship(Alfred P. Sloan Foundation); NSF CCF	We acknowlegde useful discussions with Siyu Chen, Di He, Runtian Zhai, and Xiyu Zhai. RG and TC are partially supported by the elite undergraduate training program of School of Mathematical Sciences in Peking University. LW acknowledges support by Natioanl Key R&D Program of China (no. 2018YFB 1402600), BJNSF (L172037). JDL acknowledges support of the ARO under MURI Award W91 1NF-1 1-1-0303, the Sloan Research Fellowship, and NSF CCF #1900145.	[Anonymous], ARXIV181103962; [Anonymous], 2018, ARXIV180801204; Arora Sanjeev, 2019, ARXIV190108584; Athalye Anish, 2018, ICML; Bach Francis., 2017, J MACHINE LEARNING R, V18, P629; Bach Francis R., 2017, J MACHINE LEARNING R, V18, P714; Bartlett PL, 2019, J MACH LEARN RES, V20, P1; Bietti Alberto, 2019, ARXIV190512173; Brendel Wieland, 2017, DECISION BASED ADVER; Bubeck S., 2018, ARXIV180510204; Cai T., 2019, ARXIV190511675; Cao Yuan, 2019, ADV NEURAL INFORM PR, V32; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2018, NIPS, P4939; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Cohen Jeremy M, 2019, ARXIV190202918; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Du Simon S, 2018, ARXIV180301206; Du Simon S, 2018, ARXIV181002054; Eykholt Kevin, 2017, ARXIV PREPRINT ARXIV; Gonen Alon, 2018, ARXIV181007362; Goodfellow I. J., 2015, P ICLR; Guo Chuan, 2017, ARXIV171100117, P1; Ilyas A., 2018, ICML, P2142; Jacot A., 2018, ARXIV180607572; Kurakin A, 2016, INT C LEARN REPR SAN; Liu X, 2019, CVPR; Liu Xuanqing, 2018, EUR C COMP VIS, P381; Ma Xingjun, 2018, INT C LEARN REPR; Madry A., 2018, ARXIV PREPRINT ARXIV; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Mohri Mehryar, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P124, DOI 10.1007/978-3-642-34106-9_13; Mohri M., 2018, FDN MACHINE LEARNING; Nakkiran P., 2019, ARXIV190100532; Paulsen V. I., 2016, INTRO THEORY REPROD, V152; Rahimi A, 2008, ANN ALLERTON CONF, P555, DOI 10.1109/ALLERTON.2008.4797607; Salman Hadi, 2019, ARXIV190208722; Samangouei P., 2018, P INT C LEARN REPR I; Schmidt Ludwig, 2018, ADV NEURAL INFORM PR, V31, P5014; Shafahi Ali, 2019, ADVERSARIAL TRAINING; Singh Gagandeep, 2018, ADV NEURAL INFORM PR, P10802; Song Y, 2017, ARXIV171010766; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Weng TW, 2018, PR MACH LEARN RES, V80; Wong E, 2018, PR MACH LEARN RES, V80; Xie Cihang, 2019, ARXIV190603787; Yang L, 2019, EVID-BASED COMPL ALT, V2019, DOI 10.1155/2019/3508658; Yarotsky D., 2018, PMLR, V75; Yin Dong, 2018, ARXIV181011914; Yun Chulhee, 2018, ARXIV181007770; Zhang D., 2019, ARXIV PREPRINT ARXIV; Zou D, 2018, ARXIV181108888	54	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904064
C	Golrezaei, N; Javanmard, A; Mirrokni, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Golrezaei, Negin; Javanmard, Adel; Mirrokni, Vahab			Dynamic Incentive-aware Learning: Robust Pricing in Contextual Auctions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Motivated by pricing in ad exchange markets, we consider the problem of robust learning of reserve prices against strategic buyers in repeated contextual second-price auctions. Buyers' valuations for an item depend on the context that describes the item. However, the seller is not aware of the relationship between the context and buyers' valuations, i.e., buyers' preferences. The seller's goal is to design a learning policy to set reserve prices via observing the past sales data, and her objective is to minimize her regret for revenue, where the regret is computed against a clairvoyant policy that knows buyers' heterogeneous preferences. Given the seller's goal, utility -maximizing buyers have the incentive to bid untruthfully in order to manipulate the seller's learning policy. We propose two learning policies that are robust to such strategic behavior. These policies use the outcomes of the auctions, rather than the submitted bids, to estimate the preferences while controlling the long-term effect of the outcome of each auction on the future reserve prices. The first policy called Contextual Robust Pricing (CORP) is designed for the setting where the market noise distribution is known to the seller and achieves a T-period regret of O(d log (Td) log(T)), where d is the dimension of the contextual information. The second policy, which is a variant of the first policy, is called Stable CORP (SCORP). This policy is tailored to the setting where the market noise distribution is unknown to the seller and belongs to an ambiguity set. We show that the SCORP policy has a T-period regret of O(root dlog(Td) T-2/3).	[Golrezaei, Negin] MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Javanmard, Adel] Univ Southern Calif, Data Sci & Operat Dept, Los Angeles, CA 90007 USA; [Mirrokni, Vahab] Google Res, New York, NY USA	Massachusetts Institute of Technology (MIT); University of Southern California; Google Incorporated	Golrezaei, N (corresponding author), MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	golrezae@mit.edu; ajavanma@usc.edu; mirrokni@google.com	Javanmard, Adel/ABB-5000-2020		Outlier Research in Business (iORB) grant from the USC Marshall School of Business, a Google Faculty Research Award; NSF CAREER Award [DMS-1844481]; Office of the Provost at the University of Southern California through the Zumberge Fund Individual Grant Program	Outlier Research in Business (iORB) grant from the USC Marshall School of Business, a Google Faculty Research Award; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Office of the Provost at the University of Southern California through the Zumberge Fund Individual Grant Program	A. Javanmard was supported in part by an Outlier Research in Business (iORB) grant from the USC Marshall School of Business, a Google Faculty Research Award and the NSF CAREER Award DMS-1844481. A. Javanmard would also like to acknowledge the financial support of the Office of the Provost at the University of Southern California through the Zumberge Fund Individual Grant Program.	Amin K., 2014, ADV NEURAL INFORM PR, P622; Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; [Anonymous], 2017, PERSONALIZED DYNAMIC; Aviv Y., 2015, TECHNICAL REPORT; Aviv Y, 2008, M&SOM-MANUF SERV OP, V10, P339, DOI 10.1287/msom.1070.0183; Bagnoli M, 2005, ECON THEOR, V26, P445, DOI 10.1007/s00199-004-0514-4; Borgs C, 2014, MANAGE SCI, V60, P1792, DOI 10.1287/mnsc.2013.1839; Boyd S, 2004, CONVEX OPTIMIZATION; Chen N., 2018, ARXIV180501136; Chen X., 2015, STAT LEARNING APPROA; Cohen M., 2016, FEATURE BASED DYNAMI; den Boer A.V., 2015, SURV OPER RES MANAG, V20, P1; Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008; Golrezaei N., 2018, DYNAMIC INCENTIVE AW; Golrezaei N, PROC 27 ACM SIGKDD C, P447; Guimaraes B, 2011, AM ECON REV, V101, P844, DOI 10.1257/aer.101.2.844; HART OD, 1988, REV ECON STUD, V55, P509, DOI 10.2307/2297403; Javanmard A., 2019, ARXIV190101030; JAVANMARD A, 2019, J MACHINE LEARNING R, V20, P315; Javanmard A, 2017, J MACH LEARN RES, V18, P1; Johnson JP, 2003, AM ECON REV, V93, P748, DOI 10.1257/000282803322157070; Kanoria Yash., 2017, DYNAMIC RESERVE PRIC; Koufogiannakis C, 2014, ALGORITHMICA, V70, P648, DOI 10.1007/s00453-013-9771-6; Leme RP, 2018, ANN IEEE SYMP FOUND, P268, DOI 10.1109/FOCS.2018.00034; Lobel Ilan, 2016, ARXIV161100829; Mahdian M., 2017, P 26 INT C WORLD WID; Mao J., 2018, ADV NEURAL INFORM PR, P5648; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mohri M, 2014, PR MACH LEARN RES, V32; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; SALANT SW, 1989, Q J ECON, V104, P391, DOI 10.2307/2937854; Tropp JA, 2011, ELECTRON COMMUN PROB, V16, P262, DOI 10.1214/ECP.v16-1624	33	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901039
C	Har-Peled, S; Mahabadi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Har-Peled, Sariel; Mahabadi, Sepideh			Near Neighbor: Who is the Fairest of Them All?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work we study a fair variant of the near neighbor problem. Namely, given a set of n points P and a parameter r, the goal is to preprocess the points, such that given a query point q, any point in the r-neighborhood of the query, i.e., B(q, r), have the same probability of being reported as the near neighbor. We show that LSH based algorithms can be made fair, without a significant loss in efficiency. Specifically, we show an algorithm that reports a point in the r-neighborhood of a query q with almost uniform probability. The query time is proportional to O(dns(q.r)Q(n,c)), and its space is O(S(n, c)), where Q(n, c) and S(n, c) are the query time and space of an LSH algorithm for c-approximate near neighbor, and dns(q, r) is a function of the local density around q. Our approach works more generally for sampling uniformly from a sub-collection of sets of a given collection and can be used in a few other applications. Finally, we run experiments to show performance of our approach on real data.	[Har-Peled, Sariel] Univ Illinois, Champaign, IL 61801 USA; [Mahabadi, Sepideh] Toyota Technol Inst, Chicago, IL 60637 USA	University of Illinois System; University of Illinois Urbana-Champaign; Toyota Technological Institute - Chicago	Har-Peled, S (corresponding author), Univ Illinois, Champaign, IL 61801 USA.	sariel@illinois.edu; mahabadi@ttic.edu						Adar Eytan, 2007, WORKSH QUER LOG AN S; Agarwal A, 2018, PR MACH LEARN RES, V80; Andoni A., 2005, E21SH 0 1 USER MANUA; Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; [Anonymous], 2009, CLUSTER ANAL; Aswani A.., 2018, ARXIV180203765; Aumiller Martin, 2019, ARXIV190601859; Aumuller Martin, 2017, INT C SIM SEARCH APP; Backurs A., 2019, ARXIV190203519; Beame Paul, 2017, CORR; Bera S. K., 2019, ARXIV190102393; Chierichetti F., 2019, INT C ART INT STAT, P2212; CHIERICHETTI F, 2017, NIPS, P5029; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Datar M., 2004, P ACM 20 ANN S COMP, P253; Donini M., 2018, NEURIPS, P2791; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Elzayn H, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P170, DOI 10.1145/3287560.3287571; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Harel D., 2001, P LECT NOTES COMPUTE, V2245, P18, DOI DOI 10.1007/3-540-45294-X_3; Hassanat Ahmad Basheer, 2014, CORR; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kleinberg J, 2018, Q J ECON, V133, P237, DOI 10.1093/qje/qjx032; Kleindessner Matthaus, 2019, ARXIV190108668; Kung YH, 2012, STAT PROBABIL LETT, V82, P1786, DOI 10.1016/j.spl.2012.05.017; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Megan D. P. C., 2016, BIG DATA REPORT ALGO; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Pleiss G., 2017, ADV NEURAL INFORM PR, P5680; Qi YN, 2008, INT CON DISTR COMP S, P311, DOI 10.1109/ICDCS.2008.79; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347	35	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904079
C	Khosravi, P; Choi, Y; Liang, YT; Vergari, A; Van den Broeck, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Khosravi, Pasha; Choi, Yoojung; Liang, Yitao; Vergari, Antonio; Van den Broeck, Guy			On Tractable Computation of Expected Predictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTIPLE IMPUTATION	Computing expected predictionsof discriminative models is a fundamental task in machine learning that appears in many interesting applications such as fairness, handling missing values, and data analysis. Unfortunately, computing expectations of a discriminative model with respect to a probability distribution defined by an arbitrary generative model has been proven to be hard in general. In fact, the task is intractable even for simple models such as logistic regression and a naive Bayes distribution. In this paper, we identify a pair of generative and discriminative models that enables tractable computation of expectations, as well as moments of any order, of the latter with respect to the former in case of regression. Specifically, we consider expressive probabilistic circuits with certain structural constraints that support tractable probabilistic inference. Moreover, we exploit the tractable computation of high-order moments to derive an algorithm to approximate the expectations for classification scenarios in which exact computations are intractable. Our framework to compute expected predictions allows for handling of missing data during prediction time in a principled and accurate way and enables reasoning about the behavior of discriminative models. We empirically show our algorithm to consistently outperform standard imputation techniques on a variety of datasets. Finally, we illustrate how our framework can be used for exploratory data analysis.	[Khosravi, Pasha; Choi, Yoojung; Liang, Yitao; Vergari, Antonio; Van den Broeck, Guy] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Khosravi, P (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.	pashak@cs.ucla.edu; yjchoi@cs.ucla.edu; yliang@cs.ucla.edu; aver@cs.ucla.edu; guyvdb@cs.ucla.edu		Van den Broeck, Guy/0000-0003-3434-2503; vergari, antonio/0000-0003-0036-5678	NSF [IIS-1633857, CCF-1837129]; DARPA XAI grant [N66001-17-2-4032]; NEC Research	NSF(National Science Foundation (NSF)); DARPA XAI grant; NEC Research	This work is partially supported by NSF grants #IIS-1633857, #CCF-1837129, DARPA XAI grant #N66001-17-2-4032, NEC Research, and gifts from Intel and Facebook Research.	Azur MJ, 2011, INT J METH PSYCH RES, V20, P40, DOI 10.1002/mpr.329; Buuren, 2018, FLEXIBLE IMPUTATION; Chang C.-H., 2019, INT C LEARN REPR; Choi A., 2015, 24 INT JOINT C ART I; Choi A, 2012, INT J APPROX REASON, V53, P1415, DOI 10.1016/j.ijar.2012.04.005; Choi Y., 2019, ABS190603843 CORR; Choi YooJung, 2017, P 26 INT JOINT C ART; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Darwiche A, 2002, J ARTIF INTELL RES, V17, P229, DOI 10.1613/jair.989; Darwiche A., 2003, JACM; Jin J, 2015, 2015 INTERNATIONAL SYMPOSIUM ON POWER LINE COMMUNICATIONS AND ITS APPLICATIONS (ISPLC), P24, DOI 10.1109/ISPLC.2015.7147584; Khiari J., 2018, JOINT EUR C MACH LEA, P637; Khosravi P, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2716; Kisa D, 2014, FOURTEENTH INTERNATIONAL CONFERENCE ON THE PRINCIPLES OF KNOWLEDGE REPRESENTATION AND REASONING, P558; Krause A, 2009, J ARTIF INTELL RES, V35, P557, DOI 10.1613/jair.2737; Liang H, 2019, LOGOS PNEUMA-CHIN J, P33; Liang Y, 2017, IEEE INT C NETW SENS, P174, DOI 10.1109/ICNSC.2017.8000087; Liang YT, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Little R. J., 2019, STAT ANAL MISSING DA, V793; Lundberg SM, 2017, ADV NEUR IN, V30; Nash W. J., 1994, TECHNICAL REPORT, V48, P411; Olascoaga LIG, 2019, ADV NEUR IN, V32; Poon H., 2011, P 27 C UNC ART INT, P337, DOI DOI 10.1109/ICCVW.2011; Rahman Tahrima, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P630, DOI 10.1007/978-3-662-44851-9_40; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1; Rozenholc Y, 2010, COMPUT STAT DATA AN, V54, P3313, DOI 10.1016/j.csda.2010.04.021; Schafer JL, 1999, STAT METHODS MED RES, V8, P3, DOI 10.1191/096228099671525676; Shen Y., 2018, 32 AAAI C ART INT; Shen Y., 2017, UAI; Shen Y., 2016, ADV NEURAL INFORM PR, V29; Shih A, 2019, ADV NEUR IN, V32; Vergari A., 2016, VISUALIZING UNDERSTA; Vergari A., 2018, AAAL; Xiao H., 2017, FASHION MNIST NOVEL; Yann LeCun, 2009, MNIST DATABASE HANDW; Yu S., 2009, ARTIF INTELL, P639; Zafar MB, 2017, ADV NEURAL INFORM PR, V30, P229; Zafar Muhammad Bilal, 2015, ARXIV150705259	39	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902076
C	Kulunchakov, A; Mairal, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kulunchakov, Andrei; Mairal, Julien			A Generic Acceleration Framework for Stochastic Composite Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION ALGORITHMS	In this paper, we introduce various mechanisms to obtain accelerated first-order stochastic optimization algorithms when the objective function is convex or strongly convex. Specifically, we extend the Catalyst approach originally designed for deterministic objectives to the stochastic setting. Given an optimization method with mild convergence guarantees for strongly convex problems, the challenge is to accelerate convergence to a noise-dominated region, and then achieve convergence with an optimal worst-case complexity depending on the noise variance of the gradients. A side contribution of our work is also a generic analysis that can handle inexact proximal operators, providing new insights about the robustness of stochastic algorithms when the proximal operator cannot be exactly computed.	[Kulunchakov, Andrei; Mairal, Julien] Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,UK, F-38000 Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Kulunchakov, A (corresponding author), Univ Grenoble Alpes, INRIA, CNRS, Grenoble INP,UK, F-38000 Grenoble, France.	andrei.kulunchakov@inria.fr; julien.maira@inria.fr	Mairal, Julien/AAL-5611-2021		ERC grant SOLARIS [714381]; ANR 31A MIAI@GrenobleAlpes	ERC grant SOLARIS; ANR 31A MIAI@GrenobleAlpes(French National Research Agency (ANR))	This work was supported by the ERC grant SOLARIS (number 714381) and ANR 31A MIAI@GrenobleAlpes.The authors would like to thank Anatoli Juditsky for numerous interesting discussions that greatly improved the quality of this manuscript.	Allen-Zhu Z., 2017, P S THEOR COMP STOC; Arjevani Y., 2016, ADV NEURAL INFORM PR; Asi H, 2019, SIAM J OPTIMIZ, V29, P2257, DOI 10.1137/18M1230323; Aybat N. S., 2019, 33 C NEUR INF PROC S, P8525; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas DP, 2011, MATH PROGRAM, V129, P163, DOI 10.1007/s10107-011-0472-0; Bietti A., 2017, ADV NEURAL INFORM PR; Bottou L., 2008, ADV NEURAL INFORM PR; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; CHAMBOLLE A, 2015, SIAM J COMPUT MATH, V1, P29, DOI DOI 10.5802/smai-jcm.3; Cohen M. B., 2018, P INT C MACH LEARN I; D'Aspremont A, 2008, SIAM J OPTIMIZ, V19, P1171, DOI 10.1137/060676386; Defazio A., 2014, P INT C MACH LEARN I; Defazio Aaron, 2014, ADV NEURAL INFORM PR; Devolder O., 2011, TECHNICAL REPORT; Gower Robert M, 2018, ARXIV180502632; Guler O, 1992, SIAM J OPTIMIZ, V2, P649, DOI 10.1137/0802032; Hiriart-Urruty JB., 1996, OPTIMISATION; Hofmann T, 2015, ADV NEURAL INFORM PR; Hu C., 2009, ADV NEURAL INFORM PR; Iouditski A., 2014, ARXIV14011792; Kone~cny J., 2017, FRONT APPL MATH STAT, V3, P1; Kovalev Dmitry, 2019, ARXIV190108689; Kulis B., 2010, P INT C MACH LEARN I; Kulunchakov A., 2019, ARXIV190108788; Lan GH, 2018, MATH PROGRAM, V171, P167, DOI 10.1007/s10107-017-1173-0; Lin H., 2015, ADV NEURAL INFORM PR; Lin HZ, 2019, SIAM J OPTIMIZ, V29, P1408, DOI 10.1137/17M1125157; Mairal J., 2016, ADV NEURAL INFORM PR; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nguyen L. M., 2017, P INT C MACH LEARN I; Nguyen L. M., 2018, P INT C MACH LEARN I; Paquette C., 2018, ARXIV170310993; Scieur D., 2017, ADV NEURAL INFORM PR; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Toulis P., 2018, ARXIV151000967; Toulis P., 2016, P INT C ART INT STAT; Tseng P., 2008, ACCELERATED PR UNPUB; Wager S., 2014, ADV NEURAL INFORM PR; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Yang TB, 2018, J MACH LEARN RES, V19, P1; Zheng S., 2018, P INT C MACH LEARN I; Zhou K., 2018, P INT C MACH LEARN I; Zhou K., 2019, P INT C ART INT STAT	56	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904025
C	Kunstner, F; Balles, L; Hennig, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kunstner, Frederik; Balles, Lukas; Hennig, Philipp			Limitations of the Empirical Fisher Approximation for Natural Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Natural gradient descent, which preconditions a gradient descent update with the Fisher information matrix of the underlying statistical model, is a way to capture partial second-order information. Several highly visible works have advocated an approximation known as the empirical Fisher, drawing connections between approximate second-order methods and heuristics like Adam. We dispute this argument by showing that the empirical Fisher-unlike the Fisher-does not generally capture second-order information. We further argue that the conditions under which the empirical Fisher approaches the Fisher (and the Hessian) are unlikely to be met in practice, and that, even on simple optimization problems, the pathologies of the empirical Fisher can have undesirable effects.	[Kunstner, Frederik] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Kunstner, Frederik; Balles, Lukas; Hennig, Philipp] Univ Tubingen, Tubingen, Germany; [Kunstner, Frederik; Balles, Lukas; Hennig, Philipp] Max Planck Inst Intelligent Syst, Tubingen, Germany	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Eberhard Karls University of Tubingen; Max Planck Society	Kunstner, F (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.; Kunstner, F (corresponding author), Univ Tubingen, Tubingen, Germany.; Kunstner, F (corresponding author), Max Planck Inst Intelligent Syst, Tubingen, Germany.	kunstner@cs.ubc.ca; lballes@tue.mpg.de; ph@tue.mpg.de			International Max Planck Research School for Intelligent Systems (IMPRS-IS); European Research Council through ERC StG Action [757275/PANAMA]; DFG Cluster of Excellence "Machine Learning -New Perspectives for Science" [EXC 2064/1, 390727645]; German Federal Ministry of Education and Research (BMBF) through the Tdbingen Al Center [FKZ: 01IS18039A]; Ministry of Science, Research and Arts of the State of Baden-Wtirttemberg	International Max Planck Research School for Intelligent Systems (IMPRS-IS); European Research Council through ERC StG Action(European Research Council (ERC)); DFG Cluster of Excellence "Machine Learning -New Perspectives for Science"(German Research Foundation (DFG)); German Federal Ministry of Education and Research (BMBF) through the Tdbingen Al Center(Federal Ministry of Education & Research (BMBF)); Ministry of Science, Research and Arts of the State of Baden-Wtirttemberg	Lukas Balles kindly acknowledges the support of the International Max Planck Research School for Intelligent Systems (IMPRS-IS). The authors gratefully acknowledge financial support by the European Research Council through ERC StG Action 757275/PANAMA and the DFG Cluster of Excellence "Machine Learning -New Perspectives for Science", EXC 2064/1, project number 390727645, the German Federal Ministry of Education and Research (BMBF) through the Tdbingen Al Center (FKZ: 01IS18039A) and funds from the Ministry of Science, Research and Arts of the State of Baden-Wtirttemberg.	Aaron Mishkin, 2018, ADV NEURAL INFORM PR, V31, P6248; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Balles Lukas, 2018, P MACHINE LEARNING R, V80, P413; Bernstein J, 2018, PR MACH LEARN RES, V80; Botev A., 2017, INT C MACHINE LEARNI, P557; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chaudhari Pratik, 2017, 5 INT C LEARN REPR I; ebski Stanislaw Jastrz., 2018, INT C ART NEUR NETW; George Thomas, 2018, ADV NEURAL INFORM PR, V31, P9573; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hastie T., 2009, ELEMENTS STAT LEARNI, DOI [10.1007/978-0-387-84858-7, DOI 10.1007/978-0-387-84858-7]; Heskes T, 2000, NEURAL COMPUT, V12, P881, DOI 10.1162/089976600300015637; Jones E., 2001, SCIPY OPEN SOURCE SC; Karakida R., 2019, ARXIV, P1032; Khan Mohammad Emtiyaz, 2018, P MACHINE LEARNING R, V80, P2616; Kingma D.P., 2015, INT C LEARNING REPRE; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Le Roux N., 2010, P 27 INT C MACH LEAR, P623; Liao ZB, 2020, IEEE T PATTERN ANAL, V42, P15, DOI 10.1109/TPAMI.2018.2876413; Martens J., 2014, ABS14121193 CORR; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Ollivier Y, 2015, INF INFERENCE, V4, P108, DOI 10.1093/imaiai/iav006; Osawa K, 2019, PROC CVPR IEEE, P12351, DOI 10.1109/CVPR.2019.01264; Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4; Pascanu Razvan., 2014, 2 INT C LEARN REPR I; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux N. L., 2007, ADV NEURAL INFORM PR, V20, P849; Salas Arnold, 2018, ABS181103679 CORR; Schaul T., 2013, INT C MACHINE LEARNI, P343; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; Sun Y., 2009, P 11 ANN C GENETIC E, P539, DOI DOI 10.1145/1569901.1569976; Thomas Valentin, 2019, ABS190607774 CORR; Wang Y, 2010, COMPUT STAT DATA AN, V54, P1744, DOI 10.1016/j.csda.2010.02.006; Wen Yeming, 2020, 23 INT C ART INT STA; Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255; Zeng HQ, 2017, PROC INT CONF RECON; Zhu Z., 2019, P INT C MACH LEARN, P7654	39	8	8	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304019
C	Latorre, F; Eftekhari, A; Cevher, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Latorre, Fabian; Eftekhari, Armin; Cevher, Volkan			Fast and Provable ADMM for Learning with Generative Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PENALTY	In this work, we propose a (linearized) Alternating Direction Method-of-Multipliers (ADMM) algorithm for minimizing a convex function subject to a nonconvex constraint. We focus on the special case where such constraint arises from the specification that a variable should lie in the range of a neural network. This is motivated by recent successful applications of Generative Adversarial Networks (GANs) in tasks like compressive sensing, denoising and robustness against adversarial examples. The derived rates for our algorithm are characterized in terms of certain geometric properties of the generator network, which we show hold for feedforward architectures, under mild assumptions. Unlike gradient descent (GD), it can efficiently handle non-smooth objectives as well as exploit efficient partial minimization procedures, thus being faster in many practical scenarios.	[Latorre, Fabian; Eftekhari, Armin; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Latorre, F (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.	fabian.latorre@epfl.ch; armin.eftekhari@epfl.ch; volkan.cevher@epfl.ch			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [725594]; Department of the Navy -Office of Naval Research (ONR) [N62909-17-1-2111]; Swiss National Science Foundation (SNSF) [200021_178865]; Swiss Data Science Center; 2019 Google Faculty Research Award	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); Department of the Navy -Office of Naval Research (ONR)(Office of Naval Research); Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF)); Swiss Data Science Center; 2019 Google Faculty Research Award(Google Incorporated)	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement 725594 -time-data), the Department of the Navy -Office of Naval Research (ONR) under a grant number N62909-17-1-2111, and from the Swiss National Science Foundation (SNSF) under grant number 200021_178865. FL is supported through a PhD fellowship of the Swiss Data Science Center, a joint venture between EPFL and ETH Zurich. VC acknowledges the 2019 Google Faculty Research Award.	Akkaram S, 2010, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, DETC 2010, VOL 3, A AND B, P37; [Anonymous], 2017, ARXIV171209196; Arjovsky M, 2017, PR MACH LEARN RES, V70; BERTSEKAS DP, 1976, SIAM J CONTROL, V14, P216, DOI 10.1137/0314017; Birgin EG, 2016, SIAM J OPTIMIZ, V26, P951, DOI 10.1137/15M1031631; Bora Ashish, 2017, ARXIV170303208CSMATH; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Brock K, 2019, SWEETLAND DIG RHET C, P9; Cartis Coralia, 2018, J COMPLEXITY; Chen LM, 2014, IEEE T SIGNAL PROCES, V62, P3754, DOI 10.1109/TSP.2014.2330349; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Daskalakis Constantinos, 2018, INT C LEARN REPR; Dhar Manik, 2018, INT C MACH LEARN ICM, P1214; Djork-Arn, ICLR 2016; Donahue Chris, 2019, ICLR; Duchi John, 2008, P 25 INT C MACH LEAR, p272 279, DOI 10.1145/1390156.1390191; Dugas C, 2001, ADV NEUR IN, V13, P472; Eftekhari A, 2017, DISCRETE COMPUT GEOM, V57, P641, DOI 10.1007/s00454-016-9847-6; Eftekhari A, 2015, APPL COMPUT HARMON A, V39, P67, DOI 10.1016/j.acha.2014.08.005; Engel J., 2019, ARXIV190208710; Flores-Bazan F, 2012, J GLOBAL OPTIM, V53, P185, DOI 10.1007/s10898-011-9673-6; Gabay D., 1976, Computers & Mathematics with Applications, V2, P17, DOI 10.1016/0898-1221(76)90003-1; Garling D. J. H., 2014, COURSE MATH ANAL, V1, DOI 10.1017/CB09781139424516.; Gidel G, 2019, PR MACH LEARN RES, V89; Giryes Raja, 2016, ARXIV1605092322016; GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hand P., 2017, ARXIV170507576; Hand Paul, 2018, C NEUR INF PROC SYS, P9154; He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Heckel Reinhard, 2019, DEEP DENOISING RATE; Hegde C., 2018, ALGORITHMIC ASPECTS; Heusel M., 2017, ADV NEURAL INFORM PR, V30, P6626; Hsieh Y.-P., 2018, ARXIV181102002; Kim Y., 2018, P INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; LeCun Y., 2010, MNIST HANDWRITTEN DI; Lipton Zachary C., 2017, ARXIV170204782CSSTAT; Liu Q., 2017, ARXIV170502502; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma F., 2018, ANN C NEUR INF PROC, P9651; Madry Aleksander, 2017, ARXIV; Manoel A, 2017, IEEE INT SYMP INFO, P2098, DOI 10.1109/ISIT.2017.8006899; Mescheder L, 2018, PR MACH LEARN RES, V80; Mohri M., 2018, FDN MACHINE LEARNING; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nesterov Y., 2013, APPL OPTIMIZATION; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Ouyang Y., 2018, ARXIV180802901; Oymak S, 2018, IEEE T INFORM THEORY, V64, P4129, DOI 10.1109/TIT.2017.2773497; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Qiao Linbo, 2016, P AS C MACH LEARN, p97 109; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salimans T, 2016, ADV NEUR IN, V29; Samangouei Pouya, 2018, ARXIV180506605; Shah Viraj, 2018, ARXIV180208406CSSTAT; Shen XY, 2016, IEEE SIGNAL PROC LET, V23, P934, DOI 10.1109/LSP.2016.2567482; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tripathi S., 2018, ARXIV180304477; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Xu B., 2015, ARXIV150500853, V1505, P853; Xu Y, 2017, ADV NEUR IN, V30	70	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903062
C	Lyu, YW; Cui, ZP; Li, S; Pollefeys, M; Shi, BX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lyu, Youwei; Cui, Zhaopeng; Li, Si; Pollefeys, Marc; Shi, Boxin			Reflection Separation using a Pair of Unpolarized and Polarized Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BLIND SEPARATION	When we take photos through glass windows or doors, the transmitted background scene is often blended with undesirable reflection. Separating two layers apart to enhance the image quality is of vital importance for both human and machine perception. In this paper, we propose to exploit physical constraints from a pair of unpolarized and polarized images to separate reflection and transmission layers. Due to the simplified capturing setup, the system becomes more underdetermined compared with existing polarization based solutions that take three or more images as input. We propose to solve semireflector orientation estimation first to make the physical image formation well-posed and then learn to reliably separate two layers using a refinement network with gradient loss. Quantitative and qualitative experimental results show our approach performs favorably over existing polarization and single image based solutions.	[Lyu, Youwei; Li, Si] Beijing Univ Posts & Telecommun, Beijing, Peoples R China; [Cui, Zhaopeng; Pollefeys, Marc] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Shi, Boxin] Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China; [Shi, Boxin] Peng Cheng Lab, Shenzhen, Peoples R China; [Lyu, Youwei] Peking Univ, Beijing, Peoples R China	Beijing University of Posts & Telecommunications; Swiss Federal Institutes of Technology Domain; ETH Zurich; Peking University; Peng Cheng Laboratory; Peking University	Li, S (corresponding author), Beijing Univ Posts & Telecommun, Beijing, Peoples R China.; Shi, BX (corresponding author), Peking Univ, Natl Engn Lab Video Technol, Beijing, Peoples R China.; Shi, BX (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	youweilv@gmail.com; zhpcui@gmail.com; lisi@bupt.edu.cn; marc.pollefeys@inf.ethz.ch; shiboxin@pku.edu.cn	Pollefeys, Marc/I-7607-2013		National Natural Science Foundation of China [61872012, 61702047]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Natural Science Foundation of China under Grant 61872012 and 61702047.	Arvanitopoulos Nikolaos, 2017, P CVPR; Be'ery E, 2008, IEEE T IMAGE PROCESS, V17, P340, DOI 10.1109/TIP.2007.915548; Bronstein AM, 2005, INT J IMAG SYST TECH, V15, P84, DOI 10.1002/ima.20042; Chandramouli P., 2016, P ACCV; Diamant Y., 2008, P ACCV; Fan Qingnan, 2017, P ICCV; Farid H., 1999, P CVPR; Gai K, 2012, IEEE T PATTERN ANAL, V34, P19, DOI 10.1109/TPAMI.2011.87; Hecht E., 2015, OPTICS; Hermanto, 2001, IEICE T INF SYST, VE84D, P1241; Kingma D.P, P 3 INT C LEARNING R; Kong NJ, 2014, IEEE T PATTERN ANAL, V36, P209, DOI 10.1109/TPAMI.2013.45; Levin A., 2004, P ECCV; Levin A, 2007, IEEE T PATTERN ANAL, V29, P1647, DOI 10.1109/TPAMI.2007.1106; Li Y., 2013, P ICCV; Li Y, 2014, PROCESSING OF 2014 INTERNATIONAL CONFERENCE ON MULTISENSOR FUSION AND INFORMATION INTEGRATION FOR INTELLIGENT SYSTEMS (MFI); Paszke A., 2017, AUTOMATIC DIFFERENTI; Schechner YY, 2000, J OPT SOC AM A, V17, P276, DOI 10.1364/JOSAA.17.000276; Shih YiChang, 2015, P CVPR; Wan R., 2016, P ICIP; Wan Renjie, 2018, P CVPR; Wieschollek P, 2018, P ECCV; Xue TF, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766940; Yang J., 2018, P ECCV; Zhang X, 2018, PROC CVPR IEEE, P4786, DOI 10.1109/CVPR.2018.00503; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009	26	8	8	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906024
C	Malinin, A; Gales, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Malinin, Andrey; Gales, Mark			Reverse KL-Divergence Training of Prior Networks: Improved Uncertainty and Adversarial Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Ensemble approaches for uncertainty estimation have recently been applied to the tasks of misclassification detection, out-of-distribution input detection and adversarial attack detection. Prior Networks have been proposed as an approach to efficiently emulate an ensemble of models for classification by parameterising a Dirichlet prior distribution over output distributions. These models have been shown to outperform alternative ensemble approaches, such as Monte-Carlo Dropout, on the task of out-of-distribution input detection. However, scaling Prior Networks to complex datasets with many classes is difficult using the training criteria originally proposed. This paper makes two contributions. First, we show that the appropriate training criterion for Prior Networks is the reverse KL-divergence between Dirichlet distributions. This addresses issues in the nature of the training data target distributions, enabling prior networks to be successfully trained on classification tasks with arbitrarily many classes, as well as improving out-of-distribution detection performance. Second, taking advantage of this new training criterion, this paper investigates using Prior Networks to detect adversarial attacks and proposes a generalized form of adversarial training. It is shown that the construction of successful adaptive whitebox attacks, which affect the prediction and evade detection, against Prior Networks trained on CIFAR-10 and CIFAR-100 using the proposed approach requires a greater amount of computational effort than against networks defended using standard adversarial training or MC-dropout.	[Malinin, Andrey] Yandex Res, Moscow, Russia; [Gales, Mark] Univ Cambridge, Dept Engn, Cambridge, England; [Malinin, Andrey] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge; University of Cambridge	Malinin, A (corresponding author), Yandex Res, Moscow, Russia.	am969@yandex-team.ru; mjfg@eng.cam.ac.uk			Cambridge Assessment, University of Cambridge; DTA EPSRC award	Cambridge Assessment, University of Cambridge; DTA EPSRC award(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This paper reports on research partly supported by Cambridge Assessment, University of Cambridge. This work is also partly funded by a DTA EPSRC award.	Abadi M, 2015, P 12 USENIX S OPERAT; Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2020, INT C LEARN RE UNPUB; Carlini N., 2016, ARXIV160804644; Carlini N., 2019, CORR; Carlini Nicholas, 2017, ARXIV171108478; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Gal Y., 2016, THESIS, V1, P3; Gal Y, 2016, PR MACH LEARN RES, V48; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Gong Zhitao, 2017, ARXIV170404960; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Grosse Kathrin, 2017, ARXIV170206280; Hannun A.Y., 2014, ARXIV14125567, P1; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Lakshminarayanan B., 2017, P C NEUR INF PROC SY; Liu Yang, 2016, ARXIV160509090; Madry Aleksander, 2017, ARXIV; Malinin A, 2018, ADV NEUR IN, V31; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Mikolov T., 2013, ARXIV; Mikolov T., 2013, P 2013 C N AM CHAPTE, P746, DOI DOI 10.3109/10826089109058901; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Sensoy Murat, 2018, NIPS, P3179; Smith L, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P560; Szegedy Christian, 2013, ADV NEURAL INFORM PR, P3, DOI DOI 10.5555/2999792.2999897; Villegas R, 2017, PR MACH LEARN RES, V70	38	8	8	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906023
C	Miryoosefi, S; Brantley, K; Daume, H; Dudik, M; Schapire, RE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Miryoosefi, Sobhan; Brantley, Kiante; Daume, Hal, III; Dudik, Miroslav; Schapire, Robert E.			Reinforcement Learning with Convex Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In standard reinforcement learning (RL), a learning agent seeks to optimize the overall reward. However, many key aspects of a desired behavior are more naturally expressed as constraints. For instance, the designer may want to limit the use of unsafe actions, increase the diversity of trajectories to enable exploration, or approximate expert trajectories when rewards are sparse. In this paper, we propose an algorithmic scheme that can handle a wide class of constraints in RL tasks, specifically, any constraints that require expected values of some vector measurements (such as the use of an action) to lie in a convex set. This captures previously studied constraints (such as safety and proximity to an expert), but also enables new classes of constraints (such as diversity). Our approach comes with rigorous theoretical guarantees and only relies on the ability to approximately solve standard RL tasks. As a result, it can be easily adapted to work with any model-free or model-based RL algorithm. In our experiments, we show that it matches previous algorithms that enforce safety via constraints, but can also enforce new properties that these algorithms cannot incorporate, such as diversity.	[Miryoosefi, Sobhan] Princeton Univ, Princeton, NJ 08544 USA; [Brantley, Kiante] Univ Maryland, College Pk, MD 20742 USA; [Daume, Hal, III] Univ Maryland, Microsoft Res, College Pk, MD 20742 USA; [Dudik, Miroslav; Schapire, Robert E.] Microsoft Res, Redmond, WA USA	Princeton University; University System of Maryland; University of Maryland College Park; Microsoft; University System of Maryland; University of Maryland College Park; Microsoft	Miryoosefi, S (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	miryoosefi@cs.princeton.edu; kdbrant@cs.umd.edu; me@ha13.name; mdudik@microsoft.com; schapire@microsoft.com						Abernethy J, 2011, PROC 24 ANN C LEARN, P27; Achiam J, 2017, PR MACH LEARN RES, V70; Altman E, 1999, CONSTRAINED MARKOV D, V7; Blackwell David, 1956, PAC J MATH, V6, P1, DOI [DOI 10.2140/PJM.1956.6.1, 10.2140/pjm.1956.6.1]; Boyle J. P., 1986, ADV ORDER RESTRICTED, P28, DOI DOI 10.1007/978-1-4613-9940-7_3; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Hazan E., 2018, ARXIV181202690; INGRAM JM, 1991, J APPROX THEORY, V64, P343, DOI 10.1016/0021-9045(91)90067-K; Le H. M., 2019, ARXIV190308738; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Syed U., 2008, ADV NEURAL INFORM PR; Tessler C., 2019, INT C LEARN REPR; Zinkevich Martin, 2003, P INT C MACH LEARN I	15	8	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905071
C	Munkhdalai, T; Sordoni, A; Wang, T; Trischler, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Munkhdalai, Tsendsuren; Sordoni, Alessandro; Wang, Tong; Trischler, Adam			Metalearned Neural Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODEL	We augment recurrent neural networks with an external memory mechanism that builds upon recent progress in metalearning. We conceptualize this memory as a rapidly adaptable function that we parameterize as a deep neural network. Reading from the neural memory function amounts to pushing an input (the key vector) through the function to produce an output (the value vector). Writing to memory means changing the function; specifically, updating the parameters of the neural network to encode desired information. We leverage training and algorithmic techniques from metalearning to update the neural memory function in one shot. The proposed memory-augmented model achieves strong performance on a variety of learning problems, from supervised question answering to reinforcement learning.	[Munkhdalai, Tsendsuren; Sordoni, Alessandro; Wang, Tong; Trischler, Adam] Microsoft Res, Montreal, PQ, Canada		Munkhdalai, T (corresponding author), Microsoft Res, Montreal, PQ, Canada.	tsendsuren.munkhdalai@microsoft.com						Andrychowicz M, 2016, ADV NEUR IN, V29; Ba J, 2016, ADV NEUR IN, V29; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Dumoulin Vincent, 2016, ARXIV161007629; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Finn Chelsea, 2009, P MACHINE LEARNING R, V70, P1126; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Ha David, 2017, ICLR; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; HEBB D. O., 1949; Hinton G.E., 1987, P 9 ANN C COGNITIVE, P177; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Jaderberg M, 2017, PR MACH LEARN RES, V70; Kanerva P., 1988, SPARSE DISTRIBUTED M; Khan Arbaaz, 2017, ARXIV170905706; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Miconi T, 2018, INT C MACH LEARN PML, P3559; Mikolov T., 2015, ARXIV150205698, V1502, P05698; Mishra N., 2018, SIMPLE NEURAL ATTENT; Mnih V, 2016, PR MACH LEARN RES, V48; Munkhdalai T., 2018, INT C MACH LEARN, P3664; Munkhdalai T, 2017, 15TH CONFERENCE OF THE EUROPEAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (EACL 2017), VOL 1: LONG PAPERS, P397; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Munkhdalai Tsendsuren, 2018, ARXIV180705076; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; Perez E, 2017, ARXIV170907871; Rae JW, 2016, ADV NEUR IN, V29; Ravi S., 2017, INT C LEARN REPR, P12; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Santoro A, 2016, PR MACH LEARN RES, V48; Schlag Imanol, 2018, ADV NEURAL INFORM PR, P10003; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J., 1993, INT C ART NEUR NETW, P460; Shrivastava A, 2016, PROCEEDINGS OF THE 10TH INDIACOM - 2016 3RD INTERNATIONAL CONFERENCE ON COMPUTING FOR SUSTAINABLE GLOBAL DEVELOPMENT, P2378; SMOLENSKY P, 1990, ARTIF INTELL, V46, P159, DOI 10.1016/0004-3702(90)90007-M; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sun Wen, 2018, ARXIV180706473; Trischler Adam, 2016, THESIS; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang JW, 2017, ACTA OPHTHALMOL, V95, pE10, DOI 10.1111/aos.13227; Weston J., 2014, ARXIV14103916; Wu Yan, 2018, ICLR 2018; Yan Duan, 2016, ARXIV161102779; Zemel RS, 2000, ADV NEUR IN, V12, P80; Zhang Chiyuan, 2016, ARXIV161103530	50	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905003
C	Nachmani, E; Wolf, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nachmani, Eliya; Wolf, Lior			Hyper-Graph-Network Decoders for Block Codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural decoders were shown to outperform classical message passing techniques for short BCH codes. In this work, we extend these results to much larger families of algebraic block codes, by performing message passing with graph neural networks. The parameters of the sub-network at each variable-node in the Tanner graph are obtained from a hypernetwork that receives the absolute values of the current message as input. To add stability, we employ a simplified version of the arctanh activation that is based on a high order Taylor approximation of this activation function. Our results show that for a large number of algebraic block codes, from diverse families of codes (BCH, LDPC, Polar), the decoding obtained with our method outperforms the vanilla belief propagation method as well as other learning techniques from the literature.	[Nachmani, Eliya] Facebook AI Res, Menlo Pk, CA 94025 USA; Tel Aviv Univ, Tel Aviv, Israel	Facebook Inc; Tel Aviv University	Nachmani, E (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.							Arikan E, 2008, IEEE INT SYMP INFO, P1173, DOI 10.1109/ISIT.2008.4595172; Bertinetto Luca, 2016, NIPS; Bose R., 1960, ELSEVIER INFORM CONT, V3, P68, DOI DOI 10.1016/S0019-9958(60)90287-4; Brabandere B.D., 2016, ADV NEURAL INFORM PR, P667; Cammerer S, 2017, P IEEE GLOBAL COMMUN, P1; GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683; Gruber T., 2017, 2017 51 ANN C INF SC, P1; Ha David, 2016, ARXIV160909106; Kim H., 2018, ADV NEURAL INFORM PR, P9436; Kim Hyeji, 2018, 6 INT C LEARN REPR I; Kingma D.P, P 3 INT C LEARNING R; Klein B, 2015, PROC CVPR IEEE, P4840, DOI 10.1109/CVPR.2015.7299117; Krueger D., 2017, ARXIV PREPRINT ARXIV; Lugosch L, 2017, IEEE INT SYMP INFO, P1361, DOI 10.1109/ISIT.2017.8006751; Nachmani E, 2018, IEEE J-STSP, V12, P119, DOI 10.1109/JSTSP.2017.2788405; Nachmani E, 2016, ANN ALLERTON CONF, P341, DOI 10.1109/ALLERTON.2016.7852251; Richardson T., 2008, MODERN CODING THEORY; Riegler G, 2015, IEEE I CONF COMP VIS, P522, DOI 10.1109/ICCV.2015.67; Simonyan K., 2018, INT C LEARN REPR; Teng CF, 2019, INT CONF ACOUST SPEE, P1413, DOI 10.1109/ICASSP.2019.8683778; Vasic B, 2018, 2018 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA); Zhang C, 2019, INT C LEARN REPR	23	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302034
C	Rahmattalabi, A; Vayanos, P; Fulginiti, A; Rice, E; Wilder, B; Yadav, A; Tambe, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rahmattalabi, Aida; Vayanos, Phebe; Fulginiti, Anthony; Rice, Eric; Wilder, Bryan; Yadav, Amulya; Tambe, Milind			Exploring Algorithmic Fairness in Robust Graph Covering Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECISION RULES; ADAPTABILITY	Fueled by algorithmic advances, AI algorithms are increasingly being deployed in settings subject to unanticipated challenges with complex social effects. Motivated by real-world deployment of AI driven, social-network based suicide prevention and landslide risk management interventions, this paper focuses on robust graph covering problems subject to group fairness constraints. We show that, in the absence of fairness constraints, state-of-the-art algorithms for the robust graph covering problem result in biased node coverage: they tend to discriminate individuals (nodes) based on membership in traditionally marginalized groups. To mitigate this issue, we propose a novel formulation of the robust graph covering problem with group fairness constraints and a tractable approximation scheme applicable to real-world instances. We provide a formal analysis of the price of group fairness (PoF) for this problem, where we show that uncertainty can lead to greater PoF. We demonstrate the effectiveness of our approach on several real-world social networks. Our method yields competitive node coverage while significantly improving group fairness relative to state-of-the-art methods.	[Rahmattalabi, Aida; Vayanos, Phebe; Rice, Eric] Univ Southern Calif, Los Angeles, CA 90007 USA; [Fulginiti, Anthony] Univ Denver, Denver, CO 80208 USA; [Wilder, Bryan; Tambe, Milind] Harvard Univ, Cambridge, MA 02138 USA; [Yadav, Amulya] Penn State Univ, University Pk, PA 16802 USA	University of Southern California; University of Denver; Harvard University; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Rahmattalabi, A (corresponding author), Univ Southern Calif, Los Angeles, CA 90007 USA.	rahmatta@usc.edu; phebe.vayanos@usc.edu; anthony.fulginiti@du.edu; ericr@usc.edu; bwilder@g.harvard.edu; amulya@psu.edu; milind_tambe@harvard.edu		Vayanos, Phebe/0000-0001-7800-7235	Smart & Connected Communities program of the National Science Foundation [1831770]; US Army Research Office [W911NF1710445]	Smart & Connected Communities program of the National Science Foundation; US Army Research Office	We are grateful to three anonymous referees whose comments helped substantially improve the quality of this paper. This work was supported by the Smart & Connected Communities program of the National Science Foundation under NSF award No. 1831770 and by the US Army Research Office under grant number W911NF1710445.	Aghaei Sina, 2019, P 33 AAAI C ART INT; Ahmad AR, 2017, ADVANCING CULTURE OF LIVING WITH LANDSLIDES, VOL 1: ISDR-ICL SENDAI PARTNERSHIPS 2015-2025, P437, DOI 10.1007/978-3-319-59469-9_39; Ahmed F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P35; [Anonymous], 2014, ADV NEURAL INFORM PR; [Anonymous], 2016, INTRO RANDOM GRAPHS; Azizi MJ, 2018, LECT NOTES COMPUT SC, V10848, P35, DOI 10.1007/978-3-319-93031-2_3; Barman-Adhikari A, 2016, SOC SCI RES, V58, P292, DOI 10.1016/j.ssresearch.2016.01.004; Bateni MH, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P819, DOI 10.1145/2940716.2940763; Bei XH, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P81; Ben-Tal A, 2009, PRINC SER APPL MATH, P27; Benabbou N, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P973; Benders JF, 2005, COMPUT MANAG SCI, V2, P3, DOI 10.1007/s10287-004-0020-y; Bertsimas D, 2018, MATH PROGRAM, V167, P395, DOI 10.1007/s10107-017-1135-6; Bertsimas D, 2016, OPER RES, V64, P980, DOI 10.1287/opre.2016.1515; Bertsimas D, 2015, OPER RES, V63, P610, DOI 10.1287/opre.2015.1365; Bertsimas D, 2011, SIAM REV, V53, P464, DOI 10.1137/080734510; Bertsimas D, 2011, OPER RES, V59, P17, DOI 10.1287/opre.1100.0865; Bertsimas D, 2010, IEEE T AUTOMAT CONTR, V55, P2751, DOI 10.1109/TAC.2010.2049764; Bertsimas Dimitris, 2017, OPTIMIZATION ONLINE; Bertsimas Dimitris, 1997, ATHENA SCI, V1, P997; Bertsimas Dimitris, OPTIMIZATION INTEGER, V13; Bogunovic I, 2017, PR MACH LEARN RES, V70; Chassein Andre, 2019, EUROPEAN J OPERATION; Chekuri C, 2010, ANN IEEE SYMP FOUND, P575, DOI 10.1109/FOCS.2010.60; Conitzer V., 2019, P 33 AAAI C ART INT; Currarini S, 2009, ECONOMETRICA, V77, P1003, DOI 10.3982/ECTA7528; Elzayn H, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P170, DOI 10.1145/3287560.3287571; ERDOS P, 1960, B INT STATIST INST, V38, P343; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Fienberg S.E., 1981, SOCIOL METHODOL, P156, DOI [10.2307/270741, DOI 10.2307/270741]; Fish B, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P480, DOI 10.1145/3308558.3313680; GILBERT EN, 1959, ANN MATH STAT, V30, P1141, DOI 10.1214/aoms/1177706098; Hanasusanto GA, 2015, OPER RES, V63, P877, DOI 10.1287/opre.2015.1392; Isaac M, 2009, CAN J PSYCHIAT, V54, P260, DOI 10.1177/070674370905400407; Kleinberg J., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P568, DOI 10.1109/SFFCS.1999.814631; Kube A., 2019, P AAAI C ART INT; McPherson M, 2001, ANNU REV SOCIOL, V27, P415, DOI 10.1146/annurev.soc.27.1.415; Miyagishima K, 2019, J MATH ECON, V80, P77, DOI 10.1016/j.jmateco.2018.10.005; Orlin JB, 2016, LECT NOTES COMPUT SC, V9682, P312, DOI 10.1007/978-3-319-33461-5_26; Postek K, 2016, INFORMS J COMPUT, V28, P553, DOI 10.1287/ijoc.2016.0696; Rahmattalabi A, 2018, LECT NOTES COMPUT SC, V11199, P603, DOI 10.1007/978-3-030-01554-1_35; Rawls J., 2009, THEORY JUSTICE; Segal-Halevi E, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P482; Suksompong W, 2018, MATH SOC SCI, V92, P40, DOI 10.1016/j.mathsocsci.2017.09.004; Tsang A., 2019, P 28 INT JOINT C ART, P5997, DOI DOI 10.24963/IJCAI.2019/831; Tzoumas V., 2017, P IEEE C DEC CONTR, P1362, DOI DOI 10.1109/CDC.2017.8263844; Vayanos P, 2011, IEEE DECIS CONTR P, P7368, DOI 10.1109/CDC.2011.6161382; Vayanos Phebe, ROBUST OPTIMIZATION; Walrand Jean, 2004, LECT NOTES PROBABILI; Yanikoglu I, 2019, EUR J OPER RES, V277, P799, DOI 10.1016/j.ejor.2018.08.031	50	8	8	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907043
C	Rawat, AS; Chen, JC; Yu, F; Suresh, AT; Kumar, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rawat, Ankit Singh; Chen, Jiecao; Yu, Felix; Suresh, Ananda Theertha; Kumar, Sanjiv			Sampled Softmax with Random Fourier Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The computational cost of training with softmax cross entropy loss grows linearly with the number of classes. For the settings where a large number of classes are involved, a common method to speed up training is to sample a subset of classes and utilize an estimate of the loss gradient based on these classes, known as the sampled softmax method. However, the sampled softmax provides a biased estimate of the gradient unless the samples are drawn from the exact softmax distribution, which is again expensive to compute. Therefore, a widely employed practical approach involves sampling from a simpler distribution in the hope of approximating the exact softmax distribution. In this paper, we develop the first theoretical understanding of the role that different sampling distributions play in determining the quality of sampled softmax. Motivated by our analysis and the work on kernel-based sampling, we propose the Random Fourier Softmax (RF-softmax) method that utilizes the powerful Random Fourier Features to enable more efficient and accurate sampling from an approximate softmax distribution. We show that RF-softmax leads to low bias in estimation in terms of both the full softmax distribution and the full softmax gradient. Furthermore, the cost of RF-softmax scales only logarithmically with the number of classes.	[Rawat, Ankit Singh; Chen, Jiecao; Yu, Felix; Suresh, Ananda Theertha; Kumar, Sanjiv] Google Res, New York, NY 02115 USA	Google Incorporated	Rawat, AS (corresponding author), Google Res, New York, NY 02115 USA.	ankitsrawat@google.com; chenjiecao@google.com; felixyu@google.com; theertha@google.com; sanjivk@google.com						[Anonymous], 2018, EXTREME CLASSIFICATI; Baker YS, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P10, DOI 10.1109/ISI.2013.6578776; Bengio Y, 2008, IEEE T NEURAL NETWOR, V19, P713, DOI 10.1109/TNN.2007.912312; Blanc Guy, 2018, INT C MACH LEARN; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; de Brebisson Alexandre, 2015, ARXIV151105042; Fagan F., 2018, ARXIV180308577; Graft David, 1997, 1996 ENGLISH BROADCA; Hamid R, 2014, PR MACH LEARN RES, V32, P19; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Jean Sebastien, 2014, ARXIV14122007; Joulin A., 2017, P INT C MACH LEARN I, P1302; Kar Purushottam, 2012, P INT C ART INT STAT; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar, 2015, ADV NEURAL INFORM PR, P1846; Liu W, 2017, IEEE C COMP VIS PATT, V1, P1, DOI DOI 10.3807/COPP.2017.1.1.001; Marcus M., 1999, TREEBANK 3 LDC99T42; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mnih A., 2013, ADV NEURAL INFORM PR, V26, P2265; Morin F., 2005, PROC INT WORKSHOP AR, P246; Mussmann Stephen, 2017, ARXIV170703372; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Raman P, 2016, ARXIV160404706; Reddi Sashank J, 2019, P INT C ART INT STAT; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Vembu Shankar, 2009, UAI, P557; Vijayanarasimhan Sudheendra, 2014, ARXIV14127479; Vincent Pascal, 2015, ADV NEURAL INFORM PR, P1108; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Yen IEH, 2018, PR MACH LEARN RES, V80; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975; Zhu JM, 2014, J INEQUAL APPL, DOI 10.1186/1029-242X-2014-486	35	8	8	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905050
C	Sadeghian, A; Armandpour, M; Ding, P; Wang, DZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sadeghian, Ali; Armandpour, Mohammadreza; Ding, Patrick; Wang, Daisy Zhe			DRUM: End-To-End Differentiable Rule Mining On Knowledge Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we study the problem of learning probabilistic logical rules for inductive and interpretable link prediction. Despite the importance of inductive link prediction, most previous works focused on transductive link prediction and cannot manage previously unseen entities. Moreover, they are black-box models that are not easily explainable for humans. We propose DRUM, a scalable and differentiable approach for mining first-order logical rules from knowledge graphs which resolves these problems. We motivate our method by making a connection between learning confidence scores for each rule and low-rank tensor approximation. DRUM uses bidirectional RNNs to share useful information across the tasks of learning rules for different relations. We also empirically demonstrate the efficiency of DRUM over existing rule mining methods for inductive link prediction on a variety of benchmark datasets.	[Sadeghian, Ali; Wang, Daisy Zhe] Univ Florida, Dept Comp Sci, Gainesville, FL 32611 USA; [Armandpour, Mohammadreza; Ding, Patrick] Texas A&M Univ, Dept Stat, College Stn, TX 77843 USA	State University System of Florida; University of Florida; Texas A&M University System; Texas A&M University College Station	Sadeghian, A (corresponding author), Univ Florida, Dept Comp Sci, Gainesville, FL 32611 USA.	asadeghian@ufl.edu; armand@stat.tamu.edu; patrickding@stat.tamu.edu; daisyw@ufl.edu	Sadeghian, Ali/AAV-6773-2020		NSF under IIS Award [1526753]; DARPA [FA8750-18-2-0014]	NSF under IIS Award; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank Kazem Shirani for his valuable feedback. We thank Anthony Colas and Sourav Dutta for their help in human assessment of the rules. This work is partially supported by NSF under IIS Award #1526753 and DARPA under Award #FA8750-18-2-0014 (AIDA/GAIA).	Abadi M, 2015, P 12 USENIX S OPERAT; Balazevic I., 2019, ARXIV190109590; Bordes A., 2013, ADV NEURAL INFORM PR; Chen Y, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P835, DOI 10.1145/2882903.2882954; Cohen William W., 2016, ARXIV160506523; Das Rajarshi, 2018, 6 INT C LEARN REPR I; Das Rajarshi, 2018, INT C LEARN REPR; DERAEDT L, 2015, 24 INT JOINT C ART I; Dettmers T, 2018, AAAI CONF ARTIF INTE, P1811; Dong XL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P601, DOI 10.1145/2623330.2623623; Ebisu T, 2018, AAAI CONF ARTIF INTE, P1819; Ellis Joe, 2015, TAC; Evans C., 2008, P 2008 ACM SIGMOD IN, P1247, DOI [DOI 10.1145/1376616.1376746, 10.1145/1376616]; Evans R, 2018, J ARTIF INTELL RES, V61, P1; Galarraga L, 2015, VLDB J, V24, P707, DOI 10.1007/s00778-015-0394-1; Guu K., 2015, EMNLP, P318; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kilgarriff A., 2000, WORDNET ELECT LEXICA; Kingma D.P, P 3 INT C LEARNING R; Kok S., 2007, P 24 INT C MACH LEAR, P433; Lacroix T, 2018, PR MACH LEARN RES, V80; Lao Ni, 2011, P C EMP METH NAT LAN, P529, DOI DOI 10.5555/2145432.2145494; Lin X. V., 2018, EMNLP, P3243, DOI 10.18653/v1/D18-1362; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Minervini P., 2018, ARXIV180708204; MUGGLETON S, 1995, NEW GENERAT COMPUT, V13, P245, DOI 10.1007/BF03037227; Niepert M., 2016, P 30 INT C NEUR INF, P3413; Omran PG, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2149; QUINLAN JR, 1990, MACH LEARN, V5, P239, DOI 10.1007/BF00117105; Rocktaschel T, 2017, ADV NEUR IN, V30; Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38; Schuller P, 2018, MACH LEARN, V107, P1141, DOI 10.1007/s10994-018-5708-2; Socher R., 2013, ADV NEURAL INFORM PR, V26, P1; Suchanek F.M., 2007, P 16 INT C WORLD WID, DOI 10.1145/1242572.1242667; Sun Zhiqing, 2019, ARXIV190210197; Toutanova K., 2015, P 3 WORKSH CONT VECT, DOI DOI 10.18653/V1/W15-4007; Trouillon T, 2016, PR MACH LEARN RES, V48; Vrandecic D, 2014, COMMUN ACM, V57, P78, DOI 10.1145/2629489; Wang W. Y., 2014, P 23 ACM INT C C INF, P1199, DOI DOI 10.1145/2661829.2662022; Yang B., 2015, CORR; Yang F, 2017, PROCEEDINGS OF 2017 IEEE INTERNATIONAL CONFERENCE ON UNMANNED SYSTEMS (ICUS), P231	41	8	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907005
C	Serra, J; Pascual, S; Segura, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Serra, Joan; Pascual, Santiago; Segura, Carlos			Blow: a single-scale hyperconditioned flow for non-parallel raw-audio voice conversion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					End-to-end models for raw audio generation are a challenge, specially if they have to work with non-parallel data, which is a desirable setup in many situations. Voice conversion, in which a model has to impersonate a speaker in a recording, is one of those situations. In this paper, we propose Blow, a single-scale normalizing flow using hypernetwork conditioning to perform many-to-many voice conversion between raw audio. Blow is trained end-to-end, with non-parallel data, on a frame-by-frame basis using a single speaker identifier. We show that Blow compares favorably to existing flow-based architectures and other competitive baselines, obtaining equal or better performance in both objective and subjective evaluations. We further assess the impact of its main components with an ablation study, and quantify a number of properties such as the necessary amount of training data or the preference for source or target speakers.	[Serra, Joan; Segura, Carlos] Telefonica Res, Madrid, Spain; [Pascual, Santiago] Univ Politecn Cataluna, Barcelona, Spain	Telefonica SA; Universitat Politecnica de Catalunya	Serra, J (corresponding author), Telefonica Res, Madrid, Spain.	joan.serra@telefonica.com; santi.pascual@upc.edu; carlos.seguraperales@telefonica.com			MINECO/FEDER, UE [TEC2015-69266-P]	MINECO/FEDER, UE	We are grateful to all participants of the subjective evaluation for their input and feedback. We thank Antonio Bonafonte, Ferran Diego, and Martin Pielot for helpful comments. SP acknowledges partial support from the project TEC2015-69266-P (MINECO/FEDER, UE).	[Anonymous], 2016, ARXIV160903499; Arik S., 2018, ADV NEURAL INFORM PR, P10019; Arik SO, 2017, PR MACH LEARN RES, V70; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Deco G., 1995, Advances in Neural Information Processing Systems 7, P247; Dieleman S., 2018, ADV NEURAL INFORM PR, P7989; Dinh L., 2017, P INT C LEARN REPR I; Dinh L., 2019, ARXIV190307714; Dinh L., 2015, P INT C LEARN REPR I; Donahue Chris, 2019, P INT C LEARN REPR I; Engel J, 2017, PR MACH LEARN RES, V70; Erro D, 2010, IEEE T AUDIO SPEECH, V18, P944, DOI 10.1109/TASL.2009.2038669; Grathwohl W., 2019, P INT C LEARN REPR I; Ha D., 2017, P INT C LEARN REPR I; Haque A, 2018, INTERSPEECH, P2295, DOI 10.21437/Interspeech.2018-38; Ho Jonathan, 2019, ICML; Hoogeboom E, 2019, PR MACH LEARN RES, V97; Hsu CC, 2017, INTERSPEECH, P3364, DOI 10.21437/Interspeech.2017-63; Hwang S. J., 2018, ARXIV181109897; Kaiser L., 2018, P INT C LEARN REPR I; Kalchbrenner N., 2018, INT C MACH LEARN, V80, P2410; Kameoka H., 2018, ARXIV PREPRINT ARXIV; Kameoka H, 2018, IEEE W SP LANG TECH, P266, DOI 10.1109/SLT.2018.8639535; Kaneko T., 2018, P EUSIPCO, P2114; Karras T., 2019, P C COMP VIS PATT RE; Kim S, 2018, IEEE IND ELEC, P3377, DOI 10.1109/IECON.2018.8591388; Kingma D. P., 2018, ADV NEURAL INFORM PR, P10215; Kinnunen T, 2017, INT CONF ACOUST SPEE, P5535, DOI 10.1109/ICASSP.2017.7953215; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005; Livne M., 2018, ARXIV181101837; Lorenzo-Trueba J., 2018, ARXIV180404262; Mehri S., 2017, P ICLR; Mohammadi SH, 2017, SPEECH COMMUN, V88, P65, DOI 10.1016/j.specom.2017.01.008; Mor N., 2019, P INT C LEARN REPR I; Morise M, 2016, IEICE T INF SYST, VE99D, P1877, DOI 10.1587/transinf.2015EDP7457; Mouchtaris A, 2006, IEEE T AUDIO SPEECH, V14, P952, DOI 10.1109/TSA.2005.857790; Nachmani E., 2019, P INT SPEECH COMM AS; Pascual S, 2017, INTERSPEECH, P3642, DOI 10.21437/Interspeech.2017-1428; Paszke A., 2017, NEURIPS WORKSH FUT G; Paul Boersma, 2019, PRAAT DOING PHONETIC; Prenger R., 2018, P IEEE INT C AC SPEE, P3617; REDLICH AN, 1993, NEURAL COMPUT, V5, P750, DOI 10.1162/neco.1993.5.5.750; Rezende D., 2015, ICML, P1530; Saito Y, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5274; Simonyan K., 2015, 3 INT C LEARN REPR I, P1; Tabak EG, 2013, COMMUN PUR APPL MATH, V66, P145, DOI 10.1002/cpa.21423; van den Oord Aaron, 2017, CORR, P6306; Veaux Christophe, 2017, CSTR VCTK CORPUS ENG; Wester M, 2016, INTERSPEECH, P1637, DOI 10.21437/Interspeech.2016-1331; Wu ZZ, 2012, IEEE SIGNAL PROC LET, V19, P914, DOI 10.1109/LSP.2012.2225615; Xie FL, 2016, INTERSPEECH, P287, DOI 10.21437/Interspeech.2016-116; Yamaguchi M, 2019, INT CONF ACOUST SPEE, P3647, DOI 10.1109/ICASSP.2019.8683072; Ziolko B., 2011, LECT NOTES COMPUTER, V6562	55	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306076
C	Shaw, A; Wei, W; Liu, WY; Song, L; Dai, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shaw, Albert; Wei, Wei; Liu, Weiyang; Song, Le; Dai, Bo			Meta Architecture Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural Architecture Search (NAS) has been quite successful in constructing state-of-the-art models on a variety of tasks. Unfortunately, the computational cost can make it difficult to scale. In this paper, we make the first attempt to study Meta Architecture Search which aims at learning a task-agnostic representation that can be used to speed up the process of architecture search on a large number of tasks. We propose the Bayesian Meta Architecture SEarch (BASE) framework which takes advantage of a Bayesian formulation of the architecture search problem to learn over an entire set of tasks simultaneously. We show that on Imagenet classification, we can find a model that achieves 25.7% top-1 error and 8.1% top-5 error by adapting the architecture in less than an hour from an 8 GPU days pretrained meta-network. By learning a good prior for NAS, our method dramatically decreases the required computation cost while achieving comparable performance to current state-of-the-art methods - even finding competitive models for unseen datasets with very quick adaptation. We believe our framework will open up new possibilities for efficient and massively scalable architecture search research across multiple tasks.	[Shaw, Albert; Liu, Weiyang; Song, Le; Dai, Bo] Georgia Inst Technol, Atlanta, GA 30332 USA; [Wei, Wei; Dai, Bo] Google Res, Mountain View, CA USA; [Song, Le] Ant Financial, Hangzhou, Peoples R China	University System of Georgia; Georgia Institute of Technology; Google Incorporated	Shaw, A (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	ashaw596@gatech.edu			NSF [CDSE-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, SaTC-1704701, CAREER IIS-1350983]	NSF(National Science Foundation (NSF))	We would like to thank the anonymous reviewers for their comments and suggestions. Part of this work was done while Bo Dai and Albert Shaw were at Georgia Tech. Le Song was supported in part by NSF grants CDS&E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, SaTC-1704701, and CAREER IIS-1350983.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baker Bowen, 2017, ICLR; Brock A., 2018, ICLR, P1; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2019, INT C LEARN REPR; Cai Han, 2018, ARXIV180602639; Dai Bo, 2018, NEURIPS, V31, P9713; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Finn C, 2017, PR MACH LEARN RES, V70; Garnelo M, 2018, ARXIV180701622; Garnelo Marta, 2018, ARXIV180701613, P1704; Howard A., 2019, ABS190502244 CORR; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Jang E., 2017, ICLR; Kim HH, 2019, 2019 DIGITAL IMAGE COMPUTING: TECHNIQUES AND APPLICATIONS (DICTA), P330, DOI 10.1145/3337821.3337886; LeCun Y., 1998, HDB BRAIN THEORY NEU, P255; Li YM, 2012, CHINA FOUNDRY, V9, P6; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu Hanxiao, 2018, ICLR; Liu W, 2017, AIP CONF PROC, V1794, DOI 10.1063/1.4971938; Liu WY, 2019, ADV NEUR IN, V32; Liu W, 2018, PROC CVPR IEEE, P6536, DOI 10.1109/CVPR.2018.00684; Loshchilov I., 2017, P INT C LEARNING REP; Maddison Chris J, 2017, ICLR; Nichol Alex, 2018, ABS180302999 ARXIV; Pham H, 2018, PR MACH LEARN RES, V80; Rajeswaran A, 2019, ADV NEUR IN, V32; Ravi S., 2017, INT C LEARN REPR, P12; Real E, 2019, AAAI CONF ARTIF INTE, P4780; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wistuba Martin, 2019, ABS190303536 CORR; Xie SN, 2019, IEEE I CONF COMP VIS, P1284, DOI 10.1109/ICCV.2019.00137; Xie Sirui, 2019, ICLR, V1, P13; Xue QK, 2016, PROC IEEE MICR ELECT, P1, DOI 10.1109/MEMSYS.2016.7421541; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zellner Arnold, 1988, AM STAT, V42; Zhang C, 2019, INT C LEARN REPR; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	43	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902081
C	Shen, RQ; Lee, YT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shen, Ruoqi; Lee, Yin Tat			The Randomized Midpoint Method for Log-Concave Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				HIT-AND-RUN; RANDOM-WALKS; VOLUME; CONVERGENCE; ALGORITHM; HASTINGS	Sampling from log-concave distributions is a well researched problem that has many applications in statistics and machine learning. We study the distributions of the form p* proportional to exp(-f(x)), where f : R-d -> R has an L-Lipschitz gradient and is m-strongly convex. In our paper, we propose a Markov chain Monte Carlo (MCMC) algorithm based on the underdamped Langevin diffusion (ULD). It can achieve epsilon . D error (in 2-Wasserstein distance) in (O) over tilde(kappa(7/6)/epsilon(1/3) + kappa/epsilon(2/3)) steps, where D def = root d/m is the effective diameter of the problem and kappa def = L/m is the condition number. Our algorithm performs significantly faster than the previously best known algorithm for solving this problem, which requires (O) over tilde(kappa(1.5)/epsilon) steps [7, 15]. Moreover, our algorithm can be easily parallelized to require only O (kappa log 1/epsilon) parallel steps. To solve the sampling problem, we propose a new framework to discretize stochastic differential equations. We apply this framework to discretize and simulate ULD, which converges to the target distribution p*. The framework can be used to solve not only the log-concave sampling problem, but any problem that involves simulating (stochastic) differential equations.	[Shen, Ruoqi; Lee, Yin Tat] Univ Washington, Seattle, WA 98195 USA; [Lee, Yin Tat] Microsoft Res, New York, NY USA	University of Washington; University of Washington Seattle; Microsoft	Shen, RQ (corresponding author), Univ Washington, Seattle, WA 98195 USA.	shenr3@cs.washington.edu; yintat@uw.edu						Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Applegate David, 1991, P 23 STOC, P156, DOI [10.1145/103418.103439]5, DOI 10.1145/103418.103439]5]; BELISLE CJP, 1993, MATH OPER RES, V18, P255, DOI 10.1287/moor.18.2.255; Bou-Rabee N, 2013, IMA J NUMER ANAL, V33, P80, DOI 10.1093/imanum/drs003; Chatterji N., 2018, ARXIV180205431; Chen Z., 2019, APPROXIMATION RANDOM, V145, P64; Cheng X., 2018, ARXIV180501648; Cheng X., 2017, ARXIV170509048; Cheng X., 2017, ARXIV170703663; Cousins B, 2015, ACM S THEORY COMPUT, P539, DOI 10.1145/2746539.2746563; Dalalyan A., 2019, STOCHASTIC PROCESSES; Dalalyan A. S., 2018, ARXIV180709382; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Doob J. L., 1953, STOCHASTIC PROCESSES, V101; Dua D., 2017, UCI MACHINE LEARNING; Durmus A., 2016, ARXIV160501559; Durmus A., 2019, J MACHINE LEARNING R, V20, P1; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; Dwivedi R., 2018, ARXIV180102309; DYER M, 1991, J ACM, V38, P1, DOI 10.1145/102782.102783; Dyer M. E., 1991, P S APPL MATH, V44, P123, DOI 10.1090/psapm/044/1141926; Eberle A., 2017, ARXIV170301617; GELFAND SB, 1990, PROCEEDINGS OF THE 29TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, P220, DOI 10.1109/CDC.1990.203584; Jarner SF, 2000, STOCH PROC APPL, V85, P341, DOI 10.1016/S0304-4149(99)00082-4; Kannan R, 1997, RANDOM STRUCT ALGOR, V11, P1, DOI 10.1002/(SICI)1098-2418(199708)11:1<1::AID-RSA1>3.0.CO;2-X; Kramers HA, 1940, PHYSICA, V7, P284, DOI 10.1016/S0031-8914(40)90098-2; Lee Y.T., 2018, ARXIV181206243; Lee YT, 2018, ACM S THEORY COMPUT, P1115, DOI 10.1145/3188745.3188774; Lee YT, 2017, ACM S THEORY COMPUT, P927, DOI 10.1145/3055399.3055416; Lindelof E., 1894, CRAC, V116, P454; Lovasz L, 2006, SIAM J COMPUT, V35, P985, DOI 10.1137/S009753970544727X; Lovasz L, 2006, J COMPUT SYST SCI, V72, P392, DOI 10.1016/j.jcss.2005.08.004; LOVASZ L, 1993, RANDOM STRUCT ALGOR, V4, P359, DOI 10.1002/rsa.3240040402; Lovasz L., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P346, DOI 10.1109/FSCS.1990.89553; Lovasz L, 1999, MATH PROGRAM, V86, P443, DOI 10.1007/s101070050099; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; Ma Y, 2019, ARXIV190708990; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; Mangoubi O., 2017, ARXIV170807114; Mangoubi Oren, 2019, FASTER ALGORITHMS PO; Mangoubi Oren, 2018, ADV NEURAL INFORM PR, V31, P6027; Mengersen KL, 1996, ANN STAT, V24, P101; Mou Wenlong, 2019, ARXIV190810859; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2, P2, DOI DOI 10.1201/B10905-6; Pereyra M, 2016, STAT COMPUT, V26, P745, DOI 10.1007/s11222-015-9567-4; Picard Emile, 1898, AM J MATH, P87; Pillai NS, 2012, ANN APPL PROBAB, V22, P2320, DOI 10.1214/11-AAP828; Raginsky Maxim, 2017, ARXIV170203849; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Roberts GO, 1998, J ROY STAT SOC B, V60, P255, DOI 10.1111/1467-9868.00123; Roberts GO, 1996, BIOMETRIKA, V83, P95, DOI 10.1093/biomet/83.1.95; Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070; Vempala Santosh S, 2010, IARCS ANN C FDN SOFT; Xifara T, 2014, STAT PROBABIL LETT, V91, P14, DOI 10.1016/j.spl.2014.04.002; Yudin David, 1983, WILEY INTERSCIENCE S; Zhang Yuchen, 2017, ARXIV170205575; [No title captured]	57	8	8	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302013
C	Shi, YJ; Liu, L; Yu, X; Li, HD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Yujiao; Liu, Liu; Yu, Xin; Li, Hongdong			Spatial-Aware Feature Aggregation for Cross-View Image based Geo-Localization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent works show that it is possible to train a deep network to determine the geographic location of a ground-level image (e.g., a Google street-view panorama) by matching it against a satellite map covering the wide geographic area of interest. Conventional deep networks, which often cast the problem as a metric embedding task, however, suffer from poor performance in terms of low recall rates. One of the key reasons is the vast differences between the two view modalities, i.e., ground view versus aerial/satellite view. They not only exhibit very different visual appearances, but also have distinctive geometric configurations. Existing deep methods overlook those appearance and geometric differences, and instead use a brute force training procedure, leading to inferior performance. In this paper, we develop a new deep network to explicitly address these inherent differences between ground and aerial views. We observe that pixels lying on the same azimuth direction in an aerial image approximately correspond to a vertical image column in the ground view image. Thus, we propose a two-step approach to exploit this prior. The first step is to apply a regular polar transform to warp an aerial image such that its domain is closer to that of a ground-view panorama. Note that polar transform as a pure geometric transformation is agnostic to scene content, hence cannot bring the two domains into full alignment. Then, we add a subsequent spatial-attention mechanism which brings corresponding deep features closer in the embedding space. To improve the robustness of feature representation, we introduce a feature aggregation strategy via learning multiple spatial embeddings. By the above two-step approach, we achieve more discriminative deep representations, facilitating cross-view Geo-localization more accurate. Our experiments on standard benchmark datasets show significant performance boosting, achieving more than doubled recall rate compared with the previous state of the art. Remarkably, the recall rate@top-1 improves from 22.5% in [5] (or 40.7% in [11]) to 89.8% on CVUSA benchmark, and from 20.1% [5] to 81.0% on the new CVACT dataset.	[Shi, Yujiao] Australian Natl Univ, Canberra, ACT, Australia; Australian Ctr Robot Vision, Brisbane, Qld, Australia	Australian National University	Shi, YJ (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.	yujiao.shi@anu.edu.au; liu.liu@anu.edu.au; xin.yu@anu.edu.au; hongdong.li@anu.edu.au		Yu, Xin/0000-0002-0269-5649	China Scholarship Council [201708320417]; Australia Research Council ARC Centre of Excellence for Robotics Vision [CE140100016]; ARC-Discovery [DP 190102261]; ARC-LIEF [190100080]	China Scholarship Council(China Scholarship Council); Australia Research Council ARC Centre of Excellence for Robotics Vision; ARC-Discovery(Australian Research Council); ARC-LIEF(Australian Research Council)	This research is supported in part by China Scholarship Council (201708320417), the Australia Research Council ARC Centre of Excellence for Robotics Vision (CE140100016), ARC-Discovery (DP 190102261) and ARC-LIEF (190100080), and in part by a research gift from Baidu RAL (ApolloScapes-Robotics and Autonomous Driving Lab). The authors gratefully acknowledge the GPU gift donated by NVIDIA Corporation. We thank all anonymous reviewers for their constructive comments.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic R, 2018, IEEE T PATTERN ANAL, V40, P1437, DOI [10.1109/CVPR.2016.572, 10.1109/TPAMI.2017.2711011]; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Castaldo F, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P1044, DOI 10.1109/ICCVW.2015.137; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Hu SX, 2018, PROC CVPR IEEE, P7258, DOI 10.1109/CVPR.2018.00758; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lazebnik S., 2006, 2006 IEEE COMPUTER S, V2, DOI [DOI 10.1109/CVPR.2006.68, 10.1109/cvpr.2006.68]; Lin TY, 2013, PROC CVPR IEEE, P891, DOI 10.1109/CVPR.2013.120; Liu L, 2019, PROC CVPR IEEE, P5607, DOI 10.1109/CVPR.2019.00577; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Mousavian Arsalan, 2016, ARXIV160900278; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shi Yujiao, 2019, ARXIV; Sivic J, 2006, LECT NOTES COMPUT SC, V4170, P127; Tian YR, 2019, PROC CVPR IEEE, P11008, DOI 10.1109/CVPR.2019.01127; Vo NN, 2016, LECT NOTES COMPUT SC, V9905, P494, DOI 10.1007/978-3-319-46448-0_30; Workman S, 2015, IEEE COMPUT SOC CONF; Workman S, 2015, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2015.451; Yu X, 2019, IEEE INT CONF COMP V, P2893, DOI 10.1109/ICCVW.2019.00351; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhai M, 2017, PROC CVPR IEEE, P4132, DOI 10.1109/CVPR.2017.440; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	25	8	8	3	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901069
C	Smith, KA; Mei, LJ; Yao, SY; Wu, JJ; Spelke, E; Tenenbaum, JB; Ullman, TD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Smith, Kevin A.; Mei, Lingjie; Yao, Shunyu; Wu, Jiajun; Spelke, Elizabeth; Tenenbaum, Joshua B.; Ullman, Tomer D.			Modeling Expectation Violation in Intuitive Physics with Coarse Probabilistic Object Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFANTS; PERMANENCE	From infancy, humans have expectations about how objects will move and interact. Even young children expect objects not to move through one another, teleport, or disappear. They are surprised by mismatches between physical expectations and perceptual observations, even in unfamiliar scenes with completely novel objects. A model that exhibits human-like understanding of physics should be similarly surprised, and adjust its beliefs accordingly. We propose ADEPT, a model that uses a coarse (approximate geometry) object-centric representation for dynamic 3D scene understanding. Inference integrates deep recognition networks, extended probabilistic physical simulation, and particle filtering for forming predictions and expectations across occlusion. We also present a new test set for measuring violations of physical expectations, using a range of scenarios derived from developmental psychology. We systematically compare ADEPT, baseline models, and human expectations on this test set. ADEPT outperforms standard network architectures in discriminating physically implausible scenes, and often performs this discrimination at the same level as people.	[Smith, Kevin A.; Tenenbaum, Joshua B.] MIT, BCS, Cambridge, MA 02139 USA; [Smith, Kevin A.; Spelke, Elizabeth; Tenenbaum, Joshua B.; Ullman, Tomer D.] Ctr Brains Minds & Machines, Cambridge, MA 02139 USA; [Mei, Lingjie; Wu, Jiajun; Tenenbaum, Joshua B.] MIT, CSAIL, Cambridge, MA 02139 USA; [Yao, Shunyu] Princeton Univ, Princeton, NJ 08544 USA; [Spelke, Elizabeth; Ullman, Tomer D.] Harvard Psychol, Cambridge, MA USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Princeton University	Smith, KA (corresponding author), MIT, BCS, Cambridge, MA 02139 USA.; Smith, KA (corresponding author), Ctr Brains Minds & Machines, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022		Army Research Office [W911NF-18-1-0019]; NSF STC award [CCF-1231216]; ONR MURI [N00014-13-1-0333]; Honda's Curious Minded Machines research grant	Army Research Office; NSF STC award(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Honda's Curious Minded Machines research grant	This work was sponsored by the Army Research Office accomplished under Grant Number W911NF-18-1-0019, by NSF STC award CCF-1231216, by ONR MURI N00014-13-1-0333, and by Honda's Curious Minded Machines research grant.	BAILLARGEON R, 1985, COGNITION, V20, P191, DOI 10.1016/0010-0277(85)90008-3; BAILLARGEON R, 1987, DEV PSYCHOL, V23, P655, DOI 10.1037/0012-1649.23.5.655; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W, 2016, ARXIV161200222; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Chang Angel X., 2015, ARXIV151203012CSGR P; Chang MB., 2017, 5 INT C LEARN REPR I; Che Chengqian, 2018, ARXIV180910820; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Eslami SM, 2016, NEURIPS, V1; Fragkiadaki K., 2016, 6 INT C LEARN REPR I; Gureckis TM, 2016, BEHAV RES METHODS, V48, P829, DOI 10.3758/s13428-015-0642-8; Hamrick JB, 2016, COGNITION, V157, P61, DOI 10.1016/j.cognition.2016.08.012; Huang SY, 2018, ADV NEUR IN, V31; Kidd C, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0036399; Kloss A., 2017, ARXIV171004102; Kulkarni TD, 2015, ADV NEUR IN, V28; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lerer A, 2016, PR MACH LEARN RES, V48; Liu JQ, 2016, INT C HIGH PERFORM, P2, DOI [10.1109/HiPC.2016.011, 10.1109/HiPC.2016.31]; Mottaghi R., 2016, CVPR; Mrowca Damian, 2018, ARXIV180608047; Osiurak F, 2016, PSYCHOL REV, V123, P534, DOI 10.1037/rev0000027; Piloto L., 2018, ARXIV180401128; Riochet Ronan, 2018, ARXIV180307616; Smith KA, 2013, TOP COGN SCI, V5, P185, DOI 10.1111/tops.12009; Spelke ES, 2007, DEVELOPMENTAL SCI, V10, P89, DOI 10.1111/j.1467-7687.2007.00569.x; SPELKE ES, 1992, PSYCHOL REV, V99, P605, DOI 10.1037/0033-295X.99.4.605; SPELKE ES, 1995, BRIT J DEV PSYCHOL, V13, P113, DOI 10.1111/j.2044-835X.1995.tb00669.x; Stahl AE, 2015, SCIENCE, V348, P91, DOI 10.1126/science.aaa3799; Sun BH, 2017, INT SYM COMPUT INTEL, P28, DOI 10.1109/ISCID.2017.31; Teglas E, 2011, SCIENCE, V332, P1054, DOI 10.1126/science.1196404; Ullman TD, 2018, COGNITIVE PSYCHOL, V104, P57, DOI 10.1016/j.cogpsych.2017.05.006; Vul E., 2009, ADV NEURAL INFORM PR, V22, P1; Wu JJ, 2017, ADV NEUR IN, V30; WYNN K, 1992, NATURE, V358, P749, DOI 10.1038/358749a0; Xu F, 1996, COGNITIVE PSYCHOL, V30, P111, DOI 10.1006/cogp.1996.0005; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zheng B, 2015, INT J COMPUT VISION, V112, P221, DOI 10.1007/s11263-014-0795-4	41	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900056
C	Song, J; Chen, YX; Wang, XC; Shen, CC; Song, ML		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Jie; Chen, Yixin; Wang, Xinchao; Shen, Chengchao; Song, Mingli			Deep Model Transferability from Attribution Maps	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Exploring the transferability between heterogeneous tasks sheds light on their intrinsic interconnections, and consequently enables knowledge transfer from one task to another so as to reduce the training effort of the latter. In this paper, we propose an embarrassingly simple yet very efficacious approach to estimating the transferability of deep networks, especially those handling vision tasks. Unlike the seminal work of taskonomy that relies on a large number of annotations as supervision and is thus computationally cumbersome, the proposed approach requires no human annotations and imposes no constraints on the architectures of the networks. This is achieved, specifically, via projecting deep networks into a model space, wherein each network is treated as a point and the distances between two points are measured by deviations of their produced attribution maps. The proposed approach is several-magnitude times faster than taskonomy, and meanwhile preserves a task-wise topological structure highly similar to the one obtained by taskonomy. Code is available at https://github.com/zju-vipa/TransferbilityFromAttributionMaps.	[Song, Jie; Chen, Yixin; Shen, Chengchao; Song, Mingli] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China; [Wang, Xinchao] Stevens Inst Technol, Hoboken, NJ 07030 USA; [Song, Jie; Song, Mingli] Alibaba Zhejiang Univ Joint Inst Frontier Technol, Hangzhou, Zhejiang, Peoples R China	Zhejiang University; Stevens Institute of Technology	Song, J (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.; Song, J (corresponding author), Alibaba Zhejiang Univ Joint Inst Frontier Technol, Hangzhou, Zhejiang, Peoples R China.	sjie@zju.edu.cn; chenyix@zju.edu.cn; xinchao.wang@stevens.edu; chengchaoshen@zju.edu.cn; brooksong@zju.edu.cn	Shen, Chengchao/GNP-8223-2022; Song, Jie/GLR-2301-2022; Wang, Xinchao/L-7655-2018	Wang, Xinchao/0000-0003-0057-1404; Song, Jie/0000-0003-3671-6521	National Key Research and Development Program [2016YFB1200203]; National Natural Science Foundation of China [61572428]; Key Research and Development Program of Zhejiang Province [2018C01004]; Major Scientifc Research Project of Zhejiang Lab [2019KD0AC01]	National Key Research and Development Program; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Key Research and Development Program of Zhejiang Province; Major Scientifc Research Project of Zhejiang Lab	This work is supported by National Key Research and Development Program (2016YFB1200203), National Natural Science Foundation of China (61572428), Key Research and Development Program of Zhejiang Province (2018C01004), and the Major Scientifc Research Project of Zhejiang Lab (No. 2019KD0AC01).	Ancona Marco, 2018, ICLR, DOI DOI 10.1109/TNSE.2020.2996738; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Azizpour H, 2016, IEEE T PATTERN ANAL, V38, P1790, DOI 10.1109/TPAMI.2015.2500224; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Dwivedi Kshitij, 2019, IEEE C COMP VIS PATT; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33; He K., 2018, CORR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu PY, 2017, PROC CVPR IEEE, P1522, DOI 10.1109/CVPR.2017.166; Huh Minyoung, 2016, ARXIV160808614; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Parisotto Emilio, 2015, ARXIV151106342; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Richter SR, 2017, IEEE I CONF COMP VIS, P2232, DOI 10.1109/ICCV.2017.243; Romero Adriana, 2014, ARXIV14126550; Rusu A. A., 2016, ARXIV160604671; Shrikumar A, 2017, PR MACH LEARN RES, V70; Shrikumar Avanti, 2016, ARXIV160501713; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; ZAGORUYKO S, 2017, CORR; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/nmeth.3547, 10.1038/NMETH.3547]; Zhu X., 2018, ADV NEURAL INFORM PR, P7527; Zintgraf Luisa M., 2017, P ICLR	41	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306021
C	van Steenkiste, S; Locatello, F; Schmidhuber, J; Bachem, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		van Steenkiste, Sjoerd; Locatello, Francesco; Schmidhuber, Juergen; Bachem, Olivier			Are Disentangled Representations Helpful for Abstract Visual Reasoning?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A disentangled representation encodes information about the salient factors of variation in the data independently. Although it is often argued that this representational format is useful in learning to solve many real-world down-stream tasks, there is little empirical evidence that supports this claim. In this paper, we conduct a large-scale study that investigates whether disentangled representations are more suitable for abstract reasoning tasks. Using two new tasks similar to Raven's Progressive Matrices, we evaluate the usefulness of the representations learned by 360 state-of-the-art unsupervised disentanglement models. Based on these representations, we train 3600 abstract reasoning models and observe that disentangled representations do in fact lead to better down-stream performance. In particular, they enable quicker learning using fewer samples.	[van Steenkiste, Sjoerd] IDSIA, USI, SUPSI, Manno, Switzerland; [Locatello, Francesco] Swiss Fed Inst Technol, MPI IS, Zurich, Switzerland; [Schmidhuber, Juergen] IDSIA, USI, SUPSI, NNAISENSE, Manno, Switzerland; [Bachem, Olivier] Google Res, Brain Team, Mountain View, CA USA	Universita della Svizzera Italiana; Swiss Federal Institutes of Technology Domain; ETH Zurich; Universita della Svizzera Italiana; Google Incorporated	van Steenkiste, S (corresponding author), IDSIA, USI, SUPSI, Manno, Switzerland.	sjoerd@idsia.ch; locatelf@ethz.ch; juergen@idsia.ch; bachem@google.com	Locatello, Francesco/GQY-6025-2022	Locatello, Francesco/0000-0002-4850-0683	Max Planck ETH Center for Learning Systems; Google Ph.D. Fellowship; Swiss National Science Foundation [200021_165675/1]	Max Planck ETH Center for Learning Systems; Google Ph.D. Fellowship(Google Incorporated); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	The authors thank Adam Santoro, Josip Djolonga, Paulo Rauber and the anonymous reviewers for helpful discussions and comments. This research was partially supported by the Max Planck ETH Center for Learning Systems, a Google Ph.D. Fellowship (to Francesco Locatello), and the Swiss National Science Foundation (grant 200021_165675/1 to Jurgen Schmidhuber). This work was partially done while Francesco Locatello was at Google Research.	Achille A, 2018, ADV NEUR IN, V31; Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085; Barlow HB, 1989, NEURAL COMPUT, V1, P412, DOI 10.1162/neco.1989.1.3.412; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; Barrett D.G.T., 2018, ARXIV180704225; Battaglia Peter W, 2018, ARXIV180601261; Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI [DOI 10.1038/NATURE14539, 10.1016/j.asoc.2014.05.028, DOI 10.1016/J.ASOC.2014.05.028]; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bouchacourt D, 2018, AAAI CONF ARTIF INTE, P2095; Burgess Christopher P, 2017, NEUR INF PROC SYST N; Chen T.Q., 2018, NEURIPS, P2610; Cheung B., 2014, P INT C LEARN REPR W; Cohen T, 2014, PR MACH LEARN RES, V32, P1755; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Deng ZW, 2017, PROC CVPR IEEE, P6014, DOI 10.1109/CVPR.2017.637; Eastwood Cian, 2018, REPRESENTATIONS; Eslami SM, 2016, NEURIPS, V1; Esmaeili B., 2019, PROC INT C ARTIF INT; Fortuin V., 2019, INT C LEARN REPR; Greff K., 2017, ADV NEURAL INFORM PR, P6691, DOI DOI 10.5555/3295222.3295414; Greff K., 2016, ADV NEURAL INFORM PR; Greff K, 2019, PR MACH LEARN RES, V97; Gresele Luigi, 2019, C UNC ART INT UAI; Higgins I., 2018, ICLR; Higgins I., 2017, P INT C LEARN REPR T; Higgins I., 2018, ARXIV PREPRINT ARXIV; Higgins I, 2017, PR MACH LEARN RES, V70; Hill Felix, 2019, INT C LEARN REPR; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hochreiter S, 1999, NEURAL COMPUT, V11, P679, DOI 10.1162/089976699300016629; Hochreiter S, 1999, ISCAS '99: PROCEEDINGS OF THE 1999 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL 5, P53, DOI 10.1109/ISCAS.1999.777509; Hsieh JT, 2018, ADV NEUR IN, V31; Hyvarinen Aapo, 2019, ARTIFICIAL INTELLIGE; Hyvirinen Aapo, 1999, NEURAL NETWORKS; Jutten C., 2003, P 4 INT S IND COMP A, P245; Karaletsos Theofanis, 2016, INT C LEARN REPR; Kemp C, 2008, P NATL ACAD SCI USA, V105, P10687, DOI 10.1073/pnas.0802631105; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Klys Jack, 2018, ADV NEURAL INFORM PR; Kulkarni TD, 2015, ADV NEUR IN, V28; Kumar A, 2018, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON FUTURE INDUSTRIAL COMMUNICATION NETWORKS (FICN'18), P1, DOI 10.1145/3243318.3243327; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lim WY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P787, DOI 10.1145/3132847.3132995; Locatello F, 2019, ARXIV190501258; Locatello Francesco, 2018, INT C LEARN REPR WOR; Locatello Francesco, 2018, P 36 INT C MACH LEAR, V97; Lopez R, 2018, ADV NEUR IN, V31; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Peters C, 2017, TLS-TIMES LIT SUPPL, P6; Raposo D, 2017, INT C LEARN REPR WOR; Raven JC, 1941, BRIT J MED PSYCHOL, V19, P137, DOI 10.1111/j.2044-8341.1941.tb00316.x; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Ridgeway K, 2018, ADV NEUR IN, V31; Ruiz A., 2019, ARXIV190108534; Santoro A, 2017, ADV NEUR IN, V30; Schmidhuber J, 1996, NEURAL COMPUT, V8, P773, DOI 10.1162/neco.1996.8.4.773; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Steenbrugge Xander, 2018, NEUR INF PROC SYST N; Suter R, 2019, PR MACH LEARN RES, V97; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Tschannen Michael, 2018, NEUR INF PROC SYST N; Van Steenkiste S., 2018, 6 INT C LEARNING REP, P1; van Steenkiste Sjoerd, 2018, NEUR INF PROC SYST N; Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; Whitney William F, 2016, INT C LEARN REPR WOR; Yang Jimei, 2015, NIPS	79	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905085
C	Vertes, E; Sahani, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vertes, Eszter; Sahani, Maneesh			A neurally plausible model learns successor representations in partially observable environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICTION; UNCERTAINTY; ERRORS	Animals need to devise strategies to maximize returns while interacting with their environment based on incoming noisy sensory observations. Task-relevant states, such as the agent's location within an environment or the presence of a predator, are often not directly observable but must be inferred using available sensory information. Successor representations (SR) have been proposed as a middle-ground between model-based and model-free reinforcement learning strategies, allowing for fast value computation and rapid adaptation to changes in the reward function or goal locations. Indeed, recent studies suggest that features of neural responses are consistent with the SR framework. However, it is not clear how such representations might be learned and computed in partially observed, noisy environments. Here, we introduce a neurally plausible model using distributional success or features, which builds on the distributed distributional code for the representation and computation of uncertainty, and which allows for efficient value function computation in partially observed environments via the successor representation. We show that distributional successor features can support reinforcement learning in noisy environments in which direct learning of successful policies is infeasible.	[Vertes, Eszter; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England	University of London; University College London	Vertes, E (corresponding author), UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.	eszter@gatsby.ucl.ac.uk; maneesh@gatsby.ucl.ac.uk						[Anonymous], 2014, ADV NEURAL INFORM PR; Babayan BM, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-04397-0; Barreto A., 2019, ARXIV190110964CS; Daw ND, 2011, NEURON, V69, P1204, DOI 10.1016/j.neuron.2011.02.027; Daw ND, 2005, NAT NEUROSCI, V8, P1704, DOI 10.1038/nn1560; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429; Gardner MPH, 2018, P ROY SOC B-BIOL SCI, V285, DOI 10.1098/rspb.2018.1645; Gershman SJ, 2018, J NEUROSCI, V38, P7193, DOI 10.1523/JNEUROSCI.0151-18.2018; Glascher J, 2010, NEURON, V66, P585, DOI 10.1016/j.neuron.2010.04.016; Grieves RM, 2016, ELIFE, V5, DOI 10.7554/eLife.15986; Gupta AS, 2010, NEURON, V65, P695, DOI 10.1016/j.neuron.2010.01.034; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Jadhav SP, 2012, SCIENCE, V336, P1454, DOI 10.1126/science.1217230; Kulkarni T. D., 2016, ARXIV160602396CSSTAT; Lak A, 2017, CURR BIOL, V27, P821, DOI 10.1016/j.cub.2017.02.026; Liu YZ, 2019, CELL, V178, P640, DOI 10.1016/j.cell.2019.06.012; Madl T, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0089762; Momennejad I, 2017, NAT HUM BEHAV, V1, P680, DOI 10.1038/s41562-017-0180-8; Olafsdottir HF, 2015, ELIFE, V4, DOI 10.7554/eLife.06063; Rao RPN, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00146; Russek EM, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005768; Sahani M, 2003, NEURAL COMPUT, V15, P2255, DOI 10.1162/089976603322362356; Sarno S, 2017, P NATL ACAD SCI USA, V114, pE10494, DOI 10.1073/pnas.1712479114; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; Stachenfeld KL, 2017, NAT NEUROSCI, V20, P1643, DOI 10.1038/nn.4650; Starkweather CK, 2017, NAT NEUROSCI, V20, P581, DOI 10.1038/nn.4520; Stella Federico, 2019, NEURON; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton R. S., 1998, INTRO REINFORCEMENT, V135; Takahashi YK, 2017, NEURON, V95, P1395, DOI 10.1016/j.neuron.2017.08.025; Vertes E, 2018, ADV NEURAL INFORM PR, P4166; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zemel RS, 1998, NEURAL COMPUT, V10, P403, DOI 10.1162/089976698300017818	36	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905037
C	Wang, DL; Tang, ZY; Bajaj, C; Liu, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Dilin; Tang, Ziyang; Bajaj, Chandrajit; Liu, Qiang			Stein Variational Gradient Descent with Matrix-Valued Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Stein variational gradient descent (SVGD) is a particle-based inference algorithm that leverages gradient information for efficient approximate inference. In this work, we enhance SVGD by leveraging preconditioning matrices, such as the Hessian and Fisher information matrix, to incorporate geometric information into SVGD updates. We achieve this by presenting a generalization of SVGD that replaces the scalar-valued kernels in vanilla SVGD with more general matrix-valued kernels. This yields a significant extension of SVGD, and more importantly, allows us to flexibly incorporate various preconditioning matrices to accelerate the exploration in the probability landscape. Empirical results show that our method outperforms vanilla SVGD and a variety of baseline approaches over a range of real-world Bayesian inference tasks.	[Wang, Dilin; Tang, Ziyang; Bajaj, Chandrajit; Liu, Qiang] UT Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Wang, DL (corresponding author), UT Austin, Dept Comp Sci, Austin, TX 78712 USA.	dilin@cs.utexas.edu; ztang@cs.utexas.edu; bajaj@cs.utexas.edu; lqiang@cs.utexas.edu		Bajaj, Chandrajit/0000-0002-9619-3278	NSF CRII [1830161]; NSF CAREER [1846421]; Google Cloud and Amazon Web Services (AWS)	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Cloud and Amazon Web Services (AWS)(Google Incorporated)	This work is supported in part by NSF CRII 1830161 and NSF CAREER 1846421. We would like to acknowledge Google Cloud and Amazon Web Services (AWS) for their support.	Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Bengio Y., 2014, ARXIV14061078; Berlinet A., 2011, REPRODUCING KERNEL H; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Carmeli C, 2006, ANAL APPL, V4, P377, DOI 10.1142/S0219530506000838; Chen WY, 2019, PR MACH LEARN RES, V97; Detommaso G., 2018, ADV NEURAL INFORM PR, P9187; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gan Z, 2016, ACL; Gretton A, 2012, J MACH LEARN RES, V13, P723; Haarnoja T, 2017, PR MACH LEARN RES, V70; Han J, 2018, PR MACH LEARN RES, V80; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hu MQ, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P755; Kim Taesup, 2018, ARXIV180603836; Liu CY, 2018, AAAI CONF ARTIF INTE, P346; Liu M, 2018, INT SYM COMPUT INTEL, P206, DOI 10.1109/ISCID.2018.10148; Liu Q., 2016, ADV NEURAL INFORM PR, V29, P2378; Liu Q., 2018, ADV NEURAL INFORM PR, P8867; Liu Q, 2017, ADV NEUR IN, V30; Martens J., 2018, ICLR; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2; Pang B., 2004, ANN M ASS COMP LING, P271, DOI DOI 10.3115/1218955.1218990; Pang B., 2005, P 43 ANN M ASS COMP, V43, P115, DOI DOI 10.3115/1219840.1219855; Paulsen V.I., 2016, INTRO THEORY REPRODU, V152; Pu Y, 2017, ADV NEURAL INF PROCE, P4236; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang D., 2016, ARXIV161101722; Wiebe J, 2005, LANG RESOUR EVAL, V39, P165, DOI 10.1007/s10579-005-7880-9; Zhuo J., 2018, P 35 INT C MACH LEAR, P6013	32	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID	31857781				2022-12-19	WOS:000534424307081
C	Wang, YR; Xia, YC; Tian, F; Gao, F; Qin, T; Zhai, CX; Liu, TY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Yiren; Xia, Yingce; Tian, Fei; Gao, Fei; Qin, Tao; Zhai, ChengXiang; Liu, Tie-Yan			Neural Machine Translation with Soft Prototype	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural machine translation models usually use the encoder-decoder framework and generate translation from left to right (or right to left) without fully utilizing the target-side global information. A few recent approaches seek to exploit the global information through two-pass decoding, yet have limitations in translation quality and model efficiency. In this work, we propose a new framework that introduces a soft prototype into the encoder-decoder architecture, which allows the decoder to have indirect access to both past and future information, such that each target word can be generated based on the better global understanding. We further provide an efficient and effective method to generate the prototype. Empirical studies on various neural machine translation tasks show that our approach brings substantial improvement in generation quality over the baseline model, with little extra cost in storage and inference time, demonstrating the effectiveness of our proposed framework. Specially, we achieve state-of-the-art results on WMT2014, 2015 and 2017 English -> German translation.	[Wang, Yiren; Zhai, ChengXiang] Univ Illinois, Urbana, IL 61801 USA; [Xia, Yingce; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Redmond, WA 98052 USA; [Tian, Fei] Facebook, Menlo Pk, CA USA; [Gao, Fei] Chinese Acad Sci, Inst Comp Technol, Beijing, Peoples R China	University of Illinois System; University of Illinois Urbana-Champaign; Microsoft; Facebook Inc; Chinese Academy of Sciences; Institute of Computing Technology, CAS	Xia, YC (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	yiren@illinois.edu; yingce.xia@microsoft.com; feitia@fb.com; gaofei17n@ict.ac.cn; taoqin@microsoft.com; czhai@illinois.edu; tie-yan.liu@microsoft.com		Qin, Tao/0000-0002-9095-0776				Bahdanau D., 2015, ICLR; Edunov Sergey, 2018, EMNLP; Gehring Jonas, 2017, ICML; Guu  K., 2018, T ASSOC COMPUT LING, V6, P437, DOI [10.1162/tacl_a_00030, DOI 10.1162/TACL_A_00030]; Junczys-Dowmunt Marcin, 2018, ARXIV180900196; Kingma D.P, P 3 INT C LEARNING R; Koehn P., 2004, P EMNLP, P8014; Lample G, 2018, ICLR; Lample Guillaume, 2018, P 2018 C EMP METH NA, P5039; Li J., 2018, ACL; Niehues Jan, 2016, P COLING 2016 26 INT, P1828; Ren S., 2019, AAAI; Sennrich R., 2016, ACL; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xia YC, 2017, ADV NEUR IN, V30; Yang Z., 2018, ARXIV180409057; Zhang H., 2019, ARXIV190209243; Zhang Xiao, 2018, AAAI	21	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306033
C	You, RH; Zhang, ZH; Wang, ZY; Dai, SY; Mamitsuka, H; Zhu, SF		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		You, Ronghui; Zhang, Zihan; Wang, Ziye; Dai, Suyang; Mamitsuka, Hiroshi; Zhu, Shanfeng			AttentionXML: Label Tree-based Attention-Aware Deep Model for High-Performance Extreme Multi-Label Text Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Extreme multi-label text classification (XMTC) is an important problem in the era of big data, for tagging a given text with the most relevant multiple labels from an extremely large-scale label set. XMTC can be found in many applications, such as item categorization, web page tagging, and news annotation. Traditionally most methods used bag-of-words (BOW) as inputs, ignoring word context as well as deep semantic information. Recent attempts to overcome the problems of BOW by deep learning still suffer from 1) failing to capture the important subtext for each label and 2) lack of scalability against the huge number of labels. We propose a new label tree-based deep learning model for XMTC, called AttentionXML, with two unique features: 1) a multi-label attention mechanism with raw text as input, which allows to capture the most relevant part of text to each label; and 2) a shallow and wide probabilistic label tree (PLT), which allows to handle millions of labels, especially for "tail labels". We empirically compared the performance of AttentionXML with those of eight state-of-the-art methods over six benchmark datasets, including Amazon-3M with around 3 million labels. AttentionXML outperformed all competing methods under all experimental settings. Experimental results also show that AttentionXML achieved the best performance against tail labels among label tree-based methods.	[You, Ronghui; Zhang, Zihan; Dai, Suyang; Zhu, Shanfeng] Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China; [Wang, Ziye] Fudan Univ, Sch Math Sci, Ctr Computat Syst Biol, Shanghai, Peoples R China; [Zhu, Shanfeng] Fudan Univ, Shanghai Inst Artificial Intelligence Algorithms, Shanghai, Peoples R China; [Zhu, Shanfeng] Fudan Univ, ISTBI, Shanghai, Peoples R China; [Zhu, Shanfeng] Fudan Univ, Key Lab Computat Neurosci & Brain Inspired Intell, Shanghai, Peoples R China; [Mamitsuka, Hiroshi] Kyoto Univ, Inst Chem Res, Bioinformat Ctr, Kyoto, Japan; [Mamitsuka, Hiroshi] Aalto Univ, Dept Comp Sci, Espoo, Finland; [Mamitsuka, Hiroshi] Aalto Univ, Dept Comp Sci, Helsinki, Finland	Fudan University; Fudan University; Fudan University; Fudan University; Fudan University; Kyoto University; Aalto University; Aalto University	Zhu, SF (corresponding author), Fudan Univ, Sch Comp Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China.; Zhu, SF (corresponding author), Fudan Univ, Shanghai Inst Artificial Intelligence Algorithms, Shanghai, Peoples R China.; Zhu, SF (corresponding author), Fudan Univ, ISTBI, Shanghai, Peoples R China.; Zhu, SF (corresponding author), Fudan Univ, Key Lab Computat Neurosci & Brain Inspired Intell, Shanghai, Peoples R China.	rhyou18@fudan.edu.cn; zhangzh17@fudan.edu.cn; zywang17@fudan.edu.cn; sydai16@fudan.edu.cn; mami@kuicr.kyoto-u.ac.jp; zhusf@fudan.edu.cn			National Natural Science Foundation of China [61572139, 61872094]; Shanghai Municipal Science and Technology Major Project [2017SHZDZX01, 2018SHZDZX01]; 111 Project [B18015]; key project of Shanghai Science Technology [16JC1420402]; JST ACCEL [JPMJAC1503]; MEXT Kakenhi [16H02868, 19H04169]; FiDiPro by Business Finland; AIPSE by Academy of Finland	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Municipal Science and Technology Major Project; 111 Project(Ministry of Education, China - 111 Project); key project of Shanghai Science Technology; JST ACCEL(Japan Science & Technology Agency (JST)); MEXT Kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); FiDiPro by Business Finland; AIPSE by Academy of Finland	S. Z. is supported by National Natural Science Foundation of China (No. 61572139 and No. 61872094) and Shanghai Municipal Science and Technology Major Project (No. 2017SHZDZX01). R. Y., Z. Z., Z. W., S. Y. are supported by the 111 Project (NO. B18015), the key project of Shanghai Science & Technology (No. 16JC1420402), Shanghai Municipal Science and Technology Major Project (No. 2018SHZDZX01) and ZJLab. H.M. has been supported in part by JST ACCEL [grant number JPMJAC1503], MEXT Kakenhi [grant numbers 16H02868 and 19H04169], FiDiPro by Tekes (currently Business Finland) and AIPSE by Academy of Finland.	Babbar R., 2016, J MACHINE LEARNING R, V17, P3350; Babbar R., 2013, ANN C NEURAL INFORM; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Babbar Rohit, 2019, MACH LEARN, P1; Bahdanau D., 2015, P 3 INT C LEARNING R; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Izmailov Pavel, 2018, ARXIV180305407; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Jasinska K, 2016, PR MACH LEARN RES, V48; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Khandagale S., 2019, ARXIV190408249; Kingma D.P, P 3 INT C LEARNING R; Lin J., 2018, P C EMP METH NAT LAN; Lin Z., 2017, P 5 INT C LEARN REPR; Liu JZ, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P115, DOI 10.1145/3077136.3080834; McAuley Julian, 2013, P 7 ACM C REC SYST A, DOI DOI 10.1145/2507157.2507163; Mencia EL, 2008, LECT NOTES ARTIF INT, V5212, P50, DOI 10.1007/978-3-540-87481-2_4; Nam J., 2017, ADV NEURAL INFORM PR, P5413; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Prabhu Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P993, DOI 10.1145/3178876.3185998; Prabhu Y, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P441, DOI 10.1145/3159652.3159660; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tagami Y, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P455, DOI 10.1145/3097983.3097987; Wydmuch M., 2018, ADV NEURAL INFORM PR; Yang Pengcheng, 2018, P 27 INT C COMP LING, P3915, DOI DOI 10.48550/ARXIV.1806.04822; Yen IEH, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P545, DOI 10.1145/3097983.3098083; Yen IEH, 2016, PR MACH LEARN RES, V48; Zubiaga A., 2012, ARXIV12025469	32	8	8	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305077
C	Yu, HB; Chen, YZ; Dai, ZX; Low, BKH; Jaillet, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Haibin; Chen, Yizhou; Dai, Zhongxiang; Low, Bryan Kian Hsiang; Jaillet, Patrick			Implicit Posterior Variational Inference for Deep Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A multi-layer deep Gaussian process (DGP) model is a hierarchical composition of GP models with a greater expressive power. Exact DGP inference is intractable, which has motivated the recent development of deterministic and stochastic approximation methods. Unfortunately, the deterministic approximation methods yield a biased posterior belief while the stochastic one is computationally costly. This paper presents an implicit posterior variational inference (IPVI) framework for DGPs that can ideally recover an unbiased posterior belief and still preserve time efficiency. Inspired by generative adversarial networks, our IPVI framework achieves this by casting the DGP inference problem as a two-player game in which a Nash equilibrium, interestingly, coincides with an unbiased posterior belief. This consequently inspires us to devise a best-response dynamics algorithm to search for a Nash equilibrium (i.e., an unbiased posterior belief). Empirical evaluation shows that IPVI outperforms the state-of-the-art approximation methods for DGPs.	[Yu, Haibin; Chen, Yizhou; Dai, Zhongxiang; Low, Bryan Kian Hsiang] Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore; [Jaillet, Patrick] MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	National University of Singapore; Massachusetts Institute of Technology (MIT)	Yu, HB (corresponding author), Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.	haibin@comp.nus.edu.sg; ychen041@comp.nus.edu.sg; daiz@comp.nus.edu.sg; lowkh@comp.nus.edu.sg; jaillet@mit.edu			National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) program; National Research Foundation Singapore under its AI Singapore Programme [AISG-GC-2019-002]; Singapore Ministry of Education Academic Research Fund Tier 2 [MOE2016-T2-2-156]; Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG	National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) program(National Research Foundation, Singapore); National Research Foundation Singapore under its AI Singapore Programme(National Research Foundation, Singapore); Singapore Ministry of Education Academic Research Fund Tier 2(Ministry of Education, Singapore); Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG(Singapore-MIT Alliance for Research & Technology Centre (SMART))	This research is supported by the National Research Foundation, Prime Minister's Office, Singapore under its Campus for Research Excellence and Technological Enterprise (CREATE) program, Singapore-MIT Alliance for Research and Technology (SMART) Future Urban Mobility (FM) IRG, National Research Foundation Singapore under its AI Singapore Programme Award No. AISG-GC-2019-002, and the Singapore Ministry of Education Academic Research Fund Tier 2, MOE2016-T2-2-156.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2019, P AAAI; [Anonymous], 2013, P ICLR; [Anonymous], 2016, ARXIV160903499; [Anonymous], 2015, LNCS; Awerbuch B, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P264; Bui TD, 2016, PR MACH LEARN RES, V48; Cao N., 2013, P 2013 INT C AUT AG, P7; Chen J., 2013, P ROB SCI SYST C RSS; Chen JW, 2012, FPGA 12: PROCEEDINGS OF THE 2012 ACM-SIGDA INTERNATIONAL SYMPOSIUM ON FIELD PROGRAMMABLE GATE ARRAYS, P163; Chen J, 2015, IEEE T AUTOM SCI ENG, V12, P901, DOI 10.1109/TASE.2015.2422852; CHEN JT, 2013, P UAI, V826, P152, DOI DOI 10.4028/WWW.SCIENTIFIC.NET/AMR.826.152; Cutajar K., 2017, P INT C MACH LEARN, V70, P884; Dai Z., 2016, P ICLR; Dai ZX, 2019, PR MACH LEARN RES, V97; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Daxberger EA, 2017, PR MACH LEARN RES, V70; Deisenroth MP, 2015, PR MACH LEARN RES, V37, P1481; Duvenaud D, 2014, JMLR WORKSH CONF PRO, V33, P202; Gal Yarin, 2014, ADV NEURAL INF PROCE, V2, P3257; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HAVASI M, 2018, P NEURIPS, P7517; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, P280; Hoang Q. M., 2019, P ICML, P2742; Hoang TN, 2015, PR MACH LEARN RES, V37, P569; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Huszar F., 2017, ARXIV170208235; Karras T., 2018, P ICLR; Lawrence J, 2014, ARXIV14121370; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Ling CK, 2016, AAAI CONF ARTIF INTE, P1860; Low K. H., 2011, P AAMAS, P753; Low K. H., 2009, P ICAPS, P233; Low K. H., 2008, P AAMAS, P23; Low K. H., 2012, P AAMAS, P105; Low KH, 2015, AAAI CONF ARTIF INTE, P2821; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Mescheder L, 2017, PR MACH LEARN RES, V70; Ouyang RF, 2018, AAAI CONF ARTIF INTE, P3876; Ouyang RF, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P573; Hoang QM, 2017, AAAI CONF ARTIF INTE, P2007; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Roweis S, 2002, ADV NEUR IN, V14, P889; Salimbeni H, 2017, ADV NEUR IN, V30; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias M. K, 2009, TECHNICAL REPORT; Titsias MK, 2019, PR MACH LEARN RES, V89, P167; Hoang TN, 2018, AAAI CONF ARTIF INTE, P3231; Hoang TN, 2016, PR MACH LEARN RES, V48; Wang K.-C., 2018, INT C MACH LEARN, P5177; Xu N, 2014, AAAI CONF ARTIF INTE, P2585; Luu XV, 2014, PROC INT CONF ADV, P739, DOI 10.1109/ATC.2014.7043485; Yin MZ, 2018, PR MACH LEARN RES, V80; Yu H., 2019, P IJCNN; Zhang Ruqi, 2019, ARXIV190203932; Zhang Y., 2017, NIPS WORKSHOP BAYESI; Zhang Y., 2019, P UAI; Zhang YH, 2016, AAAI CONF ARTIF INTE, P2351	61	8	8	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906019
C	Chang, JL; Gu, J; Wang, LF; Meng, GF; Xiang, SM; Pan, CH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chang, Jianlong; Gu, Jie; Wang, Lingfeng; Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong			Structure-Aware Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Convolutional neural networks (CNNs) are inherently subject to invariable filters that can only aggregate local inputs with the same topological structures. It causes that CNNs are allowed to manage data with Euclidean or grid-like structures (e.g., images), not ones with non-Euclidean or graph structures (e.g., traffic networks). To broaden the reach of CNNs, we develop structure-aware convolution to eliminate the invariance, yielding a unified mechanism of dealing with both Euclidean and non-Euclidean structured data. Technically, filters in the structure-aware convolution are generalized to univariate functions, which are capable of aggregating local inputs with diverse topological structures. Since infinite parameters are required to determine a univariate function, we parameterize these filters with numbered learnable parameters in the context of the function approximation theory. By replacing the classical convolution in CNNs with the structure-aware convolution, Structure-Aware Convolutional Neural Networks (SACNNs) are readily established. Extensive experiments on eleven datasets strongly evidence that SACNNs outperform current models on various machine learning tasks, including image classification and clustering, text categorization, skeleton-based action recognition, molecular activity detection, and taxi flow prediction.	[Chang, Jianlong; Gu, Jie; Wang, Lingfeng; Meng, Gaofeng; Xiang, Shiming; Pan, Chunhong] Chinese Acad Sci, NLPR, Inst Automat, Beijing, Peoples R China; [Chang, Jianlong; Gu, Jie; Xiang, Shiming] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Chang, JL (corresponding author), Chinese Acad Sci, NLPR, Inst Automat, Beijing, Peoples R China.; Chang, JL (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.	jianlong.chang@nlpr.ia.ac.cn; jie.gu@nlpr.ia.ac.cn; lfwang@nlpr.ia.ac.cn; gfmeng@nlpr.ia.ac.cn; smxiang@nlpr.ia.ac.cn; chpan@nlpr.ia.ac.cn			National Natural Science Foundation of China [91646207, 61773377, 61573352]; Beijing Natural Science Foundation [L172053]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work was supported by the National Natural Science Foundation of China under Grants 91646207, 61773377 and 61573352, and the Beijing Natural Science Foundation under Grants L172053. We would like to thank Lele Yu, Bin Fan, Cheng Da, Tingzhao Yu, Xue Ye, Hongfei Xiao, and Qi Zhang for their invaluable contributions in shaping the early stage of this work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna J, 2013, PROC INT C LEARN REP; Chang JL, 2018, PATTERN RECOGN, V77, P438, DOI 10.1016/j.patcog.2017.10.022; Chang JL, 2017, IEEE I CONF COMP VIS, P5880, DOI 10.1109/ICCV.2017.626; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Gilmer J, 2017, PR MACH LEARN RES, V70; Glorot X., 2011, P 14 INT C ART INT S, P315; Hammond D. K., 2009, APPL COMPUTATIONAL H, V30, P129; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henaff M, 2015, ARXIV150605163; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Kaggle, 2012, MERCK MOL ACT CHALL; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lang Ken, 1995, MACHINE LEARNING P; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li  Ruoyu, 2018, ABS180103226 CORR; Liao  Renjie, 2018, ABS180306272 CORR; Masci J, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P832, DOI 10.1109/ICCVW.2015.112; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ruderman  A., 2018, ARXIV180404438; Schlichtkrull M. S., 2017, ABS170306103 CORR; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Shahroudy A, 2016, PROC CVPR IEEE, P1010, DOI 10.1109/CVPR.2016.115; Simonovsky M, 2017, PROC CVPR IEEE, P29, DOI 10.1109/CVPR.2017.11; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Velickovic P., 2017, INT C LEARN REPR VAN; Verma Nitika, 2017, ABS170605206 CORR; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang Q, 2018, INT C PATT RECOG, P1018, DOI 10.1109/ICPR.2018.8545106	42	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300002
C	Dann, C; Jiang, N; Krishnamurthy, A; Agarwal, A; Langford, J; Schapire, RE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dann, Christoph; Jiang, Nan; Krishnamurthy, Akshay; Agarwal, Alekh; Langford, John; Schapire, Robert E.			On Oracle-Efficient PAC RL with Rich Observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the computational tractability of PAC reinforcement learning with rich observations. We present new provably sample-efficient algorithms for environments with deterministic hidden state dynamics and stochastic rich observations. These methods operate in an oracle model of computation-accessing policy and value function classes exclusively through standard optimization primitives-and therefore represent computationally efficient alternatives to prior algorithms that require enumeration. With stochastic hidden state dynamics, we prove that the only known sample-efficient algorithm, OLIVE [1], cannot be implemented in the oracle model. We also present several examples that illustrate fundamental challenges of tractable PAC reinforcement learning in such general settings.	[Dann, Christoph] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Jiang, Nan] UIUC, Urbana, IL USA; [Krishnamurthy, Akshay; Langford, John; Schapire, Robert E.] Microsoft Res, New York, NY USA; [Agarwal, Alekh] Microsoft Res, Redmond, WA USA; [Jiang, Nan] MSR NYC, New York, NY USA	Carnegie Mellon University; University of Illinois System; University of Illinois Urbana-Champaign; Microsoft; Microsoft	Dann, C (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	cdann@cdann.net; nanjiang@illinois.edu; akshay@cs.umass.edu; alekha@microsoft.com; jcl@microsoft.com; schapire@microsoft.com						ALLWEIN E, 2000, J MACHINE LEARNING R; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Auer Peter, 2009, ADV NEURAL INFORM PR, V21, P89; Azar MG, 2017, PR MACH LEARN RES, V70; Azizzadenesheli K., 2016, C LEARNING THEORY, V49, P193; Azizzadenesheli Kamyar, 2016, ARXIV161103907; Bagnell J Andrew, 2004, ADV NEURAL INFORM PR; Beygelzimer Alina, 2009, ALGORITHMIC LEARNING; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Chang KW, 2015, PR MACH LEARN RES, V37, P2058; Culotta A., 2010, ADV NEURAL INFORM PR, V23, P568; Dann Christoph, 2017, ADV NEURAL INFORM PR; Ernst D, 2005, J MACH LEARN RES, V6, P503; Gordon Geoffrey J, 1995, INT C MACH LEARN; Grande Robert, 2014, INT C MACH LEARN; Gretton A, 2012, J MACH LEARN RES, V13, P723; Grotschel Martin, 1981, COMBINATORICA; Guo Zhaohan Daniel, 2016, ARTIFICIAL INTELLIGE; Hsu D., 2010, THESIS; Jiang N, 2017, PR MACH LEARN RES, V70; Johnson M., 2016, P 25 INT JOINT C ART, P4246; Kakade Sham, 2003, P 20 INT C MACHINE L, P306; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kearns Michael, 1999, INT JOINT C ART INT; Khachiyan Leonid G, 1980, USSR COMPUTATIONAL M; Krishnamurthy A., 2016, ADV NEURAL INFORM PR, P1840; Langford J., 2008, ADV NEURAL INFORM PR, P817; Langford John, 2005, INT C COMP LEARN THE; Li L., 2006, ISAIM; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos Remi, 2008, J MACHINE LEARNING R; Osband I., 2014, ADV NEURAL INFORM PR, V27, P1466; Pazis J., 2013, AAAI C ART INT; Pazis J, 2016, AAAI CONF ARTIF INTE, P1977; Peters J, 2017, ADAPT COMPUT MACH LE; Ross S., 2014, ROSS; Ross Stephane, 2013, THESIS; Russo Daniel, 2013, ADV NEURAL INFORM PR, P2256; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Strehl Alexander L., 2005, INT C MACH LEARN; Wen Z., 2013, ADV NEURAL INFORM PR	47	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301041
C	Dhamija, AR; Gunther, M; Boult, TE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dhamija, Akshay Raj; Guenther, Manuel; Boult, Terrance E.			Reducing Network Agnostophobia	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Agnostophobia, the fear of the unknown, can be experienced by deep learning engineers while applying their networks to real-world applications. Unfortunately, network behavior is not well defined for inputs far from a networks training set. In an uncontrolled environment, networks face many instances that are not of interest to them and have to be rejected in order to avoid a false positive. This problem has previously been tackled by researchers by either a) thresholding softmax, which by construction cannot return none of the known classes, or b) using an additional background or garbage class. In this paper, we show that both of these approaches help, but are generally insufficient when previously unseen classes are encountered. We also introduce a new evaluation metric that focuses on comparing the performance of multiple approaches in scenarios where such unseen classes or unknowns are encountered. Our major contributions are simple yet effective Entropic Open-Set and Objectosphere losses that train networks using negative samples from some classes. These novel losses are designed to maximize entropy for unknown inputs while increasing separation in deep feature space by modifying magnitudes of known and unknown samples. Experiments on networks trained to classify classes from MNIST and CIFAR-10 show that our novel loss functions are significantly better at dealing with unknown inputs from datasets such as Devanagari, NotMNIST, CIFAR-100, and SVHN.	[Dhamija, Akshay Raj; Guenther, Manuel; Boult, Terrance E.] Univ Colorado, Vis & Secur Technol Lab, Colorado Springs, CO 80907 USA	University of Colorado System; University of Colorado at Colorado Springs	Dhamija, AR (corresponding author), Univ Colorado, Vis & Secur Technol Lab, Colorado Springs, CO 80907 USA.	adhamija@vast.uccs.edu; mgunther@vast.uccs.edu; tboult@vast.uccs.edu			NSF [IIS-1320956]; Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA) [2014-14071600012]	NSF(National Science Foundation (NSF)); Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA)	This research is based upon work funded in part by NSF IIS-1320956 and in part by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via IARPA R&D Contract No. 2014-14071600012. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the ODNI, IARPA, or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	[Anonymous], NEUROCOMPUTING ALGOR; [Anonymous], 2014, C COMP VIS PATT REC; Bendale  Abhijit, 2016, C COMP VIS PATT REC; Busto PP, 2017, INT C COMP VIS ICCV; Cardoso DO, 2017, MACH LEARN, V106, P1547, DOI 10.1007/s10994-017-5646-4; Chang Eric I., 1994, ADV NEURAL INFORM PR; Chow CK., 1957, IRE T ELECT COMPUTER, VEC-6, P247, DOI DOI 10.1109/TEC.1957.5222035; De Stefano C, 2000, IEEE T SYST MAN CY C, V30, P84, DOI 10.1109/5326.827457; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fumera G, 2002, LECT NOTES COMPUT SC, V2388, P68; Gal Yarin, 2016, INT C MACH LEARN ICM; Geifman Y., 2017, ADV NEURAL INFORM PR; Girshick  Ross, 2015, INT C COMP VIS CVPR; Graves A., 2011, NEURIPS; Grother P., 2016, TECHNICAL REPORT; Gunther  Manuel, 2017, INT JOINT C BIOM IJB; Hu  Peiyun, 2017, C COMP VIS PATT REC; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Lakshminarayanan B., 2017, ADV NEURAL INFORM PR; Lin T.-Y., 2014, ECCV, P740; Matan  Ofer, 1990, USPS ADV TECHN C; Mor N., 2018, WINT C APPL COMP VIS; Oksuz K., 2018, EUR C COMP VIS ECCV; Pant AK, 2012, ASIAN HIMAL INT CONF; Phillips P. J., 2011, HDB FACE RECOGNITION, V2nd; Plummer B, 2015, INT C COMP VIS ICCV; Redmon J., 2016, C COMP VIS PATT REC; Ren S., 2015, P 28 INT C NEUR INF, DOI DOI 10.5555/2969239.2969250; Russakovsky O., 2013, INT C COMP VIS ICCV; Sermanet P., 2013, OVERFEAT INTEGRATED, V1312, P6229; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Springenberg J., 2016, ADV NEURAL INFORM PR; Tommasi T, 2017, ADV COMPUT VIS PATT, P37, DOI 10.1007/978-3-319-58347-1_2; Vyas Apoorv, 2018, EUR C COMP VIS ECCV; Wang T., 2011, P ADV NEURAL INF PRO; Wen Y., 2016, EUR C COMP VIS ECCV; Xu Li, 2014, ADV NEURAL INFORM PR; Yang  Bin, 2016, C COMP VIS PATT REC; Zhang SS, 2018, IEEE T PATTERN ANAL, V40, P973, DOI 10.1109/TPAMI.2017.2700460; Zhou K, 2016, DESTECH TRANS COMP	42	8	8	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003069
C	Feldman, M; Karbasi, A; Kazemi, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Feldman, Moran; Karbasi, Amin; Kazemi, Ehsan			Do Less, Get More: Streaming Submodular Maximization with Subsampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we develop the first one-pass streaming algorithm for submodular maximization that does not evaluate the entire stream even once. By carefully subsampling each element of the data stream, our algorithm enjoys the tightest approximation guarantees in various settings while having the smallest memory footprint and requiring the lowest number of function evaluations. More specifically, for a monotone submodular function and a p-matchoid constraint, our randomized algorithm achieves a 4p approximation ratio (in expectation) with O(k) memory and O(km/p) queries per element (k is the size of the largest feasible solution and m is the number of matroids used to define the constraint). For the non-monotone case, our approximation ratio increases only slightly to 4p + 2 - o(1). To the best or our knowledge, our algorithm is the first that combines the benefits of streaming and subsampling in a novel way in order to truly scale submodular maximization to massive machine learning problems. To showcase its practicality, we empirically evaluated the performance of our algorithm on a video summarization application and observed that it outperforms the state-of-the-art algorithm by up to fifty-fold while maintaining practically the same utility. We also evaluated the scalability of our algorithm on a large dataset of Uber pick up locations.	[Feldman, Moran] Open Univ Israel, Raanana, Israel; [Karbasi, Amin; Kazemi, Ehsan] Yale Univ, New Haven, CT 06520 USA	Open University Israel; Yale University	Feldman, M (corresponding author), Open Univ Israel, Raanana, Israel.	moranfe@openu.ac.il; amin.karbasi@yale.edu; ehsan.kazemi@yale.edu	Gurfil, Pini/AAA-4072-2020; Kazemi, Ehsan/G-7735-2015	Kazemi, Ehsan/0000-0001-8427-054X; Feldman, Moran/0000-0002-1535-2979	AFOSR Young Investigator Award [FA9550-18-1-0160]	AFOSR Young Investigator Award	The work of Amin Karbasi was supported by AFOSR Young Investigator Award (FA9550-18-1-0160).	Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Bilmes J. A., 2017, ABS170108939 CORR; Buchbinder N., 2014, P 25 ANN ACM SIAM S, P1433; Buchbinder N., 2015, P ACM SIAM S DISCRET, P1202; Buchbinder  Niv, 2016, ABS161103253 CORR; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Chakrabarti A, 2015, MATH PROGRAM, V154, P225, DOI 10.1007/s10107-015-0900-7; CHEKURI C, 2015, ICALP, V9134, P318, DOI DOI 10.1007/978-3-662-47672-7_26; Chen  Jiecao, 2016, ABS161100129 CORR; Ene A, 2016, ANN IEEE SYMP FOUND, P248, DOI 10.1109/FOCS.2016.34; Epasto A, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P421, DOI 10.1145/3038912.3052699; Feldman M., 2017, P MACHINE LEARN RES, P758; FELDMAN M., 2011, ESA, P784; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195; de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004; Gharan SO, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1098; Gomes R., 2010, P 27 INT C MACHINE L, P391; Gong B., 2014, ADV NEURAL INFORM PR, P2069; GUPTA A, 2010, WINE, V6484, P246; Herbrich R., 2003, ADV NEURAL INFORM PR, P625; Karbasi A, 2017, MICCAI, P478; Kazemi E, 2018, PR MACH LEARN RES, V80; KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684; Krause A., 2005, P 21 C UNCERTAINTY A, P324, DOI DOI 10.5555/3020336.3020377; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Lee J, 2010, MATH OPER RES, V35, P795, DOI 10.1287/moor.1100.0463; Lee J, 2010, SIAM J DISCRETE MATH, V23, P2053, DOI 10.1137/090750020; Libbrecht M. W., 2018, PROTEIN-STRUCT FUNCT; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mirzasoleiman B, 2017, PR MACH LEARN RES, V70; Mirzasoleiman B, 2016, PR MACH LEARN RES, V48; Mirzasoleiman B, 2016, J MACH LEARN RES, V17; Mirzasoleiman Baharan, 2018, AAAI C ART INT; Mitrovic M., 2018, P ICML, P3593; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; SCHRIJVER A., 2003, COMBINATORIAL OPTIMI, V24; Seeger Matthias, 2004, TECHNICAL REPORT; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; UberDataset, 2014, UBERDATASET UBER PIC; Varadaraja AB, 2011, LECT NOTES COMPUT SC, V6755, P379, DOI 10.1007/978-3-642-22006-7_32; Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318; Wang  Yanhao, 2017, ABS170604764 CORR; Ward J, 2012, LEIBNIZ INT PR INFOR, V14, P42, DOI 10.4230/LIPIcs.STACS.2012.42	45	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300068
C	Figurnov, M; Mohamed, S; Mnih, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Figurnov, Michael; Mohamed, Shakir; Mnih, Andriy			Implicit Reparameterization Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					By providing a simple and efficient way of computing low-variance gradients of continuous random variables, the reparameterization trick has become the technique of choice for training a variety of latent variable models. However, it is not applicable to a number of important continuous distributions. We introduce an alternative approach to computing reparameterization gradients based on implicit differentiation and demonstrate its broader applicability by applying it to Gamma, Beta, Dirichlet, and von Mises distributions, which cannot be used with the classic reparameterization trick. Our experiments show that the proposed approach is faster and more accurate than the existing gradient estimators for these distributions.	[Figurnov, Michael; Mohamed, Shakir; Mnih, Andriy] DeepMind, London, England		Figurnov, M (corresponding author), DeepMind, London, England.	mfigurnov@google.com; shakir@google.com; amnih@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2018, INT C LEARN REPR; [Anonymous], 2014, INT C MACH LEARN; [Anonymous], 2017, INT C LEARN REPR; [Anonymous], 2017, INT C LEARN REPR; [Anonymous], 2014, INT C MACH LEARN; Baydin Atilim Gunes, 2015, ABS150205767 CORR; Bhattacharjee G., 1970, J R STAT SOC C-APPL, V19, P285; Blei D., 2006, NIPS, V18, P147, DOI [10.5555/2976248.2976267, DOI 10.5555/2976248.2976267, DOI 10.1145/1143844.1143859]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blei DM, 2006, INT C MACH LEARN ICM, V148, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859]; Blundell Charles, 2015, INT C MACH LEARN; Burda Yuri, 2016, INT C LEARN REPR; Davidson T. R., 2018, C UNC ART INT; Devroye L., 1986, NONUNIFORM RANDOM VA, P61; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; Gal Y., 2016, ADV NEURAL INFORM PR, P1019; Gal Y, 2016, PR MACH LEARN RES, V48; Glasserman P., 2013, MONTE CARLO METHODS, V53; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Graves Alex, 2016, ARXIV160705690; Hill G. W., 1977, ACM Transactions on Mathematical Software, V3, P279, DOI 10.1145/355744.355753; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hoffman MD, 2015, JMLR WORKSH CONF PRO, V38, P361; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310; Jang E., 2017, ICLR; Jankowiak  M., 2018, INT C MACH LEARN; Jankowiak  M., 2018, ARXIV180601856; Kingma D., 2014, ADAM METHOD STOCHAST; Knowles D. A., 2015, ARXIV150901631; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Maddison Chris J, 2017, ICLR; Mardia K. V., 2009, DIRECTIONAL STAT, P494; Molchanov Dmitry, 2017, INT C MACH LEARN; MOORE RJ, 1982, J R STAT SOC C-APPL, V31, P330; Naesseth CA, 2017, PR MACH LEARN RES, V54, P489; Nalisnick  E., 2016, ADV NEUR INF PROC SY, V2; Paisley J. W., 2012, INT C MACH LEARN; Paszka A., 2017, ADV NEURAL INFORM PR; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Roeder  G., 2017, ADV NEURAL INFORM PR; Ruiz Francisco J. R., 2016, ADV NEURAL INFORM PR; Ruschendorf L., 2013, MATH RISK ANAL, P3, DOI [10.1007/978-3-642-33590-7_1, DOI 10.1007/978-3-642-33590-7_1]; Srivastava Akash, 2018, ARXIV180407944; SURI R, 1988, MANAGE SCI, V34, P39, DOI 10.1287/mnsc.34.1.39; Teh Y. W., 2007, P ADV NEUR INF PROC, P1353; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; von Mises R, 1918, PHYS Z, V19, P490; Wallach H., 2009, ADV NEURAL INFORM PR, V22, P1973, DOI DOI 10.1007/S10708-008-9161-9; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]	53	8	8	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300041
C	Gillen, S; Jung, C; Kearns, M; Roth, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gillen, Stephen; Jung, Christopher; Kearns, Michael; Roth, Aaron			Online Learning with an Unknown Fairness Metric	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of online learning in the linear contextual bandits setting, but in which there are also strong individual fairness constraints governed by an unknown similarity metric. These constraints demand that we select similar actions or individuals with approximately equal probability [?], which may be at odds with optimizing reward, thus modeling settings where profit and social policy are in tension. We assume we learn about an unknown Mahalanobis similarity metric from only weak feedback that identifies fairness violations, but does not quantify their extent. This is intended to represent the interventions of a regulator who "knows unfairness when he sees it" but nevertheless cannot enunciate a quantitative fairness metric over individuals. Our main result is an algorithm in the adversarial context setting that has a number of fairness violations that depends only logarithmically on T, while obtaining an optimal O (root T) regret bound to the best fair policy.	[Gillen, Stephen; Jung, Christopher; Kearns, Michael; Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA	University of Pennsylvania	Gillen, S (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	stepe@math.upenn.edu; chrjung@cis.upenn.edu; mkearns@cis.upenn.edu; aaroth@cis.upenn.edu						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Berk R., 2017, ARXIV PREPRINT ARXIV; Celis L. E., 2018, ARXIV180208674; Chouldechova A, 2017, ARXIV170300056; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Friedler S.A., 2016, IM POSSIBILITY FAIRN; Gillen Stephen, 2018, ARXIV180206936; Hardt M., 2016, ADV NEURAL INFORM PR; Hebert-Johnson U., 2017, ARXIV171108513; Jabbari S, 2017, PR MACH LEARN RES, V70; Jain P, 2009, P ADV NEUR INF PROC, P761; Joseph M., 2016, CORR; Joseph Matthew, 2016, FAIRNESS LEARNING CL, P325; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kearns M., 2017, ARXIV171105144; Kim Michael P, 2018, ARXIV180303239; Kleinberg Jon, 2017, P 2017 ACM C INN THE; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Lobel I, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P585, DOI 10.1145/3033274.3085100; Radanovic G., 2017, CALIBRATED FAIRNESS; Rothblum Guy N, 2018, ARXIV180303242; Zemel R., 2013, P INT C MACH LEARN, P325	24	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302060
C	Hu, ZT; Yang, ZC; Salakhutdinov, R; Liang, XD; Qin, LH; Dong, HY; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hu, Zhiting; Yang, Zichao; Salakhutdinov, Ruslan; Liang, Xiaodan; Qin, Lianhui; Dong, Haoye; Xing, Eric P.			Deep Generative Models with Learnable Knowledge Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The broad set of deep generative models (DGMs) has achieved remarkable advances. However, it is often difficult to incorporate rich structured domain knowledge with the end-to-end DGMs. Posterior regularization (PR) offers a principled framework to impose structured constraints on probabilistic models, but has limited applicability to the diverse DGMs that can lack a Bayesian formulation or even explicit density evaluation. PR also requires constraints to be fully specified a priori, which is impractical or suboptimal for complex knowledge with learnable uncertain parts. In this paper, we establish mathematical correspondence between PR and reinforcement learning (RL), and, based on the connection, expand PR to learn constraints as the extrinsic reward in RL. The resulting algorithm is model-agnostic to apply to any DGMs, and is flexible to adapt arbitrary constraints with the model jointly. Experiments on human image generation and templated sentence generation show models with learned knowledge constraints by our algorithm greatly improve over base generative models.	[Hu, Zhiting] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; Petuum Inc, Pittsburgh, PA USA	Carnegie Mellon University	Hu, ZT (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zhitingh@cs.cmu.edu; zichaoy@cs.cmu.edu; rsalakhu@cs.cmu.edu; xiaodan1@cs.cmu.edu; eric.xing@petuum.com			National Science Foundation [IIS1563887]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation grant IIS1563887. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	Abdolmaleki A., 2018, INT C LEARN REPR; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Bellare K., 2009, P 25 C UNC ART INT, P43; Dayan P, 1997, NEURAL COMPUT, V9, P271, DOI 10.1162/neco.1997.9.2.271; Diao QM, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P193, DOI 10.1145/2623330.2623758; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fedus William, 2018, INT C LEARN REPR; Finn C., 2016, ABS161103852 CORR; Finn C, 2016, PR MACH LEARN RES, V48; Fu J., 2017, ARXIV171011248; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Haarnoja T, 2017, PR MACH LEARN RES, V70; Hinton G., 2015, ARXIV150302531; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Holtzman Ari, 2018, ACL; Hu Z., 2016, P C EMP METH NAT LAN, P1670, DOI DOI 10.18653/V1/D16-1173; Hu Z., 2018, ICLR; Hu ZT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P2410, DOI 10.18653/v1/p16-1228; Hu ZT, 2017, PR MACH LEARN RES, V70; Hu Zhiting, 2018, ARXIV180900794; Kim T., 2016, ARXIV160603439; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kusner MJ, 2017, PR MACH LEARN RES, V70; Larochelle H., 2011, INT C ART INT STAT; Levine Sergey, 2018, ARXIV180500909; Li CX, 2015, ADV NEUR IN, V28; Liang Percy, 2009, P 26 ANN INT C MACH, P641; Liang XD, 2017, IEEE I CONF COMP VIS, P3382, DOI 10.1109/ICCV.2017.364; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Lopez-Paz D., 2015, ARXIV151103643; Ma LQ, 2017, ADV NEUR IN, V30; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Mei SK, 2014, PR MACH LEARN RES, V32; Mohamed Shakir, 2016, ARXIV161003483; Neumann G, 2011, P 28 INT C MACH LEAR, P817; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Pumarola A, 2018, PROC CVPR IEEE, P8620, DOI 10.1109/CVPR.2018.00899; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2017, EQUIVALENCE POLICY; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tan B., 2018, CONNECTING DOTS MLE; Taskar B, 2004, ADV NEUR IN, V16, P25; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang D., 2016, ARXIV161101722; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Yang Zichao, 2018, ADV NEURAL INFORM PR; Zhai S., 2016, ARXIV161101799; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Ziebart B. D., 2008, AAAI, V8, P1433; Zweig Geoffrey, 2011, TECHNICAL REPORT	57	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005011
C	Jiang, SL; Malkomes, G; Abbott, M; Moseley, B; Garnett, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Shali; Malkomes, Gustavo; Abbott, Matthew; Moseley, Benjamin; Garnett, Roman			Efficient nonmyopic batch active search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISCOVERY	Active search is a learning paradigm for actively identifying as many members of a given class as possible. A critical target scenario is high-throughput screening for scientific discovery, such as drug or materials discovery. In these settings, specialized instruments can often evaluate multiple points simultaneously; however, all existing work on active search focuses on sequential acquisition. We bridge this gap, addressing batch active search from both the theoretical and practical perspective. We first derive the Bayesian optimal policy for this problem, then prove a lower bound on the performance gap between sequential and batch optimal policies: the "cost of parallelization." We also propose novel, efficient batch policies inspired by state-of-the-art sequential policies, and develop an aggressive pruning technique that can dramatically speed up computation. We conduct thorough experiments on data from three application domains: a citation network, material science, and drug discovery, testing all proposed policies with a wide range of batch sizes. Our results demonstrate that the empirical performance gap matches our theoretical bound, that nonmyopic policies usually significantly outperform myopic alternatives, and that diversity is an important consideration for batch policy design.	[Jiang, Shali; Malkomes, Gustavo; Abbott, Matthew; Garnett, Roman] WUSTL, CSE, St Louis, MO 63130 USA; [Moseley, Benjamin] CMU, Tepper Sch Business, Pittsburgh, PA 15213 USA; [Moseley, Benjamin] Relational AI, Pittsburgh, PA 15213 USA	Washington University (WUSTL); Carnegie Mellon University	Jiang, SL (corresponding author), WUSTL, CSE, St Louis, MO 63130 USA.	jiang.s@wustl.edu; luizgustavo@wustl.edu; mbabbott@wustl.edu; moseleyb@andrew.cmu.edu; garnett@wustl.edu		Malkomes, Gustavo/0000-0002-9039-794X	National Science Foundation (NSF) [IIA-1355406]; Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES); NSF [CNS-1560191, CCF-1830711, CCF-1824303, CCF-1733873]; Google Research Award	National Science Foundation (NSF)(National Science Foundation (NSF)); Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES)(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated)	We would like to thank all the anonymous reviewers for valuable feedbacks. SJ, GM, and RG were supported by the National Science Foundation (NSF) under award number IIA-1355406. GM was also supported by the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES). MA was supported by NSF under award number CNS-1560191. BM was supported by a Google Research Award and by NSF under awards CCF-1830711, CCF-1824303, and CCF-1733873.	Asadpour A, 2008, LECT NOTES COMPUT SC, V5385, P477, DOI 10.1007/978-3-540-92185-1_53; Chakraborty S, 2015, IEEE T NEUR NET LEAR, V26, P1747, DOI 10.1109/TNNLS.2014.2356470; Chen Y., 2013, INT C MACHINE LEARNI, P160; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Garnett R., 2012, P 29 INT C MACH LEAR; Garnett R, 2015, J COMPUT AID MOL DES, V29, P305, DOI 10.1007/s10822-015-9832-9; Ginsbourger D., 2008, TECHNICAL REPORT; Ginsbourger D, 2010, ADAPT LEARN OPTIM, V2, P131; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Hoi SCH, 2006, P 23 INT C MACH LEAR, V148, P417, DOI [10.1145/1143844.1143897, DOI 10.1145/1143844.1143897]; Jiang S., 2017, P 34 INT C MACH LEAR P 34 INT C MACH LEAR, P1714; Kushner HaroldJ., 1964, J BASIC ENG-T ASME, V86, P97, DOI [10.1115/1.3653121, DOI 10.1115/1.3653121]; Ma YF, 2015, JMLR WORKSH CONF PRO, V38, P672; Ma YF, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P542; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nguyen Viet Cuong, 2013, ADV NEURAL INFORM PR, V26, P1457; Oglic D, 2017, AAAI CONF ARTIF INTE, P2443; Oglic D, 2018, MOL INFORM, V37, DOI 10.1002/minf.201700130; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sterling T, 2015, J CHEM INF MODEL, V55, P2324, DOI 10.1021/acs.jcim.5b00559; Sutherland DJ, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P212; Vanchinathan HP, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1195, DOI 10.1145/2783258.2783360; Wang J., 2017, THESIS; Wang XZ, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P731; Warmuth MK, 2002, ADV NEUR IN, V14, P1449; Warmuth MK, 2003, J CHEM INF COMP SCI, V43, P667, DOI 10.1021/ci025620t; Williams K, 2015, J R SOC INTERFACE, V12, DOI 10.1098/rsif.2014.1289; Wu Jiajun, 2016, ADV NEURAL INFORM PR	28	8	8	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301012
C	Johnson, TB; Guestrin, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Johnson, Tyler B.; Guestrin, Carlos			Training Deep Models Faster with Robust, Approximate Importance Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In theory, importance sampling speeds up stochastic gradient algorithms for supervised learning by prioritizing training examples. In practice, the cost of computing importances greatly limits the impact of importance sampling. We propose a robust, approximate importance sampling procedure (RAIS) for stochastic gradient descent. By approximating the ideal sampling distribution using robust optimization, RAIS provides much of the benefit of exact importance sampling with drastically reduced overhead. Empirically, we find RAIS-SGD and standard SGD follow similar learning curves, but RAIS moves faster through these paths, achieving speed-ups of at least 20% and sometimes much more.	[Johnson, Tyler B.; Guestrin, Carlos] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Johnson, TB (corresponding author), Univ Washington, Seattle, WA 98195 USA.	tbjohns@washington.edu; guestrin@cs.washington.edu			PECASE [N00014-13-I -0023]	PECASE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We thank Marco Tulio Ribeiro, Tianqi Chen, Maryam Fazel, Sham Kakade, and Ali Shojaie for helpful discussion and feedback. This work was supported by PECASE N00014-13-I -0023.	Alain G., 2016, 4 INT C LEARN REPR W; Bengio Y, 2009, P 26 INT C MACH LEAR; BenTal A, 2009, PRINC SER APPL MATH, P1; Borsos Z., 2018, ARXIV180204715; Cohen T. S., 2016, P 33 INT C MACH LEAR; Gopal S., 2016, P 33 INT C MACH LEAR; He Kaiming, 2016, EUR C COMP VIS; Horgan  Daniel, 2018, 6 INT C LEARN REPR; Ioffe S., 2015, 32 INT C MACH LEARN; Katharopoulos A., 2018, P 35 INT C MACH LEAR; Katharopoulos A., 2017, ARXIV170600043; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Larochelle H., 2007, ICML, V227; LeCun Y., 1998, P IEEE; Ma P., 2014, P 31 INT C MACH LEAR; Mahoney M. W., 2011, FDN TRENDS MACHINE L, V3; McCallum A., 2017, ADV NEURAL INFORM PR, P30; Needell D, 2014, ADV NEUR IN, V27; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Schaul T., 2016, 6 INT C LEARN REPR; Shalev-Shwartz S., 2016, P 33 INT C MACH LEAR; Shrivastava A., 2016, P IEEE C COMP VIS PA; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stich S. U., 2017, ADV NEURAL INFORM PR, P30; Sutskever I, 2013, INT C MACH LEARN; Zhang C., 2018, ARXIV180402772; Zhang C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Zhao Peilin, 2015, P 32 INT C MACH LEAR	29	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001079
C	Kallus, N; Mao, XJ; Udell, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kallus, Nathan; Mao, Xiaojie; Udell, Madeleine			Causal Inference with Noisy and Missing Covariates via Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPLETION; BOUNDS	Valid causal inference in observational studies often requires controlling for confounders. However, in practice measurements of confounders may be noisy, and can lead to biased estimates of causal effects. We show that we can reduce bias induced by measurement noise using a large number of noisy measurements of the underlying confounders. We propose the use of matrix factorization to infer the confounders from noisy covariates. This flexible and principled framework adapts to missing values, accommodates a wide variety of data types, and can enhance a wide variety of causal inference methods. We bound the error for the induced average treatment effect estimator and show it is consistent in a linear regression setting, using Exponential Family Matrix Completion preprocessing. We demonstrate the effectiveness of the proposed procedure in numerical experiments with both synthetic data and real clinical data.	[Kallus, Nathan; Mao, Xiaojie; Udell, Madeleine] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Kallus, N (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	kallus@cornell.edu; xm77@cornell.edu; udell@cornell.edu		Mao, Xiaojie/0000-0003-2985-1741; Udell, Madeleine/0000-0002-3985-915X	National Science Foundation [1656996]; DARPA Award [FA8750-17-2-0101]	National Science Foundation(National Science Foundation (NSF)); DARPA Award	This work was supported by the National Science Foundation under Grant No. 1656996. This work was supported by the DARPA Award FA8750-17-2-0101.	ANGRIST JD, 1991, Q J ECON, V106, P979, DOI 10.2307/2937954; ATHEY S, 2017, ARXIV171010251; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Cai TT, 2018, ANN STAT, V46, P60, DOI 10.1214/17-AOS1541; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Cao FL, 2015, IEEE T CIRC SYST VID, V25, P1261, DOI 10.1109/TCSVT.2014.2372351; Collins M, 2002, ADV NEUR IN, V14, P617; Connors AF, 1996, JAMA-J AM MED ASSOC, V276, P889, DOI 10.1001/jama.276.11.889; FROST PA, 1979, REV ECON STAT, V61, P323, DOI 10.2307/1924606; Gunasekar S, 2014, PR MACH LEARN RES, V32, P1917; Hansen BB, 2006, J COMPUT GRAPH STAT, V15, P609, DOI 10.1198/106186006X137047; Hastie T, 2015, J MACH LEARN RES, V16, P3367; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Kallus Nathan, 2016, ARXIV161005604; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kuroki M, 2014, BIOMETRIKA, V101, P423, DOI 10.1093/biomet/ast066; Little RJ, 2014, STAT ANAL MISSING DA, V333; Louizos C, 2017, ADV NEUR IN, V30; MCCULLAGH P, 1984, EUR J OPER RES, V16, P285, DOI 10.1016/0377-2217(84)90282-0; Miao W, 2016, ARXIV160908816; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Ng Andrew Y, 2004, ICML, DOI [10.1145/1015330.1015435, DOI 10.1145/1015330.1015435]; Schnabel T, 2016, PR MACH LEARN RES, V48; Schuler A, 2016, BIOCOMPUT-PAC SYM, P144; Singh AP, 2008, LECT NOTES ARTIF INT, V5212, P358, DOI 10.1007/978-3-540-87481-2_24; Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107; Tripathi G, 1999, ECON LETT, V63, P1, DOI 10.1016/S0165-1765(99)00014-2; Udell M., 2017, ARXIV170507474; Udell M, 2016, FOUND TRENDS MACH LE, V9, P2, DOI 10.1561/2200000055; van Buuren S, 2011, J STAT SOFTW, V45, P1; Vershynin R., 2010, ARXIV10113027; WICKENS MR, 1972, ECONOMETRICA, V40, P759, DOI 10.2307/1912971; Wooldridge J.M., 2012, INTRO ECONOMETRICS M	39	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001046
C	Kawamoto, T; Tsubaki, M; Obuchi, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kawamoto, Tatsuro; Tsubaki, Masashi; Obuchi, Tomoyuki			Mean-field theory of graph neural networks in graph partitioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DYNAMICS	A theoretical performance analysis of the graph neural network (GNN) is presented. For classification tasks, the neural network approach has the advantage in terms of flexibility that it can be employed in a data-driven manner, whereas Bayesian inference requires the assumption of a specific model. A fundamental question is then whether GNN has a high accuracy in addition to this flexibility. Moreover, whether the achieved performance is predominately a result of the backpropagation or the architecture itself is a matter of considerable interest. To gain a better insight into these questions, a mean-field theory of a minimal GNN architecture is developed for the graph partitioning problem. This demonstrates a good agreement with numerical experiments.	[Kawamoto, Tatsuro; Tsubaki, Masashi] Natl Inst Adv Ind Sci & Technol, Artificial Intelligence Res Ctr, Koto Ku, 2-3-26 Aomi, Tokyo, Japan; [Obuchi, Tomoyuki] Tokyo Inst Technol, Dept Math & Comp Sci, Meguro Ku, 2-12-1 Ookayama, Tokyo, Japan	National Institute of Advanced Industrial Science & Technology (AIST); Tokyo Institute of Technology	Kawamoto, T (corresponding author), Natl Inst Adv Ind Sci & Technol, Artificial Intelligence Res Ctr, Koto Ku, 2-3-26 Aomi, Tokyo, Japan.	kawamoto.tatsuro@aist.go.jp; tsubaki.masashi@aist.go.jp; obuchi@c.titech.ac.jp	Kawamoto, Tatsuro/M-8130-2018	Kawamoto, Tatsuro/0000-0002-9898-839X; Obuchi, Tomoyuki/0000-0003-1216-489X	New Energy and Industrial Technology Development Organization (NEDO); JSPS KAKENHI [18K11463]	New Energy and Industrial Technology Development Organization (NEDO)(New Energy and Industrial Technology Development Organization (NEDO)); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors are grateful to Ryo Karakida for helpful comments. This work was supported by the New Energy and Industrial Technology Development Organization (NEDO) (T.K. and M. T.) and JSPS KAKENHI No. 18K11463 (T. O.).	[Anonymous], 2017, ARXIV PREPRINT ARXIV; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bruna J., 2017, STAT-US, V1050, P27; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; CRISANTI A, 1988, PHYS REV A, V37, P4865, DOI 10.1103/PhysRevA.37.4865; Crisanti A, 1990, PREPRINT; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Duvenaud David K, 2015, P NIPS; Gilmer J, 2017, PR MACH LEARN RES, V70; Gilmer Justin, 2016, INT C LEARN REPR; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Kawamoto T, 2015, EPL-EUROPHYS LETT, V112, DOI 10.1209/0295-5075/112/40007; Kawamoto T, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.032301; Kawamoto T, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.062803; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Lei T, 2017, PR MACH LEARN RES, V70; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; Moore C, 2017, BULL EUR ASSOC THEOR, P26; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Newman MEJ, 2016, PHYS REV LETT, V117, DOI 10.1103/PhysRevLett.117.078301; Niepert M, 2016, PR MACH LEARN RES, V48; Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735; Opper M, 2016, J PHYS A-MATH THEOR, V49, DOI 10.1088/1751-8113/49/11/114002; Peixoto TP, 2014, PHYS REV E, V89, DOI 10.1103/PhysRevE.89.012804; Peixoto TP., 2017, ADV NETWORK CLUSTERI; Poole B, 2016, ADV NEUR IN, V29; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; SOMPOLINSKY H, 1982, PHYS REV B, V25, P6860, DOI 10.1103/PhysRevB.25.6860; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tokui Seiya, 2015, P WORKSH MACH LEARN, V5, P1; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Yang L., 2016, INT JOINT C ARTIFICI, P2252, DOI [10.55553060832.3060936////, DOI 10.5555/3060832.3060936]; You J., 2018, ARXIV180208773, P1	42	8	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304038
C	Kirsch, L; Kunze, J; Barber, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kirsch, Louis; Kunze, Julius; Barber, David			Modular Networks: Learning to Decompose Neural Computation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MIXTURES	Scaling model capacity has been vital in the success of deep learning. For a typical network, necessary compute resources and training time grow dramatically with model size. Conditional computation is a promising way to increase the number of parameters with a relatively small increase in resources. We propose a training algorithm that flexibly chooses neural modules based on the data to be processed. Both the decomposition and modules are learned end-to-end. In contrast to existing approaches, training does not rely on regularization to enforce diversity in module use. We apply modular networks both to image recognition and language modeling tasks, where we achieve superior performance compared to several baselines. Introspection reveals that modules specialize in interpretable contexts.	[Kirsch, Louis; Kunze, Julius; Barber, David] UCL, Dept Comp Sci, London, England	University of London; University College London	Kirsch, L (corresponding author), UCL, Dept Comp Sci, London, England.	mail@louiskirsch.com; juliuskunze@gmail.com; david.barber@ucl.ac.uk			Alan Turing Institute under the EPSRC [EP/N510129/1]; ERC Advanced Grant [742870]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC Advanced Grant(European Research Council (ERC))	We thank Ilya Feige, Hippolyt Ritter, Tianlin Xu, Raza Habib, Alex Mansbridge, Roberto Fierimonte, and our anonymous reviewers for their feedback. This work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1. Furthermore, we thank IDSIA (The Swiss AI Lab) for the opportunity to finalize the camera ready version on their premises, partially funded by the ERC Advanced Grant (no: 742870).	Amodei Dario, 2015, DEEP SPEECH 2 END TO; Bengio  E., 2017, THESIS; Bengio E.l, 2016, P INT C LEARN REPR W; Bengio Y., 2014, ARXIV14061078; Bengio Yoshua, 2013, ARXIV; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Clavera I, 2017, IEEE INT C INT ROBOT, P1537; Clune J, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2863; Coates A., 2013, INT C MACHINE LEARNI, P1337; Eigen D., 2013, LEARNING FACTORED RE; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Legenstein RA, 2002, THEOR COMPUT SCI, V287, P239, DOI 10.1016/S0304-3975(02)00097-X; Li LS, 2008, IEEE NLP-KE 2008: PROCEEDINGS OF INTERNATIONAL CONFERENCE ON NATURAL LANGUAGE PROCESSING AND KNOWLEDGE ENGINEERING, P325; Neal R. M., 1999, LEARNING GRAPHICAL M, P355, DOI DOI 10.1007/978-94-011-5014-9; Neklyudov Kirill, 2017, ADV NEURAL INFORM PR, V30; Pakkenberg B, 2003, EXP GERONTOL, V38, P95, DOI 10.1016/S0531-5565(02)00151-1; Ramezani M, 2015, IEEE T MED IMAGING, V34, P2, DOI 10.1109/TMI.2014.2340816; Rosenbaum C., 2018, 6 INT C LEARN REPR I; Sahni H., 2017, NIPS DEEP RL S; SCHMIDHUBER J, 1989, CONNECTIONISM IN PERSPECTIVE, P439; Schmidhuber J., 2012, ARXIV12100118; Sporns O, 2016, ANNU REV PSYCHOL, V67, P613, DOI 10.1146/annurev-psych-122414-033634; Srivastava R. K., 2013, ADV NEURAL INFORM PR, P2310; Sternberg S, 2011, COGN NEUROPSYCHOL, V28, P156, DOI 10.1080/02643294.2011.557231; Wen W., 2016, ADV NEURAL INFORM PR, P2074; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	30	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302042
C	La Tour, TD; Moreau, T; Jas, M; Gramfort, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		La Tour, Tom Dupre; Moreau, Thomas; Jas, Mainak; Gramfort, Alexandre			Multivariate Convolutional Sparse Coding for Electromagnetic Brain Signals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM	Frequency-specific patterns of neural activity are traditionally interpreted as sustained rhythmic oscillations, and related to cognitive mechanisms such as attention, high level visual processing or motor control. While alpha waves (8-12 Hz) are known to closely resemble short sinusoids, and thus are revealed by Fourier analysis or wavelet transforms, there is an evolving debate that electromagnetic neural signals are composed of more complex waveforms that cannot be analyzed by linear filters and traditional signal representations. In this paper, we propose to learn dedicated representations of such recordings using a multivariate convolutional sparse coding (CSC) algorithm. Applied to electroencephalography (EEG) or magnetoencephalography (MEG) data, this method is able to learn not only prototypical temporal waveforms, but also associated spatial patterns so their origin can be localized in the brain. Our algorithm is based on alternated minimization and a greedy coordinate descent solver that leads to state-of-the-art running time on long time series. To demonstrate the implications of this method, we apply it to MEG data and show that it is able to recover biological artifacts. More remarkably, our approach also reveals the presence of non-sinusoidal mu-shaped patterns, along with their topographic maps related to the somatosensory cortex.	[La Tour, Tom Dupre; Jas, Mainak] Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France; [Moreau, Thomas; Gramfort, Alexandre] Univ Paris Saclay, INRIA, Saclay, France	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay; Inria; UDICE-French Research Universities; Universite Paris Saclay	La Tour, TD (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.		Moreau, Thomas/AAB-9940-2019; Jas, Mainak/AAW-4508-2020; la Tour, Tom Dupre/ABH-9892-2020	Moreau, Thomas/0000-0002-1523-3419; Jas, Mainak/0000-0002-3199-9027; 	ERC [SLAB ERC-YStG-676943]; ANR THALAMEEG [ANR-14-NEUC-0002-01]	ERC(European Research Council (ERC)European Commission); ANR THALAMEEG(French National Research Agency (ANR))	This work was supported by the ERC Starting Grant SLAB ERC-YStG-676943 and by the ANR THALAMEEG ANR-14-NEUC-0002-01.	[Anonymous], 2013, P INT JOINT C NEUR N; [Anonymous], 2019, STAT LEARNING SPARSI; [Anonymous], 2008, EUR SIGN PROC C; Barthelemy Q, 2013, J NEUROSCI METH, V215, P19, DOI 10.1016/j.jneumeth.2013.02.001; Barthelemy Q, 2012, IEEE T SIGNAL PROCES, V60, P1597, DOI 10.1109/TSP.2012.2183129; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Bristow H, 2013, PROC CVPR IEEE, P391, DOI 10.1109/CVPR.2013.57; Brockmeier AJ, 2016, IEEE T BIO-MED ENG, V63, P43, DOI 10.1109/TBME.2015.2499241; Buzsaki G., 2006, RHYTHMS BRAIN; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Cole S. R., 2017, TRENDS COGN SCI; Cole S.R., 2018, CYCLE BY CYCLE ANAL; Cole SR, 2017, J NEUROSCI, V37, P4830, DOI 10.1523/JNEUROSCI.2208-16.2017; Durka PJ, 2005, J NEUROSCI METH, V148, P49, DOI 10.1016/j.jneumeth.2005.04.001; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Garcia-Cardona C., 2017, ARXIV170902893; Gips B, 2017, J NEUROSCI METH, V275, P66, DOI 10.1016/j.jneumeth.2016.11.001; Gramfort A, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00267; Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027; Grosse R., 2013, 23 C UNC ART INT UAI, P149; Hari R, 2017, MEG EEG PRIMER; Hari R, 2006, PROG BRAIN RES, V159, P253, DOI 10.1016/S0079-6123(06)59017-X; Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149; Hitziger S., 2017, IEEE T SIGNAL PROCES; Jas M., 2017, ADV NEURAL INFORM PR, P1; Jones SR, 2016, CURR OPIN NEUROBIOL, V40, P72, DOI 10.1016/j.conb.2016.06.010; Jost P., 2006, ACOUSTICS SPEECH SIG, V5; Kavukcuoglu Koray, 2010, ADV NEURAL INFORM PR, V23, P1090; Li YY, 2009, INVERSE PROBL IMAG, V3, P487, DOI 10.3934/ipi.2009.3.487; Locatello F., 2018, INT C MACH LEARN; Mazaheri A, 2008, J NEUROSCI, V28, P7781, DOI 10.1523/JNEUROSCI.1631-08.2008; Moreau T., 2018, INT C MACH LEARN ICM; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Pachitariu M., 2013, ADV NEURAL INFORM PR; Python Software Foundation, 2017, PYTH LANG REF VERS 3; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Sorel M., 2016, DIGITAL SIGNAL PROCE; TUOMISTO T, 1983, NUOVO CIMENTO D, V2, P471, DOI 10.1007/BF02455946; van Ede F., 2018, TRENDS NEUROSCIENCES; Wohlberg B, 2016, IEEE SW SYMP IMAG, P57, DOI 10.1109/SSIAI.2016.7459174; Wohlberg B, 2016, IEEE T IMAGE PROCESS, V25, P301, DOI 10.1109/TIP.2015.2495260; Wright Stephen J, 1999, NUMERICAL OPTIMIZATI, V35; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	45	8	8	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303030
C	Lattimore, T; Kveton, B; Li, S; Szepesvari, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lattimore, Tor; Kveton, Branislav; Li, Shuai; Szepesvari, Csaba			TopRank: A Practical Algorithm for Online Stochastic Ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Online learning to rank is a sequential decision-making problem where in each round the learning agent chooses a list of items and receives feedback in the form of clicks from the user. Many sample-efficient algorithms have been proposed for this problem that assume a specific click model connecting rankings and user behavior. We propose a generalized click model that encompasses many existing models, including the position-based and cascade models. Our generalization motivates a novel online learning algorithm based on topological sort, which we call TopRank. TopRank is (a) more natural than existing algorithms, (b) has stronger regret guarantees than existing algorithms with comparable generality, (c) has a more insightful proof that leaves the door open to many generalizations, and (d) outperforms existing algorithms empirically.	[Lattimore, Tor; Szepesvari, Csaba] DeepMind, London, England; [Kveton, Branislav] Google, Mountain View, CA USA; [Li, Shuai] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Szepesvari, Csaba] Univ Alberta, Edmonton, AB, Canada	Google Incorporated; Chinese University of Hong Kong; University of Alberta	Lattimore, T (corresponding author), DeepMind, London, England.							Agichtein E., 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P19, DOI 10.1145/1148170.1148177; Chuklin Aleksandr, 2015, CLICK MODELS WEB SEA; Combes Richard, 2015, P 2015 ACM SIGMETRIC; Grotov Artem, 2015, P 6 INT C CLEF ASS; Katariya S, 2016, PR MACH LEARN RES, V48; Kveton B., 2015, P 32 INT C MACH LEAR; Kveton B., 2015, NIPS, V28, P1450; Lagree Paul, 2016, ADV NEURAL INFORM PR, P1597; Lattimore T, 2017, PR MACH LEARN RES, V54, P728; Li S, 2016, PR MACH LEARN RES, V48; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Radlinski F, 2008, P 25 INT C MACH LEAR, DOI DOI 10.1145/1390156.1390255; Slivkins A, 2013, J MACH LEARN RES, V14, P399; Uchiya T, 2010, LECT NOTES ARTIF INT, V6331, P375, DOI 10.1007/978-3-642-16108-7_30; Zoghi M., 2017, ICML 2017, P4199; Zoghi M, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P195, DOI 10.1145/2911451.2911500; Zong S., 2016, PROC 31TH UNCERTAINT	17	8	8	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303090
C	Li, WY; Mao, JW; Zhang, Y; Cui, SG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Wenye; Mao, Jingwei; Zhang, Yin; Cui, Shuguang			Fast Similarity Search via Optimal Sparse Lifting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				JOHNSON-LINDENSTRAUSS; NEAREST-NEIGHBOR	Similarity search is a fundamental problem in computing science with various applications and has attracted significant research attention, especially in large-scale search with high dimensions. Motivated by the evidence in biological science, our work develops a novel approach for similarity search. Fundamentally different from existing methods that typically reduce the dimension of the data to lessen the computational complexity and speed up the search, our approach projects the data into an even higher-dimensional space while ensuring the sparsity of the data in the output space, with the objective of further improving precision and speed. Specifically, our approach has two key steps. Firstly, it computes the optimal sparse lifting for given input samples and increases the dimension of the data while approximately preserving their pairwise similarity. Secondly, it seeks the optimal lifting operator that best maps input samples to the optimal sparse lifting. Computationally, both steps are modeled as optimization problems that can be efficiently and effectively solved by the Frank-Wolfe algorithm. Simple as it is, our approach has reported significantly improved results in empirical evaluations, and exhibited its high potentials in solving practical problems.	[Li, Wenye; Mao, Jingwei; Zhang, Yin; Cui, Shuguang] Chinese Univ Hong Kong, Shenzhen, Peoples R China; [Li, Wenye; Cui, Shuguang] Shenzhen Res Inst Big Data, Shenzhen, Peoples R China	Chinese University of Hong Kong, Shenzhen	Li, WY (corresponding author), Chinese Univ Hong Kong, Shenzhen, Peoples R China.; Li, WY (corresponding author), Shenzhen Res Inst Big Data, Shenzhen, Peoples R China.	wylishuguangcui@cuhk.edu.cn; 216019005@link.cuhk.edu.cn; yinzhangshuguangcui@cuhk.edu.cn; shuguangcui@cuhk.edu.cn			Shenzhen Fundamental Research Fund [JCYJ20170306141038939, KQJSCX20170728162302784, ZDSYS201707251409055]; Shenzhen Development and Reform Commission Fund; Guangdong Introducing Innovative and Entrepreneurial Teams Fund, China [2017ZT07X152]	Shenzhen Fundamental Research Fund; Shenzhen Development and Reform Commission Fund; Guangdong Introducing Innovative and Entrepreneurial Teams Fund, China	This work was supported by Shenzhen Fundamental Research Fund (JCYJ20170306141038939, KQJSCX20170728162302784, ZDSYS201707251409055), Shenzhen Development and Reform Commission Fund, and Guangdong Introducing Innovative and Entrepreneurial Teams Fund (2017ZT07X152), China.	Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Allen-Zhu Z, 2014, P NATL ACAD SCI USA, V111, P16872, DOI 10.1073/pnas.1419100111; Andoni A, 2006, ANN IEEE SYMP FOUND, P459; Andoni A, 2015, ACM S THEORY COMPUT, P793, DOI 10.1145/2746539.2746553; Baeza-Yates R., 1999, MODERN INFORM RETRIE, V463; Caron SJC, 2013, NATURE, V497, P113, DOI 10.1038/nature12063; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Dasgupta S, 2017, SCIENCE, V358, P793, DOI 10.1126/science.aam9868; Duda R.O., 2017, PATTERN CLASSIFICATI; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jaiyam S., EFFICIENT NEAREST NE; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Jortner RA, 2007, J NEUROSCI, V27, P1659, DOI 10.1523/JNEUROSCI.4171-06.2007; Kleinberg J. M., 1997, STOC, P599; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin AC, 2014, NAT NEUROSCI, V17, P559, DOI 10.1038/nn.3660; Lin Y, 2013, PROC CVPR IEEE, P446, DOI 10.1109/CVPR.2013.64; OTSU N, 1979, IEEE T SYST MAN CYB, V9, P62, DOI 10.1109/TSMC.1979.4310076; Papadopoulou M, 2011, SCIENCE, V332, P721, DOI 10.1126/science.1201835; Pehlevan C, 2018, NEURAL COMPUT, V30, P84, DOI [10.1162/neco_a_01018, 10.1162/NECO_a_01018]; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Schrijver A., 1998, THEORY LINEAR INTEGE; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Turner GC, 2008, J NEUROPHYSIOL, V99, P734, DOI 10.1152/jn.01283.2007; Zheng ZH, 2018, CELL, V174, P730, DOI 10.1016/j.cell.2018.06.019	29	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300017
C	Liu, RS; Cheng, SC; Liu, XK; Ma, L; Fan, X; Luo, ZX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Risheng; Cheng, Shichao; Liu, Xiaokun; Ma, Long; Fan, Xin; Luo, Zhongxuan			A Bridging Framework for Model Optimization and Deep Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Optimizing task-related mathematical model is one of the most fundamental methodologies in statistic and learning areas. However, generally designed schematic iterations may hard to investigate complex data distributions in real-world applications. Recently, training deep propagations (i.e., networks) has gained promising performance in some particular tasks. Unfortunately, existing networks are often built in heuristic manners, thus lack of principled interpretations and solid theoretical supports. In this work, we provide a new paradigm, named Propagation and Optimization based Deep Model (PODM), to bridge the gaps between these different mechanisms (i.e., model optimization and deep propagation). On the one hand, we utilize PODM as a deeply trained solver for model optimization. Different from these existing network based iterations, which often lack theoretical investigations, we provide strict convergence analysis for PODM in the challenging nonconvex and nonsmooth scenarios. On the other hand, by relaxing the model constraints and performing end-to-end training, we also develop a PODM based strategy to integrate domain knowledge (formulated as models) and real data distributions (learned by networks), resulting in a generic ensemble framework for challenging real-world applications. Extensive experiments verify our theoretical results and demonstrate the superiority of PODM against these state-of-the-art approaches.	[Liu, Risheng; Liu, Xiaokun; Ma, Long; Fan, Xin] Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China; [Liu, Risheng; Fan, Xin; Luo, Zhongxuan] Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Liaoning, Peoples R China; [Cheng, Shichao; Luo, Zhongxuan] Dalian Univ Technol, Sch Math Sci, Dalian, Peoples R China	Dalian University of Technology; Dalian University of Technology	Liu, RS (corresponding author), Dalian Univ Technol, Int Sch Informat Sci & Engn, Dalian, Peoples R China.; Liu, RS (corresponding author), Key Lab Ubiquitous Network & Serv Software Liaoni, Dalian, Liaoning, Peoples R China.	rsliu@dlut.edu.cn			National Natural Science Foundation of China [61672125, 61733002, 61572096, 61632019]; Fundamental Research Funds for the Central Universities	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work is partially supported by the National Natural Science Foundation of China (Nos. 61672125, 61733002, 61572096 and 61632019), and Fundamental Research Funds for the Central Universities.	Andrew G., 2013, INT C MACH LEARN, p1247?1255; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2010, P INT C MACH LEARN; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bevilacqua M, 2012, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2012, DOI 10.5244/C.26.135; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Diamond S, 2017, ARXIV170508041; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Kohler R, 2012, LECT NOTES COMPUT SC, V7578, P27, DOI 10.1007/978-3-642-33786-4_3; Krishnan D., 2009, ADV NEURAL INFORM PR, V22, P1033; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kruse J, 2017, IEEE I CONF COMP VIS, P4596, DOI 10.1109/ICCV.2017.491; Levin A, 2009, PROC CVPR IEEE, P1964, DOI 10.1109/CVPRW.2009.5206815; Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1; Li Ke, 2016, ARXIV160601885; Liu R., 2018, IEEE T NEUR NET LEAR, P1; Liu R, 2018, ARXIV180805331; Liu RS, 2014, NEURAL NETWORKS, V59, P1, DOI 10.1016/j.neunet.2014.06.005; Liu Risheng, 2018, ARXIV181004012V1; Liu Risheng, 2018, AAAI; Pan JS, 2016, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2016.306; Schmidt U, 2016, IEEE T PATTERN ANAL, V38, P677, DOI 10.1109/TPAMI.2015.2441053; Schmidt U, 2014, PROC CVPR IEEE, P2774, DOI 10.1109/CVPR.2014.349; Schuler CJ, 2013, PROC CVPR IEEE, P1067, DOI 10.1109/CVPR.2013.142; Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779; Sun L, 2013, IEEE INT C COMPUTATI, P1, DOI 10.1109/ICCPhot.2013.6528301; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Ulyanov D., 2017, ARXIV171110925; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang K, 2017, PROC CVPR IEEE, P2808, DOI 10.1109/CVPR.2017.300; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	34	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304034
C	McClure, P; Zheng, CY; Kaczmarzyk, JR; Lee, JA; Ghosh, SS; Nielson, D; Bandettini, P; Pereira, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		McClure, Patrick; Zheng, Charles Y.; Kaczmarzyk, Jakub R.; Lee, John A.; Ghosh, Satrajit S.; Nielson, Dylan; Bandettini, Peter; Pereira, Francisco			Distributed Weight Consolidation: A Brain Segmentation Case Study	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Collecting the large datasets needed to train deep neural networks can be very difficult, particularly for the many applications for which sharing and pooling data is complicated by practical, ethical, or legal concerns. However, it may be the case that derivative datasets or predictive models developed within individual sites can be shared and combined with fewer restrictions. Training on distributed data and combining the resulting networks is often viewed as continual learning, but these methods require networks to be trained sequentially. In this paper, we introduce distributed weight consolidation (DWC), a continual learning method to consolidate the weights of separate neural networks, each trained on an independent dataset. We evaluated DWC with a brain segmentation case study, where we consolidated dilated convolutional neural networks trained on independent structural magnetic resonance imaging (sMRI) datasets from different sites. We found that DWC led to increased performance on test sets from the different sites, while maintaining generalization performance for a very large and completely independent multi-site dataset, compared to an ensemble baseline.	[McClure, Patrick; Zheng, Charles Y.; Lee, John A.; Nielson, Dylan; Bandettini, Peter; Pereira, Francisco] NIMH, Bethesda, MD 20892 USA; [Kaczmarzyk, Jakub R.; Ghosh, Satrajit S.] MIT, Cambridge, MA 02139 USA	National Institutes of Health (NIH) - USA; NIH National Institute of Mental Health (NIMH); Massachusetts Institute of Technology (MIT)	McClure, P (corresponding author), NIMH, Bethesda, MD 20892 USA.	patrick.mcclure@nih.gov; charles.zheng@nih.gov; jakubk@mit.edu; john.rodgers-lee@nih.gov; satra@mit.edu; dylann.nielson@nih.gov; bandettini@nih.gov; francisco.pereira@nih.gov	Nielson, Dylan/ABD-1615-2021	Nielson, Dylan/0000-0003-4613-6643; Zheng, Charles/0000-0003-3427-0845	National Institute of Mental Health Intramural Research Program [ZIC-MH002968, ZIC-MH002960]; NIH [R01 EB020740]	National Institute of Mental Health Intramural Research Program(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported by the National Institute of Mental Health Intramural Research Program (ZIC-MH002968, ZIC-MH002960). JK's and SG's work was supported by NIH R01 EB020740.	Bachem O, 2015, PR MACH LEARN RES, V37, P209; Biswal BB, 2010, P NATL ACAD SCI USA, V107, P4734, DOI 10.1073/pnas.0911855107; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Chang  Ken, J AM MED INFORM ASS; Di Martino A, 2014, Mol Psychiatry, V19, P659, DOI 10.1038/mp.2013.78; Fedorov A, 2017, IEEE IJCNN, P3785, DOI 10.1109/IJCNN.2017.7966333; Fedorov  Alex, 2017, ARXIV171100457; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Gal Y, 2016, PR MACH LEARN RES, V48; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Huggins Jonathan, 2016, P ADV NEUR INF PROC, P4080; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kochurov  Max, 2018, ICLR WORKSH; Li WQ, 2017, LECT NOTES COMPUT SC, V10265, P348, DOI 10.1007/978-3-319-59050-9_28; Louizos C, 2017, PR MACH LEARN RES, V70; Molchanov D, 2017, PR MACH LEARN RES, V70; Nguyen Cuong V., 2018, INT C LEARN REPR; Nooner KB, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00152; Plis SM, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00365; Power JD, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0182939; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy A.G., 2018, ARXIV180104161; Thompson PM, 2014, BRAIN IMAGING BEHAV, V8, P153, DOI 10.1007/s11682-013-9269-5; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Yu  Fisher, 2015, INT C LEARN REPR; Zenke F, 2017, PR MACH LEARN RES, V70	32	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF	34376963				2022-12-19	WOS:000461823304013
C	Mishkin, A; Kunstner, F; Nielsen, D; Schmidt, M; Khan, ME		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mishkin, Aaron; Kunstner, Frederik; Nielsen, Didrik; Schmidt, Mark; Khan, Mohammad Emtiyaz			SLANG: Fast Structured Covariance Approximations for Bayesian Deep Learning with Natural Gradient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAUSSIAN VARIATIONAL APPROXIMATION	Uncertainty estimation in large deep-learning models is a computationally challenging task, where it is difficult to form even a Gaussian approximation to the posterior distribution. In such situations, existing methods usually resort to a diagonal approximation of the covariance matrix despite the fact that these matrices are known to result in poor uncertainty estimates. To address this issue, we propose a new stochastic, low-rank, approximate natural-gradient (SLANG) method for variational inference in large, deep models. Our method estimates a "diagonal plus low-rank" structure based solely on back-propagated gradients of the network log-likelihood. This requires strictly less gradient computations than methods that compute the gradient of the whole variational objective. Empirical evaluations on standard benchmarks confirm that SLANG enables faster and more accurate estimation of uncertainty than mean-field methods, and performs comparably to state-of-the-art methods.	[Mishkin, Aaron; Schmidt, Mark] Univ British Columbia, Vancouver, BC, Canada; [Kunstner, Frederik] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Nielsen, Didrik; Khan, Mohammad Emtiyaz] RIKEN Ctr AI Project, Tokyo, Japan	University of British Columbia; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Mishkin, A (corresponding author), Univ British Columbia, Vancouver, BC, Canada.	amishkin@cs.ubc.ca; frederik.kunstner@epfl.ch; didrik.nielsen@riken.jp; schmidtm@cs.ubc.ca; emtiyaz.khan@riken.jp		Mishkin, Aaron/0000-0002-5072-2314				Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Ahn S., 2012, INT C MACH LEARN ICM; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Ambikasaran  Sivaram, 2014, ABS14050223 CORR; [Anonymous], 2017, P 34 INT C MACH LEAR; Balan Anoop Korattikara, 2015, ADV NEURAL INFORM PR, P3; Barber David., 1997, ADV NEURAL INFORM PR, P395; Blundell  Charles, 2015, ABS150505424 CORR; Challis Edward, 2011, P 14 INT C ART INT S, P199; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Gal Y, 2016, PR MACH LEARN RES, V48; Goodfellow Ian J., 2015, ABS151001799 CORR; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jaakkola  Tommi, 1997, P 6 INT WORKSH ART I; Khan ME, 2018, PR MACH LEARN RES, V80; Khan ME, 2017, PR MACH LEARN RES, V54, P878; Khan Mohammad Emtiyaz, 2018, ABS180704489 CORR; Kingma Diederik P., 2014, ABS14126980 CORR; Louizos C, 2016, PR MACH LEARN RES, V48; Marlin B.M., 2011, ICML, P633; Martens J., 2014, ABS14121193 CORR; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Ong VMH, 2018, J COMPUT GRAPH STAT, V27, P465, DOI 10.1080/10618600.2017.1390472; Paszke  Adam, 2017, AUT WORKSH ANN C NEU; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Ritter H., 2018, INT C LEARN REPR; Seeger M., 2010, P 27 INT C MACH LEAR, P967; Seeger Matthias W., 1999, ADV NEURAL INFORM PR, V12, P603; Simsekli U, 2016, PR MACH LEARN RES, V48; Sun SY, 2017, PR MACH LEARN RES, V54, P1283; Tan LSL, 2018, STAT COMPUT, V28, P259, DOI 10.1007/s11222-017-9729-7; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; Wright S. J., 1999, NUMERICAL OPTIMIZATI	38	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000072
C	Mitrovic, J; Sejdinovic, D; Teh, YW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mitrovic, Jovana; Sejdinovic, Dino; Teh, Yee Whye			Causal Inference via Kernel Deviance Measures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Discovering the causal structure among a set of variables is a fundamental problem in many areas of science. In this paper, we propose Kernel Conditional Deviance for Causal Inference (KCDC) a fully nonparametric causal discovery method based on purely observational data. From a novel interpretation of the notion of asymmetry between cause and effect, we derive a corresponding asymmetry measure using the framework of reproducing kernel Hilbert spaces. Based on this, we propose three decision rules for causal discovery. We demonstrate the wide applicability and robustness of our method across a range of diverse synthetic datasets. Furthermore, we test our method on real-world time series data and the real-world benchmark dataset Tubingen Cause-Effect Pairs where we outperform state-of-the-art approaches.	[Mitrovic, Jovana; Sejdinovic, Dino; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England; [Mitrovic, Jovana; Teh, Yee Whye] DeepMind, London, England	University of Oxford	Mitrovic, J (corresponding author), Univ Oxford, Dept Stat, Oxford, England.; Mitrovic, J (corresponding author), DeepMind, London, England.	mitrovic@stats.ox.ac.uk; dino.sejdinovic@stats.ox.ac.uk; y.w.teh@stats.ox.ac.uk		Sejdinovic, Dino/0000-0001-5547-9213	Clarendon Fund of the University of Oxford; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant [617071]	Clarendon Fund of the University of Oxford; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant(European Research Council (ERC))	JM acknowledges the financial support of The Clarendon Fund of the University of Oxford. DS's and YWT's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	Bauer S, 2016, PR MACH LEARN RES, V48; Budhathoki K, 2017, ARXIV170206776; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Daniusis P., 2012, ARXIV12033475; Fonollosa J. A., 2016, ARXIV160106680; Gartner T., 2002, INT C IND LOG PROGR, P66; Goudet O., 2017, ARXIV170905321; Grunewalder S., 2012, P 29 INT C MACH LEAR, P1823; Grunwald PD, 2008, HBK PHILOS SCI, V8, P281, DOI 10.1016/B978-0-444-51726-5.50013-3; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; Janzing D, 2010, IEEE T INFORM THEORY, V56, P5168, DOI 10.1109/TIT.2010.2060095; Lemeire Jan, 2006, CAUSAL MODELS MINIMA; Lopez-Paz D, 2015, PR MACH LEARN RES, V37, P1452; Mooij J.M., 2009, P 26 ANN INT C MACH, P745; Mooij J. M., 2015, CAUSE EFFECT PAIRS R; Mooij JM, 2016, J MACH LEARN RES, V17; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Scholkopf B., 2001, LEARNING KERNELS SUP; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Spirtes P., 2000, CAUSATION PREDICTION; Stegle O., 2010, ADV NEURAL INF PROCE, V23, P1687; Sun X., 2007, EUR S ART NEUR NETW, P441; Sun X., 2007, P 24 INT C MACHINE L, P855, DOI 10.1145/1273496.1273604; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647; Zhang K., 2011, P 27 ANN C UNC ART I	26	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001052
C	Pennington, J; Worah, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pennington, Jeffrey; Worah, Pratik			The Spectrum of the Fisher Information Matrix of a Single-Hidden-Layer Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					An important factor contributing to the success of deep learning has been the remarkable ability to optimize large neural networks using simple first-order optimization algorithms like stochastic gradient descent. While the efficiency of such methods depends crucially on the local curvature of the loss surface, very little is actually known about how this geometry depends on network architecture and hyperparameters. In this work, we extend a recently-developed framework for studying spectra of nonlinear random matrices to characterize an important measure of curvature, namely the eigenvalues of the Fisher information matrix. We focus on a single-hidden-layer neural network with Gaussian data and weights and provide an exact expression for the spectrum in the limit of infinite width. We find that linear networks suffer worse conditioning than nonlinear networks and that nonlinear networks are generically non-degenerate. We also predict and demonstrate empirically that by adjusting the nonlinearity, the spectrum can be tuned so as to improve the efficiency of first-order optimization methods.	[Pennington, Jeffrey] Google Brain, Mountain View, CA 94043 USA; [Worah, Pratik] Google Res, Mountain View, CA USA	Google Incorporated; Google Incorporated	Pennington, J (corresponding author), Google Brain, Mountain View, CA 94043 USA.	jpennin@google.com; pworah@google.com		Worah, Pratik/0000-0002-2739-1246				Altae-Tran Han, 2017, LOW DATA DRUG DISCOV; Amari S., 1998, NEURAL COMPUTATION; [Anonymous], 2016, ARXIV160903499; Ba J., 2017, P 3 INT C LEARN REPR; CARLSON BC, 1988, MATH COMPUT, V51, P267, DOI 10.1090/S0025-5718-1988-0942154-7; Duchi J., 2011, J MACHINE LEARNING R; Giaquinta Mariano, 1994, CALCULUS VARIATIONS, V1; Goh G., 2017, ARXIV170104503; Grosse R, 2016, PR MACH LEARN RES, V48; Heskes T, 2000, NEURAL COMPUT, V12, P881, DOI 10.1162/089976600300015637; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Krishnan Shankar, 2018, INT C LEARN REPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Pennington J, 2017, ADV NEURAL INFORM PR, P2634; Shazeer N., 2017, ICLR; Stanley Richard, 1996, J COMBINATORIAL TH A; Wu Y., 2016, ARXIV 160908144	17	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305043
C	Rowland, M; Choromanski, K; Chalus, F; Pacchiano, A; Sarlos, T; Turner, RE; Weller, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rowland, Mark; Choromanski, Krzysztof; Chalus, Francois; Pacchiano, Aldo; Sarlos, Tamas; Turner, Richard E.; Weller, Adrian			Geometrically Coupled Monte Carlo Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EFFICIENT	Monte Carlo sampling in high-dimensional, low-sample settings is important in many machine learning tasks. We improve current methods for sampling in Euclidean spaces by avoiding independence, and instead consider ways to couple samples. We show fundamental connections to optimal transport theory, leading to novel sampling algorithms, and providing new theoretical grounding for existing strategies. We compare our new strategies against prior methods for improving sample efficiency, including quasi-Monte Carlo, by studying discrepancy. We explore our findings empirically, and observe benefits of our sampling schemes for reinforcement learning and generative modelling.	[Rowland, Mark; Chalus, Francois; Turner, Richard E.] Univ Cambridge, Cambridge, England; [Choromanski, Krzysztof] Google Brain Robot, Cambridge, England; [Pacchiano, Aldo] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Sarlos, Tamas] Google Res, Berkeley, CA USA; [Weller, Adrian] Univ Cambridge, Alan Turing Inst, Cambridge, England	University of Cambridge; University of California System; University of California Berkeley; Google Incorporated; University of Cambridge	Rowland, M (corresponding author), Univ Cambridge, Cambridge, England.	mr504@cam.ac.uk; kchoro@google.com; chalusf3@gmail.com; pacchiano@berkeley.edu; stamas@google.com; ret26@cam.ac.uk; aw665@cam.ac.uk			EPSRC [EP/L016516/1]; David MacKay Newton research fellowship at Darwin College; Leverhulme Trust via the CFI; Alan Turing Institute under EPSRC [EP/N510129/1, TU/B/000074]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); David MacKay Newton research fellowship at Darwin College; Leverhulme Trust via the CFI; Alan Turing Institute under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank Jiri Hron, Maria Lomeli, and the anonymous reviewers for helpful comments on the manuscript. We thank Yingzhen Li for her VAE implementation. MR acknowledges support by EPSRC grant EP/L016516/1 for the Cambridge Centre for Analysis. AW acknowledges support from the David MacKay Newton research fellowship at Darwin College, The Alan Turing Institute under EPSRC grant EP/N510129/1 & TU/B/000074, and the Leverhulme Trust via the CFI.	Aistleitner C, 2015, ACTA ARITH, V167, P143, DOI 10.4064/aa167-2-4; Aliprantis C.D., 2006, INFINITE DIMENSIONAL, Vthird, DOI [10.1007/3-540-29587-9, 10.1007/978-3-662-03961-8, DOI 10.1007/978-3-662-03961-8]; Arjovsky M, 2017, PR MACH LEARN RES, V70; Avron H, 2016, J MACH LEARN RES, V17; Bellemare Marc G., 2017, ABS170510743 ARXIV; Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Brauchart JS, 2012, NUMER MATH, V121, P473, DOI 10.1007/s00211-011-0444-6; Briol FX, 2017, PR MACH LEARN RES, V70; Buchholz A, 2018, PR MACH LEARN RES, V80; Chen Yutian, 2010, UAI; Choromanski  K., 2018, INT C LEARN REPR ICL; Choromanski K., 2017, NIPS, P6; Choromanski K., 2018, ARTIF INTELL; Choromanski K, 2018, PR MACH LEARN RES, V80; Coumans Erwin, 2016, PYBULLET PYTHON MODU; Dick J, 2015, J COMPLEXITY, V31, P327, DOI 10.1016/j.jco.2014.09.003; Gin E., 2015, MATH FDN INFINITE DI; Glorot X., 2010, PROC MACH LEARN RES, P249; Gretton A, 2012, J MACH LEARN RES, V13, P723; Halmos PR., 2013, MEASURE THEORY, V18; Halton J.H., 1960, NUMER MATH, V2, P84, DOI [10.1007/BF01386213, DOI 10.1007/BF01386213]; Hammersley JM, 1956, P CAMB PHILOS SOC, V52, P449; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Kingma D.P, P 3 INT C LEARNING R; Marcus M.B., 1972, P 6 BERK S MATH STAT, V2, P423; Niederreiter H., 1992, RANDOM NUMBER GENERA, V63; Paskov S. H., 1993, Journal of Complexity, V9, P291, DOI 10.1006/jcom.1993.1019; Pass  Brendan, 2014, ESAIM-MATH MODEL NUM, V49; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen Carl Edward, 2003, NEURAL INFORM PROCES; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Robert C. P., 2005, MONTE CARLO STAT MET; Sloan IH, 1998, J COMPLEXITY, V14, P1, DOI 10.1006/jcom.1997.0463; STEWART GW, 1980, SIAM J NUMER ANAL, V17, P403, DOI 10.1137/0717034; TALAGRAND M, 1987, ACTA MATH-DJURSHOLM, V159, P99, DOI 10.1007/BF02392556; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975	38	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300019
C	Shah, A; Kamath, P; Li, S; Shah, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shah, Ankit; Kamath, Pritish; Li, Shen; Shah, Julie			Bayesian Inference of Temporal Task Specifications from Demonstrations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					When observing task demonstrations, human apprentices are able to identify whether a given task is executed correctly long before they gain expertise in actually performing that task. Prior research into learning from demonstrations (LfD) has failed to capture this notion of the acceptability of an execution; meanwhile, temporal logics provide a flexible language for expressing task specifications. Inspired by this, we present Bayesian specification inference, a probabilistic model for inferring task specification as a temporal logic formula. We incorporate methods from probabilistic programming to define our priors, along with a domain-independent likelihood function to enable sampling-based inference. We demonstrate the efficacy of our model for inferring specifications with over 90% similarity between the inferred specification and the ground truth, both within a synthetic domain and a real-world table setting task.	[Shah, Ankit; Kamath, Pritish; Li, Shen; Shah, Julie] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Shah, A (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	ajshah@mit.edu; pritish@mit.edu; shenli@mit.edu; julie_a_shah@mit.edu	Kamath, Pritish/ABF-1354-2021; Shah, Ankit/CAG-8426-2022	Shah, Ankit/0000-0001-6818-0827	Lockheed Martin Corporation; Air Force Research Laboratory [88ABW-2018-2502]	Lockheed Martin Corporation; Air Force Research Laboratory	This research was funded in part by Lockheed Martin Corporation and the Air Force Research Laboratory. Approved for Public Release: distribution unlimited, 88ABW-2018-2502, 16 May 2018	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Aldous D. J., 1985, ECOLE ETE PROBABILIT, P92; [Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], ROBOTICS SCI SYSTEMS; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Arnold Thomas, 2017, 3 INT WORKSH AI ETH; Berenson D, 2011, INT J ROBOT RES, V30, P1435, DOI 10.1177/0278364910396389; Chernova Sonia, 2014, ROBOT LEARNING HUMAN, DOI [10.2200/S00568ED1V01Y201402AIM028, DOI 10.2200/S00568ED1V01Y201402AIM028]; Dwyer M. B., 1999, Proceedings of the 1999 International Conference on Software Engineering (IEEE Cat. No.99CB37002), P411, DOI 10.1109/ICSE.1999.841031; Freer CE, 2014, LECT NOTES LOGIC, V42, P195; Goodman N, 2012, ARXIV PREPRINT ARXIV; Goodman N. D., 2014, DESIGN IMPLEMENTATIO; Jin XQ, 2015, IEEE T COMPUT AID D, V34, P1704, DOI 10.1109/TCAD.2015.2421907; Kasenberg D., 2017, ARXIV171010532; Kim J, 2017, AAAI CONF ARTIF INTE, P955; Kong Z., 2014, P 17 INT C HYBRID SY, P273; Kong ZD, 2017, IEEE T AUTOMAT CONTR, V62, P1210, DOI 10.1109/TAC.2016.2585083; Konidaris G, 2012, INT J ROBOT RES, V31, P360, DOI 10.1177/0278364911428653; Kress-Gazit H, 2009, IEEE T ROBOT, V25, P1370, DOI 10.1109/TRO.2009.2030225; Laplace P S, 1951, PHILOS ESSAY PROBABI; Lemieux C, 2015, IEEE INT CONF AUTOM, P81, DOI 10.1109/ASE.2015.71; Li X, 2017, IEEE INT C INT ROBOT, P3834; Littman Michael L, 2017, ARXIV170404341; Niekum S, 2015, INT J ROBOT RES, V34, P131, DOI 10.1177/0278364914554471; Paul J., NEW PHYTOLOGIST, V11, P37; Pnueli A., 1977, 18th Annual Symposium on Foundations of Computer Science, P46, DOI 10.1109/SFCS.1977.32; Raman V., 2015, P 18 INT C HYBR SYST, P239, DOI DOI 10.1145/2728606.2728628; SHEPARD RN, 1987, SCIENCE, V237, P1317, DOI 10.1126/science.3629243; Tenenbaum J., 2017, ARXIV170709627; Tenenbaum JB, 2000, ADV NEUR IN, V12, P59; Toris R, 2015, IEEE INT CONF ROBOT, P4504, DOI 10.1109/ICRA.2015.7139823; Ziebart B. D., 2008, AAAI, V8, P1433	33	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303077
C	Tsang, M; Liu, HP; Purushotham, S; Murali, P; Liu, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tsang, Michael; Liu, Hanpeng; Purushotham, Sanjay; Murali, Pavankumar; Liu, Yan			Neural Interaction Transparency (NIT): Disentangling Learned Interactions for Improved Interpretability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural networks are known to model statistical interactions, but they entangle the interactions at intermediate hidden layers for shared representation learning. We propose a framework, Neural Interaction Transparency (NIT), that disentangles the shared learning across different interactions to obtain their intrinsic lower-order and interpretable structure. This is done through a novel regularizer that directly penalizes interaction order. We show that disentangling interactions reduces a feedforward neural network to a generalized additive model with interactions, which can lead to transparent models that perform comparably to the state-of-the-art models. NIT is also flexible and efficient; it can learn generalized additive models with maximum K-order interactions by training only O(1) models.	[Tsang, Michael; Liu, Hanpeng; Purushotham, Sanjay; Liu, Yan] Univ Southern Calif, Los Angeles, CA 90089 USA; [Murali, Pavankumar] IBM TJ Watson Res Ctr, Yorktown Hts, NY USA	University of Southern California; International Business Machines (IBM)	Tsang, M (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	tsangm@usc.edu; hanpengl@usc.edu; spurusho@usc.edu; pavanm@us.ibm.com; yanliu.cs@usc.edu	Liu, Hanpeng/HGE-9757-2022; liu, yan/HGV-1365-2022	LIU, Yan/0000-0003-4242-4840	National Science Foundation [IIS-1254206, IIS-1539608]; Samsung GRO grant	National Science Foundation(National Science Foundation (NSF)); Samsung GRO grant(Samsung)	We thank Umang Gupta and anonymous reviewers for their generous feedback. This work was supported by National Science Foundation awards IIS-1254206, IIS-1539608 and a Samsung GRO grant.	Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Che Zhengping, 2016, AMIA Annu Symp Proc, V2016, P371; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Fanaee-T H, 2014, PROG ARTIF INTELL, V2, P113, DOI 10.1007/s13748-013-0040-3; Garson DG., 1991, AI EXPERT, V6, P46; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodman B, 2017, AI MAG, V38, P50, DOI 10.1609/aimag.v38i3.2741; Gordo, 2018, ARXIV180108640; Hastie T.J., 2017, STAT MODELS S, P249, DOI DOI 10.1201/9780203738535; Hechtlinger Yotam, 2016, ARXIV161107634; Higgins I., 2017, P INT C LEARN REPR T; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kulkarni TD, 2015, ADV NEUR IN, V28; Lou Y., 2012, P 18 ACM SIGKDD INT, P150; Lou Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P623, DOI 10.1145/2487575.2487579; Louizos Christos, 2018, INT C LEARN REPR; Louzada Francisco, 2016, Surveys in Operations Research and Management Science, V21, P117, DOI 10.1016/j.sorms.2016.10.001; Olden JD, 2002, ECOL MODEL, V154, P135, DOI 10.1016/S0304-3800(02)00064-9; Pace RK, 1997, STAT PROBABIL LETT, V33, P291, DOI 10.1016/s0167-7152(96)00140-x; Potts W.J., 1999, P 5 ACM SIGKDD INT C, P194; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Sorokina D., 2008, P 25 ANN INT C MACH, P1000, DOI DOI 10.1145/1390156.1390282; Tan Sarah, 2017, AAAI ACM C ART ETH S; Tsang M., 2018, INT C LEARN REPR	31	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000032
C	Upadhyay, U; De, A; Gomez-Rodrizuez, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Upadhyay, Utkarsh; De, Abir; Gomez-Rodrizuez, Manuel			Deep Reinforcement Learning of Marked Temporal Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In a wide variety of applications, humans interact with a complex environment by means of asynchronous stochastic discrete events in continuous time. Can we design online interventions that will help humans achieve certain goals in such asynchronous setting? In this paper, we address the above problem from the perspective of deep reinforcement learning of marked temporal point processes, where both the actions taken by an agent and the feedback it receives from the environment are asynchronous stochastic discrete events characterized using marked temporal point processes. In doing so, we define the agent's policy using the intensity and mark distribution of the corresponding process and then derive a flexible policy gradient method, which embeds the agent' s actions and the feedback it receives into real-valued vectors using deep recurrent neural networks. Our method does not make any assumptions on the functional form of the intensity and mark distribution of the feedback and it allows for arbitrarily complex reward functions. We apply our methodology to two different applications in personalized teaching and viral marketing and, using data gathered from Duolingo and Twitter, we show that it may be able to find interventions to help learners and marketers achieve their goals more effectively than alternatives.	[Upadhyay, Utkarsh; De, Abir] MPI SWS, Saarbrucken, Germany		Upadhyay, U (corresponding author), MPI SWS, Saarbrucken, Germany.	utkarshu@mpi-sws.org; ade@mpi-sws.org; manuelgr@mpi-sws.org	Rodriguez, Manuel Gomez/AAB-5005-2021					Aalen OO, 2008, STAT BIOL HEALTH, P1; [Anonymous], 2016, ICML; Cha M., 2010, INT AAAI C WEB SOC M, P30, DOI DOI 10.1016/J.IPM.2016.04.003; Daley D. J., 2007, INTRO THEORY POINT P, V2; Doya K, 2000, NEURAL COMPUT, V12, P219, DOI 10.1162/089976600300015961; DU N, 2016, KDD; Duan Y., 2016, ICML; Ebbinghaus H., 1885, MEMORY CONTRIBUTION; Farajtabar M., 2017, ICML; Fremaux N, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003024; Hanson F. B., 2007, APPL STOCHASTIC PROC, V13; Jing H., 2017, P WSDM; Karimi M., 2016, KDD; Kim Jooyeon, 2018, WSDM; Leitner S., 1972, ANGEW LERNPSYCHOLOGI; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lindsey RV, 2014, PSYCHOL SCI, V25, P639, DOI 10.1177/0956797613504302; Mei H., 2017, NIPS; Mettler E, 2016, J EXP PSYCHOL GEN, V145, P897, DOI 10.1037/xge0000170; Metzler-Baddeley C, 2009, APPL COGNITIVE PSYCH, V23, P254, DOI 10.1002/acp.1454; Pashler H., 2009, NIPS; Reddy S., 2016, KDD; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Settles B, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1848; Spasojevic Nemanja, 2015, KDD; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tabibian B., 2019, P NATL ACAD SCI; Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586; Wang Y.-X., 2018, AISTATS; Wang YC, 2017, PR MACH LEARN RES, V70; Wierstra D., 2007, ICANN; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zarezade Ali, 2018, JMLR; [No title captured]	34	8	8	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303019
C	Wang, MH; Gong, MM; Zheng, XL; Zhang, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Menghan; Gong, Mingming; Zheng, Xiaolin; Zhang, Kun			Modeling Dynamic Missingness of Implicit Feedback for Recommendation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Implicit feedback is widely used in collaborative filtering methods for recommendation. It is well known that implicit feedback contains a large number of values that are missing not at random (MNAR); and the missing data is a mixture of negative and unknown feedback, making it difficult to learn user's negative preferences. Recent studies modeled exposure, a latent missingness variable which indicates whether an item is exposed to a user, to give each missing entry a confidence of being negative feedback. However, these studies use static models and ignore the information in temporal dependencies among items, which seems to be an essential underlying factor to subsequent missingness. To model and exploit the dynamics of missingness, we propose a latent variable named "user intent" to govern the temporal changes of item missingness, and a hidden Markov model to represent such a process. The resulting framework captures the dynamic item missingness and incorporate it into matrix factorization (MF) for recommendation. We also explore two types of constraints to achieve a more compact and interpretable representation of user intents. Experiments on real-world datasets demonstrate the superiority of our method against state-of-the-art recommender systems.	[Wang, Menghan; Zheng, Xiaolin] Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China; [Gong, Mingming] Univ Pittsburgh, Dept Biomed Informat, Pittsburgh, PA 15260 USA; [Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA	Zhejiang University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Carnegie Mellon University	Zheng, XL (corresponding author), Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China.	wangmengh@zju.edu.cn; mig73@pitt.edu; xlzheng@zju.edu.cn; kunz1@cmu.edu	Wang, Menghan/Y-2958-2019		National Natural Science Foundation of China [U1509221]; National Key Technology RD Program [2015BAHO7F01]; Zhejiang Province key RD program [2017C03044]; United States Air Force [FA8650-17-C-7715]; National Science Foundation under EAGER [IIS-1829681]; National Institutes of Health [NIH-1R01EB022858-01, FAINROlEB 022858, NIH-1R01LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Technology RD Program(National Key Technology R&D Program); Zhejiang Province key RD program; United States Air Force(United States Department of Defense); National Science Foundation under EAGER; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported in part by the National Natural Science Foundation of China (No.U1509221), the National Key Technology R&D Program (2015BAHO7F01), the Zhejiang Province key R&D program (No.2017C03044). This material is partially based upon work supported by United States Air Force under Contract No. FA8650-17-C-7715, by National Science Foundation under EAGER Grant No. IIS-1829681, and National Institutes of Health under Contract No. NIH-1R01EB022858-01, FAINROlEB 022858, NIH-1R01LM012087, NIH-5U54HG008540-02, and FAIN-U54HG008540. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Air Force or the National Institutes of Health or the National Science Foundation. We appreciate the comments from anonymous reviewers, which helped to improve the paper.	Chickering D, 2002, P 18 C UNC ART INT, P94; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Dongyang Li, 2017, 2017 8th International IEEE/EMBS Conference on Neural Engineering (NER), P477, DOI 10.1109/NER.2017.8008393; Ghahramani Z, 1996, ADV NEUR IN, V8, P472; He RN, 2016, IEEE DATA MINING, P191, DOI [10.1109/ICDM.2016.0030, 10.1109/ICDM.2016.88]; He Xiangnan, 2015, P 24 ACM INT C INFOR, P1661; Hernandez-Lobato JM, 2014, PR MACH LEARN RES, V32, P1512; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Lee J., 2013, P 30 INT C MACH LEAR, P82; Liang D, 2016, PR INT ASIA CONF IND, P951, DOI 10.2991/978-94-6239-145-1_93; Ling G., 2012, P 28 C UNC ART INT, P501; Mairal J, 2010, J MACH LEARN RES, V11, P19; Marlin Benjamin M, 2009, RECSYS, P5, DOI DOI 10.1145/1639714.1639717; McAuley J, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2783258.2783381; Mongillo G, 2008, NEURAL COMPUT, V20, P1706, DOI 10.1162/neco.2008.10-06-351; Moore Joshua L, 2013, ISMIR, P401; Pan R, 2008, IEEE DATA MINING, P502, DOI 10.1109/ICDM.2008.16; Rendle S., 2009, BPR BAYESIAN PERSONA, P452; Rendle S, 2010, FACTORIZING PERSONAL, P811, DOI DOI 10.1145/1772690.1772773; Sahoo N, 2012, MIS QUART, V36, P1329; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Shi Y., 2014, CSUR, V47, P3; Spirtes P., 2000, CAUSATION PREDICTION; Wang MH, 2018, AAAI CONF ARTIF INTE, P2516; Wang ZH, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P619, DOI 10.1145/3159652.3159710; Wu CY, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P495, DOI 10.1145/3018661.3018689; Xiang L, 2010, P 16 ACM SIGKDD INT, P723; Yu SZ, 2010, ARTIF INTELL, V174, P215, DOI 10.1016/j.artint.2009.11.011	28	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001023
C	Wang, YH; Xu, C; Xu, CJ; Xu, C; Tao, DC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Yunhe; Xu, Chang; Xu, Chunjing; Xu, Chao; Tao, Dacheng			Learning Versatile Filters for Efficient Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper introduces versatile filters to construct efficient convolutional neural network. Considering the demands of efficient deep learning techniques running on cost-effective hardware, a number of methods have been developed to learn compact neural networks. Most of these works aim to slim down filters in different ways, e.g. investigating small, sparse or binarized filters. In contrast, we treat filters from an additive perspective. A series of secondary filters can be derived from a primary filter. These secondary filters all inherit in the primary filter without occupying more storage, but once been unfolded in computation they could significantly enhance the capability of the filter by integrating information extracted from different receptive fields. Besides spatial versatile filters, we additionally investigate versatile filters from the channel perspective. The new techniques are general to upgrade filters in existing CNNs. Experimental results on benchmark datasets and neural networks demonstrate that CNNs constructed with our versatile filters are able to achieve comparable accuracy as that of original filters, but require less memory and FLOPs.	[Wang, Yunhe; Xu, Chunjing] Huawei Noahs Ark Lab, Shenzhen, Peoples R China; [Xu, Chang; Tao, Dacheng] Univ Sydney, FEIT, SIT, UBTECH Sydney AI Ctr, Sydney, NSW, Australia; [Xu, Chao] Peking Univ, Sch EECS, Cooperat Medianet Innovat Ctr, Key Lab Machine Percept MOE, Beijing, Peoples R China	Huawei Technologies; University of Sydney; Peking University	Wang, YH (corresponding author), Huawei Noahs Ark Lab, Shenzhen, Peoples R China.	yunhe.wang@huawei.com; c.xu@sydney.edu.au; xuchunjing@huawei.com; xuchao@cis.pku.edu.cn; dacheng.tao@sydney.edu.au	Xu, Chang/AAG-9337-2019	Xu, Chang/0000-0002-4756-0609	ARC [DE-180101438, FL-170100117, DP-180103424]; NSFC [61876007, 61872012]	ARC(Australian Research Council); NSFC(National Natural Science Foundation of China (NSFC))	This work was supported in part by the ARC DE-180101438, FL-170100117, DP-180103424, and NSFC under Grant 61876007, 61872012. We also thank Huawei Hisilicon for their technical supports.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora S, 2014, PR MACH LEARN RES, V32; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Dentinel Zarembaw, 2014, NEURIPS, P1269; Figurnov Mikhail, 2016, NEURIPS; Han S., 2016, P 4 INT C LEARN REPR, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Kim Jiwon, 2016, CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2014, ARXIV14126550; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sermanet P., 2011, IJCNN; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Wang YH, 2016, ADV NEUR IN, V29, P253; Wen W, 2016, ADV NEUR IN, V29; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou P, 2018, PROC CVPR IEEE, P528, DOI 10.1109/CVPR.2018.00062	27	8	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301058
C	Xu, YX; Wang, X		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Yixi; Wang, Xiao			Understanding Weight Normalized Deep Neural Networks with Rectified Linear Units	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CLASSIFICATION	This paper presents a general framework for norm-based capacity control for L-p,(q) weight normalized deep neural networks. We establish the upper bound on the Rademacher complexities of this family. With an L-p,(q) normalization where q <= p* and 1/p +1/p* = 1, we discuss properties of a width-independent capacity control, which only depends on the depth by a square root term. We further analyze the approximation properties of L-p,(q) weight normalized deep neural networks. In particular, for an L-i,L-infinity weight normalized network, the approximation error can be controlled by the L-1 norm of the output layer, and the corresponding generalization error only depends on the architecture by the square root of the depth.	[Xu, Yixi; Wang, Xiao] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Xu, YX (corresponding author), Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA.	xu573@purdue.edu; wangxiao@purdue.edu		Wang, Xiao/0000-0001-6576-8019				[Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Arora R., 2018, INT C LEARN REPR; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Bousquet, 2003, SUMMER SCH MACHINE L, P208; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; Golowich  Noah, 2018, P 31 C LEARN THEOR; Goodfellow I, 2016, DEEP LEARNING; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Ledoux M., 2013, PROBABILITY BANACH S, P86; LIANG S, 2017, INT C LEARN REPR; Mohri M., 2018, FDN MACHINE LEARNING; Neyshabur Behnam, 2018, INT C LEARN REPR; Safran I, 2017, PR MACH LEARN RES, V70; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sun SZ, 2016, AAAI CONF ARTIF INTE, P2066; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Zhang T, 2004, J MACH LEARN RES, V5, P1225	28	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300013
C	Zhang, Y; Wei, Y; Yang, Q		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Yu; Wei, Ying; Yang, Qiang			Learning to Multitask	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTIPLE TASKS	Multitask learning has shown promising performance in many applications and many multitask models have been proposed. In order to identify an effective multitask model for a given multitask problem, we propose a learning framework called Learning to MultiTask (L2MT). To achieve the goal, L2MT exploits historical multitask experience which is organized as a training set consisting of several tuples, each of which contains a multitask problem with multiple tasks, a multitask model, and the relative test error. Based on such training set, L2MT first uses a proposed layerwise graph neural network to learn task embeddings for all the tasks in a multitask problem and then learns an estimation function to estimate the relative test error based on task embeddings and the representation of the multitask model based on a unified formulation. Given a new multitask problem, the estimation function is used to identify a suitable multitask model. Experiments on benchmark datasets show the effectiveness of the proposed L2MT framework.	[Zhang, Yu; Yang, Qiang] HKUST, Hong Kong, Peoples R China; [Wei, Ying] Tencent AI Lab, Bellevue, WA USA	Hong Kong University of Science & Technology	Zhang, Y (corresponding author), HKUST, Hong Kong, Peoples R China.	yu.zhang.ust@gmail.com; judywei@tencent.com; qyang@cse.ust.hk	yang, qiang/GYJ-0971-2022	Wei, Ying/0000-0003-1662-4443	NSFC [61673202]; National Grant Fundamental Research (973 Program) of China [2014CB340304]; Hong Kong CERG projects [16211214/16209715/16244616]	NSFC(National Natural Science Foundation of China (NSFC)); National Grant Fundamental Research (973 Program) of China(National Basic Research Program of China); Hong Kong CERG projects	This research has been supported by NSFC 61673202, National Grant Fundamental Research (973 Program) of China under Project 2014CB340304, and Hong Kong CERG projects 16211214/16209715/16244616.	Ando RK, 2005, J MACH LEARN RES, V6, P1817; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Argyriou A., 2007, ADV NEURAL INFORM PR, P25; Argyriou A., 2006, ADV NEURAL INFORM PR, P41; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Banerjee O., 2006, P 23 INT C MACH LEAR, P89, DOI DOI 10.1145/1143844.1143856; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Bruna  J., 2013, CORRABS13126203; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen D, 2009, PROCEEDINGS OF 2009 INTERNATIONAL CONFERENCE OF MANAGEMENT SCIENCE AND INFORMATION SYSTEM, VOLS 1-4, P1375; CHEN J., 2010, P 16 ACM SIGKDD INT, P1179; Chen  Z., 2016, LIFELONG MACHINE LEA; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Han L, 2015, P 29 AAAI C ART INT; Han L, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P397, DOI 10.1145/2783258.2783393; Han Lei, 2016, P 30 AAAI C ART INT, P1638; Jacob L, 2008, ARXIV PREPRINT ARXIV; Jalali A., 2010, ADV NEURAL INF PROCE, V23, P964; Kumar A, 2012, P 29 INT C MACH LEAR; Li T, 2013, PROC INT CONF ANTI; Maurer A, 2016, J MACH LEARN RES, V17; Maurer A, 2014, LECT NOTES ARTIF INT, V8776, P245, DOI 10.1007/978-3-319-11662-4_18; Micchelli CA, 2005, J MACH LEARN RES, V6, P1099; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Niepert M, 2016, PR MACH LEARN RES, V48; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pong TK, 2010, SIAM J OPTIMIZ, V20, P3465, DOI 10.1137/090763184; Rai P., 2012, P ADV NEUR INF PROC, P3185; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sclavounos PD, 2017, PROCEEDINGS OF THE ASME 36TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2017, VOL 9; Srivastava R.K., 2015, CORRABS150500387; Wei Y, 2018, PR MACH LEARN RES, V80; Yang  S., 2012, P ACM SIGKDD C KOWNL; Zhang  Y., 2010, ADV NEURAL INFORM PR, V23; Zhang Y., 2013, PROC INT C NEURAL IN, P1896; Zhang Y., 2017, ARXIV170708114V2; Zhang Y., 2010, PROC 13 INT C ARTIF, P964; Zhang Yi, 2010, ADV NEURAL INF PROCE, P2550; Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733; Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028; Zweig A., 2013, PROC INT C MACH LEAR, P37	44	8	8	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000029
C	Berardino, A; Balle, J; Laparra, V; Simoncelli, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Berardino, Alexander; Balle, Johannes; Laparra, Valero; Simoncelli, Eero			Eigen-Distortions of Hierarchical Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RESPONSES	We develop a method for comparing hierarchical image representations in terms of their ability to explain perceptual sensitivity in humans. Specifically, we utilize Fisher information to establish a model-derived prediction of sensitivity to local perturbations of an image. For a given image, we compute the eigenvectors of the Fisher information matrix with largest and smallest eigenvalues, corresponding to the model-predicted most- and least-noticeable image distortions, respectively. For human subjects, we then measure the amount of each distortion that can be reliably detected when added to the image. We use this method to test the ability of a variety of representations to mimic human perceptual sensitivity. We find that the early layers of VGG16, a deep neural network optimized for object recognition, provide a better match to human perception than later layers, and a better match than a 4-stage convolutional neural network (CNN) trained on a database of human ratings of distorted image quality. On the other hand, we find that simple models of early visual processing, incorporating one or more stages of local gain control, trained on the same database of distortion ratings, provide substantially better predictions of human sensitivity than either the CNN, or any combination of layers of VGG16.	[Berardino, Alexander; Balle, Johannes] NYU, Ctr Neural Sci, New York, NY 10003 USA; [Laparra, Valero] Univ Valencia, Image Proc Lab, Valencia, Spain; [Simoncelli, Eero] NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10003 USA; [Simoncelli, Eero] NYU, Courant Inst Math Sci, New York, NY 10003 USA; [Balle, Johannes] Google Inc, Mountain View, CA USA	New York University; University of Valencia; Howard Hughes Medical Institute; New York University; New York University; Google Incorporated	Berardino, A (corresponding author), NYU, Ctr Neural Sci, New York, NY 10003 USA.	agb313@nyu.edu; johannes.balle@nyu.edu; valero.laparra@uv.es; eero.simoncelli@nyu.edu	Jeong, Yongwook/N-7413-2016; Laparra, Valero/AAD-1937-2019	Laparra, Valero/0000-0001-7531-9890; Simoncelli, Eero/0000-0002-1206-527X	Howard Hughes Medical Institute; NEI Visual Neuroscience Training Program; Samuel J. and Joan B. Williamson Fellowship	Howard Hughes Medical Institute(Howard Hughes Medical Institute); NEI Visual Neuroscience Training Program; Samuel J. and Joan B. Williamson Fellowship	The authors would like to thank the members of the LCV and VNL groups at NYU, especially Olivier Henaff and Najib Majaj, for helpful feedback and comments on the manuscript. Additionally, we thank Rebecca Walton and Lydia Cassard for their tireless efforts in collecting the perceptual data presented here. This work was funded in part by the Howard Hughes Medical Institute, the NEI Visual Neuroscience Training Program and the Samuel J. and Joan B. Williamson Fellowship.	Balle Johannes, 2017, 5 INT C LEARN REPR I; Carandini Matteo, 2012, NATURE REV NEUROSCIE, V13; Dodge S., 2017, STUDY COMP HUMAN DEE; Dosovitskiy Alexey, 2016, NIP2 2016 NEURAL INF; Fisher RA, 1925, P CAMB PHILOS SOC, V22, P700, DOI 10.1017/S0305004100009580; Goodfellow I.J., 2014, ICLR 2014; Henaff OJ, 2016, INT C LEARN REPR; Ioffe Sergey, 2015, ICLR; Johnson Justin, 2016, ECCV EUR C COMP VIS; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kingma D.P, P 3 INT C LEARNING R; Laparra V, 2017, J OPT SOC AM A, V34, P1511, DOI 10.1364/JOSAA.34.001511; Laparra Valero, 2010, J OPTICAL SOC AM A, V27; Lyu Siwei, 2008, P COMP VIS PATT REC; Malo J, 2006, IEEE T IMAGE PROCESS, V15, P68, DOI 10.1109/TIP.2005.860325; Mante V, 2008, NEURON, V58, P625, DOI 10.1016/j.neuron.2008.03.011; Mises RV, 1929, Z ANGEW MATH MECH, V9, P152, DOI [DOI 10.1002/ZAMM.19290090206, 10.1002/zamm.19290090206]; Nguyen J., 2015, IEEE CVPR; Ponomarenko N, 2009, ADV MODERN; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Series Peggy, 2009, NEURAL COMPUTATION; Simonyan Karen, 2015, INT C LEARN REPR; Szegedy C., 2013, P INT C LEARN REPR; Wang Z, 2008, J VISION, V8, DOI 10.1167/8.12.8; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	25	8	8	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403058
C	Calandriello, D; Lazaric, A; Valko, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Calandriello, Daniele; Lazaric, Alessandro; Valko, Michal			Efficient Second-Order Online Kernel Learning with Adaptive Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Online kernel learning (OKL) is a flexible framework for prediction problems, since the large approximation space provided by reproducing kernel Hilbert spaces often contains an accurate function for the problem. Nonetheless, optimizing over this space is computationally expensive. Not only first order methods accumulate O (root T) more loss than the optimal function, but the curse of kernelization results in a O(t) per-step complexity. Second-order methods get closer to the optimum much faster, suffering only O (log T) regret, but second-order updates are even more expensive with their O(t(2)) per-step cost. Existing approximate OKL methods reduce this complexity either by limiting the support vectors (SV) used by the predictor, or by avoiding the kernelization process altogether using embedding. Nonetheless, as long as the size of the approximation space or the number of SV does not grow over time, an adversarial environment can always exploit the approximation process. In this paper, we propose PROS-N-KONS, a method that combines Nystrom sketching to project the input point to a small and accurate embedded space; and to perform efficient second-order updates in this space. The embedded space is continuously updated to guarantee that the embedding remains accurate. We show that the per-step cost only grows with the effective dimension of the problem and not with T. Moreover, the second-order updated allows us to achieve the logarithmic regret. We empirically compare our algorithm on recent large-scales benchmarks and show it performs favorably.	[Calandriello, Daniele; Lazaric, Alessandro; Valko, Michal] INRIA Lille Nord Europe, SequeL Team, Lille, France		Calandriello, D (corresponding author), INRIA Lille Nord Europe, SequeL Team, Lille, France.	daniele.calandriello@inria.fr; alessandro.lazaric@inria.fr; michal.valko@inria.fr	Jeong, Yongwook/N-7413-2016		French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council; Inria; Univertat Potsdam associated-team north-european project Allocate; French National Research Agency [ANR-14-CE24-0010-01, ANR-16-CE23-0003]	French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council(Region Hauts-de-France); Inria; Univertat Potsdam associated-team north-european project Allocate; French National Research Agency(French National Research Agency (ANR))	The research presented was supported by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council, Inria and Univertat Potsdam associated-team north-european project Allocate, and French National Research Agency projects ExTra-Learn (n.ANR-14-CE24-0010-01) and BoB (n.ANR-16-CE23-0003).	Calandriello Daniele, 2017, INT C MACH LEARN; Calandriello Daniele, 2017, AISTATS; Cavallanti G, 2007, MACH LEARN, V69, P143, DOI 10.1007/s10994-007-5003-0; Cohen Michael B, 2016, INT WORKSH APPR RAND; Dekel O, 2008, SIAM J COMPUT, V37, P1342, DOI 10.1137/060666998; El Alaoui A, 2015, ADV NEUR IN, V28; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Hazan Elad, 2006, C LEARN THEOR SPRING; He WW, 2014, NEURAL NETWORKS, V60, P17, DOI 10.1016/j.neunet.2014.07.006; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Le Q, 2013, 30 INT C MACH LEARN; Le T., 2016, PROC 30 INT C NEURAL, P4583; Lu J, 2016, J MACH LEARN RES, V17; Luo Haipeng, 2016, NEURAL INFORM PROCES; Orabona Francesco, 2008, INT C MACH LEARN; Wang Z, 2012, J MACH LEARN RES, V13, P3103; Wathen AJ, 2015, NUMER ALGORITHMS, V70, P709, DOI 10.1007/s11075-015-9970-0; Williams Christopher, 2001, NEURAL INFORM PROCES; Xu, 2015, ARXIV151202394; Yang Tianbao, 2012, NEURAL INFORM PROCES; Yang Y., 2017, ANN STAT; Yi Sun, 2012, INT C MACH LEARN; Yi Xu, 2017, AAAI C ART INT; Zhao Peilin, 2012, INT C MACH LEARN; Zhdanov Fedor, 2010, ALGORITHMIC LEARNING; Zinkevich M., 2003, INT C MACH LEARN	26	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406021
C	Chamon, LFO; Ribeiro, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chamon, Luiz F. O.; Ribeiro, Alejandro			Approximate Supermodularity Bounds for Experimental Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SENSOR SELECTION; ALGORITHMS	This work provides performance guarantees for the greedy solution of experimental design problems. In particular, it focuses on A- and E-optimal designs, for which typical guarantees do not apply since the mean-square error and the maximum eigenvalue of the estimation error covariance matrix are not supermodular. To do so, it leverages the concept of approximate supermodularity to derive non-asymptotic worst-case suboptimality bounds for these greedy solutions. These bounds reveal that as the SNR of the experiments decreases, these cost functions behave increasingly as supermodular functions. As such, greedy A- and E-optimal designs approach (1 - e(-1))-optimality. These results reconcile the empirical success of greedy experimental design with the non-supermodularity of the A- and E-optimality criteria.	[Chamon, Luiz F. O.; Ribeiro, Alejandro] Univ Penn, Elect & Syst Engn, Philadelphia, PA 19104 USA	University of Pennsylvania	Chamon, LFO (corresponding author), Univ Penn, Elect & Syst Engn, Philadelphia, PA 19104 USA.	luizf@seas.upenn.edu; aribeiro@seas.upenn.edu	Jeong, Yongwook/N-7413-2016	Ribeiro, Alejandro/0000-0003-4230-9906	National Science Foundation [CCF 1717120]; ARO [W911NF1710438]	National Science Foundation(National Science Foundation (NSF)); ARO	This work was supported by the National Science Foundation CCF 1717120 and in part by the ARO W911NF1710438.	Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Benzi M, 2002, J COMPUT PHYS, V182, P418, DOI 10.1006/jcph.2002.7176; Bian A., 2017, ICML; Borodin A., 2014, ARXIV14016697V5; Boyd S, 2004, CONVEX OPTIMIZATION; BRAATZ RD, 1994, SIAM J CONTROL OPTIM, V32, P1763, DOI 10.1137/S0363012992238680; Chamon L., 2017, ARXIV171101501; Chamon L.F.O., 2016, GLOB C SIGN INF PROC; Das A, 2008, ACM S THEORY COMPUT, P45; Das Abhimanyu, 2011, COMPUTING RES REPOSI; Digital Equipment Corporation, EACHMOVIE DAT; Elenberg Ethan R, 2016, ARXIV161200804; Flaherty P., 2006, ADV NEURAL INFORM PR, P363; Horel T., 2016, P NIPS BARC, P3045; Horel T., 2014, LAT AM THEOR INF S; Horn R.A., 2013, MATRIX ANAL, Vsecond; Joshi S, 2009, IEEE T SIGNAL PROCES, V57, P451, DOI 10.1109/TSP.2008.2007095; Krause A., 2010, INT C MACH LEARN INT C MACH LEARN; Krause A, 2008, J MACH LEARN RES, V9, P235; Krause A, 2014, TRACTABILITY, P71; Liu SJ, 2016, IEEE T SIGNAL PROCES, V64, P3509, DOI 10.1109/TSP.2016.2550005; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Pukelsheim F, 2006, CLASS APPL MATH, V50, P1, DOI 10.1137/1.9780898719109; Sagnol G, 2013, DISCRETE APPL MATH, V161, P258, DOI 10.1016/j.dam.2012.07.016; Sagnol G, 2011, J STAT PLAN INFER, V141, P1684, DOI 10.1016/j.jspi.2010.11.031; Summers TH, 2016, IEEE T CONTROL NETW, V3, P91, DOI 10.1109/TCNS.2015.2453711; Wang Y., 2017, ARXIV160102068V5; Washizawa Y., 2009, INT WORKSH MACH LEAR; Yu K, 2006, P 23 INT C MACH LEAR, ppp1081; Zhu X.-J., 2008, SEMISUPERVISED LEARN	31	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405047
C	Eickenberg, M; Exarchakis, G; Hirn, M; Mallat, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Eickenberg, Michael; Exarchakis, Georgios; Hirn, Matthew; Mallat, Stephane			Solid Harmonic Wavelet Scattering: Predicting Quantum Molecular Energy from Invariant Descriptors of 3D Electronic Densities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a solid harmonic wavelet scattering representation, invariant to rigid motion and stable to deformations, for regression and classification of 2D and 3D signals. Solid harmonic wavelets are computed by multiplying solid harmonic functions with Gaussian windows dilated at different scales. Invariant scattering coefficients are obtained by cascading such wavelet transforms with the complex modulus nonlinearity. We study an application of solid harmonic scattering invariants to the estimation of quantum molecular energies, which are also invariant to rigid motion and stable with respect to deformations. A multilinear regression over scattering invariants provides close to state of the art results over small and large databases of organic molecules.	[Eickenberg, Michael; Exarchakis, Georgios] PSL Res Univ, Ecole Normale Super, Dept Comp Sci, F-75005 Paris, France; [Hirn, Matthew] Michigan State Univ, Dept Computat Math Sci & Engn, Dept Math, E Lansing, MI 48824 USA; [Mallat, Stephane] PSL Res Univ, Ecole Normale Super, Coll France, F-75005 Paris, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Michigan State University; UDICE-French Research Universities; PSL Research University Paris; College de France; Ecole Normale Superieure (ENS)	Eickenberg, M (corresponding author), PSL Res Univ, Ecole Normale Super, Dept Comp Sci, F-75005 Paris, France.	michael.eickenberg@nsup.org; georgios.exarchakis@ens.fr; mhirn@msu.edu	Jeong, Yongwook/N-7413-2016	Exarchakis, Georgios/0000-0003-2517-5782; Hirn, Matthew/0000-0003-0290-4292	ERC [320959]; Alfred P. Sloan Fellowship; DARPA YFA; NSF [1620216]	ERC(European Research Council (ERC)European Commission); Alfred P. Sloan Fellowship(Alfred P. Sloan Foundation); DARPA YFA; NSF(National Science Foundation (NSF))	M.E., G.E. and S.M. are supported by ERC grant InvariantClass 320959; M.H. is supported by the Alfred P. Sloan Fellowship, the DARPA YFA, and NSF grant 1620216.	BARTOK AP, 2013, PHYS REV B, V87; Collins Christopher R., 2017, CONSTANT SIZE MOL DE; De S, 2016, PHYS CHEM CHEM PHYS, V18, P13754, DOI 10.1039/c6cp00415f; Deglmann P, 2015, INT J QUANTUM CHEM, V115, P107, DOI 10.1002/qua.24811; Faber Felix A., J CHEM THEORY COMPUT; Gilmer Justin, 2017, CORR; Hansen K, 2015, J PHYS CHEM LETT, V6, P2326, DOI 10.1021/acs.jpclett.5b00831; Hansen K, 2013, J CHEM THEORY COMPUT, V9, P3404, DOI 10.1021/ct400195d; Hirn M, 2017, MULTISCALE MODEL SIM, V15, P827, DOI 10.1137/16M1075454; Huang B, 2016, J CHEM PHYS, V145, DOI 10.1063/1.4964627; Kingma D.P, P 3 INT C LEARNING R; Mallat S, 2016, PHILOS T R SOC A, V374, DOI 10.1098/rsta.2015.0203; Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413; Memisevic R, 2011, IEEE I CONF COMP VIS, P1591, DOI 10.1109/ICCV.2011.6126419; Montavon G., 2012, ADV NEURAL INFORM PR, V25, P449; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163; Tkatchenko A, 2012, PHYS REV LETT, V108, DOI [10.1103/PhysRevLett.108.058301, 10.1103/PhysRevLett.108.236402]	20	8	8	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406059
C	Ioffe, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ioffe, Sergey			Batch Renormalization: Towards Reducing Minibatch Dependence in Batch-Normalized Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Batch Normalization is quite effective at accelerating and improving the training of deep models. However, its effectiveness diminishes when the training minibatches are small, or do not consist of independent samples. We hypothesize that this is due to the dependence of model layer inputs on all the examples in the minibatch, and different activations being produced between training and inference. We propose Batch Renormalization, a simple and effective extension to ensure that the training and inference models generate the same outputs that depend on individual examples rather than the entire minibatch. Models trained with Batch Renormalization perform substantially better than batchnorm when training with small or non-i.i.d. minibatches. At the same time, Batch Renormalization retains the benefits of batchnorm such as insensitivity to initialization and training efficiency.	[Ioffe, Sergey] Google, Mountain View, CA 94043 USA	Google Incorporated	Ioffe, S (corresponding author), Google, Mountain View, CA 94043 USA.	sioffe@google.com	Jeong, Yongwook/N-7413-2016					Arpit D., 2016, ARXIV160301431; Ba L.J, 2016, P C WORKSH NEUR INF; Chen J., 2016, ABS160400981 CORR; Goldberger J., 2004, P ADV NEURAL INFORM; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Nair V., 2010, ICML, P807; Russakovsky O., 2014, IMAGENET LARGE SCALE; Salimans T, 2016, ADV NEUR IN, V29; Schroff F., 2015, CORR; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4	14	8	8	4	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401094
C	Kazerouni, A; Ghavamzadeh, M; Abbasi-Yadkori, Y; Van Roy, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kazerouni, Abbas; Ghavamzadeh, Mohammad; Abbasi-Yadkori, Yasin; Van Roy, Benjamin			Conservative Contextual Linear Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Safety is a desirable property that can immensely increase the applicability of learning algorithms in real-world decision-making problems. It is much easier for a company to deploy an algorithm that is safe, i.e., guaranteed to perform at least as well as a baseline. In this paper, we study the issue of safety in contextual linear bandits that have application in many different fields including personalized recommendation. We formulate a notion of safety for this class of algorithms. We develop a safe contextual linear bandit algorithm, called conservative linear UCB (CLUCB), that simultaneously minimizes its regret and satisfies the safety constraint, i.e., maintains its performance above a fixed percentage of the performance of a baseline strategy, uniformly over time. We prove an upper-bound on the regret of CLUCB and show that it can be decomposed into two terms: 1) an upper-bound for the regret of the standard linear UCB algorithm that grows with the time horizon and 2) a constant term that accounts for the loss of being conservative in order to satisfy the safety constraint. We empirically show that our algorithm is safe and validate our theoretical analysis.	[Kazerouni, Abbas; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA; [Ghavamzadeh, Mohammad] DeepMind, London, England; [Abbasi-Yadkori, Yasin] Adobe Res, New York, NY USA	Stanford University; Adobe Systems Inc.	Kazerouni, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	abbask@stanford.edu; ghavamza@google.com; abbasiya@adobe.com; byr@stanford.edu	Jeong, Yongwook/N-7413-2016					Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dani V, 2008, P C LEARN THEOR COLT, P355; Petrik M., 2016, ADV NEURAL INFORM PR, V29, P2298; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Swaminathan A., 2015, P 32 INT C MACH LEAR; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; Thomas P., 2015, P 29 C ART INT; Thomas PS, 2015, PR MACH LEARN RES, V37, P2380; Wu YF, 2016, PR MACH LEARN RES, V48	13	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403094
C	Kilbertus, N; Rojas-Carulla, M; Parascandolo, G; Hardt, M; Janzing, D; Scholkopf, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kilbertus, Niki; Rojas-Carulla, Mateo; Parascandolo, Giambattista; Hardt, Moritz; Janzing, Dominik; Scholkopf, Bernhard			Avoiding Discrimination through Causal Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recent work on fairness in machine learning has focused on various statistical discrimination criteria and how they trade off. Most of these criteria are observational: They depend only on the joint distribution of predictor, protected attribute, features, and outcome. While convenient to work with, observational criteria have severe inherent limitations that prevent them from resolving matters of fairness conclusively. Going beyond observational criteria, we frame the problem of discrimination based on protected attributes in the language of causal reasoning. This viewpoint shifts attention from "What is the right fairness criterion?" to "What do we want to assume about our model of the causal data generating process?" Through the lens of causality, we make several contributions. First, we crisply articulate why and when observational criteria fail, thus formalizing what was before a matter of opinion. Second, our approach exposes previously ignored subtleties and why they are fundamental to the problem. Finally, we put forward natural causal non-discrimination criteria and develop algorithms that satisfy them.	[Kilbertus, Niki; Rojas-Carulla, Mateo; Parascandolo, Giambattista; Janzing, Dominik; Scholkopf, Bernhard] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Kilbertus, Niki; Rojas-Carulla, Mateo] Univ Cambridge, Cambridge, England; [Parascandolo, Giambattista] Max Planck ETH Ctr Learning Syst, Tubingen, Germany; [Hardt, Moritz] Univ Calif Berkeley, Berkeley, CA 94720 USA	Max Planck Society; University of Cambridge; University of California System; University of California Berkeley	Kilbertus, N (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.; Kilbertus, N (corresponding author), Univ Cambridge, Cambridge, England.	nkilbertus@tue.mpg.de; mrojas@tue.mpg.de; gparascandolo@tue.mpg.de; hardt@berkeley.edu; janzing@tue.mpg.de; bs@tue.mpg.de	Schölkopf, Bernhard/A-7570-2013; Jeong, Yongwook/N-7413-2016	Schölkopf, Bernhard/0000-0002-8177-0925; 				Angrist Joshua, 2001, TECH REP; Berk R., 2017, ARXIV170309207V1; BICKEL PJ, 1975, SCIENCE, V187, P398, DOI 10.1126/science.187.4175.398; Bonchi Francesco, 2017, ARXIV151000552V3; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Chouldechova A., 2016, ARXIV161007524V1; Cornia N, 2014, P UAI 2014 C CAUS IN, V1274, P35; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Edwards Harrison, 2015, ARXIV151105897V3; Friedler Sorelle A., 2016, ARXIV160907236V1; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Kamiran F, 2013, KNOWL INF SYST, V35, P613, DOI 10.1007/s10115-012-0584-8; Kleinberg Jon, 2016, ARXIV160905807V1; Kusner Matt J., 2017, ARXIV170306856V1; Nabi R., 2017, ARXIV170510378V1; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Qureshi B., 2016, ARXIV160803735; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; VanderWeele TJ, 2014, EPIDEMIOLOGY, V25, P473, DOI 10.1097/EDE.0000000000000105; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang LF, 2017, INTERNATIONAL SYMPOSIUM 2017: SOCIAL SCIENCE MANAGEMENT AND INNOVATION, P1, DOI 10.1109/TSMC.2017.2691909	24	8	8	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400063
C	Kleindessner, M; Von Luxburg, U		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kleindessner, Matthaeus; Von Luxburg, Ulrike			Kernel functions based on triplet comparisons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Given only information in the form of similarity triplets "Object A is more similar to object B than to object C" about a data set, we propose two ways of defining a kernel function on the data set. While previous approaches construct a low-dimensional Euclidean embedding of the data set that reflects the given similarity triplets, we aim at defining kernel functions that correspond to high-dimensional embeddings. These kernel functions can subsequently be used to apply any kernel method to the data set.	[Kleindessner, Matthaeus] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA; [Von Luxburg, Ulrike] Univ Tubingen, Dept Comp Sci, Tubingen, Germany; [Von Luxburg, Ulrike] Max Planck Inst Intelligent Syst, Tubingen, Germany; [Kleindessner, Matthaeus] Univ Tubingen, Tubingen, Germany	Rutgers State University New Brunswick; Eberhard Karls University of Tubingen; Max Planck Society; Eberhard Karls University of Tubingen	Kleindessner, M (corresponding author), Rutgers State Univ, Dept Comp Sci, Piscataway, NJ 08854 USA.	mk1572@cs.rutgers.edu; luxburg@informatik.uni-tuebingen.de			Institutional Strategy of the University of Tubingen (DFG) [ZUK 63]	Institutional Strategy of the University of Tubingen (DFG)	This work has been supported by the Institutional Strategy of the University of Tubingen (DFG, ZUK 63).	Agarwal S., 2007, INT C ART INT STAT A; Amid E., 2015, INT C MACH LEARN ICM; Amid E., 2016, ARXIV161109957V1CSAI; De Silva V., 2004, TECHNICAL REPORT; Dhillon I., 2001, INT C KNOWL DISC DAT; Greene D., 2006, INT C MACH LEARN ICM; Gustavson F. G., 1978, ACM Transactions on Mathematical Software, V4, P250, DOI 10.1145/355791.355796; Haghiri Siavash, 2017, ARTIFICIAL INTELLIGE; Heikinheimo H., 2013, C HUM COMP CROWDS HC; Heim E., 2015, SIAM INT C DAT MIN S; HIGHAM NJ, 1990, ACM T MATH SOFTWARE, V16, P352, DOI 10.1145/98267.98290; Jain L., 2016, NEURAL INFORM PROCES; Jamieson K., 2011, ALL C COMM CONTR COM; Jiao Y., 2015, INT C MACH LEARN ICM; Kaplan H., 2006, S COMP GEOM SOCG; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kleindessner M., 2014, C LEARN THEOR COLT; Kleindessner M, 2017, J MACH LEARN RES, V18, P1; McFee B, 2011, J MACH LEARN RES, V12, P491; Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327; Scholkopf B., 2002, EUR C MACH LEARN ECM; SCHULTZ M, 2003, NEURAL INFORM PROCES; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Stewart N, 2005, PSYCHOL REV, V112, P881, DOI 10.1037/0033-295X.112.4.881; Tamuz O., 2011, INT C MACH LEARN ICM; Terada Y., 2014, INT C MACH LEARN ICM; Ukkonen A., 2017, INT C DAT MIN SER IC; Ukkonen A., 2015, C HUM COMP CROWDS HC; van der Maaten L, 2012, IEEE INT WORKS MACH; Wilber M., 2015, INT C COMP VIS ICCV; Wilber M. J., 2014, C HUM COMP CROWDS HC	31	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406084
C	Ma, SY; Belkin, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ma, Siyuan; Belkin, Mikhail			Diving into the shallows: a computational perspective on large-scale shallow learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Remarkable recent success of deep neural networks has not been easy to analyze theoretically. It has been particularly hard to disentangle relative significance of architecture and optimization in achieving accurate classification on large datasets. On the flip side, shallow methods (such as kernel methods) have encountered obstacles in scaling to large data, despite excellent performance on smaller datasets, and extensive theoretical analysis. Practical methods, such as variants of gradient descent used so successfully in deep learning, seem to perform below par when applied to kernel methods. This difficulty has sometimes been attributed to the limitations of shallow architecture. In this paper we identify a basic limitation in gradient descent-based optimization methods when used in conjunctions with smooth kernels. Our analysis demonstrates that only a vanishingly small fraction of the function space is reachable after a polynomial number of gradient descent iterations. That drastically limits the approximating power of gradient descent leading to over-regularization. The issue is purely algorithmic, persisting even in the limit of infinite data. To address this shortcoming in practice, we introduce EigenPro iteration, a simple and direct preconditioning scheme using a small number of approximately computed eigenvectors. It can also be viewed as learning a kernel optimized for gradient descent. Injecting this small, computationally inexpensive and SGD-compatible, amount of approximate second-order information leads to major improvements in convergence. For large data, this leads to a significant performance boost over the state-of-the-art kernel methods. In particular, we are able to match or improve the results reported in the literature at a small fraction of their computational budget. For complete version of this paper see https://arxiv.org/abs/1703.10622.	[Ma, Siyuan; Belkin, Mikhail] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Ma, SY (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	masi@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1422830, IIS-1550757]	NSF(National Science Foundation (NSF))	We thank Adam Stiff, Eric Fosler-Lussier, Jitong Chen, and Deliang Wang for providing TIMIT and HINT datasets. This work is supported by NSF IIS-1550757 and NSF CCF-1422830. Part of this work was completed while the second author was at the Simons Institute at Berkeley. In particular, he thanks Suvrit Sra, Daniel Hsu, Peter Bartlett, and Stefanie Jegelka for many discussions and helpful suggestions.	Agarwal N., 2016, ARXIV160203943, V18, P1; [Anonymous], P INT C MACH LEARN; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Avron Haim, 2016, ARXIV161103220; Bishop C.M., 2006, MACH LEARN, V128; Bordes A, 2009, J MACH LEARN RES, V10, P1737; Boyd S, 2004, CONVEX OPTIMIZATION; Braun Mikio L, 2005, THESIS; Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362; Camoriano R, 2016, JMLR WORKSH CONF PRO, V51, P1403; Chih-Chieh Cheng, 2011, 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P5200; Dai B., 2014, NIPS; Dennis J. E., 1996, NUMERICAL METHODS UN, DOI DOI 10.1137/1.9781611971200; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Erdogdu M., 2015, NIPS; Gonen A, 2016, PR MACH LEARN RES, V48; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; Jie Chen, 2016, ARXIV160800860; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kingma D.P, P 3 INT C LEARNING R; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Lu Z, 2014, ARXIV14114000; May A., 2017, ARXIV170103577; Minsker Stanislav, 2017, STAT PROBABILITY LET; Moritz P, 2016, AISTATS; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Povey D., 2011, ASRU; Que QC, 2016, JMLR WORKSH CONF PRO, V51, P1375; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Raskutti G, 2014, J MACH LEARN RES, V15, P335; Richardson LF, 1911, PHILOS T R SOC LOND, V210, P307, DOI 10.1098/rsta.1911.0009; Rosasco L, 2010, J MACH LEARN RES, V11, P905; Rosenberg S., 1997, LONDON MATH SOC STUD, DOI DOI 10.1017/CBO9780511623783; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Schraudolph N. N., 2007, PROC 11 INT C ARTIF, P436; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Shewchuk J. R., 1994, INTRO CONJUGATE GRAD; Sindhwani V., 2005, P 22 INT C MACH LEAR, P824, DOI DOI 10.1145/1102351.1102455; Steinwart I., 2008, SUPPORT VECTOR MACHI; Taka c. M, 2013, P 30 INT C MACH LEAR, V28, P1022; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tropp J. A., 2015, ARXIV150101571; Tsybakov AB, 2004, ANN STAT, V32, P135; Tu Stephen, 2016, ARXIV160205310; WILLIAMS CKI, 2001, NIPS, V13, P682; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2	49	8	8	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403082
C	Osokin, A; Bach, F; Lacoste-Julien, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Osokin, Anton; Bach, Francis; Lacoste-Julien, Simon			On Structured Prediction Theory with Calibrated Convex Surrogate Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STATISTICAL-ANALYSIS; CLASSIFICATION; CONSISTENCY; MULTICLASS; RANKING	We provide novel theoretical insights on structured prediction in the context of efficient convex surrogate loss minimization with consistency guarantees. For any task loss, we construct a convex surrogate that can be optimized via stochastic gradient descent and we prove tight bounds on the so-called "calibration function" relating the excess surrogate risk to the actual risk. In contrast to prior related work, we carefully monitor the effect of the exponential number of classes in the learning guarantees as well as on the optimization complexity. As an interesting consequence, we formalize the intuition that some task losses make learning harder than others, and that the classical 0-1 loss is ill-suited for structured prediction.	[Osokin, Anton; Bach, Francis] PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France; [Osokin, Anton] Natl Res Univ, HSE, Moscow, Russia; [Lacoste-Julien, Simon] Univ Montreal, MILA, Montreal, PQ, Canada; [Lacoste-Julien, Simon] Univ Montreal, DIRO, Montreal, PQ, Canada	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; HSE University (National Research University Higher School of Economics); Universite de Montreal; Universite de Montreal	Osokin, A (corresponding author), PSL Res Univ, CNRS, DI Ecole Normale Super, INRIA ENS, Paris, France.		Osokin, Anton/D-7398-2012; Jeong, Yongwook/N-7413-2016	Osokin, Anton/0000-0002-8807-5132; 	ERC grant Activia [307574]; NSERC [RGPIN-2017-06936]; MSR-INRIA Joint Center	ERC grant Activia; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); MSR-INRIA Joint Center	We would like to thank Pascal Germain for useful discussions. This work was partly supported by the ERC grant Activia (no. 307574), the NSERC Discovery Grant RGPIN-2017-06936 and the MSR-INRIA Joint Center.	Agarwal S, 2014, J MACH LEARN RES, V15, P1653; Avila Pires Bernardo, 2016, ARXIV160906385V1; BakIr G., 2007, PREDICTING STRUCTURE; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bousquet O., 2008, NIPS; Brouard C, 2016, J MACH LEARN RES, V17; Buffoni D., 2011, ICML; Calauz`enes C., 2012, NIPS; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Collins M., 2002, EMNLP; Cortes Corinna, 2016, NIPS; Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; Do Chuong B., 2009, NIPS; Dogan U, 2016, J MACH LEARN RES, V17; Duchi J., 2010, ICML; Durbin R., 1998, BIOL SEQUENCE ANAL P; Gao W., 2011, COLT; Gimpel Kevin, 2010, NAACL; Hazan T., 2010, NIPS; Keshet J., 2014, ADV STRUCTURED PREDI; Kotlowski Wojciech, 2011, ICML; LAFFERTY J, 2001, ICML; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Lin Y, 2004, STAT PROBABIL LETT, V68, P73, DOI 10.1016/j.spl.2004.03.002; London B, 2016, J MACH LEARN RES, V17; Long Phil, 2013, ICML; McAllester D., 2007, PREDICTING STRUCTURE; McAllester D., 2011, NIPS; Narasimhan H., 2015, ICML; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Nowozin Sebastian, 2014, ADV STRUCTURED PREDI; Orabona Francesco, 2014, NIPS; Pedregosa F, 2017, J MACH LEARN RES, V18, P1; Pires B. A, 2013, ICML; Pletscher Patrick, 2010, ECML PKDD; Ramaswamy H. G., 2013, NIPS; Ramaswamy HG, 2016, J MACH LEARN RES, V17; Rosasco Lorenzo, 2016, NIPS; Shi QF, 2015, IEEE T PATTERN ANAL, V37, P2, DOI 10.1109/TPAMI.2014.2306414; Smith N., 2011, SYNTHESIS LECT HUMAN, V4, P1; Taskar B., 2005, ICML; Taskar Ben, 2003, NIPS; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Williamson RC, 2016, J MACH LEARN RES, V17, P1; Zhang T, 2004, ANN STAT, V32, P56; Zhang T, 2004, J MACH LEARN RES, V5, P1225	50	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400029
C	Papini, M; Pirotta, M; Restelli, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Papini, Matteo; Pirotta, Matteo; Restelli, Marcello			Adaptive Batch Size for Safe Policy Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				IMPROVEMENT	Policy gradient methods are among the best Reinforcement Learning (RL) techniques to solve complex control problems. In real-world RL applications, it is common to have a good initial policy whose performance needs to be improved and it may not be acceptable to try bad policies during the learning process. Although several methods for choosing the step size exist, research paid less attention to determine the batch size, that is the number of samples used to estimate the gradient direction for each update of the policy parameters. In this paper, we propose a set of methods to jointly optimize the step and the batch sizes that guarantee (with high probability) to improve the policy performance after each update. Besides providing theoretical guarantees, we show numerical simulations to analyse the behaviour of our methods.	[Papini, Matteo; Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy; [Pirotta, Matteo] Inria Lille, SequeL Team, Lille, France	Polytechnic University of Milan	Papini, M (corresponding author), Politecn Milan, DEIB, Milan, Italy.	matteo.papini@polimi.it; matteo.pirotta@inria.fr; marcello.restelli@polimi.it	Jeong, Yongwook/N-7413-2016; Papini, Matteo/AAF-3768-2019	Restelli, Marcello/0000-0002-6322-1076; Papini, Matteo/0000-0002-3807-3171	French Ministry of Higher Education and Research; French National Research Agency (ANR) [ANR-14-CE24-0010-01]; Nord-Pas-de-Calais Regional Council	French Ministry of Higher Education and Research; French National Research Agency (ANR)(French National Research Agency (ANR)); Nord-Pas-de-Calais Regional Council(Region Hauts-de-France)	This research was supported in part by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and French National Research Agency (ANR) under project ExTra-Learn (n. ANR-14-CE24-0010-01).	Abbasi-Yadkori Y, 2016, JMLR WORKSH CONF PRO, V51, P1338; Ghavamzadeh Mohammad, 2016, SAFE POLICY IMPROVEM, P2298; Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595; Kakade S., 2002, P 19 INT C MACH LEAR; Kober J, 2008, ADV NEURAL INFORM PR, P849; Mnih V., 2008, P 25 INT C MACH LEAR; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Peters Jan, 2010, AAAI C ART INT, P24; Pinsker M. S., 1960, INFORM INFORM STABIL; Pirotta M., 2013, INT C MACH LEARN, P307; Pirotta M., 2013, ADV NEURAL INFORM PR, V26, P1394; Pirotta M, 2015, MACH LEARN, V100, P255, DOI 10.1007/s10994-015-5484-1; Scherrer B, 2014, PR MACH LEARN RES, V32, P1314; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sehnke F, 2008, LECT NOTES COMPUT SC, V5163, P387, DOI 10.1007/978-3-540-87536-9_40; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Thomas PS, 2015, PR MACH LEARN RES, V37, P2380; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhao TT, 2012, NEURAL NETWORKS, V26, P118, DOI 10.1016/j.neunet.2011.09.005	24	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403064
C	Volokitin, A; Roig, G; Poggio, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Volokitin, Anna; Roig, Gemma; Poggio, Tomaso			Do Deep Neural Networks Suffer from Crowding?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Crowding is a visual effect suffered by humans, in which an object that can be recognized in isolation can no longer be recognized when other objects, called flankers, are placed close to it. In this work, we study the effect of crowding in artificial Deep Neural Networks (DNNs) for object recognition. We analyze both deep convolutional neural networks (DCNNs) as well as an extension of DCNNs that are multi-scale and that change the receptive field size of the convolution filters with their position in the image. The latter networks, that we call eccentricity-dependent, have been proposed for modeling the feedforward path of the primate visual cortex. Our results reveal that the eccentricity-dependent model, trained on target objects in isolation, can recognize such targets in the presence of flankers, if the targets are near the center of the image, whereas DCNNs cannot. Also, for all tested networks, when trained on targets in isolation, we find that recognition accuracy of the networks decreases the closer the flankers are to the target and the more flankers there are. We find that visual similarity between the target and flankers also plays a role and that pooling in early layers of the network leads to more crowding. Additionally, we show that incorporating flankers into the images of the training set for learning the DNNs does not lead to robustness against configurations not seen at training.	[Volokitin, Anna; Roig, Gemma; Poggio, Tomaso] MIT, Ctr Brains Minds & Machines, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Roig, Gemma; Poggio, Tomaso] MIT, Ist Italiano Tecnol, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Volokitin, Anna] Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland; [Roig, Gemma] Singapore Univ Technol & Design, Singapore, Singapore	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Swiss Federal Institutes of Technology Domain; ETH Zurich; Singapore University of Technology & Design	Volokitin, A (corresponding author), MIT, Ctr Brains Minds & Machines, 77 Massachusetts Ave, Cambridge, MA 02139 USA.; Volokitin, A (corresponding author), Swiss Fed Inst Technol, Comp Vis Lab, Zurich, Switzerland.	voanna@vision.ee.ethz.ch; gemmar@mit.edu; tp@csail.mit.edu	Jeong, Yongwook/N-7413-2016		Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF -1231216]; Swiss Commission for Technology and Innovation (KTI) [2-69723-16]; SUTD SRG grant [SRG ISTD 2017 131]	Center for Brains, Minds and Machines (CBMM) - NSF STC award; Swiss Commission for Technology and Innovation (KTI); SUTD SRG grant(Singapore University of Technology & Design)	This work was supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF -1231216. A. Volokitin was also funded by Swiss Commission for Technology and Innovation (KTI, Grant No 2-69723-16), and thanks Luc Van Gool for his support. G. Roig was partly funded by SUTD SRG grant (SRG ISTD 2017 131). We also thank Xavier Boix, Francis Chen and Yena Han for helpful discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Balas B, 2009, J VISION, V9, DOI [10.1167/9.2.16, 10.1167/9.12.13]; BANKS WP, 1977, PERCEPT PSYCHOPHYS, V22, P232, DOI 10.3758/BF03199684; BOUMA H, 1970, NATURE, V226, P177, DOI 10.1038/226177a0; Burges, 1998, MNIST DATABASE HANDW; Chen F., 2017, AAAI SPRING S SERIES; Cheung B., 2016, INT C LEARN REPR; Francis G., 2016, J VISION, V16, P1114, DOI DOI 10.1167/16.12.1114.[; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; Goodfellow IJ, 2014, 3 INT C LEARNING REP; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Keshvari S, 2016, J VISION, V16, DOI 10.1167/16.3.39; KOOI FL, 1994, SPATIAL VISION, V8, P255, DOI 10.1163/156856894X00350; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Levi DM, 2008, VISION RES, V48, P635, DOI 10.1016/j.visres.2007.12.009; Luo Y., 2015, ARXIV PREPRINT ARXIV, DOI [10.48550/arXiv.1511.06292, DOI 10.48550/ARXIV.1511.06292]; Mnih V, 2014, ADV NEUR IN, V27; NAZIR TA, 1992, VISION RES, V32, P771, DOI 10.1016/0042-6989(92)90192-L; Poggio T., 2014, ARXIV14061770, V1770; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Whitney D, 2011, TRENDS COGN SCI, V15, P160, DOI 10.1016/j.tics.2011.02.005; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	27	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405069
C	Xu, YC; Zhang, HY; Miller, K; Singh, A; Dubrawski, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Yichong; Zhang, Hongyang; Miller, Kyle; Singh, Aarti; Dubrawski, Artur			Noise-Tolerant Interactive Learning Using Pairwise Comparisons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				BOUNDS	We study the problem of interactively learning a binary classifier using noisy labeling and pairwise comparison oracles, where the comparison oracle answers which one in the given two instances is more likely to be positive. Learning from such oracles has multiple applications where obtaining direct labels is harder but pairwise comparisons are easier, and the algorithm can leverage both types of oracles. In this paper, we attempt to characterize how the access to an easier comparison oracle helps in improving the label and total query complexity. We show that the comparison oracle reduces the learning problem to that of learning a threshold function. We then present an algorithm that interactively queries the label and comparison oracles and we characterize its query complexity under Tsybakov and adversarial noise conditions for the comparison and labeling oracles. Our lower bounds show that our label and total query complexity is almost optimal.	[Xu, Yichong; Zhang, Hongyang; Singh, Aarti] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Miller, Kyle; Dubrawski, Artur] Carnegie Mellon Univ, Anton Lab, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Xu, YC (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yichongx@cs.cmu.edu; hongyanz@cs.cmu.edu; mille856@andrew.cmu.edu; aarti@cs.cmu.edu; awd@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		AFRL [FA8750-17-2-0212]	AFRL(United States Department of DefenseUS Air Force Research Laboratory)	This research is supported in part by AFRL grant FA8750-17-2-0212. We thank Chicheng Zhang for insightful ideas on improving results in [6] using Rademacher complexity.	Agarwal S, 2005, LECT NOTES COMPUT SC, V3559, P32, DOI 10.1007/11503415_3; Agarwal S, 2009, J MACH LEARN RES, V10, P441; Ailon N., 2007, ARXIV07102889; Attenberg J, 2010, LECT NOTES ARTIF INT, V6321, P40, DOI 10.1007/978-3-642-15880-3_9; Awasthi P., 2016, C LEARN THEOR COLT, P152; Awasthi P, 2017, J ACM, V63, DOI 10.1145/3006384; Balcan M., 2012, P C LEARN THEOR, P20; Balcan M.-F., 2016, ARXIV160509227; Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5; Balcan Maria-Florina, 2016, ADV NEURAL INFORM PR, P2955; Balcan P. Long, 2013, C LEARNING THEORY CO, P288; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Beygelzimer A., 2016, ADV NEURAL INFORM PR, P3342; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; Dekel O, 2012, J MACH LEARN RES, V13, P2655; Furnkranz J, 2010, PREFERENCE LEARNING, P65, DOI 10.1007/978-3-642-14125-6_4; Hanneke S., 2014, THEORY ACTIVE LEARNI; Hanneke S., 2012, ARXIV12073772; Hanneke S., 2009, COLT; Heckel R., 2016, ARXIV160608842; Jamieson Kevin G, 2011, ADV NEURAL INFORM PR, P2240; Kane D. M., 2017, ARXIV170403564; Krishnamurthy A., 2015, THESIS; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; Maji S, 2014, INT J COMPUT VISION, V108, P82, DOI 10.1007/s11263-014-0716-6; Sabato Sivan, 2016, P 29 ANN C LEARN THE, P1419; Shah N. B., 2014, ARXIV14066618; Stewart N, 2005, PSYCHOL REV, V112, P881, DOI 10.1037/0033-295X.112.4.881; Tsybakov AB, 2004, ANN STAT, V32, P135; Wah C, 2014, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2014.115; Welling M., 2014, ADV NEURAL INFORM PR, V27; Yan S., 2017, ARXIV170205581; Yang L., 2009, CMUML09113	34	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402047
C	Yeh, RA; Xiong, J; Hwu, WMW; Do, MN; Schwing, AG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yeh, Raymond A.; Xiong, Jinjun; Hwu, Wen-mei W.; Do, Minh N.; Schwing, Alexander G.			Interpretable and Globally Optimal Prediction for Textual Grounding using Image Concepts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Textual grounding is an important but challenging task for human-computer interaction, robotics and knowledge mining. Existing algorithms generally formulate the task as selection from a set of bounding box proposals obtained from deep net based systems. In this work, we demonstrate that we can cast the problem of textual grounding into a unified framework that permits efficient search over all possible bounding boxes. Hence, the method is able to consider significantly more proposals and doesn't rely on a successful first stage hypothesizing bounding box proposals. Beyond, we demonstrate that the trained parameters of our model can be used as word-embeddings which capture spatial-image relationships and provide interpretability. Lastly, at the time of submission, our approach outperformed the current state-of-the-art methods on the Flickr 30k Entities and the ReferItGame dataset by 3.08% and 7.77% respectively.	[Yeh, Raymond A.; Hwu, Wen-mei W.; Do, Minh N.; Schwing, Alexander G.] Univ Illinois, Dept Elect Engn, Champaign, IL 61820 USA; [Xiong, Jinjun] IBM Thomas J Watson Res Ctr, Yorktown Hts, NY USA	University of Illinois System; University of Illinois Urbana-Champaign; International Business Machines (IBM)	Yeh, RA (corresponding author), Univ Illinois, Dept Elect Engn, Champaign, IL 61820 USA.	yeh17@illinois.edu; jinjun@us.ibm.com; w-hwu@illinois.edu; minhdo@illinois.edu; aschwing@illinois.edu	Jeong, Yongwook/N-7413-2016; N., Minh/AAX-8498-2020	N., Minh/0000-0001-5132-4986	National Science Foundation [1718221]; NVIDIA Corporation; IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizons Network	National Science Foundation(National Science Foundation (NSF)); NVIDIA Corporation; IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizons Network	This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221. This work is supported by NVIDIA Corporation with the donation of a GPU. This work is supported in part by IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR) - a research collaboration as part of the IBM Cognitive Horizons Network.	[Anonymous], 2015, P ICLR; Arandjelovic R, 2012, P BMVC; Cao Z., 2017, P CVPR; Chen K., 2017, P ICCV; Deselaers T., 2012, IJCV; Donahue J., 2015, P CVPR; Endo K., 2017, P IJCAI; Everingham M., 2010, IJCV; Frome A., 2013, P NIPS; Gong Yunchao, 2014, P ECCV, P2; Guadarrama S., 2014, P RSS; Hochreiter S, 1997, NEURAL COMPUTATION; Hoi S. C., 2006, P CVPR; Hu R., 2016, P CVPR; Hu R., 2017, P CVPR; Joachims T., 2009, MACHINE LEARNING; Johnson J., 2015, P CVPR; Karpathy A., 2014, P NIPS; Karpathy A., 2015, P CVPR; Kazemzadeh S., 2014, P EMNLP; Kiros Ryan, 2015, ARXIV14112539; Klein B, 2015, P IEEE CVF C COMP VI, P4437; Kong C., 2014, P CVPR; Krishnamurthy J., 2013, P TACL; Lampert C. H., 2009, PAMI; Lehmann A., 2011, IJCV; Lin D., 2014, P CVPR; Lin T., 2014, P ECCV; Mao J, 2015, P ICLR; Mao J., 2016, P CVPR; Matuszek C., 2012, P ICML; Nagaraja V. K., 2016, P ECCV; Oneata D., 2014, P ECCV; Plummer B. A., 2017, IJCV; Plummer B. A., 2015, P ICCV; Plummer B. A., 2017, P ICCV; Redmon Joseph, 2017, CVPR; Rohrbach A., 2016, P ECCV; Sadeghi F., 2015, P CVPR; Schwing A. G., 2012, P ECCV; Sun Q., 2015, P NIPS; Wang Liwei, 2016, P CVPR; Yan J., 2014, P CVPR; Young P., 2014, P TACL; Yu H., 2013, P ACL	45	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401091
C	Yu, HF; Hsieh, CJ; Lei, Q; Dhillon, IS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yu, Hsiang-Fu; Hsieh, Cho-Jui; Lei, Qi; Dhillon, Inderjit S.			A Greedy Approach for Budgeted Maximum Inner Product Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Maximum Inner Product Search (MIPS) is an important task in many machine learning applications such as the prediction phase of low-rank matrix factorization models and deep learning models. Recently, there has been substantial research on how to perform MIPS in sub-linear time, but most of the existing work does not have the flexibility to control the trade-off between search efficiency and search quality. In this paper, we study the important problem of MIPS with a computational budget. By carefully studying the problem structure of MIPS, we develop a novel Greedy-MIPS algorithm, which can handle budgeted MIPS by design. While simple and intuitive, Greedy-MIPS yields surprisingly superior performance compared to state-of-the-art approaches. As a specific example, on a candidate set containing half a million vectors of dimension 200, Greedy-MIPS runs 200x faster than the naive approach while yielding search results with the top-5 precision greater than 75%.	[Yu, Hsiang-Fu] Amazon Inc, Seattle, WA 98109 USA; [Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA; [Yu, Hsiang-Fu; Lei, Qi; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA	Amazon.com; University of California System; University of California Davis; University of Texas System; University of Texas Austin	Yu, HF (corresponding author), Amazon Inc, Seattle, WA 98109 USA.	rofuyu@cs.utexas.edu; chohsieh@ucdavis.edu; leiqi@ices.utexas.edu; inderjit@cs.utexas.edu	Jeong, Yongwook/N-7413-2016; Lei, Qi/T-2146-2019		NSF [CCF-1320746, IIS-1546452, CCF-1564000, RI-1719097]	NSF(National Science Foundation (NSF))	This research was supported by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000. CJH was supported by NSF grant RI-1719097.	Adams J., 2016, P 10 ACM C REC SYST; Auvolat Alex, 2016, ARXIV150705910; Bachrach Y, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P257, DOI 10.1145/2645710.2645741; Ballard Grey, 2015, P IEEE INT C DAT MIN; Chin W.-S., 2015, P PAC AS C KNOWL DIS; Dror Gideon, 2011, P 2011 INT C KDD CUP, P3; Knuth Donald E., 1998, ART CMOPUTER PROGRAM, V3; Koenigstein Noam, 2012, P CIKM 12, P535, DOI DOI 10.1145/2396761.2396831; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Mussmann Stephen, 2016, P 33 INT C INT C MAC; Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926; Ram Parikshit, 2012, P 18 ACM SIGKDD INT, P931, DOI DOI 10.1145/2339530.2339677; Shrivastava A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P812; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3; Yu H.-F., 2014, INT C MACH LEARN, P593	17	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405052
C	Zhang, YZ; Shen, DH; Wang, GY; Gan, Z; Henao, R; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhang, Yizhe; Shen, Dinghan; Wang, Guoyin; Gan, Zhe; Henao, Ricardo; Carin, Lawrence			Deconvolutional Paragraph Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Learning latent representations from long text sequences is an important first step in many natural language processing applications. Recurrent Neural Networks (RNNs) have become a cornerstone for this challenging task. However, the quality of sentences during RNN-based decoding (reconstruction) decreases with the length of the text. We propose a sequence-to-sequence, purely convolutional and deconvolutional autoencoding framework that is free of the above issue, while also being computationally efficient. The proposed method is simple, easy to implement and can be leveraged as a building block for many applications. We show empirically that compared to RNNs, our framework is better at reconstructing and correcting long paragraphs. Quantitative evaluation on semi-supervised text classification and summarization tasks demonstrate the potential for better utilization of long unlabeled text data.	[Zhang, Yizhe; Shen, Dinghan; Wang, Guoyin; Gan, Zhe; Henao, Ricardo; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA	Duke University	Zhang, YZ (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.		Jeong, Yongwook/N-7413-2016	Carin, Lawrence/0000-0001-6277-7948; Henao, Ricardo/0000-0003-4980-845X	ARO; DARPA; DOE; NGA; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research)	This research was supported in part by ARO, DARPA, DOE, NGA and ONR.	[Anonymous], 2017, ARXIV170906548; [Anonymous], 2014, CUDNN EFFICIENT PRIM; [Anonymous], 2015, NIPS; [Anonymous], 2016, NEURAL MACHINE TRANS; Bahdanau Dzmitry, 2016, ARXIV160707086; Bahdanau Dzmitry, 2015, ICLR; Bengio Samy, 2015, NIPS; Bowman S R, 2015, GENERATING SENTENCES; Chiswell Ian, 2007, MATH LOGIC, V3; Cho Kyunghyun, 2014, EMNLP; Chong Wang, 2016, ICLR; Collobert R, 2011, JMLR; Dauphin Yann N, 2016, LANGUAGE MODELING GA; Gan Zhe, 2017, EMNLP; Gehring Jonas, 2017, CONVOLUTIONAL SEQUEN; Gulehre C, 2014, NETWORKS SEQUENCE MO; Gulrajani I., 2016, PIXELVAE LATENT VARI; Gumbel E. J., 1954, STAT THEORY EXTREME; Hochreiter S., 2001, FIELD GUIDE DYNAMICA, DOI DOI 10.1109/9780470544037.CH14; Hochreiter S, 1997, NEURAL COMPUTATION; Hu B., 2014, NIPS; Huszar F., 2015, NOT TRAIN YOUR GENER; Johnson Rie, 2015, NAACL HLT; Johnson Rie, 2016, SUPERVISED SEMISUPER; Kalchbrenner N., 2014, ACL; Kim Y., 2014, ARXIV14085882; Kingma D. P., 2014, NIPS; Lapata, 2017, NEURAL EXTRACTIVE SU; Le Q., 2014, ICML; Li J., 2015, ACL; Li J, 2017, P 2017 C EMP METH NA, P2157; Li Jiwei, 2016, DEEP REINFORCEMENT L; Lin C, 2004, ACL WORKSH; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Meng Fandong, 2015, ACL; Mikolov T., 2013, ADV NEURAL INF PROCE, V26; Mikolov T., 2010, INTERSPEECH; Miyato Takeru, 2017, ICLR; Nair V., 2010, ICML, P807; Nallapati R., 2016, CONLL; Papineni K., 2002, P ANN M ASS COMP LIN; Pu Y., 2016, NIPS; Pu YC, 2016, JMLR WORKSH CONF PRO, V51, P741; Pu Yunchen, 2015, ARXIV150404054; Radford A., 2015, CVPR; Rush Alexander M., 2015, EMNLP; Semeniuta S., 2017, HYBRID CONVOLUTIONAL; Simonyan Karen, 2015, INT C LEARN REPR; Socher R., 2011, EMNLP; Springenberg J. T., 2014, STRIVING SIMPLICITY, DOI 10.1163/_q3_SIM_00374; Sutskever I., 2014, NEURIPS; Wen T.H., 2015, P 2015 C EMPIRICAL M; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Wong Kam- Fai, 2008, ICCL; Woodard JP, 1982, WORKSH STAND SPEECH; Yang Zichao, 2016, NAACL; Yang Zichao, 2017, IMPROVED VARIATIONAL; Zhang X., 2015, ADV NEURAL INFORM PR, P649; Zhang Y., 2017, ICML	59	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404024
C	Chen, W; Hu, W; Li, F; Li, J; Liu, Y; Lu, PY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Wei; Hu, Wei; Li, Fu; Li, Jian; Liu, Yu; Lu, Pinyan			Combinatorial Multi-Armed Bandit with General Reward Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MAXIMIZING EXPECTED UTILITY; OPTIMIZATION	In this paper, we study the stochastic combinatorial multi-armed bandit (CMAB) framework that allows a general nonlinear reward function, whose expected value may not depend only on the means of the input random variables but possibly on the entire distributions of these variables. Our framework enables a much larger class of reward functions such as the max() function and nonlinear utility functions. Existing techniques relying on accurate estimations of the means of random variables, such as the upper confidence bound (UCB) technique, do not work directly on these functions. We propose a new algorithm called stochastically dominant confidence bound (SDCB), which estimates the distributions of underlying random variables and their stochastically dominant confidence bounds. We prove that SDCB can achieve O (log T) distribution-dependent regret and (O) over tilde (root T) distribution-independent regret, where T is the time horizon. We apply our results to the K-MAX problem and expected utility maximization problems. In particular, for K-MAX, we provide the first polynomial-time approximation scheme (PTAS) for its offline problem, and give the first (O) over tilde (root T) bound on the (1-epsilon)-approximation regret of its online problem, for any epsilon > 0.	[Chen, Wei] Microsoft Res, Redmond, WA 98052 USA; [Hu, Wei] Princeton Univ, Princeton, NJ 08544 USA; [Li, Fu] Univ Texas Austin, Austin, TX 78712 USA; [Li, Jian; Liu, Yu] Tsinghua Univ, Beijing, Peoples R China; [Lu, Pinyan] Shanghai Univ Finance & Econ, Shanghai, Peoples R China	Microsoft; Princeton University; University of Texas System; University of Texas Austin; Tsinghua University; Shanghai University of Finance & Economics	Chen, W (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	weic@microsoft.com; huwei@cs.princeton.edu; fuli.theory.research@gmail.com; lapordge@gmail.com; liuyujyyz@gmail.com; lu.pinyan@mail.shufe.edu.cn			National Natural Science Foundation of China [61433014]; National Basic Research Program of China [2015CB358700, 2011CBA00300, 2011CBA00301]; National NSFC [61033001, 61361136003]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China(National Basic Research Program of China); National NSFC(National Natural Science Foundation of China (NSFC))	Wei Chen was supported in part by the National Natural Science Foundation of China (Grant No. 61433014). Jian Li and Yu Liu were supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61033001, 61361136003. The authors would like to thank Tor Lattimore for referring to us the DKW inequality.	Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bhalgat Anand, 2014, Integer Programming and Combinatorial Optimization. 17th International Conference, IPCO 2014. Proceedings: LNCS 8494, P126, DOI 10.1007/978-3-319-07557-0_11; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen S., 2014, NIPS; Chen W., 2016, J MACHINE LEARNING R, V17, P1; Combes R., 2015, NIPS; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Fishburn P.C., 1982, FDN EXPECTED UTILITY; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Goel A., 2006, P 25 ACM SIGMOD SIGA, P203; Goel A, 2010, ACM T ALGORITHMS, V7, DOI 10.1145/1868237.1868250; Gopalan A, 2014, PR MACH LEARN RES, V32; Kveton B, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P420; Kveton B, 2015, JMLR WORKSH CONF PRO, V38, P535; Kveton  Branislav, 2015, NIPS; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Li J, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P971; Li J, 2011, ANN IEEE SYMP FOUND, P797, DOI 10.1109/FOCS.2011.33; Lin  Tian, 2015, NIPS; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Streeter M., 2008, NIPS; Yu JJ, 2016, OPER RES LETT, V44, P180, DOI 10.1016/j.orl.2015.12.016	27	8	8	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704036
C	Gautier, A; Nguyen, Q; Hein, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gautier, A.; Nguyen, Q.; Hein, M.			Globally Optimal Training of Generalized Polynomial Neural Networks with Nonlinear Spectral Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The optimization problem behind neural networks is highly non-convex. Training with stochastic gradient descent and variants requires careful parameter tuning and provides no guarantee to achieve the global optimum. In contrast we show under quite weak assumptions on the data that a particular class of feedforward neural networks can be trained globally optimal with a linear convergence rate with our nonlinear spectral method. Up to our knowledge this is the first practically feasible method which achieves such a guarantee. While the method can in principle be applied to deep networks, we restrict ourselves for simplicity in this paper to one and two hidden layer networks. Our experiments confirm that these models are rich enough to achieve good performance on a series of real-world datasets.	[Gautier, A.; Nguyen, Q.; Hein, M.] Saarland Univ, Dept Math & Comp Sci, Saarland Informat Campus, Saarbrucken, Germany	Saarland University	Gautier, A (corresponding author), Saarland Univ, Dept Math & Comp Sci, Saarland Informat Campus, Saarbrucken, Germany.				ERC starting grant NOLEPRO [307793]	ERC starting grant NOLEPRO	The authors acknowledge support by the ERC starting grant NOLEPRO 307793.	Anthony M., 1999, NEURAL NETWORK LEARN, V9; Arora S, 2014, PR MACH LEARN RES, V32; Bermudez A. J., 1994, SAVMA Symposium 1994 Proceedings., P1; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Daniely A, 2016, ARXIV160205897V1; Gautier A., 2016, PERRON FROBENI UNPUB; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; Hardt M, 2016, PR MACH LEARN RES, V48; Horn R. A., 1986, MATRIX ANAL; Janzamin M., 2015, ARXIV150608473V3; Kirk W. A., 2001, INTRO METRIC SPACES; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lemmens B., 2012, NONLINEAR PERRON FRO; Livni R, 2014, ADV NEUR IN, V27; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Thompson A., 1963, P AM MATH SOC, V14, P438, DOI DOI 10.2307/2033816	18	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701095
C	Kaiser, L; Bengio, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kaiser, Lukasz; Bengio, Samy			Can Active Memory Replace Attention?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice.	[Kaiser, Lukasz; Bengio, Samy] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Kaiser, L (corresponding author), Google Brain, Mountain View, CA 94043 USA.	lukaszkaiser@google.com; bengio@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Bengio Y., 2014, ARXIV14061078; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Graves A., 2014, ARXIV14105401; Gregor K., 2015, CORR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Joulin A, 2015, ADV NEUR IN, V28; Kaiser Lukasz, 2016, ICLR; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Kalchbrenner Nal, 2016, ICLR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavin A., 2015, CORR; Liao Q., 2016, CORR; Meng FD, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P20; Rezende DJ, 2016, ADV NEUR IN, V29; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Toderici G., 2016, 4 INT C LEARN REPR I, P2; Tu Z., 2016, CORR; Vinyals, 2015, ADV NEURAL INFORM PR; Wierstra D., 2016, ABS161107507 CORR; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xu K, 2015, PR MACH LEARN RES, V37, P2048	27	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702042
C	Steinhardt, J; Valiant, G; Charikar, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Steinhardt, Jacob; Valiant, Gregory; Charikar, Moses			Avoiding Imposters and Delinquents: Adversarial Crowdsourcing and Peer Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider a crowdsourcing model in which n workers are asked to rate the quality of ro, items previously generated by other workers. An unknown set of an workers generate reliable ratings, while the remaining workers may behave arbitrarily and possibly adversarially. The manager of the experiment can also manually evaluate the quality of a small number of items, and wishes to curate together almost all of the high-quality items with at most an 6 fraction of low-quality items. Perhaps surprisingly, we show that this is possible with an amount of work required of the manager, and each worker, that does not scale with n: the dataset can be curated with O (1/beta alpha(3)epsilon(4)) ratings per worker, and O (1/beta epsilon(2)) ratings by the manager, where beta is the fraction of high-quality items. Our results extend to the more general setting of peer prediction, including peer grading in online classrooms.	[Steinhardt, Jacob; Valiant, Gregory; Charikar, Moses] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Steinhardt, J (corresponding author), Stanford Univ, Stanford, CA 94305 USA.				Fannie & John Hertz Foundation Fellowship; NSF Graduate Research Fellowship; Future of Life Institute grant; NSF CAREER award [CCF-1351108]; Sloan Foundation Research Fellowship; Okawa Foundation; NSF [CCF-1565581, CCF-1617577, CCF-1302518]; Simons Investigator Award	Fannie & John Hertz Foundation Fellowship; NSF Graduate Research Fellowship(National Science Foundation (NSF)); Future of Life Institute grant; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Foundation Research Fellowship(Alfred P. Sloan Foundation); Okawa Foundation; NSF(National Science Foundation (NSF)); Simons Investigator Award	JS was supported by a Fannie & John Hertz Foundation Fellowship, an NSF Graduate Research Fellowship, and a Future of Life Institute grant. GV was supported by NSF CAREER award CCF-1351108, a Sloan Foundation Research Fellowship, and a research grant from the Okawa Foundation. MC was supported by NSF grants CCF-1565581, CCF-1617577, CCF-1302518 and a Simons Investigator Award.	Abbe E., 2015, COMMUNITY DETECTION; Abbe E., 2015, DETECTION STOCHASTIC; Agarwal N., 2015, MULTISECTION STOCHAS; Banks J., 2016, INFORM THEORETIC THR; Cai TT, 2015, ANN STAT, V43, P1027, DOI 10.1214/14-AOS1290; Chen Y, 2014, INT GEOSCI REMOTE SE, DOI 10.1109/IGARSS.2014.6947442; Chin P., 2015, C LEARN THEOR COLT; Christiano P., 2016, ROBUST COLLABORATIVE; Christiano P., 2014, PROVABLY MANIPULATIO; Coja-Oghlan A., 2004, AUTOMATA LANGUAGES P; Coja-Oghlan A, 2007, J ALGORITHMS, V62, P19, DOI 10.1016/j.jalgor.2004.07.003; Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2; Dasgupta A., 2013, WWW; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Feige U, 2001, J COMPUT SYST SCI, V63, P639, DOI 10.1006/jcss.2001.1773; Feige U, 2000, RANDOM STRUCT ALGOR, V16, P195, DOI 10.1002/(SICI)1098-2418(200003)16:2<195::AID-RSA5>3.0.CO;2-A; Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599; Green JBD, 2014, J ANAL METHODS CHEM, V2014, DOI 10.1155/2014/810589; Guedon O., 2014, COMMUNITY DETECTION; Harmon A., 2004, NY TIMES; Holland P. W., 1983, SOCIAL NETWORKS; Kamble V., 2015, TRUTH SERUMS MASSIVE; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Krivelevich M, 2006, SIAM PROC S, P211; Le C. M., 2015, CONCENTRATION REGULA; Makarychev K., 2015, LEARNING COMMUNITIES; Makarychev K, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P367; Massoulie L., 2014, STOC; Mayzlin D., 2012, TECHNICAL REPORT; Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/mnsc.1050.0379; Moitra A., 2015, ROBUST ARE RECONSTRU; Mossel E., 2013, PROOF BLOCK MODEL TH; Mossel E., 2015, STOC; Mossel E., 2013, BELIEF PROPAGATION R; MOSSEL E, 2012, STOCHASTIC BLOCK MOD; Priedhorsky R, 2007, GROUP'07: PROCEEDINGS OF THE 2007 INTERNATIONAL ACM CONFERENCE ON SUPPORTING GROUP WORK, P259; Resnick P, 2007, RECSYS 07: PROCEEDINGS OF THE 2007 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P25; Shah N. B., 2015, ADV NEURAL INFORM PR; Shah NB, 2015, PR MACH LEARN RES, V37, P10; Shnayder V., 2016, STRONG TRUTHFULNESS; Vuurens J., 2011, ACM SIGIR WORKSH CRO; Zhou D., 2015, REGULARIZED MINIMAX	45	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703076
C	Turchetta, M; Berkenkamp, F; Krause, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Turchetta, Matteo; Berkenkamp, Felix; Krause, Andreas			Safe Exploration in Finite Markov Decision Processes with Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REINFORCEMENT; ROBOTICS; STATE	In classical reinforcement learning agents accept arbitrary short term loss for long term gain when exploring their environment. This is infeasible for safety critical applications such as robotics, where even a single unsafe action may cause system failure or harm the environment. In this paper, we address the problem of safely exploring finite Markov decision processes (MDP). We define safety in terms of an a priori unknown safety constraint that depends on states and actions and satisfies certain regularity conditions expressed via a Gaussian process prior. We develop a novel algorithm, SAFEMDP, for this task and prove that it completely explores the safely reachable part of the MDP without violating the safety constraint. To achieve this, it cautiously explores safe states and actions in order to gain statistical confidence about the safety of unvisited state-action pairs from noisy observations collected while navigating the environment. Moreover, the algorithm explicitly considers reachability when exploring the MDP, ensuring that it does not get stuck in any state with no safe way out. We demonstrate our method on digital terrain models for the task of exploring an unknown map with a rover.	[Turchetta, Matteo; Berkenkamp, Felix; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Turchetta, M (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	matteotu@ethz.ch; befelix@ethz.ch; krausea@ethz.ch			Max Planck ETH Center for Learning Systems; SNSF grant [200020_159557]	Max Planck ETH Center for Learning Systems; SNSF grant	This research was partially supported by the Max Planck ETH Center for Learning Systems and SNSF grant 200020_159557.	Akametalu AK, 2014, IEEE DECIS CONTR P, P1424, DOI 10.1109/CDC.2014.7039601; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Berkenkamp F., 2015, P EUR CONTR C ECC, P2501; Berkenkamp Felix, 2016, P IEEE INT C ROB AUT; Geibel P, 2005, J ARTIF INTELL RES, V24, P81, DOI 10.1613/jair.1666; Ghosal S, 2006, ANN STAT, V34, P2413, DOI 10.1214/009053606000000795; Hans A., 2008, ESANN, P143; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Lockwood MK, 2006, J SPACECRAFT ROCKETS, V43, P257, DOI 10.2514/1.20678; MCEWEN AS, 2007, J GEOPHYS RES-PLANET, V112, DOI DOI 10.1029/2005JE002605; Mockus Jonas, 1989, MATH ITS APPL, V37; Moldovan T.M., 2012, P 29 INT C MACH LEAR, P1451, DOI DOI 10.5555/3042573.3042759; MSL, 2007, MSL LAND SIT SEL US; Pecka M, 2014, LECT NOTES COMPUT SC, V8906, P357; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schaal S, 2010, IEEE ROBOT AUTOM MAG, V17, P20, DOI 10.1109/MRA.2010.936957; Scholkopf B., 2001, LEARNING KERNELS SUP; Schreiter J, 2015, LECT NOTES ARTIF INT, V9286, P133, DOI 10.1007/978-3-319-23461-8_9; Srinivas N., 2010, P INT C MACH LEARN I; Sui YA, 2015, PR MACH LEARN RES, V37, P997; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	23	8	8	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702105
C	Bachman, P; Alsharif, O; Precup, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bachman, Philip; Alsharif, Ouais; Precup, Doina			Learning with Pseudo-Ensembles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We formalize the notion of a pseudo-ensemble, a (possibly infinite) collection of child models spawned from a parent model by perturbing it according to some noise process. E.g., dropout [9] in a deep neural network trains a pseudo-ensemble of child subnetworks generated by randomly masking nodes in the parent network. We examine the relationship of pseudo-ensembles, which involve perturbation in model-space, to standard ensemble methods and existing notions of robustness, which focus on perturbation in observation-space. We present a novel regularizer based on making the behavior of a pseudo-ensemble robust with respect to the noise process generating it. In the fully-supervised setting, our regularizer matches the performance of dropout. But, unlike dropout, our regularizer naturally extends to the semi-supervised setting, where it produces state-of-the-art results. We provide a case study in which we transform the Recursive Neural Tensor Network of [19] into a pseudo-ensemble, which significantly improves its performance on a real-world sentiment analysis benchmark.	[Bachman, Philip; Alsharif, Ouais; Precup, Doina] McGill Univ, Montreal, PQ, Canada	McGill University	Bachman, P (corresponding author), McGill Univ, Montreal, PQ, Canada.	phil.bachman@gmail.com; ouais.alsharif@gmail.com; dprecup@cs.mcgill.ca						[Anonymous], 2008, ICML; [Anonymous], 2013, EMNLP; Baldi P., 2013, NIPS; Bengio Y., 2014, ARXIV13061091V5CSLG; Bergstra J., 2010, PYTH SCI COMP C SCIP; Bertsimas D, 2011, SIAM REV, V53, P464, DOI 10.1137/080734510; Goodfellow I., 2012, ICML; Grandvalet Y., 2006, SEMISUPERVISED LEARN; Hastie T., 2008, ELEMENTS STAT LEARNI; Hinton G. E., 2012, ARXIV12070580V1CSNE; Kalchbrenner N., 2014, ACL; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Le Q., 2014, ICML; Le Q. V., 2011, NIPS; Lee Dong-Hyun, 2013, ICML 2013 WORKSH CHA; Pacanu R., 2013, ICML; Rifai S., 2011, ADV NEURAL INF PROCE, V24; RIPPEL O, 2014, ICML; Van der Maaten L., 2013, ICML; Vincent Pascal., 2008, ICML; Wager S., 2013, NIPS; Warde-Farley D, 2014, ICLR; Xu H., 2009, JMLR, V10; Xu  Huan, 2009, NIPS	25	8	8	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102048
C	Gens, R; Domingos, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gens, Robert; Domingos, Pedro			Deep Symmetry Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The chief difficulty in object recognition is that objects' classes are obscured by a large number of extraneous sources of variability, such as pose and part deformation. These sources of variation can be represented by symmetry groups, sets of composable transformations that preserve object identity. Convolutional neural networks (convnets) achieve a degree of translational invariance by computing feature maps over the translation group, but cannot handle other groups. As a result, these groups' effects have to be approximated by small translations, which often requires augmenting datasets and leads to high sample complexity. In this paper, we introduce deep symmetry networks (symnets), a generalization of convnets that forms feature maps over arbitrary symmetry groups. Symnets use kernel-based interpolation to tractably tie parameters and pool over symmetry spaces of any dimension. Like convnets, they are trained with backpropagation. The composition of feature transformations through the layers of a symnet provides a new approach to deep learning. Experiments on NORB and MNIST-rot show that symnets over the affine group greatly reduce sample complexity relative to convnets by better capturing the symmetries in the data.	[Gens, Robert; Domingos, Pedro] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Gens, R (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	rcg@cs.washington.edu; pedrod@cs.washington.edu			ARO [W911NF-08-1-0242]; ONR [N00014-13-1-0720, N00014-12-1-0312]; AFRL [FA8750-13-2-0019]	ARO; ONR(Office of Naval Research); AFRL(United States Department of DefenseUS Air Force Research Laboratory)	This research was partly funded by ARO grant W911NF-08-1-0242, ONR grants N00014-13-1-0720 and N00014-12-1-0312, and AFRL contract FA8750-13-2-0019. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of ARO, ONR, AFRL, or the United States Government.	ABUMOSTAFA YS, 1993, NEURAL COMPUT, V5, P278, DOI 10.1162/neco.1993.5.2.278; [Anonymous], 2012, ADV NEUR IN; Anselmi F., 2013, 13114158 ARXIV; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bergstra J., 2010, P PYTH SCI COMP C; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Ciresan D., 2012, P IEEE C COMP VIS PA; Diaconis P., 1998, LECT NOTES MONOGRAPH, V11; Drost B., 2010, P IEEE C COMP VIS PA; Felzenszwalb P., 2008, P IEEE C COMP VIS PA; Hinton G. E., 2011, P 21 INT C ART NEUR; Kondor IR, 2008, GROUP THEORETICAL ME; Kulesza A., 2012, ARXIV PREPRINT 1207; Larochelle H., 2007, ICML, V227; LeCun Y., 2004, P IEEE C COMP VIS PA; Lee T, 2011, IMAGE VISION COMPUT, V29, P639, DOI 10.1016/j.imavis.2011.08.003; Lowe D. G., 1999, P IEEE C COMP VIS PA; Lu F, 1997, J INTELL ROBOT SYST, V18, P249, DOI 10.1023/A:1007957421070; Miller W., 1972, SYMMETRY GROUPS THEI; Niepert M., 2012, P 28 C UNC ART INT; Simard P., 1992, ADV NEURAL INFORM PR, V5; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938	24	8	8	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101095
C	Henao, R; Yuan, X; Carin, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Henao, Ricardo; Yuan, Xin; Carin, Lawrence			Bayesian Nonlinear Support Vector Machines and Discriminative Factor Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLASSIFICATION	A new Bayesian formulation is developed for nonlinear support vector machines (SVMs), based on a Gaussian process and with the SVM hinge loss expressed as a scaled mixture of normals. We then integrate the Bayesian SVM into a factor model, in which feature learning and nonlinear classifier design are performed jointly; almost all previous work on such discriminative feature learning has assumed a linear classifier. Inference is performed with expectation conditional maximization (ECM) and Markov Chain Monte Carlo (MCMC). An extensive set of experiments demonstrate the utility of using a nonlinear Bayesian SVM within discriminative feature learning and factor modeling, from the standpoints of accuracy and interpretability.	[Henao, Ricardo; Yuan, Xin; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Duke University	Henao, R (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	r.henao@duke.edu; xin.yuan@duke.edu; lcarin@duke.edu		Carin, Lawrence/0000-0001-6277-7948; Henao, Ricardo/0000-0003-4980-845X; Yuan, Xin/0000-0002-8311-7524	ARO; DARPA; DOE; NGA; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research)	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR.	ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; Carvalho CM, 2008, J AM STAT ASSOC, V103, P1438, DOI 10.1198/016214508000000869; Chang JT, 2006, BIOINFORMATICS, V22, P2926, DOI 10.1093/bioinformatics/btl483; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Kozubowski TJ., 1999, ACTUARIAL RES CLEARI, V1, P113; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Lawrence N. D., 2003, NIPS; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Mairal J., 2009, ADV NEURAL INFORM PR, P1033; Miller LD, 2005, P NATL ACAD SCI USA, V102, P13550, DOI 10.1073/pnas.0506230102; Minka T., 2001, THESIS MIT CAMBRIDGE; Murray I., 2010, JMLR W CP, V9, P541; Murray I., 2010, ADV NEURAL INF PROCE, P1723; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; POLSON N. G., 2010, BAYESIAN STAT, V9, P105, DOI DOI 10.1093/ACPR0F:0S0/9780199694587.003.0017.773; Polson NG, 2011, BAYESIAN ANAL, V6, P1, DOI 10.1214/11-BA601; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Xu M., 2012, ADV NEURAL INFORM PR, P64; Xu M, 2013, P 30 INT C MACH LEAR, P978; Zhou M, 2009, ADV NEURAL INFORM PR, P2295; Zhu J, 2009, P 26 ANN INT C MACH, P1257	26	8	8	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102068
C	Li, M; Andersen, DG; Smola, A; Yu, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Li, Mu; Andersen, David G.; Smola, Alexander; Yu, Kai			Communication Efficient Distributed Machine Learning with the Parameter Server	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper describes a third-generation parameter server framework for distributed machine learning. This framework offers two relaxations to balance system performance and algorithm efficiency. We propose a new algorithm that takes advantage of this framework to solve non-convex non-smooth problems with convergence guarantees. We present an in-depth analysis of two large scale machine learning problems ranging from l(1)-regularized logistic regression on CPUs to reconstruction ICA on GPUs, using 636TB of real data with hundreds of billions of samples and dimensions. We demonstrate using these examples that the parameter server framework is an effective and straightforward way to scale machine learning to larger problems and systems than have been previously achieved.	[Li, Mu; Andersen, David G.; Smola, Alexander] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Li, Mu; Yu, Kai] Baidu, Beijing, Peoples R China; [Smola, Alexander] Google, Mountain View, CA USA	Carnegie Mellon University; Baidu; Google Incorporated	Li, M (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	muli@cs.cmu.edu; dga@cs.cmu.edu; alex@smola.org; yukai@baidu.com						Agarwal A., 2012, IEEE CDC; Ahmed A., 2012, WSDM; Ahmed A., 2013, WWW; Barroso Luiz Andre, 2009, DATACENTER COMPUTER, V1st; Bradley J., 2011, ICML; Byers J, 2003, LECT NOTES COMPUT SC, V2735, P80; Canini K., 2012, SIBYL SYSTEM LARGE S; Dean J., 2012, ADV NEURAL INFORM PR, V25; Dean J., 2008, CACM; Domo, 2014, DAT NEV SLEEPS 2 0; Gunderson S., SNAPPY; Ho Q., 2013, NIPS; JOACHIMS T, 1999, ADV KERNEL METHODS; Langford J., 2009, NIPS; Le Q. V., 2011, NIPS; Li M., 2013, BIG LEARN NIPS WORKS; Li Mu, 2014, P 11 USENIX C OP SYS, P583; Li Mu, 2013, NIPS WORKSH OPT MACH; Low Y., 2012, PVLDB; Matsushima S., 2012, KDD; Niu F., 2011, NIPS 11; Parikh N., 2013, FDN TRENDS OPTIMIZAT; Petersen K. B., 2012, MATRIX COOKBOOK; Phanishayee A., 2012, MANAGEMENT BIG DATA; Richtarik P., 2012, MATH PROGRAMMING; Rowstron A., 2001, DISTRIBUTED SYSTEMS; Smola A. J., 2010, VLDB; Sparks Evan R., 2013, MLI API DISTRIBUTED; Sra S., 2012, NIPS; Stoica I., 2001, SIGCOMM COMPUTER COM; Teflioudi Christina, 2012, ICDM; Teo C. H., 2010, JMLR; The Apache Software Foundation, 2009, APACHE HADOOP; van Renesse R., 2004, OSDI; Yuan Guo-Xun, 2010, JMLR; Zaharia M., 2012, USENIX LOGIN; Zinkevich M., 2010, NIPS, V4, P4	37	8	8	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103048
C	Soufiani, HA; Parkes, DC; Xia, LR		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Soufiani, Hossein Azari; Parkes, David C.; Xia, Lirong			A Statistical Decision-Theoretic Framework for Social Choice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we take a statistical decision-theoretic viewpoint on social choice, putting a focus on the decision to be made on behalf of a system of agents. In our framework, we are given a statistical ranking model, a decision space, and a loss function defined on (parameter, decision) pairs, and formulate social choice mechanisms as decision rules that minimize expected loss. This suggests a general framework for the design and analysis of new social choice mechanisms. We compare Bayesian estimators, which minimize Bayesian expected loss, for the Mallows model and the Condorcet model respectively, and the Kemeny rule. We consider various normative properties, in addition to computational complexity and asymptotic behavior. In particular, we show that the Bayesian estimator for the Condorcet model satisfies some desired properties such as anonymity, neutrality, and monotonicity, can be computed in polynomial time, and is asymptotically different from the other two rules when the data are generated from the Condorcet model for some ground truth parameter.	[Soufiani, Hossein Azari] Google Res, New York, NY 10011 USA; [Parkes, David C.] Harvard Univ, Cambridge, MA 02138 USA; [Xia, Lirong] Rensselaer Polytech Inst, Troy, NY 12180 USA	Google Incorporated; Harvard University; Rensselaer Polytechnic Institute	Soufiani, HA (corresponding author), Google Res, New York, NY 10011 USA.	azari@google.com; parkes@eecs.harvard.edu; xial@cs.rpi.edu		Xia, Lirong/0000-0002-9800-6691	Siebel foundation; NSF grant CCF [1301976]; SEAS TomKat fund; RPI startup fund	Siebel foundation; NSF grant CCF; SEAS TomKat fund; RPI startup fund	We thank Shivani Agarwal, Craig Boutilier, Yiling Chen, Vincent Conitzer, Edith Elkind, Ariel Procaccia, and anonymous reviewers of AAAI-14 and NIPS-14 for helpful suggestions and discussions. Azari Soufiani acknowledges Siebel foundation for the scholarship in his last year of PhD studies. Parkes was supported in part by NSF grant CCF #1301976 and the SEAS TomKat fund. Xia acknowledges an RPI startup fund for support.	AustenSmith D, 1996, AM POLIT SCI REV, V90, P34, DOI 10.2307/2082796; Azari H, 2012, NIPS 12, P126; Berger J. O., 1985, STAT DECISION THEORY; Boutilier Craig, 2011, IJCAI 11 WORKSH SOC; Boutilier Craig, 2012, P 13 ACM C EL COMM, P197; Caragiannis I, 2011, ARTIF INTELL, V175, P1655, DOI 10.1016/j.artint.2011.03.005; Caragiannis Ioannis, 2013, P EC; Caragiannis Ioannis, 2014, P AAAI; Condorcet N, 1785, ESSAI APPL ANAL PROB; Conitzer V, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P109; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; Elkind Edith, 2014, P UAI; FISHBURN PC, 1977, SIAM J APPL MATH, V33, P469, DOI 10.1137/0133030; Ghosh S., 1999, Proceedings of the Third International Conference on Autonomous Agents, P434, DOI 10.1145/301136.301303; Hemaspaandra E, 2005, THEOR COMPUT SCI, V349, P382, DOI 10.1016/j.tcs.2005.08.031; KEMENY JG, 1959, DAEDALUS, V88, P577; Kuo Jen-Wei, 2009, P CIKM, P827; Long B, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P267; Lu T., 2011, ICML, P145; Lu Tyler, 2010, P 11 ACM C EL COMM A, P263; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Mao Andrew, 2013, P AAAI; McGarvey DC, 1953, ECONOMETRICA, V21, P608, DOI 10.2307/1907926; NITZAN S, 1984, THEOR DECIS, V17, P47, DOI 10.1007/BF00140055; Pivato M, 2013, SOC CHOICE WELFARE, V40, P581, DOI 10.1007/s00355-011-0619-1; Porello Daniele, 2013, J LOGIC COMPUTATION; Procaccia AD, 2006, LECT NOTES COMPUT SC, V4149, P317; Procaccia Ariel D., 2012, P UAI; Sandholm T., 2005, P UAI 05, P145; WALD ABRAHAM, 1950; Xia L, 2010, AAMAS 10, P399; Xia L, 2011, IJCAI 11, P446; Xia Lirong, 2014, DECIPHERING YOUNGS I; YOUNG HP, 1988, AM POLIT SCI REV, V82, P1231, DOI 10.2307/1961757; YOUNG HP, 1978, SIAM J APPL MATH, V35, P285, DOI 10.1137/0135023	35	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102098
C	Zhou, TY; Bilmes, J; Guestrin, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhou, Tianyi; Bilmes, Jeff; Guestrin, Carlos			Divide-and-Conquer Learning by Anchoring a Conical Hull	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MAXIMUM-LIKELIHOOD	We reduce a broad class of fundamental machine learning problems, usually addressed by EM or sampling, to the problem of finding the k extreme rays spanning the conical hull of al data point set. These k "anchors" lead to a global solution and a more interpretable model that can even outperform EM and sampling on generalization error. To find the k anchors, we propose a novel divide-and-conquer learning scheme "DCA" that distributes the problem to O(k log k) same-type sub-problems on different low-D random hyperplanes, each can be solved independently by any existing solver. For the 2D sub-problem, we instead present a non-iterative solver that only needs to compute an array of cosine values and its max/min entries. DCA also provides a faster subroutine inside other algorithms to check whether a point is covered in a conical hull, and thus improves these algorithms by providing significant speedups. We apply our method to GMM, HMM, LDA, NMF and subspace clustering, then show its competitive performance and scalability over other methods on large datasets.	[Zhou, Tianyi; Guestrin, Carlos] Univ Washington, Comp Sci & Engn, Seattle, WA 98195 USA; [Bilmes, Jeff] Univ Washington, Elect Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Zhou, TY (corresponding author), Univ Washington, Comp Sci & Engn, Seattle, WA 98195 USA.	tianyizh@u.washington.edu; bilmes@u.washington.edu; guestrin@u.washington.edu		Zhou, Tianyi/0000-0001-5348-0632	TerraSwarm research center - MARCO; TerraSwarm research center - DARPA; National Science Foundation [IIS-1162606]; Google; Microsoft; Intel research awards; Intel Science and Technology Center for Pervasive Computing	TerraSwarm research center - MARCO; TerraSwarm research center - DARPA; National Science Foundation(National Science Foundation (NSF)); Google(Google Incorporated); Microsoft(Microsoft); Intel research awards; Intel Science and Technology Center for Pervasive Computing	We would like to thank MELODI lab members for proof-reading and the anonymous reviewers for their helpful comments. This work is supported by TerraSwarm research center administered by the STARnet phase of the Focus Center Research Program (FCRP) sponsored by MARCO and DARPA, by the National Science Foundation under Grant No. (IIS-1162606), and by Google, Microsoft, and Intel research awards, and by the Intel Science and Technology Center for Pervasive Computing.	Anandkumar A., 2012, C LEARN THEOR, P33; Anandkumar Anima, 2012, NIPS; Arora S., 2012, STOC; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Belkin M., 2010, FOCS; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2; Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Donoho D., 2003, NIPS; Elhamifar E., 2009, CVPR; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Hsu D. J., 2009, COLT; Hughes M. C., 2012, NIPS; Kalai A. T., 2010, STOC; Kannan R., 2005, COLT; Kumar A., 2013, ICML; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; Porteous I., 2008, P 14 ACM SIGKDD INT, P569; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Soltanolkotabi M., 2013, ARXIV13012603; Titterington DM, 1985, STAT ANAL FINITE MIX; Zhou T., 2014, EXTENDED VERSION ACC; Zhou TY, 2013, IEEE DATA MINING, P917, DOI 10.1109/ICDM.2013.29	29	8	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103025
C	Crammer, K; Dekel, O; Shalev-Shwartz, S; Singer, Y		Thrun, S; Saul, K; Scholkopf, B		Crammer, K; Dekel, O; Shalev-Shwartz, S; Singer, Y			Online passive-aggressive algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				RELATIVE LOSS BOUNDS	We present a unified view for online classification, regression, and uniclass problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss.	Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Crammer, K (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.							BAUSCHKE HH, 1996, SIAM REV; Censor Y., 1997, PARALLEL OPTIMIZATIO; Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; GENTILE C, NIPS 98; GENTILE C, 2001, J MACHINE LEARNING R, V2, P213; Helmbold DP, 1999, IEEE T NEURAL NETWOR, V10, P1291, DOI 10.1109/72.809075; HELMBOLD DP, COLT 95; HERBSTER M, COLT 01; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Kivinen J, 2001, MACH LEARN, V45, P301, DOI 10.1023/A:1017938623079; KIVINEN J, NIPS 02; KLASNER N, COLT 95; Li Y, 2002, MACH LEARN, V46, P361, DOI 10.1023/A:1012435301888; Vapnik V.N, 1998, STAT LEARNING THEORY; XING E, NIPS 03; ZINKEVICH M, ICML 03	17	8	8	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1229	1236						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500153
C	Hauskrecht, M; Kveton, B		Thrun, S; Saul, K; Scholkopf, B		Hauskrecht, M; Kveton, B			Linear program approximations for factored continuous-state Markov decision processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Approximate linear programming (ALP) has emerged recently as one of the most promising methods for solving complex factored MDPs with finite state spaces. In this work we show that ALP solutions are not limited only to MDPs with finite state spaces, but that they can also be applied successfully to factored continuous-state MDPs (CMDPs). We show how one can build an ALP-based approximation for such a model and contrast it to existing solution methods. We argue that this approach offers a robust alternative for solving high dimensional continuous-state space problems. The point is supported by experiments on three CMDP problems with 24-25 continuous state factors.	Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Hauskrecht, M (corresponding author), Univ Pittsburgh, Dept Comp Sci, Pittsburgh, PA 15260 USA.	milos@cs.pitt.edu; bkveton@cs.pitt.edu						Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; BERTSEKAS DP, 1995, NEURAL COMPUT, V7, P270, DOI 10.1162/neco.1995.7.2.270; CHOW CS, 1991, IEEE T AUTOMAT CONTR, V36, P898, DOI 10.1109/9.133184; Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x; DEFARIAS DP, 2001, UNPUB MATH OPERATION; DEFARIAS DP, 2003, OPERATIONS RE, V51, P6; GUESTRIN C, 2001, P 17 INT JOINT C ART, P673; Koller D, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1332; KVETON B, 2004, IN PRESS 14 INT C AU; Poupart P, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P292; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rust J, 1997, ECONOMETRICA, V65, P487, DOI 10.2307/2171751; SCHUURMANS D, 2002, ADV NEURAL INFORMATI, V14; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TRICK M, 1993, LINEAR PROGRAMMING A; VANROY B, 1998, THESIS MIT	17	8	8	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						895	902						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500112
C	Mizutani, E; Demmel, JW		Thrun, S; Saul, K; Scholkopf, B		Mizutani, E; Demmel, JW			Iterative scaled trust-region learning in Krylov subspaces via Pearlmutter's implicit sparse Hessian-vector multiply	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The online incremental gradient (or backpropagation) algorithm is widely considered to be the fastest method for solving large-scale neural-network (NN) learning problems. In contrast, we show that, an appropriately implemented iterative batch-mode (or block-mode) learning method can be much faster. For example, it is three times faster in the UCI letter classification problem (26 outputs, 16,000 data items, 6,066 parameters with a two-hidden-layer multilayer perceptron) and 353 times faster in a nonlinear regression problem arising in color recipe prediction (10 outputs, 1,000 data items, 2,210 parameters with a neuro-fuzzy modular network). The three principal innovative ingredients in our algorithm are the following: First, we use scaled trust-region regularization with inner-outer iteration to solve the associated "overdetermined" nonlinear least squares problem, where the inner iteration performs a truncated (or inexact) Newton method. Second, we employ Pearlmutter's implicit sparse Hessian matrix-vector multiply algorithm to construct the Krylov subspaces used to solve for the truncated Newton update. Third, we exploit sparsity (for preconditioning) in the matrices resulting from the NNs having many outputs.	Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan	National Tsing Hua University	Mizutani, E (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 300, Taiwan.	eiji@wayne.cs.nthu.edu.tw; demmel@cs.berkeley.edu						Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Conn A.R., 2000, TRUST REGION METHODS, DOI [10.1137/1.9780898719857, DOI 10.1137/1.9780898719857]; Demmel JW, 1997, APPL NUMERICAL LINEA, V56; Hastie T., 2002, ELEMENTS STAT LEARNI; Jacobs M, 2001, CHEM ENG NEWS, V79, P3, DOI 10.1021/cen-v079n026.p003; Mizutani E, 2003, NEURAL NETWORKS, V16, P745, DOI 10.1016/S0893-6080(03)00085-6; Mizutani E, 2002, IEEE IJCNN, P2399, DOI 10.1109/IJCNN.2002.1007517; MIZUTANI E, 2001, P INNS IEEE INT JOIN, V1, P347; MORE JJ, 1983, SIAM J SCI STAT COMP, V4, P553, DOI 10.1137/0904038; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; STEIHAUG T, 1983, SIAM J NUMER ANAL, V20, P626, DOI 10.1137/0720042; [No title captured]	13	8	8	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						209	216						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500027
C	Monteleoni, C; Jaakkola, T		Thrun, S; Saul, K; Scholkopf, B		Monteleoni, C; Jaakkola, T			Online learning of non-stationary sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				PREDICTION	We consider an online learning scenario in which the learner can make predictions on the basis of a fixed set of experts. We derive upper and lower relative loss bounds for a class of universal learning algorithms involving a switching dynamics over the choice of the experts. On the basis of the performance bounds we provide the optimal a priori discretization for learning the parameter that governs the switching dynamics. We demonstrate the new algorithm in the context of wireless networks.	MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Monteleoni, C (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 200 Technol Sq, Cambridge, MA 02139 USA.	cmontel@ai.mit.edu; tommi@ai.mit.edu		Monteleoni, Claire/0000-0002-9488-0517				[Anonymous], 2007, P80211REVMDD40 IEEE; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Blum A., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P450, DOI 10.1109/SFFCS.1999.814617; Foster DP, 1999, GAME ECON BEHAV, V29, P7, DOI 10.1006/game.1999.0740; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Haussler D, 1998, IEEE T INFORM THEORY, V44, P1906, DOI 10.1109/18.705569; HELMBOLD DP, 1996, INT C MACH LEARN, P243; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; KRASHINSKY R, 2002, MOB 2002 ATL GA SEPT; KRASHINSKY R, 2002, MOBICOM 2002; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; LITTLESTONE N, 1989, ANN IEEE SYMP FOUND, P256, DOI 10.1109/SFCS.1989.63487; Steinbach C., 2002, THESIS MIT; Vovk V, 1999, MACH LEARN, V35, P247, DOI 10.1023/A:1007595032382	14	8	8	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1093	1100						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500136
C	Pillow, JW; Paninski, L; Simoncelli, EP		Thrun, S; Saul, K; Scholkopf, B		Pillow, JW; Paninski, L; Simoncelli, EP			Maximum likelihood estimation of a Stochastic integrate-and-fire neural model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				WHITE-NOISE ANALYSIS; SPIKE; NEURONS; ADAPTATION; RESPONSES; TIME	Recent work has examined the estimation of models of stimulus-driven neural activity in which some linear filtering process is followed by a nonlinear, probabilistic spiking stage. We analyze the estimation of one such model for which this nonlinear step is implemented by a noisy, leaky, integrate-and-fire mechanism with a spike-dependent after-current. This model is a biophysically plausible alternative to models with Poisson (memory-less) spiking, and has been shown to effectively reproduce various spiking statistics of neurons in vivo. However, the problem of estimating the model from extracellular spike train data has not been examined in depth. We formulate the problem in terms of maximum likelihood estimation, and show that the computational problem of maximizing the likelihood is tractable. Our main contribution is an algorithm and a proof that this algorithm is guaranteed to find the global optimum with reasonable speed. We demonstrate the effectiveness of our estimator with numerical simulations.	NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10012 USA	Howard Hughes Medical Institute; New York University	Pillow, JW (corresponding author), NYU, Ctr Neural Sci, Howard Hughes Med Inst, New York, NY 10012 USA.	pillow@cns.nyu.edu; liam@cns.nyu.edu; eero@cns.nyu.edu		Pillow, Jonathan/0000-0002-3638-8831				Arcas BAY, 2003, NEURAL COMPUT, V15, P1789, DOI 10.1162/08997660360675044; Berry MJ, 1998, J NEUROSCI, V18, P2200; Brown EN, 2002, NEURAL COMPUT, V14, P325, DOI 10.1162/08997660252741149; Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306; Genz A, 1992, J COMPUTATIONAL GRAP, V1, P141, DOI [10.2307/1390838, DOI 10.2307/1390838]; Gerstner W., 2002, SPIKING NEURON MODEL; Karlin S., 1981, 2 COURSE STOCHASTIC; Keat J, 2001, NEURON, V30, P803, DOI 10.1016/S0896-6273(01)00322-1; Knight BW, 2000, NEURAL COMPUT, V12, P1045, DOI 10.1162/089976600300015493; Levin JE, 1996, NATURE, V380, P165, DOI 10.1038/380165a0; Paninski L, 2003, NETWORK-COMP NEURAL, V14, P437, DOI 10.1088/0954-898X/14/3/304; Paninski L, 2003, NEUROCOMPUTING, V52-4, P877, DOI 10.1016/S0925-2312(02)00819-6; PANINSKI L, 2004, UNPUB MAXIMUM LIKELI; Pillow JW, 2003, NEUROCOMPUTING, V52-4, P109, DOI 10.1016/S0925-2312(02)00822-6; Reich DS, 1998, J NEUROSCI, V18, P10090; Rudd ME, 1997, NEURAL COMPUT, V9, P1047, DOI 10.1162/neco.1997.9.5.1047; Victor JD, 2000, BRAIN RES, V886, P33, DOI 10.1016/S0006-8993(00)02751-7; Yu YG, 2003, PHYS REV E, V68, DOI 10.1103/PhysRevE.68.011901	18	8	8	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1311	1318						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500163
C	Still, S; Bialek, W; Bottou, L		Thrun, S; Saul, K; Scholkopf, B		Still, S; Bialek, W; Bottou, L			Geometric clustering using the information bottleneck method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We argue that K-means and deterministic annealing algorithms for geometric clustering can be derived from the more general Information Bottleneck approach. If we cluster the identities of data points to preserve information about their location, the set of optimal solutions is massively degenerate. But if we treat the equations that define the optimal solution as an iterative algorithm, then a set of "smooth" initial conditions selects solutions with the desired geometrical properties. In addition to conceptual unification, we argue that this approach can be more efficient and robust than classic algorithms.	Princeton Univ, Dept Phys, Princeton, NJ 08544 USA	Princeton University	Still, S (corresponding author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.	susanna@princeton.edu; wbialek@princeton.edu; leon@bottou.org						Bialek W, 1996, PHYS REV LETT, V77, P4693, DOI 10.1103/PhysRevLett.77.4693; BIALEK W, 2001, PHYS BIOMOLECULES CE, V75, P485; Blatt M, 1996, PHYS REV LETT, V76, P3251, DOI 10.1103/PhysRevLett.76.3251; Fraley C, 2002, J AM STAT ASSOC, V97, P611, DOI 10.1198/016214502760047131; Gordon AD, 1999, CLASSIFICATION; HALL P, 1988, BIOMETRIKA, V75, P705; Horn D, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.018702; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; ROSE K, 1990, PHYS REV LETT, V65, P945, DOI 10.1103/PhysRevLett.65.945; Shannon C.E., 1963, MATH THEORY COMMUNIC; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Smyth P, 2000, STAT COMPUT, V10, P63, DOI 10.1023/A:1008940618127; STILL S, 2003, UNPUB; Tishby N., 1999, P 37 ANN ALL C	16	8	8	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1165	1172						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500145
C	Toussaint, M		Thrun, S; Saul, K; Scholkopf, B		Toussaint, M			Learning a world model and planning with a self-organizing, dynamic neural system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present a connectionist architecture that can learn a model of the relations between perceptions and actions and use this model for behavior planning. State representations are learned with a growing self-organizing layer which is directly coupled to a perception and a motor layer. Knowledge about possible state transitions is encoded in the lateral connectivity. Motor signals modulate this lateral connectivity and a dynamic field on the layer organizes a planning process. All mechanisms are local and adaptation is based on Hebbian ideas. The model is continuous in the action, perception, and time domain.	Ruhr Univ Bochum, Inst Neuroinformat, D-44780 Bochum, Germany	Ruhr University Bochum	Toussaint, M (corresponding author), Ruhr Univ Bochum, Inst Neuroinformat, ND 04, D-44780 Bochum, Germany.							ABBOTT LF, 1991, NETWORK-COMP NEURAL, V2, P245, DOI 10.1088/0954-898X/2/3/002; AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259; Bishop CM, 1997, P IEEE 5 INT C ART N; CARPENTER GA, 1992, IEEE T NEURAL NETWOR, V3, P698, DOI 10.1109/72.159059; Dayan P, 2001, THEORETICAL NEUROSCI; Fritzke B., 1995, ADV NEURAL INFORMATI, V7, P625; GRUSH R, 2003, IN PRESS BEHAV BRAIN; Hesslow G, 2002, TRENDS COGN SCI, V6, P242, DOI 10.1016/S1364-6613(02)01913-7; JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1207/s15516709cog1603_1; Kohonen T., 1995, SELF ORG MAPS, V30, DOI 10.1007/978-3-642-97610-0; KROSE B, 1994, P INT C INT ROB SYST; MAJORS M, 1997, CUEDFINENGTR286; MEULEAU N, 1998, MACH LEARN, V35, P117; Phillips WA, 1997, BEHAV BRAIN SCI, V20, P657, DOI 10.1017/S0140525X9700160X; Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V2, P7; SCHMIDHUBER J, 1991, FKI14991 TU MUN; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; von der Malsburg C, 1973, Kybernetik, V14, P85; Wiemer JC, 2003, NEURAL COMPUT, V15, P1143, DOI 10.1162/089976603765202695; Zimmer UR, 1996, NEUROCOMPUTING, V13, P247, DOI 10.1016/0925-2312(95)00097-6	21	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						929	936						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500116
C	Csato, L; Opper, M; Winther, O		Dietterich, TG; Becker, S; Ghahramani, Z		Csato, L; Opper, M; Winther, O			TAP Gibbs free energy, belief propagation and sparsity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				MODELS	The adaptive TAP Gibbs free energy for a general densely connected probabilistic model with quadratic interactions and arbritary single site constraints is derived. We show how a specific sequential minimization of the free energy leads to a generalization of Minka's expectation propagation. Lastly, we derive a sparse representation version of the sequential algorithm. The usefulness of the approach is demonstrated on classification and density estimation with Gaussian processes and on an independent component analysis problem.	Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Csato, L (corresponding author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.			Csato, Lehel/0000-0003-1052-1849; Winther, Ole/0000-0002-1966-3205				CSATO L, 2001, IN PRESS NEURAL COMP; CSATO L, 2000, ADV NEURAL INFORMATI, V12; HOJENSORENSEN PAD, 2001, IN PRESS NEURAL COMP; MINKA TP, 2000, THESIS MIT; PARISI G, 1995, J PHYS A-MATH GEN, V28, P5267, DOI 10.1088/0305-4470/28/18/016; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; SCHMIDT DM, 1998, PHYSICS9808005; Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302; YEDIDIA JS, 2001, IN PRESS ADV NEURAL	10	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						657	663						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100082
C	Dayan, P; Yu, A		Dietterich, TG; Becker, S; Ghahramani, Z		Dayan, P; Yu, A			ACh, uncertainty, and cortical inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				HELMHOLTZ MACHINE; VISUAL-CORTEX; NEUROMODULATION; MODELS; ACETYLCHOLINE; BEHAVIOR; RECOGNITION; STIMULI; MEMORY; REWARD	Acetylcholine (ACh) has been implicated in a wide variety of tasks involving attentional processes and plasticity. Following extensive animal studies, it has previously been suggested that ACh reports on uncertainty and controls hippocampal, cortical and cortico-amygdalar plasticity. We extend this view and consider its effects on cortical representational inference, arguing that ACh controls the balance between bottom-up inference, influenced by input stimuli, and top-down inference, influenced by contextual information. We illustrate our proposal using a hierarchical hidden Markov model.	Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Dayan, P (corresponding author), Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							Carpenter G. A., 1991, PATTERN RECOGNITION; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Dayan P, 1999, NEURAL COMPUT, V11, P653, DOI 10.1162/089976699300016610; Dayan P, 2000, NAT NEUROSCI, V3, P1218, DOI 10.1038/81504; DOYA K, 1999, 13 TOYOT C AFF MINDS, P46; Everitt BJ, 1997, ANNU REV PSYCHOL, V48, P649, DOI 10.1146/annurev.psych.48.1.649; Fellous JM, 1998, NEURAL COMPUT, V10, P771, DOI 10.1162/089976698300017476; Grenander U., 1976, LECT PATTERN THEORY, V1; GRENANDER U, 1976, LECT PATTERN THEORY, V3; GRENANDER U, 1976, LECT PATTERN THEORY, V2; Hasselmo ME, 1999, TRENDS COGN SCI, V3, P351, DOI 10.1016/S1364-6613(99)01365-0; HASSELMO ME, 1995, BEHAV BRAIN RES, V67, P1, DOI 10.1016/0166-4328(94)00113-T; HASSELMO ME, 1993, TRENDS NEUROSCI, V16, P218, DOI 10.1016/0166-2236(93)90159-J; Hasselmo ME, 1996, HIPPOCAMPUS, V6, P693, DOI 10.1002/(SICI)1098-1063(1996)6:6<693::AID-HIPO12>3.0.CO;2-W; Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101; Holland PC, 1997, ANIM LEARN BEHAV, V25, P373, DOI 10.3758/BF03209846; Holland PC, 1999, TRENDS COGN SCI, V3, P65, DOI 10.1016/S1364-6613(98)01271-6; KAKADE S, 2000, NIPS 2000; MacKay D. M., 1956, ANN MATH STUD, V34, P235, DOI DOI 10.1515/9781400882618-012; McGaughy J, 1996, BEHAV NEUROSCI, V110, P247, DOI 10.1037/0735-7044.110.2.247; MUMFORD D, 1994, LARGE SCALE NEURONAL, P125; PEARCE JM, 1980, PSYCHOL REV, V87, P532, DOI 10.1037/0033-295X.87.6.532; Pfluger HJ, 1999, CURR OPIN NEUROBIOL, V9, P683, DOI 10.1016/S0959-4388(99)00026-4; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rao RPN, 1997, NEURAL COMPUT, V9, P721, DOI 10.1162/neco.1997.9.4.721; Ress D, 2000, NAT NEUROSCI, V3, P940, DOI 10.1038/78856; Sarter M, 1997, BRAIN RES REV, V23, P28, DOI 10.1016/S0165-0173(96)00009-4; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; Schultz W, 1998, J NEUROPHYSIOL, V80, P1, DOI 10.1152/jn.1998.80.1.1; YU A, 2002, UNPUB NEURAL NETWORK	30	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						189	196						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100024
C	Mozer, MC; Dodier, R; Colagrosso, MD; Guerra-Salcedo, C; Wolniewicz, R		Dietterich, TG; Becker, S; Ghahramani, Z		Mozer, MC; Dodier, R; Colagrosso, MD; Guerra-Salcedo, C; Wolniewicz, R			Prodding the ROC curve: Constrained optimization of classifier performance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					When designing a two-alternative classifier, one ordinarily aims to maximize the classifier's ability to discriminate between members of the two classes. We describe a situation in a real-world business application of machine-learning prediction in which an additional constraint is placed on the nature of the solution: that the classifier achieve a specified correct acceptance or correct rejection rate (i.e., that it achieve a fixed accuracy on members of one class or the other). Our domain is predicting chum in the telecommunications industry. Chum refers to customers who switch from one service provider to another. We propose four algorithms for training a classifier subject to this domain constraint, and present results showing that each algorithm yields a reliable improvement in performance. Although the improvement is modest in magnitude, it is nonetheless impressive given the difficulty of the problem and the financial return that it achieves to the service provider.	Athene Software, Adv Technol Grp, Boulder, CO 80302 USA		Mozer, MC (corresponding author), Athene Software, Adv Technol Grp, 2060 Broadway, Boulder, CO 80302 USA.							Bertsekas D.P., 2019, REINFORCEMENT LEARNI; CHANG EI, 1994, ADV NEURAL INFORMATI, V6, P1019; Frederick ED, 1998, P SOC PHOTO-OPT INS, V3338, P241, DOI 10.1117/12.310897; Green DM, 1966, SIGNAL DETECTION THE; Mozer MC, 2000, IEEE T NEURAL NETWOR, V11, P690, DOI 10.1109/72.846740; WHITLEY D, 1989, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P116	6	8	8	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1409	1415						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100175
C	O'Reilly, RC; Busby, RS		Dietterich, TG; Becker, S; Ghahramani, Z		O'Reilly, RC; Busby, RS			Generalizable relational binding from coarse-coded distributed representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NETWORKS	We present a model of binding of relationship information in a spatial domain (e.g., square above triangle) that uses low-order coarse-coded conjunctive representations instead of more popular temporal synchrony mechanisms. Supporters of temporal synchrony argue that conjunctive representations lack both efficiency (i.e., combinatorial numbers of units are required) and systematicity (i.e., the resulting representations are overly specific and thus do not support generalization to novel exemplars). To counter these claims, we show that our model: a) uses far fewer hidden units than the number of conjunctions represented, by using coarse-coded, distributed representations where each unit has a broad tuning curve through high-dimensional conjunction space, and b) is capable of considerable generalization to novel inputs.	Univ Colorado, Dept Psychol, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	O'Reilly, RC (corresponding author), Univ Colorado, Dept Psychol, 345 UCB, Boulder, CO 80309 USA.	oreilly@psych.colorado.edu; Richard.Busby@Colorado.EDU						ELMAN JL, 1991, MACH LEARN, V7, P195, DOI 10.1007/BF00114844; GRAY CM, 1992, VISUAL NEUROSCI, V8, P337, DOI 10.1017/S0952523800005071; Hinton G.E., 1986, PARALLEL DISTRIBUTED, V1, P77; Holyoak KJ, 2000, COGNITIVE DYNAMICS C; Hummel JE, 1997, PSYCHOL REV, V104, P427, DOI 10.1037/0033-295X.104.3.427; Mel BW, 2000, NEURAL COMPUT, V12, P731, DOI 10.1162/089976600300015574; O'reilly R.C., 2000, COMPUTATIONAL EXPLOR; O'Reilly RC, 1998, TRENDS COGN SCI, V2, P455, DOI 10.1016/S1364-6613(98)01241-8; O'Reilly RC, 2001, NEURAL COMPUT, V13, P1199, DOI 10.1162/08997660152002834; SEIDENBERG MS, 1989, PSYCHOL REV, V96, P523, DOI 10.1037/0033-295X.96.4.523; SHASTRI L, 1993, BEHAV BRAIN SCI, V16, P417, DOI 10.1017/S0140525X00030910; STJOHN MF, 1990, ARTIF INTELL, V46, P217, DOI 10.1016/0004-3702(90)90008-N; TOURETZKY DS, 1986, P 8 ANN C COGN SCI S, P522; von der Malsburg C., 1994, MODELS NEURAL NETWOR, VII; WICKELGR.WA, 1969, PSYCHOL REV, V76, P1, DOI 10.1037/h0026823	15	8	8	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						75	82						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100010
C	Tsuda, K; Kawanabe, M; Ratsch, G; Sonnenburg, S; Muller, KR		Dietterich, TG; Becker, S; Ghahramani, Z		Tsuda, K; Kawanabe, M; Ratsch, G; Sonnenburg, S; Muller, KR			A new discriminative kernel from probabilistic models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Recently, Jaakkola and Haussler proposed a method for constructing kernel functions from probabilistic models. Their so called "Fisher kernel" has been combined with discriminative classifiers such as SVM and applied successfully in e.g. DNA and protein analysis. Whereas the Fisher kernel (FK) is calculated from the marginal log-likelihood, we propose the TOP kernel derived from Tangent vectors Of Posterior log-odds. Furthermore we develop a theoretical framework on feature extractors from probabilistic models and use it for analyzing FK and TOP. In experiments our new discriminative TOP kernel compares favorably to the Fisher kernel.	AIST CBRC, Koto Ku, Tokyo 1350064, Japan	National Institute of Advanced Industrial Science & Technology (AIST)	Tsuda, K (corresponding author), AIST CBRC, Koto Ku, 2-41-6 Aomi, Tokyo 1350064, Japan.		Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685; Ratsch, Gunnar/0000-0001-5486-8532				Durbin R., 1998, BIOL SEQUENCE ANAL P; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; GEIGER D, 1998, MSRTR9810; Jaakkola T, 2000, J COMPUT BIOL, V7, P95, DOI 10.1089/10665270050081405; Jaakkola TS, 1999, ADV NEUR IN, V11, P487; SMITH N, 2002, IN PRESS ADV NEURAL, V14	8	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						977	984						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100122
C	Shriki, O; Sompolinsky, H; Lee, DD		Leen, TK; Dietterich, TG; Tresp, V		Shriki, O; Sompolinsky, H; Lee, DD			An information maximization approach to overcomplete and recurrent representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				BLIND SEPARATION; ALGORITHM	The principle of maximizing mutual information is applied to learning overcomplete and recurrent representations. The underlying model consists of a network of input units driving a larger number of output units with recurrent interactions. In the limit of zero noise, the network is deterministic and the mutual information can be related to the entropy of the output units. Maximizing this entropy with respect to both the feedforward connections as well as the recurrent interactions results in simple learning rules for both sets of parameters. The conventional independent components (ICA) teaming algorithm can be recovered as a special case where there is an equal number of output units and no recurrent connections. The application of these new learning rules is illustrated on a simple two-dimensional input example.	Hebrew Univ Jerusalem, Racah Inst Phys, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Shriki, O (corresponding author), Hebrew Univ Jerusalem, Racah Inst Phys, IL-91904 Jerusalem, Israel.		Shriki, Oren/R-8638-2019; Lee, Daniel D./B-5753-2013	Shriki, Oren/0000-0003-1129-4799; Lee, Daniel/0000-0003-4239-8777				Amari S, 1996, ADV NEUR IN, V8, P757; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Haykin S, 1998, NEURAL NETWORKS COMP, V2nd; Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; LINSKER R, 1992, NEURAL COMPUT, V4, P691, DOI 10.1162/neco.1992.4.5.691; Parra L, 1996, NEURAL COMPUT, V8, P260, DOI 10.1162/neco.1996.8.2.260; Pearlmutter B. A., 1996, Progress in Neural Information Processing. Proceedings of the International Conference on Neural Information Processing, P151	11	8	8	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						612	618						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800087
C	Andrieu, C; de Freitas, JFG; Doucet, A		Solla, SA; Leen, TK; Muller, KR		Andrieu, C; de Freitas, JFG; Doucet, A			Robust full Bayesian methods for neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				COMPUTATION	In this paper, we propose a full Bayesian model for neural networks. This model treats the model dimension (number of neurons), model parameters, regularisation parameters and noise parameters as random variables that need to be estimated. We then propose a reversible jump Markov chain Monte Carlo (MCMC) method to perform the necessary computations. We find that the results are not only better than the previously reported ones, but also appear to be robust with respect to the prior specification. Moreover, we present a geometric convergence theorem for the algorithm.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Andrieu, C (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.			Doucet, Arnaud/0000-0002-7662-419X				ANDRIEU C, 1999, 343 CUEDFINFENGTR CA; BERNARDO JM, 1994, WILEY SERIES APPL PR; BESAG J, 1995, STAT SCI, V10, P3, DOI 10.1214/ss/1177010123; Buntine W. L., 1991, Complex Systems, V5, P603; DEFREITAS JFG, 1999, IN PRESS NEURAL COMP; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Holmes CC, 1998, NEURAL COMPUT, V10, P1217, DOI 10.1162/089976698300017421; INSUA R, 1998, 9802 DUK U I STAT DE; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Neal RM, 1996, LECT NOTES STAT, V118; TIERNEY L, 1994, ANN STAT, V22, P1701, DOI 10.1214/aos/1176325750	11	8	8	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						379	385						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700054
C	Graepel, T; Herbrich, R; Obermayer, K		Solla, SA; Leen, TK; Muller, KR		Graepel, T; Herbrich, R; Obermayer, K			Bayesian transduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Transduction is an inference principle that takes a training sample and aims at estimating the values of a function at given points contained in the so-called working sample as opposed to the whole of input space for induction. Transduction provides a confidence measure on single predictions rather than classifiers - a feature particularly important for risk-sensitive applications. The possibly infinite number of functions is reduced to a finite number of equivalence classes on the working sample, A rigorous Bayesian analysis reveals that for standard classification loss we cannot benefit from considering more than one test point at a time. The probability of the label of a given test point is determined as the posterior measure of the corresponding subset of hypothesis space, We consider the PAC setting of binary classification by linear discriminant functions (perceptrons) in kernel space such that the probability of labels is determined by the volume ratio in version space, We suggest to sample this region by an ergodic billiard. Experimental results on real world data indicate that Bayesian Transduction compares favourably to the well-known Support Vector Machine, in particular if the posterior probability of labellings is used as a confidence measure to exclude test points of low confidence.	Tech Univ Berlin, Dept Comp Sci, D-10587 Berlin, Germany	Technical University of Berlin	Graepel, T (corresponding author), Tech Univ Berlin, Dept Comp Sci, Franklinstr 28-29, D-10587 Berlin, Germany.	graepel2@cs.tu-berlin.de; ralph@cs.tu-berlin.de; oby@cs.tu-berlin.de						Bennett KP, 1999, ADVANCES IN KERNEL METHODS, P307; Cornfeld I.P., 1982, ERGODIC THEORY; Gammerman A., 1998, P 14 C UNC ART INT M, P148; HERBRICH R, 1999, 9911 TR TU BERL; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Rujan P, 1997, NEURAL COMPUT, V9, P99, DOI 10.1162/neco.1997.9.1.99; SHAWETAYLOR J, 1996, NC2TR1996054 U LOND; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Wahba G., 1990, SPLINE MODELS OBSERV	10	8	8	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						456	462						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700065
C	Sunderarajan, S; Keerthi, SS		Solla, SA; Leen, TK; Muller, KR		Sunderarajan, S; Keerthi, SS			Predictive approaches for choosing hyperparameters in Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Gaussian Processes are powerful regression models specified by parametrized mean and covariance functions. Standard approaches to estimate these parameters (known by the name Hyperparameters) are Maximum Likelihood (ML) and Maximum APosterior (MAP) approaches. In this paper, we propose and investigate predictive approaches, namely, maximization of Geisser's Surrogate Predictive Probability (GPP) and minimization of mean square error with respect to GPP (referred to as Geisser's Predictive mean square Error (GPE)) to estimate the hyperparameters. We also derive results for the standard Cross-Validation (CV) error and make a comparison. These approaches are tested on a number of problems and experimental results show that these approaches are strongly competitive to existing approaches.	Indian Inst Sci, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Sunderarajan, S (corresponding author), Indian Inst Sci, Bangalore 560012, Karnataka, India.							FRIEDMAN JH, 1991, ANN STAT, V19, P1, DOI 10.1214/aos/1176347963; GEISSER S, 1979, J AM STAT ASSOC, V74, P153, DOI 10.2307/2286745; GEISSER S, 1975, J AM STAT ASSOC, V70, P320, DOI 10.2307/2285815; MacKay D.J.C., 1997, GAUSSIAN PROCESSES R; Neal, 1997, PHYSICS9701026 ARXIV; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Rasmussen C.E., 1996, THESIS U TORONTO; STONE M, 1974, J R STAT SOC B, V36, P111, DOI 10.1111/j.2517-6161.1974.tb00994.x; SUNDARARAJAN S, 1999, UNPUB NEURAL COMPUTA; WILLIAMS CKI, 1996, ADV NEURAL INFORMATI, V8	10	8	8	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						631	637						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700090
C	Sykacek, P		Solla, SA; Leen, TK; Muller, KR		Sykacek, P			On input selection with reversible jump Markov chain Monte Carlo sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In this paper we will treat input selection for a radial. basis function (RBF) like classifier within a Bayesian framework. We approximate the a-posteriori distribution over both model coefficients and input subsets by samples drawn with Gibbs updates and reversible jump moves. Using some public datasets, we compare the classification accuracy of the method with a conventional ARD scheme. These datasets are also used to infer the a-posteriori probabilities of different input subsets.	Austrian Res Inst Artificial Intelligence OFAI, A-1010 Vienna, Austria		Sykacek, P (corresponding author), Austrian Res Inst Artificial Intelligence OFAI, Schottengasse 3, A-1010 Vienna, Austria.	peter@ai.univie.ac.at		Sykacek, Peter/0000-0001-8800-8354				Devijver PA, 1982, PATTERN RECOGNITION; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Ghahramani Z., 1994, P ADV NEUR INF PROC, P120; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Holmes CC, 1998, NEURAL COMPUT, V10, P1217, DOI 10.1162/089976698300017421; NEAL RM, 1986, BAYESIAN LEARNING NE; Phillips DV, 1996, MARKOV CHAIN MONTE C, P215; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; STENSMO M, 1995, ADV NEURAL INFORMATI, V7, P1077; TRAVEN HGC, 1991, IEEE T NEURAL NETWOR, V2, P366, DOI 10.1109/72.97913	10	8	8	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						638	644						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700091
C	Briegel, T; Tresp, V		Kearns, MS; Solla, SA; Cohn, DA		Briegel, T; Tresp, V			Fisher scoring and a mixture of modes approach for approximate inference and learning in nonlinear state space models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We present Monte-Carlo generalized EM equations for learning in nonlinear state space models. The difficulties lie in the Monte-Carlo E-step which consists of sampling from the posterior distribution of the hidden variables given the observations. The new idea presented in this paper is to generate samples from a Gaussian approximation to the true posterior from which it is easy to obtain independent samples. The parameters of the Gaussian approximation are either derived from the extended Kalman filter or the Fisher scoring algorithm. In case the posterior density is multimodal we propose to approximate the posterior by a sum of Gaussians (mixture of modes approach). We show that sampling from the approximate posterior densities obtained by the above algorithms leads to better models than using point estimates for the hidden states. In our experiment, the Fisher scoring algorithm obtained a better approximation of the posterior mode than the EKE For a multimodal distribution, the mixture of modes approach gave superior results.	Siemens AG, Corp Technol, Dept Informat & Commun, D-81730 Munich, Germany; Siemens AG, Corp Technol, Dept Informat & Commun, D-81730 Munich, Germany	Siemens AG; Siemens Germany; Siemens AG; Siemens Germany	Briegel, T (corresponding author), Siemens AG, Corp Technol, Dept Informat & Commun, Otto Hahn Ring 6, D-81730 Munich, Germany.							Anderson B. D. O., 1979, OPTIMAL FILTERING; FAHRMEIR L, 1991, METRIKA, V38, P37, DOI DOI 10.1007/BF02613597; GHAHRAMANI Z, 1999, ADV NEURAL INFORMATI, V11; KITAGAWA G, 1987, J AM STAT ASSOC, V82, P1032, DOI 10.2307/2289375; PUSKORIUS GV, 1994, IEEE T NEURAL NETWOR, V5, P279, DOI 10.1109/72.279191; Sage AP, 1971, ESTIMATION THEORY AP; SHUMWAY R, 1982, TIME SERIES SMOOTHIN; TRESP V, 1995, IEEE SIG P SOC, P1	8	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						403	409						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700057
C	Moll, R; Barto, AG; Perkins, TJ; Sutton, RS		Kearns, MS; Solla, SA; Cohn, DA		Moll, R; Barto, AG; Perkins, TJ; Sutton, RS			Learning instance-independent value functions to enhance local search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Reinforcement learning methods can be used to improve the performance of local search algorithms for combinatorial optimization by learning an evaluation function that predicts the outcome of search. The evaluation function is therefore able to guide search to low-cost solutions better than can the original cost function. We describe a reinforcement learning method for enhancing local search that combines aspects of previous work by Zhang and Dietterich (1995) and Boyan and Moore (1997, Boyan 1998). In an off-line learning phase, a value function is learned that is useful for guiding search for multiple problem sizes and instances. We illustrate our technique by developing several such functions for the Dial-A-Ride Problem. Our learning-enhanced local search algorithm exhibits an improvement of more then 30% over a standard local search algorithm.	Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Moll, R (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.							BOYAN JA, 1998, THESIS CARNEGIE MELL; BOYAN JA, 1997, P AI STATS 97; HEALY P, 1995, EUR J OPER RES, V83, P83, DOI 10.1016/0377-2217(93)E0292-6; PSARAFTIS HN, 1983, EUR J OPER RES, V13, P391, DOI 10.1016/0377-2217(83)90099-1; Stein D. M., 1978, Mathematics of Operations Research, V3, P89, DOI 10.1287/moor.3.2.89; Tsitsiklis J, 1996, NEURO DYNAMIC PROGRA; ZHANG W, 1995, P 14 INT JOINT C ART, P1114	7	8	8	1	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1017	1023						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700143
C	Munos, R; Moore, A		Kearns, MS; Solla, SA; Cohn, DA		Munos, R; Moore, A			Barycentric interpolators for continuous space & time reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In order to find the optimal control of continuous state-space and time reinforcement learning (RL) problems, wet approximate the value function (VF) with a particular class of functions called the barycentric interpolators. We establish sufficient conditions under which a RL algorithm converges to the optimal VF, even when we use approximate models of the state dynamics and the reinforcement functions.	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Munos, R (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.							Atkeson CG, 1997, ARTIF INTELL REV, V11, P11, DOI 10.1023/A:1006559212014; Barles G., 1991, Asymptotic Analysis, V4, P271; Barles G., 1994, MATH APPL, V17; DAVIES S, 1996, ADV NEURAL INFORMATI, P8; Gordon Geoffrey J, 1995, INT C MACH LEARN; HAROLD J, 1990, SIAM J CONTROL OPTIM, V28, P999; MUNOS R, 1997, INT JOINT C ART INT; MUNOS R, 1998, EUR C MACH LEARN; Omohundro S. M., 1987, Complex Systems, V1, P273; WENDELL H, 1993, CONTROLLED MARKOV PR	10	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1024	1030						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700144
C	Trecate, GF; Williams, CKI; Opper, M		Kearns, MS; Solla, SA; Cohn, DA		Trecate, GF; Williams, CKI; Opper, M			Finite-dimensional approximation of Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO						Univ Pavia, Dipartimento Informat & Sistemist, I-27100 Pavia, Italy	University of Pavia	Trecate, GF (corresponding author), Univ Pavia, Dipartimento Informat & Sistemist, Via Ferrata 1, I-27100 Pavia, Italy.		Trecate, Giancarlo Ferrari/E-1048-2012	Trecate, Giancarlo Ferrari/0000-0002-9492-9624				De Nicolao G, 1998, IEEE WORLD CONGRESS ON COMPUTATIONAL INTELLIGENCE, P2407; Gibbs M., 1997, EFFICIENT IMPLEMENTA; Opper M., 1997, THEORETICAL ASPECTS; Pilz J., 1991, WILEY SER PROBAB STA, V212; Ripley BD., 1996; Vivarelli F., 1998, THESIS ASTON U BIRMI; WAHBA G, 1990, CBMS NSF REG C SER A; Whittle P, 1963, PREDICTION REGULATIO; WILLIAMS C, 1998, LEARNING INFERENCE G; ZHU H, 1997, NCRG97011 AST U; Zhu HY, 1996, NEURAL COMPUT APPL, V4, P130, DOI 10.1007/BF01414873	11	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						218	224						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700031
C	Atkeson, CG		Jordan, MI; Kearns, MJ; Solla, SA		Atkeson, CG			Nonparametric model-based reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper describes some of the interactions of model learning algorithms and planning algorithms we have found in exploring model-based reinforcement learning. The paper focuses on how local trajectory optimizers can be used effectively with learned nonparametric models. We find that trajectory planners that are fully consistent with the learned model often have difficulty finding reasonable plans in the early stages of learning. Trajectory planners that balance obeying the learned model with minimizing cost (or maximizing reward) often do better, even if the plan is not fully consistent with the learned model.	Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Atkeson, CG (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.								0	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1008	1014						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700142
C	Golea, M; Bartlett, PL; Lee, WS; Mason, L		Jordan, MI; Kearns, MJ; Solla, SA		Golea, M; Bartlett, PL; Lee, WS; Mason, L			Generalization in decision trees and DNF: Does size matter?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Recent theoretical results for pattern classification with thresholded real-valued functions (such as support vector machines, sigmoid networks, and boosting) give bounds on misclassification probability that do not depend on the size of the classifier, and hence can be considerably smaller than the bounds that follow from the VC theory. In this paper, we show that these techniques can be more widely applied, by representing other boolean functions as two-layer neural networks (thresholded convex combinations of boolean functions). For example, we show that with high probability any decision tree of depth no more than d that is consistent with m training examples has misclassification probability no more than O ((1/m (N-eff VCdim(U) log(2) m log d))(1,2)), where U is the class of node decision functions, and N-eff less than or equal to N can be thought of as the effective number of leaves (it becomes small as the distribution on the leaves induced by the training data gets far from uniform). This bound is qualitatively different from the VC bound and can be considerably smaller. We use the same technique to give similar results for DNF formulae.	Australian Natl Univ, Dept Syst Engn, Res Sch Informat Sci & Engn, Canberra, ACT, Australia	Australian National University	Bartlett, PL (corresponding author), Australian Natl Univ, Dept Syst Engn, Res Sch Informat Sci & Engn, GPO Box 4, Canberra, ACT, Australia.			Bartlett, Peter/0000-0002-8760-3140					0	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						259	265						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700037
C	Kiviluoto, K; Oja, E		Jordan, MI; Kearns, MJ; Solla, SA		Kiviluoto, K; Oja, E			S-Map: A network with a simple self-organization algorithm for generative topographic mappings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The S-Map is a network with a simple learning algorithm that combines the self-organization capability of the Self-Organizing Map (SOM) and the probabilistic interpretability of the Generative Topographic Mapping (GTM). The simulations suggest that the S-Map algorithm has a stronger tendency to self-organize from random initial configuration than the GTM. The S-Map algorithm can be further simplified to employ pure Hebbian learning, without changing the qualitative behaviour of the network.	Helsinki Univ Technol, Lab Comp & Informat Sci, FIN-02015 Espoo, Finland	Aalto University	Kiviluoto, K (corresponding author), Helsinki Univ Technol, Lab Comp & Informat Sci, POB 2200, FIN-02015 Espoo, Finland.								0	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						549	555						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700078
C	Vu, VH		Jordan, MI; Kearns, MJ; Solla, SA		Vu, VH			On the infeasibility of training neural networks with small squared errors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	Advances in Neural Information Processing Systems		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We demonstrate that the problem of training neural networks with small (average) squared error is computationally intractable. Consider a data set of M points (X-i,Y-i), i = 1, 2, ..., M, where X-i are input vectors from R-d, Y-i are real outputs (Y-i is an element of R). For a network f(0) in some class F of neural networks, (1/M) Sigma(i=1)(M) (f(o)(X-i) - Y-i)(2))(1/2) - inf(f is an element of F) (1/M) Sigma(i=1)(M)(f(X-i) - Y-i)(2))(1/2) is the (average) relative error occurs when one tries to fit the data set by f(0). We will prove for several classes F of neural networks that achieving a relative error smaller than some fixed positive threshold (independent from the size of the data set) is NP-hard.	Yale Univ, Dept Math, New Haven, CT 06520 USA	Yale University	Vu, VH (corresponding author), Yale Univ, Dept Math, New Haven, CT 06520 USA.	vuha@math.yale.edu							0	8	8	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						371	377						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700053
C	Brightwell, G; Kenyon, C; PaugamMoisy, H		Mozer, MC; Jordan, MI; Petsche, T		Brightwell, G; Kenyon, C; PaugamMoisy, H			Multilayer neural networks: One or two hidden layers?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We study the number of hidden layers required by a multilayer neural network with threshold units to compute a function f from R-d to {0, 1}. In dimension d = 2, Gibson characterized the functions computable with just one hidden layer, under the assumption that there is no ''multiple intersection point'' and that f is only defined on a compact set. We consider the restriction of f to the neighborhood of a multiple intersection point or of infinity, and give necessary and sufficient conditions for it to be locally computable with one hidden layer. We show that adding these conditions to Gibson's assumptions is not sufficient to ensure global computability with one hidden layer, by exhibiting a new non-local configuration, the ''critical cycle'', which implies that f is not computable with one hidden layer.			Brightwell, G (corresponding author), LSE,DEPT MATH,HOUGHTON ST,LONDON WC2A 2AE,ENGLAND.			Kenyon, Garrett/0000-0003-4836-3938					0	8	8	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						148	154						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00021
C	Fritsch, J; Finke, M; Waibel, A		Mozer, MC; Jordan, MI; Petsche, T		Fritsch, J; Finke, M; Waibel, A			Adaptively growing hierarchial mixtures of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We propose a novel approach to automatically growing and pruning Hierarchical Mixtures of Experts. The constructive algorithm proposed here enables large hierarchies consisting of several hundred experts to be trained effectively. We show that HME's trained by our automatic growing procedure yield better generalization performance than traditional static and balanced hierarchies. Evaluation of the algorithm is performed. (1) on vowel classification and (2) within a hybrid version of the JANUS [9] speech recognition system using a subset of the Switchboard large-vocabulary speaker-independent continuous speech recognition database.			Fritsch, J (corresponding author), CARNEGIE MELLON UNIV,INTERACT SYST LABS,PITTSBURGH,PA 15213, USA.								0	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						459	465						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00065
C	Niranjan, M		Mozer, MC; Jordan, MI; Petsche, T		Niranjan, M			Sequential tracking in pricing financial options using model based and neural network approaches	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper shows how the prices of option contracts traded in financial markets can be tracked sequentially by means of the Extended Kalman Filter algorithm. I consider call and put. option pairs with identical strike price and time of maturity as a two output nonlinear system. The Black-Scholes approach popular in Finance literature and the Radial Basis Functions neural network are used in modelling the nonlinear system generating these observations. I show how both these systems may be identified recursively using the EKF algorithm. I present results of simulations on some FTSE 100 Index options data and discuss the implications of viewing the pricing problem in this sequential manner.			Niranjan, M (corresponding author), UNIV CAMBRIDGE,DEPT ENGN,CAMBRIDGE CB2 1PZ,ENGLAND.			Niranjan, Mahesan/0000-0001-7021-140X					0	8	8	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						960	966						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00135
C	Pouget, A; Zhang, KC		Mozer, MC; Jordan, MI; Petsche, T		Pouget, A; Zhang, KC			Statistically efficient estimation using cortical lateral connections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Coarse codes are widely used throughout the brain to encode sensory and motor variables. Methods designed to interpret these codes, such as population vector analysis, are either inefficient, i.e., the variance of the estimate is much larger than the smallest possible variance, or biologically implausible, like maximum likelihood. Moreover, these methods attempt to compute a scalar or vector estimate of the encoded variable. Neurons are faced with a similar estimation problem. They must read out the responses of the presynaptic neurons, but, by contrast, they typically encode the variable with a further population code rather than as a scalar. We show how a non-linear recurrent network can be used to perform these estimation in an optimal way while keeping the estimate in a coarse code format. This work suggests that lateral connections in the cortex may be involved in cleaning up uncorrelated noise among neurons representing similar variables.			Pouget, A (corresponding author), GEORGETOWN UNIV,INST COMPUTAT & COGNIT SCI,WASHINGTON,DC 20007, USA.								0	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						97	103						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00014
C	Wong, KYM		Mozer, MC; Jordan, MI; Petsche, T		Wong, KYM			Microscopic equations in rough energy landscape for neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We consider the microscopic equations for learning problems in neural networks. The aligning fields of an example are obtained from the cavity fields, which are the fields if that example were absent in the learning process. In a rough energy landscape, we assume that the density of the local minima obey an exponential distribution, yielding macroscopic properties agreeing with the first step replica symmetry breaking solution. Iterating the microscopic equations provide a learning algorithm, which results in a higher stability than conventional algorithms.			Wong, KYM (corresponding author), HONG KONG UNIV SCI & TECHNOL,DEPT PHYS,CLEAR WATER BAY,KOWLOON,HONG KONG.			Wong, Kwok Yee Michael/0000-0002-3078-4577					0	8	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						302	308						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00043
C	Jaakkola, T; Saul, LK; Jordan, MI		Touretzky, DS; Mozer, MC; Hasselmo, ME		Jaakkola, T; Saul, LK; Jordan, MI			Fast learning by bounding likelihoods in sigmoid type belief networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,DEPT BRAIN & COGNIT SCI,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)			Jordan, Michael I/C-5253-2013						0	8	8	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						528	534						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00075
C	Parra, LC		Touretzky, DS; Mozer, MC; Hasselmo, ME		Parra, LC			Symplectic nonlinear component analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SIEMENS AG,CORP RES,PRINCETON,NJ 08540	Siemens AG									0	8	9	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						437	443						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00062
C	SUN, GZ; CHEN, HH; LEE, YC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SUN, GZ; CHEN, HH; LEE, YC			GREEN-FUNCTION METHOD FOR FAST ONLINE LEARNING ALGORITHM OF RECURRENT NEURAL NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	8	8	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						333	340						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00041
C	Sulam, J; Muthukumar, R; Arora, R		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Sulam, Jeremias; Muthukumar, Ramchandran; Arora, Raman			Adversarial Robustness of Supervised Sparse Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				REPRESENTATION; SHRINKAGE	Several recent results provide theoretical insights into the phenomena of adversarial examples. Existing results, however, are often limited due to a gap between the simplicity of the models studied and the complexity of those deployed in practice. In this work, we strike a better balance by considering a model that involves learning a representation while at the same time giving a precise generalization bound and a robustness certificate. We focus on the hypothesis class obtained by combining a sparsity-promoting encoder coupled with a linear classifier, and show an interesting interplay between the expressivity and stability of the (supervised) representation map and a notion of margin in the feature space. We bound the robust risk (to l(2)-bounded perturbations) of hypotheses parameterized by dictionaries that achieve a mild encoder gap on training data. Furthermore, we provide a robustness certificate for end-to-end classification. We demonstrate the applicability of our analysis by computing certified accuracy on real data, and compare with other alternatives for certified robustness.	[Sulam, Jeremias; Muthukumar, Ramchandran; Arora, Raman] Johns Hopkins Univ, Baltimore, MD 21218 USA	Johns Hopkins University	Sulam, J (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	jsulam1@jhu.edu; rmuthuk1@jhu.edu; arora@cs.jhu.edu		Sulam, Jeremias/0000-0003-0946-1957	DARPA GARD award [HR00112020004]; NSF BIGDATA award [IIS-1546482]; NSF CAREER award [IIS-1943251]; NSF TRIPODS award [CCF-1934979]; Simons Institute as part of the program on the Foundations of Deep Learning; Institute for Advanced Study (IAS), Princeton, NJ	DARPA GARD award; NSF BIGDATA award; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF TRIPODS award; Simons Institute as part of the program on the Foundations of Deep Learning; Institute for Advanced Study (IAS), Princeton, NJ	This research was supported, in part, by DARPA GARD award HR00112020004, NSF BIGDATA award IIS-1546482, NSF CAREER award IIS-1943251 and NSF TRIPODS award CCF-1934979. Jeremias Sulam kindly thanks Aviad Aberdam for motivating and inspiring discussions. Raman Arora acknowledges support from the Simons Institute as part of the program on the Foundations of Deep Learning and the Institute for Advanced Study (IAS), Princeton, NJ, as part of the special year on Optimization, Statistics, and Theoretical Machine Learning.	Aberdam A., 2020, ARXIV200108456; Aberdam A, 2019, SIAM J MATH DATA SCI, V1, P46, DOI 10.1137/18M1183352; Aghasi A, 2020, SIAM J MATH DATA SCI, V2, P158, DOI 10.1137/19M1246468; Athalye A, 2018, PR MACH LEARN RES, V80; Bafna M., 2018, ADV NEURAL INFORM PR, V31, P10075; Balda Emilio Rafael, 2019, ARXIV190600698; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Bubeck S., 2018, ARXIV180510204; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Charles Zachary, 2019, ARXIV190509209; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Coates Adam, 2011, IMPORTANCE ENCODING; Cohen J, 2019, PR MACH LEARN RES, V97; Cullina D., 2018, ADV NEURAL INFORM PR, P228; Demontis A, 2019, PROCEEDINGS OF THE 28TH USENIX SECURITY SYMPOSIUM, P321; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Foucart S., 2017, B AM MATH SOC, V54, P151; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gribonval R, 2015, IEEE T INFORM THEORY, V61, P3469, DOI 10.1109/TIT.2015.2424238; Gu S., 2014, ARXIV14125068; Henaff M., 2011, ISMIR, V11, P2011; HUNG SY, 2018, ADV NEURAL INFORM PR, P424; Ilyas A., 2019, P ADV NEUR INF PROC; Kavukcuoglu K, 2010, ARXIV10103467; Kingma D.P, P 3 INT C LEARNING R; Li Y., 2020, ARXIV200510190; Li Yan, 2019, ARXIV190602931; Madry Aleksander, 2017, ARXIV; Mairal J, 2008, PROC CVPR IEEE, P2415; Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828; Mairal J, 2012, IEEE T PATTERN ANAL, V34, P791, DOI 10.1109/TPAMI.2011.156; Marzi Z, 2018, IEEE INT SYMP INFO, P31; Mehta N, 2008, ROUTL MEDIA CULT SOC, P32; Mendelson S, 2004, J MACH LEARN RES, V5, P219; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Mohri M., 2018, FDN MACHINE LEARNING; Moreau Thomas, 2016, ARXIV160900285; Murdock Calvin, 2020, ARXIV200313866; Papyan V, 2018, IEEE SIGNAL PROC MAG, V35, P72, DOI 10.1109/MSP.2018.2820224; Papyan V, 2017, J MACH LEARN RES, V18, P1; Raghunathan Aditi, 2018, ARXIV180109344; Ranzato Marc'Aurelio., 2007, PROC CVPR IEEE, P1, DOI [10.1109/CVPR.2007.383157, DOI 10.1109/CVPR.2007.383157]; Romano CA, 2007, HEALTH INFORM SER, P1; Salman Hadi, 2020, ARXIV200301908; Schmidt L, 2018, ADV NEUR IN, V31; Seibert Matthias, 2019, THESIS; Shafahi Ali, 2018, ARXIV180902104; Shuhang G., 2014, ADV NEURAL INFORM PR, V27, P793; Sulam Jeremias, 2019, IEEE T PATTERN ANAL; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tolooshams Bahareh, 2019, ARXIV190408827; Tramer F., 2020, ARXIV200208347; Tropp J. A., 2003, P INT C IM PROC, V1, P1; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Tsipras Dimitris, 2018, ARXIV180512152; Tu ZZ, 2019, ADV NEUR IN, V32; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Wong Eric, 2017, ARXIV PREPRINT ARXIV; Wright J, 2010, P IEEE, V98, P1031, DOI 10.1109/JPROC.2010.2044470; Xin B, 2016, ADV NEUR IN, V29; Yin D., 2018, ARXIV181011914; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	68	7	7	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000064
C	Aittala, M; Sharma, P; Murmann, L; Yedidia, AB; Wornell, GW; Freeman, WT; Durand, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aittala, Miika; Sharma, Prafull; Murmann, Lukas; Yedidia, Adam B.; Wornell, Gregory W.; Freeman, William T.; Durand, Fredo			Computational Mirrors: Blind Inverse Light Transport by Deep Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We recover a video of the motion taking place in a hidden scene by observing changes in indirect illumination in a nearby uncalibrated visible region. We solve this problem by factoring the observed video into a matrix product between the unknown hidden scene video and an unknown light transport matrix. This task is extremely ill-posed as any non-negative factorization will satisfy the data. Inspired by recent work on the Deep Image Prior, we parameterize the factor matrices using randomly initialized convolutional neural networks trained in a one-off manner, and show that this results in decompositions that reflect the true motion in the hidden scene.	[Aittala, Miika; Sharma, Prafull; Murmann, Lukas; Yedidia, Adam B.; Wornell, Gregory W.; Freeman, William T.; Durand, Fredo] MIT, Cambridge, MA 02139 USA; [Freeman, William T.] Google Res, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Aittala, M (corresponding author), MIT, Cambridge, MA 02139 USA.	miika@csail.mit.edu; prafull@mit.edu; lmurmann@mit.edu; adamy@mit.edu; gww@mit.edu; billf@mit.edu; fredo@mit.edu			DARPA [HR0011-16-C-0030]; NSF [CCF-1816209]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF))	This work was supported, in part, by DARPA under Contract No. HR0011-16-C-0030, and by NSF under Grant No. CCF-1816209. The authors wish to thank Luke Anderson for proofreading and helping with the manuscript.	[Anonymous], 2017, INT C COMP VIS; [Anonymous], 2008, P 25 INT C MACH LEAR; Asim Muhammad, 2018, ARXIV180204073; Baradad M, 2018, PROC CVPR IEEE, P6267, DOI 10.1109/CVPR.2018.00656; Fergus R, 2006, ACM T GRAPHIC, V25, P787, DOI 10.1145/1141911.1141956; Gandelsman Yossi, 2019, COMP VIS PATT REC CV; Garg G., 2006, EUROGRAPHICS S RENDE, P251; Gariepy G, 2016, NAT PHOTONICS, V10, P23, DOI [10.1038/nphoton.2015.234, 10.1038/NPHOTON.2015.234]; Heckel Reinhard, 2019, INT C LEARN REPR; Heide F, 2014, PROC CVPR IEEE, P3222, DOI 10.1109/CVPR.2014.418; Hoyer PO, 2004, J MACH LEARN RES, V5, P1457; JEFFERIES SM, 1993, ASTROPHYS J, V415, P862, DOI 10.1086/173208; Kajiya J.T., 1986, SIGGRAPH, P143, DOI [DOI 10.1145/15922.15902, 10.1145/15886.15902, DOI 10.1145/15886.15902]; Kingma D.P., 2015, INT C LEARN REPR ICL; Klein J., 2016, SCI REPORTS, V6, P2, DOI DOI 10.1038/SREP32491;PMID:27577969; Koenderink JJ, 1997, INT J COMPUT VISION, V23, P217, DOI 10.1023/A:1007971132346; Kompass R, 2007, NEURAL COMPUT, V19, P780, DOI 10.1162/neco.2007.19.3.780; Krishnan D, 2011, PROC CVPR IEEE, P233, DOI 10.1109/CVPR.2011.5995521; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Levin A., 2009, UNDERSTANDING EVALUA; Levin A., 2011, P CVPR, V2657, P2664; Liu R, 2018, ADV NEURAL INFORM PR, P9605; Pandharkar R, 2011, PROC CVPR IEEE, P265, DOI 10.1109/CVPR.2011.5995465; Peers P, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1477926.1477929; PHARR M., 2016, PHYS BASED RENDERING, V3rd; Ramamoorthi R, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1186644.1186646; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Saunders C, 2019, NATURE, V565, P472, DOI 10.1038/s41586-018-0868-6; SCHULZ TJ, 1993, J OPT SOC AM A, V10, P1064, DOI 10.1364/JOSAA.10.001064; Sen P, 2005, ACM T GRAPHIC, V24, P745, DOI 10.1145/1073204.1073257; Sharma SK, 2003, J OPT A-PURE APPL OP, V5, P294, DOI 10.1088/1464-4258/5/3/324; Shrestha S, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925928; Torralba A, 2014, INT J COMPUT VISION, V110, P92, DOI 10.1007/s11263-014-0697-5; Trigeorgis G, 2017, IEEE T PATTERN ANAL, V39, P417, DOI 10.1109/TPAMI.2016.2554555; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Van Veen Dave, 2018, ARXIV180606438; Velten A, 2012, NAT COMMUN, V3, DOI 10.1038/ncomms1747; Virtanen T, 2007, IEEE T AUDIO SPEECH, V15, P1066, DOI 10.1109/TASL.2006.885253; Wang JP, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531335; Xia L., 2011, COMP VIS PATT REC WO, P15, DOI DOI 10.1109/CVPRW.2011.5981811; Xu F., 2016, COMPUTATIONAL OPTICA; Xue HJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3203; Zhang Y, 2019, IEEE IND ELEC, P5433, DOI 10.1109/IECON.2019.8927138; Zuo W., 2019, IEEE C COMP VIS PATT	44	7	7	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906004
C	Alaa, AM; van der Schaar, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alaa, Ahmed M.; van der Schaar, Mihaela			Attentive State-Space Modeling of Disease Progression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				HEALTH	Models of disease progression are instrumental for predicting patient outcomes and understanding disease dynamics. Existing models provide the patient with pragmatic (supervised) predictions of risk, but do not provide the clinician with intelligible (unsupervised) representations of disease pathology. In this paper, we develop the attentive state-space model, a deep probabilistic model that learns accurate and interpretable structured representations for disease trajectories. Unlike Markovian state-space models, in which state dynamics are memoryless, our model uses an attention mechanism to create "memoryful" dynamics, whereby attention weights determine the dependence of future disease states on past medical history. To learn the model parameters from medical records, we develop an inference algorithm that jointly learns a compiled inference network and the model parameters, leveraging the attentive representation to construct a variational approximation of the posterior state distribution. Experiments on data from the UK Cystic Fibrosis registry show that our model demonstrates superior predictive accuracy, in addition to providing insights into disease progression dynamic.	[Alaa, Ahmed M.] Univ Calif Los Angeles, ECE Dept, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Univ Cambridge, Cambridge, England; [van der Schaar, Mihaela] Alan Turing Inst, London, England	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; University of Cambridge	Alaa, AM (corresponding author), Univ Calif Los Angeles, ECE Dept, Los Angeles, CA 90024 USA.	ahmedmalaa@ucla.edu; mv472@cam.ac.uk			National Science Foundation (NSF) [1462245, 1533983]; US Office of Naval Research (ONR)	National Science Foundation (NSF)(National Science Foundation (NSF)); US Office of Naval Research (ONR)(Office of Naval Research)	This work was supported by the National Science Foundation (NSF grants 1462245 and 1533983), and the US Office of Naval Research (ONR). The data for our experiments was provided by the UK Cystic Fibrosis Trust. We thank Dr. Janet Allen (Director of Strategic Innovation, UK Cystic Fibrosis Trust) for the vision and encouragement. We thank Rebecca Cosgriff and Elaine Gunn for the help with data access, extraction and analysis.	Alaa AM, 2017, PR MACH LEARN RES, V70; Alaa Ahmed M, 2018, J MACHINE LEARNING R; Bahdanau D., 2015, INT C LEARN REPR ICL; Bayer J, 2014, ARXIV14117610; Begleiter R, 2004, J ARTIF INTELL RES, V22, P385, DOI 10.1613/jair.1491; Bica Ioana, 2020, ARXIV200204083; Blumenthal D, 2010, NEW ENGL J MED, V363, P501, DOI 10.1056/NEJMp1006114; Braun AT, 2011, CURR OPIN PULM MED, V17, P467, DOI 10.1097/MCP.0b013e32834b8bdb; Choi E, 2016, ADV NEUR IN, V29; Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Coyne DW, 2011, CKD MEDSCAPE CME EXP; Dai Hanjun, 2016, INT C LEARN REPR; Eddy AA, 2006, J AM SOC NEPHROL, V17, P2964, DOI 10.1681/ASN.2006070704; Karl M., 2016, INT C LEARN REPR; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma D.P, P 3 INT C LEARNING R; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Kwon BC, 2019, IEEE T VIS COMPUT GR, V25, P299, DOI 10.1109/TVCG.2018.2865027; Lim B., 2018, MACH LEARN HEALTHC C; Lipton Zachary C., 2016, INT C LEARN REPR; Liu YY., 2015, ADV NEURAL INFORM PR, V28, P3600; Ma FL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1903, DOI 10.1145/3097983.3098088; Mnih Andriy, 2014, INT C MACH LEARN; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Rangapuram Syama Sundar, 2018, ADV NEURAL INF PROCE, P7796; Sanders DB, 2010, PEDIATR PULM, V45, P127, DOI 10.1002/ppul.21117; Schulman J., 2015, ARXIV PREPRINT ARXIV; Sevick MA, 2007, J GEN INTERN MED, V22, P438, DOI 10.1007/s11606-007-0316-z; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Szczesniak RD, 2017, AM J RESP CRIT CARE, V196, P471, DOI 10.1164/rccm.201612-2574OC; Topol EJ, 2019, NAT MED, V25, P44, DOI 10.1038/s41591-018-0300-7; Valderas JM, 2009, ANN FAM MED, V7, P357, DOI 10.1370/afm.983; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang X, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P85, DOI 10.1145/2623330.2623754; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012; Zheng X, 2017, ARXIV171111179CSLG	40	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903002
C	Arpit, D; Campos, V; Bengio, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arpit, Devansh; Campos, Victor; Bengio, Yoshua			How to Initialize your Network? Robust Initialization for WeightNorm & ResNets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Residual networks (ResNet) and weight normalization play an important role in various deep learning applications. However, parameter initialization strategies have not been studied previously for weight normalized networks and, in practice, initialization methods designed for un-normalized networks are used as a proxy. Similarly, initialization for ResNets have also been studied for un-normalized networks and often under simplified settings ignoring the shortcut connection. To address these issues, we propose a novel parameter initialization strategy that avoids explosion/vanishment of information across layers for weight normalized networks with and without residual connections. The proposed strategy is based on a theoretical analysis using mean field approximation. We run over 2,500 experiments and evaluate our proposal on image datasets showing that the proposed initialization outperforms existing initialization methods in terms of generalization performance, robustness to hyper-parameter values and variance between seeds, especially when networks get deeper in which case existing methods fail to even start training. Finally, we show that using our initialization in conjunction with learning rate warmup is able to reduce the gap between the performance of weight normalized and batch normalized networks.	[Arpit, Devansh] Salesforce Res, Stanford, CA 94305 USA; [Campos, Victor] Barcelona Supercomputing Ctr, Barcelona, Spain; [Bengio, Yoshua] Univ Montreal, Montreal Inst Learning Algorithms, Montreal, PQ, Canada	Salesforce; Universite de Montreal	Arpit, D (corresponding author), Salesforce Res, Stanford, CA 94305 USA.	devansharpit@gmail.com; victor.campos@bsc.es			IVADO	IVADO	We would like to thank Giancarlo Kerg for proofreading the paper. DA was supported by IVADO during his time at Mila where part of this work was done.	[Anonymous], 2019, INT C LEARN REPR; Arpit D, 2016, PR MACH LEARN RES, V48; Cooijmans T., 2016, ARXIV160309025; DeVries T., 2017, P 2017 COMPUTER VISI; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Gitman I, 2017, ARXIV170908145; Glorot X., 2010, PROC MACH LEARN RES, P249; Goyal P., 2017, LARGE MINIBATCH SGD; Hanin B., 2018, ADV NEURAL INFORM PR, P582; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jastrzebski Stanislaw, 2018, ICLR; Jastrzebski Stanislaw, 2017, ARXIV171104623; Keskar N.S., 2016, ABS160904836; Krihenbiihl Philipp, 2015, ARXIV151106856; Krizhevsky A, 2009, LEARNING MULTIPLE LA; LeCun Yann, MNIST DATABASE HANDW; Mishkin Dmytro, 2015, ICLR; Perez-Cruz F, 2018, INT C ART INT STAT, V84, P1924; Poole B, 2016, ADV NEUR IN, V29; Saxe A.M., 2014, 2 INT C LEARN REPR; Shang Wenling, 2017, AAAI; Smith S.L., 2018, P ICLR; Taki Masato, 2017, ARXIV PREPRINT ARXIV; Tarnowski Wojciech, 2018, ARXIV180908848; Xiao L, 2018, PROGNOST SYST HEALT, P100, DOI 10.1109/PHM-Chongqing.2018.00023; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang Han, 2019, ICLR	31	7	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902052
C	Bashkirova, D; Usman, B; Saenko, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bashkirova, Dina; Usman, Ben; Saenko, Kate			Adversarial Self-Defense for Cycle-Consistent GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TO-IMAGE TRANSLATION	The goal of unsupervised image-to-image translation is to map images from one domain to another without the ground truth correspondence between the two domains. State-of-art methods learn the correspondence using large numbers of unpaired examples from both domains and are based on generative adversarial networks. In order to preserve the semantics of the input image, the adversarial objective is usually combined with a cycle-consistency loss that penalizes incorrect reconstruction of the input image from the translated one. However, if the target mapping is many-to-one, e.g. aerial photos to maps, such a restriction forces the generator to hide information in low-amplitude structured noise that is undetectable by human eye or by the discriminator. In this paper, we show how such self-attacking behavior of unsupervised translation methods affects their performance and provide two defense techniques. We perform a quantitative evaluation of the proposed techniques and show that making the translation model more robust to the self-adversarial attack increases its generation quality and reconstruction reliability and makes the model less sensitive to low-amplitude perturbations. Our project page can be found at ai.bu.edu/selfadv/.	[Bashkirova, Dina; Usman, Ben; Saenko, Kate] Boston Univ, Boston, MA 02215 USA; [Saenko, Kate] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA	Boston University; Massachusetts Institute of Technology (MIT)	Bashkirova, D (corresponding author), Boston Univ, Boston, MA 02215 USA.	dbash@bu.edu; usmn@bu.edu; saenko@bu.edu		Saenko, Kate/0000-0002-7564-7218	NSF; DARPA	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This project was supported in part by NSF and DARPA.	Bansal A, 2018, LECT NOTES COMPUT SC, V11209, P122, DOI 10.1007/978-3-030-01228-1_8; Bashkirova D, 2018, ARXIV PREPRINT ARXIV; Chen Q., 2017, IEEE INT C COMP VIS, V1; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Chu C., 2017, ARXIV171202950; Efros A., 2017, GOOGLE AERIAL PHOTOS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Hoffman J, 2017, ARXIV171103213; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Karras T., 2017, PROGR GROWING GANS I; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim Taeksoo, 2017, P 34 INT C MACH LEAR, P1857, DOI [10.5555/3305381.3305573, DOI 10.5555/3305381.3305573]; Kurakin A., 2016, ARXIV PREPRINT ARXIV; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Madry Aleksander, 2017, ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Salimans T, 2016, ADV NEUR IN, V29; Sun X., 2018, ARXIV181201037; Tramer F., 2017, ARXIV; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Yan Z, 2018, ADV NEUR IN, V31; Zhou B., 2018, DONT LET YOUR DISCRI; Zhu Jun-Yan, 2017, ICCV	34	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300058
C	Belhadji, A; Bardenet, R; Chainais, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Belhadji, Ayoub; Bardenet, Remi; Chainais, Pierre			Kernel quadrature with DPPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study quadrature rules for functions from an RKHS, using nodes sampled from a determinantal point process (DPP). DPPs are parametrized by a kernel, and we use a truncated and saturated version of the RKHS kernel. This link between the two kernels, along with DPP machinery, leads to relatively tight bounds on the quadrature error, that depends on the spectrum of the RKHS kernel. Finally, we experimentally compare DPPs to existing kernel-based quadratures such as herding, Bayesian quadrature, or leverage score sampling. Numerical results confirm the interest of DPPs, and even suggest faster rates than our bounds in particular cases.	[Belhadji, Ayoub; Bardenet, Remi; Chainais, Pierre] Univ Lille, CNRS, UMR 9189, Cent Lille,CRIStAL, Villeneuve Dascq, France	Centre National de la Recherche Scientifique (CNRS); Universite de Lille - ISITE; Centrale Lille; Universite de Lille	Belhadji, A (corresponding author), Univ Lille, CNRS, UMR 9189, Cent Lille,CRIStAL, Villeneuve Dascq, France.	ayoub.belhadji@univ-lile.fr; remi.bardenet@univ-lile.fr; pierre.chainais@univ-lile.fr			ANR grant BoB [ANR-16-CE23-0003]; region Hauts-de-France	ANR grant BoB(French National Research Agency (ANR)); region Hauts-de-France(Region Hauts-de-France)	We acknowledge support from ANR grant BoB (ANR-16-CE23-0003) and region Hauts-de-France. We also thank Adrien Hardy and the reviewers for their detailed and insightful comments.	Bach F., 2012, P 29 INT C INT C MAC, P1355; Bach Francis R., 2017, J MACHINE LEARNING R, V18, P714; Bardenet R., 2016, ARXIV160500361; Belhadji A., 2018, ARXIV181209771; Berlinet A., 2011, REPRODUCING KERNEL H; BOJANOV BD, 1981, MATH COMPUT, V36, P525, DOI 10.1090/S0025-5718-1981-0606511-4; Briol FX, 2015, ADV NEUR IN, V28; Briol FX, 2019, STAT SCI, V34, P1, DOI 10.1214/18-STS660; Chen YH, 2010, ADV INTEL SOFT COMPU, V66, P109, DOI 10.1145/1866919.1866935; Davis P.J., 2007, METHODS NUMERICAL IN; Dick J., 2010, DIGITAL NETS SEQUENC, DOI 10.1017/CBO9780511761188; Dick J, 2014, LECT NOTES MATH, V2107, P539, DOI 10.1007/978-3-319-04696-9_9; Dumitriu I, 2002, J MATH PHYS, V43, P5830, DOI 10.1063/1.1507823; HALTON JH, 1964, COMMUN ACM, V7, P701, DOI 10.1145/355588.365104; Hinrichs A, 2016, SPRINGER P MATH STAT, V163, P385, DOI 10.1007/978-3-319-33507-0_19; Holtz M., 2008, THESIS; Hough J. B, 2006, PROBAB SURV; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Jagadeeswaran R., 2018, ARXIV180909803; Johansson K, 1997, ANN MATH, V145, P519, DOI 10.2307/2951843; Johansson Kurt, 2005, MATH EMATICAL STAT P, P1; Karvonen T., 2017, P IEEE 27 INT WORKSH, P1; Karvonen T., 2018, ARXIV180910227; Killip R, 2004, INT MATH RES NOTICES, V2004, P2665, DOI 10.1155/S1073792804141597; Larkin F. M., 1972, ROCKY MT J MATH, V2, P379; Liu Q., 2017, INT C ART INT STAT A; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Minka T, 2000, TECHNICAL REPORT; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Oettershagen J., 2017, THESIS U BONN; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Rasmussen C., 2006, GAUSSIANPROCESSES MA; Rasmussen CE., 2003, P 15 INT C NEUR INF, V15, P489; Robert C., 2007, BAYESIAN CHOICE DECI; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Simon B., 2005, TRACE IDEALTHEIR A, V2; SMOLIAK SA, 1963, DOKL AKAD NAUK SSSR+, V148, P1042; Soshnikov A, 2000, RUSS MATH SURV+, V55, P923, DOI 10.1070/RM2000v055n05ABEH000321; Steinwart I, 2012, CONSTR APPROX, V35, P363, DOI 10.1007/s00365-012-9153-3; Wahba G., 1990, SPLINE MODELS OBSERV, V59, DOI [10.1137/1.9781611970128, DOI 10.1137/1.9781611970128]; Wendland H., 2004, SCATTERED DATA APPRO, V17	47	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904055
C	Boursier, E; Perchet, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Boursier, Etienne; Perchet, Vianney			SIC - MMAB: Synchronisation Involves Communication in Multiplayer Multi-Armed Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRET	Motivated by cognitive radio networks, we consider the stochastic multiplayer multi-armed bandit problem, where several players pull arms simultaneously and collisions occur if one of them is pulled by several players at the same stage. We present a decentralized algorithm that achieves the same performance as a centralized one, contradicting the existing lower bounds for that problem. This is possible by "hacking" the standard model by constructing a communication protocol between players that deliberately enforces collisions, allowing them to share their information at a negligible cost. This motivates the introduction of a more appropriate dynamic setting without sensing, where similar communication protocols are no longer possible. However, we show that the logarithmic growth of the regret is still achievable for this model with a new algorithm.	[Boursier, Etienne; Perchet, Vianney] ENS Paris Saclay, CMLA, Cachan, France; [Perchet, Vianney] Criteo AI Lab, Paris, France	UDICE-French Research Universities; Universite Paris Saclay	Boursier, E (corresponding author), ENS Paris Saclay, CMLA, Cachan, France.	etienne.boursier@ens-paris-saclay.fr; vianney.perchet@normalesup.org			Investissement d'avenir project [ANR-11-LABX-0056-LMH]	Investissement d'avenir project(French National Research Agency (ANR))	This work was supported in part by a public grant as part of the Investissement d'avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH, in a joint call with Gaspard Monge Program for optimization, operations research and their interactions with data sciences.	AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934; Anandkumar A, 2011, IEEE J SEL AREA COMM, V29, P731, DOI 10.1109/JSAC.2011.110406; ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Avner Orly, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P66, DOI 10.1007/978-3-662-44848-9_5; Avner O., 2015, ARXIV150408167; Avner Orly, 2018, ARXIV180804875; Besson L., 2018, ALGORITHMIC LEARNING; Bistritz I., 2018, ADV NEURAL INFORM PR, V31, P7222; Boursier E., 2019, ARXIV190201239; Bubeck S., 2019, ARXIV190412233; Bubeck S., 2013, INT C MACHINE LEARNI, P258; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Degenne R., 2016, ICML JMLR WORKSH C P, P1587; Joshi H., 2018, P IEEE WIR COMM NETW P IEEE WIR COMM NETW, P1; Jouini W., 2009, 2009 3 INT C SIGN CI; Kalathil D, 2014, IEEE T INFORM THEORY, V60, P2331, DOI 10.1109/TIT.2014.2302471; Komiyama J, 2015, PR MACH LEARN RES, V37, P1152; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Liu KQ, 2010, IEEE T SIGNAL PROCES, V58, P5667, DOI 10.1109/TSP.2010.2062509; Lugosi G., 2018, ARXIV180808416; Perchet Vianney, 2015, COLT, P1456; Proutiere A., 2019, OPTIMAL ALGORITHM MU; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Rosenski J, 2016, PR MACH LEARN RES, V48; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tibrewal H, 2019, IEEE INFOCOM SER, P1693, DOI 10.1109/INFOCOM.2019.8737653	28	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903066
C	Bubeck, S; Jiang, QJ; Lee, YT; Li, YZ; Sidford, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bubeck, Sebastien; Jiang, Qijia; Lee, Yin Tat; Li, Yuanzhi; Sidford, Aaron			Complexity of Highly Parallel Non-Smooth Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A landmark result of non-smooth convex optimization is that gradient descent is an optimal algorithm whenever the number of computed gradients is smaller than the dimension d. In this paper we study the extension of this result to the parallel optimization setting. Namely we consider optimization algorithms interacting with a highly parallel gradient oracle, that is one that can answer poly(d) gradient queries in parallel. We show that in this case gradient descent is optimal only up to (O) over tilde(root d) rounds of interactions with the oracle. The lower bound improves upon a decades old construction by Nemirovski which proves optimality only up to d(1/3) rounds (as recently observed by Balkanski and Singer), and the suboptimality of gradient descent after root d rounds was already observed by Duchi, Bartlett and Wainwright. In the latter regime we propose a new method with improved complexity, which we conjecture to be optimal. The analysis of this new method is based upon a generalized version of the recent results on optimal acceleration for highly smooth convex optimization.	[Bubeck, Sebastien; Lee, Yin Tat] Microsoft Res, Redmond, WA 98052 USA; [Jiang, Qijia; Li, Yuanzhi; Sidford, Aaron] Stanford Univ, Stanford, CA 94305 USA; [Lee, Yin Tat] Univ Washington, Seattle, WA 98195 USA	Microsoft; Stanford University; University of Washington; University of Washington Seattle	Bubeck, S (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	sebubeck@microsoft.com; qjiang2@stanford.edu; yintat@uw.edu; yuanzhil@stanford.edu; sidford@stanford.edu	Li, Yuan/GXV-1310-2022		NSF [CCF-1740551, CCF-1749609, DMS-1839116]; NSF CAREER Award [CCF-1844855]	NSF(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	The authors thank the anonymous reviewers for their helpful feedback in preparing this final version. Further, the authors are grateful for multiple funding sources which supported this work in part, including NSF Awards CCF-1740551, CCF-1749609, and DMS-1839116 and NSF CAREER Award CCF-1844855.	[Anonymous], 2015, FOUND TRENDS MACH LE, V8, P232, DOI 10.1561/2200000050; [Anonymous], 2018, P ADV NEUR INF PROC; Balkanski Eric, 2018, ABS180803880 CORR; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Bubeck S., 2018, ARXIV181208026; Dekel O, 2012, J MACH LEARN RES, V13, P165; Diakonikolas J., 2018, ARXIV181101903; Duchi J. C., 2018, P 31 C LEARNING THEO, P3065; Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659; Frostig Roy, P 32 INT C MACH LEAR; Gasnikov A., 2018, ARXIV180900382; Jiang B., 2018, ARXIV181206557; Laurent B, 2000, ANN STAT, V28, P1302; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Monteiro RDC, 2013, SIAM J OPTIMIZ, V23, P1092, DOI 10.1137/110833786; Nesterov Y., 2018, APPL OPTIMIZATION; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; Yudin David B, 1976, EKONOMIKA MATEMATICH, V12, P128; Zhang YC, 2018, LECT NOTES MATH, V2227, P289, DOI 10.1007/978-3-319-97478-1_11	22	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905056
C	Chen, RTQ; Duvenaud, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Ricky T. Q.; Duvenaud, David			Neural Networks with Cheap Differential Operators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIFFUSION-PROCESSES; TIME	Gradients of neural networks can be computed efficiently for any architecture, but some applications require differential operators with higher time complexity. We describe a family of restricted neural network architectures that allow efficient computation of a family of differential operators involving dimension-wise derivatives, used in cases such as computing the divergence. Our proposed architecture has a Jacobian matrix composed of diagonal and hollow (non-diagonal) components. We can then modify the backward computation graph to extract dimension-wise derivatives efficiently with automatic differentiation. We demonstrate these cheap differential operators for solving root-finding subproblems in implicit ODE solvers, exact density evaluation for continuous normalizing flows, and evaluating the Fokker-Planck equation for training stochastic differential equation models.	[Chen, Ricky T. Q.; Duvenaud, David] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	Chen, RTQ (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	rtqichen@cs.toronto.edu; duvenaud@cs.toronto.edu	Chen, Ricky Tian Qi/AAS-3168-2021					Abadi M, 2015, P 12 USENIX S OPERAT; Ait-Sahalia Y, 1996, REV FINANC STUD, V9, P385, DOI 10.1093/rfs/9.2.385; Behrmann J., 2018, ARXIV181100995; Berg R. v. d., 2018, P UNC ART INT, P1; Burda Yuri, 2015, ARXIV150900519; Chen T.Q., 2018, NEURIPS, P2610; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Florens-Zmirou D., 1989, STATISTICS-ABINGDON, V20, P547, DOI [10.1080/02331888908802205, DOI 10.1080/02331888908802205]; Fokker AD, 1914, ANN PHYS-BERLIN, V43, P810; Germain M, 2015, PR MACH LEARN RES, V37, P881; Grathwohl W., 2019, P INT C LEARN REPR; Griewank A, 2008, OTHER TITL APPL MATH, V105, P1, DOI 10.1137/1.9780898717761; Hairer, 1987, SOLVING ORDINARY DIF, VI; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Huang CW, 2018, PR MACH LEARN RES, V80; HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866; Iacus Stefano M, 2011, OPTION PRICING ESTIM; Jeisman Joseph Ian, 2006, THESIS; Kessler M, 1997, SCAND J STAT, V24, P211, DOI 10.1111/1467-9469.00059; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lagaris IE, 1998, IEEE T NEURAL NETWOR, V9, P987, DOI 10.1109/72.712178; Le, 2017, ARXIV PREPRINT ARXIV; Long Z., 2017, ARXIV171009668; Martens J, 2012, ARXIV12066464; Moulton FR., 1926, NEW METHODS EXTERIOR; Oksendal B., 1998, STOCHASTIC DIFFERENT, DOI [DOI 10.1007/978-3-642-14394-6, 10.1007/978-3-642-14394-6]; OZAKI T, 1992, STAT SINICA, V2, P113; Papamakarios George, 2017, ARXIV170507057; Paszke A., 2017, AUTOMATIC DIFFERENTI; Planck Max, 1917, SATZ STAT DYNAMIK SE; Prakasa Rao BLS, 1999, STAT INFERENCE DIFFU, P8; Pritsker M, 1998, REV FINANC STUD, V11, P449, DOI 10.1093/rfs/11.3.449; Radhakrishnan Krishnan, 1993, DESCRIPTION USE LSOD; Raissi M, 2018, J MACH LEARN RES, V19; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Risken H., 1996, FOKKER PLANCK EQUATI, DOI 10.1007/978-3-642-61544-3_4; Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870; SHAMPINE LF, 1986, MATH COMPUT, V46, P135, DOI 10.1090/S0025-5718-1986-0815836-3; SKILLING J, 1989, FUND THEOR, V36, P455; Sorensen H, 2004, INT STAT REV, V72, P337; van den Oord A, 2016, PR MACH LEARN RES, V48; YOSHIDA N, 1992, J MULTIVARIATE ANAL, V41, P220, DOI 10.1016/0047-259X(92)90068-Q	47	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901057
C	Corbiere, C; Thome, N; Bar-Hen, A; Cord, M; Perez, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Corbiere, Charles; Thome, Nicolas; Bar-Hen, Avner; Cord, Matthieu; Perez, Patrick			Addressing Failure Prediction by Learning Model Confidence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Assessing reliably the confidence of a deep neural network and predicting its failures is of primary importance for the practical deployment of these models. In this paper, we propose a new target criterion for model confidence, corresponding to the True Class Probability (TCP). We show how using the TCP is more suited than relying on the classic Maximum Class Probability (MCP). We provide in addition theoretical guarantees for TCP in the context of failure prediction. Since the true class is by essence unknown at test time, we propose to learn TCP criterion on the training set, introducing a specific learning scheme adapted to this context. Extensive experiments are conducted for validating the relevance of the proposed approach. We study various network architectures, small and large scale datasets for image classification and semantic segmentation. We show that our approach consistently outperforms several strong methods, from MCP to Bayesian uncertainty, as well as recent approaches specifically designed for failure prediction.	[Corbiere, Charles; Thome, Nicolas; Bar-Hen, Avner] Conservatoire Natl Arts & Metiers, CEDRIC, Paris, France; [Corbiere, Charles; Cord, Matthieu; Perez, Patrick] Valeo Ai, Paris, France; [Cord, Matthieu] Sorbonne Univ, Paris, France	heSam Universite; Conservatoire National Arts & Metiers (CNAM); Institut Polytechnique de Paris; UDICE-French Research Universities; Sorbonne Universite	Corbiere, C (corresponding author), Conservatoire Natl Arts & Metiers, CEDRIC, Paris, France.; Corbiere, C (corresponding author), Valeo Ai, Paris, France.	charles.corbiere@valeo.com; nicolas.thome@cnam.fr; avner@cnam.fr; matthieu.cord@lip6.fr; patrick.perez@valeo.com		Corbiere, Charles/0000-0001-8024-7553				Amodei D., 2016, CONCRETE PROBLEMS AI; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Beyer Kevin, 1999, ICDT, P5; Blatz John, 2004, COLING, P5; Blundell Charles, 2015, ICML, P5; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Burges, 1998, MNIST DATABASE HANDW; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; DeVries Terrance, 2018, ARXIV180204865; Durand T., 2015, ICCV; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Gal Y., 2016, THESIS, V1, P3; Gal Y, 2016, PR MACH LEARN RES, V48; Geifman Yonatan, 2017, NIPS; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; Han L., 2019, P IEEE C COMP VIS PA, P99; Hannun A.Y., 2014, ARXIV14125567, P1; Hecker Simon, 2018, FAILURE PREDICTION A; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Jiang Heinrich, 2018, NIPS; Kendall A, 2015, P BRIT MACH VIS C 20; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEUR IN, V30; Lee Kimin, 2018, ICLR; Li H., 2018, IEEE INT C AC SPEECH; Liang Shiyu, 2018, INT C LEARN REPR; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Linda O, 2009, C HUM SYST INTERACT, P1; Liu WF, 2016, IMMUNOL INVEST, V45, P813, DOI 10.1080/08820139.2016.1186690; Mikolov T., 2013, ARXIV; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mohapatra P, 2018, PROC CVPR IEEE, P3693, DOI 10.1109/CVPR.2018.00389; Mordan T, 2018, ADV NEUR IN, V31; Mordan T, 2019, INT J COMPUT VISION, V127, P1659, DOI 10.1007/s11263-018-1109-z; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Neumann L., 2018, NIPS WORKSH; Ragni A., 2018, SLT WORKSH; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Vu TH, 2019, PROC CVPR IEEE, P2512, DOI 10.1109/CVPR.2019.00262	45	7	7	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302085
C	Elsayed, GF; Kornblith, S; Le, QV		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Elsayed, Gamaleldin F.; Kornblith, Simon; Le, Quoc V.			Saccader: Improving Accuracy of Hard Attention Models for Vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DECISIONS; NETWORKS	Although deep convolutional neural networks achieve state-of-the-art performance across nearly all image classification tasks, their decisions are difficult to interpret. One approach that offers some level of interpretability by design is hard attention, which uses only relevant portions of the image. However, training hard attention models with only class label supervision is challenging, and hard attention has proved difficult to scale to complex datasets. Here, we propose a novel hard attention model, which we term Saccader. Key to Saccader is a pretraining step that requires only class labels and provides initial attention locations for policy gradient optimization. Our best models narrow the gap to common ImageNet baselines, achieving 75% top-1 and 91% top-5 while attending to less than one-third of the image.	[Elsayed, Gamaleldin F.; Kornblith, Simon; Le, Quoc V.] Google Res, Brain Team, Mountain View, CA 94043 USA	Google Incorporated	Elsayed, GF (corresponding author), Google Res, Brain Team, Mountain View, CA 94043 USA.	gamaleldin@google.com						Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; AHMAD S, 1992, ADV NEUR IN, V4, P420; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2016, ADV NEUR IN, V29; Ashish V., 2019, P ADV NEURAL INFORM, V4, P5998; Athalye A, 2018, PR MACH LEARN RES, V80; Ba J., 2014, ARXIV; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Ballard D. H., 1989, IJCAI-89 Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, P1635; Ballard D. H., 1988, TECHNICAL REPORT; Brendel Wieland, 2019, INT C LEARN REPR; Burt P. J., 1988, 9th International Conference on Pattern Recognition (IEEE Cat. No.88CH2614-6), P977, DOI 10.1109/ICPR.1988.28419; Butko NJ, 2008, INT C DEVEL LEARN, P139, DOI 10.1109/DEVLRN.2008.4640819; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Eslami SM, 2016, NEURIPS, V1; Fukui H, 2019, PROC CVPR IEEE, P10697, DOI 10.1109/CVPR.2019.01096; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; Goodfellow I., 2017, OPENAI; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hooker S., 2018, CORR; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jain Sarthak, 2019, ARXIV190210186, DOI [10.18653/v1/N19-1357, DOI 10.18653/V1/N19-1357]; Jetley Saumya, 2018, INT C LEARN REPR; KANOPOULOS N, 1988, IEEE J SOLID-ST CIRC, V23, P358, DOI 10.1109/4.996; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Linsley, 2019, P 7 INT C LEARN REPR; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; Liu R, 2018, ADV NEUR IN, V31; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Luo Y., 2015, ARXIV PREPRINT ARXIV, DOI [10.48550/arXiv.1511.06292, DOI 10.48550/ARXIV.1511.06292]; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Madry Aleksander, 2017, ARXIV; Mnih V, 2014, ADV NEUR IN, V27; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700; Papernot N., 2018, ARXIV161000768; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Rudin C, 2019, NAT MACH INTELL, V1, P206, DOI 10.1038/s42256-019-0048-x; Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X; SCHMIDHUBER J, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P315; Schmidhuber J., 1990, LEARNING GENERATE FO; Schutz AC, 2011, J VISION, V11, DOI 10.1167/11.14.21; SEIBERT M, 1989, NEURAL NETWORKS, V2, P9, DOI 10.1016/0893-6080(89)90012-9; Sermanet P., 2015, INT C LEARN REPR ICL; SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632; Springenberg J.T., 2014, ARXIV14126806; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Uesato J, 2018, PR MACH LEARN RES, V80; Uijlings JRR, 2013, INT J COMPUT VISION, V104, P154, DOI 10.1007/s11263-013-0620-5; Wandell B.A., 1995, FDN VISION; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yosinski J., 2015, ICML DEEP LEARN WORK; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zheng Y, 2015, INT J COMPUT VISION, V113, P67, DOI 10.1007/s11263-014-0765-x; Zhou BL, 2013, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2013.392; Zintgraf Luisa M., 2017, P ICLR; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	76	7	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300064
C	Feather, J; Durango, A; Gonzalez, R; McDermott, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Feather, Jenelle; Durango, Alex; Gonzalez, Ray; McDermott, Josh			Metamers of neural networks reveal divergence from human perceptual systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	Deep neural networks have been embraced as models of sensory systems, instantiating representational transformations that appear to resemble those in the visual and auditory systems. To more thoroughly investigate their similarity to biological systems, we synthesized model metamers - stimuli that produce the same responses at some stage of a network's representation. We generated model metamers for natural stimuli by performing gradient descent on a noise signal, matching the responses of individual layers of image and audio networks to a natural image or speech signal. The resulting signals reflect the invariances instantiated in the network up to the matched layer. We then measured whether model metamers were recognizable to human observers a necessary condition for the model representations to replicate those of humans. Although model metamers from early network layers were recognizable to humans, those from deeper layers were not. Auditory model metamers became more human-recognizable with architectural modifications that reduced aliasing from pooling operations, but those from the deepest layers remained unrecognizable. We also used the metamer test to compare model representations. Cross-model metamer recognition dropped off for deeper layers, roughly at the same point that human recognition deteriorated, indicating divergence across model representations. The results reveal discrepancies between model and human representations, but also show how metamers can help guide model refinement and elucidate model representations.	[Feather, Jenelle; Durango, Alex; Gonzalez, Ray; McDermott, Josh] MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA; [Feather, Jenelle; Durango, Alex; Gonzalez, Ray; McDermott, Josh] MIT, McGovern Inst, Cambridge, MA 02139 USA; [Feather, Jenelle; Durango, Alex; Gonzalez, Ray; McDermott, Josh] MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA; [McDermott, Josh] Harvard Univ, Speech & Hearing Biosci & Technol, Cambridge, MA 02138 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Harvard University	Feather, J (corresponding author), MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA.; Feather, J (corresponding author), MIT, McGovern Inst, Cambridge, MA 02139 USA.; Feather, J (corresponding author), MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA.	jfeather@mit.edu; durangoa@mit.edu; raygon@mit.edu; jhm@mit.edu			McDonnell Scholar Award; NSF [BCS-1634050]; NIH [R01-DC017970]; DOE CSGF Fellowship	McDonnell Scholar Award; NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); DOE CSGF Fellowship(United States Department of Energy (DOE))	We thank Richard McWalter, Alex Kell, and Sam Norman-Haignere for comments on an early draft of this work. We also thank Mark Saddler and Andrew Francl for contributing to a shared codebase used in this project. This work was funded by a McDonnell Scholar Award to J.H.M., NSF grant BCS-1634050, NIH grant R01-DC017970 and a DOE CSGF Fellowship to J.J.F.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Azulay A., 2018, WHY DO DEEP CONVOLUT; Balas B, 2009, J VISION, V9, DOI [10.1167/9.2.16, 10.1167/9.12.13]; Barrett DGT, 2019, CURR OPIN NEUROBIOL, V55, P55, DOI 10.1016/j.conb.2019.01.007; Berardino Alexander, 2017, ADV NEURAL INFORM PR, P3530; Carlini M, 2018, UPDATES SURG SER, P1, DOI 10.1007/978-88-470-3955-1; Deza Arturo, 2017, INT C LEARN REPR; Feather Jenelle, 2018, 2018 C COGNITIVE COM; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; Geirhos R., 2018, ADV NEURAL INFORM PR, P7538; Geirhos Robert, 2019, INT C LEARN REPR ICL; Gemmeke J. F., 2017, P IEEE ICASSP 2017 N; GLASBERG BR, 1990, HEARING RES, V47, P103, DOI 10.1016/0378-5955(90)90170-T; Goodfellow Ian J, 2014, 2 INT C LEARN REPR I; Hannun A., 2014, ARXIV14125567; He K., 2016, ECCV; Henaff Olivier J, 2016, INT C LEARN REPR; Hershey Shawn, 2017, INT C AC SPEECH SIGN; Jacobsen Jorn-Henrik, 2019, INT C LEARN REPR ICL; Julesz B., 1962, IRE T INFORM THEOR, V8, P84, DOI 10.1109/TIT.1962.1057698; Kell AJE, 2019, CURR OPIN NEUROBIOL, V55, P121, DOI 10.1016/j.conb.2019.02.003; Kell AJE, 2018, NEURON, V98, P630, DOI 10.1016/j.neuron.2018.03.044; Kheradpisheh SR, 2016, SCI REP-UK, V6, DOI 10.1038/srep32672; Kingma D.P., 2015, INT C LEARN REPR ICL; Kohn A., 2016, P 10 INT C LANG RES; Kornblith Simon, 2019, INT C MACH LEARN; Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447; Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008; Li YB, 2015, 2015 INTERNATIONAL CONFERENCE ON FIELD PROGRAMMABLE TECHNOLOGY (FPT), P196, DOI 10.1109/FPT.2015.7393149; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094; McDermott JH, 2011, NEURON, V71, P926, DOI 10.1016/j.neuron.2011.06.032; Morcos A., 2018, ADV NEURAL INFORM PR, P5727; Mordvintsev A., 2015, INCEPTIONISM GOING D; Norman-Haignere SV, 2018, PLOS BIOL, V16, DOI 10.1371/journal.pbio.2005127; Olah C., 2017, DISTILL; PAUL DB, 1992, SPEECH AND NATURAL LANGUAGE, P357; Raghu M., 2017, ADV NEURAL INFORM PR, V30, P6076; Rajalingham R, 2015, J NEUROSCI, V35, P12127, DOI 10.1523/JNEUROSCI.0573-15.2015; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Wallis TSA, 2019, ELIFE, V8, DOI 10.7554/eLife.42512; Wallis TSA, 2017, J VISION, V17, DOI 10.1167/17.12.5; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yosinski Jason, 2015, INT C MACH LEARN DEE; Zhang Richard, 2019, INT C MACH LEARN; Zue V. W., 1996, Recent research towards advanced man-machine interface through spoken language, P515, DOI 10.1016/B978-044481607-8/50088-8	47	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901068
C	Gao, ZJ; Han, YJ; Ren, ZM; Zhou, ZQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gao, Zijun; Han, Yanjun; Ren, Zhimei; Zhou, Zhengqing			Batched Multi-armed Bandits Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRET BOUNDS; POLICIES	In this paper, we study the multi-armed bandit problem in the batched setting where the employed policy must split data into a small number of batches. While the minimax regret for the two-armed stochastic bandits has been completely characterized in [PRCS16], the effect of the number of arms on the regret for the multi-armed case is still open. Moreover, the question whether adaptively chosen batch sizes will help to reduce the regret also remains underexplored. In this paper, we propose the BaSE (batched successive elimination) policy to achieve the rate-optimal regrets (within logarithmic factors) for batched multi-armed bandits, with matching lower bounds even if the batch sizes are determined in an adaptive manner.	[Gao, Zijun; Han, Yanjun; Ren, Zhimei; Zhou, Zhengqing] Stanford Univ, Dept Stat, Elect Engn, Stat,Math, Stanford, CA 94305 USA	Stanford University	Gao, ZJ (corresponding author), Stanford Univ, Dept Stat, Elect Engn, Stat,Math, Stanford, CA 94305 USA.	zijungao@stanford.edu; yjhan@stanford.edu; zren@stanford.edu; zqzhou@stanford.edu		Gao, Zijun/0000-0003-4863-1656				Agarwal A, 2009, IMMUNE INFERTILITY, P155, DOI 10.1007/978-3-642-01379-9_3.2; ALON N, 1988, SIAM J DISCRETE MATH, V1, P269; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6; Bertsimas D, 2007, OPER RES, V55, P1120, DOI 10.1287/opre.1070.0427; BOLLOBAS B, 1983, DISCRETE APPL MATH, V6, P1, DOI 10.1016/0166-218X(83)90095-1; Braverman M, 2016, ACM S THEORY COMPUT, P851, DOI 10.1145/2897518.2897642; Bubeck S., 2013, P 26 ANN C LEARN THE, P122; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Cesa-Bianchi N., 2013, P 26 INT C NEUR INF, P1160; Cover TM, 2006, ELEMENTS INFORM THEO; Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066; Duchi J. C., 2018, P 31 C LEARNING THEO, P3065; Esfandiari Hossein, 2019, ARXIV191004959; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Jun KS, 2016, JMLR WORKSH CONF PRO, V51, P139; Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Melhem SB, 2017, 2017 IEEE 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2017), P32, DOI 10.1109/FiCloud.2017.37; Montgomery KH, 2009, IEEE NANOTECHNOL MAT, P217, DOI 10.1109/NMDC.2009.5167542; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Perchet V, 2016, ANN STAT, V44, P660, DOI 10.1214/15-AOS1381; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Shamir O., 2013, P C LEARN THEOR, P3; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Valiant L. G., 1975, SIAM Journal on Computing, V4, P348, DOI 10.1137/0204030; VOGEL W, 1960, ANN MATH STAT, V31, P430, DOI 10.1214/aoms/1177705906	35	7	7	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300046
C	Ghoshdastidar, D; Perrot, M; von Luxburg, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ghoshdastidar, Debarghya; Perrot, Michael; von Luxburg, Ulrike			Foundations of Comparison-Based Hierarchical Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We address the classical problem of hierarchical clustering, but in a framework where one does not have access to a representation of the objects or their pairwise similarities. Instead, we assume that only a set of comparisons between objects is available, that is, statements of the form "objects i and j are more similar than objects k and l." Such a scenario is commonly encountered in crowdsourcing applications. The focus of this work is to develop comparison-based hierarchical clustering algorithms that do not rely on the principles of ordinal embedding. We show that single and complete linkage are inherently comparison-based and we develop variants of average linkage. We provide statistical guarantees for the different methods under a planted hierarchical partition model. We also empirically demonstrate the performance of the proposed approaches on several datasets.	[Ghoshdastidar, Debarghya] Tech Univ Munich, Dept Informat, Munich, Germany; [Perrot, Michael; von Luxburg, Ulrike] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [von Luxburg, Ulrike] Univ Tubingen, Dept Comp Sci, Tubingen, Germany; [von Luxburg, Ulrike] Univ Tubingen, Tubingen, Germany	Technical University of Munich; Max Planck Society; Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen	Ghoshdastidar, D (corresponding author), Tech Univ Munich, Dept Informat, Munich, Germany.	ghoshdas@in.tum.de; michael.perrot@tuebingen.mpg.de; luxburg@informatik.uni-tuebingen.de			Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, DFG) [ZUK 63]; DFG Cluster of Excellence "Machine Learning -New Perspectives for Science" [EXC 2064/1, 390727645]; BMBF through the Tuebingen AI Center [FKZ: 01IS18039A]; Baden-Wurttemberg Eliteprogramm for Postdocs	Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, DFG)(German Research Foundation (DFG)); DFG Cluster of Excellence "Machine Learning -New Perspectives for Science"(German Research Foundation (DFG)); BMBF through the Tuebingen AI Center(Federal Ministry of Education & Research (BMBF)); Baden-Wurttemberg Eliteprogramm for Postdocs	This work has been supported by the Institutional Strategy of the University of Tubingen (Deutsche Forschungsgemeinschaft, DFG, ZUK 63), by the DFG Cluster of Excellence "Machine Learning -New Perspectives for Science", EXC 2064/1, project number 390727645, by the BMBF through the Tuebingen AI Center (FKZ: 01IS18039A), and by the Baden-Wurttemberg Eliteprogramm for Postdocs.	Agarwal Sameer, 2007, J MACHINE LEARNING R, P11; Amid E, 2015, PR MACH LEARN RES, V37, P1472; [Anonymous], 2017, ADV NEURAL INFORM PR; Arias-Castro E, 2017, BERNOULLI, V23, P1663, DOI 10.3150/15-BEJ792; Balakrishnan S., 2011, ADV NEURAL INFORM PR, P954; BORG I., 2005, MODERN MULTIDIMENSIO, P207; Chaudhuri K, 2014, IEEE T INFORM THEORY, V60, P7900, DOI 10.1109/TIT.2014.2361055; Chen Y., 2016, J MACH LEARN RES, V17, P1; Cohen-Addad V, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P378; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Emamjomeh-Zadeh E, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P415; Ghahramani Z., 2005, P 22 INT C MACH LEAR, P297, DOI DOI 10.1145/1102351.1102389; Haghiri S, 2018, INT C MACH LEARN, P1866; Haghiri S, 2017, PR MACH LEARN RES, V54, P851; HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840; Heikinheimo H., 2013, AAAI C HUM COMP CROW; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Jain Lalit, 2016, ADV NEURAL INFORM PR, V29, P2711; Jamieson Kevin G., 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1077; KAZEMI E, 2018, INTERNATIONAL CONFER, P1849; Kleindessner M., 2017, J MACHINE LEARNING R, V18, P1889; Kleindessner M., 2014, 27 C LEARNING THEORY, V35, P40; Kleindessner M, 2015, JMLR WORKSH CONF PRO, V38, P471; Leunis E., 2012, IEEE INT WORKSH MACH, P1, DOI DOI 10.1109/MLSP.2012.6349720; Moseley B., 2017, ADV NEURAL INFORM PR, P3097; Roy A., 2016, ADV NEURAL INFORM PR, P2316; SHEPARD RN, 1962, PSYCHOMETRIKA, V27, P125, DOI 10.1007/BF02289630; Stewart N, 2005, PSYCHOL REV, V112, P881, DOI 10.1037/0033-295X.112.4.881; Tamuz Omer, 2011, ARXIV11051033; Terada Y, 2014, PR MACH LEARN RES, V32, P847; Ukkonen A., 2017, ARXIV170908459; Ukkonen A, 2015, AAAI C HUM COMP CROW; Vikram S, 2016, PR MACH LEARN RES, V48; Young F.W., 1987, MULTIDIMENSIONAL SCA; Zhang L., 2015, ARXIV150301521	35	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307047
C	Glasser, I; Sweke, R; Pancotti, N; Eisert, J; Cirac, JI		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Glasser, Ivan; Sweke, Ryan; Pancotti, Nicola; Eisert, Jens; Cirac, J. Ignacio			Expressive power of tensor-network factorizations for probabilistic modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MATRIX PRODUCT STATES; RENORMALIZATION-GROUP; DECOMPOSITIONS; RANKS	Tensor-network techniques have recently proven useful in machine learning, both as a tool for the formulation of new learning algorithms and for enhancing the mathematical understanding of existing methods. Inspired by these developments, and the natural correspondence between tensor networks and probabilistic graphical models, we provide a rigorous analysis of the expressive power of various tensor-network factorizations of discrete multivariate probability distributions. These factorizations include non-negative tensor-trains/MPS, which are in correspondence with hidden Markov models, and Born machines, which are naturally related to the probabilistic interpretation of quantum circuits. When used to model probability distributions, they exhibit tractable likelihoods and admit efficient learning algorithms. Interestingly, we prove that there exist probability distributions for which there are unbounded separations between the resource requirements of some of these tensor-network factorizations. Of particular interest, using complex instead of real tensors can lead to an arbitrarily large reduction in the number of parameters of the network. Additionally, we introduce locally purified states (LPS), a new factorization inspired by techniques for the simulation of quantum systems, with provably better expressive power than all other representations considered. The ramifications of this result are explored through numerical experiments.	[Glasser, Ivan; Pancotti, Nicola; Cirac, J. Ignacio] Max Planck Inst Quantum Opt, D-85748 Garching, Germany; [Glasser, Ivan; Pancotti, Nicola; Cirac, J. Ignacio] Munich Ctr Quantum Sci & Technol MCQST, D-80799 Munich, Germany; [Sweke, Ryan; Eisert, Jens] Free Univ Berlin, Dahlem Ctr Complex Quantum Syst, D-14195 Berlin, Germany; [Eisert, Jens] Free Univ Berlin, Dept Math & Comp Sci, D-14195 Berlin, Germany	Max Planck Society; Free University of Berlin; Free University of Berlin	Glasser, I (corresponding author), Max Planck Inst Quantum Opt, D-85748 Garching, Germany.; Glasser, I (corresponding author), Munich Ctr Quantum Sci & Technol MCQST, D-80799 Munich, Germany.	ivan.glasser@mpq.mpg.de	Eisert, Jens/D-9640-2017	Eisert, Jens/0000-0003-3033-1292	ERC Advanced Grant QENOCOBA under the EU Horizon 2020 program [742102]; German Research Foundation (DFG) under Germany's Excellence Strategy [EXC-2111 - 390814868]; Alexander von Humboldt foundation; ExQM; German Research Foundation DFG [CRC 183, EI 519/7-1, CRC 1114, GRK 2433]; MATH+; European Union [817482]	ERC Advanced Grant QENOCOBA under the EU Horizon 2020 program; German Research Foundation (DFG) under Germany's Excellence Strategy(German Research Foundation (DFG)); Alexander von Humboldt foundation(Alexander von Humboldt Foundation); ExQM; German Research Foundation DFG(German Research Foundation (DFG)); MATH+; European Union(European Commission)	We would like to thank Vedran Dunjko for his comments on the manuscript and Joao Gouveia for his suggestion of the proof of Lemma 9 in the supplementary material. I. G., N. P. and J. I. C. are supported by an ERC Advanced Grant QENOCOBA under the EU Horizon 2020 program (grant agreement 742102) and the German Research Foundation (DFG) under Germany's Excellence Strategy through Project No. EXC-2111 - 390814868 (MCQST). R. S. acknowledges the financial support of the Alexander von Humboldt foundation. N. P. acknowledges financial support from ExQM. J. E. acknowledges financial support by the German Research Foundation DFG (CRC 183 project B2, EI 519/7-1, CRC 1114, GRK 2433) and MATH+. This work has also received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 817482 (PASQuanS).	Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Barthel T, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/7/073010; BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196; Blondel M, 2016, PR MACH LEARN RES, V48; Chen J, 2018, PHYS REV B, V97, DOI 10.1103/PhysRevB.97.085104; Cheng S, 2019, PHYS REV B, V99, DOI 10.1103/PhysRevB.99.155131; Cichocki A., 2017, MACH LEARN, V9, P431; Cichocki A, 2016, FOUND TRENDS MACH LE, V9, pI, DOI 10.1561/2200000059; COHEN JE, 1993, LINEAR ALGEBRA APPL, V190, P149, DOI 10.1016/0024-3795(93)90224-C; Cohen N., 2016, C LEARNING THEORY, V49, P698; Cohen N., 2017, ARXIV170502302; Cohen N, 2016, PR MACH LEARN RES, V48; Cohen Nadav, 2017, 5 INT C LEARN REPR I; Critch A, 2014, SYMMETRY INTEGR GEOM, V10, DOI 10.3842/SIGMA.2014.095; De las Cuevas G., 2019, ARXIV190703664; De las Cuevas G, 2013, NEW J PHYS, V15, DOI 10.1088/1367-2630/15/12/123021; Dua D., 2019, US; Eisert J., 2013, ARXIV PREPRINT ARXIV; Fawzi H, 2015, MATH PROGRAM, V153, P133, DOI 10.1007/s10107-015-0922-1; Frey B. J., 2003, UAI 03 P 19 C UNC AR, P257; Glasser I., 2018, ARXIV180605964; Glasser I, 2018, PHYS REV X, V8, DOI 10.1103/PhysRevX.8.011006; Goucha AP, 2017, SIAM J DISCRETE MATH, V31, P2612, DOI 10.1137/16M1105608; Gouveia J, 2013, MATH OPER RES, V38, P248, DOI 10.1287/moor.1120.0575; Grasedyck L, 2010, SIAM J MATRIX ANAL A, V31, P2029, DOI 10.1137/090764189; Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9; Han ZY, 2018, PHYS REV X, V8, DOI 10.1103/PhysRevX.8.031012; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Jaini P., 2018, ADV NEURAL INFORM PR, P7136; Khrulkov V., 2018, 6 INT C LEARN REPR I; Kliesch M, 2014, PHYS REV LETT, V113, DOI 10.1103/PhysRevLett.113.160503; Lee DD, 2001, ADV NEUR IN, V13, P556; Levine Yoav, 2018, 6 INT C LEARN REPR I; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Novikov A., 2017, 5 INT C LEARN REPR I; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; OSTLUND S, 1995, PHYS REV LETT, V75, P3537, DOI 10.1103/PhysRevLett.75.3537; Pestun V., 2017, ARXIV171101416; Robeva E., 2017, ARXIV171001437; Schollwock U, 2011, ANN PHYS-NEW YORK, V326, P96, DOI 10.1016/j.aop.2010.09.012; Schreiber J, 2018, J MACH LEARN RES, V18; Sedghi H, 2016, JMLR WORKSH CONF PRO, V51, P1223; Sharir O., 2016, ARXIV161004167; Shashua A., 2005, P 22 INT C MACHINE L, P792, DOI [10.1145/1102351.1102451, DOI 10.1145/1102351.1102451]; Shpilka A, 2009, FOUND TRENDS THEOR C, V5, P207, DOI 10.1561/0400000039; Stokes J., 2019, ARXIV190206888; University Medical Centre Institute of Oncology Ljubljana Yugoslavia, THIS LYMPHOGRAPHY TU; Verstraete F, 2008, ADV PHYS, V57, P143, DOI 10.1080/14789940801912366; Verstraete F, 2004, PHYS REV LETT, V93, DOI 10.1103/PhysRevLett.93.207204; Werner AH, 2016, PHYS REV LETT, V116, DOI 10.1103/PhysRevLett.116.237201; Yang CR, 2018, PHYS REV LETT, V121, DOI 10.1103/PhysRevLett.121.260602	61	7	7	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301048
C	Hao, BT; Abbasi-Yadkori, Y; Wen, Z; Cheng, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hao, Botao; Abbasi-Yadkori, Yasin; Wen, Zheng; Cheng, Guang			Bootstrapping Upper Confidence Bound	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUPREMA	Upper Confidence Bound (UCB) method is arguably the most celebrated one used in online decision making with partial information feedback. Existing techniques for constructing confidence bounds are typically built upon various concentration inequalities, which thus lead to over-exploration. In this paper, we propose a non-parametric and data-dependent UCB algorithm based on the multiplier bootstrap. To improve its finite sample performance, we further incorporate second-order correction into the above construction. In theory, we derive both problem-dependent and problem-independent regret bounds for multi-armed bandits with symmetric rewards under a much weaker tail assumption than the standard sub-Gaussianity. Numerical results demonstrate significant regret reductions by our method, in comparison with several baselines in a range of multi-armed and linear bandit problems.	[Hao, Botao; Cheng, Guang] Purdue Univ, W Lafayette, IN 47907 USA; [Abbasi-Yadkori, Yasin] VinAI, Brescia, Italy; [Wen, Zheng] Deepmind, London, England	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Hao, BT (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	haobotao000@gmail.com; yasin.abbasi@gmail.com; zhengwen@google.com; chengg@purdue.edu			NSF [DMS-1712907, DMS-1811812, DMS-1821183]; Office of Naval Research [ONR N00014-18-2759]	NSF(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research)	We thank Tor Lattimore for helpful discussions. Guang Cheng would like to acknowledge support by NSF DMS-1712907, DMS-1811812, DMS-1821183, and Office of Naval Research (ONR N00014-18-2759). In addition, Guang Cheng is a visiting member of Institute for Advanced Study, Princeton (funding provided by Eric and Wendy Schmidt) and visiting Fellow of SAMSI for the Deep Learning Program in the Fall of 2019; he would like to thank both Institutes for their hospitality.	AbbasiYadkori  Y., 2011, ADV NEURAL INFORM PR; Adamczak R, 2011, CONSTR APPROX, V34, P61, DOI 10.1007/s00365-010-9117-4; Agrawal S., 2013, ARTIF INTELL, P99; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Alon N., 2004, PROBABILISTIC METHOD; Arlot S, 2010, ANN STAT, V38, P51, DOI 10.1214/08-AOS667; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bastani H., 2017, ARXIV170409011; Bastani H, 2015, ONLINE DECISION MAKI; Bird Sarah, 2016, WORKSH FAIRN ACC TRA; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Bogucki R, 2015, STAT PROBABIL LETT, V107, P253, DOI 10.1016/j.spl.2015.09.002; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Casella G, 2002, STAT INFERENCE, V2; Chen X, 2019, ANN STAT; Chernozhukov V, 2014, ANN STAT, V42, P1564, DOI 10.1214/14-AOS1230; Dani V., 2008, STOCHASTIC LINEAR OP; De la Pena V., 2012, DECOUPLING DEPENDENC; Eckles D., 2014, ARXIV14104009; Efron B., 1982, JACKKNIFE BOOTSTRAP; Elmachtoub AN, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Hitczenko P, 1997, STUD MATH, V123, P15; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Korda N, 2013, ADV NEURAL INFORM PR, P1448; Kuchibhotla A.K, 2018, ARXIV PREPRINT ARXIV; Kveton Branislav, 2018, ARXIV181105154; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore Tor, 2018, J MACHINE LEARNING R, V19, P765; Ledoux M., 2013, PROBABILITY BANACH S, P86; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Matrana MR, 2018, SOUTHERN SPLENDOR: SAVING ARCHITECTURAL TREASURES OF THE OLD SOUTH, P1; Mnih V., 2008, P 25 INT C MACH LEAR; Romano JR, 2005, J AM STAT ASSOC, V100, P94, DOI 10.1198/016214504000000539; RUBIN DB, 1981, ANN STAT, V9, P130, DOI 10.1214/aos/1176345338; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Spokoiny V, 2015, ANN STAT, V43, P2653, DOI 10.1214/15-AOS1355; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; TALAGRAND M, 1994, AM J MATH, V116, P283, DOI 10.2307/2374931; Tang L, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P323, DOI 10.1145/2766462.2767707; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Vaswani S., 2018, ARXIV180509793; Vladimirova Mariia, 2019, ARXIV PREPRINT ARXIV; WU CFJ, 1986, ANN STAT, V14, P1261, DOI 10.1214/aos/1176350142; Yang Y., 2017, ARXIV170201330	49	7	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903073
C	Hu, B; Syed, UA		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Bin; Syed, Usman Ahmed			Characterizing the Exact Behaviors of Temporal Difference Learning Algorithms Using Markov Jump Linear System Theory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC STABILITY; PERTURBATION	In this paper, we provide a unified analysis of temporal difference learning algorithms with linear function approximators by exploiting their connections to Markov jump linear systems (MJLS). We tailor the MJLS theory developed in the control community to characterize the exact behaviors of the first and second order moments of a large family of temporal difference learning algorithms. For both the IID and Markov noise cases, we show that the evolution of some augmented versions of the mean and covariance matrix of the TD estimation error exactly follows the trajectory of a deterministic linear time-invariant (LTI) dynamical system. Applying the well-known LTI system theory, we obtain closed-form expressions for the mean and covariance matrix of the TD estimation error at any time step. We provide a tight matrix spectral radius condition to guarantee the convergence of the covariance matrix of the TD estimation error, and perform a perturbation analysis to characterize the dependence of the TD behaviors on learning rate. For the IID case, we provide an exact formula characterizing how the mean and covariance matrix of the TD estimation error converge to the steady state values at a linear rate. For the Markov case, we use our formulas to explain how the behaviors of TD learning algorithms are affected by learning rate and the underlying Markov chain. For both cases, upper and lower bounds for the mean square TD error are derived. An exact formula for the steady state mean square TD error is also provided.	[Hu, Bin; Syed, Usman Ahmed] Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Hu, B (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Urbana, IL 61801 USA.							ABOUKANDIL H, 1995, AUTOMATICA, V31, P765, DOI 10.1016/0005-1098(94)00164-E; Avrachenkov KE, 2013, LINEAR ALGEBRA APPL, V438, P1793, DOI 10.1016/j.laa.2011.10.037; Aybat N. S., 2018, ARXIV180510579; Aybat N. S., 2019, 33 C NEUR INF PROC S, P8525; Bertsekas Dimitri P, 1996, NEURODYNAMIC PROGRAM, V5; Bhandari Jalaj, 2018, C LEARN THEOR, P1691; Bhatnagar S., 2012, STOCHASTIC RECURSIVE, V434; Borkar V. S, 2009, STOCHASTIC APPROXIMA, V48; Chen C.-T., 1998, LINEAR SYSTEM THEORY; Chen Z., 2019, ARXIV190511425; CHIZECK HJ, 1986, INT J CONTROL, V43, P213, DOI 10.1080/00207178608933459; Costa O. L. V., 2006, DISCRETE TIME MARKOV; COSTA OLV, 1993, J MATH ANAL APPL, V179, P154, DOI 10.1006/jmaa.1993.1341; Cyrus S, 2018, P AMER CONTR CONF, P1376; Dalal G, 2018, AAAI CONF ARTIF INTE, P6144; Dhingra N., 2018, IEEE T AUTOMATIC CON; ElGhaoui L, 1996, INT J ROBUST NONLIN, V6, P1015, DOI 10.1002/(SICI)1099-1239(199611)6:9/10<1015::AID-RNC266>3.0.CO;2-0; Fang YG, 2002, IEEE T AUTOMAT CONTR, V47, P1204, DOI 10.1109/TAC.2002.800674; Fazlyab M., 2017, ARXIV170503615; Fazlyab M, 2017, ANN ALLERTON CONF, P354; FENG XB, 1992, IEEE T AUTOMAT CONTR, V37, P38, DOI 10.1109/9.109637; Gonzalez-Rodriguez P, 2015, J COMPUT PHYS, V299, P307, DOI 10.1016/j.jcp.2015.07.006; Han S., 2019, ARXIV190301023; Hespanha JP, 2009, LINEAR SYSTEMS THEORY, P1; Hu B., 2017, P 34 INT C MACH LEAR; Hu B, 2017, P AMER CONTR CONF, P3114, DOI 10.23919/ACC.2017.7963426; Hu B, 2018, SPRINGERBRIEFS EDUC, P43, DOI 10.1007/978-981-13-1147-5_4; JI Y, 1990, CONTR-THEOR ADV TECH, V6, P289; JI YD, 1988, INT J CONTROL, V48, P481, DOI 10.1080/00207178808906192; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Lakshminarayanan C., 2018, PROC INT C ARTIF INT, P1347; LEE D, 2019, ARXIV190410945; Lessard L., 2017, ARXIV171100987; LESSARD L, 2019, ARXIV190409046; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Moro J, 1997, SIAM J MATRIX ANAL A, V18, P793, DOI 10.1137/S0895479895294666; Nelson ZE, 2018, P AMER CONTR CONF, P597; Osabe K, 2013, IEEE INT SYMP ELEC, P138, DOI 10.1109/ISEMC.2013.6670397; Seiler P, 2003, IEEE T AUTOMAT CONTR, V48, P1651, DOI 10.1109/TAC.2003.817010; Shi M, 2017, EUR SIGNAL PR CONF, P1115, DOI 10.23919/EUSIPCO.2017.8081381; Srikant R., 2019, COLT; Sundararajan A, 2017, ANN ALLERTON CONF, P1206; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2008, ADV NEURAL INFORM PR, V21, P1609; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Van Scoy B., 2017, IEEE CONTROL SYSTEMS, V2, P49; Wang G., 2019, ARXIV190904299; Xu T., 2019, ADV NEURAL INFORM PR	54	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900011
C	Jerfel, G; Grant, E; Griffiths, TL; Heller, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jerfel, Ghassen; Grant, Erin; Griffiths, Thomas L.; Heller, Katherine			Reconciling meta-learning and continual learning with online mixtures of tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIATIONAL INFERENCE; MODEL	Learning-to-learn or meta-learning leverages data-driven inductive bias to increase the efficiency of learning on a novel task. This approach encounters difficulty when transfer is not advantageous, for instance, when tasks are considerably dissimilar or change over time. We use the connection between gradient-based meta-learning and hierarchical Bayes to propose a Dirichlet process mixture of hierarchical Bayesian models over the parameters of an arbitrary parametric model such as a neural network. In contrast to consolidating inductive biases into a single set of hyperparameters, our approach of task-dependent hyperparameter selection better handles latent distribution shift, as demonstrated on a set of evolving, image-based, few-shot learning benchmarks.	[Jerfel, Ghassen; Heller, Katherine] Duke Univ, Durham, NC 27706 USA; [Grant, Erin] Univ Calif Berkeley, Berkeley, CA USA; [Griffiths, Thomas L.] Princeton Univ, Princeton, NJ 08544 USA	Duke University; University of California System; University of California Berkeley; Princeton University	Jerfel, G (corresponding author), Duke Univ, Durham, NC 27706 USA.	gj47@duke.edu; eringrant@berkeley.edu; tomg@princeton.edu; kheller@stat.duke.edu		Griffiths, Thomas/0000-0002-5138-7255				Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2013, INT C MACH LEARN; [Anonymous], 2009, PROC AUAI 2009; Bakker B, 2004, J MACH LEARN RES, V4, P83, DOI 10.1162/153244304322765658; Bauer  M., 2017, ARXIV170600326; Baxter J, 1997, MACH LEARN, V28, P7, DOI 10.1023/A:1007327622663; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; BESAG J, 1986, J R STAT SOC B, V48, P259; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Caruana R., 1993, P 10 INT C MACH LEAR; Collins AGE, 2013, PSYCHOL REV, V120, P190, DOI 10.1037/a0030852; Deleu Tristan, 2018, ARXIV181202159; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Finn C., 2017, P 34 INT C MACH LEAR; Finn C., 2018, ADV NEURAL INFORM PR, P9516; Gao J., 2008, KDD, V2008, P283, DOI [10.1145/1401890.1401928, DOI 10.1145/1401890.1401928]; Gershman SJ, 2012, J MATH PSYCHOL, V56, P1, DOI 10.1016/j.jmp.2011.08.004; Ghahramani Z, 2000, ADV NEUR IN, V12, P449; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Gordon J., 2019, P 7 INT C LEARN REPR, P3915; Grant E., 2018, P 6 INT C LEARN REPR; Greff K., 2017, ADV NEURAL INFORM PR, P6691, DOI DOI 10.5555/3295222.3295414; Heskes T., 1998, SOLVING HUGE NUMBER; Hughes M.C., 2012, ADV NEURAL INF PROCE, V25, P1295; Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lawrence N.D., 2004, P 21 INT C MACH LEAR, P65, DOI DOI 10.1145/1015330.1015382; Lee Yoonho, 2018, ICML; Lin D., 2013, ADV NEURAL INFORM PR, P395; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; METROPOLIS N, 1949, J AM STAT ASSOC, V44, P335, DOI 10.2307/2280232; Muller P, 1998, NEURAL COMPUT, V10, P749, DOI 10.1162/089976698300017737; Neal Radford M, 2012, BAYESIAN LEARNING NE, V118; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Nguyen C. V., 2018, INT C LEARN REPR ICL; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Ravi Sachin, 2017, P 5 INT C LEARN REPR; Raykov YP, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0162259; Ritter H., 2018, ADV NEURAL INFORM PR, P3738; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rosenstein Michael T, 2005, NIPS 2005 WORKSH TRA, V898, P1; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Salakhutdinov R, 2013, IEEE T PATTERN ANAL, V35, P1958, DOI 10.1109/TPAMI.2012.269; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Sculley D., 2010, P 19 INT C WORLD WID, P1177, DOI [10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]; Snell J, 2017, ADV NEUR IN, V30; Srivastava Nitish, 2013, NIPS; Tank A, 2015, JMLR WORKSH CONF PRO, V38, P968; Thrun S., DISCOVERING STRUCTUR; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wan J, 2012, PROC CVPR IEEE, P940, DOI 10.1109/CVPR.2012.6247769; Welling M, 2006, SIAM PROC S, P474; Xue Y, 2007, J MACH LEARN RES, V8, P35; Yu K., 2005, P 22 INT C MACH LEAR, P1012, DOI DOI 10.1145/1102351.1102479; Zenke Friedemann, 2017, INT C MACH LEARN ICM; Zhang Yi, 2010, ADV NEURAL INF PROCE, P2550; Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028; Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424	61	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900068
C	Jiang, YH; Kim, H; Asnani, H; Kannan, S; Oh, S; Viswanath, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jiang, Yihan; Kim, Hyeji; Asnani, Himanshu; Kannan, Sreeram; Oh, Sewoong; Viswanath, Pramod			Turbo Autoencoder: Deep learning based channel codes for point-to-point communication channels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Designing codes that combat the noise in a communication medium has remained a significant area of research in information theory as well as wireless communications. Asymptotically optimal channel codes have been developed by mathematicians for communicating under canonical models after over 60 years of research. On the other hand, in many non-canonical channel settings, optimal codes do not exist and the codes designed for canonical models are adapted via heuristics to these channels and are thus not guaranteed to be optimal. In this work, we make significant progress on this problem by designing a fully end-to-end jointly trained neural encoder and decoder, namely, Turbo Autoencoder (TurboAE), with the following contributions: (a) under moderate block lengths, TurboAE approaches state-of-the-art performance under canonical channels; (b) moreover, TurboAE outperforms the state-of-the-art codes under non-canonical settings in terms of reliability. TurboAE shows that the development of channel coding design can be automated via deep learning, with near-optimal performance.	[Jiang, Yihan; Kannan, Sreeram] Univ Washington, ECE Dept, Seattle, WA 98195 USA; [Kim, Hyeji] Samsung AI Ctr Cambridge, Cambridge, England; [Asnani, Himanshu] Tata Inst Fundamental Res, Sch Technol & Comp Sci, Mumbai, Maharashtra, India; [Oh, Sewoong] Univ Washington, Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA; [Viswanath, Pramod] Univ Illinois, ECE Dept, Champaign, IL USA	University of Washington; University of Washington Seattle; Tata Institute of Fundamental Research (TIFR); University of Washington; University of Washington Seattle; University of Illinois System; University of Illinois Urbana-Champaign	Jiang, YH (corresponding author), Univ Washington, ECE Dept, Seattle, WA 98195 USA.	yij021@uw.edu; hkim1505@gmail.com; himanshu.asnani@tifr.res.in; ksreeram@ee.washington.edu; sewoong@cs.washington.edu; pramodv@illinois.edu		oh, sewoong/0000-0002-8975-8306	NSF [1908003, 651236, 1703403]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF awards 1908003, 651236 and 1703403.	[Anonymous], ARXIV180803242; Aoudia FA, 2018, CONF REC ASILOMAR C, P298, DOI 10.1109/ACSSC.2018.8645416; Arikan E, 2008, IEEE INT SYMP INFO, P1173, DOI 10.1109/ISIT.2008.4595172; BAHL LR, 1974, IEEE T INFORM THEORY, V20, P284, DOI 10.1109/TIT.1974.1055186; Bengio Yoshua, 2013, ARXIV13083432; BERROU C, 1993, IEEE INTERNATIONAL CONFERENCE ON COMMUNICATIONS 93 : TECHNICAL PROGRAM, CONFERENCE RECORD, VOLS 1-3, P1064, DOI 10.1109/ICC.1993.397441; Cammerer S., 2017, GLOBECOM 2017 2017 I, P1, DOI DOI 10.1109/GLOCOM.2017.8254811; Choi K., 2018, ARXIV181107557; Chung J., 2014, ARXIV14123555; Dorner S., 2017, ARXIV170703384; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Felix A, 2018, IEEE INT WORK SIGN P, P56; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gruber T., 2017, 2017 51 ANN C INF SC, P1; Hinton, 2016, ARXIV PREPRINT ARXIV; Hubara I, 2016, ADV NEUR IN, V29; Isa I, 2017, 2017 INTERNATIONAL CONFERENCE ON ELECTRICAL, ELECTRONICS AND SYSTEM ENGINEERING (ICEESE), P1; Jiang Y., 2019, ARXIV190302268; Jiang Y., 2019, DEEPTURBO DEEP TURBO; Jiang Y., 2018, ARXIV181112707; Kaiser S, 1997, GLOB TELECOMM CONF, P6, DOI 10.1109/GLOCOM.1997.632502; Kim H., 2018, INT C REPR LEARN ICL; Kim H, 2018, ADV NEUR IN, V31; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A., 2011, P EUR S ART NEUR NET, P1; Le Q.V., 2011, NEURIPS, P1017; Li J., 2013, OFDMA MOBILE BROADBA, DOI DOI 10.1017/CBO9780511736186; MacKay DJC, 1997, ELECTRON LETT, V33, P457, DOI 10.1049/el:19970362; Makhzani A, 2013, ARXIV PREPRINT ARXIV, P5663; Mohamed A-r, 2010, 11 ANN C INT SPEECH; Muller MK, 2018, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1238-7; Nachmani E, 2018, IEEE J-STSP, V12, P119, DOI 10.1109/JSTSP.2017.2788405; Nachmani E, 2016, ANN ALLERTON CONF, P341, DOI 10.1109/ALLERTON.2016.7852251; O'Shea TJ, 2016, IEEE INT SYMP SIGNAL, P223, DOI 10.1109/ISSPIT.2016.7886039; OShea T. J., 2017, ARXIV170200832; OShea T. J., 2017, CORR; Ovtcharov K., 2015, MICROSOFT RES WHITEP, V2, P1; Polyanskiy Y, 2010, IEEE T INFORM THEORY, V56, P2307, DOI 10.1109/TIT.2010.2043769; Richardson T., 2008, MODERN CODING THEORY; Sadjadpour HR, 2001, IEEE J SEL AREA COMM, V19, P831, DOI 10.1109/49.924867; Samuel N., 2017, IEEE INT WORKSHOP SI, P1; Santurkar S, 2018, ADV NEUR IN, V31; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; Tse D., 2005, FUNDAMENTALS WIRELES; Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558; Xu WH, 2017, 2017 IEEE INTERNATIONAL WORKSHOP ON SIGNAL PROCESSING SYSTEMS (SIPS); Yin Wenpeng, 2017, ARXIV170201923	48	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302072
C	Karakida, R; Akaho, S; Amari, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Karakida, Ryo; Akaho, Shotaro; Amari, Shun-ichi			The Normalization Method for Alleviating Pathological Sharpness in Wide Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Normalization methods play an important role in enhancing the performance of deep learning while their theoretical understandings have been limited. To theoretically elucidate the effectiveness of normalization, we quantify the geometry of the parameter space determined by the Fisher information matrix (FIM), which also corresponds to the local shape of the loss landscape under certain conditions. We analyze deep neural networks with random initialization, which is known to suffer from a pathologically sharp shape of the landscape when the network becomes sufficiently wide. We reveal that batch normalization in the last layer contributes to drastically decreasing such pathological sharpness if the width and sample number satisfy a specific condition. In contrast, it is hard for batch normalization in the middle hidden layers to alleviate pathological sharpness in many settings. We also found that layer normalization cannot alleviate pathological sharpness either. Thus, we can conclude that batch normalization in the last layer significantly contributes to decreasing the sharpness induced by the FIM.	[Karakida, Ryo] AIST, Tokyo, Japan; [Akaho, Shotaro] AIST, Ibaraki, Japan; [Amari, Shun-ichi] RIKEN CBS, Saitama, Japan	National Institute of Advanced Industrial Science & Technology (AIST); National Institute of Advanced Industrial Science & Technology (AIST); RIKEN	Karakida, R (corresponding author), AIST, Tokyo, Japan.	karakida.ryo@aist.go.jp; s.akaho@aist.go.jp; amari@brain.riken.jp			Japan Society for the Promotion of Science (JSPS) [19K20366]	Japan Society for the Promotion of Science (JSPS)(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	This work was partially supported by a Grant-in-Aid for Young Scientists (19K20366) from the Japan Society for the Promotion of Science (JSPS).	Amari S.-I., 2016, INFORM GEOMETRY ITS; AMARI SI, 1974, KYBERNETIK, V14, P201; [Anonymous], 2018, ADV NEURAL INFORM PR, DOI 10.5555/3327757.3327948; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Ba L.J, 2016, P C WORKSH NEUR INF; Bjorck Nils, 2018, ADV NEURAL INFORM PR, P7694; Ganguli S., 2018, PROC INT C ARTIF INT, P1924; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Karakida R., 2019, ARXIV, P1032; Keskar N.S., 2017, ARXIV160904836; Kunstner F., 2019, ARXIV190512558; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lee J., 2019, ARXIV190206720; Li H, 2018, ADV NEUR IN, V31; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Park Daniel, 2019, INT C MACH LEARN, P5042; Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4; Pascanu Razvan, 2014, ARXIV1301384; Pennington J., 2018, ADV NEURAL INFORM PR, V31, P5410; Poole B, 2016, ADV NEUR IN, V29; Sagun Levent, 2017, ARXIV170604454; Santurkar S., 2018, P ADV NEUR INF PROC; Schoenholz S. S., 2019, ARXIV190208129; Schoenholz Samuel S, 2017, ARXIV161101232; Sohl Dickstein J., 2018, ARXIV171100165; Wei Mingwei, 2019, ARXIV190302606; Xiao LC, 2018, PR MACH LEARN RES, V80; Yang G., 2017, ADV NEURAL INFORM PR, V30, P2865; Yang G., 2019, ARXIV190204760; Yao Z., 2018, ADV NEURAL INFORM PR	34	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306041
C	Karimi, B; Wai, HT; Moulines, E; Lavielle, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Karimi, Belhal; Wai, Hoi-To; Moulines, Eric; Lavielle, Marc			On the Global Convergence of (Fast) Incremental Expectation Maximization Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EM ALGORITHM	The EM algorithm is one of the most popular algorithm for inference in latent data models. The original formulation of the EM algorithm does not scale to large data set, because the whole data set is required at each iteration of the algorithm. To alleviate this problem, Neal and Hinton [1998] have proposed an incremental version of the EM (iEM) in which at each iteration the conditional expectation of the latent data (E-step) is updated only for a mini-batch of observations. Another approach has been proposed by Cappe and Moulines [2009] in which the E-step is replaced by a stochastic approximation step, closely related to stochastic gradient. In this paper, we analyze incremental and stochastic version of the EM algorithm as well as the variance reduced-version of [Chen et al., 2018] in a common unifying framework. We also introduce a new version incremental version, inspired by the SAGA algorithm by Defazio et al. [2014]. We establish non-asymptotic convergence bounds for global convergence. Numerical applications are presented in this article to illustrate our findings.	[Karimi, Belhal; Moulines, Eric] Ecole Polytech, CMAP, Palaiseau, France; [Wai, Hoi-To] Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China; [Lavielle, Marc] INRIA Saclay, Palaiseau, France	Institut Polytechnique de Paris; Chinese University of Hong Kong	Karimi, B (corresponding author), Ecole Polytech, CMAP, Palaiseau, France.	belhal.karimi@polytechnique.edu; htwai@se.cuhk.edu.hk; eric.moulines@polytechnique.edu; marc.lavielle@inria.fr			CUHK [4055113]	CUHK(Chinese University of Hong Kong)	BK and HTW contributed equally to this work. HTW's work is supported by the CUHK Direct Grant #4055113.	Ablin P., 2018, ARXIV180510054; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Cappe O, 2009, J ROY STAT SOC B, V71, P593, DOI 10.1111/j.1467-9868.2009.00698.x; Chen J., 2018, ADV NEURAL INFORM PR, P7978; Csiszar I., 1984, STAT DECISIONS, V1, P205; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gunawardana A, 2005, J MACH LEARN RES, V6, P2049; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Karimi B., 2019, C LEARN THEOR; McLachlan G., 2007, EM ALGORITHM EXTENSI, V382; Medelyan O., 2009, THESIS; Ng SK, 2003, STAT COMPUT, V13, P45, DOI 10.1023/A:1021987710829; Reddi S., 2016, ARXIV160306159; Reddi SJ, 2016, PR MACH LEARN RES, V48; Thiesson B, 2001, MACH LEARN, V45, P279, DOI 10.1023/A:1017986506241; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang Z., 2015, ADV NEURAL INFORM PR, P2521; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Xu Ji, 2016, ADV NEURAL INFORM PR, P2676; Zhu RD, 2017, PR MACH LEARN RES, V70	28	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302079
C	Keshtkaran, MR; Pandarinath, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Keshtkaran, Mohammad Reza; Pandarinath, Chethan			Enabling hyperparameter optimization in sequential autoencoders for spiking neural data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DYNAMICS	Continuing advances in neural interfaces have enabled simultaneous monitoring of spiking activity from hundreds to thousands of neurons. To interpret these large-scale data, several methods have been proposed to infer latent dynamic structure from high-dimensional datasets. One recent line of work uses recurrent neural networks in a sequential autoencoder (SAE) framework to uncover dynamics. SAEs are an appealing option for modeling nonlinear dynamical systems, and enable a precise link between neural activity and behavior on a single-trial basis. However, the very large parameter count and complexity of SAEs relative to other models has caused concern that SAEs may only perform well on very large training sets. We hypothesized that with a method to systematically optimize hyperparameters (HPs), SAEs might perform well even in cases of limited training data. Such a breakthrough would greatly extend their applicability. However, we find that SAEs applied to spiking neural data are prone to a particular form of overfitting that cannot be detected using standard validation metrics, which prevents standard HP searches. We develop and test two potential solutions: an alternate validation method ("sample validation") and a novel regularization method ("coordinated dropout"). These innovations prevent overfitting quite effectively, and allow us to test whether SAEs can achieve good performance on limited data through largescale HP optimization. When applied to data from motor cortex recorded while monkeys made reaches in various directions, large-scale HP optimization allowed SAEs to better maintain performance for small dataset sizes. Our results should greatly extend the applicability of SAEs in extracting latent dynamics from sparse, multidimensional data, such as neural population spiking activity.	[Keshtkaran, Mohammad Reza] Emory Univ, Coulter Dept Biomed Engn, Atlanta, GA 30322 USA; [Keshtkaran, Mohammad Reza; Pandarinath, Chethan] Georgia Tech, Atlanta, GA 30322 USA; [Pandarinath, Chethan] Emory Univ, Dept Neurosurg, Coulter Dept Biomed Engn, Atlanta, GA 30322 USA	Emory University; University System of Georgia; Georgia Institute of Technology; Emory University	Keshtkaran, MR (corresponding author), Emory Univ, Coulter Dept Biomed Engn, Atlanta, GA 30322 USA.; Keshtkaran, MR (corresponding author), Georgia Tech, Atlanta, GA 30322 USA.	mkeshtk@emory.edu; chethan@gatech.edu			NSF NCS [1835364]; DARPA Intelligent Neural Interfaces program	NSF NCS; DARPA Intelligent Neural Interfaces program	We thank Chris Rozell, Ali Farshchian, Raghav Tandon, Andrew Sedler, and Lahiru Wimalasena for their comments on the paper. This work has been supported by NSF NCS 1835364, and DARPA Intelligent Neural Interfaces program.	Buesing Lars, 2012, ADV NEURAL INFORM PR, P1682; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Duncker L, 2019, PR MACH LEARN RES, V97; Duncker L, 2018, ADV NEUR IN, V31; Gao YJ, 2016, ADV NEUR IN, V29; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Higgins M, 2017, PALGR COMMUN, V3, DOI 10.1057/s41599-017-0005-4; Jaderberg Max, 2017, ABS171109846 CORR; Karl M., 2016, INT C LEARN REPR; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Linderman SW, 2017, PR MACH LEARN RES, V54, P914; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Nassar J, 2018, CONF REC ASILOMAR C, P666, DOI 10.1109/ACSSC.2018.8645122; Ng A, 2011, LECT NOTES STANFORD, V72, P1; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Salinas E, 1995, NEUROBIOLOGY OF COMPUTATION, P299; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sussillo D., 2016, ARXIV160806315; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Williams AH, 2018, NEURON, V98, P1099, DOI 10.1016/j.neuron.2018.05.015; WOLD S, 1978, TECHNOMETRICS, V20, P397, DOI 10.2307/1267639; Wu A., 2017, ADV NEURAL INFORM PR, P3496; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; Zhao Y, 2017, NEURAL COMPUT, V29, P1293, DOI 10.1162/NECO_a_00953; Zhao Yuan, 2016, ADV NEURAL INF PROCE, P3333	27	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907058
C	Khodak, M; Balcan, MF; Talwalkar, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Khodak, Mikhail; Balcan, Maria-Florina; Talwalkar, Ameet			Adaptive Gradient-Based Meta-Learning Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We build a theoretical framework for designing and understanding practical meta-learning methods that integrates sophisticated formalizations of task-similarity with the extensive literature on online convex optimization and sequential prediction algorithms. Our approach enables the task-similarity to be learned adaptively, provides sharper transfer-risk bounds in the setting of statistical learning-to-learn, and leads to straightforward derivations of average-case regret bounds for efficient algorithms in settings where the task-environment changes dynamically or the tasks share a certain geometric structure. We use our theory to modify several popular meta-learning algorithms and improve their meta-test-time performance on standard problems in few-shot learning and federated learning.	[Khodak, Mikhail; Balcan, Maria-Florina; Talwalkar, Ameet] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Talwalkar, Ameet] Determined AI, San Francisco, CA USA	Carnegie Mellon University	Khodak, M (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	khodak@cmu.edu; ninamf@cs.cmu.edu; talwalkar@cmu.edu			DARPA [FA875017C0141]; National Science Foundation [CCF-1535967, CCF-1910321, IIS-1618714, IIS-1705121, IIS-1838017, IIS-1901403]; Microsoft Research Faculty Fellowship; Amazon Research Award; Bloomberg Data Science research grant; Amazon Web Services Award; Okawa Grant; Google Faculty Award; JP Morgan AI Research Faculty Award; Carnegie Bosch Institute Research Award	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation(National Science Foundation (NSF)); Microsoft Research Faculty Fellowship(Microsoft); Amazon Research Award; Bloomberg Data Science research grant; Amazon Web Services Award; Okawa Grant; Google Faculty Award(Google Incorporated); JP Morgan AI Research Faculty Award; Carnegie Bosch Institute Research Award	We thank Jeremy Cohen, Travis Dick, Nikunj Saunshi, Dravyansh Sharma, Ellen Vitercik, and our three anonymous reviewers for helpful feedback. This work was supported in part by DARPA FA875017C0141, National Science Foundation grants CCF-1535967, CCF-1910321, IIS-1618714, IIS-1705121, IIS-1838017, and IIS-1901403, a Microsoft Research Faculty Fellowship, a Bloomberg Data Science research grant, an Amazon Research Award, an Amazon Web Services Award, an Okawa Grant, a Google Faculty Award, a JP Morgan AI Research Faculty Award, and a Carnegie Bosch Institute Research Award. Any opinions, findings and conclusions, or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other funding agency.	Alquier P., 2017, P 20 INT C ART INT S; Amit Ron, 2018, P 35 INT C MACH LEAR; [Anonymous], P 6 INT C LEARN REPR; Azuma K., 1967, THOKU MATH J, V19, P357, DOI DOI 10.2748/TMJ/1178243286; Balcan Maria-Florina, 2015, P C LEARN THEOR; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bansal T, 2018, P 6 INT C LEARN REPR; Bartlett Peter L., 2008, ADV NEURAL INFORM PR; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Boyd S, 2004, CONVEX OPTIMIZATION; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Bullins B, 2019, P 30 INT C ALG LEARN; Caldas S., 2018, LEAF BENCHMARK FEDER, DOI 10.48550/arXiv.1812.01097; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-bianchi Nicol`o, 2005, ADV NEURAL INFORM PR; Cesa-Bianchi Nicolo, 2012, NEW LOOK SHIFTING RE; Chen F., 2018, FEDERATED META LEARN; Davis Chandler, 1963, P S PURE MATH; Denevi G., 2018, P C UNC ART INT; Denevi Giulia, 2019, LEARNING TO LEARN ST; Duan Yan, 2017, ADV NEURAL INFORM PR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Finn C., 2019, P 36 INT C MACH LEAR; Finn C., 2017, P 34 INT C MACH LEAR; Hall Eric C., 2016, ONLINE OPTIMIZATION; Hazan E., 2015, FDN TRENDS OPTIM, V2, P157; Hazan Elad, 2009, P 26 INT C MACH LEAR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Jadbabaie Ali, 2015, P 18 INT C ART INT S; Jerfel G., 2018, ONLINE GRADIENT BASE; Kakade S., 2008, ADV NEURAL INFORM PR; Khodak M., 2019, P 36 INT C MACH LEAR; Kim Jaehong, 2018, AUTOMETAAUTOMATED GR; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; Lake B. M., 2017, P C COGN SCI SOC COG; Li Z., 2017, META SGD LEARNING LE; LIEB EH, 1973, ADV MATH, V11, P267, DOI 10.1016/0001-8708(73)90011-X; McMahan H. B., 2017, P 20 INT C ART INT S; McMahan H. Brendan, 2010, P C LEARN THEOR; Mokhtari Aryan, 2016, P 55 IEEE C DEC CONT; Moridomi K, 2018, IEICE T INF SYST, VE101D, P1511, DOI 10.1587/transinf.2017EDP7317; Nichol A., 2018, 1 ORDER METALEARNING; Pentina A., 2014, P 31 INT C MACH LEAR; Ravi Sachin, 2017, P 5 INT C LEARN REPR; Saha Ankan, 2012, INTERPLAY STABILITY; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shalev-Shwartz Shai, 2010, J MACHINE LEARNING R, V11; Smith V., 2017, ADV NEURAL INFORM PR; Snell J., 2017, ADV NEURAL INFORM PR; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Zhang Tong, 2005, P INT C LEARN THEOR; Zhou ZH., 2017, ADV NEURAL INFORM PR; Zinkevich M., 2003, INT C MACH LEARN ICM	54	7	7	3	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305086
C	Kileel, J; Trager, M; Bruna, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kileel, Joe; Trager, Matthew; Bruna, Joan			On the Expressive Power of Deep Polynomial Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTILAYER FEEDFORWARD NETWORKS	We study deep neural networks with polynomial activations, particularly their expressive power. For a fixed architecture and activation degree, a polynomial neural network defines an algebraic map from weights to polynomials. The image of this map is the functional space associated to the network, and it is an irreducible algebraic variety upon taking closure. This paper proposes the dimension of this variety as a precise measure of the expressive power of polynomial neural networks. We obtain several theoretical results regarding this dimension as a function of architecture, including an exact formula for high activation degrees, as well as upper and lower bounds on layer widths in order for deep polynomials networks to fill the ambient functional space. We also present computational evidence that it is profitable in terms of expressiveness for layer widths to increase monotonically and then decrease monotonically. Finally, we link our study to favorable optimization properties when training weights, and we draw intriguing connections with tensor and polynomial decompositions.	[Kileel, Joe] Princeton Univ, Princeton, NJ 08544 USA; [Trager, Matthew; Bruna, Joan] NYU, New York, NY 10003 USA	Princeton University; New York University	Kileel, J (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.				Simons Collaboration on Algorithms and Geometry; Alfred P. Sloan Foundation; NSF [RI-1816753]; Samsung Electronics	Simons Collaboration on Algorithms and Geometry; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); NSF(National Science Foundation (NSF)); Samsung Electronics(Samsung)	We thank Justin Chen, Amit Moscovich, Claudiu Raicu and Steven Sam for helpful conversations. JK was partially supported by the Simons Collaboration on Algorithms and Geometry. MT and JB were partially supported by the Alfred P. Sloan Foundation, NSF RI-1816753 and Samsung Electronics.	ALEXANDER J., 1995, J ALGEBRAIC GEOM, V4, P201; Arora S., 2019, INT C LEARN REPR; Arora S, 2018, PR MACH LEARN RES, V80; Bengio, 2011, ADV NEURAL INFORM PR, P666, DOI DOI 10.5555/2986459.2986534; Bisht Pranav, 2017, THESIS; Blekherman G, 2015, MATH ANN, V362, P1021, DOI 10.1007/s00208-014-1150-3; Bruns W, 1993, COHEN MACAULAY RINGS, V39; Chizat Lenaic, 2018, ADV NEURAL INFORM PR, P3036; Cohen N., 2016, C LEARNING THEORY, V49, P698; Cohen N, 2016, PR MACH LEARN RES, V48; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Draisma J, 2016, FOUND COMPUT MATH, V16, P99, DOI 10.1007/s10208-014-9240-x; Du SS, 2018, PR MACH LEARN RES, V80; Eisenbud David, 1995, COMMUTATIVE ALGEBRA, V150; Froberg R, 2012, P NATL ACAD SCI USA, V109, P5600, DOI 10.1073/pnas.1120984109; Harris J., 1995, ALGEBRAIC GEOM, V133; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Jaffali Hamza, 2019, ARXIV190810247; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Landsberg JM., 2012, GRADUATE STUDIES MAT, V128; LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5; Lundqvist S, 2019, J PURE APPL ALGEBRA, V223, P2062, DOI 10.1016/j.jpaa.2018.08.015; Martens James, 2014, ARXIV14117717 ARXIV PREPRINT ARXIV; Mehta D., 2018, CORR; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Nicklasson L, 2017, COMMUN ALGEBRA, V45, P3390, DOI 10.1080/00927872.2016.1236931; Poon Hoifung, 2012, ARXIV12023732; Sam Steven V., 2019, ARXIV190702659; Soltanolkotabi M, 2019, IEEE T INFORM THEORY, V65, P742, DOI 10.1109/TIT.2018.2854560; The Sage Developers, 2017, SAGEMATH SAG MATH SO; Trager Matthew, 2019, ARXIV191001671; Venturi L., 2018, ARXIV PREPRINT ARXIV; [No title captured]	33	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901089
C	Kulal, S; Pasupat, P; Chandra, K; Lee, M; Padon, O; Aiken, A; Liang, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kulal, Sumith; Pasupat, Panupong; Chandra, Kartik; Lee, Mina; Padon, Oded; Aiken, Alex; Liang, Percy			SPoC: Search-based Pseudocode to Code	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the task of mapping pseudocode to executable code, assuming a one-to-one correspondence between lines of pseudocode and lines of code. Given test cases as a mechanism to validate programs, we search over the space of possible translations of the pseudocode to find a program that compiles and passes the test cases. While performing a best-first search, compilation errors constitute 88.7% of program failures. To better guide this search, we learn to predict the line of the program responsible for the failure and focus search over alternative translations of the pseudocode for that line. For evaluation, we collected the SPoC dataset (Search-based Pseudocode to Code) containing 18,356 C++ programs with human-authored pseudocode and test cases. Under a budget of 100 program compilations, performing search improves the synthesis success rate over using the top-one translation of the pseudocode from 25.6% to 44.7%.	[Kulal, Sumith; Pasupat, Panupong; Chandra, Kartik; Lee, Mina; Padon, Oded; Aiken, Alex; Liang, Percy] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Kulal, S (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	sumith@cs.stanford.edu; ppasupat@cs.stanford.edu; kach@cs.stanford.edu; minalee@cs.stanford.edu; padon@cs.stanford.edu; aaiken@cs.stanford.edu; pliang@cs.stanford.edu		Lee, Mina/0000-0002-0428-4720	NSF [CCF-1409813]; NSF CAREER Award [IIS-1552635]; Amazon	NSF(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Amazon	We thank Shivam Garg, Jason Koenig, Nadia Polikarpova, Alex Polozov and Rishabh Singh for valuable feedback at different stages of the project. This work was supported by NSF grant CCF-1409813, NSF CAREER Award IIS-1552635 and a grant from Amazon.	Allamanis M., 2018, INT C LEARN REPR ICL; Andreas Jacob, 2015, EMPIRICAL METHODS NA; [Anonymous], 2018, ABS180703100 CORR; [Anonymous], 2017, ARXIV170102810; Bansal S.., 2006, P 12 INT C ARCHITECT, P394, DOI [10.1145/1168857.1168906, DOI 10.1145/1168857.1168906]; Berant J, 2013, EMPIRICAL METHODS NA; Bhatia S., 2018, INT C SOFTW ENG ICSE; Brockschmidt M., 2019, INT C LEARN REPR ICL; Dahl Deborah A, 1994, P WORKSH HUM LANG TE, P43, DOI DOI 10.3115/1075812.1075823; Devlin J., 2017, ABS171011054 CORR; Devlin J., 2017, INT C MACH LEARN ICM; Dong L., 2016, LANGUAGE LOGICAL FOR, Vabs/ 1601.0 1280; Dong L., 2018, COARSE TO FINE DECOD; Feng Yu, 2017, PRINCIPLES PROGRAMMI; Feser John K., 2015, PROGRAMMING LANGUAGE; Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423; Gupta R., 2017, DEEPFIX FIXING COMMO; Hartmann B., 2010, C HUM FACT COMP SYST; Hashimoto T., 2018, ADV NEURAL INFORM PR; Hayati Shirley Anugrah, 2018, P 2018 C EMP METH NA, P925, DOI 10.18653/v1/D18-1111; Iyer S., 2018, EMPIRICAL METHODS NA; Iyyer M., 2016, CORR; Jha Susmit, 2010, INT C SOFTW ENG ICSE INT C SOFTW ENG ICSE; Jia Robin, 2016, DATA RECOMBINATION N; Johnson J, 2017, COMPUTER VISION PATT; Kocisky T., 2016, P 2016 C EMP METH NA, P1078; Krishnamurthy J., 2017, EMNLP; Kwiatkowski Tom, 2011, P C EMP METH NAT LAN, P1512; Liang Percy, 2011, P 49 ANN M ASS COMPU, P590; Lin X. V., 2018, LANG RES EV C LREC; Ling W, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P599; Long R., 2016, ASS COMPUTATIONAL LI; MacMahon Matt, 2006, NAT C ART INT; Marko V., 2019, INT C LEARN REPR ICL; Massalin Henry, 1987, INT C ARCH SUPP PROG; Oda Y, 2015, IEEE INT CONF AUTOM, P574, DOI 10.1109/ASE.2015.36; Parisotto Emilio, 2017, INT C LEARN REPR ICL; Poon H., 2013, ASS COMPUTATIONAL LI; Rabinovich M., 2017, ASS COMPUTATIONAL LI; REDDY GN, 2016, ASS COMPUTATIONAL LI, P127; Schkufza E., 2013, ARCHITECTURAL SUPPOR; Shi K., 2019, PRINCIPLES PROGRAMMI; Solar-Lezama A., 2006, ARCHITECTURAL SUPPOR; Talmor A., 2018, WEB KNOWLEDGE BASE A; Tate R., 2009, PRINCIPLES PROGRAMMI; Tu Zhaopeng, 2016, ACL 2016; Vadas D., 2005, AUSTR LANG TECHN WOR; Vaswani A., 2017, P 31 INT C NEURAL IN, P6000, DOI DOI 10.5555/3295222.3295349; Vinyals Oriol, 2015, ADV NEURAL INFORM PR, V28, P2674; Xiao C., 2016, ASS COMPUTATIONAL LI; Yaghmazadeh N., 2017, OBJECT ORIENTED PROG; Yaghmazadeh Navid, 2016, PROGRAMMING LANGUAGE; Yin PC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P440, DOI 10.18653/v1/P17-1041; Yu T., 2018, EMPIRICAL METHODS NA; Zavershynskyi M., 2018, WORKSH NEUR ABSTR MA; Zelle JM, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1050; Zettlemoyer Luke, 2007, P 2007 JOINT C EMP M, P678; Zhong Victor, 2017, CORR	59	7	7	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903051
C	Kumar, S; Ying, JX; Cardoso, JVD; Palomar, DP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumar, Sandeep; Ying, Jiaxi; Cardoso, Jos Vinicius de M.; Palomar, Daniel P.			Structured Graph Learning via Laplacian Spectral Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INVERSE COVARIANCE ESTIMATION; MODEL SELECTION; REGULARIZATION	Learning a graph with a specific structure is essential for interpretability and identification of the relationships among data. It is well known that structured graph learning from observed samples is an NP-hard combinatorial problem. In this paper, we first show that for a set of important graph families it is possible to convert the structural constraints of structure into eigenvalue constraints of the graph Laplacian matrix. Then we introduce a unified graph learning framework, lying at the integration of the spectral properties of the Laplacian matrix with Gaussian graphical modeling that is capable of learning structures of a large class of graph families. The proposed algorithms are provably convergent and practically amenable for large-scale semi-supervised and unsupervised graph based learning tasks. Extensive numerical experiments with both synthetic and real data sets demonstrate the effectiveness of the proposed methods. An R package containing code for all the experimental results is available at https//cran.r-project.org/package=spectralGraphTopology.	[Kumar, Sandeep; Palomar, Daniel P.] Hong Kong Univ Sci & Technol, Dept Ind Engn & Data Analyt, Clear Water Bay, Hong Kong, Peoples R China; [Ying, Jiaxi; Cardoso, Jos Vinicius de M.; Palomar, Daniel P.] Hong Kong Univ Sci & Technol, Dept Elect & Comp Engn, Clear Water Bay, Hong Kong, Peoples R China	Hong Kong University of Science & Technology; Hong Kong University of Science & Technology	Kumar, S (corresponding author), Hong Kong Univ Sci & Technol, Dept Ind Engn & Data Analyt, Clear Water Bay, Hong Kong, Peoples R China.	sandeep0kr@gmail.com; jx.ying@connect.ust.hk; jvdmc@connect.ust.hk; palomar@ust.hk			Hong Kong GRF [16207019]	Hong Kong GRF(Hong Kong Research Grants Council)	This work was supported by the Hong Kong GRF 16207019 research grant.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Ambroise C, 2009, ELECTRON J STAT, V3, P205, DOI 10.1214/08-EJS314; Anandkumar A, 2012, J MACH LEARN RES, V13, P2293; Argyriou Andreas, 2008, ADV NEURAL INFORM PR, P25; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Basu S, 2015, J MACH LEARN RES, V16, P417; Belkin M, 2002, ADV NEUR IN, V14, P585; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Benidis K, 2016, IEEE T SIGNAL PROCES, V64, P6211, DOI 10.1109/TSP.2016.2605073; Bogdanov A, 2008, LECT NOTES COMPUT SC, V5171, P331; Cai TT, 2016, STAT SINICA, V26, P445, DOI 10.5705/ss.2014.256; Cancer Genome Atlas Research Network, 2013, Nat Genet, V45, P1113, DOI 10.1038/ng.2764; Chandrasekaran V., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1610, DOI 10.1109/ALLERTON.2010.5707106; Chin A, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P263, DOI 10.1145/3308558.3313748; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Chow YT, 2016, CONF REC ASILOMAR C, P1715, DOI 10.1109/ACSSC.2016.7869675; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Cvetkovic D, 2007, LINEAR ALGEBRA APPL, V423, P155, DOI 10.1016/j.laa.2007.01.009; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Dong XW, 2016, IEEE T SIGNAL PROCES, V64, P6160, DOI 10.1109/TSP.2016.2602809; Drton M, 2008, J MACH LEARN RES, V9, P893; Du H, 2008, ADVANCING SCIENCE THROUGH COMPUTATION, P14; Egilmez HE, 2017, IEEE J-STSP, V11, P825, DOI 10.1109/JSTSP.2017.2726975; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Godsil C.D., 1982, AEQUATIONES MATH, V25, P257, DOI DOI 10.1007/BF02189621; Hao BT, 2018, J MACH LEARN RES, V18; Hassan-Moghaddam S, 2016, IEEE DECIS CONTR P, P4624, DOI 10.1109/CDC.2016.7798973; Heinavaara O, 2016, BMC BIOINFORMATICS, V17, DOI 10.1186/s12859-016-1309-x; Kalofolias V, 2016, JMLR WORKSH CONF PRO, V51, P920; Kolaczyk E.D., 2014, STAT ANAL NETWORK DA, V65; Kumar S., 2019, ARXIV190409792; Lake B. M, 2010, P ANN COGN SCI C; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Liu Q, 2011, P 14 INT C ART INT S, P40; Loukas Andreas, 2018, P MACHINE LEARNING R; Marlin B.M., 2009, P 26 INT C MACH LEAR, P705; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Meng ZS, 2014, PR MACH LEARN RES, V32, P1269; Mohan K, 2014, J MACH LEARN RES, V15, P445; Mohar B, 1997, NATO ADV SCI I C-MAT, V497, P225; Nie FP, 2017, ADV NEUR IN, V30; Nie FP, 2016, AAAI CONF ARTIF INTE, P1969; OSHERSON DN, 1991, COGNITIVE SCI, V15, P251, DOI 10.1207/s15516709cog1502_3; Park Y, 2017, PR MACH LEARN RES, V54, P1302; Pavez E, 2018, IEEE T SIGNAL PROCES, V66, P2399, DOI 10.1109/TSP.2018.2813337; Pavlopoulos GA, 2018, GIGASCIENCE, V7, DOI 10.1093/gigascience/giy014; Prabhu A, 2018, LECT NOTES COMPUT SC, V11217, P20, DOI 10.1007/978-3-030-01261-8_2; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Rue H., 2005, WWW 19; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Slawski M, 2015, LINEAR ALGEBRA APPL, V473, P145, DOI 10.1016/j.laa.2014.04.020; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Song JX, 2015, IEEE T SIGNAL PROCES, V63, P1627, DOI 10.1109/TSP.2015.2394443; Spielman DA, 2011, SIAM J COMPUT, V40, P981, DOI 10.1137/08074489X; Sun Y, 2017, IEEE T SIGNAL PROCES, V65, P794, DOI 10.1109/TSP.2016.2601299; Sundin M, 2017, EUR SIGNAL PR CONF, P151, DOI 10.23919/EUSIPCO.2017.8081187; Tarzanagh D. A., 2017, J MACHINE LEARNING R, V18, P7692; Teke O, 2017, IEEE T SIGNAL PROCES, V65, P5406, DOI 10.1109/TSP.2017.2731299; Todeschini A., 2013, ADV NEURAL INFORM PR, V26, P845; Wang JH, 2015, STAT SINICA, V25, P831, DOI 10.5705/ss.2013.192; Yang S., 2016, 30 AAAI C ART INT; Ying JX, 2018, IEEE T SIGNAL PROCES, V66, P5520, DOI 10.1109/TSP.2018.2869122; Zhu X., 2003, INT C MACH LEARN	67	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903030
C	Lee, JKW; Sattigeri, P; Wornell, GW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Joshua Ka-Wing; Sattigeri, Prasanna; Wornell, Gregory W.			Learning New Tricks From Old Dogs: Multi-Source Transfer Learning From Pre-Trained Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONNECTION	The advent of deep learning algorithms for mobile devices and sensors has led to a dramatic expansion in the availability and number of systems trained on a wide range of machine learning tasks, creating a host of opportunities and challenges in the realm of transfer learning. Currently, most transfer learning methods require some kind of control over the systems learned, either by enforcing constraints during the source training, or through the use of a joint optimization objective between tasks that requires all data be co-located for training. However, for practical, privacy, or other reasons, in a variety of applications we may have no control over the individual source task training, nor access to source training samples. Instead we only have access to features pre-trained on such data as the output of "black-boxes." For such scenarios, we consider the multi-source learning problem of training a classifier using an ensemble of pre-trained neural networks for a set of classes that have not been observed by any of the source networks, and for which we have very few training samples. We show that by using these distributed networks as feature extractors, we can train an effective classifier in a computationally-efficient manner using tools from (nonlinear) maximal correlation analysis. In particular, we develop a method we refer to as maximal correlation weighting (MCW) to build the required target classifier from an appropriate weighting of the feature functions from the source networks. We illustrate the effectiveness of the resulting classifier on datasets derived from the CIFAR-100, Stanford Dogs, and Tiny ImageNet datasets, and, in addition, use the methodology to characterize the relative value of different source tasks in learning a target task.	[Lee, Joshua Ka-Wing; Wornell, Gregory W.] MIT, Dept EECS, Cambridge, MA 02139 USA; [Sattigeri, Prasanna] IBM Res, MIT IBM Watson AI Lab, Yorktown Hts, NY USA	Massachusetts Institute of Technology (MIT); International Business Machines (IBM)	Lee, JKW (corresponding author), MIT, Dept EECS, Cambridge, MA 02139 USA.	jkjlee@mit.edu; psattig@us.ibm.com; gww@mit.edu			MIT-IBM Watson AI Lab; NSF [CCF1717610]	MIT-IBM Watson AI Lab(International Business Machines (IBM)); NSF(National Science Foundation (NSF))	This work was supported in part by the MIT-IBM Watson AI Lab, and by NSF under Grant No. CCF1717610.	Bao Yajie, 2019, INFORM THEORETIC MET; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; BREIMAN L, 1985, J AM STAT ASSOC, V80, P580, DOI 10.2307/2288473; Fang BY, 2016, MOBISYS'16: COMPANION COMPANION PUBLICATION OF THE 14TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P23, DOI 10.1145/2938559.2948802; Finn C, 2017, PR MACH LEARN RES, V70; Fredrikson M, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1322, DOI 10.1145/2810103.2813677; Gebelein H, 1941, Z ANGEW MATH MECH, V21, P364, DOI 10.1002/zamm.19410210604; Gupta R., 2008, AAAI 08, P842; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Hirschfeld H, 1935, P CAMB PHILOS SOC, V31, P520, DOI 10.1017/S0305004100013517; HUANG S, 2019, PREPRINT; Huang SL, 2017, IEEE INT SYMP INFO, P1336, DOI 10.1109/ISIT.2017.8006746; Khosla Aditya, 2011, 1 WORKSH FIN GRAIN V, P6; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li F. F., 2015, TINY IMAGENET VISUAL; Lim CP, 2003, IEEE T SYST MAN CY C, V33, P235, DOI 10.1109/TSMCC.2003.813150; Liu Pengfei, 2016, ABS160907222 CORR; Long MS, 2015, PR MACH LEARN RES, V37, P97; Manco Giuseppe, 2017, P INT WORKSH PERS AN, P23; Mishra N, 2017, ABS170703141 CORR; Nichol Alex, 2018, ABS180302999 ARXIV; Ota K, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3092831; Pan SJ., 2008, PROC NATL CONF ARTIF, V8, P677; Paszke A., 2017, AUT WORKSH C NEUR IN; Renyi A., 1959, ACTA MATH ACAD SCI H, V10, DOI [DOI 10.1007/BF02024507, 10.1007/BF02024507]; Rusu A. A., 2016, ARXIV160604671; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519	28	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304038
C	Lezcano-Casado, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lezcano-Casado, Mario			Trivializations for Gradient-Based Optimization on Manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a framework to study the transformation of problems with manifold constraints into unconstrained problems through parametrizations in terms of a Euclidean space. We call these parametrizations trivializations. We prove conditions under which a trivialization is sound in the context of gradient-based optimization and we show how two large families of trivializations have overall favorable properties, but also suffer from a performance issue. We then introduce dynamic trivializations, which solve this problem, and we show how these form a family of optimization methods that lie between trivializations and Riemannian gradient descent, and combine the benefits of both of them. We then show how to implement these two families of trivializations in practice for different matrix manifolds. To this end, we prove a formula for the gradient of the exponential of matrices, which can be of practical interest on its own. Finally, we show how dynamic trivializations improve the performance of existing methods on standard tasks designed to test long-term memory within neural networks.(1)	[Lezcano-Casado, Mario] Univ Oxford, Dept Math, Oxford, England	University of Oxford	Lezcano-Casado, M (corresponding author), Univ Oxford, Dept Math, Oxford, England.	mario.lezcanocasado@maths.ox.ac.uk			Oxford-James Martin Graduate Scholarship; "la Caixa" Banking Foundation [LCF/BQ/EU17/11590067]	Oxford-James Martin Graduate Scholarship; "la Caixa" Banking Foundation(La Caixa Foundation)	The work of MLC was supported by the Oxford-James Martin Graduate Scholarship and the "la Caixa" Banking Foundation (LCF/BQ/EU17/11590067).	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Al-Mohy AH, 2009, SIAM J MATRIX ANAL A, V31, P970, DOI 10.1137/09074721X; Al-Mohy AH, 2008, SIAM J MATRIX ANAL A, V30, P1639, DOI 10.1137/080716426; Andruchow E, 2014, J GEOM PHYS, V86, P241, DOI 10.1016/j.geomphys.2014.08.009; Arjovsky M, 2016, PR MACH LEARN RES, V48; Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996; Arsigny V, 2006, LECT NOTES COMPUT SC, V4190, P924; Becigneul G., 2019, PROC INT C LEARN REP, P1; Bengio Y., 2014, ARXIV14061078; Berg R. v. d., 2018, P UNC ART INT, P1; Besse Arthur L., 2008, EINSTEIN MANIFOLDS; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; Boumal N, 2019, IMA J NUMER ANAL, V39, P1, DOI 10.1093/imanum/drx080; do Carmo M. P, 1992, RIEMANNIAN GEOMETRY; Dreisigmeyer DW, 2018, J OPTIMIZ THEORY APP, V176, P585, DOI 10.1007/s10957-018-1225-5; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Feng R, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION, P1617, DOI 10.1109/ICMA.2016.7558806; Gallier J., 2019, DIFFERENTIAL GEOMETR; Gallot S., 2012, RIEMANNIAN GEOMETRY; Garofolo J., 1992, TIMIT ACOUSTIC PHONE; Hall B., 2015, LIE GROUPS LIE ALGEB; Helfrich KE, 2018, PR MACH LEARN RES, V80; Helgason S, 1979, DIFFERENTIAL GEOMETR, V80; Henaff M, 2016, PR MACH LEARN RES, V48; Higham N. J., 2008, FUNCTIONS MATRICES T, V104; HILLE E, 1958, MATH ANN, V136, P46, DOI 10.1007/BF01350286; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Iserles A., 2000, Acta Numerica, V9, P215, DOI 10.1017/S0962492900002154; Itoh J, 1998, TOHOKU MATH J, V50, P571, DOI 10.2748/tmj/1178224899; Jing L., 2017, INT C MACHINE LEARNI, P1733; KENNEY C, 1991, SIAM J SCI STAT COMP, V12, P488, DOI 10.1137/0912027; Kingma D.P, P 3 INT C LEARNING R; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Le Q.V., 2015, ABS150400941 CORR; LeCun Y., 2010, MNIST HANDWRITTEN DI; Lee J.M., 2013, INTRO SMOOTH MANIFOL, V2nd, DOI 10.1007/978-1-4419-9982-5; Lee John M, 2018, INTRO RIEMANNIAN MAN; Lezcano-Casado M., 2019, ARXIV190108428, P2019; Maduranga K. D. G., 2018, ARXIV181104142; MAGNUS W, 1954, COMMUN PUR APPL MATH, V7, P649, DOI 10.1002/cpa.3160070404; Mhammedi Z, 2017, PR MACH LEARN RES, V70; O'Neill B., 1983, PURE APPL MATH; Petersen P., 2016, RIEMANNIAN GEOMETRY, V171; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rossmann W., 2006, OXFORD GRADUATE TEXT; Sato H, 2015, OPTIMIZATION, V64, P1011, DOI 10.1080/02331934.2013.836650; Smith ST., 1993, GEOMETRIC OPTIMIZATI; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tomczak Jakub M, 2016, ARXIV161109630; Udriste, 1994, MATH APPL, V297, DOI [10.1007/978-94-015-8390-9, DOI 10.1007/978-94-015-8390-9]; Wang H.-C., 1969, J DIFFER GEOM, V3, P481; Wisdom S, 2016, ADV NEUR IN, V29; Zhang H., 2018, ARXIV180602812; Zhang Hongyi, 2016, ADV NEURAL INFORM PR, V29, P4592; Zhang J, 2018, 2018 13TH WORLD CONGRESS ON INTELLIGENT CONTROL AND AUTOMATION (WCICA), P801, DOI 10.1109/WCICA.2018.8630414	59	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900071
C	Liu, WY; Liu, Z; Rehg, JM; Song, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Weiyang; Liu, Zhen; Rehg, James M.; Song, Le			Neural Similarity Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Inner product-based convolution has been the founding stone of convolutional neural networks (CNNs), enabling end-to-end learning of visual representation. By generalizing inner product with a bilinear matrix, we propose the neural similarity which serves as a learnable parametric similarity measure for CNNs. Neural similarity naturally generalizes the convolution and enhances flexibility. Further, we consider the neural similarity learning (NSL) in order to learn the neural similarity adaptively from training data. Specifically, we propose two different ways of learning the neural similarity: static NSL and dynamic NSL. Interestingly, dynamic neural similarity makes the CNN become a dynamic inference network. By regularizing the bilinear matrix, NSL can be viewed as learning the shape of kernel and the similarity measure simultaneously. We further justify the effectiveness of NSL with a theoretical viewpoint. Most importantly, NSL shows promising performance in visual recognition and few-shot learning, validating the superiority of NSL over the inner product-based convolution counterparts.	[Liu, Weiyang; Rehg, James M.; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Liu, Zhen] Univ Montreal, Mila, Montreal, PQ, Canada; [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China	University System of Georgia; Georgia Institute of Technology; Universite de Montreal	Liu, WY (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	wyliu@gatech.edu; zhen.liu.2@umontreal.ca; rehg@gatech.edu; lsong@cc.gatech.edu			Baidu Fellowship; Nvidia GPU Grant; NSF [CDSE-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983]; DARPA Program on Learning with Less Labels	Baidu Fellowship; Nvidia GPU Grant; NSF(National Science Foundation (NSF)); DARPA Program on Learning with Less Labels	Weiyang Liu was supported in part by Baidu Fellowship and Nvidia GPU Grant. Le Song was supported in part by NSF grants CDS&E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983, DARPA Program on Learning with Less Labels.	Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2017, COMMUN MATH STAT; Antoniou Antreas, 2017, ARXIV171104340; Arora S., 2018, ARXIV181002281; Arora S, 2018, PR MACH LEARN RES, V80; Arora Sanjeev, 2019, ADV NEURAL INFORM PR, P7411; Bauer M., 2017, ARXIV170600326; Bengio Y., 1990, LEARNING SYNAPTIC LE; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen T.Q., 2018, ADV NEURAL INFORM PR; Chen Wenhu, 2019, P ICLR; Chen Z., 2018, ARXIV181111205; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; De Brabandere B, 2016, ADV NEUR IN, V29; Finn C, 2017, PR MACH LEARN RES, V70; Gunasekar Suriya, 2018, NIPS; Gunasekar Suriya, 2017, NIPS; Gupta M., 2004, STATIC DYNAMIC NEURA; Ha David, 2016, ARXIV160909106; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hubara I, 2016, ADV NEUR IN, V29; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jeon YH, 2017, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2017.200; Jeon Yunho, 2018, NEURIPS; Ji Z., 2018, PROC INT C LEARN REP; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Leven SJ, 1996, COGNITIVE SCI, V20, P271, DOI 10.1207/s15516709cog2002_4; Li Ke, 2016, ARXIV160601885; Li Y., 2018, COLT; Lin Rongmei, 2019, ARXIV190604892; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2018, ADV NEUR IN, V31; Liu WY, 2017, ADV NEUR IN, V30; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Weiyang, 2018, CVPR; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lu YP, 2018, PR MACH LEARN RES, V80; Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Neyshabur B., 2018, ARXIV180512076; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Ravi S., 2017, INT C LEARN REPR, P12; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rusu Andrei A, 2018, ARXIV180705960; Ruthotto L, 2020, J MATH IMAGING VIS, V62, P352, DOI 10.1007/s10851-019-00903-1; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Shaw A, 2019, ADV NEUR IN, V32; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Su YC, 2017, ADV NEUR IN, V30; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang YX, 2007, INT J COGN INFORM NA, V1, P75, DOI 10.4018/jcini.2007100106; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Wu B., 2018, CVPR; Xing E., 2002, ADV NEURAL INFORM PR, V15, P505, DOI DOI 10.5555/2968618.2968683; Yu F., 2016, ABS151107122 CORR; Zhang H, 2019, PR MACH LEARN RES, V97; Zhu X., 2018, ARXIV181111168	63	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305007
C	Liu, Y; Shen, ZH; Lin, ZX; Peng, SD; Bao, HJ; Zhou, XW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Yuan; Shen, Zehong; Lin, Zhixuan; Peng, Sida; Bao, Hujun; Zhou, Xiaowei			GIFT: Learning Transformation-Invariant Dense Visual Descriptors via Group CNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Finding local correspondences between images with different viewpoints requires local descriptors that are robust against geometric transformations. An approach for transformation invariance is to integrate out the transformations by pooling the features extracted from transformed versions of an image. However, the feature pooling may sacrifice the distinctiveness of the resulting descriptors. In this paper, we introduce a novel visual descriptor named Group Invariant Feature Transform (GIFT), which is both discriminative and robust to geometric transformations. The key idea is that the features extracted from the transformed versions of an image can be viewed as a function defined on the group of the transformations. Instead of feature pooling, we use group convolutions to exploit underlying structures of the extracted features on the group, resulting in descriptors that are both discriminative and provably invariant to the group of transformations. Extensive experiments show that GIFT outperforms state-of-the-art methods on several benchmark datasets and practically improves the performance of relative pose estimation.	[Liu, Yuan; Shen, Zehong; Lin, Zhixuan; Peng, Sida; Bao, Hujun; Zhou, Xiaowei] Zhejiang Univ, ZJU Sensetime Joint Lab 3D Vis, State Key Lab CAD&CG, Hangzhou, Peoples R China	Zhejiang University	Bao, HJ; Zhou, XW (corresponding author), Zhejiang Univ, ZJU Sensetime Joint Lab 3D Vis, State Key Lab CAD&CG, Hangzhou, Peoples R China.	bao@cad.zju.edu.cn; xzhou@cad.zju.edu.cn			NSFC [61806176]; Fundamental Research Funds for the Central Universities; ZJU-SenseTime Joint Lab of 3D Vision	NSFC(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); ZJU-SenseTime Joint Lab of 3D Vision	The authors would like to acknowledge support from NSFC (No. 61806176), Fundamental Research Funds for the Central Universities and ZJU-SenseTime Joint Lab of 3D Vision.	Balntas V., 2017, CVPR; Balntas V., 2016, P BRIT MACH VIS C YO; Bay H., 2006, ECCV; Bekkers Erik Johannes, 2018, MICCAI; Brown Matthew, 2007, ICCV; Calonder M, 2012, IEEE T PATTERN ANAL, V34, P1281, DOI 10.1109/TPAMI.2011.222; Choy Christopher B, 2016, NEURIPS; Cohen T., 2017, ICLR; Cohen Taco, 2016, ICML; Cohen Taco S, 2018, ICLR; DeTone Daniel, 2018, CVPR WORKSH; Doiphode Nehal, 2018, ABS181100438 CORR; Dong J., 2015, CVPR; Esteves Carlos, 2019, ARXIV190400993; Esteves Carlos, 2018, ICLR; Esteves Carlos, 2018, ECCV; Filliat D., 2007, ICRA; Hariharan Bharath, 2015, CVPR; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hassner T., 2012, CVPR; He Kun, 2018, CVPR; Henriques J. F., 2017, ICML; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Jaderberg Max, 2015, NEURIPS; Kaisheng Tai, 2019, ICML; Khasanova Renata, 2017, ICML; Lenc K., 2018, P 29 BRIT MACH VIS C; Lenc Karel, 2016, ECCV; Leung T., 2015, CVPR; Lin T.-Y., 2015, ICCV; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Lindeberg T, 1998, INT J COMPUT VISION, V30, P79, DOI 10.1023/A:1008045108935; Lindeberg T., 2013, SCALE SPACE THEORY C, V256; Long J.L., 2014, NEURIPS; Lowe David G, 2004, ICCV; Luo Z., 2018, ECCV; Marcos D., 2017, ICCV; Mikolajczyk Krystian, 2004, ICCV; Mishchuk A., 2017, NEURIPS; Mishkin Dmytro, 2018, ECCV, V1, P2; Morel JM, 2009, SIAM J IMAGING SCI, V2, P438, DOI 10.1137/080732730; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Nair V., 2010, ICML; Ono Y., 2018, NEURIPS; Paszke Adam, 2017, NEURIPS WORKSH; Philbin James, 2010, ECCV; Reid I. D., 2015, LEARNING LOCAL IMAGE, P1; Rublee E., 2011, ICCV; Schroff F., 2015, CVPR; Shen T., 2018, ACCV; Strecha C., 2008, CVPR; Tian Yurun, 2017, CVPR; Ulyanov D., 2016, ABS160708022 CORR; Wang S, 2017, BMVC; Wang Z., 2014, ECCV; Wei X., 2018, CVPR; Weiler Maurice, 2018, CVPR; Worrall D. E., 2017, CVPR; Xiao J., 2013, ICCV; Yang T.-Y., 2016, CVPR; Yi Kwang Moo, 2016, ECCV; Yi Kwang Moo, 2016, CVPR; Zagoruyko S., 2015, CVPR; Zhang X., 2017, CVPR; Zhou T., 2015, CVPR	65	7	9	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307005
C	Paul, S; Kurin, V; Whiteson, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Paul, Supratik; Kurin, Vitaly; Whiteson, Shimon			Fast Efficient Hyperparameter Tuning for Policy Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The performance of policy gradient methods is sensitive to hyperparameter settings that must be tuned for any new application. Widely used grid search methods for tuning hyperparameters are sample inefficient and computationally expensive. More advanced methods like Population Based Training (Jaderberg et al., 2017) that learn optimal schedules for hyperparameters instead of fixed settings can yield better results, but are also sample inefficient and computationally expensive. In this paper, we propose Hyperparameter Optimisation on the Fly (HOOF), a gradient-free algorithm that requires no more than one training run to automatically adapt the hyperparameter that affect the policy update directly through the gradient. The main idea is to use existing trajectories sampled by the policy gradient method to optimise a one-step improvement objective, yielding a sample and computationally efficient algorithm that is easy to implement. Our experimental results across multiple domains and algorithms show that using HOOF to learn these hyperparameter schedules leads to faster learning with improved performance.	[Paul, Supratik; Kurin, Vitaly; Whiteson, Shimon] Univ Oxford, Deptartment Comp Sci, Oxford, England	University of Oxford	Paul, S (corresponding author), Univ Oxford, Deptartment Comp Sci, Oxford, England.	supratik.paul@cs.ox.ac.uk; vitaly.kurin@cs.ox.ac.uk; shimon.whiteson@cs.ox.ac.uk			European Research Council (ERC) under the European Union [637713]; Samsung RD Institute UK; NVIDIA	European Research Council (ERC) under the European Union(European Research Council (ERC)); Samsung RD Institute UK(Samsung); NVIDIA	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement #637713), and Samsung R&D Institute UK. The experiments were made possible by a generous equipment grant from NVIDIA.	Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Brochu E., 2010, TUTORIAL BAYESIAN OP; Brockman G., 2016, OPENAI GYM; Chen Y., 2018, ABS181206855 CORR; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Downey C., 2010, INT C MACH LEARN; Duan Y, 2016, PR MACH LEARN RES, V48; Farquhar G., 2018, INT C LEARN REPR; Fedus William, 2019, HYPERBOLIC DISCOUNTI; Franqois-Lavet V., 2015, NIPS 2015 WORKSH DEE; Hansen N., 2001, EVOL COMPUT; Henderson P., 2018, ABS181002525 CORR; Henderson P., 2018, AAAL; Hesse C., 2017, OPENAI BASELINES; Igl M, 2018, PR MACH LEARN RES, V80; Jaderberg Max, 2017, ABS171109846 CORR; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kandasamy K, 2018, PR MACH LEARN RES, V84; Kearns M. J., 2000, C LEARN THEOR; Kingma D.P, P 3 INT C LEARNING R; Klein A., 2016, FAST BAYESIAN OPTIMI; Luketina J, 2016, PR MACH LEARN RES, V48; Mahmood A. R., 2018, PROC 2 C ROBOT LEARN, P561; Mnih V, 2016, PR MACH LEARN RES, V48; Mordatch I., 2015, ADV NEURAL INFORM PR; Pedregosa F, 2016, PR MACH LEARN RES, V48; Rajeswaran A., 2017, ADV NEURAL INFORM PR, P6550; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman J., 2016, P INT C LEARN REPR; Shah A, 2015, ADV NEURAL INFORM PR, V12, P3330; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sutton R. S., 1992, AAAI; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Swersky Kevin, 2014, ARXIV PREPRINT ARXIV, P11; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wang J., 2016, ARXIV160205149; White M, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P557; White M, 2017, PR MACH LEARN RES, V70; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu ZW, 2018, ADV NEUR IN, V31, DOI 10.1142/S0192415X1850074X	48	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304060
C	Pirinen, A; Gartner, E; Sminchisescu, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pirinen, Aleksis; Gartner, Erik; Sminchisescu, Cristian			Domes to Drones: Self-Supervised Active Triangulation for 3D Human Pose Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Existing state-of-the-art estimation systems can detect 2d poses of multiple people in images quite reliably. In contrast, 3d pose estimation from a single image is ill-posed due to occlusion and depth ambiguities. Assuming access to multiple cameras, or given an active system able to position itself to observe the scene from multiple viewpoints, reconstructing 3d pose from 2d measurements becomes well-posed within the framework of standard multi-view geometry. Less clear is what is an informative set of viewpoints for accurate 3d reconstruction, particularly in complex scenes, where people are occluded by others or by scene objects. In order to address the view selection problem in a principled way, we here introduce ACTOR, an active triangulation agent for 3d human pose reconstruction. Our fully trainable agent consists of a 2d pose estimation network (any of which would work) and a deep reinforcement learning-based policy for camera viewpoint selection. The policy predicts observation viewpoints, the number of which varies adaptively depending on scene content, and the associated images are fed to an underlying pose estimator. Importantly, training the view selection policy requires no annotations - given a pre-trained 2d pose estimator, ACTOR is trained in a self-supervised manner. In extensive evaluations on complex multi-people scenes filmed in a Panoptic dome, under multiple viewpoints, we compare our active triangulation agent to strong multi-view baselines, and show that ACTOR produces significantly more accurate 3d pose reconstructions. We also provide a proof-of-concept experiment indicating the potential of connecting our view selection policy to a physical drone observer.	[Pirinen, Aleksis; Gartner, Erik; Sminchisescu, Cristian] Lund Univ, Fac Engn, Dept Math, Lund, Sweden; [Sminchisescu, Cristian] Google Res, Mountain View, CA USA	Lund University; Google Incorporated	Pirinen, A (corresponding author), Lund Univ, Fac Engn, Dept Math, Lund, Sweden.	aleksis.pirinen@math.lth.se; erik.gartner@math.lth.se; cristian.sminchisescu@math.lth.se			CNCS-UEFISCDI [PN-III-P4-ID-PCE-2016-0535, PCCF-2016-0180]; EU; Swedish Foundation for Strategic Research (SSF) Smart Systems Program; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; European Research Council Consolidator grant SEED,	CNCS-UEFISCDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI)); EU(European Commission); Swedish Foundation for Strategic Research (SSF) Smart Systems Program; Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; European Research Council Consolidator grant SEED,	This work was supported by the European Research Council Consolidator grant SEED, CNCS-UEFISCDI PN-III-P4-ID-PCE-2016-0535 and PCCF-2016-0180, the EU Horizon 2020 Grant DE-ENIGMA, Swedish Foundation for Strategic Research (SSF) Smart Systems Program, as well as the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. We would also like to thank Patrik Persson for support with the drone experiments.	Ammirato Phil, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1378, DOI 10.1109/ICRA.2017.7989164; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arnab A, 2019, PROC CVPR IEEE, P3390, DOI 10.1109/CVPR.2019.00351; Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen CH, 2019, PROC CVPR IEEE, P5707, DOI 10.1109/CVPR.2019.00586; Das A., 2018, CVPR, V5, P6; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Drover D, 2019, LECT NOTES COMPUT SC, V11132, P78, DOI 10.1007/978-3-030-11018-5_7; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gordon D, 2018, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR.2018.00430; Han XG, 2019, PROC CVPR IEEE, P234, DOI 10.1109/CVPR.2019.00032; Haner S, 2012, LECT NOTES COMPUT SC, V7573, P545, DOI 10.1007/978-3-642-33709-3_39; Hartley RI, 1997, COMPUT VIS IMAGE UND, V68, P146, DOI 10.1006/cviu.1997.0547; JAYARAMAN D, 2016, ECCV, V9909, P489, DOI DOI 10.1007/978-3-319-46454-1_30; Jayaraman D, 2018, PROC CVPR IEEE, P1238, DOI 10.1109/CVPR.2018.00135; Johns E, 2016, PROC CVPR IEEE, P3813, DOI 10.1109/CVPR.2016.414; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Kingma D.P, P 3 INT C LEARNING R; Kocabas M, 2019, PROC CVPR IEEE, P1077, DOI 10.1109/CVPR.2019.00117; Lourakis M., 2018, STEREO TRIANGULATION; Popa Alin-Ionut, 2017, CVPR; Rhodin H, 2016, LECT NOTES COMPUT SC, V9909, P509, DOI 10.1007/978-3-319-46454-1_31; Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134; Sminchisescu C, 2005, INT J COMPUT VISION, V61, P81, DOI 10.1023/B:VISI.0000042935.43630.46; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Vasquez-Gomez JI, 2014, INT J ADV ROBOT SYST, V11, DOI 10.5772/58759; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xia F, 2018, PROC CVPR IEEE, P9068, DOI 10.1109/CVPR.2018.00945; Xiong B, 2018, LECT NOTES COMPUT SC, V11209, P3, DOI 10.1007/978-3-030-01228-1_1; Yu Zhixuan, 2018, ARXIV181200281; Zanfir A, 2018, ADV NEUR IN, V31; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229	36	7	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303085
C	Ren, ZZ; Dong, KF; Zhou, Y; Liu, Q; Peng, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ren, Zhizhou; Dong, Kefan; Zhou, Yuan; Liu, Qiang; Peng, Jian			Exploration via Hindsight Goal Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Goal-oriented reinforcement learning has recently been a practical framework for robotic manipulation tasks, in which an agent is required to reach a certain goal defined by a function on the state space. However, the sparsity of such reward definition makes traditional reinforcement learning algorithms very inefficient. Hindsight Experience Replay (HER), a recent advance, has greatly improved sample efficiency and practical applicability for such problems. It exploits previous replays by constructing imaginary goals in a simple heuristic way, acting like an implicit curriculum to alleviate the challenge of sparse reward signal. In this paper, we introduce Hindsight Goal Generation (HGG), a novel algorithmic framework that generates valuable hindsight goals which are easy for an agent to achieve in the short term and are also potential for guiding the agent to reach the actual goal in the long term. We have extensively evaluated our goal generation algorithm on a number of robotic manipulation tasks and demonstrated substantially improvement over the original HER in terms of sample efficiency.	[Ren, Zhizhou; Dong, Kefan] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China; [Ren, Zhizhou; Dong, Kefan; Peng, Jian] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; [Zhou, Yuan] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL USA; [Liu, Qiang] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	Tsinghua University; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; University of Texas System; University of Texas Austin	Ren, ZZ (corresponding author), Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.; Ren, ZZ (corresponding author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.	rzz16@mails.tsinghua.edu.cn; dkf16@mails.tsinghua.edu.cn; yuanz@illinois.edu; lqiang@cs.utexas.edu; jianpeng@illinois.edu		Peng, Jian/0000-0002-1736-2978				Ahuja R. K., 1993, NETWORK FLOWS THEORY; Asadi K, 2018, PR MACH LEARN RES, V80; Bai C., 2019, NEUROCOMPUTING; Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008; Brockman G., 2016, OPENAI GYM; Colas C., 2018, INT C MACH LEARN, P1038; Colas C, 2019, PR MACH LEARN RES, V97; Ding Y., 2019, ADV NEURAL INFORM PR; Ecoffet A, 2019, ARXIV PREPRINT ARXIV; Eysenbach B., 2019, ADV NEURAL INFORM PR; Fang M., 2019, ADV NEURAL INFORM PR; Fang M., 2019, INT C LEARN REPR; Florensa C., 2017, PROC 1 C ROBOT LEARN, P482; Florensa C, 2018, PR MACH LEARN RES, V80; Ghosh Dibya, 2019, INT C LEARN REPR; Goyal A, 2019, INT C LEARN REPR, P1; Huang Ziyue, 2019, ADV NEURAL INFORM PR; KAELBLING LP, 1993, IJCAI-93, VOLS 1 AND 2, P1094; Karatzoglou Alexandros, 2013, P 7 ACM C RECOMMENDE, P493, DOI DOI 10.1145/2507157.2508063; Levine S, 2016, J MACH LEARN RES, V17; Levy A., 2019, P INT C LEARN REPR; Luo Y., 2019, INT C LEARN REPR; Mao J., 2018, INT C LEARN REPR; Mao X., 2018, IEEE T PATTERN ANAL; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Nachum Ofir, 2013, ADV NEURAL INFORM PR, P3303; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Oh J, 2017, PR MACH LEARN RES, V70; Oudeyer, 2017, ARXIV170802190; Pathak D., 2018, INT C LEARN REPR; Pere A., 2018, P INT C LEARN REPR; Plappert Matthias, 2015, ARXIV180209464; Pong V., 2018, PROC 2 WORKSHOP LIFE; Pong V.H., 2019, ARXIV190303698; Pritzel Alexander, 2016, INT C LEARNING REPRE; Riedmiller Martin, 2020, INT C MACH LEARN, P4341; Sahni Himanshu, 2019, ADV NEURAL INFORM PR; Schaul T., 2016, INT C LEARN REPR ICL; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srinivas A., 2018, ICML, P4739; Su H.-H., 2012, P 23 ANN ACM SIAM S, P1413, DOI DOI 10.1137/1.9781611973099.111; Sukhbaatar Sainbayar, 2018, INT C LEARN REPR; Sun Hao, 2019, ADV NEURAL INFORM PR; Thomas Valentin, 2017, ARXIV170801289; Ummadisingu A., 2019, P INT C LEARN REPR N; Veeriah V., 2018, ARXIV180609605; Zhao R., 2019, P 36 INT C MACH LEAR, P7553; Zhao R., 2018, PROC 2 C ROBOT LEARN, P113	54	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905017
C	Sridhar, S; Rempe, D; Valentin, J; Bouaziz, S; Guibas, LJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sridhar, Srinath; Rempe, Davis; Valentin, Julien; Bouaziz, Sofien; Guibas, Leonidas J.			Multiview Aggregation for Learning Category-Specific Shape Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We investigate the problem of learning category-specific 3D shape reconstruction from a variable number of RGB views of previously unobserved object instances. Most approaches for multiview shape reconstruction operate on sparse shape representations, or assume a fixed number of views. We present a method that can estimate dense 3D shape, and aggregate shape across multiple and varying number of input views. Given a single input view of an object instance, we propose a representation that encodes the dense shape of the visible object surface as well as the surface behind line of sight occluded by the visible surface. When multiple input views are available, the shape representation is designed to be aggregated into a single 3D shape using an inexpensive union operation. We train a 2D CNN to learn to predict this representation from a variable number of views (1 or more). We further aggregate multiview information by using permutation equivariant layers that promote order-agnostic view information exchange at the feature level. Experiments show that our approach is able to produce dense 3D reconstructions of objects that improve in quality as more views are added.	[Sridhar, Srinath; Rempe, Davis; Guibas, Leonidas J.] Stanford Univ, Stanford, CA 94305 USA; [Valentin, Julien; Bouaziz, Sofien] Google Inc, Mountain View, CA USA; [Guibas, Leonidas J.] Facebook AI Res, Menlo Pk, CA USA	Stanford University; Google Incorporated; Facebook Inc	Sridhar, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ssrinath@cs.stanford.edu			Google Daydream University Research Program; AWS Machine Learning Awards Program; Toyota-Stanford Center for AI Research; Toyota Research Institute ("TRI")	Google Daydream University Research Program(Google Incorporated); AWS Machine Learning Awards Program; Toyota-Stanford Center for AI Research; Toyota Research Institute ("TRI")	This work was supported by the Google Daydream University Research Program, AWS Machine Learning Awards Program, and the Toyota-Stanford Center for AI Research. We would like to thank Jiahui Lei, the anonymous reviewers, and members of the Guibas Group for useful feedback. Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.	Ahmed E., 2018, ARXIV PREPRINT ARXIV; Aittala M, 2018, LECT NOTES COMPUT SC, V11212, P748, DOI 10.1007/978-3-030-01237-3_45; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Cashman TJ, 2013, IEEE T PATTERN ANAL, V35, P232, DOI 10.1109/TPAMI.2012.68; Chang Angel X., 2015, ARXIV151203012CSGR P; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Esteves C, 2019, IEEE I CONF COMP VIS, P1568, DOI 10.1109/ICCV.2019.00165; Everitt C., 2001, WHITE PAP NVIDIA, V2, P7; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Feng Y, 2018, LECT NOTES COMPUT SC, V11218, P557, DOI 10.1007/978-3-030-01264-9_33; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Gu XF, 2002, ACM T GRAPHIC, V21, P355; Gwak J, 2017, INT CONF 3D VISION, P263, DOI 10.1109/3DV.2017.00038; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; Henderson P, 2019, ARXIV190106447; Huang PH, 2018, PROC CVPR IEEE, P2821, DOI 10.1109/CVPR.2018.00298; Insafutdinov E, 2018, ADV NEUR IN, V31; Ji MQ, 2017, IEEE I CONF COMP VIS, P2326, DOI 10.1109/ICCV.2017.253; Kanazawa Angjoo, 2018, P EUR C COMP VIS; Kar A., 2017, ADV NEURAL INFORM PR; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Kushal Akash M, 2003, P EUROGRAPHICS; Lin CH, 2018, AAAI CONF ARTIF INTE, P7114; Lin CY, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P3, DOI 10.1109/ISMAR-Adjunct.2018.00021; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maron Haggai, 2018, ARXIV181209902; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Pentland A., 1988, Second International Conference on Computer Vision (IEEE Cat. No.88CH2664-1), P404, DOI 10.1109/CCV.1988.590017; Qi Charles R, 2017, ARXIV170602413; Ravanbakhsh S, 2016, ARXIV PREPRINT ARXIV; Ravanbakhsh S, 2017, PR MACH LEARN RES, V70; Richter SR, 2018, PROC CVPR IEEE, P1936, DOI 10.1109/CVPR.2018.00207; Shade Jonathan, 1998, LAYERED DEPTH IMAGES; Shin D, 2018, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2018.00323; Steder B, 2011, IEEE INT CONF ROBOT, P2601, DOI 10.1109/ICRA.2011.5980187; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tulsiani S, 2018, PROC CVPR IEEE, P2897, DOI 10.1109/CVPR.2018.00306; Valentin J, 2015, PROC CVPR IEEE, P4400, DOI 10.1109/CVPR.2015.7299069; Vicente S, 2014, PROC CVPR IEEE, P41, DOI 10.1109/CVPR.2014.13; Wang H, 2019, PROC CVPR IEEE, P2637, DOI 10.1109/CVPR.2019.00275; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Wiles Olivia, 2017, P BMVC; Wu JJ, 2017, ADV NEUR IN, V30; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zhang X, 2018, PROCEEDINGS OF 2018 THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ARTIFICIAL INTELLIGENCE (CSAI 2018) / 2018 THE 10TH INTERNATIONAL CONFERENCE ON INFORMATION AND MULTIMEDIA TECHNOLOGY (ICIMT 2018), P263, DOI 10.1145/3297156.3297174; Zhu R, 2018, IEEE WINT CONF APPL, P894, DOI 10.1109/WACV.2018.00103	46	7	7	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302036
C	Tian, Y; Zhao, L; Peng, X; Metaxas, DN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tian, Yu; Zhao, Long; Peng, Xi; Metaxas, Dimitris N.			Rethinking Kernel Methods for Node Representation Learning on Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph kernels are kernel methods measuring graph similarity and serve as a standard tool for graph classification. However, the use of kernel methods for node classification, which is a related problem to graph representation learning, is still ill-posed and the state-of-the-art methods are heavily based on heuristics. Here, we present a novel theoretical kernel-based framework for node classification that can bridge the gap between these two representation learning problems on graphs. Our approach is motivated by graph kernel methodology but extended to learn the node representations capturing the structural information in a graph. We theoretically show that our formulation is as powerful as any positive semidefinite kernels. To efficiently learn the kernel, we propose a novel mechanism for node feature aggregation and a data-driven similarity metric employed during the training phase. More importantly, our framework is flexible and complementary to other graph-based deep learning models, e.g., Graph Convolutional Networks (GCNs). We empirically evaluate our approach on a number of standard node classification benchmarks, and demonstrate that our model sets the new state of the art. The source code is publicly available at https://github.com/bluer555/KernelGCN.	[Tian, Yu; Zhao, Long; Metaxas, Dimitris N.] Rutgers State Univ, New Brunswick, NJ 08901 USA; [Peng, Xi] Univ Delaware, Newark, DE 19716 USA	Rutgers State University New Brunswick; University of Delaware	Tian, Y (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.	yt219@cs.rutgers.edu; lz311@cs.rutgers.edu; xipen@udel.edu; dnm@cs.rutgers.edu			NSF [1763523, 1747778, 1733843, 1703883];  [ARO-MURI-68985NSMUR]	NSF(National Science Foundation (NSF)); 	This work is funded by ARO-MURI-68985NSMUR and NSF 1763523, 1747778, 1733843, 1703883.	Abu- El- Haija Sami, 2018, ARXIV180208888; Ahmed A., 2013, WWW, P37; [Anonymous], 2017, IEEE DATA ENG B; [Anonymous], 2001, SPRINGE SER STAT N; Belkin M, 2002, ADV NEUR IN, V14, P585; Borgwardt K. M., 2005, P IEEE INT C DAT MIN; Chen J., 2018, ARXIV180110247; Chen Y., 2019, P BRIT MACH VIS C BM; Chiang WL, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P257, DOI 10.1145/3292500.3330925; Draief M., 2018, ADV NEURAL INFORM PR, P4051; Fouss F, 2006, IEEE DATA MINING, P863; Giles C. L., 1998, Digital 98 Libraries. Third ACM Conference on Digital Libraries, P89, DOI 10.1145/276675.276685; Gilmer J, 2017, PR MACH LEARN RES, V70; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Haussler D, 1999, TECHNICAL REPORT; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Horvath Tamas, 2004, P 10 ACM SIGKDD INT, P158, DOI DOI 10.1145/1014052.1014072; Kipf T. N., 2017, P INT C LEARN REPR, P1; Kriege N. M., 2017, ARXIV170300676; Lee H, 2018, IEEE INT CONF COMM; Li Y., 2019, P INT C MACH LEARN I; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; Mikolov T., 2013, NIPS 2013, P3111; Neuhaus M, 2005, IEEE T SYST MAN CY B, V35, P503, DOI 10.1109/TSMCB.2005.846635; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Ramon J., 2003, PROC 1 INT WORKSHOP, P65; Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061; Scholkopf B., 2001, LEARNING KERNELS SUP; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N., 2009, NEURIPS, P1660; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Shin K, 2008, INT C MACH LEARN, P944, DOI [10.1145/1390156.1390275, DOI 10.1145/1390156.1390275]; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tang Z., 2018, P EUR C COMP VIS, P339; Tian Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P942; Velikovid P., 2018, P INT C LEARN REPR I; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Xu K., 2019, P INT C LEARN REPR I; Xu K., 2018, P 34 INT C MACH LEAR; Xu KW, 2018, PROCEEDINGS OF THE 2018 IEEE/MTT-S INTERNATIONAL MICROWAVE BIOMEDICAL CONFERENCE (IMBIOC), P34, DOI 10.1109/IMBIOC.2018.8428875; Yan S., 2018, P AAAI C ART INT AAA; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Yang Z, 2016, PR MACH LEARN RES, V48; You J., 2018, CORR ABS180602473, P6410; Zhang L., 2018, ARXIV181200086; Zhang M., 2018, PP AAAI C ART INT AA; Zhang Z, 2018, ADV NEURAL INFORM PR, P3964; Zhao L., 2019, P IEEE CVF C COMP VI, P3425; Zhao Y, 2018, INT C COMP SUPP COOP, P384	52	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903033
C	Toneva, M; Wehbe, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Toneva, Mariya; Wehbe, Leila			Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.	[Toneva, Mariya; Wehbe, Leila] Carnegie Mellon Univ, Dept Machine Learning, Neurosci Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Toneva, M (corresponding author), Carnegie Mellon Univ, Dept Machine Learning, Neurosci Inst, Pittsburgh, PA 15213 USA.	mariya@cmu.edu; lwehbe@cmu.edu	Toneva, Mariya/CAH-3617-2022	Toneva, Mariya/0000-0002-2407-9871	National Science Foundation [DGE1745016]; Google	National Science Foundation(National Science Foundation (NSF)); Google(Google Incorporated)	We thank Tom Mitchell for valuable discussions. We thank the National Science Foundation for supporting this work through the Graduate Research Fellowship under Grant No. DGE1745016, and Google for supporting this work through the Google Faculty Award.	BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Blank I., 2017, J NEUROSCI, P3642; Brennan J., 2010, BRAIN LANGUAGE; Cer D., 2018, ARXIV180311175 CS; Chen Yining, 2017, ARXIV171105408; Conneau Alexis, 2018, ARXIV180501070; Dai Z., 2019, ARXIV190102860; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Fedorenko E, 2014, TRENDS COGN SCI, V18, P120, DOI 10.1016/j.tics.2013.12.006; Frank SL, 2015, BRAIN LANG, V140, P1, DOI 10.1016/j.bandl.2014.10.006; Friederici AD, 2011, PHYSIOL REV, V91, P1357, DOI 10.1152/physrev.00006.2011; Fyshe A, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P489; Gao JS, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00023; Gardner Matt, 2017, ALLENNLP DEEP SEMANT; Goldberg Yoav, 2019, ARXIV190105287; Hagoort P, 2003, NEUROIMAGE, V20, pS18, DOI 10.1016/j.neuroimage.2003.09.013; Hale J., 2018, ARXIV180604127; Hickok G, 2007, NAT REV NEUROSCI, V8, P393, DOI 10.1038/nrn2113; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Huth AG, 2016, NATURE, V532, P453, DOI 10.1038/nature17637; Jain Shailee, 2018, BIORXIV; Kiros R., 2015, ADV NEURAL INF PROCE, P3294; Kubilius J., 2019, ANN C NEUR INF PROC, P12785; Lerner Y, 2011, J NEUROSCI, V31, P2906, DOI 10.1523/JNEUROSCI.3684-10.2011; Linzen T., 2016, ARXIV161101368; Marvin R., 2018, ARXIV180809031; Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876; Nishimoto S., 2011, CURRENT BIOL; Peng H., 2018, ARXIV180809357; Peters Matthew, 2018, DEEP CONTEXTUALIZED, P2227, DOI [10.18653/v1/N18-1202, DOI 10.18653/V1/N18-1202]; Poldrack RA, 2006, TRENDS COGN SCI, V10, P59, DOI 10.1016/j.tics.2005.12.004; Rowling J. K., 2012, HARRY POTTER SORCERE; Scgaard A., 2016, P 1 WORKSH EV VECT S, P116; Speer NK, 2009, PSYCHOL SCI, V20, P989, DOI 10.1111/j.1467-9280.2009.02397.x; Sudre G, 2012, NEUROIMAGE, V62, P451, DOI 10.1016/j.neuroimage.2012.04.048; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wehbe L., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1030; Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575; Weiss G., 2018, ARXIV180504908; Zhu XJ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P632	40	7	7	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906059
C	Trapp, M; Peharz, R; Ge, H; Pernkopf, F; Ghahramani, Z		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Trapp, Martin; Peharz, Robert; Ge, Hong; Pernkopf, Franz; Ghahramani, Zoubin			Bayesian Learning of Sum-Product Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Sum-product networks (SPNs) are flexible density estimators and have received significant attention due to their attractive inference properties. While parameter learning in SPNs is well developed, structure learning leaves something to be desired: Even though there is a plethora of SPN structure learners, most of them are somewhat ad-hoc and based on intuition rather than a clear learning principle. In this paper, we introduce a well-principled Bayesian framework for SPN structure learning. First, we decompose the problem into i) laying out a computational graph, and ii) learning the so-called scope function over the graph. The first is rather unproblematic and akin to neural network architecture validation. The second represents the effective structure of the SPN and needs to respect the usual structural constraints in SPN, i.e. completeness and decomposability. While representing and learning the scope function is somewhat involved in general, in this paper, we propose a natural parametrisation for an important and widely used special case of SPNs. These structural parameters are incorporated into a Bayesian model, such that simultaneous structure and parameter learning is cast into monolithic Bayesian posterior inference. In various experiments, our Bayesian SPNs often improve test likelihoods over greedy SPN learners. Further, since the Bayesian framework protects against overfitting, we can evaluate hyper-parameters directly on the Bayesian model score, waiving the need for a separate validation set, which is especially beneficial in low data regimes. Bayesian SPNs can be applied to heterogeneous domains and can easily be extended to nonparametric formulations. Moreover, our Bayesian approach is the first, which consistently and robustly learns SPN structures under missing data.	[Trapp, Martin; Pernkopf, Franz] Graz Univ Technol, Graz, Austria; [Trapp, Martin] OFAI, Goa, India; [Peharz, Robert; Ge, Hong; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England; [Ghahramani, Zoubin] Uber AI, San Francisco, CA USA	Graz University of Technology; University of Cambridge	Trapp, M (corresponding author), Graz Univ Technol, Graz, Austria.; Trapp, M (corresponding author), OFAI, Goa, India.	martin.trapp@tugraz.at; rp587@cam.ac.uk; hg344@cam.ac.uk; pernkopf@tugraz.at; zoubin@eng.cam.ac.uk		Trapp, Martin/0000-0003-1725-3381; Peharz, Robert/0000-0002-8644-9655	Austrian Science Fund (FWF) [I2706-N31]; European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant [797223-HYBSPN]	Austrian Science Fund (FWF)(Austrian Science Fund (FWF)); European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant(SKA South Africa)	This work was partially funded by the Austrian Science Fund (FWF): I2706-N31 and has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie Grant Agreement No. 797223-HYBSPN.	Adel T., 2015, P UAI; Beretta L, 2016, BMC MED INFORM DECIS, V16, DOI 10.1186/s12911-016-0318-z; Butz CJ, 2019, AAAI CONF ARTIF INTE, P3248; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Dennis A., 2012, ADV NEURAL INFORM PR, P2042; Dennis A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P932; Di Mauro N, 2017, LECT NOTES ARTIF INT, V10534, P203, DOI 10.1007/978-3-319-71249-9_13; Friedman N, 2003, MACH LEARN, V50, P95, DOI 10.1023/A:1020249912095; Ge H, 2015, PR MACH LEARN RES, V37, P2276; Gens R., 2012, 26 ADV NEURAL INFORM, P3239; Gens R., 2013, 30 INT C MACHINE LEA, P873; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Jaini P., 2018, P PGM, V72, P181; Kalra A, 2018, ADV NEUR IN, V31; Kang H, 2016, IEEE J-STSP, V10, P130, DOI 10.1109/JSTSP.2015.2502542; Kisa D, 2014, FOURTEENTH INTERNATIONAL CONFERENCE ON THE PRINCIPLES OF KNOWLEDGE REPRESENTATION AND REASONING, P558; Koller D., 2009, PROBABILISTIC GRAPHI; Lee S., 2014, P LTPM WORKSH ICML; Liang YT, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Liu B, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL ENGINEERING AND ENGINEERING MANAGEMENT (IEEM), P1310, DOI 10.1109/IEEM.2016.7798090; MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491; Molina A, 2018, AAAI CONF ARTIF INTE, P3828; Peharz Robert, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference (ECML PKDD 2013). Proceedings: LNCS 8189, P612, DOI 10.1007/978-3-642-40991-2_39; Peharz R., 2015, P AISTATS; Peharz R., 2019, P UAI; Peharz R., 2014, P LTPM WORKSH ICML; Peharz R, 2017, IEEE T PATTERN ANAL, V39, P2030, DOI 10.1109/TPAMI.2016.2618381; Polit D. F., 2012, NURSING RES GENERATI; Poon H., 2011, P 27 C UNC ART INT, P337, DOI DOI 10.1109/ICCVW.2011; Rahman Tahrima, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P630, DOI 10.1007/978-3-662-44851-9_40; Rahman T., 2019, P MACH LEARN RES, V97, P5311; Rashwan A., 2018, P 9 INT C PROB MOD, P356; Rashwan A, 2016, JMLR WORKSH CONF PRO, V51, P1469; Rasmussen CE, 2001, ADV NEUR IN, V13, P294; Rooshenas A, 2014, PR MACH LEARN RES, V32; Seo S, 2013, KOR-JPN JT WORKS FR, P209, DOI 10.1109/FCV.2013.6485489; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Sharir O., 2016, ARXIV161004167; Suzuki J., 1993, Uncertainty in Artificial Intelligence. Proceedings of the Ninth Conference (1993), P266; Trapp M., 2019, CORR; Trapp M., 2017, P UAI; Trapp M., 2016, P BNP WORKSH NEURIPS; Vergari A., 2019, P AAAI; Vergari A, 2015, LECT NOTES ARTIF INT, V9285, P343, DOI 10.1007/978-3-319-23525-7_21; Zhao H., 2016, NIPS 16, P433; Zhao QF, 2015, INT CONF MACH LEARN, P116, DOI 10.1109/ICMLC.2015.7340908; Zoph B., 2017, P1	47	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306036
C	Vayer, T; Flamary, R; Tavenard, R; Chapel, L; Courty, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vayer, Titouan; Flamary, Remi; Tavenard, Romain; Chapel, Laetitia; Courty, Nicolas			Sliced Gromov-Wasserstein	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMAL TRANSPORT; ASSIGNMENT	Recently used in various machine learning contexts, the Gromov-Wasserstein distance (GW) allows for comparing distributions whose supports do not necessarily lie in the same metric space. However, this Optimal Transport (OT) distance requires solving a complex non convex quadratic program which is most of the time very costly both in time and memory. Contrary to GW, the Wasserstein distance (W) enjoys several properties (e.g. duality) that permit large scale optimization. Among those, the solution of W on the real line, that only requires sorting discrete samples in ID, allows defining the Sliced Wasserstein (SW) distance. This paper proposes a new divergence based on GW akin to SW. We first derive a closed form for GW when dealing with ID distributions, based on a new result for the related quadratic assignment problem. We then define a novel OT discrepancy that can deal with large scale distributions via a slicing approach and we show how it relates to the GW distance while being O(n log(n)) to compute. We illustrate the behavior of this so called Sliced Gromov-Wasserstein (SGW) discrepancy in experiments where we demonstrate its ability to tackle similar problems as GW while being several order of magnitudes faster to compute.	[Vayer, Titouan; Chapel, Laetitia; Courty, Nicolas] Univ Bretagne Sud, CNRS, IRISA, F-56000 Vannes, France; [Flamary, Remi] Univ Cote Azur, OCA, Lagrange, F-06000 Nice, France; [Tavenard, Romain] Univ Rennes, CNRS, LETG, F-35000 Rennes, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur; Observatoire de la Cote d'Azur; Centre National de la Recherche Scientifique (CNRS); Universite de Rennes	Vayer, T (corresponding author), Univ Bretagne Sud, CNRS, IRISA, F-56000 Vannes, France.	titouan.vayer@irisa.fr; remi.flamary@unice.fr; romain.tavenard@univ-rennes2.fr; laetitia.chapel@irisa.fr; nicolas.courty@irisa.fr	Flamary, Rémi/AAC-1958-2022	Flamary, Rémi/0000-0002-4212-6627	French National Research Agency (ANR) [ANR-17-CE23-0012]	French National Research Agency (ANR)(French National Research Agency (ANR))	We would like to thank Nicolas Klutchnikoff for the hepful discussions. This work benefited from the support from OATMIL ANR-17-CE23-0012 project of the French National Research Agency (ANR). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X GPU used for this research.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Alvarez-Melis D., 2018, C EMP METH NAT LANG; Alvarez-Melis D, 2019, PR MACH LEARN RES, V89; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Bonnotte N., 2013, UNIDIMENSIONAL EVOLU; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bunne C, 2019, PR MACH LEARN RES, V97; Cela E., 2013, QUADRATIC ASSIGNMENT, V1; Cela E, 2018, EUR J OPER RES, V267, P818, DOI 10.1016/j.ejor.2017.12.024; Cela E, 2015, DISCRETE APPL MATH, V186, P56, DOI 10.1016/j.dam.2015.01.005; Cela E, 2011, DISCRETE OPTIM, V8, P411, DOI 10.1016/j.disopt.2011.02.002; Charlier B., 2018, KERNEL OPERATIONSON; Chowdhury S., 2018, ARXIV180804337; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Deshpande I., 2019, IEEE C COMP VIS PATT; Deshpande I, 2018, PROC CVPR IEEE, P3483, DOI 10.1109/CVPR.2018.00367; Ezuz D, 2017, COMPUT GRAPH FORUM, V36, P49, DOI 10.1111/cgf.13244; Flamary R<prime>emi, 2017, POT PYTHON OPTIMAL T; Huang G., 2016, PROC NEURAL INF PROC, P4869; Kolouri S, 2019, INT C LEARN REPR; Kolouri S, 2016, PROC CVPR IEEE, P5258, DOI 10.1109/CVPR.2016.568; KOOPMANS TC, 1957, ECONOMETRICA, V25, P53, DOI 10.2307/1907742; Liutkus A, 2019, PR MACH LEARN RES, V97; Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032; Maclaurin D., 2015, ICML 2015 AUTOML WOR, V238; Maron H., 2018, ADV NEURAL INFORM PR, P408; Meghwanshi M., 2018, MCTORCH MANIFOLD OPT; Memoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5; Needham Tom, 2018, ARXIV181009646; Paty FP, 2019, PR MACH LEARN RES, V97; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Peyre G, 2016, PR MACH LEARN RES, V48; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Rustamov RM, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461959; Schmitzer B., 2016, ARXIV161006519; Solomon J, 2016, CYTOTHERAPY, V18, P1, DOI 10.1016/j.jcyt.2015.09.010; Sturm Karl-Theodor, 2012, ARXIV12080434; Thorpe M, 2017, J MATH IMAGING VIS, V59, P187, DOI 10.1007/s10851-017-0726-4; Townsend J, 2016, J MACH LEARN RES, V17; Vayer T, 2019, PR MACH LEARN RES, V97; Villani C., 2008, OPTIMAL TRANSPORT OL; Wu JQ, 2019, PROC CVPR IEEE, P3708, DOI 10.1109/CVPR.2019.00383; Yan YG, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2969	48	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906041
C	Wang, BX; Hegde, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Baoxiang; Hegde, Nidhi			Privacy-preserving Q-Learning with Functional Noise in Continuous Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider differentially private algorithms for reinforcement learning in continuous spaces, such that neighboring reward functions are indistinguishable. This protects the reward information from being exploited by methods such as inverse reinforcement learning. Existing studies that guarantee differential privacy are not extendable to infinite state spaces, as the noise level to ensure privacy will scale accordingly to infinity. Our aim is to protect the value function approximator, without regard to the number of states queried to the function. It is achieved by adding functional noise to the value function iteratively in the training. We show rigorous privacy guarantees by a series of analyses on the kernel of the noise space, the probabilistic bound of such noise samples, and the composition over the iterations. We gain insight into the utility analysis by proving the algorithm's approximate optimality when the state space is discrete. Experiments corroborate our theoretical findings and show improvement over existing approaches.	[Wang, Baoxiang] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Wang, Baoxiang; Hegde, Nidhi] Borealis AI, Edmonton, AB, Canada	Chinese University of Hong Kong; Borealis AG	Wang, BX (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.; Wang, BX (corresponding author), Borealis AI, Edmonton, AB, Canada.	bxwang@cse.cuhk.edu.hk; nidhi.hegde@borealisai.com						Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Abbeel P., 2004, P 21 INT C MACHINE L, P1; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Balle B., 2016, JMLR WORKSHOP C P, V48, P2130; Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437, DOI 10.1007/978-3-642-11799-2_26; Chamikara MAP, 2019, ARXIV190802997; Degris T., 2012, P 29 INT C MACH LEAR, P457; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Gajane Pratik, 2017, ARXIV170805033; Hall R, 2013, J MACH LEARN RES, V14, P703; Hu YJ, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P368, DOI 10.1145/3219819.3219846; Jain P., 2012, P 25 ANN C LEARNING, V23; Kairouz Peter, 2013, ARXIV13110776; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Kusner MJ, 2015, PR MACH LEARN RES, V37, P918; Lee C., 2017, ARXIV171110019; Liebman E, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P591; Melhem SB, 2017, 2017 IEEE 5TH INTERNATIONAL CONFERENCE ON FUTURE INTERNET OF THINGS AND CLOUD (FICLOUD 2017), P32, DOI 10.1109/FiCloud.2017.37; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Mishra N, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P592; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pan Yangchen, 2018, ARXIV180606931; Rosset C, 2018, ACM/SIGIR PROCEEDINGS 2018, P1193, DOI 10.1145/3209978.3210127; Sajed Touqir, 2019, INT C MACH LEARN; Shariff Roshan, 2018, P 32 NIPS, P4301; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Thakurta A., 2013, ADV NEURAL INFORM PR, P2733; Tossou Aristide Charles Yedia, 2017, 31 AAAI C ART INT; Tossou Aristide CY, 2016, 30 AAAI C ART INT; Venkitasubramaniam P, 2013, ANN ALLERTON CONF, P381, DOI 10.1109/Allerton.2013.6736549; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Zheng GJ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P167, DOI 10.1145/3178876.3185994	38	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903001
C	Wijesinghe, A; Wang, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wijesinghe, Asiri; Wang, Qing			DFNets: Spectral CNNs for Graphs with Feedback-Looped Filters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel spectral convolutional neural network (CNN) model on graph structured data, namely Distributed Feedback-Looped Networks (DFNets). This model is incorporated with a robust class of spectral graph filters, called feedback-looped filters, to provide better localization on vertices, while still attaining fast convergence and linear memory requirements. Theoretically, feedback-looped filters can guarantee convergence w.r.t. a specified error bound, and be applied universally to any graph without knowing its structure. Furthermore, the propagation rule of this model can diversify features from the preceding layers to produce strong gradient flows. We have evaluated our model using two benchmark tasks: semi-supervised document classification on citation networks and semi-supervised entity classification on a knowledge graph. The experimental results show that our model considerably outperforms the state-of-the-art methods in both benchmark tasks over all datasets.	[Wijesinghe, Asiri; Wang, Qing] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia	Australian National University	Wijesinghe, A (corresponding author), Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia.	asiri.wijesinghe@anu.edu.au; qing.wang@anu.edu.au		Wang, Qing/0000-0001-9504-4273				[Anonymous], 2015, DEEP CONVOLUTIONAL N; Atwood J, 2016, C WORKSH NEUR INF PR, P1993; Bruna J., 2013, INT C LEARN REPR ICL; Carlson Andrew, 2010, 24 AAAI C ART INT AA; Chung FR, 1997, SPECTRAL GRAPH THEOR, V92; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Douglas SC, 2000, IEEE T SIGNAL PROCES, V48, P1843, DOI 10.1109/78.845952; Duvenaud David K, 2015, P NIPS; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Hu W, 2015, J SENSORS, V2015, DOI 10.1155/2015/258619; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Isufi E, 2017, INT CONF ACOUST SPEE, P4119, DOI 10.1109/ICASSP.2017.7952931; Isufi E, 2017, IEEE T SIGNAL PROCES, V65, P274, DOI 10.1109/TSP.2016.2614793; Kingma DP, 2015, INT C LEARN REPR ICL; Kipf T.N., 2017, INT C LEARN REPR; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624; Li Yi, 2017, P IEEE C COMP VIS PA; Liao R., 2019, P 7 INT C LEARN REPR; Livi C., 2019, ARXIV190101343; Lu Q., 2003, P 20 INT C MACH LEAR, P496, DOI DOI 10.5555/3041838.3041901; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Rostamizadeh A., 2009, P 25 C UNC ART INT, P109, DOI DOI 10.5555/1795114.1795128; Sandryhaila A, 2013, INT CONF ACOUST SPEE, P6167, DOI 10.1109/ICASSP.2013.6638850; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Velickovic P., 2017, INT C EARN REPR ICLR; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Yang Z, 2016, PR MACH LEARN RES, V48; Zhu Xiaojin., 2003, P ICLR, P912	34	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306006
C	Xu, ZQ; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Zhiqiang; Li, Ping			Towards Practical Alternating Least-Squares for CCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Alternating least-squares (ALS) is a simple yet effective solver for canonical correlation analysis (CCA). In terms of ease of use, ALS is arguably practitioners' first choice. Despite recent provably guaranteed variants, the empirical performance often remains unsatisfactory. To promote the practical use of ALS for CCA, we propose truly alternating least-squares. Instead of approximately solving two independent linear systems, in each iteration, it simply solves two coupled linear systems of half the size. It turns out that this coupling procedure is able to bring significant performance improvements in practical setting. Inspired by the accelerated power method, we further propose faster alternating least-squares, where momentum terms are introduced into the update equations. Theoretically, both algorithms enjoy linear convergence rate. To make faster ALS even more practical, we put forward adaptive alternating least-squares to avoid tuning the momentum parameter, which is as easy to use as the plain ALS while retaining advantages of the fast version. Experiments on several datasets empirically demonstrate the superiority of the proposed algorithms to several recent variants of CCA solvers.	[Xu, Zhiqiang] Baidu Res, Cognit Comp Lab, 10 Xibeiwang East Rd, Beijing 10085, Peoples R China; 10900 NE 8th St, Bellevue, WA 98004 USA	Baidu	Xu, ZQ (corresponding author), Baidu Res, Cognit Comp Lab, 10 Xibeiwang East Rd, Beijing 10085, Peoples R China.	xuzhiqiang04@baidu.com; liping11@baidu.com	Xu, Zhiqiang/AAB-7414-2022					Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; Arora R., 2017, ADV NEURAL INFORM PR, P4778; Avron Haim, 2014, SIAM J SCI COMPUT, V36; Bhatia K., 2018, ADV NEURAL INFORM PR; Chaudhuri K., 2009, PROC INT C MACHINE L, P129, DOI DOI 10.1145/1553374.1553391; Chen Zhehui, 2019, PR MACH LEARN RES, P916; Dhillon Paramveer, 2011, ADV NEURAL INFORM PR, V24, P199; Gao C., 2017, 170206533V1 ARXIV; Garber D, 2016, PR MACH LEARN RES, V48; Ge R, 2016, PR MACH LEARN RES, V48; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Kakade SM, 2007, LECT NOTES COMPUT SC, V4539, P82, DOI 10.1007/978-3-540-72927-3_8; Karampatziakis N, 2014, PR MACH LEARN RES, V32; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lu Y., 2014, ADV NEURAL INF PROCE, P91; Ma Z, 2015, PR MACH LEARN RES, V37, P169; Snoek C.G., 2006, P 14 ANN ACM INT C M, P421, DOI DOI 10.1145/1180639.1180727; Vinod HD., 1976, J EC, V4, P147, DOI [10.1016/0304-4076(76)90010-5, DOI 10.1016/0304-4076(76)90010-5]; Wang P, 2016, PROCEEDINGS OF THE 2016 11TH INTERNATIONAL SYMPOSIUM ON ANTENNAS, PROPAGATION AND EM THEORY (ISAPE), P766, DOI 10.1109/ISAPE.2016.7834108; Westbury J. R., 1994, IEEE PERSONAL COMMUN; Xu Peng, 2018, INT C ART INT STAT A, P58; Yger Florian, 2012, P 29 INT C MACH LEAR; Zhang Zhihua, 2015, ABS151008532 CORR	23	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906042
C	Yang, C; Zhuang, PY; Shi, WH; Luu, A; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Carl; Zhuang, Peiye; Shi, Wenhan; Luu, Alan; Li, Pan			Conditional Structure Generation through Graph Variational Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIMENSIONALITY REDUCTION	Graph embedding has been intensively studied recently, due to the advance of various neural network models. Theoretical analyses and empirical studies have pushed forward the translation of discrete graph structures into distributed representation vectors, but seldom considered the reverse direction, i.e., generation of graphs from given related context spaces. Particularly, since graphs often become more meaningful when associated with semantic contexts (e.g., social networks of certain communities, gene networks of certain diseases), the ability to infer graph structures according to given semantic conditions could be of great value. While existing graph generative models only consider graph structures without semantic contexts, we formulate the novel problem of conditional structure generation, and propose a novel unified model of graph variational generative adversarial nets (CONDGEN) to handle the intrinsic challenges of flexible context-structure conditioning and permutation-invariant generation. Extensive experiments on two deliberately created benchmark datasets of real-world context-enriched networks demonstrate the supreme effectiveness and generalizability of CONDGEN.	[Yang, Carl; Zhuang, Peiye; Shi, Wenhan; Luu, Alan; Li, Pan] Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Yang, C (corresponding author), Univ Illinois, Urbana, IL 61801 USA.	jiyang3@illinois.edu; peiye@illinois.edu; wenhans2@illinois.edu; alanluu2@illinois.edu; panli2@illinois.edu			U.S. Army Research Lab [W911NF-09-2-0053]; DARPA [W911NF-17-C-0099, FA8750-19-2-1004]; National Science Foundation [IIS 16-18481, IIS 17-04532, IIS-17-41317]; DTRA [HD-TRA11810026]; NIGMS through trans-NIH Big Data to Knowledge (BD2K) initiative [1U54GM114838]	U.S. Army Research Lab(United States Department of DefenseUS Army Research Laboratory (ARL)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation(National Science Foundation (NSF)); DTRA(United States Department of DefenseDefense Threat Reduction Agency); NIGMS through trans-NIH Big Data to Knowledge (BD2K) initiative	Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreements No. W911NF-17-C-0099 and FA8750-19-2-1004, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS-17-41317, DTRA HD-TRA11810026, and grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).The results shown in this work are or part based upon data generated by the TCGA Research Network: https://www.cancer.gov/tcga.	Bai Y., 2019, ICLR; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Ben-Hamu H., 2019, ICLR; Bojchevski Aleksandar, 2018, ICML; Chen J., 2018, ICLR; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; De Cao N., 2018, ICML 2018 WORKSH THE; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Du JC, 2019, BMC GENOMICS, V20, DOI 10.1186/s12864-018-5370-x; ERDOS P, 1960, B INT STATIST INST, V38, P343; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Grover Aditya, 2017, ARXIV180310459; Gu X., 2019, ICLR; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hartford Jason, 2018, PMLR; Jegelka S., 2019, ICLR; Jin W., 2018, ICML; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kipf Thomas N., 2017, INT C LEARNING REPRE; Kipf Thomas N, 2016, NIPS WORKSHOP BAYESI; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Larsen A., 2016, ICML; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Li Q., 2018, AAAI; Li Yujia, 2018, ICML; Ma T., 2018, ARXIV180902630; Mikolov T., 2010, INTERSPEECH; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mirza M., 2014, ARXIV PREPRINT ARXIV; Murphy R. L., 2019, ICLR; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Qiu JZ, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P459, DOI 10.1145/3159652.3159706; Robins G, 2007, SOC NETWORKS, V29, P173, DOI 10.1016/j.socnet.2006.08.002; Rosca Mihaela, 2017, ARXIV170604987; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Simonovsky M, 2018, LECT NOTES COMPUT SC, V11139, P412, DOI 10.1007/978-3-030-01418-6_41; Sohn Kihyuk, 2015, NEURAL INFORM PROCES; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Velickovic P., 2018, P 6 INT C LEARN REPR; Wang H., 2018, AAAI; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Yan Zhang, 2019, NEURIPS; Yan Zhang, 2020, ICLR; Yang C., 2019, KDD; Yang C, 2018, IEEE DATA MINING, P657, DOI 10.1109/ICDM.2018.00081; Ying R., 2018, P 2018 ANN C NEUR IN; You J., 2018, ICML; You J., 2018, CORR ABS180602473, P6410; Zhang Muhan, 2018, AAAI; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zou Dongmian, 2018, ARXIV180910851	52	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301034
C	Yoshida, Y; Okada, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yoshida, Yuki; Okada, Masato			Data-Dependence of Plateau Phenomenon in Learning with Neural Network - Statistical Mechanical Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DYNAMICS	The plateau phenomenon, wherein the loss value stops decreasing during the process of learning, has been reported by various researchers. The phenomenon is actively inspected in the 1990s and found to be due to the fundamental hierarchical structure of neural network models. Then the phenomenon has been thought as inevitable. However, the phenomenon seldom occurs in the context of recent deep learning. There is a gap between theory and reality. In this paper, using statistical mechanical formulation, we clarified the relationship between the plateau phenomenon and the statistical property of the data learned. It is shown that the data whose covariance has small and dispersed eigenvalues tend to make the plateau phenomenon inconspicuous.	[Yoshida, Yuki; Okada, Masato] Univ Tokyo, Grad Sch Frontier Sci, Dept Complex Sci & Engn, 5-1-5 Kashiwanoha, Kashiwa, Chiba 2778561, Japan	University of Tokyo	Yoshida, Y (corresponding author), Univ Tokyo, Grad Sch Frontier Sci, Dept Complex Sci & Engn, 5-1-5 Kashiwanoha, Kashiwa, Chiba 2778561, Japan.	yoshida@mns.k.u-tokyo.ac.jp; okada@edu.k.u-tokyo.ac.jp			JSPS KAKENHI [18H04106]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JSPS KAKENHI Grant-in-Aid for Scientific Research(A) (No. 18H04106).	Cousseau F, 2008, IEEE T NEURAL NETWOR, V19, P1313, DOI 10.1109/TNN.2008.2000391; Emin Orhan A., 2017, ARXIV170109175; Goldt S., 2019, ARXIV190608632; Guo WL, 2018, IEEE ACCESS, V6, P60214, DOI 10.1109/ACCESS.2018.2873811; Hara K, 2016, LECT NOTES COMPUT SC, V9887, P72, DOI 10.1007/978-3-319-44781-0_9; Milnor J, 1985, THEORY CHAOTIC ATTRA, P243, DOI DOI 10.1007/978-0-387-21830-4_15; Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4; RIEGLER P, 1995, J PHYS A-MATH GEN, V28, pL507, DOI 10.1088/0305-4470/28/20/002; Straat M, 2019, ARXIV190307378; Straat M, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20100775; Takagi S, 2019, J PHYS SOC JPN, V88, DOI 10.7566/JPSJ.88.074003; Wei HK, 2008, NEURAL COMPUT, V20, P813, DOI 10.1162/neco.2007.12-06-414; Yoshida Y, 2017, J PHYS SOC JPN, V86, DOI 10.7566/JPSJ.86.044002; Yoshida Yuki, 2019, J PHYS A	16	7	6	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301068
C	Yu, LT; Yu, TH; Finn, C; Ermon, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Lantao; Yu, Tianhe; Finn, Chelsea; Ermon, Stefano			Meta-Inverse Reinforcement Learning with Probabilistic Context Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Providing a suitable reward function to reinforcement learning can be difficult in many real world applications. While inverse reinforcement learning (IRL) holds promise for automatically learning reward functions from demonstrations, several major challenges remain. First, existing IRL methods learn reward functions from scratch, requiring large numbers of demonstrations to correctly infer the reward for each task the agent may need to perform. Second, existing methods typically assume homogeneous demonstrations for a single behavior or task, while in practice, it might be easier to collect datasets of heterogeneous but related behaviors. To this end, we propose a deep latent variable model that is capable of learning rewards from demonstrations of distinct but related tasks in an unsupervised way. Critically, our model can infer rewards for new, structurally-similar tasks from a single demonstration. Our experiments on multiple continuous control tasks demonstrate the effectiveness of our approach compared to state-of-the-art imitation and inverse reinforcement learning methods.	[Yu, Lantao; Yu, Tianhe; Finn, Chelsea; Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Yu, LT (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	lantaoyu@cs.stanford.edu; tianheyu@cs.stanford.edu; cbfinn@cs.stanford.edu; ermon@cs.stanford.edu		Finn, Chelsea/0000-0001-6298-0874	Toyota Research Institute; NSF [1651565, 1522054, 1733686]; ONR [N00014-19-1-2145]; AFOSR [FA9550-19-1-0024]	Toyota Research Institute; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This research was supported by Toyota Research Institute, NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024). The authors would like to thank Chris Cundy for discussions over the paper draft.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Amodei D., 2016, CONCRETE PROBLEMS AI; Andrychowicz M, 2016, ADV NEUR IN, V29; Bengio Y., 1991, IJCNN-91-Seattle: International Joint Conference on Neural Networks (Cat. No.91CH3049-4), DOI 10.1109/IJCNN.1991.155621; Duan Y., 2016, RL2 FAST REINFORCEME; Finn C., 2016, ABS161103852 CORR; Finn C, 2017, PR MACH LEARN RES, V70; Finn C, 2016, PR MACH LEARN RES, V48; Fu J., 2017, ARXIV171011248; Gleave A., 2018, MULTITASK MAXIMUM EN; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hausman K., 2018, INT C LEARN REPR; Hausman K, 2017, ADV NEUR IN, V30; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kuefler Alex, 2018, P 17 INT C AUT AG MU; Levine Sergey, 2018, ARXIV180500909; Li K., 2017, ARXIV170300441; Li Yuxi, 2017, ARXIV170107274; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Peng Xue Bin, 2018, ARXIV PREPRINT ARXIV; Rakelly K, 2019, PR MACH LEARN RES, V97; RATLIFF N, 2006, P 23 INT C MACH LEAR; Ross St<prime>ephane, 2011, AISTATS; Saemundsson S, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P642; Santoro A, 2016, PR MACH LEARN RES, V48; Schaal S, 2003, PHILOS T R SOC B, V358, P537, DOI 10.1098/rstb.2002.1258; Schmidhuber J, 1987, THESIS; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Sharma M., 2018, INT C LEARN REPR; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Xu KYL, 2018, PR MACH LEARN RES, V80; Yu TH, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Yu Tianhe, 2018, ARXIV181011043; Zhang T., 2018, INT C ROB AUT ICRA; Zhao S., 2018, ARXIV180606514; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart B. D., 2010, THESIS	42	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903039
C	Yu, WJ; Zhou, JW; Yu, WH; Liang, XD; Xiao, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Weijiang; Zhou, Jingwen; Yu, Weihao; Liang, Xiaodan; Xiao, Nong			Heterogeneous Graph Learning for Visual Commonsense Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Visual commonsense reasoning task aims at leading the research field into solving cognition-level reasoning with the ability of predicting correct answers and meanwhile providing convincing reasoning paths, resulting in three sub-tasks i.e., Q -> A, QA -> R and Q -> AR. It poses great challenges over the proper semantic alignment between vision and linguistic domains and knowledge reasoning to generate persuasive reasoning paths. Existing works either resort to a powerful end-to-end network that cannot produce interpretable reasoning paths or solely explore intra-relationship of visual objects (homogeneous graph) while ignoring the cross-domain semantic alignment among visual concepts and linguistic words. In this paper, we propose a new Heterogeneous Graph Learning (HGL) framework for seamlessly integrating the intra-graph and inter-graph reasoning in order to bridge vision and language domain. Our HGL consists of a primal vision-to-answer heterogeneous graph (VAHG) module and a dual question-to-answer heterogeneous graph (QAHG) module to interactively refine reasoning paths for semantic agreement. Moreover, our HGL integrates a contextual voting module to exploit long-range visual context for better global reasoning. Experiments on the large-scale Visual Commonsense Reasoning benchmark demonstrate the superior performance of our proposed modules on three tasks (improving 5% accuracy on Q -> A, 3.5% on QA -> R, 5.8% on Q -> AR)(2).	[Yu, Weijiang; Zhou, Jingwen; Yu, Weihao; Xiao, Nong] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangzhou, Peoples R China; [Liang, Xiaodan] Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Peoples R China	Sun Yat Sen University; Sun Yat Sen University	Liang, XD (corresponding author), Sun Yat Sen Univ, Sch Intelligent Syst Engn, Guangzhou, Peoples R China.	weijiangyu8@gmail.com; zhoujw57@mail2.sysu.edu.cn; weihaoyu6@gmail.com; xdliang328@gmail.com; xiaon6@sysu.edu.cn			National Natural Science Foundation of China (NSFC) [61976233]; Natural Science Foundation of Guangdong Province, China [2018B030312002]	National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province, China(National Natural Science Foundation of Guangdong Province)	This work was supported in part by the National Natural Science Foundation of China (NSFC) under Grant No.1811461, in part by the National Natural Science Foundation of China (NSFC) under Grant No.61976233, and in part by the Natural Science Foundation of Guangdong Province, China under Grant No.2018B030312002.	Abu-El-Haija S, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1787, DOI 10.1145/3132847.3132959; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bruna J, 2013, PROC INT C LEARN REP; Cao SS, 2016, AAAI CONF ARTIF INTE, P1145; Cao Yu, 2019, ARXIV190404969; Chen Q, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1657, DOI 10.18653/v1/P17-1152; Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; Deng CR, 2018, PROC CVPR IEEE, P7746, DOI 10.1109/CVPR.2018.00808; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duvenaud David K, 2015, P NIPS; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Guo D., 2019, ARXIV190209774; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu RH, 2017, PROC CVPR IEEE, P4418, DOI 10.1109/CVPR.2017.470; Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Jain U., 2017, P IEEE C COMP VIS PA, P6485; Ke L., 2019, ARXIV190302547; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krishna R., 2019, ARXIV190311207; Li Y., 2018, P EUR C COMP VIS ECC, P335; LIANG XD, 2018, CVPR, P752, DOI DOI 10.1109/CVPR.2018.00085; Lim W., 2017, P INT C LEARN REPR; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289; Mostafazadeh N, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1802; Niepert M, 2016, PR MACH LEARN RES, V48; Norcliffe-Brown W, 2018, ADV NEUR IN, V31; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Schwartz I, 2017, ADV NEUR IN, V30; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Tang K., 2018, ARXIV181201880; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Wang Xin, 2018, ARXIV181110092; Xiao FY, 2017, PROC CVPR IEEE, P5253, DOI 10.1109/CVPR.2017.558; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang W, 2016, PROC CVPR IEEE, P3073, DOI 10.1109/CVPR.2016.335; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688	45	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302073
C	Zhang, GN; Zhang, JX; Hinkle, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Guannan; Zhang, Jiaxin; Hinkle, Jacob			Learning nonlinear level sets for dimensionality reduction in function approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SLICED INVERSE REGRESSION; VISUALIZATION	We developed a Nonlinear Level-set Learning (NLL) method for dimensionality reduction in high-dimensional function approximation with small data. This work is motivated by a variety of design tasks in real-world engineering applications, where practitioners would replace their computationally intensive physical models (e.g., high-resolution fluid simulators) with fast-to-evaluate predictive machine learning models, so as to accelerate the engineering design processes. There are two major challenges in constructing such predictive models: (a) high-dimensional inputs (e.g., many independent design parameters) and (b) small training data, generated by running extremely time-consuming simulations. Thus, reducing the input dimension is critical to alleviate the over-fitting issue caused by data insufficiency. Existing methods, including sliced inverse regression and active subspace approaches, reduce the input dimension by learning a linear coordinate transformation; our main contribution is to extend the transformation approach to a nonlinear regime. Specifically, we exploit reversible networks (RevNets) to learn nonlinear level sets of a high-dimensional function and parameterize its level sets in low-dimensional spaces. A new loss function was designed to utilize samples of the target functions' gradient to encourage the transformed function to be sensitive to only a few transformed coordinates. The NLL approach is demonstrated by applying it to three 2D functions and two 20D functions for showing the improved approximation accuracy with the use of nonlinear transformation, as well as to an 8D composite material design problem for optimizing the buckling-resistance performance of composite shells of rocket inter-stages.	[Zhang, Guannan; Hinkle, Jacob] Oak Ridge Natl Lab, Comp Sci & Math Div, Oak Ridge, TN 37830 USA; [Zhang, Jiaxin] Oak Ridge Natl Lab, Natl Ctr Computat Sci, Oak Ridge, TN USA	United States Department of Energy (DOE); Oak Ridge National Laboratory; United States Department of Energy (DOE); Oak Ridge National Laboratory	Zhang, GN (corresponding author), Oak Ridge Natl Lab, Comp Sci & Math Div, Oak Ridge, TN 37830 USA.	zhangg@ornl.gov; zhangj@ornl.gov; hinklejd@ornl.gov	Hinkle, Jacob/AAM-6795-2021; Zhang, Jiaxin/A-1515-2016	Hinkle, Jacob/0000-0002-7751-1760; Zhang, Jiaxin/0000-0002-7576-6110	U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program [ERKJ352]; Artificial Intelligence Initiative at the Oak Ridge National Laboratory (ORNL); U.S. Department of Energy [DEAC05-00OR22725]	U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program(United States Department of Energy (DOE)); Artificial Intelligence Initiative at the Oak Ridge National Laboratory (ORNL); U.S. Department of Energy(United States Department of Energy (DOE))	This material was based upon work supported by the U.S. Department of Energy, Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program under contract ERKJ352; and by the Artificial Intelligence Initiative at the Oak Ridge National Laboratory (ORNL). ORNL is operated by UT-Battelle, LLC., for the U.S. Department of Energy under Contract DEAC05-00OR22725.	Behrmann J., 2018, ARXIV181100995; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bengio Y., 2005, ADV NEURAL INFORM PR, V17, P129; Bridges RA, 2019, PR MACH LEARN RES, V97; Castro SGP, 2014, THIN WALL STRUCT, V74, P118, DOI 10.1016/j.tws.2013.08.011; Chang B, 2018, AAAI CONF ARTIF INTE, P2811; Constantine PG, 2014, SIAM J SCI COMPUT, V36, pA1500, DOI 10.1137/130916138; Constantine PG., 2015, ACTIVE SUBSPACES EME; COOK RD, 1991, J AM STAT ASSOC, V86, P328, DOI 10.2307/2290564; Cook RD, 2001, AUST NZ J STAT, V43, P147, DOI 10.1111/1467-842X.00164; Cook RD, 2005, J AM STAT ASSOC, V100, P410, DOI 10.1198/016214504000001501; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dinh Laurent, 2014, ARXIV14108516; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Haber E, 2018, INVERSE PROBL, V34, DOI 10.1088/1361-6420/aa9a90; Hauser Michael, 2017, NIPS; Jacobsen J.H., 2018, ICLR; Kingma D. P., 2018, P ADV NEUR INF PROC, P10215; Li B, 2005, ANN STAT, V33, P1580, DOI 10.1214/009053605000000192; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.1080/01621459.1992.10476258; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; Li LX, 2007, BIOMETRIKA, V94, P603, DOI 10.1093/biomet/asm044; PARK MJ, 2015, ADV NEURAL INFORM PR, P115; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Wang J., 2005, P ADV NEUR INF PROC, P1473; Wu Q., 2009, ADV NEURAL INFORM PR, P1785; Yeh YR, 2009, IEEE T KNOWL DATA EN, V21, P1590, DOI 10.1109/TKDE.2008.232	31	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904081
C	Zhang, GD; Li, LL; Nado, Z; Martens, J; Sachdeva, S; Dahl, GE; Shallue, CJ; Grosse, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Guodong; Li, Lala; Nado, Zachary; Martens, James; Sachdeva, Sushant; Dahl, George E.; Shallue, Christopher J.; Grosse, Roger			Which Algorithmic Choices Matter at Which Batch Sizes? Insights From a Noisy Quadratic Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC-APPROXIMATION	Increasing the batch size is a popular way to speed up neural network training, but beyond some critical batch size, larger batch sizes yield diminishing returns. In this work, we study how the critical batch size changes based on properties of the optimization algorithm, including acceleration, preconditioning and averaging, through two different lenses: large scale experiments, and analysis of a simple noisy quadratic model (NQM). We experimentally demonstrate that optimization algorithms that employ preconditioning, specifically Adam and K-FAC, result in much larger critical batch sizes than stochastic gradient descent with momentum. We also demonstrate that the NQM captures many of the essential features of real neural network training, despite being drastically simpler to work with. The NQM predicts our results with preconditioned optimizers and exponential moving average, previous results with accelerated gradient descent, and other results around optimal learning rates and large batch training, making it a useful tool to generate testable predictions about neural network optimization.	[Zhang, Guodong; Sachdeva, Sushant; Grosse, Roger] Univ Toronto, Toronto, ON, Canada; [Zhang, Guodong; Grosse, Roger] Vector Inst, Toronto, ON, Canada; [Zhang, Guodong; Li, Lala; Nado, Zachary; Dahl, George E.; Shallue, Christopher J.] Google Res, Brain Team, San Jose, CA 95101 USA; [Martens, James] DeepMind, London, England	University of Toronto; Google Incorporated	Zhang, GD (corresponding author), Univ Toronto, Toronto, ON, Canada.; Zhang, GD (corresponding author), Vector Inst, Toronto, ON, Canada.; Zhang, GD (corresponding author), Google Res, Brain Team, San Jose, CA 95101 USA.	gdzhang@cs.toronto.edu		Shallue, Christopher/0000-0002-7585-9974	CIFAR Canadian AI Chairs program; Ontario MRIS Early Researcher Award	CIFAR Canadian AI Chairs program; Ontario MRIS Early Researcher Award	RG acknowledges support from the CIFAR Canadian AI Chairs program and the Ontario MRIS Early Researcher Award.	[Anonymous], 2017, ADV NEURAL INFORM PR; Ba J., 2017, P 3 INT C LEARN REPR; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bae Juhan, 2018, ADV NEURAL INFORM PR; Balles L, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Chen L., 2018, ADV NEURAL INFORM PR, P9302; Chiang A. C., 1974, FUNDAMENTAL METHODS; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Du S.S., 2019, ARXIV191003016; ebski Stanislaw Jastrz., 2018, INT C ART NEUR NETW; George Thomas, 2018, ADV NEURAL INFORM PR, P9550; Ghorbani B, 2019, PR MACH LEARN RES, V97; Goh G., 2017, DISTILL, V2, pe6, DOI DOI 10.23915/DISTILL.00006; Golmant N., 2018, ARXIV181112941; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Grosse R, 2016, PR MACH LEARN RES, V48; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kidambi R, 2018, 2018 INFORMATION THEORY AND APPLICATIONS WORKSHOP (ITA); Kurach K., 2017, ARXIV170603200; Kyrola A., 2017, ABS170602677 ARXIV; Lee J., 2019, ARXIV190206720; Leen TK., 1994, ADV NEURAL INFORM PR, P477; Ma Siyuan, 2018, INT C MACH LEARN, P3331; Martens, 2017, INT C LEARN REPR; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Martens James, 2014, NEW INSIGHTS PERSPEC; McCandlish S., 2018, ARXIV181206162; Osawa K., 2018, ARXIV181112019; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sagun L, 2016, EIGENVALUES HESSIAN; Schaul T., 2013, INT C MACHINE LEARNI, P343; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; Shallue Christopher J, 2018, ARXIV181103600; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Smith Samuel L, 2019, THEOR PHYS DEEP LEAR; Smith SL., 2018, DONT DECAY LEARNING; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang Chaoqi, 2019, P 36 INT C MACH LEAR, P6566; Wu Y., 2018, INT C LEARN REPR; Yang L., 2018, ADV NEURAL INFORM PR, P4372; Yin D., 2018, P 21 INT C ART INT S, P1998; Yuan K, 2016, J MACH LEARN RES, V17; Zhang Guodong, 2019, ARXIV190510961; Zhang Guojun, 2019, INT C LEARN REPR, P8	53	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308024
C	Zhang, GD; Martens, J; Grosse, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Guodong; Martens, James; Grosse, Roger			Fast Convergence of Natural Gradient Descent for Overparameterized Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ERROR	Natural gradient descent has proven effective at mitigating the effects of pathological curvature in neural network optimization, but little is known theoretically about its convergence properties, especially for nonlinear networks. In this work, we analyze for the first time the speed of convergence of natural gradient descent on nonlinear neural networks with squared-error loss. We identify two conditions which guarantee efficient convergence from random initializations: (1) the Jacobian matrix (of network's output for all training cases with respect to the parameters) has full row rank, and (2) the Jacobian matrix is stable for small perturbations around the initialization. For two-layer ReLU neural networks, we prove that these two conditions do in fact hold throughout the training, under the assumptions of nondegenerate inputs and overparameterization. We further extend our analysis to more general loss functions. Lastly, we show that K-FAC, an approximate natural gradient descent method, also converges to global minima under the same assumptions, and we give a bound on the rate of this convergence.	[Zhang, Guodong; Grosse, Roger] Univ Toronto, Toronto, ON, Canada; [Zhang, Guodong; Grosse, Roger] Vector Inst, Hyderabad, Telangana, India; [Martens, James] DeepMind, London, England	University of Toronto	Zhang, GD (corresponding author), Univ Toronto, Toronto, ON, Canada.; Zhang, GD (corresponding author), Vector Inst, Hyderabad, Telangana, India.	gdzhang@cs.toronto.edu; jamesmartens@google.com; rgrosse@cs.toronto.edu			CIFAR Canadian AI Chairs program; Ontario MRIS Early Researcher Award	CIFAR Canadian AI Chairs program; Ontario MRIS Early Researcher Award	We thank Jeffrey Z. HaoChen, Shengyang Sun and Mufan Li for helpful discussion. RG acknowledges support from the CIFAR Canadian AI Chairs program and the Ontario MRIS Early Researcher Award.	Advani M. S., 2017, ARXIV171003667; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S, 1997, ADV NEUR IN, V9, P127; [Anonymous], 2018, ADV NEURAL INFORM PR, DOI 10.5555/3327757.3327948; [Anonymous], ARXIV181103962; [Anonymous], 2017, ARXIV171206541; Arora S., 2019, INT C LEARN REPR; Arora Sanjeev, 2018, ARXIV180205296; Arora Sanjeev, 2019, ARXIV190108584; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Becker S., 1989, P 1988 CONN MOD SUMM; Bernacchia A., 2018, ADV NEURAL INFORM PR, P5941; BLUM AL, 1992, NEURAL NETWORKS, V5, P117, DOI 10.1016/S0893-6080(05)80010-3; Botev Aleksandar., 2017, INT C MACH LEARN; Boyd S, 2004, CONVEX OPTIMIZATION; Brutzkus A, 2017, PR MACH LEARN RES, V70; Cai T., 2019, ARXIV190511675; Du Simon S, 2018, GRADIENT DESCENT FIN; Du Simon S., 2018, INT C LEARN REPR; Du SS., 2019, P 7 INT C LEARN REPR; Dziugaite Gintare Karolina, 2018, ADV NEURAL INFORM PR, P8440; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Forster J, 2002, J COMPUT SYST SCI, V65, P612, DOI 10.1016/S0022-0000(02)00019-3; Gao W, 2019, 22 INT C ART INT STA, P1950; Hardt M., 2016, ARXIV161104231; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; Kawaguchi K., 2018, ARXIV181009038; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J., 2019, ARXIV190206720; Lee J. D., 2016, ARXIV160204915; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Luk Kevin, 2018, ARXIV18081340; Martens J., 2018, INT C LEARN REPR; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Martens James, 2014, NEW INSIGHTS PERSPEC; Mohri M., 2018, FDN MACHINE LEARNING; Neyshabur B., 2019, INT C LEARN REPR; Neyshabur Behnam, 2017, ARXIV170709564; Ollivier Y, 2015, INF INFERENCE, V4, P108, DOI 10.1093/imaiai/iav006; Osbild R, 2019, SOC POLIT ORD TRANS, P1, DOI 10.1007/978-3-319-93665-9_1; Oymak S., 2019, ARXIV190204674; Pascanu Razvan, 2013, ARXIV13013584; Nguyen Q, 2017, PR MACH LEARN RES, V70; Roger Grosse, 2016, INT C MACH LEARN; Saxe A., 2014, INT C LEARNING REPRE; Schur J, 1911, J REINE ANGEW MATH, V140, P1; Soudry D., 2016, ARXIV PREPRINT ARXIV; TENG TS, 2018, ADV NEURAL INFORM PR, P168, DOI DOI 10.1109/ITME.2018.00046; Tian YD, 2017, PR MACH LEARN RES, V70; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; WILSON A. C., 2017, ADV NEURAL INFORM PR, V30, P4148; Wu Xiaoxia, 2019, ARXIV190207111; Wu Yuhuai, 2017, NEURAL INFORM PROCES; Xie B., 2016, ARXIV161103131; Yun C., 2018, ARXIV180203487; Zhang Chiyuan, 2016, ARXIV161103530; Zhang Xiao, 2018, ARXIV180607808; Zou D, 2018, ARXIV181108888	63	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308014
C	Abid, A; Zou, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Abid, Abubakar; Zou, James			Autowarp: Learning a Warping Distance from Unlabeled Time Series Using Sequence Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Measuring similarities between unlabeled time series trajectories is an important problem in domains as diverse as medicine, astronomy, finance, and computer vision. It is often unclear what is the appropriate metric to use because of the complex nature of noise in the trajectories (e.g. different sampling rates or outliers). Domain experts typically hand-craft or manually select a specific metric, such as dynamic time warping (DTW), to apply on their data. In this paper, we propose Autowarp, an end-to-end algorithm that optimizes and learns a good metric given unlabeled trajectories. We define a flexible and differentiable family of warping metrics, which encompasses common metrics such as DTW, Euclidean, and edit distance. Autowarp then leverages the representation power of sequence autoencoders to optimize for a member of this warping distance family. The output is a metric which is easy to interpret and can be robustly learned from relatively few trajectories. In systematic experiments across different domains, we show that Autowarp often outperforms hand-crafted trajectory similarity metrics.	[Abid, Abubakar; Zou, James] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Abid, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	a12d@stanford.edu; jamesz@stanford.edu							0	7	7	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005015
C	Agarwal, N; Suresh, AT; Yu, F; Kumar, S; McMahan, HB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Agarwal, Naman; Suresh, Ananda Theertha; Yu, Felix; Kumar, Sanjiv; McMahan, H. Brendan			cpSGD: Communication-efficient and differentially-private distributed SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				THEOREM	Distributed stochastic gradient descent is an important subroutine in distributed learning. A setting of particular interest is when the clients are mobile devices, where two important concerns are communication efficiency and the privacy of the clients. Several recent works have focused on reducing the communication cost or introducing privacy guarantees, but none of the proposed communication efficient methods are known to be privacy preserving and none of the known privacy mechanisms are known to be communication efficient. To this end, we study algorithms that achieve both communication efficiency and differential privacy. For d variables and n approximate to d clients, the proposed method uses O(log log(nd)) bits of communication per client per coordinate and ensures constant privacy. We also improve previous analysis of the Binomial mechanism showing that it achieves nearly the same utility as the Gaussian mechanism, while requiring fewer representation bits, which can be of independent interest.	[Agarwal, Naman] Google Brain, Princeton, NJ 08540 USA; [Suresh, Ananda Theertha; Yu, Felix; Kumar, Sanjiv] Google Res, New York, NY USA; [McMahan, H. Brendan] Google Res, Seattle, WA USA	Google Incorporated; Google Incorporated; Google Incorporated	Agarwal, N (corresponding author), Google Brain, Princeton, NJ 08540 USA.	namanagarwal@google.com; theertha@google.com; felixyu@google.com; sanjivk@google.com; mcmahan@google.com						Abadi M., TENSORFLOW LARGE SCA; Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Ailon N., 2006, STOC; Akiba Takuya, 2018, VARIANCE BASED GRADI; Alistarh D., 2016, ARXIV PREPRINT ARXIV; Alistarh D., 2017, COMMUNICATION EFFICI; An GC, 2014, INT SYM COMPUT INTEL, P3, DOI 10.1109/ISCID.2014.41; [Anonymous], 2016, ABS161107555 CORR; [Anonymous], 2012, ICML; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Bonawitz K, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1175, DOI 10.1145/3133956.3133982; Bottou L, INFINITE MNIST DATAS; Coates A., 2013, INT C MACHINE LEARNI, P1337; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dean J., 2012, NIPS 12, V1, P1223; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; Horadam K. J., 2012, HADAMARD MATRICES TH, DOI 10.1515/9781400842902; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Konecny J., 2016, ARXIV161005492; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Mao H., 2018, INT C LEARN REPR; McDonald Ryan, 2010, HLT; McMahan H. Brendan, 2016, ARXIV160205629; McMahan H. Brendan, 2016, P 20 INT C ART INT S; Povey D, 2014, PARALLEL TRAINING DE; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911; Seide F, 2014, INTERSPEECH, P1058; Suresh AT, 2017, PR MACH LEARN RES, V70; Sutskever I., 2014, ARXIV; Wen W., 2017, ARXIV170507878; Wu X, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1307, DOI 10.1145/3035918.3064047; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170	39	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002014
C	Balduzzi, D; Tuyls, K; Perolat, J; Graepel, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balduzzi, David; Tuyls, Karl; Perolat, Julien; Graepel, Thore			Re-evaluating Evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ARCADE LEARNING-ENVIRONMENT; INTELLIGENCE; PERFORMANCE; GAME; DYNAMICS; PAPER; GO	Progress in machine learning is measured by careful evaluation on problems of outstanding common interest. However, the proliferation of benchmark suites and environments, adversarial attacks, and other complications has diluted the basic evaluation model by overwhelming researchers with choices. Deliberate or accidental cherry picking is increasingly likely, and designing well-balanced evaluation suites requires increasing effort. In this paper we take a step back and propose Nash averaging. The approach builds on a detailed analysis of the algebraic structure of evaluation in two basic scenarios: agent-vs-agent and agent-vs-task. The key strength of Nash averaging is that it automatically adapts to redundancies in evaluation data, so that results are not biased by the incorporation of easy tasks or weak agents. Nash averaging thus encourages maximally inclusive evaluation - since there is no harm (computational cost aside) from including all available tasks and agents.	[Balduzzi, David; Tuyls, Karl; Perolat, Julien; Graepel, Thore] DeepMind, London, England		Balduzzi, D (corresponding author), DeepMind, London, England.	dbalduzzi@google.com; karltuyls@google.com; perolat@google.com; thore@google.com						[Anonymous], 2017, MASTERING CHESS SHOG; Balduzzi D., 2018, ICML; Balsubramani Akshay, 2016, COLT; Beattie C., 2016, ARXIV161203801; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bloembergen D, 2015, J ARTIF INTELL RES, V53, P659, DOI 10.1613/jair.4818; Bringsjord S, 2011, J EXP THEOR ARTIF IN, V23, P271, DOI 10.1080/0952813X.2010.502314; Brockman G., 2016, OPENAI GYM; Candogan O., 2013, ACM T EC COMP, V1; Candogan O, 2013, GAME ECON BEHAV, V82, P66, DOI 10.1016/j.geb.2013.07.001; CHAITIN GJ, 1966, J ACM, V13, P547, DOI 10.1145/321356.321363; Deng J., 2009, CVPR; Diaconis P., 1998, LECT NOTES MONOGRAPH, V11; Donoho D., 2015, BAS PRES TUK CENT WO; Dudik M., 2015, COLT; Elo Arpad E, 1978, RATING CHESSPLAYERS; Ferri C, 2009, PATTERN RECOGN LETT, V30, P27, DOI 10.1016/j.patrec.2008.08.010; Frean M, 2001, P ROY SOC B-BIOL SCI, V268, P1323, DOI 10.1098/rspb.2001.1670; Freund Y., 1996, J COMPUTER SYSTEM SC; Hambleton R. K., 1991, FUNDAMENTALS ITEM RE; Hartford J. S., 2018, ICML; Herbrich R., 2007, NIPS; Hernandez-Orallo J, 2017, ARTIF INTELL REV, V48, P397, DOI 10.1007/s10462-016-9505-7; Hernandez-Orallo J, 2012, J MACH LEARN RES, V13, P2813; Hernandez-Orallo Jose, 2017, MEASURE ALL MINDS EV; Hessel M., 2018, AAAI; Hofbauer J, 2002, ECONOMETRICA, V70, P2265, DOI 10.1111/j.1468-0262.2002.00440.x; Horn B., 2014, FDN DIGITAL GAMES; Hunter DR, 2004, ANN STAT, V32, P384; Jaderberg M., 2017, ABS171109846 CORR; Jebara T, 2003, ICML; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Jordan P. R., 2007, P 6 INT JOINT C AUT, P193; Jordan Patrick R., 2010, THESIS; Kerr B, 2002, NATURE, V418, P171, DOI 10.1038/nature00823; Kolmogorov A. N., 1968, International Journal of Computer Mathematics, V2, P157, DOI 10.1080/00207166808803030; Kondor R., 2018, NIPS; Kondor Risi Imre, 2008, THESIS; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Kurakin Alexey, 2018, ARXIV180400097; Laird R. A., 2006, AM NATURALIST, V168; Lanctot Marc, 2017, NIPS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Legg S., 2005, IJCAI; Legg S., 2013, ALGORITHMIC PROBABIL; Leibo J. Z., 2018, ARXIV180108116; Liapis A., 2013, ARTIFICIAL INTELLIGE; Machado MC, 2018, J ARTIF INTELL RES, V61, P523, DOI 10.1613/jair.5699; Martinez-Plumed F., 2017, WORKSH EV GEN PURP A; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Nielsen T., 2015, EVOAPPLICATIONS; Olson RS, 2017, BIODATA MIN, V10, DOI 10.1186/s13040-017-0154-4; Ortiz L. E., 2007, AISTATS; Ortiz L. E., 2006, TR200621 CSAIL MIT; Ostrovski G., 2017, ICML; Phelps S., 2007, 2 INT C PERS TECHN, P188; Phelps S., 2004, AGENT MEDIATED ELECT, P101; Ponsen M, 2009, ENTERTAIN COMPUT, V1, P39, DOI 10.1016/j.entcom.2009.09.002; Sandholm WilliamH., 2010, POPULATION GAMES EVO; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Silva F. de Mesentier, 2017, FDN DIGITAL GAMES FD; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P224, DOI 10.1016/S0019-9958(64)90131-7; Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107; Sukhbaatar S., 2017, ICLR; Szegedy C, 2013, 2 INT C LEARNING REP; Szolnoki A, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0735; Todorov E., 2012, IROS; Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559; Tramer F., 2018, ICLR; Tuyls K, 2018, AAMAS; Uesato Jonathan, 2018, ICML; van Hasselt H., 2016, NIPS; Vandenberg RJ, 2000, ORGAN RES METHODS, V3, P4, DOI 10.1177/109442810031002; Volz V., 2018, GECCO; WALSH WE, 2003, P 5 WORKSH AG MED EL; Wang Z., 2016, ICML; Wellman M. P., 2006, AAAI, P1552; Woolley AW, 2010, SCIENCE, V330, P686, DOI 10.1126/science.1193147; Zaheer Manzil, 2017, NIPS	83	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303028
C	Chen, PH; Si, S; Li, Y; Chelba, C; Hsieh, CJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Patrick H.; Si, Si; Li, Yang; Chelba, Ciprian; Hsieh, Cho-Jui			GroupReduce: Block-Wise Low-Rank Approximation for Neural Language Model Shrinking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Model compression is essential for serving large deep neural nets on devices with limited resources or applications that require real-time responses. As a case study, a neural language model usually consists of one or more recurrent layers sandwiched between an embedding layer used for representing input tokens and a softmax layer for generating output tokens. For problems with a very large vocabulary size, the embedding and the softmax matrices can account for more than half of the model size. For instance, the bigLSTM model achieves great performance on the One-Billion-Word (OBW) dataset with around 800k vocabulary, and its word embedding and softmax matrices use more than 6GBytes space, and are responsible for over 90% of the model parameters. In this paper, we propose GroupReduce, a novel compression method for neural language models, based on vocabulary-partition (block) based low-rank matrix approximation and the inherent frequency distribution of tokens (the power-law distribution of words). The experimental results show our method can significantly outperform traditional compression methods such as low-rank approximation and pruning. On the OBW dataset, our method achieved 6.6 times compression rate for the embedding and softmax matrices, and when combined with quantization, our method can achieve 26 times compression rate, which translates to a factor of 12.8 times compression for the entire model with very little degradation in perplexity.	[Chen, Patrick H.; Hsieh, Cho-Jui] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Si, Si; Li, Yang; Chelba, Ciprian] Google Res, Mountain View, CA USA; [Chen, Patrick H.] Google, Mountain View, CA 94043 USA	University of California System; University of California Los Angeles; Google Incorporated; Google Incorporated	Chen, PH (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.; Chen, PH (corresponding author), Google, Mountain View, CA 94043 USA.	patrickchen@g.ucla.edu; sisidaisy@google.com; liyang@google.com; ciprianchelba@google.com; chohsieh@cs.ucla.edu	Chen, Patrick/HGA-3011-2022		NSF [IIS-1719097]; Intel faculty award; Google Cloud; Nvidia	NSF(National Science Foundation (NSF)); Intel faculty award; Google Cloud(Google Incorporated); Nvidia	This research is mainly done during Patrick Chen's internship at Google Research. We also acknowledge the support by NSF via IIS-1719097, Intel faculty award, Google Cloud and Nvidia.	[Anonymous], 2017, ARXIV170102810; Cettolo M., 2014, REPORT 11 IWSLT EVAL; Chen Y., 2016, ARXIV161003950; Choi  Yoojin, 2018, ABS180202271 CORR; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dentinel Zarembaw, 2014, NEURIPS, P1269; Han S., 2015, ABS151000149 CORR; Han  Song, 2015, ABS150602626 CORR; Howard A.G., 2017, ARXIV170404861; Hubara I., 2016, ARXIV160907061; Jaderberg M., 2014, ARXIV14053866; Jozefowicz Rafal, 2016, ARXIV160202410; Kim Y., 2016, ICLR; Lam Maximilian, 2018, ARXIV180305651; Lin DD, 2016, PR MACH LEARN RES, V48; Lobacheva Ekaterina, 2017, ARXIV170800077; Lu  Zhiyun, 2016, ABS160402594 CORR; Narang  Sharan, ICLR; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Shu R., 2018, ICLR; Si  Si, J MACH LEARN RES, V32, P701; Srebro N., 2003, P 20 INT C MACHINE L, P720; Tjandra A, 2017, IEEE IJCNN, P4451, DOI 10.1109/IJCNN.2017.7966420; Wu  Jiaxiang, 2015, ABS151206473 CORR; Xu  Yuhui, 2018, ABS180303289 CORR; Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15	27	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005056
C	Deng, ZW; Chen, JC; Fu, YF; Mori, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Deng, Zhiwei; Chen, Jiacheng; Fu, Yifang; Mori, Greg			Probabilistic Neural Programmed Networks for Scene Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PRODUCTS	In this paper we address the text to scene image generation problem. Generative models that capture the variability in complicated scenes containing rich semantics is a grand goal of image generation. Complicated scene images contain varied visual elements, compositional visual concepts, and complicated relations between objects. Generative models, as an analysis-by-synthesis process, should encompass the following three core components: 1) the generation process that composes the scene; 2) what are the primitive visual elements and how are they composed; 3) the rendering of abstract concepts into their pixel-level realizations. We propose PNP-Net, a variational auto-encoder framework that addresses these three challenges: it flexibly composes images with a dynamic network structure, learns a set of distribution transformers that can compose distributions based on semantics, and decodes samples from these distributions into realistic images.	[Deng, Zhiwei; Chen, Jiacheng; Fu, Yifang; Mori, Greg] Simon Fraser Univ, Burnaby, BC, Canada	Simon Fraser University	Deng, ZW (corresponding author), Simon Fraser Univ, Burnaby, BC, Canada.	zhiweid@sfu.ca; jca348@sfu.ca; yifangf@sfu.ca; mori@cs.sfu.ca						Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; Chen X., 2016, ARXIV160603657, P2172; Chen Xi, 2016, ARXIV161102731; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Frome Andrea, 2013, NEURIPS; Ganin Y, 2018, PR MACH LEARN RES, V80; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C LEARN REPR; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hoffman MD, 2017, PR MACH LEARN RES, V70; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2016, PROC CVPR IEEE, P4565, DOI 10.1109/CVPR.2016.494; Karacan L., 2016, ARXIV161200215; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Le T. A., 2016, ARXIV161009900; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Mori G, 2017, P IEEE C COMP VIS PA, P2577; Parisotto Emilio, 2016, ARXIV161101855; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Reed S, 2016, PR MACH LEARN RES, V48; Reed Scott, 2016, ICLR; Salimans T., 2017, INT C LEARN REPR; Salimans T, 2016, ADV NEUR IN, V29; van den Oord A, 2016, PR MACH LEARN RES, V48; Vedantam R., 2017, ARXIV170510762; Villegas R, 2017, PR MACH LEARN RES, V70; Walker J, 2017, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2017.361; Weinberger KQ, 2014, ADV NEURAL INFORM PR, P1889; Williams CKI, 2002, NEURAL COMPUT, V14, P1169, DOI 10.1162/089976602753633439; Wu JJ, 2017, ADV NEUR IN, V30; Wu Jiajun, 2017, IEEE C COMP VIS PATT, P699; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu S., 2017, ICCV	40	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304007
C	Diochnos, DI; Mahloujifar, S; Mahmoody, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Diochnos, Dimitrios I.; Mahloujifar, Saeed; Mahmoody, Mohammad			Adversarial Risk and Robustness: General Definitions and Implications for the Uniform Distribution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DNF	We study adversarial perturbations when the instances are uniformly distributed over {0, 1}(n). We study both "inherent" bounds that apply to any problem and any classifier for such a problem as well as bounds that apply to specific problems and specific hypothesis classes. As the current literature contains multiple definitions of adversarial risk and robustness, we start by giving a taxonomy for these definitions based on their direct goals; we identify one of them as the one guaranteeing misclassification by pushing the instances to the error region. We then study some classic algorithms for learning monotone conjunctions and compare their adversarial robustness under different definitions by attacking the hypotheses using instances drawn from the uniform distribution. We observe that sometimes these definitions lead to significantly different bounds. Thus, this study advocates for the use of the error-region definition, even though other definitions, in other contexts with context-dependent assumptions, may coincide with the error-region definition. Using the error-region definition of adversarial perturbations, we then study inherent bounds on risk and robustness of any classifier for any classification problem whose instances are uniformly distributed over {0,1}(n). Using the isoperimetric inequality for the Boolean hypercube, we show that for initial error 0.01, there always exists an adversarial perturbation that changes O(root n) bits of the instances to increase the risk to 0.5, making classifier's decisions meaningless. Furthermore, by also using the central limit theorem we show that when n -> infinity , at most c. root n bits of perturbations, for a universal constant c < 1.17 suffice for increasing the risk to 0.5, and the same c root n bits of perturbations on average suffice to increase the risk to 1, hence bounding the robustness by c . root n.	[Diochnos, Dimitrios I.; Mahloujifar, Saeed; Mahmoody, Mohammad] Univ Virginia, Charlottesville, VA 22903 USA	University of Virginia	Diochnos, DI (corresponding author), Univ Virginia, Charlottesville, VA 22903 USA.	diochnos@virginia.edu; saeed@virginia.edu; mohammad@virginia.edu	Mahmoody, Mohammad/AAE-2435-2019	Mahmoody, Mohammad/0000-0002-6839-4697; Diochnos, Dimitrios/0000-0002-2934-606X	NSF [CAREER CCF-1350939]; University of Virginia SEAS Research Innovation Award	NSF(National Science Foundation (NSF)); University of Virginia SEAS Research Innovation Award	`Supported by NSF CAREER CCF-1350939 and University of Virginia SEAS Research Innovation Award.	Attias Idan, 2018, ARXIV181002180; Bastani O., 2016, P 30 INT C NEUR INF, P2613; BenTal A, 2009, PRINC SER APPL MATH, P1; Biggio B, 2014, IEEE T KNOWL DATA EN, V26, P984, DOI 10.1109/TKDE.2013.57; BRUNER JS, 1957, STUDY THINKING; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; DIOCHNOS DI, 2009, SAGA, V5792, P74; Diochnos DI, 2016, LECT NOTES ARTIF INT, V9925, P98, DOI 10.1007/978-3-319-46379-7_7; Fawzi Alhussein, 2018, ARXIV180208686; Feige U., 2015, C LEARN THEOR, P637; Feige U., 2018, ALGORITHMIC LEARNING, P368; Gilmer J., 2018, INT C LEARN REPR WOR; Goodfellow I. J., 2015, ICLR; Harper L.H., 1966, J COMBIN THEORY, V1, P385, DOI [10.1016/S0021-9800(66)80059-5, DOI 10.1016/S0021-9800(66)80059-5]; Huang L., 2011, PROC AISEC, P43, DOI DOI 10.1145/2046684.2046692; Jackson J. C., 2006, THEOR COMPUT, V2, P147; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Madry Aleksander, 2018, 6 INT C LEARN REPR I; Mansour Y., 2015, P 26 ANN ACM SIAM S, P449; MITCHELL TOM M., 1997, MACH LEARN, P2; MOOSAVIDEZFOOLI SM, 2016, CVPR, P2574, DOI DOI 10.1109/CVPR.2016.282; Nelson B., 2010, PSDM, P92; Nelson B, 2012, J MACH LEARN RES, V13, P1293; Nigmatullin R. G., 1967, DISKRETNYL ANAL, V9, P47; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Sakai Y, 2000, THEOR COMPUT SYST, V33, P17, DOI 10.1007/s002249910002; Schmidt Ludwig, 2018, ARXIV180411285; Sellie L, 2009, ACM S THEORY COMPUT, P45; Suggala A. S., 2018, ARXIV180602924; Szegedy Christian, 2014, ICLR; Tsipras D., 2018, ARXIV180512152; Valiant LG, 2009, J ACM, V56, DOI 10.1145/1462153.1462156; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Xu W., 2018, NETW DISTR SYST SEC	36	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004087
C	Dong, S; Van Roy, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dong, Shi; Van Roy, Benjamin			An Information-Theoretic Analysis for Thompson Sampling with Many Actions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Information-theoretic Bayesian regret bounds of Russo and Van Roy [8] capture the dependence of regret on prior uncertainty. However, this dependence is through entropy, which can become arbitrarily large as the number of actions increases. We establish new bounds that depend instead on a notion of rate-distortion. Among other things, this allows us to recover through information-theoretic arguments a near-optimal bound for the linear bandit. We also offer a bound for the logistic bandit that dramatically improves on the best previously available, though this bound depends on an information-theoretic statistic that we have only been able to quantify via computation.	[Dong, Shi; Van Roy, Benjamin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Dong, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	sdong15@stanford.edu; bvr@stanford.edu			Boeing Corporation; Jane Dwight Stanford Graduate Fellowship	Boeing Corporation; Jane Dwight Stanford Graduate Fellowship	This work was supported by a grant from the Boeing Corporation and the Herb and Jane Dwight Stanford Graduate Fellowship. We would also like to thank Daniel Russo, David Tse and Xiuyuan Lu for useful conversations.	Shipra,, 2017, J ACM, V64, DOI 10.1145/3088510; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dani V, 2008, P C LEARN THEOR COLT, P355; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li LH, 2017, PR MACH LEARN RES, V70; Russo D, 2016, J MACH LEARN RES, V17; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Russo Daniel, 2018, ARXIV180302855; Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Roy, 2014, ADV NEURAL INFORM PR, P1583	11	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304019
C	Duncker, L; Sahani, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Duncker, Lea; Sahani, Maneesh			Temporal alignment and latent Gaussian process factor inference in population spike trains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DYNAMICS	We introduce a novel scalable approach to identifying common latent structure in neural population spike-trains, which allows for variability both in the trajectory and in the rate of progression of the underlying computation. Our approach is based on shared latent Gaussian processes (GPs) which are combined linearly, as in the Gaussian Process Factor Analysis (GPFA) algorithm. We extend GPFA to handle unbinned spike-train data by incorporating a continuous time point-process likelihood model, achieving scalability with a sparse variational approximation. Shared variability is separated into terms that express condition dependence, as well as trial-to-trial variation in trajectories. Finally, we introduce a nested GP formulation to capture variability in the rate of evolution along the trajectory. We show that the new method learns to recover latent trajectories in synthetic data, and can accurately identify the trial-to-trial timing of movement-related parameters from motor cortical data without any supervision.	[Duncker, Lea; Sahani, Maneesh] UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England	University of London; University College London	Duncker, L (corresponding author), UCL, Gatsby Computat Neurosci Unit, London W1T 4JG, England.	duncker@gatsby.ucl.ac.uk; maneesh@gatsby.ucl.ac.uk			Simons Foundation [SCGB 323228, 543039]; Gatsby Charitable Foundation	Simons Foundation; Gatsby Charitable Foundation	We would like to thank Vincent Adam for early contributions to this project, Gergo Bohner for helpful discussions, and the Shenoy laboratory at Stanford University for sharing the centre-out reaching dataset. This work was funded by the Simons Foundation (SCGB 323228, 543039; MS) and the Gatsby Charitable Foundation.	ABBOTT P, 2005, MATH J, V9, P689; Adam V, 2016, P 26 INT WORKSH MACH; Afshar A, 2011, NEURON, V71, P555, DOI 10.1016/j.neuron.2011.05.047; Arribas-Gil A, 2014, COMPUT STAT DATA AN, V69, P255, DOI 10.1016/j.csda.2013.08.011; Biljana Petreska, 2011, ADV NEURAL INFORM PR, P756; Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Cuturi M, 2017, PR MACH LEARN RES, V70; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Gao YJ, 2016, ADV NEUR IN, V29; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J., 2015, P 18 INT C ART INT S; Hsu E, 2005, ACM T GRAPHIC, V24, P1082, DOI 10.1145/1073204.1073315; Kaiser M, 2017, ARXIV171002766; Kazlauskaite Ieva, 2018, ARXIV180302603; Keogh E.J., 2001, P 2001 SIAM INT C DA, P1, DOI [10.1137/1.9781611972719.1, DOI 10.1137/1.9781611972719.1]; Kollmorgen S, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003508; Lawlor PN, 2018, J COMPUTATIONAL NEUR; Lazaro-Gredilla M., 2012, P ADV NEUR INF PROC, V25, P1619; Lloyd C, 2015, P 32 INT C MACH LEAR; Macke J.H., 2015, ADV STATE SPACE METH, P137; Mena G, 2014, NEURAL COMPUT, V26, P2790, DOI 10.1162/NECO_a_00676; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Poole B, 2017, FRONTIERS NEUROSCIEN; SAKOE H, 1978, IEEE T ACOUST SPEECH, V26, P43, DOI 10.1109/TASSP.1978.1163055; Salimbeni Hugh, 2017, ADV NEURAL INFORM PR, V30, P4588; Saul AD, 2016, JMLR WORKSH CONF PRO, V51, P1431; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Wu A., 2017, ADV NEURAL INFORM PR, V30, P3499; Wu W, 2013, J COMPUT NEUROSCI, V34, P391, DOI 10.1007/s10827-012-0427-3; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; Zemel  Y., 2017, ARXIV170106876; Zhao Y, 2017, NEURAL COMPUT, V29, P1293, DOI 10.1162/NECO_a_00953; Zhou F, 2016, IEEE T PATTERN ANAL, V38, P279, DOI 10.1109/TPAMI.2015.2414429	39	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005006
C	Fan, XH; Li, B; Sisson, SA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fan, Xuhui; Li, Bin; Sisson, Scott A.			Rectangular Bounding Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGRESSION; PREDICTION	Stochastic partition models divide a multi-dimensional space into a number of rectangular regions, such that the data within each region exhibit certain types of homogeneity. Due to the nature of their partition strategy, existing partition models may create many unnecessary divisions in sparse regions when trying to describe data in dense regions. To avoid this problem we introduce a new parsimonious partition model - the Rectangular Bounding Process (RBP) - to efficiently partition multi-dimensional spaces, by employing a bounding strategy to enclose data points within rectangular bounding boxes. Unlike existing approaches, the RBP possesses several attractive theoretical properties that make it a powerful non-parametric partition prior on a hypercube. In particular, the RBP is self-consistent and as such can be directly extended from a finite hypercube to infinite (unbounded) space. We apply the RBP to regression trees and relational models as a flexible partition prior. The experimental results validate the merit of the RBP in rich yet parsimonious expressiveness compared to the state-of-the-art methods.	[Fan, Xuhui; Sisson, Scott A.] Univ New South Wales, Sch Math & Stat, Sydney, NSW, Australia; [Li, Bin] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China	University of New South Wales Sydney; Fudan University	Fan, XH (corresponding author), Univ New South Wales, Sch Math & Stat, Sydney, NSW, Australia.	xuhui.fan@unsw.edu.au; libin@fudan.edu.cn; scott.sisson@unsw.edu.au	Sisson, Scott A/B-8038-2008; Fan, Xu/GSE-2196-2022	Fan, Xuhui/0000-0002-7558-7200	Australian Research Council through the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS) [CE140100049]; Fudan University Startup Research Grant; Shanghai Municipal Science & Technology Commission [16JC1420401];  [DP160102544]	Australian Research Council through the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS)(Australian Research Council); Fudan University Startup Research Grant; Shanghai Municipal Science & Technology Commission(Science & Technology Commission of Shanghai Municipality (STCSM)); 	Xuhui Fan and Scott A. Sisson are supported by the Australian Research Council through the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS, CE140100049), and Scott A. Sisson through the Discovery Project Scheme (DP160102544). Bin Li is supported by Fudan University Startup Research Grant and Shanghai Municipal Science & Technology Commission (16JC1420401).	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Airoldi Edo M, 2009, ADV NEURAL INFORM PR, P33; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Caldas J, 2008, MACHINE LEARN SIGN P, P291, DOI 10.1109/MLSP.2008.4685495; Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285; Chung K.L., 2001, COURSE PROBABILITY T, V3; Coraddu A, 2016, P I MECH ENG M-J ENG, V230, P136, DOI 10.1177/1475090214540874; Dheeru D., 2019, UCI MACHINE LEARNING; Fan X., 2018, AISTATS, P1859; Fan XH, 2016, AAAI CONF ARTIF INTE, P1547; Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Givoni Inmar, 2006, UAI, P200; Ishiguro K., 2010, ADV NEURAL INFORM PR, P919; Ishiguro K, 2016, AAAI CONF ARTIF INTE, P1701; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kemp Charles, 2006, AAAI, DOI DOI 10.1145/1837026.1837061; Lakshminarayanan B., 2014, P ADV NEUR INF PROC, P3140; Lakshminarayanan B, 2016, JMLR WORKSH CONF PRO, V51, P1478; Leskovec J., 2010, P 19 INT C WORLD WID, P641, DOI [10.1145/1772690.1772756, DOI 10.1145/1772690.1772756]; Li B., 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454; Linero Antonio R, 2018, J AM STAT ASSOC, P1; McAuley Julian, 2012, ADV NEURAL INFORM PR, P539; Miller Kurt T., 2009, NONPARAMETRIC LATENT, P1276; Nakano M, 2014, PR MACH LEARN RES, V32, P361; Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735; Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pratola Matthew, 2017, ARXIV170907542; Pratola MT, 2014, J COMPUT GRAPH STAT, V23, P830, DOI 10.1080/10618600.2013.841584; Roy D. M., 2011, THESIS; Roy D. M., 2009, ADV NEURAL INFORM PR; Schmidt MN, 2013, IEEE SIGNAL PROC MAG, V30, P110, DOI 10.1109/MSP.2012.2235191; Tufekci P, 2014, INT J ELEC POWER, V60, P126, DOI 10.1016/j.ijepes.2014.02.027; Wang P., 2011, SIAM INT C DAT MIN, P331; Wang Y, 2015, PR MACH LEARN RES, V37, P1339; Yeh IC, 1998, CEMENT CONCRETE RES, V28, P1797, DOI 10.1016/S0008-8846(98)00165-3; Zafarani R., 2009, SOCIAL COMPUTING DAT	38	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002019
C	Guo, DD; Chen, B; Zhang, H; Zhou, MY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Dandan; Chen, Bo; Zhang, Hao; Zhou, Mingyuan			Deep Poisson gamma dynamical systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We develop deep Poisson-gamma dynamical systems (DPGDS) to model sequentially observed multivariate count data, improving previously proposed models by not only mining deep hierarchical latent structure from the data, but also capturing both first-order and long-range temporal dependencies. Using sophisticated but simple-to-implement data augmentation techniques, we derived closed-form Gibbs sampling update equations by first backward and upward propagating auxiliary latent counts, and then forward and downward sampling latent variables. Moreover, we develop stochastic gradient MCMC inference that is scalable to very long multivariate count time series. Experiments on both synthetic and a variety of real-world data demonstrate that the proposed model not only has excellent predictive performance, but also provides highly interpretable multilayer latent structure to represent hierarchical and temporal information propagation.	[Guo, Dandan; Chen, Bo; Zhang, Hao] Xidian Univ, Natl Lab Radar Signal Proc, Collaborat Innovat Ctr Informat Sensing & Underst, Xian, Shaanxi, Peoples R China; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Xidian University; University of Texas System; University of Texas Austin	Guo, DD (corresponding author), Xidian Univ, Natl Lab Radar Signal Proc, Collaborat Innovat Ctr Informat Sensing & Underst, Xian, Shaanxi, Peoples R China.	gdd_xidian@126.com; bchen@mail.xidian.edu.cn; zhanghao_xidian@163.com; mingyuan.zhou@mccombs.utexas.edu	Zhou, Mingyuan/AAE-8717-2021		Program for Young Thousand Talent by Chinese Central Government; 111 Project [B18039]; NSFC [61771361]; NSFC for Distinguished Young Scholars [61525105]; Innovation Fund of Xidian University; U.S. National Science Foundation [IIS-1812699]	Program for Young Thousand Talent by Chinese Central Government; 111 Project(Ministry of Education, China - 111 Project); NSFC(National Natural Science Foundation of China (NSFC)); NSFC for Distinguished Young Scholars(National Natural Science Foundation of China (NSFC)National Science Fund for Distinguished Young Scholars); Innovation Fund of Xidian University; U.S. National Science Foundation(National Science Foundation (NSF))	D. Guo, B. Chen, and H. Zhang acknowledge the support of the Program for Young Thousand Talent by Chinese Central Government, the 111 Project (No. B18039), NSFC (61771361), NSFC for Distinguished Young Scholars (61525105) and the Innovation Fund of Xidian University. M. Zhou acknowledges the support of Award IIS-1812699 from the U.S. National Science Foundation.	Acharya Ayan, 2015, AISTATS; ANSCOMBE FJ, 1948, BIOMETRIKA, V35, P246, DOI 10.2307/2332343; Charlin L., 2015, RECSYS ACM, P155; Cong Y., 2017, ICML; Cong YL, 2017, BAYESIAN ANAL, V12, P1017, DOI 10.1214/17-BA1052; Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; Ghahramani Z, 1999, ADV NEUR IN, V11, P431; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Gong C. Y., 2017, NIPS; Gopalan P., 2014, AISTATS; Han S, 2014, NIPS, P2663; Henao R., 2015, NIPS, P2800; Hermans M., 2013, P ADV NEUR INF PROC, V26, P190; Hinton G., 2007, AISTATS; Hosseini SA, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P847, DOI 10.1145/3097983.3098197; Kalman, 1963, J SOC IND APPL MAT A, V1, P152, DOI [10.1137/0301010, DOI 10.1137/0301010]; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; LopezPaz David, 2018, ICLR; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Patterson S., 2013, P 26 INT C NEUR INF, P3102; Ranganath  R., 2014, AISTATS, P762; Schein Aaron, 2016, NIPS; Wang C, 2008, P 24 C UNCERTAINTY A, P579; Wang J., 2006, NIPS; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Zhou M, 2016, ACSR ADV COMPUT, V44, P1; Zhou MY, 2018, BAYESIAN ANAL, V13, P1061, DOI 10.1214/17-BA1070; Zhou MY, 2015, ADV NEUR IN, V28; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211; Zhou Mingyuan, 2012, AISTATS, P1462	36	7	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003004
C	Jothimurugesan, E; Tahmasbi, A; Gibbons, PB; Tirthapura, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jothimurugesan, Ellango; Tahmasbi, Ashraf; Gibbons, Phillip B.; Tirthapura, Srikanta			Variance-Reduced Stochastic Gradient Descent on Streaming Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present an algorithm STRSAGA that can efficiently maintain a machine learning model over data points that arrive over time, and quickly update the model as new training data are observed. We present a competitive analysis that compares the sub-optimality of the model maintained by STRSAGA with that of an offline algorithm that is given the entire data beforehand. Our theoretical and experimental results show that the risk of STRSAGA is comparable to that of an offline algorithm on a variety of input arrival patterns, and its experimental performance is significantly better than prior algorithms suited for streaming data, such as SGD and SSVRG.	[Jothimurugesan, Ellango; Gibbons, Phillip B.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Tahmasbi, Ashraf; Tirthapura, Srikanta] Iowa State Univ, Ames, IA USA	Carnegie Mellon University; Iowa State University	Jothimurugesan, E (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ejothimu@cs.cmu.edu; tahmasbi@iastate.edu; gibbons@cs.cmu.edu; snt@iastate.edu			NSF [1527541, 1725702, 1725663]	NSF(National Science Foundation (NSF))	Supported in part by NSF grant 1725663; Supported in part by NSF grants 1527541 and 1725702	[Anonymous], 2016, ARXIV160306861; Bercu B., 2015, SPRINGERBRIEF MATH; Bertsekas D.P., 2016, NONLINEAR PROGRAMMIN, V3; Bottou L., 2003, NIPS; Bottou L., 2007, P NEURIPS, P161; Daneshmand H., 2016, P 33 INT C MACH LEAR, P1463; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dua D., 2017, UCI MACHINE LEARNING; Frostig R., 2015, P C LEARNING THEORY, P728; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Konecny J., 2013, ARXIV PREPRINT ARXIV; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; Mitzenmacher M., 2017, PROBABILITY COMPUTIN; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663	17	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004046
C	Kaiser, M; Otte, C; Runkler, T; Ek, CH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kaiser, Markus; Otte, Clemens; Runkler, Thomas; Ek, Carl Henrik			Bayesian Alignments of Warped Multi-Output Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a novel Bayesian approach to modelling nonlinear alignments of time series based on latent shared information. We apply the method to the real-world problem of finding common structure in the sensor data of wind turbines introduced by the underlying latent and turbulent wind field. The proposed model allows for both arbitrary alignments of the inputs and non-parametric output warpings to transform the observations. This gives rise to multiple deep Gaussian process models connected via latent generating processes. We present an efficient variational approximation based on nested variational compression and show how the model can be used to extract shared information between dependent time series, recovering an interpretable functional decomposition of the learning problem. We show results for an artificial data set and real-world data of two wind turbines.	[Kaiser, Markus; Runkler, Thomas] Tech Univ Munich, Siemens AG, Munich, Germany; [Otte, Clemens] Siemens AG, Munich, Germany; [Ek, Carl Henrik] Univ Bristol, Bristol, Avon, England	Siemens AG; Siemens Germany; Technical University of Munich; Siemens AG; Siemens Germany; University of Bristol	Kaiser, M (corresponding author), Tech Univ Munich, Siemens AG, Munich, Germany.	markus.kaiser@siemens.com; clemens.otte@siemens.com; thomas.runkler@siemens.com; carlhenrik.ek@bristol.ac.uk		Runkler, Thomas/0000-0002-5465-198X; Ek, Carl Henrik/0000-0003-1302-6309	German Federal Ministry of Education and Research [01IHB15001]	German Federal Ministry of Education and Research(Federal Ministry of Education & Research (BMBF))	The project this report is based on was supported with funds from the German Federal Ministry of Education and Research under project number 01IHB15001. The sole responsibility for the reports contents lies with the authors.	Alvarez M. A., 2009, ADV NEURAL INFORM PR, P57; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Alvarez MA, 2011, ARXIV11066251CSMATHS; Bitar E, 2013, P AMER CONTR CONF, P2898; Boyle P., 2004, ADV NEURAL INFORM PR, P217; Boyle Phillip, 2005, TECH REP; Coburn T.C., 2000, GEOSTATISTICS NATURA; Damianou Andreas C., 2012, ARXIV12110358CSMATHS; Duvenaud David, 2014, AVOIDING PATHOLOGIES; Hensman James, 2014, ARXIV14112005STAT; Hensman James, 2014, ARXIV14121370STAT; Hensman James, 2013, ARXIV13096835CSSTAT; Journel A.G., 1978, MINING GEOSTATISTICS; Lazaro-Gredilla M., 2012, P ADV NEUR INF PROC, V25, P1619; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Salimbeni Hugh, 2017, ARXIV170508933STAT; Schepers JG, 2007, J PHYS CONF SER, V75, DOI 10.1088/1742-6596/75/1/012039; Snelson Edward, 2004, WARPED GAUSSIAN PROC, P337; Snoek Jasper, 2014, ARXIV14020929CSSTAT; Soleimanzadeh M, 2011, MECHATRONICS, V21, P720, DOI 10.1016/j.mechatronics.2011.02.008; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Zhou F, 2012, PROC CVPR IEEE, P1282, DOI 10.1109/CVPR.2012.6247812	24	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001053
C	Levy, KY; Yurtsever, A; Cevher, V		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Levy, Kfir Y.; Yurtsever, Alp; Cevher, Volkan			Online Adaptive Methods, Universality and Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel method for convex unconstrained optimization that, without any modifications, ensures: (i) accelerated convergence rate for smooth objectives, (ii) standard convergence rate in the general (non-smooth) setting, and (iii) standard convergence rate in the stochastic optimization setting. To the best of our knowledge, this is the first method that simultaneously applies to all of the above settings. At the heart of our method is an adaptive learning rate rule that employs importance weights, in the spirit of adaptive online learning algorithms [12, 20], combined with an update that linearly couples two sequences, in the spirit of [2]. An empirical examination of our method demonstrates its applicability to the above mentioned scenarios and corroborates our theoretical findings.	[Levy, Kfir Y.] Swiss Fed Inst Technol, Zurich, Switzerland; [Yurtsever, Alp; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Levy, KY (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	yehuda.levy@inf.ethz.ch; alp.yurtsever@epfl.ch; volkan.cevher@epfl.ch	Yurtsever, Alp/AAB-9053-2020	Levy, Kfir Yehuda/0000-0003-1236-2626	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [725594 - time-data]; ETH Zurich Postdoctoral Fellowship; Marie Curie Actions for People COFUND program	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); ETH Zurich Postdoctoral Fellowship; Marie Curie Actions for People COFUND program	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 725594 - time-data). K.Y.L. is supported by the ETH Zurich Postdoctoral Fellowship and Marie Curie Actions for People COFUND program.	Allen-Zhu Z., 2017, STOC; Allen-Zhu Z., 2017, 8 INN THEOR COMP SCI; Arjevani Y., 2016, J MACHINE LEARNING R, V17, P4303; Attouch H., 2015, ARXIV150701367; Aujol, 2017, OPTIMAL RATE CONVERG; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bubeck S., 2015, ARXIV150608187MATHOC; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Cohen M.B., 2018, ARXIV180512591; Cutkosky A., 2018, ARXIV180206293; Diakonikolas J., 2017, ARXIV170604680; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Flammarion N., 2015, C LEARN THEOR, P658; Foucart S., 2013, MATH INTRO COMPRESSI, V1; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Hu C., 2009, ADV NEURAL INF PROCE, P781; Lan GH, 2015, MATH PROGRAM, V149, P1, DOI 10.1007/s10107-013-0737-x; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Levy K., 2017, ADV NEURAL INFORM PR, P1612; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y., 2003, INTRO LECT CONVEX OP; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Neumaier A, 2016, MATH PROGRAM, V158, P1, DOI 10.1007/s10107-015-0911-4; Orabona F, 2015, LECT NOTES ARTIF INT, V9355, P287, DOI 10.1007/978-3-319-24486-0_19; Scieur D., 2016, ADV NEURAL INFORM PR, P712; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Yurtsever Alp, 2015, ADV NEURAL INFORM PR, P3150	35	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001007
C	Luo, YC; Tian, T; Shi, JX; Zhu, J; Zhang, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Luo, Yucen; Tian, Tian; Shi, Jiaxin; Zhu, Jun; Zhang, Bo			Semi-crowdsourced Clustering with Deep Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the semi-supervised clustering problem where crowdsourcing provides noisy information about the pairwise comparisons on a small subset of data, i.e., whether a sample pair is in the same cluster. We propose a new approach that includes a deep generative model (DGM) to characterize low-level features of the data, and a statistical relational model for noisy pairwise annotations on its subset. The two parts share the latent variables. To make the model automatically trade-off between its complexity and fitting data, we also develop its fully Bayesian variant. The challenge of inference is addressed by fast (natural-gradient) stochastic variational inference algorithms, where we effectively combine variational message passing for the relational part and amortized learning of the DGM under a unified framework. Empirical results on synthetic and real-world datasets show that our model outperforms previous crowdsourced clustering methods.	[Luo, Yucen; Tian, Tian; Shi, Jiaxin; Zhu, Jun; Zhang, Bo] Tsinghua Univ, Dept Comp Sci & Tech, Inst AI, THBI Lab,BNRist Ctr,State Key Lab Intell Tech & S, Beijing, Peoples R China	Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, Inst AI, THBI Lab,BNRist Ctr,State Key Lab Intell Tech & S, Beijing, Peoples R China.	luoyc15@mails.tsinghua.edu.cn; rossowhite@163.com; shijx15@mails.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn; dcszb@mail.tsinghua.edu.cn			National Key Research and Development Program of China [2017YFA0700904]; NSFC [61620106010, 61621136008, 61332007]; Beijing NSF Project [L172037]; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Siemens; NEC; Intel	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Siemens(Siemens AG); NEC; Intel(Intel Corporation)	Yucen Luo would like to thank Matthew Johnson for helpful discussions on the SVAE algorithm [13], and Yale Chang for sharing the code of the UCI benchmark experiments. We thank the anonymous reviewers for feedbacks that greatly improved the paper. This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008, 61332007), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, and the projects from Siemens, NEC and Intel.	[Anonymous], 2006, WIRED MAGAZINE; Bilenko M, 2004, ICML; Chang YL, 2017, PR MACH LEARN RES, V70; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dua D., 2017, UCI MACHINE LEARNING; Gomes Ryan G, 2011, NEURAL INFORM PROCES, P558; Gopalan Prem K, 2012, ADV NEURAL INFORM PR, P2249; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Ren Y., 2016, ADV NEURAL INFORM PR, P2928; Rezende DJ, 2016, PR MACH LEARN RES, V48; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Shi Jiaxin, 2017, ARXIV170905870; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Tian T, 2015, ADV NEUR IN, V28; Vinayak Ramya Korlakai, 2016, NEURAL INFORM PROCES; Winn J, 2005, J MACH LEARN RES, V6, P661; Wiwie C, 2015, NAT METHODS, V12, P1033, DOI [10.1038/NMETH.3583, 10.1038/nmeth.3583]; Wu Lin, 2018, INT C LEARN REPR; Xie JY, 2016, PR MACH LEARN RES, V48; Xing E., 2002, ADV NEURAL INFORM PR, V15, P505, DOI DOI 10.5555/2968618.2968683; Yi J., 2012, P ADV NEURAL INF PRO, P1772; Zhou DY, 2014, PR MACH LEARN RES, V32, P262	32	7	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303023
C	Malkomes, G; Garnett, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Malkomes, Gustavo; Garnett, Roman			Automating Bayesian optimization with Bayesian optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GLOBAL OPTIMIZATION	Bayesian optimization is a powerful tool for global optimization of expensive functions. One of its key components is the underlying probabilistic model used for the objective function f. In practice, however, it is often unclear how one should appropriately choose a model, especially when gathering data is expensive. We introduce a novel automated Bayesian optimization approach that dynamically selects promising models for explaining the observed data using Bayesian optimization in model space. Crucially, we account for the uncertainty in the choice of model; our method is capable of using multiple models to represent its current belief about f and subsequently using this information for decision making. We argue, and demonstrate empirically, that our approach automatically finds suitable models for the objective function, which ultimately results in more-efficient optimization.	[Malkomes, Gustavo; Garnett, Roman] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA	Washington University (WUSTL)	Malkomes, G (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.	luizgustavo@wustl.edu; garnett@wustl.edu		Malkomes, Gustavo/0000-0002-9039-794X	National Science Foundation (NSF) [IIA-1355406]; Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES)	National Science Foundation (NSF)(National Science Foundation (NSF)); Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES)(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES))	GM, and RG were supported by the National Science Foundation (NSF) under award number IIA-1355406. GM was also supported by the Brazilian Federal Agency for Support and Evaluation of Graduate Education (CAPES).	Bergstra J., 2011, NEURAL INFORM PROCES, V24; Duvenaud David, 2013, INT C MACH LEARN ICM; Gardner Jacob R., 2015, C NEUR INF PROC SYST; Gardner Jacob R., 2017, INT C ART INT STAT A; Gonzalez J., 2016, INT C ART INT STAT A; Grosse Roger B, 2012, C UNC ART INT UAI; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kushner HaroldJ., 1964, J BASIC ENG-T ASME, V86, P97, DOI [10.1115/1.3653121, DOI 10.1115/1.3653121]; Li Lisha, 2018, J MACHINE LEARNING R, V18, P1; Malkomes Gustavo, 2016, C NEUR INF PROC SYST; Mockus Jonas, 1974, BAYESIAN METHODS SEE, P400; Osborne Michael A., 2012, C NEUR INF PROC SYST; Raftery AE, 1996, BIOMETRIKA, V83, P251, DOI 10.1093/biomet/83.2.251; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, C NEUR INF PROC SYST; Srinivas N., 2010, INT C MACH LEARN ICM; Surjanovic Sonja, 2017, OPTIMIZATION TEST FU	20	7	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000048
C	Ok, J; Proutiere, A; Tranos, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ok, Jungseul; Proutiere, Alexandre; Tranos, Damianos			Exploration in Structured Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We address reinforcement learning problems with finite state and action spaces where the underlying MDP has some known structure that could be potentially exploited to minimize the exploration rates of suboptimal (state, action) pairs. For any arbitrary structure, we derive problem-specific regret lower bounds satisfied by any learning algorithm. These lower bounds are made explicit for unstructured MDPs and for those whose transition probabilities and average reward functions are Lipschitz continuous w.r.t. the state and action. For Lipschitz MDPs, the bounds are shown not to scale with the sizes S and A of the state and action spaces, i.e., they are smaller than c log T where T is the time horizon and the constant c only depends on the Lipschitz structure, the span of the bias function, and the minimal action sub-optimality gap. This contrasts with unstructured MDPs where the regret lower bound typically scales as SA log T. We devise DEL (Directed Exploration Learning), an algorithm that matches our regret lower bounds. We further simplify the algorithm for Lipschitz MDPs, and show that the simplified version is still able to efficiently exploit the structure.	[Ok, Jungseul; Proutiere, Alexandre; Tranos, Damianos] KTH, EECS, Stockholm, Sweden	Royal Institute of Technology	Ok, J (corresponding author), KTH, EECS, Stockholm, Sweden.	ockjs@illinois.edu; alepro@kth.se; tranos@kth.se			Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; UIUC	Wallenberg AI, Autonomous Systems and Software Program (WASP) - Knut and Alice Wallenberg Foundation; UIUC	This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. Jungseul Ok is now with UIUC in Prof. Sewoong Oh's group. He would like to thank UIUC for financially supporting his participation to NIPS 2018 conference.	Agrawal S, 2017, ADV NEUR IN, V30; Auer P., 2007, ADV NEURAL INFORM PR, V19; Auer P., 2009, ADV NEURAL INFORM PR, V22; Bartlett Peter L., 2009, P 25 C UNC ART INT; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; COMBES R, 2017, ADV NEURAL INFORM PR, V30; Combes  Richard, 2014, P 31 INT C MACH LEAR; Filippi S., 2010, 48 ANN ALL C COMM CO; Garivier A., 2018, MATH OPERATIONS RES; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Kaufmann E, 2016, J MACH LEARN RES, V17; Lakshmanan K., 2015, 32 INT C MACH LEARN; Magureanu  Stefan, 2014, C LEARN THEOR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Osband I., 2016, ARXIV PREPRINT ARXIV; Osband  Ian, 2014, ADV NEURAL INFORM PR, V27; Ronald Ortner, 2012, ADV NEURAL INFORM PR, V25; Tewari A., 2008, ADV NEURAL INFORM PR, V20	18	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003043
C	Pan, BY; Yang, YZ; Li, H; Zhao, Z; Zhuang, YT; Cai, D; He, XF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pan, Boyuan; Yang, Yazheng; Li, Hao; Zhao, Zhou; Zhuang, Yueting; Cai, Deng; He, Xiaofei			MacNet: Transferring Knowledge from Machine Comprehension to Sequence-to-Sequence Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Machine Comprehension (MC) is one of the core problems in natural language processing, requiring both understanding of the natural language and knowledge about the world. Rapid progress has been made since the release of several benchmark datasets, and recently the state-of-the-art models even surpass human performance on the well-known SQuAD evaluation. In this paper, we transfer knowledge learned from machine comprehension to the sequence-to-sequence tasks to deepen the understanding of the text. We propose MacNet: a novel encoder-decoder supplementary architecture to the widely used attention-based sequence-to-sequence models. Experiments on neural machine translation (NMT) and abstractive text summarization show that our proposed framework can significantly improve the performance of the baseline models, and our method for the abstractive text summarization achieves the state-of-the-art results on the Gigaword dataset.	[Pan, Boyuan; Li, Hao; Cai, Deng; He, Xiaofei] Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China; [Yang, Yazheng; Zhao, Zhou; Zhuang, Yueting] Zhejiang Univ, Coll Comp Sci, Hangzhou, Zhejiang, Peoples R China; [Cai, Deng] Alibaba Zhejiang Univ Joint Inst Frontier Technol, Hangzhou, Zhejiang, Peoples R China; [He, Xiaofei] Fabu Inc, Hangzhou, Zhejiang, Peoples R China	Zhejiang University; Zhejiang University	Cai, D (corresponding author), Zhejiang Univ, State Key Lab CAD&CG, Hangzhou, Zhejiang, Peoples R China.; Cai, D (corresponding author), Alibaba Zhejiang Univ Joint Inst Frontier Technol, Hangzhou, Zhejiang, Peoples R China.	panby@zju.edu.cn; yazheng_yang@zju.edu.cn; haolics@zju.edu.cn; zhaozhou@zju.edu.cn; yzhuang@zju.edu.cn; dcai@zju.edu.cn; xiaofeihe@fabu.ai			National Nature Science Foundation of China [61751307, 61602405, U1611461]; National Youth Top-notch Talent Support Program	National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Youth Top-notch Talent Support Program	This work was supported in part by the National Nature Science Foundation of China (Grant Nos: 61751307, 61602405 and U1611461) and in part by the National Youth Top-notch Talent Support Program. The experiments are supported by Chengwei Yao in the Experiment Center of the College of Computer Science and Technology, Zhejiang University.	[Anonymous], 2018, ARXIV180511004; Bengio Y., 2014, ARXIV14061078; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Cui Y., 2017, P INT C ELECT COMMER, P1, DOI [10.1017/S0033291717001520, DOI 10.1017/S0033291717001520]; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gehring J., 2017, P ICML; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hu Minghao, 2017, CORR; Joshi M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1601, DOI 10.18653/v1/P17-1147; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Li JH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P688, DOI 10.18653/v1/P17-1064; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin Tsung-Yi, 2017, ARXIV170802002, P2980, DOI [DOI 10.1109/ICCV.2017.324, DOI 10.1109/TPAMI.2018.2858826]; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; McCann Bryan, 2017, ARXIV170800107; Min Sewon, 2017, ACL; Nallapati R, 2017, AAAI CONF ARTIF INTE, P3075; Pan B., 2017, ARXIV170709098; Pan BY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P989; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Paulus Romain, 2017, ARXIV170504304; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; RAJPURKAR P, 2016, P 2016 C EMP METH NA, V2016, P2383, DOI DOI 10.18653/V1/D16-1264; See A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1073, DOI 10.18653/v1/P17-1099; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Seo Min Joon, 2017, ICLR; Shen YL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1047, DOI 10.1145/3097983.3098177; Shi Xing, 2016, P 2016 C EMP METH NA, P1526, DOI DOI 10.18653/V1/D16-1159; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wang WH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P189, DOI 10.18653/v1/P17-1018; Wang Zhiguo, 2016, ARXIV161204211; Weissenborn Dirk, 2017, CONLL, DOI DOI 10.18653/V1/K17-1028; Williams JD, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P665, DOI 10.18653/v1/P17-1062; Wu YH, 2018, VIS COMPUT IND BIOME, V1, DOI 10.1186/s42492-018-0008-z; Xia Yingce, 2017, ADV NEURAL INFORM PR, P1784; Xiong C., 2017, ICLR 2017; Xiong Caiming, 2018, INT C LEARN REPR; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zeiler Matthew D, 2012, ARXIV12125701; Zhou QY, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1095, DOI 10.18653/v1/P17-1101	46	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000058
C	Qi, Y; Wu, QY; Wang, HN; Tang, J; Sun, MS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Qi, Yi; Wu, Qingyun; Wang, Hongning; Tang, Jie; Sun, Maosong			Bandit Learning with Implicit Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Implicit feedback, such as user clicks, although abundant in online information service systems, does not provide substantial evidence on users' evaluation of system's output. Without proper modeling, such incomplete supervision inevitably misleads model estimation, especially in a bandit learning setting where the feedback is acquired on the fly. In this work, we perform contextual bandit learning with implicit feedback by modeling the feedback as a composition of user result examination and relevance judgment. Since users' examination behavior is unobserved, we introduce latent variables to model it. We perform Thompson sampling on top of variational Bayesian inference for arm selection and model update. Our upper regret bound analysis of the proposed algorithm proves its feasibility of learning from implicit feedback in a bandit setting; and extensive empirical evaluations on click logs collected from a major MOOC platform further demonstrate its learning effectiveness in practice.	[Qi, Yi; Tang, Jie; Sun, Maosong] Tsinghua Univ, Inst Artificial Intelligence, Dept Comp Sci & Tech, State Key Lab Intell Tech & Sys, Beijing, Peoples R China; [Wu, Qingyun; Wang, Hongning] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA	Tsinghua University; University of Virginia	Qi, Y (corresponding author), Tsinghua Univ, Inst Artificial Intelligence, Dept Comp Sci & Tech, State Key Lab Intell Tech & Sys, Beijing, Peoples R China.	qi-y16@mails.tsinghua.edu.cn; qw2ky@virginia.edu; hw5x@virginia.edu; jietang@tsinghua.edu.cn; sms@tsinghua.edu.cn		Wang, Hongning/0000-0002-6524-9195	National Science Foundation [IIS-1553568, IIS-1618948]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their insightful comments. This paper is based upon work supported by a research fund from XuetangX.com and the National Science Foundation under grant IIS-1553568 and IIS-1618948.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abeille M, 2017, ELECTRON J STAT, V11, P5165, DOI 10.1214/17-EJS1341SI; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bouneffouf D, 2012, LECT NOTES COMPUT SC, V7665, P324, DOI 10.1007/978-3-642-34487-9_40; Chuklin A., 2015, SYNTHESIS LECT INF C, V7, P1; Craswell Nick, 2008, P 2008 INT C WEB SEA, P87, DOI [10.1145/1341531.1341545, DOI 10.1145/1341531.1341545]; Filippi S., 2010, NIPS, P586; Guo Fan, 2009, P 2 ACM INT C WEB SE, P124, DOI DOI 10.1145/1498759.1498818; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Jaakkola TS, 2000, STAT COMPUT, V10, P25, DOI 10.1023/A:1008932416310; Joachims Thorsten, 2017, ACM SIGIR Forum, V51, P4, DOI 10.1145/3130332.3130334; Katariya S, 2016, PR MACH LEARN RES, V48; Kawale J., 2015, ADV NEURAL INFORM PR, V28, P1297; Kelly D., 2003, SIGIR Forum, V37, P18, DOI 10.1145/959258.959260; Kveton B, 2015, PR MACH LEARN RES, V37, P767; Lagree Paul, 2016, ADV NEURAL INFORM PR, P1597; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Li W, 2010, P 16 ACM SIGKDD INT; Maillard OA, 2014, PR MACH LEARN RES, V32; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Wang HZ, 2016, CIKM'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1633, DOI 10.1145/2983323.2983847; Yue Y., 2011, ADV NEURAL INFORM PR, P2483; Zoghi M., 2017, ICML 2017, P4199	27	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001080
C	Rohekar, RY; Nisimov, S; Gurwicz, Y; Koren, G; Novik, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rohekar, Raanan Y.; Nisimov, Shami; Gurwicz, Yaniv; Koren, Guy; Novik, Gal			Constructing Deep Neural Networks by Bayesian Network Structure Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a principled approach for unsupervised structure learning of deep neural networks. We propose a new interpretation for depth and inter-layer connectivity where conditional independencies in the input distribution are encoded hierarchically in the network structure. Thus, the depth of the network is determined inherently. The proposed method casts the problem of neural network structure learning as a problem of Bayesian network structure learning. Then, instead of directly learning the discriminative structure, it learns a generative graph, constructs its stochastic inverse, and then constructs a discriminative graph. We prove that conditional-dependency relations among the latent variables in the generative graph are preserved in the class-conditional discriminative graph. We demonstrate on image classification benchmarks that the deepest layers (convolutional and dense) of common networks can be replaced by significantly smaller learned structures, while maintaining classification accuracy-state-of-the-art on tested benchmarks. Our structure learning algorithm requires a small computational cost and runs efficiently on a standard desktop CPU.	[Rohekar, Raanan Y.; Nisimov, Shami; Gurwicz, Yaniv; Koren, Guy; Novik, Gal] Intel AI Lab, Santa Clara, CA 95054 USA		Rohekar, RY (corresponding author), Intel AI Lab, Santa Clara, CA 95054 USA.	raanan.yehezkel@intel.com; shami.nisimov@intel.com; yaniv.gurwicz@intel.com; guy.koren@intel.com; gal.novik@intel.com						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ayinde Babajide O., 2018, WORKSH TRACK INT C L; Chang J.R., 2015, COMPUTER SCI; Chen TH, 2015, DES AUT CON, DOI 10.1145/2744769.2744837; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Collobert R, 2011, J MACH LEARN RES, V12, P2493; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dentinel Zarembaw, 2014, NEURIPS, P1269; Ding X., 2018, P 32 AAAI C ART INT; Donahue J, 2014, PR MACH LEARN RES, V32; Germain M, 2015, PR MACH LEARN RES, V37, P881; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Graf H. P., 2017, P INT C LEARN REPR I; Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang QG, 2018, IEEE WINT CONF APPL, P709, DOI 10.1109/WACV.2018.00083; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larochelle H., 2011, INT C ART INT STAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long MS, 2015, PR MACH LEARN RES, V37, P97; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Manessi F., 2017, ARXIV171201721; Miconi Thomas, 2016, ARXIV160606216; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Murphy K, 2001, COMPUTER SCI STAT, V33, P1024; Nair V, 2010, P 27 INT C MACHINE L, P807; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Negrinho R., 2017, ARXIV170408792; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Paige B., 2016, JMLR, V48; Pearl J., 2009, CAUSALITY MODELS REA; Raiko T, 2012, P INT C ART INT STAT, V22, P924; Real E, 2017, PR MACH LEARN RES, V70; Smith LN, 2016, PROC CVPR IEEE, P4763, DOI 10.1109/CVPR.2016.515; Smithson SC, 2016, ICCAD-IEEE ACM INT, DOI 10.1145/2966986.2967058; Spirtes P., 2000, CAUSATION PREDICTION; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173; Yehezkel R, 2009, J MACH LEARN RES, V10, P1527; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zoph B., 2016, ARXIV161101578; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	56	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303008
C	Sengupta, AM; Tepper, M; Pehlevan, C; Genkin, A; Chklovskii, DB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sengupta, Anirvan M.; Tepper, Mariano; Pehlevan, Cengiz; Genkin, Alexander; Chklovskii, Dmitri B.			Manifold-tiling Localized Receptive Fields are Optimal in Similarity-preserving Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PLASTICITY	Many neurons in the brain, such as place cells in the rodent hippocampus, have localized receptive fields, i.e., they respond to a small neighborhood of stimulus space. What is the functional significance of such representations and how can they arise? Here, we propose that localized receptive fields emerge in similarity-preserving networks of rectifying neurons that learn low-dimensional manifolds populated by sensory inputs. Numerical simulations of such networks on standard datasets yield manifold-tiling localized receptive fields. More generally, we show analytically that, for data lying on symmetric manifolds, optimal solutions of objectives, from which similarity-preserving networks are derived, have localized receptive fields. Therefore, nonnegative similarity-preserving mapping (NSM) implemented by neural networks can model representations of continuous manifolds in the brain.	[Sengupta, Anirvan M.] Rutgers State Univ, New Brunswick, NJ 08854 USA; [Sengupta, Anirvan M.; Tepper, Mariano; Pehlevan, Cengiz; Chklovskii, Dmitri B.] Flatiron Inst, New York, NY 10010 USA; [Genkin, Alexander; Chklovskii, Dmitri B.] NYU, Langone Med Ctr, New York, NY 10003 USA	Rutgers State University New Brunswick; New York University; NYU Langone Medical Center	Sengupta, AM (corresponding author), Rutgers State Univ, New Brunswick, NJ 08854 USA.; Sengupta, AM (corresponding author), Flatiron Inst, New York, NY 10010 USA.	anirvans@physics.rutgers.edu; mtepper@flatironinstitute.org; cpehlevan@flatironinstitute.org; alexander.genkin@gmail.com; dchklovskii@flatironinstitute.org		Pehlevan, Cengiz/0000-0001-9767-6063				Amini A. A., 2014, ARXIV14065647; [Anonymous], 1988, RADIA BASIS FUNCTION; [Anonymous], NIPS; ARORA S, 2015, COLT; Arora S., 2017, ARXIV170604601; Awasthi Pranjal, 2015, ITCS; Bachoc C, 2012, INT SER OPER RES MAN, V166, P219, DOI 10.1007/978-1-4614-0769-0_9; Bahroun Yanis, 2017, ICANN; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Berman A., 2003, COMPLETELY POSITIVE; Bibby J., 1979, MULTIVARIATE ANAL; BRAND M, 2003, NIPS; Burer S, 2009, LINEAR ALGEBRA APPL, V431, P1539, DOI 10.1016/j.laa.2009.05.021; Cho Youngmin, 2009, NIPS; Cox T., 2000, MULTIDIMENSIONAL SCA; Ding C. H. Q., 2005, ICDM; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Dragoi V, 2000, NEURON, V28, P287, DOI 10.1016/S0896-6273(00)00103-3; Feldman DE, 2005, SCIENCE, V310, P810, DOI 10.1126/science.1115807; Hadsell R., 2006, P CVPR; Hensch TK, 2005, NAT REV NEUROSCI, V6, P877, DOI 10.1038/nrn1787; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Jacobson, 2012, BASIC ALGEBRA COURIE; Kilgard MP, 1998, SCIENCE, V279, P1714, DOI 10.1126/science.279.5357.1714; KNUDSEN EI, 1978, SCIENCE, V202, P778, DOI 10.1126/science.715444; O'Keefe J, 1978, HIPPOCAMPUS COGNITIV; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pehlevan C, 2018, NEURAL COMPUT, V30, P84, DOI [10.1162/neco_a_01018, 10.1162/NECO_a_01018]; Pehlevan C, 2017, NEURAL COMPUT, V29, P2925, DOI 10.1162/neco_a_01007; Pehlevan C, 2015, NEURAL COMPUT, V27, P1461, DOI 10.1162/NECO_a_00745; Pehlevan Cengiz, 2015, NIPS; Pehlevan Cengiz, 2014, ACSSC; Pehlevan Cengiz, 2017, ACSSC; Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983; Pitelis Nikolaos, 2013, CVPR; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Seung HS, 2017, ARXIV170400646; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tepper Mariano, 2017, ARXIV170606028; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Weinberger KQ, 2006, INT J COMPUT VISION, V70, P77, DOI 10.1007/s11263-005-4939-z; WEINBERGER KQ, 2006, AAAI; Williams C. K. I., 2001, NIPS	45	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001061
C	Shariff, R; Sheffet, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shariff, Roshan; Sheffet, Or			Differentially Private Contextual Linear Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NOISE	We study the contextual linear bandit problem, a version of the standard stochastic multi-armed bandit (MAB) problem where a learner sequentially selects actions to maximize a reward which depends also on a user provided per-round context. Though the context is chosen arbitrarily or adversarially, the reward is assumed to be a stochastic function of a feature vector that encodes the context and selected action. Our goal is to devise private learners for the contextual linear bandit problem. We first show that using the standard definition of differential privacy results in linear regret. So instead, we adopt the notion of joint differential privacy, where we assume that the action chosen on day t is only revealed to user t and thus needn't be kept private that day, only on following days. We give a general scheme converting the classic linear-UCB algorithm into a joint differentially private algorithm using the tree-based algorithm [10, 18]. We then apply either Gaussian noise or Wishart noise to achieve joint-differentially private algorithms and bound the resulting algorithms' regrets. In addition, we give the first lower bound on the additional regret any private algorithms for the MAB problem must incur.	[Shariff, Roshan; Sheffet, Or] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada	University of Alberta	Shariff, R (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	roshan.shariff@ualberta.ca; osheffet@ualberta.ca			Natural Sciences and Engineering Research Council of Canada (NSERC) [2017-06701]; Alberta Innovates; NSF [1565387]	Natural Sciences and Engineering Research Council of Canada (NSERC)(Natural Sciences and Engineering Research Council of Canada (NSERC)); Alberta Innovates; NSF(National Science Foundation (NSF))	We gratefully acknowledge the Natural Sciences and Engineering Research Council of Canada (NSERC) for supporting R.S. with the Alexander Graham Bell Canada Graduate Scholarship and O.S. with grant #2017-06701. R.S. was also supported by Alberta Innovates and O.S. is also an unpaid collaborator on NSF grant #1565387.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abbasi-Yadkori Yasin, 2012, JMLR WORKSHOP C P, P1; Abe N, 2003, ALGORITHMICA, V37, P263, DOI 10.1007/s00453-003-1038-1; AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Berry D.A., 1985, BANDIT PROBLEMS SEQU; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Chan THH, 2010, LECT NOTES COMPUT SC, V6199, P405, DOI 10.1007/978-3-642-14162-1_34; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dani V, 2008, P C LEARN THEOR COLT, P355; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ACM S THEORY COMPUT, P715; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Dwork Cynthia, 2014, STOC, P11, DOI DOI 10.1145/2591796.2591883; Hardt M, 2010, ACM S THEORY COMPUT, P705; Jain P., 2012, COLT; Karwa Vishesh, 2017, ARXIV171103908CSMATH; Kearns Michael, 2014, MECH DESIGN LARGE GA, P403, DOI [10.1145/2554797.2554834, DOI 10.1145/2554797.2554834]; Lattimore T, 2017, PR MACH LEARN RES, V54, P728; Laurent B, 2000, ANN STAT, V28, P1302; Mishra N, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P592; Neel Seth, 2018, ICML, P3717; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Sheffet Or, 2015, ARXIV150700056CS; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; Thakurta A., 2013, ADV NEURAL INFORM PR, P2733; Tossou Aristide C. Y., 2017, ARXIV170104222CS; Tossou Aristide C. Y., 2016, 30 AAAI C ART INT MA; Vershynin Roman, 2010, ARXIV10113027CSMATH; Zhang F., 2011, MATRIX THEORY BASIC	36	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304032
C	Solin, A; Hensman, J; Turner, RE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Solin, Arno; Hensman, James; Turner, Richard E.			Infinite-Horizon Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Gaussian processes provide a flexible framework for forecasting, removing noise, and interpreting long temporal datasets. State space modelling (Kalman filtering) enables these non-parametric models to be deployed on long datasets by reducing the complexity to linear in the number of data points. The complexity is still cubic in the state dimension m which is an impediment to practical application. In certain special cases (Gaussian likelihood, regular spacing) the GP posterior will reach a steady posterior state when the data are very long. We leverage this and formulate an inference scheme for GPs with general likelihoods, where inference is based on single-sweep EP (assumed density filtering). The infinite-horizon model tackles the cubic cost in the state dimensionality and reduces the cost in the state dimension m to O(m(2)) per data point. The model is extended to online-learning of hyperparameters. We show examples for large finite-length modelling problems, and present how the method runs in real-time on a smartphone on a continuous data stream updated at 100 Hz.	[Solin, Arno] Aalto Univ, Helsinki, Finland; [Hensman, James] PROWLER Io, Cambridge, England; [Solin, Arno; Turner, Richard E.] Univ Cambridge, Cambridge, England	Aalto University; University of Cambridge	Solin, A (corresponding author), Aalto Univ, Helsinki, Finland.; Solin, A (corresponding author), Univ Cambridge, Cambridge, England.	arno.solin@aalto.fi; james@prowler.io; ret26@cam.ac.uk	Solin, Arno/G-6859-2012	Solin, Arno/0000-0002-0958-7886	Academy of Finland [308640]	Academy of Finland(Academy of Finland)	We thank the anonymous reviewers as well as Mark Rowland and Will Tebbutt for their comments on the manuscript. AS acknowledges funding from the Academy of Finland (grant number 308640).	Anderson B. D. O., 1979, OPTIMAL FILTERING; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bui TD, 2017, J MACH LEARN RES, V18; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Duvenaud D., 2013, P INT C MACH LEARN I, P1166; Gustafsson F., 2000, ADAPTIVE FILTERING C; Hartikainen Jouni, 2010, Proceedings of the 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), P379, DOI 10.1109/MLSP.2010.5589113; Hebrail G., 2012, INDIVIDUAL HOUSEHOLD; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2018, J MACH LEARN RES, V18, P1; Heskes T., 2002, UNCERTAINTY ARTIFICI, P216; Krauth K., 2017, UNCERTAINTY ARTIFICI; Lancaster P., 1995, ALGEBRAIC RICCATI EQ; LAUB AJ, 1979, IEEE T AUTOMAT CONTR, V24, P913, DOI 10.1109/TAC.1979.1102178; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Maybeck P. S., 1982, STOCHASTIC MODELS ES; Minka T.P., 2001, P 17 C UNC ART INT, P362; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Nickisch H, 2018, PR MACH LEARN RES, V80; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2010, J MACH LEARN RES, V11, P3011; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Reece S., 2010, P 13 C INF FUS FUSIO; Sarkka S., 2013, BAYESIAN FILTERING S; Sarkka S, 2013, IEEE SIGNAL PROC MAG, V30, P51, DOI 10.1109/MSP.2013.2246292; Sarkka Simo, APPL STOCHASTIC DIFF; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Solin A., 2016, THESIS; Solin A., 2014, STAT COMPUT, P1; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Tokdar ST, 2007, J STAT PLAN INFER, V137, P34, DOI 10.1016/j.jspi.2005.09.005; Turner R.E., 2014, ADV NEURAL INFORM PR, V27, P2213; Vanhatalo J, 2013, J MACH LEARN RES, V14, P1175; Wilson A., 2013, INT C MACH LEARN, P1067; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	37	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303048
C	Valkov, L; Chaudhari, D; Srivastava, A; Sutton, C; Chaudhuri, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Valkov, Lazar; Chaudhari, Dipak; Srivastava, Akash; Sutton, Charles; Chaudhuri, Swarat			HOUDINI: Lifelong Learning as Program Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks significantly accelerates the search.	[Valkov, Lazar; Srivastava, Akash] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Chaudhari, Dipak; Chaudhuri, Swarat] Rice Univ, Houston, TX 77251 USA; [Sutton, Charles] Univ Edinburgh, Alan Turing Inst, Edinburgh, Midlothian, Scotland; [Sutton, Charles] Google Brain, Edinburgh, Midlothian, Scotland	University of Edinburgh; Rice University; University of Edinburgh	Valkov, L (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.	L.Valkov@sms.ed.ac.uk; dipakc@rice.edu; Akash.Srivastava@ed.ac.uk; charlessutton@google.com; swarat@rice.edu	Chaudhari, Dipak/ABA-8075-2021	Srivastava, Akash/0000-0002-6218-1770	DARPA MUSE award [FA8750-14-2-0270]; NSF [CCF-1704883]	DARPA MUSE award; NSF(National Science Foundation (NSF))	This work was partially supported by DARPA MUSE award #FA8750-14-2-0270 and NSF award #CCF-1704883.	Allamanis Miltiadis, 2017, INT C MACH LEARN ICM; Alur R., 2013, FORMAL METHODS COMPU, P1, DOI DOI 10.3233/978-1-61499-495-4-1; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; [Anonymous], 2013, EMNLP; BACKUS J, 1978, COMMUN ACM, V21, P613, DOI 10.1145/359576.359579; Balog Matej, 2017, INT C LEARN REPR ICL; Bosnjak M, 2017, PR MACH LEARN RES, V70; Briggs Forrest, 2006, P 3 AS PAC WORKSH GE, P110; Bunel R. R., 2016, C NEURAL INFORM PROC, V29, P1444; Ellis Kevin, 2017, CORR; Feser JK, 2015, ACM SIGPLAN NOTICES, V50, P229, DOI [10.1145/2737924.2737977, 10.1145/2813885.2737977]; Gaunt A. L., 2016, CORR; Gaunt AL, 2017, PR MACH LEARN RES, V70; Gulwani S, 2017, FOUND TRENDS PROGRAM, V4, P1, DOI 10.1561/2500000010; Hu R., 2017, CORR; Kalyan Ashwin, 2018, INT C LEARN REPR ICL; Kipf T.N., 2017, INT C LEARN REPR; Le V, 2014, ACM SIGPLAN NOTICES, V49, P542, DOI [10.1145/2666356.2594333, 10.1145/2594291.2594333]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2004, COMPUTER VISION PATT; Li Y., 2016, INT C LEARN REPR ICL; Liu C., 2018, EUR C COMP VIS ECCV; Maclaurin D., 2015, AUTOGRAD REVERSE MOD; Olah C., 2015, NEURAL NETWORKS TYPE; Osera PM, 2015, ACM SIGPLAN NOTICES, V50, P619, DOI [10.1145/2737924.2738007, 10.1145/2813885.2738007]; Parisotto Emilio, 2016, 4 INT C LEARN REPR I; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pierce B. C., 2002, TYPES PROGRAMMING LA; Real E., 2017, INT C MACH LEARN ICM; Rusu A. A., 2016, PROGR NEURAL NETWORK; Schwarz Jonathan, 2018, INT C MACH LEARN ICM; Shankar Asim, 2017, EAGER EXECUTION IMPE; Solar-Lezama Armando, 2013, International Journal on Software Tools for Technology Transfer, V15, P475, DOI 10.1007/s10009-012-0249-7; Stallkamp Johannes, 2012, NEURAL NETWORKS; Stumpf A, 2017, INT C ELECTR MACH SY; THRUN S, 1995, ROBOT AUTON SYST, V15, P25, DOI 10.1016/0921-8890(95)00004-Y; Tokui S, 2015, P NIPS WORKSH MACH L; Yosinski J, 2014, ADV NEUR IN, V27; Zhao Z., 2017, GENERATING NATURAL A; Zoph B, 2017, INT C LEARNING REPRE	41	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003026
C	Wang, SW; Huang, LB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Siwei; Huang, Longbo			Multi-armed Bandits with Compensation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose and study the known-compensation multi-armed bandit (KCMAB) problem, where a system controller offers a set of arms to many short-term players for T steps. In each step, one short-term player arrives at the system. Upon arrival, the player aims to select an arm with the current best average reward and receives a stochastic reward associated with the arm. In order to incentivize players to explore other arms, the controller provide proper payment compensations to players. The objective of the controller is to maximize the total reward collected by players while minimizing the total compensation. We first provide a compensation lower bound Theta(Sigma(i) Delta(i) logT/KLi), where Delta(i) and KLi, are the expected reward gap and the Kullback-Leibler (KL) divergence between distributions of arm i and the best arm, respectively. We then analyze three algorithms for solving the KCMAB problem, and obtain their regrets and compensations. We show that the algorithms all achieve O(log T) regret and O(log T) compensation that match the theoretical lower bounds. Finally, we present experimental results to demonstrate the performance of the algorithms.	[Wang, Siwei; Huang, Longbo] Tsinghua Univ, IIIS, Beijing, Peoples R China	Tsinghua University	Wang, SW (corresponding author), Tsinghua Univ, IIIS, Beijing, Peoples R China.	wangsw15@mails.tsinghua.edu.cn; longbohuang@tsinghua.edu.cn		Wang, Siwei/0000-0003-0764-5592	National Natural Science Foundation of China [61672316, 61303195]; Tsinghua Initiative Research Grant; China Youth 1000-Talent Grant	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Tsinghua Initiative Research Grant; China Youth 1000-Talent Grant	This work is supported in part by the National Natural Science Foundation of China Grants 61672316, 61303195, the Tsinghua Initiative Research Grant, and the China Youth 1000-Talent Grant	Agrawal S., 2013, ARTIF INTELL, P99; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berry D. A., 1985, MONOGRAPHS STAT APPL; Beygelzimer A, 2011, INT C ART INT STAT, V15, P19; Che YK, 2018, Q J ECON, V133, P871, DOI 10.1093/qje/qjx044; Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P245, DOI 10.1145/2745844.2745847; Frazier P, 2014, P 15 ACM C EC COMP, P5, DOI DOI 10.1145/2600057.2602897; Gittins J.C., 1989, WILEY INTERSCIENCE S; Kalyanakrishnan S., 2012, INT C MACH LEARN; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Maillard O.-A., 2011, P 14 INT C ART INT S, P570; Mansour Y., 2016, ARXIV160207570; Mansour Yishay, 2015, P 16 ACM C EC COMP, P565, DOI DOI 10.1145/2764468.2764508; Papanastasiou Yiangos, 2017, MANAGEMENT SCI; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tran-Thanh L., 2012, AAAI; Watkins CJCH., 1989, THESIS; Xia YC, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3960	21	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305015
C	Yu, TS; Yan, JC; Wang, YL; Liu, W; Li, BX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yu, Tianshu; Yan, Junchi; Wang, Yilin; Liu, Wei; Li, Baoxin			Generalizing graph matching beyond quadratic assignment model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION	Graph matching has received persistent attention over several decades, which can be formulated as a quadratic assignment problem (QAP). We show that a large family of functions, which we define as Separable Functions, can approximate discrete graph matching in the continuous domain asymptotically by varying the approximation controlling parameters. We also study the properties of global optimality and devise convex/concave-preserving extensions to the widely used Lawler's QAP form. Our theoretical findings show the potential for deriving new algorithms and techniques for graph matching. We deliver solvers based on two specific instances of Separable Functions, and the state-of-the-art performance of our method is verified on popular benchmarks.	[Yu, Tianshu; Wang, Yilin; Li, Baoxin] Arizona State Univ, Tempe, AZ 85287 USA; [Yan, Junchi] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Liu, Wei] Tecent AI Lab, Bellevue, WA USA	Arizona State University; Arizona State University-Tempe; Shanghai Jiao Tong University	Yu, TS (corresponding author), Arizona State Univ, Tempe, AZ 85287 USA.	tianshuy@asu.edu; yanjunchi@sjtu.edu.cn; yilwang@adobe.com; wl2223@columbia.edu; baoxin.li@asu.edu		Liu, Wei/0000-0002-3865-8145	ONR; NSFC [61602176]; Tencent AI Lab Rhino-Bird Joint Research Program [JR201804]	ONR(Office of Naval Research); NSFC(National Natural Science Foundation of China (NSFC)); Tencent AI Lab Rhino-Bird Joint Research Program	This work was supported in part by a grant from ONR. Junchi Yan is supported in part by NSFC 61602176 and Tencent AI Lab Rhino-Bird Joint Research Program (No. JR201804). Any opinions expressed in this material are those of the authors and do not necessarily reflect the views of ONR.	Adamczewski K., 2015, ICCV; ALMOHAMAD HA, 1993, IEEE T PATTERN ANAL, V15, P522, DOI 10.1109/34.211474; Caelli T, 2004, IEEE T PATTERN ANAL, V26, P515, DOI 10.1109/TPAMI.2004.1265866; Caetano TS, 2006, IEEE T PATTERN ANAL, V28, P1646, DOI 10.1109/TPAMI.2006.207; Chang MC, 2011, COMPUT VIS IMAGE UND, V115, P707, DOI 10.1016/j.cviu.2010.10.013; Cho M., 2010, ECCV; Cho M., 2009, ICCV; Duchenne O., 2011, ICCV; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Jiang B., 2017, CVPR; Jiang B., 2017, ADV NEURAL INFORM PR, P3190; KOOPMANS TC, 1957, ECONOMETRICA, V25, P53, DOI 10.2307/1907742; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; LAWLER EL, 1963, MANAGE SCI, V9, P586, DOI 10.1287/mnsc.9.4.586; Lee J., 2011, CVPR; Lee J., 2010, ICPR; Leordeanu M., 2009, NIPS; LEORDEANU M, 2005, ICCV; Li B, 2018, P EUR C COMP VIS, P139; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Liu ZY, 2012, IEEE T PATTERN ANAL, V34, P1451, DOI 10.1109/TPAMI.2012.45; Loiola EM, 2007, EUR J OPER RES, V176, P657, DOI 10.1016/j.ejor.2005.09.032; Lowe D. G., 1999, ICCV; Matas J, 2004, IMAGE VISION COMPUT, V22, P761, DOI 10.1016/j.imavis.2004.02.006; Rangarajan A, 1999, NEURAL COMPUT, V11, P1455, DOI 10.1162/089976699300016313; Schellewald C., 2005, EMMCVPR; Shi  J., 2006, NIPS; Tian Y., 2012, ECCV; Torr P. H. S., 2003, AISTATS; Umeyama S., 1988, TPAMI; Wang Tiantian, 2016, ECCV; Yan J., 2016, ICMR; Yan JC, 2016, IEEE T PATTERN ANAL, V38, P1228, DOI 10.1109/TPAMI.2015.2477832; Yan JC, 2015, IEEE T IMAGE PROCESS, V24, P994, DOI 10.1109/TIP.2014.2387386; Yu T., 2018, CVPR; YUILLE AL, 1994, NEURAL COMPUT, V6, P341, DOI 10.1162/neco.1994.6.3.341; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zaslavskiy M, 2009, BIOINFORMATICS, V25, pI259, DOI 10.1093/bioinformatics/btp196; Zhao  Z., 2014, ICALIP; Zhou F., 2012, CVPR	40	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300079
C	Arik, SO; Diamos, G; Gibiansky, A; Miller, J; Peng, KN; Ping, W; Raiman, J; Zhou, YQ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Arik, Sercan O.; Diamos, Gregory; Gibiansky, Andrew; Miller, John; Peng, Kainan; Ping, Wei; Raiman, Jonathan; Zhou, Yanqi			Deep Voice 2: Multi-Speaker Neural Text-to-Speech	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a technique for augmenting neural text-to-speech (TTS) with low-dimensional trainable speaker embeddings to generate different voices from a single model. As a starting point, we show improvements over the two state-ofthe-art approaches for single-speaker neural TTS: Deep Voice 1 and Tacotron. We introduce Deep Voice 2, which is based on a similar pipeline with Deep Voice 1, but constructed with higher performance building blocks and demonstrates a significant audio quality improvement over Deep Voice 1. We improve Tacotron by introducing a post-processing neural vocoder, and demonstrate a significant audio quality improvement. We then demonstrate our technique for multi-speaker speech synthesis for both Deep Voice 2 and Tacotron on two multi-speaker TTS datasets. We show that a single neural TTS system can learn hundreds of unique voices from less than half an hour of data per speaker, while achieving high audio quality synthesis and preserving the speaker identities almost perfectly.	[Arik, Sercan O.; Diamos, Gregory; Gibiansky, Andrew; Miller, John; Peng, Kainan; Ping, Wei; Raiman, Jonathan; Zhou, Yanqi] Baidu Silicon Valley Artificial Intelligence Lab, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA		Arik, SO (corresponding author), Baidu Silicon Valley Artificial Intelligence Lab, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA.	sercanarik@baidu.com; gregdiamos@baidu.com; gibianskyandrew@baidu.com; millerjohn@baidu.com; pengkainan@baidu.com; pingwei01@baidu.com; jonathanraiman@baidu.com; zhouyanqi@baidu.com	Jeong, Yongwook/N-7413-2016; Peng, Kainan/AAF-5799-2020; Ping, Wei/O-4470-2019					Abdel-Hamid O., 2013, ICASSP; Arik S. O., 2017, ICML; Bradbury J., 2017, ICLR; Cho K., 2014, P 2014 C EMP METH NA, P1724; Fan Y., 2015, IEEE ICASSP; Graves A., 2006, ICML; Hsu C.-C., 2017, ARXIV170400849; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Lample Guillaume, 2016, P NAACL HLT; Li C, 2017, ARXIV PREPRINT ARXIV; Mehri S., 2016, ARXIV161207837; Oord A.v.d., 2016, 9 ISCA SPEECH SYNTHE; Reynolds DA, 2000, DIGIT SIGNAL PROCESS, V10, P19, DOI 10.1006/dspr.1999.0361; Ribeiro F., 2011, IEEE ICASSP; Ronanki  S., 2016, ARXIV160806134; Salimans Tim, 2016, ADV NEURAL INFORM PR; Sotelo J, 2017, ICLR WORKSH TRACK; Wang YH, 2017, INTERSPEECH, P3822, DOI 10.21437/Interspeech.2017-877; Wu Z., 2015, INTERSPEECH 2015, p09 2015; Yamagishi J., 2009, IEEE T AUDIO SPEECH; Yang S., 2016, SIGN INF PROC ASS AN; Zen H., 2015, IEEE ICASSP; Zen H., 2016, ARXIV160606061	24	7	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403003
C	Bamler, R; Zhang, C; Opper, M; Mandt, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bamler, Robert; Zhang, Cheng; Opper, Manfred; Mandt, Stephan			Perturbative Black Box Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Black box variational inference (BBVI) with reparameterization gradients triggered the exploration of divergence measures other than the Kullback-Leibler (KL) divergence, such as alpha divergences. In this paper, we view BBVI with generalized divergences as a form of estimating the marginal likelihood via biased importance sampling. The choice of divergence determines a bias-variance trade-off between the tightness of a bound on the marginal likelihood (low bias) and the variance of its gradient estimators. Drawing on variational perturbation theory of statistical physics, we use these insights to construct a family of new variational bounds. Enumerated by an odd integer order K, this family captures the standard KL bound for K = 1, and converges to the exact marginal likelihood as K -> infinity. Compared to alpha-divergences, our reparameterization gradients have a lower variance. We show in experiments on Gaussian Processes and Variational Autoencoders that the new bounds are more mass covering, and that the resulting posterior covariances are closer to the true posterior and lead to higher likelihoods on held-out data.	[Bamler, Robert; Zhang, Cheng; Mandt, Stephan] Disney Res, Pittsburgh, PA 15213 USA; [Opper, Manfred] TU Berlin, Berlin, Germany	Technical University of Berlin	Bamler, R (corresponding author), Disney Res, Pittsburgh, PA 15213 USA.	robert.bamler@disneyresearch.com; cheng.zhang@disneyresearch.com; manfred.opper@tu-berlin.de; stephan.mandt@disneyresearch.com	Jeong, Yongwook/N-7413-2016	Bamler, Robert/0000-0002-3135-8107; Mandt, Stephan/0000-0001-7836-7839				Amari S., 2012, DIFFERENTIAL GEOMETR, V28; [Anonymous], 2014, ICLR; Bamler R., 2017, ICML; Bottou L., 2010, COMPSTAT; Burda Y., 2016, ICLR; Dieng A. B., 2017, ICML; Hernandez-Lobato J., 2016, ICML; Hoffman M.D., 2015, AISTATS; Hoffman M. D., 2013, JMLR, V14; Jordan M. I., 1999, MACHINE LEARNING, V37; Kappen H. J., 2001, 2 ORDER APPROXIMATIO; Kleinert H., 2009, PATH INTEGRALS QUANT, V5th; LeCun Yann, 1998, GRADIENT BASED LEARN, V86; Li Y., 2016, NIPS; Minka Tom, 2005, TECHNICAL REPORT; Opper M, 2015, STAT PHYS OPTIMIZATI, P263; Opper M., 2013, JMLR, V14; Paquet U., 2009, JMLR, V10; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; Ranganath R., 2015, AISTATS; Ranganath R., 2016, ICML; Ranganath R., 2014, AISTATS; Rezende D.J., 2014, ICML; Robbins H., 1951, ANN MATH STAT; Ruiz F. R., 2016, NIPS; Tanaka T., 1999, NIPS; Tanaka T., 2000, NEURAL COMPUTATION, V12; Thouless D., 1977, PHILOS MAGAZINE, V35	29	7	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405016
C	Cho, M; Lee, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cho, Minhyung; Lee, Jaehyung			Riemannian approach to batch normalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GEOMETRY	Batch Normalization (BN) has proven to be an effective algorithm for deep neural network training by normalizing the input to each neuron and reducing the internal covariate shift. The space of weight vectors in the BN layer can be naturally interpreted as a Riemannian manifold, which is invariant to linear scaling of weights. Following the intrinsic geometry of this manifold provides a new learning rule that is more efficient and easier to analyze. We also propose intuitive and effective gradient clipping and regularization methods for the proposed algorithm by utilizing the geometry of the manifold. The resulting algorithm consistently outperforms the original BN on various types of network architectures and datasets.	[Cho, Minhyung] Appl Res Korea, Seongnam, South Korea; [Lee, Jaehyung] Gracenote Inc, Emeryville, CA USA		Cho, M (corresponding author), Appl Res Korea, Seongnam, South Korea.	mhyung.cho@gmail.com; jaehyung.lee@kaist.ac.kr	Jeong, Yongwook/N-7413-2016					Absil PA, 2004, ACTA APPL MATH, V80, P199, DOI 10.1023/B:ACAP.0000013855.14971.91; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arpit D, 2016, PR MACH LEARN RES, V48; Ba L.J, 2016, P C WORKSH NEUR INF; Badrinarayanan Vijay, 2015, ARXIV151101029; Do Carmo M., 1976, DIFFERENTIAL GEOMETR; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Ghahramani Zoubin, 1996, CRGTR961 U TOR; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hamm J., 2009, ADV NEURAL INFORM PR, P601; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Ioffe S., 2017, ADV NEURAL INFORM PR; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Lee Chen- Yu, 2016, INT C ART INT STAT; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Pascanu R., 2014, INT C LEARN REPR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; Zagoruyko S., 2016, BMVC	31	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405030
C	Ciliberto, C; Rudi, A; Rosasco, L; Pontil, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ciliberto, Carlo; Rudi, Alessandro; Rosasco, Lorenzo; Pontil, Massimiliano			Consistent Multitask Learning with Nonlinear Output Relations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Key to multitask learning is exploiting the relationships between different tasks in order to improve prediction performance. Most previous methods have focused on the case where tasks relations can be modeled as linear operators and regularization approaches can be used successfully. However, in practice assuming the tasks to be linearly related is often restrictive, and allowing for nonlinear structures is a challenge. In this paper, we tackle this issue by casting the problem within the framework of structured prediction. Our main contribution is a novel algorithm for learning multiple tasks which are related by a system of nonlinear equations that their joint outputs need to satisfy. We show that our algorithm can be efficiently implemented and study its generalization properties, proving universal consistency and learning rates. Our theoretical analysis highlights the benefits of non-linear multitask learning over learning the tasks independently. Encouraging experimental results show the benefits of the proposed method in practice.	[Ciliberto, Carlo; Pontil, Massimiliano] UCL, Dept Comp Sci, London, England; [Rudi, Alessandro] INRIA Sierra Project Team, Paris, France; [Rudi, Alessandro] Ecole Normale Super, Paris, France; [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA; [Rosasco, Lorenzo] Univ Genoa, Genoa, Italy; [Rosasco, Lorenzo; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy	University of London; University College London; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Massachusetts Institute of Technology (MIT); University of Genoa; Istituto Italiano di Tecnologia - IIT	Ciliberto, C (corresponding author), UCL, Dept Comp Sci, London, England.	c.ciliberto@ucl.ac.uk; alessandro.rudi@inria.fr; lrosasco@mit.edu; m.pontil@ucl.ac.uk	Jeong, Yongwook/N-7413-2016; Rudi, Alessandro/J-7323-2013	/0000-0003-0249-5273; Rudi, Alessandro/0000-0002-3879-7794	EPSRC [EP/P009069/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported in part by EPSRC grant EP/P009069/1.	Agarwal A., 2010, P ADV NEUR INF PROC, P46; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bishop C.M., 2014, ANTIMICROB AGENTS CH, V58, P7250; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Ciliberto C, 2015, INT C MACH LEARN ICM; Ciliberto C, 2015, PROC CVPR IEEE, P131, DOI 10.1109/CVPR.2015.7298608; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Cormen T.H., 2009, INTRO ALGORITHMS; Dekel Ofer, 2004, ADV NEURAL INFORM PR; Dinuzzo Francesco, 2011, INT C MACH LEARN; Duchi John C., 2010, P 27 INT C MACH LEAR, P327; Dwivedi Y, 2016, NEURAL PLAST, V2016, DOI 10.1155/2016/7383724; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Evgeniou T, 2004, P 10 ACM SIGKDD INT; Fergus Rob, 2010, EUR C COMP VIS; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Herbrich R, 2000, ADV NEUR IN, P115; Hofmann Thomas, 2007, PREDICTING STRUCTURE; Jacob L., 2008, ADV NEURAL INFORM PR; Jawanpuria P., 2015, P INT C NEUR PROC, P1189; Jawanpuria Pratik, 2012, INT C MACH LEARN; Maurer A, 2016, J MACH LEARN RES, V17; Micchelli Charles A, 2004, ADV NEURAL INFORM PR, P921; Nowozin Sebastian, 2011, FDN TRENDS COMPUTER; Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pontil M., 2013, PROC ANN C LEARN THE, P55; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Scholkopf B., 2001, LEARNING KERNELS SUP; Sciavicco L., 1996, MODELING CONTROL ROB, V8; Sindhwani Vikas, 2012, ABS12104792 CORR ABS12104792 CORR; Sra S, 2016, ADV COMPUT VIS PATT, P73, DOI 10.1007/978-3-319-45026-1_3; Steinke F., 2009, ADV NEURAL INFORM PR, P1561; Steinke F, 2010, SIAM J IMAGING SCI, V3, P527, DOI 10.1137/080744189; Thrun S., 2012, LEARNING LEARN; TSOCHANTARIDIS I, 2005, J MACHINE LEARNING R; Yu Zhang, 2010, C UNC ART INT UAI	40	7	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402004
C	Cohen-Addad, V; Kanade, V; Mallmann-Trenn, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cohen-Addad, Vincent; Kanade, Varun; Mallmann-Trenn, Frederik			Hierarchical Clustering Beyond the Worst-Case	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Hiererachical clustering, that is computing a recursive partitioning of a dataset to obtain clusters at increasingly finer granularity is a fundamental problem in data analysis. Although hierarchical clustering has mostly been studied through procedures such as linkage algorithms, or top-down heuristics, rather than as optimization problems, Dasgupta [9] recently proposed an objective function for hierarchical clustering and initiated a line of work developing algorithms that explicitly optimize an objective (see also [7, 22, 8]). In this paper, we consider a fairly general random graph model for hierarchical clustering, called the hierarchical stochastic block model (HSBM), and show that in certain regimes the SVD approach of McSherry [18] combined with specific linkage methods results in a clustering that give an O(1) approximation to Dasgupta's cost function. Finally, we report empirical evaluation on synthetic and real-world data showing that our proposed SVD-based method does indeed achieve a better cost than other widely-used heurstics and also results in a better classification accuracy when the underlying problem was that of multi-class classification.	[Cohen-Addad, Vincent] Univ Copenhagen, Copenhagen, Denmark; [Kanade, Varun] Univ Oxford, Alan Turing Inst, Oxford, England; [Mallmann-Trenn, Frederik] MIT, Cambridge, MA 02139 USA	University of Copenhagen; University of Oxford; Massachusetts Institute of Technology (MIT)	Cohen-Addad, V (corresponding author), Univ Copenhagen, Copenhagen, Denmark.	vcohenad@gmail.com; varunk@cs.ox.ac.uk; mallmann@mit.edu	Kanade, Varun/AAS-3434-2020; Jeong, Yongwook/N-7413-2016	Kanade, Varun/0000-0002-2300-4819; 	European Union [748094]; EPSRC [EP/N510129/1]; NSF [BIO-1455983, CCF-1461559, CCF-0939370]	European Union(European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); NSF(National Science Foundation (NSF))	The project leading to this application has received funding from the European Union's Horizon 2020 research and innovation programme under the Marie Sklodowska-Curie grant agreement No. 748094. This work was supported in part by EPSRC grant EP/N510129/1. This work was supported in part by NSF Award Numbers BIO-1455983, CCF-1461559, and CCF-0939370.	Asuncion D. N. A., 2007, UCI MACHINE LEARNING; Balcan M.-F., STOC 08, P671; Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575; Buitinck L., 2013, API DESIGN MACHINE L, DOI DOI 10.48550/ARXIV.1309.0238; Carlsson G, 2010, J MACH LEARN RES, V11, P1425; Castro RM, 2004, IEEE T SIGNAL PROCES, V52, P2308, DOI 10.1109/TSP.2004.831124; Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841; Cohen-Addad V., 2017, SODA 17; Dasgupta S, 2005, J COMPUT SYST SCI, V70, P555, DOI 10.1016/j.jcss.2004.10.006; Dasgupta Sanjoy, 2016, P 48 ANN ACM S THEOR; Feige U, 2001, J COMPUT SYST SCI, V63, P639, DOI 10.1006/jcss.2001.1773; Felsenstein J, 2004, INFERRING PHYLOGENIE, V2; Friedman J., 2015, PACKAGE GLASSO; GUENOCHE A, 1991, J CLASSIF, V8, P5, DOI 10.1007/BF02616245; Jardine N., 1972, WILEY SERIES PROBABI; Lin GL, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1147, DOI 10.1145/1109557.1109684; Lyzinski V, 2017, IEEE T NETW SCI ENG, V4, P13, DOI 10.1109/TNSE.2016.2634322; McSherry F., FOCS 01, P529; MURTAGH F, 1983, COMPUT J, V26, P354, DOI 10.1093/comjnl/26.4.354; Plaxton C.G., STOC 03, P40; Reddy C. K., 2013, DATA CLUSTERING ALGO, V87; Roy A., NIPS 16, P2316; SNEATH PHA, 1962, NATURE, V193, P855, DOI 10.1038/193855a0; Steinbach M., 2000, COMP DOCUMENT CLUSTE	24	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406027
C	Garcia-Duran, A; Niepert, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Garcia-Duran, Alberto; Niepert, Mathias			Learning Graph Representations with Embedding Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LINK-PREDICTION	We propose Embedding Propagation (EP), an unsupervised learning framework for graph-structured data. EP learns vector representations of graphs by passing two types of messages between neighboring nodes. Forward messages consist of label representations such as representations of words and other attributes associated with the nodes. Backward messages consist of gradients that result from aggregating the label representations and applying a reconstruction loss. Node representations are finally computed from the representation of their labels. With significantly fewer parameters and hyperparameters an instance of EP is competitive with and often outperforms state of the art unsupervised and semi-supervised learning methods on a range of benchmark data sets.	[Garcia-Duran, Alberto; Niepert, Mathias] NEC Labs Europe, Heidelberg, Germany	NEC Corporation	Garcia-Duran, A (corresponding author), NEC Labs Europe, Heidelberg, Germany.	alberto.duran@neclab.eu; mathias.niepert@neclab.eu	Jeong, Yongwook/N-7413-2016					Akoglu L, 2015, DATA MIN KNOWL DISC, V29, P626, DOI 10.1007/s10618-014-0365-y; Almeida L. B., 1990, ARTIFICIAL NEURAL NE, P102; Belkin M, 2002, ADV NEUR IN, V14, P585; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Bordes A., 2013, ADV NEURAL INFORM PR; Breitkreutz BJ, 2008, NUCLEIC ACIDS RES, V36, pD637, DOI 10.1093/nar/gkm1001; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; de Raedt L., 2016, SYNTHESIS LECT ARTIF, DOI DOI 10.2200/S00692ED1V01Y201601AIM032; Frome Andrea, 2013, NEURIPS; Getoor Lise, 2007, INTRO STAT RELATIONA; Gilmer J, 2017, PR MACH LEARN RES, V70; Glorot X., 2010, PROC MACH LEARN RES, P249; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Kersting K., 2014, 28 AAAI C ART INT; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kruskal Joseph B, 1978, MULTIDIMENSIONAL SCA, P7; Li Yujia, 2015, ARXIV151105493; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Low Y., 2010, C UNC ART INT UAI JU; Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027; Macskassy S. A., 2003, TECHNICAL REPORT DTI; Mahoney M., 2009, LARGE TEXT COMPRESSI; Malewicz G., 2010, P INT C MANAGEMENT D, P135, DOI [DOI 10.1145/1807167.1807184, 10.1145/1807167.1807184]; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Ngiam Jiquan, 2011, ICML, DOI DOI 10.5555/3104482.3104569; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Page L., 1999, PAGERANK CITATION RA; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; ROUSSEEUW PJ, 1987, J COMPUT APPL MATH, V20, P53, DOI 10.1016/0377-0427(87)90125-7; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tang L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P817; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Xin R.S., 2013, 1 INT WORKSH GRAPH D, P1, DOI DOI 10.1145/2484425.2484427; Yang Z, 2016, PR MACH LEARN RES, V48; Zafarani R., 2009, SOCIAL COMPUTING DAT; Zhu Xiaojin, 2002, TECHNICAL REPORT, P1	46	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405020
C	Jin, WG; Coley, CW; Barzilay, R; Jaakkola, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jin, Wengong; Coley, Connor W.; Barzilay, Regina; Jaakkola, Tommi			Predicting Organic Reaction Outcomes with Weisfeiler-Lehman Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The prediction of organic reaction outcomes is a fundamental problem in computational chemistry. Since a reaction may involve hundreds of atoms, fully exploring the space of possible transformations is intractable. The current solution utilizes reaction templates to limit the space, but it suffers from coverage and efficiency issues. In this paper, we propose a template-free approach to efficiently explore the space of product molecules by first pinpointing the reaction center - the set of nodes and edges where graph edits occur. Since only a small number of atoms contribute to reaction center, we can directly enumerate candidate products. The generated candidates are scored by a Weisfeiler-Lehman Difference Network that models high-order interactions between changes occurring at nodes across the molecule. Our framework outperforms the top-performing template-based approach with a 10% margin, while running orders of magnitude faster. Finally, we demonstrate that the model accuracy rivals the performance of domain experts.	[Jin, Wengong; Barzilay, Regina; Jaakkola, Tommi] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA; [Coley, Connor W.] MIT, Dept Chem Engn, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Jin, WG (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	wengong@csail.mit.edu; regina@csail.mit.edu; tommi@csail.mit.edu; ccoley@mit.edu	Jeong, Yongwook/N-7413-2016		DARPA Make-It program [ARO W911NF-16-2-0023]	DARPA Make-It program	We thank Tim Jamison, Darsh Shah, Karthik Narasimhan and the reviewers for their helpful comments. We also thank members of the MIT Department of Chemistry and Department of Chemical Engineering who participated in the human benchmarking study. This work was supported by the DARPA Make-It program under contract ARO W911NF-16-2-0023.	[Anonymous], ARXIV160305629; Chen JH, 2009, J CHEM INF MODEL, V49, P2034, DOI 10.1021/ci900157k; Christ CD, 2012, J CHEM INF MODEL, V52, P1745, DOI 10.1021/ci300116p; Coley Connor W, 2017, ACS CENTRAL SCI; Duvenaud David K, 2015, P NIPS; Gilmer Justin, 2017, ARXIV170401212; Hartenfeller M, 2011, J CHEM INF MODEL, V51, P3093, DOI 10.1021/ci200379p; Kayala MA, 2011, J CHEM INF MODEL, V51, P2209, DOI 10.1021/ci200207y; Kingma D.P., 2015, INT C LEARN REPR ICL; Law J, 2009, J CHEM INF MODEL, V49, P593, DOI 10.1021/ci800228y; Lei Tao, 2017, P 34 INT C MACH LEAR; Lowe D.M., 2014, PATENT REACTION EXTR; Segler Marwin HS, 2017, CHEM EUROPEAN J; Szymkuc S, 2016, ANGEW CHEM INT EDIT, V55, P5904, DOI 10.1002/anie.201506101; Todd MH, 2005, CHEM SOC REV, V34, P247, DOI 10.1039/b104620a; Warr WA, 2014, MOL INFORM, V33, P469, DOI 10.1002/minf.201400052; Wei JN, 2016, ACS CENTRAL SCI, V2, P725, DOI 10.1021/acscentsci.6b00219	18	7	7	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402064
C	Komiyama, J; Honda, J; Takeda, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Komiyama, Junpei; Honda, Junya; Takeda, Akiko			Position-based Multiple-play Bandit Problem with Unknown Position Bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMAL ADAPTIVE POLICIES	Motivated by online advertising, we study a multiple-play multi-armed bandit problem with position bias that involves several slots and the latter slots yield fewer rewards. We characterize the hardness of the problem by deriving an asymptotic regret bound. We propose the Permutation Minimum Empirical Divergence (PMED) algorithm and derive its asymptotically optimal regret bound. Because of the uncertainty of the position bias, the optimal algorithm for such a problem requires non-convex optimizations that are different from usual partial monitoring and semi-bandit problems. We propose a cutting-plane method and related bi-convex relaxation for these optimizations by using auxiliary variables.	[Komiyama, Junpei; Honda, Junya] Univ Tokyo, Tokyo, Japan; [Honda, Junya; Takeda, Akiko] RIKEN, Wako, Saitama, Japan; [Takeda, Akiko] Inst Stat Math, Tachikawa, Tokyo, Japan	University of Tokyo; RIKEN; Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan	Komiyama, J (corresponding author), Univ Tokyo, Tokyo, Japan.	junpei@komiyama.info; honda@stat.t.u-tokyo.ac.jp; atakeda@ism.ac.jp	Komiyama, Junpei/ABR-9951-2022		JSPS KAKENHI [17K12736, 16H00881, 15K00031]; Inamori Foundation Research Grant	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Inamori Foundation Research Grant	The authors gratefully acknowledge Kohei Komiyama for discussion on a permutation matrix and sincerely thank the anonymous reviewers for their useful comments. This work was supported in part by JSPS KAKENHI Grant Number 17K12736, 16H00881, 15K00031, and Inamori Foundation Research Grant.	Agarwal D., 2009, WWW, P21; ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; Auer P., 2002, SIAM J COMPUTING; Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663; Bubeck S., 2010, THESIS; Burnetas AN, 1996, ADV APPL MATH, V17, P122, DOI 10.1006/aama.1996.0007; Burnetas AN, 1997, MATH OPER RES, V22, P222, DOI 10.1287/moor.22.1.222; Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P231; Combes R., 2015, P 28 INT C NEUR INF, P2116; Craswell Nick, 2008, P 2008 INT C WEB SEA, P87, DOI [10.1145/1341531.1341545, DOI 10.1145/1341531.1341545]; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Garivier A., 2016, C LEARN THEOR, P998; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Guo Fan, 2009, P 2 ACM INT C WEB SE, P124, DOI DOI 10.1145/1498759.1498818; HOGAN WW, 1973, SIAM REV, V15, P591, DOI 10.1137/1015073; Honda Junya, 2010, COLT, P67; Hopcroft J. E., 1973, SIAM Journal on Computing, V2, P225, DOI 10.1137/0202019; IAB, 2017, IAB INT ADV REV REP; Kale S., 2010, ADV NEURAL INFORM PR, V23, P1054; Katariya S, 2016, PR MACH LEARN RES, V48; KEMPE D, 2008, WINE, V5385, P585; Komiyama J., 2015, ADV NEURAL INFORM PR, P1792; Komiyama J, 2015, PR MACH LEARN RES, V37, P1152; Kveton B, 2015, PR MACH LEARN RES, V37, P767; Lagree Paul, 2016, ADV NEURAL INFORM PR, P1597; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Mutapcic A, 2009, OPTIM METHOD SOFTW, V24, P381, DOI 10.1080/10556780802712889; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Vanchinathan H. P., 2014, NIPS, P1691; Yuan S., 2013, P 7 INT WORKSH DAT M, P1, DOI DOI 10.1145/2501040.2501980	33	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405008
C	Konyushkova, K; Raphael, S; Fua, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Konyushkova, Ksenia; Raphael, Sznitman; Fua, Pascal			Learning Active Learning from Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we suggest a novel data-driven approach to active learning (AL). The key idea is to train a regressor that predicts the expected error reduction for a candidate sample in a particular learning state. By formulating the query selection procedure as a regression problem we are not restricted to working with existing AL heuristics; instead, we learn strategies based on experience from previous AL outcomes. We show that a strategy can be learnt either from simple synthetic 2D datasets or from a subset of domain-specific data. Our method yields strategies that work well on real data from a wide range of domains.	[Konyushkova, Ksenia; Fua, Pascal] Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland; [Raphael, Sznitman] Univ Bern, ARTORG Ctr, Bern, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Bern	Konyushkova, K (corresponding author), Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland.	ksenia.konyushkova@epfl.ch; raphael.sznitman@artorg.unibe.ch; pascal.fua@epfl.ch	Jeong, Yongwook/N-7413-2016		European Union's Horizon 2020 Research and Innovation Programme [720270]	European Union's Horizon 2020 Research and Innovation Programme	This project has received funding from the European Union's Horizon 2020 Research and Innovation Programme under Grant Agreement No. 720270 (HBP SGA1). We would like to thank Carlos Becker and Helge Rhodin for their comments on the text, and Lucas Maystre for his discussions and attention to details.	Adam-Bourdarios C., 2015, NIPS 2014 WORKSH HIG; [Anonymous], INT C COMP VIS; Baram Y., 2004, J MACHINE LEARNING R; Chu H.-M., 2016, ARXIV160800667; Dal Pozzolo A., 2015, IEEE S SER COMP INT; Ebert S., 2012, C COMP VIS PATT REC; Everingham M., 2010, INT J COMPUTER VISIO; Gilad-Bachrach R., 2005, ADV NEURAL INF PROCE, V18; Gordillo N., 2013, MAGNETIC RESONANCE M; Hoi S. C. H., 2006, INT C MACH LEARN; Houlsby N, 2011, STAT; Hsu W.-N., 2015, AM ASS ART INT C; Huang S.-J., 2010, ADV NEURAL INFORM PR; Iglesias J., 2011, INFORM PROCESSING ME; Joshi A., 2009, C COMP VIS PATT REC; Joshi A. J., 2012, IEEE T PATTERN ANAL; Konyushkova K., 2015, INT C COMP VIS; Long J, 2015, CVPR, V3, P4; Lorena A. C., 2002, BRAZ WORKSH BIOINF; Lucchi A., 2012, EUR C COMP VIS; Luo T, 2004, INT C PATT RECOG, P478, DOI 10.1109/ICPR.2004.1334570; Maystre L., 2017, INT C MACH LEARN; Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694; Mosinska A., 2016, CONFERENCE ON COMPUT; Olsson Fredrik, 2009, LIT SURVEY ACTIVE MA; Santoro Adam, 2016, INT C MACH LEARN INT C MACH LEARN; Settles B., 2008, C EMP METH NAT LANG; Settles B., 2010, TECHNICAL REPORT; Singla A., 2016, INT C MACH LEARN; Sznitman R., 2010, IEEE T PATTERN ANAL; Tamar A., 2016, ADV NEURAL INFORM PR; Tong S., 2002, MACHINE LEARNING; Vezhnevets A., 2012, C COMP VIS PATT REC; Yang Y, 2015, INT J COMPUT VISION, V113, P113, DOI 10.1007/s11263-014-0781-x	34	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404029
C	Levine, N; Zahavy, T; Mankowitz, DJ; Tamar, A; Mannor, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Levine, Nir; Zahavy, Tom; Mankowitz, Daniel J.; Tamar, Aviv; Mannor, Shie			Shallow Updates for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ITERATION; GAME	Deep reinforcement learning (DRL) methods such as the Deep Q-Network (DQN) have achieved state-of-the-art results in a variety of challenging, high-dimensional domains. This success is mainly attributed to the power of deep neural networks to learn rich domain representations for approximating the value function or policy. Batch reinforcement learning methods with linear representations, on the other hand, are more stable and require less hyper parameter tuning. Yet, substantial feature engineering is necessary to achieve good results. In this work we propose a hybrid approach - the Least Squares Deep Q-Network (LS-DQN), which combines rich feature representations learned by a DRL algorithm with the stability of a linear least squares method. We do this by periodically re-training the last hidden layer of a DRL network with a batch least squares update. Key to our approach is a Bayesian regularization term for the least squares update, which prevents over-fitting to the more recent data. We tested LS-DQN on five Atari games and demonstrate significant improvement over vanilla DQN and Double-DQN. We also investigated the reasons for the superior performance of our method. Interestingly, we found that the performance improvement can be attributed to the large batch size used by the LS method when optimizing the last layer.	[Levine, Nir; Zahavy, Tom; Mankowitz, Daniel J.; Mannor, Shie] Technion Israel Inst Technol, Dept Elect Engn, IL-3200003 Haifa, Israel; [Tamar, Aviv] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	Technion Israel Institute of Technology; University of California System; University of California Berkeley	Levine, N (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-3200003 Haifa, Israel.	levin.nirl@gmail.com; tomzahavy@campus.technion.ac.il; danielm@tx.technion.ac.il; avivt@berkeley.edu; shie@ee.technion.ac.il	Jeong, Yongwook/N-7413-2016	Mannor, Shie/0000-0003-4439-7647	European Community's Seventh Framework Program (FP7/2007-2013) [306638]; Siemens; Viterbi Scholarship, Technion	European Community's Seventh Framework Program (FP7/2007-2013); Siemens(Siemens AG); Viterbi Scholarship, Technion	This research was supported by the European Community's Seventh Framework Program (FP7/2007-2013) under grant agreement 306638 (SUPREL). A. Tamar is supported in part by Siemens and the Viterbi Scholarship, Technion.	Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bertsekas Dimitri P, 2008, APPROXIMATE DYNAMIC; Blundell Charles, 2016, STAT, V1050, P14; Box G. E. P., 2011, BAYESIAN INFERENCE S; Crites RH, 1996, ADV NEUR IN, V8, P1017; Desai VV, 2012, OPER RES, V60, P655, DOI 10.1287/opre.1120.1044; Dinh L, 2017, PR MACH LEARN RES, V70; Donahue J, 2013, P 31 INT C MACH LEAR; Ernst D, 2005, J MACH LEARN RES, V6, P503; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Hester T., 2017, ARXIV PREPRINT ARXIV; Hinton G., 2012, NEURAL NETWORKS MACH, V264, P1; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kingma D.P, P 3 INT C LEARNING R; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kolter J. Z., 2009, P 26 ANN INT C MACH; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Liang Yitao, 2016, P 2016 INT C AUT AG; Lin Long-Ji, 1992, REINFORCEMENT LEARNI, P4; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Riedmiller M, 2005, LECT NOTES ARTIF INT, V3720, P317, DOI 10.1007/11564096_32; Scherrer B, 2015, J MACH LEARN RES, V16, P1629; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Song Z., 2016, 30 NEURIPS, V29, P4224; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tessler Chen, 2017, P NAT C ART INT AAAI; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; van Hasselt H., 2016, P NAT C ART INT AAAI; Wang Z., 2016, SER P MACHINE LEARNI, DOI DOI https://doi.org/10.1016/j.molstruc.2016.06.044; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Zahavy T, 2016, PR MACH LEARN RES, V48	35	7	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403020
C	Li, S; Fu, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Sheng; Fu, Yun			Matching on Balanced Nonlinear Representations for Treatment Effects Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PROPENSITY SCORE; CAUSAL INFERENCE; BIAS	Estimating treatment effects from observational data is challenging due to the missing counterfactuals. Matching is an effective strategy to tackle this problem. The widely used matching estimators such as nearest neighbor matching (NNM) pair the treated units with the most similar control units in terms of covariates, and then estimate treatment effects accordingly. However, the existing matching estimators have poor performance when the distributions of control and treatment groups are unbalanced. Moreover, theoretical analysis suggests that the bias of causal effect estimation would increase with the dimension of covariates. In this paper, we aim to address these problems by learning low-dimensional balanced and nonlinear representations (BNR) for observational data. In particular, we convert counterfactual prediction as a classification problem, develop a kernel learning model with domain adaptation constraint, and design a novel matching estimator. The dimension of covariates will be significantly reduced after projecting data to a low-dimensional subspace. Experiments on several synthetic and real-world datasets demonstrate the effectiveness of our approach.	[Li, Sheng] Adobe Res, San Jose, CA 95110 USA; [Fu, Yun] Northeastern Univ, Boston, MA 02115 USA	Adobe Systems Inc.; Northeastern University	Li, S (corresponding author), Adobe Res, San Jose, CA 95110 USA.	sheli@adobe.com; yunfu@ece.neu.edu	Jeong, Yongwook/N-7413-2016		NSF IIS [1651902]; ONR Young Investigator Award [N00014-14-1-0484]; U.S. Army Research Office Award [W911NF-17-1-0367]	NSF IIS(National Science Foundation (NSF)); ONR Young Investigator Award; U.S. Army Research Office Award	This research is supported in part by the NSF IIS award 1651902, ONR Young Investigator Award N00014-14-1-0484, and U.S. Army Research Office Award W911NF-17-1-0367.	Agarwal D., 2011, 14 INT C ART INT STA, P93; Athey S, 2016, P NATL ACAD SCI USA, V113, P7353, DOI 10.1073/pnas.1510489113; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Brodersen KH, 2015, ANN APPL STAT, V9, P247, DOI 10.1214/14-AOAS788; Chan D, 2010, KDD, P7, DOI DOI 10.1145/1835804.1835809; Chang YL, 2017, AAAI CONF ARTIF INTE, P1770; Diamond A, 2013, REV ECON STAT, V95, P932, DOI 10.1162/REST_a_00318; Entner D., 2013, P 16 INT C ART INT S, V31, P256; Galles D., 1998, FOUND SCI, V3, P151, DOI DOI 10.1023/A:1009602825894; Glass TA, 2013, ANNU REV PUBL HEALTH, V34, P61, DOI 10.1146/annurev-publhealth-031811-124606; He XF, 2004, ADV NEUR IN, V16, P153; Heckman JJ, 1998, REV ECON STUD, V65, P261, DOI 10.1111/1467-937X.00044; Hill DN, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1839, DOI 10.1145/2783258.2788622; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Iacus SM, 2012, POLIT ANAL, V20, P1, DOI 10.1093/pan/mpr013; Imai K, 2014, J R STAT SOC B, V76, P243, DOI 10.1111/rssb.12027; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Jin H, 2008, J AM STAT ASSOC, V103, P101, DOI 10.1198/016214507000000347; Johansson FD, 2016, PR MACH LEARN RES, V48; Jolliffe IT, 2002, ENCY STATIST BEHAV S, DOI [10.1007/0-387-22440-8_13, 10.1007/b98835]; Lee BK, 2010, STAT MED, V29, P337, DOI 10.1002/sim.3782; Li S., 2016, IJCAI, P3768; Liu QS, 2006, IEEE T NEURAL NETWOR, V17, P1081, DOI 10.1109/TNN.2006.875970; Neyman J, 1923, STAT SCI, V5, P465, DOI DOI 10.1214/SS/1177012031; Pan S.J., 2008, AAAI; PANE J. F., 2016, P 9 INT C ED DAT MIN, P207; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Peikes DN, 2008, AM STAT, V62, P222, DOI 10.1198/000313008X332016; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; RUBIN DB, 1979, J AM STAT ASSOC, V74, P318, DOI 10.2307/2286330; RUBIN DB, 1973, BIOMETRICS, V29, P159, DOI 10.2307/2529684; RUBIN DB, 1974, J EDUC PSYCHOL, V66, P688, DOI 10.1037/h0037350; Rubin DB, 2000, J AM STAT ASSOC, V95, P573, DOI 10.2307/2669400; Shalit Uri, 2016, ARXIV160603976; Silva R., 2014, ADV NEURAL INFORM PR, V27, P298; Spirtes P, 2010, J MACH LEARN RES, V11, P1643; Stuart EA, 2010, STAT SCI, V25, P1, DOI 10.1214/09-STS313; Sun W, 2015, AAAI CONF ARTIF INTE, P297; Wager S, 2015, ARXIV151004342; Wang PY, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P67, DOI 10.1145/2684822.2685294; Wang PY, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P133, DOI 10.1145/2740908.2742758; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819	46	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400089
C	Mukherjee, SS; Sarkar, P; Lin, LZ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mukherjee, Soumendu Sundar; Sarkar, Purnamrita; Lin, Lizhen			On clustering network-valued data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LIKELIHOOD	Community detection, which focuses on clustering nodes or detecting communities in (mostly) a single network, is a problem of considerable practical interest and has received a great deal of attention in the research community. While being able to cluster within a network is important, there are emerging needs to be able to cluster multiple networks. This is largely motivated by the routine collection of network data that are generated from potentially different populations. These networks may or may not have node correspondence. When node correspondence is present, we cluster networks by summarizing a network by its graphon estimate, whereas when node correspondence is not present, we propose a novel solution for clustering such networks by associating a computationally feasible feature vector to each network based on trace of powers of the adjacency matrix. We illustrate our methods using both simulated and real data sets, and theoretical justifications are provided in terms of consistency.	[Mukherjee, Soumendu Sundar] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Sarkar, Purnamrita] Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA; [Lin, Lizhen] Univ Notre Dame, Dept Appl & Computat Math & Stat, Notre Dame, IN 46556 USA	University of California System; University of California Berkeley; University of Texas System; University of Texas Austin; University of Notre Dame	Mukherjee, SS (corresponding author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	soumendu@berkeley.edu; purna.sarkar@austin.utexas.edu; lizhen.lin@nd.edu	Jeong, Yongwook/N-7413-2016		NSF-FRG [DMS-1160319]; Loeve Fellowship; NSF [IIS 1663870, DMS 1654579, DMS 1713082]; DARPA [N-66001-17-1-4041]	NSF-FRG(National Science Foundation (NSF)); Loeve Fellowship; NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank Professor Peter J. Bickel for helpful discussions. SSM was partially supported by NSF-FRG grant DMS-1160319 and a Loeve Fellowship. PS was partially supported by NSF grant DMS 1713082. LL was partially supported by NSF grants IIS 1663870, DMS 1654579 and a DARPA grant N-66001-17-1-4041.	ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3; Aliakbary S., 2015, CHAOS, V25, P177; Amini A. A., 2014, ARXIV14065647; Amini AA, 2013, ANN STAT, V41, P2097, DOI 10.1214/13-AOS1138; [Anonymous], 2017, SUPPLEMENT CLUSTERIN; Ball B, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036103; Bickel P, 2013, ANN STAT, V41, P1922, DOI 10.1214/13-AOS1124; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Chen H, 2015, ANN STAT, V43, P139, DOI 10.1214/14-AOS1269; Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Ginestet C. E., 2014, ARXIV E PRINTS; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Hoover D. N., 1979, TECHNICAL REPORT; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274; Mahadevan P, 2006, ACM SIGCOMM COMP COM, V36, P135, DOI 10.1145/1151659.1159930; Maugis Pierre-Andre G, 2017, ARXIV170505677; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Newman MEJ, 2002, PHYS REV LETT, V89, DOI 10.1103/PhysRevLett.89.208701; Olhede SC, 2014, P NATL ACAD SCI USA, V111, P14722, DOI 10.1073/pnas.1400374111; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Wolfe, 2013, ARXIV PREPRINT ARXIV; Zhang Y, 2015, ARXIV E PRINTS; Zhou S, 2004, IEEE COMMUN LETT, V8, P180, DOI 10.1109/LCOMM.2004.823426	30	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407016
C	Roulet, V; d'Aspremont, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Roulet, Vincent; d'Aspremont, Alexandre			Sharpness, Restart and Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MINIMIZATION; NONCONVEX; MINIMA	The Lojasiewicz inequality shows that sharpness bounds on the minimum of convex optimization problems hold almost generically. Sharpness directly controls the performance of restart schemes, as observed by Nemirovskii and Nesterov [1985]. The constants quantifying error bounds are of course unobservable, but we show that optimal restart strategies are robust, and searching for the best scheme only increases the complexity by a logarithmic factor compared to the optimal bound. Overall then, restart schemes generically accelerate accelerated methods.	[Roulet, Vincent] ENS, INRIA, Paris, France; [d'Aspremont, Alexandre] ENS, CNRS, Paris, France	Inria; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Roulet, V (corresponding author), ENS, INRIA, Paris, France.	vincent.roulet@inria.fr; aspremon@ens.fr	Jeong, Yongwook/N-7413-2016		chaire Economie des nouvelles donnees; fonds AXA pour la recherche; AMX fellowship	chaire Economie des nouvelles donnees; fonds AXA pour la recherche(AXA Research Fund); AMX fellowship	The authors would like to acknowledge support from the chaire Economie des nouvelles donnees with the data science joint research initiative with the fonds AXA pour la recherche, a gift from Societe Generale Cross Asset Quantitative Research and an AMX fellowship. The authors are affiliated to PSL Research University, Paris, France.	Arjevani Y, 2016, PR MACH LEARN RES, V48; Asuncion A, 2007, UCI MACHINE LEARNING; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; AUSLENDER AA, 1988, MATH OPER RES, V13, P243, DOI 10.1287/moor.13.2.243; BIERSTONE E, 1988, PUBL MATH-PARIS, P5; Bolte J., 2015, MATH PROGRAM, P1; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Burke JV, 2002, CONTROL CYBERN, V31, P439; BURKE JV, 1993, SIAM J CONTROL OPTIM, V31, P1340, DOI 10.1137/0331063; Fercoq O., 2016, ARXIV160907358; Fercoq O., 2017, ARXIV170902300; Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3; Freund R. M., 2015, ARXIV151102974; Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2; Giselsson P, 2014, IEEE DECIS CONTR P, P5058, DOI 10.1109/CDC.2014.7040179; Hoffman A. J., 1952, J RES NBS, V49; Juditski A., 2014, ARXIV14011792; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; LIN Q., 2014, ICML, V32, P73; LOJASIEWICZ S, 1993, ANN I FOURIER, V43, P1575; Lojasiewicz S., 1963, EQUATIONS DERIVEES P, P87, DOI DOI 10.1006/JDEQ.1997.3393; MANGASARIAN OL, 1985, MATH OPER RES, V10, P175, DOI 10.1287/moor.10.2.175; NEMIROVSKII AS, 1985, USSR COMP MATH MATH+, V25, P21, DOI 10.1016/0041-5553(85)90100-4; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Polyak B., 1979, IIASA WORKSH GEN LAG; Polyak B. T., 1987, INTRO OPTIMIZATION; Renegar J., 2014, ARXIV14095832; ROBINSON SM, 1975, SIAM J CONTROL, V13, P271, DOI 10.1137/0313015; Roulet V., 2015, ARXIV150603295	35	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401016
C	Sen, R; Suresh, AT; Shanmugam, K; Dimakis, AG; Shakkottai, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sen, Rajat; Suresh, Ananda Theertha; Shanmugam, Karthikeyan; Dimakis, Alexandros G.; Shakkottai, Sanjay			Model-Powered Conditional Independence Test	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SEARCH	We consider the problem of non-parametric Conditional Independence testing (CI testing) for continuous random variables. Given i.i.d samples from the joint distribution f (x, y, z) of continuous random vectors X,Y and Z, we determine whether X perpendicular to Y vertical bar Z. We approach this by converting the conditional independence test into a classification problem. This allows us to harness very powerful classifiers like gradient-boosted trees and deep neural networks. These models can handle complex probability distributions and allow us to perform significantly better compared to the prior state of the art, for high-dimensional CI testing. The main technical challenge in the classification problem is the need for samples from the conditional product distribution f(CI)(x, y, z) = f(x vertical bar z) f(y vertical bar z) f (z) - the joint distribution if and only if X perpendicular to Y vertical bar Z. - when given access only to i.i.d. samples from the true joint distribution f (x, y, z). To tackle this problem we propose a novel nearest neighbor bootstrap procedure and theoretically show that our generated samples are indeed close to f(CI) in terms of total variational distance. We then develop theoretical results regarding the generalization bounds for classification for our problem, which translate into error bounds for CI testing. We provide a novel analysis of Rademacher type classification bounds in the presence of non-i.i.d near-independent samples. We empirically validate the performance of our algorithm on simulated and real datasets and show performance gains over previous methods.	[Sen, Rajat; Dimakis, Alexandros G.; Shakkottai, Sanjay] Univ Texas Austin, Austin, TX 78712 USA; [Suresh, Ananda Theertha] Google, New York, NY USA; [Shanmugam, Karthikeyan] Thomas J Watson Ctr, IBM Res, Yorktown Hts, NY USA	University of Texas System; University of Texas Austin; Google Incorporated; International Business Machines (IBM)	Sen, R (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.		Dimakis, Alexandros G/P-6034-2019; Jeong, Yongwook/N-7413-2016; Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	NSF [SaTC 1704778, CNS 1320175]; ARO [W911NF-17-1-0359, W911NF-16-1-0377]; US DoT	NSF(National Science Foundation (NSF)); ARO; US DoT	This work is partially supported by NSF grants CNS 1320175, NSF SaTC 1704778, ARO grants W911NF-17-1-0359, W911NF-16-1-0377 and the US DoT supported D-STOP Tier 1 University Transportation Center.	[Anonymous], 1996, TECHNICAL REPORT; Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P604, DOI 10.1007/978-3-540-72927-3_43; Beygelzimer A., 2009, P 25 C UNC ART INT, P51; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Brenner Eliot, 2013, ARXIV13096820; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; de Campos LM, 2000, INT J APPROX REASON, V24, P11, DOI 10.1016/S0888-613X(99)00042-0; Doran G, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P132; Gao W., 2016, ARXIV160403006; Gao Weihao, 2016, P ADV NEUR INF PROC, P2460; Jie Cheng, 1998, LEARNING BAYESIAN NE; Kalisch M, 2007, J MACH LEARN RES, V8, P613; Koller D., 2009, PROBABILISTIC GRAPHI; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langford John, 2003, P MACH LEARN RED WOR; LopezPaz D., 2016, ARXIV161006545; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Mohri M., 2009, ADV NEURAL INFORM PR, V21, P1097; Mooij Joris, 2013, ARXIV13096849; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; RAMASUBRAMANIAN V, 1992, IEEE T SIGNAL PROCES, V40, P518, DOI 10.1109/78.120795; Roos B, 1999, J MULTIVARIATE ANAL, V69, P120, DOI 10.1006/jmva.1998.1789; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Spirtes P., 2000, CAUSATION PREDICTION; Strobl E. V., 2017, ARXIV170203877; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Vapnik VN., 2015, MEASURES COMPLEXITY, P11, DOI [10.1007/978-3-319-21852-6_3, DOI 10.1007/978-3-319-21852-6_3]; Xing E.P., 2001, ICML CITESEER, V1, P601; Zhang Kun, 2012, ARXIV12023775	32	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403002
C	Svenstrup, D; Hansen, JM; Winther, O		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Svenstrup, Dan; Hansen, Jonas Meinertz; Winther, Ole			Hash Embeddings for Efficient Word Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present hash embeddings, an efficient method for representing words in a continuous vector form. A hash embedding may be seen as an interpolation between a standard word embedding and a word embedding created using a random hash function (the hashing trick). In hash embeddings each token is represented by k d-dimensional embeddings vectors and one k dimensional weight vector. The final d dimensional representation of the token is the product of the two. Rather than fitting the embedding vectors for each token these are selected by the hashing trick from a shared pool of B embedding vectors. Our experiments show that hash embeddings can easily deal with huge vocabularies consisting of millions of tokens. When using a hash embedding there is no need to create a dictionary before training nor to perform any kind of vocabulary pruning after training. We show that models trained using hash embeddings exhibit at least the same level of performance as models trained using regular embeddings across a wide range of tasks. Furthermore, the number of parameters needed by such an embedding is only a fraction of what is required by a regular embedding. Since standard embeddings and embeddings constructed using the hashing trick are actually just special cases of a hash embedding, hash embeddings can be considered an extension and improvement over the existing regular embedding types.	[Svenstrup, Dan; Winther, Ole] Tech Univ Denmark DTU, Dept Appl Math & Comp Sci, DK-2800 Lyngby, Denmark; [Hansen, Jonas Meinertz] FindZebra, Copenhagen, Denmark	Technical University of Denmark	Svenstrup, D (corresponding author), Tech Univ Denmark DTU, Dept Appl Math & Comp Sci, DK-2800 Lyngby, Denmark.	dsve@dtu.dk; jonas@findzebra.com; olwi@dtu.dk	Jeong, Yongwook/N-7413-2016	Winther, Ole/0000-0002-1966-3205				Argerich L., 2016, ABS160808940 CORR; Bai B., 2009, P CIKM 2009, P187; Conneau A., 2016, ABS160601781 CORR; Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541; Huang Eric, 2012, P 50 ANN M ASS COMP, V1, P873; Huang PS, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P2333; Ioffe S., 2015, ICML, P448; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johansen A. R., 2016, ABS161006550 CORR; Johnson R., 2016, ABS160900718 CORR; Joulin A., 2016, ABS161203651 CORR; Joulin A., 2016, ABS160701759 CORR; Kingma Diederik P., 2014, ABS14126980 CORR; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Manning C. D, 1999, FDN STAT NATURAL LAN, V999; Mihaltz M., 2016, GOOGLES TRAINED WORD; Miyato T, 2016, STATISTICS, V1050, P25; Reisinger J., 2010, HUMAN LANGUAGE TECHN, P109; Stolcke A., 2000, CS0006025 ARXIV; Weinberger K. Q., 2009, ABS09022206 CORR; Xiao Y., 2016, ABS160200367 CORR; Yogatama Dani, 2017, ARXIV170301898; Zhang Xiang, 2015, ABS150901626 CORR	23	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405001
C	Takeishi, N; Kawahara, Y; Yairi, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Takeishi, Naoya; Kawahara, Yoshinobu; Yairi, Takehisa			Learning Koopman Invariant Subspaces for Dynamic Mode Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SPECTRAL PROPERTIES; SYSTEMS; PATTERNS	Spectral decomposition of the Koopman operator is attracting attention as a tool for the analysis of nonlinear dynamical systems. Dynamic mode decomposition is a popular numerical algorithm for Koopman spectral analysis; however, we often need to prepare nonlinear observables manually according to the underlying dynamics, which is not always possible since we may not have any a priori knowledge about them. In this paper, we propose a fully data-driven method for Koopman spectral analysis based on the principle of learning Koopman invariant subspaces from observed data. To this end, we propose minimization of the residual sum of squares of linear least-squares regression to estimate a set of functions that transforms data into a form in which the linear regression fits well. We introduce an implementation with neural networks and evaluate performance empirically using nonlinear dynamical systems and applications.	[Takeishi, Naoya; Yairi, Takehisa] Univ Tokyo, Dept Aeronaut & Astronaut, Tokyo, Japan; [Kawahara, Yoshinobu] Osaka Univ, Inst Sci & Ind Res, Suita, Osaka, Japan; [Kawahara, Yoshinobu] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan	University of Tokyo; Osaka University; RIKEN	Takeishi, N (corresponding author), Univ Tokyo, Dept Aeronaut & Astronaut, Tokyo, Japan.	takeishi@ailab.t.u-tokyo.ac.jp; ykawahara@sanken.osaka-u.ac.jp; yairi@ailab.t.u-tokyo.ac.jp	KAWAHARA, Yoshinobu/AAM-7540-2020; Kawahara, Yoshinobu/J-2462-2014; Takeishi, Naoya/AAX-6263-2021; Jeong, Yongwook/N-7413-2016	KAWAHARA, Yoshinobu/0000-0001-7789-4709; Kawahara, Yoshinobu/0000-0001-7789-4709; Takeishi, Naoya/0000-0003-0111-2269; 	JSPS KAKENHI [JP15J09172, JP26280086, JP16H01548, JP26289320]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JSPS KAKENHI Grant No. JP15J09172, JP26280086, JP16H01548, and JP26289320.	Arbabi H, 2017, SIAM J APPL DYN SYST, V16, P2096, DOI 10.1137/17M1125236; Berger E, 2015, ADV ROBOTICS, V29, P331, DOI 10.1080/01691864.2014.981292; Brunton BW, 2016, J NEUROSCI METH, V258, P1, DOI 10.1016/j.jneumeth.2015.10.010; Brunton SL, 2016, P NATL ACAD SCI USA, V113, P3932, DOI 10.1073/pnas.1517384113; Brunton SL, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150171; Budisic M, 2012, CHAOS, V22, DOI 10.1063/1.4772195; Canu S, 2006, NEUROCOMPUTING, V69, P714, DOI 10.1016/j.neucom.2005.12.009; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Froyland G, 2014, SIAM J APPL DYN SYST, V13, P1816, DOI 10.1137/130943637; Gao YJ, 2016, ADV NEUR IN, V29; Garcia SP, 2005, PHYS REV E, V72, DOI 10.1103/PhysRevE.72.027205; Ghahramani Z, 1999, ADV NEUR IN, V11, P431; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hirata Y, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.026202; Hirsch MW, 2013, DIFFERENTIAL EQUATIONS, DYNAMICAL SYSTEMS, AND AN INTRODUCTION TO CHAOS, 3RD EDITION, P1; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kar P., 2014, ADV NEURAL INFORM PR, P694; Karl M., 2017, P 5 INT C LEARN REPR; Kawahara Y., 2016, ADV NEURAL INFORM PR, V29, P911; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Koopman BO, 1931, P NATL ACAD SCI USA, V17, P315, DOI 10.1073/pnas.17.5.315; Krishnan RG, 2017, AAAI CONF ARTIF INTE, P2101; Kutz JN, 2016, OTHER TITL APPL MATH, V149; Kutz J.N., 2016, ARXIV160207647; Kutz JN, 2016, SIAM J APPL DYN SYST, V15, P713, DOI 10.1137/15M1023543; Lasota A, 1994, CHAOS FRACTALS NOISE; Li QX, 2017, CHAOS, V27, DOI 10.1063/1.4993854; Liu S, 2013, NEURAL NETWORKS, V43, P72, DOI 10.1016/j.neunet.2013.01.012; LORENZ EN, 1963, J ATMOS SCI, V20, P130, DOI 10.1175/1520-0469(1963)020<0130:DNF>2.0.CO;2; Lusch B., 2017, ARXIV171209707; Mardt A., 2017, ARXIV171006012; Mezic I, 2005, NONLINEAR DYNAM, V41, P309, DOI 10.1007/s11071-005-2824-x; Mezic I, 2013, ANNU REV FLUID MECH, V45, P357, DOI 10.1146/annurev-fluid-011212-140652; Otto S.E., 2017, ARXIV171201378; Proctor JL, 2016, SIAM J APPL DYN SYST, V15, P142, DOI 10.1137/15M1013857; Proctor JL, 2015, INT HEALTH, V7, P139, DOI 10.1093/inthealth/ihv009; Rakocevic V., 1997, MAT VESN, V49, P163; ROSSLER OE, 1976, PHYS LETT A, V57, P397, DOI 10.1016/0375-9601(76)90101-8; Rowley CW, 2009, J FLUID MECH, V641, P115, DOI 10.1017/S0022112009992059; SAUER T, 1991, J STAT PHYS, V65, P579, DOI 10.1007/BF01053745; Schmid PJ, 2010, J FLUID MECH, V656, P5, DOI 10.1017/S0022112010001217; Susuki Y, 2015, IEEE DECIS CONTR P, P7022, DOI 10.1109/CDC.2015.7403326; Takeishi N, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.033310; Takens F., 1981, DYNAMICAL SYSTEMS TU, P366, DOI [DOI 10.1007/BFB0091924, 10.1007/bfb0091924, 10.1007/BFb0091924]; Tu J.H., 2014, J COMPUT DYN, V1, P391, DOI [10.3934/jcd.2014.1.391, DOI 10.3934/JCD.2014.1.391]; Vlachos I, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.016207; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; Weigend A. S., 1993, TIME SERIES PREDICTI; Williams MO, 2015, J NONLINEAR SCI, V25, P1307, DOI 10.1007/s00332-015-9258-5; Yeung E., 2017, ARXIV170806850	52	7	7	6	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401017
C	Yu, H; Neely, MJ; Wei, XH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yu, Hao; Neely, Michael J.; Wei, Xiaohan			Online Convex Optimization with Stochastic Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRADIENT; REGRET; BOUNDS; TIME	This paper considers online convex optimization (OCO) with stochastic constraints, which generalizes Zinkevich's OCO over a known simple fixed set by introducing multiple stochastic functional constraints that are i.i.d. generated at each round and are disclosed to the decision maker only after the decision is made. This formulation arises naturally when decisions are restricted by stochastic environments or deterministic environments with noisy observations. It also includes many important problems as special case, such as OCO with long term constraints, stochastic constrained convex optimization, and deterministic constrained convex optimization. To solve this problem, this paper proposes a new algorithm that achieves O(root T) expected regret and constraint violations and O(root T log(T)) high probability regret and constraint violations. Experiments on a real-world data center scheduling problem further verify the performance of the new algorithm.	[Yu, Hao; Neely, Michael J.; Wei, Xiaohan] Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA	University of Southern California	Yu, H (corresponding author), Univ Southern Calif, Dept Elect Engn, Los Angeles, CA 90007 USA.	yuhao@usc.edu; mjneely@usc.edu; xiaohanw@usc.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1718477]	NSF(National Science Foundation (NSF))	This work is supported in part by grant NSF CCF-1718477.	Bartlett Peter L, 2008, P C LEARN THEOR COLT; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; CesaBianchi N, 1996, IEEE T NEURAL NETWOR, V7, P604, DOI 10.1109/72.501719; Cotter Andrew, 2015, P C LEARN THEOR COLT; Durrett R., 2010, CAMBRIDGE SERIES STA, V4th; Gandhi A, 2012, 2012 INTERNATIONAL GREEN COMPUTING CONFERENCE (IGCC), DOI 10.1109/igcc.2012.6322260; Gordon Geoffrey J, 1999, P C LEARN THEOR COLT; Hao Yu, 2016, ARXIV160402218; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Jenatton R., 2016, P INT C MACH LEARN I; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Lan G., 2016, ARXIV160403887; Mahdavi M, 2012, J MACH LEARN RES, V13, P2503; Mahdavi Mehrdad, 2013, ADV NEURAL INFORM PR; Mannor S, 2009, J MACH LEARN RES, V10, P569; Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7; Nesterov Y., 2018, APPL OPTIMIZATION; Qureshi Asfandyar, 2009, ACM SIGCOMM; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Tao T, 2015, ANN PROBAB, V43, P782, DOI 10.1214/13-AOP876; Tse D., 2005, FUNDAMENTALS WIRELES, DOI [10.1017/CBO9780511807213, DOI 10.1017/CBO9780511807213]; Vu VH, 2002, RANDOM STRUCT ALGOR, V20, P262, DOI 10.1002/rsa.10032; Yu H, 2017, SIAM J OPTIMIZ, V27, P759, DOI 10.1137/16M1059011; Zinkevich Martin, 2003, P INT C MACH LEARN I	28	7	7	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401045
C	Zafar, MB; Valera, I; Rodriguez, MG; Gummadi, KP; Weller, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zafar, Muhammad Bilal; Valera, Isabel; Rodriguez, Manuel Gomez; Gummadi, Krishna P.; Weller, Adrian			From Parity to Preference-based Notions of Fairness in Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The adoption of automated, data-driven decision making in an ever expanding range of applications has raised concerns about its potential unfairness towards certain social groups. In this context, a number of recent studies have focused on defining, detecting, and removing unfairness from data-driven decision systems. However, the existing notions of fairness, based on parity (equality) in treatment or outcomes for different social groups, tend to be quite stringent, limiting the overall decision making accuracy. In this paper, we draw inspiration from the fair-division and envy-freeness literature in economics and game theory and propose preference-based notions of fairness-given the choice between various sets of decision treatments or outcomes, any group of users would collectively prefer its treatment or outcomes, regardless of the (dis)parity as compared to the other groups. Then, we introduce tractable proxies to design margin-based classifiers that satisfy these preference-based notions of fairness. Finally, we experiment with a variety of synthetic and real-world datasets and show that preference-based fairness allows for greater decision accuracy than parity-based fairness.	[Zafar, Muhammad Bilal; Rodriguez, Manuel Gomez; Gummadi, Krishna P.] MPI SWS, Saarbrucken, Germany; [Valera, Isabel] MPI IS, Stuttgart, Germany; [Weller, Adrian] Univ Cambridge, Cambridge, England; [Weller, Adrian] Alan Turing Inst, London, England	University of Cambridge	Zafar, MB (corresponding author), MPI SWS, Saarbrucken, Germany.	mzafar@mpi-sws.org; isabel.valera@tue.mpg.de; manuelgr@mpi-sws.org; gummadi@mpi-sws.org; aw665@cam.ac.uk	Rodriguez, Manuel Gomez/AAB-5005-2021; Jeong, Yongwook/N-7413-2016		Alan Turing Institute under EPSRC grant [EP/N510129/1]; Leverhulme Trust via the CFI	Alan Turing Institute under EPSRC grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Leverhulme Trust via the CFI	AW acknowledges support by the Alan Turing Institute under EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI.	Altman A., 2016, STANFORD ENCY PHILOS; [Anonymous], 1996, ADULT DATA; [Anonymous], 2017, STOP QUESTION FRISK; Barocas S., 2016, CALIFORNIA LAW REV; Berliant M., 1992, J MATH EC; Bishop C.M, 2006, PATTERN RECOGN; Calders T., 2010, DATA MINING KNOWLEDG; Chapelle O., 2007, NEURAL COMPUTATION; Chouldechova A, 2016, ARXIV161007524; Corbett-Davies S., 2017, KDD; Duan JC, 2012, SPR HBK COMPU STAT, P1, DOI 10.1007/978-3-642-17254-0; Dwork C., 2012, ITCSC; Dwork Cynthia, 2017, ARXIV170706613; Feldman M., 2015, KDD; Friedler S.A., 2016, IM POSSIBILITY FAIRN; Goel R. S. Sharad, 2015, ANN APPL STAT; Goh G., 2016, NIPS; Hardt M., 2016, NIPS; Joseph M., 2016, NIPS; Kamiran F., 2010, BENELEARN; Kamishima T., 2011, PADM; Kleinberg Jon M., 2017, ITCS; Luong B. T., 2011, KDD; Megan D. P. C., 2016, BIG DATA REPORT ALGO; Nash Jr J. F, 1950, ECONOMETRICA J ECONO; NYCLU, 2017, STOP AND FRISK DAT; Pedreschi D., 2008, KDD; Shen X., 2016, ARXIV160402639; Varian H. R., 1974, J EC THEORY; Zafar M., 2017, WWW; Zafar Muhammad Bilal, 2017, AISTATS; Zemel Rich, 2013, ICML	32	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400022
C	Boumal, N; Voroninski, V; Bandeira, AS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Boumal, Nicolas; Voroninski, Vladislav; Bandeira, Afonso S.			The non-convex Burer-Monteiro approach works on smooth semidefinite programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				RANK; OPTIMIZATION; RECOVERY; CUT	Semidefinite programs (SDPs) can be solved in polynomial time by interior point methods, but scalability can be an issue. To address this shortcoming, over a decade ago, Burer and Monteiro proposed to solve SDPs with few equality constraints via rank-restricted, non-convex surrogates. Remarkably, for some applications, local optimization methods seem to converge to global optima of these non-convex surrogates reliably. Although some theory supports this empirical success, a complete explanation of it remains an open question. In this paper, we consider a class of SDPs which includes applications such as max-cut, community detection in the stochastic block model, robust PCA, phase retrieval and synchronization of rotations. We show that the low-rank Burer-Monteiro formulation of SDPs in that class almost never has any spurious local optima.	[Boumal, Nicolas] Princeton Univ, Dept Math, Princeton, NJ 08544 USA; [Voroninski, Vladislav] MIT, Dept Math, Cambridge, MA 02139 USA; [Bandeira, Afonso S.] NYU, Dept Math, New York, NY 10003 USA; [Bandeira, Afonso S.] NYU, Courant Inst Math Sci, Ctr Data Sci, New York, NY 10003 USA	Princeton University; Massachusetts Institute of Technology (MIT); New York University; New York University	Boumal, N (corresponding author), Princeton Univ, Dept Math, Princeton, NJ 08544 USA.	nboumal@math.princeton.edu; vvlad@math.mit.edu; bandeira@cims.nyu.edu			Office of Naval Research; NSF [DMS-1317308]	Office of Naval Research(Office of Naval Research); NSF(National Science Foundation (NSF))	VV was partially supported by the Office of Naval Research. ASB was supported by NSF Grant DMS-1317308. Part of this work was done while ASB was with the Department of Mathematics at the Massachusetts Institute of Technology. We thank Wotao Yin and Michel Goemans for helpful discussions.	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Absil PA, 2007, FOUND COMPUT MATH, V7, P303, DOI 10.1007/s10208-005-0179-9; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Alon N, 2006, SIAM J COMPUT, V35, P787, DOI 10.1137/S0097539704441629; Andreani R, 2010, J OPTIMIZ THEORY APP, V146, P255, DOI 10.1007/s10957-010-9671-8; [Anonymous], 2016, ADV NEURAL INFORM PR; Bandeira A., 2016, P 29 C LEARN THEOR C; Bandeira AS, 2017, MATH PROGRAM, V163, P145, DOI 10.1007/s10107-016-1059-6; Bandeira AS, 2016, MATH PROGRAM, V160, P433, DOI 10.1007/s10107-016-0993-7; BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Bhojanapalli S., 2016, C LEARN THEOR COLT; Boumal N., 2015, ARXIV150600575; Boumal N., 2016, ARXIV160508101; Boumal N, 2014, J MACH LEARN RES, V15, P1455; CVX, 2012, CVX MATLAB SOFTWARE; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Helmberg C, 1996, SIAM J OPTIMIZ, V6, P342, DOI 10.1137/0806020; Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Laurent M, 1996, SIAM J MATRIX ANAL A, V17, P530, DOI 10.1137/0617031; Lee J. M., 2012, GRADUATE TEXTS MATH; McCoy M, 2011, ELECTRON J STAT, V5, P1123, DOI 10.1214/11-EJS636; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; Rockafellar R. T., 1970, CONVEX ANAL; Ruszczynski A. P., 2006, NONLINEAR OPTIMIZATI; Shah S., 2016, ARXIV160509527; Singer A, 2011, APPL COMPUT HARMON A, V30, P20, DOI 10.1016/j.acha.2010.02.001; Toh KC, 1999, OPTIM METHOD SOFTW, V11-2, P545, DOI 10.1080/10556789908805762; VAVASIS S., 1991, NONLINEAR OPTIMIZATI; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wen ZW, 2013, MATH PROGRAM, V142, P397, DOI 10.1007/s10107-012-0584-1; Yang WH, 2014, PAC J OPTIM, V10, P415	37	7	7	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704042
C	Chen, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Bryant			Identification and Overidentification of Linear Structural Equation Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we address the problems of identifying linear structural equation models and discovering the constraints they imply. We first extend the half-trek criterion to cover a broader class of models and apply our extension to finding testable constraints implied by the model. We then show that any semi-Markovian linear model can be recursively decomposed into simpler sub-models, resulting in improved identification and constraint discovery power. Finally, we show that, unlike the existing methods developed for linear models, the resulting method subsumes the identification and constraint discovery algorithms for non-parametric models.	[Chen, Bryant] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Chen, B (corresponding author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.				NSF [IIS-1302448, IIS-1527490]; ONR [N00014-13-1-0153]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	I would like to thank Jin Tian and Judea Pearl for helpful comments and discussions. This research was supported in parts by grants from NSF #IIS-1302448 and #IIS-1527490 and ONR #N00014-13-1-0153 and #N00014-13-1-0153.	BALKE A, 1994, PROCEEDINGS OF THE TWELFTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P230; Bonissone P., 1991, P 6 C UNC ART INT, P255; Brito C., 2002, P 18 C UNC ART INT, P85; Brito C., 2004, THESIS; Chen B, 2014, R432 U CAL DEP COMP; Chen B, 2014, AAAI CONF ARTIF INTE, P2424; Drton M, 2016, SCAND J STAT, V43, P1035, DOI 10.1111/sjos.12227; Foygel R, 2012, ANN STAT, V40, P1682, DOI 10.1214/12-AOS1012; Huang Y., 2006, P 22 C UNC ART INT, P217, DOI [10.5555/3020419.3020446, DOI 10.5555/3020419.3020446]; Kang CS, 2009, J MACH LEARN RES, V10, P41; Pearl J., 2008, P 23 AAAI C ART INT, V2, P1081; Pearl J., 2009, CAUSALITY MODELS REA; Pearl Judea, 2004, P 20 C UNCERTAINTY A, P446; Shpitser I, 2006, P 22 C UNCERTAINTY A, P437, DOI [10.48550/ARXIV.1206.6876, DOI 10.48550/ARXIV.1206.6876]; Tian J, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P567; TIAN J, 2005, P NAT C ART INT, V20; TIAN J, 2009, P 21 INT JOINT C ART; TIAN J., 2007, P 23 C ANN C UNC ART; Tian J., 2002, THESIS; Tian J., 2002, P 18 C UNC ART INT, P519; Verma T., 1991, P 6 UAI C UAI 90, P255; Wright S, 1920, J AGRIC RES, V20, P0557	22	7	7	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701080
C	Chen, CY; Ding, N; Li, CY; Zhang, YZ; Carin, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Changyou; Ding, Nan; Li, Chunyuan; Zhang, Yizhe; Carin, Lawrence			Stochastic Gradient MCMC with Stale Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Stochastic gradient MCMC (SG-MCMC) has played an important role in large-scale Bayesian learning, with well-developed theoretical convergence properties. In such applications of SG-MCMC, it is becoming increasingly popular to employ distributed systems, where stochastic gradients are computed based on some outdated parameters, yielding what are termed stale gradients. While stale gradients could be directly used in SG-MCMC, their impact on convergence properties has not been well studied. In this paper we develop theory to show that while the bias and MSE of an SG-MCMC algorithm depend on the staleness of stochastic gradients, its estimation variance (relative to the expected estimate, based on a prescribed number of samples) is independent of it. In a simple Bayesian distributed system with SG-MCMC, where stale gradients are computed asynchronously by a set of workers, our theory indicates a linear speedup on the decrease of estimation variance w.r.t. the number of workers. Experiments on synthetic data and deep neural networks validate our theory, demonstrating the effectiveness and scalability of SG-MCMC with stale gradients.	[Chen, Changyou; Li, Chunyuan; Zhang, Yizhe; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA; [Ding, Nan] Google Inc, Venice, CA USA	Duke University; Google Incorporated	Chen, CY (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	cc448@duke.edu; dingnan@google.com; cl319@duke.edu; yz196@duke.edu; lcarin@duke.edu	Li, Chunyuan/AAG-1303-2020		ARO; DARPA; DOE; NGA; ONR; NSF	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	Supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.	Abadi M, 2015, P 12 USENIX S OPERAT; Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Ahn S., 2014, ICML; Ahn S, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P9, DOI 10.1145/2783258.2783373; [Anonymous], 2012, H ACM INT C WEB SEAR, DOI DOI 10.1145/2124295.2124337; Bardenet R., 2015, ARXIV150502827; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bottou L., 2012, TECHNICAL REPORT; Bottou L., 1998, ONLINE ALGORITHMS ST; Chaturapruek S., 2015, NIPS; Chen C., 2015, NIPS; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Feyzmahdavian H. R., 2015, ARXIV150504824; Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223; Jia T., 2014, ARXIV14085093; Krizhevshy A., 2012, NIPS; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; Ma Y. A., 2015, NIPS; Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527; Neiswanger W., 2014, UAI; Neven, 2014, ADV NEURAL INFORM PR, P3203; Patterson S., 2013, P 26 INT C NEUR INF, P3102; Rabinovich M., 2015, NIPS; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Scott S. L., 2013, BAYES, V250; Simsekli U., 2015, TECHNICAL REPORT; Teh Y. W., 2015, ARXIV151209327V1; Vollmer S. J., 2015, ARXIV150100438; Wang X., 2015, NIPS; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	35	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702106
C	Herlau, T; Schmidt, MN; Morup, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Herlau, Tue; Schmidt, Mikkel N.; Morup, Morten			Completely random measures for modelling block-structured sparse networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Statistical methods for network data often parameterize the edge-probability by attributing latent traits such as block structure to the vertices and assume exchangeability in the sense of the Aldous-Hoover representation theorem. These assumptions are however incompatible with traits found in real-world networks such as a power-law degree-distribution. Recently, Caron & Fox (2014) proposed the use of a different notion of exchangeability after Kallenberg (2005) and obtained a network model which permits edge-inhomogeneity, such as a power-law degree-distribution whilst retaining desirable statistical properties. However, this model does not capture latent vertex traits such as block-structure. In this work we re-introduce the use of block-structure for network models obeying Kallenberg's notion of exchangeability and thereby obtain a collapsed model which both admits the inference of block-structure and edge inhomogeneity. We derive a simple expression for the likelihood and an efficient sampling method. The obtained model is not significantly more difficult to implement than existing approaches to block-modelling and performs well on real network datasets.	[Herlau, Tue; Schmidt, Mikkel N.; Morup, Morten] Tech Univ Denmark, DTU Compute, Richard Petersens Plads 31, DK-2800 Lyngby, Denmark	Technical University of Denmark	Herlau, T (corresponding author), Tech Univ Denmark, DTU Compute, Richard Petersens Plads 31, DK-2800 Lyngby, Denmark.	tuhe@dtu.dk; mns@dtu.dk; mmor@dtu.dk		Schmidt, Mikkel N./0000-0001-6927-8869	Lundbeck Foundation [R105-9813]	Lundbeck Foundation(Lundbeckfonden)	This project was funded by the Lundbeck Foundation (grant nr. R105-9813).	ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3; [Anonymous], 2002, MATH0205093 ARXIV; [Anonymous], 2015, CLASS RANDOM GRAPHS; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Caron F., 2014, ARXIV14011137; Chen C., 2013, INT C MACH LEARN, P969; Devroye L, 2014, STAT METHOD APPL-GER, V23, P307, DOI 10.1007/s10260-014-0260-0; Herlau T, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.032819; Hoover D., 1979, RELATIONS PROBABILIT; HOUGAARD P, 1986, BIOMETRIKA, V73, P387, DOI 10.1093/biomet/73.2.387; Kallenberg Olaf, 2005, APPL PROBABILITY, V10; Kemp C., 2006, AAAI, V3, P5; KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59; Newman ME, 2001, PHYS REV E, V64; Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607; Pitman J, 2003, INST MATH S, V40, P1; Pitman J, 2006, COMBINATORIAL STOCHA; Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725; Teh Yee Whye, 2014, ARXIV14074211; WHITE HC, 1976, AM J SOCIOL, V81, P730, DOI 10.1086/226141; Xu Z., 2006, P 22 INT C UNC ART I; Zolotarev V. M., 1964, T MATEMATICHESKOGO I, V71, P46	22	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704046
C	Tan, CH; Ma, SQ; Dai, YH; Qian, YQ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tan, Conghui; Ma, Shiqian; Dai, Yu-Hong; Qian, Yuqiu			Barzilai-Borwein Step Size for Stochastic Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					One of the major issues in stochastic gradient descent (SGD) methods is how to choose an appropriate step size while running the algorithm. Since the traditional line search technique does not apply for stochastic optimization methods, the common practice in SGD is either to use a diminishing step size, or to tune a step size by hand, which can be time consuming in practice. In this paper, we propose to use the Barzilai-Borwein (BB) method to automatically compute step sizes for SGD and its variant: stochastic variance reduced gradient (SVRG) method, which leads to two algorithms: SGD-BB and SVRG-BB. We prove that SVRG-BB converges linearly for strongly convex objective functions. As a by-product, we prove the linear convergence result of SVRG with Option I proposed in [10], whose convergence result has been missing in the literature. Numerical experiments on standard data sets show that the performance of SGD-BB and SVRG-BB is comparable to and sometimes even better than SGD and SVRG with best-tuned step sizes, and is superior to some advanced SGD variants.	[Tan, Conghui; Ma, Shiqian] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Dai, Yu-Hong] Chinese Acad Sci, Beijing, Peoples R China; [Qian, Yuqiu] Univ Hong Kong, Hong Kong, Peoples R China	Chinese University of Hong Kong; Chinese Academy of Sciences; University of Hong Kong	Tan, CH (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	chtan@se.cuhk.edu.hk; sqma@se.cuhk.edu.hk; dyh@lsec.cc.ac.cn; qyq790@cannect.hku.hk			Hong Kong Research Grants Council General Research Fund [14205314]; Chinese NSF [11631013, 11331012]; National 973 Program of China [2015CB856000]	Hong Kong Research Grants Council General Research Fund(Hong Kong Research Grants Council); Chinese NSF(National Natural Science Foundation of China (NSFC)); National 973 Program of China(National Basic Research Program of China)	Research of Shiqian Ma was supported in part by the Hong Kong Research Grants Council General Research Fund (Grant 14205314). Research of Yu-Hong Dai was supported by the Chinese NSF (Nos. 11631013 and 11331012) and the National 973 Program of China (No. 2015CB856000).	[Anonymous], P INT C NEUR INF PRO; BARZILAI J, 1988, IMA J NUMER ANAL, V8, P141, DOI 10.1093/imanum/8.1.141; Dai YH, 2005, NUMER MATH, V100, P21, DOI 10.1007/s00211-004-0569-y; Dai YH, 2002, IMA J NUMER ANAL, V22, P1, DOI 10.1093/imanum/22.1.1; Dai YH, 2006, IMA J NUMER ANAL, V26, P604, DOI [10.1093/imanum/drl006, 10.1093/imanum/dri006]; Dai YH, 2013, J OPER RES SOC CHINA, V1, P187, DOI 10.1007/s40305-013-0007-x; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fletcher R, 2005, APPL OPTIM, V96, P235; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; KESTEN H, 1958, ANN MATH STAT, V29, P41, DOI 10.1214/aoms/1177706705; Mahsereci M., 2015, ARXIV150202846; Masse P.-Y., 2015, ARXIV151102540; RAYDAN M, 1993, IMA J NUMER ANAL, V13, P321, DOI 10.1093/imanum/13.3.321; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Sopyla K, 2015, INFORM SCIENCES, V316, P218, DOI 10.1016/j.ins.2015.03.073; Wang YF, 2007, INVERSE PROBL SCI EN, V15, P559, DOI 10.1080/17415970600881897; Wen ZW, 2010, SIAM J SCI COMPUT, V32, P1832, DOI 10.1137/090747695; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892	21	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702033
C	Wang, YC; Du, N; Trivedi, R; Song, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Yichen; Du, Nan; Trivedi, Rakshit; Song, Le			Coevolutionary Latent Feature Processes for Continuous-Time User-Item Interactions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Matching users to the right items at the right time is a fundamental task in recommendation systems. As users interact with different items over time, users' and items' feature may evolve and co-evolve over time. Traditional models based on static latent features or discretizing time into epochs can become ineffective for capturing the fine-grained temporal dynamics in the user-item interactions. We propose a coevolutionary latent feature process model that accurately captures the coevolving nature of users' and items' feature. To learn parameters, we design an efficient convex optimization algorithm with a novel low rank space sharing constraints. Extensive experiments on diverse real-world datasets demonstrate significant improvements in user behavior prediction compared to state-of-the-arts.	[Du, Nan] Google Res, Mountain View, CA USA; [Wang, Yichen; Trivedi, Rakshit; Song, Le] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	Google Incorporated; University System of Georgia; Georgia Institute of Technology	Wang, YC (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	yichen.wang@gatech.edu; dunan@google.com; rstrivedi@gatech.edu; lsong@cc.gatech.edu			NSF/NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSF [IIS-1218749, CAREER IIS-1350983]	NSF/NIH BIGDATA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This project was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, and NSF CAREER IIS-1350983.	Aalen OO, 2008, STAT BIOL HEALTH, P1; Agarwal D., 2009, KDD; [Anonymous], IJCAI; Baltrunas L., 2009, TIME DEPENDANT RECOM; Bhargava J. Z. J. L. Preeti, 2015, WWW; Charlin Laurent, 2015, RECSYS; Chen Y., 2009, KDD; Cox D., 2006, STAT METHODS APPL, V1, P159; Cullum J.K., 2002, LANCZOS ALGORITHMS L, V41; Du N., 2015, NIPS; Ekstrand Michael D., 2010, Foundations and Trends in Human-Computer Interaction, V4, P81, DOI 10.1561/1100000009; Farajtabar M., 2015, NIPS; Gopalan Prem, 2015, UAI; Gultekin S, 2014, IEEE DATA MINING, P140, DOI 10.1109/ICDM.2014.61; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Hidasi B., 2015, DATA MIN KNOWL DISC, V30, P1; Kapoor K., 2015, WSDM; Karatzoglou A., 2010, RECSYS; Koren Yehuda, 2009, KDD; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Salakhutdinov R., 2008, ICML; Sastry S., 1990, HONORS PROJECTS; Wang X., 2016, AAAI; Wang Y. X., 2016, ICML; Wang Yichen, 2015, KDD; Wang Yichen, 2016, ARXIV160309021; Xiong L., 2010, SDM; Yang S.-H., 2011, WWW; Yi X., 2014, RECSYS	30	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704005
C	Yi, XY; Park, D; Chen, YD; Caramanis, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yi, Xinyang; Park, Dohyung; Chen, Yudong; Caramanis, Constantine			Fast Algorithms for Robust PCA via Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				LOW-RANK; INCOHERENCE	We consider the problem of Robust PCA in the fully and partially observed settings. Without corruptions, this is the well-known matrix completion problem. From a statistical standpoint this problem has been recently well-studied, and conditions on when recovery is possible (how many observations do we need, how many corruptions can we tolerate) via polynomial-time algorithms is by now understood. This paper presents and analyzes a non-convex optimization approach that greatly reduces the computational complexity of the above problems, compared to the best available algorithms. In particular, in the fully observed case, with r denoting rank and d dimension, we reduce the complexity from O(r(2)d(2) log(1/kappa)) to O(rd(2) log(1/epsilon)) - a big savings when the rank is big. For the partially observed case, we show the complexity of our algorithm is no more than O(r(4)d log dlog(1/epsilon)). Not only is this the best-known run-time for a provable algorithm under partial observation, but in the setting where r is small compared to d, it also allows for near-linear-in-d run-time that can be exploited in the fully-observed case as well, by simply running our algorithm on a subset of the observations.	[Yi, Xinyang; Park, Dohyung; Caramanis, Constantine] Univ Texas Austin, Austin, TX 78712 USA; [Chen, Yudong] Cornell Univ, Ithaca, NY 14853 USA	University of Texas System; University of Texas Austin; Cornell University	Yi, XY (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	yixy@utexas.edu; dhpark@utexas.edu; yudong.chen@cornell.edu; constantine@utexas.edu						Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], 2013, ARXIV10095055V3; [Anonymous], 2015, ARXIV150903025; Balakrishnan Sivaraman, 2014, ARXIV14082156; Bhojanapalli S., 2015, ARXIV150903917; Bhojanapalli S., 2015, P 2015 ANN ACM SIAM, P902, DOI DOI 10.1137/1.9781611973730.62; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen YD, 2015, IEEE T INFORM THEORY, V61, P2909, DOI 10.1109/TIT.2015.2415195; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Netrapalli P., 2014, P ADV NEUR INF PROC, P1107; Sun J., 2015, ARXIV151006096; Sun RY, 2015, ANN IEEE SYMP FOUND, P270, DOI 10.1109/FOCS.2015.25; Tu S., 2015, ARXIV150703566; Wang Zhaoran, 2015, Adv Neural Inf Process Syst, V28, P2512; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Yi X., 2015, ADV NEURAL INFORM PR, P1567; Zhang H., 2016, ARXIV160303805; Zheng Q., 2015, ADV NEURAL INFORM PR, P109	30	7	7	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703081
C	Brown, N; Sandholm, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Brown, Noam; Sandholm, Tuomas			Regret-Based Pruning in Extensive-Form Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Counterfactual Regret Minimization (CFR) is a leading algorithm for finding a Nash equilibrium in large zero-sum imperfect-information games. CFR is an iterative algorithm that repeatedly traverses the game tree, updating regrets at each information set. We introduce an improvement to CFR that prunes any path of play in the tree, and its descendants, that has negative regret. It revisits that sequence at the earliest subsequent CFR iteration where the regret could have become positive, had that path been explored on every iteration. The new algorithm maintains CFR's convergence guarantees while making iterations significantly faster-even if previously known pruning techniques are used in the comparison. This improvement carries over to CFR+, a recent variant of CFR. Experiments show an order of magnitude speed improvement, and the relative speed improvement increases with the size of the game.	[Brown, Noam] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA	Carnegie Mellon University	Brown, N (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15217 USA.	noamb@cmu.edu; sandholm@cs.cmu.edu			National Science Foundation [IIS-1320620, IIS-1546752]	National Science Foundation(National Science Foundation (NSF))	This material is based on work supported by the National Science Foundation under grants IIS-1320620 and IIS-1546752, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center.	Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P7; Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2; Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324; Hoda S, 2010, MATH OPER RES, V35, P494, DOI 10.1287/moor.1100.0452; Jackson Eric Griffin, 2014, AAAI WORKSH COMP POK; Johanson Michael, 2016, AAAI C ART INT AAAI; Kroer C., 2015, P ACM C EC COMP EC; Lanctot M., 2009, ADV NEURAL INFORM PR, P1078; Pays  FranAgois, 2014, AAAI COMP POK WORKSH; Sandholm T, 2010, AI MAG, V31, P13, DOI 10.1609/aimag.v31i4.2311; Southey F., 2005, P 21 ANN C UNC ART I; Tammelin O., 2014, ARXIV14075042; Tammelin Oskari, 2015, IJCAI, V2015; Waugh K., 2009, INT C AUT AG MULT SY; Zinkevich M., 2007, ADV NEURAL INFORM PR, V20	16	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102059
C	Dauphin, YN; de Vries, H; Bengio, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dauphin, Yann N.; de Vries, Harm; Bengio, Yoshua			Equilibrated adaptive learning rates for non-convex optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Parameter-specific adaptive learning rate methods are computationally efficient ways to reduce the ill-conditioning problems encountered when training large deep networks. Following recent work that strongly suggests that most of the critical points encountered when training such networks are saddle points, we find how considering the presence of negative eigenvalues of the Hessian could help us design better suited adaptive learning rate schemes. We show that the popular Jacobi preconditioner has undesirable behavior in the presence of both positive and negative curvature, and present theoretical and empirical evidence that the so-called equilibration preconditioner is comparatively better suited to non-convex problems. We introduce a novel adaptive learning rate scheme, called ESGD, based on the equilibration preconditioner. Our experiments show that ESGD performs as well or better than RMSProp in terms of convergence speed, always clearly improving over plain stochastic gradient descent.	[Dauphin, Yann N.; de Vries, Harm; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada	Universite de Montreal	Dauphin, YN (corresponding author), Univ Montreal, Montreal, PQ, Canada.	dauphiya@iro.umontreal.ca; devries@iro.umontreal.ca; yoshua.bengio@umontreal.ca						Antonoglou I., 2013, ARXIV13126055; Bastien F., 2012, DEEP LEARN UNS FEAT; Bekas C, 2007, APPL NUMER MATH, V57, P1214, DOI 10.1016/j.apnum.2007.01.003; Bottou L, 1998, LECT NOTES COMPUTER, V1524; Bradley Andrew M, 2011, ARXIV11102805; Choromanska A., 2014, LOSS SURFACE MULTILA; Datta BN, 2010, NUMERICAL LINEAR ALGEBRA AND APPLICATIONS, SECOND EDITION, P1, DOI 10.1137/1.9780898717655; Dauphin Yann, 2014, NIPS 2014; Duchi J., 2011, J MACHINE LEARNING R; Guggenheimer H.W., 1995, COLL MATH J, V26, P2, DOI [10.1080/07468342.1995.11973657, DOI 10.1080/07468342.1995.11973657]; Martens J., 2012, ARXIV12066464; Martens J., 2010, P 27 INT C MACH LEAR, P735; Pascanu R., 2014, 2 INT C LEARN REPR I; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; Sutskever I., 2013, P 30 INT C MACH LEAR; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; VANDERSLUIS A, 1969, NUMER MATH, V14, P14, DOI 10.1007/BF02165096; Vinyals O., 2011, ARXIV11114259; Zeiler M.D, 2012, CORR ABS12125701	19	7	7	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102019
C	Hong, S; Noh, H; Han, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hong, Seunghoon; Noh, Hyeonwoo; Han, Bohyung			Decoupled Deep Neural Network for Semi-supervised Semantic Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a novel deep neural network architecture for semi-supervised semantic segmentation using heterogeneous annotations. Contrary to existing approaches posing semantic segmentation as a single task of region-based classification, our algorithm decouples classification and segmentation, and learns a separate network for each task. In this architecture, labels associated with an image are identified by classification network, and binary segmentation is subsequently performed for each identified label in segmentation network. The decoupled architecture enables us to learn classification and segmentation networks separately based on the training data with image-level and pixel-wise class labels, respectively. It facilitates to reduce search space for segmentation effectively by exploiting class-specific activation maps obtained from bridging layers. Our algorithm shows outstanding performance compared to other semi-supervised approaches with much less training images with strong annotations in PASCAL VOC dataset.	[Hong, Seunghoon; Noh, Hyeonwoo; Han, Bohyung] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea	Pohang University of Science & Technology (POSTECH)	Hong, S (corresponding author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.	maga33@postech.ac.kr; hyeonwoonoh_@postech.ac.kr; bhhan@postech.ac.kr	Hong, Seunghoon/AAF-9628-2019		ICT R&D program of MSIP/IITP [B0101-15-0307]; Samsung Electronics Co., Ltd.; ICT R&D program of MSIP/IITP [ML Center] [B0101-15-0552]; ICT R&D program of MSIP/IITP [DeepView]	ICT R&D program of MSIP/IITP; Samsung Electronics Co., Ltd.; ICT R&D program of MSIP/IITP [ML Center]; ICT R&D program of MSIP/IITP [DeepView]	This work was partly supported by the ICT R&D program of MSIP/IITP [B0101-15-0307; ML Center, B0101-15-0552; DeepView] and Samsung Electronics Co., Ltd.	[Anonymous], 2014, WORKSH INT C LEARN R; Chen LC., ARXIV 2015ABS1412706; Dai J., 2015, ICCV; Deng Jia, 2009, CVPR; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Hariharan B., 2014, ECCV; Hariharan Bharath, 2015, CVPR; Hariharan Bharath, 2011, P IEEE C COMP VIS PA; Jia Y., P ACM MULT, P675; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Mostajabi M., 2015, CVPR; Noh H., 2015, ICCV; Papandreou G., 2015, ICCV; Pathak D., 2015, ICLR; Pinheiro P.O., 2015, CVPR; Simonyan Karen, 2015, INT C LEARN REPR; Zheng S., 2015, ICCV; Zitnick C. L., 2014, ECCV	18	7	7	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102007
C	Krichene, W; Bayen, AM; Bartlett, PL		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Krichene, Walid; Bayen, Alexandre M.; Bartlett, Peter L.			Accelerated Mirror Descent in Continuous and Discrete Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SYSTEMS	We study accelerated mirror descent dynamics in continuous and discrete time. Combining the original continuous-time motivation of mirror descent with a recent ODE interpretation of Nesterov's accelerated method, we propose a family of continuous-time descent dynamics for convex functions with Lipschitz gradients, such that the solution trajectories converge to the optimum at a O (1/t(2)) rate. We then show that a large family of first-order accelerated methods can be obtained as a discretization of the ODE, and these methods converge at a O (1/k(2)) rate. This connection between accelerated mirror descent and the ODE provides an intuitive approach to the design and analysis of accelerated first-order algorithms.	[Krichene, Walid; Bayen, Alexandre M.; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Bartlett, Peter L.] QUT, Brisbane, Qld, Australia	University of California System; University of California Berkeley; Queensland University of Technology (QUT)	Krichene, W (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	walid@eecs.berkeley.edu; bayen@berkeley.edu; bartlett@berkeley.edu			NSF [CCF-1115788, CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166]; ARC [FL110100281]; Simons Institute Fall 2014 Algorithmic Spectral Graph Theory Program; ARC (ACEMS)	NSF(National Science Foundation (NSF)); ARC(Australian Research Council); Simons Institute Fall 2014 Algorithmic Spectral Graph Theory Program; ARC (ACEMS)	We gratefully acknowledge the NSF (CCF-1115788, CNS-1238959, CNS-1238962, CNS-1239054, CNS-1239166), the ARC (FL110100281 and ACEMS), and the Simons Institute Fall 2014 Algorithmic Spectral Graph Theory Program.	Allen-Zhu Z., 2014, LINEAR COUPLING ULTI; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Ben-Tal A., 2001, MPS SIAM SERIES OPTI; Bloch A., 1994, HAMILTONIAN GRADIENT; BROWN AA, 1989, J OPTIMIZ THEORY APP, V62, P211, DOI 10.1007/BF00941054; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Butcher J C, 2008, NUMERICAL METHODS OR; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Dekel O., 2011, P ICML, P1; HELMKE U, 1994, COMMUNICATIONS CONTR; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; JUDITSKY A, 2013, LECT NOTES; Khalil HK., 2002, NONLINEAR SYSTEMS, Vthird; Krichene Walid, 2015, 54 IEEE C DEC CONTR; Lyapunov A.M., 1992, CONTROL THEORY APPL; NEMIROVSKY A. S., 1983, WILEY INTERSCIENCE S; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; Rockafellar R. T., 1970, CONVEX ANAL; Schropp J, 2000, NUMER FUNC ANAL OPT, V21, P537, DOI 10.1080/01630560008816971; Su Weijie, 2014, NIPS; Teschl G., 2012, ORDINARY DIFFERENTIA, V140	29	7	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101102
C	Oh, J; Guo, XX; Lee, H; Lewis, R; Singh, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Oh, Junhyuk; Guo, Xiaoxiao; Lee, Honglak; Lewis, Richard; Singh, Satinder			Action-Conditional Video Prediction using Deep Networks in Atari Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Motivated by vision-based reinforcement learning (RL) problems, in particular Atari games from the recent benchmark Aracade Learning Environment (ALE), we consider spatio-temporal prediction problems where future image-frames depend on control variables or actions as well as previous frames. While not composed of natural scenes, frames in Atari games are high-dimensional in size, can involve tens of objects with one or more objects being controlled by the actions directly and many other objects being influenced indirectly, can involve entry and departure of objects, and can involve deep partial observability. We propose and evaluate two deep neural network architectures that consist of encoding, action-conditional transformation, and decoding layers based on convolutional neural networks and recurrent neural networks. Experimental results show that the proposed architectures are able to generate visually-realistic frames that are also useful for control over approximately 100-step action-conditional futures in some games. To the best of our knowledge, this paper is the first to make and evaluate long-term predictions on high-dimensional video conditioned by control inputs.	[Oh, Junhyuk; Guo, Xiaoxiao; Lee, Honglak; Lewis, Richard; Singh, Satinder] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Oh, J (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	junhyuk@umich.edu; guoxiao@umich.edu; honglak@umich.edu; rickl@umich.edu; baveja@umich.edu			NSF [IIS-1526059]; Bosch Research; ONR [N00014-13-1-0762]	NSF(National Science Foundation (NSF)); Bosch Research; ONR(Office of Naval Research)	This work was supported by NSF grant IIS-1526059, Bosch Research, and ONR grant N00014-13-1-0762. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsors.	[Anonymous], 2009, ICML; Bellemare M., 2013, P 30 INT C MACH LEAR; Bellemare M.G., 2014, ICML; Bellemare M. G., 2012, AAAI; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Ciresan D., 2012, CVPR; Dosovitskiy A., 2015, CVPR; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Graves Alex, 2013, ARXIV13080850 CORR; Guo Xiaoxiao, 2014, NIPS; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Jia Y., 2014, ACM MULTIMEDIA; Karpathy A., 2014, CVPR; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lenz I., 2015, RSS; Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53; Michalski V., 2014, NIPS; Mittelman R., 2014, ICML; Mnih V., 2013, ARXIV PREPRINT ARXIV; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair V., 2010, P 27 INT C MACHINE L, P807, DOI DOI 10.5555/3104322.3104425; Reed S., 2014, ICML; Rifai S., 2012, ECCV; Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Srivastava Nitish, 2015, ICML; Sutskever I., 2009, NIPS; Sutskever I., 2014, NEURIPS; Sutskever Ilya, 2011, ICML; Szegedy C., 2014, 2015 IEEE C COMP VIS, P1, DOI [DOI 10.1109/CVPR.2015.7298594, 10.48550/arxiv.1409.4842]; Szepesvari C., 2006, ECML; Taylor G. W., 2009, ICML; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tran D., 2015, P IEEE INT C COMPUTE, P4489; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Yang J., 2015, P ADV NEUR INF PROC	37	7	7	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102008
C	Podosinnikova, A; Bach, F; Lacoste-Julien, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Podosinnikova, Anastasia; Bach, Francis; Lacoste-Julien, Simon			Rethinking LDA: Moment Matching for Discrete ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider moment matching techniques for estimation in latent Dirichlet allocation (LDA). By drawing explicit links between LDA and discrete versions of independent component analysis (ICA), we first derive a new set of cumulant-based tensors, with an improved sample complexity. Moreover, we reuse standard ICA techniques such as joint diagonalization of tensors to improve over existing methods based on the tensor power method. In an extensive set of experiments on both synthetic and real datasets, we show that our new combination of tensors and orthogonal joint diagonalization techniques outperforms existing moment matching methods.	[Podosinnikova, Anastasia; Bach, Francis; Lacoste-Julien, Simon] Ecole Normale Super Paris, INRIA, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Podosinnikova, A (corresponding author), Ecole Normale Super Paris, INRIA, Paris, France.				MSR-Inria Joint Center	MSR-Inria Joint Center	This work was partially supported by the MSR-Inria Joint Center. The authors would like to thank Christophe Dupuy for helpful discussions.	Anandkumar A., 2013, CORR; Anandkumar Anima, 2012, NIPS; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; BUNSEGERSTNER A, 1993, SIAM J MATRIX ANAL A, V14, P927, DOI 10.1137/0614062; Buntine W. L., 2002, ECML; Buntine W. L., 2004, APPL DISCRETE PCA DA; Canny J., 2004, SIGIR; Cardoso J.-F., 1993, IEE P F; Cardoso J.-F., 1996, ISCAS; Cardoso J.-F., 1989, ICASSP; Cardoso J.-F., 1990, ICASSP; Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546; Cardoso JF, 1999, NEURAL COMPUT, V11, P157, DOI 10.1162/089976699300016863; Cohen S., 2014, ACL; Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Globerson A, 2007, J MACH LEARN RES, V8, P2265; Griffiths T., 2002, TECHNICAL REPORT; Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; Jutten C., 1987, THESIS; Kuleshov V., 2015, P AISTATS; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Roweis Sam, 1998, NIPS; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515	30	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100039
C	Hwang, SJ; Sigal, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hwang, Sung Ju; Sigal, Leonid			A Unified Semantic Embedding: Relating Taxonomies and Attributes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a method that learns a discriminative yet semantic space for object categorization, where we also embed auxiliary semantic entities such as supercategories and attributes. Contrary to prior work, which only utilized them as side information, we explicitly embed these semantic entities into the same space where we embed categories, which enables us to represent a category as their linear combination. By exploiting such a unified model for semantics, we enforce each category to be generated as a supercategory + a sparse combination of attributes, with an additional exclusive regularization to learn discriminative composition. The proposed reconstructive regularization guides the discriminative learning process to learn a model with better generalization. This model also generates compact semantic description of each category, which enhances interoperability and enables humans to analyze what has been learned.	[Hwang, Sung Ju; Sigal, Leonid] Disney Res, Pittsburgh, PA 15213 USA		Hwang, SJ (corresponding author), Ulsan Natl Inst Sci & Technol, Ulsan, South Korea.	sungju.hwang@disneyresearch.com; lsigal@disneyresearch.com						Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Bengio Samy, 2010, NEURAL INFORM PROCES; Donahue J, 2014, PR MACH LEARN RES, V32; Farhadi Ali, 2009, CVPR; Frome Andrea, 2013, NEURIPS; Gao TS, 2011, IEEE I CONF COMP VIS, P2072, DOI 10.1109/ICCV.2011.6126481; Griffin G, 2008, PROC CVPR IEEE, P533; Hwang S.J., 2013, P 30 ICML, V28, P639; Hwang SJ, 2011, PROC CVPR IEEE, P1761, DOI 10.1109/CVPR.2011.5995543; John Duchi, 2009, J MACHINE LEARNING R, V10; Lampert CH, 2009, PROC CVPR IEEE, P951, DOI 10.1109/CVPRW.2009.5206594; Mairal J, 2008, NIPS, P1033; Marszalek M, 2008, LECT NOTES COMPUT SC, V5305, P479, DOI 10.1007/978-3-540-88693-8_35; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Weinberger K., 2009, ADV NEURAL INFORM PR, P1737; Weston Jason, 2011, 22 INT JOINT C ART I; Zhou Y., 2010, PROC 13 INT C ARTIF, P988; Zweig A., 2013, PROC INT C MACH LEAR, P37	18	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100068
C	Kim, B; Rudin, C; Shah, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Kim, Been; Rudin, Cynthia; Shah, Julie			The Bayesian Case Model: A Generative Approach for Case-Based Reasoning and Prototype Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present the Bayesian Case Model (BCM), a general framework for Bayesian case-based reasoning (CBR) and prototype classification and clustering. BCM brings the intuitive power of CBR to a Bayesian generative framework. The BCM learns prototypes, the "quintessential" observations that best represent clusters in a dataset, by performing joint inference on cluster labels, prototypes and important features. Simultaneously, BCM pursues sparsity by learning subspaces, the sets of features that play important roles in the characterization of the prototypes. The prototype and subspace representation provides quantitative benefits in interpretability while preserving classification accuracy. Human subject experiments verify statistically significant improvements to participants' understanding when using explanations produced by BCM, compared to those given by prior art.	[Kim, Been; Rudin, Cynthia; Shah, Julie] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Kim, B (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	beenkim@csail.mit.edu; rudin@csail.mit.edu; julie_a_shah@csail.mit.edu						Aamodt A, 1991, KNOWLEDGE INTENSIVE, P27; Aamodt Agnar, 1994, AI COMMUNICATIONS; Baehrens D., 2010, JMLR; Bichindaritz I., 2006, AI IN MED; Bien J., 2011, AOAS; Blei D. M., 2003, JMLR; Carroll J.S., 1980, COGNITIVE PROCESSES; Chang J., 2009, NIPS; Cohen M.S., 1996, HUMAN FACTORS; Cover T., 1967, INFORM THEORY; Cunningham P., 2003, CBRRD; De'ath G., 2000, ECOLOGY; Eisenstein J., 2011, ICML; Freitas, 2014, ACM SIGKDD EXPLORATI; Goh S., 2014, KDD; Graf A., 2009, NEURAL COMPUTATION; Griffiths T. L., 2004, PNAS; Hofmann T, 1999, ACM SIGIR; Hull J. J., 1994, TPAMI; Huysmans J., 2011, DSS; Klein G., 1989, HFES; Lang K., 1995, ICML; Letham B., 2014, TECHNICAL REPORT; Li H., 2008, KBSI; Murdock J.W., 2003, ICCBR; Newell A, 1972, HUMAN PROBLEM SOLVIN; Phan X. H., 2013, GIBBSLDA AC C IMPLEM; Slade S., 1991, AI MAGAZINE; Sormo F., 2005, AI REV; Tibshirani R., 1996, JRSS; Ustun Berk, 2014, METHODS MODELS INTER; Williamson  S., 2010, IBP COMPOUND DIRICHL; Zhu J., 2012, JMLR	33	7	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100092
C	Felzenszwalb, PF; Huttenlocher, DP; Kleinberg, JM		Thrun, S; Saul, K; Scholkopf, B		Felzenszwalb, PF; Huttenlocher, DP; Kleinberg, JM			Fast algorithms for large-state-space HMMs with applications to web usage analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				FILTERS	In applying Hidden Markov Models to the analysis of massive data streams, it is often necessary to use an artificially reduced set of states; this is due in large part to the fact that the basic HMM estimation algorithms have a quadratic dependence on the size of the state set. We present algorithms that reduce this computational bottleneck to linear or near-linear time, when the states can be embedded in an underlying grid of parameters. This type of state representation arises in many domains; in particular, we show ail application to traffic analysis at a high-volume Web site.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Felzenszwalb, PF (corresponding author), MIT, AI Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.							AIZEN J, IN PRESS J NATL ACAD; BORGEFORS G, 1986, COMPUT VISION GRAPH, V34, P344, DOI 10.1016/S0734-189X(86)80047-0; FLEZENSZWALB P, 2000, P IEEE COMP VIS PATT, P66; GIL J, 1993, IEEE T PATTERN ANAL, V15, P504, DOI 10.1109/34.211471; KARZANOV A, 1992, CYBERNETICS SYSTEM A; KITAGAWA G, 1987, J AM STAT ASSOC, V82, P1032, DOI 10.2307/2289375; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; SCOTT SL, 2003, IN PRESS BAYESIAN ST, V7; WELLS WM, 1986, IEEE T PATTERN ANAL, V8, P234, DOI 10.1109/TPAMI.1986.4767776	10	7	7	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						409	416						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500052
C	Fink, M; Perona, P		Thrun, S; Saul, K; Scholkopf, B		Fink, M; Perona, P			Mutual boosting for contextual inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				OBJECTS	Mutual Boosting is a method aimed at incorporating contextual information to augment object detection. When multiple detectors of objects and parts are trained in parallel using AdaBoost [1], object detectors might use the remaining intermediate detectors to enrich the weak learner set. This method generalizes the efficient features suggested by Viola and Jones [2] thus enabling information inference between parts and objects in a compositional hierarchy. In our experiments eye-, nose-, mouth- and face detectors are trained using the Mutual Boosting framework. Results show that the method outperforms applications overlooking contextual information. We suggest that achieving contextual integration is a step toward human-like detection capabilities.	Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Fink, M (corresponding author), Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel.	fink@huji.ac.il; perona@vision.caltech.edu						Barnard K, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P408, DOI 10.1109/ICCV.2001.937654; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BIEDERMAN I, 1982, COGNITIVE PSYCHOL, V14, P143, DOI 10.1016/0010-0285(82)90007-X; Biedermann I., 1981, PERCEPTUAL ORG, P213, DOI [10.4324/9781315512372-8, 10.4324/9781315512372]; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3; Oliva A, 2002, LECT NOTES COMPUT SC, V2525, P263; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; TANAKA K, 1991, J NEUROPHYSIOL, V66, P170, DOI 10.1152/jn.1991.66.1.170; VIOLA VP, 2001, IEEE ICCV WORKSH STA; Weber M, 2000, LECT NOTES COMPUT SC, V1842, P18	11	7	7	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1515	1522						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500188
C	Griffiths, TL; Tenenbaum, JB		Thrun, S; Saul, K; Scholkopf, B		Griffiths, TL; Tenenbaum, JB			From algorithmic to subjective randomness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				AUTOMATA; JUDGMENT; GRAMMARS	We explore the phenomena of subjective randomness as a case study in understanding how people discover structure embedded in noise. We present a rational account of randomness perception based on the statistical problem of model selection: given a stimulus, inferring whether the process that generated it was random or regular. Inspired by the mathematical definition of randomness given by Kolmogorov complexity, we characterize regularity in terms of a hierarchy of automata that augment a finite controller with different forms of memory. We find that the regularities detected in binary sequences depend upon presentation format, and that the kinds of automata that can identify these regularities are informative about the cognitive processes engaged by different formats.	MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Griffiths, TL (corresponding author), MIT, Cambridge, MA 02139 USA.	gruffydd@mit.edu; jbt@mit.edu						AHO AV, 1968, J ACM, V15, P647, DOI 10.1145/321479.321488; CHAITIN GJ, 1969, J ACM, V16, P145, DOI 10.1145/321495.321506; CHERUBINI A, 1991, THEOR COMPUT SCI, V85, P171, DOI 10.1016/0304-3975(91)90053-5; CHOMSKY N, 1956, IRE T INFORM THEOR, V2, P113; Falk R, 1997, PSYCHOL REV, V104, P301, DOI 10.1037/0033-295X.104.2.301; GINSBURG S, 1967, J ACM, V14, P172, DOI 10.1145/321371.321385; GRIFFITHS TL, 2003, P 25 ANN C COGN SCI; KAHNEMAN D, 1972, COGNITIVE PSYCHOL, V3, P430, DOI 10.1016/0010-0285(72)90016-3; Kolmogorov A. N., 1968, International Journal of Computer Mathematics, V2, P157, DOI 10.1080/00207166808803030; SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2	10	7	7	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						953	960						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500119
C	Kelly, R; Lee, TS		Thrun, S; Saul, K; Scholkopf, B		Kelly, R; Lee, TS			Decoding V1 neuronal activity using particle filtering with volterra kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				RECONSTRUCTION	Decoding is a strategy that allows us to assess the amount of information neurons can provide about certain aspects of the visual scene. In this study, we develop a method based on Bayesian sequential updating and the particle filtering algorithm to decode the activity of V1 neurons in awake monkeys. A distinction in our method is the use of Volterra kernels to filter the particles, which live in a high dimensional space. This parametric Bayesian decoding scheme is compared to the optimal linear decoder and is shown to work consistently better than the linear optimal decoder. Interestingly, our results suggest that for decoding in real time, spike trains of as few as 10 independent but similar neurons would be sufficient for decoding a critical scene variable in a particular class of visual stimuli. The reconstructed variable can predict the neural activity about as well as the actual signal with respect to the Volterra kernels.	Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Kelly, R (corresponding author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.	rkelly@cs.cmu.edu; tai@cnbc.cmu.edu						BROCKWELL AE, 2003, UNPUB J NEUROPHYSIOL; Brown EN, 1998, J NEUROSCI, V18, P7411; EDEN UT, 2002, P COMP NEUR M CNS 02; GAO Y, 2002, PROBABILISTIC INFERE, P213; GEORGOPOULOS AP, 1989, SCIENCE, V243, P234, DOI 10.1126/science.2911737; Rieke F., 1997, SPIKES EXPLORING NEU; Romero R, 2003, NEUROCOMPUTING, V52-4, P135, DOI 10.1016/S0925-2312(02)00799-3; Stanley GB, 1999, J NEUROSCI, V19, P8036, DOI 10.1523/jneurosci.19-18-08036.1999; Stanley GB, 2002, NEURAL COMPUT, V14, P2925, DOI 10.1162/089976602760805340; Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017	10	7	7	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1359	1366						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500169
C	Kumar, S; Hebert, M		Thrun, S; Saul, K; Scholkopf, B		Kumar, S; Hebert, M			Discriminative fields for modeling spatial dependencies in natural images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					In this paper we present Discriminative Random Fields (DRF), a discriminative framework for the classification of natural image regions by incorporating neighborhood spatial dependencies in the labels as well as the observed data. The proposed model exploits local discriminative models and allows to relax the assumption of conditional independence of the observed data given the labels, commonly used in the Markov Random Field (MRF) framework. The parameters of the DRF model are learned using penalized maximum pseudo-likelihood method. Furthermore, the form of the DRF model allows the MAP inference for binary classification problems using the graph min-cut algorithms. The performance of the model was verified on the synthetic as well as the real-world images. The DRF model outperforms the MRF model in the experiments.	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Kumar, S (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	skumar@ri.cmu.edu; hebert@ri.cmu.edu						Cheng H, 2001, IEEE T IMAGE PROCESS, V10, P511, DOI 10.1109/83.913586; Collins M, 2002, P C EMP METH NAT LAN; Feng XJ, 2002, IEEE T PATTERN ANAL, V24, P467, DOI 10.1109/34.993555; FIGUEIREDO MAT, 2001, ADV NEURAL INFORMATI; GREIG DM, 1989, J ROY STAT SOC B MET, V51, P271, DOI 10.1111/j.2517-6161.1989.tb01764.x; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; KOLMOGOROV V, 2002, P EUR C COMP VIS, V3, P65; Kumar S, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1150, DOI 10.1109/ICCV.2003.1238478; Kumar S., 2003, P IEEE INT C COMP VI; Lafferty J., 2001, CONDITIONAL RANDOM F; Li S. Z., 2001, COMP SCI W; MacKay DJC, 1996, FUND THEOR, V62, P221; MCCULLAGH P, 1987, GENERALISED LINEAR M; Rubinstein Y. D., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P49; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117; Wilson R, 2003, IEEE T PATTERN ANAL, V25, P42, DOI 10.1109/TPAMI.2003.1159945	16	7	8	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1531	1538						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500190
C	Weston, J; Leslie, C		Thrun, S; Saul, K; Scholkopf, B		Weston, J; Leslie, C			Semi-supervised protein classification using cluster kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				SEQUENCES; DATABASE	A key issue in supervised protein classification is the representation of input sequences of amino acids. Recent work using string kernels for protein data has achieved state-of-the-art classification performance. However, such representations are based only on labeled data - examples with known 3D structures, organized into structural classes - while in practice, unlabeled data is far more plentiful. In this work, we develop simple and scalable cluster kernel techniques for incorporating unlabeled data into the representation of protein sequences. We show that our methods greatly improve the classification performance of string kernels and outperform standard approaches for using unlabeled data, such as adding close homologs of the positive examples to the training data. We achieve equal or superior performance to previously presented cluster kernel methods while achieving far greater computational efficiency.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Weston, J (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.							Altschul SF, 1997, NUCLEIC ACIDS RES, V25, P3389, DOI 10.1093/nar/25.17.3389; ALTSCHUL SF, 1990, J MOL BIOL, V215, P403, DOI 10.1006/jmbi.1990.9999; CHAPELLE O, 2002, NEURAL INFORMATION P, V15; JAAKKOLA T, 2000, J COMPUTATIONAL BIOL; Joachims Thorsten, 1999, P ICML; KROGH A, 1994, J MOL BIOL, V235, P1501, DOI 10.1006/jmbi.1994.1104; LESLIE C, 2002, NEURAL INFORMATION P, V15; LIAO C, 2002, P RECOMB; NG A, 2001, NEURAL PROCESSING IN, V14; Park J, 1998, J MOL BIOL, V284, P1201, DOI 10.1006/jmbi.1998.2221; Seeger M., 2010, LEARNING LABELED UNL; SMITH TF, 1981, J MOL BIOL, V147, P195, DOI 10.1016/0022-2836(81)90087-5; SZUMMER M, 2001, NEURAL INFORMATION P, V14; Zhu Xiaojin, 2002, CMU CALD TECH REPORT	15	7	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						595	602						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500075
C	Burgess, N; Hartley, T		Dietterich, TG; Becker, S; Ghahramani, Z		Burgess, N; Hartley, T			Orientational and geometric determinants of place and head-direction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				COMPLEX-SPIKE CELLS; CONJOINT CONTROL; UNIT-ACTIVITY; HIPPOCAMPUS; FIELDS; ENVIRONMENT; SYSTEMS; MEMORY; REPRESENTATION; DYNAMICS	We present a model of the firing of place and head-direction cells in rat hippocampus. The model can predict the response of individual cells and populations to parametric manipulations of both geometric (e.g. O'Keefe & Burgess, 1996) and orientational (Fenton et al., 2000a) cues, extending a previous geometric model (Hartley et al., 2000). It provides a functional description of how these cells' spatial responses are derived from the rat's environment and makes easily testable quantitative predictions. Consideration of the phenomenon of remapping (Muller Kubie, 1987; Bostock et al., 1991) indicates that the model may also be consistent with non-parametric changes in firing, and provides constraints for its future development.	UCL, Inst Cognit Neurosci, London WC1N 3AR, England; UCL, Dept Anat, London WC1N 3AR, England	University of London; University College London; University of London; University College London	Burgess, N (corresponding author), UCL, Inst Cognit Neurosci, 17 Queen Sq, London WC1N 3AR, England.		Burgess, Neil/B-2420-2009; Hartley, Tom/B-7811-2008	Burgess, Neil/0000-0003-0646-6584; Hartley, Tom/0000-0002-4072-6637				Bostock E, 1991, Hippocampus, V1, P193, DOI 10.1002/hipo.450010207; Cressant A, 1997, J NEUROSCI, V17, P2531; Fenton AA, 2000, J GEN PHYSIOL, V116, P211, DOI 10.1085/jgp.116.2.211; Fenton AA, 2000, J GEN PHYSIOL, V116, P191, DOI 10.1085/jgp.116.2.191; Fuhs MC, 2000, NEUROCOMPUTING, V32, P379, DOI 10.1016/S0925-2312(00)00189-2; Hartley T, 2000, HIPPOCAMPUS, V10, P369, DOI 10.1002/1098-1063(2000)10:4<369::AID-HIPO3>3.0.CO;2-0; HILL AJ, 1978, EXP NEUROL, V62, P282, DOI 10.1016/0014-4886(78)90058-4; Kali S, 2000, J NEUROSCI, V20, P7463; Kentros C, 1998, SCIENCE, V280, P2121, DOI 10.1126/science.280.5372.2121; LEVER CL, 2002, IN PRESS NATURE; MARR D, 1971, PHILOS T ROY SOC B, V262, P23, DOI 10.1098/rstb.1971.0078; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; MCNAUGHTON BL, 1994, COGNITIVE NEUROSCIEN, P585; Mehta MR, 1997, P NATL ACAD SCI USA, V94, P8918, DOI 10.1073/pnas.94.16.8918; MULLER RU, 1987, J NEUROSCI, V7, P1951; MULLER RU, 1994, J NEUROSCI, V14, P7235; MULLER RU, 1987, J NEUROSCI, V7, P1935; OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1; OKeefe J, 1996, NATURE, V381, P425, DOI 10.1038/381425a0; OKEEFE J, 1987, EXP BRAIN RES, V68, P1; Redish AD, 1996, ADV NEUR IN, V8, P61; Skaggs WE, 1995, ADV NEURAL INFORM PR, V7, P51; TAUBE JS, 1990, J NEUROSCI, V10, P420; TREVES A, 1992, HIPPOCAMPUS, V2, P189, DOI 10.1002/hipo.450020209; WILSON MA, 1993, SCIENCE, V261, P1055, DOI 10.1126/science.8351520; Zhang K, 1996, J NEUROSCI, V16, P2112	26	7	7	1	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						165	172						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100021
C	Fine, S; Scheinberg, K		Dietterich, TG; Becker, S; Ghahramani, Z		Fine, S; Scheinberg, K			Incremental learning and selective sampling via parametric optimization framework for SVM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SUPPORT	We propose a framework based on a parametric quadratic programming (QP) technique to solve the support vector machine (SVM) training problem. This framework, can be specialized to obtain two SVM optimization methods. The first solves the fixed bias problem, while the second starts with an optimal solution for a fixed bias problem and adjusts the bias until the optimal value is found. The later method can be applied in conjunction with any other existing technique which obtains a fixed bias solution. Moreover, the second method can also be used independently to solve the complete SVM training problem. A combination of these two methods is more flexible than each individual method and, among other things, produces an incremental algorithm which exactly solve the 1-Norm Soft Margin SVM optimization problem. Applying Selective Sampling techniques may further boost convergence.										BERKELAAR AB, 1996, 9626 DELFT U; Cauwenberghs G, 2001, ADV NEUR IN, V13, P409; Cristianini N., 2000, INTRO SUPPORT VECTOR; FINE S, 2001, UNPUB POKER PARAMETR; FRIESS T, 1998, P 15 INT C MACH LEAR, P188; Keerthi SS, 2000, IEEE T NEURAL NETWOR, V11, P124, DOI 10.1109/72.822516; Kowalczyk A, 2000, ADV NEUR IN, P75; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Rockafellar R. T., 1974, CONJUGATE DUALITY OP	9	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						705	711						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100088
C	Littman, ML; Kearns, M; Singh, S		Dietterich, TG; Becker, S; Ghahramani, Z		Littman, ML; Kearns, M; Singh, S			An efficient, exact algorithm for solving tree- structured graphical games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We describe a new algorithm for computing a Nash equilibrium in graphical games, a compact representation for multi-agent systems that we introduced in previous work. The algorithm is the first to compute equilibria both efficiently and exactly for a non-trivial class of graphical games.	AT&T Labs Res, Florham Pk, NJ 07932 USA	AT&T	Littman, ML (corresponding author), AT&T Labs Res, Florham Pk, NJ 07932 USA.							[Anonymous], 2001, UAI 01; KOLLER D, 2001, UNPUB MULTI AGENT IN; La Mura P., 2000, P 16 C UNC ART INT, P335; NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529	4	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						817	823						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100102
C	Wright, BD; Sen, K; Bialek, W; Doupe, AJ		Dietterich, TG; Becker, S; Ghahramani, Z		Wright, BD; Sen, K; Bialek, W; Doupe, AJ			Spike timing and the coding of naturalistic sounds in a central auditory area of songbirds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NEURAL CODE; INFORMATION; STIMULI; NEOSTRIATUM; RESPONSES; NEURONS; TRAINS	In nature, animals encounter high dimensional sensory stimuli that have complex statistical and dynamical structure. Attempts to study the neural coding of these natural signals face challenges both in the selection of the signal ensemble and in the analysis of the resulting neural responses. For zebra finches, naturalistic stimuli can be defined as sounds that they encounter in a colony of conspecific birds. We assembled an ensemble of these sounds by recording groups of 10-40 zebra finches, and then analyzed the response of single neurons in the songbird central auditory area (field L) to continuous playback of long segments from this ensemble. Following methods developed in the fly visual system, we measured the information that spike trains provide about the acoustic stimulus without any assumptions about which features of the stimulus are relevant. Preliminary results indicate that large amounts of information are carried by spike timing, with roughly half of the information accessible only at time resolutions better than 10 ms; additional information is still being revealed as time resolution is improved to 2 ms. Information can be decomposed into that carried by the locking of individual spikes to the stimulus (or modulations of spike rate) vs. that carried by timing in spike patterns. Initial results show that in field L, temporal patterns give at least similar to 20% extra information. Thus, single central auditory neurons can provide an informative representation of naturalistic sounds, in which spike timing may play a significant role.	Univ Calif San Francisco, Sloan Swartz Ctr Theoret Neurobiol, San Francisco, CA 94143 USA	University of California System; University of California San Francisco	Wright, BD (corresponding author), Univ Calif San Francisco, Sloan Swartz Ctr Theoret Neurobiol, San Francisco, CA 94143 USA.			Wright, Brian/0000-0002-6440-8596				ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Berry MJ, 1997, P NATL ACAD SCI USA, V94, P5411, DOI 10.1073/pnas.94.10.5411; Brenner N, 2000, NEURAL COMPUT, V12, P1531, DOI 10.1162/089976600300015259; Janata P, 1999, J NEUROSCI, V19, P5108; Lewen GD, 2001, NETWORK-COMP NEURAL, V12, P317, DOI 10.1088/0954-898X/12/3/305; Lewicki MS, 1996, J NEUROSCI, V16, P6987; Liu RC, 2001, J NEUROPHYSIOL, V86, P2789, DOI 10.1152/jn.2001.86.6.2789; MacKAY DONALD M., 1952, BULL MATH BIOPHYS, V14, P127, DOI 10.1007/BF02477711; MARGOLIASH D, 1983, J NEUROSCI, V3, P1039; Reinagel P, 2000, J NEUROSCI, V20, P5392, DOI 10.1523/JNEUROSCI.20-14-05392.2000; Rieke F, 1995, P ROY SOC B-BIOL SCI, V262, P259, DOI 10.1098/rspb.1995.0204; SCHEICH H, 1979, J COMP PHYSIOL, V132, P257, DOI 10.1007/BF00614497; Searcy WA, 1999, DESIGN OF ANIMAL COMMUNICATION, P577; Sen K, 2001, J NEUROPHYSIOL, V86, P1445, DOI 10.1152/jn.2001.86.3.1445; Stripling R, 2001, J NEUROBIOL, V48, P163, DOI 10.1002/neu.1049; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; Theunissen FE, 2000, J NEUROSCI, V20, P2315; TREVES A, 1995, NEURAL COMPUT, V7, P399, DOI 10.1162/neco.1995.7.2.399; ZARETSKY MD, 1976, BRAIN RES, V111, P167, DOI 10.1016/0006-8993(76)91058-1	20	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						309	316						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100039
C	Hsu, D; Figueroa, M; Diorio, C		Leen, TK; Dietterich, TG; Tresp, V		Hsu, D; Figueroa, M; Diorio, C			A silicon primitive for competitive learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Competitive learning is a technique for training classification and clustering networks. We have designed and fabricated an 11-transistor primitive, that we term an automaximizing bump circuit, that implements competitive learning dynamics. The circuit performs a similarity computation, affords nonvolatile storage, and implements simultaneous local adaptation and computation. We show that our primitive is suitable for implementing competitive learning in VLSI, and demonstrate its effectiveness in a standard clustering task.	Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Hsu, D (corresponding author), Univ Washington, 114 Sieg Hall,Box 352350, Seattle, WA 98195 USA.		Figueroa, Miguel/O-7395-2016	Figueroa, Miguel/0000-0002-5033-432X				ALLEN TP, 1994, Patent No. 5331215; Arbib MA, 1995, HDB BRAIN THEORY NEU; Card HC, 1998, ANALOG INTEGR CIRC S, V15, P291, DOI 10.1023/A:1008222414574; DELBRUCK T, 1993, 26 CNS CALTECH; Diorio C, 2000, IEEE T ELECTRON DEV, V47, P464, DOI 10.1109/16.822295; HASLER P, 2001, IN PRESS IEEE T CIRC, V2; HSU D, 2000, 20000701 UW CSE; Lazzaro J., 1988, ADV NEURAL INFORMATI, V1, P703; LENZLINGER M, 1969, J APPL PHYS, V40, P278, DOI 10.1063/1.1657043; Takeda E., 1995, HOT CARRIER EFFECTS	10	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						713	719						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800101
C	Moghaddam, B; Yang, MH		Leen, TK; Dietterich, TG; Tresp, V		Moghaddam, B; Yang, MH			Sex with Support Vector Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				DIFFERENCE	Nonlinear Support Vector Machines (SVMs) are investigated for visual sex classification with low resolution "thumbnail" faces (21-by-12 pixels) processed from 1,755 images from the FERET face database. The performance of SVMs is shown to be superior to traditional pattern classifiers (Linear, Quadratic, Fisher Linear Discriminant, Nearest-Neighbor) as well as more modern techniques such as Radial Basis Function (RBF) classifiers and large ensemble-RBF networks. Furthermore, the SVM performance (3.4% error) is currently the best result reported in the open literature.	Mitsubishi Elect Res Lab, Cambridge, MA 02139 USA		Moghaddam, B (corresponding author), Mitsubishi Elect Res Lab, Cambridge, MA 02139 USA.	baback@mer1.com; mhyang@vision.ai.uiuc.edu						BRUCE V, 1993, PERCEPTION, V22, P131, DOI 10.1068/p220131; BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061; Brunelli R., 1992, P DARPA IM UND WORKS, P311; BURTON AM, 1993, PERCEPTION, V22, P153, DOI 10.1068/p220153; Cortes C, 1995, MACHINE LEARNING, P20; COTTRELL G, 1991, ADV NEURAL INFORM PR, P564; COURANT R, 1953, METHODS MATH PHYSIAC, V1; Edelstein JD, 1998, J HIGH ENERGY PHYS; EVGENIOU T, 1999, 1654 AI MIT; GOLOMB B, 1991, ADV NEURAL INFORM PR, P572; Gutta S, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P194, DOI 10.1109/AFGR.1998.670948; Huang J, 1998, INT C PATT RECOG, P154, DOI 10.1109/ICPR.1998.711102; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; Osuna E, 1997, PROC CVPR IEEE, P130, DOI 10.1109/CVPR.1997.609310; OToole AJ, 1997, PERCEPTION, V26, P75, DOI 10.1068/p260075; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Tamura S, 1996, PATTERN RECOGN, V29, P331, DOI 10.1016/0031-3203(95)00073-9; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; WISKOTT L, 1995, P INT WORKSH AUT FAC, P92	20	7	7	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						960	966						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800135
C	Simoni, MF; Cymbalyuk, GS; Sorensen, MQ; Calabrese, RL; DeWeerth, SP		Leen, TK; Dietterich, TG; Tresp, V		Simoni, MF; Cymbalyuk, GS; Sorensen, MQ; Calabrese, RL; DeWeerth, SP			Development of hybrid systems: Interfacing a silicon neuron to a leech heart interneuron	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				CIRCUITS; MODEL	We have developed a silicon neuron that is inspired by a mathematical model of the leech heartbeat (HN) interneuron. The temporal and ionic current behaviors of this silicon neuron are close to that of the living cell. Because of this similarity we were able to interface this silicon neuron to a living HN cell using a dynamic clamp technique [8]. We present data showing dynamic behaviors of the hybrid half-center oscillator.	Georgia Inst Technol, Lab Neuroengn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Simoni, MF (corresponding author), Georgia Inst Technol, Lab Neuroengn, Atlanta, GA 30332 USA.			Calabrese, Ronald L./0000-0001-7135-3469				CYMBALYUK GS, 2000, IN PRESS NEURAL COMP, V12; KNIGHT CD, 1999, THESIS GEORGIA I TEC; LeMasson G, 1995, PROG BIOPHYS MOL BIO, V64, P201, DOI 10.1016/S0079-6107(96)00004-1; Mead, 1989, ANALOG VLSI NEURAL S; NADIM F, 1995, J COMPUT NEUROSCI, V2, P215, DOI 10.1007/BF00961435; PATEL G, 1999, ADV NEURAL INFORMATI, V12; SCHMIDT J, 1992, J EXP BIOL, V171, P329; Sharp AA, 1996, J NEUROPHYSIOL, V76, P867, DOI 10.1152/jn.1996.76.2.867; Simoni MF, 1999, IEEE T CIRCUITS-II, V46, P967, DOI 10.1109/82.775396; SIMONI MF, 1997, 6 ANN COMP NEUR M NI; Skinner F K, 1994, J Comput Neurosci, V1, P69, DOI 10.1007/BF00962719; WANG XJ, 1992, NEURAL COMPUT, V4, P84, DOI 10.1162/neco.1992.4.1.84	12	7	7	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						173	179						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800025
C	Choi, SPM; Yeung, DY; Zhang, NL		Solla, SA; Leen, TK; Muller, KR		Choi, SPM; Yeung, DY; Zhang, NL			An environment model for nonstationary reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Reinforcement learning in nonstationary environments is generally regarded as an important and yet difficult problem. This paper partially addresses the problem by formalizing a subclass of nonstationary environments. The environment model, called hidden-mode Markov decision process (HM-MDP), assumes that environmental changes are always confined to a small number of hidden modes. A mode basically indexes a Markov decision process (MDP) and evolves with time according to a Markov chain. While HM-MDP is a special case of partially observable Markov decision processes (POMDP), modeling an HM-MDP environment via the more general POMDP model unnecessarily increases the problem complexity. A variant of the Baum-Welch algorithm is developed for model learning requiring less data and time.	Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Choi, SPM (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Kowloon, Hong Kong, Peoples R China.							CHOI SPM, 1999, IJCAI 99 WORKSH NEUR; CHRISMAN L, 1992, AAAI 92; CRITES RH, 1996, ADV NEURAL INFORMATI, V8; Dayan P, 1996, MACH LEARN, V25, P5, DOI 10.1023/A:1018357105171; KULLBACK S, 1959, INFORMATION THEORY S; SINGH SP, 1997, ADV NEURAL INFORMATI, V9; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	7	7	7	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						987	993						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700139
C	Schmitt, M		Solla, SA; Leen, TK; Muller, KR		Schmitt, M			Lower bounds on the complexity of approximating continuous functions by sigmoidal neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MULTILAYER FEEDFORWARD NETWORKS; VC DIMENSION	We calculate lower bounds on the size of sigmoidal neural networks that approximate continuous functions. In particular, we show that for the approximation of polynomials the network size has to grow as Omega>(*) over bar *((log k)(1/4)) where k is the degree of the polynomials. This bound is valid for any input dimension, i.e. independently of the number of variables. The result is obtained by introducing a new method employing upper bounds on the Vapnik-Chervonenkis dimension for proving lower bounds on the size of networks that approximate continuous functions.	Ruhr Univ Bochum, Fak Math, Lehrstuhl Math & Informat, D-44780 Bochum, Germany	Ruhr University Bochum	Schmitt, M (corresponding author), Ruhr Univ Bochum, Fak Math, Lehrstuhl Math & Informat, D-44780 Bochum, Germany.	mschmitt@lmi.ruhr-uni-bochum.de						CHUI CK, 1992, J APPROX THEORY, V70, P131, DOI 10.1016/0021-9045(92)90081-X; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; DASGUPTA B, 1993, ADV NEURAL INFORMATI, V5, P615; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Khovanskii A., 1991, FEWNOMIALS, V88; Koiran P, 1997, J COMPUT SYST SCI, V54, P190, DOI 10.1006/jcss.1997.1479; Koiran P, 1996, IEEE CONF COMP COMPL, P81, DOI 10.1109/CCC.1996.507671; KREINOVICH VY, 1991, NEURAL NETWORKS, V4, P381, DOI 10.1016/0893-6080(91)90074-F; LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5; Maass W, 1997, ADV NEUR IN, V9, P211; Mhaskar HN, 1996, NEURAL COMPUT, V8, P164, DOI 10.1162/neco.1996.8.1.164; WARREN HE, 1968, T AM MATH SOC, V133, P167, DOI 10.2307/1994937	16	7	7	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						328	334						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700047
C	Ikeda, S; Amari, S; Nakahara, H		Kearns, MS; Solla, SA; Cohn, DA		Ikeda, S; Amari, S; Nakahara, H			Convergence of the Wake-Sleep algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NEURAL NETWORKS; EM	The W-S (Wake-Sleep) algorithm is a simple learning rule for the models with hidden variables. It is shown that this algorithm can be applied to a factor analysis model which is a linear version of the Helmholtz machine. But even for a factor analysis model, the general convergence is not proved theoretically. In this article, we describe the geometrical understanding of the W-S algorithm in contrast with the EM (Expectation-Maximization) algorithm and the em algorithm. As the result, we prove the convergence of the W-S algorithm for the factor analysis model. We also show the condition for the convergence in general models.	PRESTO, JST, Wako, Saitama 3510198, Japan	Japan Science & Technology Agency (JST)	Ikeda, S (corresponding author), PRESTO, JST, Wako, Saitama 3510198, Japan.	shiro@brain.riken.go.jp; amari@brain.riken.go.jp; hiro@brain.riken.go.jp	Ikeda, Shiro/E-1736-2016	Ikeda, Shiro/0000-0002-2462-1448				Amari S., 1985, LECT NOTES STAT, V28; Amari SI, 1995, NEURAL NETWORKS, V8, P1379, DOI 10.1016/0893-6080(95)00003-8; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; GEOFFREY JM, 1997, EM ALGORITHM EXTENSI; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Neal RM, 1997, NEURAL COMPUT, V9, P1781, DOI 10.1162/neco.1997.9.8.1781	7	7	7	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						239	245						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700034
C	Williams, CKI; Adams, NJ		Kearns, MS; Solla, SA; Cohn, DA		Williams, CKI; Adams, NJ			DTs: Dynamic trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				MULTISCALE	In this paper we introduce a new class of image models, which rye call dynamic trees or DTs. A dynamic tree model specifies a prior over a large number of trees, each one of which is a tree-structured belief net (TSBN). Experiments show that DTs are capable of generating images that are less blocky, and the models have better translation invariance properties than a fixed, "balanced" TSBN. me also show that Simulated Annealing is effective at finding trees which have high posterior probability.	Inst Adapt & Neural Computat, Div Informat, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Williams, CKI (corresponding author), Inst Adapt & Neural Computat, Div Informat, 5 Forrest Hill, Edinburgh EH1 2QL, Midlothian, Scotland.							BOUMAN CA, 1994, IEEE T IMAGE PROCESS, V3, P162, DOI 10.1109/83.277898; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; LUETTGEN MR, 1995, IEEE T IMAGE PROCESS, V4, P194, DOI 10.1109/83.342185; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4	4	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						634	640						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700090
C	Liu, SC		Jordan, MI; Kearns, MJ; Solla, SA		Liu, SC			Silicon retina with adaptive filtering properties	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper describes a small, compact circuit that captures the temporal and adaptation properties both of the photoreceptor and of the laminar layers of the fly. This circuit uses only six transistors and two capacitors. It is operated in the subthreshold domain. The circuit maintains a high transient gain by using adaptation to the background intensity as a form of gain control. The adaptation time constant of the circuit can be controlled via an external bias. Its temporal filtering properties change with the background intensity or signal-to-noise conditions. The frequency response of the circuit shows that in the frequency range of 1 to 100 Hz, the circuit response goes from highpass filtering under high light levels to lowpass filtering under low light levels (i.e., when the signal-to-noise ratio is low). A chip with 20x20 pixels has been fabricated in 1.2 mu m ORBIT CMOS nwell technology.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Liu, SC (corresponding author), CALTECH, 136-93, Pasadena, CA 91125 USA.								0	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						712	718						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700101
C	Oh, JH; Seung, HS		Jordan, MI; Kearns, MJ; Solla, SA		Oh, JH; Seung, HS			Learning generative models with the up-propagation algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Up-propagation is an algorithm for inverting and learning neural network generative models. Sensory input is processed by inverting a model that generates patterns from hidden variables using top-down connections. The inversion process is iterative, utilizing a negative feedback loop that depends on an error signal propagated by bottom-up connections. The error signal is also used to learn the generative model from examples. The algorithm is benchmarked against principal component analysis in experiments on images of handwritten digits.	AT&T Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T; Nokia Corporation; Nokia Bell Labs	Oh, JH (corresponding author), AT&T Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.								0	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						605	611						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700086
C	Rao, RPN		Jordan, MI; Kearns, MJ; Solla, SA		Rao, RPN			Correlates of attention in a model of dynamic visual recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Given a set of objects in the visual field, how does the the visual system learn to attend to a particular object of interest while ignoring the rest? How are occlusions and background clutter so effortlessly discounted for when recognizing a familiar object? In this paper, we attempt to answer these questions in the context of a Kalman filter-based model of visual recognition that has previously proved useful in explaining certain neurophysiological phenomena such as endstopping and related extra-classical receptive field effects in the visual cortex. By using results from the field of robust statistics, we describe an extension of the Kalman filter model that can handle multiple objects in the visual field. The resulting robust Kalman filter model demonstrates how certain forms of attention can be viewed as an emergent property of the interaction between top-down expectations and bottom-up signals. The model also suggests functional interpretations of certain attention-related effects that have been observed in visual cortical neurons. Experimental results are provided to help demonstrate the ability of the model to perform robust segmentation and recognition of objects and image sequences in the presence of varying degrees of occlusions and clutter.	Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of Rochester	Rao, RPN (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.			Rao, Rajesh P. N./0000-0003-0682-8952					0	7	8	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						80	86						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700012
C	Sommer, FT; Palm, G		Jordan, MI; Kearns, MJ; Solla, SA		Sommer, FT; Palm, G			Bidirectional retrieval from associative memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Similarity based fault tolerant retrieval in neural associative memories (NAM) has not lead to wiedespread applications. A drawback of the efficient Willshaw model for sparse patterns [Ste61, WBLH69], is that the high asymptotic information capacity is of little practical use because of high cross talk noise arising in the retrieval for finite sizes. Here a new bidirectional iterative retrieval method for the Willshaw model is presented, called crosswise bidirectional (CB) retrieval, providing enhanced performance. We discuss its asymptotic capacity limit, analyze the first step, and compare it in experiments with the Willshaw model. Applying the very efficient CB memory model either in information retrieval systems or as a functional model for reciprocal cortico-cortical pathways requires more than robustness against random noise in the input: Our experiments show also the segmentation ability of CB-retrieval with addresses containing the superposition of pattens, provided even at high memory load.	Univ Ulm, Dept Neural Informat Proc, D-89069 Ulm, Germany	Ulm University	Sommer, FT (corresponding author), Univ Ulm, Dept Neural Informat Proc, D-89069 Ulm, Germany.								0	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						675	681						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700096
C	Lin, JK; Grier, DG; Cowan, JD		Mozer, MC; Jordan, MI; Petsche, T		Lin, JK; Grier, DG; Cowan, JD			Source separation and density estimation by faithful equivariant SOM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We couple the tasks of source separation and density estimation by extracting the local geometrical structure of distributions obtained from mixtures of statistically independent sources. Our modifications of the self-organizing map (SOM) algorithm results in purely digital learning rules which perform non-parametric histogram density estimation. The non-parametric nature of the separation allows for source separation of non-linear mixtures. An anisotropic coupling is introduced into our SOM with the role of aligning the network locally with the independent component contours. This approach provides an exact verification condition for source separation with no prior on the source distributions.			Lin, JK (corresponding author), UNIV CHICAGO,DEPT PHYS,CHICAGO,IL 60637, USA.		Grier, David G/C-5761-2008	Grier, David G/0000-0002-4382-5139					0	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						536	542						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00076
C	Opper, M; Winther, O		Mozer, MC; Jordan, MI; Petsche, T		Opper, M; Winther, O			A mean field algorithm for Bayes learning in large feed-forward neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present an algorithm which is expected to realise Bayes optimal predictions in large feed-forward networks. It is based on mean field methods developed within statistical mechanics of disordered systems. We give a derivation for the single layer perceptron and show that the algorithm also provides a leave-one-out cross-validation test of the predictions. Simulations show excellent agreement with theoretical results of statistical mechanics.			Opper, M (corresponding author), UNIV WURZBURG,INST THEORET PHYS,HUBLAND,D-97074 WURZBURG,GERMANY.								0	7	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						225	231						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00032
C	Gold, S; Rangarajan, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Gold, S; Rangarajan, A			Softassign versus Softmax: Benchmarks in combinatorial optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						YALE UNIV,DEPT COMP SCI,NEW HAVEN,CT 06520	Yale University			Rangarajan, Anand/A-8652-2009	Rangarajan, Anand/0000-0001-8695-8436					0	7	7	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						626	632						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00089
C	Lin, T; Horne, BG; Tino, P; Giles, CL		Touretzky, DS; Mozer, MC; Hasselmo, ME		Lin, T; Horne, BG; Tino, P; Giles, CL			Learning long-term dependencies is not as difficult with NARX networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						PRINCETON UNIV,DEPT ELECT ENGN,PRINCETON,NJ 08540	Princeton University			Tino, Peter/Z-5748-2019	Giles, C Lee/0000-0002-1931-585X; Tino, Peter/0000-0003-2330-128X					0	7	7	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						577	583						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00082
C	Liu, SC; Boahen, K		Touretzky, DS; Mozer, MC; Hasselmo, ME		Liu, SC; Boahen, K			Adaptive retina with center-surround receptive field	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CALTECH,COMPUTAT & NEURAL SYST,PASADENA,CA 91125	California Institute of Technology									0	7	7	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						678	684						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00096
C	AHMAD, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		AHMAD, S			VISIT - A NEURAL MODEL OF COVERT VISUAL-ATTENTION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	7	10	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						420	427						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00052
C	DAYAN, P; GOODHILL, G		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DAYAN, P; GOODHILL, G			PERTURBING HEBBIAN RULES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Goodhill, Geoffrey J/D-9984-2013						0	7	7	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						19	26						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00003
C	HILD, H; FEULNER, J; MENZEL, W		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HILD, H; FEULNER, J; MENZEL, W			HARMONET - A NEURAL NET FOR HARMONIZING CHORALES IN THE STYLE OF BACH,J.S.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	7	7	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						267	274						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00033
C	MOORE, AW		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MOORE, AW			FAST, ROBUST ADAPTIVE-CONTROL BY LEARNING ONLY FORWARD MODELS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	7	7	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						571	578						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00070
C	OBERMAYER, K; SCHULTEN, K; BLASDEL, GG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		OBERMAYER, K; SCHULTEN, K; BLASDEL, GG			A COMPARISON BETWEEN A NEURAL NETWORK MODEL FOR THE FORMATION OF BRAIN MAPS AND EXPERIMENTAL-DATA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	7	7	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						83	90						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00011
C	PRESCOTT, TJ; MAYHEW, JEW		MOODY, JE; HANSON, SJ; LIPPMANN, RP		PRESCOTT, TJ; MAYHEW, JEW			OBSTACLE AVOIDANCE THROUGH REINFORCEMENT LEARNING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Prescott, Tony J/A-7574-2008	Prescott, Tony J/0000-0003-4927-5390					0	7	7	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						523	530						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00064
C	ZHAO, Y; ATKESON, CG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ZHAO, Y; ATKESON, CG			SOME APPROXIMATION PROPERTIES OF PROJECTION PURSUIT LEARNING NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	7	7	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						936	943						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00115
C	Dai, ZW; Shrivastava, A		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Dai, Zhenwei; Shrivastava, Anshumali			Adaptive Learned Bloom Filter (Ada-BF): Efficient Utilization of the Classifier with Application to Real-Time Information Filtering on the Web	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Recent work suggests improving the performance of Bloom filter by incorporating a machine learning model as a binary classifier. However, such learned Bloom filter does not take full advantage of the predicted probability scores. We propose new algorithms that generalize the learned Bloom filter by using the complete spectrum of the score regions. We prove our algorithms have lower false positive rate (FPR) and memory usage compared with the existing approaches to learned Bloom filter. We also demonstrate the improved performance of our algorithms on real-world information filtering tasks over the web(1).	[Dai, Zhenwei] Rice Univ, Dept Stat, Houston, TX 77005 USA; [Shrivastava, Anshumali] Rice Univ, Dept Comp Sci, Houston, TX 77005 USA	Rice University; Rice University	Dai, ZW (corresponding author), Rice Univ, Dept Stat, Houston, TX 77005 USA.	zhenwei.dai@rice.edu; anshumali@rice.edu			National Science Foundation [IIS-1652131, BIGDATA-1838177]; AFOSRYIP [FA9550-18-1-0152]; ONR DURIP Grant; ONR BRC grant on Randomized Numerical Linear Algebra	National Science Foundation(National Science Foundation (NSF)); AFOSRYIP(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR DURIP Grant; ONR BRC grant on Randomized Numerical Linear Algebra	This work was supported by National Science Foundation IIS-1652131, BIGDATA-1838177, AFOSRYIP FA9550-18-1-0152, ONR DURIP Grant, and the ONR BRC grant on Randomized Numerical Linear Algebra.	Allcott H, 2017, J ECON PERSPECT, V31, P211, DOI 10.1257/jep.31.2.211; BLOOM BH, 1970, COMMUN ACM, V13, P422, DOI 10.1145/362686.362692; Broder A., 2002, INTERNET MATH, V1; Bruck J, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P2304, DOI 10.1109/ISIT.2006.261978; Canetti R., 2020, ARXIVORG200313670; Carter L., 1978, STOC 78, P59; Dillinger P. C., 2004, FORMAL METHODS COMPU, P370; Feinstein L, 2003, DARPA INFORMATION SURVIVABILITY CONFERENCE AND EXPOSITION, VOL I, PROCEEDINGS, P303, DOI 10.1109/discex.2003.1194894; Ferguson C, 2014, COLLEGIAN, V21, P89, DOI 10.1016/j.colegn.2014.03.002; Hsu C.-Y., 2019, ICLR, P1; Kirsch A, 2006, SIAM PROC S, P41; Kleinberg J, 2003, DATA MIN KNOWL DISC, V7, P373, DOI 10.1023/A:1024940629314; Kraska T, 2018, INT CONF MANAGE DATA, P489, DOI 10.1145/3183713.3196909; Mitzenmacher M, 2002, IEEE ACM T NETWORK, V10, P604, DOI 10.1109/TNET.2002.803864; Mitzenmacher Michael, 2018, P ANN C NEUR INF PRO, P464; Ozturk P, 2015, P ANN HICSS, P2406, DOI 10.1109/HICSS.2015.288; Rae Jack W, 2019, ARXIV190604304	17	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000039
C	Foster, DJ; Gentile, C; Mohri, M; Zimmert, J		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Foster, Dylan J.; Gentile, Claudio; Mohri, Mehryar; Zimmert, Julian			Adapting to Misspecification in Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				ALGORITHMS	A major research direction in contextual bandits is to develop algorithms that are computationally efficient, yet support flexible, general-purpose function approximation. Algorithms based on modeling rewards have shown strong empirical performance, yet typically require a well-specified model, and can fail when this assumption does not hold. Can we design algorithms that are efficient and flexible, yet degrade gracefully in the face of model misspecification? We introduce a new family of oracle-efficient algorithms for epsilon-misspecified contextual bandits that adapt to unknown model misspecification-both for finite and infinite action settings. Given access to an online oracle for square loss regression, our algorithm attains optimal regret and-in particular-optimal dependence on the misspecification level, with no prior knowledge. Specializing to linear contextual bandits with infinite actions in d dimensions, we obtain the first algorithm that achieves the optimal (O) over tilde (d root T + epsilon root dT) regret bound for unknown epsilon. On a conceptual level, our results are enabled by a new optimization-based perspective on the regression oracle reduction framework of Foster and Rakhlin [20], which we believe will be useful more broadly.	[Foster, Dylan J.] MIT, Cambridge, MA 02139 USA; [Gentile, Claudio; Mohri, Mehryar; Zimmert, Julian] Google Res, Menlo Pk, CA USA; [Mohri, Mehryar] Courant Inst Math Sci, New York, NY USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Foster, DJ (corresponding author), MIT, Cambridge, MA 02139 USA.	dylanf@mit.edu; cgentile@google.com; mohri@google.com; zimmert@google.com			NSF TRIPODS grant [1740751]	NSF TRIPODS grant	DF acknowledges the support of NSF TRIPODS grant #1740751. We thank Teodor Marinov and Alexander Rakhlin for discussions on related topics.	Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abbasi-Yadkori Yasin, 2012, JMLR WORKSHOP C P, P1; Abe N, 2003, ALGORITHMICA, V37, P263, DOI 10.1007/s00453-003-1038-1; Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P3; Abernethy J. D., 2015, ADV NEURAL INFORM PR, P2197; Agarwal A., 2016, ARXIV160603966; Agarwal A., 2012, P 15 INT C ART INT S; Agarwal Alekh, 2017, C LEARN THEOR, P12; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Audibert J.-Y., 2009, P COLT; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bogunovic I., 2020, P 23 INT C ART INT S; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Crammer K, 2013, MACH LEARN, V90, P347, DOI 10.1007/s10994-012-5321-8; Du S.S., 2019, ARXIV191003016; Dudik M., 2011, UNCERTAINTY ARTIFICI; Foster D. J., 2020, ARXIV201003104; Foster D. J., 2020, INT C MACH LEARN ICM; Foster DJ, 2019, ADV NEUR IN, V32; Foster DJ, 2018, PR MACH LEARN RES, V80; Gaillard P., 2015, C LEARN THEOR, P764; Ghosh Aritra, 2017, AAAI; Kakade S., 2011, ADV NEURAL INFORM PR; Khachiyan L. G., 1990, COMPLEXITY APPROXIMA; Kumar P, 2005, J OPTIMIZ THEORY APP, V126, P1, DOI 10.1007/s10957-005-2653-6; Langford J., 2008, ADV NEURAL INFORM PR, P817; Lattimore T., 2019, ARXIV191107676; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Luo H., 2018, P 31 C LEARN THEOR, V75, P1739; Lykouris T, 2018, ACM S THEORY COMPUT, P114, DOI 10.1145/3188745.3188918; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Pacchiano A., 2020, NEURAL INFORM PROCES; Sigurdsson GA, 2017, IEEE I CONF COMP VIS, P2156, DOI 10.1109/ICCV.2017.235; Simchi-Levi David, 2020, MATH OPER RES; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sui YA, 2015, PR MACH LEARN RES, V37, P997; Syrgkanis V, 2016, PR MACH LEARN RES, V48; Tewari A., 2017, MOBILE HLTH, P495; Todd MJ, 2007, DISCRETE APPL MATH, V155, P1731, DOI 10.1016/j.dam.2007.02.013; Valko M., 2013, P 20 9 C UNCERTAINTY, P654; Xu Y., 2020, ARXIV200707876; Zanette A., 2020, ARXIV200300153; Zimmert J, 2019, PR MACH LEARN RES, V89, P467	47	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000033
C	Arjona-Medina, JA; Gillhofer, M; Widrich, M; Unterthiner, T; Brandstetter, J; Hochreiter, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arjona-Medina, Jose A.; Gillhofer, Michael; Widrich, Michael; Unterthiner, Thomas; Brandstetter, Johannes; Hochreiter, Sepp			RUDDER: Return Decomposition for Delayed Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose RUDDER, a novel reinforcement learning approach for delayed rewards in finite Markov decision processes (MDPs). In MDPs the Q-values are equal to the expected immediate reward plus the expected future rewards. The latter are related to bias problems in temporal difference (TD) learning and to high variance problems in Monte Carlo (MC) learning. Both problems are even more severe when rewards are delayed. RUDDER aims at making the expected future rewards zero, which simplifies Q-value estimation to computing the mean of the immediate reward. We propose the following two new concepts to push the expected future rewards toward zero. (i) Reward redistribution that leads to return-equivalent decision processes with the same optimal policies and, when optimal, zero expected future rewards. (ii) Return decomposition via contribution analysis which transforms the reinforcement learning task into a regression task at which deep learning excels. On artificial tasks with delayed rewards, RUDDER is significantly faster than MC and exponentially faster than Monte Carlo Tree Search (MCTS), TD(A), and reward shaping approaches. At Atari games, RUDDER on top of a Proximal Policy Optimization (PPO) baseline improves the scores, which is most prominent at games with delayed rewards. Source code is available at https //github.com/ml-jku/rudder and demonstration videos at https://goo.gl/EQerZV.	[Arjona-Medina, Jose A.; Gillhofer, Michael; Widrich, Michael; Unterthiner, Thomas; Brandstetter, Johannes] Johannes Kepler Univ Linz, Inst Machine Learning, LIT AI Lab, Linz, Austria; [Hochreiter, Sepp] Inst Adv Res Artificial Intelligence IARAI, Vienna, Austria	Johannes Kepler University Linz	Arjona-Medina, JA (corresponding author), Johannes Kepler Univ Linz, Inst Machine Learning, LIT AI Lab, Linz, Austria.		Hochreiter, Sepp/AAI-5904-2020	Hochreiter, Sepp/0000-0001-7449-2528	NVIDIA Corporation; Merck KGaA; Audi.JKU Deep Learning Center; Audi Electronic Venture GmbH; Janssen Pharmaceutica; TGW Logistics Group; ZF Friedrichshafen AG; UCB S.A.; FFG grant [871302]; LIT grant DeepToxGen; AI-SNN	NVIDIA Corporation; Merck KGaA; Audi.JKU Deep Learning Center; Audi Electronic Venture GmbH; Janssen Pharmaceutica(Johnson & JohnsonJohnson & Johnson USAJanssen Biotech Inc); TGW Logistics Group; ZF Friedrichshafen AG; UCB S.A.(UCB Pharma SA); FFG grant; LIT grant DeepToxGen; AI-SNN	This work was supported by NVIDIA Corporation, Merck KGaA, Audi.JKU Deep Learning Center, Audi Electronic Venture GmbH, Janssen Pharmaceutica (madeSMART), TGW Logistics Group, ZF Friedrichshafen AG, UCB S.A., FFG grant 871302, LIT grant DeepToxGen, and AI-SNN.	[Anonymous], THESIS; Arras L., 2019, EXPLAINING INTERPRET, P211, DOI [10.1007/978-3-030-28954-611.https://doi.org/10.1007/978-3-030-28954-611, DOI 10.1007/978-3-030-28954-611.HTTPS://DOI.ORG/10.1007/978-3-030-28954-611, DOI 10.1007/978-3-030-28954-6_11]; Aytar Y., 2018, 180511592 ARXIV; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bakker B, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON APPROXIMATE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P127, DOI 10.1109/ADPRL.2007.368179; Barreto A, 2018, PR MACH LEARN RES, V80; Barto A. G., 2015, HDB LEARNING APPROXI, P45; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Berthelot D, 2017, 170310717 ARXIV; Bolton W., 2015, INSTRUMENTATION CONT, P99; Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Brockman G., 2016, 160601540 ARXIV; Edwards A. D., 2018, 180310227 ARXIV; Fu J., 2018, 6 INT C LEARN REPR I; Goyal A., 2018, 180400379 ARXIV; Harutyunyan A., 2019, ADV NEURAL INFORM PR, P12467; Hernandez-Leal P., 2018, 181005587 ARXIV; Hochreiter S., 1990, IMPLEMENTIERUNG ANWE; Hung C., 2018, 181006721 ARXIV; Hung CC, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-13073-w; Karmakar P., 2017, MATH OPERATIONS RES; Ke N. R., 2018, ADV NEURAL INFORM PR, P7640; Long-Ji Lin, 1993, THESIS; MUNRO PW, 1987, P 9 ANN C COGN SCI S, P165; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Peters J., 2007, P 24 INT C MACHINE L, P745; Pohlen T., 2018, 180511593 ARXIV; Robinson T., 1989, P 11 C COGN SCI SOC, P836; Sahni Himanshu, 2018, REINFORCEMENT LEARNI; Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3; Schaul T., 2015, 151105952 ARXIV; Schmidhuber J, 1990, MAKING WORLD DIFFERE; Schmidhuber J, 1991, ADV NEURAL INFORM PR, P500; Schmidhuber J., 2019, 191202875 ARXIV; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Schulman J., 2015, 4 INT C LEARN REPR I; Schulman J., 2018, 170706347 ARXIV; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559; Srivastava R. K., 2019, 191202877 ARXIV; Sundararajan M., 2017, 170301365 ARXIV; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Trott A., 2019, ADV NEURAL INFORM PR, P10376; Werbos P. J., 1990, NEURAL NETWORKS CONT, P67; Wiewiora E, 2003, P 20 INT C MACH LEAR, P792	50	6	6	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905024
C	Bakhtin, A; van der Maaten, L; Johnson, J; Gustafson, L; Girshick, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bakhtin, Anton; van der Maaten, Laurens; Johnson, Justin; Gustafson, Laura; Girshick, Ross			PHYRE: A New Benchmark for Physical Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CAPUCHIN MONKEYS	Understanding and reasoning about physics is an important ability of intelligent agents. We develop the PHYRE benchmark for physical reasoning that contains a set of simple classical mechanics puzzles in a 2D physical environment. The benchmark is designed to encourage the development of learning algorithms that are sample-efficient and generalize well across puzzles. We test several modern learning algorithms on PHYRE and find that these algorithms fall short in solving the puzzles efficiently. We expect that PHYRE will encourage the development of novel sample-efficient agents that learn efficient but useful models of physics.	[Bakhtin, Anton; van der Maaten, Laurens; Johnson, Justin; Gustafson, Laura; Girshick, Ross] Facebook AI Res, Menlo Pk, CA 94025 USA	Facebook Inc	Bakhtin, A (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	yolo@fb.com; lvdmaaten@fb.com; jcjohns@fb.com; lgustafson@fb.com; rbg@fb.com						Allen K., 2019, ARXIV190706920; [Anonymous], 2015, NEURIPS; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Battaglia P., 2013, PNAS; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bird CD, 2009, CURR BIOL, V19, P1410, DOI 10.1016/j.cub.2009.07.033; Cheke LG, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040574; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Davis E., 2006, PHYS REASONING; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Groth Oliver, 2018, ECCV; He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heinze-Deml C, 2018, J CAUSAL INFERENCE, V6, DOI 10.1515/jci-2017-0016; Henaff Mikael, 2017, ARXIV171104994; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hudson Drew Arad, 2018, INT C LEARN REPR, P2; Janner M., 2019, P 7 INT C LEARNING R, P1; Jelbert SA, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0092895; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kingma D.P, P 3 INT C LEARNING R; Langford J., 2008, ADV NEURAL INFORM PR, P817; Lerer A, 2016, PR MACH LEARN RES, V48; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li Wenbin, 2016, ARXIV160400066; Loshchilov I., 2016, ARXIV; Mao Jiayuan, 2019, ARXIV190412584; Martin-Ordas G, 2008, ANIM COGN, V11, P423, DOI 10.1007/s10071-007-0132-1; MCCLOSKEY M, 1983, J EXP PSYCHOL LEARN, V9, P146, DOI 10.1037/0278-7393.9.1.146; MCCLOSKEY M, 1980, SCIENCE, V210, P1139, DOI 10.1126/science.210.4474.1139; MCCLOSKEY M, 1983, J EXP PSYCHOL LEARN, V9, P636, DOI 10.1037/0278-7393.9.4.636; Mirza M., 2017, ICLR; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Peters J, 2014, J MACH LEARN RES, V15, P2009; Piloto L., 2018, ARXIV180401128; Povinelli DJ, 2000, FOLK PHYS APES CHIMP; Riochet Ronan, 2018, ARXIV180307616; Riquelme Carlos, 2018, ARXIV180209127; Santoro A, 2017, ADV NEUR IN, V30; Savva M, 2019, IEEE I CONF COMP VIS, P9338, DOI 10.1109/ICCV.2019.00943; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Smith M., 2006, AM ASS ARTIFICIAL IN; Srinivas N., 2009, P 27 INT C MACHINE L, P1015; TAYLOR AH, 2008, P ROY SOC B, V276, P247; Teschke I, 2011, ANIM COGN, V14, P555, DOI 10.1007/s10071-011-0390-9; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; VISALBERGHI E, 1994, J COMP PSYCHOL, V108, P15, DOI 10.1037/0735-7036.108.1.15; VISALBERGHI E, 1989, PRIMATES, V30, P511, DOI 10.1007/BF02380877; Visalberghi Elisabetta, 1996, P57; Wenbin Li, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2606, DOI 10.1109/ICRA.2017.7989304; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Winograd Terry, 1971, TECHNICAL REPORT; Wu JJ, 2017, ADV NEUR IN, V30; Wu Y, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8351296; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang Renqiao, 2016, COGSCI	61	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305012
C	Balle, B; Barthe, G; Gaboardi, M; Geumlek, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balle, Borja; Barthe, Gilles; Gaboardi, Marco; Geumlek, Joseph			Privacy Amplification by Mixing and Diffusion Mechanisms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A fundamental result in differential privacy states that the privacy guarantees of a mechanism are preserved by any post-processing of its output. In this paper we investigate under what conditions stochastic post-processing can amplify the privacy of a mechanism. By interpreting post-processing as the application of a Markov operator, we first give a series of amplification results in terms of uniform mixing properties of the Markov process defined by said operator. Next we provide amplification bounds in terms of coupling arguments which can be applied in cases where uniform mixing is not available. Finally, we introduce a new family of mechanisms based on diffusion processes which are closed under post-processing, and analyze their privacy via a novel heat flow argument. On the applied side, we generalize the analysis of "privacy amplification by iteration" in Noisy SGD and show it admits an exponential improvement in the strongly convex case, and study a mechanism based on the Ornstein-Uhlenbeck diffusion process which contains the Gaussian mechanism with optimal post-processing on bounded inputs as a special case.	[Barthe, Gilles] IMDEA Software Inst, MPI Secur & Privacy, Madrid, Spain; [Gaboardi, Marco] Boston Univ, Boston, MA 02215 USA; [Geumlek, Joseph] Univ Calif San Diego, La Jolla, CA 92093 USA	Boston University; University of California System; University of California San Diego	Barthe, G (corresponding author), IMDEA Software Inst, MPI Secur & Privacy, Madrid, Spain.				NSF [CCF-1718220]	NSF(National Science Foundation (NSF))	MG was partially supported by NSF grant CCF-1718220.	[Anonymous], CORR; Bakry Dominique, 2013, ANAL ANDGEOMETRY MAR, V348; Balle B, 2018, ARXIV180701647; Bartolomeo G, 2013, IDENTIFICATION AND MANAGEMENT OF DISTRIBUTED DATA: NGN, CONTENT-CENTRIC NETWORKS AND THE WEB, P49; Beimel A, 2014, MACH LEARN, V94, P401, DOI 10.1007/s10994-013-5404-1; Beimel Amos, 2013, INNOVATIONS THEORETI, P97, DOI [10.1145/2422436.2422450, DOI 10.1145/2422436.2422450]; Bubeck S., 2015, FDN TRENDS MACHINE L; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chaudhuri K, 2006, LECT NOTES COMPUT SC, V4117, P198; Cheu A, 2019, LECT NOTES COMPUT SC, V11476, P375, DOI 10.1007/978-3-030-17653-2_13; COHEN JE, 1993, LINEAR ALGEBRA APPL, V179, P211, DOI 10.1016/0024-3795(93)90331-H; Del Moral P, 2003, PROBAB THEORY REL, V126, P395, DOI 10.1007/s00440-003-0270-6; Dobrushin RL, 1956, THEOR PROBAB APPL, V1, P65, DOI DOI 10.1137/1101006; DOEBLIN W, 1937, B MATH SOC ROUM SCI, V39, P3; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Erlingsson U., 2019, SODA, P2468; Feldman V, 2018, ANN IEEE SYMP FOUND, P521, DOI 10.1109/FOCS.2018.00056; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; LEVIN D. A., 2017, AM MATH SOC, DOI [10.1090/mbk/107, DOI 10.1090/MBK/107]; Li N., 2012, ASIA CCS, P32, DOI 10.1145/2414456.2414474; Lindvall T., 2002, LECT COUPLING METHOD; Meyn S. P., 2012, MARKOV CHAINS STOCHA; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Nummelin E, 2004, GEN IRREDUCIBLE MARK, V83; Oksendal Bernt, 2003, STOCHASTIC DIFFERENT, P2; Raginsky M, 2016, IEEE T INFORM THEORY, V62, P3355, DOI 10.1109/TIT.2016.2549542; Wang Yu-Xiang, 2019, P 22 INT C ART INT S	27	6	6	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904088
C	Bielski, A; Favaro, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bielski, Adam; Favaro, Paolo			Emergence of Object Segmentation in Perturbed Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a framework to learn object segmentation from a collection of images without any manual annotation. We build on the observation that the location of object segments can be perturbed locally relative to a given background without affecting the realism of a scene. First, we train a generative model of a layered scene. The layered representation consists of a background image, a foreground image and the mask of the foreground. A composite image is then obtained by overlaying the masked foreground image onto the background. The generative model is trained in an adversarial fashion against a discriminator, which forces the generative model to produce realistic composite images. To force the generator to learn a representation where the foreground layer corresponds to an object, we perturb the output of the generative model by introducing a random shift of both the foreground image and mask relative to the background. Because the generator is unaware of the shift before computing its output, it must produce layered representations that are realistic for any such random perturbation. Second, we learn to segment an image by defining an autoencoder consisting of an encoder, which we train, and the pre-trained generator as the decoder, which we fix. The encoder maps an image to the input of the generator, which then outputs a composite image matching the original input image. Because the generator outputs an explicit layered representation of the scene, the encoder learns to detect and segment objects. We demonstrate this framework on real images of several object categories.	[Bielski, Adam; Favaro, Paolo] Univ Bern, Bern, Switzerland	University of Bern	Bielski, A (corresponding author), Univ Bern, Bern, Switzerland.	adam.bielski@inf.unibe.ch; paolo.favaro@inf.unibe.ch						Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; [Anonymous], [No title captured]; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; Burgess Christopher P, 2019, ARXIV190111390; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Eslami SM, 2016, NEURIPS, V1; Ghahramani Zoubin, 2004, UNSUPERVISED LEARNIN, P72, DOI DOI 10.1007/978-3-540-28650-95; Greff K., 2016, ADV NEURAL INFORM PR; Greff K, 2019, PR MACH LEARN RES, V97; Gulrajani I., 2017, INT C NEURAL INF PRO; Gupta V, 2016, 2016 INTERNATIONAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (ICONSIP); Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Hu Ronghang, 2018, P IEEE C COMP VIS PA; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kanezaki Asako, 2018, P IEEE INT C AC SPEE; Karras T., 2018, ARXIV181204948; Karras Tero, 2018, INT C LEARN REPR; Khoreva A., 2017, IEEE C COMP VIS PATT; Kim J, 2015, LECT NOTES COMPUT SC, V9386, P752, DOI 10.1007/978-3-319-25903-1_65; Kingma DP, 2015, INT C LEARN REPR ICL; Kwak H., 2016, ARXIV160705387; Lin T.-Y., 2014, ECCV EUR C COMP VIS; Lutz Sebastian, 2018, BRIT MACH VIS C, P2; Ostyakov Pavel, 2018, ABS181107630 CORR; Remez T, 2018, EUR C COMP VIS ECCV; Rother C., 2004, ACM T GRAPHICS SIGGR; van Steenkiste Sjoerd, 2018, ARXIV181010340; van Steenkiste Sjoerd, 2017, ADV NEURAL INFORM PR, P6691; Wah C., 2011, CALTECHUCSD BIRDS 20; Xia X., 2017, ABS171108506 CORR; Xu Ning, 2017, IEEE C COMP VIS PATT; Yang Jianwei, 2017, INT C LEARN REPR; Yu F., 2015, ARXIVABS150603365 CO	35	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307029
C	Brooks, D; Schwander, O; Barbaresco, F; Schneider, JY; Cord, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Brooks, Daniel; Schwander, Olivier; Barbaresco, Frederic; Schneider, Jean-Yves; Cord, Matthieu			Riemannian batch normalization for SPD neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COVARIANCE; MATRICES; RADAR	Covariance matrices have attracted attention for machine learning applications due to their capacity to capture interesting structure in the data. The main challenge is that one needs to take into account the particular geometry of the Riemannian manifold of symmetric positive definite (SPD) matrices they belong to. In the context of deep networks, several architectures for these matrices have recently been proposed. In our article, we introduce a Riemannian batch normalization (batch-norm) algorithm, which generalizes the one used in Euclidean nets. This novel layer makes use of geometric operations on the manifold, notably the Riemannian barycenter, parallel transport and non-linear structured matrix transformations. We derive a new manifold-constrained gradient descent algorithm working in the space of SPD matrices, allowing to learn the batchnorm layer. We validate our proposed approach with experiments in three different contexts on diverse data types: a drone recognition dataset from radar observations, and on emotion and action recognition datasets from video and motion capture data. Experiments show that the Riemannian batchnorm systematically gives better classification performance compared with leading methods and a remarkable robustness to lack of data.	[Brooks, Daniel; Barbaresco, Frederic; Schneider, Jean-Yves] BU ARC, Thales Land & Air Syst, Limours, France; [Brooks, Daniel; Schwander, Olivier; Cord, Matthieu] Sorbonne Univ, CNRS, LIP6, Paris, France	Thales Group; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	Brooks, D (corresponding author), BU ARC, Thales Land & Air Syst, Limours, France.; Brooks, D (corresponding author), Sorbonne Univ, CNRS, LIP6, Paris, France.							Amari SI, 2016, APPL MATH SCI, V194, P1, DOI 10.1007/978-4-431-55978-8; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arsigny V, 2006, MAGN RESON MED, V56, P411, DOI 10.1002/mrm.20965; Atkinson C., 1981, SANKHYA A, V43, P345; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Barachant A, 2013, NEUROCOMPUTING, V112, P172, DOI 10.1016/j.neucom.2012.12.039; Barachant A, 2012, IEEE T BIO-MED ENG, V59, P920, DOI 10.1109/TBME.2011.2172210; Barbaresco F., 2019, GEOMETRIC STRUCTURES, P333, DOI DOI 10.1007/978-3-030-02520-512; Bhatia R, 2015, POSITIVE DEFINITE MA; Boissonnat J.-D., 2010, DISCRETE COMPUT GEOM, P200; Bonnabel S, 2009, SIAM J MATRIX ANAL A, V31, P1055, DOI 10.1137/080731347; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Brodskii M., 1965, AM MATH SOC TRANSL 2, V47; Brooks DA, 2019, INT CONF ACOUST SPEE, P3672, DOI 10.1109/ICASSP.2019.8683056; Brooks DA, 2018, INT RADAR SYMP PROC; Cavazza J, 2017, IEEE COMPUT SOC CONF, P1251, DOI 10.1109/CVPRW.2017.165; Cencov N. N., 2000, AM MATH SOC; Chakraborty R., 2018, ARXIV180906211CS; Chakraborty R., 2018, ADV NEURAL INFORM PR, V31, P8883; Charon N., 2009, NEW APPROACH TARGET; Chen VC, 2006, IEEE T AERO ELEC SYS, V42, P2, DOI 10.1109/TAES.2006.1603402; Dhall A, 2011, 2011 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCV WORKSHOPS); Dong Z., 2017, AAAI; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Engin M., 2017, ARXIV171104047CS; Frechet M, 1943, REV I INT STATIST, V11, P182; Gao Z., 2017, ARXIV171106540CS; Huang Z., 2016, ARXIV161205877CS; Huang Z., 2016, ARXIV160804233CS; Huang Z., 2016, ARXIV161105742CS; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jaquier N, 2017, IEEE INT C INT ROBOT, P59; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105; Mao Y., COSONET COMPACT 2 OR, P16; Marceau-Caron G, 2017, LECT NOTES COMPUT SC, V10589, P451, DOI 10.1007/978-3-319-68445-1_53; Nielsen F., 2013, MATRIX INFORM GEOMET; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Rao C.R., 1992, BREAKTHROUGHS STAT F, P235, DOI [DOI 10.1007/978-1-4612-0919-5, 10.1007/978-1-4612-0919-5\_16]; Said S., 2015, ARXIV150701760MATHST; Siahkamari A., 2019, ARXIV190511545CSSTAT; Sun K., 2019, ARXIV190304154CSSTAT; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tuzel O, 2006, LECT NOTES COMPUT SC, V3952, P589; Yair O., 2018, PARALLEL TRANSPORT C; Yang L, 2010, EUROP RADAR CONF, P415; Yger F., 2015, ARXIV150203505CS; Yger F., 2013, IEEE INT WORKSH MACH, P1; Zhang T, 2018, IEEE IMAGE PROC, P4098, DOI 10.1109/ICIP.2018.8451626	50	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907018
C	Cai, CX; Li, G; Poor, HV; Chen, YX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cai, Changxiao; Li, Gen; Poor, H. Vincent; Chen, Yuxin			Nonconvex Low-Rank Symmetric Tensor Completion from Noisy Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MATRIX FACTORIZATION; OPTIMIZATION; RECOVERY; DECOMPOSITIONS; ALGORITHMS; MODELS	We study a completion problem of broad practical interest: the reconstruction of a low-rank symmetric tensor from highly incomplete and randomly corrupted observations of its entries. While a variety of prior work has been dedicated to this problem, prior algorithms either are computationally too expensive for large-scale applications, or come with sub-optimal statistical guarantees. Focusing on "incoherent" and well-conditioned tensors of a constant CP rank, we propose a two-stage nonconvex algorithm - (vanilla) gradient descent following a rough initialization- that achieves the best of both worlds. Specifically, the proposed nonconvex algorithm faithfully completes the tensor and retrieves individual tensor factors within nearly linear time, while at the same time enjoying near-optimal statistical guarantees (i.e. minimal sample complexity and optimal l(2) and l(infinity) statistical accuracy). The insights conveyed through our analysis of nonconvex optimization might have implications for other tensor estimation problems.	[Cai, Changxiao; Poor, H. Vincent; Chen, Yuxin] Princeton Univ, Princeton, NJ 08544 USA; [Li, Gen] Tsinghua Univ, Beijing, Peoples R China	Princeton University; Tsinghua University	Cai, CX (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.		Poor, H. Vincent/S-5027-2016; Cai, Changxiao/AAD-9809-2022	Poor, H. Vincent/0000-0002-2062-131X; 	AFOSR YIP award [FA9550-19-1-0030]; ONR [N00014-19-1-2120]; ARO [W911NF-18-1-0303]; NSF [CCF-1907661, IIS-1900140, DMS-1736417]	AFOSR YIP award; ONR(Office of Naval Research); ARO; NSF(National Science Foundation (NSF))	Y. Chen is supported in part by the AFOSR YIP award FA9550-19-1-0030, by the ONR grant N00014-19-1-2120, by the ARO grant W911NF-18-1-0303, by the NSF grants CCF-1907661 and IIS-1900140. H. V. Poor is supported in part by the NSF grant DMS-1736417.	Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Anandkumar Animashree, 2014, ARXIV14025180; [Anonymous], 2017, ARXIV PREPRINT ARXIV; [Anonymous], 2017, ADV NEURAL INF PROCE; [Anonymous], 2016, ARXIV160507051; [Anonymous], IEEE T INFORM THEORY; [Anonymous], 2015, PROBAB THEORY REL; [Anonymous], 2015, ARXIV150903025; Bubeck S., 2015, FDN TRENDS MACHINE L; Cai C., 2019, ARXIV191004267; Cai C., 2019, ARXIV191104436; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen JY, 2017, SCI REP-UK, V7, DOI 10.1038/srep42001; Chen Y., 2019, P NATL ACAD SCI PNAS; Chen YW, 2019, MOL PSYCHIATR, V24, P710, DOI 10.1038/s41380-018-0245-8; Chen YD, 2018, IEEE SIGNAL PROC MAG, V35, P14, DOI 10.1109/MSP.2018.2821706; Chen YX, 2018, COMMUN PUR APPL MATH, V71, P1648, DOI 10.1002/cpa.21760; Chen YX, 2017, COMMUN PUR APPL MATH, V70, P822, DOI 10.1002/cpa.21638; Chen YX, 2014, IEEE T INFORM THEORY, V60, P6576, DOI 10.1109/TIT.2014.2343623; Chi YJ, 2019, IEEE T SIGNAL PROCES, V67, P5239, DOI 10.1109/TSP.2019.2937282; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; Ding L., 2018, ARXIV180307554; Ely G, 2013, SEG TECHNICAL PROGRA, P3639, DOI [10.1190/segam2013-1143.1, DOI 10.1190/SEGAM2013-1143.1]; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Gilboa D., 2018, ARXIV180910313; Goldfarb D, 2014, SIAM J MATRIX ANAL A, V35, P225, DOI 10.1137/130905010; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Hao B., 2018, ARXIV180109326; Hao B., 2019, ARXIV190400479; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hopkins SB, 2016, ACM S THEORY COMPUT, P178, DOI 10.1145/2897518.2897529; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Ji TY, 2016, INFORM SCIENCES, V326, P243, DOI 10.1016/j.ins.2015.07.049; Kasai H, 2016, PR MACH LEARN RES, V48; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kilmer ME, 2013, SIAM J MATRIX ANAL A, V34, P148, DOI 10.1137/110837711; Kim SJ, 2016, PORTL INT CONF MANAG, P2460, DOI 10.1109/PICMET.2016.7806794; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kreimer N, 2013, GEOPHYSICS, V78, pV273, DOI 10.1190/GEO2013-0022.1; Li XT, 2017, AAAI CONF ARTIF INTE, P2210; Liu J, 2013, IEEE T PATTERN ANAL, V35, P208, DOI 10.1109/TPAMI.2012.39; Lounici K, 2014, BERNOULLI, V20, P1029, DOI 10.3150/12-BEJ487; Ma C., 2018, FDN COMPUTATIONAL MA; Montanari A, 2018, COMMUN PUR APPL MATH, V71, P2381, DOI 10.1002/cpa.21748; Mu C, 2014, PR MACH LEARN RES, V32, P73; Pananjady A., 2019, ARXIV190908749; Potechin A., 2017, P 30 C LEARN THEOR C, P1619; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Richard E., 2014, P ADV NEUR INF PROC, P2897; Romera-Paredes B., 2013, ADV NEURAL INFORM PR, V2, P2967; Semerci O, 2014, IEEE T IMAGE PROCESS, V23, P1678, DOI 10.1109/TIP.2014.2305840; Shah D., 2019, ARXIV190801241; Sidiropoulos ND, 2017, IEEE T SIGNAL PROCES, V65, P3551, DOI 10.1109/TSP.2017.2690524; Steinlechner M, 2016, SIAM J SCI COMPUT, V38, pS461, DOI 10.1137/15M1010506; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Tang GG, 2015, PR MACH LEARN RES, V37, P1491; Tomioka R., 2010, ARXIV10100789; WANG W, 2016, ARXIV160905587; Xia D., 2017, ARXIV170206980; Xu YY, 2015, INVERSE PROBL IMAG, V9, P601, DOI 10.3934/ipi.2015.9.601; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795; Yao Q, 2018, ARXIV180708725; Ying JX, 2017, IEEE T SIGNAL PROCES, V65, P3702, DOI 10.1109/TSP.2017.2695566; Yuan M, 2017, IEEE T INFORM THEORY, V63, P6753, DOI 10.1109/TIT.2017.2724549; Yuan M, 2016, FOUND COMPUT MATH, V16, P1031, DOI 10.1007/s10208-015-9269-5; Zhang AR, 2018, IEEE T INFORM THEORY, V64, P7311, DOI 10.1109/TIT.2018.2841377; Zhang CH, 2017, STAT OPTIMAL C UNPUB; ZHANG H, 2017, J MACH LEARN RES, V18, P5164; Zhang ZM, 2017, IEEE T SIGNAL PROCES, V65, P1511, DOI 10.1109/TSP.2016.2639466; Zhong Y., 2018, SIAM J OPTIMIZATION	78	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301081
C	Campbell, T; Li, XL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Campbell, Trevor; Li, Xinglong			Universal Boosting Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION	Boosting variational inference (BVI) approximates an intractable probability density by iteratively building up a mixture of simple component distributions one at a time, using techniques from sparse convex optimization to provide both computational scalability and approximation error guarantees. But the guarantees have strong conditions that do not often hold in practice, resulting in degenerate component optimization problems; and we show that the ad-hoc regularization used to prevent degeneracy in practice can cause BVI to fail in unintuitive ways. We thus develop universal boosting variational inference (UBVI), a BVI scheme that exploits the simple geometry of probability densities under the Hellinger metric to prevent the degeneracy of other gradient-based BVI methods, avoid difficult joint optimizations of both component and weight, and simplify fully-corrective weight optimizations. We show that for any target density and any mixture component family, the output of UBVI converges to the best possible approximation in the mixture family, even when the mixture family is misspecified. We develop a scalable implementation based on exponential family mixture components and standard stochastic optimization techniques. Finally, we discuss statistical benefits of the Hellinger distance as a variational objective through bounds on posterior probability, moment, and importance sampling errors. Experiments on multiple datasets and models show that UBVI provides reliable, accurate posterior approximations.	[Campbell, Trevor; Li, Xinglong] Univ British Columbia, Dept Stat, Vancouver, BC V6T 1Z4, Canada	University of British Columbia	Campbell, T (corresponding author), Univ British Columbia, Dept Stat, Vancouver, BC V6T 1Z4, Canada.	trevor@stat.ubc.ca; xinglong.li@stat.ubc.ca			National Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant; NSERC Discovery Launch Supplement	National Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSERC Discovery Launch Supplement	T. Campbell and X. Li are supported by a National Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant and an NSERC Discovery Launch Supplement.	Alquier Pierre, 2018, ANN STAT; Atchade Y F, 2017, ARXIV170803625; Bardenet R, 2017, J MACH LEARN RES, V18, P1; Betancourt M, 2015, PR MACH LEARN RES, V37, P533; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Campbell T, 2018, PR MACH LEARN RES, V80; Campbell T, 2019, J MACH LEARN RES, V20; Chatterjee S, 2018, ANN APPL PROBAB, V28, P1099, DOI 10.1214/17-AAP1326; CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Chen Yutian, 2010, UAI; Cherief-Abdellatif BE, 2018, ELECTRON J STAT, V12, P2995, DOI 10.1214/18-EJS1475; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dehaene G, 2018, J R STAT SOC B, V80, P199, DOI 10.1111/rssb.12241; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gelman A., 2021, BAYESIAN DATA ANAL, V3rd ed.; Gershman S.J., 2012, P 29 INT C MACH LEAR, P235; Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228; Gorham J, 2015, ADV NEUR IN, V28; Guo F., 2016, ADV NEURAL INFORM PR; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Lawson C. L., 1995, SOLVING LEAST SQUARE; Liu Q, 2016, PR MACH LEARN RES, V48; Locatello F, 2018, PR MACH LEARN RES, V84; Locatello Francesco, 2018, ADV NEURAL INFORM PR; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Miller AC, 2017, PR MACH LEARN RES, V70; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Petersen K.B., 2012, MATRIX COOKBOOK; Pollard D., 2002, CAMBRIDGE SERIES STA; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rosenthal JS., 2004, PROBAB SURV, V1, P20, DOI [10.1214/154957804100000024, DOI 10.1214/154957804100000024]; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; Scott SL, 2016, INT J MANAG SCI ENG, V11, P78, DOI 10.1080/17509653.2016.1142191; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang X., 2016, THESIS; Wang Y, 2019, J LOW FREQ NOISE V A, V38, P1008, DOI 10.1177/1461348418795813; Yang Yun, 2018, ANN STAT; Yao YL, 2018, PR MACH LEARN RES, V80; Zhang T, 2003, IEEE T INFORM THEORY, V49, P682, DOI 10.1109/TIT.2002.808136; Zobay O, 2014, ELECTRON J STAT, V8, P355, DOI 10.1214/14-EJS887	59	6	6	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303047
C	Chen, P; Wu, KY; Chen, J; O'Leary-Roseberry, T; Ghattas, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Peng; Wu, Keyi; Chen, Joshua; O'Leary-Roseberry, Thomas; Ghattas, Omar			Projected Stein Variational Newton: A Fast and Scalable Bayesian Inference Method in High Dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INVERSE PROBLEMS; MCMC; APPROXIMATIONS; ALGORITHMS; REDUCTION; FLOW	We propose a projected Stein variational Newton (pSVN) method for high-dimensional Bayesian inference. To address the curse of dimensionality, we exploit the intrinsic low-dimensional geometric structure of the posterior distribution in the high-dimensional parameter space via its Hessian (of the log posterior) operator and perform a parallel update of the parameter samples projected into a low-dimensional subspace by an SVN method. The subspace is adaptively constructed using the eigenvectors of the averaged Hessian at the current samples. We demonstrate fast convergence of the proposed method, complexity independent of the parameter and sample dimensions, and parallel scalability.	[Chen, Peng; Wu, Keyi; Chen, Joshua; O'Leary-Roseberry, Thomas; Ghattas, Omar] Univ Texas Austin, Oden Inst Computat Engn & Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Chen, P (corresponding author), Univ Texas Austin, Oden Inst Computat Engn & Sci, Austin, TX 78712 USA.	peng@oden.utexas.edu; keyi@oden.utexas.edu; joshua@oden.utexas.edu; tom@oden.utexas.edu; omar@oden.utexas.edu	O'Leary-Roseberry, Thomas/AAY-4375-2020	O'Leary-Roseberry, Thomas/0000-0002-8938-7074				Alain Guillaume, 2019, ARXIV190202366; Bashir O, 2008, INT J NUMER METH ENG, V73, P844, DOI 10.1002/nme.2100; Beskos A, 2017, J COMPUT PHYS, V335, P327, DOI 10.1016/j.jcp.2016.12.041; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Chen P, 2019, J COMPUT PHYS, V385, P163, DOI 10.1016/j.jcp.2019.01.047; Chen P, 2017, COMPUT METHOD APPL M, V327, P147, DOI 10.1016/j.cma.2017.08.016; Chen P, 2015, COMPUT METHOD APPL M, V297, P84, DOI 10.1016/j.cma.2015.08.006; Chen Peng, 2018, ARXIV180910255; Chen Wilson Ye, 2018, ARXIV180310161; Cui TG, 2016, J COMPUT PHYS, V315, P363, DOI 10.1016/j.jcp.2016.03.055; Cui TG, 2016, J COMPUT PHYS, V304, P109, DOI 10.1016/j.jcp.2015.10.008; Detommaso G., 2018, ADV NEURAL INFORM PR, P9187; Ghorbani Behrooz, 2019, ARXIV190110159; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Isaac T, 2015, J COMPUT PHYS, V296, P348, DOI 10.1016/j.jcp.2015.04.047; Liu CY, 2018, AAAI CONF ARTIF INTE, P346; Liu M, 2018, INT SYM COMPUT INTEL, P206, DOI 10.1109/ISCID.2018.10148; Liu Q., 2016, NEURIPS; Martin J, 2012, SIAM J SCI COMPUT, V34, pA1460, DOI 10.1137/110845598; MARZOUK Y., 2016, HDB UNCERTAINTY QUAN, P1; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Petra N, 2014, SIAM J SCI COMPUT, V36, pA1525, DOI 10.1137/130934805; Sagun L, 2016, EIGENVALUES HESSIAN; Schillings C, 2016, ESAIM-MATH MODEL NUM, V50, P1825, DOI 10.1051/m2an/2016005; Schillings C, 2013, INVERSE PROBL, V29, DOI 10.1088/0266-5611/29/6/065011; Schwab C, 2012, INVERSE PROBL, V28, DOI 10.1088/0266-5611/28/4/045003; Spantini A, 2015, SIAM J SCI COMPUT, V37, pA2451, DOI 10.1137/140977308; Stuart AM, 2010, ACTA NUMER, V19, P451, DOI 10.1017/S0962492910000061; Tan BT, 2012, INVERSE PROBL, V28, DOI 10.1088/0266-5611/28/5/055001; Zhuo Jingwei, 2017, ARXIV171104425	32	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906075
C	Chen, XY; Liu, SJ; Xu, KD; Li, XG; Lin, X; Hong, MY; Cox, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Xiangyi; Liu, Sijia; Xu, Kaidi; Li, Xingguo; Lin, Xue; Hong, Mingyi; Cox, David			ZO-AdaMM: Zeroth-Order Adaptive Momentum Method for Black-Box Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	The adaptive momentum method (AdaMM), which uses past gradients to update descent directions and learning rates simultaneously, has become one of the most popular first-order optimization methods for solving machine learning problems. However, AdaMM is not suited for solving black-box optimization problems, where explicit gradient forms are difficult or infeasible to obtain. In this paper, we propose a zeroth-order AdaMM (ZO-AdaMM) algorithm, that generalizes AdaMM to the gradient-free regime. We show that the convergence rate of ZO-AdaMM for both convex and nonconvex optimization is roughly a factor of O(root d) worse than that of the first-order AdaMM algorithm, where d is problem size. In particular, we provide a deep understanding on why Mahalanobis distance matters in convergence of ZO-AdaMM and other AdaMM-type methods. As a byproduct, our analysis makes the first step toward understanding adaptive learning rate methods for nonconvex constrained optimization. Furthermore, we demonstrate two applications, designing per-image and universal adversarial attacks from blackbox neural networks, respectively. We perform extensive experiments on ImageNet and empirically show that ZO-AdaMM converges much faster to a solution of high accuracy compared with 6 state-of-the-art ZO optimization methods.	[Chen, Xiangyi; Hong, Mingyi] Univ Minnesota, Minneapolis, MN 55455 USA; [Liu, Sijia; Cox, David] IBM Res, MIT IBM Watson Lab, Minneapolis, MN USA; [Xu, Kaidi] Northeastern Univ, Boston, MA 02115 USA; [Li, Xingguo] Princeton Univ, Princeton, NJ 08544 USA	University of Minnesota System; University of Minnesota Twin Cities; International Business Machines (IBM); Northeastern University; Princeton University	Chen, XY (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.		Chen, Xiangyi/ABE-9600-2020; Hong, Mingyi/H-6274-2013	Chen, Xiangyi/0000-0003-3908-9797; 	National Science Foundation [CNS-1932351]; NSF [CMMI-172775, CIF-1910385]; ARO [73202-CS]	National Science Foundation(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); ARO	This work is partly supported by National Science Foundation CNS-1932351. M. Hong is supported in part by NSF under Grant CMMI-172775, CIF-1910385 and by ARO under grant 73202-CS.	Audet C, 2006, SIAM J OPTIMIZ, V17, P188, DOI 10.1137/040603371; Balasubramanian K., 2018, ADV NEURAL INFORM PR, P3455; Balles L., 2018, P 35 INT C MACH LEAR, V80, P404; Bernstein J., 2018, ARXIV180204434; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen J., 2018, ARXIV181110828; Chen J., 2018, ARXIV180606763; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen X., 2019, INT C LEARN REPR; Cheng M., 2018, INT C LEARN REPR; Conn AR, 2009, MOS-SIAM SER OPTIMIZ, V8, P1; Conn AR, 2009, SIAM J OPTIMIZ, V20, P387, DOI 10.1137/060673424; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Feurer Matthias, 2015, ADV NEURAL INFORM PR, P2962; Gao  X., 2014, OPTIMIZATION ONLINE, V12; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gu B., 2016, ARXIV161201425; Ilyas A., 2018, P 35 INT C MACH LEAR; Ilyas Andrew, 2018, PRIOR CONVICTIONS BL; Kingma D. P., 2014, ADAM METHOD STOCHAST; Kotthoff L, 2017, J MACH LEARN RES, V18; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Le Digabel S, 2011, ACM T MATH SOFTWARE, V37, DOI 10.1145/1916461.1916468; Lian Xiangru, 2016, P C NEUR INF PROC SY, P3054; Lin J., 2018, ARXIV180408598; Liu L., 2018, ARXIV180511811; Liu S., 2019, INT C LEARN REPR NEW; Liu S., 2018, ADV NEURAL INFORMATI; LIU S, 2018, P 21 INT C ART INT S, V84, P288; Madry A., 2018, ARXIV PREPRINT ARXIV; Nesterov Y., 2015, FUNDATIONS COMPUTATI, V2, P527; Phuong TT, 2019, ARXIV190403590; Powell M. J. D., 2009, NA200906 U CAMBR, P26; POWELL MJD, 1993, MATH APPL, V275, P51; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi S.J., 2018, INT C LEARN REPR; Rios LM, 2013, J GLOBAL OPTIM, V56, P1247, DOI 10.1007/s10898-012-9951-y; Sahu A. K., 2018, ARXIV181003233; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Suya Fnu, 2017, ARXIV171208713; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tu CC, 2018, ARXIV180511770; Vaz AIF, 2009, OPTIM METHOD SOFTW, V24, P669, DOI 10.1080/10556780902909948; WHITLEY D, 1994, STAT COMPUT, V4, P65, DOI 10.1007/BF00175354; Xu Keyulu, 2019, INT C LEARN REPR; Zhou D., 2018, CONVERGENCE ADAPTIVE	51	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307024
C	Chzhen, E; Denis, C; Hebiri, M; Oneto, L; Pontil, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chzhen, Evgenii; Denis, Christophe; Hebiri, Mohamed; Oneto, Luca; Pontil, Massimiliano			Leveraging Labeled and Unlabeled Data for Consistent Fair Binary Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RATES	We study the problem of fair binary classification using the notion of Equal Opportunity. It requires the true positive rate to distribute equally across the sensitive groups. Within this setting we show that the fair optimal classifier is obtained by recalibrating the Bayes classifier by a group-dependent threshold. We provide a constructive expression for the threshold. This result motivates us to devise a plug-in classification procedure based on both unlabeled and labeled datasets. While the latter is used to learn the output conditional probability, the former is used for calibration. The overall procedure can be computed in polynomial time and it is shown to be statistically consistent both in terms of the classification error and fairness measure. Finally, we present numerical experiments which indicate that our method is often superior or competitive with the state-of-the-art methods on benchmark datasets.	[Chzhen, Evgenii; Denis, Christophe; Hebiri, Mohamed] Univ Paris Est, Champs Sur Marne, France; [Chzhen, Evgenii] Univ Paris Sud, Orsay, France; [Oneto, Luca] Univ Pisa, Pisa, Italy; [Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy; [Pontil, Massimiliano] UCL, London, England	Universite Gustave-Eiffel; UDICE-French Research Universities; Universite Paris Saclay; University of Pisa; Istituto Italiano di Tecnologia - IIT; University of London; University College London	Chzhen, E (corresponding author), Univ Paris Est, Champs Sur Marne, France.; Chzhen, E (corresponding author), Univ Paris Sud, Orsay, France.	evgenii.chzhen@math.u-psud.fr; christophe.denis@u-pem.fr; mohamed.hebiri@u-pem.fr; luca.oneto@unipi.it; massimiliano.pontil@iit.it		Oneto, Luca/0000-0002-8445-395X	SAP SE; Amazon AWS Machine Learning Research Award; CISCO; Labex Bezout of Universite Paris-Est	SAP SE; Amazon AWS Machine Learning Research Award; CISCO; Labex Bezout of Universite Paris-Est	This work was supported in part by SAP SE, by the Amazon AWS Machine Learning Research Award, by CISCO, and by the Labex Bezout of Universite Paris-Est.	Adebayo J., 2016, C FAIRN ACC TRANSP M; Agarwal Alekh, 2018, ARXIV180302453; Arlot S., 2014, ARXIV14073939; Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Barocas Solon, 2018, FAIRNESS MACHINE LEA; Beutel A., 2017, C FAIRN ACC TRANSP M; Breiman L., 2004, TECHNICAL REPORT; Calders T., 2009, IEEE INT C DAT MIN; Calmon F., 2017, NEURAL INFORM PROCES; Chierichetti F., 2017, NEURAL INFIRIATION P; Chzhen E., 2019, ARXIV190412527; Cotter A., 2018, ARXIV180700028; Denis C., 2017, J MACHINE LEARNING R, V18, P3571; Denis Christophe, 2019, J NONPARAMETR STAT, P1; DEVROYE LP, 1978, IEEE T INFORM THEORY, V24, P142, DOI 10.1109/TIT.1978.1055865; Donini M., 2018, NEURAL INFORM PROCES; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Dwork C, 2018, P 1 C FAIRNESS ACCOU; Feldman Michael, 2015, INT C KNOWL DISC DAT; Genuer R, 2012, J NONPARAMETR STAT, V24, P543, DOI 10.1080/10485252.2012.677843; Hardt Moritz, 2016, NEURAL INFORM PROCES; Jabbari S., 2016, C FAIRN ACC TRANSP M; Joseph Matthew, 2016, NEURAL INFORM PROCES; Kamiran F., 2009, INT C COMP CONTR COM; Kamiran F., 2010, MACH LEARN C; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kilbertus N., 2017, NEURAL INFORM PROCES; Koltchinskii V., 2011, ORACLE INEQUALITIESI, V2033; Koyejo O., 2015, NEURAL INFORM PROCES; Kusner M. J., 2017, NEURAL INFORM PROCES; Lei J, 2014, BIOMETRIKA, V101, P755, DOI 10.1093/biomet/asu038; Lum K., 2016, ARXIV161008077; Menon A. K., 2018, C FAIRN ACC TRANSP; Oneto L., 2019, ARXIV190110080; Oneto L., 2019, AAAI ACM C AI ETH SO; Pleiss G., 2017, NEURAL INFORM PROCES; Roemer J. E., 2015, HDB INCOME DISTRIBUT; Sadinle M., 2018, J AM STAT ASSOC, P1; Scornet E, 2015, ANN STAT, V43, P1716, DOI 10.1214/15-AOS1321; van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929; Vapnik V.N., 2015, MEASURES COMPLEXITY; Vasconcellos K.L.P., 2000, BRAZ REV EC RIO DE J, V20, P269, DOI [10.12660/bre.v20n22000.2760, DOI 10.12660/BRE.V20N22000.2760]; Wellner J., 2005, TECHNICAL REPORT; Yan B., 2018, INT C MACH LEARN; Yang YH, 1999, IEEE T INFORM THEORY, V45, P2271; Yao S., 2017, NEURAL INFORM PROCES; Zafar M. B., 2017, INT C WORLD WID WEB; Zemel R, 2013, INT C MACH LEARN; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033; Zliobaite I., 2015, 2 WORKSH FAIRN ACC T	52	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904041
C	d'Autume, CD; Rosca, M; Rae, J; Mohamed, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		d'Autume, Cyprien de Masson; Rosca, Mihaela; Rae, Jack; Mohamed, Shakir			Training Language GANs from Scratch	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generative Adversarial Networks (GANs) enjoy great success at image generation, but have proven difficult to train in the domain of natural language. Challenges with gradient estimation, optimization instability, and mode collapse have lead practitioners to resort to maximum likelihood pre-training, followed by small amounts of adversarial fine-tuning. The benefits of GAN fine-tuning for language generation are unclear, as the resulting models produce comparable or worse samples than traditional language models. We show it is in fact possible to train a language GAN from scratch - without maximum likelihood pre-training. We combine existing techniques such as large batch sizes, dense rewards and discriminator regularization to stabilize and improve language GANs. The resulting model, ScratchGAN, performs comparably to maximum likelihood training on EMNLP2017 News and WikiText-103 corpora according to quality and diversity metrics.	[d'Autume, Cyprien de Masson; Rosca, Mihaela; Rae, Jack; Mohamed, Shakir] DeepMind, London, England		d'Autume, CD (corresponding author), DeepMind, London, England.	cyprien@google.com; mihaelacr@google.com; jwrae@google.com; shakir@google.com		Mohamed, Shakir/0000-0002-1184-5776				Allahyari M, 2017, INT J ADV COMPUT SC, V8, P397; [Anonymous], 2017, ABS170207983 CORR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bai S., 2018, ARXIV PREPRINT ARXIV; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Brock Andrew, 2018, ARXIV180911096; Caccia Massimo, 2018, ABS181102549 CORR; Cer D, 2018, CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING (EMNLP 2018): PROCEEDINGS OF SYSTEM DEMONSTRATIONS, P169; Chung J., 2014, ARXIV14123555; Dauphin Yann N., 2016, CORR; Fedus W., 2017, ARXIV171008446; Fedus William, 2018, INT C LEARN REPR; Ganin Y, 2018, PR MACH LEARN RES, V80; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu Jiatao, 2017, ARXIV171102281; Gulrajani I, 2017, P NIPS 2017; Guo J., 2017, ARXIV170908624; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hinton, 2016, ARXIV PREPRINT ARXIV; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Holtzman Ari, 2018, ARXIV180506087; Holtzman Ari, 2019, P INT C LEARN REPR; Huszir Ferenc, 2015, ARXIV151105101; Jang E., 2016, ARXIV; Jolicoeur-Martineau Alexia, 2018, ARXIV180700734; Jozefowicz Rafal, 2016, ARXIV160202410; Kannan Anjuli, 2017, ARXIV170108198; Kingma D.P, P 3 INT C LEARNING R; KNESER R, 1995, INT CONF ACOUST SPEE, P181, DOI 10.1109/ICASSP.1995.479394; Lample Guillaume, 2017, INT C LEARN REPR; Li Jiwei, 2016, P EMNLP; Lin K, 2017, ADV NEUR IN, V30; Maddison Chris J, 2016, ARXIV161100712; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mohamed Shakir, 2016, ARXIV161003483; Nie Weili, 2019, INT C LEARN REPR; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Press Ofir, 2017, ARXIV170601399; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; REITER MD, 2018, COMPUT LINGUIST, P1; Santoro A, 2017, ADV NEUR IN, V30; Semeniuta Stanislau, 2019, ACCURATE EVALUATION; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tevet Guy, 2018, ARXIV181012686; Theis Lucas, 2015, ARXIV151101844; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yu Yong, 2017, SEQGAN SEQUENCE GENE; Zhang YZ, 2017, PR MACH LEARN RES, V70; Zhu YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P1097, DOI 10.1145/3209978.3210080	58	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304032
C	Dong, JH; Lin, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dong, Jinhao; Lin, Tong			MarginGAN: Adversarial Training in Semi-Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A Margin Generative Adversarial Network (MarginGAN) is proposed for semi-supervised learning problems. Like Triple-GAN, the proposed MarginGAN consists of three components-a generator, a discriminator and a classifier, among which two forms of adversarial training arise. The discriminator is trained as usual to distinguish real examples from fake examples produced by the generator. The new feature is that the classifier attempts to increase the margin of real examples and to decrease the margin of fake examples. On the contrary, the purpose of the generator is yielding realistic and large-margin examples in order to fool the discriminator and the classifier simultaneously. Pseudo labels are used for generated and unlabeled examples in training. Our method is motivated by the success of large-margin classifiers and the recent viewpoint that good semi-supervised learning requires a "bad" GAN. Experiments on benchmark datasets testify that MarginGAN is orthogonal to several state-of-the-art methods, offering improved error rates and shorter training time as well.	[Dong, Jinhao] Xidian Univ, Sch Comp Sci & Technol, Xian 710126, Peoples R China; [Lin, Tong] Peking Univ, MOE Sch EECS, Key Lab Machine Percept, Beijing, Peoples R China; [Lin, Tong] Peng Cheng Lab, Shenzhen, Peoples R China	Xidian University; Peking University; Peng Cheng Laboratory	Lin, T (corresponding author), Peking Univ, MOE Sch EECS, Key Lab Machine Percept, Beijing, Peoples R China.; Lin, T (corresponding author), Peng Cheng Lab, Shenzhen, Peoples R China.	jhdong@stu.xidian.edu.cn; lintong@pku.edu.cn						Agrawala A., 1970, IEEE TIT; Bennett K., 2002, KDD; Blum A., 2001, P INT C MACH LEARN I, P19, DOI DOI 10.1184/R1/6606860.V1; Blum A., 1998, COMBINING LABELED UN; Chen X., 2016, ARXIV160603657, P2172; Dai Z., 2017, NEURIPS; Deng Z., 2017, NEURIPS; Fralick S., 1967, IEEE TIT; Gan Z., 2017, NEURIPS; Gastaldi Xavier, 2017, ARXIV170507485; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Laine S., 2016, INT C LEARN REPR; Lee D.H., 2013, ICML 2013 WORKSHOP C; Li C., 2017, NEURIPS; Miyato T., 2016, ICLR; Miyato T., 2017, ARXIV170403976; Rasmus A., 2015, NEURIPS; Sajjadi Mehdi, 2016, ICIP; Salimans T., 2016, NEURIPS; Schapire R.E., BOOSTING FDN ALGORIT; Scudder H., 1965, PROBABILITY ERROR SO; Springenberg J. T., 2016, ICLR; Tarvainen Antti, 2017, NEURIPS; Zhu Xiaojin., 2003, P ICLR, P912	24	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902011
C	Nguyen, DT; Dax, M; Mummadi, CK; Ngo, TPN; Nguyen, THP; Lou, ZY; Brox, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Duc Tam Nguyen; Dax, Maximilian; Mummadi, Chaithanya Kumar; Thi Phuong Nhung Ngo; Thi Hoai Phuong Nguyen; Lou, Zhongyu; Brox, Thomas			DeepUSPS: Deep Robust Unsupervised Saliency PredictionWith Self-Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OBJECT DETECTION	Deep neural network (DNN) based salient object detection in images based on high-quality labels is expensive. Alternative unsupervised approaches rely on careful selection of multiple handcrafted saliency methods to generate noisy pseudo-ground-truth labels. In this work, we propose a two-stage mechanism for robust unsupervised object saliency prediction, where the first stage involves refinement of the noisy pseudo-labels generated from different handcrafted methods. Each handcrafted method is substituted by a deep network that learns to generate the pseudo-labels. These labels are refined incrementally in multiple iterations via our proposed self-supervision technique. In the second stage, the refined labels produced from multiple networks representing multiple saliency methods are used to train the actual saliency detection network. We show that this self-learning procedure outperforms all the existing unsupervised methods over different datasets. Results are even comparable to those of fully-supervised state-of-the-art approaches. The code is available at https://tinyurl.com/wtlhgo3	[Duc Tam Nguyen; Mummadi, Chaithanya Kumar; Brox, Thomas] Univ Freiburg, Comp Vis Grp, Freiburg, Germany; [Duc Tam Nguyen; Dax, Maximilian; Lou, Zhongyu] Bosch GmbH, Bosch Res, Stuttgart, Germany; [Mummadi, Chaithanya Kumar; Thi Phuong Nhung Ngo] Bosch GmbH, Bosch Ctr AI, Stuttgart, Germany; [Thi Hoai Phuong Nguyen] Karlsruhe Inst Technol, Karlsruhe, Germany	University of Freiburg; Helmholtz Association; Karlsruhe Institute of Technology	Nguyen, DT (corresponding author), Univ Freiburg, Comp Vis Grp, Freiburg, Germany.; Nguyen, DT (corresponding author), Bosch GmbH, Bosch Res, Stuttgart, Germany.	Ductam.Nguyen@de.bosch.com; fixed-term.Maximilian.Dax@de.bosch.com						Alpert S, 2012, IEEE T PATTERN ANAL, V34, P315, DOI 10.1109/TPAMI.2011.130; Borji A., 2014, ARXIV14115878; Borji A, 2015, IEEE T IMAGE PROCESS, V24, P5706, DOI 10.1109/TIP.2015.2487833; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Croitoru I, 2019, INT J COMPUT VISION, V127, P1279, DOI 10.1007/s11263-019-01183-3; Nguyen DT, 2019, PR MACH LEARN RES, V97; Goferman S, 2012, IEEE T PATTERN ANAL, V34, P1915, DOI 10.1109/TPAMI.2011.272; Jiang BW, 2013, IEEE I CONF COMP VIS, P1665, DOI 10.1109/ICCV.2013.209; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370; Liu T, 2011, IEEE T PATTERN ANAL, V33, P353, DOI 10.1109/TPAMI.2010.70; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Makansi Osama, 2018, ARXIV180806389; Mi JX, 2017, 2017 INTERNATIONAL CONFERENCE ON SECURITY, PATTERN ANALYSIS, AND CYBERNETICS (SPAC), P660; Nguyen Duc Tam, 2019, ARXIV191001842; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shetty R.R., 2018, P NIPS 2018 32 ANN C, P7706; Show A., 2015, TELL NEURAL IMAGE CA, V23; Wang LZ, 2016, LECT NOTES COMPUT SC, V9908, P825, DOI 10.1007/978-3-319-46493-0_50; Wang TT, 2017, IEEE I CONF COMP VIS, P4039, DOI 10.1109/ICCV.2017.433; Yan Q, 2013, PROC CVPR IEEE, P1155, DOI 10.1109/CVPR.2013.153; Yang C, 2013, PROC CVPR IEEE, P3166, DOI 10.1109/CVPR.2013.407; Zhang DW, 2017, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2017.436; Zhang J, 2018, PROC CVPR IEEE, P9029, DOI 10.1109/CVPR.2018.00941; Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zheng JB, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON BIG KNOWLEDGE (IEEE ICBK 2017), P320, DOI 10.1109/ICBK.2017.58; Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360; Zou WB, 2015, IEEE I CONF COMP VIS, P406, DOI 10.1109/ICCV.2015.54	36	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300019
C	Frei, S; Cao, Y; Gu, QQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Frei, Spencer; Cao, Yuan; Gu, Quanquan			Algorithm-Dependent Generalization Bounds for Overparameterized Deep Residual Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The skip-connections used in residual networks have become a standard architecture choice in deep learning due to the increased training stability and generalization performance with this architecture, although there has been limited theoretical understanding for this improvement. In this work, we analyze overparameterized deep residual networks trained by gradient descent following random initialization, and demonstrate that (i) the class of networks learned by gradient descent constitutes a small subset of the entire neural network function class, and (ii) this subclass of networks is sufficiently large to guarantee small training error. By showing (i) we are able to demonstrate that deep residual networks trained with gradient descent have a small generalization gap between training and test error, and together with (ii) this guarantees that the test error will be small. Our optimization and generalization guarantees require overparameterization that is only logarithmic in the depth of the network, while all known generalization bounds for deep non-residual networks have overparameterization requirements that are at least polynomial in the depth. This provides an explanation for why residual networks are preferable to non-residual ones.	[Frei, Spencer] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA; [Cao, Yuan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Frei, S (corresponding author), Univ Calif Los Angeles, Dept Stat, Los Angeles, CA 90095 USA.	spencerfrei@ucla.edu; yuancao@cs.ucla.edu; qgu@cs.ucla.edu		Frei, Spencer/0000-0002-7068-1409	National Science Foundation [IIS-1903202, IIS-1906169]; Salesforce Deep Learning Research Grant	National Science Foundation(National Science Foundation (NSF)); Salesforce Deep Learning Research Grant	We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1903202 and IIS-1906169. QG is also partially supported by the Salesforce Deep Learning Research Grant. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	[Anonymous], ARXIV181103962; Arora S, 2018, PR MACH LEARN RES, V80; Arora Sanjeev, 2019, ARXIV190108584; BARTLETT P., 2017, SPECTRALLY NORMALIZE; CAO Y., 2019, ARXIV190201384; Cao Y., 2019, P ADV NEUR INF PROC, P32, DOI 10.48550/ARXIV.1905.13210; Choi S., 2019, ARXIV190403814; Du Simon S., 2018, CORR; Du SS., 2019, P 7 INT C LEARN REPR; Dziugaite G. K., 2017, C UNC ART INT; Golowich N., 2018, PROC C LEARN THEORY, P297; Iandola F.N., 2016, ARXIV PREPRINT ARXIV; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li X., 2018, ARXIV180605159; Liang Y, 2018, P 32 INT C NEUR INF, P8168; Ma C., 2019, ARXIV190405263; Neyshabur Behnam, 2018, P 6 INT C LEARN REPR; Rahimi A., 2008, ADV NEURAL INFORM PR, P1313; Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1478; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Tang R, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P5484; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Zhang C., 2017, ICLR; Zhang H., 2019, ARXIV190307120; Zou D., 2019, C NEUR INF PROC SYST; Zou D, 2018, ARXIV181108888	28	6	6	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906045
C	Geifman, Y; El-Yaniv, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Geifman, Yonatan; El-Yaniv, Ran			Deep Active Learning with a Neural Architecture Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider active learning of deep neural networks. Most active learning works in this context have focused on studying effective querying mechanisms and assumed that an appropriate network architecture is a priori known for the problem at hand. We challenge this assumption and propose a novel active strategy whereby the learning algorithm searches for effective architectures on the fly, while actively learning. We apply our strategy using three known querying techniques (softmax response, MC-dropout, and coresets) and show that the proposed approach over-whelmingly outperforms active learning using fixed architectures.	[Geifman, Yonatan; El-Yaniv, Ran] Technion Israel Inst Technol, Haifa, Israel	Technion Israel Institute of Technology	Geifman, Y (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	yonatan.g@cs.technion.ac.il; rani@cs.technion.ac.il			Israel Science Foundation [81/017]	Israel Science Foundation(Israel Science Foundation)	This research was supported by The Israel Science Foundation (grant No. 81/017).	Balcan P. Long, 2013, C LEARNING THEORY CO, P288; Baram Y, 2004, J MACH LEARN RES, V5, P255; Bartlett PL, 2019, J MACH LEARN RES, V20, P1; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Freund Y, 1993, ADV NEURAL INFORMATI, P483; Gal Y., 2017, ARXIV170302910; Garnett R., 2015, ADV NEURAL INFORM PR, V28, P2755; Geifman Y., 2017, ADV NEURAL INFORM PR, P4878; Geifman Y., 2017, ARXIV171100941; Gelbhart Roei, 2017, ARXIV170306536; Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843; Howard A. G., 2017, MOBILENETS EFFICIENT; Iandola F., 1869, COMPUT SCI COMPUT VI, V1404; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Liu C., 2017, ARXIV171200559; Liu H., 2018, ARXIV180609055; Liu H., 2017, ARXIV171100436; Pham H, 2018, 35 INT C MACH LEARN; Real E., 2018, ARXIV180201548; Sener O, 2018, INT C LEARN REPR; Tan Mingxing, 2018, ARXIV180711626; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Wang Keze, 2016, IEEE T CIRCUITS SYST; Wiener Y, 2015, J MACH LEARN RES, V16, P713; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zoph B., 2016, ICLR; Zoph Barret, 2017, ARXIV170707012, V2	29	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306003
C	Guo, C; Mousavi, A; Wu, X; Holtmann-Rice, D; Kale, S; Reddi, S; Kumar, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Guo, Chuan; Mousavi, Ali; Wu, Xiang; Holtmann-Rice, Daniel; Kale, Satyen; Reddi, Sashank; Kumar, Sanjiv			Breaking the Glass Ceiling for Embedding-Based Classifiers for Large Output Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In extreme classification settings, embedding-based neural network models are currently not competitive with sparse linear and tree-based methods in terms of accuracy. Most prior works attribute this poor performance to the low-dimensional bottleneck in embedding-based methods. In this paper, we demonstrate that theoretically there is no limitation to using low-dimensional embedding-based methods, and provide experimental evidence that overfitting is the root cause of the poor performance of embedding-based methods. These findings motivate us to investigate novel data augmentation and regularization techniques to mitigate overfitting. To this end, we propose GLaS, a new regularizer for embedding-based neural network approaches. It is a natural generalization from the graph Laplacian and spread-out regularizers, and empirically it addresses the drawback of each regularizer alone when applied to the extreme classification setup. With the proposed techniques, we attain or improve upon the state-of-the-art on most widely tested public extreme classification datasets with hundreds of thousands of labels.	[Guo, Chuan] Cornell Univ, Ithaca, NY 14853 USA; [Mousavi, Ali; Holtmann-Rice, Daniel; Kale, Satyen; Reddi, Sashank; Kumar, Sanjiv] Google Res, Mountain View, CA USA; [Wu, Xiang] ByteDance, Beijing, Peoples R China; [Guo, Chuan; Wu, Xiang] Google, Mountain View, CA 94043 USA	Cornell University; Google Incorporated; Google Incorporated	Guo, C (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.; Guo, C (corresponding author), Google, Mountain View, CA 94043 USA.	cg563@cornell.edu; alimous@google.com; xiang.wu@bytedance.com; dhr@google.com; satyenkale@google.com; sashank@google.com; sanjivk@google.com						[Anonymous], 2013, SER JMLR WORKSHOP C; Auvolat A., 2015, ABS150705910 CORR; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Babbar Rohit, 2019, MACH LEARN, P1; Baker YS, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P10, DOI 10.1109/ISI.2013.6578776; Bellet A, 2013, ABS13066709 CORR; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Bi W., 2013, ICML, P405; Blalock DW, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P727, DOI 10.1145/3097983.3098195; Gallinari P, 2013, ADV NEURAL INFORM PR, P1851; Guo RQ, 2016, JMLR WORKSH CONF PRO, V51, P482; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Iyyer M, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1681; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Krichene W., 2018, ABS180707187 CORR; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lin ZJ, 2014, PR MACH LEARN RES, V32, P325; Liu JZ, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P115, DOI 10.1145/3077136.3080834; Liu Wei, 2011, P 28 INT C INT C MAC; Prabhu Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P993, DOI 10.1145/3178876.3185998; Prabhu Y, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P441, DOI 10.1145/3159652.3159660; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Reddi Sashank J, 2019, AISTATS; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tai F, 2012, NEURAL COMPUT, V24, P2508, DOI 10.1162/NECO_a_00320; Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016; Wu LY, 2012, SYNLETT, P1529, DOI 10.1055/s-0031-1291042; Wu X., 2017, ADV NEURAL INFORM PR, P5749; Xu C, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1275, DOI 10.1145/2939672.2939798; Yang Z., 2018, BREAKING SOFTMAX BOT; Yen IEH, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P545, DOI 10.1145/3097983.3098083; Yen IEH, 2016, PR MACH LEARN RES, V48; You R., 2019, NEURIPS; Yu H.-F., 2014, INT C MACH LEARN, P593	36	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304090
C	Gupta, R; Kanade, A; Shevade, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gupta, Rahul; Kanade, Aditya; Shevade, Shirish			Neural Attribution for Semantic Bug-Localization in Student Programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Providing feedback is an integral part of teaching. Most open online courses on programming make use of automated grading systems to support programming assignments and give real-time feedback. These systems usually rely on test results to quantify the programs' functional correctness. They return failing tests to the students as feedback. However, students may find it difficult to debug their programs if they receive no hints about where the bug is and how to fix it. In this work, we present NeuralBugLocator, a deep learning based technique, that can localize the bugs in a faulty program with respect to a failing test, without even running the program. At the heart of our technique is a novel tree convolutional neural network which is trained to predict whether a program passes or fails a given test. To localize the bugs, we analyze the trained network using a state-of-the-art neural prediction attribution technique and see which lines of the programs make it predict the test outcomes. Our experiments show that NeuralBugLocator is generally more accurate than two state-of-the-art program-spectrum based and one syntactic difference based bug-localization baselines.	[Gupta, Rahul; Kanade, Aditya; Shevade, Shirish] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, KA, India; [Kanade, Aditya] Google Brain, Mountain View, CA USA	Indian Institute of Science (IISC) - Bangalore; Google Incorporated	Gupta, R (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, KA, India.	rahulg@iisc.ac.in; kanade@iisc.ac.in; shirish@iisc.ac.in			Sonata Software Ltd.	Sonata Software Ltd.	We gratefully acknowledge Sonata Software Ltd. for partially funding this work. We thank Prof. Amey Karkare and his research group from IIT Kanpur for making the dataset available. We also thank the anonymous reviewers for their helpful feedback on the earlier versions of the paper.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Abreu R, 2006, 12TH PACIFIC RIM INTERNATIONAL SYMPOSIUM ON DEPENDABLE COMPUTING, PROCEEDINGS, P39; Ahmed UZ, 2018, PROC INT CONF SOFTW, P78, DOI 10.1145/3183377.3183383; Allamanis M, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3212695; [Anonymous], 2010, PYCPARSER; Chollet F., 2015, KERAS; Glassman EL, 2015, ACM T COMPUT-HUM INT, V22, DOI 10.1145/2699751; Goodfellow I, 2016, DEEP LEARNING; GULWANI S, 2018, PLDI, V53, P465, DOI DOI 10.1145/3192366.3192387; Gupta R., 2019, AAAI; Gupta R, 2017, AAAI CONF ARTIF INTE, P1345; Gupta Rahul, 2019, SUPPLEMENT NEURAL AT; Jones James A, 2001, ICSE WORKSH SOFTW VI; Kaleeswaran S, 2016, FSE'16: PROCEEDINGS OF THE 2016 24TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON FOUNDATIONS OF SOFTWARE ENGINEERING, P739, DOI 10.1145/2950290.2950363; Kim D, 2016, ACM SIGPLAN NOTICES, V51, P311, DOI 10.1145/3022671.2984031; Kingma D.P., 2015, INT C LEARN REPR, P1; Mou L., 2016, AAAI, V2, P4; Nguyen A, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P491, DOI 10.1145/2566486.2568023; Piech C, 2015, PR MACH LEARN RES, V37, P1093; Pu YW, 2016, COMPANION PROCEEDINGS OF THE 2016 ACM SIGPLAN INTERNATIONAL CONFERENCE ON SYSTEMS, PROGRAMMING, LANGUAGES AND APPLICATIONS: SOFTWARE FOR HUMANITY (SPLASH COMPANION'16), P39, DOI 10.1145/2984043.2989222; Sharma S, 2018, LECT NOTES ARTIF INT, V10948, P322, DOI 10.1007/978-3-319-93846-2_60; Singh R, 2013, ACM SIGPLAN NOTICES, V48, P15, DOI 10.1145/2499370.2462195; Sundararajan M, 2017, PR MACH LEARN RES, V70; Vasic M., 2019, ICLR; Wang S, 2016, IEEE INT CONF AUTOM, P708, DOI 10.1145/2970276.2970341; Wong WE, 2016, IEEE T SOFTWARE ENG, V42, P707, DOI 10.1109/TSE.2016.2521368; Yoo S, 2012, SOFTW TEST VERIF REL, V22, P67, DOI [10.1002/stv.430, 10.1002/stvr.430]; Zeller A, 1999, LECT NOTES COMPUT SC, V1687, P253, DOI 10.1145/318774.318946; Zhang J, 2018, 2018 14TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND SECURITY (CIS), P481, DOI 10.1109/CIS2018.2018.00114	31	6	7	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903049
C	He, HW; Huang, G; Yuan, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		He, Haowei; Huang, Gao; Yuan, Yang			Asymmetric Valleys: Beyond Sharp and Flat Local Minima	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite the non-convex nature of their loss functions, deep neural networks are known to generalize well when optimized with stochastic gradient descent (SGD). Recent work conjectures that SGD with proper configuration is able to find wide and flat local minima, which are correlated with good generalization performance. In this paper, we observe that local minima of modern deep networks are more than being flat or sharp. Instead, at a local minimum there exist many asymmetric directions such that the loss increases abruptly along one side, and slowly along the opposite side - we formally define such minima as asymmetric valleys. Under mild assumptions, we first prove that for asymmetric valleys, a solution biased towards the flat side generalizes better than the exact empirical minimizer. Then, we show that performing weight averaging along the SGD trajectory implicitly induces such biased solutions. This provides theoretical explanations for a series of intriguing phenomena observed in recent work [25, 5, 51]. Finally, extensive empirical experiments on both modern deep networks and simple 2 layer networks are conducted to validate our assumptions and analyze the intriguing properties of asymmetric valleys.	[He, Haowei] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China; [Huang, Gao] Tsinghua Univ, Dept Automat, Beijing, Peoples R China; [Yuan, Yang] Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China	Tsinghua University; Tsinghua University	He, HW (corresponding author), Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.	hhw19@mails.tsinghua.edu.cn; gaohuang@tsinghua.edu.cn; yuanyang@tsinghua.edu.cn			Zhongguancun Haihua Institute for Frontier Information Technology; Beijing Academy of Artificial Intelligence (BAAI) [BAAI2019QN0106]	Zhongguancun Haihua Institute for Frontier Information Technology; Beijing Academy of Artificial Intelligence (BAAI)	This work has been supported in part by the Zhongguancun Haihua Institute for Frontier Information Technology. Gao Huang is supported in part by Beijing Academy of Artificial Intelligence (BAAI) under grant BAAI2019QN0106.	Allen-Zhu Z, 2018, ADV NEUR IN, V31; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora S, 2018, PR MACH LEARN RES, V80; Athiwaratkun B., 2019, INT C LEARN REPR; Cesa-Bianchi N., 2001, P 15 ANN C NEUR INF, P359; Chaudhari Pratik, 2017, ICLR POSTER; Choromanska A., 2015, COLT, P1756; Cooper Y., 2018, CORR; Dinh L, 2017, PR MACH LEARN RES, V70; Draxler F, 2018, PR MACH LEARN RES, V80; Garipov T, 2018, P ADV NEURAL INF PRO, P8803; GE R., 2018, P 6 INT C LEARN REPR; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goyal Priya, 2017, ARXIV170602677; Gunasekar S, 2018, PR MACH LEARN RES, V80; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S., 1995, Advances in Neural Information Processing Systems 7, P529; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2017, ARXIV PREPRINT ARXIV; Izmailov P, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P876; Jastrzebski S., 2017, CORR; Ji Z., 2018, CORR; JIN C., 2018, ADV NEURAL INFORM PR, V31, P4901; Jin C., 2018, P 31 C LEARNING THEO, P1042; Jin C, 2017, PR MACH LEARN RES, V70; Kawaguchi Kenji, 2017, CORR, P1; Keskar N.S., 2017, ICLR; Kleinberg R., 2018, ARXIV180206175; Masters D., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.07612; Mehta D., 2018, CORR; Neyshabur B., 2018, CORR; Neyshabur Behnam, 2018, INT C LEARN REPR; Pennington J, 2017, PR MACH LEARN RES, V70; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; RAKHLIN A., 2012, P INT C MACH LEARN, P1571; Sagun L., 2018, ICLR WORKSH; Shalev-Shwartz S., 2009, P 22 C LEARN THEOR C, P1; Smith Samuel L., 2018, INT C LEARN REPR; Soudry D, 2018, J MACH LEARN RES, V19; WANG Y, 2018, NEURIPS; Wu L., 2017, CORR; Xu Y, 2018, ADV NEUR IN, V31; Yang G., 2019, ICML; Zhou Wangchunshu, 2019, INT C LEARN REPR	50	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302054
C	Huang, SY; Chen, YX; Yuan, T; Qi, SY; Zhu, YX; Zhu, SC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Huang, Siyuan; Chen, Yixin; Yuan, Tao; Qi, Siyuan; Zhu, Yixin; Zhu, Song-Chun			PerspectiveNet: 3D Object Detection from a Single RGB Image via Perspective Points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GESTALT PSYCHOLOGY; PERCEPTION; RECOGNITION	Detecting 3D objects from a single RGB image is intrinsically ambiguous, thus requiring appropriate prior knowledge and intermediate representations as constraints to reduce the uncertainties and improve the consistencies between the 2D image plane and the 3D world coordinate. To address this challenge, we propose to adopt perspective points as a new intermediate representation for 3D object detection, defined as the 2D projections of local Manhattan 3D keypoints to locate an object; these perspective points satisfy geometric constraints imposed by the perspective projection. We further devise PerspectiveNet, an end-to-end trainable model that simultaneously detects the 2D bounding box, 2D perspective points, and 3D object bounding box for each object from a single RGB image. PerspectiveNet yields three unique advantages: (i) 3D object bounding boxes are estimated based on perspective points, bridging the gap between 2D and 3D bounding boxes without the need of category-specific 3D shape priors. (ii) It predicts the perspective points by a template-based method, and a perspective loss is formulated to maintain the perspective constraints. (iii) It maintains the consistency between the 2D perspective points and 3D bounding boxes via a differentiable projective function. Experiments on SUN RGB-D dataset show that the proposed method significantly outperforms existing RGB-based approaches for 3D object detection.	[Huang, Siyuan; Chen, Yixin; Yuan, Tao; Zhu, Yixin; Zhu, Song-Chun] Dept Stat, Los Angeles, CA 90095 USA; [Qi, Siyuan] Dept Comp Sci, Los Angeles, CA USA		Huang, SY (corresponding author), Dept Stat, Los Angeles, CA 90095 USA.	huangsiyuan@ucla.edu; ethanchen@ucla.edu; taoyuan@ucla.edu; syqi@cs.ucla.edu; yixin.zhu@ucla.edu; sczhu@stat.ucla.edu			MURI ONR [N00014-16-1-2007]; DARPA [XAI N66001-17-2-4029]; ONR [N00014-19-1-2153]; NVIDIA GPU donation grant	MURI ONR(MURIOffice of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NVIDIA GPU donation grant	This work reported herein is supported by MURI ONR N00014-16-1-2007, DARPA XAI N66001-17-2-4029, ONR N00014-19-1-2153, and an NVIDIA GPU donation grant.	[Anonymous], 1938, SOURCE BOOK GESTALT; [Anonymous], 1975, PSYCHOL COMPUTER VIS; BARROW HG, 1981, ARTIF INTELL, V17, P75, DOI 10.1016/0004-3702(81)90021-7; Binford I, 1971, IEEE C SYST CONTR; Bosse Michael, 2003, VISUAL COMPUTER; Broadbent Donald, 1985, QUESTION LEVELS COMM; BROOKS RA, 1981, ARTIF INTELL, V17, P285, DOI 10.1016/0004-3702(81)90028-X; Chen XZ, 2015, ADV NEUR IN, V28; Chen Xiaozhi, 2016, C COMP VIS PATT REC; Choi W., 2013, C COMP VIS PATT REC; Coughlan James M, 2003, NEURAL COMPUTATION; Coughlan James M, 1999, C COMP VIS PATT REC; Dai CH, 2016, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON NUCLEAR ENGINEERING, 2016, VOL 2; Delage E, 2007, SPRINGER TRAC ADV RO, V28, P305; Furukawa Yasutaka, 2009, C COMP VIS PATT REC; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Ghanem Bernard, 2015, C COMP VIS PATT REC; GHAZALI N, 2012, PERCEPTUAL ORG VISUA, V4, DOI DOI 10.1186/1758-3284-4-32; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Graser A, 2015, LECT NOTES GEOINF CA, P3, DOI 10.1007/978-3-319-11879-6_1; Guo CE, 2007, COMPUT VIS IMAGE UND, V106, P5, DOI 10.1016/j.cviu.2005.09.004; Guo Cheng-en, 2003, INT C COMP VIS; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He T, 2019, AAAI CONF ARTIF INTE, P8409; Hedau Varsha, 2010, EUR C COMP VIS ECCV; Hedau Varsha, 2009, C COMP VIS PATT REC; Huang SY, 2018, LECT NOTES COMPUT SC, V11211, P194, DOI 10.1007/978-3-030-01234-2_12; Huang SY, 2018, ADV NEUR IN, V31; Izadinia Hamid, 2017, P IEEE C COMP VIS PA, P3; JULESZ B, 1981, NATURE, V290, P91, DOI 10.1038/290091a0; Julesz B., 1962, IRE T INFORM THEOR, V8, P84, DOI 10.1109/TIT.1962.1057698; KANADE T, 1981, ARTIF INTELL, V17, P409, DOI 10.1016/0004-3702(81)90031-X; Koffka K., 1935, PRINCIPLES GESTALT P; Kohler, 1920, PHYSISCHEN GESTALTEN; Kohler W., 1938, SOURCE BOOK GESTALT, P17, DOI DOI 10.1037/11496-003; Kroeger Till, 2015, C COMP VIS PATT REC; Kundu A, 2018, PROC CVPR IEEE, P3559, DOI 10.1109/CVPR.2018.00375; Lee Chen-Yu, 2017, INT C COMP VIS; Lee DC, 2009, PROC CVPR IEEE, P2136, DOI 10.1109/CVPRW.2009.5206872; Lin D., 2013, INT C COMP VIS ICCV; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Liu XB, 2018, IEEE T PATTERN ANAL, V40, P710, DOI 10.1109/TPAMI.2017.2689007; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; MARR D, 1978, PROC R SOC SER B-BIO, V200, P269, DOI 10.1098/rspb.1978.0020; Marr D., 1982, Vision. A computational investigation into the human representation and processing of visual information; Mousavian A., 2017, PROC CVPR IEEE, P7074, DOI DOI 10.1109/CVPR.2017.597; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Nitzberg Mark, 1990, ICCV; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pentland A. P., 1987, READINGS COMPUTER VI; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Ren Zhile, 2016, C COMP VIS PATT REC; Rezende DJ, 2016, ADV NEUR IN, V29; Schindler Grant, 2004, C COMP VIS PATT REC; Schwing Alexander G, 2012, C COMP VIS PATT REC; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Straub Julian, 2014, INT C COMP VIS ICCV; Suwajanakorn S, 2018, ADV NEUR IN, V31; Tekin B, 2018, PROC CVPR IEEE, P292, DOI 10.1109/CVPR.2018.00038; Tulsiani S, 2018, PROC CVPR IEEE, P302, DOI 10.1109/CVPR.2018.00039; Ulhaq A, 2017, INT CONF IMAG VIS; Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333; Wagemans J, 2012, PSYCHOL BULL, V138, P1218, DOI 10.1037/a0029334; Wang John YA, 1993, C COMP VIS PATT REC; WANG JYA, 1994, IEEE T IMAGE PROCESS, V3, P625, DOI 10.1109/83.334981; Wertheimer M, 1923, PSYCHOL FORSCH, V4, P301, DOI 10.1007/BF00410640; Wertheimer M, 1912, Z PSYCHOL PHYSIOL SI, V61, P161; Wu Jiajun, 2017, ADV NEURAL INFORM PR, V3; Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0; Xiao J., 2014, INT J COMPUT VISION, V110, P258, DOI [10.1007/s11263-014-0711-y, DOI 10.1007/s11263-014-0711-y]; Xiao JX, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00506; Xu Bin, 2018, C COMP VIS PATT REC; Yao Shunyu, 2018, ADV NEURAL INFORM PR; Zhang XM, 2018, ADV NEUR IN, V31; Zhang Yinda, 2014, P ECCV; Zhao Y., 2011, ADV NEURAL INFORM PR, P73; Zhao YB, 2013, PROC CVPR IEEE, P3119, DOI 10.1109/CVPR.2013.401; ZHU JY, 2018, ADV NEURAL INFORM PR; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 2005, INT J COMPUT VISION, V62, P121, DOI 10.1007/s11263-005-4638-1; Zou C, 2018, OXY-FUEL COMBUSTION: FUNDAMENTALS, THEORY AND PRACTICE, P31, DOI 10.1016/B978-0-12-812145-0.00003-7	84	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900049
C	Jha, S; Raj, S; Fernandes, SL; Jha, SK; Jha, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jha, Susmit; Raj, Sunny; Fernandes, Steven Lawrence; Jha, Sumit Kumar; Jha, Somesh			Attribution-Based Confidence Metric For Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a novel confidence metric, namely, attribution-based confidence (ABC) for deep neural networks (DNNs). ABC metric characterizes whether the output of a DNN on an input can be trusted. DNNs are known to be brittle on inputs outside the training distribution and are, hence, susceptible to adversarial attacks. This fragility is compounded by a lack of effectively computable measures of model confidence that correlate well with the accuracy of DNNs. These factors have impeded the adoption of DNNs in high-assurance systems. The proposed ABC metric addresses these challenges. It does not require access to the training data, the use of ensembles, or the need to train a calibration model on a held-out validation set. Hence, the new metric is usable even when only a trained model is available for inference. We mathematically motivate the proposed metric and evaluate its effectiveness with two sets of experiments. First, we study the change in accuracy and the associated confidence over out-of-distribution inputs. Second, we consider several digital and physically realizable attacks such as FGSM, CW, DeepFool, PGD, and adversarial patch generation methods. The ABC metric is low on out-of-distribution data and adversarial examples, where the accuracy of the model is also low. These experiments demonstrate the effectiveness of the ABC metric towards creating more trustworthy and resilient DNNs.	[Jha, Susmit] SRI Int, Comp Sci Lab, 333 Ravenswood Ave, Menlo Pk, CA 94025 USA; [Raj, Sunny; Fernandes, Steven Lawrence; Jha, Sumit Kumar] Univ Cent Florida, Comp Sci Dept, Orlando, FL 32816 USA; [Jha, Somesh] Univ Wisconsin, Madison, WI USA; [Jha, Somesh] Xaipient, New York, NY USA	SRI International; State University System of Florida; University of Central Florida; University of Wisconsin System; University of Wisconsin Madison	Jha, S (corresponding author), SRI Int, Comp Sci Lab, 333 Ravenswood Ave, Menlo Pk, CA 94025 USA.		raj, sunny/GSO-0154-2022; Jha, Sumit K/B-5177-2013	raj, sunny/0000-0002-7523-1642; Jha, Sumit K/0000-0003-0354-2940	U.S. Army Research Laboratory Cooperative Research Agreement [W911NF-17-2-0196]; DARPA Assured Autonomy [FA8750-19-C-0089]; U.S. National Science Foundation(NSF) [1422257, 1740079, 1750009, 1822976, 1836978, 1804648]; ARO [W911NF-17-1-0405]; Royal Bank of Canada; U.S. Air Force Young Investigator award [FA9550-18-1-0166]; Cyber Florida	U.S. Army Research Laboratory Cooperative Research Agreement; DARPA Assured Autonomy; U.S. National Science Foundation(NSF)(National Science Foundation (NSF)); ARO; Royal Bank of Canada; U.S. Air Force Young Investigator award; Cyber Florida	The authors acknowledge support from the U.S. Army Research Laboratory Cooperative Research Agreement W911NF-17-2-0196, DARPA Assured Autonomy under contract FA8750-19-C-0089, U.S. National Science Foundation(NSF) grants #1422257, #1740079, #1750009, #1822976, #1836978, #1804648, ARO grant W911NF-17-1-0405, Royal Bank of Canada, Cyber Florida, U.S. Air Force Young Investigator award, and FA9550-18-1-0166. The views, opinions and/or findings expressed are those of the author(s) and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Adusumalli M, 2018, ROUT INT HANDB, P121; [Anonymous], 2017, ARXIV171209196; Bengtsson T, 2008, PROBABILITY STAT ESS, V2, P316, DOI [DOI 10.1214/193940307000000518, 10.1214/193940307000000518]; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Brown Tom B, 2017, ARXIV171209665; Bulatov Y., 2011, TECH REP; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Denker J., 1991, ADV NEURAL INFORM PR, V3, P853, DOI DOI 10.5555/2986766.2986882; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Dubey P., 1975, International Journal of Game Theory, V4, P131, DOI 10.1007/BF01780630; Duda R.O., 1973, J ROYAL STAT SOC SER; Engstrom L., 2018, ARXIV180710272; Gal Y., 2016, THESIS, V1, P3; Gal Y, 2016, PR MACH LEARN RES, V48; Ghorbani A, 2017, ARXIV171010547; GROVES P M, 1970, Psychological Review, V77, P419, DOI 10.1037/h0029810; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ian J., 2014, ARXIV14126572; Jha S, 2019, J AUTOM REASONING, V63, P1055, DOI 10.1007/s10817-018-9499-8; Jha S, 2018, IEEE MILIT COMMUN C, P547, DOI 10.1109/MILCOM.2018.8599691; Jha S, 2017, LECT NOTES COMPUT SC, V10227, P99, DOI 10.1007/978-3-319-57288-8_7; Jha Susmit, 2019, SAF MACH LEARN WORKS; Jiang H, 2018, ADV NEUR IN, V31; Jiang XQ, 2012, J AM MED INFORM ASSN, V19, P263, DOI 10.1136/amiajnl-2011-000291; Kahneman D, 2011, THINKING FAST SLOW; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Karmon D., 2018, ARXIV180102608; Kilbertus N., 2018, ARXIV181200524; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuleshov V, 2015, ADV NEUR IN, V28; Kurakin A, 2016, INT C LEARN REPR SAN; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; LeCun Y, 1998, THE MNIST DATABASE; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Liu Yanpei, 2016, ARXIV161102770; Lundberg SM, 2017, ADV NEUR IN, V30; MACKAY DJC, 1995, NETWORK-COMP NEURAL, V6, P469, DOI 10.1088/0954-898X/6/3/011; Madry Aleksander, 2017, ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Naeini MP, 2015, AAAI CONF ARTIF INTE, P2901; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Papernot N, 2018, ARXIV180304765; Papernot Nicolas, 2016, ISSP 16; Papernot Nicolas, 2016, ARXIV1610O0768; Papernot Nicolas, 2017, ACCS 17; Park K, 2018, PATTERN RECOGN, V80, P143, DOI 10.1016/j.patcog.2018.03.007; Platt JC, 2000, ADV NEUR IN, P61; Raghunathan Aditi, 2018, ARXIV180109344; Rozsa A, 2016, IEEE COMPUT SOC CONF, P410, DOI 10.1109/CVPRW.2016.58; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schonherr L, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23288; Schuster M., 1997, CORR, V45, P2673; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Seto D, 1998, P AMER CONTR CONF, P3504, DOI 10.1109/ACC.1998.703255; Sundararajan M, 2017, PR MACH LEARN RES, V70; Tramer F., 2017, ARXIV; University of Montreal, 1998, ROTAT MNIST BACKGR I; Veronika M, 2009, BMC BIOINFORMATICS, V10, DOI 10.1186/1471-2105-10-S15-S4; Xiao H., 2017, FASHION MNIST NOVEL; Xiong Wayne, 2017, TASLP, P2410; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Young T., 2017, RECENT TRENDS DEEP L; Zadrozny Bianca, 2001, ICML; Zhang Chiyuan, 2016, ARXIV161103530; Zhang HY, 2019, PR MACH LEARN RES, V97	69	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903046
C	Keriven, N; Peyre, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Keriven, Nicolas; Peyre, Gabriel			Universal Invariant and Equivariant Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph Neural Networks (GNN) come in many flavors, but should always be either invariant (permutation of the nodes of the input graph does not affect the output) or equivariant (permutation of the input permutes the output). In this paper, we consider a specific class of invariant and equivariant networks, for which we prove new universality theorems. More precisely, we consider networks with a single hidden layer, obtained by summing channels formed by applying an equivariant linear operator, a pointwise non-linearity, and either an invariant or equivariant linear output layer. Recently, Maron et al. (2019b) showed that by allowing higher-order tensorization inside the network, universal invariant GNNs can be obtained. As a first contribution, we propose an alternative proof of this result, which relies on the Stone-Weierstrass theorem for algebra of real-valued functions. Our main contribution is then an extension of this result to the equivariant case, which appears in many practical applications but has been less studied from a theoretical point of view. The proof relies on a new generalized Stone-Weierstrass theorem for algebra of equivariant functions, which is of independent interest. Additionally, unlike many previous works that consider a fixed number of nodes, our results show that a GNN defined by a single set of parameters can approximate uniformly well a function defined on graphs of varying size.	[Keriven, Nicolas; Peyre, Gabriel] Ecole Normale Super, Paris, France; [Peyre, Gabriel] CNRS, Paris, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite	Keriven, N (corresponding author), Ecole Normale Super, Paris, France.	nicolas.keriven@ens.fr; gabriel.peyre@ens.fr	Keriven, Nicolas/CAF-8143-2022; Keriven, Nicolas/AAR-4705-2020	Keriven, Nicolas/0000-0002-3846-8763; 				Battaglia Peter W, 2016, ARXIV161200222; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; BROSOWSKI B, 1981, P AM MATH SOC, V81, P89, DOI 10.2307/2043993; Bruna J, 2013, PROC INT C LEARN REP; Chen ZX, 2019, IEEE RAD CONF; Cohen TS, 2016, PR MACH LEARN RES, V48; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; de Bie G., 2019, P ICML 2019; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Fout A., 2017, ADV NEURAL INFORM PR, V30, P6512; Gens R., 2014, NIPS; GLIMM J, 1960, ANN MATH, V72, P216, DOI 10.2307/1970133; Hartford J, 2018, PR MACH LEARN RES, V80; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Kingma D.P, P 3 INT C LEARNING R; Kondor R., 2018, ARXIV180102144; Kondor R, 2018, PR MACH LEARN RES, V80; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Maron H., 2019, P INT C LEARN REPR, P1; Maron H, 2019, PR MACH LEARN RES, V97; Ravanbakhsh S, 2017, PR MACH LEARN RES, V70; Sanchez-Gonzalez A, 2018, PR MACH LEARN RES, V80; SANFELIU A, 1983, IEEE T SYST MAN CYB, V13, P353, DOI 10.1109/TSMC.1983.6313175; Sannai A., 2019, ARXIV190301939; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; SHAWETAYLOR J, 1993, IEEE T NEURAL NETWOR, V4, P816, DOI 10.1109/72.248459; Wood J, 1996, DISCRETE APPL MATH, V69, P33, DOI 10.1016/0166-218X(95)00075-3; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Yarotsky D., 2018, ARXIV180410306, P1; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zhou J, 2018, ARTIF CELL NANOMED B, V46, pS1016, DOI 10.1080/21691401.2018.1442841	35	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307014
C	Krauth, K; Tu, S; Recht, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Krauth, Karl; Tu, Stephen; Recht, Benjamin			Finite-time Analysis of Approximate Policy Iteration for the Linear Quadratic Regulator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the sample complexity of approximate policy iteration (PI) for the Linear Quadratic Regulator (LQR), building on a recent line of work using LQR as a testbed to understand the limits of reinforcement learning (RL) algorithms on continuous control tasks. Our analysis quantifies the tension between policy improvement and policy evaluation, and suggests that policy evaluation is the dominant factor in terms of sample complexity. Specifically, we show that to obtain a controller that is within epsilon of the optimal LQR controller, each step of policy evaluation requires at most (n + d)(3)/epsilon(2) samples, where n is the dimension of the state vector and d is the dimension of the input vector. On the other hand, only log(1/epsilon) policy improvement steps suffice, resulting in an overall sample complexity of (n + d)(3)epsilon(-2) log(1/epsilon). We furthermore build on our analysis and construct a simple adaptive procedure based on epsilon-greedy exploration which relies on approximate PI as a sub-routine and obtains T-2/3 regret, improving upon a recent result of Abbasi-Yadkori et al. [3].	[Krauth, Karl; Tu, Stephen; Recht, Benjamin] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Krauth, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	karlk@berkeley.edu; stephentu@berkeley.edu; brecht@berkeley.edu			Google; ONR [N00014-17-1-2191, N00014-17-1-2401, N00014-18-1-2833]; DARPA Assured Autonomy program [FA8750-18-C-0101]; Lagrange program [W911NF-16-1-0552]; Siemens Futuremakers Fellowship; Amazon AWS AI Research Award	Google(Google Incorporated); ONR(Office of Naval Research); DARPA Assured Autonomy program; Lagrange program; Siemens Futuremakers Fellowship; Amazon AWS AI Research Award	We thank the anonymous reviewers for their valuable feedback. We also thank the authors of Abbasi-Yadkori et al. [3] for providing us with an implementation of their model-free LQ algorithm. ST is supported by a Google PhD fellowship. This work is generously supported in part by ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, a Siemens Futuremakers Fellowship, and an Amazon AWS AI Research Award.	Abbasi-Yadkori Y., 2019, AISTATS; Abbasi-Yadkori Y., 2011, C LEARN THEOR; Abeille M., 2018, INT C MACH LEARN; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Bertsekas D. P., 2007, DYNAMIC PROGRAMMING, V2; Bertsekas DP, 2017, IEEE T NEUR NET LEAR, V28, P500, DOI 10.1109/TNNLS.2015.2503980; Bhandari Jalaj, 2018, C LEARN THEOR; Boyan J. A., 1999, INT C MACH LEARN; Bradtke S. J., 1994, THESIS; Cohen A., 2019, ARXIV190206223; Dean S., 2017, ARXIV171001688; Dean S., 2018, NEURAL INFORM PROCES; Dhruv Malik Ashwin, 2019, AISTATS; Faradonbeh MKS, 2018, AUTOMATICA, V96, P342, DOI 10.1016/j.automatica.2018.07.008; Farahmand AM, 2016, J MACH LEARN RES, V17; Fazel M., 2018, INT C MACH LEARN; Fiechter C.-N., 1997, C LEARN THEOR; Ibrahimi M., 2012, NEURAL INFORM PROCES; Krauth K., 2019, ARXIV190512842; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lazaric A, 2012, J MACH LEARN RES, V13, P3041; Lee H, 2008, NONLINEARITY, V21, P857, DOI 10.1088/0951-7715/21/4/011; Lincoln B, 2006, IEEE T AUTOMAT CONTR, V51, P1249, DOI 10.1109/TAC.2006.878720; Mania H., 2018, NEURAL INFORM PROCES; Mania Horia, 2019, ARXIV190207826; massoud Farahmand A., 2010, NEURAL INFORM PROCES; Melo F. S., 2008, INT C MACH LEARN; Munos R., 2003, INT C MACH LEARN; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Ouyang Y., 2017, 55 ANN ALL C COMM CO; Sarkar T., 2019, INT C MACH LEARN; Simchowitz M., 2018, C LEARN THEOR; Tu S., 2019, C LEARN THEOR; Tu S., 2018, INT C MACH LEARN; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zou S., 2019, ARXIV190202234CSSTAT	36	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900014
C	Lee, GH; Yuan, Y; Chang, SY; Jaakkola, TS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Guang-He; Yuan, Yang; Chang, Shiyu; Jaakkola, Tommi S.			Tight Certificates of Adversarial Robustness for Randomly Smoothed Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Strong theoretical guarantees of robustness can be given for ensembles of classifiers generated by input randomization. Specifically, an l(2) bounded adversary cannot alter the ensemble prediction generated by an additive isotropic Gaussian noise, where the radius for the adversary depends on both the variance of the distribution as well as the ensemble margin at the point of interest. We build on and considerably expand this work across broad classes of distributions. In particular, we offer adversarial robustness guarantees and associated algorithms for the discrete case where the adversary is l(0) bounded. Moreover, we exemplify how the guarantees can be tightened with specific assumptions about the function class of the classifier such as a decision tree. We empirically illustrate these results with and without functional restrictions across image and molecule datasets.	[Lee, Guang-He; Yuan, Yang; Jaakkola, Tommi S.] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA; [Yuan, Yang] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China; [Chang, Shiyu] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Tsinghua University; Massachusetts Institute of Technology (MIT)	Lee, GH (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	guanghe@csail.mit.edu; yangyuan@csail.mit.edu; shiyu.chang@ibm.com; tommi@csail.mit.edu			Siemens Corporation	Siemens Corporation	GH and TJ were in part supported by a grant from Siemens Corporation.	Adusumalli M, 2018, ROUT INT HANDB, P121; [Anonymous], 2018, INT C LEARN REPR; Bemis GW, 1996, J MED CHEM, V39, P2887, DOI 10.1021/jm9602928; Cao XY, 2017, ANN COMPUT SECURITY, P278, DOI 10.1145/3134600.3134606; Carlini N., 2017, ARXIV170910207; Chen P.-Y., 2018, NIPS, P4939; Clopper CJ, 1934, BIOMETRIKA, V26, P404, DOI 10.2307/2331986; Cohen J. M., 2019, 36 INT C MACH LEARN; Croce F., 2018, 22 INT C ART INT STA; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dvijotham K, 2018, ARXIV180510265; Dvijotham K., 2018, 34 ANN C UNC ART INT; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; Finlay C., 2019, ARXIV190310396; Fischetti M, 2018, CONSTRAINTS, V23, P296, DOI 10.1007/s10601-018-9285-6; Goodfellow I. J., 2015, P ICLR; Gowal Sven, 2018, EFFECTIVENESS INTERV; Huang P.-S., 2019, ARXIV190901492; Jia R., 2019, ARXIV190900986; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Lecuyer M., 2019, IEEE S SEC PRIV SP; Lee G.-H., 2019, INT C LEARN REPR; Li B., 2018, ARXIV180903113, P1; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Lomuscio A., 2017, CORR; Madry Aleksander, 2018, ICLR; Mirman M., 2018, 35 INT C MACH LEARN; Neyman J, 1933, PHILOS T R SOC LOND, V231, P289, DOI 10.1098/rsta.1933.0009; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Raghunathan A., 2018, NEURIPS; Rogers D, 2010, J CHEM INF MODEL, V50, P742, DOI 10.1021/ci100050t; Scheibler K., 2015, MBMV, P30; Singh Gagandeep, 2018, ADV NEURAL INFORM PR, P10802; Tjeng V., 2017, INT C LEARN REPR; TOCHER KD, 1950, BIOMETRIKA, V37, P130, DOI 10.2307/2332156; Weng Tsui-Wei, 2018, 35 INT C MACH LEARN; Wong E., 2018, 35 INT C MACH LEARN; Wong E, 2018, ARXIV180512514; Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a	42	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304087
C	Lee, SY; Choi, S; Chung, SY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lee, Su Young; Choi, Sungik; Chung, Sae-Young			Sample-Efficient Deep Reinforcement Learning via Episodic Backward Update	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose Episodic Backward Update (EBU) - a novel deep reinforcement learning algorithm with a direct value propagation. In contrast to the conventional use of the experience replay with uniform random sampling, our agent samples a whole episode and successively propagates the value of a state to its previous states. Our computationally efficient recursive algorithm allows sparse and delayed rewards to propagate directly through all transitions of the sampled episode. We theoretically prove the convergence of the EBU method and experimentally demonstrate its performance in both deterministic and stochastic environments. Especially in 49 games of Atari 2600 domain, EBU achieves the same mean and median human normalized performance of DQN by using only 5% and 10% of samples, respectively.	[Lee, Su Young; Choi, Sungik; Chung, Sae-Young] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Lee, SY (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	suyoung.l@kaist.ac.kr; si_choi@kaist.ac.kr; schung@kaist.ac.kr			ICT R&D program of MSIP/IITP [2016-0-00563]	ICT R&D program of MSIP/IITP	This work was supported by the ICT R&D program of MSIP/IITP. [2016-0-00563, Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion]	Arjona-Medina J. A., 2018, ARXIV PREPRINT ARXIV; Bellemare MG, 2016, AAAI CONF ARTIF INTE, P1476; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Blundell C., 2016, ARXIV160604460; Hansen S., 2018, ADV NEURAL INFORM PR, P10590; Harutyunyan A, 2016, LECT NOTES ARTIF INT, V9925, P305, DOI 10.1007/978-3-319-46379-7_21; He F. S., 2017, INT C LEARN REPR ICL; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lengyel M., 2007, ADV NEURAL INFORM PR, V20, P889; Li YL, 2016, 2016 9TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2016), P1995; LIN LJ, 1991, PROCEEDINGS : NINTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 AND 2, P781; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Melo F. S., 2001, TECH REP; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pritzel A, 2017, PR MACH LEARN RES, V70; Schaul T., 2016, INT C LEARN REPR ICL; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Watkins CJCH., 1989, THESIS	23	6	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302014
C	Li, BY; Wu, F; Weinberger, KQ; Belongie, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Boyi; Wu, Felix; Weinberger, Kilian Q.; Belongie, Serge			Positional Normalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TEXTURE SYNTHESIS; IMAGE	A popular method to reduce the training time of deep neural networks is to normalize activations at each layer. Although various normalization schemes have been proposed, they all follow a common theme: normalize across spatial dimensions and discard the extracted statistics. In this paper, we propose an alternative normalization method that noticeably departs from this convention and normalizes exclusively across channels. We argue that the channel dimension is naturally appealing as it allows us to extract the first and second moments of features extracted at a particular image position. These moments capture structural information about the input image and extracted features, which opens a new avenue along which a network can benefit from feature normalization: Instead of disregarding the normalization constants, we propose to re-inject them into later layers to preserve or transfer structural information in generative networks.	[Li, Boyi; Wu, Felix; Weinberger, Kilian Q.; Belongie, Serge] Cornell Univ, Ithaca, NY 14853 USA; [Li, Boyi; Belongie, Serge] Cornell Tech, New York, NY 10044 USA	Cornell University	Li, BY (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.; Li, BY (corresponding author), Cornell Tech, New York, NY 10044 USA.	bl728@cornell.edu; fw245@cornell.edu; kilian@cornell.edu; sjb344@cornell.edu		Belongie, Serge/0000-0002-0388-5217	Facebook; National Science Foundation [III-1618134, III-1526012, IIS1149882, IIS-1724282, TRIPODS-1740822]; Office of Naval Research DOD [N00014-17-1-2175]; Bill and Melinda Gates Foundation; Zillow; SAP America Inc.	Facebook(Facebook Inc); National Science Foundation(National Science Foundation (NSF)); Office of Naval Research DOD(Office of Naval ResearchUnited States Department of Defense); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Zillow; SAP America Inc.	This research is supported in part by the grants from Facebook, the National Science Foundation (III-1618134, III-1526012, IIS1149882, IIS-1724282, and TRIPODS-1740822), the Office of Naval Research DOD (N00014-17-1-2175), Bill and Melinda Gates Foundation. We are thankful for generous support by Zillow and SAP America Inc.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora S., 2019, INT C LEARN REPR; Bishop, 1995, NEURAL NETWORKS PATT; Bjorck J, 2018, ADV NEUR IN, V31; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen Y, 2018, PROC CVPR IEEE, P9465, DOI 10.1109/CVPR.2018.00986; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; de Vries H, 2017, ADV NEUR IN, V30; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Dryden Ian L, 2014, SHAPE ANAL; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dumoulin V., 2017, P INT C LEARN REPR T; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648; Hensel M, 2017, ADV NEUR IN, V30; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang X, 2018, LECT NOTES COMPUT SC, V11207, P179, DOI 10.1007/978-3-030-01219-9_11; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kim T, 2017, INTERSPEECH, P2411, DOI 10.21437/Interspeech.2017-556; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laffont PY, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601101; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lee HY, 2018, LECT NOTES COMPUT SC, V11205, P36, DOI 10.1007/978-3-030-01246-5_3; Li BY, 2018, AAAI CONF ARTIF INTE, P7016; Li BY, 2017, IEEE I CONF COMP VIS, P4780, DOI 10.1109/ICCV.2017.511; Li H, 2018, ADV NEUR IN, V31; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo P., 2018, ARXIV180610779; Luo P., 2019, ICLR, P1; Lyu S., 2008, COMPUTER VISION PATT; Orr GB, 2003, NEURAL NETWORKS TRIC; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Qiao Siyuan, 2019, ARXIV190310520, P2; Ripley BD., 1996; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Santurkar S, 2018, ADV NEUR IN, V31; Shao WQ, 2020, INT J COMPUT VISION, V128, P2107, DOI 10.1007/s11263-019-01269-y; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Srivatsan R, 2015, SEVA, SAVIOUR AND STATE: CASTE POLITICS, TRIBAL WELFARE AND CAPITALIST DEVELOPMENT, P1; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Ting Chen, 2018, ARXIV181001365; Ulyanov D., 2016, ARXIV160708022; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; Wu Felix, 2019, INT C LEARN REPR; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Zhang Han, 2019, ICLR; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	77	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301059
C	Ma, W; Chen, GH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Wei; Chen, George H.			Missing Not at Random in Matrix Completion: The Effectiveness of Estimating Missingness Probabilities Under a Low Nuclear Norm Assumption	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Matrix completion is often applied to data with entries missing not at random (MNAR). For example, consider a recommendation system where users tend to only reveal ratings for items they like. In this case, a matrix completion method that relies on entries being revealed at uniformly sampled row and column indices can yield overly optimistic predictions of unseen user ratings. Recently, various papers have shown that we can reduce this bias in MNAR matrix completion if we know the probabilities of different matrix entries being missing. These probabilities are typically modeled using logistic regression or naive Bayes, which make strong assumptions and lack guarantees on the accuracy of the estimated probabilities. In this paper, we suggest a simple approach to estimating these probabilities that avoids these shortcomings. Our approach follows from the observation that missingness patterns in real data often exhibit low nuclear norm structure. We can then estimate the missingness probabilities by feeding the (always fully-observed) binary matrix specifying which entries are revealed or missing to an existing nuclear-norm-constrained matrix completion algorithm by Davenport et al. [2014]. Thus, we tackle MNAR matrix completion by solving a different matrix completion problem first that recovers missingness probabilities. We establish finite-sample error bounds for how accurate these probability estimates are and how well these estimates debias standard matrix completion losses for the original matrix to be completed. Our experiments show that the proposed debiasing strategy can improve a variety of existing matrix completion algorithms, and achieves downstream matrix completion accuracy at least as good as logistic regression and naive Bayes debiasing baselines that require additional auxiliary information.	[Ma, Wei; Chen, George H.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Ma, W (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	weima@cmu.edu; georgechen@cmu.edu		Ma, Wei/0000-0001-8945-5877				Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cai TT, 2016, ELECTRON J STAT, V10, P1493, DOI 10.1214/16-EJS1147; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Funk S., 2006, NETFLIX UPDATE TRY T; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; HESTERBERG T, 1995, TECHNOMETRICS, V37, P185, DOI 10.2307/1269620; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kluger Y, 2003, GENOME RES, V13, P703, DOI 10.1101/gr.648603; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Liang DW, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P951, DOI 10.1145/2872427.2883090; Little R. J., 2019, STAT ANAL MISSING DA, V793; Mazumder Rahul, 2016, J MACHINE LEARNING R, V11, P2287; Mnih A., 2007, ADV NEURAL INFORM PR, V20, P1257; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Schnabel T, 2016, PR MACH LEARN RES, V48; Seginer Y, 2000, COMB PROBAB COMPUT, V9, P149, DOI 10.1017/S096354830000420X; Sportisse Aude, 2018, ARXIV181211409; Sportisse Aude, 2019, ESTIMATION IMPUTATIO; Srebro N., 2010, PROC INT C NEURAL IN, P2056; Steck H., 2010, P 16 ACM SIGKDD INT, P713, DOI DOI 10.1145/1835804.1835895; Swaminathan A, 2015, ADV NEUR IN, V28; Taghavi E, 2016, 2016 19TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1155; Thompson S, 2012, SAMPLING; Trotter H.F., 1956, S MONTE CARLO METHOD, P64; Wang M, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P666, DOI 10.1145/3241539.3267745; Wang Menghan, 2018, AAAI C ART INT; Wang Xiaojie, 2018, INT C MACH LEARN; Zhu Z., 2019, ARXIV190612125	35	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906054
C	Madarasz, TJ; Behrens, TE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Madarasz, Tamas J.; Behrens, Timothy E.			Better transfer learning with inferred successor maps	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Humans and animals show remarkable flexibility in adjusting their behaviour when their goals, or rewards in the environment change. While such flexibility is a hallmark of intelligent behaviour, these multi-task scenarios remain an important challenge for machine learning algorithms and neurobiological models alike. We investigated two approaches that could enable this flexibility: factorized representations, which abstract away general aspects of a task from those prone to change, and nonparametric, memory-based approaches, which can provide a principled way of using similarity to past experiences to guide current behaviour. In particular, we combine the successor representation (SR), that factors the value of actions into expected outcomes and corresponding rewards, with evaluating task similarity through clustering the space of rewards. The proposed algorithm inverts a generative model over tasks, and dynamically samples from a flexible number of distinct SR maps while accumulating evidence about the current task context through amortized inference. It improves SR's transfer capabilities and outperforms competing algorithms and baselines in settings with both known and unsignalled rewards changes. Further, as a neurobiological model of spatial coding in the hippocampus, it explains important signatures of this representation, such as the "flickering" behaviour of hippocampal maps, and trajectory-dependent place cells (so-called splitter cells) and their dynamics. We thus provide a novel algorithmic approach for multi-task learning, as well as a common normative framework that links together these different characteristics of the brain's spatial representation.	[Madarasz, Tamas J.; Behrens, Timothy E.] Univ Oxford, Oxford, England	University of Oxford	Madarasz, TJ (corresponding author), Univ Oxford, Oxford, England.	tamas.madarasz@ndcn.ox.ac.uk; behrens@fmrib.ox.ac.uk		Behrens, Timothy/0000-0003-0048-1177				Ans B., 1997, COMPTES RENDUS ACA 3; Antoniak Charles E, 1974, ANN STAT, V2, P3; Barreto A., 2019, TRANSFER DEEP REINFO; Barreto A, 2017, ADV NEUR IN, V30; Boccara C. N., 2019, SCIENCE; Borsa D., 2019, INT C LEARN REPR; Botvinick M., 2019, TRENDS COGN SCI; Brown T. I., 2016, SCIENCE; Carpenter G. A., 1987, COMPUT VISION GRAPH; Chan S. C. Y., 2016, J NEUROSCI; Courville A., 2004, ADV NEURAL INF PROCE, V16; Dayan P., 2008, NEURAL COMPUT; Dudchenko P. A., 2014, SPACE TIME MEM HIPPO; Dupret D., 2013, NEURON; FERNANDEZ F, 2006, P 5 INT JOINT C AUT; Finn Chelsea, 2017, 170303400 ARXIV; Franklin N. T., 2018, PLOS COMPUTATIONAL B; Gershman SJ, 2017, ANNU REV PSYCHOL, V68, P101, DOI 10.1146/annurev-psych-122414-033625; Gershman SJ, 2010, PSYCHOL REV, V117, P197, DOI 10.1037/a0017808; Grieves RM, 2016, ELIFE, V5, DOI 10.7554/eLife.15986; Isele D., 2018, AAAI C ART INT; Jezek K., 2011, NATURE; Kay K., 2019, BIORXIV; Lazaric A, 2010, P 27 INT C MACH LEAR; Lehnert L., 2017, 170800102 ARXIV; Lengyel M., 2007, ADV NEURAL INF PROCE; Levine S., 2018, CORR; Ma C., 2018, ARXIV PREPRINT ARXIV; Madarasz T. J., 2016, NAT NEUROSCI; McCloskey M., 1989, PSYCHOL LEARN MOTIV; Meder D, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-02169-w; Milan K, 2016, ADV NEUR IN, V29; Mnih V, 2015, NATURE, V518, P529; Momennejad I., 2017, NAT HUM BEHAV; Ng A. Y., 1999, P 16 INT C MACH LEAR; ODoherty J., 2004, SCIENCE; Pritzel A., 2017, 170301988 ARXIV; Rolls E. T., 1996, HIPPOCAMPUS; Rolnick D., 2018, 181111682 ARXIV; Russek E. M., 2017, PLOS COMPUTATIONAL B; Schaul T., 32 INT C MACH LEARN; Silver D., 2016, NATURE; Stachenfeld K. L., 2017, NAT NEUROSCI; Sutton R. S., 2011, AAMAS 2011; Teh Y., 2017, ADV NEURAL INF PROCE; Teh Yee Whye, 2006, J AM STAT ASS; Watkins CJCH., 1989, THESIS; Wilson A., 2007, P 274 INT C MACH LEA; Yassa M. A., 2011, TRENDS NEUROSCIENCES	49	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900060
C	Malach, E; Shalev-Shwartz, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Malach, Eran; Shalev-Shwartz, Shai			Is Deeper Better only when Shallow is Good?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Understanding the power of depth in feed-forward neural networks is an ongoing challenge in the field of deep learning theory. While current works account for the importance of depth for the expressive power of neural-networks, it remains an open question whether these benefits are exploited during a gradient-based optimization process. In this work we explore the relation between expressivity properties of deep networks and the ability to train them efficiently using gradient-based algorithms. We give a depth separation argument for distributions with fractal structure, showing that they can be expressed efficiently by deep networks, but not with shallow ones. These distributions have a natural coarse-to-fine structure, and we show that the balance between the coarse and fine details has a crucial effect on whether the optimization process is likely to succeed. We prove that when the distribution is concentrated on the fine details, gradient-based algorithms are likely to fail. Using this result we prove that, at least in some distributions, the success of learning deep networks depends on whether the distribution can be approximated by shallower networks, and we conjecture that this property holds in general.	[Malach, Eran; Shalev-Shwartz, Shai] Hebrew Univ Jerusalem, Sch Comp Sci, Jerusalem, Israel	Hebrew University of Jerusalem	Malach, E (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci, Jerusalem, Israel.	eran.malach@mail.huji.ac.il; shais@cs.huji.ac.il			European Research Council (TheoryDL project)	European Research Council (TheoryDL project)	This research is supported by the European Research Council (TheoryDL project).	[Anonymous], 2013, ARXIV13126098; Belilovsky E., 2018, ARXIV181211446; Bengio, 2011, ADV NEURAL INFORM PR, P666, DOI DOI 10.5555/2986459.2986534; Cohen N., 2016, C LEARNING THEORY, V49, P698; Daniely A., 2017, ARXIV170208489; Dym N., 2019, ARXIV190511345; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; Kingma D.P, P 3 INT C LEARNING R; Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924; Montufar Guido, 2017, NOTES NUMBER LINEAR; Poggio T, 2017, INT J AUTOM COMPUT, V14, P503, DOI 10.1007/s11633-017-1054-2; Poole B, 2016, ADV NEUR IN, V29; Raghu M., 2016, ARXIV160605336; Safran I., 2016, ARXIV161009887; Serra T., 2017, ARXIV171102114; Shalev-Shwartz S., 2017, ARXIV170307950; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Telgarsky M, 2015, ARXIV150908101V2CSLG; Telgarsky M., 2016, COLT	20	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306043
C	Mao, CZ; Zhong, ZY; Yang, JF; Vondrick, C; Ray, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mao, Chengzhi; Zhong, Ziyuan; Yang, Junfeng; Vondrick, Carl; Ray, Baishakhi			Metric Learning for Adversarial Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep networks are well-known to be fragile to adversarial attacks. We conduct an empirical analysis of deep representations under the state-of-the-art attack method called PGD, and find that the attack causes the internal representation to shift closer to the "false" class. Motivated by this observation, we propose to regularize the representation space under attack with metric learning to produce more robust classifiers. By carefully sampling examples for metric learning, our learned representation not only increases robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness accuracy by up to 4% and detection efficiency by up to 6% according to Area Under Curve score over prior work. The code of our work is available at https: //github.com/columbia/Metric_Learning_Adversarial_Robustness.	[Mao, Chengzhi; Zhong, Ziyuan; Yang, Junfeng; Vondrick, Carl; Ray, Baishakhi] Columbia Univ, New York, NY 10027 USA	Columbia University	Mao, CZ (corresponding author), Columbia Univ, New York, NY 10027 USA.	cm3797@columbia.edu; ziyuan.zhong@columbia.edu; junfeng@cs.columbia.edu; vondrick@cs.columbia.edu; rayb@cs.columbia.edu			NSF [CCF-1845893, CNS-15-64055, CRII 1850069]; ONR [N00014-16-1-2263, N00014-17-1-2788]; JP Morgan Faculty Research Award; DiDi Faculty Research Award; Google Cloud grant; Amazon Web Services grant	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); JP Morgan Faculty Research Award; DiDi Faculty Research Award; Google Cloud grant(Google Incorporated); Amazon Web Services grant	We thank the anonymous reviewers, Prof. Suman Jana, Prof. Shih-Fu Chang, Prof. Janet Kayfetz, Ji Xu, Hao Wang, and Vaggelis Atlidakis for their valuable comments, which substantially improved our paper. This work is in part supported by NSF grant CNS-15-64055; ONR grants N00014-16-1-2263 and N00014-17-1-2788; a JP Morgan Faculty Research Award; a DiDi Faculty Research Award; a Google Cloud grant; an Amazon Web Services grant; NSF CRII 1850069; and NSF CCF-1845893.	Abadi M, 2015, P 12 USENIX S OPERAT; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Arora Sanjeev, 2018, COLT 2018; Athalye A, 2018, PR MACH LEARN RES, V80; Buckman Jacob, 2018, INT C LEARN REPR, DOI DOI 10.1109/TCYB.2016.2523000; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen W., 2017, CORR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Dhillon Guneet S., 2018, 6 INT C LEARN REPR; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Duan YQ, 2018, PROC CVPR IEEE, P2780, DOI 10.1109/CVPR.2018.00294; Engstrom Logan, 2018, CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo CA, 2017, PR MACH LEARN RES, V70; Hannun A.Y., 2014, ARXIV14125567, P1; Hoffer E, 2015, LECT NOTES COMPUT SC, V9370, P84, DOI 10.1007/978-3-319-24261-3_7; Kannan Harini, 2018, ARXIV180306373; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2017, CORR; Madry Aleksander, 2017, ARXIV; Mao CZ, 2018, IEEE GLOB COMM CONF; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mosbach Marius, 2018, CORR; Oberman Adam M., 2018, CORR; Pang Tianyu, 2019, CORR; Papernot N., 2015, ARXIV151104508; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Paschali Magdalini, 2018, CORR; Pei K., 2017, CORR; Rakin Adnan Siraj, 2018, CORR; Samangouei Pouya, 2018, ARXIV180506605; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Song C., 2018, CORR; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Tian Y., 2017, ARXIV170808559; Tramer F., 2017, CORR; Tsipras Dimitris, 2018, ARXIV180512152; Uesato Jonathan, 2019, CORR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang H, 2019, AAAI CONF ARTIF INTE, P766; Wang J, 2017, IEEE I CONF COMP VIS, P2612, DOI 10.1109/ICCV.2017.283; Wang SQ, 2018, IEEE INFOCOM SER, P63, DOI 10.1109/infocom.2018.8486403; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu X, 2018, PR MACH LEARN RES, V80; Xie C., 2018, CORR; Yan Z, 2018, ADV NEUR IN, V31; Yang Yuzhe, 2019, P 36 INT C MACH LEAR; Zhang HG, 2019, PROC CVPR IEEE, P5971, DOI 10.1109/CVPR.2019.00613; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485; Zheng Wenzhao, 2019, CORR; Zheng Z., 2018, ADV NEURAL INFORM PR, V31, P7913	52	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300044
C	Marteau-Ferey, U; Bach, F; Rudi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Marteau-Ferey, Ulysse; Bach, Francis; Rudi, Alessandro			Globally Convergent Newton Methods for Ill-conditioned Generalized Self-concordant Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we study large-scale convex optimization algorithms based on the Newton method applied to regularized generalized self-concordant losses, which include logistic regression and softmax regression. We first prove that our new simple scheme based on a sequence of problems with decreasing regularization parameters is provably globally convergent, that this convergence is linear with a constant factor which scales only logarithmically with the condition number. In the parametric setting, we obtain an algorithm with the same scaling than regular first-order methods but with an improved behavior, in particular in ill-conditioned problems. Second, in the non-parametric machine learning setting, we provide an explicit algorithm combining the previous scheme with Nystrom projection techniques, and prove that it achieves optimal generalization bounds with a time complexity of order O(ndf(lambda)), a memory complexity of order O(df(lambda)(2)) and no dependence on the condition number, generalizing the results known for least-squares regression. Here n is the number of observations and df(lambda) is the associated degrees of freedom. In particular, this is the first large-scale algorithm to solve logistic and softmax regressions in the non-parametric setting with large condition numbers and theoretical guarantees.	[Marteau-Ferey, Ulysse; Bach, Francis; Rudi, Alessandro] PSL Reasearch Univ, INRIA Ecole Normale Super, Paris, France		Marteau-Ferey, U (corresponding author), PSL Reasearch Univ, INRIA Ecole Normale Super, Paris, France.	ulysse.marteau-ferey@inria.fr; francis.bach@inria.fr; alessandro.rudi@inria.fr			European Research Council [SEQUOIA 724063]	European Research Council(European Research Council (ERC)European Commission)	We acknowledge support from the European Research Council (grant SEQUOIA 724063).	Agarwal N, 2017, J MACH LEARN RES, V18; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bach F, 2014, J MACH LEARN RES, V15, P595; Bach F, 2010, ELECTRON J STAT, V4, P384, DOI 10.1214/09-EJS521; Bollapragada R, 2019, IMA J NUMER ANAL, V39, P545, DOI 10.1093/imanum/dry009; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540; Boyd S, 2004, CONVEX OPTIMIZATION; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Deuflhard P., 2011, NEWTON METHODS NONLI, DOI 10.1007/978-3-642-23899-4; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; Erdogdu Murat A., 2015, 150802810 ARXIV; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Gower R., 2018, ADV NEURAL INFORM PR, V31, P1619; Karimireddy Sai Praneeth, 2018, ABS180600413 CORR; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Marteau-Ferey Ulysse, 2019, P C COMP LEARN THEOR; Nemirovskii A., 1994, SOC IND APPL MATH; Pilanci M, 2017, SIAM J OPTIMIZ, V27, P205, DOI 10.1137/15M1021106; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Roosta-Khorasani F, 2019, MATH PROGRAM, V174, P293, DOI 10.1007/s10107-018-1346-5; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3888; Rudi Alessandro, 2018, ADV NEURAL INFORM PR, P5672; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013; Williams CKI, 2001, ADV NEUR IN, V13, P682	34	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307063
C	May, A; Zhang, J; Dao, T; Re, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		May, Avner; Zhang, Jian; Dao, Tri; Re, Christopher			On the Downstream Performance of Compressed Word Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Compressing word embeddings is important for deploying NLP models in memory-constrained settings. However, understanding what makes compressed embeddings perform well on downstream tasks is challenging-existing measures of compression quality often fail to distinguish between embeddings that perform well and those that do not. We thus propose the eigenspace overlap score as a new measure. We relate the eigenspace overlap score to downstream performance by developing generalization bounds for the compressed embeddings in terms of this score, in the context of linear and logistic regression. We then show that we can lower bound the eigenspace overlap score for a simple uniform quantization compression method, helping to explain the strong empirical performance of this method. Finally, we show that by using the eigenspace overlap score as a selection criterion between embeddings drawn from a representative set we compressed, we can efficiently identify the better performing embedding with up to 2 x lower selection error rates than the next best measure of compression quality, and avoid the cost of training a model for each task of interest.	[May, Avner; Zhang, Jian; Dao, Tri; Re, Christopher] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	May, A (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	avnermay@cs.stanford.edu; zjian@cs.stanford.edu; trid@cs.stanford.edu; chrismre@cs.stanford.edu			DARPA [FA87501720095, FA86501827865, FA86501827882]; NIH [U54EB020405]; NSF [CCF1763315, CCF1563078, 1937301]; ONR [N000141712266]; Moore Foundation; NXP; Xilinx; LETI-CEA; Intel; IBM; Microsoft; NEC; Toshiba; TSMC; ARM; Hitachi; BASF; Accenture; Ericsson; Qualcomm; Analog Devices; Okawa Foundation; American Family Insurance; Google Cloud; Swiss Re; Teradata; Facebook; Google; Ant Financial; VMWare; Infosys	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Moore Foundation(Gordon and Betty Moore Foundation); NXP; Xilinx; LETI-CEA; Intel(Intel Corporation); IBM(International Business Machines (IBM)); Microsoft(Microsoft); NEC; Toshiba; TSMC; ARM; Hitachi; BASF(BASF); Accenture; Ericsson(Ericsson); Qualcomm; Analog Devices; Okawa Foundation; American Family Insurance; Google Cloud(Google Incorporated); Swiss Re; Teradata; Facebook(Facebook Inc); Google(Google Incorporated); Ant Financial; VMWare; Infosys	We gratefully acknowledge the support of DARPA under Nos. FA87501720095 (D3M), FA86501827865 (SDH), and FA86501827882 (ASED); NIH under No. U54EB020405 (Mobilize), NSF under Nos. CCF1763315 (Beyond Sparsity), CCF1563078 (Volume to Velocity), and 1937301 (RTML); ONR under No. N000141712266 (Unifying Weak Supervision); the Moore Foundation, NXP, Xilinx, LETI-CEA, Intel, IBM, Microsoft, NEC, Toshiba, TSMC, ARM, Hitachi, BASF, Accenture, Ericsson, Qualcomm, Analog Devices, the Okawa Foundation, American Family Insurance, Google Cloud, Swiss Re, and members of the Stanford DAWN project: Teradata, Facebook, Google, Ant Financial, NEC, VMWare, and Infosys. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views, policies, or endorsements, either expressed or implied, of DARPA, NIH, ONR, or the U.S. Government.	Andor D., 2016, ACL; Andrews Martin, 2016, ICONIP; Avron H., 2017, ICML; Bertoldi Nicola, 2014, IWSLT; Chen DQ, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1870, DOI 10.18653/v1/P17-1171; Chen Ting, 2018, ICML; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; De Sa Christopher, 2018, ARXIV180303383; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Gersho A., 1977, IEEE Communications Society Magazine, V15, P16, DOI 10.1109/MCOM.1977.1089500; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han S., 2016, P 4 INT C LEARN REPR, P1; Hotelling Harold, 1933, J EDUC PSYCHOL, V24, P417; Khrulkov Valentin, 2019, ARXIV190110787; KIEFER J, 1953, P AM MATH SOC, V4, P502, DOI 10.2307/2032161; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kingma D.P, P 3 INT C LEARNING R; Levy O, 2014, ADV NEUR IN, V27; Micikevicius Paulius, 2018, P 6 INT C LEARN REPR, DOI [10.48550/arXiv.1710.03740, DOI 10.1109/CAMAD.2018.8514963]; Mikolov T., 2013, ARXIV; Mikolov T, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P52; Mostafa H., 2019, ICML; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Popoviciu T, 1935, MATHEMATICA, V9, P129; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Rahimi Ali, 2007, NEURIPS; RAJPURKAR P, 2016, P 2016 C EMP METH NA, V2016, P2383, DOI DOI 10.18653/V1/D16-1264; Shiebler Dan, 2018, EMBEDDINGS TWITTER; Shu Rui, 2018, P 6 INT C LEARN REPR, P2; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sohoni Nimit Sharad, 2019, ARXIV190410631; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang Alex, 2019, ICLR; Williams C. K. I., 2000, NEURIPS; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yin Zi, 2018, NEURIPS; Zhang Jian, 2019, AISTATS	41	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE	31885428				2022-12-19	WOS:000535866903042
C	Mendis, C; Yang, C; Pu, YW; Amarasinghe, S; Carbin, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mendis, Charith; Yang, Cambridge; Pu, Yewen; Amarasinghe, Saman; Carbin, Michael			Compiler Auto-Vectorization with Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern microprocessors are equipped with single instruction multiple data (SIMD) or vector instruction sets which allow compilers to exploit fine-grained data level parallelism. To exploit this parallelism, compilers employ auto-vectorization techniques to automatically convert scalar code into vector code. Larsen & Amarasinghe (2000) first introduced superword level parallelism (SLP) based vectorization, which is a form of vectorization popularly used by compilers. Current compilers employ hand-crafted heuristics and typically only follow one SLP vectorization strategy which can be suboptimal. Recently, Mendis & Amarasinghe (2018) formulated the instruction packing problem of SLP vectorization by leveraging an integer linear programming (ILP) solver, achieving superior runtime performance. In this work, we explore whether it is feasible to imitate optimal decisions made by their ILP solution by fitting a graph neural network policy. We show that the learnt policy, Vernal, produces a vectorization scheme that is better than the well-tuned heuristics used by the LLVM compiler. More specifically, the learnt agent produces a vectorization strategy that has a 22.6% higher average reduction in cost compared to the LLVM compiler when measured using its own cost model, and matches the runtime performance of the ILP based solution in 5 out of 7 applications in the NAS benchmark suite.	[Mendis, Charith; Yang, Cambridge; Pu, Yewen; Amarasinghe, Saman; Carbin, Michael] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mendis, C (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	charithm@mit.edu; camyang@csail.mit.edu; yewenpu@mit.edu; saman@csail.mit.edu; mcarbin@csail.mit.edu		Mendis, Charith/0000-0002-8140-2321	DARPA D3M Award [FA875017-2-0126]; DARPA HACCS Award [HR0011-18-C-0059]; DARPA SDH Award [HR0011-18-30007]	DARPA D3M Award; DARPA HACCS Award; DARPA SDH Award	We would like to thank Darsh Shah who was initially involved with this project and all reviewers for insightful comments and suggestions. This research was supported by DARPA D3M Award #FA875017-2-0126, DARPA HACCS Award #HR0011-18-C-0059 and DARPA SDH Award #HR0011-18-30007. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.	Allamanis M, 2017, LEARNING REPRESENT P; ALLEN R, 1987, ACM T PROGR LANG SYS, V9, P491, DOI 10.1145/29873.29875; BAILEY DH, 1991, INT J SUPERCOMPUT AP, V5, P63, DOI 10.1177/109434209100500306; Bello I., 2016, ABS161109940 ARXIV; Bucek J, 2018, COMPANION OF THE 2018 ACM/SPEC INTERNATIONAL CONFERENCE ON PERFORMANCE ENGINEERING (ICPE '18), P41, DOI 10.1145/3185768.3185771; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cummins C, 2017, INT CONFER PARA, P219, DOI 10.1109/PACT.2017.24; Khalil Elias, 2017, ADV NEURAL INFORM PR, P1; Larsen S., 2000, SIGPLAN Notices, V35, P145, DOI 10.1145/358438.349320; Lattner C, 2004, INT SYM CODE GENER, P75, DOI 10.1109/CGO.2004.1281665; Li Yujia, 2015, ARXIV151105493; Li ZW, 2018, ADV NEUR IN, V31; Liu J, 2012, PROCEEDINGS OF THE ASME INTERNATIONAL DESIGN ENGINEERING TECHNICAL CONFERENCES AND COMPUTERS AND INFORMATION IN ENGINEERING CONFERENCE, VOL 6, P347, DOI 10.1145/2345156.2254106; McGovern A, 2002, MACH LEARN, V49, P141, DOI 10.1023/A:1017976211990; Mendis C, 2018, P ACM PROGRAM LANG, V2, DOI 10.1145/3276480; Nuzman D, 2008, PACT'08: PROCEEDINGS OF THE SEVENTEENTH INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, P2, DOI 10.1145/1454115.1454119; Polozov Oleksandr, 2018, ICLR; Porpodas V, 2015, INT CONFER PARA, P432, DOI 10.1109/PACT.2015.32; Porpodas V, 2015, INT SYM CODE GENER, P190, DOI 10.1109/CGO.2015.7054199; Ross St<prime>ephane, 2011, AISTATS; Shin J, 2002, 2002 INTERNATIONAL CONFERENCE ON PARALLEL ARCHITECTURES AND COMPILATION TECHNIQUES, PROCEEDINGS, P45, DOI 10.1109/PACT.2002.1106003; Shin J., 2003, EXPLOITING SUPERWORD, V5; Shin JW, 2005, INT SYM CODE GENER, P165; Sreraman N, 2000, INT J PARALLEL PROG, V28, P363, DOI 10.1023/A:1007559022013; Stephenson M, 2003, ACM SIGPLAN NOTICES, V38, P77, DOI 10.1145/780822.781141; Stock K, 2012, ACM T ARCHIT CODE OP, V8, DOI 10.1145/2086696.2086729; World Health Organization, 2012, SIGARCH COMPUT ARCHI, P1, DOI [DOI 10.1145/1186736.1186737, 10.1145/1186736.1186737]; Zhou J., 2020, OPEN, V1, DOI [10.1016/j.aiopen.2021.01.001, DOI 10.1016/J.AIOPEN.2021.01.001]	28	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906030
C	Metevier, B; Giguere, S; Brockman, S; Kobren, A; Brun, Y; Brunskill, E; Thomas, PS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Metevier, Blossom; Giguere, Stephen; Brockman, Sarah; Kobren, Ari; Brun, Yuriy; Brunskill, Emma; Thomas, Philip S.			Offline Contextual Bandits with High Probability Fairness Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present RobinHood, an offline contextual bandit algorithm designed to satisfy a broad family of fairness constraints. Our algorithm accepts multiple fairness definitions and allows users to construct their own unique fairness definitions for the problem at hand. We provide a theoretical analysis of RobinHood, which includes a proof that it will not return an unfair solution with probability greater than a user-specified threshold. We validate our algorithm on three applications: a user study with an automated tutoring system, a loan approval setting using the Statlog German credit data set, and a criminal recidivism problem using data released by ProPublica. To demonstrate the versatility of our approach, we use multiple well-known and custom definitions of fairness. In each setting, our algorithm is able to produce fair policies that achieve performance competitive with other offline and online contextual bandit algorithms.	[Metevier, Blossom; Giguere, Stephen; Brockman, Sarah; Kobren, Ari; Brun, Yuriy; Thomas, Philip S.] Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Brunskill, Emma] Stanford Univ, Comp Sci Dept, Stanford, CA 94305 USA	University of Massachusetts System; University of Massachusetts Amherst; Stanford University	Metevier, B (corresponding author), Univ Massachusetts Amherst, Coll Informat & Comp Sci, Amherst, MA 01003 USA.				National Science Foundation [CCF-1453474, IIS-1753968, CCF-1763423]; NSF CAREER award; Army Research Laboratory [W911NF-17-2-0196]	National Science Foundation(National Science Foundation (NSF)); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL))	This work is supported by a gift from Adobe and by the National Science Foundation under grants no. CCF-1453474, IIS-1753968, CCF-1763423, and an NSF CAREER award to Brunskill. Research reported in this paper was sponsored in part by the Army Research Laboratory under Cooperative Agreement W911NF-17-2-0196. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government.	Agarwal A, 2018, PR MACH LEARN RES, V80; AMEMIYA T., 1985, ADV ECONOMETRICS; [Anonymous], 1908, BIOMETRIKA, V6, P1; [Anonymous], [No title captured]; Bartlett Robert, 2018, CONSUMER LENDI UNPUB; Berk R, 2021, SOCIOL METHOD RES, V50, P3, DOI 10.1177/0049124118782533; Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129; Byrnes Nanette, 2016, MIT TECHNOLOGY REV; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Chawla NV, 2004, ACM SIGKDD EXPLOR NE, V6, P1, DOI [DOI 10.1145/1007730.1007733, 10.1145/1007730.1007733]; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Cotter A., 2018, ARXIV180700028; de Bruijne M, 2016, MED IMAGE ANAL, V33, P94, DOI 10.1016/j.media.2016.06.032; Doroudi S, 2019, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON LEARNING ANALYTICS & KNOWLEDGE (LAK'19), P335, DOI 10.1145/3303772.3303838; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Efron B., 1994, INTRO BOOTSTRAP; Friedler S. A., 2016, 160907236 ARXIV; Galhotra S, 2017, ESEC/FSE 2017: PROCEEDINGS OF THE 2017 11TH JOINT MEETING ON FOUNDATIONS OF SOFTWARE ENGINEERING, P498, DOI 10.1145/3106237.3106277; Gillen S, 2018, ADV NEUR IN, V31; Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jabbari Shahin, 2016, ARXIV161103071; Joseph M., 2016, ARXIV161009559, V1, P1; Joseph M, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P158, DOI 10.1145/3278721.3278764; Joseph M, 2016, ADV NEUR IN, V29; Kakade Sham M., 2003, THESIS; Kazerouni A, 2017, ADV NEUR IN, V30; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Komorowski M, 2018, NAT MED, V24, P1716, DOI 10.1038/s41591-018-0213-5; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Larson Jeff, 2016, PROPUBLICA; Le HM, 2019, PR MACH LEARN RES, V97; Lichman M, 2013, UCI MACHINE LEARNING; Liu LT, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6196; Massart P, 2007, LECT NOTES MATH, V1896, P1, DOI 10.1007/978-3-540-48503-2; Miller Claire Cain, 2015, NY TIMES; Murray T., 1999, INT J ARTIFICIAL INT, V10, P98; Precup D., 2000, P 17 INT C MACH LEAR; Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987; Rothblum Guy N, 2018, ARXIV180303242; Sen P. K., 1993, LARGE SAMPLE METHODS; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; Tekin C, 2018, IEEE T SIGNAL PROCES, V66, P3799, DOI 10.1109/TSP.2018.2841822; Thomas P. S., 2015, THESIS U MASSACHUSET; Thomas PS, 2019, SCIENCE, V366, P999, DOI 10.1126/science.aag3311; Turgay Eralp, 2018, ARXIV180304015; Zafar Muhammad Bilal, 2015, FAIRNESS ACCOUNTABIL	50	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906056
C	Olascoaga, LIG; Meert, W; Shah, N; Verhelst, M; Van den Broeck, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Olascoaga, Laura I. Galindez; Meert, Wannes; Shah, Nimish; Verhelst, Marian; Van den Broeck, Guy			Towards Hardware-Aware Tractable Learning of Probabilistic Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE	Smart portable applications increasingly rely on edge computing due to privacy and latency concerns. But guaranteeing always-on functionality comes with two major challenges: heavily resource-constrained hardware; and dynamic application conditions. Probabilistic models present an ideal solution to these challenges: they are robust to missing data, allow for joint predictions and have small data needs. In addition, ongoing efforts in the field of tractable learning have resulted in probabilistic models with strict inference efficiency guarantees. However, the current notions of tractability are often limited to model complexity, disregarding the hardware's specifications and constraints. We propose a novel resource-aware cost metric that takes into consideration the hardware's properties in determining whether the inference task can be efficiently deployed. We use this metric to evaluate the performance versus resource trade-off relevant to the application of interest, and we propose a strategy that selects the device settings that can optimally meet users' requirements. We showcase our framework on a mobile activity recognition scenario, and on a variety of benchmark datasets representative of the field of tractable learning and of the applications of interest.	[Olascoaga, Laura I. Galindez; Shah, Nimish; Verhelst, Marian] Katholieke Univ Leuven, Elect Engn Dept, Leuven, Belgium; [Meert, Wannes] Katholieke Univ Leuven, Comp Sci Dept, Leuven, Belgium; [Van den Broeck, Guy] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA	KU Leuven; KU Leuven; University of California System; University of California Los Angeles	Olascoaga, LIG (corresponding author), Katholieke Univ Leuven, Elect Engn Dept, Leuven, Belgium.	laura.galindez@esat.kuleuven.be; wannes.meert@cs.kuleuven.be; nimish.shah@esat.kuleuven.be; marian.verhelst@esat.kuleuven.be; guyvdb@cs.ucla.edu	Shah, Nimish/AAJ-2838-2021; Verhelst, Marian/E-5739-2011; Verhelst, Marian/ABE-4823-2020; Meert, Wannes/C-7019-2013	Shah, Nimish/0000-0003-3234-0715; Verhelst, Marian/0000-0003-3495-9263; Verhelst, Marian/0000-0003-3495-9263; Meert, Wannes/0000-0001-9560-3872	EU ERC Project Re-SENSE [ERC-2016-STG-71503]; NSF [IIS-1633857, CCF-1837129]; DARPA XAI grant [N66001-17-2-4032]; NEC Research; Flemish Government under the "Onderzoeksprogramma Artificile Intelligentie (AI) Vlaanderen" programme	EU ERC Project Re-SENSE; NSF(National Science Foundation (NSF)); DARPA XAI grant; NEC Research; Flemish Government under the "Onderzoeksprogramma Artificile Intelligentie (AI) Vlaanderen" programme	We thank Yitao Liang for the LearnPSDD algorithm and for helpful feedback. This work is partially supported by the EU ERC Project Re-SENSE under Grant ERC-2016-STG-71503; NSF grants #IIS-1633857, #CCF-1837129, DARPA XAI grant #N66001-17-2-4032, NEC Research, and gifts from Intel and Facebook Research. This research received funding from the Flemish Government under the "Onderzoeksprogramma Artificile Intelligentie (AI) Vlaanderen" programme.	Anguita D., 2013, ESANN, V3, P3; [Anonymous], 2019, IEEE STD 1619 2018 R, P1, DOI [10.1109/IEEESTD.2019.8766229, DOI 10.1109/IEEESTD.2019.8766229]; Bach FR, 2002, ADV NEUR IN, V14, P569; Buchfuhrer D, 2008, LECT NOTES COMPUT SC, V5125, P24, DOI 10.1007/978-3-540-70575-8_3; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Darwiche A, 2002, J ARTIF INTELL RES, V17, P229, DOI 10.1613/jair.989; Dua D., 2017, UCI MACHINE LEARNING; Fayyad UM, 1993, P 13 INT JOINT C ART; Fierens D., 2015, THEOR PRACT LOG PROG, V15; Friedman N, 1997, MACH LEARN, V29, P131, DOI 10.1023/A:1007465528199; Galindez L, 2018, IEEE J EM SEL TOP C, V8, P858, DOI 10.1109/JETCAS.2018.2850451; Gens R., 2013, 30 INT C MACHINE LEA, P873; Guyon I., 2003, NIPS FEATURE SELECTI; Heckerman D., 1993, Uncertainty in Artificial Intelligence. Proceedings of the Ninth Conference (1993), P122; Holtzen S., 2019, P ICML WORKSH TRACT; Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323; Howard A. G., 2017, MOBILENETS EFFICIENT; Johnson BA, 2013, INT J REMOTE SENS, V34, P6969, DOI 10.1080/01431161.2013.810825; Khan OU, 2016, IEEE T VLSI SYST, V24, P837, DOI 10.1109/TVLSI.2015.2420663; Khosravi P., 2019, P 28 INT JOINT C ART; Liang H, 2019, LOGOS PNEUMA-CHIN J, P33; Liang YT, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Lowd D., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P334, DOI 10.1109/ICDM.2010.128; Lowd D., 2012, ARXIV12063271; Lowd D., 2013, P 16 INT C ART INT S, P406; Manhaeve R., 2018, ADV NEURAL INFORM PR, V31; Moons B, 2018, IEEE CUST INTEGR CIR; Olascoaga L. I. Galindez, 2019, P NEURIPS WORKSH EN; Pace RK, 1997, STAT PROBABIL LETT, V33, P291, DOI 10.1016/s0167-7152(96)00140-x; Piatkowski N, 2016, NEUROCOMPUTING, V173, P9, DOI 10.1016/j.neucom.2015.01.091; Poon H., 2011, 2011 IEEE International Conference on Computer Vision Workshops (ICCV Workshops), P689, DOI 10.1109/ICCVW.2011.6130310; Rooshenas A, 2014, PR MACH LEARN RES, V32; Schumann J., 2015, INT J PROGNOSTICS HL; Shah N., 2019, P 56 ANN DES AUT C 2, P190; Sim SH, 2018, INT J COOP INF SYST, V27, DOI 10.1142/S0218843018500028; Sommer L, 2018, PR IEEE COMP DESIGN, P350, DOI 10.1109/ICCD.2018.00060; Tan Mingxing, 2018, ARXIV180711626; Tarkoma S, 2014, SMARTPHONE ENERGY CONSUMPTION: MODELING AND OPTIMIZATION, P1, DOI 10.1017/CBO9781107326279; Tschiatschek S, 2015, IEEE T PATTERN ANAL, V37, P774, DOI 10.1109/TPAMI.2014.2353620; Vlasselaer J., 2016, INT J APPROXIMAT JUN; Xu J., 2018, P 35 INT C MACH LEAR; Zermani S., 2015, 2015 IEEE C PROGN HL, P1, DOI [10.1109/ICPHM.2015.7245057, DOI 10.1109/ICPHM.2015.7245057]	43	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905040
C	Rajasegaran, J; Hayat, M; Khan, S; Khan, FS; Shao, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rajasegaran, Jathushan; Hayat, Munawar; Khan, Salman; Khan, Fahad Shahbaz; Shao, Ling			Random Path Selection for Incremental Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Incremental life-long learning is a main challenge towards the long-standing goal of Artificial General Intelligence. In real-life settings, learning tasks arrive in a sequence and machine learning models must continually learn to increment already acquired knowledge. Existing incremental learning approaches, fall well below the state-of-the-art cumulative models that use all training classes at once. In this paper, we propose a random path selection algorithm, called RPS-Net, that progressively chooses optimal paths for the new tasks while encouraging parameter sharing. Since the reuse of previous paths enables forward knowledge transfer, our approach requires a considerably lower computational overhead. As an added novelty, the proposed model integrates knowledge distillation and retrospection along with the path selection strategy to overcome catastrophic forgetting. In order to maintain an equilibrium between previous and newly acquired knowledge, we propose a simple controller to dynamically balance the model plasticity. Through extensive experiments, we demonstrate that the proposed method surpasses the state-of-the-art performance on incremental learning and by utilizing parallel computation this method can run in constant time with nearly the same efficiency as a conventional deep convolutional neural network.	[Rajasegaran, Jathushan; Hayat, Munawar; Khan, Salman; Khan, Fahad Shahbaz; Shao, Ling] Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates		Rajasegaran, J (corresponding author), Incept Inst Artificial Intelligence, Abu Dhabi, U Arab Emirates.	jathushan.rajasegaran@inceptioniai.org; munawar.hayat@inceptioniai.org; salman.khan@inceptioniai.org; fahad.khan@inceptioniai.org; ling.shao@inceptioniai.org	Khan, Fahad Shahbaz/ABD-6646-2021; Hayat, Munawar/AAU-8658-2020; Khan, Salman Hameed/M-4834-2016	Khan, Fahad Shahbaz/0000-0002-4263-3143; Hayat, Munawar/0000-0002-2706-5985; Khan, Salman Hameed/0000-0002-9502-1749				Abraham WC, 2005, TRENDS NEUROSCI, V28, P73, DOI 10.1016/j.tins.2004.12.003; Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9; Chaudhry A, 2018, LECT NOTES COMPUT SC, V11215, P556, DOI 10.1007/978-3-030-01252-6_33; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Gepperth A, 2016, COGN COMPUT, V8, P924, DOI 10.1007/s12559-016-9389-5; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Hou SH, 2018, LECT NOTES COMPUT SC, V11207, P452, DOI 10.1007/978-3-030-01219-9_27; Hsu Y.-C., 2018, RE EVALUATING CONTIN; Kamra Nitin, 2017, ABS171010368 CORR; Kemker R, 2018, AAAI CONF ARTIF INTE, P3390; Kemker Ronald, 2018, P INT C LEARN REPR I, P2; Khan S, 2018, GUIDE CONVOLUTIONAL, V8, P1, DOI [DOI 10.2200/S00822ED1V01Y201712COV015, 10.2200/S00822ED1V01Y201712COV015]; Kingma D.P, P 3 INT C LEARNING R; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lopez-Paz D, 2017, ADV NEUR IN, V30; Parisi G. I., 2018, CORR; Pham H, 2018, PR MACH LEARN RES, V80; Rebuffi Sylvestre-Alvise, 2017, PROC CVPR IEEE, P8, DOI DOI 10.1109/CVPR.2017.587; Rusu A. A., 2016, ARXIV160604671; Schwarz J, 2018, PR MACH LEARN RES, V80; Shin H, 2017, ADV NEUR IN, V30; van de Ven Gido M, 2018, ARXIV180910635; Xiao TJ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P177, DOI 10.1145/2647868.2654926; Xie SN, 2019, IEEE I CONF COMP VIS, P1284, DOI 10.1109/ICCV.2019.00137; Zenke F, 2017, PR MACH LEARN RES, V70; Zhang Jiawei, 2019, ABS190905729 ARXIV; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	30	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904033
C	Rowland, M; Omidshafiei, S; Tuyls, K; Perolat, J; Valko, M; Piliouras, G; Munos, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rowland, Mark; Omidshafiei, Shayegan; Tuyls, Karl; Perolat, Julien; Valko, Michal; Piliouras, Georgios; Munos, Remi			Multiagent Evaluation under Incomplete Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper investigates the evaluation of learned multiagent strategies in the incomplete information setting, which plays a critical role in ranking and training of agents. Traditionally, researchers have relied on Elo ratings for this purpose, with recent works also using methods based on Nash equilibria. Unfortunately, Elo is unable to handle intransitive agent interactions, and other techniques are restricted to zero-sum, two-player settings or are limited by the fact that the Nash equilibrium is intractable to compute. Recently, a ranking method called a-Rank, relying on a new graph-based game-theoretic solution concept, was shown to tractably apply to general games. However, evaluations based on Elo or a-Rank typically assume noise-free game outcomes, despite the data often being collected from noisy simulations, making this assumption unrealistic in practice. This paper investigates multiagent evaluation in the incomplete information regime, involving general-sum many-player games with noisy outcomes. We derive sample complexity guarantees required to confidently rank agents in this setting. We propose adaptive algorithms for accurate ranking, provide correctness and sample complexity guarantees, then introduce a means of connecting uncertainties in noisy match outcomes to uncertainties in rankings. We evaluate the performance of these approaches in several domains, including Bernoulli games, a soccer meta-game, and Kuhn poker.	[Rowland, Mark; Perolat, Julien] DeepMind London, London, England; [Omidshafiei, Shayegan; Tuyls, Karl; Valko, Michal; Munos, Remi] DeepMind Paris, Paris, France; [Piliouras, Georgios] Singapore Univ Technol & Design, Singapore, Singapore	Singapore University of Technology & Design	Rowland, M (corresponding author), DeepMind London, London, England.	markrowland@google.com; somidshafiei@google.com; karltuyls@google.com; perolat@google.com; valkom@deepmind.com; georgios@sutd.edu.sg; munos@google.com			MOE AcRF Tier 2 Grant [2016-T2-1-170, PIE-SGP-AI-2018-01]; NRF 2018 Fellowship [NRF-NRFF2018-07]	MOE AcRF Tier 2 Grant; NRF 2018 Fellowship	We thank Daniel Hennes and Thore Graepel for extensive feedback on an earlier version of this paper, and the anonymous reviewers for their comments and suggestions to improve the paper. Georgios Piliouras acknowledges MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and NRF 2018 Fellowship NRF-NRFF2018-07.	Aghassi M, 2006, MATH PROGRAM, V107, P231, DOI 10.1007/s10107-005-0686-0; Arneson B, 2010, IEEE T COMP INTEL AI, V2, P251, DOI 10.1109/TCIAIG.2010.2067212; Arpad E, 1978, RATING CHESSPLAYERS; Balduzzi David, 2018, ADV NEURAL INFORM PR; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Chao XR, 2018, EUR J OPER RES, V265, P239, DOI 10.1016/j.ejor.2017.07.030; Clopper CJ, 1934, BIOMETRIKA, V26, P404, DOI 10.2307/2331986; Como G, 2015, IEEE TRANS NETW SCI, V2, P53, DOI 10.1109/TNSE.2015.2438818; Coulom R, 2008, LECT NOTES COMPUT SC, V5131, P113, DOI 10.1007/978-3-540-87608-3_11; Csaji BC, 2014, DISCRETE APPL MATH, V169, P73, DOI 10.1016/j.dam.2014.01.007; Csdji Balzs Csand, 2010, INT C ALG LEARN THEO; Daskalakis C., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P71, DOI 10.1145/1132516.1132527; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Fagin R, 2006, SIAM J DISCRETE MATH, V20, P628, DOI 10.1137/05063088X; Fearnley J, 2015, J MACH LEARN RES, V16, P1305; Fercoq O, 2013, IEEE T AUTOMAT CONTR, V58, P134, DOI 10.1109/TAC.2012.2226103; Gabillon V., 2012, ADV NEURAL INFORM PR, P3212; Garivier Aur6lien, 2011, P C LEARN THEOR COLT; Gruslys Audrunas, 2018, P INT C LEARN REPR I; Harsanyi J. C., 1988, GEN THEORY EQUILIBRI; Heinrich Johannes, 2015, INT C MACH LEARN; Hennes Daniel, 2013, AAMAS WORKSH AUT ROB; Herbrich Ralf, 2007, ADV NEURAL INFORM PR; Hollanders Romain, 2014, INT S MATH THEOR NET; Jaderberg M, 2019, SCIENCE, V364, P859, DOI 10.1126/science.aau6249; Jaderberg Max, 2017, ABS171109846 CORR; Jain Prateek, 2013, P ACM S THEOR COMP S; Jordan Patrick R, 2008, P INT C AUT AG MULT; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Lai M., 2015, ARXIV150901549; Lanctot M, 2017, ADV NEUR IN, V30; Liu Siqi, 2019, P INT C LEARN REPR I; McMahan H. Brendan, 2003, P INT C MACH LEARN I; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Omidshafiei S, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-45619-9; Phelps Steve, 2007, ADAPTIVE AGENTS MULT; Phelps Steve, 2004, AGENT MEDIATED ELECT; Ponsen M, 2009, ENTERTAIN COMPUT, V1, P39, DOI 10.1016/j.entcom.2009.09.002; Prakash Achintya, 2015, P 2 ACM WORKSH MOV T; Silver D, 2018, SCIENCE, V362, P1140, DOI 10.1126/science.aar6404; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sokota S, 2019, AAAI CONF ARTIF INTE, P2173; Solan E, 2003, J APPL PROBAB, V40, P107, DOI 10.1017/S0021900200022294; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Tuyls K, 2007, ARTIF INTELL, V171, P406, DOI 10.1016/j.artint.2007.01.004; Tuyls K, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-19194-4; Tuyls Karl, 2018, P INT C AUT AG MULT; Vorobeychik Y, 2010, ACM T MODEL COMPUT S, V20, DOI 10.1145/1842713.1842719; Walsh William E., 2003, P 5 WORKSH AG MED EL; Walsh William E., 2002, AAAI WORKSH GAM THEO; Wellman Michael P., 2006, P NAT C ART INT INN; Wellman Michael P., 2013, P 12 WORKSH EC INF S; Wiedenbeck Bryce, 2012, P INT C AUT AG MULT; Zhou YC, 2017, PR MACH LEARN RES, V70	56	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903086
C	Saralajew, S; Holdijk, L; Rees, M; Asan, E; Villmann, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Saralajew, Sascha; Holdijk, Lars; Rees, Maike; Asan, Ebubekir; Villmann, Thomas			Classification-by-Components: Probabilistic Modeling of Reasoning over a Set of Components	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural networks are state-of-the-art classification approaches but are generally difficult to interpret. This issue can be partly alleviated by constructing a precise decision process within the neural network. In this work, a network architecture, denoted as Classification-By-Components network (CBC), is proposed. It is restricted to follow an intuitive reasoning based decision process inspired by BIEDERMAN's recognition-by-components theory from cognitive psychology. The network is trained to learn and detect generic components that characterize objects. In parallel, a class-wise reasoning strategy based on these components is learned to solve the classification problem. In contrast to other work on reasoning, we propose three different types of reasoning: positive, negative, and indefinite. These three types together form a probability space to provide a probabilistic classifier. The decomposition of objects into generic components combined with the probabilistic reasoning provides by design a clear interpretation of the classification decision process. The evaluation of the approach on MNIST shows that CBCs are viable classifiers. Additionally, we demonstrate that the inherent interpretability offers a profound understanding of the classification behavior such that we can explain the success of an adversarial attack. The method's scalability is successfully tested using the IMAGENET dataset.	[Saralajew, Sascha; Holdijk, Lars; Rees, Maike; Asan, Ebubekir] Dr Ingn Hc Porsche AG, Weissach, Germany; [Villmann, Thomas] Univ Appl Sci Mittweida, Mittweida, Germany		Saralajew, S (corresponding author), Dr Ingn Hc Porsche AG, Weissach, Germany.	sascha.saralajew@porsche.de; thomas.villmann@hs-mittweida.de						Adebayo J., 2018, NEURIPS, P9505; Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Arik S. O., 2019, ARXIV190206292; Bertinetto L, 2016, LECT NOTES COMPUT SC, V9914, P850, DOI 10.1007/978-3-319-48881-3_56; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Biehl M, 2016, WIRES COGN SCI, V7, P92, DOI 10.1002/wcs.1378; Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Brendel Wieland, 2018, INT C LEARN REPR; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Chen Chaofan, 2018, ARXIV180610574; Chen XL, 2018, PROC CVPR IEEE, P7239, DOI 10.1109/CVPR.2018.00756; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Crammer K., 2003, ADV NEURAL INFORM PR, V15, P479; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Erhan D, 2009, BERNOULLI, V1341, P1; Gal Y, 2016, PR MACH LEARN RES, V48; Geirhos R., 2019, IMAGENET TRAINED CNN; Gershman S.J., 2017, BEHAV BRAIN SCI, V40; Ghiasi-Shirazi Kamaledin, 2019, NEURAL PROCESS LETT, P1; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Goodfellow I. J., 2015, P ICLR; Ilyas A, 2019, ADV NEUR IN, V32; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jiang C, 2018, IEEE INT SEMICONDUCT, P159; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Koch G., 2015, INT C MACH LEARN DEE; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li O, 2018, AAAI CONF ARTIF INTE, P3530; Marino K., 2017, **DROPPED REF**, p2673 2681, DOI DOI 10.1109/CVPR.2017.10; Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35; Nguyen A., 2019, ARXIV190408939; Papernot N, 2018, ARXIV180304765; Plotz T., 2018, ADV NEURAL INFORM PR, P1093; Ramachandran P., 2017, SEARCHING ACTIVATION; Rudin C, 2019, NAT MACH INTELL, V1, P206, DOI 10.1038/s42256-019-0048-x; Salakhutdinov R., 2007, P ART INT STAT, P412; Santoro A, 2016, PR MACH LEARN RES, V48; Saralajew S., 2018, ARXIV181201214; Sato A, 1996, ADV NEUR IN, V8, P423; Shafer G., 1976, MATH THEORY EVIDENCE, VVolume 1; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Tokmakov P., 2018, ARXIV181209213; Villmann T, 2017, J ARTIF INTELL SOFT, V7, P65, DOI 10.1515/jaiscr-2017-0005; Vinyals Oriol, 2016, ARXIV160604080, P3630; Yang HM, 2018, PROC CVPR IEEE, P3474, DOI 10.1109/CVPR.2018.00366; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou B, 2016, PROC CVPR IEEE, P2921, DOI 10.1109/CVPR.2016.319	49	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302075
C	Schwartz, D; Toneva, M; Wehbe, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schwartz, Dan; Toneva, Mariya; Wehbe, Leila			Inducing brain-relevant bias in natural language processing models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INTERFERENCE; REVEALS; MEG	Progress in natural language processing (NLP) models that estimate representations of word sequences has recently been leveraged to improve the understanding of language processing in the brain. However, these models have not been specifically designed to capture the way the brain represents language meaning. We hypothesize that fine-tuning these models to predict recordings of brain activity of people reading text will lead to representations that encode more brain-activity-relevant language information. We demonstrate that a version of BERT, a recently introduced and powerful language model, can improve the prediction of brain activity after fine-tuning. We show that the relationship between language and brain activity learned by BERT during this fine-tuning transfers across multiple participants. We also show that, for some participants, the fine-tuned representations learned from both magnetoencephalography (MEG) and functional magnetic resonance imaging (fMRI) are better for predicting fMRI than the representations learned from fMRI alone, indicating that the learned representations capture brain-activity-relevant information that is not simply an artifact of the modality. While changes to language representations help the model predict brain activity, they also do not harm the model's ability to perform downstream NLP tasks. Our findings are notable for research on language understanding in the brain.	[Schwartz, Dan; Toneva, Mariya; Wehbe, Leila] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Schwartz, D (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	drschwar@cs.cmu.edu; mariya@cmu.edu; lwehbe@cmu.edu	Toneva, Mariya/CAH-3617-2022	Toneva, Mariya/0000-0002-2407-9871	National Institutes of Health [U01NS098969]; National Science Foundation Graduate Research Fellowship [DGE1745016]	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	This work is supported in part by National Institutes of Health grant no. U01NS098969 and in part by the National Science Foundation Graduate Research Fellowship under Grant No. DGE1745016.	BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Cichy RM, 2016, CEREB CORTEX, V26, P3563, DOI 10.1093/cercor/bhw135; Clark K, 2019, ARXIV190604341; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Fedorenko E, 2014, TRENDS COGN SCI, V18, P120, DOI 10.1016/j.tics.2013.12.006; Fedorenko E, 2010, J NEUROPHYSIOL, V104, P1177, DOI 10.1152/jn.00032.2010; Fischl B, 2012, NEUROIMAGE, V62, P774, DOI 10.1016/j.neuroimage.2012.01.021; Fyshe A, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P489; Gao JS, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00023; He B, 2018, ANNU REV BIOMED ENG, V20, P171, DOI 10.1146/annurev-bioeng-062117-120853; Huth AG, 2016, NATURE, V532, P453, DOI 10.1038/nature17637; Jain Shailee, 2018, BIORXIV; Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713; Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876; Murphy Brian, 2012, P 1 JOINT C LEX COMP, P114; Nishimoto S., 2011, CURRENT BIOL; Palatucci M.M., 2011, THOUGHT RECOGNITION; Pereira F, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03068-4; Rayner K, 1998, PSYCHOL BULL, V124, P372, DOI 10.1037/0033-2909.124.3.372; Rowling J. K., 1999, HARRY POTTER SORCERE, V1; Schwartz D., 2019, LONG SHORT PAPERS, V1, P43; Taulu S, 2006, PHYS MED BIOL, V51, P1759, DOI 10.1088/0031-9155/51/7/008; Taulu S, 2004, BRAIN TOPOGR, V16, P269, DOI 10.1023/b:brat.0000032864.93890.f9; VANPETTEN C, 1990, MEM COGNITION, V18, P380, DOI 10.3758/BF03197127; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wang A., 2018, P 2018 EMNLP WORKSH; Wang SY, 2017, IEEE INT CONF AUTOMA, P216, DOI 10.1109/FG.2017.35; Wehbe L., 2014, P 2014 C EMP METH NA, DOI 10.3115/v1/D14-1030; Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	30	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905074
C	Simon, D; Elad, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Simon, Dror; Elad, Michael			Rethinking the CSC Model for Natural Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPARSE; ALGORITHMS	Sparse representation with respect to an overcomplete dictionary is often used when regularizing inverse problems in signal and image processing. In recent years, the Convolutional Sparse Coding (CSC) model, in which the dictionary consists of shift invariant filters, has gained renewed interest. While this model has been successfully used in some image processing problems, it still falls behind traditional patch-based methods on simple tasks such as denoising. In this work we provide new insights regarding the CSC model and its capability to represent natural images, and suggest a Bayesian connection between this model and its patch-based ancestor. Armed with these observations, we suggest a novel feed-forward network that follows an MMSE approximation process to the CSC model, using strided convolutions. The performance of this supervised architecture is shown to be on par with state of the art methods while using much fewer parameters.	[Simon, Dror; Elad, Michael] Technion, Dept Comp Sci, Haifa, Israel	Technion Israel Institute of Technology	Simon, D (corresponding author), Technion, Dept Comp Sci, Haifa, Israel.	dror.simon@cs.technion.ac.il; elad@cs.technion.ac.il	, Miki/AAH-4640-2019		Technion Hiroshi Fujiwara Cyber Security Research Center; Israel Cyber Directorate	Technion Hiroshi Fujiwara Cyber Security Research Center; Israel Cyber Directorate	The research leading to these results has received funding from the Technion Hiroshi Fujiwara Cyber Security Research Center and the Israel Cyber Directorate.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; [Anonymous], [No title captured]; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Batenkov D, 2017, APPL NUMER HARMON AN, P1, DOI 10.1007/978-3-319-69802-1_1; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; BURGER HC, 2012, PROC CVPR IEEE, P2392, DOI DOI 10.1109/CVPR.2012.6247952; Carrera D, 2017, IEEE SIGNAL PROC LET, V24, P1468, DOI 10.1109/LSP.2017.2734119; CHEN SB, 1994, CONF REC ASILOMAR C, P41, DOI 10.1109/ACSSC.1994.471413; Chen YJ, 2017, IEEE T PATTERN ANAL, V39, P1256, DOI 10.1109/TPAMI.2016.2596743; Chun IY, 2018, IEEE T IMAGE PROCESS, V27, P1697, DOI 10.1109/TIP.2017.2761545; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Dong WS, 2011, IEEE T IMAGE PROCESS, V20, P1838, DOI 10.1109/TIP.2011.2108306; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Elad M, 2009, IEEE T INFORM THEORY, V55, P4701, DOI 10.1109/TIT.2009.2027565; Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624; Garcia-Cardona C., 2018, IEEE T COMPUTATIONAL; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Grosse R., 2007, P 23 C UNCERTAINTY A, P149; Gu SH, 2015, IEEE I CONF COMP VIS, P1823, DOI 10.1109/ICCV.2015.212; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149; Kingma D.P, P 3 INT C LEARNING R; Larsson EG, 2007, IEEE T SIGNAL PROCES, V55, P451, DOI 10.1109/TSP.2006.887109; Lewis AS, 1992, IEEE T IMAGE PROCESS, V1, P244, DOI 10.1109/83.136601; Liu D., 2018, P 32 C NEURAL INFORM, P1673; Liu Y, 2016, IEEE SIGNAL PROC LET, V23, P1882, DOI 10.1109/LSP.2016.2618776; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Mairal J, 2009, IEEE I CONF COMP VIS, P2272, DOI 10.1109/ICCV.2009.5459452; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Papyan V, 2017, IEEE I CONF COMP VIS, P5306, DOI 10.1109/ICCV.2017.566; Plaut E, 2019, SIAM J IMAGING SCI, V12, P186, DOI 10.1137/18M1165116; Plumbley MD, 2010, P IEEE, V98, P995, DOI 10.1109/JPROC.2009.2030345; Ravishankar S, 2011, IEEE T MED IMAGING, V30, P1028, DOI 10.1109/TMI.2010.2090538; Rey-Otero I., 2018, ARXIV181001169; Rolfe J. T., 2013, DISCRIMINATIVE RECUR; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Schneider P, 2008, PSYCHOANAL PSYCHOL, V25, P326, DOI 10.1037/0736-9735.25.2.326; Silva G, 2017, INT CONF ACOUST SPEE, P6035, DOI 10.1109/ICASSP.2017.7953315; Simon D, 2019, IEEE T SIGNAL PROCES, V67, P4597, DOI 10.1109/TSP.2019.2929464; Sreter H, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2191; Sulam J, 2015, LECT NOTES COMPUT SC, V8932, P99, DOI 10.1007/978-3-319-14612-6_8; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szlam A., 2010, ARXIV10100422; Wang ZW, 2015, IEEE I CONF COMP VIS, P370, DOI 10.1109/ICCV.2015.50; Wohlberg B, 2016, IEEE T IMAGE PROCESS, V25, P301, DOI 10.1109/TIP.2015.2495260; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zisselman E., 2019, C COMP VIS PATT REC; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	55	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302029
C	Snyder, C; Do, MN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Snyder, Corey; Do, Minh N.			STREETS: A Novel Camera Network Dataset for Traffic Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VISION	In this paper, we introduce STREETS, a novel traffic flow dataset from publicly available web cameras in the suburbs of Chicago, IL. We seek to address the limitations of existing datasets in this area. Many such datasets lack a coherent traffic network graph to describe the relationship between sensors. The datasets that do provide a graph depict traffic flow in urban population centers or highway systems and use costly sensors like induction loops. These contexts differ from that of a suburban traffic body. Our dataset provides over 4 million still images across 2.5 months and one hundred web cameras in suburban Lake County, IL. We divide the cameras into two distinct communities described by directed graphs and count vehicles to track traffic statistics. Our goal is to give researchers a benchmark dataset for exploring the capabilities of inexpensive and non-invasive sensors like web cameras to understand complex traffic bodies in communities of any size. We present benchmarking tasks and baseline results for one such task to guide how future work may use our dataset.	[Snyder, Corey; Do, Minh N.] Univ Illinois, Chicago, IL 60680 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Snyder, C (corresponding author), Univ Illinois, Chicago, IL 60680 USA.	cesnyde2@illinois.edu; minhdo@illinois.edu	N., Minh/AAX-8498-2020	N., Minh/0000-0001-5132-4986	Lab Directed Research and Development grant from Sandia National Laboratories	Lab Directed Research and Development grant from Sandia National Laboratories	This work was partly supported by a Lab Directed Research and Development grant from Sandia National Laboratories<SUP>3</SUP>. We would also like to thank Lake County Passage and its employees for their cooperation and answering our questions while performing this work.	Abdulla W., 2017, GITHUB REPOSITORY, DOI DOI 10.1109/CVPR.2017.106; [Anonymous], 2015, CORR; Argote-Cabanero J, 2015, TRANSPORT RES C-EMER, V60, P298, DOI 10.1016/j.trc.2015.08.013; California Department of Transportation, 2018, PEF MEAS SYST PEMS D; Chan FH, 2017, LECT NOTES COMPUT SC, V10114, P136, DOI 10.1007/978-3-319-54190-7_9; Chen Siheng, 2016, CORR; Chen ZZ, 2012, IEEE INT C INTELL TR, P951, DOI 10.1109/ITSC.2012.6338852; Cheng X., 2017, CORR; Choe T. E., 2010, 1 IEEE WORKSH CAM NE, P9; Cui Z., 2018, CORR; Dan Donovan Brian, 2016, BRAIN WORK NEW YORK; Datondji SRE, 2016, IEEE T INTELL TRANSP, V17, P2681, DOI 10.1109/TITS.2016.2530146; Deri JA, 2015, 2015 49TH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS AND COMPUTERS, P1829, DOI 10.1109/ACSSC.2015.7421468; Dong XW, 2013, INT CONF ACOUST SPEE, P3118, DOI 10.1109/ICASSP.2013.6638232; Du SD, 2017, 2017 12TH INTERNATIONAL CONFERENCE ON INTELLIGENT SYSTEMS AND KNOWLEDGE ENGINEERING (IEEE ISKE); Ericson Lars, 2019, IARPARFI1906; Garg K., 2016, PROC IEEE WINTER C A, P1; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; He K., 2017, CORR; He K., 2015, CORR; Herrman John, 2018, GOOGLE KNOWS YOUVE B; Hu Shan, 2012, J INFORM COMPUTATION, V9, P2907; Hu WM, 2006, IEEE T PATTERN ANAL, V28, P1450, DOI 10.1109/TPAMI.2006.176; Jodoin JP, 2014, IEEE WINT CONF APPL, P885, DOI 10.1109/WACV.2014.6836010; Lenze David G., 2018, LOCAL AREA PERSONAL; Li Y., 2017, CORR; Lopez-Sastre Roberto, 2015, IB C PATT REC IM AN; Loy CC, 2011, PATTERN RECOGN, V44, P117, DOI 10.1016/j.patcog.2010.07.023; Mohan DM, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1707, DOI 10.1109/ITSC.2014.6957939; Morris BT, 2011, IEEE T PATTERN ANAL, V33, P2287, DOI 10.1109/TPAMI.2011.64; Nagel H H, KIT INTERSECTION MON; Ozkurt Celil, 2009, AUTOMATIC TRAFFIC DE; Ren S., 2015, CORR; Rossi RA, 2015, AAAI CONF ARTIF INTE, P4292; Shah Ankit, 2018, CORR; Snyder Corey, 2019, DATA STREETS NOVEL C; Strigel E, 2014, 2014 IEEE 17TH INTERNATIONAL CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS (ITSC), P1900, DOI 10.1109/ITSC.2014.6957976; Tewolde G.S., 2012, 2012 IEEE INT C EL I, P1; Toropov E, 2015, IEEE IMAGE PROC, P3802, DOI 10.1109/ICIP.2015.7351516; Treiber M., 2013, TRAFFIC FLOW DYNAMIC, DOI [10.1007/978-3-642-32460-4, DOI 10.1007/978-3-642-32460-4]; Valdivia P, 2015, IEEE CONF VIS ANAL, P1, DOI 10.1109/VAST.2015.7347624; Vlahogianni EI, 2014, TRANSPORT RES C-EMER, V43, P3, DOI 10.1016/j.trc.2014.01.005; Wang XG, 2009, IEEE T PATTERN ANAL, V31, P539, DOI 10.1109/TPAMI.2008.87; Wen L., 2015, CORR; Yu Bing, 2017, CORR; Zhang S., 2017, CORR	48	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901083
C	Song, Y; Meng, CL; Ermon, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Yang; Meng, Chenlin; Ermon, Stefano			MintNet: Building Invertible Neural Networks with Masked Convolutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new way of constructing invertible neural networks by combining simple building blocks with a novel set of composition rules. This leads to a rich set of invertible architectures, including those similar to ResNets. Inversion is achieved with a locally convergent iterative procedure that is parallelizable and very fast in practice. Additionally, the determinant of the Jacobian can be computed analytically and efficiently, enabling their generative use as flow models. To demonstrate their flexibility, we show that our invertible neural networks are competitive with ResNets on MNIST and CIFAR-10 classification. When trained as generative models, our invertible networks achieve competitive likelihoods on MNIST, CIFAR-10 and ImageNet 32 x 32, with bits per dimension of 0.98, 3.32 and 4.06 respectively.	[Song, Yang; Meng, Chenlin; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Song, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yangsong@cs.stanford.edu; chenlin@cs.stanford.edu; ermon@cs.stanford.edu			Intel Corporation; NSF [1651565, 1522054, 1733686]; ONR [N00014-19-1-2145]; AFOSR [FA9550-19-1-0024]; Amazon AWS; TRI	Intel Corporation(Intel Corporation); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Amazon AWS; TRI	This research was supported by Intel Corporation, Amazon AWS, TRI, NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024).	Behrmann J., 2019, ARXIV181100995; Chen R. T., 2018, ADV NEURAL INFORM PR, P6571; Dinh L., 2015, 3 INT C LEARN REPR I; Dinh L., 2016, ARXIV160508803CSSTAT; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Grathwohl Will, 2018, ARXIV181001367; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Ho Jonathan, 2019, FLOW IMPROVING FLOW; Hoffman M.D, 2018, INT C LEARN REPR; Hoogeboom E, 2019, PR MACH LEARN RES, V97; Jacobsen Joern-Henrik, 2019, ICLR; Jacobsen Jorn-Henrik, 2018, INT C LEARN REPR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D. P., 2018, PROC NEURIPS; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Loshchilov Ilya, 2016, INT C LEARN REPR; Ma Xuezhe, 2019, ARXIV190204208; MacKay M., 2018, ADV NEURAL INFORM PR, V31, P9029; Ortega J. M., 1970, ITERATIVE SOLUTION N, V30; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Phuong TT, 2019, ARXIV190403590; Reddi S.J., 2018, INT C LEARN REPR; Rezende D., 2015, ICML, P1530; Song J., 2017, ADV NEURAL INFORM PR, P5140; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; van den Berg Rianne, 2018, ARXIV180305649; van den Oord A, 2016, PR MACH LEARN RES, V48	27	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902061
C	Titsias, MK; Dellaportas, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Titsias, Michalis K.; Dellaportas, Petros			Gradient-based Adaptive Markov Chain Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				METROPOLIS ALGORITHMS; ERGODICITY	We introduce a gradient-based learning method to automatically adapt Markov chain Monte Carlo (MCMC) proposal distributions to intractable targets. We define a maximum entropy regularised objective function, referred to as generalised speed measure, which can be robustly optimised over the parameters of the proposal distribution by applying stochastic gradient optimisation. An advantage of our method compared to traditional adaptive MCMC methods is that the adaptation occurs even when candidate state values are rejected. This is a highly desirable property of any adaptation strategy because the adaptation starts in early iterations even if the initial proposal distribution is far from optimum. We apply the framework for learning multivariate random walk Metropolis and Metropolis-adjusted Langevin proposals with full covariance matrices, and provide empirical evidence that our method can outperform other MCMC algorithms, including Hamiltonian Monte Carlo schemes.	[Titsias, Michalis K.] DeepMind, London, England; [Dellaportas, Petros] UCL, Dept Stat Sci, London, England; [Dellaportas, Petros] Athens Univ Econ & Business, Dept Stat, Athens, Greece; [Dellaportas, Petros] Alan Turing Inst, London, England	University of London; University College London; Athens University of Economics & Business	Titsias, MK (corresponding author), DeepMind, London, England.	mtitsias@google.com	Dellaportas, Petros/AAI-7042-2021					Andrieu C, 2007, ELECTRON COMMUN PROB, V12, P336, DOI 10.1214/ECP.v12-1320; Andrieu C, 2006, ANN APPL PROBAB, V16, P1462, DOI 10.1214/105051606000000286; Andrieu C, 2008, STAT COMPUT, V18, P343, DOI 10.1007/s11222-008-9110-y; Atchade Y., 2009, PREPRINT; Atchade YF, 2005, BERNOULLI, V11, P815, DOI 10.3150/bj/1130077595; Bedard M, 2008, J COMPUT GRAPH STAT, V17, P312, DOI 10.1198/108571108X319970; Bedard M, 2007, ANN APPL PROBAB, V17, P1222, DOI 10.1214/105051607000000096; Bedard M, 2008, STOCH PROC APPL, V118, P2198, DOI 10.1016/j.spa.2007.12.005; Beskos A, 2013, BERNOULLI, V19, P1501, DOI 10.3150/12-BEJ414; Bishop CM, 2006, PATTERN RECOGNITION; Carbonetto P., 2009, ADV NEURAL INFORM PR, V22; Espeholt L, 2018, PR MACH LEARN RES, V80; Giordani P, 2010, J COMPUT GRAPH STAT, V19, P243, DOI 10.1198/jcgs.2009.07174; Haario H, 2005, COMPUTATION STAT, V20, P265, DOI 10.1007/BF02789703; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Habib Raza, 2019, ICLR 2019; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Jordan M. I., 1999, LEARNING GRAPHICAL M; Kingma D.P, P 3 INT C LEARNING R; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Levy D., 2018, INT C LEARN REPR VAN; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Mnih V, 2016, PR MACH LEARN RES, V48; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neklyudov K., 2018, ARXIV181007151; Paisley J., 2012, ARXIV12066430; Pasarica C, 2010, STAT SINICA, V20, P343; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; Roberts GO, 2007, J APPL PROBAB, V44, P458, DOI 10.1239/jap/1183667414; Roberts GO, 2009, J COMPUT GRAPH STAT, V18, P349, DOI 10.1198/jcgs.2009.06134; Roberts GO, 1998, J ROY STAT SOC B, V60, P255, DOI 10.1111/1467-9868.00123; Roberts GO, 2001, STAT SCI, V16, P351, DOI 10.1214/ss/1015346320; Rosenthal Jeffrey S, 2011, HDB MARKOV CHAIN MON, P114; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Schulman J, 2015, P 32 INT C MACH LEAR; Song JM, 2017, ADV NEUR IN, V30; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971	40	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907039
C	Tu, ZZ; Zhang, JW; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tu, Zhuozhuo; Zhang, Jingwei; Tao, Dacheng			Theoretical Analysis of Adversarial Learning: A Minimax Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COVERING NUMBER; STABILITY	In this paper, we propose a general theoretical method for analyzing the risk bound in the presence of adversaries. Specifically, we try to fit the adversarial learning problem into the minimax framework. We first show that the original adversarial learning problem can be transformed into a minimax statistical learning problem by introducing a transport map between distributions. Then, we prove a new risk bound for this minimax problem in terms of covering numbers under a weak version of Lipschitz condition. Our method can be applied to multi-class classification and popular loss functions including the hinge loss and ramp loss. As some illustrative examples, we derive the adversarial risk bounds for SVMs and deep neural networks, and our bounds have two data-dependent terms, which can be optimized for achieving adversarial robustness.	[Tu, Zhuozhuo; Zhang, Jingwei; Tao, Dacheng] Univ Sydney, UBTECH Sydney AI Ctr, Sch Comp Sci, Sydney, NSW, Australia; [Zhang, Jingwei] HKUST, Dept Comp Sci & Engn, Hong Kong, Peoples R China	University of Sydney; Hong Kong University of Science & Technology	Tu, ZZ (corresponding author), Univ Sydney, UBTECH Sydney AI Ctr, Sch Comp Sci, Sydney, NSW, Australia.	zhtu3055@uni.sydney.edu.au; jzhangey@cse.ust.hk; dacheng.tao@sydney.edu.au			ARC [FL-170100117]	ARC(Australian Research Council)	We thank the reviewers for their constructive comments that helped improve the paper significantly. This work was supported by the ARC FL-170100117.	Alabdulmohsin I. M., 2015, ADV NEURAL INFORM PR, P19; Ambroladze Amiran, 2007, ADV NEURAL INFORM PR, P9; [Anonymous], 2017, ADV NEURAL INFORM PR; Arora Sanjeev, 2018, ARXIV180205296; Athalye A., 2018, P 35 INT C MACH LEAR; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Biggio Battista, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P387, DOI 10.1007/978-3-642-40994-3_25; Biggio B., 2012, ARXIV12066389; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; Cullina D., 2018, ADV NEURAL INFORM PR, P228; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Dekel O, 2010, MACH LEARN, V81, P149, DOI 10.1007/s10994-009-5124-8; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Gao R., 2016, MATH OPER RES; Gilmer J., 2018, INT C LEARN REPR WOR; Globerson Amir, 2006, P 23 INT C MACH LEAR, P353, DOI DOI 10.1145/1143844.1143889; Goodfellow I. J., 2014, ARXIV14126572; Gu S., 2014, ARXIV14125068; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Herbrich R, 2003, J MACH LEARN RES, V3, P175, DOI 10.1162/153244303765208368; Khim J., 2018, ARXIV181009519V2; KHIM J, 2018, ARXIV PREPRINT ARXIV; Langford J, 2005, J MACH LEARN RES, V6, P273; Lee J., 2017, ARXIV170507815; Lee Jaeho, 2018, ADV NEURAL INFORM PR, P2692; Liu T., 2017, ARXIV170208712; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Lowd D., 2005, CEAS, V2005; Madry Aleksander, 2018, ICLR; Mohri M., 2018, FDN MACHINE LEARNING; Neyshabur Behnam, 2017, ARXIV170709564; Raghunathan Aditi, 2018, INT C LEARN REPR; Russo D, 2016, JMLR WORKSH CONF PRO, V51, P1232; Schmidt Ludwig, 2018, ARXIV180411285; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Sinha A., 2018, ICLR; Suggala A. S., 2018, ARXIV180602924; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Talagrand Michel, 2014, UPPER LOWER BOUNDS S, V60; van den Oord Aaron, 2018, ARXIV180205666; Vapnik V., 2013, NATURE STAT LEARNING; Vapnik VN, 1999, IEEE T NEURAL NETWOR, V10, P988, DOI 10.1109/72.788640; Wang Y., 2017, ARXIV170603922; Wong E, 2018, PR MACH LEARN RES, V80; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Yin D., 2018, ARXIV181011914; Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713; Zhou DX, 2002, J COMPLEXITY, V18, P739, DOI 10.1006/jcom.2002.0635	53	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903085
C	Veeriah, V; Hessel, M; Xu, ZW; Lewis, R; Rajendran, J; Oh, J; van Hasselt, H; Silver, D; Singh, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Veeriah, Vivek; Hessel, Matteo; Xu, Zhongwen; Lewis, Richard; Rajendran, Janarthanan; Oh, Junhyuk; van Hasselt, Hado; Silver, David; Singh, Satinder			Discovery of Useful Questions as Auxiliary Tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Arguably, intelligent agents ought to be able to discover their own questions so that in learning answers for them they learn unanticipated useful knowledge and skills; this departs from the focus in much of machine learning on agents learning answers to externally defined questions. We present a novel method for a reinforcement learning (RL) agent to discover questions formulated as general value functions or GVFs, a fairly rich form of knowledge representation. Specifically, our method uses non-myopic meta-gradients to learn GVF-questions such that learning answers to them, as an auxiliary task, induces useful representations for the main task faced by the RL agent. We demonstrate that auxiliary tasks based on the discovered GVFs are sufficient, on their own, to build representations that support main task learning, and that they do so better than popular hand-designed auxiliary tasks from the literature. Furthermore, we show, in the context of Atari 2600 videogames, how such auxiliary tasks, meta-learned alongside the main task, can improve the data efficiency of an actor-critic agent.	[Veeriah, Vivek; Lewis, Richard; Rajendran, Janarthanan; Singh, Satinder] Univ Michigan, Ann Arbor, MI 48109 USA; [Hessel, Matteo; Xu, Zhongwen; Oh, Junhyuk; van Hasselt, Hado; Silver, David; Singh, Satinder] DeepMind, London, England	University of Michigan System; University of Michigan	Veeriah, V (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	vveeriah@umich.edu	Veeriah, Vimal/H-5772-2015		DARPAs L2M program; NSF [IIS-1526059]	DARPAs L2M program; NSF(National Science Foundation (NSF))	We thank John Holler and Zeyu Zheng for many useful comments and discussions. The work of the authors at the University of Michigan was supported by a grant from DARPAs L2M program and by NSF grant IIS-1526059. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the views of the sponsors.	Al-Shedivat M., 2018, 6 INT C LEARN REPR I; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2018, ARXIV180604640; [Anonymous], P INT C LEARN REPR V; [Anonymous], 2015, THESIS U ALBERTA EDM; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellemare Marc G., 2019, ABS190111530 CORR; BI JS, 2017, ADV NEURAL INFORM PR, P108; Chen Y., 2016, ARXIV161103824; Degris T., 2012, ICML12; Espeholt L., 2018, INT C MACH LEARN, P1406; EYSENBACH B, 2018, 6 INT C LEARN REPR I, P1; Finn C, 2017, PR MACH LEARN RES, V70; Florensa C, 2018, PR MACH LEARN RES, V80; Griewank A, 2008, OTHER TITL APPL MATH, V105, P1, DOI 10.1137/1.9780898717761; Hessel M, 2019, AAAI CONF ARTIF INTE, P3796; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Hessel Matteo, 2019, ARXIV190702908; Hubert Silver, 2017, ARXIV171201815; Jaderberg Max, 2017, 5 INT C LEARN REPR I; Makino T., 2008, P 25 INT C MACHINE L, P632; Malik J., 2017, 5 INT C LEARN REPR I; Mankowitz D.J., 2018, ARXIV180208294; Mirowski Piotr, 2017, 5 INT C LEARN REPR I; Mishra N., 2018, 6 INT C LEARN REPR I; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ravi S, 2017, 5 INT C LEARN REPR I; Riedmiller Martin A., 2018, INT C MACH LEARN, P4341; Schlegel Matthew, 2018, DISCOVERY PREDICTIVE; Schmidhuber J., 1996, SIMPLE PRINCIPLES ME; Schulman J., 2015, ARXIV150205477; Schulman J, 2017, ARXIV17070634; Shelhamer Evan, 2017, 5 INT C LEARN REPR I; Singh S., 2004, P 20 C UNCERTAINTY A, P512; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Stadie Bradly C, 2018, ARXIV180301118; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton R.S., 2005, ADV NEURAL INFORM PR, P1377; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Thrun S, 1998, LEARNING TO LEARN, P3; van Hasselt H, 2019, ARXIV190703687; Van Hasselt H., 2016, AAAI; Veeriah V., 2018, ARXIV180609605; Watkins CJCH., 1989, THESIS; Wichrowska O, 2017, PR MACH LEARN RES, V70; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu Z., 2018, 32 C NEUR INF PROC S, P2396; Zheng Z., 2018, ADV NEURAL INFORM PR	51	6	6	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900085
C	Wai, HT; Hong, MY; Yang, ZR; Wang, ZR; Tang, KX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wai, Hoi-To; Hong, Mingyi; Yang, Zhuoran; Wang, Zhaoran; Tang, Kexin			Variance Reduced Policy Evaluation with Smooth Function Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Policy evaluation with smooth and nonlinear function approximation has shown great potential for reinforcement learning. Compared to linear function approximation, it allows for using a richer class of approximation functions such as the neural networks. Traditional algorithms are based on two timescales stochastic approximation whose convergence rate is often slow. This paper focuses on an offline setting where a trajectory of m state-action pairs are observed. We formulate the policy evaluation problem as a non-convex primal-dual, finite-sum optimization problem, whose primal sub-problem is non-convex and dual sub-problem is strongly concave. We suggest a single-timescale primal-dual gradient algorithm with variance reduction, and show that it converges to an.-stationary point using O(m/epsilon) calls (in expectation) to a gradient oracle.	[Wai, Hoi-To] Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China; [Hong, Mingyi; Tang, Kexin] Univ Minnesota, Minneapolis, MN USA; [Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA; [Wang, Zhaoran] Northwestern Univ, Evanston, IL USA	Chinese University of Hong Kong; University of Minnesota System; University of Minnesota Twin Cities; Princeton University; Northwestern University	Wai, HT (corresponding author), Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China.	htwai@se.cuhk.edu.hk; mhong@umn.edu; zy6@princeton.edu; zhaoranwang@gmail.com; tangk@umn.edu	Wang, Zhaoran/P-7113-2018; Hong, Mingyi/H-6274-2013		CUHK Direct Grant [4055113]; NSF [CCF-1651825, CMMI-172775, CIF-1910385]; AFOSR [19RT0424]	CUHK Direct Grant(Chinese University of Hong Kong); NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	H.-T. Wai is supported by the CUHK Direct Grant #4055113. M. Hong is supported in part by NSF under Grant CCF-1651825, CMMI-172775, CIF-1910385 and by AFOSR under grant 19RT0424.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Bhandari Jalaj, 2018, C LEARN THEOR, P1691; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Borkar V. S, 2009, STOCHASTIC APPROXIMA, V48; Cassano L., 2018, ARXIV181007792; Chung W., 2019, ICLR; Dalal G., 2017, ARXIV170305376; Daskalakis C., 2018, P 32 INT C NEURAL IN, V31, P9256; Daskalakis Constantinos, 2017, ARXIV171100141; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Doan T. T., 2019, ARXIV190207393; Du SS, 2017, PR MACH LEARN RES, V70; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Konda VR, 2000, ADV NEUR IN, V12, P1008; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Lagoudakis M. G., 2003, J MACHINE LEARNING, P1107, DOI DOI 10.1162/JMLR.2003.4.6.1107; Lange S, 2012, ADAPT LEARN OPTIM, V12, P45; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Q., 2018, ARXIV PREPRINT ARXIV; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Lu S., 2019, ARXIV190208294; Mertikopoulos P., 2018, ABS180702629 CORR; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Nouiehed Maher, 2019, ARXIV190208297; Qian Qi, 2018, ABS180507588 CORR; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Sanjabi M., 2018, ARXIV181202878; Sanjabi M., 2018, P ADV NEUR INF PROC, P7088; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Srikant R., 2019, COLT; Stankovic MS, 2016, P AMER CONTR CONF, P167, DOI 10.1109/ACC.2016.7524910; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2009, ADV NEURAL INFORM PR, V21, P1609; Touati A., 2017, ARXIV170509322; Tsitsiklis JN, 1997, ADV NEUR IN, V9, P1075; Wai H.-T., 2018, NIPS 18, P9649; Wang Yue, 2017, ADV NEURAL INFORM PR, P5510	45	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305074
C	Wick, M; Panda, S; Tristan, JB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wick, Michael; Panda, Swetasudha; Tristan, Jean-Baptiste			Unlocking Fairness: a Trade-off Revisited	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The prevailing wisdom is that a model's fairness and its accuracy are in tension with one another. However, there is a pernicious modeling-evaluating dualism bedeviling fair machine learning in which phenomena such as label bias are appropriately acknowledged as a source of unfairness when designing fair models, only to be tacitly abandoned when evaluating them. We investigate fairness and accuracy, but this time under a variety of controlled conditions in which we vary the amount and type of bias. We find, under reasonable assumptions, that the tension between fairness and accuracy is illusive, and vanishes as soon as we account for these phenomena during evaluation. Moreover, our results are consistent with an opposing conclusion: fairness and accuracy are sometimes in accord. This raises the question, might there be a way to harness fairness to improve accuracy after all? Since many notions of fairness are with respect to the model's predictions and not the ground truth labels, this provides an opportunity to see if we can improve accuracy by harnessing appropriate notions of fairness over large quantities of unlabeled data with techniques like posterior regularization and generalized expectation. We find that semi-supervision improves both accuracy and fairness while imparting beneficial properties of the unlabeled data on the classifier.	[Wick, Michael; Panda, Swetasudha; Tristan, Jean-Baptiste] Oracle Labs, Burlington, MA 01803 USA	Oracle	Wick, M (corresponding author), Oracle Labs, Burlington, MA 01803 USA.	michael.wick@oracle.com; swetasudha.panda@oracle.com; jean.baptiste.tristan@oracle.com						[Anonymous], 2018, ADV NEURAL INFORM PR; BALCAN M, 2006, SEMISUPERVISED LEARN; Berk Richard, 2017, CORR; Chouldechova Alexandra, 2016, CORR; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Fish B., 2016, SDM; Friedler Sorelle A., 2016, CORR; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kleinberg Jon, 2018, ACM SIGMETRICS Performance Evaluation Review, V46, DOI 10.1145/3292040.3219634; Kleinberg Jon M., 2017, ITCS; Larson Jeff, 2016, PROPUBLICA COMPAS DA; Mann GS, 2010, J MACH LEARN RES, V11, P955; Menon Aditya Krishna, 2018, P C FAIRNESS ACCOUNT, P107; Rudin C., 2018, SCIENCE REVIEW, V2, DOI [10.1162/99608f92.6ed64b30, DOI 10.1162/99608F92.6ED64B30]; Wang X., 2006, P 12 ACM SIGKDD INT, P424; Zafar Muhammad Bilal, 2017, AISTATS	21	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900038
C	Worrall, DE; Welling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Worrall, Daniel E.; Welling, Max			Deep Scale-spaces: Equivariance Over Scale	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce deep scale-spaces (DSS), a generalization of convolutional neural networks, exploiting the scale symmetry structure of conventional image recognition tasks. Put plainly, the class of an image is invariant to the scale at which it is viewed. We construct scale equivariant cross-correlations based on a principled extension of convolutions, grounded in the theory of scale-spaces and semigroups. As a very basic operation, these cross-correlations can be used in almost any modern deep learning architecture in a plug-and-play manner. We demonstrate our networks on the Patch Camelyon and Cityscapes datasets, to prove their utility and perform introspective studies to further understand their properties.	[Worrall, Daniel E.; Welling, Max] Univ Amsterdam, Philips Lab, AMLAB, Amsterdam, Netherlands	Philips; University of Amsterdam	Worrall, DE (corresponding author), Univ Amsterdam, Philips Lab, AMLAB, Amsterdam, Netherlands.	d.e.worrall@uva.nl; m.welling@uva.nl						[Anonymous], 2018, ADV NEURAL INFORM PR; [Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; [Anonymous], 2013, COMPUT SCI; BARNARD E, 1991, IEEE T NEURAL NETWOR, V2, P498, DOI 10.1109/72.134287; Bekkers EJ, 2018, LECT NOTES COMPUT SC, V11070, P440, DOI 10.1007/978-3-030-00928-1_50; BURT PJ, 1981, COMPUT VISION GRAPH, V16, P20, DOI 10.1016/0146-664X(81)90092-7; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cohen Taco, 2018, ABS181102017 CORR; Cohen TS, 2016, PR MACH LEARN RES, V48; Cohen Taco S., 2018, ABS180110130 CORR, P8; Cohen Taco S., 2016, ABS161208498 CORR; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Esteves C, 2018, LECT NOTES COMPUT SC, V11217, P54, DOI 10.1007/978-3-030-01261-8_4; Esteves Carlos, 2017, ABS170901889 CORR; FLORACK LMJ, 1992, IMAGE VISION COMPUT, V10, P376, DOI 10.1016/0262-8856(92)90024-W; Henriques JF, 2017, PR MACH LEARN RES, V70; Hilbert Adam, 2018, INT C MED IM DEEP LE; Hoogeboom E., 2018, INT C LEARN REPR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Gao, 2018, 6 INT C LEARN REPR I; Iijima T., 1959, P TECH GROUP AUT AUT, P3; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kanazawa A., 2014, ABS14125104 CORR; Ke TW, 2017, PROC CVPR IEEE, P4067, DOI 10.1109/CVPR.2017.433; Kokkinos Iasonas, 2015, INT C LEARN REPR ICL; Kondor R., 2018, P 35 INT C MACH LEAR, P2752; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; LINDEBERG T, 1990, IEEE T PATTERN ANAL, V12, P234, DOI 10.1109/34.49051; Lindeberg T, 1997, COMP IMAG VIS, V8, P75; Liu Yun, 2017, ABS170302442 CORR; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Lunter Gerton, 2018, EQUIVARIANT BAYESIAN, DOI [10.1101/351254, DOI 10.1101/351254]; Mallat S, 2009, WAVELET TOUR OF SIGNAL PROCESSING: THE SPARSE WAY, P1; Marcos D., 2018, ABS180711783 CORR; PAUWELS EJ, 1995, IEEE T PATTERN ANAL, V17, P691, DOI 10.1109/34.391411; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Saxena S., 2016, ABS160602492 CORR; Shelhamer Evan, 2019, ABS190411487 CORR; Sokolic J, 2017, PR MACH LEARN RES, V54, P1094; Thomas N., 2018, ABS180208219 CORR; Veeling BS, 2018, LECT NOTES COMPUT SC, V11071, P210, DOI 10.1007/978-3-030-00934-2_24; Weiler M, 2018, ADV NEUR IN, V31; Weiler M, 2018, PROC CVPR IEEE, P849, DOI 10.1109/CVPR.2018.00095; Witkin A.P., 1983, P 8 INT JOINT C ART, P1019, DOI DOI 10.1007/978-3-8348-9190-729; Worrall DE, 2017, PROC CVPR IEEE, P7168, DOI 10.1109/CVPR.2017.758; Yu F., 2016, ABS151107122 CORR; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75	49	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307039
C	Yadav, C; Bottou, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yadav, Chhavi; Bottou, Leon			Cold Case: the Lost MNIST Digits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Although the popular MNIST dataset [LeCun et al., 1994] is derived from the NIST database [Grother and Hanaoka, 1995], the precise processing steps for this derivation have been lost to time. We propose a reconstruction that is accurate enough to serve as a replacement for the MNIST dataset, with insignificant changes in accuracy. We trace each MNIST digit to its NIST source and its rich metadata such as writer identifier, partition identifier, etc. We also reconstruct the complete MNIST test set with 60,000 samples instead of the usual 10,000. Since the balance 50,000 were never distributed, they can be used to investigate the impact of twenty-five years of MNIST experiments on the reported testing performances. Our limited results unambiguously confirm the trends observed by Recht et al. [2018, 2019]: although the misclassification rates are slightly off, classifier ordering and model selection remain broadly reliable. We attribute this phenomenon to the pairing benefits of comparing classifiers on the same digits.	[Yadav, Chhavi; Bottou, Leon] NYU, New York, NY 10003 USA; [Bottou, Leon] Facebook AI Res, New York, NY USA	New York University; Facebook Inc	Yadav, C (corresponding author), NYU, New York, NY 10003 USA.	chhavi@nyu.edu; leon@bottou.org						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bonferroni C.E., 1936, PUBBLICAZIONI R I SU; BOTTOU L, 1994, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1994.576879; Bottou Leon, 2001, LUSH REFERENCE MANUA; Bottou Leon, 1988, P NEURONIMES 88 NIM, V88, P371; Grother PJ, 1995, HANDPRINTED FORMS CH, P10; Hardt M, 2019, P INT C MACH LEARN, P1892; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1994, MNIST DATABASE HANDW; Recht B, 2018, ARXIV180600451; Recht Benjamin, 2019, P 36 INT C MACH LEAR; Vapnik V., 1982, ESTIMATION DEPENDENC	13	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905015
C	Yu, Y; Wu, JX; Huang, LB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Yue; Wu, Jiaxiang; Huang, Longbo			Double Quantization for Communication-Efficient Distributed Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern distributed training of machine learning models often suffers from high communication overhead for synchronizing stochastic gradients and model parameters. In this paper, to reduce the communication complexity, we propose double quantization,a general scheme for quantizing both model parameters and gradients. Three communication-efficient algorithms are proposed based on this general scheme. Specifically, (i) we propose a low-precision algorithm AsyLPG with asynchronous parallelism, (ii) we explore integrating gradient sparsification with double quantization and develop Sparse-AsyLPG, (iii) we show that double quantization can be accelerated by the momentum technique and design accelerated AsyLPG. We establish rigorous performance guarantees for the algorithms, and conduct experiments on a multi-server test-bed with real-world datasets to demonstrate that our algorithms can effectively save transmitted bits without performance degradation, and significantly outperform existing methods with either model parameter or gradient quantization.	[Yu, Yue; Huang, Longbo] Tsinghua Univ, IIIS, Beijing, Peoples R China; [Wu, Jiaxiang] Tencent AI Lab, Bellevue, WA USA	Tsinghua University	Huang, LB (corresponding author), Tsinghua Univ, IIIS, Beijing, Peoples R China.	yu-y14@mails.tsinghua.edu.cn; jonathanwu@tencent.com; longbohuang@tsinghua.edu.cn			National Natural Science Foundation of China [61672316]; Zhongguancun Haihua Institute for Frontier Information Technology; Turing AI Institute of Nanjing	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Zhongguancun Haihua Institute for Frontier Information Technology; Turing AI Institute of Nanjing	The work of Yue Yu and Longbo Huang was supported in part by the National Natural Science Foundation of China Grant 61672316, the Zhongguancun Haihua Institute for Frontier Information Technology and the Turing AI Institute of Nanjing. We would like to thank anonymous reviewers for their insightful suggestions.	Aji A.F., 2017, P 2017 C EMP METH NA; Alistarh D, 2018, ADV NEUR IN, V31; Alistarh D, 2017, ADV NEUR IN, V30; Bernstein J., 2019, INT C LEARN REPR ICL; Bernstein J, 2018, PR MACH LEARN RES, V80; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; De Sa C., 2018, 180303383 ARXIV; De Sa Christopher, 2017, Proc Int Symp Comput Archit, V2017, P561, DOI 10.1145/3140659.3080248; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hogan E, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON INTELLIGENCE AND SECURITY INFORMATICS: BIG DATA, EMERGENT THREATS, AND DECISION-MAKING IN SECURITY INFORMATICS, P315, DOI 10.1109/ISI.2013.6578850; Huo Z, 2016, ARXIV160403584; Jiang P, 2018, ADV NEUR IN, V31; Karimireddy SP, 2019, PR MACH LEARN RES, V97; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li JL, 2017, INT CONF MACH LEARN, P35; Nesterov Y., 2018, APPL OPTIMIZATION; Recht B, 2011, ADV NEURAL INFORM PR, V2011, P693; Reddi S., 2016, ADV NEURAL INFORM PR; Seide F, 2014, INTERSPEECH, P1058; Shang F., 2017, FAST STOCHASTIC VARI; Smola, 2015, ADV NEURAL INFORM PR, P2629; Stich SU, 2018, ADV NEUR IN, V31; Tang HL, 2019, PR MACH LEARN RES, V97; Tang HL, 2018, ADV NEUR IN, V31; Wang HY, 2018, ADV NEUR IN, V31; Wang J., 2017, C LEARN THEOR COLT; Wang JL, 2017, PR MACH LEARN RES, V70; Wangni J., 2018, ADV NEURAL INFORM PR, P1299; Wen W., 2017, ADV NEURAL INFORM PR, P1, DOI DOI 10.1109/ICC.2017.7997306; Wu JR, 2018, PR MACH LEARN RES, V80; Yin PH, 2018, SIAM J IMAGING SCI, V11, P2205, DOI 10.1137/18M1166134; Yu Y., 2019, P 22 INT C ART INT S; Zhang X, 2019, IEEE INFOCOM SER, P2431, DOI 10.1109/INFOCOM.2019.8737489	38	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304044
C	Zhang, D; Khoreva, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Dan; Khoreva, Anna			Progressive Augmentation of GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Training of Generative Adversarial Networks (GANs) is notoriously fragile, requiring to maintain a careful balance between the generator and the discriminator in order to perform well. To mitigate this issue we introduce a new regularization technique - progressive augmentation of GANs (PA-GAN). The key idea is to gradually increase the task difficulty of the discriminator by progressively augmenting its input or feature space, thus enabling continuous learning of the generator. We show that the proposed progressive augmentation preserves the original GAN objective, does not compromise the discriminator's optimality and encourages a healthy competition between the generator and discriminator, leading to the better-performing generator. We experimentally demonstrate the effectiveness of PA-GAN across different architectures and on multiple benchmarks for the image synthesis task, on average achieving similar to 3 point improvement of the FID score.	[Zhang, Dan; Khoreva, Anna] Bosch Ctr Artificial Intelligence, Renningen, Germany		Zhang, D (corresponding author), Bosch Ctr Artificial Intelligence, Renningen, Germany.	dan.zhang2@bosch.com; anna.khoreva@bosch.com						Arjovsky M, 2017, PR MACH LEARN RES, V70; Arjovsky Martin, 2017, ADV NEURAL INFORM PR; Brock A., 2019, INT C LEARNING REPRE; Chen T., 2019, INT C LEARN REPR ICL; Chen TS, 2019, PROC CVPR IEEE, P6156, DOI 10.1109/CVPR.2019.00632; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Durugkar Ishan, 2017, ICLR; Gidaris Spyros, 2018, ARXIV180307728; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Huszar Ferenc, 2015, ABS151105101 CORR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurach K., 2018, ARXIV180704720; Lin C, 2019, IEEE I CONF COMP VIS, P6578, DOI 10.1109/ICCV.2019.00668; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucic Mario, 2018, ADV NEURAL INFORM PR; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mescheder L, 2018, PR MACH LEARN RES, V80; Nowozin S, 2016, ADV NEUR IN, V29; Odena Augustus, 2017, INT C LEARN REPR ICL; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Sajjadi MSM, 2018, PR MACH LEARN RES, V80; Salimans T, 2016, ADV NEUR IN, V29; Sonderby C.K., 2017, ICLR, P1; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutherland Danica J, 2018, ICLR; Theis Lucas, 2016, ICLR; Xiao H., 2017, FASHION MNIST NOVEL; Zhang Han, 2018, ARXIV180508318	35	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306027
C	Zhang, JY; Xiao, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Junyu; Xiao, Lin			A Stochastic Composite Gradient Method with Incremental Variance Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the problem of minimizing the composition of a smooth (nonconvex) function and a smooth vector mapping, where the inner mapping is in the form of an expectation over some random variable or a finite sum. We propose a stochastic composite gradient method that employs an incremental variance-reduced estimator for both the inner vector mapping and its Jacobian. We show that this method achieves the same orders of complexity as the best known first-order methods for minimizing expected-value and finite-sum nonconvex functions, despite the additional outer composition which renders the composite gradient estimator biased. This finding enables a much broader range of applications in machine learning to benefit from the low complexity of incremental variance-reduction methods.	[Zhang, Junyu] Univ Minnesota, Minneapolis, MN 55455 USA; [Xiao, Lin] Microsoft Res, Redmond, WA 98052 USA	University of Minnesota System; University of Minnesota Twin Cities; Microsoft	Zhang, JY (corresponding author), Univ Minnesota, Minneapolis, MN 55455 USA.	zhan4393@umn.edu; lin.xiao@microsoft.com						Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Beck A, 2017, MOS-SIAM SER OPTIMIZ, P1, DOI 10.1137/1.9781611974997; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dentcheva D, 2017, ANN I STAT MATH, V69, P737, DOI 10.1007/s10463-016-0559-8; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Ghadimi Saeed, 2018, ARXIV181201094; Huo ZY, 2018, AAAI CONF ARTIF INTE, P3287; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Koshal J, 2013, IEEE T AUTOMAT CONTR, V58, P594, DOI 10.1109/TAC.2012.2215413; Kovalev Dmitry, 2019, ARXIV190108689; Lei LH, 2017, ADV NEUR IN, V30; Lian XR, 2017, PR MACH LEARN RES, V54, P1159; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nguyen LM, 2017, PR MACH LEARN RES, V70; Nguyen Lam M., 2019, ARXIV190107648; Pham N. H., 2019, ARXIV190205679; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi SJ, 2016, IEEE DECIS CONTR P, P1971, DOI 10.1109/CDC.2016.7798553; Rockafellar R. T., 1970, CONVEX ANAL; Rockafellar R. Tyrrell, 2007, INFORMS TUTORIALS OP; Ruszczynski A., 2013, INFORMS TUTORIALS OP; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Wang MD, 2017, J MACH LEARN RES, V18; Wang MD, 2017, MATH PROGRAM, V161, P419, DOI 10.1007/s10107-016-1017-3; Wang Zhe, 2018, ARXIV181010690; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang Junyu, 2019, P MACHINE LEARNING R	36	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900064
C	Zhang, ST; Boehmer, W; Whiteson, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Shangtong; Boehmer, Wendelin; Whiteson, Shimon			Generalized Off-Policy Actor-Critic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new objective, the counterfactual objective, unifying existing objectives for off-policy policy gradient algorithms in the continuing reinforcement learning (RL) setting. Compared to the commonly used excursion objective, which can be misleading about the performance of the target policy when deployed, our new objective better predicts such performance. We prove the Generalized Off-Policy Policy Gradient Theorem to compute the policy gradient of the counterfactual objective and use an emphatic approach to get an unbiased sample from this policy gradient, yielding the Generalized Off-Policy Actor-Critic (Geoff-PAC) algorithm. We demonstrate the merits of Geoff-PAC over existing algorithms in Mujoco robot simulation tasks, the first empirical success of emphatic algorithms in prevailing deep RL benchmarks.	[Zhang, Shangtong; Boehmer, Wendelin; Whiteson, Shimon] Univ Oxford, Dept Comp Sci, Oxford, England	University of Oxford	Zhang, ST (corresponding author), Univ Oxford, Dept Comp Sci, Oxford, England.	shangtong.zhang@cs.ox.ac.uk; wendelin.boehmer@cs.ox.ac.uk; shimon.whiteson@cs.ox.ac.uk			Engineering and Physical Sciences Research Council (EPSRC); European Research Council under the European Union [637713]; NVIDIA	Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union(European Research Council (ERC)); NVIDIA	SZ is generously funded by the Engineering and Physical Sciences Research Council (EPSRC). This project has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713). The experiments were made possible by a generous equipment grant from NVIDIA. The authors thank Richard S. Sutton, Matthew Fellows, Huizhen Yu for the valuable discussion.	[Anonymous], 2018, ARXIV180201561; [Anonymous], 2016, ARXIV161101224; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; [Anonymous], ADV NEURAL INFORM PR; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Asadi Kavosh, 2016, ARXIV161206000; Bellemare M. G., 2013, J ARTIFICIAL INTELLI; Borkar V. S., 2009, STOCHASTIC APPROXIMA, V48; Brockman G., 2016, OPENAI GYM; Ciosek K., 2017, ARXIV170605374; Clemente A. V., 2017, MULTIDISCIPLINARY C; Dasgupta S., 2001, P 18 INT C MACH LEAR; Degris T., 2012, P 29 INT C MACH LEAR, P457; Fujimoto S., 2018, ARXIV180209477; Gelada C., 2019, P 33 AAAI C ART INT; Ghiassian S., 2018, ONLINE OFF POLICY PR; Gu S. S., 2017, ADV NEURAL INFORM PR; Haarnoja T., 2018, P 35 INT C MACH LEAR; Haarnoja T, 2017, PR MACH LEARN RES, V70; Hallak A., 2016, P 30 AAAI C ART INT; Hallak A., 2017, P 34 INT C MACH LEAR; Imani E., 2018, ADV NEURAL INFORM PR; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lin Long-Ji, 1992, MACHINE LEARNING; Liu Q., 2018, ADV NEURAL INFORM PR; Liu Y., 2019, ARXIV190408473; Maei H. R., 2018, ARXIV180207842; Marbach P., 2001, IEEE T AUTOMATIC CON; Mnih V., 2016, P 33 INT C MACH LEAR; Mnih V, 2015, NATURE, V518, P529; Nachum O., 2017, ARXIV170701891; Puterman M.L., 2014, MARKOV DECISION PROC; Schulman John, 2017, EQUIVALENCE POLICY; Silver D., 2014, P 31 INT C MACH LEAR; Silver D., 2015, POLICY GRADIENT METH; Sutton R. S., 1988, MACHINE LEARNING; Sutton R. S., 2016, J MACHINE LEARNING R; Sutton R. S., 2000, ADV NEURAL INFORM PR; Sutton R. S., 2009, ADV NEURAL INFORM PR; Tsitsiklis John N, 1997, ADV NEURAL INFORM PR; Watkins C. J., 1992, MACHINE LEARNING; White M., 2017, P 34 INT C MACH LEAR; Yu H., 2015, C LEARN THEOR; Yu H., 2005, 21 C UNC ART INT	45	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302004
C	Zhang, Z; Xiang, YJ; Wu, LF; Xue, B; Nehorai, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Zhen; Xiang, Yijian; Wu, Lingfei; Xue, Bing; Nehorai, Arye			KerGM: Kernelized Graph Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALIGNMENT; ALGORITHM	Graph matching plays a central role in such fields as computer vision, pattern recognition, and bioinformatics. Graph matching problems can be cast as two types of quadratic assignment problems (QAPs): Koopmans-Beckmann's QAP or Lawler's QAP. In our paper, we provide a unifying view for these two problems by introducing new rules for array operations in Hilbert spaces. Consequently, Lawler's QAP can be considered as the Koopmans-Beckmann's alignment between two arrays in reproducing kernel Hilbert spaces (RKHS), making it possible to efficiently solve the problem without computing a huge affinity matrix. Furthermore, we develop the entropy-regularized Frank-Wolfe (EnFW) algorithm for optimizing QAPs, which has the same convergence rate as the original FW algorithm while dramatically reducing the computational burden for each outer iteration. We conduct extensive experiments to evaluate our approach, and show that our algorithm significantly outperforms the state-of-the-art in both matching accuracy and scalability.	[Zhang, Zhen; Xiang, Yijian; Xue, Bing; Nehorai, Arye] Washington Univ, St Louis, MO 63110 USA; [Wu, Lingfei] IBM Res, Yorktown Hts, NY USA	Washington University (WUSTL); International Business Machines (IBM)	Zhang, Z (corresponding author), Washington Univ, St Louis, MO 63110 USA.	zhen.zhang@wustl.edu; yijian.xiang@wustl.edu; lwu@email.wm.edu; xuebing@wustl.edu; nehorai@wustl.edu	Wu, Lingfei/ABC-1000-2020; Xiang, Yijian/AAQ-3641-2020	Wu, Lingfei/0000-0002-3660-651X; 	AFOSR [FA9550-16-1-0386]	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported in part by the AFOSR grant FA9550-16-1-0386.	Aflalo Y, 2015, P NATL ACAD SCI USA, V112, P2942, DOI 10.1073/pnas.1401651112; ALMOHAMAD HA, 1993, IEEE T PATTERN ANAL, V15, P522, DOI 10.1109/34.211474; Brendel W, 2011, IEEE I CONF COMP VIS, P778, DOI 10.1109/ICCV.2011.6126316; Chinchuluun A, 2005, APPL OPTIMIZAT, V99, P251; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Collins SR, 2007, MOL CELL PROTEOMICS, V6, P439, DOI 10.1074/mcp.M600381-MCP200; Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228; Cour T., 2007, P ADV NEURAL INFORM, P313; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Elmsallati A, 2016, IEEE ACM T COMPUT BI, V13, P689, DOI 10.1109/TCBB.2015.2474391; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Gaur U, 2011, IEEE I CONF COMP VIS, P2595, DOI 10.1109/ICCV.2011.6126548; Gold S, 1996, IEEE T PATTERN ANAL, V18, P377, DOI 10.1109/34.491619; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Hu N, 2014, PROC CVPR IEEE, P2313, DOI 10.1109/CVPR.2014.296; Jaggi M., 2013, P 30 INT C MACHINE L, P427; KOOPMANS TC, 1957, ECONOMETRICA, V25, P53, DOI 10.2307/1907742; Kriege N.M., 2012, INT C MACHINE LEARNI, P291; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Kushinsky Y, 2019, SIAM J IMAGING SCI, V12, P716, DOI 10.1137/18M1196480; LAWLER EL, 1963, MANAGE SCI, V9, P586, DOI 10.1287/mnsc.9.4.586; LEE DT, 1980, INT J COMPUT INF SCI, V9, P219, DOI 10.1007/BF00977785; Lee J, 2011, PROC CVPR IEEE, P1633, DOI 10.1109/CVPR.2011.5995387; Leordeanu M, 2012, INT J COMPUT VISION, V96, P28, DOI 10.1007/s11263-011-0442-2; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Liu ZY, 2014, IEEE T PATTERN ANAL, V36, P1258, DOI 10.1109/TPAMI.2013.223; Maciel J, 2003, IEEE T PATTERN ANAL, V25, P187, DOI 10.1109/TPAMI.2003.1177151; Maron H., 2018, ADV NEURAL INFORM PR, P408; Mohammadi S, 2017, IEEE ACM T COMPUT BI, V14, P1446, DOI 10.1109/TCBB.2016.2595583; Patro R, 2012, BIOINFORMATICS, V28, P3105, DOI 10.1093/bioinformatics/bts592; Pedregosa F., 2018, ARXIV180605123; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Saraph V, 2014, BIOINFORMATICS, V30, P2931, DOI 10.1093/bioinformatics/btu409; Schellewald C., 2001, Pattern Recognition. 23rd DAGM Symposium. Proceedings (Lecture Notes in Computer Science Vol.2191), P361; Vogelstein JT, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121002; Wang T, 2018, IEEE T PATTERN ANAL, V40, P2853, DOI 10.1109/TPAMI.2017.2767591; Wang T, 2016, LECT NOTES COMPUT SC, V9906, P508, DOI 10.1007/978-3-319-46475-6_32; Wu LF, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1418, DOI 10.1145/3292500.3330918; Wu LF, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1265, DOI 10.1145/2939672.2939794; Yan JC, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P167, DOI 10.1145/2911996.2912035; Yan JC, 2015, IEEE T IMAGE PROCESS, V24, P994, DOI 10.1109/TIP.2014.2387386; Yu TS, 2018, ADV NEUR IN, V31; Zaslavskiy M, 2009, IEEE T PATTERN ANAL, V31, P2227, DOI 10.1109/TPAMI.2008.245; Zass R, 2008, PROC CVPR IEEE, P1221; Zhang Z, 2018, ADV NEUR IN, V31; Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802; Zhou F, 2012, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2012.6247667	49	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303034
C	Zhou, ZY; Xu, RY; Blanchet, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Zhengyuan; Xu, Renyuan; Blanchet, Jose			Learning in Generalized Linear Contextual Bandits with Stochastic Delays	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we consider online learning in generalized linear contextual bandits where rewards are not immediately observed. Instead, rewards are available to the decision maker only after some delay, which is unknown and stochastic, even though a decision must be made at each time step for an incoming set of contexts. We study the performance of upper confidence bound (UCB) based algorithms adapted to this delayed setting. In particular, we design a delay-adaptive algorithm, which we call Delayed UCB, for generalized linear contextual bandits using UCB-style exploration and establish regret bounds under various delay assumptions. In the important special case of linear contextual bandits, we further modify this algorithm and establish a tighter regret bound under the same delay assumptions. Our results contribute to the broad landscape of contextual bandits literature by establishing that UCB algorithms, which are widely deployed in modern recommendation engines, can be made robust to delays.	[Zhou, Zhengyuan] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Zhou, Zhengyuan] Bytedance Inc, Beijing, Peoples R China; [Xu, Renyuan] Univ Calif Berkeley, Dept Ind Engn & Operat Res, Berkeley, CA USA; [Blanchet, Jose] Stanford Univ, Dept Management Sci & Engn, Stanford, CA 94305 USA	Stanford University; University of California System; University of California Berkeley; Stanford University	Zhou, ZY (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.; Zhou, ZY (corresponding author), Bytedance Inc, Beijing, Peoples R China.							Abeille M, 2017, ELECTRON J STAT, V11, P5165, DOI 10.1214/17-EJS1341SI; Agrawal S., 2013, ARTIF INTELL, P99; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Bastani H, 2015, ONLINE DECISION MAKI; Bertsekas DP., 2002, INTRO PROBABILITY; Bistritz I, 2019, ADV NEUR IN, V32; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2019, J MACH LEARN RES, V20; Chapelle O, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1097, DOI 10.1145/2623330.2623634; Chen KN, 1999, ANN STAT, V27, P1155; Chow SC, 2012, CH CRC BIOSTAT SER, P1; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Dudik M., 2011, UNCERTAINTY ARTIFICI; Filippi S., 2010, NIPS, P586; Garrabrant S., 2016, ARXIV160405280; Grover A., 2018, ARXIV180310937, V84, P833; Grover A., 2018, ARXIV PREPRINT ARXIV, P1802; Guo X, 2019, ADV NEUR IN, V32; He J, 2010, NINTH WUHAN INTERNATIONAL CONFERENCE ON E-BUSINESS, VOLS I-III, P1061; Joulani, 2013, INT C MACH LEARN, V28, P1453, DOI DOI 10.14288/1.0044651; Jun KS, 2017, ADV NEUR IN, V30; Kannan P. K., 2001, Proceedings of the 34th Annual Hawaii International Conference on System Sciences, DOI 10.1109/HICSS.2001.927209; Kitagawa T, 2018, ECONOMETRICA, V86, P591, DOI 10.3982/ECTA13288; Li LH, 2017, PR MACH LEARN RES, V70; Mahdian S., P3708; Mandel T, 2015, AAAI CONF ARTIF INTE, P2849; McCullagh P, 2018, GEN LINEAR MODELS; Mehdian S, 2017, P AMER CONTR CONF, P1747, DOI 10.23919/ACC.2017.7963205; Mertikopoulos P, 2019, MATH PROGRAM, V173, P465, DOI 10.1007/s10107-018-1254-8; Mesterharm C, 2005, LECT NOTES ARTIF INT, V3734, P399; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; NEU G, 2010, ADV NEURAL INFORM PR, P1804; Ostrovsky E., 2014, ARXIV14056749; Pike-Burke C., 2017, ARXIV170906853; Pinedo M., SCHEDULING, V29; Quanrud K., 2015, P ADV NEUR INF PROC, P1270; Russo D, 2016, J MACH LEARN RES, V17; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Schwartz EM, 2017, MARKET SCI, V36, P500, DOI 10.1287/mksc.2016.1023; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; Vernade C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Vershynin R., 2010, ARXIV10113027; Wainwright M.J., 2019, HIGH DIMENSIONAL STA, V48; Weinberger MJ, 2002, IEEE T INFORM THEORY, V48, P1959, DOI 10.1109/TIT.2002.1013136; Zhou Z., 2018, ARXIV181004778; Zhou ZY, 2018, PR MACH LEARN RES, V80; Zhou ZY, 2017, ADV NEUR IN, V30	49	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305022
C	Ashtiani, H; Ben-David, S; Harvey, NJA; Liaw, C; Mehrabian, A; Plan, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ashtiani, Hassan; Ben-David, Shai; Harvey, Nicholas J. A.; Liaw, Christopher; Mehrabian, Abbas; Plan, Yaniv			Nearly tight sample complexity bounds for learning mixtures of Gaussians via sample compression schemes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We prove that (Theta) over tilde (kd(2)/epsilon(2) ) samples are necessary and sufficient for learning a mixture of k Gaussians in R-d , up to error E in total variation distance. This improves both the known upper bounds and lower bounds for this problem. For mixtures of axis-aligned Gaussians, we show that (O) over tilde (kd/epsilon(2) ) samples suffice, matching a known lower bound. The upper bound is based on a novel technique for distribution learning based on a notion of sample compression. Any class of distributions that allows such a sample compression scheme can also be learned with few samples. Moreover, if a class of distributions has such a compression scheme, then so do the classes of products and mixtures of those distributions. The core of our main result is showing that the class of Gaussians in R-d has a small-sized sample compression.	[Ashtiani, Hassan] McMaster Univ, Dept Comp & Software, Hamilton, ON, Canada; [Ashtiani, Hassan] Vector Inst, Toronto, ON, Canada; [Ben-David, Shai] Univ Waterloo, Sch Comp Sci, Waterloo, ON, Canada; [Harvey, Nicholas J. A.; Liaw, Christopher; Plan, Yaniv] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada; [Mehrabian, Abbas] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada	McMaster University; University of Waterloo; University of British Columbia; McGill University	Ashtiani, H (corresponding author), McMaster Univ, Dept Comp & Software, Hamilton, ON, Canada.; Ashtiani, H (corresponding author), Vector Inst, Toronto, ON, Canada.	zokaeiam@mcmaster.ca; shai@uwaterloo.ca; nickhar@cs.ubc.ca; cvliaw@cs.ubc.ca; abbasmehrabian@gmail.com; yaniv@math.ubc.ca			CRM-ISM postdoctoral fellowship; IVADO-Apogee-CFREF postdoctoral fellowship; NSERC Discovery Grant; NSERC graduate award; NSERC [22R23068]	CRM-ISM postdoctoral fellowship; IVADO-Apogee-CFREF postdoctoral fellowship; NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); NSERC graduate award; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We thank Yaoliang Yu for pointing out a mistake in an earlier version of this paper, and Luc Devroye for fruitful discussions. Abbas Mehrabian was supported by a CRM-ISM postdoctoral fellowship and an IVADO-Apogee-CFREF postdoctoral fellowship. Nicholas Harvey was supported by an NSERC Discovery Grant. Christopher Liaw was supported by an NSERC graduate award. Yaniv Plan was supported by NSERC grant 22R23068.	Anthony M., 1999, NEURAL NETWORK LEARN, V9; Ashtiani H, 2018, AAAI CONF ARTIF INTE, P2679; Ashtiani Hassan, NEAR OPTIMAL SAMPLE; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Chan SO, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P604, DOI 10.1145/2591796.2591848; Devroye L, 1987, COURSE DENSITY ESTIM; Devroye L., 2018, ARXIV180606887; Diakonikolas I, 2016, HDB OF BIG DATA, P267; Diakonikolas I, 2017, ANN IEEE SYMP FOUND, P73, DOI 10.1109/FOCS.2017.16; Diakonikolas Ilias, 2017, COLT 17, V65, P1; Ibragimov I, 2001, INST MATH S, V36, P359, DOI 10.1214/lnms/1215090078; Kalai Adam, 2012, COMMUNICATIONS ACM, V55; Kearns M., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P273, DOI 10.1145/195058.195155; Littlestone N., 1986, RELATING DATA COMPRE; Litvak AE, 2005, ADV MATH, V195, P491, DOI 10.1016/j.aim.2004.08.004; Lucic M, 2018, J MACH LEARN RES, V18; Moran S, 2016, J ACM, V63, DOI 10.1145/2890490; Silverman BW., 1986, DENSITY ESTIMATION S, V26, DOI 10.1201/9781315140919; Suresh A. T., 2014, ADV NEURAL INFORM PR, V27, P1395; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	21	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303041
C	Brown, N; Sandholm, T; Amos, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Brown, Noam; Sandholm, Tuomas; Amos, Brandon			Depth-Limited Solving for Imperfect-Information Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GO	A fundamental challenge in imperfect-information games is that states do not have well-defined values. As a result, depth-limited search algorithms used in single-agent settings and perfect-information games do not apply. This paper introduces a principled way to conduct depth-limited solving in imperfect-information games by allowing the opponent to choose among a number of strategies for the remainder of the game at the depth limit. Each one of these strategies results in a different set of values for leaf nodes. This forces an agent to be robust to the different strategies an opponent may employ. We demonstrate the effectiveness of this approach by building a master-level heads-up no-limit Texas hold' em poker AI that defeats two prior top agents using only a 4-core CPU and 16 GB of memory. Developing such a powerful agent would have previously required a supercomputer.	[Brown, Noam; Sandholm, Tuomas; Amos, Brandon] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Brown, N (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	noamb@cs.cmu.edu; sandholm@cs.cmu.edu; bamos@cs.cmu.edu			National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]	National Science Foundation(National Science Foundation (NSF)); ARO	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082, as well as XSEDE computing resources provided by the Pittsburgh Supercomputing Center. We thank Thore Graepel, Marc Lanctot, David Silver, Ariel Procaccia, Fei Fang, and our anonymous reviewers for helpful inspiration, feedback, suggestions, and support.	[Anonymous], 2017, P ADV NEUR INF PROC; [Anonymous], 2012, P 26 AAAI C ART INT; [Anonymous], 2008, P 7 AAMAS; [Anonymous], 1950, PHILOMAG, DOI [DOI 10.1007/978-1-4757-1968-0_1, DOI 10.1080/14786445008521796]; Ba J., 2017, P 3 INT C LEARN REPR; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N., 2016, P 25 INT JOINT C ART, P4238; Brown N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P7; Brown Noam, 2015, P INT JOINT C ARTIFI; Burch N, 2014, AAAI CONF ARTIF INTE, P602; Burch Neil, 2016, AIVAT NEW VARIANCE R; Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1; Cermak Jiri, 2018, ARXIV180305392; Ganzfried S, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P37; Ganzfried Sam, 2013, P 23 INT JOINT C ART, P120; Ganzfried Sam, 2014, AAAI C ART INT AAAI; Hart P.E., 1972, ACM SIGART B, V37, P28, DOI DOI 10.1145/1056777.1056779; Jackson E. G., 2017, AAAI WORKSH COMP POK; Jackson Eric Griffin, 2014, AAAI WORKSH COMP POK; Johanson M, 2013, P 2013 INT C AUTONOM, P271; Lanctot M., 2009, ADV NEURAL INFORM PR, P1078; Lanctot M, 2017, ADV NEUR IN, V30; LIN S, 1965, AT&T TECH J, V44, P2245, DOI 10.1002/j.1538-7305.1965.tb04146.x; Lisy V., 2016, ARXIV161207547; McMahan H Brendan, 2003, P 20 INT C MACH LEAR, P536; Moravcik M., 2017, ARXIV170101724V3; Moravcik Matej, 2016, AAAI C ART INT AAAI; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Newell Allen, 1965, P IFIP C, V65, P17; Nilsson N.J., 1971, PROBLEM SOLVING METH; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Schnizlein D, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P278; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; Tesauro G, 2002, ARTIF INTELL, V134, P181, DOI 10.1016/S0004-3702(01)00110-2	39	6	6	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002023
C	Chen, T; Murali, A; Gupta, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Tao; Murali, Adithyavairavan; Gupta, Abhinav			Hardware Conditioned Policies for Multi-Robot Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep reinforcement learning could be used to learn dexterous robotic policies but it is challenging to transfer them to new robots with vastly different hardware properties. It is also prohibitively expensive to learn a new policy from scratch for each robot hardware due to the high sample complexity of modern state-of-the-art algorithms. We propose a novel approach called Hardware Conditioned Policies where we train a universal policy conditioned on a vector representation of robot hardware. We considered robots in simulation with varied dynamics, kinematic structure, kinematic lengths and degrees-of-freedom. First, we use the kinematic structure directly as the hardware encoding and show great zero-shot transfer to completely novel robots not seen during training. For robots with lower zero-shot success rate, we also demonstrate that fine-tuning the policy network is significantly more sample-efficient than training a model from scratch. In tasks where knowing the agent dynamics is important for success, we learn an embedding for robot hardware and show that policies conditioned on the encoding of hardware tend to generalize and transfer well. Videos of experiments are available at: https://sites.google.com/view/robot-transfer-hcp.	[Chen, Tao; Murali, Adithyavairavan; Gupta, Abhinav] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Chen, T (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	taoc1@cs.cmu.edu; amurali@cs.cmu.edu; abhinavg@cs.cmu.edu			ONR MURI [N000141612007]; Army Research Office [W911NF-18-1-0019]; Sloan Research Fellowship; Uber Fellowship	ONR MURI(MURIOffice of Naval Research); Army Research Office; Sloan Research Fellowship(Alfred P. Sloan Foundation); Uber Fellowship	This research is partly sponsored by ONR MURI N000141612007 and the Army Research Office and was accomplished under Grant Number W911NF-18-1-0019. Abhinav was supported in part by Sloan Research Fellowship and Adithya was partly supported by a Uber Fellowship. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein. The authors would also like to thank Senthil Purushwalkam and Deepak Pathak for feedback on the early draft and Lerrel Pinto and Roberto Shu for insightful discussions.	Abbeel P., 2006, MACHINE LEARNING P 2, P1, DOI DOI 10.1145/1143844.1143845; Al-Shedivat M., 2017, ARXIV171003641; Andrychowicz M., 2017, CORR; [Anonymous], 2017, DOMAIN RANDOMIZATION; [Anonymous], 2016, PROGR NEURAL NETWORK; Aravind Rajeswaran, 2017, ICLR; ARTSTEIN Z, 1980, SIAM REV, V22, P172, DOI 10.1137/1022026; Barrett S., 2010, 9 INT C AUT AG MULT, V1; Battaglia P, 2018, ARXIV180601242; Bocsi B., 2013, INT JOINT C NEUR NET, P1; Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250; Erez T, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P73; Fu J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4019, DOI 10.1109/IROS.2016.7759592; Gupta A., 2017, ARXIV170302949; Gupta A., 2018, ICRA; Helwa Mohamed K., 2017, MULTIROBOT TRANSFER; Kahn Gregory, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3342, DOI 10.1109/ICRA.2017.7989379; Levine S, 2016, J MACH LEARN RES, V17; Lillicrap T., 2015, CORR; Mahler  J., 2017, RSS; Mandlekar A., 2017, IROS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mordatch I, 2015, IEEE INT C INT ROBOT, P5307, DOI 10.1109/IROS.2015.7354126; Nagarajan U., 2009, P IEEE INT C ROB AUT, P998; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Park Hae-Won, 2010, DYN WALK C DW MIT JU; Peng X. B., 2017, ABS171006537 CORR; Pinto Lerrel, 2017, ICRA; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schulman J., 2017, CORR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; SLOTINE JJE, 1988, IEEE T AUTOMAT CONTR, V33, P995, DOI 10.1109/9.14411; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wang T., 2018, P INT C LEARN REPR V; Xijian Huo, 2012, Proceedings of the 2012 IEEE International Conference on Robotics and Biomimetics (ROBIO), P277, DOI 10.1109/ROBIO.2012.6490979; Yu W., 2017, ARXIV170202453	37	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003085
C	Chen, Y; Yang, ZR; Xie, YC; Wang, ZR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Yi; Yang, Zhuoran; Xie, Yuchen; Wang, Zhaoran			Contrastive Learning from Pairwise Measurements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning from pairwise measurements naturally arises from many applications, such as rank aggregation, ordinal embedding, and crowdsourcing. However, most existing models and algorithms are susceptible to potential model misspecification. In this paper, we study a semiparametric model where the pairwise measurements follow a natural exponential family distribution with an unknown base measure. Such a semiparametric model includes various popular parametric models, such as the Bradley-Terry-Luce model and the paired cardinal model, as special cases. To estimate this semiparametric model without specifying the base measure, we propose a data augmentation technique to create virtual examples, which enables us to define a contrastive estimator. In particular, we prove that such a contrastive estimator is invariant to model misspecification within the natural exponential family, and moreover, attains the optimal statistical rate of convergence up to a logarithmic factor. We provide numerical experiments to corroborate our theory.	[Chen, Yi; Xie, Yuchen; Wang, Zhaoran] Northwestern Univ, Evanston, IL 60208 USA; [Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA	Northwestern University; Princeton University	Chen, Y (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.	yichen2016@u.northwestern.edu; zy6@princeton.edu; ycxie@u.northwestern.edu; zhaoran.wang@northwestern.edu	Wang, Zhaoran/P-7113-2018					Blei D., 2014, ADV NEURAL INFORM PR; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; CAO Y., 2015, INF THEOR ISIT 2015; Chan KCG, 2013, BIOMETRIKA, V100, P269, DOI 10.1093/biomet/ass056; Diao GQ, 2012, INT J BIOSTAT, V8, DOI 10.1515/1557-4679.1372; Geer S.A., 2000, EMPIRICAL PROCESSES, V6; Guiver J., 2009, P 26 ANN INT C MACH; Gunasekar S., 2014, INT C MACH LEARN; JAIN L, 2016, ADV NEURAL INFORM PR; Khetan A., 2016, ADV NEURAL INFORM PR; Lafond J, 2015, C LEARN THEOR; Ledoux M., 2005, CONCENTRATION MEASUR, V89; Liang KY, 2000, J ROY STAT SOC B, V62, P773, DOI 10.1111/1467-9868.00263; LU Y, 2015, COMM CONTR COMP ALL; Luce R. D., 2005, INDIVIDUAL CHOICE BE; NEGAHBAN S, 2017, ARXIV170407228; Negahban S., 2012, ADV NEURAL INFORM PR; Negahban S, 2017, OPER RES, V65, P266, DOI 10.1287/opre.2016.1534; Ning Y, 2017, ANN STAT, V45, P2299, DOI 10.1214/16-AOS1483; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; Rajkumar A., 2014, INT C MACH LEARN; SAMBASIVAN A. V, 2018, IEEE T INFORM THEORY; SHAH N, 2015, ARTIFICIAL INTELLIGE; SOUFIANI H. A, 2013, ADV NEURAL INFORM PR; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Yang Z., 2014, ARXIV14128697	31	6	6	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005049
C	Chen, YX; Singla, A; Mac Aodha, O; Perona, P; Yue, YS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Yuxin; Singla, Adish; Mac Aodha, Oisin; Perona, Pietro; Yue, Yisong			Understanding the Role of Adaptivity in Machine Teaching: The Case of Version Space Learners	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SAMPLE	In real-world applications of education, an effective teacher adaptively chooses the next example to teach based on the learner's current state. However, most existing work in algorithmic machine teaching focuses on the batch setting, where adaptivity plays no role. In this paper, we study the case of teaching consistent, version space learners in an interactive setting. At any time step, the teacher provides an example, the learner performs an update, and the teacher observes the learner's new state. We highlight that adaptivity does not speed up the teaching process when considering existing models of version space learners, such as the "worst-case" model (the learner picks the next hypothesis randomly from the version space) and the "preference-based" model (the learner picks hypothesis according to some global preference). Inspired by human teaching, we propose a new model where the learner picks hypotheses according to some local preference defined by the current hypothesis. We show that our model exhibits several desirable properties, e.g., adaptivity plays a key role, and the learner's transitions over hypotheses are smooth/interpretable. We develop adaptive teaching algorithms, and demonstrate our results via simulation and user studies.	[Chen, Yuxin; Mac Aodha, Oisin; Perona, Pietro; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA; [Singla, Adish] MPI SWS, Saarbrucken, Germany	California Institute of Technology	Chen, YX (corresponding author), CALTECH, Pasadena, CA 91125 USA.	chenyux@caltech.edu; adishs@mpi-sws.org; macaodha@caltech.edu; perona@caltech.edu; yyue@caltech.edu	Singla, Adish/ABG-8960-2021	Chen, Yuxin/0000-0003-2133-140X	Northrop Grumman; Bloomberg; AWS Research Credits; Google as part of the Visipedia project; Swiss NSF Early Mobility Postdoctoral Fellowship	Northrop Grumman; Bloomberg; AWS Research Credits; Google as part of the Visipedia project(Google Incorporated); Swiss NSF Early Mobility Postdoctoral Fellowship	This work was supported in part by Northrop Grumman, Bloomberg, AWS Research Credits, Google as part of the Visipedia project, and a Swiss NSF Early Mobility Postdoctoral Fellowship.	Aodha Oisin Mac, 2018, CORR; Balbach FJ, 2008, THEOR COMPUT SCI, V397, P94, DOI 10.1016/j.tcs.2008.02.025; Balbach FJ, 2011, INFORM COMPUT, V209, P296, DOI 10.1016/j.ic.2010.11.005; BALBACH RJ, 2005, ALT, V3734, P474; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bonawitz E, 2014, COGNITIVE PSYCHOL, V74, P35, DOI 10.1016/j.cogpsych.2014.06.003; Cakmak M., 2012, 26 AAAI C ART INT; Chen Yukang, 2018, CORR; Chen Yuxin, 2018, AISTATS; Doliwa T, 2014, J MACH LEARN RES, V15, P3107; E Mark Gold, 1967, INFORM CONTROL, V10; Gao ZY, 2017, J MACH LEARN RES, V18, P1; Gao Ziyuan, 2017, ALT, P185; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Haug Luis, 2018, ADV NEURAL INFORM PR; Hellerstein L, 2015, LECT NOTES COMPUT SC, V9079, P235, DOI 10.1007/978-3-319-18173-8_17; Hengst B, 2010, HIERARCHICAL REINFOR, P495, DOI [10.1007/978-0-387-30164-8, DOI 10.1007/978-0-387-30164-8]; Jha S, 2017, ACTA INFORM, V54, P693, DOI 10.1007/s00236-017-0294-5; Johns E, 2015, PROC CVPR IEEE, P2616, DOI 10.1109/CVPR.2015.7298877; Lange S, 1996, J COMPUT SYST SCI, V53, P88, DOI 10.1006/jcss.1996.0051; Levine M., 1975, COGNITIVE THEORY LEA; Liu J, 2016, PR MACH LEARN RES, V48; Liu WY, 2017, PR MACH LEARN RES, V70; Mei SK, 2015, AAAI CONF ARTIF INTE, P2871; Meng DY, 2017, INFORM SCIENCES, V414, P319, DOI 10.1016/j.ins.2017.05.043; Rafferty AN, 2016, COGNITIVE SCI, V40, P1290, DOI 10.1111/cogs.12290; Ross DA, 2008, INT J COMPUT VISION, V77, P125, DOI 10.1007/s11263-007-0075-7; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Singla A., 2013, NIPS WORKSH DAT DRIV; Singla A, 2014, PR MACH LEARN RES, V32, P154; Tekin C, 2015, INT CONF ACOUST SPEE, P5545, DOI 10.1109/ICASSP.2015.7179032; Vygotsky L, 1987, MIND SOC DEV HIGHER, V5291, P157; Weld Daniel S, 2012, HCOMP; Zhu X., 2013, ADV NEURAL INFORM PR, P1905; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083; Zhu Xiaojin, 2018, CORR; Zilles S, 2011, J MACH LEARN RES, V12, P349	37	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301046
C	Cullina, D; Bhagoji, AN; Mittal, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cullina, Daniel; Bhagoji, Arjun Nitin; Mittal, Prateek			PAC-learning in the presence of evasion adversaries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ROBUSTNESS	The existence of evasion attacks during the test phase of machine learning algorithms represents a significant challenge to both their deployment and understanding. These attacks can be carried out by adding imperceptible perturbations to inputs to generate adversarial examples and finding effective defenses and detectors has proven to be difficult. In this paper, we step away from the attack-defense arms race and seek to understand the limits of what can be learned in the presence of an evasion adversary. In particular, we extend the Probably Approximately Correct (PAC)-learning framework to account for the presence of an adversary. We first define corrupted hypothesis classes which arise from standard binary hypothesis classes in the presence of an evasion adversary and derive the Vapnik-Chervonenkis (VC)-dimension for these, denoted as the adversarial VC-dimension. We then show that sample complexity upper bounds from the Fundamental Theorem of Statistical learning can be extended to the case of evasion adversaries, where the sample complexity is controlled by the adversarial VC-dimension. We then explicitly derive the adversarial VC-dimension for halfspace classifiers in the presence of a sample-wise norm-constrained adversary of the type commonly studied for evasion attacks and show that it is the same as the standard VC-dimension. Finally, we prove that the adversarial VC-dimension can be either larger or smaller than the standard VC-dimension depending on the hypothesis class and adversary, making it an interesting object of study in its own right.	[Cullina, Daniel; Bhagoji, Arjun Nitin; Mittal, Prateek] Princeton Univ, Princeton, NJ 08544 USA	Princeton University	Cullina, D (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	dcullina@princeton.edu; abhagoji@princeton.edu; pmittal@princeton.edu		Cullina, Daniel/0000-0002-7471-2102	National Science Foundation [CNS-1553437, CIF-1617286, CNS-1409415]; Intel; Office of Naval Research through the Young Investigator Program (YIP) Award	National Science Foundation(National Science Foundation (NSF)); Intel(Intel Corporation); Office of Naval Research through the Young Investigator Program (YIP) Award(Office of Naval Research)	This work was supported by the National Science Foundation under grants CNS-1553437, CIF-1617286 and CNS-1409415, by Intel through the Intel Faculty Research Award and by the Office of Naval Research through the Young Investigator Program (YIP) Award.	Abbasi M, 2017, ARXIV PREPRINT ARXIV; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; [Anonymous], 2016, ARXIV160704311; Arnab A., 2018, CVPR; Bagnall A, 2017, ARXIV171204006; Bhagoji A. N., 2018, ARXIV180102780; Bhagoji AN, 2017, ARXIV PREPRINT ARXIV; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Biggio B., 2017, ARXIV171203141; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Brendel W., 2018, PROC 6 INT C LEARN R; Brown N., 2017, SCIENCE; Carlini N., 2016, ARXIV160804644; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009; Carlini Nicholas, 2017, ARXIV171108478; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Chen Shang-Tse, 2018, ARXIV180405810; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, ADV NEUR IN, V30; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Das N., 2018, ARXIV180206816; Deng L, 2013, IEEE INT NEW CIRC; DUDLEY R.M, 1967, J FUNCT ANAL, V1, P290, DOI DOI 10.1016/0022-1236(67)90017-1; Dziugaite Gintare Karolina, 2016, ARXIV160800853; EricWong Zico, 2018, INT C MACH LEARN, P5286; Evtimov I., 2018, CVPR; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Feinman R., 2017, ARXIV PREPRINT ARXIV; Fischer V., 2017, ARXIV170301101; Gilmer Justin, 2018, ARXIV180102774; Gong Zhitao, 2017, ARXIV170404960; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow Ian, 2017, 5 INT C LEARN REPR I; Grosse K., 2017, EUR S RES COMP SEC, VProceedings of the ESORICS (2), P62, DOI DOI 10.1007/978-3-319-66399-9_4; Grosse Kathrin, 2017, ARXIV170206280; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; He Warren, 2018, ICLR WORKSH; Hein M, 2017, NIPS 17; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Julian K. D., 2016, DAS 16; Kantchelian A, 2016, PR MACH LEARN RES, V48; KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052; Kos J., 2017, ARXIV170206832; Kos Jernej, 2017, ARXIV PREPRINT ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Liu Q, 2018, IEEE ACCESS, V6, P12103, DOI 10.1109/ACCESS.2018.2805680; Liu Yanpei, 2017, ICLR; Lu Jiajun, 2017, ADVERSARIAL EXAMPLES; Madry A., 2018, P ICLR VANC BC CAN; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Papernot Nicolas, 2016, ARXIV161103814; Raghunathan Aditi, 2018, ARXIV180109344; Rubinstein Benjamin I. P., 2009, Performance Evaluation Review, V37, P73, DOI 10.1145/1639562.1639592; Schmidt L, 2018, ADV NEUR IN, V31; Shaham U., 2018, DEFENDING ADVERSARIA; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sinha A., 2018, ICLR; Smutz C, 2016, 23RD ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2016), DOI 10.14722/ndss.2016.23078; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Wang QL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1145, DOI 10.1145/3097983.3098158; Wang YZ, 2018, PR MACH LEARN RES, V80; Weng TW, 2018, PR MACH LEARN RES, V80; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu W, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBER SITUATIONAL AWARENESS, DATA ANALYTICS AND ASSESSMENT (CYBER SA); Yuan XJ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P49	83	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300022
C	Du, SS; Hu, W; Lee, JD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Du, Simon S.; Hu, Wei; Lee, Jason D.			Algorithmic Regularization in Learning Deep Homogeneous Models: Layers are Automatically Balanced	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the implicit regularization imposed by gradient descent for learning multi-layer homogeneous functions including feed-forward fully connected and convolutional deep neural networks with linear, ReLU or Leaky ReLU activation. We rigorously prove that gradient flow (i.e. gradient descent with infinitesimal step size) effectively enforces the differences between squared norms across different layers to remain invariant without any explicit regularization. This result implies that if the weights are initially small, gradient flow automatically balances the magnitudes of all layers. Using a discretization argument, we analyze gradient descent with positive step size for the non-convex low-rank asymmetric matrix factorization problem without any regularization. Inspired by our findings for gradient flow, we prove that gradient descent with step sizes eta(t) = O (t(-(1/2+delta))) (0 < delta <= 1/2) automatically balances two low-rank factors and converges to a bounded global optimum. Furthermore, for rank-1 asymmetric matrix factorization we give a finer analysis showing gradient descent with constant step size converges to the global minimum at a globally linear rate. We believe that the idea of examining the invariance imposed by first order algorithms in learning homogeneous models could serve as a fundamental building block for studying optimization for learning deep models.	[Du, Simon S.] Carnegie Mellon Univ, Machine Learning Dept, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Hu, Wei] Princeton Univ, Comp Sci Dept, Princeton, NJ 08544 USA; [Lee, Jason D.] Univ Southern Calif, Dept Data Sci & Operat, Marshall Sch Business, Los Angeles, CA 90089 USA	Carnegie Mellon University; Princeton University; University of Southern California	Du, SS (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Sch Comp Sci, Pittsburgh, PA 15213 USA.	ssdu@cs.cmu.edu; huwei@cs.princeton.edu; jasonlee@marshall.usc.edu		Lee, Jason/0000-0003-0064-7800	ARO [W911NF-11-1-0303]	ARO	We thank Phil Long for his helpful comments on an earlier draft of this paper. JDL acknowledges support from ARO W911NF-11-1-0303.	Absil PA, 2005, SIAM J OPTIMIZ, V16, P531, DOI 10.1137/040605266; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; ARORA S., 2018, ARXIV180206509; Bartlett Peter L, 2018, ARXIV180206093; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Clarke FH., 2008, NONSMOOTH ANAL CONTR, V178; Davis  Damek, 2018, ARXIV180407795; Drusvyatskiy D, 2015, SIAM J CONTROL OPTIM, V53, P114, DOI 10.1137/130920216; Du Simon S, 2017, ARXIV170906129; Du Simon S, 2018, ARXIV180301206; Du Simon S, 2017, ARXIV171200779; Freeman C.D., 2016, ARXIV161101540; Ge R., 2017, ARXIV PREPRINT ARXIV; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ge R, 2017, PR MACH LEARN RES, V70; Giryes R, 2017, ARXIV171204741; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; Hardt M., 2016, ARXIV161104231; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Lee J. D., 2016, C LEARN THEOR, P1246; LI Y., 2017, ARXIV170509886; Ma S., 1688, P INT C MACH LEARN S, V80; Neyshabur B., 2015, ARXIV151106747; Nguyen  Quynh, 2017, ARXIV171010928; Nguyen  Quynh, 2017, ARXIV170408045; Panageas I., 2016, ARXIV160500405; SAFRAN I., 2017, ARXIV171208968; Safran I, 2016, PR MACH LEARN RES, V48; Shamir O., 2018, ARXIV180406739; Tian Yuandong, 2017, ARXIV170300560; Tu S., 2015, ARXIV150703566; Zhang J., 2018, ARXIV180500521; Zhong  Kai, 2017, ARXIV170603175; Zhou Pan, 2017, ARXIV170507038	39	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300036
C	Du, SS; Wang, YN; Zhai, XY; Balakrishnan, S; Salakhutdinov, R; Singh, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Du, Simon S.; Wang, Yining; Zhai, Xiyu; Balakrishnan, Sivaraman; Salakhutdinov, Ruslan; Singh, Aarti			How Many Samples are Needed to Estimate a Convolutional Neural Network?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A widespread folklore for explaining the success of Convolutional Neural Networks (CNNs) is that CNNs use a more compact representation than the Fully-connected Neural Network (FNN) and thus require fewer training samples to accurately estimate their parameters. We initiate the study of rigorously characterizing the sample complexity of estimating CNNs. We show that for an m-dimensional convolutional filter with linear activation acting on a d-dimensional input, the sample complexity of achieving population prediction error of epsilon is (O) over tilde (m/epsilon(2))(2), whereas the sample-complexity for its FNN counterpart is lower bounded by Omega (d/epsilon(2)) samples. Since, in typical settings m << d, this result demonstrates the advantage of using a CNN. We further consider the sample complexity of estimating a onehidden-layer CNN with linear activation where both the m-dimensional convolutional filter and the r-dimensional output weights are unknown. For this model, we show that the sample complexity is (O) over tilde((m + r)/epsilon(2)) when the ratio between the stride size and the filter size is a constant. For both models, we also present lower bounds showing our sample complexities are tight up to logarithmic factors. Our main tools for deriving these results are a localized empirical process analysis and a new lemma characterizing the convolutional structure. We believe that these tools may inspire further developments in understanding CNNs.	[Du, Simon S.; Wang, Yining; Balakrishnan, Sivaraman; Salakhutdinov, Ruslan; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhai, Xiyu] MIT, Cambridge, MA 02139 USA	Carnegie Mellon University; Massachusetts Institute of Technology (MIT)	Du, SS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.			Wang, Yining/0000-0001-9410-0392	AFRL grant [FA8750-17-2-0212]; DARPA [D17AP00001]	AFRL grant; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This research was partly funded by AFRL grant FA8750-17-2-0212 and DARPA D17AP00001.	[Anonymous], 2017, ARXIV170906010; [Anonymous], 2018, ARXIV180301206; Arora Sanjeev, 2018, ARXIV180205296; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Bartlett P. L., 2017, ARXIV1703; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Du Simon S., 2018, INT C LEARN REPR; DUDLEY R.M, 1967, J FUNCT ANAL, V1, P290, DOI DOI 10.1016/0022-1236(67)90017-1; Feng J., 2017, ARXIV170507038; Freeman C.D., 2016, ARXIV161101540; Ge R., 2017, ARXIV PREPRINT ARXIV; Ge R, 2017, PR MACH LEARN RES, V70; GOEL S., 2016, ARXIV161110258; GRAHAM RL, 1980, IEEE T INFORM THEORY, V26, P37, DOI 10.1109/TIT.1980.1056141; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; Hardt M., 2016, ARXIV161104231; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Huang F., 2015, COMPUT SCI, V44, P116; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Klivans A., 2017, ARXIV170803708; Konstantinos P., 2017, ARXIV180100171; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; LI Y., 2017, ARXIV170509886; Meka R., 2018, ARXIV180202547; Neyshabur Behnam, 2017, ARXIV170709564; Nguyen Q., 2017, ARXIV170408045; Nguyen Quynh, 2017, ARXIV171010928; Qu Q., 2017, ARXIV171200716; SAFRAN I., 2017, ARXIV171208968; Safran I, 2016, PR MACH LEARN RES, V48; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Singh S., 2018, P INT C ART INT STAT, P1327; Song L., 2017, ADV NEURAL INFORM PR, P5514; Tian Yuandong, 2017, ARXIV170300560; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Van de Geer S, 2000, EMPIRICAL PROCESSES, V6; Van der Vaart AW, 1998, ASYMPTOTIC STAT, V3; Vershynin R, 2012, J THEOR PROBAB, V25, P655, DOI 10.1007/s10959-010-0338-z; Wang Y.-X., 2016, AAAI; Wasserman L., 2013, ALL STAT CONCISE COU; Yu A.W., 2018, P INT C LEARN REPR I; Yu B., 1997, FESTSCHRIFT L LECAM, P423; ZHANG Y., 2015, ARXIV151107948; Zhang Y., 2017, P IEEE C COMPUTER VI, P4894; Zhong K., 2017, ARXIV171103440; Zhong  Kai, 2017, ARXIV170603175	51	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300035
C	Dumitrascu, B; Feng, KR; Engelhardt, BE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dumitrascu, Bianca; Feng, Karen; Engelhardt, Barbara E.			PG-TS: Improved Thompson Sampling for Logistic Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGRET	We address the problem of regret minimization in logistic contextual bandits, where a learner decides among sequential actions or arms given their respective contexts to maximize binary rewards. Using a fast inference procedure with Polya-Gamma distributed augmentation variables, we propose an improved version of Thompson Sampling, a Bayesian formulation of contextual bandits with near-optimal performance. Our approach, Polya-Gamma augmented Thompson Sampling (PG-TS), achieves state-of-the-art performance on simulated and real data. PG-TS explores the action space efficiently and exploits high-reward arms, quickly converging to solutions of low regret. Its explicit estimation of the posterior distribution of the context feature covariance leads to substantial empirical gains over approximate approaches. PG-TS is the first approach to demonstrate the benefits of PolyaGamma augmentation in bandits and to propose an efficient Gibbs sampler for approximating the analytically unsolvable integral of logistic contextual bandits.	[Dumitrascu, Bianca] Princeton Univ, Lewis Sigler Inst Integrat Genom, Princeton, NJ 08540 USA; [Feng, Karen; Engelhardt, Barbara E.] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA	Princeton University; Princeton University	Dumitrascu, B (corresponding author), Princeton Univ, Lewis Sigler Inst Integrat Genom, Princeton, NJ 08540 USA.	biancad@princeton.edu; karenfeng@princeton.edu; bee@princeton.edu		Dumitrascu, Bianca/0000-0001-8328-2354				Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P12; Abeille M., 2017, AISTATS 2017; AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Shipra,, 2017, J ACM, V64, DOI 10.1145/3088510; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barber RF., 2016, STAT ANAL HIGH DIMEN, P15, DOI [10.1007/978-3-319-27099-9_2, DOI 10.1007/978-3-319-27099-9_2]; Bay S.D., 2000, ACM SIGKDD EXPLOR NE, V2, P81; Choi HM, 2013, ELECTRON J STAT, V7, P2054, DOI 10.1214/13-EJS837; Chu W., 2009, P KDD; Devroye L., 1986, NONUNIFORM RANDOM VA, DOI [10.1007/978-1-4613-8643-8, DOI 10.1007/978-1-4613-8643-8]; Devroye L, 2009, STAT PROBABIL LETT, V79, P2251, DOI 10.1016/j.spl.2009.07.028; Duff M., 2003, P 20 INT C MACH LEAR, P131; Filippi S., 2010, ADV NEURAL INFORM PR, V23, P586; Gentile C, 2014, PR MACH LEARN RES, V32, P757; Glynn C., 2015, ARXIV151103947; Hazan E., 2014, C LEARNING THEORY, P197; Jun KS, 2017, ADV NEUR IN, V30; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Linderman Scott, 2015, ADV NEURAL INFORM PR, P3456; McMahan H. B., 2012, C LEARN THEOR, P44; Osband I., 2015, ARXIV150700300; Pillow JW., 2012, ADV NEURAL INFORM PR, V3, P1898, DOI [10.5555/2999325.2999347, DOI 10.5555/2999325.2999347]; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Russo D, 2016, J MACH LEARN RES, V17; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Russo Daniel, 2017, ARXIV170702038; Strens, 2000, P 17 INT C MACH LEAR, P943; Tewari A., 2017, MOBILE HLTH, P495; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Urteaga I., 2017, ARXIV170903162; Windle J., 2014, ARXIV14050506; WOODROOFE M, 1979, J AM STAT ASSOC, V74, P799, DOI 10.2307/2286402; Zhou Mingyuan, 2012, Proc Int Conf Mach Learn, V2012, P1343	38	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304062
C	Dutordoir, V; Salimbeni, H; Deisenroth, MP; Hensman, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dutordoir, Vincent; Salimbeni, Hugh; Deisenroth, Marc Peter; Hensman, James			Gaussian Process Conditional Density Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Conditional Density Estimation (CDE) models deal with estimating conditional distributions. The conditions imposed on the distribution are the inputs of the model. CDE is a challenging task as there is a fundamental trade-off between model complexity, representational capacity and overfitting. In this work, we propose to extend the model's input with latent variables and use Gaussian processes (GPs) to map this augmented input onto samples from the conditional distribution. Our Bayesian approach allows for the modeling of small datasets, but we also provide the machinery for it to be applied to big data using stochastic variational inference. Our approach can be used to model densities even in sparse data regions, and allows for sharing learned structure between conditions. We illustrate the effectiveness and wide-reaching applicability of our model on a variety of real-world problems, such as spatio-temporal density estimation of taxi drop-offs, non-Gaussian noise modeling, and few-shot learning on omniglot images.	[Dutordoir, Vincent; Salimbeni, Hugh; Deisenroth, Marc Peter; Hensman, James] PROWLER Io, Cambridge, England; [Salimbeni, Hugh; Deisenroth, Marc Peter] Imperial Coll London, London, England	Imperial College London	Dutordoir, V (corresponding author), PROWLER Io, Cambridge, England.	vincent@prowler.io; hugh@prowler.io; marc@prowler.io; james@prowler.io						Adams Ryan P, 2009, ARXIV09124896; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Arbel Michael, 2018, KERNEL CONDITIONAL E; Bishop C.M., 1994, MIXTURE DENSITY NETW; Bodin Erik, 2017, ARXIV170705534; Bui T. D., 2015, BLACK BOX LEARN INF; Cremer C, 2018, PR MACH LEARN RES, V80; Dai Z., 2017, ADV NEURAL INFORM PR, P6510; Dai Zhenwen, 2015, INT C LEARN REPR; Damianou A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P228; Depeweg S., 2016, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lawrence N. D., 2006, INT C MACH LEARN; Matthews AGD, 2016, JMLR WORKSH CONF PRO, V51, P231; Rasmussen C.E., 2003, P ADV NEUR INF PROC, P545, DOI DOI 10.1177/1475090214540874; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sohn Kihyuk, ADV NEURAL INFORM PR; Sriperumbudur B, 2017, J MACH LEARN RES, V18; Titsias M. K., 2013, ADV NEURAL INFORM PR; Titsias Michalis, 2010, ARTIFICIAL INTELLIGE; Trippe Brian L, 2017, BAYES DEEP LEARN WOR; Wang C., 2012, GAUSSIAN PROCESS REG; Wu Y., 2016, ARXIV161104273	30	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302040
C	Frecon, J; Salzo, S; Pontil, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Frecon, Jordan; Salzo, Saverio; Pontil, Massimiliano			Bilevel Learning of the Group Lasso Structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				OPTIMIZATION; SPARSITY; REGULARIZERS; REGRESSION; SELECTION	Regression with group-sparsity penalty plays a central role in high-dimensional prediction problems. However, most existing methods require the group structure to be known a priori. In practice, this may be a too strong assumption, potentially hampering the effectiveness of the regularization method. To circumvent this issue, we present a method to estimate the group structure by means of a continuous bilevel optimization problem where the data is split into training and validation sets. Our approach relies on an approximation scheme where the lower level problem is replaced by a smooth dual forward-backward algorithm with Bregman distances. We provide guarantees regarding the convergence of the approximate procedure to the exact problem and demonstrate the well behaviour of the proposed method on synthetic experiments. Finally, a preliminary application to genes expression data is tackled with the purpose of unveiling functional groups.	[Frecon, Jordan; Salzo, Saverio; Pontil, Massimiliano] Ist Italiano Tecnol, Computat Stat & Machine Learning, Genoa, Italy; [Pontil, Massimiliano] UCL, Dept Comp Sci, London, England	Istituto Italiano di Tecnologia - IIT; University of London; University College London	Frecon, J (corresponding author), Ist Italiano Tecnol, Computat Stat & Machine Learning, Genoa, Italy.		Salzo, Saverio/AAZ-7481-2021	Salzo, Saverio/0000-0003-0494-9101	SAP SE	SAP SE	We wish to thank Luca Franceschi and the anonymous referees for their useful comments. We also would like to thank Giorgio Valentini for providing the gene expression dataset. This work was supported in part by SAP SE.	Ahmed A, 2009, P NATL ACAD SCI USA, V106, P11878, DOI 10.1073/pnas.0901910106; Bauschke HH, 2017, CONVEX ANAL MONOTONE, DOI 10.1007/978-3-319-48311-5; Bengio Y, 2000, NEURAL COMPUT, V12, P1889, DOI 10.1162/089976600300015187; Calatroni Luca, 2016, VARIATIONAL METHODS; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dempe S., 2002, FDN BILEVEL PROGRAMM; Franceschi L., 2018, P 35 INT C MACH LEAR, V80, P1568; Franceschi L, 2017, PR MACH LEARN RES, V70; Hernandez-~Lobato D., 2013, PROC INT C NEURAL IN, P746; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Jenatton R, 2011, J MACH LEARN RES, V12, P2777; Kim S, 2012, ANN APPL STAT, V6, P1095, DOI 10.1214/12-AOAS549; Lounici K, 2011, ANN STAT, V39, P2164, DOI 10.1214/11-AOS896; Ma SG, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-60; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Maurer A, 2012, J MACH LEARN RES, V13, P671; Micchelli CA, 2013, ADV COMPUT MATH, V38, P455, DOI 10.1007/s10444-011-9245-9; Ochs P, 2016, J MATH IMAGING VIS, V56, P175, DOI 10.1007/s10851-016-0663-7; Nguyen QV, 2017, VIETNAM J MATH, V45, P519, DOI 10.1007/s10013-016-0238-3; Re M., 2009, INT M COMP INT METH; Reddi S. J., 2016, ADV NEURAL INFORM PR, P1145; Shervashidze N, 2015, IEEE T SIGNAL PROCES, V63, P4894, DOI 10.1109/TSP.2015.2446432; Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhao P, 2009, ANN STAT, V37, P3468, DOI 10.1214/07-AOS584	31	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002081
C	Freksen, C; Kamma, L; Larsen, KG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Freksen, Casper; Kamma, Lior; Larsen, Kasper Green			Fully Understanding The Hashing Trick	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				JOHNSON-LINDENSTRAUSS; RANDOM PROJECTIONS	Feature hashing, also known as the hashing trick, introduced by Weinberger et al. (2009), is one of the key techniques used in scaling-up machine learning algorithms. Loosely speaking, feature hashing uses a random sparse projection matrix A : R-n -> R-m (where m << n) in order to reduce the dimension of the data from n to m while approximately preserving the Euclidean norm. Every column of A contains exactly one non-zero entry, equals to either -1 or 1. Weinberger et al. showed tail bounds on parallel to Ax parallel to(2)(2). Specifically they showed that for every epsilon, delta, if parallel to x parallel to(infinity)/parallel to x parallel to(2) is sufficiently small, and m is sufficiently large, then Pr[ vertical bar parallel to Ax parallel to(2)(2) - parallel to x parallel to(2)(2) vertical bar < epsilon parallel to x parallel to(2)(2)] >= 1 - delta. These bounds were later extended by Dasgupta et al. (2010) and most recently refined by Dahlgaard et al. (2017), however, the true nature of the performance of this key technique, and specifically the correct tradeoff between the pivotal parameters parallel to x parallel to(infinity)/parallel to x parallel to(2), m, epsilon, delta remained an open question. We settle this question by giving tight asymptotic bounds on the exact tradeoff between the central parameters, thus providing a complete understanding of the performance of feature hashing. We complement the asymptotic bound with empirical data, which shows that the constants "hiding" in the asymptotic notation are, in fact, very close to 1, thus further illustrating the tightness of the presented bounds in practice.	[Freksen, Casper; Kamma, Lior; Larsen, Kasper Green] Aarhus Univ, Dept Comp Sci, Aarhus, Denmark	Aarhus University	Freksen, C (corresponding author), Aarhus Univ, Dept Comp Sci, Aarhus, Denmark.	cfreksen@cs.au.dk; lior.kamma@cs.au.dk; larsen@cs.au.dk		Larsen, Kasper Green/0000-0001-8841-5929	Villum Young Investigator Grant	Villum Young Investigator Grant(Villum Fonden)	This work was supported by a Villum Young Investigator Grant.	Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; Cohen Michael B, 2018, 1 S SIMPL ALG SOSA S; Dahlgaard S, 2015, ANN IEEE SYMP FOUND, P1292, DOI 10.1109/FOCS.2015.83; Dalessandro B, 2013, BIG DATA-US, V1, P110, DOI 10.1089/big.2013.0010; Dasgupta A, 2010, ACM S THEORY COMPUT, P341; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dheeru D., 2019, UCI MACHINE LEARNING; Freksen  C.B., 2017, 28 INT S ALG COMP IS; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Hegde C., 2008, ADV NEURAL INF PROCE, V20, P641; Jayram TS, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483706; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902; Larsen KG, 2017, ANN IEEE SYMP FOUND, P633, DOI 10.1109/FOCS.2017.64; Maillard O., 2009, ADV NEURAL INFORM PR, P1213; Matousek J, 2008, RANDOM STRUCT ALGOR, V33, P142, DOI 10.1002/rsa.20218; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Nelson J, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P101; Paul S, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2641760; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Suthaharan S, 2015, MACHINE LEARNING MOD; Thorup, 2017, P 31 INT C NEUR INF, P6615; Thorup M, 2013, ANN IEEE SYMP FOUND, P90, DOI 10.1109/FOCS.2013.18; Thorup M, 2012, SIAM J COMPUT, V41, P293, DOI 10.1137/100800774; Vempala S. S., 2005, DIMACS SERIES DISCRE; Weinberger K., 2009, ANN INT C MACH LEARN, P1113, DOI DOI 10.1145/1553374.1553516	32	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305041
C	Fruit, R; Pirotta, M; Lazaric, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fruit, Ronan; Pirotta, Matteo; Lazaric, Alessandro			Near Optimal Exploration-Exploitation in Non-Communicating Markov Decision Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					While designing the state space of an MDP, it is common to include states that are transient or not reachable by any policy (e.g., in mountain car, the product space of speed and position contains configurations that are not physically reachable). This results in weakly-communicating or multi-chain MDPs. In this paper, we introduce TUCRL, the first algorithm able to perform efficient exploration-exploitation in any finite Markov Decision Process (MDP) without requiring any form of prior knowledge. In particular, for any MDP with S-C communicating states, A actions and Gamma(C) <= S-C possible communicating next states, we derive a (O) over tilde (D-C root Gamma(C)S(C)AT) regret bound, where D-C is the diameter (i.e., the length of the longest shortest path between any two states) of the communicating part of the MDP. This is in contrast with existing optimistic algorithms (e.g., UCRL, Optimistic PSRL) that suffer linear regret in weakly-communicating MDPs, as well as posterior sampling or regularised algorithms (e.g., REGAL), which require prior knowledge on the bias span of the optimal policy to achieve sub-linear regret. We also prove that in weakly-communicating MDPs, no algorithm can ever achieve a logarithmic growth of the regret without first suffering a linear regret for a number of steps that is exponential in the parameters of the MDP. Finally, we report numerical simulations supporting our theoretical findings and showing how TUCRL overcomes the limitations of the state-of-the-art.	[Fruit, Ronan; Pirotta, Matteo] Inria Lille, Sequel Team, Lille, France; [Lazaric, Alessandro] Facebook AI Res, Menlo Pk, CA USA	Facebook Inc	Fruit, R (corresponding author), Inria Lille, Sequel Team, Lille, France.	ronan.fruit@inria.fr; matteo.pirotta@inria.fr; lazaric@fb.com			French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council; French National Research Agency (ANR) under project ExTra-Learn [ANR-14-CE24-0010-01]	French Ministry of Higher Education and Research; Nord-Pas-de-Calais Regional Council(Region Hauts-de-France); French National Research Agency (ANR) under project ExTra-Learn(French National Research Agency (ANR))	This research was supported in part by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and French National Research Agency (ANR) under project ExTra-Learn (n. ANR-14-CE24-0010-01).	Abbasi-Yadkori Y, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P2; AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; Audibert JY, 2007, LECT NOTES ARTIF INT, V4754, P150; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Fruit Ronan, 2018, CORR; Fruit Ronan, 2017, NIPS, P3169; Gopalan A., 2015, P 28 C LEARNING THEO, P861; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Maillard Odalric-Ambrym, 2013, P 30 INT C MACH LEAR, V28, P543; Maurer Andreas, 2009, ARXIV09073740; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moore A.W., 1990, TECHNICAL REPORT; Osband I., 2016, ARXIV160802732; Osband I, 2017, PR MACH LEARN RES, V70; OUYANG Y, 2017, ADV NEURAL INFORM PR, V30, P1333; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Talebi Mohammad Sadegh, 2018, ALT, V83, P770; Theocharous Georgios, 2017, CORR; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Zaremba W., 2016, CORR	25	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303003
C	Gimelfarb, M; Sanner, S; Lee, CG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gimelfarb, Michael; Sanner, Scott; Lee, Chi-Guhn			Reinforcement Learning with Multiple Experts: A Bayesian Model Combination Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Potential based reward shaping is a powerful technique for accelerating convergence of reinforcement learning algorithms. Typically, such information includes an estimate of the optimal value function and is often provided by a human expert or other sources of domain knowledge. However, this information is often biased or inaccurate and can mislead many reinforcement learning algorithms. In this paper, we apply Bayesian Model Combination with multiple experts in a way that learns to trust a good combination of experts as training progresses. This approach is both computationally efficient and general, and is shown numerically to improve convergence across discrete and continuous domains and different reinforcement learning algorithms.	[Gimelfarb, Michael; Sanner, Scott; Lee, Chi-Guhn] Univ Toronto, Mech & Ind Engn, Toronto, ON, Canada	University of Toronto	Gimelfarb, M (corresponding author), Univ Toronto, Mech & Ind Engn, Toronto, ON, Canada.	mike.gimelfarb@mail.utoronto.ca; ssanner@mie.utoronto.ca; cglee@mie.utoronto.ca		Lee, Chi-Guhn/0000-0002-0916-0241				Asmuth J., 2009, P 25 C UNC ART INT, P19; Bertsekas D. P., 2004, STOCHASTIC OPTIMAL C; Brockman G., 2016, OPENAI GYM; Brys T, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3352; Christiano Paul F, 2017, P 31 INT C NEURAL IN, P4302; Dearden R, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P761; Devlin SM, 2012, P 11 INT C AUTONOMOU, V1, P433; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Downey C., 2010, ICML, P311; Eck A., 2013, P 12 INT C AUT AG MU, P1123; Geva S., 1993, IEEE Control Systems Magazine, V13, P40, DOI 10.1109/37.236324; Griffith S., 2013, ADV NEURAL INFORM PR, P2625; Grzes M., 2009, P AAMAS 2009 WORKSH, V115; Grzes M, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P565; Grzes M, 2010, NEURAL NETWORKS, V23, P541, DOI 10.1016/j.neunet.2010.01.001; Harutyunyan A, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1913; Hsu W.-S., 2016, ADV NEURAL INFORM PR, P4536; Konidaris G., 2006, P 23 INT C MACH LEAR, P489, DOI DOI 10.1145/1143844.1143906; Lantz B., 2019, MACHINE LEARNING R; Li Yuxi, 2017, ARXIV170107274; Maclin R., 2005, P 20 NAT C ART INT, P819; Marom O., 2018, AAAI; Minka TP., 2000, BAYESIAN MODEL AVERA, P1; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; O'Hagan A, 2004, WAG UR FRON, V3, P31; Omar F., 2016, ONLINE BAYESIAN LEAR; Philipp P, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P962; Randlov J., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P463; Sam D, 2011, 10 INT C AUTONOMOUS, V1, P225; Suay HB, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P429; Taylor ME, 2007, J MACH LEARN RES, V8, P2125; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Tenorio-Gonzalez AC, 2010, LECT NOTES ARTIF INT, V6433, P483; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Wiewiora E, 2003, J ARTIF INTELL RES, V19, P205, DOI 10.1613/jair.1190; Wiewiora E, 2003, P 20 INT C MACH LEAR, P792	38	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004012
C	Guo, WB; Huang, S; Tao, YZ; Xing, XY; Lin, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Wenbo; Huang, Sui; Tao, Yunzhe; Xing, Xinyu; Lin, Lin			Explaining Deep Learning Models - A Bayesian Non-parametric Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Understanding and interpreting how machine learning (ML) models make decisions have been a big challenge. While recent research has proposed various technical approaches to provide some clues as to how an ML model makes individual predictions, they cannot provide users with an ability to inspect a model as a complete entity. In this work, we propose a novel technical approach that augments a Bayesian non-parametric regression mixture model with multiple elastic nets. Using the enhanced mixture model, we can extract generalizable insights for a target model through a global approximation. To demonstrate the utility of our approach, we evaluate it on different ML models in the context of image recognition. The empirical results indicate that our proposed approach not only outperforms the state-of-the-art techniques in explaining individual decisions but also provides users with an ability to discover the vulnerabilities of the target ML models.	[Guo, Wenbo; Xing, Xinyu; Lin, Lin] Penn State Univ, University Pk, PA 16802 USA; [Huang, Sui] Netflix Inc, Los Gatos, CA USA; [Tao, Yunzhe] Columbia Univ, New York, NY 10027 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; Netflix, Inc.; Columbia University	Guo, WB (corresponding author), Penn State Univ, University Pk, PA 16802 USA.	wzg13@ist.psu.edu; shuang@netflix.com; y.tao@columbia.edu; xxing@ist.psu.edu; llin@psu.edu		Guo, Wenbo/0000-0002-6890-4503; Lin, Lin/0000-0002-7464-1172	NSF [CNS-1718459]; NVIDIA Corporation	NSF(National Science Foundation (NSF)); NVIDIA Corporation	We gratefully acknowledge the funding from NSF grant CNS-1718459 and the support of NVIDIA Corporation with the donation of the GPU. We also would like to thank anonymous reviewers, Kaixuan Zhang, Xinran Li and Chenxin Ma for their helpful comments.	[Anonymous], 2017, ARXIV171109784; [Anonymous], 2015, ICLR 2015 INT C LEAR; Bach Sebastian, 2015, PLOS ONE; Chollet F., 2015, KERAS; Cron A. J., 2011, AM STAT; Dabkowski P., 2017, P 31 C NEUR INF PROC; Deng J., 2009, P 22 C COMP VIS PATT; Fong R. C., 2017, P 16 INT C COMP VIS; Gan C, 2015, PROC CVPR IEEE, P2568, DOI 10.1109/CVPR.2015.7298872; Hans C., 2011, JOURNAL OF THE AMERI; Hennig C., 2010, ADV DATA ANAL CLASSI; Hughes M.C., 2018, P 32 AAAI C ART INT; Ishwaran H., 2001, J AM STAT ASS; Kim B., 2016, P 30 C NEUR INF PROC; Knight W., 2017, DARK SECRET HEART AI; Knight W., 2017, FINANCIAL WORLD WANT; Koh Pang Wei, 2017, P 34 INT C MACH LEAR; Li Jiwei, 2016, ARXIV161208220; Li Q., 2010, BAYESIAN ANAL; Liang F., 2016, TECHNOMETRICS; Lipton Zachary C., 2016, ARXIV160603490CSSTAT; Lundberg S. M., 2017, P 31 C NEUR INF PROC; Marin J.-M., 2005, HDB STAT; Ribeiro Marco Tulio, 2016, P 22 INT C KNOWL DIS; Selvaraju Ramprasaath R, 2016, ARXIVORGABS161002391; Shrikumar A., 2017, P 34 INT C MACH LEAR; Simonyan K., 2013, DEEP INSIDE CONVOLUT; SMITH AFM, 1993, J ROY STAT SOC B MET, V55, P3; Springenberg J. T., 2015, P 3 INT C LEARN REPR; Srivastava S., 2015, P 18 INT C ART INT S; Suchard M., 2010, J COMPUTATIONAL GRAP; Sundararajan M, 2016, ARXIV PREPRINT ARXIV; Xiao H., 2017, ARXIV 170807747; Yang H., 2011, MULTIPLE BAYES UNPUB; Zeiler M. D., 2014, P 13 EUR C COMP VIS; Zintgraf L. M., 2017, P 5 INT C LEARN REPR; Zou H., 2005, J ROYAL STAT SOC B	38	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304052
C	Han, K; Wen, HG; Zhang, YZ; Fu, D; Culurciello, E; Liu, ZM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Han, Kuan; Wen, Haiguang; Zhang, Yizhen; Fu, Di; Culurciello, Eugenio; Liu, Zhongming			Deep Predictive Coding Network with Local Recurrent Processing for Object Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL-NETWORKS; VISUAL-CORTEX; VISION; MODEL	Inspired by "predictive coding" - a theory in neuroscience, we develop a bidirectional and dynamic neural network with local recurrent processing, namely predictive coding network (PCN). Unlike feedforward-only convolutional neural networks, PCN includes both feedback connections, which carry top-down predictions, and feedforward connections, which carry bottom-up errors of prediction. Feedback and feedforward connections enable adjacent layers to interact locally and recurrently to refine representations towards minimization of layer-wise prediction errors. When unfolded over time, the recurrent processing gives rise to an increasingly deeper hierarchy of non-linear transformation, allowing a shallow network to dynamically extend itself into an arbitrarily deep network. We train and test PCN for image classification with SVHN, CIFAR and ImageNet datasets. Despite notably fewer layers and parameters, PCN achieves competitive performance compared to classical and state-of-the-art models. Further analysis shows that the internal representations in PCN converge over time and yield increasingly better accuracy in object recognition. Errors of top-down prediction also reveal visual saliency or bottom-up attention.	[Han, Kuan; Wen, Haiguang; Zhang, Yizhen; Fu, Di; Culurciello, Eugenio; Liu, Zhongming] Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA; [Culurciello, Eugenio; Liu, Zhongming] Purdue Univ, Weldon Sch Biomed Engn, W Lafayette, IN 47907 USA; [Han, Kuan; Wen, Haiguang; Zhang, Yizhen; Fu, Di; Liu, Zhongming] Purdue Univ, Purdue Inst Integrat Neurosci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Liu, ZM (corresponding author), Purdue Univ, Sch Elect & Comp Engn, W Lafayette, IN 47907 USA.; Liu, ZM (corresponding author), Purdue Univ, Weldon Sch Biomed Engn, W Lafayette, IN 47907 USA.; Liu, ZM (corresponding author), Purdue Univ, Purdue Inst Integrat Neurosci, W Lafayette, IN 47907 USA.	zmliu@purdue.edu		Zhang, Yizhen/0000-0002-2836-2666	NIH [R01MH104402]; College of Engineering at Purdue University	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); College of Engineering at Purdue University	The research was supported by NIH R01MH104402 and the College of Engineering at Purdue University.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], [No title captured]; Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038; Boehler CN, 2008, P NATL ACAD SCI USA, V105, P8742, DOI 10.1073/pnas.0801999105; Bressler SL, 2008, J NEUROSCI, V28, P10056, DOI 10.1523/JNEUROSCI.1776-08.2008; Buffalo EA, 2010, P NATL ACAD SCI USA, V107, P361, DOI 10.1073/pnas.0907658106; Camprodon JA, 2010, J COGNITIVE NEUROSCI, V22, P1262, DOI 10.1162/jocn.2009.21253; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010; Dumoulin Vincent, 2016, ARXIV E PRINTS; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Friston KJ, 2010, NAT REV NEUROSCI, V11, P127, DOI 10.1038/nrn2787; García Herrera Arístides Lázaro, 2017, Rev.Med.Electrón., P1; George D, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000532; Goodfellow I. J., 2013, ARXIV13024389; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Han K, 2019, NEUROIMAGE, V198, P125, DOI 10.1016/j.neuroimage.2019.05.039; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He Kaiming, 2015, CVPR, DOI [10.1109/CVPR.2015.7299173, DOI 10.1109/CVPR.2015.7299173]; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Huang YP, 2011, WIRES COGN SCI, V2, P580, DOI 10.1002/wcs.142; Jastrzebski S., 2017, ARXIV PREPRINT ARXIV; Jehee JFM, 2006, J PHYSIOL-PARIS, V100, P125, DOI 10.1016/j.jphysparis.2006.09.011; Kietzmann TC, 2018, BIORXIV; Koivisto M, 2011, J NEUROSCI, V31, P2488, DOI 10.1523/JNEUROSCI.3074-10.2011; Kriegeskorte N, 2015, ANNU REV VIS SCI, V1, P417, DOI 10.1146/annurev-vision-082114-035447; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson G., 2016, ARXIV PREPRINT ARXIV; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liao Q., 2016, CORR; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Lotter William, 2016, ARXIV160508104; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Nair V, 2010, P 27 INT C MACHINE L, P807; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; O'Reilly RC, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00124; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Romero Adriana, 2014, ARXIV14126550; Seeliger K, 2018, NEUROIMAGE, V181, P775, DOI 10.1016/j.neuroimage.2018.07.043; Serre T, 2007, PROG BRAIN RES, V165, P33, DOI 10.1016/S0079-6123(06)65004-8; Shen Guohua, 2017, BIORXIV; Shi JX, 2018, HUM BRAIN MAPP, V39, P2269, DOI 10.1002/hbm.24006; Shin HC, 2016, IEEE T MED IMAGING, V35, P1285, DOI 10.1109/TMI.2016.2528162; Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551; Spratling MW, 2017, COGN COMPUT, V9, P151, DOI 10.1007/s12559-016-9445-1; Spratling MW, 2017, BRAIN COGNITION, V112, P92, DOI 10.1016/j.bandc.2015.11.003; Spratling MW, 2012, NEURAL COMPUT, V24, P60, DOI 10.1162/NECO_a_00222; Spratling MW, 2008, FRONT COMPUT NEUROSC, V2, P1, DOI 10.3389/neuro.10.004.2008; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; St-Yves G., 2018, BIORXIV; Stollenga M.F., 2014, ADV NEURAL INFORM PR, P3545; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; VANESSEN DC, 1983, TRENDS NEUROSCI, V6, P370, DOI 10.1016/0166-2236(83)90167-4; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wen HG, 2018, PR MACH LEARN RES, V80; Wen HG, 2018, CEREB CORTEX, V28, P4136, DOI 10.1093/cercor/bhx268; Wyatte D, 2014, FRONT PSYCHOL, V5, DOI 10.3389/fpsyg.2014.00674; Wyatte D, 2012, J COGNITIVE NEUROSCI, V24, P2248, DOI 10.1162/jocn_a_00282; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196	70	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003073
C	Harer, JA; Ozdemir, O; Lazovich, T; Reale, CP; Russell, RL; Kim, LY; Chin, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Harer, Jacob A.; Ozdemir, Onur; Lazovich, Tomo; Reale, Christopher P.; Russell, Rebecca L.; Kim, Louis Y.; Chin, Peter			Learning to Repair Software Vulnerabilities with Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Motivated by the problem of automated repair of software vulnerabilities, we propose an adversarial learning approach that maps from one discrete source domain to another target domain without requiring paired labeled examples or source and target domains to be bijections. We demonstrate that the proposed adversarial learning approach is an effective technique for repairing software vulnerabilities, performing close to seq2seq approaches that require labeled pairs. The proposed Generative Adversarial Network approach is application-agnostic in that it can be applied to other problems similar to code repair, such as grammar correction or sentiment translation.	[Harer, Jacob A.; Ozdemir, Onur; Lazovich, Tomo; Reale, Christopher P.; Russell, Rebecca L.; Kim, Louis Y.] Draper, Cambridge, MA 02139 USA; [Harer, Jacob A.; Chin, Peter] Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA; [Lazovich, Tomo] Lightmatter, Boston, MA USA	Boston University	Harer, JA (corresponding author), Draper, Cambridge, MA 02139 USA.; Harer, JA (corresponding author), Boston Univ, Dept Comp Sci, 111 Cummington St, Boston, MA 02215 USA.	jharer@draper.com; oozdemir@draper.com; tomo@lightmatter.ai; creale@draper.com; rrussell@draper.com; lkim@draper.com; spchin@cs.bu.edu	Russell, Rebecca/X-2670-2019	Russell, Rebecca/0000-0002-4012-4513; Lazovich, Tomo/0000-0001-8183-2295; Reale, Christopher/0000-0002-3978-9577	Air Force Research Laboratory (AFRL) as part of the DARPA MUSE program	Air Force Research Laboratory (AFRL) as part of the DARPA MUSE program	This project was sponsored by the Air Force Research Laboratory (AFRL) as part of the DARPA MUSE program.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Creswell A, 2018, IEEE SIGNAL PROC MAG, V35, P53, DOI 10.1109/MSP.2017.2765202; Devlin J., 2017, ABS171011054 CORR; Gomez A. N., 2018, INT C LEARN REPR ICL; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, NEURAL INFORMATION P; Gupta R, 2017, AAAI CONF ARTIF INTE, P1345; Harer J.A., 2018, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jang E., 2017, ICLR; Ji JS, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P753, DOI 10.18653/v1/P17-1070; Lample Guillaume, 2018, ICLR; LaToza T. D., 2006, INT C SOFTW ENG ICSE; Le X. B. D., 2016, SOFTWARE ANAL EVOLUT; Long F, 2016, ACM SIGPLAN NOTICES, V51, P298, DOI 10.1145/2914770.2837617; Luong Minh-Thang, 2015, EMPIRICAL METHODS NA; Maddison Chris J, 2017, ICLR; Mirza M., 2014, ARXIV; MITRE, COMM VULN EXP; Monperrus M, 2018, ACM COMPUT SURV, V51, DOI 10.1145/3105906; Okun V., 2013, TECHNICAL REPORT; Press O., 2017, 1 WORKSH SUBW CHAR L; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rajeswar S., 2017, 2 WORKSH REPR LEARN; Schmaltz A., 2017, EMPIRICAL METHODS NA; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Xie Ziang, 2016, ARXIV160309727; Yang Z., 2018, N AM CHAPTER ASS COM; Yu L., 2017, ASS ADVANCEMENT ARTI; Yuan Zheng, 2016, N AM CHAPTER ASS COM; Zhang YC, 2017, PR MACH LEARN RES, V70; Zhu Jun-Yan, 2017, ICCV	36	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002048
C	Kallus, N; Zhou, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kallus, Nathan; Zhou, Angela			Confounding-Robust Policy Improvement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of learning personalized decision policies from observational data while accounting for possible unobserved confounding in the data-generating process. Unlike previous approaches that assume unconfoundedness, i.e., no unobserved confounders affected both treatment assignment and outcomes, we calibrate policy learning for realistic violations of this unverifiable assumption with uncertainty sets motivated by sensitivity analysis in causal inference. Our framework for confounding-robust policy improvement optimizes the minimax regret of a candidate policy against a baseline or reference "status quo" policy, over an uncertainty set around nominal propensity weights. We prove that if the uncertainty set is well-specified, robust policy learning can do no worse than the baseline, and only improve if the data supports it. We characterize the adversarial subproblem and use efficient algorithmic solutions to optimize over parametrized spaces of decision policies such as logistic treatment assignment. We assess our methods on synthetic data and a large clinical trial, demonstrating that confounded selection can hinder policy learning and lead to unwarranted harm, while our robust approach guarantees safety and focuses on well-evidenced improvement.	[Kallus, Nathan; Zhou, Angela] Cornell Univ, New York, NY 10021 USA; [Kallus, Nathan; Zhou, Angela] Cornell Tech, New York, NY 10044 USA	Cornell University	Kallus, N (corresponding author), Cornell Univ, New York, NY 10021 USA.; Kallus, N (corresponding author), Cornell Tech, New York, NY 10044 USA.	kallus@cornell.edu; az434@cornell.edu			National Science Foundation [1656996]; National Defense Science & Engineering Graduate Fellowship Program	National Science Foundation(National Science Foundation (NSF)); National Defense Science & Engineering Graduate Fellowship Program	This material is based upon work supported by the National Science Foundation under Grant No. 1656996. Angela Zhou is supported through the National Defense Science & Engineering Graduate Fellowship Program.	Aronow P., 2012, BIOMETRIKA; Berge  E., 2002, COCHRANE LIB SYSTEMA; Beygelzimer Alina, 2009, P 15 ACM SIGKDD INT; Bottou Leon, 2013, J MACHINE LEARNING R; Charnes A., 1962, NAVAL RES LOGISTICS; Dudik M., 2014, STAT SCI; Fogarty  C., 2017, EXTENDED SENSITIVITY; Hasegawa  R., 2017, BIOMETRICS; Hsu JY, 2013, BIOMETRICS, V69, P803, DOI 10.1111/biom.12101; I. S. T. C. Group, 1997, LANCET; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Kallus N., 2017, P 34 INT C MACH LEAR; Kitagawa T., 2015, EMPIRICAL WELFARE MA; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Li Lihong, 2011, P 4 ACM INT C WEB SE; Lipsitch  M., 2010, EPIDEMIOLOGY; Manski C., 2005, ECONOMETRIC I LECT; Masten M., 2018, ECONOMETRICA; Miratrix L. W., 2018, BIOMETRIKA; Petrik  M., 2016, 29 C NEUR INF PROC S; Qian M, 2011, ANN STAT, V39, P1180, DOI 10.1214/10-AOS864; Rosenbaum P. R., 1983, BIOMETRIKA; Rosenbaum PR, 2002, SPRINGER SERIES STAT; Rubin DB, 1974, J ED PSYCHOL; Still G., 2018, LECT PARAMETRIC OPTI; Swaminathan A., 2015, P NIPS; Swaminathan A., 2015, J MACHINE LEARNING R; Thomas P. S., 2015, P 32 INT C MACH LEAR; Wager S., 2017, EFFICIENT POLICY LEA; Wager Stefan, 2017, J AM STAT ASS; Wang Y., 2017, P NEUR INF PROC SYST; Zhang JZ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1340; Zhao Q., 2017, SENSITIVITY ANAL INV	34	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003079
C	Khadka, S; Tumer, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Khadka, Shauharda; Tumer, Kagan			Evolution-Guided Policy Gradient in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL-NETWORKS; NEUROEVOLUTION	Deep Reinforcement Learning (DRL) algorithms have been successfully applied to a range of challenging control tasks. However, these methods typically suffer from three core difficulties: temporal credit assignment with sparse rewards, lack of effective exploration, and brittle convergence properties that are extremely sensitive to hyperparameters. Collectively, these challenges severely limit the applicability of these approaches to real-world problems. Evolutionary Algorithms (EAs), a class of black box optimization techniques inspired by natural evolution, are well suited to address each of these three challenges. However, EAs typically suffer from high sample complexity and struggle to solve problems that require optimization of a large number of parameters. In this paper, we introduce Evolutionary Reinforcement Learning (ERL), a hybrid algorithm that leverages the population of an EA to provide diversified data to train an RL agent, and reinserts the RL agent into the EA population periodically to inject gradient information into the EA. ERL inherits EA's ability of temporal credit assignment with a fitness metric, effective exploration with a diverse set of policies, and stability of a population-based approach and complements it with off-policy DRL's ability to leverage gradients for higher sample efficiency and faster learning. Experiments in a range of challenging continuous control benchmarks demonstrate that ERL significantly outperforms prior DRL and EA methods.	[Khadka, Shauharda; Tumer, Kagan] Oregon State Univ, Collaborat Robot & Intelligent Syst Inst, Corvallis, OR 97331 USA	Oregon State University	Khadka, S (corresponding author), Oregon State Univ, Collaborat Robot & Intelligent Syst Inst, Corvallis, OR 97331 USA.	khadkas@oregonstate.edu; kagan.tumer@oregonstate.edu						Ackley D., 1991, SFI STUDIES SCI COMP, V10, P487; Ahn CW, 2003, IEEE T EVOLUT COMPUT, V7, P367, DOI 10.1109/TEVC.2003.814633; [Anonymous], 2018, ARXIV180201561; [Anonymous], 2016, ARXIV161101224; [Anonymous], 2017, CORR; Bellemare M., 2016, NEURIPS; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Brockman G., 2016, OPENAI GYM; Colas C, 2018, ARXIV180205054; Conti E., 2017, ARXIV171206560; Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422; DeAsis K., 2017, ARXIV170301327; Donahue J., 2017, ARXIV171109846; Drugan MM, 2018, SWARM EVOLUTIONARY C; Duan Y, 2016, INT C MACH LEARN, P1329; EYSENBACH B, 2018, 6 INT C LEARN REPR I, P1; Fernando C, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P109, DOI 10.1145/2908812.2908890; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Fogel DB, 2006, EVOLUTIONARY COMPUTATION: TOWARD A NEW PHILOSOPHY OF MACHINE INTELLIGENCE, 3RD EDITION, P1, DOI 10.1002/0471749214; Fortunato M., 2017, NOISY NETWORKS EXPLO; Fujimoto S., 2018, ARXIV180209477; Gangwani Tanmay, 2017, ARXIV171101012; Gu, 2017, ADV NEURAL INFORM PR, V30, P3846; Haarnoja T., 2018, P 35 INT C MACH LEAR; Harutyunyan A., 2016, ALG LEARN THEOR 27 I, P305; Henderson P., 2017, ABS170906560 CORR; Hesse C., 2017, OPENAI BASELINES; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Islam R., 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Lehman J., 2008, ALIFE, P329; Lillicrap TP, 2016, 4 INT C LEARN REPR; Liu H., 2017, ARXIV171100436; Luders B, 2017, LECT NOTES COMPUT SC, V10199, P886, DOI 10.1007/978-3-319-55849-3_57; Mahmood A. R., 2017, ARXIV170203006; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ostrovski G., 2017, ARXIV170301310; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Plappert Matthias, 2017, ARXIV170601905; Pugh JK, 2016, FRONT ROBOT AI, V3, DOI 10.3389/frobt.2016.00040; Risi S, 2017, IEEE T COMP INTEL AI, V9, P25, DOI 10.1109/TCIAIG.2015.2494596; Salimans T., 2017, ARXIV170303864; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Sherstan Craig, 2018, ARXIV180108287; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Spears W. M., 1993, Machine Learning: ECML-93. European Conference on Machine Learning Proceedings, P442; Stafylopatis A, 1998, EUR J OPER RES, V108, P306, DOI 10.1016/S0377-2217(97)00372-X; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tang H., 2017, NEURIPS; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Turney P, 1996, EVOL COMPUT, V4, pIV, DOI 10.1162/evco.1996.4.3.iv; Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823; Whiteson S, 2006, J MACH LEARN RES, V7, P877	60	6	6	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301020
C	Kroer, C; Farina, G; Sandholm, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kroer, Christian; Farina, Gabriele; Sandholm, Tuomas			Solving Large Sequential Games with the Excessive Gap Technique	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					There has been tremendous recent progress on equilibrium-finding algorithms for zero-sum imperfect-information extensive-form games, but there has been a puzzling gap between theory and practice. First-order methods have significantly better theoretical convergence rates than any counterfactual-regret minimization (CFR) variant. Despite this, CFR variants have been favored in practice. Experiments with first-order methods have only been conducted on small- and medium-sized games because those methods are complicated to implement in this setting, and because CFR variants have been enhanced extensively for over a decade they perform well in practice. In this paper we show that a particular first-order method, a state-ofthe-art variant of the excessive gap technique-instantiated with the dilated entropy distance function-can efficiently solve large real-world problems competitively with CFR and its variants. We show this on large endgames encountered by the Libratus poker AI, which recently beat top human poker specialist professionals at no-limit Texas hold'em. We show experimental results on our variant of the excessive gap technique as well as a prior version. We introduce a numerically friendly implementation of the smoothed best response computation associated with first-order methods for extensive-form game solving. We present, to our knowledge, the first GPU implementation of a first-order method for extensive-form games. We present comparisons of several excessive gap technique and CFR variants.	[Kroer, Christian; Farina, Gabriele; Sandholm, Tuomas] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Kroer, C (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	ckroer@cs.cmu.edu; gfarina@cs.cmu.edu; sandholm@cs.cmu.edu		Kroer, Christian/0000-0002-9009-8683	National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]; Facebook Fellowship	National Science Foundation(National Science Foundation (NSF)); ARO; Facebook Fellowship(Facebook Inc)	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Christian Kroer is supported by a Facebook Fellowship.	[Anonymous], 2011, IJCAI, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-054; Bowling M, 2015, SCIENCE, V347, P145, DOI 10.1126/science.1259433; Brown N., 2017, SCIENCE; Brown N., 2015, INT C AUT AG MULT SY; Brown N., 2017, ADV NEURAL INFORM PR, P689; Brown N, 2017, PR MACH LEARN RES, V70; Brown N, 2017, AAAI CONF ARTIF INTE, P421; Cermak J, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1813; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Farina G, 2017, PR MACH LEARN RES, V70; Gilpin A, 2007, J ACM, V54, DOI 10.1145/1284320.1284324; Hoda S, 2010, MATH OPER RES, V35, P494, DOI 10.1287/moor.1100.0452; Johanson M., 2013, CORR; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; Koller D, 1997, ARTIF INTELL, V94, P167, DOI 10.1016/S0004-3702(97)00023-4; Koller D, 1996, GAME ECON BEHAV, V14, P247, DOI 10.1006/game.1996.0051; Kroer C., 2017, ARXIV170204849; Kroer C., 2015, P ACM C EC COMP EC; Kroer C, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P295; Lanctot M., 2009, ADV NEURAL INFORM PR, P1078; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y. E., 2005, MATH PROGRAMMING, V103; Romanovskii I., 1962, SOVIET MATH, V3; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; vonStengel B, 1996, GAME ECON BEHAV, V14, P220, DOI 10.1006/game.1996.0050; Zinkevich M., 2007, ADV NEURAL INFORM PR, V7, P1729	29	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300080
C	Lacombe, T; Cuturi, M; Oudot, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lacombe, Theo; Cuturi, Marco; Oudot, Steve			Large Scale computation of Means and Clusters for Persistence Diagrams using Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BARYCENTERS	Persistence diagrams (PDs) are now routinely used to summarize the underlying topology of complex data. Despite several appealing properties, incorporating PDs in learning pipelines can be challenging because their natural geometry is not Hilbertian. Indeed, this was recently exemplified in a string of papers which show that the simple task of averaging a few PDs can be computationally prohibitive. We propose in this article a tractable framework to carry out standard tasks on PDs at scale, notably evaluating distances, estimating barycenters and performing clustering. This framework builds upon a reformulation of PD metrics as optimal transport (OT) problems. Doing so, we can exploit recent computational advances: the OT problem on a planar grid, when regularized with entropy, is convex can be solved in linear time using the Sinkhom algorithm and convolutions. This results in scalable computations that can stream on GPUs. We demonstrate the efficiency of our approach by carrying out clustering with diagrams metrics on several thousands of PDs, a scale never seen before in the literature.	[Lacombe, Theo; Oudot, Steve] Inria Saclay, Datashape, Palaiseau, France; [Cuturi, Marco] Google Brain, Mountain View, CA USA; [Cuturi, Marco] ENSAE, CREST, Palaiseau, France	Google Incorporated; Institut Polytechnique de Paris	Lacombe, T (corresponding author), Inria Saclay, Datashape, Palaiseau, France.	theo.lacombe@inria.fr; cuturi@google.com; steve.oudot@inria.fr			AMX; Ecole polytechnique; Chaire d'Excellence de l'Idex Paris-Saclay	AMX; Ecole polytechnique; Chaire d'Excellence de l'Idex Paris-Saclay	We thank the anonymous reviewers for the fruitful discussion. TL was supported by the AMX, Ecole polytechnique. MC acknowledges the support of a Chaire d'Excellence de l'Idex Paris-Saclay.	Adams H, 2017, J MACH LEARN RES, V18; Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Anderes E, 2016, MATH METHOD OPER RES, V84, P389, DOI 10.1007/s00186-016-0549-x; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bubenik P, 2015, J MACH LEARN RES, V16, P77; Carlier G, 2015, ESAIM-MATH MODEL NUM, V49, P1621, DOI 10.1051/m2an/2015033; Carriere M., 2017, 34 INT C MACH LEARN; Carriere M, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12692; Chazal F., 2014, IEEE C COMP VIS PATT, V2014, P1995; Chazal F, 2014, GEOMETRIAE DEDICATA, V173, P193, DOI 10.1007/s10711-013-9937-z; Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Dvurechensky P, 2018, PR MACH LEARN RES, V80; Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454; Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO; Frechet M., 1948, ANN I H POINCARE, V10, P215; GUITTET K, 2002, THESIS; Hiraoka Y, 2016, P NATL ACAD SCI USA, V113, P7035, DOI 10.1073/pnas.1520877113; Kerber M., 2017, ACM J EXP ALGORITHMI, V22, DOI 10.1145/3064175; Lucet Y, 2010, SIAM REV, V52, P505, DOI 10.1137/100788458; Lum PY, 2013, SCI REP-UK, V3, DOI 10.1038/srep01236; Makarenko N, 2016, OPEN ENG, V6, P326, DOI 10.1515/eng-2016-0044; Mileyko Y, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/12/124007; Nicolau M, 2011, P NATL ACAD SCI USA, V108, P7265, DOI 10.1073/pnas.1102826108; Obayashi I., 2018, J APPL COMPUT TOPOL, V1, P421, DOI [10.1007/s41468-018-0013-5, DOI 10.1007/S41468-018-0013-5]; Peyre G., 2018, COMPUTATIONAL OPTIMA; Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106; Schrijver A., 1998, THEORY LINEAR INTEGE; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Sumner RW, 2004, ACM T GRAPHIC, V23, P399, DOI 10.1145/1015706.1015736; Turner  K., 2013, ARXIV13078300; Turner K, 2014, DISCRETE COMPUT GEOM, V52, P44, DOI 10.1007/s00454-014-9604-7; Villani C., 2003, TOPICS OPTIMAL TRANS; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Zeppelzauer M, 2016, LECT NOTES COMPUT SC, V9667, P77, DOI 10.1007/978-3-319-39441-1_8; Zomorodian A, 2005, DISCRETE COMPUT GEOM, V33, P249, DOI 10.1007/s00454-004-1146-y	42	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004034
C	Laha, A; Chemmengath, SA; Agrawal, P; Khapra, MM; Sankaranarayanan, K; Ramaswamy, HG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Laha, Anirban; Chemmengath, Saneem A.; Agrawal, Priyanka; Khapra, Mitesh M.; Sankaranarayanan, Karthik; Ramaswamy, Harish G.			On Controllable Sparse Alternatives to Softmax	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Converting an n-dimensional vector to a probability distribution over n objects is a commonly used component in many machine learning tasks like multiclass classification, multilabel classification, attention mechanisms etc. For this, several probability mapping functions have been proposed and employed in literature such as softmax, sum-normalization, spherical softmax, and sparsemax, but there is very little understanding in terms how they relate with each other. Further, none of the above formulations offer an explicit control over the degree of sparsity. To address this, we develop a unified framework that encompasses all these formulations as special cases. This framework ensures simple closed-form solutions and existence of sub-gradients suitable for learning via backpropagation. Within this framework, we propose two novel sparse formulations, sparsegen-lin and sparsehourglass, that seek to provide a control over the degree of desired sparsity. We further develop novel convex loss functions that help induce the behavior of aforementioned formulations in the multilabel classification setting, showing improved performance. We also demonstrate empirically that the proposed formulations, when used to compute attention weights, achieve better or comparable performance on standard seq2seq tasks like neural machine translation and abstractive summarization.	[Laha, Anirban; Chemmengath, Saneem A.; Agrawal, Priyanka; Sankaranarayanan, Karthik] IBM Res, Yorktown Hts, NY 10598 USA; [Khapra, Mitesh M.; Ramaswamy, Harish G.] IIT Madras, Robert Bosch Ctr DS & AI, Chennai, Tamil Nadu, India; [Khapra, Mitesh M.; Ramaswamy, Harish G.] IIT Madras, Dept CSE, Chennai, Tamil Nadu, India	International Business Machines (IBM); Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Laha, A; Chemmengath, SA (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	anirlaha@in.ibm.com; saneem.cg@in.ibm.com						Aly M., 2005, NEURAL NETWORKS, V19, P1; Bach Francis, 2011, FDN TRENDS R MACHINE; Bahdanau D., 2015, INT C LEARN REPR ICL; Bridle John S, 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28; Cho K, 2015, IEEE T MULTIMEDIA, V17, P1875, DOI 10.1109/TMM.2015.2477044; Duchi J., 2008, PROC 25 INT C MACH L, P272; Gao B., 2017, ARXIV E PRINTS; Kapoor A., 2012, ADV NEURAL INFORM PR, P2645; Klein Guillaume, 2017, CORR; Martins AFT, 2016, PR MACH LEARN RES, V48; Niculae Vlad, 2017, NIPS; Rush Alexander M, 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044; Sorower MS., 2010, LIT SURVEY ALGORITHM, V18, P1; Sukhbaatar S., 2015, P 28 INT C NEURAL IN, V28, P2440; Sutton R. S., 1998, REINFORCEMENT LEARNI, V1; Vincent Pascal, 2016, P INT C LEARN REPR I; Vincent Pascal, 2015, ADV NEURAL INFORM PR, P1108; Xu K, 2015, PR MACH LEARN RES, V37, P2048	19	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000088
C	Lin, C; Zhong, Z; Wu, W; Yan, JJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lin, Chen; Zhong, Zhao; Wu, Wei; Yan, Junjie			Synaptic Strength For Convolutional Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Convolutional Neural Networks(CNNs) are both computation and memory intensive which hindered their deployment in mobile devices. Inspired by the relevant concept in neural science literature, we propose Synaptic Pruning: a data-driven method to prune connections between input and output feature maps with a newly proposed class of parameters called Synaptic Strength. Synaptic Strength is designed to capture the importance of a connection based on the amount of information it transports. Experiment results show the effectiveness of our approach. On CIFAR-10, we prune connections for various CNN models with up to 96%, which results in significant size reduction and computation saving. Further evaluation on ImageNet demonstrates that synaptic pruning is able to discover efficient models which is competitive to state-of-the-art compact CNNs such as MobileNet-V2 and NasNet-Mobile. Our contribution is summarized as following: (1) We introduce Synaptic Strength, a new class of parameters for CNNs to indicate the importance of each connections. (2) Our approach can prune various CNNs with high compression without compromising accuracy. (3) Further investigation shows, the proposed Synaptic Strength is a better indicator for kernel pruning compared with the previous approach in both empirical result and theoretical analysis.	[Lin, Chen; Zhong, Zhao; Wu, Wei; Yan, Junjie] SenseTime Res, Beijing, Peoples R China; [Zhong, Zhao] Univ Chinese Acad Sci, CASIA, NLPR, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; University of Chinese Academy of Sciences, CAS	Lin, C (corresponding author), SenseTime Res, Beijing, Peoples R China.	linchen@sensetime.com; zhao.zhong@nlpr.ia.ac.cn; wuwei@sensetime.com; yanjunjie@sensetime.com						Aila T., 2016, ARXIV PREPRINT ARXIV; Anwar S., 2016, ARXIV PREPRINT ARXIV; Bruna J., 2014, ADV NEURAL INFORM PR; Chechik G, 1998, NEURAL COMPUT, V10, P1759, DOI 10.1162/089976698300017124; Chetlur S., 2014, ARXIV; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Craik FIM, 2006, TRENDS COGN SCI, V10, P131, DOI 10.1016/j.tics.2006.01.007; Denker John S, 1990, OPTIMAL BRAIN DAMAGE, P598; Gray Scott, 2017, ARXIV171109224; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hinton G., 2015, ARXIV150302531; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang  Gao, 2017, ARXIV171109224; Huang Zehao, 2017, ARXIV170701213; Lavin Andrew, 2016, FAST ALGORITHMS CONV; Liu Xingyu, 2018, INT C LEARN REPR; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Louizos Christos, 2018, INT C LEARN REPR; Mao Huizi, 2017, EXPLORING GRANULARIT; Park J., 2017, ABS170208597 CORR; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Romero Adriana, 2014, ARXIV14126550; Sandler Mark, 2018, ARXIV180104381, DOI DOI 10.1109/CVPR.2018.00474; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Vanderhaeghen P, 2010, CSH PERSPECT BIOL, V2, DOI 10.1101/cshperspect.a001859; Vedaldi A., 2014, BMVC; Wen W, 2016, ADV NEUR IN, V29; Wu Jianxin, 2017, ARXIV170706342; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zhang Ting, 2017, CORR; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou A, 2017, INCREMENTAL NETWORK; Zoph B., 2016, ARXIV161101578	39	6	7	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004068
C	Mangoubi, O; Vishnoi, NK		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mangoubi, Oren; Vishnoi, Nisheeth K.			Dimensionally Tight Bounds for Second-Order Hamiltonian Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				LOGISTIC-REGRESSION; DIFFUSION LIMITS; ALGORITHM	Hamiltonian Monte Carlo (HMC) is a widely deployed method to sample from high-dimensional distributions in Statistics and Machine learning. HMC is known to run very efficiently in practice and its popular second-order "leapfrog" implementation has long been conjectured to run in d(1/4) gradient evaluations. Here we show that this conjecture is true when sampling from strongly log-concave target distributions that satisfy a weak third-order regularity property associated with the input data. Our regularity condition is weaker than the Lipschitz Hessian property and allows us to show faster convergence bounds for a much larger class of distributions than would be possible with the usual Lipschitz Hessian constant alone. Important distributions that satisfy our regularity condition include posterior distributions used in Bayesian logistic regression for which the data satisfies an "incoherence" property. Our result compares favorably with the best available bounds for the class of strongly log-concave distributions, which grow like d(1/2) gradient evaluations with the dimension. Moreover, our simulations on synthetic data suggest that, when our regularity condition is satisfied, leapfrog HMC performs better than its competitors - both in terms of accuracy and in terms of the number of gradient evaluations it requires.	[Mangoubi, Oren; Vishnoi, Nisheeth K.] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Mangoubi, O (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	omangoubi@gmail.com; nisheeth.vishnoi@gmail.com						[Anonymous], 1992, CRGTR921 U TOR DEP C; [Anonymous], 2004, SIMULATING HAMILTONI; Beskos A, 2013, BERNOULLI, V19, P1501, DOI 10.3150/12-BEJ414; Betancourt M., 2017, ARXIV170102434; Betancourt  MJ, 2014, ARXIV14116669; Bou-Rabee N., 2018, ARXIV180500452; Carpenter B., 2017, J STAT SOFTW, DOI [10.18637/jss.v076.i01, DOI 10.18637/JSS.V076.I01]; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Cheng X., 2018, C LEARNING THEORY, P300; Cheng X., 2018, P 29 INT C ALG LEARN, P186; Cheng  Xiang, 2018, ARXIV180501648; Chopin N, 2017, STAT SCI, V32, P64, DOI 10.1214/16-STS581; CREUTZ M, 1988, PHYS REV D, V38, P1228, DOI 10.1103/PhysRevD.38.1228; Dalalyan A. S., 2017, ARXIV170404752; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Durmus A., 2016, ARXIV160501559; Durmus A., 2017, ARXIV170500166; Durmus  Alain, 2018, ARXIV180209188; Durmus  Alain, ANN APPL PROBABILITY; Dwivedi R., 2018, C LEARNING THEORY PM, P793; Faes C, 2011, J AM STAT ASSOC, V106, P959, DOI 10.1198/jasa.2011.tm10301; Genkin A, 2007, TECHNOMETRICS, V49, P291, DOI 10.1198/004017007000000245; Goodman J, 2010, COMM APP MATH COM SC, V5, P65, DOI 10.2140/camcos.2010.5.65; Hairer E., 2003, Acta Numerica, V12, P399, DOI 10.1017/S0962492902000144; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Johnson VE, 2013, BAYESIAN ANAL, V8, P741, DOI 10.1214/13-BA818; Kennedy A. D., 1991, Nuclear Physics B, Proceedings Supplements, V20, P118, DOI 10.1016/0920-5632(91)90893-J; Lee Yin Tat, 2017, P STOC 2018; Livingstone S., 2016, ARXIV160108057; Lovasz L, 2003, PREPRINT; Madigan D, 2005, AIP CONF PROC, V803, P509, DOI 10.1063/1.2149832; Mangoubi O., 2017, ARXIV170807114; Mangoubi Oren, 2018, ARXIV180208898; Mattingly JC, 2012, ANN APPL PROBAB, V22, P881, DOI 10.1214/10-AAP754; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Pillai NS, 2012, ANN APPL PROBAB, V22, P2320, DOI 10.1214/11-AAP828; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Roberts GO, 1997, ANN APPL PROBAB, V7, P110, DOI 10.1214/aoap/1034625254; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Seiler Christof, 2014, P 27 ADV NEUR INF P, P586; Vempala  Santosh, 2005, COMBINATORIAL COMPUT, V52, P2; Zhang T, 2001, INFORM RETRIEVAL, V4, P5, DOI 10.1023/A:1011441423217	44	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000052
C	Mao, XY; Sarkar, P; Chakrabarti, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mao, Xueyu; Sarkar, Purnamrita; Chakrabarti, Deepayan			Overlapping Clustering Models, and One (class) SVM to Bind Them All	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					People belong to multiple communities, words belong to multiple topics, and books cover multiple genres; overlapping clusters are commonplace. Many existing overlapping clustering methods model each person (or word, or book) as a non-negative weighted combination of "exemplars" who belong solely to one community, with some small noise. Geometrically, each person is a point on a cone whose corners are these exemplars. This basic form encompasses the widely used Mixed Membership Stochastic Blockmodel of networks [1] and its degree-corrected variants [16], as well as topic models such as LDA [9]. We show that a simple one-class SVM yields provably consistent parameter inference for all such models, and scales to large datasets. Experimental results on several simulated and real datasets show our algorithm (called SVM-cone) is both accurate and scalable.	[Mao, Xueyu; Sarkar, Purnamrita; Chakrabarti, Deepayan] Univ Texas Austin, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Mao, XY (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	xmao@cs.utexas.edu; purna.sarkar@austin.utexas.edu; deepay@utexas.edu	Mao, Xueyu/AAQ-5247-2021	Mao, Xueyu/0000-0003-0076-7431; Chakrabarti, Deepayan/0000-0002-3863-4928	NSF [DMS 1713082]; Facebook Faculty Research Award	NSF(National Science Foundation (NSF)); Facebook Faculty Research Award(Facebook Inc)	X.M. and P.S. were partially supported by NSF grant DMS 1713082. D.C. was partially supported by a Facebook Faculty Research Award.	Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Anandkumar A, 2014, J MACH LEARN RES, V15, P2239; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Awasthi P, 2018, PR MACH LEARN RES, V84; Bansal T., 2014, ADV NEURAL INFORM PR, P1997; Bing X., 2018, ARXIV180506837; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Ding W., 2013, P 30 INT C MACH LEAR; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Hopkins SB, 2017, ANN IEEE SYMP FOUND, P379, DOI 10.1109/FOCS.2017.42; Huang K., 2016, ADV NEURAL INFORM PR, P1786; Jin J., 2017, ARXIV170807852; Kaufmann E, 2016, LECT NOTES ARTIF INT, V9925, P355, DOI 10.1007/978-3-319-46379-7_24; Ke Z. T., 2017, ARXIV170407016; Latouche P, 2011, ANN APPL STAT, V5, P309, DOI 10.1214/10-AOAS382; Mao X., 2017, ARXIV170900407; Mao XY, 2017, PR MACH LEARN RES, V70; Ng AY, 2002, ADV NEUR IN, V14, P849; Panov M., 2017, INT C COMPLEX NETWOR, P53; Psorakis I, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.066114; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Ray A, 2014, ANN ALLERTON CONF, P278, DOI 10.1109/ALLERTON.2014.7028467; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Rubin-Delanchy P., 2017, ARXIV PREPRINT ARXIV; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Yurochkin M., 2016, ADV NEURAL INFORM PR, P2505; Zhang Y., 2014, ARXIV14123432	32	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302016
C	Morcos, AS; Raghu, M; Bengio, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Morcos, Ari S.; Raghu, Maithra; Bengio, Samy			Insights on representational similarity in neural networks with canonical correlation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Comparing different neural network representations and determining how representations evolve over time remain challenging open questions in our understanding of the function of neural networks. Comparing representations in neural networks is fundamentally difficult as the structure of representations varies greatly, even across groups of networks trained on identical tasks, and over the course of training. Here, we develop projection weighted CCA (Canonical Correlation Analysis) as a tool for understanding neural networks, building off of SVCCA, a recently proposed method [22]. We first improve the core method, showing how to differentiate between signal and noise, and then apply this technique to compare across a group of CNNs, demonstrating that networks which generalize converge to more similar representations than networks which memorize, that wider networks converge to more similar solutions than narrow networks, and that trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations. We also investigate the representational dynamics of RNNs, across both training and sequential timesteps, finding that RNNs converge in a bottom-up pattern over the course of training and that the hidden state is highly variable over the course of a sequence, even when accounting for linear transforms. Together, these results provide new insights into the function of CNNs and RNNs, and demonstrate the utility of using CCA to understand representations.	[Morcos, Ari S.] DeepMind, London, England; [Raghu, Maithra; Bengio, Samy] Google Brain, Mountain View, CA 14853 USA; [Raghu, Maithra] Cornell Univ, Ithaca, NY 14853 USA; [Morcos, Ari S.] Facebook Res FAIR, Menlo Pk, CA 94025 USA	Google Incorporated; Cornell University; Facebook Inc	Morcos, AS (corresponding author), DeepMind, London, England.; Raghu, M (corresponding author), Google Brain, Mountain View, CA 14853 USA.; Raghu, M (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.; Morcos, AS (corresponding author), Facebook Res FAIR, Menlo Pk, CA 94025 USA.	arimorcos@gmail.com; maithrar@gmail.com; bengio@google.com						[Anonymous], DEEP LEARN WORKSH IN; Anwar S, 2017, ACM J EMERG TECH COM, V13, DOI 10.1145/3005348; Arpit Devansh, 2017, P 34 INT C MACH LEAR; Bartlett MS, 1941, BIOMETRIKA, V32, P29; Bau David, 2017, COMPUTER VISION PATT; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; de G Matthews A.G., 2018, INT C LEARN REPR ICL; Faruqui  M., 2014, ASS COMPUTATIONAL LI; Figurnov Mikhail, 2016, NEURIPS; Frankle J., 2018, ABS180303635 CORR; Han S., 2015, CORR; Han S., 2015, P 4 INT C LEARN REPR; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Karpathy Andrej, 2016, INT C LEARN REPR WOR; Lee J, 2018, INT C LEARN REPR ICL; Li Chunyuan, 2018, INT C LEARN REPR APR; Li H., 2017, P INT C LEARNING REP; Li Y., 2015, FE NIPS, P196; Merity Stephen, 2017, ICLR; Merity Stephen, 2018, ARXIV180308240; Molchanov P., 2016, INT C LEARN REPR ICL; Morcos Ari S., 2018, INT C LEARN REPR ICL; Raghu Maithra, 2017, ADV NEURAL INFORM PR; Sussillo D, 2015, NAT NEUROSCI, V18, P1025, DOI 10.1038/nn.4042; Uurtio V, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3136624; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang C., 2016, INT C LEARN REPR ICL	27	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000025
C	Neumann, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Neumann, Stefan			Bipartite Stochastic Block Models with Tiny Clusters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of finding clusters in random bipartite graphs. We present a simple two-step algorithm which provably finds even tiny clusters of size O (n(epsilon)), where n is the number of vertices in the graph and epsilon > 0. Previous algorithms were only able to identify clusters of size Omega(root n). We evaluate the algorithm on synthetic and on real-world data; the experiments show that the algorithm can find extremely small clusters even in presence of high destructive noise.	[Neumann, Stefan] Univ Vienna, Fac Comp Sci, Vienna, Austria	University of Vienna	Neumann, S (corresponding author), Univ Vienna, Fac Comp Sci, Vienna, Austria.	stefan.neumann@univie.ac.at			Doctoral Programme "Vienna Graduate School on Computational Optimization" - Austrian Science Fund (FWF) [W1260-N35]	Doctoral Programme "Vienna Graduate School on Computational Optimization" - Austrian Science Fund (FWF)(Austrian Science Fund (FWF))	The author gratefully acknowledges the financial support from the Doctoral Programme "Vienna Graduate School on Computational Optimization" which is funded by the Austrian Science Fund (FWF, project no. W1260-N35).	Abbe E., 2017, CORR; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Baigneres T, 2004, LECT NOTES COMPUT SC, V3329, P432; Ban Frank, 2018, CORR; Bilu Yonatan, 2010, INNOVATIONS COMPUTER, P332; Chandran L. S., 2016, IPEC, V11, P1; Cohen-Addad V, 2017, ANN IEEE SYMP FOUND, P49, DOI 10.1109/FOCS.2017.14; Dasgupta A, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1036; Eren K, 2013, BRIEF BIOINFORM, V14, P279, DOI 10.1093/bib/bbs032; Florescu L., 2016, C LEARN THEOR, P943; Fomin Fedor V., 2018, CORR; Fortelius M., 2003, NEW OLD WORLDS DATAB; Gao C, 2017, J MACH LEARN RES, V18, P1; Hajek B., 2015, C LEARNING THEORY, P899; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; Lim SH, 2015, PR MACH LEARN RES, V37, P1679; Liu YT, 2014, 2014 INTERNATIONAL CONFERENCE ON MECHATRONICS AND CONTROL (ICMC), P21, DOI 10.1109/ICMC.2014.7231508; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; MIETTINEN P, 2014, TKDD, V8; Miettinen P., 2009, THESIS; Miettinen P, 2008, IEEE T KNOWL DATA EN, V20, P1348, DOI 10.1109/TKDE.2008.53; Mitra Pradipta, SIMPLE ALGORITHM CLU; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Ravanbakhsh S, 2016, PR MACH LEARN RES, V48; Rukat T., 2018, ICML, P4410; Rukat T, 2017, PR MACH LEARN RES, V70; Zhou Zongwei, 2018, Deep Learn Med Image Anal Multimodal Learn Clin Decis Support (2018), V11045, P3, DOI 10.1007/978-3-030-00889-5_1	29	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303083
C	Richardson, E; Weiss, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Richardson, Eitan; Weiss, Yair			On GANs and GMMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A longstanding problem in machine learning is to find unsupervised methods that can learn the statistical structure of high dimensional signals. In recent years, GANs have gained much attention as a possible solution to the problem, and in particular have shown the ability to generate remarkably realistic high resolution sampled images. At the same time, many authors have pointed out that GANs may fail to model the full distribution ("mode collapse") and that using the learned models for anything other than generating samples may be very difficult. In this paper, we examine the utility of GANs in learning statistical models of images by comparing them to perhaps the simplest statistical model, the Gaussian Mixture Model. First, we present a simple method to evaluate generative models based on relative proportions of samples that fall into predetermined bins. Unlike previous automatic methods for evaluating models, our method does not rely on an additional neural network nor does it require approximating intractable computations. Second, we compare the performance of GANs to GMMs trained on the same datasets. While GMMs have previously been shown to be successful in modeling small patches of images, we show how to train them on full sized images despite the high dimensionality. Our results show that GMMs can generate realistic samples (although less sharp than those of GANs) but also capture the full distribution, which GANs fail to do. Furthermore, GMMs allow efficient inference and explicit representation of the underlying statistical structure. Finally, we discuss how GMMs can be used to generate sharp images.(1)	[Richardson, Eitan; Weiss, Yair] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel	Hebrew University of Jerusalem	Richardson, E (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.	eitanrich@cs.huji.ac.il; yweiss@cs.huji.ac.il			Israeli Science Foundation; Gatsby Foundation	Israeli Science Foundation(Israel Science Foundation); Gatsby Foundation	Supported by the Israeli Science Foundation and the Gatsby Foundation.	Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], 2018, BIOSTATISTICS FDN AN; Arjovsky M, 2017, PR MACH LEARN RES, V70; Arora S., 2018, P INT C LEARN REPR; Bell AJ, 1997, ADV NEUR IN, V9, P831; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Dayan P., 2017, ARXIV PREPRINT ARXIV; Dinh L, 2017, 5 INT C LEARN REPR I; Fedus William, 2018, INT C LEARN REPR; Ghahramani Zoubin, 1996, CRGTR961 U TOR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Hou XX, 2017, IEEE WINT CONF APPL, P1133, DOI 10.1109/WACV.2017.131; Im DJ, 2018, INT C LEARN REPR; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kingma D.P, P 3 INT C LEARNING R; Knott Martin, 1999, LATENT VARIABLE MODE, V7; Kolesnikov A, 2017, PR MACH LEARN RES, V70; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lucic Mario, 2018, ADV NEURAL INFORM PR; Metz Luke, 2017, ICLR; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Odena A, 2017, PR MACH LEARN RES, V70; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Uria Benigno, 2013, P 26 INT C NEURAL IN; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Wu Yuhuai, 2017, QUANTITATIVE ANAL DE; Zoran D, 2011, IEEE I CONF COMP VIS, P479, DOI 10.1109/ICCV.2011.6126278	41	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000036
C	Sajjadi, MSM; Bachem, O; Lucic, M; Bousquet, O; Gelly, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sajjadi, Mehdi S. M.; Bachem, Olivier; Lucic, Mario; Bousquet, Olivier; Gelly, Sylvain			Assessing Generative Models via Precision and Recall	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent advances in generative modeling have led to an increased interest in the study of statistical divergences as means of model comparison. Commonly used evaluation methods, such as the Frechet Inception Distance (FID), correlate well with the perceived quality of samples and are sensitive to mode dropping. However, these metrics are unable to distinguish between different failure cases since they only yield one-dimensional scores. We propose a novel definition of precision and recall for distributions which disentangles the divergence into two separate dimensions. The proposed notion is intuitive, retains desirable properties, and naturally leads to an efficient algorithm that can be used to evaluate generative models. We relate this notion to total variation as well as to recent evaluation metrics such as Inception Score and FID. To demonstrate the practical utility of the proposed approach we perform an empirical study on several variants of Generative Adversarial Networks and Variational Autoencoders. In an extensive set of experiments we show that the proposed metric is able to disentangle the quality of generated samples from the coverage of the target distribution.	[Sajjadi, Mehdi S. M.] Max Planck ETH Ctr Learning Syst, MPI Intelligent Syst, Tubingen, Germany; [Bachem, Olivier; Lucic, Mario; Bousquet, Olivier; Gelly, Sylvain] Google Brain, Mountain View, CA 94043 USA	Max Planck Society; Google Incorporated	Bachem, O; Lucic, M (corresponding author), Google Brain, Mountain View, CA 94043 USA.	bachem@google.com; lucic@google.com						Bachem O, 2016, AAAI; Bachem Olivier, 2016, ADV NEURAL INFORM PR; Barratt S., 2018, ARXIV180101973; Binkowski Mikolaj, 2018, INT C LEARN REPR IC; Borji Ali, 2018, PROS CONS GAN EVALUA; Cifka Ondrej, 2018, ARXIV180407972; Conneau A, 2017, EMNLP, DOI 10.18653/v1/D17-1070; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heusel M, 2017, ADV NEURAL INFORM PR; Huszar Ferenc, 2015, ABS151105101 CORR; Im Daniel Jiwoong, 2018, INT C LEARN REPR ICL; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurach K., 2018, ARXIV180704720; LeCun Y., 1998, IEEE; Liu Ziwei, 2015, P INT C COMP VIS ICC; Lopez-Paz David, 2016, INT C LEARN REPR REP; Lucic Mario, 2018, ADV NEURAL INFORM PR; Odena A., 2016, DISTILL, V1, pe3, DOI [10.23915/distill.00003, DOI 10.23915/DISTILL.00003]; Salimans T, 2016, ADV NEURAL INFORM PR; Sculley David, 2010, INT C WORLD WID WEB; Theis Lucas, 2016, INT C LEARN REPR ICL; Williams Adina, 2017, ARXIV170405426; Wu Yuxin, 2017, INT C LEARN REPR ICL; Xiao H., 2017, ARXIV 170807747	25	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305026
C	Do, TDT; Cao, LB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Trong Dinh Thac Do; Cao, Longbing			Gamma-Poisson Dynamic Matrix Factorization Embedded with Metadata Influence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					A conjugate Gamma-Poisson model for Dynamic Matrix Factorization incorporated with metadata influence (mGDMF for short) is proposed to effectively and efficiently model massive, sparse and dynamic data in recommendations. Modeling recommendation problems with a massive number of ratings and very sparse or even no ratings on some users/items in a dynamic setting is very demanding and poses critical challenges to well-studied matrix factorization models due to the large-scale, sparse and dynamic nature of the data. Our proposed mGDMF tackles these challenges by introducing three strategies: (1) constructing a stable Gamma-Markov chain model that smoothly drifts over time by combining both static and dynamic latent features of data; (2) incorporating the user/item metadata into the model to tackle sparse ratings; and (3) undertaking stochastic variational inference to efficiently handle massive data. mGDMF is conjugate, dynamic and scalable. Experiments show that mGDMF significantly (both effectively and efficiently) outperforms the state-of-the-art static and dynamic models on large, sparse and dynamic data.	[Trong Dinh Thac Do; Cao, Longbing] Univ Technol Sydney, Adv Analyt Inst, Sydney, NSW, Australia	University of Technology Sydney	Cao, LB (corresponding author), Univ Technol Sydney, Adv Analyt Inst, Sydney, NSW, Australia.	thacdtd@gmail.com; longbing.cao@gmail.com		cao, longbing/0000-0003-1562-9429				Acharya A, 2015, JMLR WORKSH CONF PRO, V38, P1; Acharya A, 2015, LECT NOTES ARTIF INT, V9284, P283, DOI 10.1007/978-3-319-23528-8_18; Basbug ME, 2016, PR MACH LEARN RES, V48; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Berry MW, 2007, COMPUT STAT DATA AN, V52, P155, DOI 10.1016/j.csda.2006.11.006; Blei D., 2014, P 27 INT C NEURAL IN, P3176; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bottou L, 1998, NEURAL NETWORKS, V17, P142; Canny J., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P122, DOI 10.1145/1008992.1009016; Cao LB, 2016, ENGINEERING, V2, P212, DOI 10.1016/J.ENG.2016.02.013; Cao LB, 2015, INFORM PROCESS MANAG, V51, P167, DOI 10.1016/j.ipm.2014.08.007; Cao LB, 2012, IEEE T KNOWL DATA EN, V24, P1378, DOI 10.1109/TKDE.2011.129; Cemgil AT, 2007, LECT NOTES COMPUT SC, V4666, P697; Cemgil Ali Taylan, 2009, Comput Intell Neurosci, P785152, DOI 10.1155/2009/785152; Chaney A.J.B., 2015, P 9 ACM C REC SYST, P43, DOI DOI 10.1145/2792838.2800193; Charlin L., 2015, RECSYS ACM, P155; Chatzis SP, 2014, AAAI CONF ARTIF INTE, P1731; Chua FCT, 2013, IEEE DATA MINING, P91, DOI 10.1109/ICDM.2013.25; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; Fan XH, 2017, IEEE T CYBERNETICS, V47, P589, DOI 10.1109/TCYB.2016.2521376; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; Gong C., 2017, NIPS, P1665; Gopalan P, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P326; Gopalan P, 2014, JMLR WORKSH CONF PRO, V33, P275; HANLEY JA, 1982, RADIOLOGY, V29, P143, DOI DOI 10.1148/RADI0L0GY.143.1.7063747; Herrada OCC., 2009, MUSIC RECOMMENDATION; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hosseini SA, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P847, DOI 10.1145/3097983.3098197; Hu CW, 2016, JMLR WORKSH CONF PRO, V51, P1133; Hu L, 2017, ACM T INFORM SYST, V35, DOI 10.1145/3052769; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Jerfel G, 2017, PR MACH LEARN RES, V54, P738; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Koren Y, 2010, COMMUN ACM, V53, P89, DOI 10.1145/1721654.1721677; Li B., 2011, P 22 INT JOINT C ART, P2293, DOI DOI 10.5555/2283696.2283780; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mnih A., 2007, ADV NEURAL INF PROCE, V20, P1257; Schein A., 2016, ADV NEURAL INFORM PR, P5006; Sun JZ, 2014, IEEE T SIGNAL PROCES, V62, P3499, DOI 10.1109/TSP.2014.2326618; Do TDT, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5010; Do TDT, 2018, AAAI CONF ARTIF INTE, P2918; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wei Wei, 2014, P SDM 14, P776; Xiong L., 2010, P SDM COL OH, P211; Xu J, 2018, LECT NOTES ARTIF INT, V10937, P285, DOI 10.1007/978-3-319-93034-3_23; Zhang W, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1455, DOI 10.1145/2783258.2783336; Zhang YZ, 2016, IEEE DATA MINING, P1359, DOI [10.1109/ICDM.2016.111, 10.1109/ICDM.2016.0186]; Zhao H, 2017, PR MACH LEARN RES, V70; Zhou Mingyuan, 2012, AISTATS, P1462	49	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000034
C	Wang, D; Gaboardi, M; Xu, JH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Di; Gaboardi, Marco; Xu, Jinhui			Empirical Risk Minimization in Non-interactive Local Differential Privacy Revisited	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we revisit the Empirical Risk Minimization problem in the non-interactive local model of differential privacy. In the case of constant or low dimensions (p << n), we first show that if the loss function is (infinity,T)-smooth, we can avoid a dependence of the sample complexity, to achieve error a, on the exponential of the dimensionality p with base 1/alpha (i.e., alpha(-p)), which answers a question in [19]. Our approach is based on polynomial approximation. Then, we propose player-efficient algorithms with 1-bit communication complexity and O(1) computation cost for each player. The error bound is asymptotically the same as the original one. With some additional assumptions, we also give an efficient algorithm for the server. In the case of high dimensions (n << p), we show that if the loss function is a convex generalized linear function, the error can be bounded by using the Gaussian width of the constrained set, instead of p, which improves the one in [19].	[Wang, Di; Gaboardi, Marco; Xu, Jinhui] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Wang, D (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.	dwang45@buffalo.edu; gaboardi@buffalo.edu; jinhui@buffalo.edu			National Science Foundation (NSF) [CCF-1422324, CCF-1716400, CCF-1718220, CNS-1565365]	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea)	This research was supported in part by the National Science Foundation (NSF) under Grant No. CCF-1422324, CCF-1716400, CCF-1718220 and CNS-1565365.	Alda F, 2017, AAAI CONF ARTIF INTE, P1705; Bassily R, 2015, ACM S THEORY COMPUT, P127, DOI 10.1145/2746539.2746632; Beimel A, 2008, LECT NOTES COMPUT SC, V5157, P451, DOI 10.1007/978-3-540-85174-5_25; Di Wang, 2019, 33 AAAI C ART INT AA; Di Wang, 2019, ALGORITHMIC LEARNING; Dirksen S, 2016, FOUND COMPUT MATH, V16, P1367, DOI 10.1007/s10208-015-9280-x; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Duchi John C, 2017, J AM STAT ASS; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Haney S, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1339, DOI 10.1145/3035918.3035940; Kasiviswanathan S.P., 2016, P 33 INT C MACHINE L, P488; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Lorentz G. G., 1986, AMS CHELSEA PUBLISHI; Micchelli C., 1973, J APPROX THEORY, V8, P1; Near J., 2018, ENIGMA 2018; Nissim Kobbi, 2017, CORR; Risteski Andrej, 2016, ADV NEURAL INFORM PR, P4745; Shalev-Shwartz S., 2009, P 22 C LEARN THEOR C, P1; Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35; Tang J., 2017, CORR; Thaler J, 2012, LECT NOTES COMPUT SC, V7391, P810, DOI 10.1007/978-3-642-31594-7_68; Vershynin R, 2015, APPL NUMER HARMON AN, P3, DOI 10.1007/978-3-319-19749-4_1; Wang D, 2017, ADV NEUR IN, V30; Wang Ziteng, 2016, J MACHINE LEARNING R, V17, P1779; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zheng K, 2017, PR MACH LEARN RES, V70	27	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300089
C	Wang, PQ; Xie, XF; Deng, L; Li, GQ; Wang, DS; Xie, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Peiqi; Xie, Xinfeng; Deng, Lei; Li, Guoqi; Wang, Dongsheng; Xie, Yuan			HitNet: Hybrid Ternary Recurrent Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Quantization is a promising technique to reduce the model size, memory footprint, and computational cost of neural networks for the employment on embedded devices with limited resources. Although quantization has achieved impressive success in convolutional neural networks (CNNs), it still suffers from large accuracy degradation on recurrent neural networks (RNNs), especially in the extremely low-bit cases. In this paper, we first investigate the accuracy degradation of RNNs under different quantization schemes and visualize the distribution of tensor values in the full precision models. Our observation reveals that due to the different distributions of weights and activations, different quantization methods should be used for each part. Accordingly, we propose HitNet, a hybrid ternary RNN, which bridges the accuracy gap between the full precision model and the quantized model with ternary weights and activations. In HitNet, we develop a hybrid quantization method to quantize weights and activations. Moreover, we introduce a sloping factor into the activation functions to address the error-sensitive problem, further closing the mentioned accuracy gap. We test our method on typical RNN models, such as Long-Short-Term Memory (LSTM) and Gated Recurrent Unit (GRU). Overall, HitNet can quantize RNN models into ternary values of { -1, 0, 1} and significantly outperform the state-of-the-art methods towards extremely quantized RNNs. Specifically, we improve the perplexity per word (PPW) of a ternary LSTM on Penn Tree Bank (PTB) corpus from 126 to 110.3 and a ternary GRU from 142 to 113.5.	[Wang, Peiqi; Wang, Dongsheng] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Wang, Peiqi; Wang, Dongsheng] Beijing Natl Res Ctr Informat Sci & Technol, Beijing, Peoples R China; [Li, Guoqi] Tsinghua Univ, Dept Precis Instrument, Beijing, Peoples R China; [Wang, Peiqi; Xie, Xinfeng; Deng, Lei; Xie, Yuan] Univ Calif Santa Barbara, Dept Elect & Comp Engn, Santa Barbara, CA 93106 USA	Tsinghua University; Tsinghua University; University of California System; University of California Santa Barbara	Wang, DS (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China.; Wang, DS (corresponding author), Beijing Natl Res Ctr Informat Sci & Technol, Beijing, Peoples R China.	wpq14@mails.tsinghua.edu.cn; xinfeng@ucsb.edu; leideng@ucsb.edu; liguoqi@mail.tsinghua.edu.cn; wds@mail.tsinghua.edu.cn; yuanxie@ucsb.edu	LI, Guoqi/AAG-7110-2020	LI, Guoqi/0000-0002-8994-431X	Beijing National Research Center for Information Science and Technology; Beijing Innovation Center for Future Chip; Scalable Energy-efficient Architecture Lab (SEAL) in UCSB; National Key Research and Development Plan of China [2016YFB1000303]; National Science Foundations(NSF) [1725447, 1730309]; National Natural Science Foundation of China [61876215]; China Scholarship Council	Beijing National Research Center for Information Science and Technology; Beijing Innovation Center for Future Chip; Scalable Energy-efficient Architecture Lab (SEAL) in UCSB; National Key Research and Development Plan of China; National Science Foundations(NSF)(National Science Foundation (NSF)); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Scholarship Council(China Scholarship Council)	The authors were supported by the Beijing National Research Center for Information Science and Technology, the Beijing Innovation Center for Future Chip, and the Scalable Energy-efficient Architecture Lab (SEAL) in UCSB. This research was supported in part by the National Key Research and Development Plan of China under Grant No. 2016YFB1000303, the National Science Foundations(NSF) under Grant No. 1725447 and 1730309, and the National Natural Science Foundation of China under Grant No. 61876215. This work was also supported in part by a grant from the China Scholarship Council.	Albericio J, 2016, CONF PROC INT SYMP C, P1, DOI 10.1109/ISCA.2016.11; [Anonymous], 2018, ARXIV180200150; Bengio Y., 2014, ARXIV14061078; Bengio Yoshua, 2013, ARXIV13083432; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Deng L, 2018, NEURAL NETWORKS, V100, P49, DOI 10.1016/j.neunet.2018.01.010; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Gray RM, 1998, IEEE T INFORM THEORY, V44, P2325, DOI 10.1109/18.720541; Guo Y., 2017, ARXIV170602021; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han S, 2017, FPGA'17: PROCEEDINGS OF THE 2017 ACM/SIGDA INTERNATIONAL SYMPOSIUM ON FIELD-PROGRAMMABLE GATE ARRAYS, P75, DOI 10.1145/3020078.3021745; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He Q., 2016, P CVPR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Horowitz M, 2014, ISSCC DIG TECH PAP I, V57, P10, DOI 10.1109/ISSCC.2014.6757323; Hubara Itay, 2016, QUANTIZED NEURAL NET, P3; Iyyer M., 2014, P 2014 C EMPIRICAL M, P633, DOI DOI 10.3115/V1/D14-1070; Kapur Supriya, 2017, ARXIV PREPRINT ARXIV; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Li GQ, 2016, SCI REP-UK, V6, DOI 10.1038/srep19133; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Ott J., 2016, ARXIV160806902; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Sak H, 2014, INTERSPEECH, P338; TANG CZ, 1993, IEEE T SIGNAL PROCES, V41, P2724, DOI 10.1109/78.229903; Taylor Ann, 2003, TREEBANKS, P5, DOI DOI 10.1007/978-94-010-0201-1_1; van de Burgt Y, 2017, NAT MATER, V16, P414, DOI [10.1038/NMAT4856, 10.1038/nmat4856]; Wen W, 2016, ADV NEUR IN, V29; Zhang S., 2016, MATH PROBL ENG, V2016, P1, DOI DOI 10.1109/MICR0.2016.7783723; Zhou S., 2016, ARXIV160606160; Zhou SC, 2017, J COMPUT SCI TECH-CH, V32, P667, DOI 10.1007/s11390-017-1750-y; Zhu Chenzhuo, 2016, ARXIV161201064	35	6	7	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300056
C	Wang, YH; Squires, C; Belyaeva, A; Uhler, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Yuhao; Squires, Chandler; Belyaeva, Anastasiya; Uhler, Caroline			Direct Estimation of Differences in Causal Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CANCER; NETWORKS; KINASES	We consider the problem of estimating the differences between two causal directed acyclic graph (DAG) models with a shared topological order given i.i.d. samples from each model. This is of interest for example in genomics, where changes in the structure or edge weights of the underlying causal graphs reflect alterations in the gene regulatory networks. We here provide the first provably consistent method for directly estimating the differences in a pair of causal DAGs without separately learning two possibly large and dense DAG models and computing their difference. Our two-step algorithm first uses invariance tests between regression coefficients of the two data sets to estimate the skeleton of the difference graph and then orients some of the edges using invariance tests between regression residual variances. We demonstrate the properties of our method through a simulation study and apply it to the analysis of gene expression data from ovarian cancer and during T-cell activation.	[Wang, Yuhao; Squires, Chandler; Belyaeva, Anastasiya; Uhler, Caroline] MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA; [Wang, Yuhao; Squires, Chandler; Belyaeva, Anastasiya; Uhler, Caroline] MIT, Inst Data Syst & Soc, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Wang, YH (corresponding author), MIT, Informat & Decis Syst Lab, Cambridge, MA 02139 USA.; Wang, YH (corresponding author), MIT, Inst Data Syst & Soc, Cambridge, MA 02139 USA.	yuhaow@mit.edu; csquires@mit.edu; belyaeva@mit.edu; cuhler@mit.edu			ONR [N00014-17-1-2147]; NSF [DMS-1651995]; MIT-IBM Watson AI Lab; NSF Graduate Research Fellowship [1122374]; Abdul Latif Jameel World Water and Food Security Lab (J-WAFS) at MIT; Sloan Fellowship	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); MIT-IBM Watson AI Lab; NSF Graduate Research Fellowship(National Science Foundation (NSF)); Abdul Latif Jameel World Water and Food Security Lab (J-WAFS) at MIT; Sloan Fellowship(Alfred P. Sloan Foundation)	Yuhao Wang was supported by ONR (N00014-17-1-2147), NSF (DMS-1651995) and the MIT-IBM Watson AI Lab. Anastasiya Belyaeva was supported by an NSF Graduate Research Fellowship (1122374) and the Abdul Latif Jameel World Water and Food Security Lab (J-WAFS) at MIT. Caroline Uhler was partially supported by ONR (N00014-17-1-2147), NSF (DMS-1651995), and a Sloan Fellowship.	Andersson SA, 1997, ANN STAT, V25, P505; Barabasi AL, 2004, NAT REV GENET, V5, P101, DOI 10.1038/nrg1272; Barabasi AL, 2011, NAT REV GENET, V12, P56, DOI 10.1038/nrg2918; Cheadle C, 2008, BMC MED GENOMICS, V1, DOI 10.1186/1755-8794-1-43; Chiaradonna F, 2008, FRONT BIOSCI-LANDMRK, V13, P5257, DOI 10.2741/3079; Cho-Chung YS, 1999, PHARMACOL THERAPEUT, V82, P437, DOI 10.1016/S0163-7258(98)00043-6; Cui Y, 2016, CELL REP, V15, P256, DOI 10.1016/j.celrep.2016.03.016; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Ghassami A. E., 2017, P NIPS, P3015; Hatton L. A., 2013, THESIS; Hudson NJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000382; Johnstone RW, 2008, NAT REV CANCER, V8, P782, DOI 10.1038/nrc2465; Jonsson JM, 2014, FAM CANCER, V13, P537, DOI 10.1007/s10689-014-9728-1; Kalisch M, 2007, J MACH LEARN RES, V8, P613; Lauritzen Steffen L., 1996, OXFORD STAT SCI SERI, V17; Lin SW, 2014, FOUND COMPUT MATH, V14, P1079, DOI 10.1007/s10208-014-9205-0; Liu S., 2017, BEHAVIORMETRIKA, V44, P265; Liu S, 2014, NEURAL COMPUT, V26, P1169, DOI 10.1162/NECO_a_00589; Marchini S, 2013, EUR J CANCER, V49, P520, DOI 10.1016/j.ejca.2012.06.026; Meek C., 1997, PHD THESIS; Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x; Mikalsen T, 2006, BIOTECHNOL ANN REV, V12, P153, DOI 10.1016/S1387-2656(06)12006-2; Nandy P., 2015, ANN STAT; Obata Y, 2014, NAT IMMUNOL, V15, P571, DOI 10.1038/ni.2886; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Peixoto A, 2007, J EXP MED, V204, P1193, DOI 10.1084/jem.20062349; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Pimanda JE, 2007, P NATL ACAD SCI USA, V104, P17692, DOI 10.1073/pnas.0707045104; Ramsey J, 2006, P 22 C UNC ART INT, P401; Robins J. M., 2000, MARGINAL STRUCTURAL; Sanei S., 2013, EEG SIGNAL PROCESSIN; Sarkar S, 2008, J EXP MED, V205, P625, DOI 10.1084/jem.20071641; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Singer M, 2016, CELL, V166, P1500, DOI 10.1016/j.cell.2016.08.052; Solus L., 2017, CONSISTENCY GUARANTE; Spirtes P., 2000, CAUSATION PREDICTION; Tothill RW, 2008, CLIN CANCER RES, V14, P5198, DOI 10.1158/1078-0432.CCR-08-0196; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080; Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085; Zhang K., 2017, IJCAI P C NIH PUBL A, V2017, P1347; Zhao SD, 2014, BIOMETRIKA, V101, P253, DOI 10.1093/biomet/asu009	43	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303074
C	Wu, CS; Herranz, L; Liu, XL; Wang, YX; van de Weijer, J; Raducanu, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Chenshen; Herranz, Luis; Liu, Xialei; Wang, Yaxing; van de Weijer, Joost; Raducanu, Bogdan			Memory Replay GANs: learning to generate images from new categories without forgetting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Previous works on sequential learning address the problem of forgetting in discriminative models. In this paper we consider the case of generative models. In particular, we investigate generative adversarial networks (GANs) in the task of learning new categories in a sequential fashion. We first show that sequential fine tuning renders the network unable to properly generate images from previous categories (i.e. forgetting). Addressing this problem, we propose Memory Replay GANs (MeRGANs), a conditional GAN framework that integrates a memory replay generator. We study two methods to prevent forgetting by leveraging these replays, namely joint training with replay and replay alignment. Qualitative and quantitative experimental results in MNIST, SVHN and LSUN datasets show that our memory replay approach can generate competitive images while significantly mitigating the forgetting of previous categories.(1)	[Wu, Chenshen; Herranz, Luis; Liu, Xialei; Wang, Yaxing; van de Weijer, Joost; Raducanu, Bogdan] Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona, Spain	Autonomous University of Barcelona; Centre de Visio per Computador (CVC)	Wu, CS (corresponding author), Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona, Spain.	chenshen@cvc.uab.es; lherranz@cvc.uab.es; xialei@cvc.uab.es; yaxing@cvc.uab.es; joost@cvc.uab.es; bogdan@cvc.uab.es	Liu, Xialei/HHN-2835-2022; liu, xialei/AAP-4986-2020; Herranz, Luis/B-4573-2016	liu, xialei/0000-0001-8534-3026; Herranz, Luis/0000-0002-7022-3395; van de Weijer, Joost/0000-0002-9656-9706	Chinese Scholarship Council (CSC) [201709110103, 201506290018, 201507040048]; European Union research and innovation program under the Marie Sklodowska-Curie grant [6655919]; CHISTERA project M2CR of the Spanish Ministry [PCIN-2015-251]; ACCIO agency; CERCA Programme / Generalitat de Catalunya; EU Project CybSpeed MSCA-RISE-2017 [777720]; NVIDIA;  [TIN2016-79717-R]	Chinese Scholarship Council (CSC)(China Scholarship Council); European Union research and innovation program under the Marie Sklodowska-Curie grant; CHISTERA project M2CR of the Spanish Ministry; ACCIO agency; CERCA Programme / Generalitat de Catalunya; EU Project CybSpeed MSCA-RISE-2017; NVIDIA; 	C. Wu, X. Liu, and Y. Wang, acknowledge the Chinese Scholarship Council (CSC) grant No. 201709110103, No. 201506290018 and No. 201507040048. Luis Herranz acknowledges the European Union research and innovation program under the Marie Sklodowska-Curie grant agreement No. 6655919. This work was supported by TIN2016-79717-R, and the CHISTERA project M2CR (PCIN-2015-251) of the Spanish Ministry, the ACCIO agency and CERCA Programme / Generalitat de Catalunya, and the EU Project CybSpeed MSCA-RISE-2017-777720. We also acknowledge the generous GPU support from NVIDIA.	Abadi M., TENSORFLOW LARGE SCA; Bottou L., 2017, ARXIV170107875STATML; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Gais S, 2007, P NATL ACAD SCI USA, V104, P18778, DOI 10.1073/pnas.0705454104; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani Ishaan, 2017, NIPS; Heusel M., 2017, NIPS; Karras Tero, 2018, ICLR; Kemker R., 2018, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Li Z., 2016, ECCV; Liu Xiyan, 2018, ICPR; Lopez-Paz D., 2017, NIPS; Mao X., 2017, ICCV; Mirza M., 2014, CONDITIONAL GENERATI; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Odena A., 2017, ICML; Radford A., 2016, ICLR; Rebuffi S.A., 2017, CVPR; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Seff Ari, 2017, ARXIV170508395V1; Shin H., 2017, NIPS; Simonyan Karen, 2015, INT C LEARN REPR; Wang Y., 2018, ECCV; Yu F., 2015, ARXIVABS150603365 CO	28	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000046
C	Wu, XD; Liu, XW; Li, W; Wu, Q		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wu, Xundong; Liu, Xiangwen; Li, Wei; Wu, Qing			Improved Expressivity Through Dendritic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PLASTICITY; CAPACITY; STORAGE	A typical biological neuron, such as a pyramidal neuron of the neocortex, receives thousands of afferent synaptic inputs on its dendrite tree and sends the efferent axonal output downstream. In typical artificial neural networks, dendrite trees are modeled as linear structures that funnel weighted synaptic inputs to the cell bodies. However, numerous experimental and theoretical studies have shown that dendritic arbors are far more than simple linear accumulators. That is, synaptic inputs can actively modulate their neighboring synaptic activities; therefore, the dendritic structures are highly nonlinear. In this study, we model such local nonlinearity of dendritic trees with our dendritic neural network (DENN) structure and apply this structure to typical machine learning tasks. Equipped with localized nonlinearities, DENNs can attain greater model expressivity than regular neural networks while maintaining efficient network inference. Such strength is evidenced by the increased fitting power when we train DENNs with supervised machine learning tasks. We also empirically show that the locality structure of DENNs can improve the generalization performance, as exemplified by DENNs outranking naive deep neural network architectures when tested on classification tasks from the UCI machine learning repository.	[Wu, Xundong; Liu, Xiangwen; Li, Wei; Wu, Qing] Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China	Hangzhou Dianzi University	Wu, XD (corresponding author), Hangzhou Dianzi Univ, Sch Comp Sci & Technol, Hangzhou, Zhejiang, Peoples R China.	wuxundong@gmail.com; wuq@hdu.edu.cn		Wu, Xundong/0000-0002-6643-4384				[Anonymous], 2013, ARXIV13126098; Branco T, 2010, CURR OPIN NEUROBIOL, V20, P494, DOI 10.1016/j.conb.2010.07.009; Bryll R, 2003, PATTERN RECOGN, V36, P1291, DOI 10.1016/S0031-3203(02)00121-8; Cayco-Gajic NA, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01109-y; Chklovskii DB, 2004, NATURE, V431, P782, DOI 10.1038/nature03012; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I. J., 2013, ARXIV13024389; Guerguiev J, 2017, ELIFE, V6, DOI 10.7554/eLife.22901; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Hinton, 2016, ARXIV PREPRINT ARXIV; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Hussain S, 2014, IEEE INT SYMP CIRC S, P2640, DOI 10.1109/ISCAS.2014.6865715; Kingma D.P, P 3 INT C LEARNING R; Klambauer Gnter, 2017, SELF NORMALIZING NEU; KOCH C, 1983, P NATL ACAD SCI-BIOL, V80, P2799, DOI 10.1073/pnas.80.9.2799; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee H., 2007, ADV NEURAL INF PROCE, P801; Litwin-Kumar A, 2017, NEURON, V93, P1153, DOI 10.1016/j.neuron.2017.01.030; Losonczy A, 2006, NEURON, V50, P291, DOI 10.1016/j.neuron.2006.03.016; Losonczy A, 2008, NATURE, V452, P436, DOI 10.1038/nature06725; MEL BW, 1993, J NEUROPHYSIOL, V70, P1086, DOI 10.1152/jn.1993.70.3.1086; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Nair V, 2010, P 27 INT C MACHINE L, P807; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Poirazi P, 2001, NEURON, V29, P779, DOI 10.1016/S0896-6273(01)00252-5; Poirazi P, 2003, NEURON, V37, P989, DOI 10.1016/S0896-6273(03)00149-1; Raghu M, 2017, PR MACH LEARN RES, V70; Ritter GX, 2003, IEEE T NEURAL NETWOR, V14, P282, DOI 10.1109/TNN.2003.809427; Rolnick David, 2017, ARXIV170505502; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Sacramento Joao, 2017, ARXIV180100062; Schiller J, 2000, NATURE, V404, P285, DOI 10.1038/35005094; Srinivas S, 2017, IEEE COMPUT SOC CONF, P455, DOI 10.1109/CVPRW.2017.61; Stuart G, 2016, DENDRITES; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wainberg Michael, 2016, J MACHINE LEARNING R, V17, P3837; Wang SN, 2004, IEEE T CIRCUITS-I, V51, P1889, DOI 10.1109/TCSI.2004.834521; Wen W, 2016, ADV NEUR IN, V29; Wu XE, 2009, NEURON, V62, P31, DOI 10.1016/j.neuron.2009.02.021; Xiao H., 2017, FASHION MNIST NOVEL; Yuste R, 2011, NEURON, V71, P772, DOI 10.1016/j.neuron.2011.07.024; Zaslavsky Thomas, 1975, FACING ARRANGEMENTS, V154; Zenobi G, 2001, EUROPEAN C MACHINE L, P576, DOI DOI 10.1007/3-540-44795-4_49	49	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002059
C	Ye, NY; Zhu, ZX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ye, Nanyang; Zhu, Zhanxing			Bayesian Adversarial Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks have been known to be vulnerable to adversarial attacks, raising lots of security concerns in the practical deployment. Popular defensive approaches can be formulated as a (distributionally) robust optimization problem, which minimizes a "point estimate" of worst-case loss derived from either perdatum perturbation or adversary data-generating distribution within certain predefined constraints. This point estimate ignores potential test adversaries that are beyond the pre-defined constraints. The model robustness might deteriorate sharply in the scenario of stronger test adversarial data. In this work, a novel robust training framework is proposed to alleviate this issue, Bayesian Robust Learning, in which a distribution is put on the adversarial data-generating distribution to account for the uncertainty of the adversarial data-generating process. The uncertainty directly helps to consider the potential adversaries that are stronger than the point estimate in the cases of distributionally robust optimization. The uncertainty of model parameters is also incorporated to accommodate the full Bayesian framework. We design a scalable Markov Chain Monte Carlo sampling strategy to obtain the posterior distribution over model parameters. Various experiments are conducted to verify the superiority of BAL over existing adversarial training methods. The code for BAL is available at https : //tinyur1.com/ycxsaewr.	[Ye, Nanyang] Univ Cambridge, Cambridge, England; [Zhu, Zhanxing] Peking Univ, Ctr Data Sci, BIBDR, Beijing, Peoples R China	University of Cambridge; Peking University	Zhu, ZX (corresponding author), Peking Univ, Ctr Data Sci, BIBDR, Beijing, Peoples R China.	yn272@cam.ac.uk; zhanxing.zhu@pku.edu.cn	Zhu, Zhanxing/GQA-7335-2022		National Natural Science Foundation of China [61806009]; Beijing Natural Science Foundation [4184090]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	Dr. Zhanxing Zhu is supported by National Natural Science Foundation of China (Grant No: 61806009) and Beijing Natural Science Foundation (Grant No: 4184090).	Akhtar N., 2018, ARXIV180100553; [Anonymous], 2018, ARXIV180102610; Ben-Tal A, 1998, MATH OPER RES, V23, P769, DOI 10.1287/moor.23.4.769; Bruckner M, 2012, J MACH LEARN RES, V13, P2617; Bulo SR, 2017, IEEE T NEUR NET LEAR, V28, P2466, DOI 10.1109/TNNLS.2016.2593488; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741; Goodfellow I. J., 2014, ARXIV14126572; Grosshans M., 2013, INT C MACH LEARN, P55; He W., 2017, ARXIV170604701; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kingma D., 2014, ADAM METHOD STOCHAST; KOLTER JZ, 2017, ARXIV171100851; Kos J., 2017, ARXIV170206832; Kurakin A, 2016, INT C LEARN REPR SAN; Larsen ABL, 2016, PR MACH LEARN RES, V48; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Madry A., 2018, ARXIV PREPRINT ARXIV; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; Ritter H., 2018, INT C LEARN REPR; Saatci Y., 2017, ADV NEURAL INFORM PR, P3622; Sinha A., 2018, ICLR; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Stallkamp Johannes, 2012, NEURAL NETWORKS; Szegedy C, 2013, 2 INT C LEARNING REP; Tramer F., 2017, ARXIV170507204; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xiao C., 2018, INT C LEARN REPR; Xiao H., 2017, CORR	30	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001043
C	Zhang, X; Solar-Lezama, A; Singh, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Xin; Solar-Lezama, Armando; Singh, Rishabh			Interpreting Neural Network Judgments via Minimal, Stable, and Symbolic Corrections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a new algorithm to generate minimal, stable, and symbolic corrections to an input that will cause a neural network with ReLU activations to change its output. We argue that such a correction is a useful way to provide feedback to a user when the network's output is different from a desired output. Our algorithm generates such a correction by solving a series of linear constraint satisfaction problems. The technique is evaluated on three neural network models: one predicting whether an applicant will pay a mortgage, one predicting whether a first-order theorem can be proved efficiently by a solver using certain heuristics, and the final one judging whether a drawing is an accurate rendition of a canonical drawing of a cat.	[Zhang, Xin; Solar-Lezama, Armando] MIT, CSAIL, Cambridge, MA 02139 USA; [Singh, Rishabh] Google Brain, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Zhang, X (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	xzhang@csail.mit.edu; asolar@csail.mit.edu; rising@google.com			ONR PERISCOPE MURI [N00014-17-1-2699]	ONR PERISCOPE MURI	We thank the reviewers for their insightful comments and useful suggestions. This work was funded in part by ONR PERISCOPE MURI, award N00014-17-1-2699.	Nguyen A, 2016, ADV NEUR IN, V29; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bastani O., 2016, P 30 INT C NEUR INF, P2613; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Dhurandhar A., 2018, ADV NEURAL INFORM PR; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gurobi Optimization, 2022, GUROBI OPTIMIZER REF; Ha David, 2017, ARXIV170403477; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; I. Google, 2017, QUICK DRAW DAT; James S. B. H., 2013, 1 ORDER THEOREM PROV; Kim C., 2017, ARXIV170609773; Kindermans P., 2018, P INT C LEARNING REP, P1; Koh PW, 2017, PR MACH LEARN RES, V70; Krakovna V., 2016, ABS160605320 CORR; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Li O., 2017, ABS171004806 CORR; Lundberg SM, 2017, ADV NEUR IN, V30; Mae Fannie, 2017, FANNIE MAE SINGLE FA; McDaniel Patrick, 2017, ARXIV PREPRINT ARXIV; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Pinheiro PO, 2015, PROC CVPR IEEE, P1713, DOI 10.1109/CVPR.2015.7298780; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Ribeiro Marco Tulio, 2018, AAAI; Sirignano J., 2016, ARXIV160702470; Tan S, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P617, DOI 10.1109/ASRU.2015.7404853; van den Oord A, 2016, PR MACH LEARN RES, V48; Wu CY, 2016, INTERSPEECH, P400, DOI 10.21437/Interspeech.2016-580; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	33	6	6	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304085
C	Zhu, ZH; Wang, YF; Robinson, D; Naiman, D; Vidal, R; Tsakiris, MC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhu, Zhihui; Wang, Yifan; Robinson, Daniel; Naiman, Daniel; Vidal, Rene; Tsakiris, Manolis C.			Dual Principal Component Pursuit: Improved Analysis and Efficient Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				M-ESTIMATOR; SUBSPACE	Recent methods for learning a linear subspace from data corrupted by outliers are based on convex l(1) and nuclear norm optimization and require the dimension of the subspace and the number of outliers to be sufficiently small [27]. In sharp contrast, the recently proposed Dual Principal Component Pursuit (DPCP) method [22] can provably handle subspaces of high dimension by solving a non-convex l(1) optimization problem on the sphere. However, its geometric analysis is based on quantities that are difficult to interpret and are not amenable to statistical analysis. In this paper we provide a refined geometric analysis and a new statistical analysis that show that DPCP can tolerate as many outliers as the square of the number of inliers, thus improving upon other provably correct robust PCA methods. We also propose a scalable Projected Sub-Gradient Method (DPCP-PSGM) for solving the DPCP problem and show that it achieves linear convergence even though the underlying optimization problem is non-convex and non-smooth. Experiments on road plane detection from 3D point cloud data demonstrate that DPCP-PSGM can be more efficient than the traditional RANSAC algorithm, which is one of the most popular methods for such computer vision applications.	[Zhu, Zhihui; Vidal, Rene] Johns Hopkins Univ, MINDS, Baltimore, MD 21218 USA; [Wang, Yifan; Tsakiris, Manolis C.] ShanghaiTech Univ, SIST, Shanghai, Peoples R China; [Robinson, Daniel; Naiman, Daniel] Johns Hopkins Univ, AMS, Baltimore, MD 21218 USA	Johns Hopkins University; ShanghaiTech University; Johns Hopkins University	Zhu, ZH (corresponding author), Johns Hopkins Univ, MINDS, Baltimore, MD 21218 USA.	zzhu29@jhu.edu; wangyf@shanghaitech.edu.cn; daniel.p.robinson@jhu.edu; daniel.naiman@jhu.edu; rvidal@jhu.edu; mtsakiris@shanghaitech.edu.cn	Zhu, Zhihui/AAR-5029-2020	Zhu, Zhihui/0000-0002-3856-0375; Robinson, Daniel/0000-0003-0251-4227	NSF [1704458]	NSF(National Science Foundation (NSF))	The co-authors from JHU were supported by NSF grant 1704458. We thank Tianyu Ding of JHU for carefully proof-reading the longer version of this manuscript and catching some mistakes, Yunchen Yang and Tianjiao Ding of ShanghaiTech for refining the 3 D point cloud experiment, Ziyu Liu of ShanghaiTech for his help in deriving the concentration inequality for eta<INF>O</INF>, and the three anonymous reviewers for their constructive comments.	Boyd S, 2003, STANFORD U LECT NOTE; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Cherapanamjeri Y., 2017, ARXIV170205571; Davis D., 2018, ARXIV180302461; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; GOFFIN JL, 1977, MATH PROGRAM, V13, P329, DOI 10.1007/BF01584346; Hartley R., 2003, MULTIPLE VIEW GEOMET, DOI 10.1016/S0143-8166(01)00145-2; I. Gurobi Optimization, 2015, GUROBI OPTIMIZER REF; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Lerman G., 2018, ARXIV180301013CSLG; Lerman G., 2017, INFORM INF A J IMA, V7, P277; Lerman G, 2015, FOUND COMPUT MATH, V15, P363, DOI 10.1007/s10208-014-9221-0; Lerman G, 2014, CONSTR APPROX, V40, P329, DOI 10.1007/s00365-014-9242-6; Li X, 2018, ARXIV180909237; Maunu T., 2017, ARXIV170603896CSLG; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Rahmani M., 2016, ARXIV160904789; Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199; SPATH H, 1987, NUMER MATH, V51, P531, DOI 10.1007/BF01400354; Tsakiris MC, 2017, PR MACH LEARN RES, V70; Tsakiris MC, 2018, J MACH LEARN RES, V19; Tsakiris MC, 2015, 2015 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOP (ICCVW), P850, DOI 10.1109/ICCVW.2015.114; TYLER DE, 1987, ANN STAT, V15, P234, DOI 10.1214/aos/1176350263; Wang Y, 2015, PROC CVPR IEEE, P3261, DOI 10.1109/CVPR.2015.7298946; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; You C, 2017, PROC CVPR IEEE, P4323, DOI 10.1109/CVPR.2017.460; Zhang T, 2016, INF INFERENCE, V5, P1, DOI 10.1093/imaiai/iav012; Zhang T, 2014, J MACH LEARN RES, V15, P749	30	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302020
C	Zimmer, C; Meister, M; Nguyen-Tuong, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zimmer, Christoph; Meister, Mona; Duy Nguyen-Tuong			Safe Active Learning for Time-Series Modeling with Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning time-series models is useful for many applications, such as simulation and forecasting. In this study, we consider the problem of actively learning time-series models while taking given safety constraints into account. For time-series modeling we employ a Gaussian process with a nonlinear exogenous input structure. The proposed approach generates data appropriate for time series model learning, i.e. input and output trajectories, by dynamically exploring the input space. The approach parametrizes the input trajectory as consecutive trajectory sections, which are determined stepwise given safety requirements and past observations. We analyze the proposed algorithm and evaluate it empirically on a technical application. The results show the effectiveness of our approach in a realistic technical use case.	[Zimmer, Christoph; Meister, Mona; Duy Nguyen-Tuong] Bosch Ctr Artificial Intelligence, Renningen, Germany		Zimmer, C (corresponding author), Bosch Ctr Artificial Intelligence, Renningen, Germany.	christoph.zimmer@de.bosch.com; mona.meister@de.bosch.com; duy.nguyen-tuong@de.bosch.com						Auer P., 2002, J MACHINE LEARNING R; Berkenkamp F., 2016, TECHNICAL REPORT; Billings SA, 2013, NONLINEAR SYSTEM IDENTIFICATION: NARMAX METHODS IN THE TIME, FREQUENCY, AND SPATIO-TEMPORAL DOMAINS, P1, DOI 10.1002/9781118535561; Byrd RH, 2000, MATH PROGRAM, V89, P149, DOI 10.1007/PL00011391; Coleman T. F., 1992, TECHNICAL REPORT; Deflorian M., 2011, P 18 WORLD C INT FED; Deflorian M., 2010, 6 IFAC S ADV AUT CON; Fedorov V., 2012, LECT NOTES STAT; Galichet N, 2013, P 5 AS C MACH LEARN; Geibel P., 2001, P 18 INT C MACH LEAR, P162; Guestrin Carlos, 2005, P 22 INT C MACH LEAR; Joshi AJ, 2009, PROC CVPR IEEE, P2364; Ljung L., 1985, MIT PRESS SERIES SIG; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Minka Tom, 2001, UNCERTAINTY ARTIFICI; Moldovan T. M., 2012, P 29 INT C MACH LEAR; Pintelon R., 2012, SYSTEM IDENTIFICATIO; Quinonero-~Candela J., 2005, J MACHINE LEARNING R; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schreiter J., 2015, ECML PKDD, V9286; SEEGER M, 2007, LOW RANK UPDATES CHO; Snelson E., 2006, ADV NEURAL INFORM PR, V18; Srinivas N., 2012, T INFORM THEORY; Sui Y., 2018, 35 INT C MACH LEARN; Tietze N., 2014, 14 INT STUTTG S; Titsias M.K., 2009, ARTIF INTELL	26	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302072
C	Zohar, A; Wolf, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zohar, Amit; Wolf, Lior			Automatic Program Synthesis of Long Programs with a Learned Garbage Collector	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider the problem of generating automatic code given sample input-output pairs. We train a neural network to map from the current state and the outputs to the program's next statement. The neural network optimizes multiple tasks concurrently: the next operation out of a set of high level commands, the operands of the next statement, and which variables can be dropped from memory. Using our method we are able to create programs that are more than twice as long as existing state-of-the-art solutions, while improving the success rate for comparable lengths, and cutting the run-time by two orders of magnitude. Our code, including an implementation of various literature baselines, is publicly available at https://github.com/amitz25/PCCoder	[Zohar, Amit; Wolf, Lior] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel; [Wolf, Lior] Facebook AI Res, Tel Aviv, Israel	Tel Aviv University; Facebook Inc	Zohar, A (corresponding author), Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.				ICRC grant	ICRC grant	This work was supported by an ICRC grant.	[Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2017, ICLR OPEN REV DEEPCO; Balog M., 2017, ICLR; Devlin J., 2017, ICML; Gulwani S, 2016, LECT NOTES ARTIF INT, V9706, P9, DOI 10.1007/978-3-319-40229-1_2; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kalyan A., 2018, ICLR; King DB, 2015, ACS SYM SER, V1214, P1; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Polozov O, 2015, ACM SIGPLAN NOTICES, V50, P107, DOI [10.1145/2858965.2814310, 10.1145/2814270.2814310]; Rocktaschel T., 2016, ABS160506640 CORR; Singh R., 2015, CAV; Vedantam R, 2015, PROC CVPR IEEE, P4566, DOI 10.1109/CVPR.2015.7299087; Zhang WX, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P425	14	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302013
C	Alacaoglu, A; Tran-Dinh, Q; Fercoq, O; Cevher, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alacaoglu, Ahmet; Tran-Dinh, Quoc; Fercoq, Olivier; Cevher, Volkan			Smooth Primal-Dual Coordinate Descent Algorithms for Nonsmooth Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MINIMIZATION; EFFICIENCY; PARALLEL	We propose a new randomized coordinate descent method for a convex optimization template with broad applications. Our analysis relies on a novel combination of four ideas applied to the primal-dual gap function: smoothing, acceleration, homotopy, and coordinate descent with non-uniform sampling. As a result, our method features the first convergence rate guarantees among the coordinate descent methods, that are the best-known under a variety of common structure assumptions on the template. We provide numerical evidence to support the theoretical results with a comparison to state-of-the-art algorithms.	[Alacaoglu, Ahmet; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland; [Tran-Dinh, Quoc] Univ N Carolina, Dept Stat & Operat Res, Chapel Hill, NC USA; [Fercoq, Olivier] Univ Paris Saclay, LTCI, Telecom ParisTech, Paris, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of North Carolina; University of North Carolina Chapel Hill; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Alacaoglu, A (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.	ahmet.alacaoglu@epfl.ch; quoctd@email.unc.edu; olivier.fercoq@telecom-paristech.fr; volkan.cevher@epfl.ch	Jeong, Yongwook/N-7413-2016; Tran-Dinh, Quoc/AAX-8950-2020	Tran-Dinh, Quoc/0000-0002-1077-2579	public grant as part of the Investissement d'avenir project, LabEx LMH [ANR-11-LABX-0056-LMH]; NSF [DMS-1619884]; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [725594]	public grant as part of the Investissement d'avenir project, LabEx LMH(French National Research Agency (ANR)); NSF(National Science Foundation (NSF)); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	The work of O. Fercoq was supported by a public grant as part of the Investissement d'avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH. The work of Q. Tran-Dinh was partly supported by NSF grant, DMS-1619884, USA. The work of A. Alacaoglu and V. Cevher was supported by European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 725594 - time-data).	Vu BC, 2013, ADV COMPUT MATH, V38, P667, DOI 10.1007/s10444-011-9254-8; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Dohmatob Elvis D., 2014, P 2014 INT WORKSH PA, P1; Fercoq O., 2013, ARXIV13095885; Fercoq O., 2015, ARXIV150804625; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Guyon I., 2005, ADV NEURAL INFORM PR, V17, P545; Lee YT, 2013, ANN IEEE SYMP FOUND, P147, DOI 10.1109/FOCS.2013.24; Lichman M, 2013, UCI MACHINE LEARNING; Necoara I, 2016, SIAM J OPTIMIZ, V26, P197, DOI 10.1137/130950288; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360; Richtarik P, 2016, MATH PROGRAM, V156, P433, DOI 10.1007/s10107-015-0901-6; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Tran-Dinh Q., 2015, ARXIV150706243; Tseng P., 2008, SIAM J OPTIM UNPUB	25	6	6	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405090
C	Feizi, S; Javadi, H; Tse, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Feizi, Soheil; Javadi, Hamid; Tse, David			Tensor Biclustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Consider a dataset where data is collected on multiple features of multiple individuals over multiple times. This type of data can be represented as a three dimensional individual/feature/time tensor and has become increasingly prominent in various areas of science. The tensor biclustering problem computes a subset of individuals and a subset of features whose signal trajectories over time lie in a low-dimensional subspace, modeling similarity among the signal trajectories while allowing different scalings across different individuals or different features. We study the information-theoretic limit of this problem under a generative model. Moreover, we propose an efficient spectral algorithm to solve the tensor biclustering problem and analyze its achievability bound in an asymptotic regime. Finally, we show the efficiency of our proposed method in several synthetic and real datasets.	[Feizi, Soheil; Javadi, Hamid; Tse, David] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Feizi, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	sfeizi@stanford.edu; hrhakim@stanford.edu; dntse@stanford.edu	Jeong, Yongwook/N-7413-2016	Javadi, Hamid/0000-0003-4424-7120				Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Anandkumar A, 2014, J MACH LEARN RES, V15, P2239; Anandkumar Animashree, 2014, ARXIV14025180; [Anonymous], 2005, HDB COMPUTATIONAL MO; Ardlie KG, 2015, SCIENCE, V348, P648, DOI 10.1126/science.1262110; Bandeira A. S., 2016, ARXIV161207728; Chen R, 2012, CELL, V148, P1293, DOI 10.1016/j.cell.2012.02.009; Chen Y., 2014, ARXIV14021267; Hopkins Samuel B, 2015, ARXIV151202337; Hopkins SB., 2015, PROC 28 C LEARN THEO, P956; Hore V, 2016, NAT GENET, V48, P1094, DOI 10.1038/ng.3624; Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248; Lesieur T., 2017, ARXIV170108010; Montanari A., 2015, ADV NEURAL INFORM PR, P217; Rakhlin, 2015, COMPUTATIONAL STAT B; Richard E., 2014, P ADV NEUR INF PROC, P2897; Zhang A., 2017, ARXIV170302724; Zhong Y., 2017, ARXIV170306605	18	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401034
C	Hou, BJ; Zhang, LJ; Zhou, ZH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hou, Bo-Jian; Zhang, Lijun; Zhou, Zhi-Hua			Learning with Feature Evolvable Streams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Learning with streaming data has attracted much attention during the past few years. Though most studies consider data stream with fixed features, in real practice the features may be evolvable. For example, features of data gathered by limited-lifespan sensors will change when these sensors are substituted by new ones. In this paper, we propose a novel learning paradigm: Feature Evolvable Streaming Learning where old features would vanish and new features would occur. Rather than relying on only the current features, we attempt to recover the vanished features and exploit it to improve performance. Specifically, we learn two models from the recovered features and the current features, respectively. To benefit from the recovered features, we develop two ensemble methods. In the first method, we combine the predictions from two models and theoretically show that with the assistance of old features, the performance on new features can be improved. In the second approach, we dynamically select the best single prediction and establish a better performance guarantee when the best model switches. Experiments on both synthetic and real data validate the effectiveness of our proposal.	[Hou, Bo-Jian; Zhang, Lijun; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Hou, BJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	houbj@lamda.nju.edu.cn; zhanglj@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn	Jeong, Yongwook/N-7413-2016		NSFC [61333014, 61603177]; JiangsuSF [BK20160658]; Huawei Fund [YBN2017030027]; Collaborative Innovation Center of Novel Software Technology and Industrialization	NSFC(National Natural Science Foundation of China (NSFC)); JiangsuSF; Huawei Fund(Huawei Technologies); Collaborative Innovation Center of Novel Software Technology and Industrialization	This research was supported by NSFC (61333014, 61603177), JiangsuSF (BK20160658), Huawei Fund (YBN2017030027) and Collaborative Innovation Center of Novel Software Technology and Industrialization.	Aggarwal CC, 2006, IEEE T KNOWL DATA EN, V18, P577, DOI 10.1109/TKDE.2006.69; Aggarwal CC, 2010, SCIENTIFIC DATA MINING AND KNOWLEDGE DISCOVERY: PRINCIPLES AND FOUNDATIONS, P377, DOI 10.1007/978-3-642-02788-8_14; Amini M.R., 2009, ADV NEURAL INFORM PR, P28, DOI DOI 10.5555/2984093.2984097; Bifet A, 2010, J MACH LEARN RES, V11, P1601; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gaber MM, 2005, SIGMOD REC, V34, P18, DOI 10.1145/1083784.1083789; Gama J, 2009, STUD COMPUT INTELL, V206, P29; Guan SU, 2001, NEURAL PROCESS LETT, V14, P241, DOI 10.1023/A:1012799113953; Nguyen HL, 2015, KNOWL INF SYST, V45, P535, DOI 10.1007/s10115-014-0808-1; Hai-Long Nguyen, 2012, Advances in Knowledge Discovery and Data Mining. Proceedings 16th Pacific-Asia Conference (PAKDD 2012), P1, DOI 10.1007/978-3-642-30220-6_1; Hashemi S, 2009, IEEE T KNOWL DATA EN, V21, P624, DOI 10.1109/TKDE.2008.181; Herrmann, 2009, P 12 INT C EXT DAT T, P311; Hoi SCH, 2014, J MACH LEARN RES, V15, P495; Khalid S, 2014, 2014 SCIENCE AND INFORMATION CONFERENCE (SAI), P372, DOI 10.1109/SAI.2014.6918213; Kibria, 2007, TECHNOMETRICS, V49, P230; Leite DF, 2009, IEEE IJCNN, P2204, DOI 10.1109/IJCNN.2009.5178895; Li SY, 2014, AAAI CONF ARTIF INTE, P1968; Muslea I., 2002, INT C MACH LEARN, VVolume 2, P435; Oza NC, 2005, IEEE SYS MAN CYBERN, P2340; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pfahringer, 2011, P 2 WORKSH APPL PATT, P19; Raina R, 2007, 24 ANN INT C MACH LE, V227, P759, DOI [10.1145/1273496.1273592, DOI 10.1145/1273496.1273592]; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Tsang IW, 2007, P 24 INT C MACH LEAR, P911; [王衬 Wang Chen], 2016, [中国海洋药物, Chinese Journal of Marine Drugs], V35, P1; Wang H., 2003, P 9 ACM SIGKDD INT C, P226, DOI DOI 10.1145/956750.956778; Xu C., 2013, ARXIV; Zhang P., 2011, P 17 ACM SIGKDD INT, P177; Zhao PL, 2014, ARTIF INTELL, V216, P76, DOI 10.1016/j.artint.2014.06.003; Zhou G., 2012, 15 INT C ARTIFICIAL, P1453; Zhou Z.-H, 2012, ENSEMBLE METHODS FDN, DOI DOI 10.1201/B12207; Zhou ZH, 2016, FRONT COMPUT SCI-CHI, V10, P589, DOI 10.1007/s11704-016-6906-3; Zhou Zhi- Hua, 2016, ARXIV160509082; Zinkevich Martin, 2003, P INT C MACH LEARN, P928; [No title captured]	38	6	6	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401044
C	Jas, M; La Tour, TD; Simsekli, U; Gramfort, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jas, Mainak; La Tour, Tom Dupre; Simsekli, Umut; Gramfort, Alexandre			Learning the Morphology of Brain Signals Using Alpha-Stable Convolutional Sparse Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OSCILLATIONS; ALGORITHM	Neural time-series data contain a wide variety of prototypical signal waveforms (atoms) that are of significant importance in clinical and cognitive research. One of the goals for analyzing such data is hence to extract such 'shift-invariant' atoms. Even though some success has been reported with existing algorithms, they are limited in applicability due to their heuristic nature. Moreover, they are often vulnerable to artifacts and impulsive noise, which are typically present in raw neural recordings. In this study, we address these issues and propose a novel probabilistic convolutional sparse coding (CSC) model for learning shift-invariant atoms from raw neural signals containing potentially severe artifacts. In the core of our model, which we call alpha CSC, lies a family of heavy-tailed distributions called alpha-stable distributions. We develop a novel, computationally efficient Monte Carlo expectation-maximization algorithm for inference. The maximization step boils down to a weighted CSC problem, for which we develop a computationally efficient optimization algorithm. Our results show that the proposed algorithm achieves state-of-the-art convergence speeds. Besides, alpha CSC is significantly more robust to artifacts when compared to three competing algorithms: it can extract spike bursts, oscillations, and even reveal more subtle phenomena such as cross-frequency coupling when applied to noisy neural time series.	[Jas, Mainak; La Tour, Tom Dupre; Simsekli, Umut; Gramfort, Alexandre] Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France; [Gramfort, Alexandre] Univ Paris Saclay, INRIA, Saclay, France	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay; Inria; UDICE-French Research Universities; Universite Paris Saclay	Jas, M (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, Paris, France.		Jeong, Yongwook/N-7413-2016; la Tour, Tom Dupré/AAX-4266-2020; la Tour, Tom Dupre/ABH-9892-2020	la Tour, Tom Dupré/0000-0002-2674-1670; Jas, Mainak/0000-0002-3199-9027	French National Research Agency [ANR-14-NEUC-0002-01, ANR-13-CORD-0008-02, ANR-16-CE23-0014]; ERC [SLAB ERC-YStG-676943]	French National Research Agency(French National Research Agency (ANR)); ERC(European Research Council (ERC)European Commission)	The work was supported by the French National Research Agency grants ANR-14-NEUC-0002-01, ANR-13-CORD-0008-02, and ANR-16-CE23-0014 (FBIMATRIX), as well as the ERC Starting Grant SLAB ERC-YStG-676943.	Agarwal A., 2014, C LEARN THEOR, P123; [Anonymous], 2008, EUR SIGN PROC C; Barthelemy Q, 2013, J NEUROSCI METH, V215, P19, DOI 10.1016/j.jneumeth.2013.02.001; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Brockmeier AJ, 2016, IEEE T BIO-MED ENG, V63, P43, DOI 10.1109/TBME.2015.2499241; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; CHIB S, 1995, AM STAT, V49, P327, DOI 10.2307/2684568; Cohen MX, 2014, ISS CLIN COGN NEUROP, P1; Cole S. R., 2017, TRENDS COGN SCI; Dallerac G, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13920; Gips B, 2017, J NEUROSCI METH, V275, P66, DOI 10.1016/j.jneumeth.2016.11.001; Godsill S., 1999, P APPL HEAV TAIL DIS; Grosse R., 2007, P 23 C UNCERTAINTY A, P149; Hari R, 2017, MEG EEG PRIMER; Heide F, 2015, PROC CVPR IEEE, P5135, DOI 10.1109/CVPR.2015.7299149; Hitziger S., 2017, IEEE T SIGNAL PROCES; Huber P., 1981, ROBUST STAT; Jensen O, 2007, TRENDS COGN SCI, V11, P267, DOI 10.1016/j.tics.2007.05.003; Jones SR, 2016, CURR OPIN NEUROBIOL, V40, P72, DOI 10.1016/j.conb.2016.06.010; Jost P., 2006, ACOUSTICS SPEECH SIG, V5; Kavukcuoglu Koray, 2010, ADV NEURAL INFORM PR, V23, P1090; Kuruoglu E. E., 1999, THESIS; Leglaive S, 2017, INT CONF ACOUST SPEE, P576, DOI 10.1109/ICASSP.2017.7952221; Liu J. S., 2008, MONTE CARLO STRATEGI; Mandelbrot B.B., 2013, FRACTALS SCALING FIN, Ve; Mazaheri A, 2008, J NEUROSCI, V28, P7781, DOI 10.1523/JNEUROSCI.1631-08.2008; MOULINES E, 1995, IEEE T SIGNAL PROCES, V43, P516, DOI 10.1109/78.348133; Pachitariu M., 2013, ADV NEURAL INFORM PR; Samorodnitsky G., 1994, STABLE NONGAUSSIAN R, V1; Simsekli U, 2015, IEEE SIGNAL PROC LET, V22, P2289, DOI 10.1109/LSP.2015.2477535; Sorel M., 2016, DIGITAL SIGNAL PROCE; Tort ABL, 2010, J NEUROPHYSIOL, V104, P1195, DOI 10.1152/jn.00106.2010; Wang YM, 2016, J NEURAL ENG, V13, DOI 10.1088/1741-2560/13/5/056009; Wohlberg B, 2016, IEEE T IMAGE PROCESS, V25, P301, DOI 10.1109/TIP.2015.2495260; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	37	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401014
C	Kamp, M; Boley, M; Missura, O; Gartner, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kamp, Michael; Boley, Mario; Missura, Olana; Gartner, Thomas			Effective Parallelisation for Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS; COMPLEXITY	We present a novel parallelisation scheme that simplifies the adaptation of learning algorithms to growing amounts of data as well as growing needs for accurate and confident predictions in critical applications. In contrast to other parallelisation techniques, it can be applied to a broad class of learning algorithms without further mathematical derivations and without writing dedicated code, while at the same time maintaining theoretical performance guarantees. Moreover, our parallelisation scheme is able to reduce the runtime of many learning algorithms to polylogarithmic time on quasi-polynomially many processing units. This is a significant step towards a general answer to an open question on the efficient parallelisation of machine learning algorithms in the sense of Nick's Class (NC). The cost of this parallelisation is in the form of a larger sample complexity. Our empirical study confirms the potential of our parallelisation scheme with fixed numbers of processors and instances in realistic application scenarios.	[Kamp, Michael] Univ Bonn, Bonn, Germany; [Kamp, Michael] Fraunhofer IAIS, St Augustin, Germany; [Boley, Mario] Max Planck Inst Informat, Saarbrucken, Germany; [Boley, Mario] Saarland Univ, Saarbrucken, Germany; [Missura, Olana] Google Inc, Mountain View, CA USA; [Gartner, Thomas] Univ Nottingham, Nottingham, England	University of Bonn; Max Planck Society; Saarland University; Google Incorporated; University of Nottingham	Kamp, M (corresponding author), Univ Bonn, Bonn, Germany.; Kamp, M (corresponding author), Fraunhofer IAIS, St Augustin, Germany.	kamp@cs.uni-bonn.de; mboley@mpi-inf.mpg.de; olanam@google.com; thomas.gaertner@nottingham.ac.uk	Jeong, Yongwook/N-7413-2016; Kamp, Michael/ACC-1261-2022	Kamp, Michael/0000-0001-6231-0694; Boley, Mario/0000-0002-0704-4968	German Science Foundation (DFG) [GA 1615/1-1, GA 1615/2-1]	German Science Foundation (DFG)(German Research Foundation (DFG))	Part of this work was conducted while Mario Boley, Olana Missura, and Thomas Gartner were at the University of Bonn and partially funded by the German Science Foundation (DFG, under ref. GA 1615/1-1 and GA 1615/2-1). The authors would like to thank Dino Oglic, Graham Hutton, Roderick MacKenzie, and Stefan Wrobel for valuable discussions and comments.	Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090; Balcan MF, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P725, DOI 10.1145/2939672.2939796; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BLUM M, 1967, J ACM, V14, P322, DOI 10.1145/321386.321395; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Clarkson KL, 1996, INT J COMPUT GEOM AP, V6, P357, DOI 10.1142/S021819599600023X; Cook S.A., 1979, P STOC 1979, P338; Dekel O, 2012, J MACH LEARN RES, V13, P165; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Freund Yoav, 2001, P 8 INT WORKSH ART I; Greenlaw R., 1995, LIMITS PARALLEL COMP; Hanneke S, 2016, J MACH LEARN RES, V17; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; KAY DC, 1971, PAC J MATH, V38, P471, DOI 10.2140/pjm.1971.38.471; KRUSKAL CP, 1990, THEOR COMPUT SCI, V71, P95, DOI 10.1016/0304-3975(90)90192-K; Kumar V., 1994, INTRO PARALLEL COMPU, P110; Lichman M, 2013, UCI MACHINE LEARNING; Lin SB, 2017, J MACH LEARN RES, V18; Long PM, 2013, J MACH LEARN RES, V14, P3105; Ma C, 2017, OPTIM METHOD SOFTW, V32, P813, DOI 10.1080/10556788.2016.1278445; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Meng X., 2016, J MACH LEARN RES, V17, P1235, DOI DOI 10.1145/2882903.2912565; Moler C, 1986, HYPERCUBE MULTIPROCE, V86, P31; Nouretdinov I, 2011, NEUROIMAGE, V56, P809, DOI 10.1016/j.neuroimage.2010.05.023; Oglic D, 2017, PR MACH LEARN RES, V70; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Radon J, 1921, MATH ANN, V83, P113, DOI 10.1007/BF01464231; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rosenblatt JD, 2016, INF INFERENCE, V5, P379, DOI 10.1093/imaiai/iaw013; Rubinov AM., 2013, ABSTRACT CONVEXITY G, V44; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Shamir O, 2014, ANN ALLERTON CONF, P850, DOI 10.1109/ALLERTON.2014.7028543; Sommer R, 2010, P IEEE S SECUR PRIV, P305, DOI 10.1109/SP.2010.25; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; Stockmeyer L.J, 1976, 17 ANN S FDN COMP SC, P98; Tukey J., 1975, P INT C MATH, V2; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; VITTER JS, 1992, INFORM COMPUT, V96, P179, DOI 10.1016/0890-5401(92)90047-J; von Luxburg U, 2011, HBK HIST LOGIC, V10, P651; Witten IH, 2017, DATA MINING: PRACTICAL MACHINE LEARNING TOOLS AND TECHNIQUES, 4TH EDITION, pCP1; Zhang YC, 2013, J MACH LEARN RES, V14, P3321; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	46	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406053
C	Kanitscheider, I; Fiete, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kanitscheider, Ingmar; Fiete, Ila			Training recurrent networks to generate hypotheses about how the brain solves hard navigation problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				HIPPOCAMPAL PLACE CELLS; SPATIAL MAP; DYNAMICS; CONTEXT	Self-localization during navigation with noisy sensors in an ambiguous world is computationally challenging, yet animals and humans excel at it. In robotics, Simultaneous Location and Mapping (SLAM) algorithms solve this problem through joint sequential probabilistic inference of their own coordinates and those of external spatial landmarks. We generate the first neural solution to the SLAM problem by training recurrent LSTM networks to perform a set of hard 2D navigation tasks that require generalization to completely novel trajectories and environments. Our goal is to make sense of how the diverse phenomenology in the brain's spatial navigation circuits is related to their function. We show that the hidden unit representations exhibit several key properties of hippocampal place cells, including stable tuning curves that remap between environments. Our result is also a proof of concept for end-to-end-learning of a SLAM algorithm using recurrent networks, and a demonstration of why this approach may have some advantages for robotic SLAM.	[Kanitscheider, Ingmar; Fiete, Ila] Univ Texas Austin, Dept Neurosci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Kanitscheider, I (corresponding author), Univ Texas Austin, Dept Neurosci, Austin, TX 78712 USA.	ikanitscheider@mail.clm.utexas.edu; ilafiete@mail.clm.utexas.edu	Jeong, Yongwook/N-7413-2016	Fiete, Ila/0000-0003-4738-2539	NSF [CRCNS 26-1004-04xx]; HFSP award [26-6302-87]; Simons Foundation through the Simons Collaboration on the Global Brain	NSF(National Science Foundation (NSF)); HFSP award; Simons Foundation through the Simons Collaboration on the Global Brain	This work is supported by the NSF (CRCNS 26-1004-04xx), an HFSP award to IRF (26-6302-87), and the Simons Foundation through the Simons Collaboration on the Global Brain. The authors acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin (URL: http://www.tacc.utexas.edu) for providing HPC resources that have contributed to the research results reported within this paper.	Burak Y, 2012, P NATL ACAD SCI USA, V109, P17645, DOI 10.1073/pnas.1117386109; Buzsaki G, 2014, NAT REV NEUROSCI, V15, P264, DOI 10.1038/nrn3687; Chen Zetao, 2014, CORR; Cheung A, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002651; Fiser J, 2010, TRENDS COGN SCI, V14, P119, DOI 10.1016/j.tics.2010.01.003; Forster Alexander, 2007, 15TH EUROPEAN SYMPOS, P537; Graves A, 2013, ARXIV13080850; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; Hayman RM, 2008, HIPPOCAMPUS, V18, P1301, DOI 10.1002/hipo.20513; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Lin, 2015, ARXIV150204156; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Marblestone A. H., 2016, ARXIV160603813; Milford M, 2010, INT J ROBOT RES, V29, P1131, DOI 10.1177/0278364909340592; Muller Robert U., 1991, SPATIAL FIRING CORRE, P296; MULLER RU, 1987, J NEUROSCI, V7, P1951; MULLER RU, 1994, J NEUROSCI, V14, P7235; OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1; OKEEFE J, 1979, BEHAV BRAIN SCI, V2, P487, DOI 10.1017/S0140525X00063949; OKEEFE J, 1978, EXP BRAIN RES, V31, P573; Rubin A, 2014, J NEUROSCI, V34, P1067, DOI 10.1523/JNEUROSCI.5393-12.2014; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Save E, 2000, HIPPOCAMPUS, V10, P64, DOI 10.1002/(SICI)1098-1063(2000)10:1<64::AID-HIPO7>3.0.CO;2-Y; Smith DM, 2006, HIPPOCAMPUS, V16, P716, DOI 10.1002/hipo.20208; Sunderhauf N, 2015, IEEE INT C INT ROBOT, P4297, DOI 10.1109/IROS.2015.7353986; Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754; WILSON MA, 1993, SCIENCE, V261, P1055, DOI 10.1126/science.8351520; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zhang Jingwei, 2017, ARXIV170609520	30	6	6	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404058
C	Lin, K; Sharpnack, J; Rinaldo, A; Tibshirani, RJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lin, Kevin; Sharpnack, James; Rinaldo, Alessandro; Tibshirani, Ryan J.			A Sharp Error Analysis for the Fused Lasso, with Application to Approximate Changepoint Screening	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In the 1-dimensional multiple changepoint detection problem, we derive a new fast error rate for the fused lasso estimator, under the assumption that the mean vector has a sparse number of changepoints. This rate is seen to be suboptimal (compared to the minimax rate) by only a factor of log log n. Our proof technique is centered around a novel construction that we call a lower interpolant. We extend our results to misspecified models and exponential family distributions. We also describe the implications of our error analysis for the approximate screening of changepoints.	[Lin, Kevin; Rinaldo, Alessandro; Tibshirani, Ryan J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Sharpnack, James] Univ Calif Davis, Davis, CA 95616 USA	Carnegie Mellon University; University of California System; University of California Davis	Lin, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kevin11@andrew.cmu.edu; jsharpna@ucdavis.edu; arinaldo@stat.cmu.edu; ryantibs@stat.cmu.edu			NSF [DMS-1554123, DMS-1712996]	NSF(National Science Foundation (NSF))	JS was supported by NSF Grant DMS-1712996. RT was supported by NSF Grant DMS-1554123.	Aston JAD, 2012, ANN APPL STAT, V6, P1906, DOI 10.1214/12-AOAS565; Boysen L, 2009, ANN STAT, V37, P157, DOI 10.1214/07-AOS558; Chan NH, 2014, J AM STAT ASSOC, V109, P590, DOI 10.1080/01621459.2013.866566; Chen J., 2000, PARAMETRIC STAT CHAN, DOI 10.1007/978-1-4757-3131-6; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Dumbgen L, 2008, ANN STAT, V36, P1758, DOI 10.1214/07-AOS521; Eckley I. A., 2011, BAYESIAN TIME SERIES, P205, DOI 10.1017/CBO9780511984679.011; Fryzlewicz P, 2007, J AM STAT ASSOC, V102, P1318, DOI 10.1198/016214507000000860; Fryzlewicz Piotr, 2016, TAIL GREEDY BOTTOM U; Guntuboyina Adityanand, 2017, ARXIV1702050113; Hernan Oscar, 2016, ARXIV160803384; Johnstone, 2015, GAUSSIAN ESTIMATION; Kim SJ, 2009, SIAM REV, V51, P339, DOI 10.1137/070690274; Mammen E, 1997, ANN STAT, V25, P387; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Steidl G, 2006, INT J COMPUT VISION, V70, P241, DOI 10.1007/s11263-006-8066-7; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 2008, BIOSTATISTICS, V9, P18, DOI 10.1093/biostatistics/kxm013; Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189	21	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406091
C	Mazumdar, A; Saha, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mazumdar, Arya; Saha, Barna			Query Complexity of Clustering with Side Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Suppose, we are given a set of n elements to be clustered into k (unknown) clusters, and an oracle/expert labeler that can interactively answer pair-wise queries of the form, "do two elements u and v belong to the same cluster?". The goal is to recover the optimum clustering by asking the minimum number of queries. In this paper, we provide a rigorous theoretical study of this basic problem of query complexity of interactive clustering, and give strong information theoretic lower bounds, as well as nearly matching upper bounds. Most clustering problems come with a similarity matrix, which is used by an automated process to cluster similar points together. However, obtaining an ideal similarity function is extremely challenging due to ambiguity in data representation, poor data quality etc., and this is one of the primary reasons that makes clustering hard. To improve accuracy of clustering, a fruitful approach in recent years has been to ask a domain expert or crowd to obtain labeled data interactively. Many heuristics have been proposed, and all of these use a similarity function to come up with a querying strategy. Even so, there is a lack systematic theoretical study. Our main contribution in this paper is to show the dramatic power of side information aka similarity matrix on reducing the query complexity of clustering. A similarity matrix represents noisy pair-wise relationships such as one computed by some function on attributes of the elements. A natural noisy model is where similarity values are drawn independently from some arbitrary probability distribution f(+) when the underlying pair of elements belong to the same cluster, and from some f(-) otherwise. We show that given such a similarity matrix, the query complexity reduces drastically from circle minus(nk) (no similarity matrix) to O(k(2)log n/H-2(f(+)parallel to f(-)) where H-2 denotes the squared Hellinger divergence. Moreover, this is also information-theoretic optimal within an O(log n) factor. Our algorithms are all efficient, and parameter free, i.e., they work without any knowledge of k, f(+) and f(-), and only depend logarithmically with n. Our lower bounds could be of independent interest, and provide a general framework for proving lower bounds for classification problems in the interactive setting. Along the way, our work also reveals intriguing connection to popular community detection models such as the stochastic block model and opens up many avenues for interesting future research.	[Mazumdar, Arya; Saha, Barna] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Mazumdar, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	arya@cs.umass.edu; barna@cs.umass.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303]; Yahoo ACE Award; Google Faculty Research Award	NSF(National Science Foundation (NSF)); Yahoo ACE Award; Google Faculty Research Award(Google Incorporated)	This work is supported in part by NSF awards CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303, a Yahoo ACE Award and a Google Faculty Research Award. We are particularly thankful to an anonymous reviewer whose comments led to notable improvement of the presentation of the paper.	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; [Anonymous], 2016, ARXIV160401839; Ashtiani H., 2016, NIPS; Awasthi P, 2014, PR MACH LEARN RES, V32, P550; Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27; BOLLOBAS B, 1990, SIAM J DISCRETE MATH, V3, P21, DOI 10.1137/0403003; Chaudhuri Kamalika, 2012, J MACHINE LEARNING R, P35; CHEN Y., 2016, INT C MACH LEARN, P689; Chin P., 2015, ARXIV150105021; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066; DYER ME, 1989, J ALGORITHM, V10, P451, DOI 10.1016/0196-6774(89)90001-1; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; FELLEGI IP, 1969, J AM STAT ASSOC, V64, P1183, DOI 10.2307/2286061; Firmani D, 2016, PROC VLDB ENDOW, V9, P384; Gadde A, 2016, IEEE INT SYMP INFO, P1889, DOI 10.1109/ISIT.2016.7541627; Getoor L, 2012, PROC VLDB ENDOW, V5, P2018, DOI 10.14778/2367502.2367564; Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599; Gokhale C, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P601; Guntuboyina A, 2011, IEEE T INFORM THEORY, V57, P2386, DOI 10.1109/TIT.2011.2110791; HAJEK B., 2015, C LEARNING THEORY, P899; Hajek B, 2016, IEEE T INFORM THEORY, V62, P2788, DOI 10.1109/TIT.2016.2546280; HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Kopcke H, 2010, PROC VLDB ENDOW, V3, P484; Lim Shiau Hong, 2014, ADV NEURAL INFORM PR, P1188; Mazumdar A., 2017, ADV NEURAL INFORM PR, V31; Mazumdar A., 2017, 31 AAAI C ART INT AA; Mossel E, 2015, ACM S THEORY COMPUT, P69, DOI 10.1145/2746539.2746603; Polyanskiy Y., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1327, DOI 10.1109/ALLERTON.2010.5707067; Sason I, 2016, IEEE T INFORM THEORY, V62, P5973, DOI 10.1109/TIT.2016.2603151; Shi F, 2013, 2013 INTERNATIONAL CONFERENCE ON MANAGEMENT AND INFORMATION TECHNOLOGY, P299; Verroios V, 2015, PROC INT CONF DATA, P219, DOI 10.1109/ICDE.2015.7113286; Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982; Vinayak R.K., 2016, ADV NEURAL INFORM PR, P1316; Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263; [No title captured]	42	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404073
C	McAllister, RT; Rasmussen, CE		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		McAllister, Rowan Thomas; Rasmussen, Carl Edward			Data-Efficient Reinforcement Learning in Continuous State-Action Gaussian-POMDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a data-efficient reinforcement learning method for continuous state-action systems under significant observation noise. Data-efficient solutions under small noise exist, such as PILCO which learns the cartpole swing-up task in 30s. PILCO evaluates policies by planning state-trajectories using a dynamics model. However, PILCO applies policies to the observed state, therefore planning in observation space. We extend PILCO with filtering to instead plan in belief space, consistent with partially observable Markov decisions process (POMDP) planning. This enables data-efficient learning under significant observation noise, outperforming more naive methods such as post-hoc application of a filter to policies optimised by the original (unfiltered) PILCO algorithm. We test our method on the cartpole swing-up task, which involves nonlinear dynamics and requires nonlinear control.	[McAllister, Rowan Thomas; Rasmussen, Carl Edward] Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	McAllister, RT (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.	rtm26@cam.ac.uk; cer54@cam.ac.uk	Jeong, Yongwook/N-7413-2016					Candela JQ, 2003, INT CONF ACOUST SPEE, P701; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Dallaire P, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P2604, DOI 10.1109/IROS.2009.5354013; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth Marc, 2012, EUR WORKSH REINF LEA; Duff M, 2002, THESIS; Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x; Lillicrap TP, 2016, 4 INT C LEARN REPR; McHutchon A., 2014, THESIS; Poupart P., 2006, ICML, P697; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ross S, 2008, IEEE INT CONF ROBOT, P2845, DOI 10.1109/ROBOT.2008.4543641; van den Berg Jur, 2012, ASS ADVANCEMENT ARTI; Webb DJ, 2014, IEEE INT CONF ROBOT, P5998, DOI 10.1109/ICRA.2014.6907743	14	6	6	3	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402009
C	Pennington, J; Worah, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pennington, Jeffrey; Worah, Pratik			Nonlinear random matrix theory for deep learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Neural network configurations with random weights play an important role in the analysis of deep learning. They define the initial loss landscape and are closely related to kernel and random feature methods. Despite the fact that these networks are built out of random matrices, the vast and powerful machinery of random matrix theory has so far found limited success in studying them. A main obstacle in this direction is that neural networks are nonlinear, which prevents the straightforward utilization of many of the existing mathematical results. In this work, we open the door for direct applications of random matrix theory to deep learning by demonstrating that the pointwise nonlinearities typically applied in neural networks can be incorporated into a standard method of proof in random matrix theory known as the moments method. The test case for our study is the Gram matrix (YY)-Y-T, Y = f(W X), where W is a random weight matrix, X is a random data matrix, and f is a pointwise nonlinear activation function. We derive an explicit representation for the trace of the resolvent of this matrix, which defines its limiting spectral distribution. We apply these results to the computation of the asymptotic performance of single-layer random feature networks on a memorization task and to the analysis of the eigenvalues of the data covariance matrix as it propagates through a neural network. As a byproduct of our analysis, we identify an intriguing new class of activation functions with favorable properties.	[Pennington, Jeffrey] Google Brain, Mountain View, CA 94043 USA; [Worah, Pratik] Google Res, Mountain View, CA USA	Google Incorporated; Google Incorporated	Pennington, J (corresponding author), Google Brain, Mountain View, CA 94043 USA.	jpennin@google.com; pworah@google.com	Jeong, Yongwook/N-7413-2016	Worah, Pratik/0000-0002-2739-1246				AMIT DJ, 1985, PHYS REV A, V32, P1007, DOI 10.1103/PhysRevA.32.1007; [Anonymous], 1994, THESIS U TORONTO TOR; [Anonymous], 2014, INT C LEARN REPR; Cheng XY, 2013, RANDOM MATRICES-THEO, V2, DOI 10.1142/S201032631350010X; Choromanska A., 2015, AISTATS; Daniely A., 2016, ARXIV160205897; Dupic T., 2014, ARXIV14017802; El Karoui N, 2010, ANN STAT, V38, P1, DOI 10.1214/08-AOS648; GARDNER E, 1988, J PHYS A-MATH GEN, V21, P271, DOI 10.1088/0305-4470/21/1/031; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Louart C., 2017, ARXIV170205419; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; Neal R.M., 1994, CRGTR941 U TOR; Oord A. v. d., 2016, ARXIV160903499; Pennington J., 2017, ADV NEURAL INFORM PR; Poole B., 2016, ARXIV160605340; Raghu M., 2016, ARXIV160605336; Rahimi Ali, 2007, NEURAL INFOMRATION P; Schoenholz S. S., 2017, ARXIV E PRINTS; Schoenholz S. S., 2016, ARXIV E PRINTS; Shazeer N., 2017, ICLR; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144	25	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402067
C	Premont-Schwarz, I; Ilin, A; Hao, TH; Rasmus, A; Boney, R; Valpola, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Premont-Schwarz, Isabeau; Ilin, Alexander; Hao, Tele Hotloo; Rasmus, Antti; Boney, Rinu; Valpola, Harri			Recurrent Ladder Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a recurrent extension of the Ladder networks [22] whose structure is motivated by the inference required in hierarchical latent variable models. We demonstrate that the recurrent Ladder is able to handle a wide variety of complex learning tasks that benefit from iterative inference and temporal modeling. The architecture shows close-to-optimal results on temporal modeling of video data, competitive results on music modeling, and improved perceptual grouping based on higher order abstractions, such as stochastic textures and motion cues. We present results for fully supervised, semi-supervised, and unsupervised tasks. The results suggest that the proposed architecture and principles are powerful tools for learning a hierarchy of abstractions, learning iterative inference and handling temporal information.	[Premont-Schwarz, Isabeau; Ilin, Alexander; Hao, Tele Hotloo; Rasmus, Antti; Boney, Rinu; Valpola, Harri] Curious AI Co, Helsinki, Finland		Premont-Schwarz, I (corresponding author), Curious AI Co, Helsinki, Finland.	isabeau@cai.fi; alexilin@cai.fi; hotloo@cai.fi; antti@cai.fi; rinu@cai.fi; harri@cai.fi						Alain G., 2012, ABS12114246 CORR; Arponen H., 2017, ARXIV170902797; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Berglund M., 2015, ADV NEURAL INFORM PR; Bishop CM, 2006, PATTERN RECOGNITION; Boulanger-Lewandowski N, 2012, P 29 INT C MACH LEAR, P1159; Brodatz P., 1966, TEXTURES PHOTOGRAPHI; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cricri F., 2016, ABS161201756 CORR; Eyjolfsdottir E., 2016, ARXIV161100094; Greff K., 2017, ICLR WORKSH; Greff K., 2015, ABS151106418 CORR; Greff K, 2016, ADV NEUR IN, V29; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Johnson D. D., 2017, INT C EV BIOL INSP M; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Laukien E, 2016, ARXIV160903971; Newell A, 2016, PROCEEDINGS OF THE ELEVENTH EUROPEAN CONFERENCE ON COMPUTER SYSTEMS, (EUROSYS 2016), DOI 10.1145/2901318.2901343; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Springenberg J.T., 2014, ARXIV14126806; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Tarvainen Antti, 2017, CORR, Vabs/1703; Tietz M., 2017, INT C ART NEUR NETW; Valpola H., 2015, ADV INDEPENDENT COMP; Vinh NX, 2010, J MACH LEARN RES, V11, P2837	29	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406009
C	Qin, C; Klabjan, D; Russo, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Qin, Chao; Klabjan, Diego; Russo, Daniel			Improving the Expected Improvement Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				EFFICIENT GLOBAL OPTIMIZATION; CONVERGENCE-RATES	The expected improvement (EI) algorithm is a popular strategy for information collection in optimization under uncertainty. The algorithm is widely known to be too greedy, but nevertheless enjoys wide use due to its simplicity and ability to handle uncertainty and noise in a coherent decision theoretic framework. To provide rigorous insight into EI, we study its properties in a simple setting of Bayesian optimization where the domain consists of a finite grid of points. This is the so-called best-arm identification problem, where the goal is to allocate measurement effort wisely to confidently identify the best arm using a small number of measurements. In this framework, one can show formally that EI is far from optimal. To overcome this shortcoming, we introduce a simple modification of the expected improvement algorithm. Surprisingly, this simple change results in an algorithm that is asymptotically optimal for Gaussian best-arm identification problems, and provably outperforms standard EI by an order of magnitude.	[Qin, Chao; Russo, Daniel] Columbia Business Sch, New York, NY 10027 USA; [Klabjan, Diego] Northwestern Univ, Evanston, IL 60208 USA	Columbia University; Northwestern University	Qin, C (corresponding author), Columbia Business Sch, New York, NY 10027 USA.	cqin22@gsb.columbia.edu; d-klabjan@northwestern.edu; djr2174@gsb.columbia.edu	Klabjan, Diego/B-7469-2009; Jeong, Yongwook/N-7413-2016					Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Bull AD, 2011, J MACH LEARN RES, V12, P2879; Chen CH, 2000, DISCRETE EVENT DYN S, V10, P251, DOI 10.1023/A:1008349927281; CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Frazier PI, 2008, SIAM J CONTROL OPTIM, V47, P2410, DOI 10.1137/070693424; Gabillon V., 2012, ADV NEURAL INFORM PR, P3212; Garivier A., 2016, C LEARN THEOR, P998; Glynn Peter, 2004, SIM C 2004 P 2004 WI, V1; Jamieson K., 2014, C LEARN THEOR, P423; Jennison C., 1982, STAT DECISION THEORY, V2, P55; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kaufmann E., 2014, C LEARN THEOR PMLR, P461; Kaufmann E., 2013, C LEARNING THEORY, P228; Kaufmann E, 2016, J MACH LEARN RES, V17; Mannor Shie, 2004, J MACHINE LEARNING R, V5, P2004; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; Russo D., 2016, C LEARNING THEORY, P1417; Ryzhov IO, 2016, OPER RES, V64, P1515, DOI 10.1287/opre.2016.1494; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Soare M., 2014, ARXIV14096110	22	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405045
C	Sadhanala, V; Wang, YX; Sharpnack, J; Tibshirani, RJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang; Sharpnack, James; Tibshirani, Ryan J.			Higher-Order Total Variation Classes on Grids: Minimax Theory and Trend Filtering Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				TOTAL VARIATION MINIMIZATION; FUSED LASSO; REGRESSION; ALGORITHM; PATH	We consider the problem of estimating the values of a function over n nodes of a d-dimensional grid graph (having equal side lengths n(1/d)) from noisy observations. The function is assumed to be smooth, but is allowed to exhibit different amounts of smoothness at different regions in the grid. Such heterogeneity eludes classical measures of smoothness from nonparametric statistics, such as Holder smoothness. Meanwhile, total variation (TV) smoothness classes allow for heterogeneity, but are restrictive in another sense: only constant functions count as perfectly smooth (achieve zero TV). To move past this, we define two new higher-order TV classes, based on two ways of compiling the discrete derivatives of a parameter across the nodes. We relate these two new classes to Holder classes, and derive lower bounds on their minimax errors. We also analyze two naturally associated trend filtering methods; when d = 2, each is seen to be rate optimal over the appropriate class.	[Sadhanala, Veeranjaneyulu; Wang, Yu-Xiang; Tibshirani, Ryan J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Wang, Yu-Xiang] Amazon AI, Palo Alto, CA 94303 USA; [Sharpnack, James] Univ Calif Davis, Davis, CA 95616 USA	Carnegie Mellon University; University of California System; University of California Davis	Sadhanala, V (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	vsadhana@cs.cmu.edu; yuxiangw@amazon.com; jsharpna@ucdavis.edu; ryantibs@stat.cmu.edu	Jeong, Yongwook/N-7413-2016	Wang, Yu-Xiang/0000-0002-6403-212X				Barbero A lvaro, 2014, MODULAR PROXIMAL OPT; Bogoya JM, 2016, LINEAR ALGEBRA APPL, V493, P606, DOI 10.1016/j.laa.2015.12.017; Bredies K, 2010, SIAM J IMAGING SCI, V3, P492, DOI 10.1137/090769521; Chambolle A, 1997, NUMER MATH, V76, P167, DOI 10.1007/s002110050258; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chambolle A, 2009, INT J COMPUT VISION, V84, P288, DOI 10.1007/s11263-009-0238-9; Condat Laurent, 2012, HAL00675043; Donoho DL, 1998, ANN STAT, V26, P879; Harchaoui Z, 2010, J AM STAT ASSOC, V105, P1480, DOI 10.1198/jasa.2010.tm09181; Hoefling H, 2010, J COMPUT GRAPH STAT, V19, P984, DOI 10.1198/jcgs.2010.09208; Johnson NA, 2013, J COMPUT GRAPH STAT, V22, P246, DOI 10.1080/10618600.2012.681238; Kim SJ, 2009, SIAM REV, V51, P339, DOI 10.1137/070690274; Korostelev Aleksandr P., 2003, MINIMAX THEORY IMAGE; Kovac A, 2011, J COMPUT GRAPH STAT, V20, P432, DOI 10.1198/jcgs.2011.09203; Mammen E, 1997, ANN STAT, V25, P387; Padilla Oscar Hernan Madrid, 2016, ARXIV160803384; Poschl C, 2008, CONTEMP MATH, V451, P219; Rinaldo A, 2009, ANN STAT, V37, P2922, DOI 10.1214/08-AOS665; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sadhanala V, 2016, ADV NEUR IN, V29; Sadhanala Veeranjaneyulu, 2017, ARXIV170205037; Sharpnack James, 2012, INT C ART INT STAT, V15; Steidl G, 2006, INT J COMPUT VISION, V70, P241, DOI 10.1007/s11263-006-8066-7; Tansey Wesley, 2015, ARXIV150506475; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani RJ, 2014, ANN STAT, V42, P285, DOI 10.1214/13-AOS1189; Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Wang Y.-X., 2014, INT C MACH LEARN, V31; Wang YJ, 2016, J MACH LEARN RES, V17, P1	31	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405085
C	Shi, Z; Zhang, XH; Yu, YL		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shi, Zhan; Zhang, Xinhua; Yu, Yaoliang			Bregman Divergence for Stochastic Variance Reduction: Saddle-Point and Adversarial Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MINIMIZATION	Adversarial machines, where a learner competes against an adversary, have regained much recent interest in machine learning. They are naturally in the form of saddle-point optimization, often with separable structure but sometimes also with unmanageably large dimension. In this work we show that adversarial prediction under multivariate losses can be solved much faster than they used to be. We first reduce the problem size exponentially by using appropriate sufficient statistics, and then we adapt the new stochastic variance-reduced algorithm of Balamurugan & Bach (2016) to allow any Bregman divergence. We prove that the same linear rate of convergence is retained and we show that for adversarial prediction using KL-divergence we can further achieve a speedup of #example times compared with the Euclidean alternative. We verify the theoretical findings through extensive experiments on two example applications: adversarial prediction and LPboosting.	[Shi, Zhan; Zhang, Xinhua] Univ Illinois, Chicago, IL 60661 USA; [Yu, Yaoliang] Univ Waterloo, Waterloo, ON N2L 3G1, Canada	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital; University of Waterloo	Shi, Z (corresponding author), Univ Illinois, Chicago, IL 60661 USA.	zshi22@uic.edu; zhangx@uic.edu; yaoliang.yu@uwaterloo.ca						[Anonymous], 2015, NIPS; Asif K., 2015, UAI; Babanezhad R., 2015, NIPS; Balamurugan P., 2016, NIPS; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10; DEFAZIO A, 2014, NIPS; Duchi J. C., 2010, P ANN C COMP LEARN T; Farnia F., 2016, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; JOHNSON R., 2013, NIPS; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nitanda, 2014, ADV NEURAL INFORM PR; Rockafellar R.T., 1970, NONLINEAR FUNCTIONAL, V18, P397; Schmidt M., 2016, MATH PROGRAMMING; Shalev-Shwartz, 2014, ICML; SHALEVSHWARTZ S, 2016, ICML; Tseng P., 2009, SIAM J OPTIMIZ UNPUB; Wang H., 2014, NIPS; Wang H., 2015, NIPS; Warmuth MK, 2008, LECT NOTES ARTIF INT, V5254, P256, DOI 10.1007/978-3-540-87987-9_23; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; ZHANG Y, 2015, ICML; Zhu ZX, 2015, LECT NOTES ARTIF INT, V9284, P645, DOI 10.1007/978-3-319-23528-8_40; [No title captured]	30	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406011
C	Siddharth, N; Paige, B; van de Meent, JW; Desmaison, A; Goodman, ND; Kohli, P; Wood, F; Torr, PHS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Siddharth, N.; Paige, Brooks; van de Meent, Jan-Willem; Desmaison, Alban; Goodman, Noah D.; Kohli, Pushmeet; Wood, Frank; Torr, Philip H. S.			Learning Disentangled Representations with Semi-Supervised Deep Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Variational autoencoders (VAEs) learn representations of data by jointly training a probabilistic encoder and decoder network. Typically these models encode all features of the data into a single variable. Here we are interested in learning disentangled representations that encode distinct aspects of the data into separate variables. We propose to learn such representations using model architectures that generalise from standard VAEs, employing a general graphical model structure in the encoder and decoder. This allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable variables and rely on the flexibility of neural networks to learn representations for the remaining variables. We further define a general objective for semi-supervised learning in this model class, which can be approximated using an importance sampling procedure. We evaluate our framework's ability to learn disentangled representations, both by qualitative exploration of its generative capacity, and quantitative evaluation of its discriminative ability on a variety of models and datasets.	[Siddharth, N.; Desmaison, Alban; Wood, Frank; Torr, Philip H. S.] Univ Oxford, Oxford, England; [Paige, Brooks] Univ Cambridge, Alan Turing Inst, Cambridge, England; [van de Meent, Jan-Willem] Northeastern Univ, Boston, MA 02115 USA; [Goodman, Noah D.] Stanford Univ, Stanford, CA 94305 USA; [Kohli, Pushmeet] Deepmind, London, England	University of Oxford; University of Cambridge; Northeastern University; Stanford University	Siddharth, N (corresponding author), Univ Oxford, Oxford, England.	nsid@robots.ox.ac.uk; bpaige@turing.ac.uk; j.vandemeent@northeastern.edu; alban@robots.ox.ac.uk; ngoodman@stanford.edu; pushmeet@google.com; fwood@robots.ox.ac.uk; philip.torr@eng.ox.ac.uk	Jeong, Yongwook/N-7413-2016		EPSRC [EP/M013774/1]; ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC/MURI [EP/N019474/1]; Alan Turing Institute under the EPSRC [EP/N510129/1]; DARPA PPAML through the U.S. AFRL [FA8750-14-2-0006]; Northeastern University; Intel; DARPA D3M [FA8750-17-2-0093]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ERC(European Research Council (ERC)European Commission); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); DARPA PPAML through the U.S. AFRL; Northeastern University; Intel(Intel Corporation); DARPA D3M	This work was supported by the EPSRC, ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1, and EPSRC/MURI grant EP/N019474/1. BP & FW were supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. JWM, FW & NDG were supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006. JWM was additionally supported through startup funds provided by Northeastern University. FW was additionally supported by Intel and DARPA D3M, under Cooperative Agreement FA8750-17-2-0093.	Berant J., 2014, EMNLP; Burda Yuri, 2015, ICLR; Christopher K. I., 2017, P WORKSH LEARN DIS R; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Eslami S., 2016, ARXIV160308575; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Gershman Samuel, 2014, COGSCI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodman N. D., 2008, P 24 C UNCERTAINTY A, P220; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Jampani V, 2015, JMLR WORKSH CONF PRO, V38, P425; Jang Eric, 2017, P 5 INT C LEARN REPR; Johnson Matthew J., 2016, ADV NEURAL INFORM PR; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Kingma D.P, P 3 INT C LEARNING R; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Koller D., 2009, PROBABILISTIC GRAPHI; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; Kulkarni Tejas D, 2015, ADV NEURAL INFORM PR, P2530; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; LeAnh Tuan, 2016, ARXIV161009900; Maaloe L., 2016, ARXIV160205473; Maddison Chris J, 2016, ARXIV161100712; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Ritchie D., 2016, ARXIV161005735; Schull J, 2015, ASSETS'15: PROCEEDINGS OF THE 17TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS & ACCESSIBILITY, P1, DOI 10.1145/2700648.2809870; Siddharth N, 2014, PROC CVPR IEEE, P732, DOI 10.1109/CVPR.2014.99; Sonderby C. K., 2016, ADV NEURAL INFORM PR; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Wingate D., 2011, J MACH LEARN RES, V15, P770; Wood F, 2014, JMLR WORKSH CONF PRO, V33, P1024	34	6	6	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406001
C	Tian, K; Kong, WH; Valiant, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tian, Kevin; Kong, Weihao; Valiant, Gregory			Learning Populations of Parameters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Consider the following estimation problem: there are n entities, each with an unknown parameter p(i) is an element of [0, 1], and we observe n independent random variables, X-1, . . . , X-n, with X-i similar to Binomial (t, p(i)). How accurately can one recover the "histogram" (i.e. cumulative density function) of the p(i)'s? While the empirical estimates would recover the histogram to earth mover distance circle minus(1/root t) (equivalently, l(1) distance between the CDFs), we show that, provided n is sufficiently large, we can achieve error O (1/t) which is information theoretically optimal. We also extend our results to the multi-dimensional parameter case, capturing settings where each member of the population has multiple associated parameters. Beyond the theoretical results, we demonstrate that the recovery algorithm performs well in practice on a variety of datasets, providing illuminating insights into several domains, including politics, sports analytics, and variation in the gender ratio of offspring.	[Tian, Kevin; Kong, Weihao; Valiant, Gregory] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Tian, K (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	kjtian@stanford.edu; whkong@stanford.edu; valiant@stanford.edu			NSF CAREER [CCF-1351108]; ONR [N00014-17-1-2562]; NSF Graduate Fellowship [DGE-1656518]; Google Faculty Fellowship	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); NSF Graduate Fellowship(National Science Foundation (NSF)); Google Faculty Fellowship(Google Incorporated)	We thank Kaja Borge and Ane Nodtvedt for sharing an anonymized dataset on sex composition of dog litters, based on data collected by the Norwegian Kennel Club. This research was supported by NSF CAREER Award CCF-1351108, ONR Award N00014-17-1-2562, NSF Graduate Fellowship DGE-1656518, and a Google Faculty Fellowship.	Acharya J, 2009, ITW: 2009 IEEE INFORMATION THEORY WORKSHOP ON NETWORKING AND INFORMATION THEORY, P251, DOI 10.1109/ITWNIT.2009.5158581; Acharya Jayadev, 2016, ARXIV161102960; Bagby Thomas, 2002, CONSTRUCTIVE APPROXI, V18; BARLOW P, 1970, NATURE, V226, P961, DOI 10.1038/226961a0; Daskalakis C, 2015, ALGORITHMICA, V72, P316, DOI 10.1007/s00453-015-9971-3; Diakonikolas I., 2016, P 29 C LEARN THEOR C, P850; Kong W., 2016, ARXIV160200061; Korneichuk N.P., 1991, EXACT CONSTANTS APPR, V38; Levi R., 2013, THEORY COMPUT, V9, P295; Levi R, 2014, SIAM J DISCRETE MATH, V28, P1699, DOI 10.1137/120903737; Orlitsky A., 2004, PROC 20 C UNCERTAINT, P426; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Penfold LM, 1998, MOL REPROD DEV, V50, P323, DOI 10.1002/(SICI)1098-2795(199807)50:3&lt;323::AID-MRD8&gt;3.0.CO;2-L; Stein C., 1956, P BERK S MATH STAT P, V1, P197, DOI DOI 10.1525/9780520313880-018; Valiant G, 2016, ACM S THEORY COMPUT, P142, DOI 10.1145/2897518.2897641; Valiant G, 2017, J ACM, V64, DOI 10.1145/3125643; Valiant G, 2011, ACM S THEORY COMPUT, P685; Zou James, 2016, NATURE COMMUNICATION, V7	18	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405083
C	Varma, P; He, B; Bajaj, P; Khandwala, N; Banerjee, I; Rubin, D; Re, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Varma, Paroma; He, Bryan; Bajaj, Payal; Khandwala, Nishith; Banerjee, Imon; Rubin, Daniel; Re, Christopher			Inferring Generative Model Structure with Static Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SEGMENTATION; SELECTION	Obtaining enough labeled data to robustly train complex discriminative models is a major bottleneck in the machine learning pipeline. A popular solution is combining multiple sources of weak supervision using generative models. The structure of these models affects the quality of the training labels, but is difficult to learn without any ground truth labels. We instead rely on weak supervision sources having some structure by virtue of being encoded programmatically. We present Coral, a paradigm that infers generative model structure by statically analyzing the code for these heuristics, thus significantly reducing the amount of data required to learn structure. We prove that Coral's sample complexity scales quasilinearly with the number of heuristics and number of relations identified, improving over the standard sample complexity, which is exponential in n for learning nth degree relations. Empirically, Coral matches or outperforms traditional structure learning approaches by up to 3.81 F1 points. Using Coral to model dependencies instead of assuming independence results in better performance than a fully supervised model by 3.07 accuracy points when heuristics are used to label radiology data without ground truth labels.	[Varma, Paroma] Stanford Univ, Elect Engn, Stanford, CA 94305 USA; [He, Bryan; Bajaj, Payal; Khandwala, Nishith; Re, Christopher] Stanford Univ, Comp Sci, Stanford, CA 94305 USA; [Banerjee, Imon; Rubin, Daniel] Stanford Univ, Biomed Data Sci, Stanford, CA 94305 USA; [Rubin, Daniel] Stanford Univ, Radiol, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University; Stanford University	Varma, P (corresponding author), Stanford Univ, Elect Engn, Stanford, CA 94305 USA.	paroma@stanford.edu; bryanhe@stanford.edu; pabajaj@stanford.edu; nishith@stanford.edu; imonb@stanford.edu; rubin@stanford.edu; chrismre@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Defense Advanced Research Projects Agency (DARPA) [FA8750-17-2-0095]; DARPA SIMPLEX program [N66001-15-C-4043]; DARPA [FA8750-12-2-0335, FA8750-13-2-0039]; National Science Foundation (NSF) Graduate Research Fellowship [DGE-114747]; Joseph W. and Hon Mai Goodman Stanford Graduate Fellowship; National Institute of Health (NIH) [U54EB020405]; Office of Naval Research (ONR) [N000141210041, N000141310129]; Moore Foundation; Okawa Research Grant; American Family Insurance; Accenture; Toshiba; Intel; Microsoft; Teradata; VMware; DOE [108845]	Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA SIMPLEX program; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation (NSF) Graduate Research Fellowship(National Science Foundation (NSF)); Joseph W. and Hon Mai Goodman Stanford Graduate Fellowship; National Institute of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Office of Naval Research (ONR)(Office of Naval Research); Moore Foundation(Gordon and Betty Moore Foundation); Okawa Research Grant; American Family Insurance; Accenture; Toshiba; Intel(Intel Corporation); Microsoft(Microsoft); Teradata; VMware; DOE(United States Department of Energy (DOE))	We thank Shoumik Palkar, Stephen Bach, and Sen Wu for their helpful conversations and feedback. We are grateful to Darvin Yi for his assistance with the DDSM dataset based experiments and associated deep learning models. We acknowledge the use of the bone tumor dataset annotated by Drs. Christopher Beaulieu and Bao Do and carefully collected over his career by the late Henry H. Jones, M.D. (aka "Bones Jones"). This material is based on research sponsored by Defense Advanced Research Projects Agency (DARPA) under agreement number FA8750-17-2-0095. We gratefully acknowledge the support of the DARPA SIMPLEX program under No. N66001-15-C-4043, DARPA FA8750-12-2-0335 and FA8750-13-2-0039, DOE 108845, the National Science Foundation (NSF) Graduate Research Fellowship under No. DGE-114747, Joseph W. and Hon Mai Goodman Stanford Graduate Fellowship, National Institute of Health (NIH) U54EB020405, the Office of Naval Research (ONR) under awards No. N000141210041 and No. N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, and Intel. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, AFRL, NSF, NIH, ONR, or the U.S. government.	Alfonseca E., 2012, P 50 ANN M ASS COMP, P54; Bach Stephen H, 2017, ICML; Balsubramani A., 2015, ADV NEURAL INFORM PR, P1351; Banerjee I., 2016, ARXIV161200408; Blaschko M., 2010, ADV NEURAL INFORM PR; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Branson S, 2011, IEEE I CONF COMP VIS, P1832, DOI 10.1109/ICCV.2011.6126450; Bunescu R., 2007, ACL; Heilbron FC, 2015, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2015.7298698; Chen X, 2016, J MACH LEARN RES, V17; Craven M, 1999, Proc Int Conf Intell Syst Mol Biol, P77; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; HARALICK RM, 1973, IEEE T SYST MAN CYB, VSMC3, P610, DOI 10.1109/TSMC.1973.4309314; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hoffmann R., 2011, P 49 ANN M ASS COMP, V1, P541, DOI DOI 10.5555/2002472; Joglekar M, 2015, PROC INT CONF DATA, P195, DOI 10.1109/ICDE.2015.7113284; Kang D., 2017, ABS170302529 CORR; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kaus MR, 2001, RADIOLOGY, V218, P586, DOI 10.1148/radiology.218.2.r01fe44586; Krishna Ranjay, 2016, ARXIV160207332; Kurtz C, 2014, MED IMAGE ANAL, V18, P1082, DOI 10.1016/j.media.2014.06.009; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Mintz M., 2009, P ACL, P1003, DOI DOI 10.3115/1690219.1690287; Oliver A, 2010, MED IMAGE ANAL, V14, P87, DOI 10.1016/j.media.2009.12.005; Oquab M, 2015, PROC CVPR IEEE, P685, DOI 10.1109/CVPR.2015.7298668; Ratner A, 2016, ADV NEUR IN, V29; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; Riedel S, 2010, LECT NOTES ARTIF INT, V6323, P148, DOI 10.1007/978-3-642-15939-8_10; Roth Benjamin, 2013, EMNLP, P24; Sawyer-Lee Rebecca, 2016, CURATED BREAST IMAGI, P2; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Sharma N, 2010, J MED PHYS, V35, P3, DOI 10.4103/0971-6203.58777; Shin J, 2015, PROC VLDB ENDOW, V8, P1310, DOI 10.14778/2809974.2809991; Takamatsu Shingo, 2012, LONG PAPERS, V1, P721; Varma P., 2017, ARXIV161008123; Xia W, 2013, IEEE I CONF COMP VIS, P2176, DOI 10.1109/ICCV.2013.271; Yi D., 2016, ARXI161104534; Zhao P, 2006, J MACH LEARN RES, V7, P2541	41	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400023
C	Yan, SB; Zhang, CC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yan, Songbai; Zhang, Chicheng			Revisiting Perceptron: Efficient and Label-Optimal Learning of Halfspaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE; COMPLEXITY	It has been a long-standing problem to efficiently learn a halfspace using as few labels as possible in the presence of noise. In this work, we propose an efficient Perceptron-based algorithm for actively learning homogeneous halfspaces under the uniform distribution over the unit sphere. Under the bounded noise condition [49], where each label is flipped with probability at most eta < 1/2, our algorithm achieves a near-optimal label complexity of <(O)over tilde>(d/(1-2 eta)(2) ln 1/epsilon)(2) in time (O) over tilde (d(2)/epsilon(1-2 eta)(3)). Under the adversarial noise condition [6, 45, 42], where at most a (Omega) over tilde (epsilon) fraction of labels can be flipped, our algorithm achieves a near-optimal label complexity of (O) over tilde (d ln 1/epsilon) in time (O) over tilde (d(2)/epsilon) Furthermore, we show that our active learning algorithm can be converted to an efficient passive learning algorithm that has near-optimal sample complexities with respect to epsilon and d.	[Yan, Songbai; Zhang, Chicheng] Univ Calif San Diego, La Jolla, CA 92093 USA; [Zhang, Chicheng] Microsoft Res, New York, NY USA	University of California System; University of California San Diego; Microsoft	Yan, SB (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	yansongbai@ucsd.edu; chicheng.zhang@microsoft.com			NSF [IIS-1167157, 1162581]	NSF(National Science Foundation (NSF))	The authors thank Kamalika Chaudhuri for help and support, Hongyang Zhang for thought-provoking initial conversations, Jiapeng Zhang for helpful discussions, and the anonymous reviewers for their insightful feedback. Much of this work is supported by NSF IIS-1167157 and 1162581.	Agarwal A., 2013, INT C MACHINE LEARNI, P1220; Ailon N, 2014, J MACH LEARN RES, V15, P885; Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Arora S., 1993, Proceedings. 34th Annual Symposium on Foundations of Computer Science (Cat. No.93CH3368-8), P724, DOI 10.1109/SFCS.1993.366815; Awasthi P., 2015, PROC 28 ANN C LEARN, P167; Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Awasthi Pranjal, 2016, P 28 C LEARN THEOR C; Balcan M.-F., 2007, COLT; Balcan M. F, 2013, COLT; Balcan M.-F.F., 2013, ADV NEURAL INFORM PR, V26, P1295; Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Balcan Maria- Florina, 2017, ARXIV170307758; Beygelzimer A., 2010, NIPS; Beygelzimer Alina, 2009, 26 INT C MACH LEARN; Blum A, 1998, ALGORITHMICA, V22, P35, DOI 10.1007/PL00013833; Cesa-Bianchi N., 2011, INT C MACHINE LEARNI, P433; Cesa-Bianchi N., 2009, P 26 ANN INT C MACHI, P121; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen Lin, 2017, 31 AAAI C ART INT; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Cristianini N., 2000, INTRO SUPPORT VECTOR; Daniely Amit, 2015, ARXIV150505800; Dasgupta S, 2005, LECT NOTES COMPUT SC, V3559, P249, DOI 10.1007/11503415_17; Dasgupta S., 2007, ADV NEURAL INFORM PR, V20; DASGUPTA S, 2005, NIPS; Dasgupta S, 2011, THEOR COMPUT SCI, V412, P1767, DOI 10.1016/j.tcs.2010.12.054; Dekel O, 2012, J MACH LEARN RES, V13, P2655; DUNAGAN J, 2004, P 36 ANN ACM S THEOR, P315; Feldman V, 2006, ANN IEEE SYMP FOUND, P563; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Guillory Andrew, 2009, ARTIF INTELL, P201; Hanneke S., 2009, THESIS; Hanneke S, 2007, ICML; Hanneke S., 2012, ARXIV12073772; Hanneke S, 2015, LECT NOTES ARTIF INT, V9355, P149, DOI 10.1007/978-3-319-24486-0_10; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843; Hsu D., 2010, THESIS; Huang Tzu- Kuo, 2015, CORR; Kalai AT, 2008, SIAM J COMPUT, V37, P1777, DOI 10.1137/060649057; KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052; Klivans A.R., 2014, LIPICS, P793; Koltchinskii V., 2010, JMLR; KULKARNI SR, 1993, MACH LEARN, V11, P23, DOI 10.1023/A:1022627018023; LONG PM, 1995, IEEE T NEURAL NETWOR, V6, P1556, DOI 10.1109/72.471352; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; Monteleoni C, 2006, LECT NOTES ARTIF INT, V4005, P650, DOI 10.1007/11776420_47; MOTZKIN TS, 1954, CAN J MATH, V6, P393, DOI 10.4153/CJM-1954-038-x; Raginsky M., 2011, NIPS, P1026; Settles Burr, 2010, ACTIVE LEARNING LIT, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Tosh Christopher, 2017, P 34 INT C MACH LEAR, P3444; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Wang LW, 2011, J MACH LEARN RES, V12, P2269; Wang Y.-X., 2016, AAAI; Welling M., 2014, ADV NEURAL INFORM PR, V27; Zhang Y., 2017, C LEARN THEOR PMLR, P1980	61	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401010
C	Zhou, ZY; Mertikopoulos, P; Bambos, N; Glynn, P; Tomlin, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhou, Zhengyuan; Mertikopoulos, Panayotis; Bambos, Nicholas; Glynn, Peter; Tomlin, Claire			Countering Feedback Delays in Multi-Agent Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ONLINE; ALGORITHMS	We consider a model of game-theoretic learning based on online mirror descent (OMD) with asynchronous and delayed feedback information. Instead of focusing on specific games, we consider a broad class of continuous games defined by the general equilibrium stability notion, which we call lambda-variational stability. Our first contribution is that, in this class of games, the actual sequence of play induced by OMD-based learning converges to Nash equilibria provided that the feedback delays faced by the players are synchronous and bounded. Subsequently, to tackle fully decentralized, asynchronous environments with (possibly) unbounded delays between actions and feedback, we propose a variant of OMD which we call delayed mirror descent (DMD), and which relies on the repeated leveraging of past information. With this modification, the algorithm converges to Nash equilibria with no feedback synchronicity assumptions and even when the delays grow superlinearly relative to the horizon of play.	[Zhou, Zhengyuan; Bambos, Nicholas; Glynn, Peter] Stanford Univ, Stanford, CA 94305 USA; [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, Inria, LIG, Grenoble, France; [Tomlin, Claire] Univ Calif Berkeley, Berkeley, CA 94720 USA	Stanford University; Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); University of California System; University of California Berkeley	Zhou, ZY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	zyzhou@stanford.edu; panayotis.mertikopoulos@imag.fr; bambos@stanford.edu; glynn@stanford.edu; tomlin@eecs.berkeley.edu	Jeong, Yongwook/N-7413-2016	Bambos, Nicholas/0000-0001-9250-4553; Mertikopoulos, Panayotis/0000-0003-2026-9616	Stanford Graduate Fellowship; Huawei Innovation Research Program ULTRON; ANR JCJC project ORACLESS [ANR-16-CE33-0004-01]; NSF CPS: FORCES grant [CNS-1239166]	Stanford Graduate Fellowship(Stanford University); Huawei Innovation Research Program ULTRON; ANR JCJC project ORACLESS(French National Research Agency (ANR)); NSF CPS: FORCES grant	Zhengyuan Zhou is supported by Stanford Graduate Fellowship and he would like to thank Walid Krichene and Alex Bayen for stimulating discussions (and their charismatic research style) that have firmly planted the initial seeds for this work. Panayotis Mertikopoulos gratefully acknowledges financial support from the Huawei Innovation Research Program ULTRON and the ANR JCJC project ORACLESS (grant no. ANR-16-CE33-0004-01). Claire Tomlin is supported in part by the NSF CPS: FORCES grant (CNS-1239166).	Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Balandat M., 2016, NIPS 16; Blum A, 1998, LECT NOTES COMPUT SC, V1442, P306, DOI 10.1007/BFb0029575; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Hazan E., 2016, FDN TRENDSR OPTIMIZA; Heliou A., 2017, P ADV NEUR INF PROC, V30, P6369; Joulani P., 2013, ICML, P1453; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Krichene S, 2015, ANN ALLERTON CONF, P480, DOI 10.1109/ALLERTON.2015.7447043; Krichene W, 2015, SIAM J CONTROL OPTIM, V53, P1056, DOI 10.1137/140980685; Lam Kiet, 2016, 2016 ACM/IEEE 7th International Conference on Cyber-Physical Systems (ICCPS), P1, DOI 10.1109/ICCPS.2016.7479108; MEHTA R., 2015, ITCS 15; MERTIKOPOULOS P., 2016, LEARNING GAMES CONTI; MERTIKOPOULOS P., SODA 18; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; PALAIOPANOS G., 2017, NIPS 17; Quanrud K., 2015, P ADV NEUR INF PROC, P1270; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Shalev-Shwartz S., 2007, ADV NEURAL INFORM PR, P1265; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Viossat Y, 2013, J ECON THEORY, V148, P825, DOI 10.1016/j.jet.2012.07.003; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zhou ZY, 2016, LECT NOTES COMPUT SC, V9996, P114, DOI 10.1007/978-3-319-47413-7_7; Zhou ZY, 2016, P AMER CONTR CONF, P3802, DOI 10.1109/ACC.2016.7525505; Zhu MH, 2016, AUTOMATICA, V63, P82, DOI 10.1016/j.automatica.2015.10.012; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	29	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406024
C	Boscaini, D; Masci, J; Rodola, E; Bronstein, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Boscaini, Davide; Masci, Jonathan; Rodola, Emanuele; Bronstein, Michael			Learning shape correspondence with anisotropic convolutional neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Convolutional neural networks have achieved extraordinary results in many computer vision and pattern recognition applications; however, their adoption in the computer graphics and geometry processing communities is limited due to the non-Euclidean structure of their data. In this paper, we propose Anisotropic Convolutional Neural Network (ACNN), a generalization of classical CNNs to non-Euclidean domains, where classical convolutions are replaced by projections over a set of oriented anisotropic diffusion kernels. We use ACNNs to effectively learn intrinsic dense correspondences between deformable shapes, a fundamental problem in geometry processing, arising in a wide variety of applications. We tested ACNNs performance in challenging settings, achieving state-of-the-art results on recent correspondence benchmarks.	[Boscaini, Davide; Masci, Jonathan; Rodola, Emanuele; Bronstein, Michael] USI Lugano, Lugano, Switzerland; [Bronstein, Michael] Tel Aviv Univ, Tel Aviv, Israel; [Bronstein, Michael] Intel, Haifa, Israel	Universita della Svizzera Italiana; Tel Aviv University; Intel Corporation	Boscaini, D (corresponding author), USI Lugano, Lugano, Switzerland.	davide.boscaini@usi.ch; jonathan.masci@usi.ch; emanuele.rodola@usi.ch; michael.Bronstein@usi.ch			ERC [307047]; Google; Nvidia	ERC(European Research Council (ERC)European Commission); Google(Google Incorporated); Nvidia	The authors wish to thank Matteo Sala for the textured models. This research was supported by the ERC Starting Grant No. 307047 (COMET), a Google Faculty Research Award, and Nvidia equipment grant.	Andreux M., 2014, P NORDIA; Bergstra J., 2010, P SCIPY JUN; Bogo F., 2014, P CVPR; Boscaini D, 2015, COMPUT GRAPH FORUM, V34, P13, DOI 10.1111/cgf.12693; Boscaini D., 2016, COMPUTER GRAPHICS FO, V35; Cosmo L., 2016, P 3DOR; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974; Kingma D.P., 2015, INT C LEARN REPR, P1; Kokkinos I., 2012, P CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; LeCun Y, 2014, P ICLR; Litman R., 2014, PAMI, V36, P170; Masci J., 2015, P 3DRR; Memoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Rodola E., 2016, COMPUTER GRAPHICS FO; Rodola E., 2014, P CVPR; Salti S, 2014, COMPUT VIS IMAGE UND, V125, P251, DOI 10.1016/j.cviu.2014.04.011; Shuman D. I, 2013, ARXIV13075708; Solomon J, 2012, COMPUT GRAPH FORUM, V31, P1617, DOI 10.1111/j.1467-8659.2012.03167.x; Su H., 2015, P ICCV; Wei L., 2016, P CVPR; Windheuser T., 2014, P BMVC; Wu Z., 2015, P CVPR; Zhang H., 2010, COMPUT GRAPH FORUM, P1	28	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700010
C	Ge, R; Lee, JD; Ma, TY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ge, Rong; Lee, Jason D.; Ma, Tengyu			Matrix Completion has No Spurious Local Minimum	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Matrix completion is a basic machine learning problem that has wide applications, especially in collaborative filtering and recommender systems. Simple non-convex optimization algorithms are popular and effective in practice. Despite recent progress in proving various non-convex algorithms converge from a good initial point, it remains unclear why random or arbitrary initialization suffices in practice. We prove that the commonly used non-convex objective function for positive semidefinite matrix completion has no spurious local minima - all local minima must also be global. Therefore, many popular optimization algorithms such as (stochastic) gradient descent can provably solve positive semidefinite matrix completion with arbitrary initialization in polynomial time. The result can be generalized to the setting when the observed entries contain noise. We believe that our main proof strategy can be useful for understanding geometric properties of other statistical problems involving partial or noisy observations.	[Ge, Rong] Duke Univ, 308 Res Dr, Durham, NC 27708 USA; [Lee, Jason D.] Univ Southern Calif, 3670 Trousdale Pkwy, Los Angeles, CA 90089 USA; [Ma, Tengyu] Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA	Duke University; University of Southern California; Princeton University	Ge, R (corresponding author), Duke Univ, 308 Res Dr, Durham, NC 27708 USA.	rongge@cs.duke.edu; jasonlee@marshall.usc.edu; tengyu@cs.princeton.edu						Amit Y., 2007, ICML 07 P 24 INT C M, P17, DOI DOI 10.1145/1273496.1273499; [Anonymous], 2015, ARXIV150903025; Bandeira A. S., 2016, JMLR WORKSHOP C P; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; GE R, 2015, ARXIV150302101; Hardt M., 2014, FOCS 2014; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Koren Yehuda, 2009, NETFLIX PRIZE DOCUME, V81; Li YZ, 2016, PR MACH LEARN RES, V48; Loh P.-L., 2014, ARXIV14125632; Loh PL, 2015, J MACH LEARN RES, V16, P559; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Oliveira R. Imbuzeiro, 2010, ARXIV E PRINTS; PEMANTLE R, 1990, ANN PROBAB, V18, P698, DOI 10.1214/aop/1176990853; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Sun J., 2015, ARXIV151006096; Sun RY, 2015, ANN IEEE SYMP FOUND, P270, DOI 10.1109/FOCS.2015.25; [No title captured]	28	6	6	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700013
C	Grinchuk, O; Lebedev, V; Lempitsky, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Grinchuk, Oleg; Lebedev, Vadim; Lempitsky, Victor			Learnable Visual Markers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a new approach to designing visual markers (analogous to QR-codes, markers for augmented reality, and robotic fiducial tags) based on the advances in deep generative networks. In our approach, the markers are obtained as color images synthesized by a deep network from input bit strings, whereas another deep network is trained to recover the bit strings back from the photos of these markers. The two networks are trained simultaneously in a joint backpropagation process that takes characteristic photometric and geometric distortions associated with marker fabrication and marker scanning into account. Additionally, a stylization loss based on statistics of activations in a pretrained classification network can be inserted into the learning in order to shift the marker appearance towards some texture prototype. In the experiments, we demonstrate that the markers obtained using our approach are capable of retaining bit strings that are long enough to be practical. The ability to automatically adapt markers according to the usage scenario and the desired capacity as well as the ability to combine information encoding with artistic stylization are the unique properties of our approach. As a byproduct, our approach provides an insight on the structure of patterns that are most suitable for recognition by ConvNets and on their ability to distinguish composite patterns.	[Grinchuk, Oleg; Lebedev, Vadim; Lempitsky, Victor] Skolkovo Inst Sci & Technol, Moscow, Russia; [Lebedev, Vadim] Yandex, Moscow, Russia	Skolkovo Institute of Science & Technology	Grinchuk, O (corresponding author), Skolkovo Inst Sci & Technol, Moscow, Russia.		Grinchuk, Oleg/AAH-9301-2019					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Belussi LFF, 2013, J MATH IMAGING VIS, V45, P277, DOI 10.1007/s10851-012-0355-x; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bergamasco F, 2013, MACH VISION APPL, V24, P1295, DOI 10.1007/s00138-012-0469-6; Chih-Chung Lo, 1995, 1995 International IEEE/IAS Conference on Industrial Automation and Control: Emerging Technologies (Cat. No.95TH8070), P485, DOI 10.1109/IACET.1995.527607; Claus D, 2004, LECT NOTES COMPUT SC, V2034, P469; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Fiala M, 2005, PROC CVPR IEEE, P590; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; Hara M., 1998, U. S. Patent, Patent No. 5726435; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kaltenbrunner M., 2007, P 1 INT C TANG EMB I, P69, DOI [DOI 10.1145/1226969.1226983, 10.1145/1226969.1226983]; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kingma D.P, P 3 INT C LEARNING R; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Longacre A., 1997, US Patent, Patent No. [5,591,956, 5591956]; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Mooser J, 2006, 2006 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO - ICME 2006, VOLS 1-5, PROCEEDINGS, P1301, DOI 10.1109/ICME.2006.262777; Nguyen Anh, 2015, C COMP VIS PATT REC; Olson Edwin, 2011, 2011 IEEE International Conference on Robotics and Automation, P3400; Petitcolas FAP, 1999, P IEEE, V87, P1062, DOI 10.1109/5.771065; Richardson A, 2013, IEEE INT CONF ROBOT, P631, DOI 10.1109/ICRA.2013.6630639; Scharstein D, 2001, IMAGE VISION COMPUT, V19, P763, DOI 10.1016/S0262-8856(00)00105-0; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Ulyanov D, 2016, PR MACH LEARN RES, V48; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Woodland N. J., 1952, US Patent, Patent No. [US2612994, 2612994, US2612994A]; Yahyanejad S., 2010, CVPRW, V7, P41; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler MD, 2011, IEEE I CONF COMP VIS, P2018, DOI 10.1109/ICCV.2011.6126474	33	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702070
C	Hyvarinen, A; Morioka, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hyvarinen, Aapo; Morioka, Hiroshi			Unsupervised Feature Extraction by Time-Contrastive Learning and Nonlinear ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INDEPENDENT COMPONENT ANALYSIS; BLIND SOURCE SEPARATION; SLOW FEATURE ANALYSIS; NETWORKS	Nonlinear independent component analysis (ICA) provides an appealing framework for unsupervised feature learning, but the models proposed so far are not identifiable. Here, we first propose a new intuitive principle of unsupervised deep learning from time series which uses the nonstationary structure of the data. Our learning principle, time-contrastive learning (TCL), finds a representation which allows optimal discrimination of time segments (windows). Surprisingly, we show how TCL can be related to a nonlinear ICA model, when ICA is redefined to include temporal nonstationarities. In particular, we show that TCL combined with linear ICA estimates the nonlinear ICA model up to point-wise transformations of the sources, and this solution is unique - thus providing the first identifiability result for nonlinear ICA which is rigorous, constructive, as well as very general.	[Hyvarinen, Aapo; Morioka, Hiroshi] Univ Helsinki, Dept Comp Sci, Helsinki, Finland; [Hyvarinen, Aapo; Morioka, Hiroshi] Univ Helsinki, HIIT, Helsinki, Finland; [Hyvarinen, Aapo] UCL, Gatsby Computat Neurosci Unit, London, England	University of Helsinki; Aalto University; University of Helsinki; University of London; University College London	Hyvarinen, A (corresponding author), Univ Helsinki, Dept Comp Sci, Helsinki, Finland.; Hyvarinen, A (corresponding author), Univ Helsinki, HIIT, Helsinki, Finland.; Hyvarinen, A (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.				JSPS KAKENHI [16J08502]; Academy of Finland	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Academy of Finland(Academy of Finland)	This research was supported in part by JSPS KAKENHI 16J08502 and the Academy of Finland.	Almeida LB, 2004, J MACH LEARN RES, V4, P1297; Brookes MJ, 2011, P NATL ACAD SCI USA, V108, P16783, DOI 10.1073/pnas.1112685108; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; de Pasquale F, 2012, NEURON, V74, P753, DOI 10.1016/j.neuron.2012.03.031; Dinh L, 2015, ARXIV14108516CSLG; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Glorot X., 2010, AISTATS 10; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goroshin R., 2015, ARXIV150402518; Gutmann M. U., 2014, ARXIV14074981STATCO; Gutmann MU, 2012, J MACH LEARN RES, V13, P307; Harmeling S, 2003, NEURAL COMPUT, V15, P1089, DOI 10.1162/089976603765202677; Hinton G. E., 1994, ADV NEURAL INF PROCE; Hinton GE, 2007, TRENDS COGN SCI, V11, P428, DOI 10.1016/j.tics.2007.09.004; Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722; Hyvarinen A, 1999, NEURAL NETWORKS, V12, P429, DOI 10.1016/S0893-6080(98)00140-3; Hyvarinen A, 2001, IEEE T NEURAL NETWOR, V12, P1471, DOI 10.1109/72.963782; Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1; Ioffe S., 2015, P 32 INT C MACH LEAR, P448; Jutten C, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P549, DOI 10.1016/B978-0-12-374726-6.00019-9; Kingma DP., 2014, ARXIV13126114STATML; MATSUOKA K, 1995, NEURAL NETWORKS, V8, P411, DOI 10.1016/0893-6080(94)00083-X; Mobahi H., 2009, P 26 ANN INT C MACHI, P737, DOI DOI 10.1145/1553374.1553469; Pham DT, 2001, IEEE T SIGNAL PROCES, V49, P1837, DOI 10.1109/78.942614; Ramkumar P, 2012, HUM BRAIN MAPP, V33, P1648, DOI 10.1002/hbm.21303; Sprekeler H, 2014, J MACH LEARN RES, V15, P921; Springenberg JT, 2012, LECT NOTES COMPUT SC, V7663, P347, DOI 10.1007/978-3-642-34475-6_42; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tan Y, 2001, IEEE T NEURAL NETWOR, V12, P124, DOI 10.1109/72.896801; Valpola H., 2015, ADV INDEPENDENT COMP, P143, DOI [10.1016/B978-0-12-802806-3.00008-7, DOI 10.1016/B978-0-12-802806-3.00008-7]; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938	32	6	6	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703031
C	Kim, JH; Lee, SW; Kwak, D; Heo, MO; Kim, J; Ha, JW; Zhang, BT		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kim, Jin-Hwa; Lee, Sang-Woo; Kwak, Donghyun; Heo, Min-Oh; Kim, Jeonghee; Ha, Jung-Woo; Zhang, Byoung-Tak			Multimodal Residual Learning for Visual QA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep neural networks continue to advance the state-of-the-art of image recognition tasks with various methods. However, applications of these methods to multimodality remain limited. We present Multimodal Residual Networks (MRN) for the multimodal residual learning of visual question-answering, which extends the idea of the deep residual learning. Unlike the deep residual learning, MRN effectively learns the joint representation from vision and language information. The main idea is to use element-wise multiplication for the joint residual mappings exploiting the residual learning of the attentional models in recent studies. Various alternative models introduced by multimodality are explored based on our study. We achieve the state-of-the-art results on the Visual QA dataset for both Open-Ended and Multiple-Choice tasks. Moreover, we introduce a novel method to visualize the attention effect of the joint representations for each learning block using back-propagation algorithm, even though the visual features are collapsed without spatial information.	[Kim, Jin-Hwa; Lee, Sang-Woo; Kwak, Donghyun; Heo, Min-Oh; Zhang, Byoung-Tak] Seoul Natl Univ, Seoul, South Korea; [Kim, Jeonghee; Ha, Jung-Woo] Naver Corp, Naver Labs, Seongnam, South Korea; [Zhang, Byoung-Tak] Surromind Robot, Seoul, South Korea	Seoul National University (SNU)	Kim, JH (corresponding author), Seoul Natl Univ, Seoul, South Korea.	jhkim@bi.snu.ac.kr; slee@bi.snu.ac.kr; dhkwak@bi.snu.ac.kr; moheo@bi.snu.ac.kr; jeonghee.kim@navercorp.com; jungwoo.ha@navercorp.com; btzhang@bi.snu.ac.kr	Heo, Min-Oh/ABA-3113-2021; Ha, Jung-Woo/ABI-5223-2020	Heo, Min-Oh/0000-0001-5649-0158; Ha, Jung-Woo/0000-0002-7400-7681	Naver Corp.; Korea government [IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR]	Naver Corp.; Korea government(Korean Government)	The authors would like to thank Patrick Emaase for helpful comments and editing. This work was supported by Naver Corp. and partly by the Korea government (IITP-R0126-16-1072-SW.StarLab, KEIT-10044009-HRI.MESSI, KEIT-10060086-RISF, ADD-UD130070ID-BMRR).	Agrawal Aishwarya, 2015, INT C COMP VIS; Andreas J, 2016, ARXIV160101705; Bird S., 2009, NATURAL LANGUAGE PRO; Cho K., 2014, P SSST 8 8 WORKSH SY, P103; diaeresis>aschel Tim Rockt<spacing, 2016, INT C LEARN REPR ICL; Gal Y., 2015, ARXIV151205287; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton GE, 2012, IMPROVING NEURAL NET; Ilievski I., 2016, ARXIV160401485; Ioffe S., 2015, P 32 INT C MACH LEAR; Karpathy Andrej, 2015, 28 IEEE C COMP VIS P; Kim J.H., 2016, P KIIS SPRING C, V26, P165; Kiros R., 2015, ARXIV150606726; Leonard N., 2015, ARXIV151107889; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu J., 2015, DEEPER LSTM NORMALIZ, V6, P1; Malinowski M, 2015, ARXIV150501121; Nair V., 2010, P 27 INT C MACHINE L, P807, DOI DOI 10.5555/3104322.3104425; Ngiam J, 2011, P 28 INT C MACH LEAR, V28, P689, DOI DOI 10.5555/3104482.3104569; Noh H., 2015, ARXIV151105756; Ren MY, 2015, ADV NEUR IN, V28; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Simonyan Karen, 2015, VERY DEEP CONVOLUTIO; Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI [DOI 10.1162/NEC0_A_00311, DOI 10.1109/CVPR.2013.49]; Sukhbaatar S., 2015, P 28 INT C NEURAL IN, V28, P2440; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Wu Q., 2016, IEEE C COMP VIS PATT; Xiong C., 2016, ARXIV160301417; Yang Z., 2015, ARXIV151102274	29	6	6	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703082
C	Ng, YC; Chilinski, P; Silva, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ng, Yin Cheng; Chilinski, Pawel; Silva, Ricardo			Scaling Factorial Hidden Markov Models: Stochastic Variational Inference without Messages	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHM	Factorial Hidden Markov Models (FHMMs) are powerful models for sequential data but they do not scale well with long sequences. We propose a scalable inference and learning algorithm for FHMMs that draws on ideas from the stochastic variational inference, neural network and copula literatures. Unlike existing approaches, the proposed algorithm requires no message passing procedure among latent variables and can be distributed to a network of computers to speed up learning. Our experiments corroborate that the proposed algorithm does not introduce further approximation bias compared to the proven structured mean-field algorithm, and achieves better performance with long sequences and large FHMMs.	[Ng, Yin Cheng; Silva, Ricardo] UCL, Dept Stat Sci, London, England; [Chilinski, Pawel] UCL, Dept Comp Sci, London, England	University of London; University College London; University of London; University College London	Ng, YC (corresponding author), UCL, Dept Stat Sci, London, England.	y.ng.12@ucl.ac.uk; ucabchi@ucl.ac.uk; r.silva@ucl.ac.uk	Ng, Wing Yin/HHN-1743-2022					Archer E., 2015, ARXIV PREPRINT ARXIV; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Elidan G., 2013, COPULAE MATH QUANTIT, P39, DOI DOI 10.1007/978-3-642-35407-6_3; Fan Kai, 2016, UNIFYING VARIATIONAL; Foti N., 2014, ADV NEURAL INFORM PR, P3599; Gao X, 2011, STAT SINICA, V21, P165; Gershman SJ, 2014, P 36 ANN C COGN SCI; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Han Shaobo, 2015, ARXIV150605860; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Johnson Matthew J, 2016, ARXIV160306277; Kingma D. P., 2013, AUTO ENCODING VARIAT; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Lichman M, 2013, UCI MACHINE LEARNING; Maclaurin D., 2015, AUTOGRAD REVERSE MOD; Nelsen R. B, 2013, INTRO COPULAS, V139; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Teh Y. W., 2007, ADV NEURAL INFORM PR, V19; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tran Dustin, 2015, ADV NEURAL INFORM PR, P3550; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	24	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704058
C	Nogueira, R; Cho, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nogueira, Rodrigo; Cho, Kyunghyun			End-to-End Goal-Driven Web Navigation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a goal-driven web navigation as a benchmark task for evaluating an agent with abilities to understand natural language and plan on partially observed environments. In this challenging task, an agent navigates through a website, which is represented as a graph consisting of web pages as nodes and hyperlinks as directed edges, to find a web page in which a query appears. The agent is required to have sophisticated high-level reasoning based on natural languages and efficient sequential decision-making capability to succeed. We release a software tool, called WebNav, that automatically transforms a website into this goal-driven web navigation task, and as an example, we make WikiNav, a dataset constructed from the English Wikipedia. We extensively evaluate different variants of neural net based artificial agents on WikiNav and observe that the proposed goal-driven web navigation well reflects the advances in models, making it a suitable benchmark for evaluating future progress. Furthermore, we extend the WikiNav with question-answer pairs from Jeopardy! and test the proposed agent based on recurrent neural networks against strong inverted index based search engines. The artificial agents trained on WikiNav outperforms the engined based approaches, demonstrating the capability of the proposed goal-driven navigation as a good proxy for measuring the progress in real-world tasks such as focused crawling and question-answering.	[Nogueira, Rodrigo] NYU, Tandon Sch Engn, New York, NY 10003 USA; [Cho, Kyunghyun] NYU, Courant Inst Math Sci, New York, NY 10003 USA	New York University; New York University Tandon School of Engineering; New York University	Nogueira, R (corresponding author), NYU, Tandon Sch Engn, New York, NY 10003 USA.	rodrigonogueira@nyu.edu; kyunghyun.cho@nyu.edu	Nogueira, Rodrigo/AAX-1610-2020	Nogueira, Rodrigo/0000-0002-2600-6035				Alvarez Manuel, 2007, P 3 INT WORKSH DAT E, P18; Chakrabarti S, 1999, COMPUT NETW, V31, P1623, DOI 10.1016/S1389-1286(99)00052-3; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; He Ji, 2015, ARXIV151104636; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358; Mikolov T., 2013, ARXIV; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Narasimhan K., 2015, ARXIV150608941; Risi Sebastian, 2014, ARXIV14107326; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; West R., 2012, WWW, P619, DOI DOI 10.1145/2187836.2187920; West R, 2012, ICWSM; West R, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1598	14	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700028
C	Scieur, D; D'Aspremont, A; Bach, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Scieur, Damien; D'Aspremont, Alexandre; Bach, Francis			Regularized Nonlinear Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				OPTIMIZATION; CONVERGENCE; ALGORITHMS	We describe a convergence acceleration technique for generic optimization problems. Our scheme computes estimates of the optimum from a nonlinear average of the iterates produced by any optimization method. The weights in this average are computed via a simple and small linear system, whose solution can be updated online. This acceleration scheme runs in parallel to the base algorithm, providing improved estimates of the solution on the fly, while the original optimization method is running. Numerical experiments are detailed on classical classification problems.	[Scieur, Damien; Bach, Francis] Ecole Normale Super, INRIA, Paris, France; [Scieur, Damien; D'Aspremont, Alexandre; Bach, Francis] Ecole Normale Super, DI, UMR 8548, Paris, France; [D'Aspremont, Alexandre] Ecole Normale Super, CNRS, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Scieur, D (corresponding author), Ecole Normale Super, INRIA, Paris, France.; Scieur, D (corresponding author), Ecole Normale Super, DI, UMR 8548, Paris, France.	damien.scieur@inria.fr; aspremon@di.ens.fr; francis.bach@inria.fr			European Union [607290 SpaRTaN]; ERC SIPA; chaire Economie des nouvelles donnees; fonds AXA pour la recherche	European Union(European Commission); ERC SIPA; chaire Economie des nouvelles donnees; fonds AXA pour la recherche(AXA Research Fund)	The research leading to these results has received funding from the European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under grant agreement no 607290 SpaRTaN, as well as support from ERC SIPA and the chaire Economie des nouvelles donnees with the data science joint research initiative with the fonds AXA pour la recherche.	Aitken AC, 1926, P R SOC EDINB, V46, P289, DOI DOI 10.1017/S0370164600022070; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Ben-Tal A., 2001, MPS SIAM SERIES OPTI; Brezinski C, 1977, LECT NOTES MATH; CABAY S, 1976, SIAM J NUMER ANAL, V13, P734, DOI 10.1137/0713060; Drori Y, 2014, MATH PROGRAM, V145, P451, DOI 10.1007/s10107-013-0653-0; Durbin J., 1960, ECONOMETRICA, P233, DOI 10.2307/1401322 0101.35604; Eddy R., 1979, INFORM LINKAGE APPL, P387; Golub Gene H, 1961, NUMER MATH, V3, P157; Heinig G, 2011, LINEAR ALGEBRA APPL, V435, P1, DOI 10.1016/j.laa.2010.12.001; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Levinson N, 1949, EXTRAPOLATION INTERP; Lin Hongzhou, 2015, ADV NEURAL INFORM PR, P3366; Mesina M., 1977, Computer Methods in Applied Mechanics and Engineering, V10, P165, DOI 10.1016/0045-7825(77)90004-4; NEMIROVSKII AS, 1985, USSR COMP MATH MATH+, V25, P21, DOI 10.1016/0041-5553(85)90100-4; Nesterov Y., 2000, HIGH PERFORMANCE OPT, V33, P405, DOI [DOI 10.1007/978-1-4757-3216-0_17, 10.1007/978-1-4757-3216-017, DOI 10.1007/978-1-4757-3216-017]; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y., 2003, INTRO LECT CONVEX OP; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Parrilo PA., 2000, THESIS CALTECH; Shanks Daniel, 1955, J MATH PHYS, V34, P1, DOI DOI 10.1080/00207167308803075; SIDI A, 1986, SIAM J NUMER ANAL, V23, P178, DOI 10.1137/0723013; SMITH DA, 1987, SIAM REV, V29, P199, DOI 10.1137/1029042; TYRTYSHNIKOV EE, 1994, NUMER MATH, V67, P261, DOI 10.1007/s002110050027; Wibisono Andre, 2015, TECHNICAL REPORT; Wynn Peter, 1956, MTAC, V10, P91, DOI DOI 10.2307/2002183	28	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702014
C	Vezhnevets, A; Mnih, V; Agapiou, J; Osindero, S; Graves, A; Vinyals, O; Kavukcuoglu, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vezhnevets, Alexander (Sasha); Mnih, Volodymyr; Agapiou, John; Osindero, Simon; Graves, Alex; Vinyals, Oriol; Kavukcuoglu, Koray			Strategic Attentive Writer for Learning Macro-Actions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a novel deep recurrent neural network architecture that learns to build implicit plans in an end-to-end manner purely by interacting with an environment in reinforcement learning setting. The network builds an internal plan, which is continuously updated upon observation of the next input from the environment. It can also partition this internal representation into contiguous sub-sequences by learning for how long the plan can be committed to - i.e. followed without replaning. Combining these properties, the proposed model, dubbed STRategic Attentive Writer (STRAW) can learn high-level, temporally abstracted macro-actions of varying lengths that are solely learnt from data without any prior information. These macro-actions enable both structured exploration and economic computation. We experimentally demonstrate that STRAW delivers strong improvements on several ATARI games by employing temporally extended planning strategies (e.g. Ms. Pacman and Frostbite). It is at the same time a general algorithm that can be applied on any sequence data. To that end, we also show that when trained on text prediction task, STRAW naturally predicts frequent n-grams (instead of macro-actions), demonstrating the generality of the approach.	[Vezhnevets, Alexander (Sasha); Mnih, Volodymyr; Agapiou, John; Osindero, Simon; Graves, Alex; Vinyals, Oriol; Kavukcuoglu, Koray] Google DeepMind, London, England	Google Incorporated	Vezhnevets, A (corresponding author), Google DeepMind, London, England.	vezhnick@google.com; vmnih@google.com; jagapiou@google.com; osindero@google.com; gravesa@google.com; vinyals@google.com; korayk@google.com						[Anonymous], 2000, THESIS; [Anonymous], 2016, ICML; [Anonymous], 2014, ICLR; Bacon P.-L., 2015, NIPS DEEP REINF LEAR; Bellemare M.G., 2012, J ARTIFICIAL INTELLI; Bengio Yoshua, 2013, ARXIV; BOTVINICK M, 2009, COGNITION; Boutilier Craig, 1997, IJCAI; D. Kulkarni T., 2016, ARXIV160406057; Dayan P., 1993, NEURAL COMPUTATION; Dayan Peter, 1993, NIPS; Dietterich Thomas G., 2000, J ARTIF INTELL RES J; Gers F. A., 2000, NEURAL COMPUTATION; Graves Alex, 2013, ARXIV13080850 CORR; Gregor K., 2015, ICML; Kaelbling Leslie Pack, 2014, ICML; Levine S., 2015, ARXIV150400702; Marcus Mitchell P., 1993, COMPUT LINGUIST; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mozer Michael C, 1989, COMPLEX SYSTEMS; Parr R., 1998, NIPS; Precup Doina, 1998, EUR C MACH LEARN ECM; Precup Doina, 1997, TECHNICAL REPORT; Rezende D.J., 2014, ICML; Schmidhuber J., 1991, 14891 FKI; Schulman J., 2015, ICML; Sutton R. S., 1999, ARTIFICIAL INTELLIGE; Sutton Richard S., 1995, ICML; Tessler Chen, 2016, AAAI; Wiering Marco, 1997, ADAPTIVE BEHAV	30	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703050
C	Anava, O; Hazan, E; Mannor, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Anava, Oren; Hazan, Elad; Mannor, Shie			Online Learning for Adversaries with Memory: Price of Past Mistakes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The framework of online learning with memory naturally captures learning problems with temporal effects, and was previously studied for the experts setting. In this work we extend the notion of learning with memory to the general Online Convex Optimization (OCO) framework, and present two algorithms that attain low regret. The first algorithm applies to Lipschitz continuous loss functions, obtaining optimal regret bounds for both convex and strongly convex losses. The second algorithm attains the optimal regret bounds and applies more broadly to convex losses without requiring Lipschitz continuity, yet is more complicated to implement. We complement the theoretical results with two applications: statistical arbitrage in finance, and multi-step ahead prediction in statistics.	[Anava, Oren; Mannor, Shie] Technion, Haifa, Israel; [Hazan, Elad] Princeton Univ, New York, NY USA	Princeton University	Anava, O (corresponding author), Technion, Haifa, Israel.	oanava@tx.technion.ac.il; ehazan@cs.princeton.edu; shie@ee.technion.ac.il		Hazan, Elad/0000-0002-1566-3216; Mannor, Shie/0000-0003-4439-7647	European Community [306638]	European Community(European Commission)	This work has been supported by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL).	Anava Oren, 2013, ARXIV13026927; Anava Oren, 2015, ICML; Arora Raman, 2012, ONLINE BANDIT LEARNI; Cesa-Bianchi N., 2013, C NEUR INF PROC SYS, V26; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Clements MP, 1996, OXFORD B ECON STAT, V58, P657; Cuturi Marco, 2013, MEAN REVERSION VARIA, V28, P271; D'Aspremont A, 2011, QUANT FINANC, V11, P351, DOI 10.1080/14697688.2010.481634; Dekel O., 2013, ARXIV13102997; Geulen S., 2010, P 23 ANN C LEARN THE, P132; Gofer E., 2014, P 27 C LEARN THEOR, P210; Gyorgy A., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2218, DOI 10.1109/ISIT.2011.6033954; Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287; JOHANSEN S, 1991, ECONOMETRICA, V59, P1551, DOI 10.2307/2938278; Jurek Jakub W, 2007, EFA 2006 M PAP; Kim, 1998, UNIT ROOTS COINTEGRA, DOI 10.1017/CBO9780511751974; LITTLESTONE N, 1989, ANN IEEE SYMP FOUND, P256, DOI 10.1109/SFCS.1989.63487; LOVASZ L, 2003, FOCS, P640; Marcellino M, 2006, J ECONOMETRICS, V135, P499, DOI 10.1016/j.jeconom.2005.07.020; Merhav N, 2002, IEEE T INFORM THEORY, V48, P1947, DOI 10.1109/TIT.2002.1013135; Narayanan H., 2010, ADV NEURAL INFORM PR, P1777; Puterman ML, 2014, MARKOV DECISION PROC; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; [No title captured]	25	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103063
C	Esser, SK; Appuswamy, R; Merolla, PA; Arthur, JV; Modha, DS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Esser, Steve K.; Appuswamy, Rathinakumar; Merolla, Paul A.; Arthur, John V.; Modha, Dharmendra S.			Backpropagation for Energy-Efficient Neuromorphic Computing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NETWORK; SYSTEM	Solving real world problems with embedded neural networks requires both training algorithms that achieve high performance and compatible hardware that runs in real time while remaining energy efficient. For the former, deep learning using backpropagation has recently achieved a string of successes across many domains and datasets. For the latter, neuromorphic chips that run spiking neural networks have recently achieved unprecedented energy efficiency. To bring these two advances together, we must first resolve the incompatibility between backpropagation, which uses continuous-output neurons and synaptic weights, and neuromorphic designs, which employ spiking neurons and discrete synapses. Our approach is to treat spikes and discrete synapses as continuous probabilities, which allows training the network using standard backpropagation. The trained network naturally maps to neuromorphic hardware by sampling the probabilities to create one or more networks, which are merged using ensemble averaging. To demonstrate, we trained a sparsely connected network that runs on the TrueNorth chip using the MNIST dataset. With a high performance network (ensemble of 64), we achieve 99.42% accuracy at 108 mu J per image, and with a high efficiency network (ensemble of 1) we achieve 92.7% accuracy at 0.268 mu J per image.	[Esser, Steve K.; Appuswamy, Rathinakumar; Merolla, Paul A.; Arthur, John V.; Modha, Dharmendra S.] IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA	International Business Machines (IBM)	Esser, SK (corresponding author), IBM Res Almaden, 650 Harry Rd, San Jose, CA 95120 USA.	sesser@us.ibm.com; rappusw@us.ibm.com; pameroll@us.ibm.com; arthurjo@us.ibm.com; dmodha@us.ibm.com			Defense Advanced Research Projects Agency [HR0011- 09-C-0002, FA9453-15-C-0055]	Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This research was sponsored by the Defense Advanced Research Projects Agency under contracts No. HR0011- 09-C-0002 and No. FA9453-15-C-0055. The views, opinions, and/or findings contained in this paper are those of the authors and should not be interpreted as representing the official views or policies of the Department of Defense or the U.S. Government.	BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Benjamin B, 2014, P IEEE, V102, P699, DOI 10.1109/JPROC.2014.2313565; Cao YQ, 2015, INT J COMPUT VISION, V113, P54, DOI 10.1007/s11263-014-0788-3; Cassidy AS, 2013, IEEE IJCNN; Cheng Zhiyong, 2015, ARXIV150303562; Diehl PU, 2015, IEEE IJCNN; Fiesler Emile, 1990, OPTICAL INTERCONNECT, V1281, P164, DOI DOI 10.1117/12.20700; Hannun A., 2014, ARXIV14125567; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2012, IMPROVING NEURAL NET; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Merolla PA, 2014, SCIENCE, V345, P668, DOI 10.1126/science.1254642; Moerland P., 1997, HDB NEURAL COMPUTATI; Muller L. K., 2015, ARXIV150405767; Ouyang WL, 2013, IEEE I CONF COMP VIS, P2056, DOI 10.1109/ICCV.2013.257; Painkras E, 2013, IEEE J SOLID-ST CIRC, V48, P1943, DOI 10.1109/JSSC.2013.2259038; Pfeil T, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00011; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Stromatias Evangelos, 2015, INT JOINT C NEUR NET; Zhao JY, 1996, NEURAL NETWORKS, V9, P991, DOI 10.1016/0893-6080(96)00025-1	21	6	6	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102011
C	Mirzasoleiman, B; Karbasi, A; Badanidiyuru, A; Krause, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mirzasoleiman, Baharan; Karbasi, Amin; Badanidiyuru, Ashwinkumar; Krause, Andreas			Distributed Submodular Cover: Succinctly Summarizing Massive Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					How can one find a subset, ideally as small as possible, that well represents a massive dataset? I.e., its corresponding utility, measured according to a suitable utility function, should be comparable to that of the whole dataset. In this paper, we formalize this challenge as a submodular cover problem. Here, the utility is assumed to exhibit submodularity, a natural diminishing returns condition prevalent in many data summarization applications. The classical greedy algorithm is known to provide solutions with logarithmic approximation guarantees compared to the optimum solution. However, this sequential, centralized approach is impractical for truly large-scale problems. In this work, we develop the first distributed algorithm - DISCOVER - for submodular set cover that is easily implementable using MapReduce-style computations. We theoretically analyze our approach, and present approximation guarantees for the solutions returned by DISCOVER. We also study a natural trade-off between the communication cost and the number of rounds required to obtain such a solution. In our extensive experiments, we demonstrate the effectiveness of our approach on several applications, including active set selection, exemplar based clustering, and vertex cover on tens of millions of data points using Spark.	[Mirzasoleiman, Baharan; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland; [Karbasi, Amin] Yale Univ, New Haven, CT 06520 USA; [Badanidiyuru, Ashwinkumar] Google, Mountain View, CA USA	Swiss Federal Institutes of Technology Domain; ETH Zurich; Yale University; Google Incorporated	Mirzasoleiman, B (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.			Krause, Andreas/0000-0001-7260-9673	ERC StG [307036]; Microsoft Faculty Fellowship; ETH Fellowship	ERC StG; Microsoft Faculty Fellowship(Microsoft); ETH Fellowship	This research was supported by ERC StG 307036, a Microsoft Faculty Fellowship and an ETH Fellowship.	Badanidiyuru A., 2014, SIGKDD; Barbosa Rafael, 2015, POWER RANDOMIZATION; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Berger Bonnie, 1994, J COMPUTER SYSTEM SC; Blelloch G. E., 2011, SPAA; Chierichetti Flavio, 2010, WWW; Dean J., 2004, OSDI; El-Arini K, 2011, KDD; El-Arini K., 2009, KDD; Feige U., 1998, J ACM; Golovin D., 2011, J ARTIFICIAL INTELLI; Golovin D, 2013, TRACTABILITY PRACTIC; Gomes R., 2010, ICML; Iyer Rishabh K., 2013, P 8 C WORKSHOP NEURA, P2436; KEMPE D, 2003, P 9 ACM SIGKDD; Krause Andreas, 2009, TUT INT JOINT C ART; Kulesza Alex, 2012, MACH LEARN; Kumar R., 2013, SPAA; Lattanzi Silvio, 2011, SPAA; Mirrokni Vahab, 2015, STOC; Mirzasoleiman B., 2013, NIPS; Mirzasoleiman B., 2015, AAAI; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Stergiou Stergios, 2015, SIGKDD; Torralba Antonio, 2008, TPAMI; Tsanas Athanasios, 2010, ICASSP; Tschiatschek S., 2014, NIPS; Wolsey Laurence A., 1982, COMBINATORICA; Yang J, 2015, KNOWL INF SYST, V42, P181, DOI 10.1007/s10115-013-0693-z; Zaharia Matei, 2010, SPARK CLUSTER COMPUT, P181	30	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101012
C	Shah, A; Ghahramani, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shah, Amar; Ghahramani, Zoubin			Parallel Predictive Entropy Search for Batch Global Optimization of Expensive Objective Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We develop parallel predictive entropy search (PPES), a novel algorithm for Bayesian optimization of expensive black-box objective functions. At each iteration, PPES aims to select a batch of points which will maximize the information gain about the global maximizer of the objective. Well known strategies exist for suggesting a single evaluation point based on previous observations, while far fewer are known for selecting batches of points to evaluate in parallel. The few batch selection schemes that have been studied all resort to greedy methods to compute an optimal batch. To the best of our knowledge, PPES is the first non-greedy batch Bayesian optimization strategy. We demonstrate the benefit of this approach in optimization performance on both synthetic and real world applications, including problems in machine learning, rocket science and robotics.	[Shah, Amar] Univ Cambridge, Dept Engn, Cambridge, England; [Ghahramani, Zoubin] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge; University of Cambridge	Shah, A (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	as793@cam.ac.uk; zoubin@eng.cam.ac.uk						AHMAD IA, 1976, IEEE T INFORM THEORY, V22, P372, DOI 10.1109/TIT.1976.1055550; ANDERSON B, 2000, ICML; [Anonymous], 2015, ICML; Azimi J., 2010, NIPS; Bochner S., 1959, LECT FOURIER INTEGRA; BROCHU E, 2009, TR200923 U BRIT COL; Burrows EH, 2009, BIOTECHNOL PROGR, V25, P1009, DOI 10.1002/btpr.213; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Cunningham J. P., 2013, GAUSSIAN PROBABILITI; Desautels T., 2012, ICML; Ginsbourger D., 2011, DEALING ASYNCHRONICI; Gutin G, 2002, DISCRETE APPL MATH, V117, P81, DOI 10.1016/S0166-218X(01)00195-0; Hasbun J. E., 2008, CLASSICAL MECH MATLA; Hennig P., 2012, JMLR; Hernandez-Lobato J. M., 2014, NIPS; Houlsby N., 2012, NIPS; Lichman M, 2013, UCI MACHINE LEARNING; Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944; Lizotte DJ, 2008, PRACTICAL BAYESIAN O; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Minka T., 2001, THESIS MIT CAMBRIDGE; Mockus J., 1989, BAYESIAN APPROACH GL; Neal R. M., 2012, BAYESIAN LEARNING NE; Rahimi A., 2007, NIPS; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Seeger M., 2008, TECHNICAL REPORT; Shah Amar, 2014, AISTATS; Shekel J., 1971, INFORM SCI SYSTEMS; Snoek J., 2012, NIPS; Srinivas N., 2010, ICML; Wang GG, 2007, J MECH DESIGN, V129, P370, DOI 10.1115/1.2429697; Westervelt E., 2007, CONTROL AUTOMATION S; Ziemba WT, 2006, STOCHASTIC OPTIMIZAT	34	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101064
C	Shah, P; Rao, N; Tang, GG		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shah, Parikshit; Rao, Nikhil; Tang, Gongguo			Sparse and Low-Rank Tensor Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MODELS	Motivated by the problem of robust factorization of a low-rank tensor, we study the question of sparse and low-rank tensor decomposition. We present an efficient computational algorithm that modifies Leurgans' algoirthm for tensor factorization. Our method relies on a reduction of the problem to sparse and low-rank matrix decomposition via the notion of tensor contraction. We use well-understood convex techniques for solving the reduced matrix sub-problem which then allows us to perform the full decomposition of the tensor. We delineate situations where the problem is recoverable and provide theoretical guarantees for our algorithm. We validate our algorithm with numerical experiments.				parikshit@yahoo-inc.com; nikhilr@cs.utexas.edu; gtang@mines.edu						Anandhi A, 2015, THEOR APPL CLIMATOL, V119, P551, DOI 10.1007/s00704-013-1043-5; Anandkumar A, 2014, J MACH LEARN RES, V15, P2239; Beckmann CF, 2005, NEUROIMAGE, V25, P294, DOI 10.1016/j.neuroimage.2004.10.043; Bhaskara A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P594, DOI 10.1145/2591796.2591881; Bhojanapalli S., 2015, ARXIV150205023; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Cattell RB, 1944, PSYCHOMETRIKA, V9, P267, DOI 10.1007/BF02288739; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chen Y., 2011, PROC 28 INT C MACHIN, P873; Goyal N, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P584, DOI 10.1145/2591796.2591875; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; Krishnamurthy A., 2013, ADV NEURAL INFORM PR; KRUSKAL JB, 1977, LINEAR ALGEBRA APPL, V18, P95, DOI 10.1016/0024-3795(77)90069-6; Kuleshov V., 2015, TENSOR FACTORIZATION; LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071; LI Q., 2015, IEEE INT WORKSH COMP; Mesgarani N, 2006, IEEE T AUDIO SPEECH, V14, P920, DOI 10.1109/TSA.2005.858055; Mu C., 2013, ARXIV13075870; Netrapalli P, 2014, ADV NEURAL INFORM PR; Rao N., 2014, SIGN SYST COMP 2013; Shah P., 2015, OPTIMAL LOW RANK TEN; Tang GG, 2015, PR MACH LEARN RES, V37, P1491; Tomioka R., 2011, ARXIV10100789; Yuan M., 2014, ARXIV14051773	27	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103055
C	Tobar, F; Bui, TD; Turner, RE		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Tobar, Felipe; Bui, Thang D.; Turner, Richard E.			Learning Stationary Time Series using Gaussian Processes with Nonparametric Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce the Gaussian Process Convolution Model (GPCM), a two-stage non-parametric generative procedure to model stationary signals as the convolution between a continuous-time white-noise process and a continuous-time linear filter drawn from Gaussian process. The GPCM is a continuous-time nonparametric-window moving average process and, conditionally, is itself a Gaussian process with a nonparametric kernel defined in a probabilistic fashion. The generative model can be equivalently considered in the frequency domain, where the power spectral density of the signal is specified using a Gaussian process. One of the main contributions of the paper is to develop a novel variational free-energy approach based on inter-domain inducing variables that efficiently learns the continuous-time linear filter and infers the driving white-noise process. In turn, this scheme provides closed-form probabilistic estimates of the covariance kernel and the noise-free signal both in denoising and prediction scenarios. Additionally, the variational inference procedure provides closed-form expressions for the approximate posterior of the spectral density given the observed data, leading to new Bayesian nonparametric approaches to spectrum estimation. The proposed GPCM is validated using synthetic and real-world signals.	[Tobar, Felipe] Univ Chile, Ctr Math Modeling, Santiago, Chile; [Bui, Thang D.; Turner, Richard E.] Univ Cambridge, Dept Engn, Cambridge, England	Universidad de Chile; University of Cambridge	Tobar, F (corresponding author), Univ Chile, Ctr Math Modeling, Santiago, Chile.	ftobar@dim.uchile.cl; tdb40@cam.ac.uk; ret26@cam.ac.uk	Bui, Thang/AAZ-5360-2021	Bui, Thang/0000-0002-7878-9748	CONICYT-PAI grant [82140061]; Basal-CONICYT Center for Mathematical Modeling (CMM); EPSRC [EP/L000776/1, EP/M026957/1]	CONICYT-PAI grant; Basal-CONICYT Center for Mathematical Modeling (CMM); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	Part of this work was carried out when F.T. was with the University of Cambridge. F.T. thanks CONICYT-PAI grant 82140061 and Basal-CONICYT Center for Mathematical Modeling (CMM). R.T. thanks EPSRC grants EP/L000776/1 and EP/M026957/1. T.B. thanks Google. We thank Mark Rowland, Shane Gu and the anonymous reviewers for insightful feedback.	Archambeau C., 2007, J MACH LEARN RES, V1, P1; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Duvenaud D., 2013, P INT C MACH LEARN I, P1166; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Gonen M, 2011, J MACH LEARN RES, V12, P2211; GULL SF, 1989, FUND THEOR, V36, P53; Jazwinski A.H., 1970, STOCHASTIC PROCESSES; Lazaro-Gredilla M., 2009, ADV NEURAL INFORM PR, V22, P1087; MacKay D. J. C., 1998, Neural Networks and Machine Learning. Proceedings, P133; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Matthews A. G. d. G., 2015, ARXIV150407027; Minka T. P., 2000, TECH REP; Oksendal B., 2013, STOCHASTIC DIFFERENT; Oppenheim A., 1997, SIGNALS SYSTEMS; Percival Donald B., 1993, SPECTRAL ANAL PHYS A; Qi Y, 2002, INT CONF ACOUST SPEE, P1473; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Scholkopf B., 2001, LEARNING KERNELS SUP; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Tobar FA, 2014, IEEE T NEUR NET LEAR, V25, P265, DOI 10.1109/TNNLS.2013.2272594; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; Turner R.E., 2014, ADV NEURAL INFORM PR, V27, P2213; Turner RE, 2010, THESIS; Turner RE, 2014, IEEE T SIGNAL PROCES, V62, P6171, DOI 10.1109/TSP.2014.2362100; Wilson A. G., 2013, P INT C MACH LEARN	26	6	6	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101032
C	Wang, YN; Wang, YX; Singh, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Yining; Wang, Yu-Xiang; Singh, Aarti			Differentially Private Subspace Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ROBUST	Subspace clustering is an unsupervised learning problem that aims at grouping data points into multiple "clusters" so that data points in a single cluster lie approximately on a low-dimensional linear subspace. It is originally motivated by 3D motion segmentation in computer vision, but has recently been generically applied to a wide range of statistical machine learning problems, which often involves sensitive datasets about human subjects. This raises a dire concern for data privacy. In this work, we build on the framework of differential privacy and present two provably private subspace clustering algorithms. We demonstrate via both theory and experiments that one of the presented methods enjoys formal privacy and utility guarantees; the other one asymptotically preserves differential privacy while having good performance in practice. Along the course of the proof, we also obtain two new provable guarantees for the agnostic subspace clustering and the graph connectivity problem which might be of independent interests.	[Wang, Yining; Wang, Yu-Xiang; Singh, Aarti] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, YN (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yiningwa@cs.cmu.edu; yuxiangw@cs.cmu.edu; aarti@cs.cmu.edu		Wang, Yining/0000-0001-9410-0392; Wang, Yu-Xiang/0000-0002-6403-212X	NSF CAREER [IIS-1252412]; NSF [BCS-0941518]; Singapore National Research Foundation under its International Research Centre@ Singapore Funding Initiative	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); Singapore National Research Foundation under its International Research Centre@ Singapore Funding Initiative(National Research Foundation, Singapore)	This research is supported in part by grant NSF CAREER IIS-1252412, NSF Award BCS-0941518, and a grant by Singapore National Research Foundation under its International Research Centre@ Singapore Funding Initiative administered by the IDM Programme Office.	Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Blum A., 2015, PODS; Bradley P. S., 2000, J GLOBAL OPTIMIZATIO, V16; Chaudhuri Kamalika, 2012, NIPS; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork  C., 2014, STOC; Dwork C., 2006, TCC; Dwork C, 2006, EUROCRYPT; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Feldman Dan, 2013, SODA; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Heckel R., 2013, ARXIV13074891; Ho J., 2003, CVPR; Hoff PD, 2009, J COMPUT GRAPH STAT, V18, P438, DOI 10.1198/jcgs.2009.07177; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; McSherry Frank, 2007, FOCS; McWilliams B, 2014, DATA MIN KNOWL DISC, V28, P736, DOI 10.1007/s10618-013-0317-y; Mir D. J., 2013, THESIS; Nasihatkon B., 2011, CVPR; Nissim K., 2007, STOC; Ostrovksy R., 2006, FOCS; Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Su D., 2015, DIFFERENTIALLY PRIVA; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Wang Y., 2015, ICML; Wang Y., 2015, CLUSTERING CONSISTEN; [王应德 Wang Yingde], 2013, [高分子通报, Polymer Bulletin], P89; Zhang  A., 2012, GUESS WHO RATED THIS; Zhang Zhihua, 2004, AAAI	32	6	6	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103029
C	Mairal, J; Konius, P; Harchaoui, Z; Schmid, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mairal, Julien; Konius, Piotr; Harchaoui, Zaid; Schmid, Cordelia			Convolutional Kernel Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					An important goal in visual recognition is to devise image representations that are invariant to particular transformations. In this paper, we address this goal with a new type of convolutional neural network (CNN) whose invariance is encoded by a reproducing kernel. Unlike traditional approaches where neural networks are learned either to represent data or for solving a classification task, our network learns to approximate the kernel feature map on training data. Such an approach enjoys several benefits over classical ones. First, by teaching CNNs to be invariant, we obtain simple network architectures that achieve a similar accuracy to more complex ones, while being easy to train and robust to overfitting. Second, we bridge a gap between the neural network literature and kernels, which are natural tools to model invariance. We evaluate our methodology on visual recognition tasks where CNNs have proven to perform well, e.g., digit recognition with the MNIST dataset, and the more challenging CIFAR-10 and STL-10 datasets, where our accuracy is competitive with the state of the art.	[Mairal, Julien; Konius, Piotr; Harchaoui, Zaid; Schmid, Cordelia] Inria, Rennes, France	Inria	Mairal, J (corresponding author), Univ Grenoble Alpes, LEAR Team, Inria Grenoble, Lab Jean Kuntzmann,CNRS, Grenoble, France.	julien.mairal@inria.fr; piotr.konius@inria.fr; zaid.harchaoui@inria.fr; cordelia.schmid@inria.fr	Mairal, Julien/AAL-5611-2021		ANR [MACARON ANR-14-CE23-0003-01]; MSR-Inria joint centre; European Research Council; CNRS-Mastodons program; LabEx PERSYVAL-Lab [ANR-11-LABX-0025]	ANR(French National Research Agency (ANR)); MSR-Inria joint centre; European Research Council(European Research Council (ERC)European Commission); CNRS-Mastodons program; LabEx PERSYVAL-Lab	This work was partially supported by grants from ANR (project MACARON ANR-14-CE23-0003-01), MSR-Inria joint centre, European Research Council (project ALLEGRO), CNRS-Mastodons program (project GARGANTUA), and the LabEx PERSYVAL-Lab (ANR-11-LABX-0025).	[Anonymous], 2014, P ECCV; Bengio Y., 2009, FDN TRENDS MACH LEAR; Bo L., 2011, P CVPR; Bo L., 2009, ADV NIPS; Bo L., 2013, EXPT ROBOTICS; Bo L., 2010, ADV NIPS; Bottou L., 2007, LARGE SCALE KERNEL M; Bouvrie J. V., 2009, ADV NIPS; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Cho Y., 2010, NEURAL COMPUT, V22; Ciresan D., 2012, P CVPR; Coates A., 2011, P AISTATS; Coates A., 2011, ADV NIPS; Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458; Donahue J., 2013, 31 INT C MACH LEARN; Gens R, 2012, ADV NIPS; Goodfellow I. J., 2013, P ICML; Jarrett K., 2009, P ICCV; Krizhevsky A, 2012, NEURAL INF PROCESS S, ppp1097; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P6; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Rahimi A., 2007, ADV NIPS; Ranzato M., 2007, P CVPR; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Sohn K., 2012, P ICML; Swersky K., 2013, ADV NIPS; Wahba G., 1990, SPLINE MODELS OBSERV; Wan L., 2013, P ICML; Williams C., 2001, ADV NIPS; Zeiler M., 2013, P ICLR	33	6	6	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101019
C	Wang, J; Ye, JP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Jie; Ye, Jieping			Two-Layer Feature Reduction for Sparse-Group Lasso via Decomposition of Convex Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				REGRESSION; SELECTION; PREDICTORS	Sparse-Group Lasso (SGL) has been shown to be a powerful regression technique for simultaneously discovering group and within-group sparse patterns by using a combination of the l(1) and l(2) norms. However, in large-scale applications, the complexity of the regularizers entails great computational challenges. In this paper, we propose a novel two-layer feature reduction method (TLFre) for SGL via a decomposition of its dual feasible set. The two-layer reduction is able to quickly identify the inactive groups and the inactive features, respectively, which are guaranteed to be absent from the sparse representation and can be removed from the optimization. Existing feature reduction methods are only applicable for sparse models with one sparsity-inducing regularizer. To our best knowledge, TLFre is the first one that is capable of dealing with multiple sparsity-inducing regularizers. Moreover, TLFre has a very low computational cost and can be integrated with any existing solvers. Experiments on both synthetic and real data sets show that TLFre improves the efficiency of SGL by orders of magnitude.	[Wang, Jie; Ye, Jieping] Arizona State Univ, Comp Sci & Engn, Tempe, AZ 85287 USA	Arizona State University; Arizona State University-Tempe	Wang, J (corresponding author), Arizona State Univ, Comp Sci & Engn, Tempe, AZ 85287 USA.	jie.wang.ustc@asu.edu; jieping.ye@asu.edu						Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Borwein J.M., 2006, CONVEX ANAL NONLINEA; Boyd S, 2004, CONVEX OPTIMIZATION; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Friedman J, ARXIV10010736; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hiriart-Urruty J.-B., 1988, NONSMOOTH OPTIMIZATI; Hiriart-Urruty JB, 2006, ADV MECH MATH, V12, P35; Liu J., 2009, SLEP SPARSE LEARNING; Liu J., 2010, 23TH ANN C ADV NEURA, P1459; Liu J, 2014, PR MACH LEARN RES, V32, P289; Ogawa K., 2014, ARXIV14016740; Peng J, 2010, ANN APPL STAT, V4, P53, DOI 10.1214/09-AOAS271; Ruszczynski A. P., 2006, NONLINEAR OPTIMIZATI; Simon N, 2013, J COMPUT GRAPH STAT, V22, P231, DOI 10.1080/10618600.2012.681250; Sprechmann P, 2011, IEEE T SIGNAL PROCES, V59, P4183, DOI 10.1109/TSP.2011.2157912; Takeuchi, 2013, P 30 INT C MACH LEAR, P1382; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Vidyasagar M., 2014, P ROYAL SOC A; Vincent M, 2014, COMPUT STAT DATA AN, V71, P771, DOI 10.1016/j.csda.2013.06.004; Wang J., 2014, ARXIV14104210V1; Wang J., ARXIV13074156V1; Wang J, 2014, PR MACH LEARN RES, V32, P523; Wang Jie, 2013, ADV NEURAL INFORM PR; Xiang ZJ, 2012, INT CONF ACOUST SPEE, P2137, DOI 10.1109/ICASSP.2012.6288334; Yogatama D, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P786; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	29	6	6	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100106
C	Bondy, J; Bruce, IC; Becker, S; Raykin, S		Thrun, S; Saul, K; Scholkopf, B		Bondy, J; Bruce, IC; Becker, S; Raykin, S			Predicting speech intelligibility from a population of neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				ARTICULATION INDEX; HEARING	A major issue in evaluating speech enhancement and hearing compensation algorithms is to come up with a suitable metric that predicts intelligibility as judged by a human listener. Previous methods such as the widely used Speech Transmission Index (STI) fail to account for masking effects that arise from the highly nonlinear cochlear transfer function. We therefore propose a Neural Articulation Index (NAI) that estimates speech intelligibility from the instantaneous neural spike rate over time, produced when a signal is processed by an auditory neural model. By using a well developed model of the auditory periphery and detection theory we show that human perceptual discrimination closely matches the modeled distortion in the instantaneous spike rates of the auditory nerve. In highly rippled frequency transfer conditions the NAI's prediction error is 8% versus the STI's prediction error of 10.8%.	McMaster Univ, Dept Elect Engn, Hamilton, ON, Canada	McMaster University	Bondy, J (corresponding author), McMaster Univ, Dept Elect Engn, Hamilton, ON, Canada.		Bruce, Ian/A-1232-2008	Bruce, Ian/0000-0002-5169-4538; Becker, Suzanna/0000-0002-2645-070X				ANSI/ASA, 1997, METHODS CALCULATION, pS3; Bruce IC, 2003, J ACOUST SOC AM, V113, P369, DOI 10.1121/1.1519544; FLETCHER H, 1950, J ACOUST SOC AM, V22, P89, DOI 10.1121/1.1906605; FRENCH NR, 1947, J ACOUST SOC AM, V19, P90, DOI 10.1121/1.1916407; HOUTGAST T, 1973, ACUSTICA, V28, P66; HOUTGAST T, 1991, P EUR 91 GEN, P285; KRYTER KD, 1962, J ACOUST SOC AM, V34, P1689, DOI 10.1121/1.1909094; KRYTER KD, 1962, J ACOUST SOC AM, V34, P1698, DOI 10.1121/1.1909096; Sachs MB, 2002, ANN BIOMED ENG, V30, P157, DOI 10.1114/1.1458592; STEENEKEN HJM, 1980, J ACOUST SOC AM, V67, P318, DOI 10.1121/1.384464; STEENEKEN HJM, 1992, THESIS U AMSTERDAM; van Schijndel NH, 2001, J ACOUST SOC AM, V110, P529, DOI 10.1121/1.1378345; VANSON RJJ, 2001, EUROSPEECH 200U	13	6	7	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1409	1416						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500175
C	Chechik, G; Globerson, A; Tishby, N; Weiss, Y		Thrun, S; Saul, K; Scholkopf, B		Chechik, G; Globerson, A; Tishby, N; Weiss, Y			Information bottleneck for Gaussian variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					The problem of extracting the relevant aspects of data was addressed through the information, bottleneck (IB) method, by (soft) clustering one variable while preserving information about another - relevance - variable. An interesting question addressed in the current work is the extension of these ideas to obtain continuous representations that preserve relevant information, rather than discrete clusters. We give a formal definition of the general continuous IB problem and obtain an analytic solution for the optimal representation for the important case of multivariate Gaussian variables. The obtained optimal representation is a noisy linear projection to eigenvectors of the normalized correlation matrix Sigma(x|y)Sigma(x)(-1), which is also the basis obtained in Canonical Correlation Analysis. However, in Gaussian IB, the compression tradeoff parameter uniquely determines the dimension, as well as the scale of each eigenvector. This introduces a, novel interpretation where solutions of different ranks lie on a continuum parametrized by the compression level. Our analysis also provides an analytic expression for the optimal tradeoff - the information curve - in terms of the eigenvatue spectrum.	Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Chechik, G (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.							BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0; BECKER S, 1996, NETWORK COMPUTATION, P7; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; BERGER T, 1999, IEEE T INFORMATION T, P1520; CHECHIK G, 2003, 4 HEBR U; CHECHIK G, 2002, ADV NEURAL INFORMATI, V15; GILADBACHRACH R, 2003, P COLT WASH; Hotelling H, 1935, J EDUC PSYCHOL, V26, P139, DOI 10.1037/h0058165; MIKA S, 2000, ADV NEURAL INFORMAT, V12; SLONIM N, 2002, ADV NEURAL INFORMATI, V15; SLONIM N, 2003, THESIS HEBREW U JERU; Tishby N., 1999, P 37 ALL C COMM COMP; WYNER AD, 1975, IEEE T INFORM THEORY, V21, P294, DOI 10.1109/TIT.1975.1055374	13	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1213	1220						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500151
C	Ferguson, D; Morris, A; Hahnel, D; Baker, C; Omohundro, Z; Reverte, C; Thayer, S; Whittaker, C; Whittaker, W; Burgard, W; Thrun, S		Thrun, S; Saul, K; Scholkopf, B		Ferguson, D; Morris, A; Hahnel, D; Baker, C; Omohundro, Z; Reverte, C; Thayer, S; Whittaker, C; Whittaker, W; Burgard, W; Thrun, S			An autonomous robotic system for mapping abandoned mines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present the software architecture of a robotic system for mapping abandoned mines. The software is capable of acquiring consistent 2D maps of large mines with many cycles, represented as Markov random fields. 3D C-space maps are acquired from local 3D range scans, which are used to identify navigable paths using A* search. Our system has been deployed in three abandoned mines, two of which inaccessible to people, where it has acquired maps of unprecedented detail and accuracy.	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Ferguson, D (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.		Burgard, Wolfram/N-2381-2019	Burgard, Wolfram/0000-0002-5680-6500				BAKER C, FSR 03; BESL P, 1992, PAMI, V14; BOSSE M, ICRA 03; ELIAZAR A, IJCAI 03; GUPTA A, 1997, T PARALLEL DISTRIB S, V8; GUTMANN JS, CIRA 00; HAHNEL D, UNPUB IROS 03; HAHNEL D, IROS 0I; HAHNEL D, 2003, 11 INT S ROB RES SIE; Latombe J.-C, 2012, ROBOT MOTION PLANNIN, V124; Lu F., 1997, AUTONOMOUS ROBOTS, V4; MONTEMERLO M, IJCAI 03; MURPHY K, NIPS 99; MURPHY KP, UAI 99; Press WH, 1988, NUMERICAL RECIPES C; SIMMONS R, AAAI 00; Wainwright M.J., 2002, THESIS MIT	17	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						587	594						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500074
C	Kim, W; Navarro, DJ; Pitt, MA; Myung, IJ		Thrun, S; Saul, K; Scholkopf, B		Kim, W; Navarro, DJ; Pitt, MA; Myung, IJ			An MCMC-based method of comparing connectionist models in cognitive science	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				PROBABILITY-DISTRIBUTIONS; INFORMATION	Despite the popularity of connectionist models in cognitive science, their performance can often be difficult to evaluate. Inspired by the geometric approach to statistical model selection, we introduce a conceptually similar method to examine the global behavior of a connectionist model, by counting the number and types of response patterns it can simulate. The Markov Chain Monte Carlo-based algorithm that we constructed finds these patterns efficiently. We demonstrate the approach using two localist network models of speech perception.	Ohio State Univ, Dept Psychol, Columbus, OH 43210 USA	Ohio State University	Navarro, DJ (corresponding author), Ohio State Univ, Dept Psychol, 1827 Neil Ave Mall, Columbus, OH 43210 USA.			Navarro, Danielle/0000-0001-7648-6578				Balasubramanian V, 1997, NEURAL COMPUT, V9, P349, DOI 10.1162/neco.1997.9.2.349; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; MCCLELLAND JL, 1986, COGNITIVE PSYCHOL, V18, P1, DOI 10.1016/0010-0285(86)90015-0; Myung IJ, 2000, P NATL ACAD SCI USA, V97, P11170, DOI 10.1073/pnas.170283897; Norris D, 2000, BEHAV BRAIN SCI, V23, P299, DOI 10.1017/S0140525X00003241; Rissanen J, 2001, IEEE T INFORM THEORY, V47, P1712, DOI 10.1109/18.930912; Rissanen JJ, 1996, IEEE T INFORM THEORY, V42, P40, DOI 10.1109/18.481776	7	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						937	944						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500117
C	Moallemi, CC; Van Roy, B		Thrun, S; Saul, K; Scholkopf, B		Moallemi, CC; Van Roy, B			Distributed optimization in adaptive networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				INFINITE-HORIZON	We develop a protocol for optimizing dynamic behavior of a network of simple electronic components, such as a sensor network, an ad hoc network of mobile devices, or a network of communication switches. This protocol requires only local communication and simple computations which are distributed among devices. The protocol is scalable to large networks. As a motivating example, we discuss a problem involving optimization of power consumption, delay, and buffer overflow in a sensor network. Our approach builds on policy gradient methods for optimization of Markov decision processes. The protocol can be viewed as an extension of policy gradient methods to a context involving a team of agents optimizing aggregate performance through asynchronous distributed communication and computation. We establish that the dynamics of the protocol approximate the solution to an ordinary differential equation that follows the gradient of the performance objective.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	Moallemi, CC (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ciamac@stanford.edu; bvr@stanford.edu						Bartlett PL, 2002, J COMPUT SYST SCI, V64, P133, DOI 10.1006/jcss.2001.1793; Bartlett PL, 2000, IEEE DECIS CONTR P, P124, DOI 10.1109/CDC.2000.912744; Baxter J, 2001, J ARTIF INTELL RES, V15, P351, DOI 10.1613/jair.807; Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345; KUSHNER HJ, 1997, STOCHASTIC APPROXIM; Marbach P, 2001, IEEE T AUTOMAT CONTR, V46, P191, DOI 10.1109/9.905687; MARBACH P, 1998, IEEE C DEC CONTR; MOALLEMI CC, APPENDIX NIPS SUBMIS	9	6	6	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						887	894						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500111
C	Quinlan, MJ; Chalup, SK; Middleton, RH		Thrun, S; Saul, K; Scholkopf, B		Quinlan, MJ; Chalup, SK; Middleton, RH			Application of SVMs for colour classification and collision detection with AIBO robots	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				SUPPORT	This article addresses the issues of colour classification and collision detection as they occur in the legged league robot soccer environment of RoboCup. We show how the method of one-class classification with support vector machines (SVMs) can be applied to solve these tasks satisfactorily using the limited hardware capacity of the prescribed Sony AIBO quadruped robots. The experimental evaluation shows an improvement over our previous methods of ellipse fitting for colour classification and the statistical approach used for collision detection.	Univ Newcastle, Sch Elect Engn & Comp Sci, Callaghan, NSW 2308, Australia	University of Newcastle	Quinlan, MJ (corresponding author), Univ Newcastle, Sch Elect Engn & Comp Sci, Callaghan, NSW 2308, Australia.	mquinlan@eecs.newcastle.edu.au; chalup@eecs.newcastle.edu.au; rick@eecs.newcastle.edu.au	Middleton, Richard H/A-2950-2013; Chalup, Stephan/O-8914-2019; CHALUP, STEPHAN KONRAD/G-7560-2013	Middleton, Richard H/0000-0001-9885-8803; Chalup, Stephan/0000-0002-7886-3653; 				Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; BUNTING J, 2003, RETURN NUBOTS 2003 N; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Peters J, 2017, ADAPT COMPUT MACH LE; QUINLAN MJ, 2003, ROB 2003 S; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Shapiro LG, 2001, COMPUTER VISION; OPEN R SDK	10	6	6	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						635	642						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500080
C	Samejima, K; Doya, K; Ueda, Y; Kimura, M		Thrun, S; Saul, K; Scholkopf, B		Samejima, K; Doya, K; Ueda, Y; Kimura, M			Estimating internal variables and parameters of a learning agent by a particle filter	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					When we model a higher order functions, such as learning and memory, we face a difficulty of comparing neural activities with hidden variables that depend on the history of sensory and motor signals and the dynamics of the network. Here, we propose novel method for estimating hidden variables of a learning agent, such as connection weights from sequences of observable variables. Bayesian estimation is a method to estimate the posterior probability of hidden variables from observable data sequence using a dynamic model of hidden and observable variables. In this paper, we apply particle filter for estimating internal parameters and metaparameters of a reinforcement learning model. We verified the effectiveness of the method using both artificial data and real animal behavioral data.	JST, CRST, ATR Computat Neurosci Labs, Dept Computat Neurobiol, Kyoto 6190288, Japan	Japan Science & Technology Agency (JST)	Samejima, K (corresponding author), JST, CRST, ATR Computat Neurosci Labs, Dept Computat Neurobiol, Keihan Sci City, Kyoto 6190288, Japan.	samejima@atr.jp; doya@atr.jp; yasu@basic.kpu-m.ac.jp; mkimura@basic-kpu-m.ac.jp						Doucet A, 2001, STAT ENG IN, P3; O'Doherty JP, 2003, NEURON, V38, P329, DOI 10.1016/S0896-6273(03)00169-7; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; UEDA Y, 2002, REWARD VALUE DEPENDE	5	6	6	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1335	1342						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500166
C	Shi, RZ; Horiuchi, T		Thrun, S; Saul, K; Scholkopf, B		Shi, RZ; Horiuchi, T			A summating, exponentially-decaying CMOS synapse for spiking neural systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Synapses are a critical element of biologically-realistic, spike-based neural computation, serving the role of communication, computation, and modification. Many different circuit implementations of synapse function exist with different computational goals in mind. In this paper we describe a new CMOS synapse design that separately controls quiescent leak current, synaptic gain, and time-constant of decay. This circuit implements part of a commonly-used kinetic model of synaptic conductance. We show a theoretical analysis and experimental data for prototypes fabricated in a commercially-available 1.5 mum CMOS process.	Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park	Shi, RZ (corresponding author), Univ Maryland, Dept Elect & Comp Engn, College Pk, MD 20742 USA.	rshi@glue.umd.edu; timmer@isr.umd.edu						[Anonymous], 1994, SPRINGER INT SERIES; Boahen KA, 1997, ANALOG INTEGR CIRC S, V13, P53, DOI 10.1023/A:1008215524165; Boegerhausen M, 2003, NEURAL COMPUT, V15, P331, DOI 10.1162/089976603762552942; CHEELY M, 2003, IN PRESS EURASIP J; Deiss SR, 1998, PULSED NEURAL NETWORKS, P157; Destexhe A, 1994, J Comput Neurosci, V1, P195, DOI 10.1007/BF00961734; DESTEXHE A, 1994, NEURAL COMPUT, V6, P14, DOI 10.1162/neco.1994.6.1.14; Frey DR, 1996, IEEE T CIRCUITS-I, V43, P34, DOI 10.1109/81.481459; Gerstner W., 2002, SPIKING NEURON MODEL; Hafliger P, 1997, ADV NEUR IN, V9, P692; INDIVERI G, 2002, ADV NEURAL INFORMATI, V15; Lazzaro J., 1994, SILICON IMPLEMENTATI, P153; Liu S.-C., 2002, ANALOG VLSI CIRCUITS; MCEWAN A, 2000, P ICSC S INT SYST AP; Mead, 1989, ANALOG VLSI NEURAL S; Mortara A, 1998, ANALOG CIRCUITS SIG, P201; RALL W, 1967, J NEUROPHYSIOL, V30, P1138, DOI 10.1152/jn.1967.30.5.1138; Rasche C, 2001, BIOL CYBERN, V84, P57, DOI 10.1007/s004220170004; SEEVINCK E, 1990, ELECTRON LETT, V26, P2046, DOI 10.1049/el:19901319; Tsividis Y, 1997, IEEE T CIRCUITS-II, V44, P65, DOI 10.1109/82.554425; VITTOZ E, 1977, IEEE J SOLID-ST CIRC, V12, P224, DOI 10.1109/JSSC.1977.1050882	21	6	6	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1003	1010						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500125
C	Sigal, L; Isard, M; Sigelman, BH; Black, MJ		Thrun, S; Saul, K; Scholkopf, B		Sigal, L; Isard, M; Sigelman, BH; Black, MJ			Attractive people: Assembling loose-limbed models using non-parametric belief propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The detection and pose estimation of people in images and video is made challenging by the variability of human appearance, the complexity of natural scenes, and the high dimensionality of articulated body models. To cope with these problems we represent the 3D human body as a graphical model in which the relationships between the body parts are represented by conditional probability distributions. We formulate the pose estimation problem as one of probabilistic inference over a graphical model where the random variables correspond to the individual limb parameters (position and orientation). Because the limbs are described by 6-dimensional vectors encoding pose in 3-space, discretization is impractical and the random variables in our model must be continuous-valued. To approximate belief propagation in such a graph we exploit a recently introduced generalization of the particle filter. This framework facilitates the automatic initialization of the body-model from low level cues and is robust to occlusion of body parts and scene clutter.	Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Brown University	Sigal, L (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.	ls@cs.brown.edu; misard@microsoft.com; bhsigelm@cs.brown.edu; black@cs.brown.edu						Bregler C, 1998, PROC CVPR IEEE, P8, DOI 10.1109/CVPR.1998.698581; BURL M, 1998, ECCV, P628; COUGHLAN J, 2002, ECCV, V3, P453; Deutscher J, 2002, LECT NOTES COMPUT SC, V2353, P175; DEUTSCHER J, 1999, ICCV, P1144; DEUTSCHER J, 2001, COMPUTER VISION PATT, V2, P669; Doucet A, 2001, STAT ENG IN, P3; FELZENSZWALB P, 2000, CVPR, V2, P66; FISCHLER MA, 1973, IEEE T COMPUT, VC 22, P67, DOI 10.1109/T-C.1973.223602; GAO J, 2003, CMURITR0305; Ioffe S, 2001, INT J COMPUT VISION, V43, P45, DOI 10.1023/A:1011179004708; Isard M, 2003, PROC CVPR IEEE, P613; Jordan M. I., 2001, GRAPHICAL MODELS FDN; Ju SX, 1996, PROCEEDINGS OF THE SECOND INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION, P38, DOI 10.1109/AFGR.1996.557241; MACCORMICK I, 2000, ECCV, P3; PAVOLVIC V, 1999, ICCV, P94; RAMANAN D, 2003, CVPR, V2, P467; Sidenbladh H, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P709, DOI 10.1109/ICCV.2001.937696; Sidenbladh H., 2000, LNCS, V2, P702; SIGELMAN B, 2003, CS0308 BROWN U DEP C; SMINCHISESCU C, 2001, CVPR, V1, P447; Sudderth EB, 2003, PROC CVPR IEEE, P605; Wu Y, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1094, DOI 10.1109/ICCV.2003.1238471; Yedidia JS, 2001, ADV NEUR IN, V13, P689; Yu S., 2003, ADV NEURAL INFORM PR, P1407	25	6	6	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1539	1546						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500191
C	Steinwart, I		Thrun, S; Saul, K; Scholkopf, B		Steinwart, I			Sparseness of support vector machines - Some asymptotically sharp bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The decision functions constructed by support vector machines (SVM's) usually depend only on a subset of the training set-the so-called support vectors. We derive asymptotically sharp lower and upper bounds on the number of support vectors for several standard types of SVM's. In particular, we show for the Gaussian RBF kernel that the fraction of support vectors tends to twice the Bayes risk for the L1-SVM, to the probability of noise for the L2-SVM, and to 1 for the LS-SVM.	Los Alamos Natl Lab, Modeling Algorithms & Informat Grp, Los Alamos, NM 87545 USA	United States Department of Energy (DOE); Los Alamos National Laboratory	Steinwart, I (corresponding author), Los Alamos Natl Lab, Modeling Algorithms & Informat Grp, CCS-3,Mail Stop B256, Los Alamos, NM 87545 USA.	ingo@lanl.gov						ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cristianini N., 2000, INTRO SUPPORT VECTOR; Devroye L., 1997, PROBABILISTIC THEORY; Kowalczyk A, 2001, ADV NEUR IN, V13, P252; Range RM, 1986, HOLOMORPHIC FUNCTION; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; STEINWART I, IN PRESS IEEE T INFO; STEINWART I, 2003, J MACHINE LEARNING R, V4, P1071; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742	12	6	6	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1069	1076						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500133
C	Verbeek, JJ; Roweis, ST; Vlassis, N		Thrun, S; Saul, K; Scholkopf, B		Verbeek, JJ; Roweis, ST; Vlassis, N			Non-linear CCA and PCA by alignment of local models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We propose a non-linear Canonical Correlation Analysis (CCA) method which works by coordinating or aligning mixtures of linear models. In the same way that CCA extends the idea of PCA, our work extends recent methods for non-linear dimensionality reduction to the case where multiple embeddings of the same underlying low dimensional coordinates are observed, each lying on a different high dimensional manifold. We also show that a special case of our method, when applied to only a single manifold, reduces to the Laplacian Eigenmaps algorithm. As with previous alignment schemes, once the mixture models have been estimated, all of the parameters of our model can be estimated in closed form without local optima in the learning. Experimental results illustrate the viability of the approach as a non-linear extension of CCA.	Univ Amsterdam, Inst Informat, NL-1012 WX Amsterdam, Netherlands	University of Amsterdam	Verbeek, JJ (corresponding author), Univ Amsterdam, Inst Informat, NL-1012 WX Amsterdam, Netherlands.							BELKIN M, 2002, ADV NEURAL INFORMATI, V14; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; BRAND M, 2003, ADV NEURAL INFORMATI, V15; BREGLER C, 1994, ADV NEURAL INFORMATI, V6; HAM JH, 2003, ICML 03 WORKSH CONT; Kohonen T, 2001, SELF ORG MAPS, Vthird, DOI DOI 10.1007/978-3-642-56927-2; Peter M, 2002, SEMIN REPROD MED, V20, P249, DOI 10.1055/s-2002-35389; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; ROWEIS ST, 2002, ADV NEURAL INFORMATI, V14; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; TEH YW, 2003, ADV NEURAL INFORMATI, V15; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319	12	6	6	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						297	304						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500038
C	Werfel, J; Xie, XH; Seung, HS		Thrun, S; Saul, K; Scholkopf, B		Werfel, J; Xie, XH; Seung, HS			Learning curves for stochastic gradient descent in linear feedforward networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				NEURAL NETWORKS	Gradient-following learning methods can encounter problems of implementation in many applications, and stochastic variants are frequently used to overcome these difficulties. We derive quantitative learning curves for three online training methods used with a linear perceptron: direct gradient descent, node perturbation, and weight perturbation. The maximum learning rate for the stochastic methods scales inversely with the first power of the dimensionality of the noise injected into the system; with sufficiently small learning rate, all three methods give identical learning curves. These results suggest guidelines for when these stochastic methods will be limited in their utility, and considerations for architectures. in which they will be effective.	MIT, Dept EECS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Werfel, J (corresponding author), MIT, Dept EECS, Cambridge, MA 02139 USA.							BALDI PF, 1995, IEEE T NEURAL NETWOR, V6, P837, DOI 10.1109/72.392248; Bartlett P. L., 1999, HEBBIAN SYNAPTIC MOD; Cauwenberghs G, 1996, IEEE T NEURAL NETWOR, V7, P346, DOI 10.1109/72.485671; CAUWENBERGHS G, 1993, ADV NEURAL INFORMATI, V5, P244; FIETE I, COMMUNICATION; FLOWER B, 1993, ADV NEURAL INFORMATI, V5, P212; JABRI M, 1992, IEEE T NEURAL NETWOR, V3, P154, DOI 10.1109/72.105429; WIDROW B, 1990, P IEEE, V78, P1415, DOI 10.1109/5.58323; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	9	6	6	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1197	1204						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500149
C	Mannor, S; Shimkin, N		Dietterich, TG; Becker, S; Ghahramani, Z		Mannor, S; Shimkin, N			The steering approach for multi-criteria reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We consider the problem of learning to attain multiple goals in a dynamic environment, which is initially unknown. In addition, the environment may contain arbitrarily varying elements related to actions of other agents or to non-stationary moves of Nature. This problem is modelled as a stochastic (Markov) game between the learning agent and an arbitrary player, with a vector-valued reward function. The objective of the learning agent is to have its long-term average reward vector belong to a given target set. We devise an algorithm for achieving this task, which is based on the theory of approachability for stochastic games. This algorithm combines, in an appropriate way, a finite set of standard, scalar-reward learning algorithms. Sufficient conditions are given for the convergence of the learning algorithm to a general target set. The specialization of these results to the single-controller Markov decision problem axe discussed as well.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Mannor, S (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.			Mannor, Shie/0000-0003-4439-7647				ABOUNADI J, 1998, 2434 LIDSP; Barto A., 1998, REINFORCEMENT LEARNI; Blackwell David, 1956, PAC J MATH, V6, P1, DOI [DOI 10.2140/PJM.1956.6.1, 10.2140/pjm.1956.6.1]; Brafman RI, 2000, ARTIF INTELL, V121, P31, DOI 10.1016/S0004-3702(00)00039-4; Derman C., 1970, FINITE STATE MARKOVI; Filar J., 1996, COMPETITIVE MARKOV D; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; KEARNS M, 1998, P 15 INT C MACH LEAR, P260; LITTMAN ML, 1994, 11 INT C MACH LEARN, P157; Mahadevan S, 1996, MACH LEARN, V22, P159, DOI 10.1007/BF00114727; MANNOR S, 2000, EE1262 TECHN ISR; Mertens J.-F., 1981, International Journal of Game Theory, V10, P53, DOI 10.1007/BF01769259; SCHWARTZ A, 1993, P 10 INT C MACH LEAR, P298; SHIMKIN N, 1993, IEEE T AUTOMAT CONTR, V38, P84, DOI 10.1109/9.186314	14	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1563	1570						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100194
C	Raphael, C		Dietterich, TG; Becker, S; Ghahramani, Z		Raphael, C			A Bayesian network for real-time musical accompaniment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				GRAPHICAL ASSOCIATION MODELS	We describe a computer system that provides a real-time musical accompaniment for a live soloist in a piece of non-improvised music for soloist and accompaniment. A Bayesian network is developed that represents the joint distribution on the times at which the solo and accompaniment notes are played, relating the two parts through a layer of hidden variables. The network is first constructed using the rhythmic information contained in the musical score. The network is then trained to capture the musical interpretations of the soloist and accompanist in an off-line rehearsal phase. During live accompaniment the learned distribution of the network is combined with a real-time analysis of the soloist's acoustic signal, performed with a hidden Markov model, to generate a musically principled accompaniment that respects all available sources of knowledge. A live demonstration will be provided.	Univ Massachusetts, Dept Math & Stat, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Raphael, C (corresponding author), Univ Massachusetts, Dept Math & Stat, Amherst, MA 01003 USA.	raphael@math.umass.edu						COWELL RG, 1999, PROBABILISTIC NETWOR; Lauritzen S., 1999, R992014 AALB U DEP M; LAURITZEN SL, 1992, J AM STAT ASSOC, V87, P1098, DOI 10.2307/2290647; LAURITZEN SL, 1995, COMPUT STAT DATA AN, V19, P191, DOI 10.1016/0167-9473(93)E0056-A; Raphael C, 2001, J NEW MUSIC RES, V30, P59, DOI 10.1076/jnmr.30.1.59.7121; Raphael C, 1999, IEEE T PATTERN ANAL, V21, P360, DOI 10.1109/34.761266; Raphael C, 2001, J COMPUT GRAPH STAT, V10, P487, DOI 10.1198/106186001317115081; RAPHAEL C, 2001, P 8 INT WORKSH ART I, P113; SPIEGELHALTER DJ, 1993, STAT SCI, V8, P219, DOI 10.1214/ss/1177010888	9	6	6	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1433	1439						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100178
C	Shawe-Taylor, J; Cristianini, N; Kandola, J		Dietterich, TG; Becker, S; Ghahramani, Z		Shawe-Taylor, J; Cristianini, N; Kandola, J			On the concentration of spectral properties	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We consider the problem of measuring the eigenvalues of a randomly drawn sample of points. We show that these values can be reliably estimated as can the sum of the tail of eigenvalues. Furthermore, the residuals when data is projected into a subspace is shown to be reliably estimated on a random sample. Experiments are presented that confirm the theoretical results.	Univ London Royal Holloway & Bedford New Coll, London NW1 4NS, England	University of London; Royal Holloway University London	Shawe-Taylor, J (corresponding author), Univ London Royal Holloway & Bedford New Coll, London NW1 4NS, England.	john@cs.rhul.ac.uk; nello@support-vector.net; jaz@cs.rhul.ac.uk		Shawe-Taylor, John/0000-0002-2030-0073				Brin S., 1998, P PAP PRES 7 INT WOR; CRISTIANINI N, 2000, NCTR00080 NEUR WORK; Kleinberg J., 1998, P 9 ACM SIAM S DISCR; MCDIARMID C, 1989, LOND MATH S, V141, P148; MIKA S, 1998, ADV NEURAL INFORMATI, V11; NY AY, 2001, IN PRESS 17 INT JOIN	6	6	6	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						511	517						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100064
C	Graepel, T; Herbrich, R		Leen, TK; Dietterich, TG; Tresp, V		Graepel, T; Herbrich, R			The kernel Gibbs sampler	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NETWORKS	We present an algorithm that samples the hypothesis space of kernel classifiers. Given a uniform prior over normalised weight vectors and a likelihood based on a model of label noise leads to a piecewise constant posterior that can be sampled by the kernel Gibbs sampler (KGS). The KGS is a Markov Chain Monte Carlo method that chooses a random direction in parameter space and samples from the resulting piecewise constant density along the line chosen. The KGS can be used as an analytical tool for the exploration of Bayesian transduction, Bayes point machines, active learning, and evidence-based model selection on small data sets that are contaminated with label noise. For a simple toy example we demonstrate experimentally how a Bayes point machine based on the KGS outperforms an SVM that is incapable of taking into account label noise.	Tech Univ Berlin, Dept Comp Sci, Stat Res Grp, Berlin, Germany	Technical University of Berlin	Graepel, T (corresponding author), Tech Univ Berlin, Dept Comp Sci, Stat Res Grp, Berlin, Germany.							CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Graepel T, 2000, ADV NEUR IN, V12, P456; HERBRICH R, 1999, 9911 TR TU BERL; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; Neal RM, 1993, CRGTR931 U TOR DEP C; Sollich P, 2000, ADV NEUR IN, V12, P349; WAHBA G, 1997, TRNO984 U WISC DEP S	9	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						514	520						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800073
C	Hernandez-Gardiol, N; Mahadevan, S		Leen, TK; Dietterich, TG; Tresp, V		Hernandez-Gardiol, N; Mahadevan, S			Hierarchical memory-based reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A key challenge for reinforcement learning is scaling up to large partially observable domains. In this paper, we show how a hierarchy of behaviors can be used to create and select among variable length short-term memories appropriate for a task. At higher levels in the hierarchy, the agent abstracts over lower-level details and looks back over a variable number of high-level decisions in time. We formalize this idea in a framework called Hierarchical Suffix Memory (HSM). HSM uses a memory-based SMDP learning method to rapidly propagate delayed reward across long decision sequences. We describe a detailed experimental study comparing memory vs. hierarchy using the HSM framework on a realistic corridor navigation task.	MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Hernandez-Gardiol, N (corresponding author), MIT, Artificial Intelligence Lab, Cambridge, MA 02139 USA.							DIETTERICH TG, 1998, AUTONOMOUS ROBOTS J; McCallum AK., 1995, THESIS U ROCHESTER; PARR R, 1998, THESIS U CALIFORNIA; Ron D, 1996, MACH LEARN, V25, P117, DOI 10.1007/BF00114008; Sutton R. S., 1998, P 15 INT C MACH LEAR, P556	5	6	7	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1047	1053						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800147
C	Mizutani, E; Demmel, JW		Leen, TK; Dietterich, TG; Tresp, V		Mizutani, E; Demmel, JW			On iterative Krylov-dogleg trust-region steps for solving neural networks nonlinear least squares problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				OPTIMIZATION	This paper describes a method of dogleg trust-region steps, or restricted Levenberg-Marquardt steps, based on a projection process onto the Krylov subspaces for neural networks nonlinear least squares problems. In particular, the linear conjugate gradient (CG) method works as the inner iterative algorithm for solving the linearized Gauss-Newton normal equation, whereas the outer nonlinear algorithm repeatedly takes so-called "Krylov-dogleg" steps, relying only on matrix-vector multiplication without explicitly forming the Jacobian matrix or the Gauss-Newton model Hessian. That is, our iterative dogleg algorithm can reduce both operational counts and memory space by a factor of O(n) (the number of parameters) in comparison with a direct linear-equation solver. This memory-less property is useful for large-scale problems.	Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan	National Tsing Hua University	Mizutani, E (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu 30043, Taiwan.							DEMBO RS, 1983, MATH PROGRAM, V26, P190, DOI 10.1007/BF02592055; Demmel JW, 1997, APPL NUMERICAL LINEA, V56; Demuth H.B., 1998, NEURAL NETWORK TOOLB; Masters T., 1995, ADV ALGORITHMS NEURA; MIZUTANI E, 2000, P INT WORKSH INT SYS, P100; MIZUTANI E, 1999, P 8 INT FUZZ SYST AS, V2, P959; MIZUTANI E, 1999, P 8 IEEE INT C FUZZ, V2, P858; MIZUTANI E, 2001, UNPUB GEN DOGLEG TRU; MIZUTANI E, 1997, NEUROFUZZY SOFT COMP, P129; MIZUTANI E, 1999, P IEEE INT C NEUR NE, V2, P1239; MOLLER MF, 1993, NEURAL NETWORKS, V6, P525, DOI 10.1016/S0893-6080(05)80056-5; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; POWELL M. J. D., 1970, NONLINEAR PROGRAMMIN, DOI DOI 10.1016/B978-0-12-597050-1.50006-3; Shepherd A.J., 1997, 2 ORDER METHODS NEUR; STEIHAUG T, 1983, SIAM J NUMER ANAL, V20, P626, DOI 10.1137/0720042; TOINT PL, 1987, SIAM J SCI STAT COMP, V8, P416, DOI 10.1137/0908042	16	6	6	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						605	611						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800086
C	Nelson, JD; Movellan, JR		Leen, TK; Dietterich, TG; Tresp, V		Nelson, JD; Movellan, JR			Active inference in concept learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				DATA SELECTION; INFORMATION	People are active experimenters, not just passive observers, constantly seeking new information relevant to their goals. A reasonable approach to active information gathering is to ask questions and conduct experiments that maximize the expected information gain, given current beliefs (Fedorov 1972, MacKay 1992, Oaksford & Chater 1994). In this paper we present results on an exploratory experiment designed to study people's active information gathering behavior on a concept learning task (Tenenbaum 2000). The results of the experiment are analyzed in terms of the expected information gain of the questions asked by subjects.	Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Nelson, JD (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.			Nelson, Jonathan/0000-0002-1956-6691				Anderson J. R., 1990, ADAPTIVE CHARACTER T; Fedorov V.V., 1972, THEORY OPTIMAL EXPT; KLAYMAN J, 1987, PSYCHOL REV, V94, P211, DOI 10.1037/0033-295X.94.2.211; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; OAKSFORD M, 1994, PSYCHOL REV, V101, P608, DOI 10.1037/0033-295X.101.4.608; Popper K. R. S, 1959, LOGIC SCI DISCOVERY; Tenenbaum JB, 2000, ADV NEUR IN, V12, P59; WASON PC, 1960, Q J EXP PSYCHOL, V12, P129, DOI 10.1080/17470216008416717	8	6	6	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						45	51						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800007
C	St-Aubin, R; Hoey, J; Boutilier, C		Leen, TK; Dietterich, TG; Tresp, V		St-Aubin, R; Hoey, J; Boutilier, C			APRICODD: Approximate policy construction using decision diagrams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We propose a method of approximate dynamic programming for Markov decision processes (MDPs) using algebraic decision diagrams (ADDs). We produce near-optimal value functions and policies with much lower time and space requirements than exact dynamic programming. Our method reduces the sizes of the intermediate value functions generated during value iteration by replacing the values at the terminals of the ADD with ranges of values. Our method is demonstrated on a class of large MDPs (with up to 34 billion states), and we compare the results with the optimal value functions.	Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada	University of British Columbia	St-Aubin, R (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada.							BOUTILIER C, 1996, P ICML 96 BAR IT; BOUTILIER C, 1995, P 14 INT C AI IJCAI; Bryant RE, 1986, IEEE T COMPUT, VC-35, P8; CLARKE EM, 1993, DES AUT C, P54; Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x; Dearden R, 1997, ARTIF INTELL, V89, P219, DOI 10.1016/S0004-3702(96)00023-9; HOEY J, 2000, TR0005 UBC; HOEY J, 1999, P UAI99 STOCKH; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; Somenzi F., 1998, CUDD CU DECISION DIA; [No title captured]	12	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1089	1095						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800153
C	Tjan, BS		Leen, TK; Dietterich, TG; Tresp, V		Tjan, BS			Adaptive object representation with hierarchically-distributed memory sites	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				RECOGNITION	Theories of object recognition often assume that only one representation scheme is used within one visual-processing pathway. Versatility of the visual system comes from having multiple visual-processing pathways, each specialized in a different category of objects. We propose a theoretically simpler alternative, capable of explaining the same set of data and more. A single primary visual-processing pathway, loosely modular, is assumed. Memory modules are attached to sites along this pathway. Object-identity decision is made independently at each site. A site's response time is a monotonic-decreasing function of its confidence regarding its decision. An observer's response is the first-arriving response from any site. The effective representation(s) of such a system, determined empirically, can appear to be specialized for different tasks and stimuli, consistent with recent clinical and functional-imaging findings. This, however, merely reflects a decision being made at its appropriate level of abstraction. The system itself is intrinsically flexible and adaptive.	Univ So Calif, Dept Psychol, Los Angeles, CA 90089 USA	University of Southern California	Tjan, BS (corresponding author), Univ So Calif, Dept Psychol, Los Angeles, CA 90089 USA.	btjan@usc.edu						BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; BIEDERMAN I, 1993, J EXP PSYCHOL HUMAN, V19, P1162, DOI 10.1037/0096-1523.19.6.1162; EDELMAN S, 1991, FEATURES RECOGNITION; Farah M.J., 1990, VISUAL AGNOSIA DISOR; Farah MJ., 1992, CURR DIR PSYCHOL SCI, V1, P164, DOI [10.1111/1467, DOI 10.1111/1467-8721.EP11510333]; Gauthier I, 1999, NAT NEUROSCI, V2, P568, DOI 10.1038/9224; ISHAI A, 1997, NEUROIMAGE, V5; JOLICOEUR P, 1985, MEM COGNITION, V13, P289, DOI 10.3758/BF03202498; JOLICOEUR P, 1984, COGNITIVE PSYCHOL, V16, P243, DOI 10.1016/0010-0285(84)90009-4; Kanwisher N, 1996, COGNITIVE BRAIN RES, V5, P55, DOI 10.1016/S0926-6410(96)00041-9; Kanwisher N, 1997, J COGNITIVE NEUROSCI, V9, P133, DOI 10.1162/jocn.1997.9.1.133; KANWISHER N, 1997, J NEUROSCI, V17, P1; Marr D., 1982, VISION; TARR MJ, 1995, J EXP PSYCHOL HUMAN, V21, P1494, DOI 10.1037/0096-1523.21.6.1494; Tjan BS, 1998, VISION RES, V38, P2335, DOI 10.1016/S0042-6989(97)00255-1	15	6	6	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						66	72						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800010
C	Liao, YS; Moody, J		Solla, SA; Leen, TK; Muller, KR		Liao, YS; Moody, J			Constructing heterogeneous committees using input feature grouping: Application to economic forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The committee approach has been proposed for reducing model uncertainty and improving generalization performance. The advantage of committees depends on (1) the performance of individual members and (2) the correlational structure of errors between members. This paper presents an input grouping technique for designing a heterogeneous committee. With this technique, all input variables are first grouped based on their mutual-information. Statistically similar variables are assigned to the same group. Each member's input set is then formed by input variables extracted from different groups. Our designed committees have less error correlation between its members, since each member observes different input variable combinations. The individual member's feature sets contain less redundant information, because highly correlated variables will not be combined together. The member feature sets contain almost complete information, since each set contains a feature from each information group. An empirical study for a noisy and nonstationary economic forecasting problem shows that committees constructed by our proposed technique outperform committees formed using several existing techniques.	Oregon Grad Inst, Dept Comp Sci, Portland, OR 97291 USA		Liao, YS (corresponding author), Oregon Grad Inst, Dept Comp Sci, POB 91000, Portland, OR 97291 USA.							[Anonymous], 1974, CLUSTER ANAL; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Cherkauer K. J., 1996, AAAI WORKSH INT MULT, P15; Moody J., 1993, NEURAL NETW WORLD, V3, P791; OPITZ D, 1996, ADV NEURAL INFORMATI, V8; Raviv Y., 1996, Connection Science, V8, P355, DOI 10.1080/095400996116811; Rosen B. E., 1996, Connection Science, V8, P373, DOI 10.1080/095400996116820; Tumer K., 1996, Connection Science, V8, P385, DOI 10.1080/095400996116839; WU L, 1996, NEURAL COMPUT, P463	10	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						921	927						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700130
C	Sallans, B		Solla, SA; Leen, TK; Muller, KR		Sallans, B			Learning factored representations for partially observable Markov decision processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				BELIEF NETWORKS	The problem of reinforcement learning in a non-Markov environment is explored using a dynamic Bayesian network, where conditional independence assumptions between random variables are compactly represented by network parameters. The parameters are learned on-line, and approximations are used to perform inference and to compute the optimal value function. The relative effects of inference and value function approximations on the quality of the final policy are investigated, by learning to solve a moderately difficult driving task. The two value function approximations, linear and quadratic, were found to perform similarly but the quadratic model was more sensitive to initialization. Both performed below the level of human performance on the task. The dynamic Bayesian network performed comparably to a model using a localist hidden state representation, while requiring exponentially fewer parameters.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 2Z9, Canada	University of Toronto	Sallans, B (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 2Z9, Canada.	sallans@cs.toronto.edu						BYEN X, 1998, P UAI 98; CHRISMAN L, 1992, 10 ANT C AI; COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D; DEAN T, 1989, COMPUTATIONAL INTELL, P5; Ghahramani Z., 1997, MACHINE LEARNING; JORDAN M, 1999, IN PRESS MACHINE LEA; KOLLER D, 1999, P IJCAI 99; Littman M., 1995, P INT C MACH LEARN; McCallum A. K., 1995, THESIS U ROCHESTER R; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Neal RM, 1993, CRGTR931 U TOR DEP C; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; RODRIGUEZ A, 2000, ADV NEURAL INFORMATI, V12; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]; SONDIK EJ, 1978, OPER RES, V26, P282, DOI 10.1287/opre.26.2.282; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; WHITEHEAD SD, 1991, MACH LEARN, P7	18	6	6	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1050	1056						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700148
C	Schuster, M		Solla, SA; Leen, TK; Muller, KR		Schuster, M			Better generative models for sequential data problems: bidirectional recurrent mixture density networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					This paper describes bidirectional recurrent mixture density networks, which can model multi-modal distributions of the type P(x(t)|y(1)(T)) andP (x(t)|x(1), x(2),..., x(t-1), y(1)(T)) without any explicit assumptions about the use of context. These expressions occur frequently in pattern recognition problems with sequential data, for example in speech recognition. Experiments show that the proposed generative models give a higher likelihood on test data compared to a traditional modeling approach, indicating that they can summarize the statistical properties of the data better.	ATR, Interpreting Telecommun Res Labs, Kyoto 61902, Japan		Schuster, M (corresponding author), ATR, Interpreting Telecommun Res Labs, 2-2 Hikaridai,Seika Cho, Kyoto 61902, Japan.							Bishop, 1995, NEURAL NETWORKS PATT; Bishop C.M., 1994, MIXTURE DENSITY NETW; *LING DAT CONS, 1993, TIMIT AC PHON CONT S; Riedmiller M., 1993, P IEEE INT C NEUR NE, P586; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; SCHUSTER M, 1999, THESIS NARA I SCI TE; Young S, 1996, IEEE SIGNAL PROC MAG, V13, P45, DOI 10.1109/79.536824	7	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						589	595						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700084
C	Vaithyanathan, S; Dom, B		Solla, SA; Leen, TK; Muller, KR		Vaithyanathan, S; Dom, B			Generalized model selection for unsupervised learning in high dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We describe a Bayesian approach to model selection in unsupervised learning that determines both the feature set and the number of clusters, We then evaluate this scheme (based on marginal likelihood) and one based on cross-validated likelihood, For the Bayesian scheme we derive a closed-form solution of the marginal likelihood by assuming appropriate forms of the likelihood function and prior. Extensive experiments compare these approaches and all results are verified by comparison against ground truth. In these experiments the Bayesian scheme using our objective function gave better results than cross-validation.	IBM, Almaden Res Ctr, San Jose, CA 95136 USA	International Business Machines (IBM)	Vaithyanathan, S (corresponding author), IBM, Almaden Res Ctr, 650 Harry Rd, San Jose, CA 95136 USA.							Baker D.L., 1998, DISTRIBUTIONAL CLUST; Bernardo J. M., 1994, BAYESIAN THEORY; CHURCH KW, 1995, POISSON MIXTURES NAT, V1; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DEMPSTER A, 1977, MAXIMUM LIKELIHOOD I, P39; EPREIRA FCN, 1993, DISTRIBUTIONAL CLUST; Hanson R., 1991, BAYESIAN CLASSIFICAT; Harshman R. A., 1990, INDEXING LATENT SEMA; IYENGAR G, 1998, CLUSTERING IMAGES US; KATZ SM, 1996, DISTRIBUTION CONTENT, P2; KONTKANEN PT, 1996, ISIS 96 C; MEILA M, MSRTR9806; Nigam K., 1998, LEARNING CLASSIFY TE; RISSANEN J, 1992, US JAP C FRONT STAT; Rissanen Jorma, 1989, STOCHASTIC COMPLEXIT; SINGHAL A, 1996, PIVOTED DOCUMENT LEN; Smyth P, 1996, CLUSTERING USING MON; VAITHYANATHAN S, 1998, RJ1013795012 IBM	18	6	6	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						970	976						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700137
C	Granger, E; Grossberg, S; Rubin, MA; Streilein, WW		Kearns, MS; Solla, SA; Cohn, DA		Granger, E; Grossberg, S; Rubin, MA; Streilein, WW			Familiarity discrimination of radar pulses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NEURAL-NETWORK ARCHITECTURE; FUZZY ARTMAP; RECOGNITION; EMAP	The ARTMAP-FD neural network performs both identification (placing test patterns in classes encountered during training) and familiarity discrimination (judging whether a test pattern belongs to any of the classes encountered during training). The performance of ARTMAP-FD is tested on radar pulse data obtained in the field, and compared to that of the nearest-neighbor-based NEN algorithm and to a k > 1 extension of NEN.	Ecole Polytech, Dept Elect & Comp Engn, Montreal, PQ H3C 3A7, Canada	Universite de Montreal; Polytechnique Montreal	Granger, E (corresponding author), Ecole Polytech, Dept Elect & Comp Engn, Montreal, PQ H3C 3A7, Canada.							CARPENTER GA, 1995, IEEE T NEURAL NETWOR, V6, P805, DOI 10.1109/72.392245; CARPENTER GA, 1991, NEURAL NETWORKS, V4, P759, DOI 10.1016/0893-6080(91)90056-B; CARPENTER GA, 1992, IEEE T NEURAL NETWOR, V3, P698, DOI 10.1109/72.159059; CARPENTER GA, 1997, ICNN 97 P IEEE INT C; CARPENTER GA, 1997, INTELLIGENT ENG SYST, V7, P23; Dasarathy B.V., 1991, NEAREST NEIGHBOR NN; DASARATHY BV, 1977, APPLICATIONS RES INF, V1, P114; Granger E, 1998, SIGNAL PROCESS, V64, P249, DOI 10.1016/S0165-1684(97)00194-1; Rubin MA, 1995, NEURAL NETWORKS, V8, P1109, DOI 10.1016/0893-6080(95)00064-X	9	6	6	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						875	881						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700123
C	Koenig, S		Kearns, MS; Solla, SA; Cohn, DA		Koenig, S			Exploring unknown environments with real-time search or reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Learning Real-Time A* (LRTA*) is a popular control method that interleaves planning and plan execution and has been shown to solve search problems in known environments efficiently. In this paper, we apply LRTA* to the problem of getting to a given goal location in an initially unknown environment. Uninformed LRTA* with maximal lookahead always moves on a shortest path to the closest unvisited state, that is, to the closest potential goal state. This was believed to be a good exploration heuristic, but we show that it does not minimize the worst-case plan-execution time compared to other uninformed exploration methods. This result is also of interest to reinforcement-learning researchers since many reinforcement learning methods use asynchronous dynamic programming, interleave planning and plan execution, and exhibit optimism in the face of uncertainty, just like LRTA*.	Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Koenig, S (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	skoenig@cc.gatech.edu						BARTO AG, 1995, ARTIF INTELL, V72, P81, DOI 10.1016/0004-3702(94)00011-O; DASGUPTA P, 1994, ARTIF INTELL, V71, P195, DOI 10.1016/0004-3702(94)90066-3; Davies S, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P753; KEARNS M, 1998, P 15 INT C MACH LEAR, P260; Koenig S., 1996, Proceedings of the Thirteenth National Conference on Artificial Intelligence and the Eighth Innovative Applications of Artificial Intelligence Conference; Koenig S., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P19, DOI 10.1145/238061.238065; KOENIG S, 1995, P 14 INT JOINT C ART, P1660; KOENIG S, 1997, P AAAI 97 WORKSH LIN; KORF RE, 1990, ARTIF INTELL, V42, P189, DOI 10.1016/0004-3702(90)90054-4; MOORE AW, 1993, MACH LEARN, V13, P103, DOI 10.1007/BF00993104; PEMBERTON J, 1992, P INT C ART INT PLAN, P179; RUSSELL SJ, 1991, P 12 INT JOINT C ART, P212; Thrun S, 1998, ARTIFICIAL INTELLIGENCE AND MOBILE ROBOTS, P21	13	6	6	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1003	1009						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700141
C	Love, BC		Kearns, MS; Solla, SA; Cohn, DA		Love, BC			Utilizing time: Asynchronous binding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				CORTEX	Historically, connectionist systems have not excelled at representing and manipulating complex structures. How can a system composed of simple neuron-like computing elements encode complex relations? Recently, researchers have begun to appreciate that representations can extend in both time and space. Many researchers have proposed that the synchronous firing of units can encode complex representations. I identify the limitations of this approach and present an asynchronous model of binding that effectively represents complex structures. The asynchronous model extends the synchronous approach. I argue that our cognitive architecture utilizes a similar mechanism.	Northwestern Univ, Dept Psychol, Evanston, IL 60208 USA	Northwestern University	Love, BC (corresponding author), Northwestern Univ, Dept Psychol, Evanston, IL 60208 USA.							ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629; Bienenstock E, 1996, BRAIN THEORY BIOL BA; GRAY CM, 1989, P NATL ACAD SCI USA, V86, P1698, DOI 10.1073/pnas.86.5.1698; HUMMEL JE, 1992, PSYCHOL REV, V99, P480, DOI 10.1037/0033-295X.99.3.480; LOVE BC, 1998, UNPUB ASYNCHRONOUS C; Markman A.B, 1993, ADV NEURAL INFORM PR, P855; MIYASHITA Y, 1988, NATURE, V331, P68, DOI 10.1038/331068a0; POLLACK JB, 1990, ARTIF INTELL, V46, P77, DOI 10.1016/0004-3702(90)90005-K; RALL W, 1967, J NEUROPHYSIOL, V30, P1169, DOI 10.1152/jn.1967.30.5.1169; RATCLIFF R, 1982, J EXP PSYCHOL-HUM L, V8, P16, DOI 10.1037/0278-7393.8.1.16; SHASTRI L, 1993, BEHAV BRAIN SCI, V16, P417, DOI 10.1017/S0140525X00030910; Softky WR, 1996, NEURAL NETWORKS, V9, P15, DOI 10.1016/0893-6080(95)00012-7; TANG AC, 1996, P 19 ANN C COGN SCI, P852; TREISMAN A, 1982, COGNITIVE PSYCHOL, V14, P107, DOI 10.1016/0010-0285(82)90006-8; VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0; VONDERMALSBURG C, 1981, 812 M PLANCK I BIOL; [No title captured]	17	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						38	44						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700006
C	Opper, M; Winther, O		Kearns, MS; Solla, SA; Cohn, DA		Opper, M; Winther, O			Mean field methods for classification with Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				FEEDFORWARD NEURAL NETWORKS; SPIN-GLASS; EQUATIONS; MODEL	We discuss the application of TAP mean field methods known from the Statistical Mechanics of disordered systems to Bayesian classification models with Gaussian processes. In contrast to previous approaches, no knowledge about the distribution of inputs is needed. Simulation results for the Sonar data set are given.	Aston Univ, Div Elect Engn & Comp Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Opper, M (corresponding author), Aston Univ, Div Elect Engn & Comp Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.							Barber D, 1997, ADV NEUR IN, V9, P340; GIBBS MN, 1997, VARIATIONAL GAUSSIAN; GORMAN RP, 1988, NEURAL NETWORKS, V1, P75, DOI 10.1016/0893-6080(88)90023-8; MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018; MEZARD M, 1987, LECT NOTES PHYSICS, V9; Neal R. M., 1996, LECT NOTES STAT; Neal RM, 1997, 9702 U TOR DEP STAT; Opper M, 1996, PHYS REV LETT, V76, P1964, DOI 10.1103/PhysRevLett.76.1964; Opper M, 1997, ADV NEUR IN, V9, P225; PARISI G, 1995, J PHYS A-MATH GEN, V28, P5267, DOI 10.1088/0305-4470/28/18/016; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Williams CKI, 1996, ADV NEUR IN, V8, P514; WONG KYM, 1995, EUROPHYS LETT, V30, P245, DOI 10.1209/0295-5075/30/4/010	15	6	8	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						309	315						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700044
C	Oyama, E; Tachi, S		Kearns, MS; Solla, SA; Cohn, DA		Oyama, E; Tachi, S			Coordinate transformation learning of hand position feedback controller by using change of position error norm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In order to grasp an object, we need to solve the inverse kinematics problem, i.e., the coordinate transformation from the visual coordinates to the joint angle vector coordinates of the arm. Although several models of coordinate transformation learning have been proposed, they suffer from a number of drawbacks. In human motion control, the learning of the hand position error feedback controller in the inverse kinematics solver is important. This paper proposes a navel model of the coordinate transformation learning of the human visual feedback controller that uses the change of the joint angle vector and the corresponding change of the square of the hand position error norm. The feasibility of the proposed model is illustrated using numerical simulations.	Mech Engn Lab, Tsukuba, Ibaraki 3058564, Japan	National Institute of Advanced Industrial Science & Technology (AIST)	Oyama, E (corresponding author), Mech Engn Lab, Namiki 1-2, Tsukuba, Ibaraki 3058564, Japan.		Oyama, Eimei/K-1259-2012	Oyama, Eimei/0000-0002-0647-1976				Guenther F.H., 1997, SELF ORG COMPUTATION, P383; JORDAN MI, 1988, 8827 COINS, P1; KAWATO M, 1987, BIOL CYBERN, V57, P169, DOI 10.1007/BF00364149; MCRUER DT, 1963, IEEE T HUM FACT ENG, V8, P38; Ogino T, 1997, HAND SURG, V2, P79	5	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1038	1044						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700146
C	Ghahramani, Z; Hinton, GE		Jordan, MI; Kearns, MJ; Solla, SA		Ghahramani, Z; Hinton, GE			Hierarchical non-linear factor analysis and topographic maps	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We first describe a hierarchical, generative model that can be viewed as a non-linear generalisation of factor analysis and can be implemented in a neural network. The model performs perceptual inference in a probabilistically consistent manner by using top-down, bottom-up and lateral connections. These connections can be learned using simple rules that require only locally available information. We then show how to incorporate lateral connections into the generative model. The model extracts a sparse, distributed, hierarchical representation of depth from simplified random-dot stereograms and the localised disparity detectors in the first hidden layer form a topographic map. When presented with image patches from natural scenes, the model develops topographically organised local feature detectors.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada	University of Toronto	Ghahramani, Z (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.								0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						486	492						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700069
C	Marbach, P; Mihatsch, O; Schulte, M; Tsitsiklis, JN		Jordan, MI; Kearns, MJ; Solla, SA		Marbach, P; Mihatsch, O; Schulte, M; Tsitsiklis, JN			Reinforcement learning for call admission control and routing in integrated service networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In integrated service communication networks, an important problem is to exercise call admission control and routing so as to optimally use the network resources. This problem is naturally formulated as a dynamic programming problem, which, however, is too complex to be solved exactly. We use methods of reinforcement learning (RL), together with a decomposition approach, to find call admission control and routing policies. The performance of our policy for a network with approximately 10(45) different feature configurations is compared with a commonly used heuristic policy.	MIT, LIDS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Marbach, P (corresponding author), MIT, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.								0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						922	928						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700130
C	Moss, E; Utgoff, P; Cavazos, J; Precup, D; Stefanovic, D; Brodley, C; Scheeff, D		Jordan, MI; Kearns, MJ; Solla, SA		Moss, E; Utgoff, P; Cavazos, J; Precup, D; Stefanovic, D; Brodley, C; Scheeff, D			Learning to schedule straight-line code	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Program execution speed on modern computers is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor. To realize potential execution efficiency, an optimizing compiler must employ a heuristic algorithm for instruction scheduling. Such algorithms are painstakingly hand-crafted, which is expensive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, obtaining the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code (also called basic blocks of instructions). Our empirical results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly optimally with respect to the features used.	Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Moss, E (corresponding author), Univ Massachusetts, Dept Comp Sci, Amherst, MA 01003 USA.								0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						929	935						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700131
C	Vinje, WE; Gallant, JL		Jordan, MI; Kearns, MJ; Solla, SA		Vinje, WE; Gallant, JL			Modeling complex cells in an awake macaque during natural image viewing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We model the responses of cells in visual area V1 during natural vision. Our model consists of a classical energy mechanism whose output is divided by nonclassical gain control and texture contrast mechanisms. We apply this model to review movies, a stimulus sequence that replicates the stimulation a cell receives during free viewing of natural images. Data were collected from three cells using five different review movies, and the model was fit separately to the data from each movie. For the energy mechanism alone we find modest but significant correlations (r(E) = 0.41, 0.43, 0.59, 0.35) between model and data. These correlations are improved somewhat when we allow for suppressive surround effects (r(E+G) = 0.42, 0.56, 0.60, 0.37). In one case the inclusion of a delayed suppressive surround dramatically improves the fit to the data by modifying the time course of the model's response.	Univ Calif Berkeley, Dept Mol & Cellular Biol, Div Neurobiol, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Vinje, WE (corresponding author), Univ Calif Berkeley, Dept Mol & Cellular Biol, Div Neurobiol, Berkeley, CA 94720 USA.								0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						236	242						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700034
C	Brown, TX		Mozer, MC; Jordan, MI; Petsche, T		Brown, TX			Adaptive access control applied to ethernet data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper presents a method that decides which combinations of traffic can be accepted on a packet data link, so that quality of service (QoS) constraints can be met. The method uses samples of QoS results at different load conditions to build a neural network decision function. Previous similar approaches to the problem have a significant bias. This bias is likely to occur in any real system and results in accepting loads that miss QoS targets by orders of magnitude. Preprocessing the data to either remove the bias or provide a confidence level, the method was applied to sources based on difficult-to-analyze ethernet data traces. With this data, the method produces an accurate access control function that dramatically outperforms analytic alternatives. Interestingly, the results depend on throwing away more than 93% of the data.			Brown, TX (corresponding author), UNIV COLORADO,DEPT ELECT & COMP ENGN,BOULDER,CO 80309, USA.								0	6	6	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						932	938						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00131
C	Finch, AM; Wilson, RC; Hancock, ER		Mozer, MC; Jordan, MI; Petsche, T		Finch, AM; Wilson, RC; Hancock, ER			Softening discrete relaxation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper describes a new framework for relational graph matching. The starting point is a recently reported Bayesian consistency measure which gauges structural differences using Hamming distance. The main contributions of the work are threefold. Firstly, we demonstrate how the discrete components of the cost function can be softened. The second contribution is to show how the softened cost function can be used to locate matches using continuous non-linear optimisation. Finally, we show how the resulting graph matching algorithm relates to the standard quadratic assignment problem.			Finch, AM (corresponding author), UNIV YORK,DEPT COMP SCI,YORK YO1 5DD,N YORKSHIRE,ENGLAND.		Hancock, Edwin R/C-6071-2008; Hancock, Edwin/N-7548-2019	Hancock, Edwin R/0000-0003-4496-2028; Hancock, Edwin/0000-0003-4496-2028					0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						438	444						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00062
C	Kruger, WF; Hasler, P; Minch, BA; Koch, C		Mozer, MC; Jordan, MI; Petsche, T		Kruger, WF; Hasler, P; Minch, BA; Koch, C			An adaptive WTA using floating gate technology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We have designed, fabricated, and tested an adaptive Winner-Take-All (WTA) circuit based upon the classic WTA of Lazzaro, et al [1]. We have added a time dimension (adaptation) to this circuit to make the input derivative an important factor in winner selection. To accomplish this, we have modified the classic WTA circuit by adding floating gate transistors which slowly null their inputs over time. We present a simplified analysis and experimental data of this adaptive WTA fabricated in a standard CMOS 2 mu m process.			Kruger, WF (corresponding author), CALTECH,PASADENA,CA 91125, USA.			Koch, Christof/0000-0001-6482-8067					0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						720	726						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00102
C	Plate, T; Bert, J; Band, P; Grace, J		Mozer, MC; Jordan, MI; Petsche, T		Plate, T; Bert, J; Band, P; Grace, J			A comparison between neural networks and other statistical techniques for modeling the relationship between tobacco and alcohol and cancer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Epidemiological data is traditionally analyzed with very simple techniques. Flexible models, such as neural networks, have the potential to discover unanticipated features in the data. However, to be useful, flexible models must have effective control on overfitting. This paper reports on a comparative study of the predictive quality of neural networks and other flexible models applied to real and artificial epidemiological data. The results suggest that there are no major unanticipated complex features in the real data, and also demonstrate that MacKay's [1995] Bayesian neural network methodology provides effective control on overfitting while retaining the ability to discover complex features in the artificial data.			Plate, T (corresponding author), BRITISH COLUMBIA CANC AGCY,601 W 10TH AVE,VANCOUVER,BC V5Z 1L3,CANADA.								0	6	6	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						967	973						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00136
C	Riesenhuber, M; Dayan, P		Mozer, MC; Jordan, MI; Petsche, T		Riesenhuber, M; Dayan, P			Neural models for part-whole hierarchies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present a connectionist method for representing images that explicitly addresses their hierarchical nature. It blends data from neuroscience about whole-object viewpoint sensitive cells in inferotemporal cortex(8) and attentional basis-field modulation in V4(3) with ideas about hierarchical descriptions based on microfeatures.(5,11) The resulting model makes critical use of bottom-up and top-down pathways for analysis and synthesis.(6) We illustrate the model with a simple example of representing information about faces.			Riesenhuber, M (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E25-618,CAMBRIDGE,MA 02139, USA.			Riesenhuber, Maximilian/0000-0002-5744-0408					0	6	6	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						17	23						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00003
C	Tresp, V; Neuneier, R; Zimmermann, HG		Mozer, MC; Jordan, MI; Petsche, T		Tresp, V; Neuneier, R; Zimmermann, HG			Early brain damage	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Optimal Brain Damage (OBD) is a method for reducing the number of weights in a neural network. OBD estimates the increase in cost function if weights are pruned and is a valid approximation if the learning algorithm has converged into a local minimum. On the other hand it is often desirable to terminate the learning process before a local minimum is reached (early stopping). In this paper we show that OBD estimates the increase in cost function incorrectly if the network is not in a local minimum. We also show how OBD can be extended such that it can be used in connection with early stopping. We call this new approach Early Brain Damage, EBD. EBD also allows to revive already pruned weights. We demonstrate the improvements achieved by EBD using three publicly available data sets.			Tresp, V (corresponding author), SIEMENS AG,CORP TECHNOL,OTTO HAHN RING 6,D-81730 MUNICH,GERMANY.								0	6	6	1	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						669	675						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00095
C	Zeevi, AJ; Meir, R; Adler, RJ		Mozer, MC; Jordan, MI; Petsche, T		Zeevi, AJ; Meir, R; Adler, RJ			Time series prediction using mixtures of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We consider the problem of prediction of stationary time series, using the architecture known as mixtures of experts (MEM). Here we suggest a mixture which blends several autoregressive models. This study focuses on some theoretical foundations of the prediction problem in this context. More precisely, it is demonstrated that this model is a universal approximator, with respect to learning the unknown prediction function. This statement is strengthened as upper bounds on the mean squared error are established. Based on these results it is possible to compare the MEM to other families of models (e.g., neural networks and state dependent models). It is shown that a degenerate version of the MEM is in fact equivalent to a neural network, and the number of experts in the architecture plays a similar role to the number of hidden units in the latter model.			Zeevi, AJ (corresponding author), STANFORD UNIV,DEPT ELECT ENGN,INFORMAT SYST LAB,STANFORD,CA 94305, USA.								0	6	6	0	8	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						309	315						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00044
C	DeWeese, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		DeWeese, M			Optimization principles for the neural code	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,SLOAN CTR,LA JOLLA,CA 92037	Salk Institute									0	6	6	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						281	287						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00040
C	Kremer, SC		Touretzky, DS; Mozer, MC; Hasselmo, ME		Kremer, SC			Finite state automata that recurrent cascade-correlation cannot represent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV ALBERTA,DEPT COMP SCI,EDMONTON,AB T6H 5B5,CANADA	University of Alberta									0	6	6	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						612	618						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00087
C	Redish, AD; Touretzky, DS		Touretzky, DS; Mozer, MC; Hasselmo, ME		Redish, AD; Touretzky, DS			Modeling interactions of the rat's place and head direction systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,DEPT COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	6	6	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						61	67						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00009
C	Schraudolph, NN; Sejnowski, TJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		Schraudolph, NN; Sejnowski, TJ			Tempering backpropagation networks: Not all weights are created equal	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						EVOTEC BIOSYST GMBH,D-22529 HAMBURG,GERMANY	Evotec			Sejnowski, Terrence/AAV-5558-2021						0	6	6	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						563	569						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00080
C	Stone, P; Veloso, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Stone, P; Veloso, M			Beating a defender in robotic soccer: Memory-based learning of a continuous function	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,DEPT COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	6	7	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						896	902						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00126
C	DORNAY, M; UNO, Y; KAWATO, M; SUZUKI, R		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DORNAY, M; UNO, Y; KAWATO, M; SUZUKI, R			SIMULATION OF OPTIMAL MOVEMENTS USING THE MINIMUM-MUSCLE-TENSION-CHANGE MODEL	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	6	6	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						627	634						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00077
C	HAUSSLER, D; OPPER, M; KEARNS, M; SCHAPIRE, R		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HAUSSLER, D; OPPER, M; KEARNS, M; SCHAPIRE, R			ESTIMATING AVERAGE-CASE LEARNING-CURVES USING BAYESIAN, STATISTICAL PHYSICS AND VC-DIMENSION METHODS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	6	6	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						855	862						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00105
C	Atzmon, Y; Kreuk, F; Shalit, U; Chechik, G		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Atzmon, Yuval; Kreuk, Felix; Shalit, Uri; Chechik, Gal			A causal view of compositional zero-shot recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					People easily recognize new visual categories that are new combinations of known components. This compositional generalization capacity is critical for learning in real-world domains like vision and language because the long tail of new combinations dominates the distribution. Unfortunately, learning systems struggle with compositional generalization because they often build on features that are correlated with class labels even if they are not "essential" for the class. This leads to consistent misclassification of samples from a new distribution, like new combinations of known components. Here we describe an approach for compositional generalization that builds on causal ideas. First, we describe compositional zero-shot learning from a causal perspective, and propose to view zero-shot inference as finding "which intervention caused the image?". Second, we present a causal-inspired embedding model that learns disentangled representations of elementary components of visual objects from correlated (confounded) training data. We evaluate this approach on two datasets for predicting new combinations of attribute-object pairs: A well-controlled synthesized images dataset and a real-world dataset which consists of fine-grained types of shoes. We show improvements compared to strong baselines.	[Atzmon, Yuval; Kreuk, Felix; Chechik, Gal] NVIDIA Res, Tel Aviv, Israel; [Kreuk, Felix; Chechik, Gal] Bar Ilan Univ, Ramat Gan, Israel; [Shalit, Uri] Technion Israel Inst Technol, Haifa, Israel	Bar Ilan University; Technion Israel Institute of Technology	Atzmon, Y (corresponding author), NVIDIA Res, Tel Aviv, Israel.	yatzmon@nvidia.com; gchechik@nvidia.com			Israel Science Foundation [1950/19]	Israel Science Foundation(Israel Science Foundation)	We thank Daniel Greenfeld, Idan Schwartz, Eli Meirom, Haggai Maron, Lior Bracha and Ohad Amosy for their helpful feedback on the early version. Uri Shalit was partially supported by the Israel Science Foundation (grant No. 1950/19).	Agrawal A, 2017, INT J COMPUT VISION, V123, P4, DOI 10.1007/s11263-016-0966-6; Agrawal Aishwarya, 2017, ARXIV170408243; Akuzawa Kei, 2019, P EUR C MACH LEARN P; Andreas J., 2016, ANN C N AM CHAPT ASS; [Anonymous], 2017, ARXIV171008347; Arjovsky Martin, 2019, ARXIV190702893; Atzmon Y., 2016, ARXIV160807639; Atzmon Yuval, 2018, P 34 C UNC ART INT; Atzmon Yuval, 2019, IEEE C COMP VIS PATT; Ba J., 2017, P 3 INT C LEARN REPR; Bahdanau Dzmitry, 2019, CLOSURE ASSESSING SY; Bauer Stefan, 2019, ICML; Bengio Yoshua, 2020, INT C LEARN REPR; Burgess Christopher P., 2018, PROC 31 C NEURAL INF; Chao R., 2016, ICCV; Chen T. Q. R., 2018, ADV NEURAL INFORM PR, P2610; de Haan Pim, 2019, NEURIPS; Ganin Y, 2016, J MACH LEARN RES, V17; Gao WH, 2016, PR MACH LEARN RES, V48; Gong MM, 2016, PR MACH LEARN RES, V48; Greenfeld Daniel, 2019, ARXIV191000270; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Heinze-Deml C., 2017, ARXIV171011469; Higgins I., 2017, ICLR; Isola P, 2015, PROC CVPR IEEE, P1383, DOI 10.1109/CVPR.2015.7298744; Jayaraman D., 2014, CVPR CVPR 14; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kato K, 2018, LECT NOTES COMPUT SC, V11218, P247, DOI 10.1007/978-3-030-01264-9_15; Ke Nan Rosemary, 2019, ARXIV191001075; Kilbertus N., 2018, ARXIV181200524; Kim B, 2019, PROC CVPR IEEE, P9004, DOI 10.1109/CVPR.2019.00922; Kingma D.P., 2015, INT C LEARN REPR, P1; Kocaoglu M., 2018, INT C LEARN REPR; Lake B., 2014, MORE HUMAN LIKE CONC; Lake Brenden M, 2017, BEHAV BRAIN SCI; Lampert CH, 2009, IEEE I CONF COMP VIS, P987, DOI 10.1109/ICCV.2009.5459359; Li Y., 2018, P EUR C COMP VIS, P624; Li Yingwei, 2020, CVPR; Locatello F., 2018, ARXIV181112359; Locatello Francesco, 2020, ICLR; Lopez-Paz D, 2017, PROC CVPR IEEE, P58, DOI 10.1109/CVPR.2017.14; Mathieu Emile, 2019, ICML; Misra I, 2017, PROC CVPR IEEE, P1160, DOI 10.1109/CVPR.2017.129; Mukherjee T., 2017, ARXIV171106047; Nagarajan T., 2018, P EUR C COMP VIS ECC, P169; Nagarajan Tushar, 2018, P EUR C COMP VIS ECC, P169; Nan Zhixiong, 2019, AAAI, V33; Neuberg LG, 2003, ECONOMET THEOR, V19, P675, DOI 10.1017/S026646603004109; Patterson Genevieve, 2016, EUR C COMP VIS; Paz-Argaman Tzuf, 2020, ZEST ZERO SHOT LEARN; Purushwalkam Senthil, 2019, ICCV; Rothenhausler D., 2018, ARXIV PREPRINT ARXIV; Rubenstein Paul K, 2018, ICLR WORKSH; Sanchez Eduardo Hugo, 2019, ARXIV191203915; Scholkopf B., 2012, P 29 INT C INT C MAC, P459; Scholkopf B., 2019, CAUSALITY MACHINE LE, DOI 1911.10500; Song L, 2012, J MACH LEARN RES, V13, P1393; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Suter Raphael, 2018, ARXIV181100007; Wei K, 2019, IEEE I CONF COMP VIS, P3740, DOI 10.1109/ICCV.2019.00384; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Yu A., 2014, COMPUTER VISION PATT; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819	64	5	5	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000012
C	Biggs, B; Ehrhardt, S; Joo, H; Graham, B; Vedaldi, A; Novotny, D		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Biggs, Benjamin; Ehrhardt, Sebastien; Joo, Hanbyul; Graham, Benjamin; Vedaldi, Andrea; Novotny, David			3D Multi-bodies: Fitting Sets of Plausible 3D Human Models to Ambiguous Image Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We consider the problem of obtaining dense 3D reconstructions of humans from single and partially occluded views. In such cases, the visual evidence is usually insufficient to identify a 3D reconstruction uniquely, so we aim at recovering several plausible reconstructions compatible with the input data. We suggest that ambiguities can be modelled more effectively by parametrizing the possible body shapes and poses via a suitable 3D model, such as SMPL for humans. We propose to learn a multi-hypothesis neural network regressor using a best-of-M loss, where each of the M hypotheses is constrained to lie on a manifold of plausible human poses by means of a generative model. We show that our method outperforms alternative approaches in ambiguous pose recovery on standard benchmarks for 3D humans, and in heavily occluded versions of these benchmarks.	[Biggs, Benjamin] Univ Cambridge, Dept Engn, Cambridge, England; [Ehrhardt, Sebastien] Univ Oxford, Visual Geometry Grp, Oxford, England; [Biggs, Benjamin; Joo, Hanbyul] Facebook AI Res, Menlo Pk, CA USA; [Biggs, Benjamin; Graham, Benjamin; Vedaldi, Andrea; Novotny, David] Facebook AI Res, London, England	University of Cambridge; University of Oxford; Facebook Inc; Facebook Inc	Biggs, B (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	bjb56@cam.ac.uk; hyenal@robots.ox.ac.uk; hjoo@fb.com; benjamingraham@fb.com; vedaldi@fb.com; dnovotny@fb.com			Facebook AI Research	Facebook AI Research(Facebook Inc)	The authors would like to thank Richard Turner for useful technical discussions relating to normalizing flows, and Philippa Liggins, Thomas Roddick and Nicholas Biggs for proof reading. This work was entirely funded by Facebook AI Research.	Akhter I., 2015, P CVPR; Andriluka M., 2014, P CVPR; Anguelov D., 2005, ACM T GRAPHICS; Bishop Christopher M, 1994, TECH REP; Bogo F., 2016, P ECCV; Chen YM, 2019, ENVIRON SCI ENG, P3, DOI 10.1007/978-981-13-2221-1_1; Dinh L, 2017, P ICLR; Guan P., 2009, P ICCV; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Huang YL, 2017, BILINGUAL MIND BRAIN, V5, P3, DOI 10.1007/978-3-319-64099-0_1; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Ionescu Cristian Sminchisescu Catalin, 2011, P ICCV; Johnson Sam, 2011, P CVPR; Joo Hanbyul, 2018, P CVPR; Kanazawa Angjoo, 2018, P CVPR; Kanazawa Angjoo, 2018, P ECCV; Kohli P, 2012, P ADV NEUR INF PROC, P1799; Kolotouros Nikos, 2019, P CVPR; Kolotouros Nikos, 2019, P ICCV; Lassner C., 2017, P IEEE C COMP VIS PA, V2, P3; Li C., 2019, P CVPR; Lin T., 2014, P ECCV; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Loper M, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818013; Martinez J., 2017, P CVPR; Mehta D., 2017, P SIGGRAPH; Mehta Dushyant, 2017, P 3DV, DOI [10.1109/ 3dv. 2017.00064, DOI 10.1109/3DV.2017.00064.URL]; Omran Mohamed, 2018, P 3DV; Pavlakos Georgios, 2018, P CVPR; Pavlakos Georgios, 2019, P CVPR; Rogez Gregory, 2018, PAMI; Rupprecht C, 2017, P ICCV; Sharma Saurabh, 2019, P ICCV; Sidenbladh H., 2000, LNCS, V2, P702; Sigal Leonid, 2008, P NEURIPS; Sminchisescu C, 2003, PROC CVPR IEEE, P69; SMINCHISESCU C, 2005, DISCRIMINATIVE DENSI, P390, DOI DOI 10.1109/CVPR.2005.132; Sohn Kihyuk, 2015, P NEURIPS; Sun Xiao, 2018, P ECCV; Tan V., 2017, P BMVC; Tung Hsiao-Yu Fish, 2017, P NEURIPS; Varol G., 2018, P ECCV; von Marcard Timo, 2018, P ECCV; Xiang Donglai, 2019, P CVPR; Xu Hongyi, 2020, P CVPR; Zanfir A., 2020, P ECCV; Zanfir Andrei, 2018, P CVPR	48	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000001
C	Goldblum, M; Fowl, L; Goldstein, T		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Goldblum, Micah; Fowl, Liam; Goldstein, Tom			Adversarially Robust Few-Shot Learning: A Meta-Learning Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Previous work on adversarially robust neural networks for image classification requires large training sets and computationally expensive training procedures. On the other hand, few-shot learning methods are highly vulnerable to adversarial examples. The goal of our work is to produce networks which both perform well at few-shot classification tasks and are simultaneously robust to adversarial examples. We develop an algorithm, called Adversarial Querying (AQ), for producing adversarially robust meta-learners, and we thoroughly investigate the causes for adversarial vulnerability. Moreover, our method achieves far superior robust performance on few-shot image classification tasks, such as Mini-ImageNet and CIFAR-FS, than robust transfer learning.	[Goldblum, Micah; Fowl, Liam] Univ Maryland, Dept Math, College Pk, MD 20742 USA; [Goldstein, Tom] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA	University System of Maryland; University of Maryland College Park; University System of Maryland; University of Maryland College Park	Goldblum, M (corresponding author), Univ Maryland, Dept Math, College Pk, MD 20742 USA.	goldblum@umd.edu; lfowl@umd.edu; tomg@cs.umd.edu			DARPA GARD program; DARPA QED program	DARPA GARD program; DARPA QED program	This work was supported by the DARPA GARD and DARPA QED programs, in addition to the National Science Foundation Division of Mathematical Sciences.	[Anonymous], 2017, ARXIV171209196; Athalye A., 2018, P 35 INT C MACH LEAR; Bengio Y., 2012, P ICML WORKSH UNS TR, P17, DOI DOI 10.1109/83.902291; Bertinetto Luca, 2018, INT C LEARN REPR; BI JS, 2017, ADV NEURAL INFORM PR, P108; Chen P.-Y., 2017, P ACM WORKSH ART INT; Chen W.-Y., 2019, INT C LEARN REPR; Finn C, 2017, PR MACH LEARN RES, V70; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Goldblum M., 2020, ARXIV200206753; Goldblum M., 2019, ARXIV190509747; Goldblum M., 2020, ARXIV200209565; Hu RZ, 2018, 2018 IEEE ASIA PACIFIC CONFERENCE ON CIRCUITS AND SYSTEMS (APCCAS 2018), P187, DOI 10.1109/APCCAS.2018.8605572; Kaiser O., 2017, P INT C LEARN REPR; Karras T., 2017, ICLR; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Ledig C., 2017, 2017 IEEE C COMP VIS; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Madry A., 2018, ARXIV PREPRINT ARXIV; Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Mustafa Aamir, 2019, IMAGE SUPER RESOLUTI; Ni Renkun, 2020, ARXIV201007092; Pfister T, 2014, LECT NOTES COMPUT SC, V8694, P814, DOI 10.1007/978-3-319-10599-4_52; Saadatpanah P., 2020, P INT C MACHINE LEAR, P8307; Samangouei P., 2018, P INT C LEARN REPR I; Shafahi Ali, 2019, ARXIV190508232; Shu Manli, 2020, ARXIV200908965; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Vartak M, 2017, ADV NEURAL INFORM PR, P6904; Vinyals Oriol, 2016, ARXIV160604080, P3630; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Yin Chengxiang, 2018, ARXIV180603316; Zhang H., 2019, ARXIV190108573; Zhao Yue, 2018, ARXIV181210217	35	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000062
C	Lyle, C; Schut, L; Ru, BX; Gal, YR; van der Wilk, MR		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Lyle, Clare; Schut, Lisa; Ru, Binxin; Gal, Yarin; van der Wilk, Mark			A Bayesian Perspective on Training Speed and Model Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We take a Bayesian perspective to illustrate a connection between training speed and the marginal likelihood in linear models. This provides two major insights: first, that a measure of a model's training speed can be used to estimate its marginal likelihood. Second, that this measure, under certain conditions, predicts the relative weighting of models in linear model combinations trained to minimize a regression loss. We verify our results in model selection tasks for linear models and for the infinite-width limit of deep neural networks. We further provide encouraging empirical evidence that the intuition developed in these settings also holds for deep neural networks trained with stochastic gradient descent. Our results suggest a promising new direction towards explaining why neural networks trained with stochastic gradient descent are biased towards functions that generalize well.	[Lyle, Clare; Schut, Lisa; Ru, Binxin; Gal, Yarin] Univ Oxford, OATML Grp, Oxford, England; [van der Wilk, Mark] Imperial Coll London, London, England	University of Oxford; Imperial College London	Lyle, C (corresponding author), Univ Oxford, OATML Grp, Oxford, England.	clare.lyle@cs.ox.ac.uk			Accenture Labs and Alan Turing Institute	Accenture Labs and Alan Turing Institute	PLisa Schut was supported by the Accenture Labs and Alan Turing Institute.	[Anonymous], 1993, P 6 ANN C COMPUTATIO, DOI DOI 10.1145/168304.168306; [Anonymous], 1992, THESIS CALIFORNIA I; Arora Sanjeev, 2019, ARXIV190108584; Basu D, 1955, SANKHYA, V15, P377; Belkin M., 2018, ARXIV181211118; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Blundell Charles, 2015, INT C MACH LEARN, V37, P1613; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; deAlexander G, 2017, NEURAL INFORM PROCES; Dutordoir V, 2020, PR MACH LEARN RES, V108, P1529; Duvenaud D, 2016, JMLR WORKSH CONF PRO, V51, P1070; Dziugaite G. K., 2018, ADV NEURAL INFORM PR, P8430; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Fort S., 2019, ARXIV190109491; Frankle J., 2019, TRAINABLE NEURAL NET, P05; Gal Y, 2016, PR MACH LEARN RES, V48; Germain P., 2016, ADV NEURAL INFORM PR, P1884; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Hardt Moritz, 2015, BENJAMIN RECHT YORAM; He Bobby, 2020, ARXIV200705864; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Jiang Y., 2019, FANTASTIC GENERALIZA; Kalimeris D., 2019, ADV NEURAL INFORM PR, V32, P3496; Khan Mohammad Emtiyaz E., 2019, P ADV NEURAL INFORM, V3094; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lee Jaehoon, 2018, INT C LEARN REPR; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Maddox Wesley J, 2019, ADV NEURAL INFORM PR, P13132; Matthews Alexander G. de G., 2018, P 7 INT C LEARN REPR, V3; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Nagarajan Vaishnavh, 2019, ADV NEURAL INFORM PR, P11615; Nakkiran P., 2019, ARXIV191202292; Negrea Jeffrey, 2019, ADV NEURAL INFORM PR, P11015; Osband I., 2018, ADV NEURAL INFORM PR, V31; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rasmussen CE, 2004, LECT NOTES ARTIF INT, V3176, P63, DOI 10.1007/978-3-540-28650-9_4; Rasmussen CE, 2001, ADV NEUR IN, V13, P294; Ru Binxin, 2020, REVISITING TRAIN LOS; Smith Samuel L, 2017, ARXIV171006451; Smith SL., 2018, DONT DECAY LEARNING; Valle-Perez Guillermo, 2018, ARXIV180508522; vanderWilk M., 2018, ARXIV E PRINTS; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wilson A.G., 2020, ARXIV200208791; Zhang Chiyuan, 2016, ARXIV161103530	47	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000006
C	Rezaee, M; Ferraro, F		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Rezaee, Mehdi; Ferraro, Francis			A Discrete Variational Recurrent Topic Model without the Reparametrization Trick	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We show how to learn a neural topic model with discrete random variables-one that explicitly models each word's assigned topic-using neural variational inference that does not rely on stochastic backpropagation to handle the discrete variables. The model we utilize combines the expressive power of neural methods for representing sequences of text with the topic model's ability to capture global, thematic coherence. Using neural variational inference, we show improved perplexity and document understanding across multiple corpora. We examine the effect of prior parameters both on the model and variational parameters, and demonstrate how our approach can compete and surpass a popular topic model implementation on an automatic measure of topic quality.	[Rezaee, Mehdi; Ferraro, Francis] Univ Maryland Baltimore Cty, Dept Comp Sci, Baltimore, MD 21250 USA	University System of Maryland; University of Maryland Baltimore County	Rezaee, M (corresponding author), Univ Maryland Baltimore Cty, Dept Comp Sci, Baltimore, MD 21250 USA.	rezaee1@umbc.edu; ferraro@umbc.edu			National Science Foundation [IIS-1940931]; Air Force Research Laboratory (AFRL), DARPA [FA8750-19-2-1003]	National Science Foundation(National Science Foundation (NSF)); Air Force Research Laboratory (AFRL), DARPA	We would like to thank members and affiliates of the UMBC CSEE Department, including Edward Raff, Cynthia Matuszek, Erfan Noury and Ahmad Mousavi. We would also like to thank the anonymous reviewers for their comments, questions, and suggestions. Some experiments were conducted on the UMBC HPCF. We'd also like to thank the reviewers for their comments and suggestions. This material is based in part upon work supported by the National Science Foundation under Grant No. IIS-1940931. This material is also based on research that is in part supported by the Air Force Research Laboratory (AFRL), DARPA, for the KAIROS program under agreement number FA8750-19-2-1003. The U.S.Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either express or implied, of the Air Force Research Laboratory (AFRL), DARPA, or the U.S. Government.	Nguyen AT, 2012, IEEE INT CONF AUTOM, P70, DOI 10.1145/2351676.2351687; Arora Sanjeev, 2018, ICLR; Batmanghelich Kayhan, 2016, P 54 ANN M ASS COMP, V2; Blei D.M., 2006, P 23 INT C MACH LEAR, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859, 10.1145/1143844.114385]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chowdhury Shammur Absar, 2018, P 27 INT C COMP LING; Crain Steven P, 2010, AMIA Annu Symp Proc, V2010, P132; Dieng Adji B., 2017, 5 INT C LEARN REPR I; Eisenstein J., 2011, ICML; Ferraro F., 2017, THESIS; Ferret Olivier, 1998, 36 ANN M ASS COMP LI, V2; Figurnov Mikhail, 2018, ADV NEURAL INFORM PR, V31, P441; Gao Jun, 2019, INT C LEARN REPR; Gupta Pankaj, 2018, ARXIV181003947; Gururangan Suchin, 2019, P 57 ANN M ASS COMP; Hajicova Eva, 2018, P 11 INT C LANG RES; Joo Weonyoung, 2019, DIRICHLET VARIATIONA; Kapoor K, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1061; Kingma D.P., 2013, P 2 INT C LEARN REPR; Lakretz Y, 2019, P 2019 C N AM CHAPT, V1; Larochelle H., 2012, ADV NEURAL INFORM PR, P2708; Lau JH, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P355, DOI 10.18653/v1/P17-1033; Li SY, 2017, AAAI CONF ARTIF INTE, P3223; LopezPaz David, 2018, ICLR; Lund Jeffrey, 2019, ACL; May Chandler, 2015, EMNLP; Merlo P, 2001, COMPUT LINGUIST, V27, P373, DOI 10.1162/089120101317066122; Miao YS, 2016, PR MACH LEARN RES, V48; Mikolov T, 2012, IEEE W SP LANG TECH, P234, DOI 10.1109/SLT.2012.6424228; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Mimno D., 2011, EMNLP; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mousavi Seyedahmad, 2019, ARXIV190801014; Nallapati Ramesh, 2017, ABS170800308 CORR; Paul Michael John, 2015, THESIS; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Reisinger J., 2010, P 27 INT C MACH LEAR, P903; Rezaee Mehdi, 2020, ARXIV201004361; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Ruiz Francisco JR, 2016, ARXIV161005683; Salakhutdinov R, 2009, ADV NEURAL INFORM PR; Samatthiyadikun P, 2018, PROCEEDINGS OF THE 7TH INTERNATIONAL CONFERENCE ON PATTERN RECOGNITION APPLICATIONS AND METHODS (ICPRAM 2018), P196, DOI 10.5220/0006654901960201; Srivastava Akash, 2017, ICLR; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tan S, 2016, IEEE W SP LANG TECH, P43, DOI 10.1109/SLT.2016.7846243; Wahabzada M, 2016, SCI REP-UK, V6, DOI 10.1038/srep22482; Wallach Hanna M, 2009, ADV NEURAL INFORM PR, P1973; Wang W, 2019, EURASIP J WIREL COMM, DOI 10.1186/s13638-018-1327-7; Wang Wenlin, 2017, AISTATS; Wang X., 2011, VISUAL ANAL HUMANS, P311, DOI DOI 10.1007/978-0-85729-997-0_16; Wen Tsung-Hsien, 2018, ARXIV180907070; Zaheer Manzil, 2015, P 53 ANN M ASS COMP, V1; Zhang YQ, 2017, CHIN OPT LETT, V15, DOI 10.3788/COL201715.030007	53	5	5	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000050
C	Smith, EJ; Calandra, R; Romero, A; Gkioxari, G; Meger, D; Malik, J; Drozdzal, M		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Smith, Edward J.; Calandra, Roberto; Romero, Adriana; Gkioxari, Georgia; Meger, David; Malik, Jitendra; Drozdzal, Michal			3D Shape Reconstruction from Vision and Touch	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					When a toddler is presented a new toy, their instinctual behaviour is to pick it up and inspect it with their hand and eyes in tandem, clearly searching over its surface to properly understand what they are playing with. At any instance here, touch provides high fidelity localized information while vision provides complementary global context. However, in 3D shape reconstruction, the complementary fusion of visual and haptic modalities remains largely unexplored. In this paper, we study this problem and present an effective chart-based approach to multi-modal shape understanding which encourages a similar fusion vision and touch information. To do so, we introduce a dataset of simulated touch and vision signals from the interaction between a robotic hand and a large array of 3D objects. Our results show that (1) leveraging both vision and touch signals consistently improves singlemodality baselines; (2) our approach outperforms alternative modality fusion methods and strongly benefits from the proposed chart-based structure; (3) the reconstruction quality increases with the number of grasps provided; and (4) the touch information not only enhances the reconstruction at the touch site but also extrapolates to its local neighborhood.	[Smith, Edward J.; Calandra, Roberto; Romero, Adriana; Gkioxari, Georgia; Malik, Jitendra; Drozdzal, Michal] Facebook AI Res, Menlo Pk, CA 94025 USA; [Smith, Edward J.; Romero, Adriana; Meger, David] McGill Univ, Montreal, PQ, Canada; [Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA	Facebook Inc; McGill University; University of California System; University of California Berkeley	Smith, EJ (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.; Smith, EJ (corresponding author), McGill Univ, Montreal, PQ, Canada.	ejsmith@fb.com			NSERC Canadian Robotics Network; Natural Sciences and Engineering Research Council; Fonds de recherche du Quebec -Nature et Technologies	NSERC Canadian Robotics Network; Natural Sciences and Engineering Research Council(Natural Sciences and Engineering Research Council of Canada (NSERC)); Fonds de recherche du Quebec -Nature et Technologies	We would like to acknowledge the NSERC Canadian Robotics Network, the Natural Sciences and Engineering Research Council, and the Fonds de recherche du Quebec -Nature et Technologies for their funding support, as granted to the McGill University authors. We would also like to thank Scott Fujimoto and Shaoxiong Wang for their helpful feedback.	Allen P., 1984, International Conference on Robotics, P394; [Anonymous], 2018, ARXIV180401654; [Anonymous], 2018, BLEND 3D MOD REND PA; Belhumeur PN, 1997, PROC CVPR IEEE, P1060, DOI 10.1109/CVPR.1997.609461; Bierbaum A, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3200, DOI 10.1109/IROS.2008.4650982; Bierbaum Alexander, 2008, 2008 8th IEEE-RAS International Conference on Humanoid Robots (Humanoids 2008), P360, DOI 10.1109/ICHR.2008.4756005; Bjorkman Marten, 2013, IEEE INT C INT ROB S; Bruna J., 2014, INT C LEARNING REPRE; Choy CB, 2016, LECT NOTES COMPUT SC, V9912, P628, DOI 10.1007/978-3-319-46484-8_38; Coumans Erwin, 2016, PYBULLET PYTHON MODU; Dame A, 2013, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2013.170; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Deprelle Theo, 2019, ADV NEURAL INFORM PR, P7435; Driess Danny, 2017, N IEEE RSJ INT C INT; Fan Haoqiang, 2017, IEEE C COMP VIS PATT, V38; Fouhey DF, 2013, IEEE I CONF COMP VIS, P3392, DOI 10.1109/ICCV.2013.421; Gandler GZ, 2020, ROBOT AUTON SYST, V126, DOI 10.1016/j.robot.2020.103433; Gao J., 2019, ADV NEURAL INFORM PR, P9609; Gkioxari G, 2019, IEEE I CONF COMP VIS, P9784, DOI 10.1109/ICCV.2019.00988; Groueix T., 2018, P EUR C COMP VIS ECC, P230; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Guo LH, 2017, ICT EXPRESS, V3, P67, DOI 10.1016/j.icte.2017.05.004; Hane C, 2014, PROC CVPR IEEE, P652, DOI 10.1109/CVPR.2014.89; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hane Christian, 2017, ARXIV170400710; Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084; Henderson Paul, 2018, ARXIV180709259; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Ilonen J, 2014, INT J ROBOT RES, V33, P321, DOI 10.1177/0278364913497816; Insafutdinov Eldar, 2018, ARXIV181009381; Jack Dominic, 2018, ARXIV180310932; Jamali N, 2016, IEEE-RAS INT C HUMAN, P179, DOI 10.1109/HUMANOIDS.2016.7803275; JATAVALLABHULA K, 2019, ARXIV191105063; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jost J, 2008, UNIVERSITEXT, P1; Kanazawa Angjoo, 2018, ARXIV180307549; Kar A., 2017, P NIPS, P365; Kato H., 2017, ARXIV171107566; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; KOENDERINK JJ, 1992, PERCEPT PSYCHOPHYS, V52, P487, DOI 10.3758/BF03206710; Lambeta M, 2020, IEEE ROBOT AUTOM LET, V5, P3838, DOI 10.1109/LRA.2020.2977257; Lee MA, 2019, IEEE INT CONF ROBOT, P8943, DOI 10.1109/ICRA.2019.8793485; Lim J. H., 2019, ADV NEURAL INFORM PR, P8994; Luo S., 2017, ROBOTIC TACTILE PERC; Martinez-Hernandez Uriel, 2013, 2013 WORLD HAPT C WH; Mescheder L., 2019, P IEEE C COMP VIS PA; Murthy J. Krishna, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P724, DOI 10.1109/ICRA.2017.7989089; Novotny D, 2017, IEEE I CONF COMP VIS, P5228, DOI 10.1109/ICCV.2017.558; Ottenhaus S, 2016, IEEE-RAS INT C HUMAN, P850, DOI 10.1109/HUMANOIDS.2016.7803372; Pezzementi Zachary, 2011, IEEE International Conference on Robotics and Automation, P5942; PHONG BT, 1975, COMMUN ACM, V18, P311, DOI 10.1145/360825.360839; Qi C. R., 2017, IEEE P COMPUT VIS PA, V1, P4, DOI DOI 10.1109/CVPR.2017.16; Riegler G, 2017, PROC CVPR IEEE, P6620, DOI 10.1109/CVPR.2017.701; Rock J, 2015, PROC CVPR IEEE, P2484, DOI 10.1109/CVPR.2015.7298863; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; SimLab, 2016, ALL HAND OV; Smith Edward, 2019, P MACHINE LEARNING R, V97, P5866; Smith Edward J., 2018, ADV NEURAL INFORM PR, P6479; Smith Edward J., 2017, ABS170709557 CORR; Sommer N, 2014, IEEE INT CONF ROBOT, P6400, DOI 10.1109/ICRA.2014.6907804; Sun Xingyuan, 2018, ABS180404610 CORR; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Velickovic Petar, 2018, INT C LEAR REPR ICLR; Wang SX, 2018, IEEE INT C INT ROBOT, P1606, DOI 10.1109/IROS.2018.8593430; Watkins-Valls D, 2019, IEEE INT CONF ROBOT, P7339, DOI 10.1109/ICRA.2019.8794233; Welling M, 2017, 5 INT C LEARN REPRES; Wu Jiajun, 2017, ADV NEURAL INFORM PR; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yi ZK, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4925, DOI 10.1109/IROS.2016.7759723; Yuan W, 2018, INT CONF 3D VISION, P728, DOI 10.1109/3DV.2018.00088	72	5	5	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000003
C	Amid, E; Warmuth, MK; Anil, R; Koren, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Amid, Ehsan; Warmuth, Manfred K.; Anil, Rohan; Koren, Tomer			Robust Bi-Tempered Logistic Loss Based on Bregman Divergences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RELATIVE LOSS BOUNDS	We introduce a temperature into the exponential function and replace the softmax output layer of the neural networks by a high-temperature generalization. Similarly, the logarithm in the loss we use for training is replaced by a low-temperature logarithm. By tuning the two temperatures, we create loss functions that are non-convex already in the single layer case. When replacing the last layer of the neural networks by our bi-temperature generalization of the logistic loss, the training becomes more robust to noise. We visualize the effect of tuning the two temperatures in a simple setting and show the efficacy of our method on large datasets. Our methodology is based on Bregman divergences and is superior to a related two-temperature method that uses the Tsallis divergence.	[Amid, Ehsan; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA; [Koren, Tomer] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel; [Amid, Ehsan; Warmuth, Manfred K.; Anil, Rohan; Koren, Tomer] Google Brain, Mountain View, CA 94043 USA	University of California System; University of California Santa Cruz; Tel Aviv University; Google Incorporated	Amid, E (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.; Amid, E (corresponding author), Google Brain, Mountain View, CA 94043 USA.	eamid@google.com; manfred@google.com; rohananil@google.com; tkoren@google.com			NSF [IIS-1546459]	NSF(National Science Foundation (NSF))	We would like to thank Jerome Rony for pointing out that early stopping improves the accuracy of the logistic loss on the noisy MNIST experiment. This research was partially supported by the NSF grant IIS-1546459.	Abadi M, 2015, P 12 USENIX S OPERAT; Amid E, 2019, PR MACH LEARN RES, V89; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Buja A., 2005, LOSS FUNCTIONS BINAR; Ding N., 2010, ADV NEURAL INF PROCE, V23, P514; Ding Nan, 2013, THESIS; HARDT M., 2017, P 5 INT C LEARN REPR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Helmbold DP, 1999, IEEE T NEURAL NETWOR, V10, P1291, DOI 10.1109/72.809075; Kivinen J, 2001, MACH LEARN, V45, P301, DOI 10.1023/A:1017938623079; Krizhevsky A, 2009, LEARNING MULTIPLE LA; LeCun Y., 1998, MNIST DATABASE HANDW; Long P. M., 2008, P 25 INT C MACH LEAR, P608, DOI DOI 10.1145/1390156.1390233; Naudts J, 2002, PHYSICA A, V316, P323, DOI 10.1016/S0378-4371(02)01018-X; Reid MD, 2009, P 26 ANN INT C MACH, P897; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Williamson RC, 2016, J MACH LEARN RES, V17, P1; Zeiler Matthew D, 2012, ARXIV12125701; Zhang ZL, 2018, ADV NEUR IN, V31	22	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906064
C	Apostolopoulou, I; Linderman, S; Miller, K; Dubrawski, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Apostolopoulou, Ifigeneia; Linderman, Scott; Miller, Kyle; Dubrawski, Artur			Mutually Regressive Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TIME-RESCALING THEOREM; HAWKES PROCESSES; MODELS	Many real-world data represent sequences of interdependent events unfolding over time. They can be modeled naturally as realizations of a point process. Despite many potential applications, existing point process models are limited in their ability to capture complex patterns of interaction. Hawkes processes admit many efficient inference algorithms, but are limited to mutually excitatory effects. Nonlinear Hawkes processes allow for more complex influence patterns, but for their estimation it is typically necessary to resort to discrete-time approximations that may yield poor generative models. In this paper, we introduce the first general class of Bayesian point process models extended with a nonlinear component that allows both excitatory and inhibitory relationships in continuous time. We derive a fully Bayesian inference algorithm for these processes using Polya-Gamma augmentation and Poisson thinning. We evaluate the proposed model on single and multi-neuronal spike train recordings. Results demonstrate that the proposed model, unlike existing point process models, can generate biologically-plausible spike trains, while still achieving competitive predictive likelihoods.	[Apostolopoulou, Ifigeneia] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Linderman, Scott] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Miller, Kyle; Dubrawski, Artur] Carnegie Mellon Univ, AutonLab, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Stanford University; Carnegie Mellon University	Apostolopoulou, I (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	iapostol@andrew.cmu.edu; scott.linderman@stanford.edu; mille856@andrew.cmu.edu; awd@cs.cmu.edu		Linderman, Scott/0000-0002-3878-9073	DARPA [FA8750-17-2-013]; Alexander Onassis Foundation; A. G. Leventis Foundation graduate fellowship	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Alexander Onassis Foundation; A. G. Leventis Foundation graduate fellowship	This work was partially supported by DARPA under award FA8750-17-2-013, in part by the Alexander Onassis Foundation graduate fellowship and in part by the A. G. Leventis Foundation graduate fellowship. We would also like to thank Sibi Venkatesan and Jeremy Cohen for their useful feedback on the paper and Alex Reinhart for helpful discussions.	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Alaa Ahmed M, 2017, P 34 INT C MACH LEAR, V70, P60; [Anonymous], JAMIA; [Anonymous], 2014, ICML; Bacry E., 2015, MARKET MICROSTRUCTUR, V1, P1; Blanche Tim, 2016, MULTINEURON RECORDIN; Brown EN, 2002, NEURAL COMPUT, V14, P325, DOI 10.1162/08997660252741149; Chen Y, 2019, J COMPUT NEUROSCI, V46, P19, DOI 10.1007/s10827-018-0695-7; Choi Edward, 2016, JMLR Workshop Conf Proc, V56, P301; COX DR, 1955, J ROY STAT SOC B, V17, P129; Daley D. J., 2007, INTRO THEORY POINT P, V2; Darlington TR, 2018, NAT NEUROSCI, V21, P1442, DOI 10.1038/s41593-018-0233-y; Donner C., 2018, J MACHINE LEARNING R, V19, P2710; Eichler M, 2017, J TIME SER ANAL, V38, P225, DOI 10.1111/jtsa.12213; Faul AC, 2002, ADV NEUR IN, V14, P383; FOTHERINGHAM AS, 1991, ENVIRON PLANN A, V23, P1025, DOI 10.1068/a231025; Gerhard F, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005390; Gerhard F, 2011, NEURAL COMPUT, V23, P1452, DOI 10.1162/NECO_a_00126; Gross A, 2019, COMMUN BIOL, V2, DOI 10.1038/s42003-018-0268-3; Guillame-Bert M, 2017, J MACH LEARN RES, V18; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; HAWKES AG, 1974, J APPL PROBAB, V11, P493, DOI 10.2307/3212693; Hawkes AG, 2018, QUANT FINANC, V18, P193, DOI 10.1080/14697688.2017.1403131; Johnson DH, 1996, J COMPUT NEUROSCI, V3, P275, DOI 10.1007/BF00161089; Kass RE, 2014, SPRINGER SER STAT, P491, DOI 10.1007/978-1-4614-9602-1_17; Kingman John Frank Charles, 1992, POISSON PROCESSES, V3; Lemonnier RAlmi, 2017, AAAI; LEWIS PAW, 1979, NAV RES LOG, V26, P403, DOI 10.1002/nav.3800260304; Linderman Scott, 2016, ADV NEURAL INFORM PR, P2002; Linderman Scott, 2015, ADV NEURAL INFORM PR, P3456; Linderman SW, 2015, ARXIV150703228; Maffei A, 2004, NAT NEUROSCI, V7, P1353, DOI 10.1038/nn1351; Mark B, 2019, IEEE T INFORM THEORY, V65, P2953, DOI 10.1109/TIT.2018.2875766; Mei H., 2017, ADV NEURAL INFORM PR, P6754; Miscouridou Xenia, 2018, ADV NEURAL INFORM PR, V31, P2343; Mohler GO, 2011, J AM STAT ASSOC, V106, P100, DOI 10.1198/jasa.2011.ap09546; Mongillo G, 2018, NAT NEUROSCI, V21, P1463, DOI 10.1038/s41593-018-0226-x; Parr T, 2018, FRONT HUM NEUROSCI, V12, DOI 10.3389/fnhum.2018.00061; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5; Reinhart A, 2018, STAT SCI, V33, P299, DOI 10.1214/17-STS629; Ross RJH, 2017, NPJ SYST BIOL APPL, V3, DOI 10.1038/s41540-017-0010-7; RUBIN I, 1972, IEEE T INFORM THEORY, V18, P547, DOI 10.1109/TIT.1972.1054897; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Vernon I, 2018, BMC SYST BIOL, V12, DOI 10.1186/s12918-017-0484-3; Wang QH, 2021, NAT PROD RES, V35, P1292, DOI 10.1080/14786419.2019.1645660; Wang YC, 2016, GEOTECH SP, P226; Xu H, 2016, INT C MACH LEARN, P1717; Xu H., 2017, ADV NEURAL INFORM PR, P1354; Xu HT, 2017, PR MACH LEARN RES, V70; Xu Hongteng, 2018, P MACH LEARN RES, V80, P5443; Yang SQ, 2013, 2013 1ST INTERNATIONAL WORKSHOP ON THE ENGINEERING OF MOBILE-ENABLED SYSTEMS (MOBS), P1, DOI 10.1109/MOBS.2013.6614215; Yang Y., 2017, NEURAL INFORM PROCES; Young R, 2016, PROCEEDINGS OF CIE 2016 LIGHTING QUALITY AND ENERGY EFFICIENCY, P79	57	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305015
C	Barak, B; Chou, CN; Lei, ZX; Schramm, T; Sheng, YQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Barak, Boaz; Chou, Chi-Ning; Lei, Zhixian; Schramm, Tselil; Sheng, Yueqi			(Nearly) Efficient Algorithms for the Graph Matching Problem on Correlated Random Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the graph matching/similarity problem of determining how similar two given graphs G(0),G(1) are and recovering the permutation pi on the vertices of G(1) that minimizes the symmetric difference between the edges of G(0) and pi(G(1)). Graph matching/similarity has applications for pattern matching, computer vision, social network anonymization, malware analysis, and more. We give the first efficient algorithms proven to succeed in the correlated Erdos-Renyi model (Pedarsani and Grossglauser, 2011). Specifically, we give a polynomial time algorithm for the graph similarity/hypothesis testing task which works for every constant level of correlation between the two graphs that can be arbitrarily close to zero. We also give a quasi-polynomial (n(O(log n)) time) algorithm for the graph matching task of recovering the permutation minimizing the symmetric difference in this model. This is the first algorithm to do so without requiring as additional input a "seed" of the values of the ground truth permutation on at least n(Omega(1)) vertices. Our algorithms follow a general framework of counting the occurrences of subgraphs from a particular family of graphs allowing for tradeoffs between efficiency and accuracy.	[Barak, Boaz; Chou, Chi-Ning; Lei, Zhixian; Schramm, Tselil; Sheng, Yueqi] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Barak, B (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	b@boazbarak.org; chiningchou@g.harvard.edu; leizhixian.research@gmail.com; tselil@seas.harvard.edu; ysheng@g.harvard.edu		Schramm, Tselil/0000-0002-8494-5778	NSF [CCF 1565264, CNS 1618026]	NSF(National Science Foundation (NSF))	Supported by NSF awards CCF 1565264 and CNS 1618026.	Bai Y., 2018, ARXIV; Bai YS, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P384, DOI 10.1145/3289600.3290967; Berg AC, 2005, PROC CVPR IEEE, P26; Bollobas Bela, 1981, RANDOM GRAPHS COMBIN, P80; Cho M, 2012, PROC CVPR IEEE, P398, DOI 10.1109/CVPR.2012.6247701; Conte D, 2004, INT J PATTERN RECOGN, V18, P265, DOI 10.1142/S0218001404003228; Cour T., 2007, P ADV NEURAL INFORM, P313; Cullina D, 2017, ARXIV171106783; Cullina D, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P63, DOI [10.1145/2964791.2901460, 10.1145/2896377.2901460]; Gascon H, 2013, P ACM C COMPUTER COM, P45, DOI [10.1145/2517312.2517315, DOI 10.1145/2517312.2517315]; Hattori Masahiro, 2003, Genome Inform, V14, P144; Heymans M, 2003, BIOINFORMATICS, V19, pi138, DOI 10.1093/bioinformatics/btg1018; Janson S, 2012, ANN APPL PROBAB, V22, P1989, DOI 10.1214/11-AAP822; Kazemi E, 2015, PROC VLDB ENDOW, V8, P1010, DOI 10.14778/2794367.2794371; Kipf TN, 2016, P INT C LEARN REPR; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; Korula N, 2014, PROC VLDB ENDOW, V7, P377, DOI 10.14778/2732269.2732274; Livi L, 2013, PATTERN ANAL APPL, V16, P253, DOI 10.1007/s10044-012-0284-8; Lyzinski V, 2016, IEEE T PATTERN ANAL, V38, P60, DOI 10.1109/TPAMI.2015.2424894; Lyzinski V, 2014, J MACH LEARN RES, V15, P3513; Lyzinski Vince, 2014, ARXIV14013813; Melnik S, 2002, PROC INT CONF DATA, P117, DOI 10.1109/ICDE.2002.994702; Narayanan A, 2009, P IEEE S SECUR PRIV, P173, DOI 10.1109/SP.2009.22; Park Y, 2010, P 6 ANN WORKSH CYB S, P1, DOI DOI 10.1145/1852666.1852716; Pedarsani Pedram, 2011, P 17 ACM SIGKDD INT, P1235, DOI [10.1145/2020408.2020596, DOI 10.1145/2020408.2020596]; Raymond JW, 2002, J CHEM INF COMP SCI, V42, P305, DOI 10.1021/ci010381f; Runwal N, 2012, J COMPUT VIROL HACKI, V8, P37, DOI 10.1007/s11416-012-0160-5; Singh R, 2008, P NATL ACAD SCI USA, V105, P12763, DOI 10.1073/pnas.0806627105; Vogelstein JT, 2011, ARXIV11125507; Xu K., 2018, INT C LEARN REPR; Yartseva L, 2013, P 1 ACM C ONL SOC NE, P119, DOI [10.1145/2512938.2512952, DOI 10.1145/2512938.2512952]; Zager LA, 2008, APPL MATH LETT, V21, P86, DOI 10.1016/j.aml.2007.01.006	33	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900074
C	Barreto, A; Borsa, D; Hou, SB; Comanici, G; Aygun, E; Hamel, P; Toyama, D; Hunt, J; Mourad, S; Silver, D; Precup, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Barreto, Andre; Borsa, Diana; Hou, Shaobo; Comanici, Gheorghe; Aygun, Eser; Hamel, Philippe; Toyama, Daniel; Hunt, Jonathan; Mourad, Shibl; Silver, David; Precup, Doina			The Option Keyboard Combining Skills in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The ability to combine known skills to create new ones may be crucial in the solution of complex reinforcement learning problems that unfold over extended periods. We argue that a robust way of combining skills is to define and manipulate them in the space of pseudo-rewards (or "cumulants"). Based on this premise, we propose a framework for combining skills using the formalism of options. We show that every deterministic option can be unambiguously represented as a cumulant defined in an extended domain. Building on this insight and on previous results on transfer learning, we show how to approximate options whose cumulants are linear combinations of the cumulants of known options. This means that, once we have learned options associated with a set of cumulants, we can instantaneously synthesise options induced by any linear combination of them, without any learning involved. We describe how this framework provides a hierarchical interface to the environment whose abstract actions correspond to combinations of basic skills. We demonstrate the practical benefits of our approach in a resource management problem and a navigation task involving a quadrupedal simulated robot.	[Barreto, Andre; Borsa, Diana; Hou, Shaobo; Comanici, Gheorghe; Aygun, Eser; Hamel, Philippe; Toyama, Daniel; Hunt, Jonathan; Mourad, Shibl; Silver, David; Precup, Doina] DeepMind, London, England		Barreto, A (corresponding author), DeepMind, London, England.	andrebarreto@google.com; borsa@google.com; shaobohou@google.com; gcomanici@google.com; eser@google.com; hamelphi@google.com; kenjitoyama@google.com; jjhunt@google.com; shibl@google.com; davidsilver@google.com; doinap@google.com						[Anonymous], 2018, INT C LEARN REPR; Bacon P.-L., 2017, AAAI; Barreto A., UNPUB; Barreto A., 2018, P INT C MACH LEARN I; Barreto A, 2017, ADV NEURAL INFORM PR; Borsa D., 2019, INT C LEARN REPR; da Silva M., 2009, ACM T GRAPHIC, V28, P82; Dayan P., 1993, ADV NEURAL INFORM PR; Degris T., 2014, P INT C MACH LEARN I; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Fox R., 2016, P C UNC ART INT UAI; Haarnoja T., 2018, ICML; Haarnoja T., 2017, ICML; Haarnoja T., 2018, IEEE INT C ROB AUT I; Heess Nicolas, 2016, CORR; Hunt J. J., 2019, P INT C MACH LEARN I; Kaelbling L. P., 2014, P INT C MACH LEARN I; Parr R., 1997, P C ADV NEUR INF PRO; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Saxe A. M., 2017, P INT C MACH LEARN I; Schulman J, 2016, INT C LEARNING REPRE; Sutton R., 2016, BARB WORKSH REINF LE; Sutton R. S., 2000, ADV NEURAL INFORM PR; Sutton R. S., 2011, INT C AUT AG MULT SY; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Todorov E., 2007, ADV NEURAL INFORM PR; Todorov E., 2012, INTELLIGENT ROBOTS S; Todorov E., 2009, ADV NEURAL INFORM PR; Van Niekerk B., 2019, P INT C MACH LEARN I; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Ziebart B. D., 2010, THESIS	33	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904066
C	Bechavod, Y; Ligett, K; Roth, A; Waggoner, B; Wu, ZS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bechavod, Yahav; Ligett, Katrina; Roth, Aaron; Waggoner, Bo; Wu, Zhiwei Steven			Equal Opportunity in Online Classification with Partial Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study an online classification problem with partial feedback in which individuals arrive one at a time from a fixed but unknown distribution, and must be classified as positive or negative. Our algorithm only observes the true label of an individual if they are given a positive classification. This setting captures many classification problems for which fairness is a concern: for example, in criminal recidivism prediction, recidivism is only observed if the inmate is released; in lending applications, loan repayment is only observed if the loan is granted. We require that our algorithms satisfy common statistical fairness constraints (such as equalizing false positive or negative rates - introduced as "equal opportunity" in [18]) at every round, with respect to the underlying distribution. We give upper and lower bounds characterizing the cost of this constraint in terms of the regret rate (and show that it is mild), and give an oracle efficient algorithm that achieves the upper bound.	[Bechavod, Yahav; Ligett, Katrina] Hebrew Univ Jerusalem, Jerusalem, Israel; [Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA; [Waggoner, Bo] Univ Colorado, Boulder, CO 80309 USA; [Wu, Zhiwei Steven] Univ Minnesota, Minneapolis, MN 55455 USA	Hebrew University of Jerusalem; University of Pennsylvania; University of Colorado System; University of Colorado Boulder; University of Minnesota System; University of Minnesota Twin Cities	Bechavod, Y (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.	yahav.bechavod@cs.huji.ac.il; katrina@cs.huji.ac.il; aaroth@cis.upenn.edu; bwag@colorado.edu; zsw@umn.edu		Waggoner, Bo/0000-0002-1366-1065; Wu, Steven/0000-0002-8125-8227; Ligett, Katrina/0000-0003-2780-6656	Israel Science Foundation (ISF) [1044/16]; United States Air Force; DARPA [FA8750-16-C-0022]; Federmann Cyber Security Center; Israel national cyber directorate; NSF [CCF-1763307]; Google Faculty Research Award; J.P. Morgan Faculty Award; Mozilla research grant; Facebook Research Award	Israel Science Foundation (ISF)(Israel Science Foundation); United States Air Force(United States Department of Defense); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Federmann Cyber Security Center; Israel national cyber directorate; NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); J.P. Morgan Faculty Award; Mozilla research grant; Facebook Research Award(Facebook Inc)	We thank Nati Srebro for a conversation leading to the question we study here. We thank Michael Kearns for helpful discussions at an early stage of this work. YB and KL were funded in part by Israel Science Foundation (ISF) grant 1044/16, the United States Air Force and DARPA under contract FA8750-16-C-0022, and the Federmann Cyber Security Center in conjunction with the Israel national cyber directorate. AR was funded in part by NSF grant CCF-1763307 and the United States Air Force and DARPA under contract FA8750-16-C-0022. ZSW was supported in part by a Google Faculty Research Award, a J.P. Morgan Faculty Award, a Mozilla research grant, and a Facebook Research Award. Part of this work was done while KL and ZSW were visiting the Simons Institute for the Theory of Computing, and BW was a postdoc at the University of Pennsylvania's Warren Center and at Microsoft Research, New York City. Any opinions, findings and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of JP Morgan, the United States Air Force and DARPA.	Agarwal A, 2018, PR MACH LEARN RES, V80; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bartok G, 2010, LECT NOTES ARTIF INT, V6331, P224, DOI 10.1007/978-3-642-16108-7_20; Berk Richard, 2018, SOCIOLOGICAL METHODS, V0; Beygelzimer A, 2011, INT C ART INT STAT, V15, P19; Blum Avrim, 2018, ADV NEURAL INFORM PR, P8386; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Cotter Andrew, 2018, ARXIV180904198; Daniely A., 2013, ARXIV13112272; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Ensign Danielle, 2018, PMLR, P160; Ensign Danielle, 2018, P 29 C ALG LEARN THE, V83, P359; Feldman M., 2015, KDD; Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Helmbold DP, 2000, INFORM COMPUT, V161, P85, DOI 10.1006/inco.2000.2870; Joseph M, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P158, DOI 10.1145/3278721.3278764; Joseph Matthew, 2016, NIPS, P325; Kalai AT, 2008, SIAM J COMPUT, V37, P1777, DOI 10.1137/060649057; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Radanovic G., 2017, CALIBRATED FAIRNESS	26	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900055
C	Bu, ZQ; Klusowski, JM; Rush, C; Su, WJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bu, Zhiqi; Klusowski, Jason M.; Rush, Cynthia; Su, Weijie			Algorithmic Analysis and Statistical Estimation of SLOPE via Approximate Message Passing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				THRESHOLDING ALGORITHM; REGRESSION SHRINKAGE; VARIABLE SELECTION; LASSO; SPARSITY	SLOPE is a relatively new convex optimization procedure for high-dimensional linear regression via the sorted l(1) penalty: the larger the rank of the fitted coefficient, the larger the penalty. This non-separable penalty renders many existing techniques invalid or inconclusive in analyzing the SLOPE solution. In this paper, we develop an asymptotically exact characterization of the SLOPE solution under Gaussian random designs through solving the SLOPE problem using approximate message passing (AMP). This algorithmic approach allows us to approximate the SLOPE solution via the much more amenable AMP iterates. Explicitly, we characterize the asymptotic dynamics of the AMP iterates relying on a recently developed state evolution analysis for non-separable penalties, thereby overcoming the difficulty caused by the sorted l(1) penalty. Moreover, we prove that the AMP iterates converge to the SLOPE solution in an asymptotic sense, and numerical simulations show that the convergence is surprisingly fast. Our proof rests on a novel technique that specifically leverages the SLOPE problem. In contrast to prior literature, our work not only yields an asymptotically sharp analysis but also offers an algorithmic, flexible, and constructive approach to understanding the SLOPE problem.	[Bu, Zhiqi] Univ Penn, Dept Appl Math & Computat Sci, Philadelphia, PA 19104 USA; [Klusowski, Jason M.] Rutgers State Univ, Dept Stat, New Brunswick, NJ 08854 USA; [Rush, Cynthia] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Su, Weijie] Univ Penn, Dept Stat, Philadelphia, PA 19104 USA	University of Pennsylvania; Rutgers State University New Brunswick; Columbia University; University of Pennsylvania	Bu, ZQ (corresponding author), Univ Penn, Dept Appl Math & Computat Sci, Philadelphia, PA 19104 USA.	zbu@sas.upenn.edu; jason.klusowski@rutgers.edu; cynthia.rush@columbia.edu; suw@wharton.upenn.edu						Barber RF, 2015, ANN STAT, V43, P2055, DOI 10.1214/15-AOS1337; Bayati M., 2013, ADV NEURAL INFORM PR, V26, P944; Bayati M, 2015, ANN APPL PROBAB, V25, P753, DOI 10.1214/14-AAP1010; Bayati M, 2012, IEEE T INFORM THEORY, V58, P1997, DOI 10.1109/TIT.2011.2174612; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bellec PC, 2018, ANN STAT, V46, P3603, DOI 10.1214/17-AOS1670; Berthier R., 2017, ARXIV170803950; Borgerding M, 2016, IEEE GLOB CONF SIG, P227, DOI 10.1109/GlobalSIP.2016.7905837; Brzyski D., 2018, J AM STAT ASSOC, P1; Bu Z., 2019, ARXIV190707502; Celentano M., 2019, ARXIV PREPRINT ARXIV; Chambolle A, 1998, IEEE T IMAGE PROCESS, V7, P319, DOI 10.1109/83.661182; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Donoho DL, 2011, IEEE T INFORM THEORY, V57, P6920, DOI 10.1109/TIT.2011.2165823; Doob J. L., 1953, STOCHASTIC PROCESSES, V101; Figueiredo MAT, 2016, JMLR WORKSH CONF PRO, V51, P930; Hu H., 2019, ARXIV190311582; Manoel A., 2018, ARXIV180906304; Mezard M., 2012, J STAT MECH THEORY E; Montanari A., 2012, COMPRESSED SENSING T, P394, DOI [10.1017/CBO9780511794308.010, DOI 10.1017/CBO9780511794308.010]; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Rangan S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2168, DOI 10.1109/ISIT.2011.6033942; Rangan S., 2019, IEEE T INFORM THEORY; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Royden H.L., 1968, REAL ANAL; Su WJ, 2017, ANN STAT, V45, P2133, DOI 10.1214/16-AOS1521; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Zeng X., 2014, LETTERS, V21, P1240	35	5	5	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901004
C	Caselles-Dupre, H; Garcia-Ortiz, M; Filliat, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Caselles-Dupre, Hugo; Garcia-Ortiz, Michael; Filliat, David			Symmetry-Based Disentangled Representation Learning requires Interaction with Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Finding a generally accepted formal definition of a disentangled representation in the context of an agent behaving in an environment is an important challenge towards the construction of data-efficient autonomous agents. Higgins et al. (2018) recently proposed Symmetry-Based Disentangled Representation Learning, a definition based on a characterization of symmetries in the environment using group theory. We build on their work and make observations, theoretical and empirical, that lead us to argue that Symmetry-Based Disentangled Representation Learning cannot only be based on static observations: agents should interact with the environment to discover its symmetries. Our experiments can be reproduced in Colab(1) and the code is available on GitHub(2).	[Caselles-Dupre, Hugo; Filliat, David] ENSTA Paris, Flowers Lab, Palaiseau, France; [Caselles-Dupre, Hugo; Filliat, David] INRIA, Rocquencourt, France; [Caselles-Dupre, Hugo; Garcia-Ortiz, Michael] Al Lab Softbank Robot Europe, Plymouth, Devon, England	Institut Polytechnique de Paris; Inria	Caselles-Dupre, H (corresponding author), ENSTA Paris, Flowers Lab, Palaiseau, France.; Caselles-Dupre, H (corresponding author), INRIA, Rocquencourt, France.; Caselles-Dupre, H (corresponding author), Al Lab Softbank Robot Europe, Plymouth, Devon, England.	caselles@ensta.fr; mgarciaortiz@softbankrobotics.com; david.filliat@ensta.fr	FILLIAT, David/ABD-6165-2020	FILLIAT, David/0000-0002-5739-1618				Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Caselles-DuprE Hugo, 2019, ARXIV190209434; Caselles-Dupre Hugo, 2018, WORKSH CONT UNS SENS; Ha D, 2018, ADV NEUR IN, V31; Higgins I., 2018, ARXIV PREPRINT ARXIV; Higgins I, 2017, PR MACH LEARN RES, V70; Kingma D.P, P 3 INT C LEARNING R; Locatello F, 2019, PR MACH LEARN RES, V97; Locatello Francesco, 2019, NEURIPS 2019; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Raffin Antonin, 2019, ICLR 2019 WORKSH STR; Soatto. Stefano, 2011, TECHNICAL REPORT; Thomas Valentin, 2017, ARXIV170801289; van Steenkiste Sjoerd, 2019, NEURIPS 2019	15	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304059
C	Chen, LS; Su, H; Ji, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Lisha; Su, Hui; Ji, Qiang			Deep Structured Prediction for Facial Landmark Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FACE ALIGNMENT; SHAPE; MODEL; NETWORK	Existing deep learning based facial landmark detection methods have achieved excellent performance. These methods, however, do not explicitly embed the structural dependencies among landmark points. They hence cannot preserve the geometric relationships between landmark points or generalize well to challenging conditions or unseen data. This paper proposes a method for deep structured facial landmark detection based on combining a deep Convolutional Network with a Conditional Random Field. We demonstrate its superior performance to existing state-of-the-art techniques in facial landmark detection, especially a better generalization ability on challenging datasets that include large pose and occlusion.	[Chen, Lisha; Su, Hui; Ji, Qiang] Rensselaer Polytech Inst, Troy, NY 12181 USA; [Su, Hui] IBM Res, Redmond, WA USA	Rensselaer Polytechnic Institute; International Business Machines (IBM)	Chen, LS (corresponding author), Rensselaer Polytech Inst, Troy, NY 12181 USA.	chenl21@rpi.edu; huisuibmres@us.ibm.com; jiq@rpi.edu	Chen, Lisha/HGC-6247-2022		NSF award IIS [1539012]; RPI-IBM Cognitive Immersive Systems Laboratory (CISL), a center in IBM's AI Horizon Network	NSF award IIS; RPI-IBM Cognitive Immersive Systems Laboratory (CISL), a center in IBM's AI Horizon Network(International Business Machines (IBM))	The work described in this paper is supported in part by NSF award IIS #1539012 and by RPI-IBM Cognitive Immersive Systems Laboratory (CISL), a center in IBM's AI Horizon Network.	Baltrusaitis T, 2014, LECT NOTES COMPUT SC, V8692, P593, DOI 10.1007/978-3-319-10593-2_39; Bengio Y, 1994, ADV NEURAL INF PROCE, P937; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Cao XD, 2014, INT J COMPUT VISION, V107, P177, DOI 10.1007/s11263-013-0667-3; Chen D, 2014, LECT NOTES COMPUT SC, V8694, P109, DOI 10.1007/978-3-319-10599-4_8; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen X, 2014, MITOCHONDRIAL DNA, V1736, P1, DOI DOI 10.3109/19401736.2013.855906; Chu X, 2016, PROC CVPR IEEE, P4715, DOI 10.1109/CVPR.2016.510; Chu Xiao, 2016, ADV NEURAL INFORM PR, P316; Cootes TF, 2001, IEEE T PATTERN ANAL, V23, P681, DOI 10.1109/34.927467; Corneanu Ciprian A., 2018, ECCV; Do T., 2010, INT C ART INT STAT, P177; Dong XY, 2018, PROC CVPR IEEE, P379, DOI 10.1109/CVPR.2018.00047; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Ghiasi Golnaz, 2015, ARXIV150608347; Girshick R, 2015, PROC CVPR IEEE, P437, DOI 10.1109/CVPR.2015.7298641; Jaderberg M., 2014, DEEP STRUCTURED OUTP; Jourabloo A, 2017, INT J COMPUT VISION, V124, P187, DOI 10.1007/s11263-017-1012-z; Kahraman F, 2010, TURK J ELECTR ENG CO, V18, P677, DOI 10.3906/elk-0906-48; Kaisheng Yao, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4077, DOI 10.1109/ICASSP.2014.6854368; Kumar Neeraj, 2008, 10 EUR C COMP VIS EC; LIN GS, 2016, PROC CVPR IEEE, P3194, DOI DOI 10.1109/CVPR.2016.348; Matthews I, 2004, INT J COMPUT VISION, V60, P135, DOI 10.1023/B:VISI.0000029666.37597.d3; Milborrow S, 2008, LECT NOTES COMPUT SC, V5305, P504, DOI 10.1007/978-3-540-88693-8_37; Morin F., 2005, PROC INT WORKSHOP AR, P246; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ning F, 2005, IEEE T IMAGE PROCESS, V14, P1360, DOI 10.1109/TIP.2005.852470; Radosavljevic V, 2010, FRONT ARTIF INTEL AP, V215, P809, DOI 10.3233/978-1-60750-606-5-809; Ristovski K., 2013, PROC AAAI C ARTIF IN, V27, P840; Sagonas C, 2016, IMAGE VISION COMPUT, V47, P3, DOI 10.1016/j.imavis.2016.01.002; Saragih J, 2007, IEEE I CONF COMP VIS, P2173; Saragih JM, 2011, INT J COMPUT VISION, V91, P200, DOI 10.1007/s11263-010-0380-4; Sun Xiao, 2017, ARXIV171108229; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Torresani L, 2008, IEEE T PATTERN ANAL, V30, P878, DOI 10.1109/TPAMI.2007.70752; Trigeorgis G, 2016, PROC CVPR IEEE, P4177, DOI 10.1109/CVPR.2016.453; Wang S, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON NANOSCALE ARCHITECTURE, P1; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wu WY, 2018, PROC CVPR IEEE, P2129, DOI 10.1109/CVPR.2018.00227; Xiao ST, 2017, IEEE I CONF COMP VIS, P1642, DOI 10.1109/ICCV.2017.181; Xiong XH, 2015, PROC CVPR IEEE, P2664, DOI 10.1109/CVPR.2015.7298882; Zadeh A, 2017, IEEE INT CONF COMP V, P2519, DOI 10.1109/ICCVW.2017.296; Zafeiriou S, 2017, IEEE COMPUT SOC CONF, P2116, DOI 10.1109/CVPRW.2017.263; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu SZ, 2016, PROC CVPR IEEE, P3409, DOI 10.1109/CVPR.2016.371; Zhu SZ, 2015, PROC CVPR IEEE, P4998, DOI 10.1109/CVPR.2015.7299134; Zhu Xiangyu, 2017, IEEE T PATTERN ANAL, DOI DOI 10.1109/TBDATA.2017.2736547	53	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302045
C	Chiley, V; Sharapov, I; Kosson, A; Koster, U; Reece, R; de la Fuente, SS; Subbiah, V; James, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chiley, Vitaliy; Sharapov, Ilya; Kosson, Atli; Koster, Urs; Reece, Ryan; de la Fuente, Sofia Samaniego; Subbiah, Vishal; James, Michael			Online Normalization for Training Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Online Normalization is a new technique for normalizing the hidden activations of a neural network. Like Batch Normalization, it normalizes the sample dimension. While Online Normalization does not use batches, it is as accurate as Batch Normalization. We resolve a theoretical limitation of Batch Normalization by introducing an unbiased technique for computing the gradient of normalized activations. Online Normalization works with automatic differentiation by adding statistical normalization as a primitive. This technique can be used in cases not covered by some other normalizers, such as recurrent networks, fully connected networks, and networks with activation memory requirements prohibitive for batching. We show its applications to image classification, image segmentation, and language modeling. We present formal proofs and experimental results on ImageNet, CIFAR, and PTB datasets.	[Chiley, Vitaliy; Sharapov, Ilya; Kosson, Atli; Koster, Urs; Reece, Ryan; de la Fuente, Sofia Samaniego; Subbiah, Vishal; James, Michael] Cerebras Syst, 175 S San Antonio Rd, Los Altos, CA 94022 USA		James, M (corresponding author), Cerebras Syst, 175 S San Antonio Rd, Los Altos, CA 94022 USA.	michael@cerebras.net						Amodei D., 2015, CORR; [Anonymous], 2018, ARXIV PREPRINT ARXIV; Arpit D, 2016, PR MACH LEARN RES, V48; Ba L. J., 2016, CORRABS160706450; CHAN TF, 1983, AM STAT, V37, P242, DOI 10.2307/2683386; Chiley Vitaliy, 2019, ONLINE NORMALIZATION; CICEK O., 2016, CORR; Cooijmans Tim, 2016, CORR; Finch T., 2009, U CAMB; Gitman I., 2017, COMP BATCH NORMALIZA; Goyal P., 2017, ACCURATE LARGE MINIB; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Ioffe Sergey, 2017, CORR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A., 2014, CORR; Krizhevsky Alex, CIFAR 10; Krogh A., 1991, ADV NEURAL INFORM PR, P950; Laurent C, 2016, INT CONF ACOUST SPEE, P2657, DOI 10.1109/ICASSP.2016.7472159; LeCun Y, 2010, ATT LAB; Lempitsky V., 2016, ARXIV160708022V3; Liao Qianli, 2016, CORR; Marcus Mitchell, 1994, P WORKSH HUM LANG TE, P114, DOI [10.3115/1075812.1075835, DOI 10.3115/1075812.1075835]; Page D., 2018, TRAIN YOUR RESNET; Press O., 2016, CORR; Ronneberger O., 2015, U NET CONVOLUTIONAL; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T., 2016, CORR; Shang WL, 2017, AAAI CONF ARTIF INTE, P1509; Sutskever I, 2013, P 30 INT C INT C MAC, V28; Usuyama Naoto, 2018, SIMPLE PYTORCH IMPLE; van Laarhoven T., 2017, CORR; Xiao H., 2017, ARXIV170807747	32	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900007
C	Ciosek, K; Vuong, Q; Loftin, R; Hofmann, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ciosek, Kamil; Quan Vuong; Loftin, Robert; Hofmann, Katja			Better Exploration with Optimistic Actor-Critic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFINITE-HORIZON; REINFORCEMENT	Actor-critic methods, a type of model-free Reinforcement Learning, have been successfully applied to challenging tasks in continuous control, often achieving state-of-the art performance. However, wide-scale adoption of these methods in real-world domains is made difficult by their poor sample efficiency. We address this problem both theoretically and empirically. On the theoretical side, we identify two phenomena preventing efficient exploration in existing state-of-the-art algorithms such as Soft Actor Critic. First, combining a greedy actor update with a pessimistic estimate of the critic leads to the avoidance of actions that the agent does not know about, a phenomenon we call pessimistic underexploration. Second, current algorithms are directionally uninformed, sampling actions with equal probability in opposite directions from the current mean. This is wasteful, since we typically need actions taken along certain directions much more than others. To address both of these phenomena, we introduce a new algorithm, Optimistic Actor Critic, which approximates a lower and upper confidence bound on the state-action value function. This allows us to apply the principle of optimism in the face of uncertainty to perform directed exploration using the upper bound while still using the lower bound to avoid overestimation. We evaluate OAC in several challenging continuous control tasks, achieving state-of the art sample efficiency.	[Ciosek, Kamil; Loftin, Robert; Hofmann, Katja] Microsoft Res Cambridge, Cambridge, England; [Quan Vuong] Univ Calif San Diego, La Jolla, CA 92093 USA; [Quan Vuong] Microsoft Res, Cambridge, England	Microsoft; University of California System; University of California San Diego; Microsoft	Ciosek, K (corresponding author), Microsoft Res Cambridge, Cambridge, England.	kamil.ciosek@microsoft.com; qvuong@ucsd.edu; t-roloft@microsoft.com; katja.hofmann@microsoft.com						Abdolmaleki A, 2015, ADV NEUR IN, V28; Abdolmaleki Abbas, 2018, 6 INT C LEARN REPR I; Akrour R, 2016, PR MACH LEARN RES, V48; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 2016, ABS160304467 CORR; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Baxter J, 2001, J ARTIF INTELL RES, V15, P351, DOI 10.1613/jair.807; Baxter J, 2000, ISCAS 2000: IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS - PROCEEDINGS, VOL III, P271, DOI 10.1109/ISCAS.2000.856049; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Callahan J. J., 2010, ADV CALCULUS GEOMETR; Ciosek K, 2018, AAAI CONF ARTIF INTE, P2868; Ciosek Kamil, 2018, ABS180103326 CORR; Depeweg S., 2016, ARXIV160507127; Efron Bradley, 1994, SIAM REV, V36, P677, DOI [10.1137/1036171, DOI 10.1137/1036171]; Fujimoto S., 2018, P 35 INT C MACH LEAR, P1582; Gal Y., 2016, DAT EFF MACH LEARN W, V4; Ghavamzadeh M, 2007, P 24 INT C MACH LEAR, P297, DOI DOI 10.1145/1273496.1273534; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Ghavamzadeh Mohammad, 2016, J MACHINE LEARNING R, V17; Gu, 2017, ADV NEURAL INFORM PR, V30, P3846; Gu S, 2017, 5 INT C LEARN REPR I; Ha S., 2018, ABS181205905 CORR; Haarnoja T., 2018, INT C MACH LEARN, P1856; Heess N., 2015, NIPS; Hunt J. J., 2016, P 4 INT C LEARN REPR; Jin C., 2018, ADV NEURAL INFORM PR; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kingma Diederik P., 2015, 3 INT C LEARN REPR I; Lim Sungsu, 2018, ABS181009103 CORR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; O'Donoghue Brendan, 2018, P 35 INT C MACH LEAR, P3836; Osband Ian, 2018, ADV NEURAL INFORM PR, V31, P8626; Peters J, 2006, 2006 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-12, P2219, DOI 10.1109/IROS.2006.282564; Puterman M.L., 2014, MARKOV DECISION PROC; Schulman, 2017, ARXIV170601502; Silver D, 2014, ICML ICML 14, P387; Sutton RS, 1996, ADV NEUR IN, V8, P1038; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Van Hasselt H, 2010, ADV NEURAL INFORM PR, P2613; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; van Hasselt Hado, 2018, ABS181202648 CORR; van Seijen H, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P177; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Ziebart B. D., 2010, THESIS	49	5	5	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301074
C	d'Ascoli, S; Sagun, L; Bruna, J; Biroli, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		d'Ascoli, Stephane; Sagun, Levent; Bruna, Joan; Biroli, Giulio			Finding the Needle in the Haystack with Convolutions: on the benefits of architectural bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite the phenomenal success of deep neural networks in a broad range of learning tasks, there is a lack of theory to understand the way they work. In particular, Convolutional Neural Networks (CNNs) are known to perform much better than Fully-Connected Networks (FCNs) on spatially structured data: the architectural structure of CNNs benefits from prior knowledge on the features of the data, for instance their translation invariance. The aim of this work is to understand this fact through the lens of dynamics in the loss landscape. We introduce a method that maps a CNN to its equivalent FCN (denoted as eFCN). Such an embedding enables the comparison of CNN and FCN training dynamics directly in the FCN space. We use this method to test a new training protocol, which consists in training a CNN, embedding it to FCN space at a certain "relax time", then resuming the training in FCN space. We observe that for all relax times, the deviation from the CNN subspace is small, and the final performance reached by the eFCN is higher than that reachable by a standard FCN of same architecture. More surprisingly, for some intermediate relax times, the eFCN outperforms the CNN it stemmed, by combining the prior information of the CNN and the expressivity of the FCN in a complementary way. The practical interest of our protocol is limited by the very large size of the highly sparse eFCN. However, it offers interesting insights into the persistence of architectural bias under stochastic gradient dynamics. It shows the existence of some rare basins in the FCN loss landscape associated with very good generalization. These can only be accessed thanks to the CNN prior, which helps navigate the landscape during the early stages of optimization.	[d'Ascoli, Stephane; Biroli, Giulio] Univ Paris Diderot, Sorbonne Univ, Univ PSL,Lab Phys, Sorbonne Paris Cite,Ecole Normale Super ENS,CNRS, Paris, France; [Sagun, Levent] Facebook, Facebook AI Res, Paris, France; [Bruna, Joan] NYU, Courant Inst Math, New York, NY USA; [Bruna, Joan] NYU, Ctr Data Sci, New York, NY USA	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Sorbonne Universite; Universite Paris Cite; Facebook Inc; New York University; New York University	d'Ascoli, S (corresponding author), Univ Paris Diderot, Sorbonne Univ, Univ PSL,Lab Phys, Sorbonne Paris Cite,Ecole Normale Super ENS,CNRS, Paris, France.	stephane.dascoli@ens.fr; leventsagun@fb.com; bruna@cims.nyu.edu; giulio.biroli@lps.ens.fr	dAscoli, Stéphane/AAE-1535-2022	d'Ascoli, Stephane/0000-0002-3131-3371	Simons Foundation [454935]; Alfred P. Sloan Foundation; NSF [RI-1816753]; NSF CAREER CIF [1845360]; Samsung Electronics	Simons Foundation; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); NSF(National Science Foundation (NSF)); NSF CAREER CIF; Samsung Electronics(Samsung)	We would like to thank Alp Riza Guler and Ilija Radosavovic for helpful discussions. We acknowledge funding from the Simons Foundation (#454935, Giulio Biroli). JB acknowledges the partial support by the Alfred P. Sloan Foundation, NSF RI-1816753, NSF CAREER CIF 1845360, and Samsung Electronics.	Achille A, 2017, ARXIV171108856; Anandkumar A., 2016, ARXIV161009322; Arora S, 2018, PR MACH LEARN RES, V80; Baldassi C, 2016, P NATL ACAD SCI USA, V113, pE7655, DOI 10.1073/pnas.1608103113; Baldassi Carlo, 2019, ARXIV190507833; Belkin M., 2018, ARXIV181211118; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Chaudhari P, 2016, ARXIV161101838; Chen TH, 2015, DES AUT CON, DOI 10.1145/2744769.2744837; Coates A., 2011, ADV NEURAL INFORM PR, P2528, DOI DOI 10.1016/J.PSYCHRES.2009.03.008; Daniel Freeman C, 2016, ARXIV161101540; Draxler F, 2018, PR MACH LEARN RES, V80; Du S. S., 2018, ADV NEURAL INFORM PR, P373; Frankle Jonathan, 2018, ARXIV180303635; Garipov T, 2018, ADV NEUR IN, V31; Geiger M., 2018, ARXIV180909349; Geiger M., 2019, ARXIV190101608, p2019a; Gunasekar S, 2018, PR MACH LEARN RES, V80; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Hinton G., 2015, ARXIV150302531; Jastrzebski Stanislaw, 2017, ARXIV171104623; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee Jaeho, 2018, ARXIV181209658; Long Philip M, 2019, ARXIV190512600; Neal Brady, 2018, ARXIV181008591; Neyshabur B., 2018, ARXIV180512076; Novak Roman, 2018, BAYESIAN DEEP CONVOL; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Sagun Levent, 2017, ARXIV170604454; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Soudry D, 2018, J MACH LEARN RES, V19; Venturi L., 2018, ARXIV PREPRINT ARXIV; Wu L., 2017, ARXIV170610239; Zhang Chiyuan, 2016, ARXIV161103530	38	5	4	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901001
C	Denevi, G; Stamos, D; Ciliberto, C; Pontil, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Denevi, Giulia; Stamos, Dimitris; Ciliberto, Carlo; Pontil, Massimiliano			Online-Within-Online Meta-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	We study the problem of learning a series of tasks in a fully online Meta-Learning setting. The goal is to exploit similarities among the tasks to incrementally adapt an inner online algorithm in order to incur a low averaged cumulative error over the tasks. We focus on a family of inner algorithms based on a parametrized variant of online Mirror Descent. The inner algorithm is incrementally adapted by an online Mirror Descent meta-algorithm using the corresponding within-task minimum regularized empirical risk as the meta-loss. In order to keep the process fully online, we approximate the meta-subgradients by the online inner algorithm. An upper bound on the approximation error allows us to derive a cumulative error bound for the proposed method. Our analysis can also be converted to the statistical setting by online-to-batch arguments. We instantiate two examples of the framework in which the meta-parameter is either a common bias vector or feature map. Finally, preliminary numerical experiments confirm our theoretical findings.	[Denevi, Giulia; Pontil, Massimiliano] Ist Italiano Tecnol, Genoa, Italy; [Denevi, Giulia] Univ Genoa, Genoa, Italy; [Stamos, Dimitris; Ciliberto, Carlo; Pontil, Massimiliano] UCL, London, England; [Ciliberto, Carlo] Imperial Coll London, London, England	Istituto Italiano di Tecnologia - IIT; University of Genoa; University of London; University College London; Imperial College London	Denevi, G (corresponding author), Ist Italiano Tecnol, Genoa, Italy.; Denevi, G (corresponding author), Univ Genoa, Genoa, Italy.	giulia.denevi@iit.it; c.ciliberto@imperial.ac.uk; d.stamos@ucl.ac.uk; m.pontil@ucl.ac.uk	Denevi, Giulia/AAF-9385-2021	Denevi, Giulia/0000-0001-7181-9660	EPSRC [EP/P009069/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported in part by EPSRC Grant N. EP/P009069/1.	Alquier P, 2017, PR MACH LEARN RES, V54, P261; [Anonymous], THESIS; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Balcan M.-F., 2019, INT C MACH LEARN, V97, P424; Bauschke H. H., 2011, CONVEX ANAL MONOTONE, V408; Baxter J, 1998, LEARNING TO LEARN, P71; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Boyd S, 2004, CONVEX OPTIMIZATION; Bullins B., 2019, ALGORITHMIC LEARNING, P235; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Ciliberto C, 2015, PR MACH LEARN RES, V37, P1548; Denevi G., 2019, INT C MACH LEARN, P1566; Denevi G., 2018, P 34 C UNC ART INT U; Denevi G., 2018, ADV NEURAL INFORM PR, P10190; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Finn C, 2019, PR MACH LEARN RES, V97; Finn C, 2017, PR MACH LEARN RES, V70; Gupta R, 2017, SIAM J COMPUT, V46, P992, DOI 10.1137/15M1050276; Jacob L., 2009, P 21TH NIPS, P745; Jean-Baptiste H.-U., 2010, CONVEX ANAL MINIMIZA; Kakade S., 2009, DUALITY STRONG UNPUB, V2; Khodak M., 2019, ARXIV190602717; Littlestone N., 1989, Proceedings of the Second Annual Workshop on Computational Learning Theory, P269; Maurer A, 2005, J MACH LEARN RES, V6, P967; Maurer A., 2013, INT C MACH LEARN; Maurer A, 2016, J MACH LEARN RES, V17; Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7; McDonald A. M., 2016, J MACHINE LEARNING R, V17, P1; Micchelli CA, 2013, ADV COMPUT MATH, V38, P455, DOI 10.1007/s10444-011-9245-9; Pentina A, 2016, ADV NEUR IN, V29; Pentina A, 2014, PR MACH LEARN RES, V32, P991; Ravi S., 2017, 15 INT C LEARN REPR; Shalev-Shwartz S., 2007, LOGARITHMIC REGRET A; Shalev-Shwartz S., 2007, ADV NEURAL INFORM PR, P1265; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shalev-Shwartz Shai, 2009, ADV NEURAL INFORM PR, P1457; Stange P., 2008, APP MATH MECH, V8, P10827, DOI DOI 10.1002/PAMM.200810827; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2; Triantafillou Eleni, 2019, ARXIV190303096	41	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904071
C	Diakonikolas, I; Gouleakis, T; Tzamos, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Diakonikolas, Ilias; Gouleakis, Themis; Tzamos, Christos			Distribution-Independent PAC Learning of Halfspaces with Massart Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POLYNOMIAL-TIME ALGORITHM; MODEL; PERCEPTRON	We study the problem of distribution -independent PAC learning of halfspaces in the presence of Massart noise. Specifically, we are given a set of labeled examples (x, y) drawn from a distribution 7) on Rd+1 such that the marginal distribution on the unlabeled points x is arbitrary and the labels y are generated by an unknown halfspace corrupted with Massart noise at noise rate n < 1/2. The goal is to find a hypothesis h that minimizes the misclassification error Pr(x.y)_,D [h(x) y]. We give a poly (d, 1 1 6) time algorithm for this problem with misclassification error n + E. We also provide evidence that improving on the error guarantee of our algorithm might be computationally hard. Prior to our work, no efficient weak (distribution -independent) learner was known in this model, even for the class of disjunctions. The existence of such an algorithm for halfspaces (or even disjunctions) has been posed as an open question in various works, starting with Sloan (1988), Cohen (1997), and was most recently highlighted in Avrim Blum's FOCS 2003 tutorial.	[Diakonikolas, Ilias; Tzamos, Christos] Univ Wisconsin, Madison, WI 53706 USA; [Gouleakis, Themis] Max Planck Inst Informat, Munich, Germany	University of Wisconsin System; University of Wisconsin Madison; Max Planck Society	Diakonikolas, I (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	ilias@cs.wisc.edu; tgouleak@mpi-inf.mpg.de; tzamos@wisc.edu			NSF [CCF-1652862]; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	Part of this work was performed while Ilias Diakonikolas was at the Simons Institute for the Theory of Computing during the program on Foundations of Data Science. Ilias Diakonikolas is supported by Supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. This research was performed while Themis Gouleakis was a postdoctoral researcher at USC.	Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Awasthi P., 2016, C LEARN THEOR COLT, P152; Awasthi P., 2015, PROC 28 ANN C LEARN, P167; Awasthi P, 2017, J ACM, V63, DOI 10.1145/3006384; BERNHOLT T, 2006, TECHNICAL REPORT; Blum A, 1998, ALGORITHMICA, V22, P35, DOI 10.1007/PL00013833; Blum A, 1996, AN S FDN CO, P330, DOI 10.1109/SFCS.1996.548492; Blum A., 2003, 44 S FDN COMP SCI FO, P11; Bylander T., 1994, Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, COLT 94, P340, DOI 10.1145/180139.181176; Cohen E, 1997, ANN IEEE SYMP FOUND, P514, DOI 10.1109/SFCS.1997.646140; Daniely A, 2016, ACM S THEORY COMPUT, P105, DOI 10.1145/2897518.2897520; Diakonikolas I, 2019, PR MACH LEARN RES, V97; Diakonikolas I, 2018, ACM S THEORY COMPUT, P1061, DOI 10.1145/3188745.3188754; Diakonikolas I, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2683; Diakonikolas I, 2017, PR MACH LEARN RES, V70; DUCHI JOHN C., 2016, INTRO LECT STOCHASTI; Dunagan J, 2004, J COMPUT SYST SCI, V68, P335, DOI 10.1016/j.jcss.2003.07.013; DUNAGAN J, 2004, P 36 ANN ACM S THEOR, P315; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Feldman V, 2006, ANN IEEE SYMP FOUND, P563; Guruswami V, 2006, ANN IEEE SYMP FOUND, P543; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Kearns M., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P392, DOI 10.1145/167088.167200; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Klivans A., 2018, C LEARNING THEORY, P1420; Klivans A., 2009, P 17 INT C ALG LANG; Lai K. A., 2016, P FOCS 16; Long PM, 2010, MACH LEARN, V78, P287, DOI 10.1007/s10994-009-5165-z; MAASS W, 1994, COMPUTATIONAL LEARNING THEORY AND NATURAL LEARNING SYSTEMS, VOL I: CONSTRAINTS AND PROSPECTS, P381; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; RIVEST RL, 1994, INFORM COMPUT, V114, P88, DOI 10.1006/inco.1994.1081; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Sloan R. H., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, DOI 10.1145/130385.130433; Sloan R. H., 1996, PAC LEARNING NOISE G, P21; SLOAN RH, 1988, P 1988 WORKSH COMP L, P91; Valiant L. G., 1984, Communications of the ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1982, ESTIMATION DEPENDENC; Yan Songbai, 2017, ADV NEURAL INFORM PR, P1056; Zhang Y., 2017, C LEARN THEOR PMLR, P1980	42	5	5	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304072
C	Dong, YH; Hopkins, SB; Li, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dong, Yihe; Hopkins, Samuel B.; Li, Jerry			Quantum Entropy Scoring for Fast Robust Mean Estimation and Improved Outlier Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study two problems in high-dimensional robust statistics: robust mean estimation and outlier detection. In robust mean estimation the goal is to estimate the mean mu of a distribution on R-d given n independent samples, an epsilon-fraction of which have been corrupted by a malicious adversary. In outlier detection the goal is to assign an outlier score to each element of a data set such that elements more likely to be outliers are assigned higher scores. Our algorithms for both problems are based on a new outlier scoring method we call QUE-scoring based on quantum entropy regularization. For robust mean estimation, this yields the first algorithm with optimal error rates and nearly-linear running time (O) over tilde (nd) in all parameters, improving on the previous fastest running time (O) over tilde (min(nd/epsilon(6), nd(2))). For outlier detection, we evaluate the performance of QUE-scoring via extensive experiments on synthetic and real data, and demonstrate that it often performs better than previously proposed algorithms. Code for these experiments is available at https://github.com/twistedcubic/que-outlier-detection.	[Dong, Yihe; Li, Jerry] Microsoft Res, Redmond, WA 98052 USA; [Hopkins, Samuel B.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Microsoft; University of California System; University of California Berkeley	Dong, YH (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	yihedong@gmail.com; hopkins@berkeley.edu; jerrl@microsoft.com		Dong, Yihe/0000-0002-5554-5484				Allen-Zhu Z, 2015, ACM S THEORY COMPUT, P237, DOI 10.1145/2746539.2746610; [Anonymous], 1960, TECHNOMETRICS, DOI DOI 10.1080/00401706.1960.10489888; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Breunig MM, 2000, SIGMOD REC, V29, P93, DOI 10.1145/335191.335388; Campos GO, 2016, DATA MIN KNOWL DISC, V30, P891, DOI 10.1007/s10618-015-0444-8; Cheng Yu, P 32 ANN C LEARN THE; Cheng Yu, 2019, P 30 ANN ACM SIAM S, P2755; Diakonikolas I, 2019, SIAM J COMPUT, V48, P742, DOI 10.1137/17M1126680; Diakonikolas I, 2017, PR MACH LEARN RES, V70; Diakonikolas I, 2017, ANN IEEE SYMP FOUND, P73, DOI 10.1109/FOCS.2017.16; Hawkins D.M, 1980, IDENTIFICATION OUTLI, V11; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Knorr E. M., 1997, Proceedings of the Third International Conference on Knowledge Discovery and Data Mining, P219; Knorr E. M., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P392; Lai KA, 2016, ANN IEEE SYMP FOUND, P665, DOI 10.1109/FOCS.2016.76; Lecue G., 2019, ARXIV190603058; Li J. Z., 2018, THESIS; Liu FT, 2008, IEEE DATA MINING, P413, DOI 10.1109/ICDM.2008.17; Lugosi G, 2019, ANN STAT, V47, P783, DOI 10.1214/17-AOS1639; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ramaswamy S, 2000, SIGMOD REC, V29, P427, DOI 10.1145/335191.335437; Rousseeuw PJ, 1999, TECHNOMETRICS, V41, P212, DOI 10.2307/1270566; Steinhardt Jacob, 2018, THESIS STANFORD U; Tukey J., 1975, P INT C MATH, V2; Tukey J.W., 1960, CONTRIBUTIONS PROBAB, P448	26	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306011
C	Efroni, Y; Merlis, N; Ghavamzadeh, M; Mannor, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Efroni, Yonathan; Merlis, Nadav; Ghavamzadeh, Mohammad; Mannor, Shie			Tight Regret Bounds for Model-Based Reinforcement Learning with Greedy Policies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					State-of-the-art efficient model-based Reinforcement Learning (RL) algorithms typically act by iteratively solving empirical models, i.e., by performingfull-planning on Markov Decision Processes (MDPs) built by the gathered experience. In this paper, we focus on model-based RL in the finite-state finite-horizon undiscounted MDP setting and establish that exploring with greedy policies - act by 1-step planning - can achieve tight minimax performance in terms of regret, 6(HSAT). Thus, full-planning in model-based RL can be avoided altogether without any performance degradation, and, by doing so, the computational complexity decreases by a factor of S. The results are based on a novel analysis of real-time dynamic programming, then extended to model-based RL. Specifically, we generalize existing algorithms that perform full-planning to act by 1-step planning. For these generalizations, we prove regret bounds with the same rate as their full-planning counterparts.	[Efroni, Yonathan; Merlis, Nadav; Mannor, Shie] Technion, Haifa, Israel; [Ghavamzadeh, Mohammad] Facebook AI Res, Menlo Pk, CA USA	Facebook Inc	Efroni, Y (corresponding author), Technion, Haifa, Israel.			Mannor, Shie/0000-0003-4439-7647	Israel Science Foundation under ISF [1380/16]	Israel Science Foundation under ISF	We thank Oren Louidor for illuminating discussions relating the Decreasing Bounded Process, and Esther Derman for the very helpful comments. This work was partially funded by the Israel Science Foundation under ISF grant number 1380/16.	AGRAWAL S, 2017, ADV NEURAL INFORM PR, P1184; Azar MG, 2017, PR MACH LEARN RES, V70; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; BARTO AG, 1995, ARTIF INTELL, V72, P81, DOI 10.1016/0004-3702(94)00011-O; Bertsekas Dimitri P, 1996, NEURODYNAMIC PROGRAM, V5; Bonet B., 2003, Proceedings, Thirteenth International Conference on Automated Planning and Scheduling, P12; Dann C., 2017, ADV NEURAL INFORM PR, P5713; Dann Christopher, 2018, ARXIV181103056; de la Pena V. H., 2008, SELF NORMALIZED PROC; de la Pena VH, 2007, PROBAB SURV, V4, P172, DOI 10.1214/07-PS119; Efroni Y., 2019, P AAAI C ARTIFICIAL, V33, P3494, DOI [10.1609/aaai.v33i01.33013494, DOI 10.1609/AAAI.V33I01.33013494]; Efroni Y., 2018, ADV NEURAL INFORM PR, P5238; Fruit Ronan, 2018, ARXIV180204020; Gopalan A., 2015, P 28 C LEARNING THEO, P861; Hafner D., 2018, ARXIV181104551; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jiang Daniel R, 2018, ARXIV180505935; Jin C., 2018, ADV NEURAL INFORM PR, P4863; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Maurer A., 2009, COLT; McMahan H.B., 2005, ICML, P569; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; MOORE AW, 1993, MACH LEARN, V13, P103, DOI 10.1007/BF00993104; Osband I., 2016, ARXIV PREPRINT ARXIV; Osband I, 2017, PR MACH LEARN RES, V70; Peng Baolin, 2018, ARXIV180106176; Smith T., 2006, AAAI, P1227; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl Alexander L, 2012, ARXIV12066870; Strehl Alexander L, 2006, P AAAI WORKSH LEARN; Sutton R.S., 1991, ACM SIGART B, V2, P160, DOI [10.1145/122344.122377, DOI 10.1145/122344.122377]; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Talebi MS, 2018, ALGORITHMIC LEARNING, P770; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Van Seijen Harm, 2013, ARXIV13012343; Weissman Tsachy, 2003, TECH REP; Zanette Andrea, 2019, ARXIV190100210	38	5	5	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903080
