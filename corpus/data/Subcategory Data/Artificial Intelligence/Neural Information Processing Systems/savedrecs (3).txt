PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Stollenga, MF; Masci, J; Gomez, F; Schmidhuber, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Stollenga, Marijn F.; Masci, Jonathan; Gomez, Faustino; Schmidhuber, Juergen			Deep Networks with Internal Selective Attention through Feedback Connections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FEEDFORWARD	Traditional convolutional neural networks (CNN) are stationary and feedforward. They neither change their parameters during evaluation nor use feedback from higher to lower layers. Real brains, however, do. So does our Deep Attention Selective Network (dasNet) architecture. DasNet's feedback structure can dynamically alter its convolutional filter sensitivities during classification. It harnesses the power of sequential processing to improve classification performance, by allowing the network to iteratively focus its internal attention on some of its convolutional filters. Feedback is trained through direct policy search in a huge million-dimensional parameter space, through scalable natural evolution strategies (SNES). On the CIFAR-10 and CIFAR-100 datasets, dasNet outperforms the previous state-of-the-art model on unaugmented datasets.	[Stollenga, Marijn F.; Masci, Jonathan; Gomez, Faustino; Schmidhuber, Juergen] USI SUPSI, IDSIA, Manno Lugano, Switzerland	Universita della Svizzera Italiana	Stollenga, MF (corresponding author), USI SUPSI, IDSIA, Manno Lugano, Switzerland.	marijn@idsia.ch; jonathan@idsia.ch; tino@idsia.ch; juergen@idsia.ch	Peters, Jan/P-6027-2019	Peters, Jan/0000-0002-5266-8091				[Anonymous], 2013, ARXIV201313013557; Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103; Behnke S, 2005, NEURAL COMPUT APPL, V14, P97, DOI 10.1007/s00521-004-0444-x; Behnke S., 2001, International Journal of Computational Intelligence and Applications, V1, P427, DOI 10.1142/S1469026801000342; Branson S, 2010, LECT NOTES COMPUT SC, V6314, P438, DOI 10.1007/978-3-642-15561-1_32; Bullier J, 2004, METH NE FRO NEUROSCI, P181; Cires D. C., 2011, PROC 22 INT JOINT C, V22, P1237; Ciresan D., 2012, CVPR; Ciresan Dan, 2012, ADV NEURAL INFORM PR, P2843, DOI DOI 10.5555/2999325.2999452; Ciresan DC, 2013, LECT NOTES COMPUT SC, V8150, P411, DOI 10.1007/978-3-642-40763-5_51; Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Fukushima K, 2003, LECT NOTES COMPUT SC, V2714, P393; Fukushima K., 1979, Transactions of the Institute of Electronics and Communication Engineers of Japan, Section E (English), VE62, P675; Gabor D., 1946, J I ELECT ENG, V93, P429, DOI DOI 10.1049/JI-3-2.1946.0074; Gilbert CD, 2007, NEURON, V54, P677, DOI 10.1016/j.neuron.2007.05.019; Glasmachers T., 2010, P 12 ANN C GENETIC E, P393, DOI [10.1145/1830483.1830557, DOI 10.1145/1830483.1830557]; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Hupe JM, 1998, NATURE, V394, P784, DOI 10.1038/29537; Itti L., 2007, VISUAL SALIENCE, V2, P3327; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Knoll B, 2012, IEEE DATA COMPR CONF, P377, DOI 10.1109/DCC.2012.44; Krizhevsky A., 2012, ADV NEURAL INFORM PR, V1, P4; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Lamme VAF, 2000, TRENDS NEUROSCI, V23, P571, DOI 10.1016/S0166-2236(00)01657-X; Lamme VAF, 2001, ACTA PSYCHOL, V107, P209, DOI 10.1016/S0001-6918(01)00020-8; Larochelle H., 2010, IMAGE, V1, px2; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lin Min, 2013, CORR; O'Reilly RC, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00124; OReilly RC, 1996, NEURAL COMPUT, V8, P895, DOI 10.1162/neco.1996.8.5.895; Rechenberg I, 1971, THESIS; Schaul T, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P845; Schmidhuber J., 1991, International Journal of Neural Systems, V2, P125, DOI 10.1142/S012906579100011X; Schmidhuber J., 2014, IDSIA0314 SWISS AI L; Sermanet P., 2013, CVPR; Srivastava R. K., 2013, NIPS; Stollenga M. F., 2011, THESIS; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; VanRullen R, 2006, VISION RES, V46, P3017, DOI 10.1016/j.visres.2005.07.009; Wan L., 2013, P INT C MACHINE LEAR, P1058; Warde-Farley D., 2013, ICML; Welinder Peter., 2010, CNSTR2010001 CALTECH, V200; Weng J., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P576, DOI 10.1109/IJCNN.1992.287150; WHITEHEAD S, 1992, THESIS; Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255; Wyatte D, 2012, J COGNITIVE NEUROSCI, V24, P2248, DOI 10.1162/jocn_a_00282; Wyatte D, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00182; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler MD, 2013, ARXIV13112901CSCV	52	15	15	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100055
C	Fischer, B; Roth, V; Buhmann, JM		Thrun, S; Saul, K; Scholkopf, B		Fischer, B; Roth, V; Buhmann, JM			Clustering with the connectivity kernel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Clustering aims at extracting hidden structure in dataset. While the problem of finding compact clusters has been widely studied in the literature, extracting arbitrarily formed elongated structures is considered a much harder problem. In this paper we present a novel clustering algorithm which tackles the problem by a two step procedure: first the data are transformed in such a way that elongated structures become compact ones. In a second step, these new objects are clustered by optimizing a compactness-based criterion. The advantages of the method over related approaches are threefold: (i) robustness properties of compactness-based criteria naturally transfer to the problem of extracting elongated structures, leading to a model which is highly robust against outlier objects; (ii) the transformed distances induce a Mercer kernel which allows us to formulate a polynomial approximation scheme to the generally NP-hard clustering problem; (iii) the new method does not contain free kernel parameters in contrast to methods like spectral clustering or mean-shift clustering.	Swiss Fed Inst Technol, Inst Computat Sci, CH-8092 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Universita della Svizzera Italiana	Fischer, B (corresponding author), Swiss Fed Inst Technol, Inst Computat Sci, CH-8092 Zurich, Switzerland.	bernd.fischer@inf.ethz.ch; volker.roth@inf.ethz.ch; jbuhmann@inf.ethz.ch	Roth, Volker/Q-4025-2017; Fischer, Bernd/E-7461-2011	Roth, Volker/0000-0003-0991-0273; Fischer, Bernd/0000-0001-9437-2099				Brucker P, 1977, OPTIMIZATION OPERATI, P45; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Comaniciu D, 2003, IEEE T PATTERN ANAL, V25, P281, DOI 10.1109/TPAMI.2003.1177159; DEZA M, 1994, J COMPUT APPL MATH, V55, P191, DOI 10.1016/0377-0427(94)90020-5; Drineas P, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P291; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Fischer B, 2003, IEEE T PATTERN ANAL, V25, P513, DOI 10.1109/TPAMI.2003.1190577; Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; Ng AY, 2002, ADV NEUR IN, V14, P849; Ostrovsky R, 2002, J ACM, V49, P139, DOI 10.1145/506147.506149; PUZICHA J, 2000, PATTERN RECOGNITION; ROTH V, 2003, IN PRESS NIPS, V15; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Young G, 1938, PSYCHOMETRIKA, V3, P19, DOI 10.1007/BF02287916	15	15	16	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						89	96						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500012
C	Wu, JX; Rehg, JM; Mullin, MD		Thrun, S; Saul, K; Scholkopf, B		Wu, JX; Rehg, JM; Mullin, MD			Learning a rare event detection cascade by direct feature selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Face detection is a canonical example of a rare event detection problem, in which target patterns occur with much lower frequency than non-targets. Out of millions of face-sized windows in an input image, for example, only a few will typically contain a face. Viola and Jones recently proposed a cascade architecture for face detection which successfully addresses the rare event nature of the task. A central part of their method is a feature selection algorithm based on AdaBoost. We present a novel cascade learning algorithm based on forward feature selection which is two orders of magnitude faster than the Viola-Jones approach and yields classifiers of equivalent quality. This faster method could be used for more demanding classification tasks, such as on-line learning.	Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Wu, JX (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	wujx@cc.gatech.edu; rehg@cc.gatech.edu; mdmullin@cc.gatech.edu	Wu, Jianxin/A-3700-2011; Wu, Jianxin/B-8539-2012; Rehg, James/AAM-6888-2020	Rehg, James/0000-0003-1793-5462				Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; CARMICHAEL O, 2002, BRIT MACH VIS C SEPT, V1, P103; FAN W, 2000, P 11 ECML; Heisele B, 2001, PROC CVPR IEEE, P18; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; KARAKOULAS GJ, 1999, NIPS, V11, P253; Keren D, 2001, IEEE T PATTERN ANAL, V23, P747, DOI 10.1109/34.935848; LAZEBNIK S, 2003, P CVPR; LEUNG TK, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P637, DOI 10.1109/ICCV.1995.466878; LI SZ, 2002, NIPS, V15; Lienhart R., 2002, EMPIRICAL ANAL DETEC; Romdhani S, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P695, DOI 10.1109/ICCV.2001.937694; Rowley HA, 1998, IEEE T PATTERN ANAL, V20, P23, DOI 10.1109/34.655647; Schatten G, 1998, J LAW MED ETHICS, V26, P29, DOI 10.1111/j.1748-720X.1998.tb01903.x; Schneiderman H., 2000, IEEE C COMP VIS PATT; Sung KK, 1998, IEEE T PATTERN ANAL, V20, P39, DOI 10.1109/34.655648; Viola P, 2001, PROC CVPR IEEE, P511, DOI 10.1109/cvpr.2001.990517; VIOLA P, 2002, NIPS, V14; Webb A., 1999, STAT PATTERN RECOGNI; Yang MH, 2002, IEEE T PATTERN ANAL, V24, P34, DOI 10.1109/34.982883	20	15	17	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1523	1530						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500189
C	Chapelle, O; Scholkopf, B		Dietterich, TG; Becker, S; Ghahramani, Z		Chapelle, O; Scholkopf, B			Incorporating invariances in nonlinear support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The choice of an SVM kernel corresponds to the choice of a representation of the data in a feature space and, to improve performance, it should therefore incorporate prior knowledge such as known transformation invariances. We propose a technique which extends earlier work and aims at incorporating invariances in nonlinear kernels. We show on a digit recognition task that the proposed approach is superior to the Virtual Support Vector method, which previously had been the method of choice.	LIP6, Paris, France	UDICE-French Research Universities; Sorbonne Universite	Chapelle, O (corresponding author), LIP6, Paris, France.	olivier.chapelle@lip6.fr; bernhard.schoelkopf@tuebingen.mpg.de	Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				Chapelle O, 2002, MACH LEARN, V46, P131, DOI 10.1023/A:1012450327387; CHAPELLE O, 2001, INCORPORATING INVARI; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; DECOSTE D, 2001, IN PRESS MACHINE LEA; LEEN TK, 1995, NIPS, V7; Platt J. C., 2000, ADV LARGE MARGIN CLA; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SCHOLKOPF B, 1998, NIPS, V10; SCHOLKOPF B, 1995, 1 INT C KNOWL DISC D; SCHOLKOPF B, 1996, LECT NOTES COMPUT SC, V1112, P47; Scholkopf B., 1999, ADV KERNEL METHODS S, DOI [10.1109/72.870050, DOI 10.1109/72.870050]; Simard Patrice Y, 1998, NEURAL NETWORKS TRIC; Tsuda K., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P183; Vapnik V.N, 1998, STAT LEARNING THEORY	15	15	15	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						609	616						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100076
C	Kappen, HJ; Wiegerinck, W		Dietterich, TG; Becker, S; Ghahramani, Z		Kappen, HJ; Wiegerinck, W			Novel iteration schemes for the cluster variation method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The Cluster Variation method is a class of approximation methods containing the Bethe and Kikuchi approximations as special cases. We derive two novel iteration schemes for the Cluster Variation Method. One is a fixed point iteration scheme which gives a significant improvement over loopy BP, mean field and TAP methods on directed graphical models. The other is a gradient based method, that is guaranteed to converge and is shown to give useful results on random graphs with mild frustration. We conclude that the methods are of significant practical value for large inference problems.	Univ Nijmegen, Dept Biophys, Nijmegen, Netherlands	Radboud University Nijmegen	Kappen, HJ (corresponding author), Univ Nijmegen, Dept Biophys, Nijmegen, Netherlands.	bert@mbfys.kun.nl; wimw@mbfys.kun.nl	Kappen, H.J./L-4425-2015					BEINLICH I, 1989, 2 EUR C AI MED; Kappen HJ, 2001, ADV NEUR IN, V13, P238; KAPPEN HJ, 2002, IN PRESS MODELING BI; KIKUCHI R, 1951, PHYS REV, V81, P988, DOI 10.1103/PhysRev.81.988; LAURITZEN SL, 1988, J ROY STAT SOC B MET, V50, P157; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; TEH Y, 2002, IN PRESS ADV NEURAL, V14; YEDIDIA JS, 2001, IN PRESS ADV NEURAL, V13; YUILLE AL, 2002, IN PRESS ADV NEURAL, V14	10	15	15	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						415	422						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100052
C	Langford, J; Caruana, R		Dietterich, TG; Becker, S; Ghahramani, Z		Langford, J; Caruana, R			(Not) bounding the true error	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a new approach to bounding the true error rate of a continuous valued classifier based upon PAC-Bayes bounds. The method first constructs a distribution over classifiers by determining how sensitive each parameter in the model is to noise. The true error rate of the stochastic classifier found with the sensitivity analysis can then be tightly bounded using a PAC-Bayes bound. In this paper we demonstrate the method on artificial neural networks with results of a 2 - 3 order of magnitude improvement vs. the best deterministic neural net bounds.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Langford, J (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.							BARTLETT P, 1998, IEEE T INFORMATION T, V44; KOLTCHINSKII V, EMPIRICAL MARGIN DIS; Langford J., 2001, BOUNDS AVERAGING CLA; MACKAY DJ, PROBABLE NETWORKS PL; MCALLESTER D, COLT 1999	5	15	15	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						809	816						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100101
C	Saul, LK; Lee, DD		Dietterich, TG; Becker, S; Ghahramani, Z		Saul, LK; Lee, DD			Multiplicative updates for classification by mixture models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We investigate a learning algorithm for the classification of nonnegative data by mixture models. Multiplicative update rules are derived that directly optimize the performance of these models as classifiers. The update rules have a simple closed form and an intuitive appeal. Our algorithm retains the main virtues of the Expectation-Maximization (EM) algorithm-its guarantee of monotonic improvement, and its absence of tuning parameters-with the added advantage of optimizing a discriminative objective function. The algorithm reduces as a special case to the method of generalized iterative scaling for log-linear models. The learning rate of the algorithm is controlled by the sparseness of the training data. We use the method of nonnegative matrix factorization (NMF) to discover sparse distributed representations of the data. This form of feature selection greatly accelerates learning and makes the algorithm practical on large problems. Experiments show that discriminatively trained mixture models lead to much better classification than comparably sized models trained by EM.	Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA	University of Pennsylvania	Saul, LK (corresponding author), Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.		Lee, Daniel D./B-5753-2013	Lee, Daniel/0000-0003-4239-8777				COLLINS M, 2000, P 13 ANN C COMP LEAR; DARROCH JN, 1972, ANN MATH STAT, V43, P1470, DOI 10.1214/aoms/1177692379; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; GOPALAKRISHNAN PS, 1991, IEEE T INFORM THEORY, V37, P107, DOI 10.1109/18.61108; JEBARA T, 1998, ADV NEURAL INFORMATI, V11, P494; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; LeCun Y., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P53; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; LEE DD, 2000, ADV NEURAL INFORMATI, V13; McLachlan G.J., 1988, MIXTURE MODELS INFER, V38; Normandin Y., 1991, THESIS MCGILL U MONT; OSULLIVAN JA, 1998, CODES CURVES SIGNALS; Vapnik V.N., 1999, NATURE STAT LEARNING	14	15	15	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						897	904						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100112
C	Gentile, C		Leen, TK; Dietterich, TG; Tresp, V		Gentile, C			A new approximate maximal margin classification algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A new incremental learning algorithm is described which approximates the maximal margin hyperplane w.r.t. norm p greater than or equal to 2 for a set of linearly separable data. Our algorithm, called ALMA(p) (Approximate Large Margin gin algorithm w.r.t. norm p), takes O [GRAPHICS] corrections to separate the data with p-norm margin larger than (1 - alpha) gamma, where gamma is the p-norm margin of the data and X is a bound on the p-norm of the instances. ALMAp avoids quadratic (or higher-order) programming methods. It is very easy to implement and is as fast as on-line algorithms, such as Rosenblatt's perceptron. We report on some experiments comparing ALMA(p) to two incremental algorithms: Perceptron and Li and Long's ROMMA. Our algorithm seems to perform quite better than both. The accuracy levels achieved by ALMA, are slightly inferior to those obtained by Support vector Machines (SVMs). On the other hand, ALMAp is quite faster and easier to implement than standard SVMs training algorithms.	Univ Milan, DSI, I-20135 Milan, Italy	University of Milan	Gentile, C (corresponding author), Univ Milan, DSI, Via Comelico 39, I-20135 Milan, Italy.	gentile@dsi.unimi-it						Anthony M., 1999, NEURAL NETWORK LEARN, V9; AUER P, 2000, 13 COLT, P107; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; FRIESS TT, 1998, 15 ICML; GENTILE C, 1999, 12 COLT, P1; GENTILE C, 1999, 11 NIPS, P225; GROVE AJ, 1997, 10 COLT, P171; HELMBOLD DP, 1995, J COMPUT SYST SCI, V50, P551, DOI 10.1006/jcss.1995.1044; KOWALCZYK A, 1999, ADV LARGE MARGIN CLA; LeCun Y., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P53; LI Y, 2000, 12 NIPS, P498; LI Y, 2000, THESIS NATL U SINGAP; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Mangasarian OL, 1997, DATA MIN KNOWL DISC, V1, P183, DOI 10.1023/A:1009735908398; NACHBAR P, 1993, 1993 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS : PROCEEDINGS, VOLS 1-4 ( ISCAS 93 ), P2152, DOI 10.1109/ISCAS.1993.394184; Platt J C, 1999, ADV KERNEL METHODS S; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582	18	15	16	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						500	506						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800071
C	Perez-Cruz, F; Alarcon-Diana, PL; Navia-Vazquez, A; Artes-Rodriguez, A		Leen, TK; Dietterich, TG; Tresp, V		Perez-Cruz, F; Alarcon-Diana, PL; Navia-Vazquez, A; Artes-Rodriguez, A			Fast training of support vector classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In this communication we present a new algorithm for solving Support Vector Classifiers (SVC) with large training data sets. The new algorithm is based on an Iterative Re-Weighted Least Squares procedure which is used to optimize the SVC. Moreover, a novel sample selection strategy for the working set is presented, which randomly chooses the working set among the training samples that do not fulfill the stopping criteria. The validity of both proposals, the optimization procedure and sample selection strategy, is shown by means of computer experiments using well-known data sets.	Univ Alcala de Henares, Escuela Politecn, Dpto Teoria Senal & Com, Alcala De Henares 28871, Spain	Universidad de Alcala	Perez-Cruz, F (corresponding author), Univ Alcala de Henares, Escuela Politecn, Dpto Teoria Senal & Com, Alcala De Henares 28871, Spain.	fernando@tsc.uc3m.es	Navia-Vazquez, Angel/C-6176-2014	perez-cruz, fernando/0000-0001-8996-5076				[Anonymous], 1999, ADV KERNEL METHODS S, DOI DOI 10.17877/DE290R-5098; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482; Haykin S, 1998, NEURAL NETWORKS COMP, V2nd; HOLLAND PW, 1977, COMMUN STAT A-THEOR, V6, P813, DOI 10.1080/03610927708827533; JOACHIMS T, 1998, TECHNICAL REPORT U D; Osuna E, 1997, NEURAL NETWORKS FOR SIGNAL PROCESSING VII, P276, DOI 10.1109/NNSP.1997.622408; OSUNA E, 1998, ICPR 98 BRISB AUSTR; PEREZCRUZ F, 1999, P 5 BAYON WORKSH EM, V1, P116; PEREZCRUZ F, 2000, P EUSIPCO 00 TAMP FI; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Press WH., 1994, NUMERICAL RECIPES C; Vapnik V.N, 1998, STAT LEARNING THEORY	14	15	16	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						734	740						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800104
C	Tishby, N; Slonim, N		Leen, TK; Dietterich, TG; Tresp, V		Tishby, N; Slonim, N			Data clustering by Markovian relaxation and the Information Bottleneck Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PATTERNS	We introduce a new, non-parametric and principled, distance based clustering method. This method combines a pairwise based approach with a vector-quantization method which provide a meaningful interpretation to the resulting clusters. The idea is based on turning the distance matrix into a Markov process and then examine the decay of mutual-information during the relaxation of this process. The clusters emerge as quasi-stable structures during this relaxation, and then are extracted using the information bottleneck method. These clusters capture the information about the initial point of the relaxation in the most effective way. The method can cluster data with no geometric or other bias and makes no assumption about the underlying distribution.	Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Tishby, N (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, IL-91904 Jerusalem, Israel.	tishby@cs.huji.ac.il; noamm@cs.huji.ac.il						Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; BENDOR A, 2000, IN PRESS J COMPUTATI; Blatt M, 1997, NEURAL COMPUT, V9, P1805, DOI 10.1162/neco.1997.9.8.1805; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; GDALYAHU Y, 1998, P NIPS 11, P424; Hofmann T, 1997, IEEE T PATTERN ANAL, V19, P1, DOI 10.1109/34.566806; PEREIRA F, 1993, 31ST ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P183; Rose K, 1998, P IEEE, V86, P2210, DOI 10.1109/5.726788; SLONIM N, 1999, P NIPS 12; Tishby N., 1999, P 37 ANN ALL C COMM, P368	12	15	15	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						640	646						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800091
C	Kegl, B; Krzyzak, A; Linder, T; Zeger, K		Kearns, MS; Solla, SA; Cohn, DA		Kegl, B; Krzyzak, A; Linder, T; Zeger, K			A polygonal line algorithm for constructing principal curves	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Principal curves have been defined as "self consistent" smooth curves which pass through the "middle" of a d-dimensional probability distribution or data cloud. Recently, we [1] have offered a new approach by defining principal curves as continuous curves of a given length which minimize the expected squared distance between the curve and points of the space randomly chosen according to a given distribution. The new definition made it possible to carry out a theoretical analysis of learning principal curves from training data. In this paper we propose a practical construction based on the new definition. Simulation results demonstrate that the new algorithm compares favorably with previous methods both in terms of performance and computational complexity.	Concordia Univ, Dept Comp Sci, Montreal, PQ H3G 1M8, Canada	Concordia University - Canada	Kegl, B (corresponding author), Concordia Univ, Dept Comp Sci, 1450 Maisonneuve Blvd W, Montreal, PQ H3G 1M8, Canada.	kegl@cs.concordia.ca; krzyzak@cs.concordia.ca; linder@mast.queensu.ca; zeger@ucsd.edu						BANFIELD JD, 1992, J AM STAT ASSOC, V87, P7, DOI 10.2307/2290446; DELICADO P, 1998, 309 U POMP FABR DEP; DER R, 1998, NONLINEAR PRINCIPAL; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hastie T., 1984, THESIS STANFORD U; Kegl B, 1998, 1998 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY - PROCEEDINGS, P387, DOI 10.1109/ISIT.1998.708992; MULIER F, 1995, NEURAL COMPUT, V7, P1165, DOI 10.1162/neco.1995.7.6.1165; Tibshirani R., 1992, Statistics and Computing, V2, P183, DOI 10.1007/BF01889678; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	9	15	15	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						501	507						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700071
C	Vasconcelos, N; Lippman, A		Kearns, MS; Solla, SA; Cohn, DA		Vasconcelos, N; Lippman, A			Learning mixture hierarchies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The hierarchical representation of data has various applications in domains such as data mining, machine vision, or information retrieval. In this paper we introduce an extension of the Expectation-Maximization (EM) algorithm that learns mixture hierarchies in a computationally efficient manner. Efficiency is achieved by progressing in a bottom-up fashion, i.e. by clustering the mixture components of a given level in the hierarchy to obtain those of the level above. This clustering requires only knowledge of the mixture parameters, there being no need to resort to intermediate samples. In addition to practical applications, the algorithm allows a new interpretation of EM that makes clear the relationship with non-parametric kernel-based estimation methods, provides explicit control over the trade-off between the bias and variance of EM estimates, and offers new insights about the behavior of deterministic annealing methods commonly used with EM to escape local minima of the likelihood.	MIT, Media Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Vasconcelos, N (corresponding author), MIT, Media Lab, 20 Ames St,E15-320M, Cambridge, MA 02139 USA.	nuno@media.mit.edu; lip@media.mit.edu		Vasconcelos, Nuno/0000-0002-9024-4302				DEMPSTER A, 1977, J ROY STAT SOC, pB39; MURASE H, 1995, INT J COMPUT VISION, V14, P5, DOI 10.1007/BF01421486; ROSE K, 1992, IEEE T INFORMATION T, V38; Simonoff J.S., 1996, SMOOTING METHODS STA, V2nd; VASCONCELOS N, 1997, P INT C IM PROC SANT; VASCONCELOS N, 1998, LEARNING MIXTURE HIE	6	15	15	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						606	612						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700086
C	Hopfield, JJ; Carlos, CD; Roweis, S		Jordan, MI; Kearns, MJ; Solla, SA		Hopfield, JJ; Carlos, CD; Roweis, S			Computing with action potentials	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Most computational engineering based loosely on biology uses continuous variables to represent neural activity. Yet most neurons communicate with action potentials. The engineering view is equivalent to using a rate-code for representing information and for computing. An increasing number of examples are being discovered in which biology may not be using rate codes. Information can be represented using the timing of action potentials, and efficiently computed with in this representation. The "analog match" problem of odour identification is a simple problem which can be efficiently solved using action potential timing and an underlying rhythm. By using adapting units to effect a fundamental change of representation of a problem, we map the recognition of words (having uniform time-warp) in connected speech into the same analog match problem. We describe the architecture and preliminary results of such a recognition system. Using the fast events of biology in conjunction with an underlying rhythm is one way to overcome the limits of an event-driven view of computation. When the intrinsic hardware is much faster than the time scale of change of inputs, this approach can greatly increase the effective computation per unit time on a given quantity of hardware.	Princeton Univ, Dept Mol Biol, Princeton, NJ 08544 USA	Princeton University	Hopfield, JJ (corresponding author), Princeton Univ, Dept Mol Biol, Princeton, NJ 08544 USA.								0	15	15	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						166	172						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700024
C	Mundel, T; Dimitrov, A; Cowan, JD		Mozer, MC; Jordan, MI; Petsche, T		Mundel, T; Dimitrov, A; Cowan, JD			Visual cortex circuitry and orientation tuning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A simple mathematical model for the large-scale circuitry of primary visual cortex is introduced. It is shown that a basic cortical architecture of recurrent local excitation and lateral inhibition can account quantitatively for such properties as orientation tuning. The model can also account for such local effects as cross-orientation suppression. It is also shown that nonlocal state-dependent coupling between similar orientation patches, when added to the model, can satisfactorily reproduce such effects as non-local iso-orientation suppression, and non-local cross-orientation enhancement. Following this an account is given of perceptual phenomena involving object segmentation, such as ''pop-out'', and the direct and indirect tilt illusions.			Mundel, T (corresponding author), UNIV CHICAGO,DEPT NEUROL,5841 S MARYLAND AVE,CHICAGO,IL 60637, USA.		Dimitrov, Alexander/E-8577-2014	Dimitrov, Alexander/0000-0002-8812-3536					0	15	15	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						887	893						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00125
C	Petsche, T; Marcantonio, A; Darken, C; Hanson, SJ; Kuhn, GM; Santoso, I		Touretzky, DS; Mozer, MC; Hasselmo, ME		Petsche, T; Marcantonio, A; Darken, C; Hanson, SJ; Kuhn, GM; Santoso, I			A neural network autoassociator for induction motor failure prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SIEMENS CORP RES INC,PRINCETON,NJ 08853	Siemens AG									0	15	15	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						924	930						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00130
C	Stevens, CF; Zador, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Stevens, CF; Zador, A			When is an integrate-and-fire neuron like a Poisson neuron?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,MNL S,LA JOLLA,CA 92037	Salk Institute									0	15	15	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						103	109						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00015
C	TOWELL, G; SHAVLIK, JW		MOODY, JE; HANSON, SJ; LIPPMANN, RP		TOWELL, G; SHAVLIK, JW			INTERPRETATION OF ARTIFICIAL NEURAL NETWORKS - MAPPING KNOWLEDGE-BASED NEURAL NETWORKS INTO RULES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	15	15	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						977	984						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00120
C	Agarwal, N; Hazan, E; Singh, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Agarwal, Naman; Hazan, Elad; Singh, Karan			Logarithmic Regret for Online Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study optimal regret bounds for control in linear dynamical systems under adversarially changing strongly convex cost functions, given the knowledge of transition dynamics. This includes several well studied and fundamental frameworks such as the Kalman filter and the linear quadratic regulator. State of the art methods achieve regret which scales as O (root T), where T is the time horizon. We show that the optimal regret in this setting can be significantly smaller, scaling as O(poly(log T)). This regret bound is achieved by two different efficient iterative methods, online gradient descent and online natural gradient.	[Agarwal, Naman; Hazan, Elad; Singh, Karan] Google AI Princeton, Lawrenceville, NJ 08648 USA; [Hazan, Elad; Singh, Karan] Princeton Univ, Comp Sci, Princeton, NJ 08544 USA	Princeton University	Agarwal, N (corresponding author), Google AI Princeton, Lawrenceville, NJ 08648 USA.	namanagarwal@google.com; ehazan@princeton.edu; karans@princeton.edu	Singh, Karan/AAR-1348-2020	Singh, Karan/0000-0002-6992-1655; Hazan, Elad/0000-0002-1566-3216	NSF [CCF-1704860]	NSF(National Science Foundation (NSF))	The authors thank Sham Kakade and Cyril Zhang for various thoughtful discussions. Elad Hazan acknowledges funding from NSF grant # CCF-1704860.	Abbasi-Yadkori Y., 2011, P 24 ANN C LEARNING, P1; Abbasi-Yadkori Y., 2019, 22 INT C ART INT STA, P3108; Abbasi-Yadkori Y, 2014, PR MACH LEARN RES, V32; Agarwal Naman, 2019, P 36 INT C MACH LEAR, P111; Anava O., 2015, ADV NEURAL INFORM PR, P784; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Arora S., 2018, PROVABLE CONTROL UNK; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cohen A, 2018, PR MACH LEARN RES, V80; Dean S., 2018, ARXIV180509388; Fazel M., 2018, INT C MACHINE LEARNI, P1466; Goel S, 2018, PR MACH LEARN RES, V80; Hazan E., 2018, ADV NEURAL INFORM PR; Hazan E., 2017, ADV NEURAL INFORM PR, P6702; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Stengel R. F., 1994, OPTIMAL CONTROL ESTI; Strang Gilbert, INTRO LINEAR ALGEBRA, V3; Tewari A., 2012, P 29 INT C MACH LEAR	18	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901077
C	Bhagoji, AN; Cullina, D; Mittal, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bhagoji, Arjun Nitin; Cullina, Daniel; Mittal, Prateek			Lower Bounds on Adversarial Robustness from Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					While progress has been made in understanding the robustness of machine learning classifiers to test-time adversaries (evasion attacks), fundamental questions remain unresolved. In this paper, we use optimal transport to characterize the minimum possible loss in an adversarial classification scenario. In this setting, an adversary receives a random labeled example from one of two classes, perturbs the example subject to a neighborhood constraint, and presents the modified example to the classifier. We define an appropriate cost function such that the minimum transportation cost between the distributions of the two classes determines the minimum 0 - 1 loss for any classifier. When the classifier comes from a restricted hypothesis class, the optimal transportation cost provides a lower bound. We apply our framework to the case of Gaussian data with norm-bounded adversaries and explicitly show matching bounds for the classification and transport problems as well as the optimality of linear classifiers. We also characterize the sample complexity of learning in this setting, deriving and extending previously known results as a special case. Finally, we use our framework to study the gap between the optimal classification performance possible and that currently achieved by state-of-the-art robustly trained neural networks for datasets of interest, namely, MNIST, Fashion MNIST and CIFAR-10.	[Bhagoji, Arjun Nitin; Mittal, Prateek] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA; [Cullina, Daniel] Penn State Univ, Dept Elect Engn, University Pk, PA 16802 USA; [Cullina, Daniel] Princeton Univ, Princeton, NJ 08544 USA	Princeton University; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; Princeton University	Bhagoji, AN (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA.	abhagoji@princeton.edu; cullina@psu.edu; pmittal@princeton.edu			National Science Foundation [CNS-1553437, CNS1704105, CIF-1617286, EARS-1642962]; Intel through the Intel Faculty Research Award; Office of Naval Research through the Young Investigator Program (YIP) Award; Army Research Office through the Young Investigator Program (YIP) Award; Schmidt DataX Award; Siemens through the FutureMakers Fellowship	National Science Foundation(National Science Foundation (NSF)); Intel through the Intel Faculty Research Award; Office of Naval Research through the Young Investigator Program (YIP) Award(Office of Naval Research); Army Research Office through the Young Investigator Program (YIP) Award; Schmidt DataX Award; Siemens through the FutureMakers Fellowship	We would like to thank Chawin Sitawarin for providing part of the code used in our experiments. This research was sponsored by the National Science Foundation under grants CNS-1553437, CNS1704105, CIF-1617286 and EARS-1642962, by Intel through the Intel Faculty Research Award, by the Office of Naval Research through the Young Investigator Program (YIP) Award, by the Army Research Office through the Young Investigator Program (YIP) Award and a Schmidt DataX Award. ANB would like to thank Siemens for supporting him through the FutureMakers Fellowship.	Abbasi M, 2017, ARXIV PREPRINT ARXIV; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, ARXIV160704311; Arnab A., 2018, CVPR; Athalye A, 2018, PR MACH LEARN RES, V80; Bagnall A, 2017, ARXIV171204006; Bhagoji A. N., 2018, ARXIV180102780; Bhagoji AN, 2017, ARXIV PREPRINT ARXIV; Bhagoji AN, 2018, LECT NOTES COMPUT SC, V11216, P158, DOI 10.1007/978-3-030-01258-8_10; Bhagoji AN, 2019, PR MACH LEARN RES, V97; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Biggio B., 2017, ARXIV171203141; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Brendel W., 2018, PROC 6 INT C LEARN R; Brown N., 2017, SCIENCE; Bubeck S., 2018, ARXIV180510204; Burges, 1998, MNIST DATABASE HANDW; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini N, 2018, 2018 IEEE SYMPOSIUM ON SECURITY AND PRIVACY WORKSHOPS (SPW 2018), P1, DOI 10.1109/SPW.2018.00009; Carlini Nicholas, 2017, ARXIV171108478; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Cullina D., 2018, ADV NEURAL INFORM PR, P228; Das N., 2018, ARXIV180206816; Deng L, 2013, IEEE INT NEW CIRC; Diochnos DI, 2018, ADV NEUR IN, V31; Dziugaite Gintare Karolina, 2016, ARXIV160800853; EricWong Zico, 2018, INT C MACH LEARN, P5286; Evtimov I., 2018, CVPR; Fan Lingling, 2018, ARXIV PREPRINT ARXIV; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Fischer V., 2017, ARXIV170301101; Ford Nic, 2019, ICML; Gilmer Justin, 2018, ARXIV180102774; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow Ian, 2017, 5 INT C LEARN REPR I; Gowal Sven, 2018, EFFECTIVENESS INTERV; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Ilyas A., 2019, P ADV NEUR INF PROC; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Jones E., 2001, SCIPY OPEN SOURCE SC; Julian K. D., 2016, DAS 16; Kingma D.P, P 3 INT C LEARNING R; Kos J., 2017, ARXIV170206832; Kos Jernej, 2017, ARXIV PREPRINT ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Liu Q, 2018, IEEE ACCESS, V6, P12103, DOI 10.1109/ACCESS.2018.2805680; Liu Yanpei, 2017, ICLR; Lu Jiajun, 2017, ADVERSARIAL EXAMPLES; Madry A., 2018, P ICLR VANC BC CAN; Mahloujifar S, 2019, AAAI CONF ARTIF INTE, P4536; Montasser Omar, 2019, ARXIV190204217, V99, P2512; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Moravcik M, 2017, SCIENCE, V356, P508, DOI 10.1126/science.aam6960; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Papernot Nicolas, 2016, ARXIV161103814; Raghunathan Aditi, 2018, ARXIV180109344; Rubinstein Benjamin I. P., 2009, Performance Evaluation Review, V37, P73, DOI 10.1145/1639562.1639592; Schmidt L, 2018, ADV NEUR IN, V31; Shaham U., 2018, DEFENDING ADVERSARIA; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sinha A., 2018, ICLR; Smutz C, 2016, 23RD ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2016), DOI 10.14722/ndss.2016.23078; Suggala Arun Sai, 2019, 22 INT C ART INT STA, P2331; Tramonti F, 2019, PSYCHOL HEALTH MED, V24, P27, DOI 10.1080/13548506.2018.1510131; Tsipras Dimitris, 2018, ARXIV180512152; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang QL, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1145, DOI 10.1145/3097983.3098158; Xiao H., 2017, FASHION MNIST NOVEL; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu W, 2018, 2018 INTERNATIONAL CONFERENCE ON CYBER SITUATIONAL AWARENESS, DATA ANALYTICS AND ASSESSMENT (CYBER SA); Yin D, 2019, PR MACH LEARN RES, V97; Yuan XJ, 2018, PROCEEDINGS OF THE 27TH USENIX SECURITY SYMPOSIUM, P49; Zhang HY, 2019, PR MACH LEARN RES, V97	87	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307051
C	Fang, M; Zhou, TY; Du, YL; Han, L; Zhang, ZY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fang, Meng; Zhou, Tianyi; Du, Yali; Han, Lei; Zhang, Zhengyou			Curriculum-guided Hindsight Experience Replay	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In off-policy deep reinforcement learning, it is usually hard to collect sufficient successful experiences with sparse rewards to learn from. Hindsight experience replay (HER) enables an agent to learn from failures by treating the achieved state of a failed experience as a pseudo goal. However, not all the failed experiences are equally useful to different learning stages, so it is not efficient to replay all of them or uniform samples of them. In this paper, we propose to 1) adaptively select the failed experiences for replay according to the proximity to true goals and the curiosity of exploration over diverse pseudo goals, and 2) gradually change the proportion of the goal-proximity and the diversity-based curiosity in the selection criteria: we adopt a human-like learning strategy that enforces more curiosity in earlier stages and changes to larger goal-proximity later. This "Goal-and-Curiosity-driven Curriculum Learning" leads to "Curriculum-guided HER (CHER)", which adaptively and dynamically controls the exploration-exploitation trade-off during the learning process via hindsight experience selection. We show that CHER improves the state of the art in challenging robotics environments.	[Fang, Meng; Han, Lei; Zhang, Zhengyou] Tencent Robot X, Beijing, Peoples R China; [Zhou, Tianyi] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA; [Du, Yali] UCL, London, England	University of Washington; University of Washington Seattle; University of London; University College London	Fang, M (corresponding author), Tencent Robot X, Beijing, Peoples R China.; Zhou, TY (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.	mfang@tencent.com; tianyizh@uw.edu	zhang, zheng/HCH-9684-2022	Zhou, Tianyi/0000-0001-5348-0632				Allgower Eugene L, 2003, INTRO NUMERICAL CONT; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, INT C MACH LEARN; [Anonymous], 2018, ARXIV180209464; Basu S, 2013, 27 AAAI C ART INT; Batra D, 2012, LECT NOTES COMPUT SC, V7576, P1, DOI 10.1007/978-3-642-33715-4_1; Bengio Y., 2014, GROWING ADAPTIVE MAC, P109; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Brockman G., 2016, OPENAI GYM; CONFORTI M, 1984, DISCRETE APPL MATH, V7, P251, DOI 10.1016/0166-218X(84)90003-9; Cornuejols G., 1977, ANN DISCRET MATH, V1, P163, DOI DOI 10.1016/S0167-5060(08)70732-5; Czarnecki W., 2018, P 35 INT C MACH LEAR, P1087; Duan Y, 2016, INT C MACH LEARN, P1329; Eppe M., 2018, ARXIV180906146; Fang M., 2019, INT C LEARN REPR; Fiterau M., 2012, ADV NEURAL INFORM PR, P3023; Florensa C., 2017, PROC 1 C ROBOT LEARN, P482; Fournier P., 2018, ARXIV180609614; FRANK M, 2013, FRONTIERS NEUROROBOT; Gillenwater J., 2012, ADV NEURAL INFORM PR; Gu SX, 2016, PR MACH LEARN RES, V48; Justesen N., 2018, C COMPUTATIONAL INTE, P1; Khan Faisal, 2011, ADV NEURAL INFORM PR, P1449; Klimov O, 2019, INT C LEARNING REPRE; Kumar A, 2010, ASIA PACIF MICROWAVE, P1189; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lin H., 2009, IEEE AUT SPEECH REC; Lin M, 2011, 2011 INTERNATIONAL CONFERENCE ON ELECTRONICS, COMMUNICATIONS AND CONTROL (ICECC), P510, DOI 10.1109/ICECC.2011.6067730; Liu Hanxiao, 2019, INT C LEARNING REPRE; Metz L., 2017, ARXIV170505035; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mirzasoleiman B, 2016, J MACH LEARN RES, V17; Mirzasoleiman B, 2015, AAAI CONF ARTIF INTE, P1812; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Pong V., 2018, PROC 2 WORKSHOP LIFE; Prasad Adarsh, 2014, ADV NEURAL INFORM PR, V2, P2645; Savinov N., 2019, INT C LEARN REPR; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Spitkovsky V., 2009, ADV NEURAL INFORM PR; Supancic JS, 2013, PROC CVPR IEEE, P2379, DOI 10.1109/CVPR.2013.308; Tang K., 2012, ADV NEURAL INFORM PR, P638; Tang Y., 2012, ACM INT C MULT, P833, DOI DOI 10.1145/2393347.2396324; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Ummadisingu A., 2019, P INT C LEARN REPR N; Wei K., 2014, INT C MACH LEARN; Wu Yuxin, 2017, INT C LEARN REPR; Zhao R., 2018, PROC 2 C ROBOT LEARN, P113; Zhou H, 2018, PROCEEDINGS OF THE SEVENTH NORTHWAST ASIA INTERNATIONAL SYMPOSIUM ON LANGUAGE, LITERATURE AND TRANSLATION, P590; Zhou T., 2018, INT C LEARN REPR	54	14	15	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904029
C	Fu, Y; Feng, YS; Cunningham, JP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fu, Yao; Feng, Yansong; Cunningham, John P.			Paraphrase Generation with Latent Bag of Words	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Paraphrase generation is a longstanding important problem in natural language processing. In addition, recent progress in deep generative models has shown promising results on discrete latent variables for text generation. Inspired by variational autoencoders with discrete latent structures, in this work, we propose a latent bag of words (BOW) model for paraphrase generation. We ground the semantics of a discrete latent variable by the BOW from the target sentences. We use this latent variable to build a fully differentiable content planning and surface realization model. Specifically, we use source words to predict their neighbors and model the target BOW with a mixture of softmax. We use Gumbel top-k reparameterization to perform differentiable subset sampling from the predicted BOW distribution. We retrieve the sampled word embeddings and use them to augment the decoder and guide its generation search space. Our latent BOW model not only enhances the decoder, but also exhibits clear interpretability. We show the model interpretability with regard to (i) unsupervised learning of word neighbors (ii) the step-by-step generation procedure. Extensive experiments demonstrate the transparent and effective generation process of this model.(1)	[Fu, Yao] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Feng, Yansong] Peking Univ, Inst Comp Sci & Technol, Beijing, Peoples R China; [Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA	Columbia University; Peking University; Columbia University	Fu, Y (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.	yao.fu@columbia.edu; fengyansong@pku.edu.cn; jpc2181@columbia.edu			China Scholarship Council; Sloan Fellowship; McKnight Fellowship; NIH; NSF	China Scholarship Council(China Scholarship Council); Sloan Fellowship(Alfred P. Sloan Foundation); McKnight Fellowship; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	We thank the reviewers for their detailed feedbacks and suggestions. We thank Luhuan Wu and Yang Liu for the meaningful discussions. This research is supported by China Scholarship Council, Sloan Fellowship, McKnight Fellowship, NIH, and NSF	Bengio Y., 2014, ARXIV14061078; Bolshakov IA, 2004, LECT NOTES COMPUT SC, V3136, P312; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Buck Christian, 2018, INT C LEARN REPR; Cao Ziqiang, 2017, 31 AAAI C ART INT; Choi J, 2018, AAAI CONF ARTIF INTE, P5094; Collins Michael, STAT MACHINE TRANSLA; Dieng Adji B., 2018, ABS180704863 CORR; Dong Li, 2017, P 2017 C EMP METH NA, P875, DOI [DOI 10.18653/V1/D17-1091, 10.18653/ v1/D17-1091]; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fu Yao, 2018, DEEP GENERATIVE MODE; Gehrmann Sebastian, 2018, INLG; Gu JT, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1631; Gupta Ankush, 2018, AAAI; He Junxian, 2019, ARXIV190105534; Higgins I., 2017, P INT C LEARN REPR T; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hokamp C, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1535, DOI 10.18653/v1/P17-1141; Jang Eric, 2017, ABS161101144 CORR; Kauchak David, 2006, P MAIN C HUM LANG TE, P455, DOI DOI 10.3115/1220835.1220893; Kim Yoon, 2018, ABS181206834 CORR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kool Wouter, 2019, ABS190306059 CORR; Li Zichao, 2018, EMNLP; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin Tsung-Yi, 2014, ECCV; Liu Chia-Wei, 2016, P EMP METH NAT LANG; Liu Tianyu, 2018, AAAI; Ma SM, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P332; Maddison Chris J., 2017, ABS161100712 CORR; McKeown K. R., 1983, American Journal of Computational Linguistics, V9, P1; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Moryossef Amit, 2019, ABS190403396 CORR; Narayan Shashi, 2016, ARXIV160106068; Novikova Jekaterina, 2017, EMNLP; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pavlick Ellie, 2015, ASS COMPUTATIONAL LI; Prakash A., 2016, P COLING 2016 26 INT, P2923; Puduppully Ratish, 2019, ABS180900582 CORR; Sha Lei, 2018, 32 AAAI C ART INT; Su Yu, 2017, ARXIV170405974; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang Wenlin, 2019, ABS190307137 CORR; Wang Xin, 2018, ARXIV180409160; Wiseman Sam, 2018, P 2018 C EMP METH NA, P3174, DOI DOI 10.18653/V1/D18-1356; Xie Sang Michael, 2019, ABS190110517 CORR; Xu Jingjing, 2018, P 2018 C EMP METH NA, P4306; Yang Zhilin, 2017, ARXIV171103953; Zhao JK, 2018, PR MACH LEARN RES, V80; Ziegler Zachary M., 2019, ABS190110548 CORR	53	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905031
C	Kerenidis, I; Landman, J; Luongo, A; Prakash, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kerenidis, Iordanis; Landman, Jonas; Luongo, Alessandro; Prakash, Anupam			q-means: A quantum algorithm for unsupervised machine learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Quantum information is a promising new paradigm for fast computations that can provide substantial speedups for many algorithms we use today. Among them, quantum machine learning is one of the most exciting applications of quantum computers. In this paper, we introduce q-means, a new quantum algorithm for clustering. It is a quantum version of a robust k-means algorithm, with similar convergence and precision guarantees. We also design a method to pick the initial centroids equivalent to the classical k-means++ method. Our algorithm provides currently an exponential speedup in the number of points of the dataset, compared to the classical k-means algorithm. We also detail the running time of q-means when applied to well-clusterable datasets. We provide a detailed runtime analysis and numerical simulations for specific datasets. Along with the algorithm, the theorems and tools introduced in this paper can be reused for various applications in quantum machine learning.	[Kerenidis, Iordanis; Landman, Jonas; Luongo, Alessandro; Prakash, Anupam] Univ Paris Diderot, IRIF, CNRS, Paris, France; [Landman, Jonas] Ecole Polytech, Palaiseau, France; [Luongo, Alessandro] Atos Quantum Lab, Les Clayes Sous Bois, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Institut Polytechnique de Paris	Kerenidis, I (corresponding author), Univ Paris Diderot, IRIF, CNRS, Paris, France.	jkeren@irif.fr; landman@irif.fr; aluongo@irif.fr; anupam.prakash@irif.fr						Achlioptas D., 2001, P 33 ANN ACM S THEOR, P611; Allcock J, 2018, ARXIV181203089; Ambainis A, 2012, LEIBNIZ INT PR INFOR, V14, P636, DOI 10.4230/LIPIcs.STACS.2012.636; Arrazola J. Miguel, 2019, ARXIV190510415; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Brassard G., 2002, CONTEMP MATH, V305, DOI [10.1090/conm/305/05215, DOI 10.1090/CONM/305/05215]; Chakraborty S., 2018, ARXIV180401973; Chia Nai-Hui, 2019, ARXIV191006151; Cong Iris, 2015, ARXIV151000113; Drineas P, 2004, MACH LEARN, V56, P9, DOI 10.1023/B:MACH.0000033113.59016.96; Drineas Petros, 2002, P 34 ACM S THEORY CO, P82, DOI [10.1145/509907.509922, DOI 10.1145/509907.509922]; DURR C, 1996, QUANTPH9607014 ARXIV; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Gilyen A., 2018, ARXIV181104909; Gilyen A, 2018, ARXIV180601838; Harrow AW, 2009, PHYS REV LETT, V103, DOI 10.1103/PhysRevLett.103.150502; Jethwani D., 2019, ARXIV191005699; Kerenidis I, 2018, ARXIV180508837; Kerenidis I, 2018, ARXIV180809266; Kerenidis I, 2017, ARXIV170404992QUANTP; Kerenidis Iordanis, 2017, P 8 INN THEOR COMP S; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; Layton Robert, 1999, DEMO K MEANS CLUSTER; Lloyd S., 2013, ARXIV13070411; Lloyd S, 2014, NAT PHYS, V10, P631, DOI [10.1038/NPHYS3029, 10.1038/nphys3029]; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Nielsen M.A., 2002, QUANTUM COMPUTATION; Schuyler Hong S., 2017, ARXIV171205771QUANTP; Ta-Shma A, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P881; Tang E., 2018, ARXIV180704271; Tang E., 2018, ARXIV181100414; Wiebe N, 2015, QUANTUM INF COMPUT, V15, P316	35	14	14	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304017
C	Lin, X; Baweja, HS; Kantor, G; Held, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lin, Xingyu; Baweja, Harjatin Singh; Kantor, George; Held, David			Adaptive Auxiliary Task Weighting for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reinforcement learning is known to be sample inefficient, preventing its application to many real-world problems, especially with high dimensional observations like images. Transferring knowledge from other auxiliary tasks is a powerful tool for improving the learning efficiency. However, the usage of auxiliary tasks has been limited so far due to the difficulty in selecting and combining different auxiliary tasks. In this work, we propose a principled online learning algorithm that dynamically combines different auxiliary tasks to speed up training for reinforcement learning. Our method is based on the idea that auxiliary tasks should provide gradient directions that, in the long term, help to decrease the loss of the main task. We show in various environments that our algorithm can effectively combine a variety of different auxiliary tasks and achieves significant speedup compared to previous heuristic approaches of adapting auxiliary task weights.	[Lin, Xingyu; Baweja, Harjatin Singh; Kantor, George; Held, David] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Lin, X (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	xlin3@andrew.cmu.edu; harjatis@andrew.cmu.edu; kantor@andrew.cmu.edu; dheld@andrew.cmu.edu	Lin, Xingyu/AAY-6769-2020	Kantor, George/0000-0001-7088-8533; Held, David/0000-0003-0537-1508	United States United States Air Force [FA8750-18-C-0092]; DARPA [FA8750-18-C-0092]; National Science Foundation [IIS-1849154]; USDA Specialty Crop Research Initiative Efficient Vineyards Project [2015-51181-24393]	United States United States Air Force(United States Department of Defense); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation(National Science Foundation (NSF)); USDA Specialty Crop Research Initiative Efficient Vineyards Project	This material is based upon work supported by the United States United States Air Force and DARPA under Contract No. FA8750-18-C-0092, National Science Foundation under Grant No. IIS-1849154 and USDA Specialty Crop Research Initiative Efficient Vineyards Project 2015-51181-24393.	Agrawal P., 2016, P ADV NEURAL INFORM, P5074; Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; [Anonymous], 2018, ARXIV180209464; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen Z., 2017, GRADNORM GRADIENT NO; Du Yunshu, 2018, ARXIV181202224; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Goel V., 2018, ADV NEURAL INFORM PR, P5683; Hu H., 2017, ARXIV170806832; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kendall A, 2018, PROC CVPR IEEE, P7482, DOI 10.1109/CVPR.2018.00781; Lee M. A., 2018, ARXIV181010191; Levine S, 2018, INT J ROBOT RES, V37, P421, DOI 10.1177/0278364917710318; Li X., 2015, ARXIV150903044; Lillicrap TP, 2016, 4 INT C LEARN REPR; Lin L., 1992, MEMORY APPROACHES RE; Long M., 2017, ADV NEURAL INFO PROC, P1594; Matas J, 2018, C ROBOT LEARNING COR; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Riedmiller M., 2018, ARXIV180210567; Ruder S., 2017, ARXIV; Sax Alexander, 2018, ARXIV181211971; Shelhamer Evan, 2016, ARXIV161207307; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton RS, 2000, ADV NEUR IN, V12, P1057; SUTTON RS, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P171; Tassa Y., 2018, ARXIV180100690; Thrun S., 2012, LEARNING LEARN; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Xu Z., 2018, 32 C NEUR INF PROC S, P2396; Zamir AR, 2018, PROC CVPR IEEE, P3712, DOI 10.1109/CVPR.2018.00391; Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028; Zheng Z., 2018, ADV NEURAL INFORM PR	39	14	14	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304074
C	Liu, J; Kumar, A; Ba, J; Kiros, J; Swersky, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Jenny; Kumar, Aviral; Ba, Jimmy; Kiros, Jamie; Swersky, Kevin			Graph Normalizing Flows	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce graph normalizing flows: a new, reversible graph neural network model for prediction and generation. On supervised tasks, graph normalizing flows perform similarly to message passing neural networks, but at a significantly reduced memory footprint, allowing them to scale to larger graphs. In the unsupervised case, we combine graph normalizing flows with a novel graph auto-encoder to create a generative model of graph structures. Our model is permutation-invariant, generating entire graphs with a single feed-forward pass, and achieves competitive results with the state-of-the art auto-regressive models, while being better suited to parallel computing architectures.	[Liu, Jenny; Ba, Jimmy] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Kumar, Aviral] Univ Calif Berkeley, Berkeley, CA USA; [Kiros, Jamie; Swersky, Kevin] Google Res, Mountain View, CA USA; [Kumar, Aviral] Google, Mountain View, CA 94043 USA	University of Toronto; University of California System; University of California Berkeley; Google Incorporated; Google Incorporated	Liu, J (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	jyliu@cs.toronto.edu; aviralk@berkeley.edu; jba@cs.toronto.edu; kiros@google.com; kswersky@google.com						Almeida L. B., 1987, IEEE First International Conference on Neural Networks, P609; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], ABS161107308 CORR; [Anonymous], 2017, ICLR; Bahdanau D., 2015, INT C LEARN REPR ICL; Dinh L., 2014, ARXIV; Duvenaud David K, 2015, P NIPS; Gilmer J, 2017, PR MACH LEARN RES, V70; Goodfellow I., 2016, INT C LEARN REPR; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Kingma D.P, P 3 INT C LEARNING R; Kipf T. N., 2017, INT C LEARN REPR, DOI [DOI 10.1109/ICDM.2008.17, DOI 10.1109/ICDM.2019.00070]; Kipf TN, 2016, P INT C LEARN REPR; Li Y., 2018, ARXIV; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liao Renjie, 2018, INT C MACH LEARN, V80, P3082; Pineda F. J., 1988, NEURAL INFORM PROCES, P602; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Rezende D., 2015, ICML, P1530; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sen Prithviraj, 2008, TECHNICAL REPORT; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Velickovic P., 2018, P INT C LEARN REPR, DOI DOI 10.17863/CAM.48429; Wang Tingwu, 2018, ICLR; Yang Z, 2016, PR MACH LEARN RES, V48; You JX, 2018, PR MACH LEARN RES, V80; You Jiaxuan, 2018, CODE GRAPHRNN GENERA; Zitnik M, 2017, BIOINFORMATICS, V33, pI190, DOI 10.1093/bioinformatics/btx252	31	14	14	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905025
C	Lou, Q; Jiang, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lou, Qian; Jiang, Lei			SHE: A Fast and Accurate Deep Neural Network for Encrypted Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Homomorphic Encryption (HE) is one of the most promising security solutions to emerging Machine Learning as a Service (MLaaS). Leveled-HE (LHE)-enabled Convolutional Neural Networks (LHECNNs) are proposed to implement MLaaS to avoid large bootstrapping overhead. However, prior LHECNNs have to pay significant computing overhead but achieve only low inference accuracy, due to their polynomial approximation activations and poolings. Stacking many polynomial approximation activation layers in a network greatly reduces inference accuracy, since the polynomial approximation activation errors lead to a low distortion of the output distribution of the next batch normalization layer. So the polynomial approximation activations and poolings have become the obstacle to a fast and accurate LHECNN model. In this paper, we propose a Shift-accumulation-based LHE-enabled deep neural network (SHE) for fast and accurate inferences on encrypted data. We use the binaryoperation-friendly Leveled Fast Homomorphic Encryption over Torus (LTFHE) encryption scheme to implement ReLU activations and max poolings. We also adopt the logarithmic quantization to accelerate inferences by replacing expensive LTFHE multiplications with cheap LTFHE shifts. We propose a mixed bitwidth accumulator to accelerate accumulations. Since the LTFHE ReLU activations, max poolings, shifts and accumulations have small multiplicative depth overhead, SHE can implement much deeper network architectures with more convolutional and activation layers. Our experimental results show SHE achieves the state-of-the-art inference accuracy and reduces the inference latency by 76.21% similar to 94.23% over prior LHECNNs on MNIST and CIFAR-10.	[Lou, Qian; Jiang, Lei] Indiana Univ, Bloomington, IN 47405 USA	Indiana University System; Indiana University Bloomington	Lou, Q (corresponding author), Indiana Univ, Bloomington, IN 47405 USA.	louqian@iu.edu; jiang60@iu.edu		Lou, Qian/0000-0001-5462-2567	NSF [CCF-1908992, CCF-1909509]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF CCF-1908992 and CCF-1909509.	Boemer F, 2019, CF '19 - PROCEEDINGS OF THE 16TH ACM INTERNATIONAL CONFERENCE ON COMPUTING FRONTIERS, P3, DOI 10.1145/3310273.3323047; Boura Christina, 2018, IACR CRYPTOLOGY; Bourse Florian, 2018, ADV CRYPTOLOGY; Brutzkus A, 2019, PR MACH LEARN RES, V97; Chabanne Herve, 2017, IACR CRYPTOLOGY; Chillotti Ilaria, 2018, IACR CRYPTOLOGY; Chou E., 2018, ARXIV181109953; Dowlin N, 2016, PR MACH LEARN RES, V48; Han S., 2016, P 4 INT C LEARN REPR, P1; Hesamifard Ehsan, 2019, ACM C DAT APPL SEC P; Le Quoc V., 2015, ABS150400941 ARXIV; Lee EH, 2017, INT CONF ACOUST SPEE, P5900, DOI 10.1109/ICASSP.2017.7953288; Riazi M. Sadegh, 2018, ACM AS C COMP COMM S; Rouhani Bita Darvish, 2018, ACM IEEE DES AUT C; Sanyal Amartya, 2018, ABS180603461 ARXIV; Zhang Xiangyu, 2018, IEEE C COMP VIS PATT	16	14	14	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901064
C	Maretic, HP; Gheche, MEL; Chierchia, G; Frossard, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maretic, Hermina Petric; Gheche, Mireille E. L.; Chierchia, Giovanni; Frossard, Pascal			GOT: An Optimal Transport framework for Graph comparison	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPECTRAL DISTANCES	We present a novel framework based on optimal transport for the challenging problem of comparing graphs. Specifically, we exploit the probabilistic distribution of smooth graph signals defined with respect to the graph topology. This allows us to derive an explicit expression of the Wasserstein distance between graph signal distributions in terms of the graph Laplacian matrices. This leads to a structurally meaningful measure for comparing graphs, which is able to take into account the global structure of graphs, while most other measures merely observe local changes independently. Our measure is then used for formulating a new graph alignment problem, whose objective is to estimate the permutation that minimizes the distance between two graphs. We further propose an efficient stochastic algorithm based on Bayesian exploration to accommodate for the non-convexity of the graph alignment problem. We finally demonstrate the performance of our novel framework on different tasks like graph alignment, graph classification and graph signal prediction, and we show that our method leads to significant improvement with respect to the state-of-art algorithms.	[Maretic, Hermina Petric; Gheche, Mireille E. L.; Frossard, Pascal] Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Lausanne, Switzerland; [Chierchia, Giovanni] Univ Paris Est, LIGM UMR 8049, CNRS, ENPC,ESIEE Paris,UPEM, F-93162 Noisy Le Grand, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; Universite Gustave-Eiffel; ESIEE Paris	Maretic, HP (corresponding author), Ecole Polytech Fed Lausanne, Signal Proc Lab LTS4, Lausanne, Switzerland.	hermina.petricmaretic@epfl.ch; mireille.elgheche@epfl.ch; giovanni.chierchia@esiee.fr; pascal.frossard@epfl.ch			CNRS INS2I JCJC project [2019OSCI]	CNRS INS2I JCJC project	Giovanni Chierchia was supported by the CNRS INS2I JCJC project under grant 2019OSCI.	Aflalo Y, 2015, P NATL ACAD SCI USA, V112, P2942, DOI 10.1073/pnas.1401651112; ALVAREZMELIS D, 2018, ARXIV180900013; [Anonymous], 2014, ARXIV13126114; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Caelli T, 2004, IEEE T PATTERN ANAL, V26, P515, DOI 10.1109/TPAMI.2004.1265866; Cho M, 2010, LECT NOTES COMPUT SC, V6315, P492; Courty N., 2018, ARXIV180509114; Cuturi M., 2018, ARXIV180300567; CUTURI M., 2013, P INT C ADV NEURAL I, V26; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Dong X., 2018, ARXIV180600848; Dong XW, 2016, IEEE T SIGNAL PROCES, V64, P6160, DOI 10.1109/TSP.2016.2602809; Dym Nadav, 2017, ARXIV170506148; Emami P., 2018, ARXIV180507010; Ferradans S., 2013, SCAL SPAC VAR METH C, P428; Figurnov Mikhail, 2018, ADV NEURAL INFORM PR, V31, P441; Fiori M, 2015, INF INFERENCE, V4, P63, DOI 10.1093/imaiai/iav002; Flamary R., 2014, NIPS 2014; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Genevay A, 2018, PR MACH LEARN RES, V84; Gera Ralucca, 2018, Appl Netw Sci, V3, P2, DOI 10.1007/s41109-017-0042-3; Gu J, 2015, DISCRETE APPL MATH, V190, P56, DOI 10.1016/j.dam.2015.04.011; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jiang B., 2017, ADV NEURAL INFORM PR, P3187; Jovanovic I, 2012, LINEAR ALGEBRA APPL, V436, P1425, DOI 10.1016/j.laa.2011.08.019; Kantorovich L., 1942, TRANSFER MASSES DOKL, P227; Khan M. E., 2017, ARXIV171105560; Leordeanu Marius, 2009, ADV NEURAL INFORM PR; Lin T. Y., 2017, BRIT MACH VIS C; Luise G., 2018, ADV NEURAL INFORM PR, P5859; Memoli F, 2011, FOUND COMPUT MATH, V11, P417, DOI 10.1007/s10208-011-9093-5; Mena Gonzalo, 2018, INT C LEARN REPR; Monge M., 1781, MEMOIRE THEORIE DEBL; Nikolentzos G, 2017, AAAI CONF ARTIF INTE, P2429; Peyre G, 2016, PR MACH LEARN RES, V48; Reddi S.J., 2018, INT C LEARN REPR; RUE H., 2005, GAUSSIAN MARKOV RAND; Schellewald C, 2005, LECT NOTES COMPUT SC, V3757, P171, DOI 10.1007/11585978_12; Segarra S, 2017, IEEE T SIGNAL INF PR, V3, P467, DOI 10.1109/TSIPN.2017.2731051; Shahid N, 2015, IEEE I CONF COMP VIS, P2812, DOI 10.1109/ICCV.2015.322; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; Steger A, 1999, COMB PROBAB COMPUT, V8, P377, DOI 10.1017/S0963548399003867; Takatsu A, 2011, OSAKA J MATH, V48, P1005; Tsitsulin A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2347, DOI 10.1145/3219819.3219991; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; WANG W, 2007, ADV NEURAL INFORM PR, P313; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Yan JC, 2016, ICMR'16: PROCEEDINGS OF THE 2016 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P167, DOI 10.1145/2911996.2912035; Yu T., 2018, ADV NEURAL INFORM PR, P853; Zhou F, 2016, IEEE T PATTERN ANAL, V38, P1774, DOI 10.1109/TPAMI.2015.2501802; Zhou F, 2013, PROC CVPR IEEE, P2922, DOI 10.1109/CVPR.2013.376; Zhu Xiaojin., 2003, P ICLR, P912	52	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905054
C	Negrea, J; Haghifam, M; Dziugaite, GK; Khisti, A; Roy, DM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Negrea, Jeffrey; Haghifam, Mahdi; Dziugaite, Gintare Karolina; Khisti, Ashish; Roy, Daniel M.			Information-Theoretic Generalization Bounds for SGLD via Data-Dependent Estimates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this work, we improve upon the stepwise analysis of noisy iterative learning algorithms initiated by Pensia, Jog, and Loh (2018) and recently extended by Bu, Zou, and Veeravalli (2019). Our main contributions are significantly improved mutual information bounds for Stochastic Gradient Langevin Dynamics via data-dependent estimates. Our approach is based on the variational characterization of mutual information and the use of data-dependent priors that forecast the mini-batch gradient based on a subset of the training samples. Our approach is broadly applicable within the information-theoretic framework of Russo and Zou (2015) and Xu and Raginsky (2017). Our bound can be tied to a measure of flatness of the empirical risk surface. As compared with other bounds that depend on the squared norms of gradients, empirical investigations show that the terms in our bounds are orders of magnitude smaller.	[Negrea, Jeffrey; Haghifam, Mahdi; Khisti, Ashish; Roy, Daniel M.] Univ Toronto, Toronto, ON, Canada; [Negrea, Jeffrey; Roy, Daniel M.] Vector Inst, Toronto, ON, Canada; [Haghifam, Mahdi; Dziugaite, Gintare Karolina] Element AI, Montreal, PQ, Canada	University of Toronto	Negrea, J (corresponding author), Univ Toronto, Toronto, ON, Canada.; Negrea, J (corresponding author), Vector Inst, Toronto, ON, Canada.				NSERC Vanier Canada Graduate Scholarship; Vector Institute; MITACS Accelerate Fellowship; NSERC Discovery Grant; Ontario Early Researcher Award; Element AI	NSERC Vanier Canada Graduate Scholarship; Vector Institute; MITACS Accelerate Fellowship; NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); Ontario Early Researcher Award(Ministry of Research and Innovation, Ontario); Element AI	JN is supported by an NSERC Vanier Canada Graduate Scholarship, and by the Vector Institute. MH was supported by a MITACS Accelerate Fellowship with Element AI. DMR is supported by an NSERC Discovery Grant and an Ontario Early Researcher Award. This research was carried out in part while GKD and DMR were visiting the Simons Institute for the Theory of Computing.	Ambroladze Amiran, 2007, ADV NEURAL INFORM PR, P9; [Anonymous], 2017, ADV NEURAL INFORM PR; Asadi A., 2018, ADV NEURAL INFORM PR, P7234; Bassily R., 2018, ALGORITHMIC LEARNING, V83, P25; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bu Y., 2019, IEEE INT S INF THEOR; CATONI O, 2007, I MATH STAT LECT NOT, V56; DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1, DOI 10.1002/cpa.3160280102; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; Dziugaite GK, 2018, ADV NEUR IN, V31; ERMAK DL, 1975, J CHEM PHYS, V62, P4189, DOI 10.1063/1.430300; Feldman V., 2018, C LEARNING THEORY, P535; GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055; Hardt Moritz, 2016, INT C MACH LEARN; Ibrahim A. E. I., 2019, ARXIV190303787; Jiao J., 2017, IEEE INT S INF THEOR; Kallenberg O., 2006, FDN MODERN PROBABILI; Kemperman J.H.B., 1974, INDAGATIONES MATH, V77, P101, DOI 10.1016/1385-7258(74)90000-6; LeCun Y, 2010, ATT LAB; Li J., 2019, ARXIV190200621; Lopez A., 2018, IEEE INF THEOR WORKS; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Mou W., 2018, P MACHINE LEARNING R, P605; Parrado-Hernandez E, 2012, J MACH LEARN RES, V13, P3507; Pensia A, 2018, IEEE INT SYMP INFO, P546; Poole B., 2019, ARXIV190506922; Raginsky M., 2017, P C LEARN THEOR COLT; Raginsky M, 2016, 2016 IEEE INFORMATION THEORY WORKSHOP (ITW); Rivasplata O., 2018, ADV NEURAL INFORM PR, P9214; Russo D., 2015, ARXIV151105219; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shawe-Taylor J., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P2, DOI 10.1145/267460.267466; Thomas V., 2019, ARXIV190607774; Wang YX, 2016, LECT NOTES COMPUT SC, V9867, P121, DOI 10.1007/978-3-319-45381-1_10; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	35	14	14	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902062
C	Niu, XS; Han, H; Shan, SG; Chen, XL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Niu, Xuesong; Han, Hu; Shan, Shiguang; Chen, Xilin			Multi-label Co-regularization for Semi-supervised Facial Action Unit Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EXPRESSION	Facial action units (AUs) recognition is essential for emotion analysis and has been widely applied in mental state analysis. Existing work on AU recognition usually requires big face dataset with accurate AU labels. However, manual AU annotation requires expertise and can be time-consuming. In this work, we propose a semi-supervised approach for AU recognition utilizing a large number of web face images without AU labels and a small face dataset with AU labels inspired by the co-training methods. Unlike traditional co-training methods that require provided multi-view features and model re-training, we propose a novel co-training method, namely multi-label co-regularization, for semi-supervised facial AU recognition. Two deep neural networks are used to generate multi-view features for both labeled and unlabeled face images, and a multi-view loss is designed to enforce the generated features from the two views to be conditionally independent representations. In order to obtain consistent predictions from the two views, we further design a multi-label co-regularization loss aiming to minimize the distance between the predicted AU probability distributions of the two views. In addition, prior knowledge of the relationship between individual AUs is embedded through a graph convolutional network (GCN) for exploiting useful information from the big unlabeled dataset. Experiments on several benchmarks show that the proposed approach can effectively leverage large datasets of unlabeled face images to improve the AU recognition robustness and outperform the state-of-the-art semi-supervised AU recognition methods. Code is available(1).	[Niu, Xuesong; Han, Hu; Shan, Shiguang; Chen, Xilin] Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China; [Han, Hu; Shan, Shiguang] Peng Cheng Lab, Shenzhen, Peoples R China; [Niu, Xuesong; Shan, Shiguang; Chen, Xilin] Univ Chinese Acad Sci, Beijing 100049, Peoples R China; [Shan, Shiguang] CAS Ctr Excellence Brain Sci & Intelligence Techn, Shanghai, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Peng Cheng Laboratory; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Niu, XS (corresponding author), Chinese Acad Sci, Inst Comp Technol, Key Lab Intelligent Informat Proc, Beijing 100190, Peoples R China.; Niu, XS (corresponding author), Univ Chinese Acad Sci, Beijing 100049, Peoples R China.	xuesong.niu@vipl.ict.ac.cn; hanhu@ict.ac.cn; sgshan@ict.ac.cn; xlchen@ict.ac.cn		Shan, Shiguang/0000-0002-8348-392X	National Key R&D Program of China [2017Y-FA0700800]; Natural Science Foundation of China [61672496, 61702481]; External Cooperation Program of Chinese Academy of Sciences (CAS) [GJHZ1843]; Youth Innovation Promotion Association CAS [2018135]	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); External Cooperation Program of Chinese Academy of Sciences (CAS); Youth Innovation Promotion Association CAS	This research was supported in part by the National Key R&D Program of China (grant 2017Y-FA0700800), Natural Science Foundation of China (grants 61672496 and 61702481), External Cooperation Program of Chinese Academy of Sciences (CAS) (grant GJHZ1843), and Youth Innovation Promotion Association CAS (2018135).	Almaev T., 2015, P IEEE ICCV; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Cai YH, 2016, IEEE CONF COMPUT; Corneanu C., 2018, P ECCV; Cui JY, 2018, INT CONF BIOMETR, P140, DOI 10.1109/ICB2018.2018.00031; Ekman Paul, 1997, WHAT FACE REVEALS BA, P2; Eleftheriadis S., 2015, P IEEE ICCV; Endres DM, 2003, IEEE T INFORM THEORY, V49, P1858, DOI 10.1109/TIT.2003.813506; Fabian Benitez-Quiroz C., 2016, P IEEE CVPR; FELDMAN RS, 1979, CHILD DEV, V50, P350, DOI 10.2307/1129409; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Hand E. M., 2018, P AAAI; He K, 2016, IEEE INT CONF MULTI; Joachims Thorsten, 1999, P ICML; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Li JN, 2019, IEEE ICC; Li W., 2017, P IEEE CVPR; Liu Z., 2015, P IEEE ICCV; Lucey P., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P57, DOI 10.1109/FG.2011.5771462; Nigam K., 2000, P CIKM; Niu X., 2018, P ICMI; Pan HY, 2018, PROC CVPR IEEE, P5285, DOI 10.1109/CVPR.2018.00554; Paszke A., 2017, AUTOMATIC DIFFERENTI; Peng G., 2019, P AAAI; Peng G., 2018, P IEEE CVPR; Qiao S., 2018, P ECCV; RUBINOW DR, 1992, BIOL PSYCHIAT, V31, P947, DOI 10.1016/0006-3223(92)90120-O; Shao Z., 2018, P ECCV; SHI QZ, 2017, 2017 IEEE INT C; Taigman Y., 2014, P IEEE CVPR; Tarvainen Antti, 2017, CORR, Vabs/1703; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang F, 2017, IEEE INT CONF AUTOMA, P173, DOI 10.1109/FG.2017.30; Xing Y., 2018, P IJCAI; Xu B., 2015, P ICML; Zhang X, 2014, IMAGE VISION COMPUT, V32, P692, DOI 10.1016/j.imavis.2014.06.002; Zhang Y, 2018, 2018 IEEE INTERNATIONAL WORKSHOP ON ANTENNA TECHNOLOGY (IWAT); Zhao K., 2015, P IEEE CVPR; Zhao K, 2018, 2018 2ND IEEE CONFERENCE ON ENERGY INTERNET AND ENERGY SYSTEM INTEGRATION (EI2)	40	14	14	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300082
C	Raff, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Raff, Edward			A Step Toward Quantifying Independently Reproducible Machine Learning Research	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					What makes a paper independently reproducible? Debates on reproducibility center around intuition or assumptions but lack empirical results. Our field focuses on releasing code, which is important, but is not sufficient for determining reproducibility. We take the first step toward a quantifiable answer by manually attempting to implement 255 papers published from 1984 until 2017, recording features of each paper, and performing statistical analysis of the results. For each paper, we did not look at the authors code, if released, in order to prevent bias toward discrepancies between code and paper.	[Raff, Edward] Univ Maryland Baltimore Cty, Baltimore, MD 21228 USA	University System of Maryland; University of Maryland Baltimore County	Raff, E (corresponding author), Univ Maryland Baltimore Cty, Baltimore, MD 21228 USA.	raff.edward@umbc.edu		Raff, Edward/0000-0002-9900-1972				Dror R., 2017, T ASSOC COMPUT LING, V5, P471, DOI 10.1162/tacl_a_00074; Drummond C., 2009, P EV METH MACH LARN; DUNN OJ, 1961, J AM STAT ASSOC, V56, P52, DOI 10.2307/2282330; Forde J., 2018, REPR ML WORKSH ICML; Gundersen OE, 2018, AAAI CONF ARTIF INTE, P1644; Hutson M, 2018, SCIENCE, V359, P725, DOI 10.1126/science.359.6377.725; JASP Team, 2018, JASP VERS 0 9; KRUSKAL WH, 1952, J AM STAT ASSOC, V47, P583, DOI 10.1080/01621459.1952.10483441; MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491; Pearson K, 1900, PHILOS MAG, V50, P157, DOI 10.1080/14786440009463897; Publio G. C., 2018, REPR ML WORKSH ICML; Raff E, 2017, J MACH LEARN RES, V18; Rahimi A., 2017, NIPS 2017 TEST OF TI; Sculley D., 2018, ICLR WORKSH TRACK; SHAPIRO SS, 1965, BIOMETRIKA, V52, P591, DOI 10.1093/biomet/52.3-4.591; Tatman R., 2018, REPR ML WORKSH ICML; Yates F, 1934, J ROYAL STAT SOC S, V1, P217, DOI DOI 10.2307/2983604	17	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305047
C	Rubanova, Y; Chen, RTQ; Duvenaud, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rubanova, Yulia; Chen, Ricky T. Q.; Duvenaud, David			Latent ODEs for Irregularly-Sampled Time Series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Time series with non-uniform intervals occur in many applications, and are difficult to model using standard recurrent neural networks (RNNs). We generalize RNNs to have continuous-time hidden dynamics defined by ordinary differential equations (ODEs), a model we call ODE-RNNs. Furthermore, we use ODE-RNNs to replace the recognition network of the recently-proposed Latent ODE model. Both ODE-RNNs and Latent ODEs can naturally handle arbitrary time gaps between observations, and can explicitly model the probability of observation times using Poisson processes. We show experimentally that these ODE-based models outperform their RNN-based counterparts on irregularly-sampled data.	[Rubanova, Yulia] Univ Toronto, Toronto, ON, Canada; Vector Inst, Toronto, ON, Canada	University of Toronto	Rubanova, Y (corresponding author), Univ Toronto, Toronto, ON, Canada.	rubanova@cs.toronto.edu; rtqichen@cs.toronto.edu; duvenaud@cs.toronto.edu	Chen, Ricky Tian Qi/AAS-3168-2021					Ayed I., 2019, ARXIV190211136; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Cao Wei, 2018, BRITS BIDIRECTIONAL; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Chen T.Q., 2018, ADV NEURAL INFORM PR; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; De Brouwer E., 2019, ARXIV190512374; Futoma J, 2017, PR MACH LEARN RES, V70; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lipton Zachary C, 2016, MACHINE LEARNING HEA, P253; Luo YH, 2018, ADV NEUR IN, V31; Mei HY, 2017, ADV NEUR IN, V30; Mozer Michael C., 2017, DISCRETE EVENT CONTI; Palm C., 1943, ERICSSON TECHNICS; Pontryagin L.S., 1962, MATH THEORY OPTIMAL; Rajkomar A, 2018, NPJ DIGIT MED, V1, DOI 10.1038/s41746-018-0029-1; Shukla SN, 2019, P INT C LEARN REPR; Silva Ikaro, 2012, Comput Cardiol (2010), V39, P245; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tassa Y., 2018, ARXIV180100690	21	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305033
C	Sato, R; Yamada, M; Kashima, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sato, Ryoma; Yamada, Makoto; Kashima, Hisashi			Approximation Ratios of Graph Neural Networks for Combinatorial Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS; LOCALITY; TIME	In this paper, from a theoretical perspective, we study how powerful graph neural networks (GNNs) can be for learning approximation algorithms for combinatorial problems. To this end, we first establish a new class of GNNs that can solve a strictly wider variety of problems than existing GNNs. Then, we bridge the gap between GNN theory and the theory of distributed local algorithms. We theoretically demonstrate that the most powerful GNN can learn approximation algorithms for the minimum dominating set problem and the minimum vertex cover problem with some approximation ratios with the aid of the theory of distributed local algorithms. We also show that most of the existing GNNs such as GIN, GAT, GCN, and GraphSAGE cannot perform better than with these ratios. This paper is the first to elucidate approximation ratios of GNNs for combinatorial problems. Furthermore, we prove that adding coloring or weak-coloring to each node feature improves these approximation ratios. This indicates that preprocessing and feature engineering theoretically strengthen model capabilities.	[Sato, Ryoma; Yamada, Makoto; Kashima, Hisashi] Kyoto Univ, Kyoto, Japan; [Sato, Ryoma; Yamada, Makoto; Kashima, Hisashi] RIKEN AIP, Tokyo, Japan; [Yamada, Makoto] JST PRESTO, Tokyo, Japan	Kyoto University; RIKEN; Japan Science & Technology Agency (JST)	Sato, R (corresponding author), Kyoto Univ, Kyoto, Japan.; Sato, R (corresponding author), RIKEN AIP, Tokyo, Japan.	r.sato@ml.ist.i; myamada@i.kyoto-u.ac.jp; kashima@i.kyoto-u.ac.jp			JSPS KAKENHI [15H01704]; JST PRESTO program [JPMJPR165A]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST PRESTO program	This work was supported by JSPS KAKENHI Grant Number 15H01704. MY is supported by the JST PRESTO program JPMJPR165A.	Angluin D., 1980, PROC 12 ANN ACM S TH, P82, DOI DOI 10.1145/800141.804655; [Anonymous], 2018, ABS181000826 CORR; [Anonymous], 2012, ACM S PRINC DISTR CO, DOI DOI 10.1145/2332432.2332466; Astrand M, 2009, LECT NOTES COMPUT SC, V5805, P191, DOI 10.1007/978-3-642-04355-0_21; Astrand Matti, 2010, ABS10020125 CORR; Bello Irwan, 2016, ABS161109940 CORR; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Czygrinow A, 2008, LECT NOTES COMPUT SC, V5218, P78, DOI 10.1007/978-3-540-87779-0_6; Dai Hanjun, 2017, P 31 INT C NEUR INF, P6351; Gilmer J, 2017, PR MACH LEARN RES, V70; Hamilton WL, 2017, P 31 INT C NEUR INF, P1025; Kipf TN, 2016, P INT C LEARN REPR; Kubisch M, 2003, IEEE WCNC, P558; Lenzen C, 2008, LECT NOTES COMPUT SC, V5218, P394, DOI 10.1007/978-3-540-87779-0_27; Lenzen C, 2009, LECT NOTES COMPUT SC, V5873, P17, DOI 10.1007/978-3-642-05118-0_2; Li Zhuwen, 2018, ADV NEURAL INFORM PR, V31, P537, DOI DOI 10.1007/978-3-030-04221-9_48; NAOR M, 1995, SIAM J COMPUT, V24, P1259, DOI 10.1137/S0097539793254571; Nguyen HN, 2008, ANN IEEE SYMP FOUND, P327, DOI 10.1109/FOCS.2008.81; Parnas M, 2007, THEOR COMPUT SCI, V381, P183, DOI 10.1016/j.tcs.2007.04.040; Ribeiro LFR, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P385, DOI 10.1145/3097983.3098061; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schlichtkrull M. S., 2017, ABS170306103 CORR; Suomela J, 2013, ACM COMPUT SURV, V45, DOI 10.1145/2431211.2431223; Velickovic P., 2018, ICLR; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wattenhofer M, 2004, LECT NOTES COMPUT SC, V3274, P335; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890	31	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304012
C	Spielberg, A; Zhao, A; Du, T; Hu, YM; Rus, D; Matusik, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Spielberg, Andrew; Zhao, Allan; Du, Tao; Hu, Yuanming; Rus, Daniela; Matusik, Wojciech			Learning-In-The-Loop Optimization: End-To-End Control And Co-Design of Soft Robots Through Learned Deep Latent Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Soft robots have continuum solid bodies that can deform in an infinite number of ways. Controlling soft robots is very challenging as there are no closed form solutions. We present a learning-in-the-loop co-optimization algorithm in which a latent stale representation is learned as the robot figures out how to solve the task. Our solution marries hybrid particle-grid-based simulation with deep, variational convolutional autoencoder architectures that can capture salient features of robot dynamics with high efficacy. We demonstrate our dynamics-aware feature learning algorithm on both 21) and 31) soft robots, and show that it is more robust and faster converging than the dynamics-oblivious baseline. We validate the behavior of our algorithm with visualizations of the learned representation.	[Spielberg, Andrew; Zhao, Allan; Du, Tao; Hu, Yuanming; Rus, Daniela; Matusik, Wojciech] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Spielberg, A (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	aespielberg@csail.mit.edu; azhao@mit.edu; taodu@csail.mit.edu; yuanming@mit.edu; rus@csail.mit.edu; wojciech@csail.mit.edu			NSF [1138967]; Unity Global Graduate Fellowship; IARPA [2019-19020100001]; MIT EECS David S. Y. Wong Fellowship	NSF(National Science Foundation (NSF)); Unity Global Graduate Fellowship; IARPA; MIT EECS David S. Y. Wong Fellowship	This work was supported by NSF grant No. 1138967, the Unity Global Graduate Fellowship, IARPA grant No. 2019-19020100001, and The MIT EECS David S. Y. Wong Fellowship.	Amini A, 2018, IEEE INT C INT ROBOT, P568, DOI 10.1109/IROS.2018.8594386; Barbic J, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531359; Barbic J, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409060.1409116; Chen D, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073669; Cheney N, 2018, J R SOC INTERFACE, V15, DOI 10.1098/rsif.2017.0937; Cheney N, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P167; Corucci F., 2016, ALIFE 15, V6; Della Santina C, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON SOFT ROBOTICS (ROBOSOFT), P46, DOI 10.1109/robosoft.2018.8404895; Doersch Carl, 2016, ARXIV160605908, DOI DOI 10.3389/FPHYS.2016.00108; Goury O, 2018, IEEE T ROBOT, V34, P1565, DOI 10.1109/TRO.2018.2861900; Ha S, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Heess Nicolas, 2017, ABS170702286 CORR, P3; Hu YM, 2019, IEEE INT CONF ROBOT, P6265, DOI 10.1109/ICRA.2019.8794333; Hu YM, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201293; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Ma PX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201334; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Sifakis E., 2012, ACM SIGGRAPH 2012 CO; Spielberg Andrew, INT C ROB AUT ICRA; Thieffry M, 2018, 2018 EUROPEAN CONTROL CONFERENCE (ECC), P629; Wampler K, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1531326.1531366; Wang T., 2019, ARXIV190605370	26	14	14	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308032
C	Wu, YK; Zhang, L; Wu, XT; Tong, HH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Yongkai; Zhang, Lu; Wu, Xintao; Tong, Hanghang			PC-Fairness: A Unified Framework for Measuring Causality-based Fairness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A recent trend of fair machine learning is to define fairness as causality-based notions which concern the causal connection between protected attributes and decisions. However, one common challenge of all causality-based fairness notions is identifiability, i.e., whether they can be uniquely measured from observational data, which is a critical barrier to applying these notions to real-world situations. In this paper, we develop a framework for measuring different causality-based fairness. We propose a unified definition that covers most of previous causality-based fairness notions, namely the path-specific counterfactual fairness (PC fairness). Based on that, we propose a general method in the form of a constrained optimization problem for bounding the path-specific counterfactual fairness under all unidentifiable situations. Experiments on synthetic and real-world datasets show the correctness and effectiveness of our method.	[Wu, Yongkai; Zhang, Lu; Wu, Xintao] Univ Arkansas, Fayetteville, AR 72701 USA; [Tong, Hanghang] Univ Illinois, Urbana, IL USA	University of Arkansas System; University of Arkansas Fayetteville; University of Illinois System; University of Illinois Urbana-Champaign	Wu, YK (corresponding author), Univ Arkansas, Fayetteville, AR 72701 USA.	yw009@uark.edu; lz006@uark.edu; xintaowu@uark.edu; htong@illinois.edu		Wu, Xintao/0000-0002-2823-3063; Wu, Yongkai/0000-0002-7313-9439; Tong, Hanghang/0000-0003-4405-3887	NSF [1646654, 1920920, 1940093]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF 1646654, 1920920, and 1940093.	Avin C, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P357; Balke A., 1994, Uncertainty in Artificial Intelligence. Proceedings of the Tenth Conference (1994), P46; Chiappa S, 2019, AAAI CONF ARTIF INTE, P7801; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Kusner M.J, 2017, ADV NEURAL INFORM PR, V30, P4066; Malinsky Daniel, 2019, Proc Mach Learn Res, V89, P3080; Nabi R, 2018, AAAI CONF ARTIF INTE, P1931; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Russell C, 2017, ADV NEUR IN, V30; Scheines R, 1998, MULTIVAR BEHAV RES, V33, P65, DOI 10.1207/s15327906mbr3301_3; Shpitser I., 2007, P 23 C UNC ART INT, P352; Shpitser I, 2008, J MACH LEARN RES, V9, P1941; TIAN J, 2000, P 16 C UNC ART INT S, P589; Wu YK, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1438; Zhang J, 2018, 32 AAAI C ART INT; Zhang Junzhe, 2018, ADV NEURAL INFORM PR, P3675; Zhang L., 2016, IJCAI, P2718; Zhang LM, 2019, IEEE T FUZZY SYST, V27, P1052, DOI 10.1109/TFUZZ.2018.2872125; Zhang L, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3929; Zhang L, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1335, DOI 10.1145/3097983.3098167	20	14	14	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303040
C	Yadati, N; Nimishakavi, M; Yadav, P; Nitin, V; Louis, A; Talukdar, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yadati, Naganand; Nimishakavi, Madhav; Yadav, Prateek; Nitin, Vikram; Louis, Anand; Talukdar, Partha			HyperGCN: A New Method of Training Graph Convolutional Networks on Hypergraphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In many real-world networks such as co-authorship, co-citation, etc., relationships are complex and go beyond pairwise associations. Hypergraphs provide a flexible and natural modeling tool to model such complex relationships. The obvious existence of such complex relationships in many real-world networks naturally motivates the problem of learning with hypergraphs. A popular learning paradigm is hypergraph-based semi-supervised learning (SSL) where the goal is to assign labels to initially unlabelled vertices in a hypergraph. Motivated by the fact that a graph convolutional network (GCN) has been effective for graph-based SSL, we propose HyperGCN, a novel way of training a GCN for SSL on hypergraphs based on tools from sepctral theory of hypergraphs. We demonstrate HyperGCN's effectiveness through detailed experimentation on real-world hypergraphs for SSL and combinatorial optimisation and analyse when it is going to be more effective than state-of-the art baselines. We have made the source code available.	[Yadati, Naganand; Nimishakavi, Madhav; Yadav, Prateek; Nitin, Vikram; Louis, Anand; Talukdar, Partha] Indian Inst Sci, Bangalore, Karnataka, India; [Nitin, Vikram] Birla Inst Technol & Sci, Pilani, Rajasthan, India	Indian Institute of Science (IISC) - Bangalore; Birla Institute of Technology & Science Pilani (BITS Pilani)	Yadati, N (corresponding author), Indian Inst Sci, Bangalore, Karnataka, India.	y.naganand@gmail.com; cse.madhav@gmail.com; ugprateek@gmail.com; vikramnitin9@gmail.com; anandl@iisc.ac.in; partha@talukdar.net			SERB Award [ECR/2017/003296]; Pratiksha Trust Young Investigator Award; Google India	SERB Award; Pratiksha Trust Young Investigator Award; Google India(Google Incorporated)	Anand Louis was supported in part by SERB Award ECR/2017/003296 and a Pratiksha Trust Young Investigator Award. We acknowledge the support of Google India and NeurIPS in the form of an International Travel Grant, which enabled Naganand Yadati to attend the conference.	Agarwal S., 2006, ICML, P17, DOI DOI 10.1145/1143844.1143847; Amburg Ilya, 2019, ARXIV19050583 CORR; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Avelar Pedro H. C., 2019, P 32 C ASS ADV ART I; Battaglia Peter W, 2018, ARXIV180601261; Bronstein M.M., 2017, IEEE SIGNAL PROCESS; Bul`o S. R., 2009, ADV NEURAL INFORM PR, P1571; Chan THH, 2020, THEOR COMPUT SYST, V64, P999, DOI 10.1007/s00224-019-09958-4; Chan THH, 2018, LECT NOTES COMPUT SC, V10976, P441, DOI 10.1007/978-3-319-94776-1_37; Chan THH, 2018, J ACM, V65, DOI 10.1145/3178123; Chapelle O., 2003, ADV NEURAL INFORM PR; Chapelle Olivier, 2010, SEMISUPERVISED LEARN, V2, P5; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Chien E., 2019, 22 INT C ART INT STA, P2466; Chitra U, 2019, PR MACH LEARN RES, V97; Chlamtac E, 2018, SIAM J DISCRETE MATH, V32, P1458, DOI 10.1137/16M1096402; Du JB, 2019, INT CONF WIRE COMMUN; Feng FL, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1523, DOI 10.1145/3178876.3186064; Feng Yifan, 2019, P 33 C ASS ADV ART I; Gilmer J, 2017, PR MACH LEARN RES, V70; Gong Yu, 2019, KDD; Hamilton WL, 2017, REPRESENTATION LEARN; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Hein M., 2013, P ADV NEUR INF PROC, V26; Hung Nguyen, 2019, ARXIV19010792 CORR; Jin TS, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2670; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Lemos Henrique, 2019, P 28 INT JOINT C ART; LI P, 2017, ADV NEURAL INFORM PR, V30, P2308; Li P., 2018, P 35 INT C MACHINE L, P3014; Li Pan, 2018, ADV NEURAL INFORM PR, P1054; Li ZW, 2018, ADV NEUR IN, V31; Louis A, 2015, ACM S THEORY COMPUT, P713, DOI 10.1145/2746539.2746555; Mallat S., 1998, WAVELET TOUR SIGNAL, DOI 10.1016/B978-0-12-374370-1.X0001-8; Marcheggiani D., 2017, P 2017 C EMP METH NA, P1506, DOI [10.18653/v1/D17-1159, DOI 10.18653/V1/D17-1159]; Monti Federico, 2018, ABS180600770; Norcliffe-Brown W, 2018, ADV NEUR IN, V31; Pan Li, 2018, ADV NEURAL INFORM PR, V31, P2237; SATCHIDANAND SN, 2015, IJCAI, P3791; Shashua A, 2006, LECT NOTES COMPUT SC, V3954, P595; Shuman David, 2013, IEEE SIGNAL PROCESSI, V30, P3, DOI DOI 10.1109/MSP.2012.2235192; Subramanya A, 2014, GRAPH BASED SEMISUPE; Tu Ke, 2018, P 32 C ASS ADV ART I; Vashishth Shikhar, 2019, INT C ART INT STAT A; Vashishth Shikhar, 2019, P 57 ANN M ASS COMP; Velickovic P., 2018, ICLR; Wendler C, 2019, ADV NEUR IN, V32; Weston J., 2008, P 25 INT C MACHINE L, P1168, DOI [DOI 10.1145/1390156.1390303, 10.1145/1390156.1390303]; Zhang Muhan, 2018, P 32 C ASS ADV ART I; Zhou D, 2006, P 2006 C ADV NEURAL, V19, DOI 10.7551/mitpress/7503.003.0205; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu CS, 2017, INTEGR MED RES, V6, P1, DOI 10.1016/j.imr.2017.01.008; Zhu X., 2003, INT C MACH LEARN; Zhu X., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [10.2200/S00196ED1V01Y200906AIM006, DOI 10.2200/S00196ED1V01Y200906AIM006]	54	14	14	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301049
C	Yu, H; Lee, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Hyeonwoo; Lee, Beomhee			Zero-shot Learning via Simultaneous Generating and Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					To overcome the absence of training data for unseen classes, conventional zero-shot learning approaches mainly train their model on seen datapoints and leverage the semantic descriptions for both seen and unseen classes. Beyond exploiting relations between classes of seen and unseen, we present a deep generative model to provide the model with experience about both seen and unseen classes. Based on the variational auto-encoder with class-specific multi-modal prior, the proposed method learns the conditional distribution of seen and unseen classes. In order to circumvent the need for samples of unseen classes, we treat the non-existing data as missing examples. That is, our network aims to find optimal unseen datapoints and model parameters, by iteratively following the generating and learning strategy. Since we obtain the conditional generative model for both seen and unseen classes, classification as well as generation can be performed directly without any off-the-shell classifiers. In experimental results, we demonstrate that the proposed generating and learning strategy makes the model achieve the outperforming results compared to that trained only on the seen classes, and also to the several state-of-the-art methods.	[Yu, Hyeonwoo; Lee, Beomhee] Seoul Natl Univ, Dept Elect & Comp Engn, Automat & Syst Res Inst ASRI, Seoul, South Korea	Seoul National University (SNU)	Yu, H (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Automat & Syst Res Inst ASRI, Seoul, South Korea.	bgus2000@snu.ac.kr; bhlee@snu.ac.kr			National Research Foundation of Korea(NRF) - Korea government(MSIP) [2017R1A2B2002608]; Automation and Systems Research Institute (ASRI); Brain Korea 21 Plus Project	National Research Foundation of Korea(NRF) - Korea government(MSIP)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea); Automation and Systems Research Institute (ASRI); Brain Korea 21 Plus Project	We would like to thank Jihoon Moon and Hanjun Kim, who give us intuitive advices. This work was supported by the National Research Foundation of Korea(NRF) grant funded by the Korea government(MSIP) (No. 2017R1A2B2002608), in part by Automation and Systems Research Institute (ASRI), and in part by the Brain Korea 21 Plus Project.	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Annadani Y, 2018, PROC CVPR IEEE, P7603, DOI 10.1109/CVPR.2018.00793; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bishop CM, 2006, PATTERN RECOGNITION; Bucher M, 2016, LECT NOTES COMPUT SC, V9909, P730, DOI 10.1007/978-3-319-46454-1_44; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Dilokthanakul Nat, 2016, ARXIV161102648; Frome Andrea, 2013, NEURIPS; Gal Y, 2016, PR MACH LEARN RES, V48; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Mishra A, 2018, IEEE COMPUT SOC CONF, P2269, DOI 10.1109/CVPRW.2018.00294; Norouzi Mohammad, 2014, ICLR; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Romera-Paredes Bernardino, 2015, ICML; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sohn Kihyuk, 2015, ADV NEURAL INFORM PR, P3483, DOI DOI 10.5555/2969442.2969628; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tang L, 2017, WINT SIMUL C PROC, P2021; Tian BW, 2017, INT GEOSCI REMOTE SE, P5759, DOI 10.1109/IGARSS.2017.8128316; Verma VK, 2018, PROC CVPR IEEE, P4281, DOI 10.1109/CVPR.2018.00450; Wah C, 2011, IEEE I CONF COMP VIS, P2524, DOI 10.1109/ICCV.2011.6126539; Wang WL, 2018, AAAI CONF ARTIF INTE, P4211; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Yang Yongxin, 2014, ARXIV14127489; Yu HW, 2019, IEEE INT CONF ROBOT, P5866, DOI 10.1109/ICRA.2019.8794111; Yu HW, 2018, IEEE INT C INT ROBOT, P3605, DOI 10.1109/IROS.2018.8593831; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; ZHANG ZM, 2016, PROC CVPR IEEE, P6034, DOI DOI 10.1109/CVPR.2016.649; Zhu YZ, 2018, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2018.00111	36	14	17	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300005
C	Zhao, H; Gordon, GJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Han; Gordon, Geoffrey J.			Inherent Tradeoffs in Learning Fair Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIVERGENCE	With the prevalence of machine learning in high-stakes applications, especially the ones regulated by anti-discrimination laws or societal norms, it is crucial to ensure that the predictive models do not propagate any existing bias or discrimination. Due to the ability of deep neural nets to learn rich representations, recent advances in algorithmic fairness have focused on learning fair representations with adversarial techniques to reduce bias in data while preserving utility simultaneously. In this paper, through the lens of information theory, we provide the first result that quantitatively characterizes the tradeoff between demographic parity and the joint utility across different population groups. Specifically, when the base rates differ between groups, we show that any method aiming to learn fair representations admits an information-theoretic lower bound on the joint error across these groups. To complement our negative results, we also prove that if the optimal decision functions across different groups are close, then learning fair representations leads to an alternative notion of fairness, known as the accuracy parity, which states that the error rates are close between groups. Finally, our theoretical findings are also confirmed empirically on real-world datasets.	[Zhao, Han] Carnegie Mellon Univ, Sch Comp Sci, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Zhao, Han; Gordon, Geoffrey J.] Microsoft Res, Montreal, PQ, Canada; [Gordon, Geoffrey J.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Zhao, H (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Machine Learning Dept, Pittsburgh, PA 15213 USA.; Zhao, H (corresponding author), Microsoft Res, Montreal, PQ, Canada.	han.zhao@cs.cmu.edu; geoff.gordon@microsoft.com			DARPA XAI project [FA87501720152]; Nvidia GPU grant	DARPA XAI project; Nvidia GPU grant	HZ and GG would like to acknowledge support from the DARPA XAI project, contract #FA87501720152 and an Nvidia GPU grant. HZ would also like to thank Jianfeng Chi for helpful discussions on the relationship between algorithmic fairness and privacy-preservation learning.	Adel T, 2019, AAAI CONF ARTIF INTE, P2412; ALI SM, 1966, J ROY STAT SOC B, V28, P131; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Berk R, 2021, SOCIOL METHOD RES, V50, P3, DOI 10.1177/0049124118782533; Beutel Alex, 2017, ABS170700075 CORR; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Csiszar I., 1963, AKAD MAT KUTAT INT K, V8, P85; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Executive Office of the President, 2016, BIG DATA REPORT ALGO; Ganin Y., 2016, JMLR, V17, P2096; Hamm J., 2017, J MACHINE LEARNING R, V18, P4704; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Johndrow JE, 2019, ANN APPL STAT, V13, P189, DOI 10.1214/18-AOAS1201; Johnson M., 2017, T ASSOC COMPUT LING, DOI 10.1162/tacl_a_00065; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Khosravifard M, 2007, IEICE T FUND ELECTR, VE90A, P1848, DOI 10.1093/ietfec/e90-a.9.1848; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Liese F, 2006, IEEE T INFORM THEORY, V52, P4394, DOI 10.1109/TIT.2006.881731; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Louizos C., 2015, ARXIV PREPRINT ARXIV; Lum Kristian, 2016, ARXIV161008077; Madras D, 2018, PR MACH LEARN RES, V80; Narayanan A., 2018, PROC C FAIRNESS ACCO; Pleiss G, 2017, ADV NEURAL INFORM PR, V30, P5680; Song JM, 2019, PR MACH LEARN RES, V89; Zafar Muhammad Bilal, 2015, ARXIV150705259; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang BH, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P335, DOI 10.1145/3278721.3278779; Zhao H, INT C LEARN REPR; Zhao Han, 2019, ARXIV190607902; Zhao HY, 2020, INT J MACH LEARN CYB, V11, P1483, DOI 10.1007/s13042-019-01052-y; Zliobaite I., 2015, 2 WORKSH FAIRN ACC T	37	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907034
C	Finn, C; Xu, K; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Finn, Chelsea; Xu, Kelvin; Levine, Sergey			Probabilistic Model-Agnostic Meta-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Meta-learning for few-shot learning entails acquiring a prior over previous tasks and experiences, such that new tasks be learned from small amounts of data. However, a critical challenge in few-shot learning is task ambiguity: even when a powerful prior can be meta-learned from a large number of prior tasks, a small dataset for a new task can simply be too ambiguous to acquire a single model (e.g., a classifier) for that task that is accurate. In this paper, we propose a probabilistic meta-learning algorithm that can sample models for a new task from a model distribution. Our approach extends model-agnostic meta-learning, which adapts to new tasks via gradient descent, to incorporate a parameter distribution that is trained via a variational lower bound. At meta-test time, our algorithm adapts via a simple procedure that injects noise into gradient descent, and at meta-training time, the model is trained such that this stochastic adaptation procedure produces samples from the approximate model posterior. Our experimental results show that our method can sample plausible classifiers and regressors in ambiguous few-shot learning problems. We also show how reasoning about ambiguity can also be used for downstream active learning problems.	[Finn, Chelsea; Xu, Kelvin; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Finn, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	cbfinn@eecs.berkeley.edu; kelvinxu@eecs.berkeley.edu; svlevine@eecs.berkeley.edu			NSF Graduate Research Fellowship; NSF [IIS-1651843]; Office of Naval Research; NVIDIA	NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); NVIDIA	We thank Marvin Zhang and Dibya Ghosh for feedback on an early draft of this paper. This research was supported by an NSF Graduate Research Fellowship, NSF IIS-1651843, the Office of Naval Research, and NVIDIA.	Barber D., 1998, NEURAL INFORM PROCES; Blundell C., 2015, ARXIV PREPRINT ARXIV; Braun D. A., 2010, BEHAV BRAIN RES; Daume III H., 2009, C UNC ART INT UAI; Duan Y., 2016, RL2 FAST REINFORCEME; Edwards H., 2017, INT C LEARN REPR; Fei-Fei L., 2003, C COMP VIS PATT REC; Finn C., 2017, INT C MACH LEARN ICM; Finn C., 2017, ARXIV171011622; Finn Chelsea, 2017, C ROB LEARN PMLR, P357; Fortunato Meire, 2017, ARXIV170402798; Gao J., 2008, ACM SIGKDD C KNOWL D; Garcia Victor, 2017, ARXIV171104043; Grant E., 2018, INT C LEARN REPR ICL; Grant E., 2017, NIPS WORKSH COGN INF; Graves A., 2011, ADV NEURAL INFORM PR; Higgins Irina, 2017, ICLR 2017 INT C LEAR; Hinton Geoffrey E., 1993, C COMP LEARN THEOR; Hochreiter S., 2001, INT C ART NEUR NETW; Hoffman M. D., 2013, J MACHINE LEARNING R; Johnson M. J., 2016, NEURAL INFORM PROCES; Kingma DP, 2 INT C LEARN REPR I, P1; Lacoste  A., 2017, ARXIV171205016; Lake B. M., 2015, SCIENCE; Lawrence N.D., 2004, P 21 INT C MACH LEAR, P65, DOI DOI 10.1145/1015330.1015382; Li Zhenguo, 2017, METASGD LEARNING LEA; MacKay D., 1992, NEURAL COMPUTATION; Mishra Nikhil, 2018, P ICLR; Neal R. M., 2012, BAYESIAN LEARNING NE; Nichol Alex, 2018, ABS180302999 ARXIV; Ravi S., 2017, P INT C LEARN REPR, P1; Rezende D.J., 2014, PROC INT CONFER ENCE; Santoro Adam, 2016, INT C MACH LEARN ICM; Santos R. J., 1996, LINEAR ALGEBRA ITS A; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Shu R., 2018, ARXIV180508913; Snell J, 2017, NEURAL INFORM PROCES; Sung Flood, 2017, CORR; Tenenbaum J.B., 1999, THESIS; Vinyals Oriol, 2016, NEURAL INFORM PROCES; Wan J., 2012, C COMP VIS PATT REC; Wang Y.-X., 2016, EUR C COMP VIS ECCV; Yu K., 2005, INT C MACH LEARN ICM	43	14	14	2	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004011
C	George, T; Laurent, C; Bouthillier, X; Ballas, N; Vincent, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		George, Thomas; Laurent, Cesar; Bouthillier, Xavier; Ballas, Nicolas; Vincent, Pascal			Fast Approximate Natural Gradient Descent in a Kronecker-factored Eigenbasis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Optimization algorithms that leverage gradient covariance information, such as variants of natural gradient descent (Amari, 1998), offer the prospect of yielding more effective descent directions. For models with many parameters, the covariance matrix they are based on becomes gigantic, making them inapplicable in their original form. This has motivated research into both simple diagonal approximations and more sophisticated factored approximations such as KFAC (Heskes, 2000; Martens & Grosse, 2015; Grosse & Martens, 2016). In the present work we draw inspiration from both to propose a novel approximation that is provably better than KFAC and amendable to cheap partial updates. It consists in tracking a diagonal variance, not in parameter coordinates, but in a Kronecker-factored eigenbasis, in which the diagonal approximation is likely to be more effective. Experiments show improvements over KFAC in optimization speed for several deep network architectures.	[George, Thomas; Laurent, Cesar; Bouthillier, Xavier; Vincent, Pascal] Univ Montreal, Mila, Montreal, PQ, Canada; [Ballas, Nicolas; Vincent, Pascal] Facebook AI Res, Menlo Pk, CA USA; [Vincent, Pascal] CIFAR, Toronto, ON, Canada	Universite de Montreal; Facebook Inc; Canadian Institute for Advanced Research (CIFAR)	George, T (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.	thomas.george@umontreal.ca; cesar.laurent@umontreal.ca; xavier.bouthillier@umontreal.ca; ballasn@fb.com; pascal@fb.com			Facebook; CIFAR; Calcul Quebec; Compute Canada	Facebook(Facebook Inc); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Calcul Quebec; Compute Canada	The experiments were conducted using PyTorch (Paszke et al. (2017)). The authors would like to thank Facebook, CIFAR, Calcul Quebec and Compute Canada, for research funding and computational resources.	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba J., 2017, ICLR; Becker S., 1988, P 1988 CONN MOD SUMM, P29; Bottou L., 2016, OPTIMIZATION METHODS; Desjardins G., 2015, P 28 C NEUR PROC SYS, P2071; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fujimoto Y, 2018, LECT NOTES ARTIF INT, V10841, P47, DOI 10.1007/978-3-319-91253-0_5; Gehring J., 2017, ICLR; Goyal Priya, 2017, ARXIV170602677; Grosse R, 2016, PR MACH LEARN RES, V48; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Heskes T, 2000, NEURAL COMPUT, V12, P881, DOI 10.1162/089976600300015637; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Kingma D.P, P 3 INT C LEARNING R; Laurent Cesar, 2018, ICLR WORKSH; Le Roux Nicolas, 2008, NIPS; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Martens James, 2015, ICML; Ollivier Yann, 2015, INFORM INFERENCE J I; Paszke A., 2017, AUTOMATIC DIFFERENTI; Schraudolph Nicol N, 2001, INT C ART NEUR NETW; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Zeiler Matthew D, 2012, ARXIV12125701	26	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004014
C	Jain, S; Huth, AG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jain, Shailee; Huth, Alexander G.			Incorporating Context into Language Encoding Models for fMRI	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Language encoding models help explain language processing in the human brain by learning functions that predict brain responses from the language stimuli that elicited them. Current word embedding-based approaches treat each stimulus word independently and thus ignore the influence of context on language understanding. In this work, we instead build encoding models using rich contextual representations derived from an LSTM language model. Our models show a significant improvement in encoding performance relative to state-of-the-art embeddings in nearly every brain area. By varying the amount of context used in the models and providing the models with distorted context, we show that this improvement is due to a combination of better word embeddings learned by the LSTM language model and contextual information. We are also able to use our models to map context sensitivity across the cortex. These results suggest that LSTM language models learn high-level representations that are related to representations in the human brain.	[Jain, Shailee; Huth, Alexander G.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78751 USA; [Huth, Alexander G.] Univ Texas Austin, Dept Neurosci, Austin, TX 78751 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Jain, S (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78751 USA.	shailee@cs.utexas.edu; huth@cs.utexas.edu			NSF [IIS-1208203]; NIH NEI [EY019684-01A1]; Burroughs-Wellcome Fund; NVIDIA	NSF(National Science Foundation (NSF)); NIH NEI(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); Burroughs-Wellcome Fund(Burroughs Wellcome Fund); NVIDIA	We thank Jack Gallant, Wendy de Heer, Frederic Theunissen, and Thomas Griffiths for helping design the fMRI experiment and collecting the data used here; Brittany Griffin and Anwar Nuflez for segmenting and flattening cortical surfaces; and Niko Kriegeskorte for useful discussions. Data collection was supported by NSF grant IIS-1208203 and NIH NEI grant EY019684-01A1. This work was supported by grants from the Burroughs-Wellcome Fund and NVIDIA. We also acknowledge the Texas Advanced Computing Center (TACC) at The University of Texas at Austin for providing HPC resources that have contributed to the research results reported within this paper.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Agrawal Pulkit, 2014, ABS14075104 CORR; Baldassano C, 2017, NEURON, V95, P709, DOI 10.1016/j.neuron.2017.06.041; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; de Heer WA, 2017, J NEUROSCI, V37, P6539, DOI 10.1523/JNEUROSCI.3267-16.2017; Eickenberg M, 2017, NEUROIMAGE, V152, P184, DOI 10.1016/j.neuroimage.2016.10.001; Gao JS, 2015, FRONT NEUROINFORM, V9, DOI 10.3389/fninf.2015.00023; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Huth AG, 2016, NATURE, V532, P453, DOI 10.1038/nature17637; Kay KN, 2008, NATURE, V452, P352, DOI 10.1038/nature06713; Kell AJE, 2018, NEURON, V98, P630, DOI 10.1016/j.neuron.2018.03.044; Lerner Y, 2011, J NEUROSCI, V31, P2906, DOI 10.1523/JNEUROSCI.3684-10.2011; McCann B., 2017, ABS170800107 CORR ABS170800107 CORR; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mitchell TM, 2008, SCIENCE, V320, P1191, DOI 10.1126/science.1152876; Naselaris T, 2011, NEUROIMAGE, V56, P400, DOI 10.1016/j.neuroimage.2010.07.073; Pereira F, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03068-4; Peters M. E., 2018, P NAACL; Peters ME, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1756, DOI 10.18653/v1/P17-1161; Ruder  Sebastian, 2016, ABS160904747 CORR; Sundermeyer M., 2012, 13 ANN C INT SPEECH; Wehbe L., 2014, EMNLP; Wehbe L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0112575; Wu MCK, 2006, ANNU REV NEUROSCI, V29, P477, DOI 10.1146/annurev.neuro.29.051605.113024; Xu Haoyan, 2016, P 2016 C EMP METH NA, P2017	25	14	15	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001019
C	Kaliszyk, C; Urban, J; Michalewski, H; Olsak, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kaliszyk, Cezary; Urban, Josef; Michalewski, Henryk; Olsak, Mirek			Reinforcement Learning of Theorem Proving	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAME; GO	We introduce a theorem proving algorithm that uses practically no domain heuristics for guiding its connection-style proof search. Instead, it runs many Monte-Carlo simulations guided by reinforcement learning from previous proof attempts. We produce several versions of the prover, parameterized by different learning and guiding algorithms. The strongest version of the system is trained on a large corpus of mathematical problems and evaluated on previously unseen problems. The trained system solves within the same number of inferences over 40% more problems than a baseline prover, which is an unusually high improvement in this hard AI domain. To our knowledge this is the first time reinforcement learning has been convincingly applied to solving general mathematical problems on a large scale.	[Kaliszyk, Cezary] Univ Innsbruck, Innsbruck, Austria; [Urban, Josef] Czech Tech Univ, Prague, Czech Republic; [Michalewski, Henryk] Univ Warsaw, Inst Math, Polish Acad Sci, Warsaw, Poland; [Olsak, Mirek] Charles Univ Prague, Prague, Czech Republic	University of Innsbruck; Czech Technical University Prague; Polish Academy of Sciences; Institute of Mathematics of the Polish Academy of Sciences; University of Warsaw; Charles University Prague	Kaliszyk, C (corresponding author), Univ Innsbruck, Innsbruck, Austria.		Kaliszyk, Cezary/H-8791-2016	Kaliszyk, Cezary/0000-0002-8273-6059	ERC [714034 SMART]; AI4REASON ERC Consolidator grant [649043]; Czech project AIReasoning [CZ.02.1.01/0.0/0.0/15 003/0000466]; European Regional Development Fund; Academic Computer Center Cyfronet of the AGH University of Science and Technology in Krakow	ERC(European Research Council (ERC)European Commission); AI4REASON ERC Consolidator grant; Czech project AIReasoning; European Regional Development Fund(European Commission); Academic Computer Center Cyfronet of the AGH University of Science and Technology in Krakow	Kaliszyk was supported by ERC grant no. 714034 SMART. Urban was supported by the AI4REASON ERC Consolidator grant number 649043, and by the Czech project AI&Reasoning CZ.02.1.01/0.0/0.0/15 003/0000466 and the European Regional Development Fund. Michalewski and Kaliszyk acknowledge support of the Academic Computer Center Cyfronet of the AGH University of Science and Technology in Krakow and their Prometheus supercomputer.	Alama J, 2014, J AUTOM REASONING, V52, P191, DOI 10.1007/s10817-013-9286-5; Alemi AA, 2016, ADV NEUR IN, V29; [Anonymous], 2001, HDB AUTOMATED REASON; Anthony T., 2017, ADV NEURAL INFORM PR, P5360; Bachmair L., 1994, Journal of Logic and Computation, V4, P217, DOI 10.1093/logcom/4.3.217; Blanchette JC, 2016, J AUTOM REASONING, V57, P219, DOI 10.1007/s10817-016-9362-8; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Farber M, 2017, LECT NOTES ARTIF INT, V10395, P563, DOI 10.1007/978-3-319-63046-5_34; Gauthier T., 2017, EPIC SERIES COMPUTIN, V46, P125; Gauthier T, 2015, CPP'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON CERTIFIED PROGRAMS AND PROOFS, P49, DOI 10.1145/2676724.2693173; Grabowski A, 2010, J FORMALIZ REASON, V3, P153; Gransden T, 2015, LECT NOTES ARTIF INT, V9195, P246, DOI 10.1007/978-3-319-21401-6_16; Jakubuv Jan, 2017, Intelligent Computer Mathematics. 10th International Conference, CICM 2017. Proceedings: LNAI 10383, P292, DOI 10.1007/978-3-319-62075-6_20; Jakubuv J, 2018, AI COMMUN, V31, P237, DOI 10.3233/AIC-180761; Kaliszyk C., 2013, EPIC SER, V14, P87, DOI [10.29007/5gzr, DOI 10.29007/5GZR]; Kaliszyk C., 2014, EPIC SERIES COMPUTIN, V31, P60; Kaliszyk C, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3084; Kaliszyk C, 2015, LECT NOTES COMPUT SC, V9450, P88, DOI 10.1007/978-3-662-48899-7_7; Kaliszyk C, 2015, CPP'15: PROCEEDINGS OF THE 2015 ACM CONFERENCE ON CERTIFIED PROGRAMS AND PROOFS, P59, DOI 10.1145/2676724.2693176; Kaliszyk C, 2015, J AUTOM REASONING, V55, P245, DOI 10.1007/s10817-015-9330-8; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Kovacs Laura, 2013, Computer Aided Verification. 25th International Conference, CAV 2013. Proceedings. LNCS 8044, P1, DOI 10.1007/978-3-642-39799-8_1; Kuhlwein Daniel, 2012, Automated Reasoning. Proceedings 6th International Joint Conference, IJCAR 2012, P378, DOI 10.1007/978-3-642-31365-3_30; Kuhlwein D, 2015, J AUTOM REASONING, V55, P91, DOI 10.1007/s10817-015-9329-1; LETZ R, 1994, J AUTOM REASONING, V13, P297, DOI 10.1007/BF00881947; Loos S. M., 2017, 21 INT C LOGIC PROGR, P85; McCune W., 1990, LNCS, V449, P663, DOI DOI 10.1007/3-540-52885-7_131; Meng J, 2008, J AUTOM REASONING, V40, P35, DOI 10.1007/s10817-007-9085-y; Otten J, 2003, J SYMB COMPUT, V36, P139, DOI 10.1016/S0747-7171(03)00037-3; Otten J, 2010, AI COMMUN, V23, P159, DOI 10.3233/AIC-2010-0464; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Piotrowski B, 2018, LECT NOTES ARTIF INT, V10900, P566, DOI 10.1007/978-3-319-94205-6_37; ROBINSON JA, 1965, J ACM, V12, P23, DOI 10.1145/321250.321253; Schafer S., 2015, EPIC SERIES COMPUTER, P263; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver David, 2017, CORR; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Urban J., 2015, EPIC SERIES COMPUTER, V36, P312; Urban J, 2006, J AUTOM REASONING, V37, P21, DOI 10.1007/s10817-006-9032-3; Urban J, 2011, LECT NOTES ARTIF INT, V6793, P263, DOI 10.1007/978-3-642-22119-4_21; Whalen D., 2016, CORR	48	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31						8836	8847						12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003038
C	Kallus, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kallus, Nathan			Balanced Policy Evaluation and Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BILEVEL OPTIMIZATION; PROPENSITY SCORE	We present a new approach to the problems of evaluating and learning personalized decision policies from observational data of past contexts, decisions, and outcomes. Only the outcome of the enacted decision is available and the historical policy is unknown. These problems arise in personalized medicine using electronic health records and in internet advertising. Existing approaches use inverse propensity weighting (or, doubly robust versions) to make historical outcome (or, residual) data look like it were generated by a new policy being evaluated or learned. But this relies on a plug-in approach that rejects data points with a decision that disagrees with the new policy, leading to high variance estimates and ineffective learning. We propose a new, balance-based approach that too makes the data look like the new policy but does so directly by finding weights that optimize for balance between the weighted data and the target policy in the given, finite sample, which is equivalent to minimizing worst-case or posterior conditional mean square error. Our policy learner proceeds as a two-level optimization problem over policies and weights. We demonstrate that this approach markedly outperforms existing ones both in evaluation and learning, which is unsurprising given the wider support of balance-based weights. We establish extensive theoretical consistency guarantees and regret bounds that support this empirical success.	[Kallus, Nathan] Cornell Univ, Ithaca, NY 14853 USA; [Kallus, Nathan] Cornell Tech, New York, NY 10044 USA	Cornell University	Kallus, N (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.; Kallus, N (corresponding author), Cornell Tech, New York, NY 10044 USA.	kallus@cornell.edu			National Science Foundation [1656996]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1656996.	Athey S., 2017, ARXIV170202896; Austin PC, 2015, STAT MED, V34, P3661, DOI 10.1002/sim.6607; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bennett KP, 2008, LECT NOTES COMPUT SC, V5050, P25; Bertsimas D., 2016, ARXIV160502347; Bertsimas D, 2017, DIABETES CARE, V40, P210, DOI 10.2337/dc16-0826; Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Boyd S, 2004, CONVEX OPTIMIZATION; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chernozhukov V, 2016, DOUBLE DEBIASED MACH; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; Dudik Miroslav, 2011, ICML; Elliott MR, 2008, J OFF STAT, V24, P517; Fletcher R., 2013, PRACTICAL METHODS OP; Gretton A., 2006, NIPS, P513; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Imbens GW, 2000, BIOMETRIKA, V87, P706, DOI 10.1093/biomet/87.3.706; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456; Kallus, 2016, ARXIV PREPRINT ARXIV; Kallus  N., 2018, CONFOUNDING ROBUST P; Kallus N, 2018, P 21 INT C ART INT S, P1243; Kallus N, 2017, PR MACH LEARN RES, V70; Kallus N, 2018, J R STAT SOC B, V80, P85, DOI 10.1111/rssb.12240; Kang JDY, 2007, STAT SCI, V22, P523, DOI 10.1214/07-STS227; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Lunceford JK, 2004, STAT MED, V23, P2937, DOI 10.1002/SIM.1903; Ochs P, 2016, J MATH IMAGING VIS, V56, P175, DOI 10.1007/s10851-016-0663-7; Qian M, 2011, ANN STAT, V39, P1180, DOI 10.1214/10-AOS864; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robins J., P AM STAT ASS 1999, P6; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Royden HL, 1988, REAL ANAL; Sabach S, 2017, SIAM J OPTIMIZ, V27, P640, DOI 10.1137/16M105592X; Scholkopf B., 2001, LEARNING KERNELS SUP; Sriperumbudur Bharath K, 2010, ARXIV10030887; Strehl A, 2010, ADV NEURAL INFORM PR, V2, P2217; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; Swaminathan A, 2015, ADV NEUR IN, V28; Zhou X, 2017, J AM STAT ASSOC, V112, P169, DOI 10.1080/01621459.2015.1093947	44	14	14	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003045
C	Ma, FC; Ayaz, U; Karaman, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ma, Fangchang; Ayaz, Ulas; Karaman, Sertac			Invertibility of Convolutional Generative Networks from Partial Measurements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The problem of inverting generative neural networks (i.e., to recover the input latent code given partial network output), motivated by image inpainting, has recently been studied by a prior work that focused on fully-connected networks. In this work, we present new theoretical results on convolutional networks, which are more widely used in practice. The network inversion problem is highly non-convex, and hence is typically computationally intractable and without optimality guarantees. However, we rigorously prove that, for a 2-layer convolutional generative network with ReLU and Gaussian-distributed random weights, the input latent code can be deduced from the network output efficiently using simple gradient descent. This new theoretical finding implies that the mapping from the low-dimensional latent space to the high-dimensional image space is one-to-one, under our assumptions. In addition, the same conclusion holds even when the network output is only partially observed (i.e., with missing pixels). We further demonstrate, empirically, that the same conclusion extends to networks with multiple layers, other activation functions (leaky ReLU, sigmoid and tanh), and weights trained on real datasets.	[Ma, Fangchang; Ayaz, Ulas; Karaman, Sertac] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Ma, FC (corresponding author), MIT, Cambridge, MA 02139 USA.	fcma@mit.edu; uayaz@mit.edu; sertac@mit.edu						Arora Sanjeev, ICLR WORKSH; Bora A, 2017, PR MACH LEARN RES, V70; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Donoho DL, 2006, COMMUN PUR APPL MATH, V59, P797, DOI 10.1002/cpa.20132; Gilbert AC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1703; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hand P., 2017, ARXIV170507576; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma F, 2017, ARXIV170907492; Ma F., 2017, ARXIV170301398; Ma Fangchang, 2018, NIPS; Mirza M., 2014, ARXIV; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; SOLTANOLKOTABI M., 2017, ARXIV170504591; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Yeh RA, 2017, PROC CVPR IEEE, P6882, DOI 10.1109/CVPR.2017.728; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	22	14	14	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004021
C	Shalev, G; Adi, Y; Keshet, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shalev, Gabi; Adi, Yossi; Keshet, Joseph			Out-of-Distribution Detection using Multiple Semantic Label Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep Neural Networks are powerful models that attained remarkable results on a variety of tasks. These models are shown to be extremely efficient when training and test data are drawn from the same distribution. However, it is not clear how a network will act when it is fed with an out-of-distribution example. In this work, we consider the problem of out-of-distribution detection in neural networks. We propose to use multiple semantic dense representations instead of sparse representation as the target label. Specifically, we propose to use several word representations obtained from different corpora or architectures as target labels. We evaluated the proposed model on computer vision, and speech commands detection tasks and compared it to previous methods. Results suggest that our method compares favorably with previous work. Besides, we present the efficiency of our approach for detecting wrongly classified and adversarial examples.	[Shalev, Gabi; Adi, Yossi; Keshet, Joseph] Bar Ilan Univ, Ramat Gan, Israel	Bar Ilan University	Shalev, G (corresponding author), Bar Ilan Univ, Ramat Gan, Israel.	shalev.gabi@gmail.com; yossiadidrum@gmail.com; jkeshet@cs.biu.ac.il		Adi, Yossi/0000-0003-2237-3898				Amodei D., 2016, CONCRETE PROBLEMS AI; Amodei D, 2016, PR MACH LEARN RES, V48; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andrews Jerone, 2016, JMLR; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Chelba Ciprian, 2013, ARXIV13123005; Cisse M, 2017, ADV NEUR IN, V30; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Frome Andrea, 2013, NEURIPS; Fuchs T, 2017, IEEE J-STSP, V11, P1310, DOI 10.1109/JSTSP.2017.2764268; Gal Y, 2016, PR MACH LEARN RES, V48; Goodfellow IJ, 2014, 3 INT C LEARNING REP; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hinton G., 2015, ARXIV150302531; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kingma D.P, P 3 INT C LEARNING R; KREUK F, 2018, ARXIV180103339; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; Lapidoth Amos., 2017, FDN DIGITAL COMMUNIC; Leacock C, 1998, LANG SPEECH & COMMUN, P265; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee K., 2017, ARXIV171109325; Lee Stefan, 2015, ARXIV151106314, P2; Liang Shiyu, ENHANCING RELIABILIT; Littwin E, 2016, PROC CVPR IEEE, P3957, DOI 10.1109/CVPR.2016.429; Mackay D.J.C., 1992, THESIS CALIFORNIA I; Mikolov T, 2018, PROCEEDINGS OF THE ELEVENTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION (LREC 2018), P52; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Naaman Einat, 2017, P INT; Neal RM, 1996, LECT NOTES STAT, V118; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Parker Robert, 2011, GOOGL SCHOL; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Taigman Y, 2015, PROC CVPR IEEE, P2746, DOI 10.1109/CVPR.2015.7298891; Theis L., 2015, ICLR; WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7; Zhang Chiyuan, 2016, ARXIV161103530; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31	44	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001089
C	Tripuraneni, N; Stern, M; Jin, C; Regier, J; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tripuraneni, Nilesh; Stern, Mitchell; Jin, Chi; Regier, Jeffrey; Jordan, Michael I.			Stochastic Cubic Regularization for Fast Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper proposes a stochastic variant of a classic algorithm-the cubic-regularized Newton method [Nesterov and Polyak, 2006]. The proposed algorithm efficiently escapes saddle points and finds approximate local minima for general smooth, nonconvex functions in only (O) over tilde (c(-3.5)) stochastic gradient and stochastic Hessian-vector product evaluations. The latter can be computed as efficiently as stochastic gradients. This improves upon the (O) over tilde(epsilon(-4)) rate of stochastic gradient descent. Our rate matches the best-known result for finding local minima without requiring any delicate acceleration or variance-reduction techniques.	[Tripuraneni, Nilesh; Stern, Mitchell; Jin, Chi; Regier, Jeffrey; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Tripuraneni, N (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	nilesh_tripuraneni@berkeley.edu; mitchell@berkeley.edu; chijin@berkeley.edu; regier@berkeley.edu; jordan@cs.berkeley.edu	Jordan, Michael I/C-5253-2013	Jordan, Michael/0000-0001-8935-817X; Regier, Jeffrey/0000-0002-1472-5235				Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Agarwal Naman, 2017, P 49 ANN ACM SIGACT; ALLEN- ZHU Z., 2017, ARXIV170808694; Allen-Zhu Zeyuan, 2017, ARXIV171106673; CARMON Y, 2016, ARXIV161100756; CARMON Y, 2016, ARXIV161200547; Cartis C, 2011, MATH PROGRAM, V130, P295, DOI 10.1007/s10107-009-0337-y; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Curtis FE, 2017, MATH PROGRAM, V162, P1, DOI 10.1007/s10107-016-1026-2; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ge Rong, 2015, P 28 C LEARN THEOR; Ge Rong, 2017, P 34 INT C MACH LEAR; Jain Prateek, 2016, C LEARN THEOR, P1147; Jin Chi, 2017, P 34 INT C MACH LEAR; Kohler JM, 2017, PR MACH LEARN RES, V70; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; REDDI SJ, 2017, ARXIV170901434; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Royer Clement W, 2017, ARXIV170603131; Sun Ju, 2016, IEEE T INFORM THEORY; Sun Ju, 2016, IEEE INT S INF THEOR; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Xu Peng, 2017, ARXIV170807164; Xu Yi, 2017, ARXIV171101944; Zhang, 2013, ADV NEURAL INFORM PR, P315	27	14	14	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302088
C	Zhang, YL; Rohe, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Yilin; Rohe, Karl			Understanding Regularized Spectral Clustering via Graph Conductance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper uses the relationship between graph conductance and spectral clustering to study (i) the failures of spectral clustering and (ii) the benefits of regularization. The explanation is simple. Sparse and stochastic graphs create several "dangling sets", or small trees that are connected to the core of the graph by only one edge. Graph conductance is sensitive to these noisy dangling sets and spectral clustering inherits this sensitivity. The second part of the paper starts from a previously proposed form of regularized spectral clustering and shows that it is related to the graph conductance on a "regularized graph". When graph conductance is computed on the regularized graph, we call it CoreCut. Based upon previous arguments that relate graph conductance to spectral clustering (e.g. Cheeger inequality), minimizing CoreCut relaxes to regularized spectral clustering. Simple inspection of CoreCut reveals why it is less sensitive to dangling sets. Together, these results show that unbalanced partitions from spectral clustering can be understood as overfitting to noise in the periphery of a sparse and stochastic graph. Regularization fixes this overfitting. In addition to this statistical benefit, these results also demonstrate how regularization can improve the computational speed of spectral clustering. We provide simulations and data examples to illustrate these results.	[Zhang, Yilin; Rohe, Karl] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Zhang, YL (corresponding author), Univ Wisconsin, Dept Stat, Madison, WI 53706 USA.	yilin.zhang@wisc.edu; karl.rohe@wisc.edu			NSF [DMS-1612456]; ARO [W911NF-15-1-0423]	NSF(National Science Foundation (NSF)); ARO	The authors gratefully acknowledge support from NSF grant DMS-1612456 and ARO grant W911NF-15-1-0423. We thank Yeganeh Ali Mohammadi and Mobin YahyazadehJeloudar for their helpful comments.	Amini AA, 2013, ANN STAT, V41, P2097, DOI 10.1214/13-AOS1138; Binkiewicz N, 2017, BIOMETRIKA, V104, P361, DOI 10.1093/biomet/asx008; Borgatti SP, 1999, SOC NETWORKS, V21, P375; Bruna J., 2014, INT C LEARNING REPRE; Chaudhuri Kamalika, 2012, J MACHINE LEARNING R, P35; Chung Fan RK, 1996, COMBINATORICS P ERDO, V2, P13; Chung FR, 1997, SPECTRAL GRAPH THEOR, V92; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Joseph A, 2016, ANN STAT, V44, P1765, DOI 10.1214/16-AOS1447; Kipf Thomas N, 2016, 5 INT C LEARN REPR I; Le Can M, 2015, ARXIV150203049; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; Lehoucq R.B., 1998, ARPACK USERS GUIDE S, V6; Leskovec J, 2014, SNAP DATASETS STANFO; Leskovec J, 2009, INTERNET MATH, V6, P29, DOI 10.1080/15427951.2009.10129177; Levie R., 2017, ARXIV170507664; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Qiu Y., 2016, **DATA OBJECT**; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Zhang Yilin, 2017, ARXIV170806872; Zhang YN, 2018, NEW MEDIA SOC, V20, P3161, DOI 10.1177/1461444817744390	22	14	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005023
C	Zhao, A; Ding, MY; Guan, JC; Lu, ZW; Xiang, T; Wen, JR		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhao, An; Ding, Mingyu; Guan, Jiechao; Lu, Zhiwu; Xiang, Tao; Wen, Ji-Rong			Domain-Invariant Projection Learning for Zero-Shot Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAUSSIAN MIXTURE; CLASSIFICATION; ALGORITHM	Zero-shot learning (ZSL) aims to recognize unseen object classes without any training samples, which can be regarded as a form of transfer learning from seen classes to unseen ones. This is made possible by learning a projection between a feature space and a semantic space (e.g. attribute space). Key to ZSL is thus to learn a projection function that is robust against the often large domain gap between the seen and unseen classes. In this paper, we propose a novel ZSL model termed domain-invariant projection learning (DIPL). Our model has two novel components: (1) A domain-invariant feature self-reconstruction task is introduced to the seen/unseen class data, resulting in a simple linear formulation that casts ZSL into a min-min optimization problem. Solving the problem is non-trivial, and a novel iterative algorithm is formulated as the solver, with rigorous theoretic algorithm analysis provided. (2) To further align the two domains via the learned projection, shared semantic structure among seen and unseen classes is explored via forming superclasses in the semantic space. Extensive experiments show that our model outperforms the state-of-the-art alternatives by significant margins.	[Zhao, An; Ding, Mingyu; Guan, Jiechao; Lu, Zhiwu; Wen, Ji-Rong] Renmin Univ China, Sch Informat, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China; [Xiang, Tao] Queen Mary Univ London, Sch EECS, London E1 4NS, England; [Xiang, Tao] Samsung AI Ctr, Cambridge, England	Renmin University of China; University of London; Queen Mary University London	Lu, ZW (corresponding author), Renmin Univ China, Sch Informat, Beijing Key Lab Big Data Management & Anal Method, Beijing 100872, Peoples R China.	zhiwu.lu@gmail.com; t.xiang@qmul.ac.uk			National Natural Science Foundation of China [61573363]; Fundamental Research Funds for the Central Universities; Renmin University of China [15XNLQ01]; European Research Council FP7 Project SUNNY [313243]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Renmin University of China; European Research Council FP7 Project SUNNY	This work was partially supported by National Natural Science Foundation of China (61573363), the Fundamental Research Funds for the Central Universities and the Research Funds of Renmin University of China (15XNLQ01), and European Research Council FP7 Project SUNNY (313243).	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, CVPR; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Baldi P., 2012, ICML UNSUPERVISED TR, V27, P37; BARTELS RH, 1972, COMMUN ACM, V15, P820, DOI 10.1145/361573.361582; Bucher M, 2017, IEEE INT CONF COMP V, P2666, DOI 10.1109/ICCVW.2017.308; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Dean J, 2014, C WORKSH NEUR INF PR; Donahue J, 2014, PR MACH LEARN RES, V32; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Frome Andrea, 2013, NEURIPS; Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; FU ZY, 2015, PROC CVPR IEEE, P2635, DOI DOI 10.1109/CVPR.2015.7298879; Hwang Sung Ju, 2014, ADV NEURAL INFORM PR, P271; Jiang B, 2017, PROC CVPR IEEE, P550, DOI 10.1109/CVPR.2017.66; Jiang HJ, 2017, IEEE I CONF COMP VIS, P4233, DOI 10.1109/ICCV.2017.453; Kankuekul P, 2012, PROC CVPR IEEE, P3657, DOI 10.1109/CVPR.2012.6248112; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li AX, 2017, IEEE T GEOSCI REMOTE, V55, P4157, DOI 10.1109/TGRS.2017.2689071; Li X, 2015, IEEE I CONF COMP VIS, P4211, DOI 10.1109/ICCV.2015.479; Li Y., 2017, CVPR, P3279; Long T, 2018, PATTERN RECOGN LETT, V109, P27, DOI 10.1016/j.patrec.2017.09.030; Lu XG, 2013, INTERSPEECH, P436; LU Y, 2015, ARXIV150600990; Lu Z., 2013, P 27 AAAI C ART INT, P640; Lu ZW, 2006, NEUROCOMPUTING, V69, P1674, DOI 10.1016/j.neucom.2006.01.001; Lu ZW, 2014, AAAI CONF ARTIF INTE, P1258; Lu ZW, 2010, PATTERN RECOGN LETT, V31, P36, DOI 10.1016/j.patrec.2009.09.003; Lu ZW, 2009, IEEE T SYST MAN CY B, V39, P901, DOI 10.1109/TSMCB.2008.2012119; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mishra A., 2017, ARXIV170900663; Patterson G, 2014, INT J COMPUT VISION, V108, P59, DOI 10.1007/s11263-013-0695-z; Radovanovic M, 2010, J MACH LEARN RES, V11, P2487; Rahman S., 2017, ARXIV170608653; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sarath C.A.P., 2014, ADV NEURAL INFORM PR, V3, P1853; Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256; Shigeto Y, 2015, LECT NOTES ARTIF INT, V9284, P135, DOI 10.1007/978-3-319-23528-8_9; Shojaee S. M., 2016, ARXIV160509016; Silva TC, 2012, IEEE T NEUR NET LEAR, V23, P385, DOI 10.1109/TNNLS.2011.2181866; Socher Richard, 2013, NEURIPS; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wah C., 2011, CALTECHUCSD BIRDS 20; Wang HZ, 2018, INT GEOL REV, V60, P1684, DOI 10.1080/00206814.2018.1428829; Wang Q, 2017, INT J COMPUT VISION, V124, P356, DOI 10.1007/s11263-017-1027-5; Wang WL, 2018, AAAI CONF ARTIF INTE, P4211; Xian Y, 2017, P IEEE C COMP VIS PA, P4582; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xu X, 2017, INT J COMPUT VISION, V123, P309, DOI 10.1007/s11263-016-0983-5; Xu X, 2015, IEEE IMAGE PROC, P63, DOI 10.1109/ICIP.2015.7350760; Ye M., 2017, CVPR, P7140; YU Y, 2017, ARXIV170308893; Zhang L., 2017, P IEEE C COMP VIS PA, P2021; Zhang X, 2013, ARXIV PREPRINT ARXIV; Zhang ZM, 2016, LECT NOTES COMPUT SC, V9911, P533, DOI 10.1007/978-3-319-46478-7_33; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; ZHANG ZM, 2016, PROC CVPR IEEE, P6034, DOI DOI 10.1109/CVPR.2016.649	69	14	15	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301005
C	Zieba, M; Semberecki, P; El-Gaaly, T; Trzcinski, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zieba, Maciej; Semberecki, Piotr; El-Gaaly, Tarek; Trzcinski, Tomasz			BinGAN: Learning Compact Binary Descriptors with a Regularized GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we propose a novel regularization method for Generative Adversarial Networks, which allows the model to learn discriminative yet compact binary representations of image patches (image descriptors). We employ the dimensionality reduction that takes place in the intermediate layers of the discriminator network and train binarized low-dimensional representation of the penultimate layer to mimic the distribution of the higher-dimensional preceding layers. To achieve this, we introduce two loss terms that aim at: (i) reducing the correlation between the dimensions of the binarized low-dimensional representation of the penultimate layer (i.e. maximizing joint entropy) and (ii) propagating the relations between the dimensions in the high-dimensional space to the low-dimensional space. We evaluate the resulting binary image descriptors on two challenging applications, image matching and retrieval, and achieve state-of-the-art results.	[Zieba, Maciej; Semberecki, Piotr; Trzcinski, Tomasz] Wroclaw Univ Sci & Technol, Tooploox, Wroclaw, Poland; [El-Gaaly, Tarek] Voyage, Santa Clara, CA USA	Wroclaw University of Science & Technology	Zieba, M (corresponding author), Wroclaw Univ Sci & Technol, Tooploox, Wroclaw, Poland.	maciej.zieba@pwr.edu.pl; piotr.semberecki@pwr.edu.pl; tarek@voyage.auto; t.trzcinski@ii.pw.edu.pl	Szymczak, Piotr/A-9268-2008	Szymczak, Piotr/0000-0001-8940-7891; Zieba, Maciej/0000-0003-4217-7712	Polish National Science Centre [UMO-2016/21/D/ST6/01946]; Google Sponsor Research Agreement under the project "Efficient visual localization on mobile devices"; Ministry of Science and Higher Education, Republic of Poland	Polish National Science Centre; Google Sponsor Research Agreement under the project "Efficient visual localization on mobile devices"; Ministry of Science and Higher Education, Republic of Poland(Ministry of Science and Higher Education, Poland)	This research was partially supported by the Polish National Science Centre grant no. UMO-2016/21/D/ST6/01946 as well as Google Sponsor Research Agreement under the project "Efficient visual localization on mobile devices".r The research conducted by Maciej Zieba has been partially co-financed by the Ministry of Science and Higher Education, Republic of Poland.	Alahi Alexandre, 2012, CVPR, V2; Andoni A., 2006, FOCS; [Anonymous], 2005, THESIS MIT CAMBRIDGE; [Anonymous], 2013, COMPUT SCI; Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54; Calonder M., 2010, ECCV; Cao Y., 2018, ICLR; Duan Y., 2017, CVPR; Fan B, 2014, IEEE T IMAGE PROCESS, V23, P2583, DOI 10.1109/TIP.2014.2317981; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2013, CVPR; Heo J.-P., 2012, CVPR; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Leutenegger S., 2011, ICCV; Lin K., 2016, CVPR; Liong V. E., 2015, CVPR; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Qiu Zhaofan, 2017, SIGIR; Radford A., 2016, ICLR; Salakhutdinov R, 2009, INT J APPROX REASON, V50, P969, DOI 10.1016/j.ijar.2008.11.006; Salimans Tim, 2016, ADV NEURAL INFORM PR; Simo-Serra E., 2015, ICCV; Song J., 2018, AAAI; Strecha C, 2012, IEEE T PATTERN ANAL, V34, P66, DOI 10.1109/TPAMI.2011.103; Tian Yurun, 2017, CVPR; Trzcinski T., 2013, CVPR; Trzcinski T., 2012, ECCV; Wang J., 2010, CVPR; Weiss Y., 2009, NIPS	30	14	14	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303059
C	Alaa, AM; van der Schaar, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alaa, Ahmed M.; van der Schaar, Mihaela			Deep Multi-task Gaussian Processes for Survival Analysis with Competing Risks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REGRESSION; MODELS	Designing optimal treatment plans for patients with comorbidities requires accurate cause-specific mortality prognosis. Motivated by the recent availability of linked electronic health records, we develop a nonparametric Bayesian model for survival analysis with competing risks, which can be used for jointly assessing a patient's risk of multiple (competing) adverse outcomes. The model views a patient's survival times with respect to the competing risks as the outputs of a deep multi-task Gaussian process (DMGP), the inputs to which are the patients' covariates. Unlike parametric survival analysis methods based on Cox and Weibull models, our model uses DMGPs to capture complex non-linear interactions between the patients' covariates and cause-specific survival times, thereby learning flexible patient-specific and cause-specific survival curves, all in a data-driven fashion without explicit parametric assumptions on the hazard rates. We propose a variational inference algorithm that is capable of learning the model parameters from time-to-event data while handling right censoring. Experiments on synthetic and real data show that our model outperforms the state-of-the-art survival models.	[Alaa, Ahmed M.] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Univ Oxford, Dept Engn Sci, Oxford, England	University of California System; University of California Los Angeles; University of Oxford	Alaa, AM (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.	ahmedmalaa@ucla.edu; mihaela.vanderschaar@eng.ox.ac.uk	Jeong, Yongwook/N-7413-2016	van der schaar, Mihaela/0000-0003-3933-6049				Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; [Anonymous], 2016, J MACH LEARN RES; Austin PC, 2016, CIRCULATION, V133, P601, DOI 10.1161/CIRCULATIONAHA.115.017719; Barrett J. E., 2013, ARXIV13121591; Bonilla E.V., 2007, ADV NEURAL INF PROCE, V20, P601; Collins FS, 2015, NEW ENGL J MED, V372, P793, DOI 10.1056/NEJMp1500523; COX DR, 1972, J R STAT SOC B, V34, P187; Crowder M., 2001, CLASSICAL COMPETING; De Iorio M, 2009, BIOMETRICS, V65, P762, DOI 10.1111/j.1541-0420.2008.01166.x; Fernndez T., 2016, NIPS; Fine JP, 1999, J AM STAT ASSOC, V94, P496, DOI 10.2307/2670170; Gooley TA, 1999, STAT MED, V18, P695, DOI 10.1002/(SICI)1097-0258(19990330)18:6<695::AID-SIM60>3.3.CO;2-F; Henry J, 2016, ONC DATA BRIEF; Ishwaran H, 2008, ANN APPL STAT, V2, P841, DOI 10.1214/08-AOAS169; KAPLAN EL, 1958, J AM STAT ASSOC, V53, P457, DOI 10.2307/2281868; Kingma D.P, P 3 INT C LEARNING R; Koene R., 2016, CIRCULATION; Lambert PC, 2010, STAT MED, V29, P885, DOI 10.1002/sim.3762; Lawrence N., 2013, AISTATS; Lee MLT, 2006, STAT SCI, V21, P501, DOI 10.1214/088342306000000330; Lim HJ, 2010, BMC MED RES METHODOL, V10, DOI 10.1186/1471-2288-10-97; Martino S, 2011, SCAND J STAT, V38, P514, DOI 10.1111/j.1467-9469.2010.00715.x; Satagopan JM, 2004, BRIT J CANCER, V91, P1229, DOI 10.1038/sj.bjc.6602102; Snelson E., 2004, NIPS; Steck H., 2008, ADV NEURAL INFORM PR, P1209; Titsias Michalis K., 2010, AISTATS; TSIATIS A, 1975, P NATL ACAD SCI USA, V72, P20, DOI 10.1073/pnas.72.1.20; Wolbers M, 2014, BIOSTATISTICS, V15, P526, DOI 10.1093/biostatistics/kxt059; Yu C.N., 2011, ADV NEURAL INF PROCE, V24, P1845	29	14	14	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402037
C	Costa, RP; Assael, YM; Shillingford, B; de Freitas, N; Vogels, TP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Costa, Rui Ponte; Assael, Yannis M.; Shillingford, Brendan; de Freitas, Nando; Vogels, Tim P.			Cortical microcircuits as gated-recurrent neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SHUNTING INHIBITION; SYNAPTIC PLASTICITY; WORKING-MEMORY; EXCITATION; CORTEX; MODEL; HETEROGENEITY; CONNECTIVITY; DYNAMICS; BALANCE	Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits. Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.	[Costa, Rui Ponte; Vogels, Tim P.] Univ Oxford, Dept Physiol Anat & Genet, Ctr Neural Circuits & Behav, Oxford, England; [Assael, Yannis M.; Shillingford, Brendan] Univ Oxford, Dept Comp Sci, Oxford, England; [Assael, Yannis M.; Shillingford, Brendan; de Freitas, Nando] DeepMind, London, England	University of Oxford; University of Oxford	Costa, RP (corresponding author), Univ Oxford, Dept Physiol Anat & Genet, Ctr Neural Circuits & Behav, Oxford, England.	rui.costa@cncb.ox.ac.uk; yannis.assael@cs.ox.ac.uk; brendan.shillingford@cs.ox.ac.uk; nandodefreitas@google.com; tim.vogels@cncb.ox.ac.uk	Jeong, Yongwook/N-7413-2016; Assael, Yannis/E-8160-2013	Assael, Yannis/0000-0001-7408-3847; Costa, Rui Ponte/0000-0003-2595-2027	Wellcome Trust; Royal Society [WT 100000]; EPSRC; Research Council UK (RCUK); Clarendon Fund	Wellcome Trust(Wellcome Trust); Royal Society(Royal Society of London); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Research Council UK (RCUK)(UK Research & Innovation (UKRI)); Clarendon Fund	We would like to thank Everton Agnes, Caglar Gulcehre, Gabor Melis and Jake Stroud for helpful comments and discussion. R.P.C. and T.P.V. were supported by a Sir Henry Dale Fellowship by the Wellcome Trust and the Royal Society (WT 100000). Y.M.A. was supported by the EPSRC and the Research Council UK (RCUK). B.S. was supported by the Clarendon Fund.	Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453; Assael Yannis M., 2016, ARXIV161101599; Bartho P, 2009, EUR J NEUROSCI, V30, P1767, DOI 10.1111/j.1460-9568.2009.06954.x; Bastos AM, 2012, NEURON, V76, P695, DOI 10.1016/j.neuron.2012.10.038; Bhalla U. S., 2017, HIPPOCAMPUS; Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005; Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027; Chung J., 2014, ARXIV14123555; Costa RP, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00075; Costa RP, 2017, NEURON, V96, P177, DOI 10.1016/j.neuron.2017.09.021; Costa RP, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0153; Costa RP, 2015, ELIFE, V4, DOI 10.7554/eLife.09457; Cox DD, 2014, CURR BIOL, V24, pR921, DOI 10.1016/j.cub.2014.08.026; Deneve S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243; Doiron B, 2001, NEURAL COMPUT, V13, P227, DOI 10.1162/089976601300014691; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; Douglas RJ, 1989, NEURAL COMPUT, V1, P480, DOI 10.1162/neco.1989.1.4.480; Egorov AV, 2002, NATURE, V420, P173, DOI 10.1038/nature01171; El-Boustani S, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms6689; Froemke R. C., 2007, NATURE; Froemke RC, 2015, ANNU REV NEUROSCI, V38, P195, DOI 10.1146/annurev-neuro-071714-034002; Gerstner W., 2002, SINGLE NEURONS POPUL; Gerstner W., 2014, SINGLE NEURONS NETWO; Glorot X., 2010, PROC MACH LEARN RES, P249; GOLDMANRAKIC PS, 1995, NEURON, V14, P477, DOI 10.1016/0896-6273(95)90304-6; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; Graves A., 2013, 2013 IEEE INT C AC S; Graves A., 2013, RECURRENT NEURAL NET; Greff Klaus, 2015, LSTM SEARCH SPACE OD; Harris KD, 2013, NATURE, V503, P51, DOI 10.1038/nature12654; Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011; Hennequin G, 2017, ANNU REV NEUROSCI, V40, P557, DOI 10.1146/annurev-neuro-072116-031005; Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Holt GR, 1997, NEURAL COMPUT, V9, P1001, DOI 10.1162/neco.1997.9.5.1001; Huang Y, 2016, ELIFE, V5, DOI 10.7554/eLife.15441; Jiang XL, 2015, SCIENCE, V350, DOI 10.1126/science.aac9462; Kandel ER, 2000, PRINCIPLES NEURAL SC; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Kornblith S., 2017, CURRENT BIOL; Kremkow J, 2010, J NEUROSCI, V30, P15760, DOI 10.1523/JNEUROSCI.3874-10.2010; Krueger KA, 2009, COGNITION, V110, P380, DOI 10.1016/j.cognition.2008.11.014; Kuchibhotla KV, 2017, NAT NEUROSCI, V20, P62, DOI 10.1038/nn.4436; Le Q.V., 2015, ABS150400941 CORR; Letzkus J. J., 2015, NEURON; Luczak A., 2015, NATURE REV NEUROSCIE; Marblestone AH, 2016, FRONT COMPUT NEUROSC, V10, DOI 10.3389/fncom.2016.00094; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Markram H, 2004, NAT REV NEUROSCI, V5, P793, DOI 10.1038/nrn1519; Mejias JF, 2013, AIP CONF PROC, V1510, P185, DOI 10.1063/1.4776513; O'Reilly RC, 2006, NEURAL COMPUT, V18, P283, DOI 10.1162/089976606775093909; Paken JMP, 2016, ELIFE, V5, DOI 10.7554/eLife.14985; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pfister JP, 2006, J NEUROSCI, V26, P9673, DOI 10.1523/JNEUROSCI.1425-06.2006; Poort J, 2015, NEURON, V86, P1478, DOI 10.1016/j.neuron.2015.05.037; Prescott SA, 2003, P NATL ACAD SCI USA, V100, P2076, DOI 10.1073/pnas.0337591100; Sakata S, 2009, NEURON, V64, P404, DOI 10.1016/j.neuron.2009.09.020; Senn W, 2001, NEURAL COMPUT, V13, P35, DOI 10.1162/089976601300014628; Seybold BA, 2015, NEURON, V87, P1181, DOI 10.1016/j.neuron.2015.09.013; Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068; Strata P, 1999, BRAIN RES BULL, V50, P349, DOI 10.1016/S0361-9230(99)00100-8; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Thomson AM, 2002, CEREB CORTEX, V12, P936, DOI 10.1093/cercor/12.9.936; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; van den Oord Aaron, 2016, ARXIV160605328; van Kerkoerle T, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13804; vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724; Vogels TP, 2009, NAT NEUROSCI, V12, P483, DOI 10.1038/nn.2276; Wang Y, 2006, NAT NEUROSCI, V9, P534, DOI 10.1038/nn1670; Xue MS, 2014, NATURE, V511, P596, DOI 10.1038/nature13321; York LC, 2009, J COMPUT NEUROSCI, V27, P607, DOI 10.1007/s10827-009-0172-4; Zenke F, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms7922	73	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400026
C	Orabona, F; Tommasi, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Orabona, Francesco; Tommasi, Tatiana			Training Deep Networks without Learning Rates Through Coin Betting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep learning methods achieve state-of-the-art performance in many application scenarios. Yet, these methods require a significant amount of hyperparameters tuning in order to achieve the best results. In particular, tuning the learning rates in the stochastic optimization process is still one of the main bottlenecks. In this paper, we propose a new stochastic gradient descent procedure for deep networks that does not require any learning rate setting. Contrary to previous methods, we do not adapt the learning rates nor we make use of the assumed curvature of the objective function. Instead, we reduce the optimization process to a game of betting on a coin and propose a learning-rate-free optimal algorithm for this scenario. Theoretical convergence is proven for convex and quasi-convex functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms.	[Orabona, Francesco] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Tommasi, Tatiana] Sapienza Rome Univ, Dept Comp Control & Management Engn, Rome, Italy	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Sapienza University Rome	Orabona, F (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	francesco@orabona.com; tommasi@dis.uniroma1.it	Jeong, Yongwook/N-7413-2016		National Science Foundation [1531492]; ERC [637076 - RoboExNovo]; Google Research Award	National Science Foundation(National Science Foundation (NSF)); ERC(European Research Council (ERC)European Commission); Google Research Award(Google Incorporated)	The authors thank the Stony Brook Research Computing and Cyberinfrastructure, and the Institute for Advanced Computational Science at Stony Brook University for access to the high-performance SeaWulf computing system, which was made possible by a $1.4M National Science Foundation grant (#1531492). The authors also thank Akshay Verma for the help with the TensorFlow implementation and Matej Kristan for reporting a bug in the pseudocode in the previous version of the paper. T.T. was supported by the ERC grant 637076 - RoboExNovo. F.O. is partly supported by a Google Research Award.	Abadi M, 2015, P 12 USENIX S OPERAT; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26; Cutkosky A., 2017, C LEARN THEOR COLT, P643; Cutkosky A, 2016, ADV NEUR IN, V29; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hardt M., 2016, ARXIV160905191; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; McMahan H. B., 2014, P 27 C LEARN THEOR, P1020; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Nocedal J., 2006, NUMERICAL OPTIMIZATI; Orabona F., 2013, ADV NEURAL INFORM PR, P1806; Orabona F., 2016, ADV NEURAL INFORM PR, V29, P577; Orabona F, 2014, ADV NEUR IN, V27; Orabona F, 2015, LECT NOTES ARTIF INT, V9355, P287, DOI 10.1007/978-3-319-24486-0_19; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Ross S., 2013, P 29 C UNC ART INT U; Schaul T., 2013, INT C MACHINE LEARNI, P343; Streeter M., 2012, ADV NEURAL INFORM PR; Streeter M., 2010, ARXIV10024862; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Zaremba Wojciech, 2014, ABS14092329 CORR; Zeiler Matthew D, 2012, ARXIV12125701; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332	36	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402021
C	Quadrianto, N; Sharmanska, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Quadrianto, Novi; Sharmanska, Viktoriia			Recycling Privileged Learning and Distribution Matching for Fairness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Equipping machine learning models with ethical and legal constraints is a serious issue; without this, the future of machine learning is at risk. This paper takes a step forward in this direction and focuses on ensuring machine learning models deliver fair decisions. In legal scholarships, the notion of fairness itself is evolving and multi-faceted. We set an overarching goal to develop a unified machine learning framework that is able to handle any definitions of fairness, their combinations, and also new definitions that might be stipulated in the future. To achieve our goal, we recycle two well-established machine learning techniques, privileged learning and distribution matching, and harmonize them for satisfying multi-faceted fairness definitions. We consider protected characteristics such as race and gender as privileged information that is available at training but not at test time; this accelerates model training and delivers fairness through unawareness. Further, we cast demographic parity, equalized odds, and equality of opportunity as a classical two-sample problem of conditional distributions, which can be solved in a general form by using distance measures in Hilbert Space. We show several existing models are special cases of ours. Finally, we advocate returning the Pareto frontier of multi-objective minimization of error and unfairness in predictions. This will facilitate decision makers to select an operating point and to be accountable for it.	[Quadrianto, Novi] Univ Sussex, PAL, Brighton, E Sussex, England; [Sharmanska, Viktoriia] Imperial Coll London, Dept Comp, London, England; [Quadrianto, Novi] Natl Res Univ, Higher Sch Econ, Moscow, Russia	University of Sussex; Imperial College London; HSE University (National Research University Higher School of Economics)	Quadrianto, N (corresponding author), Univ Sussex, PAL, Brighton, E Sussex, England.	n.quadrianto@sussex.ac.uk; sharmanska.v@gmail.com	Jeong, Yongwook/N-7413-2016	Quadrianto, Novi/0000-0001-8819-306X; Sharmanska, Viktoriia/0000-0003-0192-9308	UK EPSRC [EP/P03442X/1]; Russian Academic Excellence Project '5-100'; IC Research Fellowship	UK EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Russian Academic Excellence Project '5-100'(Ministry of Education and Science, Russian FederationProject 5-100, Ministry of Education and Science, Russian Federation); IC Research Fellowship	NQ is supported by the UK EPSRC project EP/P03442X/1 'EthicalML: Injecting Ethical and Legal Constraints into Machine Learning Models' and the Russian Academic Excellence Project '5-100'. VS is supported by the IC Research Fellowship. We thank NVIDIA for GPU donation and Amazon for AWS Cloud Credits. We thank Kristian Kersting and Oliver Thomas for discussions, Muhammad Bilal Zafar for his implementations of [4] and [5], and Sienna Quadrianto for supporting the work.	Adler Philip, 2016, ICDM; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Collette Y., 2003, MULTIOBJECTIVE OPTIM; Deb K., 2000, Parallel Problem Solving from Nature PPSN VI. 6th International Conference. Proceedings (Lecture Notes in Computer Science Vol.1917), P849; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Executive Office of the President, 2016, BIG DATA REPORT ALGO; Feyereisl J, 2014, ADV NEURAL INFORM PR, P208; Fortin FA, 2012, J MACH LEARN RES, V13, P2171; Goh G, 2016, ADV NEUR IN, V29; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hernandez-Lobato D, 2014, ADV NEUR IN, V27; Hinton G., 2015, STAT-US, V1050, P9; Joseph Matthew, 2016, NIPS, P325; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kleinberg Jon, 2016, P INN THEOR COMP SCI; Kusner Matt J., 2017, ADV NEURAL INFORM PR, V30; Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167; Lopez-Paz D., 2016, INT C LEARN REPR ICL; Louizos Christos, 2015, ABS151100830 CORR; Luong Binh Thanh, 2011, P 17 ACM SIGKDD INT, P502; Pan SJ, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1187; Pechyony D., 2010, ADV NEURAL INFORM PR, P1894; Quadrianto N., 2009, ADV NEURAL INFORM PR, P1500; Romei A, 2014, KNOWL ENG REV, V29, P582, DOI 10.1017/S0269888913000039; Ruggieri S, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754432; Sharmanska V, 2016, PROC CVPR IEEE, P2194, DOI 10.1109/CVPR.2016.241; Sharmanska V, 2016, PROC CVPR IEEE, P3967, DOI 10.1109/CVPR.2016.430; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Song Le, 2009, P 26 INT C MACHINE L, P961, DOI 10.1145/1553374.1553497; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; The Royal Society Working Group, 2017, TECHNICAL REPORT; Vapnik V, 2015, J MACH LEARN RES, V16, P2023; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang Xu, 2015, ABS150300591 CORR	44	14	14	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400065
C	Tian, YD; Gong, QC; Shang, WL; Wu, YX; Zitnick, CL		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tian, Yuandong; Gong, Qucheng; Shang, Wenling; Wu, Yuxin; Zitnick, C. Lawrence			ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frame-per-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs end-to-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE [4]. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AT more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https : //github.com/facebookresearch/ELF.	[Tian, Yuandong; Gong, Qucheng; Wu, Yuxin; Zitnick, C. Lawrence] Facebook AI Res, Menlo Pk, CA 94025 USA; [Shang, Wenling] Oculus, Menlo Pk, CA USA	Facebook Inc	Tian, YD (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	yuandong@fb.com; qucheng@fb.com; wendy.shang@oculus.com; yuxinwu@fb.com; zitnick@fb.com	Jeong, Yongwook/N-7413-2016					Babaeizadeh M, 2017, LEARNING REPRESENTAT; BattleCode, 2000, BATTL MITS AI PROGR; Beattie C., 2016, ARXIV161203801; Bellemare M. G, 2012, CORR; Bhonker Nadav, 2016, CORR; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Buro Michael, 2005, GAMEON NA 2005 C MON, P23; Chaplot D. S, 2016, ARXIV160905521; Chaslot GMJB, 2008, LECT NOTES COMPUT SC, V5131, P60, DOI 10.1007/978-3-540-87608-3_6; Coumans Erwin, 2010, BULLET PHYS ENGINE; Johnson M., 2016, P 25 INT JOINT C ART, P4246; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Mirowski P., 2017, PROC INT C LEARN REP, P1; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A., 2015, ARXIV150704296; Ontan S., 2013, P 9 AAAI C ARTIFICIA, P58; Peng Peng, 2017, CORR; Pumpkin Studios, 1999, WARZ 2100; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Synnaeve Gabriel, 2016, CORR; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tian Yuandong, 2015, ARXIV151106410; Usunier N, 2017, INT C LEARN REPR; Wu Yuxin, 2017, INT C LEARN REPR ICL; Wydmuch M., 2016, ARXIV160502097, P1, DOI DOI 10.1109/CIG.2016.7860433; Zaremba W., 2016, CORR; NON TRADITIONAL REF	31	14	14	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402069
C	Venkatraman, A; Rhinehart, N; Sun, W; Pinto, L; Hebert, M; Boots, B; Kitani, KM; Bagnell, JA		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Venkatraman, Arun; Rhinehart, Nicholas; Sun, Wen; Pinto, Lerrel; Hebert, Martial; Boots, Byron; Kitani, Kris M.; Bagnell, J. Andrew			Predictive-State Decoders: Encoding the Future into Recurrent Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recurrent neural networks (RNNs) are a vital modeling technique that rely on internal states learned indirectly by optimization of a supervised, unsupervised, or reinforcement training loss. RNNs are used to model dynamic processes that are characterized by underlying latent states whose form is often unknown, precluding its analytic representation inside an RNN. In the Predictive-State Representation (PSR) literature, latent state processes are modeled by an internal state representation that directly models the distribution of future observations, and most recent work in this area has relied on explicitly representing and targeting sufficient statistics of this probability distribution. We seek to combine the advantages of RNNs and PSRs by augmenting existing state-of-the-art recurrent neural networks with PREDICTIVE-STATE DECODERS (PSDs), which add supervision to the network's internal state representation to target predicting future observations. PSDs are simple to implement and easily incorporated into existing training pipelines via additional loss regularization. We demonstrate the effectiveness of PSDs with experimental results in three different domains: probabilistic filtering, Imitation Learning, and Reinforcement Learning. In each, our method improves statistical performance of state-of-the-art recurrent baselines and does so with fewer iterations and less data.	[Venkatraman, Arun; Rhinehart, Nicholas; Sun, Wen; Pinto, Lerrel; Hebert, Martial; Kitani, Kris M.; Bagnell, J. Andrew] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA; [Boots, Byron] Georgia Inst Technol, Sch Interact Comp, Atlanta, GA 30332 USA	Carnegie Mellon University; University System of Georgia; Georgia Institute of Technology	Venkatraman, A; Rhinehart, N (corresponding author), Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA.	arunvenk@cs.cmu.edu; nrhineha@cs.cmu.edu	Rhinehart, Nicholas/M-1311-2019; Jeong, Yongwook/N-7413-2016	Rhinehart, Nicholas/0000-0003-4242-1236; 	Office of Naval Research (ONR) [N000141512365]; National Science Foundation NRI [1637758]	Office of Naval Research (ONR)(Office of Naval Research); National Science Foundation NRI(National Science Foundation (NSF))	This material is based upon work supported in part by: Office of Naval Research (ONR) contract N000141512365, and National Science Foundation NRI award number 1637758.	Abadi M, 2015, P 12 USENIX S OPERAT; Abbeel P., 2005, NIPS, P1; Abbeel P., 2005, ROBOTICS SCI SYSTEMS, V2, P1; Abbeel P, 2005, ICML ACM INT C PROCE, V119, P1, DOI 10.1145/1102351.1102352; Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2014, ARXIV14061078; Boots B., 2012, THESIS; Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092; Boots Byron, 2013, UAI 2013; Bowden Roger J, 1990, INSTRUMENTAL VARIABL, V8; Brockman G., 2016, OPENAI GYM; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Coates Adam, 2008, P 25 INT C MACH LEAR, P144; D'Alche-Buc F., 2004, NIPS; Deisenroth M.P., 2009, ANN INT C MACH LEARN, P225, DOI 10.1145/1553374.1553403; Duan Y, 2016, PR MACH LEARN RES, V48; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Greff K., 2015, ARXIV150304069, DOI 10.1109/TNNLS.2016.2582924; Haarnoja T., 2016, NIPS; Hausknecht Matthew, 2015, 2015 AAAI FALL S SER; Hefny Ahmed, 2015, NIPS; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsu D. J., 2009, COLT; Kakade S, 2002, ADV NEUR IN, V14, P1531; Ko J., 2007, GP UKF UNSCENTED KAL, P1901; Kokkinos Iasonas, 2016, CORR; Langford J., 2009, P 26 ANN INT C MACH, P593; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Levine S, 2016, J MACH LEARN RES, V17; Ondruska P, 2016, AAAI CONF ARTIF INTE, P3361; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pinto L, 2017, PR MACH LEARN RES, V70; Ranzato M, 2016, ICLR; Ross S., 2011, CVPR; Ross St<prime>ephane, 2011, AISTATS; Roweis S. T., 1999, LEARNING NONLINEAR D, P431; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Singh S., 2006, INT C MACH LEARN ICM, P1017; Singh S., 2004, P 20 C UNCERTAINTY A, P512; Song L., 2010, P 27 INT C MACH LEAR, P991; strom K. J. A, 2010, FEEDBACK SYSTEMS; Sun W., 2017, ICML; Sun W., 2016, P 33 INT C MACH LEAR, P1197; Sun Wen, 2016, P INT C UNC ART INT; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754; van den Oord A, 2016, PR MACH LEARN RES, V48; Van Overschee P., 2012, SUBSPACE IDENTIFICAT, DOI [10.1007/978-1-4613-0465-4, DOI 10.1007/978-1-4613-0465-4]; Vega-Brown W, 2013, IEEE INT C INT ROBOT, P1907, DOI 10.1109/IROS.2013.6696609; Venkatraman A, 2015, AAAI CONF ARTIF INTE, P3024; Venkatraman Arun, 2016, 25 INT JOINT C ART I; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337	56	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401021
C	Bauer, M; van der Wilk, M; Rasmussen, CE		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bauer, Matthias; van der Wilk, Mark; Rasmussen, Carl Edward			Understanding Probabilistic Sparse Gaussian Process Approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Good sparse approximations are essential for practical inference in Gaussian Processes as the computational cost of exact methods is prohibitive for large datasets. The Fully Independent Training Conditional (FITC) and the Variational Free Energy (VFE) approximations are two recent popular methods. Despite superficial similarities, these approximations have surprisingly different theoretical properties and behave differently in practice. We thoroughly investigate the two methods for regression both analytically and through illustrative examples, and draw conclusions to guide practical application.	[Bauer, Matthias; van der Wilk, Mark; Rasmussen, Carl Edward] Univ Cambridge, Dept Engn, Cambridge, England; [Bauer, Matthias] Max Planck Inst Intelligent Syst, Tubingen, Germany	University of Cambridge; Max Planck Society	Bauer, M (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.; Bauer, M (corresponding author), Max Planck Inst Intelligent Syst, Tubingen, Germany.	msb55@cam.ac.uk; mv310@cam.ac.uk; cer54@cam.ac.uk						Bengio Y, 2009, P NIPS 08 P 21 INT C; Bui T. D., 2016, UNIFYING FRAMEWORK S; Csato Lehel, 2002, NEURAL COMPUTATION, V14; de Garis Matthews A. G., 2016, THESIS; Hensman J., 2013, C UNC ART INT; Hensman  J., 2015, P 18 INT C ART INT S; Lazaro-Aredilla  M., 2009, ADV NEURAL INFORM PR; Lazaro-Gredilla Miguel, 2010, J MACHINE LEARNING R, V11; Matthews Alexander G, 2016, P 19 INT C ART INT S; Qi Y., 2010, P 26 C UNC ART INT; Quinonero-~Candela J., 2005, J MACHINE LEARNING R, V6; Rasmussen Carl Edward, 2001, ADV NEURAL INFORM PR; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Seeger M., 2003, P INT WORKSH ART INT; Smola A. J., 2001, ADV NEURAL INFORM PR; Snelson  E., 2006, NEURAL INFORM PROCES, V18; Snelson E.L., 2007, FLEXIBLE EFFICIENT G; Titsias M. K., 2009, TECH REP; Titsias M.K., 2009, ARTIF INTELL; Turner R. E., 2011, BAYESIAN TIME SERIES; Yang  Z., 2015, ARTIFICIAL INTELLIGE	21	14	14	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704002
C	Guclu, U; Thielen, J; Hanke, M; van Gerven, MAJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Guclu, Umut; Thielen, Jordy; Hanke, Michael; van Gerven, Marcel A. J.			Brains on Beats	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INFORMATION; RESPONSES	We developed task-optimized deep neural networks (DNNs) that achieved state-of-the-art performance in different evaluation scenarios for automatic music tagging. These DNNs were subsequently used to probe the neural representations of music. Representational similarity analysis revealed the existence of a representational gradient across the superior temporal gyrus (STG). Anterior STG was shown to be more sensitive to low-level stimulus features encoded in shallow DNN layers whereas posterior STG was shown to be more sensitive to high-level stimulus features encoded in deep DNN layers.	[Guclu, Umut; Thielen, Jordy; van Gerven, Marcel A. J.] Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands; [Hanke, Michael] Otto von Guericke Univ, Ctr Behav Brain Sci, Magdeburg, Germany	Radboud University Nijmegen; Otto von Guericke University	Guclu, U (corresponding author), Radboud Univ Nijmegen, Donders Inst Brain Cognit & Behav, Nijmegen, Netherlands.	u.guclu@donders.ru.nl; j.thielen@psych.ru.nl; michael.hanke@ovgu.de; m.vangerven@donders.ru.nl	Hanke, Michael/F-1382-2013; Güçlü, Umut/AAX-6105-2020; Thielen, Jordy/Q-8599-2018; Thielen, Jordy/AAD-3685-2021; Hanke, Michael/A-7726-2013	Hanke, Michael/0000-0001-6398-6370; Güçlü, Umut/0000-0003-4753-159X; Thielen, Jordy/0000-0002-6264-0367; Hanke, Michael/0000-0001-6398-6370	German federal state of Saxony-Anhalt; European Regional Development Fund (ERDF), project: Center for Behavioral Brain Sciences; Netherlands Organization for Scientific Research (NWO) [639.072.513]	German federal state of Saxony-Anhalt; European Regional Development Fund (ERDF), project: Center for Behavioral Brain Sciences; Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	supported by the German federal state of Saxony-Anhalt and the European Regional Development Fund (ERDF), project: Center for Behavioral Brain Sciences.; supported by VIDI grant 639.072.513 of the Netherlands Organization for Scientific Research (NWO).	Agrawal P, 2014, ARXIV14075104; Alluri V, 2013, NEUROIMAGE, V83, P627, DOI 10.1016/j.neuroimage.2013.06.064; Alluri V, 2012, NEUROIMAGE, V59, P3677, DOI 10.1016/j.neuroimage.2011.11.019; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Casey M., 2011, MLINI; Chollet F., 2015, KERAS; Cichy R. M, 2016, ARXIV160102970; Cichy RM, 2016, NEUROIMAGE; Dieleman S., 2014, ICASSP; Dieleman S., 2013, ISMIR; Fuster J., 2003, CORTEX MIND UNIFYING; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; Guclu U., 2015, NEUROIMAGE; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; Hanke M., 2015, F1000RESEARCH; Horikawa Tomoyasu, 2015, ARXIV151006479; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kay KN, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00247; Kell A., 2016, COSYNE; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; King DB, 2015, ACS SYM SER, V1214, P1; Kriegeskorte N, 2006, P NATL ACAD SCI USA, V103, P3863, DOI 10.1073/pnas.0600244103; Kriegeskorte N., 2008, FRONTIERS SYSTEMS NE; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Law E, 2009, ISMIR; McFarland JM, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003143; Moerel M, 2015, SCI REP-UK, V5, DOI 10.1038/srep17048; Patterson RD, 2002, NEURON, V36, P767, DOI 10.1016/S0896-6273(02)01060-7; Santoro R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003412; Schwartz B.L., 2015, SENSATION PERCEPTION; Seibert D., 2016, BIORXIV; Staeren N, 2009, CURR BIOL, V19, P498, DOI 10.1016/j.cub.2009.01.066; Toiviainen P, 2014, NEUROIMAGE, V88, P170, DOI 10.1016/j.neuroimage.2013.11.017; van den Oord A, 2014, ISMIR; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	35	14	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701079
C	Beygelzimer, A; Hazan, E; Kale, S; Luo, HP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Beygelzimer, Alina; Hazan, Elad; Kale, Satyen; Luo, Haipeng			Online Gradient Boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHMS	We extend the theory of boosting for regression problems to the online learning setting. Generalizing from the batch setting for boosting, the notion of a weak learning algorithm is modeled as an online learning algorithm with linear loss functions that competes with a base class of regression functions, while a strong learning algorithm is an online learning algorithm with smooth convex loss functions that competes with a larger class of regression functions. Our main result is an online gradient boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the linear span of the base class. We also give a simpler boosting algorithm that converts a weak online learning algorithm into a strong one where the larger class of functions is the convex hull of the base class, and prove its optimality.	[Beygelzimer, Alina; Kale, Satyen] Yahoo Labs, New York, NY 10036 USA; [Hazan, Elad; Luo, Haipeng] Princeton Univ, Princeton, NJ 08540 USA	Princeton University	Beygelzimer, A (corresponding author), Yahoo Labs, New York, NY 10036 USA.	beygel@yahoo-inc.com; ehazan@cs.princeton.edu; satyen@yahoo-inc.com; haipengl@cs.princeton.edu		Hazan, Elad/0000-0002-1566-3216				Bartlett PL, 2007, J MACH LEARN RES, V8, P2347; Beygelzimer Alina, 2015, ICML; Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439; Chen S.-T., 2012, ICML; Chen Shang-Tse, 2014, ICML; Collins Michael, 2000, COLT; Duffy N, 2002, MACH LEARN, V47, P153, DOI 10.1023/A:1013685603443; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Grabner H, 2006, IEEE C COMP VIS PATT, P260; Grabner H, 2008, LECT NOTES COMPUT SC, V5302, P234, DOI 10.1007/978-3-540-88682-2_19; Hastie T, 2009, ELEMENTS STAT LEARNI; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; Liu X., 2007, P IEEE INT C COMP VI, P1; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; MASON L, 2000, NIPS; Russell S.J, 2001, ARTIFICIAL INTELLIGE, P229; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Telgarsky Matus, 2013, COLT; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255; Zinkevich M, 2003, ICML	24	14	14	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100093
C	Pinheiro, PO; Collobert, R; Dollar, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Pinheiro, Pedro O.; Collobert, Ronan; Dollar, Piotr			Learning to Segment Object Candidates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recent object detection systems rely on two critical steps: (1) a set of object proposals is predicted as efficiently as possible, and (2) this set of candidate proposals is then passed to an object classifier. Such approaches have been shown they can be fast, while achieving the state of the art in detection performance. In this paper, we propose a new way to generate object proposals, introducing an approach based on a discriminative convolutional network. Our model is trained jointly with two objectives: given an image patch, the first part of the system outputs a class-agnostic segmentation mask, while the second part of the system outputs the likelihood of the patch being centered on a full object. At test time, the model is efficiently applied on the whole test image and generates a set of segmentation masks, each of them being assigned with a corresponding object likelihood score. We show that our model yields significant improvements over state-of-the-art object proposal algorithms. In particular, compared to previous approaches, our model obtains substantially higher object recall using fewer proposals. We also show that our model is able to generalize to unseen categories it has not seen during training. Unlike all previous approaches for generating object masks, we do not rely on edges, superpixels, or any other form of low-level segmentation.	[Pinheiro, Pedro O.; Collobert, Ronan; Dollar, Piotr] Facebook AI Res, Menlo Pk, CA USA	Facebook Inc	Pinheiro, PO (corresponding author), Idiap Res Inst, Martigny, Switzerland.	pedro@opinheiro.com; locronan@fb.com; pdollar@fb.com						Alexe B., 2012, PAMI; Chavali N., 2015, ARXIV150505836; Chen LC., ARXIV 2015ABS1412706; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Eigen D., 2014, 6 INT C LEARN REPR I; Erhan D., 2014, CVPR; Everingham M., 2010, IJCV; Felzenszwalb P. F., 2010, PAMI; Girshick R., 2015, ARXIV150408083; Girshick R, 2014, PROC CVPR IEEE, P580, DOI 10.1109/CVPR.2014.81; Hariharan Bharath, 2015, CVPR; He K., 2014, ECCV; He Kaiming, 2014, ECCV; Hosang J., 2015, ARXIV150205082; Humayun A., 2014, CVPR; Krahenbuhl P., 2014, ECCV; Krahenbuhl P., 2015, CVPR; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kuo W., 2015, ARXIV50502146V1, V2; LeCun Y., 1998, P IEEE; Lin T.Y., 2014, P EUR C COMP VIS 201; Oquab M., 2015, CVPR; Pinheiro Pedro H. O., 2014, ICML; Pont-Tuset J., 2015, ARXIV150300848; Ren S., 2015, P NEURAL INFORM PROC, P91; Simonyan Karen, 2015, INT C LEARN REPR; Srivastava N., 2014, JMLR; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Szegedy C., 2014, ARXIV14121441; Uijlings J., 2013, IJCV; Viola P., 2004, IJCV; Zhu Z. Y., 2015, CVPR, V1; Zitnick C. L., 2014, ECCV	33	14	14	5	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102001
C	Borodin, A; El-Yaniv, R; Gogan, V		Thrun, S; Saul, K; Scholkopf, B		Borodin, A; El-Yaniv, R; Gogan, V			Can we learn to beat the best stock	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				INDIVIDUAL SEQUENCES; COMPRESSION; PORTFOLIOS	A novel algorithm for actively trading stocks is presented. While traditional universal algorithms (and technical trading heuristics) attempt to predict winners or trends, our approach relies on predictable statistical relations between all pairs of stocks in the market. Our empirical results on historical markets provide strong evidence that this type of technical trading can "beat the market" and moreover, can beat the best stock in the market. In doing so we utilize a new idea for smoothing critical parameters in the context of expert learning.	Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Borodin, A (corresponding author), Univ Toronto, Dept Comp Sci, 100 Coll St, Toronto, ON, Canada.							Blum A, 1998, MACH LEARN, V30, P23, DOI 10.1023/A:1007402410823; Borodin A., 1998, ONLINE COMPUTATION C; BROCK W, 1992, J FINANC, V47, P1731, DOI 10.2307/2328994; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; CHOU A, 1995, P 6 ANN ACM SIAM S D; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Cover TM, 1996, IEEE T INFORM THEORY, V42, P348, DOI 10.1109/18.485708; COVER TM, 1986, ADV APPL MATH, V7, P170, DOI 10.1016/0196-8858(86)90029-1; COVER TM, 1991, ELEMENT INFORMATION; FEDER M, 1991, IEEE T INFORM THEORY, V37, P1459, DOI 10.1109/18.133269; Helmbold DP, 1998, MATH FINANC, V8, P325, DOI 10.1111/1467-9965.00058; LANGDON GG, 1983, IEEE T INFORM THEORY, V29, P284, DOI 10.1109/TIT.1983.1056645; Lo Andrew W., 1999, NONRANDOM WALK WALL; LUGOSI G, 2001, LECT PREDICTION INDI; Markowitz H, 1959, PORTFOLIO SELECTION; Raghavan P., 1992, DIMACS SERIES DISCRE, V7, P79; RISSANEN J, 1983, IEEE T INFORM THEORY, V29, P656, DOI 10.1109/TIT.1983.1056741; ZIV J, 1978, IEEE T INFORM THEORY, V24, P530, DOI 10.1109/TIT.1978.1055934	18	14	14	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						345	352						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500044
C	Ihler, AT; Sudderth, EB; Freeman, WT; Willsky, AS		Thrun, S; Saul, K; Scholkopf, B		Ihler, AT; Sudderth, EB; Freeman, WT; Willsky, AS			Efficient multiscale sampling from products of Gaussian mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The problem of approximating the product of several Gaussian mixture distributions arises in a number of contexts, including the nonparametric belief propagation (NBP) inference algorithm and the training of product of experts models. This paper develops two multiscale algorithms for sampling from a product of Gaussian mixtures, and compares their performance to existing methods. The first is a multiscale variant of previously proposed Monte Carlo techniques, with comparable theoretical guarantees but improved empirical convergence rates. The second makes use of approximate kernel density evaluation methods to construct a fast approximate sampler, which is guaranteed to sample points to within a tunable parameter c of their true probability. We compare both multiscale samplers on a set of computational examples motivated by NBP, demonstrating significant improvements over existing methods.	MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Ihler, AT (corresponding author), MIT, Dept Elect Engn & Comp Sci, Cambridge, MA 02139 USA.	ihler@mit.edu; esuddert@mit.edu; billf@ai.mit.edu; willsky@mit.edu		Ihler, Alexander/0000-0002-4331-1015; Sudderth, Erik/0000-0002-0595-9726				DENG K, 1995, IJCAI; Doucet A., 2001, SEQUENTIAL MONTE CAR; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRAY AG, 2003, JSM; HINTON G, 2000, 2000004 GATSB COMP N; Isard M., 2003, CVPR; Liu JS, 2000, BIOMETRIKA, V87, P353, DOI 10.1093/biomet/87.2.353; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; STRAIN J, 1991, SIAM J SCI STAT COMP, V12, P1131, DOI 10.1137/0912059; SUDDERTH EB, 2003, NONPARAMETRIC BELIEF; THRUN S, 1999, ICML, P415	11	14	17	0	5	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1	8						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500001
C	Kearns, M; Ortiz, LE		Thrun, S; Saul, K; Scholkopf, B		Kearns, M; Ortiz, LE			Algorithms for interdependent security games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA						Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA	University of Pennsylvania	Kearns, M (corresponding author), Univ Penn, Dept Comp & Informat Sci, 200 S 33Rd St, Philadelphia, PA 19104 USA.							[Anonymous], 2001, UAI 01; Garey M.R., 1979, COMPUTERS INTRACTABI; Heal G., 2003, YOU ONLY DIE ONCE MA; KEARNS M, 2002, P C UNC ART INT; KUNREUTHER H, 2003, IN PRESS J RISK UNCE; Schelling T., 1978, MICROMOTIVES MACROBE	6	14	14	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						561	568						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500071
C	Rosenberg, C; Minka, T; Ladsariya, A		Thrun, S; Saul, K; Scholkopf, B		Rosenberg, C; Minka, T; Ladsariya, A			Bayesian color constancy with non-Gaussian models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We present a Bayesian approach to color constancy which utilizes a non-Gaussian probabilistic model of the image formation process. The parameters of this model are estimated directly from an uncalibrated image set and a small number of additional algorithmic parameters are chosen using cross validation. The algorithm is empirically shown to exhibit RMS error lower than other color constancy algorithms based on the Lambertian surface reflectance model when estimating the illuminants of a set of test images. This is demonstrated via a direct performance comparison utilizing a publicly available set of real world test images and code base.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Rosenberg, C (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.							Barnard K, 2002, IEEE T IMAGE PROCESS, V11, P985, DOI 10.1109/TIP.2002.802529; Barnard K, 2002, COLOR RES APPL, V27, P147, DOI 10.1002/col.10049; BARNARD K, 2000, P 6 EUR C COMP VIS, P275; Brainard DH, 1997, J OPT SOC AM A, V14, P1393, DOI 10.1364/JOSAA.14.001393; BUCHSBAUM G, 1980, J FRANKLIN I, V10, P1; Finlayson G. D., 1999, P 7 IEEE INT C COMP, V2, P835; FUNT B, 1996, P IS T SID 4 COL IM, P58; FUNT B, 1999, P SPIE ELECT IMAGING, V4, P3644; FUNT B, 1998, P 5 EUR C COMP VIS, P445; TRUSSELL HJ, 1991, P INT C AC SPEECH SI, P2513	10	14	13	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1595	1602						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500198
C	Theocharous, G; Kaelbling, LP		Thrun, S; Saul, K; Scholkopf, B		Theocharous, G; Kaelbling, LP			Approximate planning in POMDPs with macro-actions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MARKOV DECISION-PROCESSES	Recent research has demonstrated that useful POMDP solutions do not require consideration of the entire belief space. We extend this idea with the notion of temporal abstraction. We present and explore a new reinforcement learning algorithm over grid-points in belief space, which uses macro-actions and Monte Carlo updates of the Q-values. We apply the algorithm to a large scale robot navigation task and demonstrate that with temporal abstraction we can consider an even smaller part of the belief space, we can learn POMDP policies faster, and we can do information gathering more efficiently.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Theocharous, G (corresponding author), MIT, AI Lab, 200 Technol Sq, Cambridge, MA 02139 USA.							Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678; LOVEJOY WS, 1991, OPER RES, V39, P162, DOI 10.1287/opre.39.1.162; MADANI O, 1999, P 16 NAT C ART INT, P409; MAHADEVAN S, 1998, MACHINE LEARNING J, V31, P239; Ng A. Y., 1999, P 16 INT C MACH LEAR; PAPADIMITRIOU C, 1987, MATH OPERATION RES, V12; Pineau J., 2003, INT JOINT C ART INT; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; ROY N, 2003, ADV NEURAL INFORMATI; Russell SJ, 1995, ARTIF INTELL, V4th; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; ZHOU R, 2001, P 1M INT C ART INT I	12	14	14	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						775	782						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500097
C	Vovk, V; Shafer, G; Nouretdinov, I		Thrun, S; Saul, K; Scholkopf, B		Vovk, V; Shafer, G; Nouretdinov, I			Self-calibrating probability forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					In the problem of probability forecasting the learner's goal is to output, given a training set and a new object, a suitable probability measure on the possible values of the new object's label. An on-line algorithm for probability forecasting is said to be well-calibrated if the probabilities it outputs agree with the observed frequencies. We give a natural non-asymptotic formalization of the notion of well-calibratedness, which we then study under the assumption of randomness (the object/label pairs are independent and identically distributed). It turns out that, although no probability forecasting algorithm is automatically well-calibrated in our sense, there exists a wide class of algorithms for "multiprobability forecasting" (such algorithms are allowed to output a set, ideally very narrow, of probability measures) which satisfy this property; we call the algorithms in this class "Venn probability machines". Our experimental results demonstrate that a 1-Nearest Neighbor Venn probability machine performs reasonably well on a standard benchmark data set, and one of our theoretical results asserts that a simple Venn probability machine asymptotically approaches the true conditional probabilities regardless, and without knowledge, of the true probability measure generating the examples.	Univ London, Royal Holloway & Bedford New Coll, Dept Comp Sci, Comp Learning Res Ctr, Egham TW20 0EX, Surrey, England	University of London; Royal Holloway University London	Vovk, V (corresponding author), Univ London, Royal Holloway & Bedford New Coll, Dept Comp Sci, Comp Learning Res Ctr, Egham TW20 0EX, Surrey, England.	vovk@cs.rhul.ac.uk; gshafer@andromeda.rutgers.edu; ilia@cs.rhul.ac.uk		Vovk, Vladimir/0000-0003-2602-6877				Dawid AP., 1986, ENCY STAT SCI, P210, DOI DOI 10.1002/0471667196.ESS2064.PUB2; KILINC BE, 2001, PROBABILITY THEORY P, P97; LITTLESTONE N, 1986, RELATING DATA COMPRE; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; Saunders C, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P722; Shafer G., 2001, PROBABILITY FINANCE; Vovk V, 2002, ANN IEEE SYMP FOUND, P187, DOI 10.1109/SFCS.2002.1181895; Vovk V, 2003, LECT NOTES ARTIF INT, V2777, P358, DOI 10.1007/978-3-540-45167-9_27; Vovk V, 1999, MACHINE LEARNING, PROCEEDINGS, P444; Vovk V., 2003, P 20 INT C MACH LEAR, P768; VOVK V, IN PRESS ALGORITHMIC	14	14	14	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1133	1140						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500141
C	Cristianini, N; Shawe-Taylor, J; Kandola, J		Dietterich, TG; Becker, S; Ghahramani, Z		Cristianini, N; Shawe-Taylor, J; Kandola, J			Spectral kernel methods for clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In this paper we introduce new algorithms for unsupervised learning based on the use of a kernel matrix. All the information required by such algorithms is contained in the eigenvectors of the matrix or of closely related matrices. We use two different but related cost functions, the Alignment and the 'cut cost'. The first one is discussed in a companion paper [3], the second one is based on graph theoretic concepts. Both functions measure the level of clustering of a labeled dataset, or the correlation between data clusters and labels. We state the problem of unsupervised learning as assigning labels so as to optimize these cost functions. We show how the optimal solution can be approximated by slightly relaxing the corresponding optimization problem, and how this corresponds to using eigenvector information. The resulting simple algorithms are tested on real world data with positive results.						Shawe-Taylor, John/0000-0002-2030-0073				BERRY MW, 1996, MATEMATICS NUMERICAL, P99; CRISTIANINI N, 2001, UNPUB P NEUR INF PRO; CRISTIANINI N, 2000, NCTR00080 COLT WORK; Cristianini N., 2000, INTRO SUPPORT VECTOR; POTHEN A, 1990, SIAM J MATRIX ANAL A, V11, P430, DOI 10.1137/0611030	5	14	18	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						649	655						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100081
C	Domeniconi, C; Gunopulos, D		Dietterich, TG; Becker, S; Ghahramani, Z		Domeniconi, C; Gunopulos, D			Adaptive nearest neighbor classification using support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				REGRESSION	The nearest neighbor technique is a simple and appealing method to address classification problems. It relies on the assumption of locally constant class conditional probabilities. This assumption becomes invalid in high dimensions with a finite number of examples due to the curse of dimensionality. We propose a technique that computes a locally flexible metric by means of Support Vector Machines (SVMs). The maximum margin boundary found by the SVM is used to determine the most discriminant direction over the query's neighborhood. Such direction provides a local weighting scheme for input features. We present experimental evidence of classification performance improvement over the SVM algorithm alone and over a variety of adaptive learning schemes, by using both simulated and real data sets.	Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA	University of California System; University of California Riverside	Domeniconi, C (corresponding author), Univ Calif Riverside, Dept Comp Sci, Riverside, CA 92521 USA.			Gunopulos, Dimitrios/0000-0001-6339-1879				Amari S, 1999, NEURAL NETWORKS, V12, P783, DOI 10.1016/S0893-6080(99)00032-5; BELLMAN BE, 1961, ADAPTIVE CONTROL PRO; BROWN M, 1999, KNOWLEDGE BASED ANAL; CLEVELAND WS, 1988, J AM STAT ASSOC, V83, P596, DOI 10.2307/2289282; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; DOMENICONI C, 2000, ADV NEURAL INFORMATI; DOMENICONI C, 2001, UCRCSE0104 DEP COMP; Duda R.O., 1973, J ROYAL STAT SOC SER; Friedman J. H., 1994, FLEXIBLE METRIC NEAR; Hastie T, 1996, IEEE T PATTERN ANAL, V18, P607, DOI 10.1109/34.506411; Joachims T., 1999, ADV KERNEL METHODS S; Joachims T., 1998, LNCS, DOI [10.1007/bfb0026683, 10.1007/BFb0026683, DOI 10.1007/BFB0026683]; LOWE DG, 1995, NEURAL COMPUT, V7, P72, DOI 10.1162/neco.1995.7.1.72; OSUNA E, 1997, P COMP VIS PATT REC; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309	16	14	15	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						665	672						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100083
C	Sykacek, P; Roberts, S		Dietterich, TG; Becker, S; Ghahramani, Z		Sykacek, P; Roberts, S			Bayesian time series classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					This paper proposes an approach to classification of adjacent segments of a time series as being either of K classes. We use a hierarchical model that consists of a feature extraction stage and a generative classifier which is built on top of these features. Such two stage approaches are often used in signal and image processing. The novel part of our work is that we link these stages probabilistically by using a latent feature space. To use one joint model is a Bayesian requirement, which has the advantage to fuse information according to its certainty. The classifier is implemented as hidden Markov model with Gaussian and Multinomial observation distributions defined on a suitably chosen representation of autoregressive models. The Markov dependency is motivated by the assumption that successive classifications will be correlated. Inference is done with Markov chain Monte Carlo (MCMC) techniques. We apply the proposed approach to synthetic data and to classification of EEG that was recorded while the subjects performed different cognitive tasks. All experiments show that using a latent feature space results in a significant improvement in generalization accuracy. Hence we expect that this idea generalizes well to other hierarchical models.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Sykacek, P (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.	psyk@robots.ox.ac.uk; sjrob@robots.ox.ac.uk		Sykacek, Peter/0000-0001-8800-8354				Bernardo J. M., 1994, BAYESIAN THEORY; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Neal RM., 1996, BAYESIAN LEARNING NE, P29; ORUANAIDH JJK, 1995, NUMERICAL BAYESIAN M; Raftery A. E., 1996, MARKOV CHAIN MONTE C, P115, DOI DOI 10.1007/978-1-4899-4485-6.1269; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; Robert C.P., 1996, MARKOV CHAIN MONTE C, P441; Schuster-Bockler Benjamin, 2007, Curr Protoc Bioinformatics, VAppendix 3, p3A, DOI [10.1109/MASSP.1986.1165342, 10.1002/0471250953.bia03as18]; Sykacek P, 2000, ADV NEUR IN, V12, P638	11	14	14	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						937	944						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100117
C	Zibulevsky, M; Kisilev, P; Zeevi, YY; Pearlmutter, B		Dietterich, TG; Becker, S; Ghahramani, Z		Zibulevsky, M; Kisilev, P; Zeevi, YY; Pearlmutter, B			Blind source separation via multinode sparse representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We consider a problem of blind source separation from a set of instantaneous linear mixtures, where the mixing matrix is unknown. It was discovered recently, that exploiting the sparsity of sources in an appropriate representation according to some signal dictionary, dramatically improves the quality of separation. In this work we use the property of multiscale transforms, such as wavelet or wavelet packets, to decompose signals into sets of local features with various degrees of sparsity. We use this intrinsic property for selecting the best (most sparse) subsets of features for further separation. The performance of the algorithm is verified on noise-free and noisy data. Experiments with simulated signals, musical sounds and images demonstrate significant improvement of separation quality over previously reported results.	Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Zibulevsky, M (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.		Pearlmutter, Barak A/M-8791-2014; Pearlmutter, Barak A./AAL-8999-2020	Pearlmutter, Barak A/0000-0003-0521-4553; 				AMARI S, 1996, ADV NEURAL INFORMATI, V8; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cardoso JF, 1997, IEEE SIGNAL PROC LET, V4, P112, DOI 10.1109/97.566704; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Hyvarinen A., 1999, Neural Computing Surveys, V2; Kisilev P., 2000, 317 CCIT; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; MAKEIG S, 1999, ICA EEG TOOLBOX COMP; Prieto A, 1998, SIGNAL PROCESS, V64, P315, DOI 10.1016/S0165-1684(97)00198-9; Zibulevsky M, 2001, NEURAL COMPUT, V13, P863, DOI 10.1162/089976601300014385; [No title captured]	11	14	15	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1049	1056						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100131
C	Csato, L; Fokoue, E; Opper, M; Schottky, B; Winther, O		Solla, SA; Leen, TK; Muller, KR		Csato, L; Fokoue, E; Opper, M; Schottky, B; Winther, O			Efficient approaches to Gaussian Process classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present three simple approximations for the calculation of the posterior mean in Gaussian Process classification. The first two methods are related to mean field ideas known in Statistical Physics. The third approach is based on Bayesian online approach which was motivated by recent results in the Statistical Mechanics of Neural Networks. We present simulation results showing: 1. that the mean field Bayesian evidence may be used for hyperparameter tuning and 2. that the online approach may achieve a low training error fast.	Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Csato, L (corresponding author), Aston Univ, Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.							GIBBS MN, 1997, VARIATIONAL GAUSSIAN; Gorman R. P., 1988, NEURAL NETWORKS, V1; JAAKKOLA T, 1999, ONLINE P 7 INT WORKS; Neal, 1997, PHYSICS9701026 ARXIV; Opper M, 1996, PHYS REV LETT, V77, P4671, DOI 10.1103/PhysRevLett.77.4671; OPPER M, 1999, GAUSSIAN PROCESSES C; Ripley BD., 1996; Saad D, 1998, ONLINE LEARNING NEUR; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; WILLIAMS CKI, 1996, NEURAL INFORMATION P, P514; ZINNJUSTIN J, 1990, QUANTUM FIELD THEORY	11	14	14	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						251	257						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700036
C	Hinton, GE; Brown, AD		Solla, SA; Leen, TK; Muller, KR		Hinton, GE; Brown, AD			Spiking Boltzmann machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We first show how to represent sharp posterior probability distributions using real valued coefficients on broadly-tuned basis functions. Then we show how the precise times of spikes can be used to convey the real-valued coefficients on the basis functions quickly and accurately. Finally we describe a simple simulation in which spiking neurons learn to model an image sequence by fitting a dynamic generative model.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Hinton, GE (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.							ANDERSON CH, 1994, COMPUTATIONAL INTELL, P213; Hinton GE, 1999, IEE CONF PUBL, P1, DOI 10.1049/cp:19991075; HOPFIELD JJ, 1995, NATURE, V376, P33, DOI 10.1038/376033a0; McClelland JL, 1986, PARALLEL DISTRIBUTED, V1, P1	4	14	14	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						122	128						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700018
C	Li, Y; Long, PM		Solla, SA; Leen, TK; Muller, KR		Li, Y; Long, PM			The Relaxed Online Maximum Margin Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We describe a new incremental algorithm for training linear threshold functions: the Relaxed Online Maximum Margin Algorithm, or ROMMA. ROMMA can be viewed as an approximation to the algorithm that repeatedly chooses the hyperplane that classifies previously seen examples correctly with the maximum margin. It is known that such a maximum-margin hypothesis can be computed by minimizing the length of the weight vector subject to a number of linear constraints. ROMMA works by maintaining a relatively simple relaxation of these constraints that can be efficiently updated. We prove a mistake bound for ROMMA that is the same as that proved for the perceptron algorithm. Our analysis implies chat the more computationally intensive maximum-margin algorithm also satisfies this mistake bound; this is the first worst-case performance guarantee for this algorithm. We describe some experiments using ROMMA and a variant that updates its hypothesis more aggressively as batch algorithms to recognize handwritten digits. The computational complexity and simplicity of these algorithms is similar to that of perceptron algorithm, but their generalization is much better. We describe a sense in which the performance of ROMMA converges to that of SVM in the limit if bias isn't considered.	Natl Univ Singapore, Dept Comp Sci, Singapore 119260, Singapore	National University of Singapore	Li, Y (corresponding author), Natl Univ Singapore, Dept Comp Sci, Singapore 119260, Singapore.	liyi@comp.nus.edu.sg; plong@comp.nus.edu.sg						Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Freund Y., 1998, P 1998 C COMP LEARN; Friess TT, 1998, P 15 INT C MACH LEAR; KEERTHI SS, TRISL9903 IND I SCI; KOWALCZYK A, 1999, ADV LARGE MARGIN CLA; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; LITTLESTONE N, 1989, THESIS UC SANTA CRUZ; Platt J C, 1999, ADV KERNEL METHODS S; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582; SHAWETAYLOR J, 1996, P 1996 C COMP LEARN; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	14	14	14	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						498	504						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700071
C	Schneidman, E; Segev, I; Tishby, N		Solla, SA; Leen, TK; Muller, KR		Schneidman, E; Segev, I; Tishby, N			Information capacity and robustness of stochastic neuron models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SPIKE TRAINS; PRECISION; DYNAMICS; RELIABILITY; VARIABILITY; CHANNELS; CORTEX; NOISE	The reliability and accuracy of spike trains have been shown to depend on the nature of the stimulus that the neuron encodes. Adding ion channel stochasticity to neuronal models results in a macroscopic behavior that replicates the input-dependent reliability and precision of real neurons. We calculate the amount of information that an ion channel based stochastic Hodgkin-Huxley (HH) neuron model can encode about a wide set of stimuli. We show that both the information rate and the information per spike of the stochastic model are similar to the values reported experimentally Moreover, the amount of information that the neuron encodes is correlated with the amplitude of fluctuations in the input, and less so with the average firing rate of the neuron. We also show that for the HH ion channel density, the information capacity is robust to changes in the density of ion channels in the membrane, whereas changing the ratio between the Na+ and K+ ion channels has a considerable effect an the information that the neuron can encode. Finally, we suggest that neurons may maximize their information capacity by appropriately balancing the density of the different ion channels that underlie neuronal excitability.	Hebrew Univ Jerusalem, Inst Comp Sci, Dept Neurobiol, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Schneidman, E (corresponding author), Hebrew Univ Jerusalem, Inst Comp Sci, Dept Neurobiol, IL-91904 Jerusalem, Israel.			Segev, Idan/0000-0001-7279-9630				Bair W, 1996, NEURAL COMPUT, V8, P1185, DOI 10.1162/neco.1996.8.6.1185; Barkai N, 1997, NATURE, V387, P913, DOI 10.1038/43199; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DEFELICE L, 1981, INTRO MEMEBRANE NOIS; FITZHUGH R, 1965, J CELL COMPAR PHYSL, V66, P111, DOI 10.1002/jcp.1030660518; Hille B., 1992, IONIC CHANNELS EXCIT; HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764; MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778; Marder E, 1996, P NATL ACAD SCI USA, V93, P13481, DOI 10.1073/pnas.93.24.13481; Nowak LG, 1997, CEREB CORTEX, V7, P487, DOI 10.1093/cercor/7.6.487; Reich DS, 1997, J NEUROPHYSIOL, V77, P2836, DOI 10.1152/jn.1997.77.5.2836; RIEKE F, 1997, SPIKE EXPLORING NEUR; Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089; Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0; SKAUGEN E, 1979, ACTA PHYSIOL SCAND, V107, P343, DOI 10.1111/j.1748-1716.1979.tb06486.x; SOFTKY WR, 1993, J NEUROSCI, V13, P334; Stemmler M, 1999, NAT NEUROSCI, V2, P521, DOI 10.1038/9173; STRASSBERG AF, 1993, NEURAL COMPUT, V5, P843, DOI 10.1162/neco.1993.5.6.843; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; Toib A, 1998, J NEUROSCI, V18, P1893; vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805; White JA, 1998, J NEUROPHYSIOL, V80, P262, DOI 10.1152/jn.1998.80.1.262	22	14	14	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						178	184						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700026
C	Baluja, S		Kearns, MS; Solla, SA; Cohn, DA		Baluja, S			Probabilistic modeling for face orientation discrimination: Learning from labeled and unlabeled data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					This paper presents probabilistic modeling methods to solve the problem of discriminating between five facial orientations with very little labeled data. Three models are explored. The first model maintains no inter-pixel dependencies, the second model is capable of modeling a set of arbitrary pair-wise dependencies, and the last model allows dependencies only between neighboring pixels. We show that for all three of these models, the accuracy of the learned models can be greatly improved by augmenting a small number of labeled training images with a large set of unlabeled images using Expectation-Maximization. This is important because it is often difficult to obtain image labels, while many unlabeled images are readily available. Through a large set of empirical tests, we examine the benefits of unlabeled data for each of the models. By using only two randomly selected labeled examples per class, we can discriminate between the five facial orientations with an accuracy of 94%; with six labeled examples, we achieve an accuracy of 98%.	Carnegie Mellon Univ, Justsyst Pittsburgh Res Ctr, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Baluja, S (corresponding author), Carnegie Mellon Univ, Justsyst Pittsburgh Res Ctr, Pittsburgh, PA 15213 USA.							BARTLETT M, 1997, ADV NEURAL INFORMATI; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cohn DA, 1996, J ARTIF INTELL RES, V4, P129, DOI 10.1613/jair.295; COTTRELL G, 1991, 3 NIPS; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FRIEDMAN N, 1997, MACH LEARN, V1, P29; MCCALLUM A, 1998, ICML 98; MILLER D, 1997, ADV NEURAL INFORMATI, V9; NIGAM K, 1998, IN PRESS AAAI 98; ROWLEY HA, 1998, IEEE T PATTERN ANAL, V20; SHAHSHAHANI B, 1994, IEEE T GEOSC REMOTE; SUNG KK, 1996, THESIS MIT AI LAB; TURK M, 1991, J COG NEUROSCI, V3; 1994, 6 NIPS	14	14	14	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						854	860						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700120
C	Neuneier, R; Mihatsch, O		Kearns, MS; Solla, SA; Cohn, DA		Neuneier, R; Mihatsch, O			Risk sensitive reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					As already known, the expected return of a policy in Markov Decision Problems is not always the most suitable optimality criterion. For many applications control strategies have to meet various constraints like avoiding very bad states (risk-avoiding) or generating high profit within a short time (risk-seeking) although this might probably cause significant costs. We propose a modified Q-learning algorithm which uses a single continuous parameter kappa is an element of [-1, 1] to determine in which sense the resulting policy is optimal. For kappa = 0, the policy is optimal with respect to the usual expected return criterion, while kappa --> 1 generates a solution which is optimal in worst case. Analogous, the closer kappa is to -1 the more risk seeking the policy becomes. In contrast to other related approaches in the field of MDPs we do not have to transform the cost model or to increase the state space in order to take risk into account. Our new approach is evaluated by computing optimal investment strategies for an artificial stock market.	Siemens AG, Corp Technol, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Neuneier, R (corresponding author), Siemens AG, Corp Technol, D-81730 Munich, Germany.							BERTSEKAS D, 1996, NEURODYNAMIC PROGR; HEGER M, 1994, P 11 INT C; KOENIG S, 1994, P 4 INT C PRINC KNOW; LITTMAN ML, 1996, INT C MACH LEARN 96; NEUNEIER R, 1998, ADV NEUROL INFORMATI, V10; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; SZEPESVARI C, 1997, NONMARKOVIAN POLICIE; TSITSIKLIS JN, 1997, ADV NEURAL INFORMATI, V9	8	14	14	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1031	1037						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700145
C	Itti, L; Braun, J; Lee, DK; Koch, C		Jordan, MI; Kearns, MJ; Solla, SA		Itti, L; Braun, J; Lee, DK; Koch, C			A model of early visual processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We propose a model for early visual processing in primates. The model consists of a population of linear spatial filters which interact through non-linear excitatory and inhibitory pooling. Statistical estimation theory is then used to derive human psychophysical thresholds from the responses of the entire population of units. The model is able to reproduce human thresholds for contrast and orientation discrimination tasks, and to predict contrast thresholds in the presence of masks of varying orientation and spatial frequency.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Itti, L (corresponding author), CALTECH, MSC 139-74, Pasadena, CA 91125 USA.			Koch, Christof/0000-0001-6482-8067					0	14	14	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						173	179						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700025
C	Kivinen, J; Warmuth, MK		Jordan, MI; Kearns, MJ; Solla, SA		Kivinen, J; Warmuth, MK			Relative loss bounds for multidimensional regression problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We study on-line generalized linear regression with multidimensional outputs, i.e., neural networks with multiple output nodes but no hidden nodes. We allow at the final layer transfer functions such as the soft-max function that need to consider the linear activations to all the output neurons. We use distance functions of a certain kind in two completely independent roles in deriving and analyzing on-line learning algorithms for such tasks. We use one distance function to define a matching loss function for the (possibly multidimensional) transfer function, which allows us to generalize earlier results from one-dimensional to multidimensional outputs. We use another distance function as a tool for measuring progress made by the on-line updates. This shows how previously studied algorithms such as gradient descent and exponentiated gradient fit into a common framework. We evaluate the performance of the algorithms using relative loss bounds that compare the loss of the on-line algoritm to the best off-line predictor from the relevant model class, thus completely eliminating probabilistic assumptions about the data.	Univ Helsinki, Dept Comp Sci, FIN-00014 Helsinki, Finland	University of Helsinki	Kivinen, J (corresponding author), Univ Helsinki, Dept Comp Sci, Teollisuuskatu 23,POB 26, FIN-00014 Helsinki, Finland.								0	14	15	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						287	293						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700041
C	Bregler, C; Malik, J		Mozer, MC; Jordan, MI; Petsche, T		Bregler, C; Malik, J			Learning appearance based models: Mixtures of Second Moment experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper describes a new technique fbr object recognition based on learning appearance models. The image is decomposed into local regions which are described by a new texture representation called ''Generalized Second Moments'' that are derived from the output of multiscale, multiorientation filter banks. Class-characteristic local texture features and their global composition is learned by a hierarchical mixture of experts architecture (Jordan & Jacobs). The technique is applied to a vehicle database consisting of 5 general car categories (Sedan, Van with back-doors, Van without back-doors, old Sedan, and Volkswagen Bug). This is a difficult problem with considerable in-class variation. The new technique has a 6.5% misclassification rate, compared to eigen-images which give 17.4% misclassification rate, and nearest neighbors which give 15.7% misclassification rate.			Bregler, C (corresponding author), UNIV CALIF BERKELEY,DIV COMP SCI,BERKELEY,CA 94720, USA.								0	14	14	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						845	851						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00119
C	Jackson, JC; Craven, MW		Touretzky, DS; Mozer, MC; Hasselmo, ME		Jackson, JC; Craven, MW			Learning sparse perceptrons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						DUQUESNE UNIV,DEPT MATH & COMP SCI,PITTSBURGH,PA 15282	Duquesne University									0	14	14	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						654	660						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00093
C	BENSON, RG; DELBRUCK, T		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BENSON, RG; DELBRUCK, T			DIRECTION SELECTIVE SILICON RETINA THAT USES NULL INHIBITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	14	18	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						756	763						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00093
C	MCMILLAN, C; MOZER, MC; SMOLENSKY, P		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MCMILLAN, C; MOZER, MC; SMOLENSKY, P			RULE INDUCTION THROUGH INTEGRATED SYMBOLIC AND SUBSYMBOLIC PROCESSING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	14	14	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						969	976						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00119
C	Yang, X; Deng, C; Wei, K; Yan, JC; Liu, W		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Yang, Xu; Deng, Cheng; Wei, Kun; Yan, Junchi; Liu, Wei			Adversarial Learning for Robust Deep Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Deep clustering integrates embedding and clustering together to obtain the optimal nonlinear embedding space, which is more effective in real-world scenarios compared with conventional clustering methods. However, the robustness of the clustering network is prone to being attenuated especially when it encounters an adversarial attack. A small perturbation in the embedding space will lead to diverse clustering results since the labels are absent. In this paper, we propose a robust deep clustering method based on adversarial learning. Specifically, we first attempt to define adversarial samples in the embedding space for the clustering network. Meanwhile, we devise an adversarial attack strategy to explore samples that easily fool the clustering layers but do not impact the performance of the deep embedding. We then provide a simple yet efficient defense algorithm to improve the robustness of the clustering network. Experimental results on two popular datasets show that the proposed adversarial learning method can significantly enhance the robustness and further improve the overall clustering performance. Particularly, the proposed method is generally applicable to multiple existing clustering frameworks to boost their robustness. The source code is available at https://github.com/xdxuyang/ALRDC.	[Yang, Xu; Deng, Cheng; Wei, Kun] Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China; [Yan, Junchi] Shanghai Jiao Tong Univ, Dept CSE, Shanghai, Peoples R China; [Yan, Junchi] Shanghai Jiao Tong Univ, MoE Key Lab Artificial Intelligence, Shanghai, Peoples R China; [Liu, Wei] Tencent AI Lab, Shenzhen, Peoples R China	Xidian University; Shanghai Jiao Tong University; Shanghai Jiao Tong University; Tencent	Deng, C (corresponding author), Xidian Univ, Sch Elect Engn, Xian 710071, Peoples R China.	xuyang.xd@gmail.com; chdeng.xd@gmail.com; weikunsk@gmail.com; yanjunchi@sjtu.edu.cn; wl2223@columbia.edu	Yang, Xu/ACW-5509-2022	Yang, Xu/0000-0002-0405-6816	National Natural Science Foundation of China [62071361]; National Key R&D Program of China [2017YFE0104100, 2020AAA0107600]; CCF-Tencent Open Fund [RAGR20200113]; Tencent AI Lab Rhino-Bird Visiting Scholars Program	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key R&D Program of China; CCF-Tencent Open Fund; Tencent AI Lab Rhino-Bird Visiting Scholars Program	Our work was supported in part by the National Natural Science Foundation of China under Grant 62071361, the National Key R&D Program of China under Grant 2017YFE0104100 and 2020AAA0107600. Junchi Yan was sponsored by CCF-Tencent Open Fund RAGR20200113 and Tencent AI Lab Rhino-Bird Visiting Scholars Program.	Biggio B., 2013, P 2013 ACM WORKSH AR, P87, DOI DOI 10.1145/2517312.2517321; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Chen T, 2020, PR MACH LEARN RES, V119; Crussell J., 2015, P 2015 SIAM INT C DA, P235; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dang ZY, 2020, PROC CVPR IEEE, P6657, DOI 10.1109/CVPR42600.2020.00669; Deng C, 2020, IEEE T MULTIMEDIA, V22, P885, DOI 10.1109/TMM.2019.2934833; Dizaji KG, 2017, IEEE I CONF COMP VIS, P5747, DOI 10.1109/ICCV.2017.612; Grauman K., 2006, P IEEE INT C COMP VI, V1, p19 ; Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Hjelm R.D., 2018, P INT C LEARN REPR; Jiang Z., 2016, ARXIV PREPRINT ARXIV; Joo Weonyoung, 2019, ARXIV190102739; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ma BY, 2021, NEURAL COMPUT APPL, V33, P5793, DOI 10.1007/s00521-020-05358-9; Min SB, 2020, IEEE T IMAGE PROCESS, V29, P4996, DOI 10.1109/TIP.2020.2977457; Min SB, 2019, AAAI CONF ARTIF INTE, P4578, DOI 10.1609/aaai.v33i01.33014578; Nalisnick Eric, 2016, NIPS WORKSH BAYES DE, V2; Ng PC, 2003, NUCLEIC ACIDS RES, V31, P3812, DOI 10.1093/nar/gkg509; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Benavent AP, 2009, IEEE T NEURAL NETWOR, V20, P1756, DOI 10.1109/TNN.2009.2030190; Shaham Uri, 2018, ICLR; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Skillicorn DB, 2009, IEEE INTELL SYST, V24, P54, DOI 10.1109/MIS.2009.108; Tzoreff Elad, 2018, ARXIV180510795; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wei K, 2020, PROCEEDINGS OF THE TWENTY-NINTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P551; Xiao H., 2017, FASHION MNIST NOVEL; Xie JY, 2016, PR MACH LEARN RES, V48; Xu W., 2003, P 26 ANN INT ACM SIG, P267, DOI DOI 10.1145/860435.860485; Yang JW, 2016, PROC CVPR IEEE, P5147, DOI 10.1109/CVPR.2016.556; Yang X, 2022, IEEE T PATTERN ANAL, V44, P1992, DOI 10.1109/TPAMI.2020.3026079; Yang X, 2019, PROC CVPR IEEE, P10677, DOI 10.1109/CVPR.2019.01094; Yang Xu, 2018, AAAI	38	13	13	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000060
C	Baruch, M; Baruch, G; Goldberg, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Baruch, Moran; Baruch, Gilad; Goldberg, Yoav			A Little Is Enough: Circumventing Defenses For Distributed Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Distributed learning is central for large-scale training of deep-learning models. However, it is exposed to a security threat in which Byzantine participants can interrupt or control the learning process. Previous attack models assume that the rogue participants (a) are omniscient (know the data of all other participants), and (b) introduce large changes to the parameters. Accordingly, most defense mechanisms make a similar assumption and attempt to use statistically robust methods to identify and discard values whose reported gradients are far from the population mean. We observe that if the empirical variance between the gradients of workers is high enough, an attacker could take advantage of this and launch a non-omniscient attack that operates within the population variance. We show that the variance is indeed high enough even for simple datasets such as MNIST, allowing an attack that is not only undetected by existing defenses, but also uses their power against them, causing those defense mechanisms to consistently select the byzantine workers while discarding legitimate ones. We demonstrate our attack method works not only for preventing convergence but also for repurposing of the model behavior ("backdooring"). We show that less than 25% of colluding workers are sufficient to degrade the accuracy of models trained on MNIST, CIFAR10 and CIFAR100 by 50%, as well as to introduce backdoors without hurting the accuracy for MNIST and CIFAR10 datasets, but with a degradation for CIFAR100.	[Baruch, Moran; Baruch, Gilad; Goldberg, Yoav] Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel; [Goldberg, Yoav] Allen Inst Artificial Intelligence, Seattle, WA USA	Bar Ilan University	Baruch, M (corresponding author), Bar Ilan Univ, Dept Comp Sci, Ramat Gan, Israel.	moran.baruch@biu.ac.il; gilad.baruch@biu.ac.il; yogo@cs.biu.ac.il			BIU Center for Research in Applied Cryptography and Cyber Security; Israel National Cyber Bureau in the Prime Minister's Office	BIU Center for Research in Applied Cryptography and Cyber Security; Israel National Cyber Bureau in the Prime Minister's Office	This research was supported by the BIU Center for Research in Applied Cryptography and Cyber Security in conjunction with the Israel National Cyber Bureau in the Prime Minister's Office.	Ahmed A., 2014, P 11 USENIX C OP SYS, P583; [Anonymous], 2012, P 29 INT COFERENCE I; Baydin A. G., 2017, J MACH LEARN RES, V18, P1; Blanchard P, 2017, ADV NEUR IN, V30; Chen B., 2018, ADV NEURAL INFORM PR; Chen L., 2018, P INT C MACH LEARN, P903; Chen X., 2017, TARGETED BACKDOOR AT; Dean J., 2012, NIPS 12, V1, P1223; Dou W, 2018, 2018 IEEE PHOTONICS SOCIETY SUMMER TOPICAL MEETING SERIES (SUM), P31; Duchi J. C., 2010, P ADV NEUR INF PROC, P550; Fung C., 2018, CORR, Vabs/1808.04866; Gorbonos E, 2018, 2018 IEEE/WIC/ACM INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE (WI 2018), P25, DOI 10.1109/WI.2018.0-111; Hua Y., 2018, ARXIV PREPRINT ARXIV; Keskar N.S., 2017, ICLR; Kleinberg R. D., 2018, INT COF MACH LEARN I; Konecny J., 2016, ARXIV161005492; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Li Mu, 2014, ADV NEURAL INFORM PR, V27, P19; McMahan H. Brendan, 2016, ARXIV160205629; MHAMDI EE, 2018, P 35 INT C MACH LEAR, V80, P3521; Neelakantan A, 2016, PROC 4 INT C LEARN R, P327; Qiao M., 2017, ARXIV171108113; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Shen SQ, 2016, ANN COMPUT SECURITY, P508, DOI 10.1145/291079.2991125; Steinhardt J, 2017, ADV NEUR IN, V30; Xie C., 2018, ARXIV180210116; Yin D., 2018, P INT C MACH LEARN I; Yudong Chen, 2017, Proceedings of the ACM on Measurement and Analysis of Computing Systems, V1, DOI 10.1145/3154503; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang H., 2017, POSEIDON EFFICIENT C	31	13	13	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900025
C	Bertasius, G; Feichtenhofer, C; Tran, D; Shi, JB; Torresani, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bertasius, Gedas; Feichtenhofer, Christoph; Tran, Du; Shi, Jianbo; Torresani, Lorenzo			Learning Temporal Pose Estimation from Sparsely-Labeled Videos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern approaches for multi-person pose estimation in video require large amounts of dense annotations. However, labeling every frame in a video is costly and labor intensive. To reduce the need for dense annotations, we propose a PoseWarper network that leverages training videos with sparse annotations (every k frames) to learn to perform dense temporal pose propagation and estimation. Given a pair of video frames-a labeled Frame A and an unlabeled Frame B-we train our model to predict human pose in Frame A using the features from Frame B by means of deformable convolutions to implicitly learn the pose warping between A and B. We demonstrate that we can leverage our trained PoseWarper for several applications. First, at inference time we can reverse the application direction of our network in order to propagate pose information from manually annotated frames to unlabeled frames. This makes it possible to generate pose annotations for the entire video given only a few manually-labeled frames. Compared to modern label propagation methods based on optical flow, our warping mechanism is much more compact (6M vs 39M parameters), and also more accurate (88:7% mAP vs 83:8% mAP). We also show that we can improve the accuracy of a pose estimator by training it on an augmented dataset obtained by adding our propagated poses to the original manual labels. Lastly, we can use our PoseWarper to aggregate temporal pose information from neighboring frames during inference. This allows us to obtain state-of-the-art pose detection results on PoseTrack2017 and PoseTrack2018 datasets. Code has been made available at: https://github.com/facebookresearch/PoseWarper.	[Bertasius, Gedas; Feichtenhofer, Christoph; Tran, Du; Torresani, Lorenzo] Facebook AI, Menlo Pk, CA 94025 USA; [Bertasius, Gedas; Shi, Jianbo] Univ Penn, Philadelphia, PA 19104 USA	Facebook Inc; University of Pennsylvania	Bertasius, G (corresponding author), Facebook AI, Menlo Pk, CA 94025 USA.; Bertasius, G (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.		Tran, Du/AAB-5973-2021					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Andriluka M, 2010, PROC CVPR IEEE, P623, DOI 10.1109/CVPR.2010.5540156; Andriluka M, 2009, PROC CVPR IEEE, P1014, DOI 10.1109/CVPRW.2009.5206754; Belagiannis V., 2017, FG; Bertasius G, 2017, PROC CVPR IEEE, P6137, DOI 10.1109/CVPR.2017.650; Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Charles J, 2016, PROC CVPR IEEE, P3063, DOI 10.1109/CVPR.2016.334; Chen X., 2014, P 27 ANN C NEURAL IN, P1736, DOI DOI 10.1109/CVPR.2018.00742; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dantone M, 2013, PROC CVPR IEEE, P3041, DOI 10.1109/CVPR.2013.391; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fang HS, 2017, IEEE I CONF COMP VIS, P2353, DOI 10.1109/ICCV.2017.256; Farneback G, 2003, LECT NOTES COMPUT SC, V2749, P363, DOI 10.1007/3-540-45103-x_50; Girdhar R, 2018, PROC CVPR IEEE, P350, DOI 10.1109/CVPR.2018.00044; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Guo Hengkai, 2018, COMP VIS ECCV 2018 2, P209; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Kaiming, 2014, COMPUTER VISION ECCV; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Insafutdinov E, 2017, PROC CVPR IEEE, P1293, DOI 10.1109/CVPR.2017.142; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Iqbal U, 2017, PROC CVPR IEEE, P4654, DOI 10.1109/CVPR.2017.495; Johnson Sam, 2010, BMVC, DOI [10.5244/C.24.12, DOI 10.5244/C.24.12]; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lan XY, 2005, IEEE I CONF COMP VIS, P470; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Ouyang WL, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.299; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Pishchulin L, 2016, PROC CVPR IEEE, P4929, DOI 10.1109/CVPR.2016.533; Pishchulin L, 2013, IEEE I CONF COMP VIS, P3487, DOI 10.1109/ICCV.2013.433; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sigal L., 2006, PROC IEEE C COMPUT V, P2041; Song Jie, 2017, CVPR, P4420; Sun KK, 2021, IEEE T SYST MAN CY-S, V51, P3968, DOI 10.1109/TSMC.2019.2958072; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Ulhaq A, 2017, INT CONF IMAG VIS; Wang Y, 2008, LECT NOTES COMPUT SC, V5304, P710, DOI 10.1007/978-3-540-88690-7_53; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Wiles Olivia, 2018, ARXIV180806882; Xiao B, 2018, LECT NOTES COMPUT SC, V11210, P472, DOI 10.1007/978-3-030-01231-1_29; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xiu Y., 2018, BMVC; YANG Y, 2011, PROC CVPR IEEE, P1385, DOI [10.1109/CVPR.2011.5995741, DOI 10.1109/CVPR.2011.5995741]; Yosinski J., 2015, ICML DEEP LEARN WORK; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang DW, 2018, PROC CVPR IEEE, P6762, DOI 10.1109/CVPR.2018.00707	60	13	13	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303006
C	Chen, M; Artieres, T; Denoyer, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Mickael; Artieres, Thierry; Denoyer, Ludovic			Unsupervised Object Segmentation by Redrawing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Object segmentation is a crucial problem that is usually solved by using supervised learning approaches over very large datasets composed of both images and corresponding object masks. Since the masks have to be provided at pixel level, building such a dataset for any new domain can be very time-consuming. We present ReDO, a new model able to extract objects from images without any annotation in an unsupervised way. It relies on the idea that it should be possible to change the textures or colors of the objects without changing the overall distribution of the dataset. Following this assumption, our approach is based on an adversarial architecture where the generator is guided by an input sample: given an image, it extracts the object mask, then redraws a new object at the same location. The generator is controlled by a discriminator that ensures that the distribution of generated images is aligned to the original one. We experiment with this method on different datasets and demonstrate the good quality of extracted masks.	[Chen, Mickael] Sorbonne Univ, CNRS, LIP6, F-75005 Paris, France; [Artieres, Thierry] Univ Toulon & Var, Aix Marseille Univ, CNRS, LIS,Ecole Cent Marseille, Marseille, France; [Denoyer, Ludovic] Facebook Artificial Intelligence Res, New York, NY USA	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Aix-Marseille Universite; Universite de Toulon; Facebook Inc	Chen, M (corresponding author), Sorbonne Univ, CNRS, LIP6, F-75005 Paris, France.	mickael.chen@lip6.fr; thierry.artieres@centrale-marseille.fr; denoyer@fb.com			French National Research Agency [ANR-15-CE23-0026-03, ANR-16-CE23-0006]	French National Research Agency(French National Research Agency (ANR))	This work was supported by the French National Research Agency projects LIVES (grant number ANR-15-CE23-0026-03) and "Deep in France" (grant number ANR-16-CE23-0006).	Almahairi A, 2018, PR MACH LEARN RES, V80; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Brock A., 2019, INT C LEARNING REPRE; Burgess Christopher P, 2019, ARXIV190111390; Chen Liang-Chieh, 2018, P EUROPEAN C COMPUTE, P801; Chen M., 2018, INT C LEARN REPR; Chen T.Q., 2018, NEURIPS, P2610; Chen Ting, 2019, INT C LEARN REPR; Donahue Chris, 2018, P ICLR; Dumoulin Vincent, 2017, ICLR; Durand Thibaut, 2017, CVPR, DOI DOI 10.1109/CVPR.2017.631; Eslami SM, 2016, NEURIPS, V1; Esmaeili B., 2019, PROC INT C ARTIF INT; Ganin Yaroslav, 2015, ICML; Gary B., 2014, LABELED FACES WILD U; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greff K, 2019, PR MACH LEARN RES, V97; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Higgins Irina, 2017, 5 INT CLEARN REPR IC; Hsu KJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P748; Hsu Kuang-Jui, 2019, PC COMP VIS PATT REC; Huang GS, 2007, 2007 7TH IEEE CONFERENCE ON NANOTECHNOLOGY, VOL 1-3, P7, DOI 10.1109/NANO.2007.4601129; Huang GB, 2007, IEEE I CONF COMP VIS, P237, DOI 10.1109/iccv.2007.4408858; Ji Xu, 2019, INT C COMP VIS ICCV; Jiang Y, 2017, IGGRAPH ASIA 2017 TECHNICAL BRIEFS (SA'17), DOI 10.1145/3145749.3149440; Kae A, 2013, PROC CVPR IEEE, P2019, DOI 10.1109/CVPR.2013.263; Kaiming He, 2020, IEEE Transactions on Pattern Analysis and Machine Intelligence, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Kanezaki Asako, 2018, P IEEE INT C AC SPEE; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Lample Guillaume, 2017, ARXIV170600409; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lim Jae Hyun, 2017, ABS170502894 ARXIV; Long MS, 2018, ADV NEUR IN, V31; Lucic M, 2019, PR MACH LEARN RES, V97; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Nilsback Maria-Elena, 2007, BMVC, V2007, P1; Ostyakov P, 2018, ARXIV181107630; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Pham TT, 2018, IEEE INT CONF ROBOT, P3213; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Remez T, 2018, LECT NOTES COMPUT SC, V11211, P39, DOI 10.1007/978-3-030-01234-2_3; Rother C., 2006, P IEEE CVPR, V1, P993, DOI DOI 10.1109/CVPR.2006.91; Saxe Andrew M., 2014, ICLR 2014; Sbai O, 2018, ARXIV181205484; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Tran Dustin, 2017, ARXIV1708896; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van Steenkiste Sjoerd, 2018, ARXIV181010340; Wah CK, 2011, HOUS SOC SER, P1; Wang XL, 2016, LECT NOTES COMPUT SC, V9908, P318, DOI 10.1007/978-3-319-46493-0_20; Xia Xide, 2017, ARXIV171108506; Zhang Han, 2018, ARXIV180508318; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	60	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904038
C	Gasse, M; Chetelat, D; Ferroni, N; Charlin, L; Lodi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gasse, Maxime; Chetelat, Didier; Ferroni, Nicola; Charlin, Laurent; Lodi, Andrea			Exact Combinatorial Optimization with Graph Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Combinatorial optimization problems are typically tackled by the branch-and-bound paradigm. We propose a new graph convolutional neural network model for learning branch-and-bound variable selection policies, which leverages the natural variable-constraint bipartite graph representation of mixed-integer linear programs. We train our model via imitation learning from the strong branching expert rule, and demonstrate on a series of hard problems that our approach produces policies that improve upon state-of-the-art machine-learning methods for branching and generalize to instances significantly larger than seen during training. Moreover, we improve for the first time over expert-designed branching rules implemented in a state-of-the-art solver on large problems.	[Gasse, Maxime; Lodi, Andrea] Polytech Montreal, Mila, Montreal, PQ, Canada; [Chetelat, Didier] Polytech Montreal, Montreal, PQ, Canada; [Ferroni, Nicola] Univ Bologna, Bologna, Italy; [Charlin, Laurent] HEC Montreal, Mila, Montreal, PQ, Canada	Universite de Montreal; Polytechnique Montreal; Universite de Montreal; Polytechnique Montreal; University of Bologna; Universite de Montreal; HEC Montreal	Gasse, M (corresponding author), Polytech Montreal, Mila, Montreal, PQ, Canada.	maxime.gasse@polymtl.ca; didier.chetelat@polymtl.ca; n.ferroni@specialvideo.it; laurent.charlin@hec.ca; andrea.lodi@polymtl.ca			Canada First Research Excellence Fund (CFREF); CIFAR; Canada Excellence Research Chairs (CERC); IVADO; GERAD	Canada First Research Excellence Fund (CFREF); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Canada Excellence Research Chairs (CERC); IVADO; GERAD	This work was supported by the Canada First Research Excellence Fund (CFREF), IVADO, CIFAR, GERAD, and Canada Excellence Research Chairs (CERC).	Achterberg Tobias, 2009, INTEGRATION AI OR TE; Alvarez AM, 2017, INFORMS J COMPUT, V29, P185, DOI 10.1287/ijoc.2016.0723; ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X; BALAS E, 1980, COMBINATORIAL OPTIMI, V12, P37; Balcan Maria-Florina, 2018, P INT C MACH LEARN; Balunovic Mislav, 2018, ADV NEURAL INFORM PR, V31, P10338; Bengio Y., 2018, ARXIV181106128; Bergman D, 2016, ARTIF INTELL-FOUND, P1, DOI 10.1007/978-3-319-42849-9; Bruna J., 2014, P 2 INT C LEARN REPR; Burges C., 2010, TECH REP MSR T 2010; CORNUEJOLS G, 1991, EUR J OPER RES, V50, P280, DOI 10.1016/0377-2217(91)90261-S; Duvenaud David K, 2015, P NIPS; Fischetti M, 2012, OPER RES LETT, V40, P159, DOI 10.1016/j.orl.2012.01.008; Geurts P, 2006, MACH LEARN, V63, P3, DOI 10.1007/s10994-006-6226-1; Gilmer J, 2017, PR MACH LEARN RES, V70; Gleixner A., 2019, MIPLIB 2017 DATA DRI; Gleixner A., 2018, SCIP OPTIMIZATION SU; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hansknecht C., 2018, PRIMAL HEURISTICS LE; He H., 2014, ADV NEURAL INFORM PR, P3293; Howard Ronald A., 1960, DYNAMIC PROGRAMMING; Hu W., 2019, INT C LEARN REPR; Hussein A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054912; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; Karzan FK, 2009, MATH PROGRAM COMPUT, V1, P249, DOI 10.1007/s12532-009-0009-1; Khalil E., 2017, ADV NEURAL INFORM PR, P6348; Khalil EB, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P659; Kulesza Alex, 2008, ADV NEURAL INFORM PR, P785; LAND AH, 1960, ECONOMETRICA, V28, P497, DOI 10.2307/1910129; Lederman G., 2018, ARXIV180708058; Leyton-Brown K., 2000, EC'00. Proceedings of the 2nd ACM Conference on Electronic Commerce, P66, DOI 10.1145/352871.352879; Li Y., 2016, 4 INT C LEARNING REP; Liang JH, 2016, LECT NOTES COMPUT SC, V9710, P123, DOI 10.1007/978-3-319-40970-2_9; Paschos Vangelis T., 2014, MATH STAT; Patel J, 2007, MATH PROGRAM, V110, P445, DOI 10.1007/s10107-006-0009-0; Pochet Y., 2006, SPRING S OPERAT RES, DOI 10.1007/0-387-33477-7.; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Selsam D., 2019, ICLR; Song Jialin, 2018, LEARNING SEARCH VIA; Srikumar V, 2012, P 2012 JOINT C EMP M, P1114; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Welling M, 2017, 5 INT C LEARN REPRES; Wolsey L.A., 1988, INTEGER PROGRAMMING	53	13	13	2	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907026
C	Jabri, A; Hsu, K; Eysenbach, B; Gupta, A; Levine, S; Finn, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jabri, Allan; Hsu, Kyle; Eysenbach, Benjamin; Gupta, Abhishek; Levine, Sergey; Finn, Chelsea			Unsupervised Curricula for Visual Meta-Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In principle, meta-reinforcement learning algorithms leverage experience across many tasks to learn fast reinforcement learning (RL) strategies that transfer to similar tasks. However, current meta-RL approaches rely on manually-defined distributions of training tasks, and hand-crafting these task distributions can be challenging and time-consuming. Can "useful" pre-training tasks be discovered in an unsupervised manner? We develop an unsupervised algorithm for inducing an adaptive meta-training task distribution, i.e. an automatic curriculum, by modeling unsupervised interaction in a visual environment. The task distribution is scaffolded by a parametric density model of the meta-learner's trajectory distribution. We formulate unsupervised meta-RL as information maximization between a latent task variable and the meta-learner's data distribution, and describe a practical instantiation which alternates between integration of recent experience into the task distribution and meta-learning of the updated tasks. Repeating this procedure leads to iterative reorganization such that the curriculum adapts as the meta-learner's data distribution shifts. In particular, we show how discriminative clustering for visual representation can support trajectory-level task acquisition and exploration in domains with pixel observations, avoiding pitfalls of alternatives. In experiments on vision-based navigation and manipulation domains, we show that the algorithm allows for unsupervised meta-leaming that transfers to downstream tasks specified by hand-crafted reward functions and serves as pre-training for more efficient supervised meta-learning of test task distributions.	[Jabri, Allan; Gupta, Abhishek; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94704 USA; [Hsu, Kyle] Univ Toronto, Toronto, ON, Canada; [Eysenbach, Benjamin] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Finn, Chelsea] Stanford Univ, Stanford, CA 94305 USA	University of California System; University of California Berkeley; University of Toronto; Carnegie Mellon University; Stanford University	Jabri, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94704 USA.			Finn, Chelsea/0000-0001-6298-0874	PD Soros Fellowship; National Science Foundation [IIS-1651843, IIS-1700697, IIS-1700696]; Google	PD Soros Fellowship; National Science Foundation(National Science Foundation (NSF)); Google(Google Incorporated)	We thank the BAIR community for helpful discussion, and Michael Janner and Oleh Rybkin in particular for feedback on an earlier draft. AJ thanks Alexei Efros for his steadfastness and advice, and Sasha Sax and Ashish Kumar for discussion. KH thanks his family for their support. AJ is supported by the PD Soros Fellowship. This work was supported in part by the National Science Foundation, IIS-1651843, IIS-1700697, and IIS-1700696, as well as Google.	Achiam J., 2018, ARXIV180710299; Antoniou A., 2019, ARXIV PREPRINT ARXIV; Barber D, 2004, ADV NEUR IN, V16, P201; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bellemare M., 2016, NEURIPS; Bojanowski P, 2017, PR MACH LEARN RES, V70; Botvinick M, 2019, TRENDS COGN SCI, V23, P408, DOI 10.1016/j.tics.2019.02.006; Calhoun AJ, 2014, ELIFE, V3, DOI 10.7554/eLife.04220; Caron M, 2018, LECT NOTES COMPUT SC, V11218, P139, DOI 10.1007/978-3-030-01264-9_9; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chaplot DS, 2018, AAAI CONF ARTIF INTE, P2819; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Duan Y., 2016, RL2 FAST REINFORCEME; Ebert Frederik, 2017, ARXIV171005268; ELMAN JL, 1993, COGNITION, V48, P71, DOI 10.1016/0010-0277(93)90058-4; Eysenbach B., 2019, INT C LEARNING REPRE; Fergus R., 2018, INT C LEARN REPR ICL, P1; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Finn C, 2017, PR MACH LEARN RES, V70; Florensa C., 2017, PROC 1 C ROBOT LEARN, P482; Florensa C., 2017, 170506399 ARXIV; Fu J, 2017, ADV NEUR IN, V30; Graves A, 2017, PR MACH LEARN RES, V70; Gupta Abhishek, 2018, ARXIV180604640; Hadfield-Menell D, 2017, ADV NEUR IN, V30; Hastie T., 2009, UNSUPERVISED LEARNIN, P485; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Houthooft R, 2018, ADV NEUR IN, V31; Hsu K., 2019, UNSUPERVISED LEARNIN; Kempka M, 2016, IEEE CONF COMPU INTE; Khodadadeh S., 2018, ARXIV181111819; Kingma D. P, 2014, ARXIV13126114; Klimov O, 2019, INT C LEARNING REPRE; Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025; Lopes M., 2012, ADV NEURAL INFORM PR, V1; Matiisen T, 2020, IEEE T NEUR NET LEAR, V31, P3732, DOI 10.1109/TNNLS.2019.2934906; Mishra N., 2018, INT C LEARN REPR, P1; Mohamed S., 2015, NIPS; Osband Ian, 2018, NEURAL INFORM PROCES; Oudeyer, 2017, ARXIV170802190; Pathak D, 2017, PR MACH LEARN RES, V70; Pathak D, 2018, IEEE COMPUT SOC CONF, P2131, DOI 10.1109/CVPRW.2018.00278; Piaget J., 1954, CONSTR REALITY CHILD, DOI [10.1037/11168-000, DOI 10.1037/11168-000]; Pong V., 2018, PROC 2 WORKSHOP LIFE; Rakelly K, 2019, PR MACH LEARN RES, V97; Rothfuss J., 2019, INT C LEARN REPR; Salge C, 2014, EMERGENCE COMPLEX CO, V9, P67, DOI 10.1007/978-3-642-53734-9_4; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schmidhuber J, 2011, POWERPLAY TRAINING I; Schmidhuber J, 1987, THESIS; Schmidhuber J, 2009, LECT NOTES COMPUT SC, V5499, P48, DOI 10.1007/978-3-642-02565-5_4; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shyam P, 2019, PR MACH LEARN RES, V97; Stadie BC, 2018, ADV NEUR IN, V31; Sung F., 2017, ARXIV170609529; Thrun S, 1998, LEARNING TO LEARN, P181; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van den Oord Aaron, 2018, ARXIV180703748; Wang JW, 2017, ACTA OPHTHALMOL, V95, pE10, DOI 10.1111/aos.13227; Warde-Farley David, 2019, INT C LEARN REPR; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; WHITE RW, 1959, PSYCHOL REV, V66, P297, DOI 10.1037/h0040934; Wierstra D., 2016, ABS161107507 CORR; Xie A., 2018, C ROBOT LEARNING P M, P40	65	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902018
C	Jagatap, G; Hegde, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jagatap, Gauri; Hegde, Chinmay			Algorithmic Guarantees for Inverse Imaging with Untrained Network Priors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SPARSE PHASE RETRIEVAL	Deep neural networks as image priors have been recently introduced for problems such as denoising, super-resolution and inpainting with promising performance gains over hand-crafted image priors such as sparsity. Unlike learned generative priors they do not require any training over large datasets. However, few theoretical guarantees exist in the scope of using untrained network priors for inverse imaging problems. We explore new applications and theory for untrained neural network priors. Specifically, we consider the problem of solving linear inverse problems, such as compressive sensing, as well as non-linear problems, such as compressive phase retrieval. We model images to lie in the range of an untrained deep generative network with a fixed seed. We further present a projected gradient descent scheme that can be used for both compressive sensing and phase retrieval and provide rigorous theoretical guarantees for its convergence. We also show both theoretically as well as empirically that with deep neural network priors, one can achieve better compression rates for the same image quality as compared to when hand crafted priors are used.	[Jagatap, Gauri; Hegde, Chinmay] NYU, New York, NY 10003 USA	New York University	Jagatap, G (corresponding author), NYU, New York, NY 10003 USA.	gauri.jagatap@nyu.edu; chinmay.h@nyu.edu			NSF [CAREER CCF-2005804, CCF-1815101]; Black and Veatch Foundation	NSF(National Science Foundation (NSF)); Black and Veatch Foundation	This work was supported in part by NSF grants CAREER CCF-2005804, CCF-1815101, and a faculty fellowship from the Black and Veatch Foundation.	Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894; Bora A, 2017, PR MACH LEARN RES, V70; Cai TT, 2016, ANN STAT, V44, P2221, DOI 10.1214/16-AOS1443; Chang JHR, 2017, IEEE I CONF COMP VIS, P5889, DOI 10.1109/ICCV.2017.627; Chen G, 2015, 6TH INTERNATIONAL SYMPOSIUM ON HIGH-TEMPERATURE METALLURGICAL PROCESSING, P739; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Dabov K, 2006, PROC SPIE, V6064, DOI 10.1117/12.643267; Dimakis A., 2019, ARXIV190408594; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Du S., 2018, INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hand P, 2018, ADV NEUR IN, V31; Heckel R., 2019, ARXIV190703100; Heckel R., 2018, P ICLR, P1; Hyder R., 2019, ARXIV190302707; Jagatap G., 2019, IEEE T INFORM THEORY; Jagatap G., 2017, P ADV NEUR INF PROC, P4917; Li C., USERS GUIDE TVAL3 TV; Lillicrap T., 2019, ARXIV190506723; Mallat S, 1999, WAVELET TOUR SIGNAL, DOI DOI 10.1016/B978-012466606-1/50004-0; Metzler CA, 2018, PR MACH LEARN RES, V80; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Oymak S., 2019, ARXIV190204674; Papyan V, 2017, IEEE I CONF COMP VIS, P5306, DOI 10.1109/ICCV.2017.566; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Shah V, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P4609; Shamshad F, 2019, INT CONF ACOUST SPEE, P7720, DOI 10.1109/ICASSP.2019.8682179; Sulam J, 2018, IEEE T SIGNAL PROCES, V66, P4090, DOI 10.1109/TSP.2018.2846226; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Van Veen D., 2018, ARXIV180606438; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang G, 2018, IEEE T SIGNAL PROCES, V66, P479, DOI 10.1109/TSP.2017.2771733; Wolf L., 2019, ARXIV190407612; Zhang H., 2016, ADV NEURAL INFORM PR, V29, P2630	37	13	13	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906048
C	Jia, JT; Benson, AR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jia, Junteng; Benson, Austin R.			Neural Jump Stochastic Differential Equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	Many time series are effectively generated by a combination of deterministic continuous flows along with discrete jumps sparked by stochastic events. However, we usually do not have the equation of motion describing the flows, or how they are affected by jumps. To this end, we introduce Neural Jump Stochastic Differential Equations that provide a data-driven approach to learn continuous and discrete dynamic behavior, i.e., hybrid systems that both flow and jump. Our approach extends the framework of Neural Ordinary Differential Equations with a stochastic process term that models discrete events. We then model temporal point processes with a piecewise-continuous latent trajectory, where the discontinuities are caused by stochastic events whose conditional intensity depends on the latent state. We demonstrate the predictive capabilities of our model on a range of synthetic and real-world marked point process datasets, including classical point processes (such as Hawkes processes), awards on Stack Overflow, medical records, and earthquake monitoring.	[Jia, Junteng; Benson, Austin R.] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Jia, JT (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	jj585@cornell.edu; arb@cs.cornell.edu			NSF Award [DMS-1830274]; ARO Award [W911NF19-1-0057]; ARO MURI	NSF Award(National Science Foundation (NSF)); ARO Award; ARO MURI(MURI)	This research was supported by NSF Award DMS-1830274, ARO Award W911NF19-1-0057, and ARO MURI.	Anderson A., 2013, P 22 INT C WORLD WID, P95, DOI DOI 10.1145/2488388.2488398; [Anonymous], 2018, ARXIV180701883; Blundell C, 2012, NIPS; Branicky Michael S., 2005, INTRO HYBRID SYSTEMS; Chen T.Q., 2018, ADV NEURAL INFORM PR; Corner Sebastien, 2018, ARXIV180207188; COX JC, 1976, J FINANC ECON, V3, P145, DOI 10.1016/0304-405X(76)90023-4; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Farajtabar M, 2015, ADV NEUR IN, V28; Farajtabar Mehrdad, 2014, Adv Neural Inf Process Syst, V27; Glover W, 2004, LECT NOTES COMPUT SC, V2993, P372; Hardiman SJ, 2013, EUR PHYS J B, V86, DOI 10.1140/epjb/e2013-40107-3; Hespanha JP, 2004, LECT NOTES COMPUT SC, V2993, P387; Innes M., 2019, ARXIV190707587; Khoo Y, 2019, RES MATH SCI, V6, DOI 10.1007/s40687-018-0160-2; Kobayashi Ryota, 2016, P 10 INT AAAI C WEB; LAUB P. J., 2015, ARXIV150702822; Li LD, 2014, AAAI CONF ARTIF INTE, P101; Li LD, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1667; Li XF, 2017, EURASIP J BIOINFORM, DOI [10.1186/s13637-017-0061-5, 10.1186/s13637-017-0061]; Long ZC, 2018, PR MACH LEARN RES, V80; Mei HY, 2017, ADV NEUR IN, V30; MERTON RC, 1976, J FINANC ECON, V3, P125, DOI 10.1016/0304-405X(76)90022-2; OGATA Y, 1984, STOCH PROC APPL, V17, P337, DOI 10.1016/0304-4149(84)90009-7; Ogata Y, 1999, PURE APPL GEOPHYS, V155, P471, DOI 10.1007/s000240050275; Peluchetti Stefano, 2019, ARXIV190511065; Rackauckas C., 2019, ARXIV190202376; Rackauckas Christopher, 2018, ARXIV181201892; Raissi M., 2018, PREPRINT; Raissi M, 2018, J COMPUT PHYS, V357, P125, DOI 10.1016/j.jcp.2017.11.039; Rizoiu MA, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P735, DOI 10.1145/3038912.3052650; Rubanova Y, 2019, ARXIV PREPRINT ARXIV; Ryder T, 2018, PR MACH LEARN RES, V80; Stomakhin A, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/11/115013; Tzen B., 2019, NEURAL STOCHASTIC DI; Valera I, 2015, IEEE DATA MINING, P409, DOI 10.1109/ICDM.2015.40; Van der Schaft A., 2000, INTRO HYBRID DYNAMIC, V251; Vere-Jones, 2003, INTRO THEORY POINT P; Wang YC, 2018, PR MACH LEARN RES, V84; Xiao S, 2017, ADV NEUR IN, V30; Xiao Shuai, 2017, AAAI; Xu H., 2017, P 31 INT C NEURAL IN, P1354; Xu HT, 2017, PR MACH LEARN RES, V70; Zarezade A, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P51, DOI 10.1145/3018661.3018684; Zhou K, 2013, ICML	45	13	13	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901047
C	Kerg, G; Goyette, K; Touzel, MP; Gidel, G; Vorontsov, E; Bengio, Y; Lajoie, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kerg, Giancarlo; Goyette, Kyle; Touzel, Maximilian Puelma; Gidel, Gauthier; Vorontsov, Eugene; Bengio, Yoshua; Lajoie, Guillaume			Non-normal Recurrent Neural Network (nnRNN): learning long time dependencies while improving expressivity with transient dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MEMORY	A recent strategy to circumvent the exploding and vanishing gradient problem in RNNs, and to allow the stable propagation of signals over long time scales, is to constrain recurrent connectivity matrices to be orthogonal or unitary. This ensures eigenvalues with unit norm and thus stable dynamics and training. However this comes at the cost of reduced expressivity due to the limited variety of orthogonal transformations. We propose a novel connectivity structure based on the Schur decomposition, though we avoid computing it explicitly, and a splitting of the Schur form into normal and non-normal parts. This allows to parametrize matrices with unit-norm eigenspectra without orthogonality constraints on eigenbases. The resulting architecture ensures access to a larger space of spectrally constrained matrices, of which orthogonal matrices are a subset. This crucial difference retains the stability advantages and training speed of orthogonal RNNs while enhancing expressivity, especially on tasks that require computations over ongoing input sequences.	[Kerg, Giancarlo; Goyette, Kyle; Touzel, Maximilian Puelma; Gidel, Gauthier; Vorontsov, Eugene; Bengio, Yoshua; Lajoie, Guillaume] Mila Quebec AI Inst, Montreal, PQ, Canada; [Kerg, Giancarlo; Goyette, Kyle; Gidel, Gauthier; Bengio, Yoshua] Univ Montreal, Dept Informat & Rech Oprationelle, Montreal, PQ, Canada; [Goyette, Kyle] Univ Montreal, CIRRELT, Montreal, PQ, Canada; [Vorontsov, Eugene] Ecole Polytech Montreal, Montreal, PQ, Canada; [Lajoie, Guillaume] Ecole Polytech Montreal, Dept Math & Stat, Montreal, PQ, Canada	Universite de Montreal; Universite de Montreal; Universite de Montreal; Polytechnique Montreal; Universite de Montreal; Polytechnique Montreal	Lajoie, G (corresponding author), Mila Quebec AI Inst, Montreal, PQ, Canada.	lajoie@dms.umontreal.ca			CIFAR; Microsoft; NSERC; IVADO; NSERC Discovery Grant [RGPIN-2018-04821]; FRQNT Young Investigator Startup Program [2019-NC-253251]; FRQS Research Scholar Award, Junior 1 [LAJGU0401-253188]	CIFAR(Canadian Institute for Advanced Research (CIFAR)); Microsoft(Microsoft); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); IVADO; NSERC Discovery Grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); FRQNT Young Investigator Startup Program; FRQS Research Scholar Award, Junior 1	We would like to thank Tim Cooijmans, Sarath Chandar, Jonathan Binas, Anirudh Goyal, Cesar Laurent and Tianyu Li for useful discussions. YB ackowledges support from CIFAR, Microsoft and NSERC. MPT acknowledges IVADO support. GL is funded by an NSERC Discovery Grant (RGPIN-2018-04821), an FRQNT Young Investigator Startup Program (2019-NC-253251), and an FRQS Research Scholar Award, Junior 1 (LAJGU0401-253188).	Anderson E., 1999, LAPACK USERS GUIDE; Arjovsky M, 2016, PR MACH LEARN RES, V48; Arpit Devansh, 2019, ICLR; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Chandar Sarath, 2019, AAAI; Chang Bo, 2019, ICLR; Chen Minmin, 2018, ICML; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Ganguli S., 2014, INT C LEARN REPR; Ganguli S, 2008, P NATL ACAD SCI USA, V105, P18970, DOI 10.1073/pnas.0804451105; Goldman MS, 2009, NEURON, V61, P621, DOI 10.1016/j.neuron.2008.12.012; Helfrich Kyle, 2018, ICML; Henaff M, 2016, PR MACH LEARN RES, V48; Hennequin G, 2012, PHYS REV E, V86, DOI 10.1103/PhysRevE.86.011909; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jiang Bo, 2015, MATH PROGRAM, V153, P1; Jing L., 2017, INT C MACHINE LEARNI, P1733; Jing L, 2019, NEURAL COMPUTATIONS; Lax PD, 2007, LINEAR ALGEBRA ITS A; Le Q.V., 2015, ABS150400941 CORR; Lezcano-Casado Mario, 2019, ICML; Li SY, 2018, PROC CVPR IEEE, P6526, DOI 10.1109/CVPR.2018.00683; Maduranga Kehelwala D G, 2019, AAAI; Merity Stephen, 2018, ARXIV180308240; Mhammedi Z, 2017, PR MACH LEARN RES, V70; Orhan A Emin, 2019, IMPROVED MEMORY RECU; Parcollet Titouan, 2019, INT C LEARN REPR; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Raghu M, 2017, PR MACH LEARN RES, V70; Tallec Corentin, 2018, ICLR; Vorontsov E, 2017, PR MACH LEARN RES, V70; Wisdom Scott, 2016, ADV NEURAL INFORM PR; Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173; Zhang J, 2018, PR MACH LEARN RES, V80	35	13	13	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905028
C	Kniaz, VV; Knyaz, VA; Remondino, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kniaz, Vladimir V.; Knyaz, Vladimir A.; Remondino, Fabio			The Point Where Reality Meets Fantasy: Mixed Adversarial Generators for Image Splice Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FORGERY DETECTION; LOCALIZATION	Modern photo editing tools allow creating realistic manipulated images easily. While fake images can be quickly generated, learning models for their detection is challenging due to the high variety of tampering artifacts and the lack of large labeled datasets of manipulated images. In this paper, we propose a new framework for training of discriminative segmentation model via an adversarial process. We simultaneously train four models: a generative retouching model G(R) that translates manipulated image to the real image domain, a generative annotation model G(A) that estimates the pixel-wise probability of image patch being either real or fake, and two discriminators D-R and D-A that qualify the output of G(R) and G(A). The aim of model G(R) is to maximize the probability of model G(A) making a mistake. Our method extends the generative adversarial networks framework with two main contributions: (1) training of a generative model G(R) against a deep semantic segmentation network G(A) that learns rich scene semantics for manipulated region detection, (2) proposing per class semantic loss that facilitates semantically consistent image retouching by the G(R). We collected large-scale manipulated image dataset to train our model. The dataset includes 16k real and fake images with pixel-level annotations of manipulated areas. The dataset also provides ground truth pixel-level object annotations. We validate our approach on several modern manipulated image datasets, where quantitative results and ablations demonstrate that our method achieves and surpasses the state-of-the-art in manipulated image detection. We made our code and dataset publicly available(1).	[Kniaz, Vladimir V.; Knyaz, Vladimir A.] State Res Inst Aviat Syst GosNIIAS, 7 Victorenko Str, Moscow 125319, Russia; [Kniaz, Vladimir V.; Knyaz, Vladimir A.] Moscow Inst Phys & Technol MIPT, 9 Inst Skiy Per, Dolgoprudnyi 141701, Russia; [Remondino, Fabio] Fde Bruno Kessler FBK, Via Sommar 18, Trento, Italy	Moscow Institute of Physics & Technology	Kniaz, VV (corresponding author), State Res Inst Aviat Syst GosNIIAS, 7 Victorenko Str, Moscow 125319, Russia.; Kniaz, VV (corresponding author), Moscow Inst Phys & Technol MIPT, 9 Inst Skiy Per, Dolgoprudnyi 141701, Russia.	vl.kniaz@gosniias.ru; knyaz@gosniias.ru; remondino@fbk.eu	Remondino, Fabio/C-5503-2018; Knyaz, Vladimir/GPS-7701-2022; Kniaz, Vladimir V./P-4300-2014; Knyaz, Vladimir/I-8817-2014	Remondino, Fabio/0000-0001-6097-5342; Kniaz, Vladimir V./0000-0003-2912-9986; Knyaz, Vladimir/0000-0002-4466-244X	Russian Science Foundation (RSF) [19-11-11008]; Russian Foundation for Basic Research (RFBR) [17-29-04509]	Russian Science Foundation (RSF)(Russian Science Foundation (RSF)); Russian Foundation for Basic Research (RFBR)(Russian Foundation for Basic Research (RFBR))	The reported study was funded by the Russian Science Foundation (RSF) according to the research project No 19-11-11008 and the Russian Foundation for Basic Research (RFBR) according to the research project No 17-29-04509. We want to thank Belgian Surrealist artist Rene Magritte for teaching us through his art how to find the point where fantasy meets reality.	AbdAlmageed Wael, 2019, IEEE C COMP VIS PATT; Amerini I., 2013, SIGNAL PROCESS-IMAGE, V28, P659, DOI DOI 10.1016/j.image.2013.03.006; Amodio Matthew, 2019, CORR; Bappy JH, 2017, IEEE I CONF COMP VIS, P4980, DOI 10.1109/ICCV.2017.532; Bayar Y, 2016, TRANSYLV REV ADM SCI, P5; Bo Liu, 2015, International Journal of Computer and Communication Engineering, V4, P33, DOI 10.7763/IJCCE.2015.V4.378; Bondi L, 2017, IEEE COMPUT SOC CONF, P1855, DOI 10.1109/CVPRW.2017.232; Bondi L, 2017, IEEE SIGNAL PROC LET, V24, P259, DOI 10.1109/LSP.2016.2641006; Chang IC, 2013, IMAGE VISION COMPUT, V31, P57, DOI 10.1016/j.imavis.2012.09.002; Cherian A, 2019, IEEE WINT CONF APPL, P1797, DOI 10.1109/WACV.2019.00196; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Crum WR, 2006, IEEE T MED IMAGING, V25, P1451, DOI 10.1109/TMI.2006.880587; de Carvalho TJ, 2013, IEEE T INF FOREN SEC, V8, P1182, DOI 10.1109/TIFS.2013.2265677; Ferrara P, 2012, IEEE T INF FOREN SEC, V7, P1566, DOI 10.1109/TIFS.2012.2202227; Goljan M, 2014, IEEE INT WORKS INFOR, P185, DOI 10.1109/WIFS.2014.7084325; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta A., 2013, INT J SCI RES PUBLIC, V3, P2250; Hu WC, 2016, MULTIMED TOOLS APPL, V75, P3495, DOI 10.1007/s11042-015-2449-0; Hu WC, 2013, INT J COMPUT SCI ENG, V8, P297; Hu WC, 2012, INT CONF GENET EVOL, P245, DOI 10.1109/ICGEC.2012.114; Huang Xun, 2018, ECCV; Huh M, 2018, LECT NOTES COMPUT SC, V11215, P106, DOI 10.1007/978-3-030-01252-6_7; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jing Dong, 2013, 2013 IEEE China Summit and International Conference on Signal and Information Processing (ChinaSIP), P422, DOI 10.1109/ChinaSIP.2013.6625374; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Julliand Thibaut, 2016, Digital Forensics and Watermarking. 14th International Workshop, IWDW 2015. Revised Selected Papers: LNCS 9569, P3, DOI 10.1007/978-3-319-31960-5_1; Kaur Harpreet, 2015, INT J ELECT ELECT CO, V4, P62; Kniaz VV, 2019, INT ARCH PHOTOGRAMM, V42-2, P111, DOI 10.5194/isprs-archives-XLII-2-W12-111-2019; Kniaz VV, 2019, INT ARCH PHOTOGRAMM, V42-2, P403, DOI 10.5194/isprs-archives-XLII-2-W9-403-2019; Kniaz VV, 2019, LECT NOTES COMPUT SC, V11134, P606, DOI 10.1007/978-3-030-11024-6_46; Kniaz VV, 2019, MULTIMODAL SCENE UNDERSTANDING: ALGORITHMS, APPLICATIONS AND DEEP LEARNING, P135, DOI 10.1016/B978-0-12-817358-9.00012-3; Knyaz VA, 2019, LECT NOTES COMPUT SC, V11129, P601, DOI 10.1007/978-3-030-11009-3_37; Korus P, 2017, IEEE T INF FOREN SEC, V12, P809, DOI 10.1109/TIFS.2016.2636089; Liu B, 2014, INT J AEROSPACE ENG, V2014, DOI 10.1155/2014/540235; Liu B, 2014, APPL MECH MATER, V556-562, P2825, DOI 10.4028/www.scientific.net/AMM.556-562.2825; Liu Ming-Yu, 2017, NIPS; Mahdian B, 2009, IMAGE VISION COMPUT, V27, P1497, DOI 10.1016/j.imavis.2009.02.001; Mejjati YA, 2018, ADV NEUR IN, V31; Ng Tian-Tsong, 2004, TECHNICAL REPORT; Pun CM, 2016, J VIS COMMUN IMAGE R, V38, P195, DOI 10.1016/j.jvcir.2016.03.005; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roy A., 2020, DIGITAL IMAGE FORENS, P65; Royer Amelie, 2018, XGAN UNSUPERVISED IM; Salloum R, 2018, J VIS COMMUN IMAGE R, V51, P201, DOI 10.1016/j.jvcir.2018.01.010; Sangkloy P, 2017, PROC CVPR IEEE, P6836, DOI 10.1109/CVPR.2017.723; Su Y. G., 2009, 2009 IEEE INT C IND, P1; Tripathy S., 2018, ARXIV180503189; Wan CH, 2019, INT CONF ACOUST SPEE, P496, DOI 10.1109/ICASSP.2019.8682383; Wang X, 2008, 2008 FOURTH INTERNATIONAL CONFERENCE ON INTELLIGENT INFORMATION HIDING AND MULTIMEDIA SIGNAL PROCESSING, PROCEEDINGS, P192, DOI 10.1109/IIH-MSP.2008.165; Wen BH, 2016, IEEE IMAGE PROC, P161, DOI 10.1109/ICIP.2016.7532339; Wen Longyin, 2018, ACM T MULTIM COMPUT, V14, P1; Wu WY, 2019, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2019.00820; Ye SM, 2007, 2007 IEEE INTERNATIONAL CONFERENCE ON MULTIMEDIA AND EXPO, VOLS 1-5, P12; Zeng F, 2015, 2015 10TH INTERNATIONAL CONFERENCE ON P2P, PARALLEL, GRID, CLOUD AND INTERNET COMPUTING (3PGCIC), P466, DOI 10.1109/3PGCIC.2015.50; Zhang H., 2017, ICCV; Zhang H, 2017, 2017 IEEE INTERNATIONAL JOINT CONFERENCE ON BIOMETRICS (IJCB), P100; Zhang T., 2017, TV GAN GENERATIVE AD; Zhou Peng, 2018, IEEE C COMP VIS PATT; Zhu Jun-Yan, 2017, ICCV	62	13	14	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32						215	226						12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300020
C	Li, B; Chen, CY; Wang, WL; Carin, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Bai; Chen, Changyou; Wang, Wenlin; Carin, Lawrence			Certified Adversarial Robustness with Additive Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The existence of adversarial data examples has drawn significant attention in the deep-learning community; such data are seemingly minimally perturbed relative to the original data, but lead to very different outputs from a deep-learning algorithm. Although a significant body of work on developing defensive models has been considered, most such models are heuristic and are often vulnerable to adaptive attacks. Defensive methods that provide theoretical robustness guarantees have been studied intensively, yet most fail to obtain non-trivial robustness when a large-scale model and data are present. To address these limitations, we introduce a framework that is scalable and provides certified bounds on the norm of the input manipulation for constructing adversarial examples. We establish a connection between robustness against adversarial perturbation and additive random noise, and propose a training strategy that can significantly improve the certified bounds. Our evaluation on MNIST, CIFAR-10 and ImageNet suggests that the proposed method is scalable to complicated models and large data sets, while providing competitive robustness to state-of-the-art provable defense methods.	[Li, Bai] Duke Univ, Dept Stat Sci, Durham, NC 27706 USA; [Chen, Changyou] SUNY Buffalo, Dept CSE, Buffalo, NY USA; [Wang, Wenlin; Carin, Lawrence] Duke Univ, Dept ECE, Durham, NC 27706 USA	Duke University; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; Duke University	Li, B (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27706 USA.	bai.li@duke.edu; cchangyou@gmail.com; wenlin.wang@duke.edu; lcarin@duke.edu			DARPA; DOE; NIH; NSF; ONR	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This research was supported in part by DARPA, DOE, NIH, NSF and ONR.	[Anonymous], 2020, INT C LEARN RE UNPUB; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Athalye A., 2018, P 35 INT C MACH LEAR; Bachman Philip, 2014, ADV NEURAL INFORM PR, P2; Brendel Wieland, 2017, DECISION BASED ADVER; Carlini N., 2019, CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini Nicholas, 2017, ARXIV171108478; Cohen Jeremy M, 2019, ARXIV190202918; DVIJOTHAM K, 2018, ARXIV180306567; Engstrom Logan, 2018, ARXIV180710272; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Ford N, 2019, PR MACH LEARN RES, V97; Goodfellow I., 2018, ARXIV180908352; Goodfellow I. J., 2014, ARXIV14126572; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hein M., 2017, ADV NEURAL INFORM PR, V30, P2266; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Junyuan X, 2012, ADV NEURAL INF PROCE, P341; Kannan Harini, 2018, ARXIV180306373; KOLTER JZ, 2017, ARXIV171100851; Kurakin A, 2016, INT C LEARN REPR SAN; Lecuyer M., 2018, ARXIV180203471; Li Bai, 2019, NORM AGNOSTIC ROBUST; Madry A., 2018, ARXIV PREPRINT ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Namkoong Hongseok, 2017, NEURIPS, P2971; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Raghunathan Aditi, 2018, INT C LEARN REPR; Sinha Aman, 2017, ARXIV PREPRINT ARXIV; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Tramer F., 2017, ARXIV170507204; van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500; Wong E., 2018, ARXIV180512514; Zantedeschi Valentina, 2017, P 10 ACM WORKSHOP AR, P39, DOI [10.1145/3128572.3140449, DOI 10.1145/3128572.3140449]; Zhang H., 2019, ARXIV190108573; Zhang Huan, 2018, ARXIV180409699; Zhang Huan, 2018, ADV NEURAL INFORM PR, P4944; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485	40	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901013
C	Liu, S; Davison, AJ; Johns, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Shikun; Davison, Andrew J.; Johns, Edward			Self-Supervised Generalisation with Meta Auxiliary Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning with auxiliary tasks can improve the ability of a primary task to generalise. However, this comes at the cost of manually labelling auxiliary data. We propose a new method which automatically learns appropriate labels for an auxiliary task, such that any supervised learning task can be improved without requiring access to any further data. The approach is to train two neural networks: a label-generation network to predict the auxiliary labels, and a multi-task network to train the primary task alongside the auxiliary task. The loss for the label-generation network incorporates the loss of the multi-task network, and so this interaction between the two networks can be seen as a form of meta learning with a double gradient. We show that our proposed method, Meta AuXiliary Learning (MAXL), outperforms single-task learning on 7 image datasets, without requiring any additional data. We also show that MAXL outperforms several other baselines for generating auxiliary labels, and is even competitive when compared with human-defined auxiliary labels. The self-supervised nature of our method leads to a promising new direction towards automated generalisation. Source code can be found at https://github.com/lorenmt/maxl.	[Liu, Shikun; Davison, Andrew J.; Johns, Edward] Imperial Coll London, Dept Comp, London, England	Imperial College London	Liu, S (corresponding author), Imperial Coll London, Dept Comp, London, England.	shikun.liu17@imperial.ac.uk; a.davison@imperial.ac.uk; e.johns@imperial.ac.uk		Johns, Edward/0000-0002-8914-8786				Agarwal Rishabh, 2019, P MACH LEARN RES, P130; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2017, 5 INT C LEARN REPR I; Bengio Samy, 1992, C OPT ART BIOL NEUR, P6; Bengio Y., 1990, LEARNING SYNAPTIC LE; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Darlow Luke Nicholas, 2018, CORR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Du Yunshu, 2018, ARXIV181202224; Finn C, 2017, PR MACH LEARN RES, V70; Flynn J, 2016, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2016.595; Goodfellow I. J., 2013, ARXIV13126082; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Jaderberg M, 2017, PR MACH LEARN RES, V70; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kokkinos I., 2017, P IEEE C COMP VIS PA, P6129; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Zhenguo, 2017, METASGD LEARNING LEA; Liebel Lukas, 2018, ARXIV180506334; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Liu SK, 2019, PROC CVPR IEEE, P1871, DOI 10.1109/CVPR.2019.00197; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Ravi S., 2016, OPTIMIZATION MODEL F; Ruder S., 2017, PREPRINT; Santoro A, 2016, PR MACH LEARN RES, V48; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Soomro K., 2012, COMPUT SCI; Toshniwal S, 2017, INTERSPEECH, P3532, DOI 10.21437/Interspeech.2017-1118; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinyals Oriol, 2016, ARXIV160604080, P3630; Zhang Y., 2018, P EUR C COMP VIS ECC, P233; Zhou T., 2017, P IEEE C COMP VIS PA, P1851	36	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301064
C	Maalouf, A; Jubran, I; Feldman, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maalouf, Alaa; Jubran, Ibrahim; Feldman, Dan			Fast and Accurate Least-Mean-Squares Solvers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGRESSION; SELECTION	Least-mean squares (LMS) solvers such as Linear / Ridge / Lasso-Regression, SVD and Elastic-Net not only solve fundamental machine learning problems, but are also the building blocks in a variety of other methods, such as decision trees and matrix factorizations. We suggest an algorithm that gets a finite set of n d-dimensional real vectors and returns a weighted subset of d + 1 vectors whose sum is exactly the same. The proof in Caratheodory's Theorem (1907) computes such a subset in O(n(2)d(2)) time and thus not used in practice. Our algorithm computes this subset in O(nd) time, using O(log n) calls to Caratheodory's construction on small but "smart" subsets. This is based on a novel paradigm of fusion between different data summarization techniques, known as sketches and coresets. As an example application, we show how it can be used to boost the performance of existing LMS solvers, such as those in scikit-learn library, up to x100. Generalization for streaming and distributed (big) data is trivial. Extensive experimental results and complete open source code are also provided.	[Maalouf, Alaa; Jubran, Ibrahim; Feldman, Dan] Univ Haifa, Robot & Big Data Lab, Dept Comp Sci, Haifa, Israel	University of Haifa	Maalouf, A (corresponding author), Univ Haifa, Robot & Big Data Lab, Dept Comp Sci, Haifa, Israel.	Alaamalouf12@gmail.com; ibrahim.jub@gmail.com; dannyf.post@gmail.com						Afrabandpey Homayun, 2016, FILM 2016; Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736; Alaa Maalouf, 2019, SUPPLEMENTARY MAT; [Anonymous], 2011, PRINCIPAL COMPONENT; Bauckhage Christian, 2015, RESEARCHGATE NET MAR; Bj orck A., 1967, BIT, V7, P1, DOI DOI 10.1007/BF01934122; Caratheodory C, 1907, MATH ANN, V64, P95, DOI 10.1007/BF01449883; Clarkson KL, 2017, J ACM, V63, DOI 10.1145/3019134; Clarkson KL, 2009, ACM S THEORY COMPUT, P205; COOK WD, 1972, CAN MATH BULLETIN, V15, P293, DOI 10.4153/CMB-1972-053-6; COPAS JB, 1983, J R STAT SOC B, V45, P311; Cormen Thomas H, 2009, INTRO ALGORITHMS; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Feldman D., 2016, ADV NEURAL INFORM PR; Feldman D, 2010, PROC APPL MATH, V135, P630; Gallagher Neil, 2017, ADV NEURAL INFORM PR, P6842; Golub G.H., 1971, HDB AUTOMATIC COMPUT, V186, P134, DOI [DOI 10.1007/978-3-662-39778-7_10, 10.1007/978- 3- 642-86940-2 10, DOI 10.1007/978-3-642-86940-210]; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Intel LTD, 2019, ACC PYTH PERF; Jubran I., 2019, ARXIV191008707; Jubran Ibrahim, 2019, ARXIV190210407; Kang Byung, 2011, P NIPS; Kaul M, 2013, 2013 IEEE 14TH INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT (MDM 2013), VOL 1, P137, DOI 10.1109/MDM.2013.24; Kohavi R., 1995, IJCAI-95. Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, P1137; Laparra V, 2015, IEEE J-STSP, V9, P1026, DOI 10.1109/JSTSP.2015.2417833; Liang Y., 2013, BIG LEARN WORKSH NIP, V2013; Maalouf A., 2019, ARXIV190701433; Maalouf A., 2019, OPEN SOURCE CODE ALL; Nasser S., 2015, ARXIV151109120; Pearson K, 1900, PHILOS MAG, V50, P157, DOI 10.1080/14786440009463897; Phillips J. M., 2016, ARXIV160100617; Porco Aldo, 2015, WORKSH NETW SOC INF; Rohe K, 2018, ADV NEURAL INFORM PR, P10631; SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458; Seber G. A., 2012, LINEAR REGRESSION AN, V329; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Xi Peng, 2015, 29 AAAI C ART INT; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	39	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308034
C	Nandwani, Y; Pathak, A; Mausam; Singla, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nandwani, Yatin; Pathak, Abhishek; Mausam; Singla, Parag			A Primal-Dual Formulation for Deep Learning with Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE	For several problems of interest, there are natural constraints which exist over the output label space. For example, for the joint task of NER and POS labeling, these constraints might specify that the NER label 'organization' is consistent only with the POS labels 'noun' and 'preposition'. These constraints can be a great way of injecting prior knowledge into a deep learning model, thereby improving overall performance. In this paper, we present a constrained optimization formulation for training a deep network with a given set of hard constraints on output labels. Our novel approach first converts the label constraints into soft logic constraints over probability distributions outputted by the network. It then converts the constrained optimization problem into an alternating min-max optimization with Lagrangian variables defined for each constraint. Since the constraints are independent of the target labels, our framework easily generalizes to semi-supervised setting. We experiment on the tasks of Semantic Role Labeling (SRL), Named Entity Recognition (NER) tagging, and fine-grained entity typing and show that our constraints not only significantly reduce the number of constraint violations, but can also result in state-of-the-art performance.	[Nandwani, Yatin; Pathak, Abhishek; Mausam; Singla, Parag] Indian Inst Technol Delhi, Dept Comp Sci & Engn, New Delhi, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Nandwani, Y (corresponding author), Indian Inst Technol Delhi, Dept Comp Sci & Engn, New Delhi, India.	yatin.nandwani@cse.iitd.ac.in; abhishek.pathak.cs115@cse.iitd.ac.in; mausam@cse.iitd.ac.in; parags@cse.iitd.ac.in		Mausam, ./0000-0003-4088-4296	Google; Bloomberg; 1MG; DARPA Explainable Artificial Intelligence (XAI) Program [N66001-17-2-4032]; Visvesvaraya Young Faculty Fellowships by Govt. of India; IBM SUR awards	Google(Google Incorporated); Bloomberg; 1MG; DARPA Explainable Artificial Intelligence (XAI) Program; Visvesvaraya Young Faculty Fellowships by Govt. of India; IBM SUR awards(International Business Machines (IBM))	We thank IIT Delhi HPC facility<SUP>7</SUP> for computational resources, which allows us to run experiments at large scale. We thank Guy Van den Broeck, Yitao Liang, Sanket Mehta, Shikhar Murty, Dan Roth, Alexander Rush and Vivek Srikumar for useful discussions on the work. We also thank Deepanshu Jindal for proofreading our code. Mausam is supported by grants from Google, Bloomberg and 1MG. Parag Singla is supported by the DARPA Explainable Artificial Intelligence (XAI) Program with number N66001-17-2-4032. Both Mausam and Parag Singla are supported by the Visvesvaraya Young Faculty Fellowships by Govt. of India and IBM SUR awards. Any opinions, findings, conclusions or recommendations expressed in this paper are those of the authors and do not necessarily reflect the views or official policies, either expressed or implied, of the funding agencies.	Bos Johan, 2017, HDB LINGUISTIC ANNOT; Brocheler M., 2010, P 26 C UNCERTAINTY A, P73; Chang Kai-Wei, 2013, P 2013 C EMPIRICAL M, P601; Chen J., 2013, ARXIV13116091; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Diligenti M, 2017, 2017 16TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P920, DOI 10.1109/ICMLA.2017.00-37; Evans C., 2008, P 2008 ACM SIGMOD IN, P1247, DOI [DOI 10.1145/1376616.1376746, 10.1145/1376616]; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Hu Zhiting, 2016, P 54 ANN M ASS COMP, V1; Huang Z., 2015, CORR; Knibelreiter Patrick, 2017, CVPR; Koller D., 2009, PROBABILISTIC GRAPHI; Lample G., 2016, C N AM CHAPTER ASS C, P260, DOI [10.18653/v1/N16-1030, 10.18653/v1/n16-1030, DOI 10.18653/V1/N16-1030]; Lee JY, 2019, AAAI CONF ARTIF INTE, P4147; Mdrquez-Neila Pablo, 2017, CORR; Mehta Sanket Vaibhav, 2018, P 2018 C EMP METH NA, P4958; MILLER GA, 1995, COMMUN ACM, V38, P39, DOI 10.1145/219717.219748; Murty S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P97, DOI 10.18653/v1/P18-1010; Murty Shikhar, 2017, ARXIV171105795; Novak Vilem, 1987, STUD LOGICA, V46, P87; Pradhan Sameer, 2012, JOINT C EMNLP CONLL; Punyakanok V, 2008, COMPUT LINGUIST, V34, P257, DOI 10.1162/coli.2008.34.2.257; Roth D., 2005, P 22 INT C MACH LEAR, P736, DOI DOI 10.1145/1102351.1102444; Rush AM, 2012, J ARTIF INTELL RES, V45, P305, DOI 10.1613/jair.3680; Verga Patrick, 2017, P 15 C EUR CHAPT ASS, V1, P613; Xu JY, 2018, PR MACH LEARN RES, V80; Yao Limin, 2013, P 2013 WORKSH AUT KN, P79	29	13	13	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903076
C	Rajput, S; Wang, HY; Charles, Z; Papailiopoulos, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rajput, Shashank; Wang, Hongyi; Charles, Zachary; Papailiopoulos, Dimitris			DETOX: A Redundancy-based Framework for Faster and More Robust Gradient Aggregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					To improve the resilience of distributed training to worst-case, or Byzantine node failures, several recent approaches have replaced gradient averaging with robust aggregation methods. Such techniques can have high computational costs, often quadratic in the number of compute nodes, and only have limited robustness guarantees. Other methods have instead used redundancy to guarantee robustness, but can only tolerate limited number of Byzantine failures. In this work, we present DETOX, a Byzantine-resilient distributed training framework that combines algorithmic redundancy with robust aggregation. DETOX operates in two steps, a filtering step that uses limited redundancy to significantly reduce the effect of Byzantine nodes, and a hierarchical aggregation step that can be used in tandem with any state-of-the-art robust aggregation method. We show theoretically that this leads to a substantial increase in robustness, and has a per iteration runtime that can be nearly linear in the number of compute nodes. We provide extensive experiments over real distributed setups across a variety of large-scale machine learning tasks, showing that DETOX leads to orders of magnitude accuracy and speedup improvements over many state-of-the-art Byzantine-resilient approaches.	[Rajput, Shashank; Wang, Hongyi; Charles, Zachary; Papailiopoulos, Dimitris] Univ Wisconsin Madison, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Rajput, S (corresponding author), Univ Wisconsin Madison, Madison, WI 53706 USA.	rajput3@wisc.edu; hongyiwang@cs.wisc.edu; zcharles@math.wisc.edu; dimitris@papail.io			NSF CAREER Award [1844951]; Sony Faculty Innovation Award; AFOSR & AFRL Center of Excellence Award [FA9550-18-1-0166]; NSF TRIPODS Award [1740707]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sony Faculty Innovation Award; AFOSR & AFRL Center of Excellence Award; NSF TRIPODS Award	This research is supported by an NSF CAREER Award #1844951, a Sony Faculty Innovation Award, an AFOSR & AFRL Center of Excellence Award FA9550-18-1-0166, and an NSF TRIPODS Award #1740707. The authors also thank Ankit Pensia for useful discussions about the Median of Means approach.	Alistarh Dan, 2018, P ADV NEURAL INF PRO, V31, P4618; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baruch M., 2019, ARXIV190206156; Bernstein J., 2018, SIGNSGD MAJORITY VOT; Blanchard P., 2017, ADV NEURAL INFORM PR, P118; Blanchard P., 2017, ADV NEURAL INFORM PR, V30, P119; Dalcin LD, 2011, ADV WATER RESOUR, V34, P1124, DOI 10.1016/j.advwatres.2011.04.013; Damaskinos Georgios, 2019, C SYST MACH LEARN; Damaskinos Georgios, 2019, SYSML; Data D, 2018, ANN ALLERTON CONF, P863, DOI 10.1109/ALLERTON.2018.8636017; El-Mhamdi E., 2019, ARXIV190504374; El-Mhamdi E.-M., 2019, ARXIV190503853; ElMhamdi El Mahdi, 2018, ARXIV180207927; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176; Linial N., 2014, ARXIV14037739; Lugosi G, 2019, ANN STAT, V47, P783, DOI 10.1214/17-AOS1639; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pelekis C, 2017, MEDITERR J MATH, V14, DOI 10.1007/s00009-017-1043-2; Xie C., 2018, ARXIV180510032; Xie C., 2018, ARXIV180210116; Xie Cong, 2019, ARXIV190303936; Yin D, 2018, PR MACH LEARN RES, V80; Yin Dong, 2018, CORR; Yu Q., 2018, NIPS SYST ML WORKSH; Yudong Chen, 2017, Proceedings of the ACM on Measurement and Analysis of Computing Systems, V1, DOI 10.1145/3154503; Zhang L, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SMART CITY AND SYSTEMS ENGINEERING (ICSCSE), P902, DOI 10.1109/ICSCSE.2018.00194	28	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901090
C	Tan, YC; Celis, LE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tan, Yi Chern; Celis, L. Elisa			Assessing Social and Intersectional Biases in Contextualized Word Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Social bias in machine learning has drawn significant attention, with work ranging from demonstrations of bias in a multitude of applications, curating definitions of fairness for different contexts, to developing algorithms to mitigate bias. In natural language processing, gender bias has been shown to exist in context-free word embeddings. Recently, contextual word representations have outperformed word embeddings in several downstream NLP tasks. These word representations are conditioned on their context within a sentence, and can also be used to encode the entire sentence. In this paper, we analyze the extent to which state-of-the-art models for contextual word representations, such as BERT and GPT-2, encode biases with respect to gender, race, and intersectional identities. Towards this, we propose assessing bias at the contextual word level. This novel approach captures the contextual effects of bias missing in context-free word embeddings, yet avoids confounding effects that underestimate bias at the sentence encoding level. We demonstrate evidence of bias at the corpus level, find varying evidence of bias in embedding association tests, show in particular that racial bias is strongly encoded in contextual word models, and observe that bias effects for intersectional minorities are exacerbated beyond their constituent minority identities. Further, evaluating bias effects at the contextual word level captures biases that are not captured at the sentence level, confirming the need for our novel approach.	[Tan, Yi Chern; Celis, L. Elisa] Yale Univ, New Haven, CT 06520 USA	Yale University	Tan, YC (corresponding author), Yale Univ, New Haven, CT 06520 USA.	yichern.tan@yale.edu; elisa.celis@yale.edu						Basta Christine, 2019, ABS190408783 CORR; Bolukbasi Tolga, 2016, ABS160606121 CORR; Brunet ME, 2019, PR MACH LEARN RES, V97; Caliskan A, 2017, SCIENCE, V356, DOI 10.1126/science.aal4230; Celis LE, 2019, FAT*'19: PROCEEDINGS OF THE 2019 CONFERENCE ON FAIRNESS, ACCOUNTABILITY, AND TRANSPARENCY, P319, DOI 10.1145/3287560.3287586; Chelba Ciprian, 2014, 15 ANN C INT SPEECH, DOI DOI 10.21437/INTERSPEECH.2014-564; Collins P.H., 2005, BLACK SEXUAL POLITIC; Crenshaw KimberlU., 1991, STANFORD LAW REV, V43, DOI 10.2307/1229039; Devkota P, 2018, MAINTENANCE, SAFETY, RISK, MANAGEMENT AND LIFE-CYCLE PERFORMANCE OF BRIDGES, P2799; Devlin J., 2019, P 2019 C N AM CHAPTE, P4171, DOI [10.18653/v1/n19-1423, DOI 10.18653/V1/N19-1423]; Elliott SJ, 2018, GEOGR HEALTH, P10; Gebru Timnit, 2018, 5 WORKSH FAIRN ACC T; Gonen Hila, 2019, P 2019 C N AM CHAPT, V1, P609; Greenwald AG, 1998, J PERS SOC PSYCHOL, V74, P1464, DOI 10.1037/0022-3514.74.6.1464; Heilman ME, 2004, J APPL PSYCHOL, V89, P416, DOI 10.1037/0021-9010.89.3.416; Hendricks LA, 2018, LECT NOTES COMPUT SC, V11207, P793, DOI 10.1007/978-3-030-01219-9_47; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Kiritchenko Svetlana, 2018, P 7 JOINT C LEX COMP, P43, DOI DOI 10.18653/V1/S18-2005; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Manzini T., 2019, ACL, V1, P615, DOI DOI 10.18653/V1/N19-1062; May Chandler, 2019, P 2019 C N AM CHAPT, V1, P622, DOI DOI 10.18653/V1/N19-1063; Mikolov T, 2013, P 26 INT C NEURAL IN, P3111; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Rudinger Rachel, 2017, P 1 ACL WORKSH ETH N, P74, DOI [10.18653/v1/W17-1609, DOI 10.18653/V1/W17-1609]; Sweeney L, 2013, COMMUN ACM, V56, P44, DOI 10.1145/2447976.2447990; Thelwall M, 2018, ONLINE INFORM REV, V42, P45, DOI 10.1108/OIR-05-2017-0139; Verma S, 2018, 2018 IEEE/ACM INTERNATIONAL WORKSHOP ON SOFTWARE FAIRNESS (FAIRWARE 2018), P1, DOI 10.1145/3194770.3194776; Zhao Jieyu, 2019, P 2019 C N AM CHAPT, P629, DOI DOI 10.18653/V1/N19-1064; Zhao Jieyu, 2017, P 2017 C EMP METH NA, P2941, DOI [10.18653/v1/D17-1323, DOI 10.18653/V1/D17-1323]; Zhao Jieyu, 2018, P 2018 C EMP METH NA, P4847, DOI DOI 10.18653/V1/D18-1521; Zhao Jieyu, 2018, ARXIV180406876; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	36	13	13	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904082
C	Zhang, C; Jia, BX; Gao, F; Zhu, YX; Lu, HJ; Zhu, SC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Chi; Jia, Baoxiong; Gao, Feng; Zhu, Yixin; Lu, Hongjing; Zhu, Song-Chun			Learning Perceptual Inference by Contrasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STRUCTURAL ALIGNMENT; DISCRIMINATION; INTELLIGENCE	"Thinking in pictures," [1] i.e., spatial-temporal reasoning, effortless and instantaneous for humans, is believed to be a significant ability to perform logical induction and a crucial factor in the intellectual history of technology development. Modern Artificial Intelligence (AI), fueled by massive datasets, deeper models, and mighty computation, has come to a stage where (super-)human-level performances are observed in certain specific tasks. However, current AI's ability in "thinking in pictures" is still far lacking behind. In this work, we study how to improve machines' reasoning ability on one challenging task of this kind: Raven's Progressive Matrices (RPM). Specifically, we borrow the very idea of "contrast effects" from the field of psychology, cognition, and education to design and train a permutation-invariant model. Inspired by cognitive studies, we equip our model with a simple inference module that is jointly trained with the perception backbone. Combining all the elements, we propose the Contrastive Perceptual Inference network (CoPINet) and empirically demonstrate that CoPINet sets the new state-of-the-art for permutation-invariant models on two major datasets. We conclude that spatial-temporal reasoning depends on envisaging the possibilities consistent with the relations between objects and can be solved from pixel-level inputs.	[Zhang, Chi; Jia, Baoxiong; Zhu, Song-Chun] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA; [Lu, Hongjing] Univ Calif Los Angeles, Dept Psychol, Los Angeles, CA USA; [Gao, Feng; Zhu, Yixin; Zhu, Song-Chun] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA USA; [Zhang, Chi; Gao, Feng; Zhu, Yixin; Zhu, Song-Chun] Int Ctr AI & Robot Auton CARA, Los Angeles, CA USA	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; University of California System; University of California Los Angeles	Zhang, C (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.; Zhang, C (corresponding author), Int Ctr AI & Robot Auton CARA, Los Angeles, CA USA.	chi.zhang@ucla.edu; baoxiongjia@ucla.edu; f.gao@ucla.edu; yixin.zhu@ucla.edu; hongjing@ucla.edu; sczhu@ucla.edu			MURI ONR [N00014-16-1-2007]; DARPA [XAI N66001-17-2-4029]; ONR [N00014-19-1-2153]; NSF [BSC-1827374]; NVIDIA GPU donation grant	MURI ONR(MURIOffice of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); NVIDIA GPU donation grant	This work reported herein is supported by MURI ONR N00014-16-1-2007, DARPA XAI N66001-17-2-4029, ONR N00014-19-1-2153, NSF BSC-1827374, and an NVIDIA GPU donation grant.	AMSEL A, 1962, PSYCHOL REV, V69, P306, DOI 10.1037/h0046200; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arnheim R., 1969, VISUAL THINKING; Barrett David, 2018, P INT C MACH LEARN I; BOWER GH, 1961, J EXP PSYCHOL, V62, P196, DOI 10.1037/h0048109; CARPENTER PA, 1990, PSYCHOL REV, V97, P404, DOI 10.1037/0033-295X.97.3.404; CATRAMBONE R, 1989, J EXP PSYCHOL LEARN, V15, P1147, DOI 10.1037/0278-7393.15.6.1147; Chalmers D. J., 1992, Journal of Experimental and Theoretical Artificial Intelligence, V4, P185, DOI 10.1080/09528139208953747; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Dai B, 2017, ADV NEUR IN, V30; Galton Francis, 1883, INQUIRIES HUMAN FACU, DOI DOI 10.1037/14178-000; GENTNER D, 1983, COGNITIVE SCI, V7, P155, DOI 10.1207/s15516709cog0702_3; GENTNER D, 1994, PSYCHOL SCI, V5, P152, DOI 10.1111/j.1467-9280.1994.tb00652.x; Gentner D, 2001, MEM COGNITION, V29, P565, DOI 10.3758/BF03200458; Gibson J.J., 1979, ECOLOGICAL APPROACH; GIBSON JJ, 1955, PSYCHOL REV, V62, P32, DOI 10.1037/h0048826; GICK ML, 1992, CAN J PSYCHOL, V46, P539, DOI 10.1037/h0084333; Grandin Temple., 2006, THINKING PICTURES OT; Gutmann Michael, 2010, P 13 INT C ART INT S; Hammer R, 2009, COGNITION, V112, P105, DOI 10.1016/j.cognition.2009.03.012; Haryu E, 2011, CHILD DEV, V82, P674, DOI 10.1111/j.1467-8624.2010.01567.x; hase Catherine C, 2010, P 9 INT C LEARN SCI; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hill Felix, 2019, ARXIV190200120; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hofstadter D.R., 1995, FLUID CONCEPTS CREAT; Hoshen D., 2017, ARXIV PREPRINT ARXIV; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Jaeggi SM, 2008, P NATL ACAD SCI USA, V105, P6829, DOI 10.1073/pnas.0801268105; Jang E., 2016, ARXIV; Johnson J, 2017, IEEE I CONF COMP VIS, P3008, DOI 10.1109/ICCV.2017.325; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LAWSON R, 1957, J COMP PHYSIOL PSYCH, V50, P35, DOI 10.1037/h0044896; Little Daniel R, 2012, P ANN M COGN SCI SOC; Lovett A., 2010, P ANN M COGN SCI SOC, V32, P185; Lovett A, 2017, PSYCHOL REV, V124, P60, DOI 10.1037/rev0000039; Lovett A, 2009, COGNITIVE SCI, V33, P1192, DOI 10.1111/j.1551-6709.2009.01052.x; Maddison Chris J, 2016, ARXIV161100712; McGreggor K, 2014, ARTIF INTELL, V215, P1, DOI 10.1016/j.artint.2014.05.005; McGreggor Keith, 2014, P AAAI C ART INT AAA; Mekik CS, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1576; MEYER DR, 1951, J EXP PSYCHOL, V41, P268, DOI 10.1037/h0055149; Mikolov T., 2013, ARXIV; Mitchell M., 1993, ANALOGY MAKING PERCE; Park Seyoung, 2015, P INT C COMP VIS ICC; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Raven J. C., 1936, THESIS; Raven JC., 1998, RAVENS PROGRESSIVE M, V759; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Santoro A, 2017, ADV NEUR IN, V30; SCHRIER AM, 1956, J COMP PHYSIOL PSYCH, V49, P117, DOI 10.1037/h0041746; Schwartz DL, 2011, J EDUC PSYCHOL, V103, P759, DOI 10.1037/a0025140; Schwartz DL, 2004, COGNITION INSTRUCT, V22, P129, DOI 10.1207/s1532690xci2202_1; SHAPLEY RM, 1978, J PHYSIOL-LONDON, V285, P275, DOI 10.1113/jphysiol.1978.sp012571; Shegheva Snejana, 2018, P AAAI C ART INT AAA; Smith Linsey, 2014, P ANN M COGN SCI SOC; Smith Noah A, 2005, P ANN M ASS COMP LIN; Snow R., 1984, ADV PSYCHOL HUMAN IN, P47; Spearman C., 1923, NATURE INTELLIGENCE; Spearman C., 1927, ABILITIES MAN THEIR; Steenbrugge Xander, 2018, P 32 C NEUR INF PROC; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wang K, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P903; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Wu Tian-Fu, 2007, P IEEE C COMP VIS PA; Wu YN, 2018, ANN MATH SCI APPL, V3, P211; Wu YN, 2010, INT J COMPUT VISION, V90, P198, DOI 10.1007/s11263-009-0287-0; Xie JW, 2016, PR MACH LEARN RES, V48; Zhang C, 2019, PROC CVPR IEEE, P5312, DOI 10.1109/CVPR.2019.00546; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; Zhu SC, 2006, FOUND TRENDS COMPUT, V2, P259, DOI 10.1561/0600000018	77	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301011
C	Zhou, S; Gordon, ML; Krishna, R; Narcomey, A; Li, FF; Bernstein, MS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Sharon; Gordon, Mitchell L.; Krishna, Ranjay; Narcomey, Austin; Li Fei-Fei; Bernstein, Michael S.			HYPE: A Benchmark for Human eYe Perceptual Evaluation of Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PSYCHOMETRIC FUNCTION; PERFORMANCE	Generative models often use human evaluations to measure the perceived quality of their outputs. Automated metrics are noisy indirect proxies, because they rely on heuristics or pretrained embeddings. However, up until now, direct human evaluation strategies have been ad-hoc, neither standardized nor validated. Our work establishes a gold standard human benchmark for generative realism. We construct HUMAN EYE PERCEPTUAL EVALUATION (HYPE), a human benchmark that is (1) grounded in psychophysics research in perception, (2) reliable across different sets of randomly sampled outputs from a model, (3) able to produce separable model performances, and (4) efficient in cost and time. We introduce two variants: one that measures visual perception under adaptive time constraints to determine the threshold at which a model's outputs appear real (e.g. 250ms), and the other a less expensive variant that measures human error rate on fake and real images sans time constraints. We test HYPE across six state-of-the-art generative adversarial networks and two sampling techniques on conditional and unconditional image generation using four datasets: CelebA, FFHQ, CIFAR-10, and ImageNet. We find that HYPE can track the relative improvements between models, and we confirm via bootstrap sampling that these measurements are consistent and replicable. [GRAPHICS] .	[Zhou, Sharon; Gordon, Mitchell L.; Krishna, Ranjay; Narcomey, Austin; Li Fei-Fei; Bernstein, Michael S.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Zhou, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	sharonz@cs.stanford.edu; mgord@cs.stanford.edu; ranjaykrishna@cs.stanford.edu; aon2@cs.stanford.edu; feifeili@cs.stanford.edu; msb@cs.stanford.edu		Bernstein, Michael/0000-0001-8020-9434	Junglee Corporation Stanford Graduate Fellowship; Alfred P. Sloan fellowship; Toyota Research Institute ("TRI")	Junglee Corporation Stanford Graduate Fellowship; Alfred P. Sloan fellowship(Alfred P. Sloan Foundation); Toyota Research Institute ("TRI")	We thank Kamyar Azizzadenesheli, Tatsu Hashimoto, and Maneesh Agrawala for insightful conversations and support. We also thank Durim Morina and Gabby Wright for their contributions to the HYPE system and website. M.L.G. was supported by a Junglee Corporation Stanford Graduate Fellowship. This work was supported in part by an Alfred P. Sloan fellowship. Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.	Barratt S., 2018, ARXIV180101973; Bernstein M.S., 2011, P 24 ANN ACM S US IN, P33, DOI DOI 10.1145/2047196.2047201; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Bishop CM, 2006, PATTERN RECOGNITION; Biswas P, 2005, I CONF VLSI DESIGN, P651; Borji Ali, 2018, COMPUTER VISION IMAG; Brock Andrew, 2018, ARXIV180911096; Chellappa R, 2010, COMPUTER, V43, P46, DOI 10.1109/MC.2010.37; Cornsweet Tom N, 1962, STAIRCRASE METHOD PS; Dakin SC, 2009, VISION RES, V49, P2285, DOI 10.1016/j.visres.2009.06.016; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Denton Emily L, 2015, NEURIPS, V2, P4; diaeresis>ossler Andreas R <spacing, 2019, FACEFORENSICS LEARNI; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fei-Fei L, 2007, J VISION, V7, DOI 10.1167/7.1.10; FELSENSTEIN J, 1985, EVOLUTION, V39, P783, DOI 10.1111/j.1558-5646.1985.tb00420.x; FRAISSE P, 1984, ANNU REV PSYCHOL, V35, P1, DOI 10.1146/annurev.ps.35.020184.000245; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greene MR, 2009, PSYCHOL SCI, V20, P464, DOI 10.1111/j.1467-9280.2009.02316.x; Gulrajani I, 2017, P NIPS 2017; Hashimoto TB, 2019, P 2019 C N AM CHAPT, V1, P1689, DOI DOI 10.18653/V1/N19-1169; Hata K, 2017, CSCW'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK AND SOCIAL COMPUTING, P889, DOI 10.1145/299818.2998248; Hensel M, 2017, ADV NEUR IN, V30; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Karras T., 2017, PROGR GROWING GANS I; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453; Klein SA, 2001, PERCEPT PSYCHOPHYS, V63, P1421, DOI 10.3758/BF03194552; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Krishna R, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3167, DOI 10.1145/2858036.2858115; Krizhevsky A, 2009, LEARNING MULTIPLE LA; KRUEGER GP, 1989, WORK STRESS, V3, P129, DOI 10.1080/02678378908256939; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Le J., 2010, PROC WORKSHOP CROWDS, V2126, P22; LEVITT H, 1971, J ACOUST SOC AM, V49, P467, DOI 10.1121/1.1912375; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mani Inderjeet, 1999, ADV AUTOMATIC TEXT S; Mitra T, 2015, CHI 2015: PROCEEDINGS OF THE 33RD ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, P1345, DOI 10.1145/2702123.2702553; Olsson C, 2018, ARXIV180804888; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ravuri S, 2018, PR MACH LEARN RES, V80; Rayner K, 2009, PSYCHOL SCI, V20, P6, DOI 10.1111/j.1467-9280.2008.02243.x; Rosca Mihaela, 2017, ARXIV170604987; Rzeszotarski  J.M., 2013, 1 AAAI C HUM COMP CR; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T, 2016, ADV NEUR IN, V29; SPERLING G, 1963, HUM FACTORS, V5, P19, DOI 10.1177/001872086300500103; Sutherland Danica J, 2018, ICLR; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Theis Lucas, 2015, ARXIV151101844; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Warde-Farley David, 2016, IMPROVING GENERATIVE; Weld DS, 2015, HANDBOOK OF COLLECTIVE INTELLIGENCE, P89; Weld Daniel S, 2016, P 2016 C N AM CHAPT, P897; Wichmann FA, 2001, PERCEPT PSYCHOPHYS, V63, P1293, DOI 10.3758/BF03194544	60	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303044
C	Asadi, AR; Abbe, E; Verdu, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Asadi, Amir R.; Abbe, Emmanuel; Verdu, Sergio			Chaining Mutual Information and Tightening Generalization Bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Bounding the generalization error of learning algorithms has a long history, which yet falls short in explaining various generalization successes including those of deep learning. Two important difficulties are (i) exploiting the dependencies between the hypotheses, (ii) exploiting the dependence between the algorithm's input and output. Progress on the first point was made with the chaining method, originating from the work of Kolmogorov, and used in the VC-dimension bound. More recently, progress on the second point was made with the mutual information method by Russo and Zou '15. Yet, these two methods are currently disjoint. In this paper, we introduce a technique to combine the chaining and mutual information methods, to obtain a generalization bound that is both algorithm-dependent and that exploits the dependencies between the hypotheses. We provide an example in which our bound significantly outperforms both the chaining and the mutual information bounds. As a corollary, we tighten Dudley's inequality when the learning algorithm chooses its output from a small subset of hypotheses with high probability.	[Asadi, Amir R.; Abbe, Emmanuel] Princeton Univ, Princeton, NJ 08544 USA; [Abbe, Emmanuel] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Princeton University; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Asadi, AR (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	aasadi@princeton.edu			NSF CAREER Award [CCF-1552131]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We gratefully acknowledge discussions with Ramon van Handel on the topic of chaining. This work was partly supported by the NSF CAREER Award CCF-1552131.	[Anonymous], 2017, ADV NEURAL INFORM PR; Audibert JY, 2007, J MACH LEARN RES, V8, P863; Audibert JY, 2004, ADV NEUR IN, V16, P1125; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Bassily Raef, 2017, ARXIV171005233; Belkin M., 2018, ARXIV180201396; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; DUDLEY R.M, 1967, J FUNCT ANAL, V1, P290, DOI DOI 10.1016/0022-1236(67)90017-1; Fernique Xavier, 1976, PROBABILITY BANACH S, P67; Jiao J., 2017, ARXIV170809041; Jiao JT, 2017, IEEE INT SYMP INFO, P1475, DOI 10.1109/ISIT.2017.8006774; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Littlestone N., 1986, RELATING DATA COMPRE; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Pensia Ankit, 2018, ARXIV180104295; Russo D., 2015, ARXIV151105219; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Talagrand Michel, 2014, UPPER LOWER BOUNDS S, V60; van Handel Ramon, 2016, PROBABILITY HIGH DIM; Vershynin R, 2018, HIGH DIMENSIONAL PRO; Zhang Chiyuan, 2017, INT C LEARN REPR ICL	24	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001076
C	Aubin, B; Maillard, A; Barbier, J; Krzakala, F; Macris, N; Zdeborova, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent; Macris, Nicolas; Zdeborova, Lenka			The committee machine: Computational to statistical gaps in learning a two-layers neural network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MESSAGE-PASSING ALGORITHMS; SPACE	Heuristic tools from statistical physics have been used in the past to locate the phase transitions and compute the optimal learning and generalization errors in the teacher-student scenario in multi-layer neural networks. In this contribution, we provide a rigorous justification of these approaches for a two-layers neural network model called the committee machine. We also introduce a version of the approximate message passing (AMP) algorithm for the committee machine that allows to perform optimal learning in polynomial time for a large set of parameters. We find that there are regimes in which a low generalization error is information-theoretically achievable while the AMP algorithm fails to deliver it; strongly suggesting that no efficient algorithm exists for those cases, and unveiling a large computational gap.	[Aubin, Benjamin; Zdeborova, Lenka] CNRS, Inst Phys Theor, Saclay, France; [Aubin, Benjamin; Zdeborova, Lenka] CEA, Saclay, France; [Aubin, Benjamin; Zdeborova, Lenka] Univ Paris Saclay, Saclay, France; [Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent] CNRS, Lab Phys Stat, Paris, France; [Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent] Sorbonnes Univ, Paris, France; [Aubin, Benjamin; Maillard, Antoine; Barbier, Jean; Krzakala, Florent] PSL Univ, Ecole Normale Super, Paris, France; [Barbier, Jean; Macris, Nicolas] Ecole Polytech Fed Lausanne, Fac Informat & Commun, Lab Theorie Commun, Lausanne, Switzerland; [Barbier, Jean] Abdus Salaam Int Ctr Theoret Phys, Trieste, Italy	CEA; Centre National de la Recherche Scientifique (CNRS); CEA; UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Abdus Salam International Centre for Theoretical Physics (ICTP)	Aubin, B (corresponding author), CNRS, Inst Phys Theor, Saclay, France.; Aubin, B (corresponding author), CEA, Saclay, France.; Aubin, B (corresponding author), Univ Paris Saclay, Saclay, France.; Aubin, B (corresponding author), CNRS, Lab Phys Stat, Paris, France.; Aubin, B (corresponding author), Sorbonnes Univ, Paris, France.; Aubin, B (corresponding author), PSL Univ, Ecole Normale Super, Paris, France.		Barbier, Jean/ABD-8759-2020; Krzakala, Florent/Q-9652-2019	Krzakala, Florent/0000-0003-2313-2578; Barbier, Jean/0000-0002-2652-6727	ERC under the European Union [714608-SMiLe]; French Agence Nationale de la Recherche [ANR-17-CE23-0023-01 PAIL]; NVIDIA Corporation; Swiss National Foundation [200021-156672]	ERC under the European Union; French Agence Nationale de la Recherche(French National Research Agency (ANR)); NVIDIA Corporation; Swiss National Foundation(Swiss National Science Foundation (SNSF))	This work is supported by the ERC under the European Union's Horizon 2020 Research and Innovation Program 714608-SMiLe, as well as by the French Agence Nationale de la Recherche under grant ANR-17-CE23-0023-01 PAIL. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research. Jean Barbier was supported by the Swiss National Foundation grant no 200021-156672. We also thank Leo Miolane for fruitful discussions.	Aubin Benjamin, 2018, ARXIV180605451; Aubin Benjamin, 2018, AMP IMPLEMENTATION C; Baity-Jesi M., 2018, INT C MACH LEARN PML, P314; Baldassi C, 2007, P NATL ACAD SCI USA, V104, P11079, DOI 10.1073/pnas.0700324104; Bandeira Afonso S, 2018, ARXIV180311132; Barbier J., 2017, ARXIV170803395; Barbier J., 2017, PROBABILITY THEORY R; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Chaudhari P, 2016, ICLR 2017; Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y; Donoho DL, 2013, IEEE T INFORM THEORY, V59, P3396, DOI 10.1109/TIT.2013.2239356; El Alaoui A, 2017, IEEE INT SYMP INFO, P2780, DOI 10.1109/ISIT.2017.8007036; El Alaoui Ahmed, 2016, ARXIV161109981; Guerra F, 2003, COMMUN MATH PHYS, V233, P1, DOI 10.1007/s00220-002-0773-5; Kabashima Y, 2008, J PHYS C SER, V95, DOI DOI 10.1088/1742-6596/95/1/012001; Kamilov U. S., 2012, P ADV NEUR INF PROC, P2438; Krzakala F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08009; Martin C.H, 2017, ARXIV171009553; MATO G, 1992, J PHYS A-MATH GEN, V25, P5047, DOI 10.1088/0305-4470/25/19/017; MEZARD M, 1989, J PHYS A-MATH GEN, V22, P2181, DOI 10.1088/0305-4470/22/12/018; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Mezard M., 1987, SPIN GLASS THEORY IN, V9, DOI DOI 10.1142/0271; MONASSON R, 1995, PHYS REV LETT, V75, P2432, DOI 10.1103/PhysRevLett.75.2432; Monasson R, 1995, MOD PHYS LETT B, V9, P1887, DOI 10.1142/S0217984995001868; Opper M, 1996, PHYS REV LETT, V76, P1964, DOI 10.1103/PhysRevLett.76.1964; Rangan S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2168, DOI 10.1109/ISIT.2011.6033942; SAFRAN I., 2017, ARXIV171208968; Schniter P, 2016, CONF REC ASILOMAR C, P1525, DOI 10.1109/ACSSC.2016.7869633; SCHWARZE H, 1992, EUROPHYS LETT, V20, P375, DOI 10.1209/0295-5075/20/4/015; SCHWARZE H, 1993, EUROPHYS LETT, V21, P785, DOI 10.1209/0295-5075/21/7/012; SCHWARZE H, 1993, J PHYS A-MATH GEN, V26, P5781, DOI 10.1088/0305-4470/26/21/017; TALAGRAND M, 2003, SPIN GLASSES CHALLEN, V46; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; Vapnik V.N, 1998, STAT LEARNING THEORY; Zhang Chiyuan, 2016, ICLR 2017	44	13	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303024
C	Balunovic, M; Bielik, P; Vechev, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balunovic, Mislav; Bielik, Pavol; Vechev, Martin			Learning to Solve SMT Formulas	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM; CONFIGURATION	We present a new approach for learning to solve SMT formulas. We phrase the challenge of solving SMT formulas as a tree search problem where at each step a transformation is applied to the input formula until the formula is solved. Our approach works in two phases: first, given a dataset of unsolved formulas we learn a policy that for each formula selects a suitable transformation to apply at each step in order to solve the formula, and second, we synthesize a strategy in the form of a loop-free program with branches. This strategy is an interpretable representation of the policy decisions and is used to guide the SMT solver to decide formulas more efficiently, without requiring any modification to the solver itself and without needing to evaluate the learned policy at inference time. We show that our approach is effective in practice - it solves 17% more formulas over a range of benchmarks and achieves up to 100 x runtime improvement over a state-of-the-art SMT solver.	[Balunovic, Mislav; Bielik, Pavol; Vechev, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Balunovic, M (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	bmislav@ethz.ch; pavol.bielik@inf.ethz.ch; martin.vechev@inf.ethz.ch						Alemi A. A., 2016, P 30 INT C NEUR INF, P2243; Amadini R, 2014, THEOR PRACT LOG PROG, V14, P509, DOI 10.1017/S1471068414000179; Ansotegui C, 2009, LECT NOTES COMPUT SC, V5732, P142, DOI 10.1007/978-3-642-04244-7_14; Barrett Clark, 2011, Computer Aided Verification. Proceedings 23rd International Conference, CAV 2011, P171, DOI 10.1007/978-3-642-22110-1_14; Barrett C, 2016, SATISFIABILITY MODUL; Barrett C., 2016, APROVE BENCHMARKS; Barrett C., 2016, HYCOMP BENCHMARKS; Barrett C., 2016, CORE BENCHMARKS; Barrett C., 2016, LEIPZIG BENCHMARKS; Barrett C., 2016, SAGE2 BENCHMARKS; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Bridge D., 2012, AUTONOMOUS SEARCH, P73; Cimatti A., 2013, LNCS, V7795; Clare A., 2001, EUR C PRINC DAT MIN, V2168, P42; de Moura Leonardo, 2013, Automated Reasoning and Mathematics. Essays in Memory of William W. McCune, P15, DOI 10.1007/978-3-642-36675-8_2; De Moura L, 2011, COMMUN ACM, V54, P69, DOI 10.1145/1995376.1995394; Dutertre B, 2014, LECT NOTES COMPUT SC, V8559, P737, DOI 10.1007/978-3-319-08867-9_49; Giesl J, 2017, J AUTOM REASONING, V58, P3, DOI 10.1007/s10817-016-9388-y; Glorot X., 2010, AISTATS; Godefroid Patrice, 2008, AUTOMATED WHITEBOX F; Hurley Barry, 2014, Integration of AI and OR Techniques in Constraint Programming. 11th International Conference, CPAIOR 2014. Proceedings: LNCS 8451, P301, DOI 10.1007/978-3-319-07046-9_22; Hutter F, 2010, LECT NOTES COMPUT SC, V6073, P281, DOI 10.1007/978-3-642-13800-3_30; Hutter F, 2009, J ARTIF INTELL RES, V36, P267, DOI 10.1613/jair.2861; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; KhudaBukhsh AR, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P517; Kingma Diederik P., 2014, ABS14126980 CORR; Lagoudakis M. G., 2001, ELECT NOTES DISCRETE, V9, P344, DOI DOI 10.1016/S1571-0653(04)00332-4; Loos S. M., 2017, 21 INT C LOGIC PROGR, P85; Loth M, 2013, LECT NOTES COMPUT SC, V8124, P464, DOI 10.1007/978-3-642-40627-0_36; Malitsky Y., 2010, P 19 EUROPEAN C ARTI, P751; Niemetz A., 2014, J SATISFIABILITY BOO, V9, P53; Nudelman E, 2004, LECT NOTES COMPUT SC, V3258, P438; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Ramirez NG, 2016, PROC INT C TOOLS ART, P247, DOI [10.1109/ICTAI.2016.0046, 10.1109/ICTAI.2016.43]; Ross St<prime>ephane, 2011, AISTATS; Samulowitz H., 2007, P 22 C ARTIFICIAL IN, P255; Somol P, 2004, IEEE T PATTERN ANAL, V26, P900, DOI 10.1109/TPAMI.2004.28; Wang MZ, 2017, ADV NEUR IN, V30; Xu L, 2008, J ARTIF INTELL RES, V32, P565, DOI 10.1613/jair.2490	44	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004083
C	Bellec, G; Salaj, D; Subramoney, A; Legenstein, R; Maass, W		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bellec, Guillaume; Salaj, Darjan; Subramoney, Anand; Legenstein, Robert; Maass, Wolfgang			Long short-term memory and learning-to-learn in networks of spiking neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PROCESSOR	Recurrent networks of spiking neurons (RSNNs) underlie the astounding computing and learning capabilities of the brain. But computing and learning capabilities of RSNN models have remained poor, at least in comparison with artificial neural networks (ANNs). We address two possible reasons for that. One is that RSNNs in the brain are not randomly connected or designed according to simple rules, and they do not start learning as a tabula rasa network. Rather, RSNNs in the brain were optimized for their tasks through evolution, development, and prior experience. Details of these optimization processes are largely unknown. But their functional contribution can be approximated through powerful optimization methods, such as backpropagation through time (BPTT). A second major mismatch between RSNNs in the brain and models is that the latter only show a small fraction of the dynamics of neurons and synapses in the brain. We include neurons in our RSNN model that reproduce one prominent dynamical process of biological neurons that takes place at the behaviourally relevant time scale of seconds: neuronal adaptation. We denote these networks as LSNNs because of their Long short-term memory. The inclusion of adapting neurons drastically increases the computing and learning capability of RSNNs if they are trained and configured by deep learning (BPTT combined with a rewiring algorithm that optimizes the network architecture). In fact, the computational performance of these RSNNs approaches for the first time that of LSTM networks. In addition RSNNs with adapting neurons can acquire abstract knowledge from prior learning in a Learning-to-Learn (L2L) scheme, and transfer that knowledge in order to learn new but related tasks from very few examples. We demonstrate this for supervised learning and reinforcement learning.	[Bellec, Guillaume; Salaj, Darjan; Subramoney, Anand; Legenstein, Robert; Maass, Wolfgang] Graz Univ Technol, Inst Theoret Comp Sci, Graz, Austria	Graz University of Technology	Bellec, G (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, Graz, Austria.	bellec@igi.tugraz.at; salaj@igi.tugraz.at; subramoney@igi.tugraz.at; legenstein@igi.tugraz.at; maass@igi.tugraz.at	Subramoney, Anand/AAE-1880-2020	Subramoney, Anand/0000-0002-7333-9860	HBP Joint Platform - European Union's Horizon 2020 Framework Programme for Research and Innovation [720270, 785907]; NVIDIA Corporation; European Union [604102]	HBP Joint Platform - European Union's Horizon 2020 Framework Programme for Research and Innovation; NVIDIA Corporation; European Union(European Commission)	This research/project was supported by the HBP Joint Platform, funded from the European Union's Horizon 2020 Framework Programme for Research and Innovation under the Specific Grant Agreement No. 720270 (Human Brain Project SGA1) and under the Specific Grant Agreement No. 785907 (Human Brain Project SGA2). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Quadro P6000 GPU used for this research. Research leading to these results has in parts been carried out on the Human Brain Project PCP Pilot Systems at the Julich Supercomputing Centre, which received co-funding from the European Union (Grant Agreement No. 604102). We gratefully acknowledge Sandra Diaz, Alexander Peyser and Wouter Klijn from the Simulation Laboratory Neuroscience of the Julich Supercomputing Centre for their support. The computational results presented have been achieved in part using the Vienna Scientific Cluster (VSC).	Allen Institute, 2018, ALL CELL TYP DAT CEL; Bellec G., 2018, INT C LEARN REPR ICL; Bellec  Guillaume, 2018, COMPUTATIONAL UNPUB; Costa RP, 2017, ADV NEUR IN, V30; Davies M, 2018, IEEE MICRO, V38, P82, DOI 10.1109/MM.2018.112130359; DePasquale B., 2016, ARXIV160107620; Duan Y., 2016, RL2 FAST REINFORCEME; Esser SK, 2016, P NATL ACAD SCI USA, V113, P11441, DOI 10.1073/pnas.1604850113; Furber SB, 2013, IEEE T COMPUT, V62, P2454, DOI 10.1109/TC.2012.142; Gerstner W, 2014, NEURONAL DYNAMICS: FROM SINGLE NEURONS TO NETWORKS AND MODELS OF COGNITION, P1, DOI 10.1017/CBO9781107447615; Gouwens NW, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02718-3; Greff K., 2017, IEEE T NEURAL NETWOR; Hasson U, 2015, TRENDS COGN SCI, V19, P304, DOI 10.1016/j.tics.2015.04.006; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Huh D., 2017, ARXIV170604698; Kappel D, 2018, ENEURO, V5, DOI 10.1523/ENEURO.0301-17.2018; Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485; Le Quoc V., 2015, ABS150400941 CORR; Mikolov Tomas, 2014, P 3 INT C LEARN REPR; Mongillo G, 2008, SCIENCE, V319, P1543, DOI 10.1126/science.1150769; MORRIS R, 1984, J NEUROSCI METH, V11, P47, DOI 10.1016/0165-0270(84)90007-4; Nicola W, 2017, NAT COMMUN, V8, DOI 10.1038/s41467-017-01827-3; Perich Matthew G, 2018, NEURON; Pozzorini C, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004275; Qiao N, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00141; Schemmel J, 2010, IEEE INT SYMP CIRC S, P1947, DOI 10.1109/ISCAS.2010.5536970; Schulman J., 2017, ABS170706347 CORR; Stokes MG, 2015, TRENDS COGN SCI, V19, P394, DOI 10.1016/j.tics.2015.05.004; Subramoney  Anand, 2018, RECURRENT NETW UNPUB; Teeter  Corinne, 2018, NATURE COMMUNICATION, V1, P1; Vasilaki E, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000586; Wang J.X., 2016, ARXIV161105763; Wang Jane X, 2018, NATURE NEUROSCIENCE	35	13	13	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300073
C	Detommaso, G; Cui, TG; Spantini, A; Marzouk, Y; Scheichl, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Detommaso, Gianluca; Cui, Tiangang; Spantini, Alessio; Marzouk, Youssef; Scheichl, Robert			A Stein variational Newton method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INVERSE PROBLEMS	Stein variational gradient descent (SVGD) was recently proposed as a general purpose nonparametric variational inference algorithm [Liu & Wang, NIPS 2016]: it minimizes the Kullback-Leibler divergence between the target distribution and its approximation by implementing a form of functional gradient descent on a reproducing kernel Hilbert space. In this paper, we accelerate and generalize the SVGD algorithm by including second-order information, thereby approximating a Newton-like iteration in function space. We also show how second-order information can lead to more effective choices of kernel. We observe significant computational gains over the original SVGD algorithm in multiple test cases.	[Detommaso, Gianluca] Univ Bath, Bath, Avon, England; [Detommaso, Gianluca] Alan Turing Inst, London, England; [Cui, Tiangang] Monash Univ, Clayton, Vic, Australia; [Spantini, Alessio; Marzouk, Youssef] MIT, Cambridge, MA 02139 USA; [Scheichl, Robert] Heidelberg Univ, Heidelberg, Germany	University of Bath; Monash University; Massachusetts Institute of Technology (MIT); Ruprecht Karls University Heidelberg	Detommaso, G (corresponding author), Univ Bath, Bath, Avon, England.; Detommaso, G (corresponding author), Alan Turing Inst, London, England.	gd391@bath.ac.uk; Tiangang.Cui@monash.edu; spantini@mit.edu; ymarz@mit.edu; r.scheichl@uni-heidelberg.de	Cui, Tiangang/AFU-5240-2022	Cui, Tiangang/0000-0002-4840-8545; Scheichl, Robert/0000-0001-8493-4393	EPSRC Centre for Doctoral Training in Statistical Applied Mathematics at Bath [EP/L015684/1]; Alan Turing Institute; AFOSR Computational Mathematics Program	EPSRC Centre for Doctoral Training in Statistical Applied Mathematics at Bath(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute; AFOSR Computational Mathematics Program(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	G. Detommaso is supported by the EPSRC Centre for Doctoral Training in Statistical Applied Mathematics at Bath (EP/L015684/1) and by a scholarship from the Alan Turing Institute. T. Cui, G. Detommaso, A. Spantini, and Y. Marzouk acknowledge support from the MATRIX Program on "Computational Inverse Problems" held at the MATRIX Institute, Australia, where this joint collaboration was initiated. A. Spantini and Y. Marzouk also acknowledge support from the AFOSR Computational Mathematics Program.	ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Chen W. Y., 2018, INT C MACH LEARN; Coram M., 2012, ARXIV12055314; Cui T, 2014, INVERSE PROBL, V30, DOI 10.1088/0266-5611/30/11/114015; Cui TG, 2016, J COMPUT PHYS, V304, P109, DOI 10.1016/j.jcp.2015.10.008; FRANCOIS D, 2005, P INT S APPL STOCH M, P238; Gershman S., 2012, ARXIV12064665; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Han J., 2017, ARXIV170405201; Khan M. E., 2017, ARXIV171105560; Khan M. Emtiyaz, 2017, ARXIV171201038; Liu C., 2017, ARXIV171111216; Liu Q., 2016, NEURIPS; Liu Q, 2017, ADV NEUR IN, V30; Liu Y., 2017, ARXIV170402399; Luenberger D., 1997, OPTIMIZATION VECTOR; Martin J, 2012, SIAM J SCI COMPUT, V34, pA1460, DOI 10.1137/110845598; MARZOUK Y., 2016, HDB UNCERTAINTY QUAN, P1; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Pu Y., 2017, ARXIV170405155; Rezende Danilo Jimenez, 2015, ARXIV150505770; Spantini A., 2018, J MACHINE LEARNING R; Stuart AM, 2010, ACTA NUMER, V19, P451, DOI 10.1017/S0962492910000061; Tabak E. G., 2013, COMMUNICATIONS PURE, P145; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang D., 2017, ARXIV171107168; Wright S. J., 1999, NUMERICAL OPTIMIZATI; Zhuo Jingwei, 2017, ARXIV171104425	30	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003070
C	Hayes, J; Ohrimenko, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hayes, Jamie; Ohrimenko, Olga			Contamination Attacks and Mitigation in Multi-Party Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Machine learning is data hungry; the more data a model has access to in training, the more likely it is to perform well at inference time. Distinct parties may want to combine their local data to gain the benefits of a model trained on a large corpus of data. We consider such a case: parties get access to the model trained on their joint data but do not see each others individual datasets. We show that one needs to be careful when using this multi-party model since a potentially malicious party can taint the model by providing contaminated data. We then show how adversarial training can defend against such attacks by preventing the model from learning trends specific to individual parties data, thereby also guaranteeing party-level membership privacy.	[Hayes, Jamie] UCL, London, England; [Hayes, Jamie; Ohrimenko, Olga] Microsoft Res, Cambridge, England	University of London; University College London; Microsoft	Hayes, J (corresponding author), UCL, London, England.; Hayes, J (corresponding author), Microsoft Res, Cambridge, England.	j.hayes@cs.ucl.ac.uk; oohrim@microsoft.com		Ohrimenko, Olga/0000-0002-9735-0538				Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Alfeld S, 2016, AAAI CONF ARTIF INTE, P1452; Allen Joshua, 2018, CORR; [Anonymous], 2012, P 29 INT COFERENCE I; Bittau A., 2017, ACM S OP SYST PRINC; Bonawitz K, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1175, DOI 10.1145/3133956.3133982; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen X., 2017, ARXIV171205526; Dowlin N, 2016, PR MACH LEARN RES, V48; Dwork C., 2012, C INN THEOR COMP SCI; Edwards H., 2016, INT C LEARN REPR ICL, V2; Goodfellow I. J., 2014, ARXIV14126572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu T., 2017, ARXIV170806733; Hamm J, 2017, J MACH LEARN RES, V18; Hamm J, 2016, PR MACH LEARN RES, V48; Hayes J., 2018, P PRIV ENH TECHN POP; Hesamifard E., 2018, PROC 1 ENHANCING TEC, P123; Hitaj B, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P603, DOI 10.1145/3133956.3134012; Hoekstra M., 2013, WORKSH HARDW ARCH SU; Jagielski M, 2018, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2018.00057; Kim Y., 2014, P 2014 C EMP METH NA; Koh PW, 2017, PR MACH LEARN RES, V70; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Long Yunhui, 2018, ARXIV180204889; Louppe Gilles, 2017, ADV NEURAL INFORM PR, P982; McMahan H.B., 2018, INT C LEARN REPR; Mohassel P, 2017, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2017.12; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nasr M, 2018, PROCEEDINGS OF THE 2018 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'18), P634, DOI 10.1145/3243734.3243855; Nikolaenko V., 2013, ACM CCS, P801, DOI DOI 10.1145/2508859.2516751; Ohrimenko O, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P619; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Pathak M., 2010, ADV NEURAL INFORM PR, P1876; Rajkumar A., 2012, ARTIF INTELL, P933; Schuster F, 2015, P IEEE S SECUR PRIV, P38, DOI 10.1109/SP.2015.10; Shen SQ, 2016, ANN COMPUT SECURITY, P508, DOI 10.1145/291079.2991125; Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41; Shokri R, 2015, CCS'15: PROCEEDINGS OF THE 22ND ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1310, DOI 10.1145/2810103.2813687; Song CZ, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P587, DOI 10.1145/3133956.3134077; Xiao H, 2015, PR MACH LEARN RES, V37, P1689; Zemel R., 2013, P INT C MACH LEARN, P325	45	13	13	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001017
C	Ke, NR; Goyal, A; Bilaniuk, O; Binas, J; Mozer, MC; Pal, C; Bengio, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ke, Nan Rosemary; Goyal, Anirudh; Bilaniuk, Olexa; Binas, Jonathan; Mozer, Michael C.; Pal, Chris; Bengio, Yoshua			Sparse Attentive Backtracking: Temporal Credit Assignment Through Reminding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				HIPPOCAMPAL PLACE CELLS; REVERSE REPLAY; NETWORK; SIMILARITY; ALGORITHM	Learning long-term dependencies in extended temporal sequences requires credit assignment to events far back in the past. The most common method for training recurrent neural networks, back-propagation through time (BPTT), requires credit information to be propagated backwards through every single step of the forward computation, potentially over thousands or millions of time steps. This becomes computationally expensive or even infeasible when used with long sequences. Importantly, biological brains are unlikely to perform such detailed reverse replay over very long sequences of internal states (consider days, months, or years.) However, humans are often reminded of past memories or mental states which are associated with the current mental state. We consider the hypothesis that such memory associations between past and present could be used for credit assignment through arbitrarily long sequences, propagating the credit assigned to the current state to the associated past state. Based on this principle, we study a novel algorithm which only back-propagates through a few of these temporal skip connections, realized by a learned attention mechanism that associates current states with relevant past states. We demonstrate in experiments that our method matches or outperforms regular BPTT and truncated BPTT in tasks involving particularly longterm dependencies, but without requiring the biologically implausible backward replay through the whole history of states. Additionally, we demonstrate that the proposed method transfers to longer sequences significantly better than LSTMs trained with BPTT and LSTMs trained with full self-attention.	[Ke, Nan Rosemary; Goyal, Anirudh; Bilaniuk, Olexa; Binas, Jonathan; Pal, Chris; Bengio, Yoshua] Univ Montreal, Mila, Montreal, PQ, Canada; [Ke, Nan Rosemary; Pal, Chris] Polytech Montreal, Mila, Montreal, PQ, Canada; [Mozer, Michael C.] Univ Colorado, Boulder, CO 80309 USA; [Pal, Chris] Element AI, Montreal, PQ, Canada; [Bengio, Yoshua] CIFAR, Toronto, ON, Canada	Universite de Montreal; Universite de Montreal; Polytechnique Montreal; University of Colorado System; University of Colorado Boulder; Canadian Institute for Advanced Research (CIFAR)	Ke, NR (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.; Ke, NR (corresponding author), Polytech Montreal, Mila, Montreal, PQ, Canada.				NSERC; CIFAR; Google; Samsung; SNSF; Nuance; IBM; Canada Research Chairs; National Science Foundation [EHR-1631428, SES-1461535]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Google(Google Incorporated); Samsung(Samsung); SNSF(Swiss National Science Foundation (SNSF)); Nuance; IBM(International Business Machines (IBM)); Canada Research Chairs(Canada Research ChairsCGIAR); National Science Foundation(National Science Foundation (NSF))	The authors would like to thank Hugo Larochelle, Walter Senn, Alex Lamb, Remi Le Priol, Matthieu Courbariaux, Gaetan Marceau Caron, Sandeep Subramanian for the useful discussions, as well as NSERC, CIFAR, Google, Samsung, SNSF, Nuance, IBM, Canada Research Chairs, National Science Foundation awards EHR-1631428 and SES-1461535 for funding. We would also like to thank Compute Canada and NVIDIA for computing resources. The authors would also like to thank Alex Lamb for code review. The authors would also like to express debt of gratitude towards those who contributed to Theano over the years (now that it is being sunset), for making it such a great tool.	Ambrose RE, 2016, NEURON, V91, P1124, DOI 10.1016/j.neuron.2016.07.047; Arjovsky M, 2016, PR MACH LEARN RES, V48; Bengio, 2016, ARXIV160901704; Benjamin Aaron S, 2010, SUCCESSFUL REMEMBERI; Berntsen D, 2013, J EXP PSYCHOL GEN, V142, P426, DOI 10.1037/a0029128; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Ciaramelli E, 2008, NEUROPSYCHOLOGIA, V46, P1828, DOI 10.1016/j.neuropsychologia.2008.03.022; Cooijmans T., 2016, ARXIV160309025; Davidson TJ, 2009, NEURON, V63, P497, DOI 10.1016/j.neuron.2009.07.027; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; ElHihi S, 1996, ADV NEUR IN, V8, P493; FORBUS KD, 1995, COGNITIVE SCI, V19, P141, DOI 10.1016/0364-0213(95)90016-0; Foster DJ, 2006, NATURE, V440, P680, DOI 10.1038/nature04587; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Gulcehre Caglar, 2017, ARXIV170108718; Gupta AS, 2010, NEURON, V65, P695, DOI 10.1016/j.neuron.2010.01.034; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kadar Akos, 2018, ARXIV180703595; Ke NR, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Koutnik Jan, 2014, ARXIV14023511; Lee Dong-Hyun, 2014, CORR; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Luong M., 2015, ARXIV150804025; Mahoney Matt, 2011, LARGE TEXT COMPRESSI; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790; Mikolov T., 2012, SUBWORD LANGUAGE MOD, V8; Mozer M. C., 2017, ARXIV171004110; NOVICK LR, 1988, J EXP PSYCHOL LEARN, V14, P510, DOI 10.1037/0278-7393.14.3.510; Ollivier Y., 2015, TRAINING RECURRENT N; READ SJ, 1991, J EXP SOC PSYCHOL, V27, P1, DOI 10.1016/0022-1031(91)90008-T; Scellier Benjamin, 2016, CORR; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wharton CM, 1996, MEM COGNITION, V24, P629, DOI 10.3758/BF03201088; Whittington JCR, 2017, NEURAL COMPUT, V29, P1229, DOI 10.1162/NECO_a_00949; Williams RJ, 1990, NEURAL COMPUT, V2, P490, DOI 10.1162/neco.1990.2.4.490	40	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002021
C	Kim, H; Jiang, YH; Kannan, S; Oh, S; Viswanath, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kim, Hyeji; Jiang, Yihan; Kannan, Sreeram; Oh, Sewoong; Viswanath, Pramod			Deepcode: Feedback Codes via Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ADDITIVE NOISE CHANNELS; CODING SCHEME; AWGN CHANNEL	The design of codes for communicating reliably over a statistically well defined channel is an important endeavor involving deep mathematical research and wide-ranging practical applications. In this work, we present the first family of codes obtained via deep learning, which significantly beats state-of-the-art codes designed over several decades of research. The communication channel under consideration is the Gaussian noise channel with feedback, whose study was initiated by Shannon; feedback is known theoretically to improve reliability of communication, but no practical codes that do so have ever been successfully constructed. We break this logjam by integrating information theoretic insights harmoniously with recurrent-neural-network based encoders and decoders to create novel codes that outperform known codes by 3 orders of magnitude in reliability. We also demonstrate several desirable properties in the codes: (a) generalization to larger block lengths; (b) composability with known codes; (c) adaptation to practical constraints. This result also presents broader ramifications to coding theory: even when the channel has a clear mathematical model, deep learning methodologies, when combined with channel-specific information-theoretic insights, can potentially beat state-of-the-art codes, constructed over decades of mathematical research.	[Kim, Hyeji] Samsung AI Ctr Cambridge, Cambridge, England; [Jiang, Yihan; Kannan, Sreeram] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Oh, Sewoong; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Champaign, IL USA; [Oh, Sewoong] UIUC, Dept Ind & Enterprise Syst Engn, Champaign, IL USA; [Viswanath, Pramod] UIUC, Dept Elect Engn, Champaign, IL USA	University of Washington; University of Washington Seattle; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Kim, H (corresponding author), Samsung AI Ctr Cambridge, Cambridge, England.	hkim1505@gmail.com; yihanrogerjiang@gmail.com; ksreeram@uw.edu; swoh@illinois.edu; pramodv@illinois.edu			National Science Foundation [CCF-1553452, RI-1815535]; Army Research Office [W911NF-18-1-0384]; Amazon Catalyst award; NSF [1651236, 1703403]	National Science Foundation(National Science Foundation (NSF)); Army Research Office; Amazon Catalyst award; NSF(National Science Foundation (NSF))	We thank Shrinivas Kudekar and Saurabh Tavildar for helpful discussions and providing references to the state-of-the-art feedforward codes. We thank Dina Katabi for a detailed discussion that prompted work on system implementation. This work is in part supported by National Science Foundation awards CCF-1553452 and RI-1815535, Army Research Office under grant number W911NF-18-1-0384, and Amazon Catalyst award. Y. Jiang and S. Kannan would also like to acknowledge NSF awards 1651236 and 1703403.	Alias P., 1955, IRE CONV REC, P37; Ben-Yishai A, 2017, IEEE T INFORM THEORY, V63, P2409, DOI 10.1109/TIT.2017.2648821; Cammerer S., 2018, INT ZUR SEM INF COMM, P51; Chance Z, 2011, IEEE T INFORM THEORY, V57, P6633, DOI 10.1109/TIT.2011.2165796; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Duman T. M., 1997, Proceeding. 1997 IEEE International Symposium on Information Theory (Cat. No.97CH36074), DOI 10.1109/ISIT.1997.613019; Erseghe T., 2014, IEEE T INFORM THEORY, V61; Farsad N, 2018, 2018 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH AND SIGNAL PROCESSING (ICASSP), P2331; Felix A, 2018, IEEE INT WORK SIGN P, P56; Gallager RG, 2010, IEEE T INFORM THEORY, V56, P6, DOI 10.1109/TIT.2009.2034896; He HT, 2018, IEEE GLOB CONF SIG, P584, DOI 10.1109/GlobalSIP.2018.8646357; Hinton G., 2015, ARXIV150302531; Huawei H., 2016, 87 3GPP TSGRAN WGI; Il C. J., 2010, P 16 ANN INT C MOB C; Kim H., 2018, INT C REPR LEARN ICL; Kim YH, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P1416, DOI 10.1109/ISIT.2007.4557421; Kosaian J., 2018, ARXIV180601259; Lapidoth A, 1996, IEEE T INFORM THEORY, V42, P1520, DOI 10.1109/18.532892; Miwa K., 2012, IEICE TECHNICAL REPO, P1; Nachmani E, 2018, IEEE J-STSP, V12, P119, DOI 10.1109/JSTSP.2017.2788405; Nachmani E, 2016, ANN ALLERTON CONF, P341, DOI 10.1109/ALLERTON.2016.7852251; O'Shea TJ, 2016, IEEE INT SYMP SIGNAL, P223, DOI 10.1109/ISSPIT.2016.7886039; OShea T. J., 2017, ARXIV170200832; OShea T. J., 2017, CORR; Polyanskiy Y, 2010, IEEE T INFORM THEORY, V56, P2307, DOI 10.1109/TIT.2010.2043769; Qi H., 2009, 2009 INT C WIR COMM, P1; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; SCHALKWIJK JP, 1966, IEEE T INFORM THEORY, V12, P183, DOI 10.1109/TIT.1966.1053880; SCHALKWIJK JP, 1966, IEEE T INFORM THEORY, V12, P172, DOI 10.1109/TIT.1966.1053879; Seo J, 2018, INT CONF COMPUT NETW, P238; SHANNON CE, 1956, IRE T INFORM THEOR, V2, P8, DOI 10.1109/TIT.1956.1056798; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; Sun H., 2018, IEEE T SIGNAL PROCES; Tan X., 2018, IMPROVING MASSIVE MI; Wen C.-K., 2017, IEEE WIRELESS COMMUN; Zhao J, 2017, 2017 IEEE 2ND ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P813, DOI 10.1109/IAEAC.2017.8054128	37	13	14	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004004
C	Lim, B; Alaa, A; van der Schaar, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lim, Bryan; Alaa, Ahmed; van der Schaar, Mihaela			Forecasting Treatment Responses Over Time Using Recurrent Marginal Structural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS	Electronic health records provide a rich source of data for machine learning methods to learn dynamic treatment responses over time. However, any direct estimation is hampered by the presence of time-dependent confounding, where actions taken are dependent on time-varying variables related to the outcome of interest. Drawing inspiration from marginal structural models, a class of methods in epidemiology which use propensity weighting to adjust for time-dependent confounders, we introduce the Recurrent Marginal Structural Network - a sequence-to-sequence architecture for forecasting a patient's expected response to a series of planned treatments. Using simulations of a state-of-the-art pharmacokinetic-pharmacodynamic (PK-PD) model of tumor growth [12], we demonstrate the ability of our network to accurately learn unbiased treatment responses from observational data - even under changes in the policy of treatment assignments - and performance gains over benchmarks.	[Lim, Bryan] Univ Oxford, Dept Engn Sci, Oxford, England; [Alaa, Ahmed] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Alan Turing Inst, London, England	University of Oxford; University of California System; University of California Los Angeles	Lim, B (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	bryan.lim@eng.ox.ac.uk; ahmedmalaa@ucla.edu; mschaar@turing.ac.uk			Oxford-Man Institute of Quantitative Finance; US Office of Naval Research (ONR); Alan Turing Institute	Oxford-Man Institute of Quantitative Finance; US Office of Naval Research (ONR)(Office of Naval Research); Alan Turing Institute	This research was supported by the Oxford-Man Institute of Quantitative Finance, the US Office of Naval Research (ONR), and the Alan Turing Institute.	Alaa Ahmed M., 2017, P 31 C NEUR INF PROC; Alaa Ahmed M., 2017, INT C MACH LEARN ICM; Atan Mihaela van der Schaar Onur, 2018, AAAI; Atan Mihaela van der Schaar Onur, 2018, MACHINE LEARNING; Barbolosi D, 2001, COMPUT BIOL MED, V31, P157, DOI 10.1016/S0010-4825(00)00032-9; Bartsch Helmut, 2007, V174, P19; Carrara L, 2017, EXPERT OPIN DRUG DIS, V12, P785, DOI 10.1080/17460441.2017.1340271; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Daniel RM, 2013, STAT MED, V32, P1584, DOI 10.1002/sim.5686; Djork-Arn, ICLR 2016; Egger PH, 2013, ECON LETT, V119, P32, DOI 10.1016/j.econlet.2013.01.006; Eigenmann MJ, 2017, J PHARMACOKINET PHAR, V44, P617, DOI 10.1007/s10928-017-9553-x; Geng CR, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-13646-z; Hartford J, 2017, PR MACH LEARN RES, V70; Hernan MA, 2000, EPIDEMIOLOGY, V11, P561, DOI 10.1097/00001648-200009000-00012; Hernan MA, 2001, J AM STAT ASSOC, V96, P440, DOI 10.1198/016214501753168154; Hernan MA, 2018, CAUSAL INFERENCE; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoiles William, 2016, NEURAL INFORM PROCES; Howe CJ, 2012, EPIDEMIOLOGY, V23, P574, DOI 10.1097/EDE.0b013e31824d1ccb; Kingma Diederik P., 2015, INT C LEAR REPR ICLR; Lusivika-Nzinga C, 2017, BMC MED RES METHODOL, V17, DOI 10.1186/s12874-017-0434-1; Mansournia MA, 2012, EPIDEMIOLOGY, V23, P631, DOI 10.1097/EDE.0b013e31824cc1c3; Mansournia Mohammad Ali, 2017, BMJ, V359; McCulloch CE., 2014, GEN LINEAR MIXED MOD; Mortimer KM, 2005, AM J EPIDEMIOL, V162, P382, DOI 10.1093/aje/kwi208; Mould DR, CPT PHARMACOMETRICS, V4, P12; Park K, 2017, YONSEI MED J, V58, P1, DOI 10.3349/ymj.2017.58.1.1; Richardson TS, 2014, STAT SCI, V29, P459, DOI 10.1214/14-STS505; Robins JM, 2000, EPIDEMIOLOGY, V11, P550, DOI 10.1097/00001648-200009000-00011; Roy J, 2017, BIOSTATISTICS, V18, P32, DOI 10.1093/biostatistics/kxw029; Schulam P., 2017, NEURAL INFORM PROCES; Silva Ricardo, 2016, NEURAL INFORM PROCES; Soleimani H, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; Swaminathan Adith, 2015, CORR; Thoemmes F, 2016, EMERG ADULTHOOD, V4, P40, DOI 10.1177/2167696815621645; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Vlachostergios Panagiotis J., 2018, NATURE REV CLIN ONCO; Wager Stefan, 2017, J AM STAT ASS; Xiao YL, 2010, INT J BIOSTAT, V6, DOI 10.2202/1557-4679.1208; Xu Yanbo, 2016, P 1 MACH LEARN HEALT; Yoon Jaehong, 2018, LIFELONG LEARNING DY	44	13	13	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002007
C	Metelli, AM; Papini, M; Faccio, F; Restelli, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Metelli, Alberto Maria; Papini, Matteo; Faccio, Francesco; Restelli, Marcello			Policy Optimization via Importance Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DIVERGENCE	Policy optimization is an effective reinforcement learning approach to solve continuous control tasks. Recent achievements have shown that alternating online and offline optimization is a successful choice for efficient trajectory reuse. However, deciding when to stop optimizing and collect new trajectories is non-trivial, as it requires to account for the variance of the objective function estimate. In this paper, we propose a novel, model-free, policy search algorithm, POIS, applicable in both action-based and parameter-based settings. We first derive a high-confidence bound for importance sampling estimation; then we define a surrogate objective function, which is optimized offline whenever a new batch of trajectories is collected. Finally, the algorithm is tested on a selection of continuous control tasks, with both linear and deep policies, and compared with state-of-the-art policy optimization methods.	[Metelli, Alberto Maria; Papini, Matteo; Faccio, Francesco; Restelli, Marcello] Politecn Milan, Milan, Italy; [Faccio, Francesco] USI, SUPSI, IDSIA, Lugano, Switzerland	Polytechnic University of Milan; Universita della Svizzera Italiana	Metelli, AM (corresponding author), Politecn Milan, Milan, Italy.	albertomaria.metelli@polimi.it; matteo.papini@polimi.it; francesco.faccio@mail.polimi.it; marcello.restelli@polimi.it	Metelli, Alberto Maria/AAY-5206-2020; Papini, Matteo/AAF-3768-2019	Metelli, Alberto Maria/0000-0002-3424-5212; Papini, Matteo/0000-0002-3807-3171; Restelli, Marcello/0000-0002-6322-1076	Lombardy Region (Announcement PORFESR 2014-2020); ERC [742870]; NVIDIA Corporation	Lombardy Region (Announcement PORFESR 2014-2020); ERC(European Research Council (ERC)European Commission); NVIDIA Corporation	The study was partially funded by Lombardy Region (Announcement PORFESR 2014-2020). F. F. was partially funded through ERC Advanced Grant (no: 742870). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40cm, Titan XP and Tesla V100 used for this research.	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Amari S, 2010, B POL ACAD SCI-TECH, V58, P183, DOI 10.2478/v10175-010-0019-1; Amari S., 2012, DIFFERENTIAL GEOMETR, V28; Bercu Bernard, 2015, CONCENTRATION INEQUA, P11; BURBEA J, 1984, UTILITAS MATHEMATICA, V26, P171; Cochran William G., 2007, SAMPLING TECHNIQUES; Cortes C., 2010, PROC ADV NEURAL INF, V10, P442; Degris, 2012, ARXIV12054839, P179; Doroudi S, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Duan Y, 2016, PR MACH LEARN RES, V48; Glorot X., 2010, PROC MACH LEARN RES, P249; Gruttner  Mandy, 2010, MULTIDIMENSIONAL DEE; Guo  Zhaohan, 2017, ADV NEURAL INFORM PR, P2489; Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398; Hesterberg T.C., 1988, THESIS; Hoef JMV, 2012, AM STAT, V66, P124, DOI 10.1080/00031305.2012.687494; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kakade S., 2002, P 19 INT C MACH LEAR; Kobayashi S., 2010, ADV NEURAL INFORM PR, P1660; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Konda VR, 2000, ADV NEUR IN, V12, P1008; Kong A., 1992, NOTE IMPORTANCE SAMP; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Matsubara T, 2010, JMLR WORKSH CONF PRO, V13, P285; Ng AY, 2000, P 16 C UNCERTAINTY A, P406; Peng J., 1994, MACHINE LEARNING P, P226, DOI [10.1016/B978-1-55860-335-6.50035-0, DOI 10.1016/B978-1-55860-335-6.50035-0]; Peters J., 2007, P 24 INT C MACHINE L, P745; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Pirotta M., 2013, INT C MACH LEARN, P307; Precup D., 2000, P 17 INT C MACH LEAR; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rajeswaran A., 2017, ADV NEURAL INFORM PR, P6550; Rao C.R., 1992, BREAKTHROUGHS STAT F, P235, DOI [DOI 10.1007/978-1-4612-0919-5, 10.1007/978-1-4612-0919-5\_16]; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Rubinstein R.Y., 1999, METHODOL COMPUT APPL, V1, P127, DOI DOI 10.1023/A:1010091220143; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Sehnke F, 2008, LECT NOTES COMPUT SC, V5163, P387, DOI 10.1007/978-3-540-87536-9_40; Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004; Silver D, 2014, PR MACH LEARN RES, V32; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Sun Y., 2009, P 11 ANN C GENETIC E, P539, DOI DOI 10.1145/1569901.1569976; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Szita I, 2006, NEURAL COMPUT, V18, P2936, DOI 10.1162/neco.2006.18.12.2936; Tedrake R., 2004, 2004 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS) (IEEE Cat. No.04CH37566), P2849; Thomas PS, 2015, AAAI CONF ARTIF INTE, P3000; Thomas PS, 2015, PR MACH LEARN RES, V37, P2380; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Tucker G, 2018, PR MACH LEARN RES, V80; van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500; Wang Z., 2016, ARXIV161101224; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu C., 2018, INT C LEARN REPR; Zhao T, 2011, ADV NEURAL INFORM PR, P262; Zhao TT, 2013, NEURAL COMPUT, V25, P1512, DOI 10.1162/NECO_a_00452	67	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305046
C	Munkhoeva, M; Kapushev, Y; Burnaev, E; Oseledets, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Munkhoeva, Marina; Kapushev, Yermek; Burnaev, Evgeny; Oseledets, Ivan			Quadrature-based features for kernel approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				INTEGRATION RULES; GENERATION	We consider the problem of improving kernel approximation via randomized feature maps. These maps arise as Monte Carlo approximation to integral representations of kernel functions and scale up kernel methods for larger datasets. Based on an efficient numerical integration technique, we propose a unifying approach that reinterprets the previous random features methods and extends to better estimates of the kernel approximation. We derive the convergence behaviour and conduct an extensive empirical study that supports our hypothesis(1).	[Munkhoeva, Marina; Kapushev, Yermek; Burnaev, Evgeny; Oseledets, Ivan] Skolkovo Inst Sci & Technol, Moscow, Russia; [Oseledets, Ivan] Russian Acad Sci, Inst Numer Math, Moscow, Russia	Skolkovo Institute of Science & Technology; Russian Academy of Sciences	Munkhoeva, M (corresponding author), Skolkovo Inst Sci & Technol, Moscow, Russia.		Burnaev, Evgeny/A-3703-2016	Burnaev, Evgeny/0000-0001-8424-0690	Ministry of Science and Education of Russian Federation [14.756.31.0001]	Ministry of Science and Education of Russian Federation(Ministry of Education and Science, Russian Federation)	This work was supported by the Ministry of Science and Education of Russian Federation as a part of Mega Grant Research Project 14.756.31.0001.	Alan Gen z., 1998, MONTE CARLO QUASIMON, P199; ANDERSON TW, 1987, SIAM J SCI STAT COMP, V8, P625, DOI 10.1137/0908055; Avron H, 2016, TECHNOMETRICS, V58, P341, DOI 10.1080/00401706.2015.1111261; BACH F, 2017, J MACHINE LEARNING R, V18; Baker JA, 1997, AM MATH MON, V104, P36, DOI 10.2307/2974821; Bergstra J., 2013, JMLR WORKSHOP C P IC, V28, P115, DOI [10.5555/3042817.3042832, DOI 10.5555/3042817.3042832]; Bochner S, 1933, MATH ANN, V108, P378, DOI 10.1007/BF01452844; Chen XX, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3395; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Choromanski Krzysztof, 2016, ARXIV160509049; Choromanski Krzysztof, 2017, ARXIV170300864; Dao Tri, 2017, Adv Neural Inf Process Syst, V30, P6109; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fang KT, 1997, COMPUT STAT DATA AN, V24, P29; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Forrester A., 2008, ENG DESIGN VIA SURRO, P168; Genz A, 1998, SIAM J SCI COMPUT, V19, P426, DOI 10.1137/S1064827595286803; Genz A, 1999, J COMPUT APPL MATH, V112, P71, DOI 10.1016/S0377-0427(99)00214-9; Golub TR, 1999, SCIENCE, V286, P531, DOI 10.1126/science.286.5439.531; Haykin S, 2012, COGNITIVE DYNAMIC SY; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Le Q., 2013, ICML; Lyu YM, 2017, PR MACH LEARN RES, V70; Mezzadri F., 2006, NOT AM MATH SOC, V54; Monahan J, 1997, J AM STAT ASSOC, V92, P664; Owen A. B., 1998, ACM Transactions on Modeling and Computer Simulation, V8, P71, DOI 10.1145/272991.273010; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudin W., 2017, FOURIER ANAL GROUPS; Smola Alex J, 2000, SPARSE GREEDY MATRIX; STEWART GW, 1980, SIAM J NUMER ANAL, V17, P403, DOI 10.1137/0717034; Sutherland Dougal J, 2015, ARXIV150602785; Yang JY, 2014, PR MACH LEARN RES, V32; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yu F. X., 2015, ARXIV150303893; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975	37	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003068
C	Salimbeni, H; Cheng, CA; Boots, B; Deisenroth, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Salimbeni, Hugh; Cheng, Ching-An; Boots, Byron; Deisenroth, Marc			Orthogonally Decoupled Variational Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Gaussian processes (GPs) provide a powerful non-parametric framework for reasoning over functions. Despite appealing theory, its superlinear computational and memory complexities have presented a long-standing challenge. State-of-the-art sparse variational inference methods trade modeling accuracy against complexity. However, the complexities of these methods still scale superlinearly in the number of basis functions, implying that that sparse GP methods are able to learn from large datasets only when a small model is used. Recently, a decoupled approach was proposed that removes the unnecessary coupling between the complexities of modeling the mean and the covariance functions of a GP. It achieves a linear complexity in the number of mean parameters, so an expressive posterior mean function can be modeled. While promising, this approach suffers from optimization difficulties due to ill-conditioning and non-convexity. In this work, we propose an alternative decoupled parametrization. It adopts an orthogonal basis in the mean function to model the residues that cannot be learned by the standard coupled approach. Therefore, our method extends, rather than replaces, the coupled approach to achieve strictly better performance. This construction admits a straightforward natural gradient update rule, so the structure of the information manifold that is lost during decoupling can be leveraged to speed up learning. Empirically, our algorithm demonstrates significantly faster convergence in multiple experiments.	[Salimbeni, Hugh; Deisenroth, Marc] Imperial Coll London, London, England; [Cheng, Ching-An; Boots, Byron] Georgia Inst Technol, Atlanta, GA 30332 USA	Imperial College London; University System of Georgia; Georgia Institute of Technology	Salimbeni, H (corresponding author), Imperial Coll London, London, England.	hrs13@ic.ac.uk; cacheng@gatech.edu; bboots@gatech.edu; mpd37@ic.ac.uk	Cheng, Ching-An/AAZ-1802-2020	Cheng, Ching-An/0000-0002-0610-2070				Abadi M., 2016, S OP SYST DES IMPL; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Aronszajn N., 1950, T AM MATH SOC; Cheng C.-A., 2016, ADV NEURAL INFORM PR, P4410; Cheng CA, 2017, ADV NEUR IN, V30; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth MP, 2015, PR MACH LEARN RES, V37, P1481; Diggle PJ, 2007, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-48536-2; Gardner JR, 2018, PR MACH LEARN RES, V84; Hartikainen Jouni, 2010, Proceedings of the 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), P379, DOI 10.1109/MLSP.2010.5589113; Havasi M., 2018, ARXIV180102939; Hensman J, 2013, GAUSSIAN PROCESSES B; Hensman J, 2018, J MACH LEARN RES, V18, P1; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Kingma D.P, P 3 INT C LEARNING R; KIVINEN J, 2004, IEEE T SIGNAL PROCES; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Martens James, 2014, NEW INSIGHTS PERSPEC; Matthews A. G. D. G., 2017, THESIS U CAMBRIDGE P; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Matthews Alexander G. de G., 2016, INT C ART INT STAT; Nguyen TV, 2014, PR MACH LEARN RES, V32; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen C. E., 2002, ADV NEURAL INFORM PR, P881; Rulliere D, 2018, STAT COMPUT, V28, P849, DOI 10.1007/s11222-017-9766-2; Salimbeni H, 2018, PR MACH LEARN RES, V84; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Snelson E., 2006, ADV NEURAL INFORM PR, V18, P1259; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Tresp V, 2000, NEURAL COMPUT, V12, P2719, DOI 10.1162/089976600300014908; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	34	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003028
C	Sohn, S; Oh, J; Lee, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sohn, Sungryull; Oh, Junhyuk; Lee, Honglak			Hierarchical Reinforcement Learning for Zero-shot Generalization with Subtask Dependencies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce a new RL problem where the agent is required to generalize to a previously-unseen environment characterized by a subtask graph which describes a set of subtasks and their dependencies. Unlike existing hierarchical multitask RL approaches that explicitly describe what the agent should do at a high level, our problem only describes properties of subtasks and relationships among them, which requires the agent to perform complex reasoning to find the optimal subtask to execute. To solve this problem, we propose a neural subtask graph solver (NSGS) which encodes the subtask graph using a recursive neural network embedding. To overcome the difficulty of training, we propose a novel non-parametric gradient-based policy, graph reward propagation, to pre-train our NSGS agent and further finetune it through actor-critic method. The experimental results on two 2D visual domains show that our agent can perform complex reasoning to find a near-optimal way of executing the subtask graph and generalize well to the unseen subtask graphs. In addition, we compare our agent with a Monte-Carlo tree search (MCTS) method showing that our method is much more efficient than MCTS, and the performance of NSGS can be further improved by combining it with MCTS.	[Sohn, Sungryull; Oh, Junhyuk] Univ Michigan, Ann Arbor, MI 48109 USA; [Lee, Honglak] Univ Michigan, Google Brain, Ann Arbor, MI 48109 USA; [Oh, Junhyuk] DeepMind, London, England	University of Michigan System; University of Michigan; Google Incorporated; University of Michigan System; University of Michigan	Sohn, S (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	srsohn@umich.edu; junhyuk@google.com; honglak@google.com	Sohn, Sungryull/GQI-4606-2022	Sohn, Sungryull/0000-0001-7733-4293	ICT R&D program of MSIP/IITP [2016-0-00563]; DARPA Explainable AI (XAI) program [313498]; Sloan Research Fellowship	ICT R&D program of MSIP/IITP; DARPA Explainable AI (XAI) program; Sloan Research Fellowship(Alfred P. Sloan Foundation)	This work was supported mainly by the ICT R&D program of MSIP/IITP (2016-0-00563: Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion) and partially by DARPA Explainable AI (XAI) program #313498 and Sloan Research Fellowship.	Andre D., 2000, NIPS; Andre D., 2002, AAAI IAAI; Andreas J., 2017, ICML; [Anonymous], 2015, ARXIV151106295; [Anonymous], ARXIV170309831; [Anonymous], 2000, THESIS; Asano Takao, 1985, FOCS; Auer P., 2002, MACHINE LEARNING; Bloch Mitchell Keith, 2009, TECHNICAL REPORT; Canny John, 1987, FOCS; Canny John, 1985, ICRA; Castillo Luis, 2005, CAEPIA; Da Silva B, 2012, ARXIV12066398; Denil M., 2017, ARXIV170606383; Dietterich T. G., 2000, JAIR; Erol K., 1996, THESIS; Erol Kutluhan, 1994, AIPS; Faverjon Bernard, 1987, ICRA; Ghavamzadeh Mohammad, 2003, ICML; Keil J Mark, 1985, MACHINE INTELLIGENCE; Konidaris G., 2007, IJCAI; Nau D., 1999, IJCAI; Oh J., 2017, ICML; Parisotto Emilio, 2015, ACTOR MIMIC DEEP MUL; Parisotto Emilio, 2016, ARXIV161101855; Parr Ronald, 1997, NIPS; Sacerdoti E.D., 1975, TECHNICAL REPORT; Schulman John, 2015, ARXIV150602438; Silver D., 2016, NATURE; Stolle Martin, 2002, ISARA; Sukhbaatar S., 2015, ARXIV151107401; Sutton R. S., 1999, ARTIFICIAL INTELLIGE	32	13	13	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001068
C	Wang, QL; Gao, ZL; Xie, JT; Zuo, WM; Li, PH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Qilong; Gao, Zilin; Xie, Jiangtao; Zuo, Wangmeng; Li, Peihua			Global Gated Mixture of Second-order Pooling for Improving Deep Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In most of existing deep convolutional neural networks (CNNs) for classification, global average (first-order) pooling (GAP) has become a standard module to summarize activations of the last convolution layer as final representation for prediction. Recent researches show integration of higher-order pooling (HOP) methods clearly improves performance of deep CNNs. However, both GAP and existing HOP methods assume unimodal distributions, which cannot fully capture statistics of convolutional activations, limiting representation ability of deep CNNs, especially for samples with complex contents. To overcome the above limitation, this paper proposes a global Gated Mixture of Second-order Pooling (GM-SOP) method to further improve representation ability of deep CNNs. To this end, we introduce a sparsity-constrained gating mechanism and propose a novel parametric SOP as component of mixture model. Given a bank of SOP candidates, our method can adaptively choose Top-K(K > 1) candidates for each input sample through the sparsity-constrained gating module, and performs weighted sum of outputs of K selected candidates as representation of the sample. The proposed GM-SOP can flexibly accommodate a large number of personalized SOP candidates in an efficient way, leading to richer representations. The deep networks with our GM-SOP can be end-to-end trained, having potential to characterize complex, multi-modal distributions. The proposed method is evaluated on two large scale image benchmarks (i.e., downsampled ImageNet-1K and Places365), and experimental results show our GM-SOP is superior to its counterparts and achieves very competitive performance. The source code will be available at http : //www.peihuali.org/GM- SOP.	[Wang, Qilong] Tianjin Univ, Tianjin, Peoples R China; [Wang, Qilong; Gao, Zilin; Xie, Jiangtao; Li, Peihua] Dalian Univ Technol, Dalian, Peoples R China; [Zuo, Wangmeng] Harbin Inst Technol, Harbin, Heilongjiang, Peoples R China	Tianjin University; Dalian University of Technology; Harbin Institute of Technology	Li, PH (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.	qlwang@tju.edu.cn; gzl@mail.dlut.edu.cn; jiangtaoxie@mail.dlut.edu.cn; wmzuo@hit.edu.cn; peihuali@dlut.edu.cn	Zuo, Wangmeng/B-3701-2008	Gao, Zilin/0000-0003-1757-9349	National Natural Science Foundation of China [61471082, 61671182, 61806140]; State Key Program of National Natural Science Foundation of China [61732011]; China Post-doctoral Programme Foundation for Innovative Talent	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); State Key Program of National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Post-doctoral Programme Foundation for Innovative Talent	The work was supported by the National Natural Science Foundation of China (Grant No. 61471082, 61671182, 61806140) and the State Key Program of National Natural Science Foundation of China (Grant No. 61732011). Qilong Wang was supported by China Post-doctoral Programme Foundation for Innovative Talent. We thank NVIDIA corporation for donating GPU.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic Relja, 2016, CVPR; Arsigny  V., 2005, MICCAI; Arslan O, 2004, J STAT PLAN INFER, V118, P115, DOI 10.1016/S0378-3758(02)00402-0; Carreira J, 2015, IEEE T PATTERN ANAL, V37, P1177, DOI 10.1109/TPAMI.2014.2361137; Chen Y., 2017, NIPS; Chrabaszcz  P., 2017, ABS170708819 ARXIV; Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Donoho D. L., 2014, 13110851 ARXIV; Dryden L., 2009, ANN APPL STAT; Everitt B. S., 1981, FINITE MIXTURE DISTR, DOI 10.1007/978-94-009-5897-5; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Higham NJ, 2008, FUNCTIONS MATRICES T; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Ionescu C., 2015, ARXIV150907838; Ionescu C, 2015, IEEE I CONF COMP VIS, P2965, DOI 10.1109/ICCV.2015.339; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Koniusz P, 2017, IEEE T PATTERN ANAL, V39, P313, DOI 10.1109/TPAMI.2016.2545667; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li PH, 2018, PROC CVPR IEEE, P947, DOI 10.1109/CVPR.2018.00105; Li PH, 2017, IEEE I CONF COMP VIS, P2089, DOI 10.1109/ICCV.2017.228; Li YH, 2017, IEEE I CONF COMP VIS, P2098, DOI 10.1109/ICCV.2017.229; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; McLachlan G, 2005, FINITE MIXTURE MODEL; Nguyen M. T., 2016, ABS161201942 ARXIV; Pascal F, 2013, IEEE T SIGNAL PROCES, V61, P5960, DOI 10.1109/TSP.2013.2282909; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun G., 2018, P IEEE CVF C COMP VI, P7132, DOI DOI 10.1109/CVPR.2018.00745; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Viroli  C., 2017, ABS171106929 ARXIV; Wang QS, 2016, PROC CVPR IEEE, P535, DOI 10.1109/CVPR.2016.64; Wang QL, 2017, PROC CVPR IEEE, P6507, DOI 10.1109/CVPR.2017.689; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang T, 2013, IEEE T SIGNAL PROCES, V61, P4141, DOI 10.1109/TSP.2013.2267740; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009	44	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301028
C	Wolter, M; Yao, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wolter, Moritz; Yao, Angela			Complex Gated Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RECOGNITION; GRADIENT	Complex numbers have long been favoured for digital signal processing, yet complex representations rarely appear in deep learning architectures. RNNs, widely used to process time series and sequence information, could greatly benefit from complex representations. We present a novel complex gated recurrent cell, which is a hybrid cell combining complex-valued and norm-preserving state transitions with a gating mechanism. The resulting RNN exhibits excellent stability and convergence properties and performs competitively on the synthetic memory and adding task, as well as on the real-world tasks of human motion prediction.	[Wolter, Moritz] Univ Bonn, Inst Comp Sci, Bonn, Germany; [Yao, Angela] Natl Univ Singapore, Sch Comp, Singapore, Singapore	University of Bonn; National University of Singapore	Wolter, M (corresponding author), Univ Bonn, Inst Comp Sci, Bonn, Germany.	wolter@cs.uni-bonn.de; yaoa@comp.nus.edu.sg			DFG [YA 447/2-1]	DFG(German Research Foundation (DFG))	Research was supported by the DFG project YA 447/2-1 (DFG Research Unit FOR 2535 Anticipating Human Behavior). We also gratefully acknowledge NVIDIA's donation of a Titan X Pascal GPU.	Arjovsky M, 2016, PR MACH LEARN RES, V48; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; BENVENUTO N, 1992, IEEE T SIGNAL PROCES, V40, P967, DOI 10.1109/78.127967; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Gehring J., 2017, P ICML; GEORGIOU GM, 1992, IEEE T CIRCUITS-II, V39, P330, DOI 10.1109/82.142037; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Guberman N., 2016, TECHNICAL REPORT; Hirose A, 2013, COMPLEX VALUED NEURA; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hyland S., 2017, AAAI; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Jain A, 2016, PROC CVPR IEEE, P5308, DOI 10.1109/CVPR.2016.573; Jing L., 2017, INT C MACHINE LEARNI, P1733; Jing L., 2018, AAAI WORKSH; Kim T., 2001, ICASSP; Koppula HS, 2016, IEEE T PATTERN ANAL, V38, P14, DOI 10.1109/TPAMI.2015.2430335; Kovar L, 2002, ACM T GRAPHIC, V21, P473, DOI 10.1145/566570.566605; Kreutz-Delgado K., 2009, ARXIV PREPRINT ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LEUNG H, 1991, IEEE T SIGNAL PROCES, V39, P2101, DOI 10.1109/78.134446; Li H., 2008, EURASIP J ADV SIGNAL; Liouville J., 1879, J REINE ANGEW MATH Z, P1879; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Mandic D.P., 2009, COMPLEX VALUED NONLI, DOI 10.1002/9780470742624; Martinez J, 2017, PROC CVPR IEEE, P4674, DOI 10.1109/CVPR.2017.497; Mikolov T., 2012, TECHNICAL REPORT; Nair V, 2010, P 27 INT C MACHINE L, P807; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Ravanelli M, 2017, INTERSPEECH, P1308, DOI 10.21437/Interspeech.2017-775; Reichert D. P., 2014, ICLR; Tagare Hemant D., 2011, TECHNICAL REPORT; Thickstun J., 2017, INT C LEARN REPR ICL; Trabalón Carina I., 2018, Polis, V17, P163, DOI 10.32735/s0718-6568/2018-n51-1354; VANDENBOS A, 1994, IEE P-VIS IMAGE SIGN, V141, P380, DOI 10.1049/ip-vis:19941555; Virtue P, 2017, IEEE IMAGE PROC, P3953; Wayne G., 2016, ICML; Wirtinger W., 1927, MATH ANN; Wisdom S, 2016, ADV NEUR IN, V29; Yao A, 2012, INT J COMPUT VISION, V100, P16, DOI 10.1007/s11263-012-0532-9; Zeiler MD, 2013, INT CONF ACOUST SPEE, P3517, DOI 10.1109/ICASSP.2013.6638312	42	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005014
C	Yang, X; Xu, K; Chen, SZ; He, SF; Yin, BC; Lau, RWH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yang, Xin; Xu, Ke; Chen, Shaozhe; He, Shengfeng; Yin, Baocai; Lau, Rynson W. H.			Active Matting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Image matting is an ill-posed problem. It requires a user input trimap or some strokes to obtain an alpha matte of the foreground object. A fine user input is essential to obtain a good result, which is either time consuming or suitable for experienced users who know where to place the strokes. In this paper, we explore the intrinsic relationship between the user input and the matting algorithm to address the problem of where and when the user should provide the input. Our aim is to discover the most informative sequence of regions for user input in order to produce a good alpha matte with minimum labeling efforts. To this end, we propose an active matting method with recurrent reinforcement learning. The proposed framework involves human in the loop by sequentially detecting informative regions for trivial human judgement. Comparing to traditional matting algorithms, the proposed framework requires much less efforts, and can produce satisfactory results with just 10 regions. Through extensive experiments, we show that the proposed model reduces user efforts significantly and achieves comparable performance to dense trimaps in a user-friendly manner. We further show that the learned informative knowledge can be generalized across different matting algorithms.	[Yang, Xin; Chen, Shaozhe; Yin, Baocai] Dalian Univ Technol, Dalian, Peoples R China; [Yang, Xin; Xu, Ke; Lau, Rynson W. H.] City Univ Hong Kong, Hong Kong, Peoples R China; [He, Shengfeng] South China Univ Technol, Guangzhou, Guangdong, Peoples R China	Dalian University of Technology; City University of Hong Kong; South China University of Technology	He, SF (corresponding author), South China Univ Technol, Guangzhou, Guangdong, Peoples R China.	xinyang@dlut.edu.cn; kkangwing@mail.dlut.edu.cn; csz@mail.dlut.edu.cn; hesfe@scut.edu.cn; ybc@dlut.edu.cn; rynson.lau@cityu.edu.hk	He, Shengfeng/E-5682-2016; , kk/AAH-8764-2020	He, Shengfeng/0000-0002-3802-4644; , Xin/0000-0002-8046-722X; XU, Ke/0000-0001-5855-3810	SRG grant from City University of Hong Kong [7004889]; NSFC grants from National Natural Science Foundation of China [91748104, 61632006, 61425002, 61702194]; Guangzhou Key Industrial Technology Research fund [201802010036]	SRG grant from City University of Hong Kong; NSFC grants from National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Guangzhou Key Industrial Technology Research fund	We thank the anonymous reviewers for the insightful and constructive comments, and NVIDIA for generous donation of GPU cards for our experiments. This work is in part supported by an SRG grant from City University of Hong Kong (Ref. 7004889), NSFC grants from National Natural Science Foundation of China (Ref. 91748104, 61632006, 61425002, 61702194), and the Guangzhou Key Industrial Technology Research fund (No. 201802010036).	Abadi M, 2015, P 12 USENIX S OPERAT; Aksoy Y, 2017, PROC CVPR IEEE, P228, DOI 10.1109/CVPR.2017.32; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba J., 2014, ARXIV; Chen QF, 2013, IEEE T PATTERN ANAL, V35, P2175, DOI 10.1109/TPAMI.2013.18; Cho D, 2016, LECT NOTES COMPUT SC, V9906, P626, DOI 10.1007/978-3-319-46475-6_39; Chuang YY, 2001, PROC CVPR IEEE, P264; Gastal ESL, 2010, COMPUT GRAPH FORUM, V29, P575, DOI 10.1111/j.1467-8659.2009.01627.x; Guan Y, 2006, COMPUT GRAPH FORUM, V25, P567, DOI 10.1111/j.1467-8659.2006.00976.x; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kaiming He, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2049, DOI 10.1109/CVPR.2011.5995495; Levin A, 2008, IEEE T PATTERN ANAL, V30, P228, DOI 10.1109/TPAMI.2007.1177; Mnih V, 2014, ADV NEUR IN, V27; Rhemann C, 2008, CVPR, P1; Rhemann C, 2009, PROC CVPR IEEE, P1826, DOI 10.1109/CVPRW.2009.5206503; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Shahrian E, 2013, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2013.88; Shen X, 2016, LECT NOTES COMPUT SC, V9905, P92, DOI 10.1007/978-3-319-46448-0_6; Sun J, 2004, ACM T GRAPHIC, V23, P315, DOI 10.1145/1015706.1015721; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Wang J, 2005, IEEE I CONF COMP VIS, P936; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zheng YJ, 2009, IEEE I CONF COMP VIS, P889, DOI 10.1109/ICCV.2009.5459326; Zhu QS, 2015, IEEE T NEUR NET LEAR, V26, P185, DOI 10.1109/TNNLS.2014.2369426	24	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304059
C	Acerbi, L; Ma, WJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Acerbi, Luigi; Ma, Wei Ji			Practical Bayesian Optimization for Model Fitting with Bayesian Adaptive Direct Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GLOBAL OPTIMIZATION; NONLINEAR OPTIMIZATION; MEMORY; ALGORITHMS	Computational models in fields such as computational neuroscience are often evaluated via stochastic simulation or numerical approximation. Fitting these models implies a difficult optimization problem over complex, possibly noisy parameter landscapes. Bayesian optimization (BO) has been successfully applied to solving expensive black-box problems in engineering and machine learning. Here we explore whether BO can be applied as a general tool for model fitting. First, we present a novel hybrid BO algorithm, Bayesian adaptive direct search (BADS), that achieves competitive performance with an affordable computational overhead for the running time of typical models. We then perform an extensive benchmark of BADS vs. many common and state-of-the-art nonconvex, derivative-free optimizers, on a set of model-fitting problems with real data and models from six studies in behavioral, cognitive, and computational neuroscience. With default settings, BADS consistently finds comparable or better solutions than other methods, including 'vanilla' BO, showing great promise for advanced BO techniques, and BADS in particular, as a general model-fitting tool.	[Acerbi, Luigi; Ma, Wei Ji] NYU, Ctr Neural Sci, New York, NY 10003 USA; [Ma, Wei Ji] NYU, Dept Psychol, New York, NY 10003 USA; [Acerbi, Luigi] Univ Geneva, CMU, Dept Neurosci Fondamentales, 1 Rue Michel Servet, CH-1206 Geneva, Switzerland	New York University; New York University; University of Geneva	Acerbi, L (corresponding author), NYU, Ctr Neural Sci, New York, NY 10003 USA.; Acerbi, L (corresponding author), Univ Geneva, CMU, Dept Neurosci Fondamentales, 1 Rue Michel Servet, CH-1206 Geneva, Switzerland.	luigi.acerbi@nyu.edu; weijima@nyu.edu	Jeong, Yongwook/N-7413-2016					Acerbi L., 2017, BIORXIV150052; Adler W. T., 2017, BIORXIV093203; Audet C, 2006, SIAM J OPTIMIZ, V17, P188, DOI 10.1137/040603371; Audet C, 2007, SIAM J OPTIMIZ, V18, P1501, DOI 10.1137/060671267; Bergstra J. S., 2011, P 24 INT C NEUR INF, P2546, DOI DOI 10.1145/3065386; Bergstra J, 2012, J MACH LEARN RES, V13, P281; BRATLEY P, 1988, ACM T MATH SOFTWARE, V14, P88, DOI 10.1145/42288.214372; Brochu E, 2010, ARXIV PREPRINT ARXIV; Clarke F.H, 1990, CANADIAN MATH SOC SE, V2; Csendes T, 2008, OPTIM LETT, V2, P445, DOI 10.1007/s11590-007-0072-3; Gill P. E., 1981, PRACTICAL OPTIMIZATI; Goldberg DE, 1989, GENETIC ALGORITHMS S; Goris RLT, 2015, NEURON, V88, P819, DOI 10.1016/j.neuron.2015.10.009; Gramacy RB, 2015, PAC J OPTIM, V11, P419; Gramacy RB, 2012, STAT COMPUT, V22, P713, DOI 10.1007/s11222-010-9224-x; Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970; Hansen N, 2009, REAL PARAMETER BLACK; Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142; Hoffman M., 2011, P 27 C UNCERTAINTY A, P327; Huyer W, 1999, J GLOBAL OPTIM, V14, P331, DOI 10.1023/A:1008382309369; Jastrebski GA, 2006, IEEE C EVOL COMPUTAT, P2799, DOI 10.1109/cec.2006.1688662; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kording KP, 2007, PLOS ONE, V2, DOI 10.1371/journal.pone.0000943; Kushner HaroldJ., 1964, J BASIC ENG-T ASME, V86, P97, DOI [10.1115/1.3653121, DOI 10.1115/1.3653121]; Lagarias JC, 1998, SIAM J OPTIMIZ, V9, P112, DOI 10.1137/S1052623496303470; Ma WJ, 2011, NAT NEUROSCI, V14, P783, DOI 10.1038/nn.2814; Martinez-Cantin R, 2014, J MACH LEARN RES, V15, P3735; Mazyar H, 2012, J VISION, V12, DOI 10.1167/12.6.10; Mickes L, 2007, PSYCHON B REV, V14, P858, DOI 10.3758/BF03194112; Mockus J., 1978, Towards global optimisation. II, P117; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Picheny V, 2014, COMPUT STAT DATA AN, V71, P1035, DOI 10.1016/j.csda.2013.03.018; Picheny V, 2013, TECHNOMETRICS, V55, P2, DOI 10.1080/00401706.2012.707580; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rios LM, 2013, J GLOBAL OPTIM, V56, P1247, DOI 10.1007/s10898-012-9951-y; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Shiffrin RM, 1997, PSYCHON B REV, V4, P145, DOI 10.3758/BF03209391; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Taddy MA, 2009, TECHNOMETRICS, V51, P389, DOI 10.1198/TECH.2009.08007; Talgorn B, 2015, J MECH DESIGN, V137, DOI 10.1115/1.4028756; van den Berg R, 2017, PSYCHOL REV, V124, P197, DOI 10.1037/rev0000060; van den Berg R, 2012, P NATL ACAD SCI USA, V109, P8780, DOI 10.1073/pnas.1117465109; van Opheusden B, 2016, LECT NOTES COMPUT SC, V10068, P212, DOI 10.1007/978-3-319-50935-8_20; Waltz RA, 2006, MATH PROGRAM, V107, P391, DOI 10.1007/s10107-004-0560-5	49	13	13	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401084
C	Benaim, S; Wolf, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Benaim, Sagie; Wolf, Lior			One-Sided Unsupervised Domain Mapping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In unsupervised domain mapping, the learner is given two unmatched datasets A and B. The goal is to learn a mapping G(AB) that translates a sample in A to the analog sample in B. Recent approaches have shown that when learning simultaneously both G(AB) and the inverse mapping G(BA), convincing mappings are obtained. In this work, we present a method of learning G(AB) without learning G(BA). This is done by learning a mapping that maintains the distance between a pair of samples. Moreover, good mappings are obtained, even by maintaining the distance between different parts of the same sample before and after mapping. We present experimental results that the new method not only allows for one sided mapping learning, but also leads to preferable numerical results over the existing circularity-based constraint. Our entire code is made publicly available at https : //github. com/sagiebenaim/DistanceGAN.	[Benaim, Sagie; Wolf, Lior] Tel Aviv Univ, Blavatn Sch Comp Sci, Tel Aviv, Israel; [Wolf, Lior] Facebook AI Res, New York, NY USA	Tel Aviv University; Facebook Inc	Benaim, S (corresponding author), Tel Aviv Univ, Blavatn Sch Comp Sci, Tel Aviv, Israel.		Jeong, Yongwook/N-7413-2016		European Research Council (ERC) under the European Union [ERC CoG 725974]	European Research Council (ERC) under the European Union(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant ERC CoG 725974). The authors would like to thank Laurens van der Maaten and Ross Girshick for insightful discussions.	[Anonymous], 2017, INT C MACH LEARN ICM; [Anonymous], 2017, INT C LEARN REPR ICL; Bousmalis Konstantinos, 2017, CVPR; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Fidler S, 2012, NIPS, P620; GATYS L. A., 2016, CVPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hoffman J., 2016, FCNS WILD PIXEL LEVE, V12; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Isola P., 2017, CVPR; Johnson J, 2016, ECCV; LeCun Y, 2010, ATT LAB; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Liu Ziwei, 2015, P INT C COMP VIS ICC; Mao X., 2016, ARXIV161104076; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Parkhi O. M., 2015, BRIT MACH VIS C; Paysan Pascal, 2009, AVSS; Radford A., 2015, P COMP C; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Shrivastava A, 2016, ARXIV161207828; Van Belle Werner, 2006, CORRELATION INPRODUC; Vedaldi Andrea, 2016, TEXTURE NETWORKS FEE; Vinyals O., 2016, ICLR WORKSH; Xia Y., 2016, ARXIV161100179; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; Yi Z., 2017, ARXIV170402510; Zhu J.-Y., 2017, ARXIV170310593	28	13	13	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400072
C	Speiser, A; Yan, JY; Archer, E; Buesing, L; Turaga, SC; Macke, JH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Speiser, Artur; Yan, Jinyao; Archer, Evan; Buesing, Lars; Turaga, Srinivas C.; Macke, Jakob H.			Fast amortized inference of neural activity from calcium imaging data with variational autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SELECTIVITY; BRAIN	Calcium imaging permits optical measurement of neural activity. Since intracellular calcium concentration is an indirect measurement of neural activity, computational tools are necessary to infer the true underlying spiking activity from fluorescence measurements. Bayesian model inversion can be used to solve this problem, but typically requires either computationally expensive MCMC sampling, or faster but approximate maximum-a-posteriori optimization. Here, we introduce a flexible algorithmic framework for fast, efficient and accurate extraction of neural spikes from imaging data. Using the framework of variational autoencoders, we propose to amortize inference by training a deep neural network to perform model inversion efficiently. The recognition network is trained to produce samples from the posterior distribution over spike trains. Once trained, performing inference amounts to a fast single forward pass through the network, without the need for iterative optimization or sampling. We show that amortization can be applied flexibly to a wide range of nonlinear generative models and significantly improves upon the state of the art in computation time, while achieving competitive accuracy. Our framework is also able to represent posterior distributions over spike-trains. We demonstrate the generality of our method by proposing the first probabilistic approach for separating backpropagating action potentials from putative synaptic inputs in calcium imaging of dendritic spines.	[Speiser, Artur; Macke, Jakob H.] Research Ctr Caesar, Bonn, Germany; [Speiser, Artur] IMPRS Brain & Behav Bonn Florida, Bonn, Germany; [Yan, Jinyao; Turaga, Srinivas C.] HHMI Janelia Res Campus, Ashburn, VA USA; [Archer, Evan; Buesing, Lars] Columbia Univ, New York, NY 10027 USA; [Archer, Evan] Cogitai Inc, Anaheim, CA USA; [Buesing, Lars] DeepMind, London, England; [Macke, Jakob H.] Tech Univ Darmstadt, Ctr Cognit Sci, Darmstadt, Germany	Columbia University; Technical University of Darmstadt	Speiser, A (corresponding author), Research Ctr Caesar, Bonn, Germany.; Speiser, A (corresponding author), IMPRS Brain & Behav Bonn Florida, Bonn, Germany.	artur.speiser@caesar.de; turagas@janelia.hhmi.org; jakob.macke@caesar.de	Jeong, Yongwook/N-7413-2016		German Research Foundation (DFG) [SFB 1089]; IMPRS for Brain & Behavior scholarship by the Max Planck Society	German Research Foundation (DFG)(German Research Foundation (DFG)); IMPRS for Brain & Behavior scholarship by the Max Planck Society	We thank T. W. Chen, K. Svoboda and the GENIE project at Janelia Research Campus for sharing their published GCaMP6 data, available at http://crcns.org.We also thank T. Deneux for sharing his results for comparison and comments on the manuscript and D. Greenberg, L. Paninski and A. Mnih for discussions. This work was supported by SFB 1089 of the German Research Foundation (DFG) to J.H. Macke. A. Speiser was funded by an IMPRS for Brain & Behavior scholarship by the Max Planck Society.	Ahrens MB, 2012, NATURE, V485, P471, DOI 10.1038/nature11057; [Anonymous], NIPS WORKSH ADV APPR; Apthorpe N., 2016, ADV NEURAL INFORM PR, P3270; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344; Burda Yuri, 2015, ARXIV150900519; Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Deneux T., 2016, NATURE COMMUNICATION, V7; Friedrich J., 2016, FAST ACTIVE SET METH; Ganmor E., 2016, ARXIV160100364; Greenberg D., 2015, 2015 NEUR M PLANN; Grienberger C, 2012, NEURON, V73, P862, DOI 10.1016/j.neuron.2012.02.011; Kerr JND, 2008, NAT REV NEUROSCI, V9, P195, DOI 10.1038/nrn2338; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Larochelle H., 2011, INT C ART INT STAT; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Pachitariu M., 2017, BIORXIV; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Pnevmatikakis E. A., 2016, NEURON; Pnevmatikakis EA, 2014, ARXIV14092903; Pnevmatikakis EA, 2013, CONF REC ASILOMAR C, P349, DOI 10.1109/ACSSC.2013.6810293; Rahmati V, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004736; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Smith SL, 2013, NATURE, V503, P115, DOI 10.1038/nature12600; Sonderby C. K., 2016, 33 INT C MACH LEARN; Theis L, 2016, NEURON, V90, P471, DOI 10.1016/j.neuron.2016.04.014; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; TSIEN RY, 1980, BIOCHEMISTRY-US, V19, P2396, DOI 10.1021/bi00552a018; van den Oord A, 2016, PR MACH LEARN RES, V48; Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009; Vogelstein JT, 2009, BIOPHYS J, V97, P636, DOI 10.1016/j.bpj.2008.08.005	36	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404010
C	Wei, CY; Hong, YT; Lu, CJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wei, Chen-Yu; Hong, Yi-Te; Lu, Chi-Jen			Online Reinforcement Learning in Stochastic Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study online reinforcement learning in average-reward stochastic games (SGs). An SG models a two-player zero-sum game in a Markov environment, where state transitions and one-step payoffs are determined simultaneously by a learner and an adversary. We propose the UCSG algorithm that achieves a sublinear regret compared to the game value when competing with an arbitrary opponent. This result improves previous ones under the same setting. The regret bound has a dependency on the diameter, which is an intrinsic value related to the mixing property of SGs. If we let the opponent play an optimistic best response to the learner, UCSG finds an epsilon-maximin stationary policy with a sample complexity of (O) over tilde (poly(1/epsilon)), where epsilon is the gap to the best policy.	[Wei, Chen-Yu; Hong, Yi-Te; Lu, Chi-Jen] Acad Sinica, Inst Informat Sci, Taipei, Taiwan	Academia Sinica - Taiwan	Wei, CY (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.	bahh723@iis.sinica.edu.tw; ted0504@iis.sinica.edu.tw; cjlu@iis.sinica.edu.tw	Lu, Chi-Jen/AAQ-3728-2021; Jeong, Yongwook/N-7413-2016					Abbasi Yasin, 2013, ADV NEURAL INFORM PR; Bartlett Peter L, 2009, P C UNC ART INT; Bowling Michael, 2001, INT JOINT C ART INT; Brafman R. I., 2002, J MACHINE LEARNING R; Bubeck Sebastien, 2012, C LEARN THEOR; Cho Grace E, 2000, LINEAR ALGEBRA ITS A; Conitzer V., 2007, MACHINE LEARNING; Dann Christoph, 2015, ADV NEURAL INFORM PR; Dick Travis, 2014, P INT C MACH LEARN; Even-Dar Eyal, 2009, MATH OPERATIONS RES; Federgruen Awi, 1978, ADV APPL PROBABILITY; Garivier A., 2016, C LEARN THEOR, P1028; Hordijk Arie, 1974, MC TRACTS; Hunter Jeffrey J, 2005, LINEAR ALGEBRA ITS A; Hunter Jeffrey J, 1982, LINEAR ALGEBRA ITS A; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Jaakkola Tommi, 1994, NEURAL COMPUTATION; Jaksch Thomas, 2010, J MACHINE LEARNING R; Kakade Sham M., 2003, THESIS; Lagoudakis Michail G, 2002, P C UNC ART INT; Lattimore Tor, 2012, INT C ALG LEARN THEO; Lim SH, 2016, MATH OPER RES, V41, P1325, DOI 10.1287/moor.2016.0779; Littman Michael, 1994, P INT C MACH LEARN; MAURER A., 2009, C LEARN THEOR; Mertens J- F, 1981, INT J GAME THEORY; Neu G., 2010, ADV NEURAL INFORM PR; Neu Gergely, 2012, AISTATS; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Perolat J., 2015, P INT C MACH LEARN; Prasad HL, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1371; SHAPLEY LS, 1953, P NATL ACAD SCI; Szepesvari Csaba, 1996, P INT C MACH LEARN; Van der Wal J, 1980, INT J GAME THEORY; Yu Jia Yuan, 2009, P C DEC CONTR	35	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405007
C	Xu, Y; Liu, MR; Lin, QH; Yang, TB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Yi; Liu, Mingrui; Lin, Qihang; Yang, Tianbao			ADMM without a Fixed Penalty Parameter: Faster Convergence with New Adaptive Penalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALTERNATING DIRECTION METHOD; OPTIMIZATION; ALGORITHMS	Alternating direction method of multipliers (ADMM) has received tremendous interest for solving numerous problems in machine learning, statistics and signal processing. However, it is known that the performance of ADMM and many of its variants is very sensitive to the penalty parameter of a quadratic penalty applied to the equality constraints. Although several approaches have been proposed for dynamically changing this parameter during the course of optimization, they do not yield theoretical improvement in the convergence rate and are not directly applicable to stochastic ADMM. In this paper, we develop a new ADMM and its linearized variant with a new adaptive scheme to update the penalty parameter. Our methods can be applied under both deterministic and stochastic optimization settings for structured non-smooth objective function. The novelty of the proposed scheme lies at that it is adaptive to a local sharpness property of the objective function, which marks the key difference from previous adaptive scheme that adjusts the penalty parameter per-iteration based on certain conditions on iterates. On theoretical side, given the local sharpness characterized by an exponent theta is an element of (0; 1], we show that the proposed ADMM enjoys an improved iteration complexity of (O) over tilde (1/epsilon(1-theta))(1) in the deterministic setting and an iteration complexity of (O) over tilde (1/epsilon(2(1-theta))) in the stochastic setting without smoothness and strong convexity assumptions. The complexity in either setting improves that of the standard ADMM which only uses a fixed penalty parameter. On the practical side, we demonstrate that the proposed algorithms converge comparably to, if not much faster than, ADMM with a fine-tuned fixed penalty parameter.	[Xu, Yi; Liu, Mingrui; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Lin, Qihang] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA	University of Iowa; University of Iowa	Xu, Y (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	yi-xu@uiowa.edu; mingrui-liu@uiowa.edu; qihang-lin@uiowa.edu; tianbao-yang@uiowa.edu		Liu, Mingrui/0000-0002-5181-3429	National Science Foundation [IIS-1463988, IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their helpful comments. Y. Xu, M. Liu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995). Y. Xu would like to thank Yan Yan for useful discussions on the low-rank representation experiments.	[Anonymous], 2011, JMLR WORKSHOP C P; Bolte Jerome, 2015, CORR; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Goldstein T, 2014, SIAM J IMAGING SCI, V7, P1588, DOI 10.1137/120896219; He BS, 2015, NUMER MATH, V130, P567, DOI 10.1007/s00211-014-0673-6; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434; Hong MY, 2017, MATH PROGRAM, V162, P165, DOI 10.1007/s10107-016-1034-2; Kurdyka K, 1998, ANN I FOURIER, V48, P769, DOI 10.5802/aif.1638; Li GY, 2013, MATH PROGRAM, V137, P37, DOI 10.1007/s10107-011-0481-z; Lin Z., 2011, PROC INT 25 C NEURAL, P612, DOI DOI 10.1007/S11263-013-0611-6; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu M., 2017, CORR; Luo Z.Q, 2000, HIGH PERFORMANCE OPT, P383; Monteiro RDC, 2013, SIAM J OPTIMIZ, V23, P475, DOI 10.1137/110849468; Necoara I., 2015, CORR; Ouyang Hua, 2013, P 30 INT C MACH LEAR, P80; Ouyang YY, 2015, SIAM J IMAGING SCI, V8, P644, DOI 10.1137/14095697X; Suzuki T., 2013, P INT C MACH LEARN A; Suzuki T, 2014, PR MACH LEARN RES, V32; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878; Xu Y, 2017, PR MACH LEARN RES, V70; Xu Yi, 2016, ADV NEURAL INFORM PR, P1208; Xu Z., 2016, CORR; Yang T.-J., 2016, CORR; Zhang XG, 2016, I C OPT COMMUN NETW; Zhang XQ, 2011, J SCI COMPUT, V46, P20, DOI 10.1007/s10915-010-9408-8; Zhang XQ, 2010, SIAM J IMAGING SCI, V3, P253, DOI 10.1137/090746379; Zhao PL, 2015, PR MACH LEARN RES, V37, P69; Zheng S., 2016, 25 INT JOINT C ART I; Zhong LW, 2014, PR MACH LEARN RES, V32	37	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401030
C	Zhang, ZM; Brand, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhang, Ziming; Brand, Matthew			Convergent Block Coordinate Descent for Training Tikhonov Regularized Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMIZATION	By lifting the ReLU function into a higher dimensional space, we develop a smooth multi-convex formulation for training feed-forward deep neural networks (DNNs). This allows us to develop a block coordinate descent (BCD) training algorithm consisting of a sequence of numerically well-behaved convex optimizations. Using ideas from proximal point methods in convex analysis, we prove that this BCD algorithm will converge globally to a stationary point with R-linear convergence rate of order one. In experiments with the MNIST database, DNNs trained with this BCD algorithm consistently yielded better test-set error rates than identical DNN architectures trained via all the stochastic gradient descent (SGD) variants in the Caffe toolbox.	[Zhang, Ziming; Brand, Matthew] MERL, Cambridge, MA 02139 USA		Zhang, ZM (corresponding author), MERL, Cambridge, MA 02139 USA.	zzhang@merl.com; brand@merl.com	Zhang, ziming/HGA-8604-2022					Baldassarre L., ADV TOPICS MACHINE 2; Baldassi C, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.128101; Bauschke HH, 1996, SIAM REV, V38, P367, DOI 10.1137/S0036144593251710; Bengio, 2014, ARXIV14077906; Bottou L., 2016, ARXIV160604838; Bottou Leon, 2012, NEURAL NETWORKS TRIC, P421, DOI [10.1007/978-3-642-35289-8_25, DOI 10.1007/978-3-642-35289-8_25]; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Burges, 1998, MNIST DATABASE HANDW; Chaudhari P, 2016, ARXIV161101838; Chaudhari Pratik, 2017, ICLR POSTER; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Eriksson K., 2003, APPL MATH BODY SOUL, VI-III; Gal Y., 2015, ADV APPR BAYES INF W; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Glorot X., 2010, PROC MACH LEARN RES, P249; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kingma D.P, P 3 INT C LEARNING R; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Krogh Anders, 1991, P NIPS, P950; Lanckriet G. R., 2009, P 22 INT C NEURAL IN, P1759; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nishihara R, 2015, PR MACH LEARN RES, V37, P343; Nocedal J., 2006, NUMERICAL OPTIMIZATI; Razaviyayn M., 2014, P NEUR INF PROC NIPS, P1440; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Taylor G, 2016, PR MACH LEARN RES, V48; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tikhonov A.N., 1979, SIAM REV, V21, P266, DOI [10.1137/1021044, DOI 10.1137/1021044]; Wang Y., 2015, ARXIV151106324, DOI [10.1155/2015/893074, DOI 10.1155/2015/712349]; XU Y., 2014, ARXIV14101386; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795; Zeiler Matthew D, 2012, ARXIV12125701; Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79	41	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401073
C	Li, B; Wang, YN; Singh, A; Vorobeychik, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Bo; Wang, Yining; Singh, Aarti; Vorobeychik, Yevgeniy			Data Poisoning Attacks on Factorization-Based Collaborative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recommendation and collaborative filtering systems are important in modern information and e-commerce applications. As these systems are becoming increasingly popular in the industry, their outputs could affect business decision making, introducing incentives for an adversarial party to compromise the availability or integrity of such systems. We introduce a data poisoning attack on collaborative filtering systems. We demonstrate how a powerful attacker with full knowledge of the learner can generate malicious data so as to maximize his/her malicious objectives, while at the same time mimicking normal user behavior to avoid being detected. While the complete knowledge assumption seems extreme, it enables a robust assessment of the vulnerability of collaborative filtering schemes to highly motivated attacks. We present efficient solutions for two popular factorization-based collaborative filtering algorithms: the alternative minimization formulation and the nuclear norm minimization method. Finally, we test the effectiveness of our proposed algorithms on real-world data and discuss potential defensive strategies.	[Li, Bo; Vorobeychik, Yevgeniy] Vanderbilt Univ, 221 Kirkland Hall, Nashville, TN 37235 USA; [Wang, Yining; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Vanderbilt University; Carnegie Mellon University	Li, B (corresponding author), Vanderbilt Univ, 221 Kirkland Hall, Nashville, TN 37235 USA.	bo.li.2@vanderbilt.edu; ynwang.yining@gmail.com; aarti@cs.cmu.edu; yevgeniy.vorobeychik@vanderbilt.edu			NSF [CNS-1238959, IIS-1526860]; ONR [N00014-15-1-2621]; ARO [W911NF-16-1-0069]; AFRL [FA8750-14-2-0180]; Sandia National Laboratories; Symantec Labs Graduate Research Fellowship	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO; AFRL(United States Department of DefenseUS Air Force Research Laboratory); Sandia National Laboratories(United States Department of Energy (DOE)); Symantec Labs Graduate Research Fellowship	This research was partially supported by the NSF (CNS-1238959, IIS-1526860), ONR (N00014-15-1-2621), ARO (W911NF-16-1-0069), AFRL (FA8750-14-2-0180), Sandia National Laboratories, and Symantec Labs Graduate Research Fellowship.	Alfeld Scott, 2016, AAAI; Barreno M., 2006, P 2006 ACM S INFORM, P16, DOI DOI 10.1145/1128817.1128824; BIGGIO B., 2012, ICML; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes Emmanuel, 2007, FDN COMPUTATIONAL MA, V9, P717; Chen Y., 2011, ICML; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Jain Prateek, 2013, STOC; Klopp Olga, 2014, ARXIV14128132; Li B, 2015, JMLR WORKSH CONF PRO, V38, P599; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Mei S, 2015, AAAI; Mei S., 2015, ARTIFICIAL INTELLIGE; Mobasher B., 2005, P 2005 WEBKDD WORKSH; Nie Feiping, 2012, ICDM; O'Mahony M. P., 2002, Database and Expert Systems Applications. 13th International Conference, DEXA 2002. Proceedings (Lecture Notes in Computer Science Vol.2453), P494; Wang Jun, 2006, SIGIR; Wang Y, 2012, ICML; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xiao Huang, 2015, ICML	22	13	13	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700106
C	Lindgren, EM; Wu, SS; Dimakis, AG		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lindgren, Erik M.; Wu, Shanshan; Dimakis, Alexandros G.			Leveraging Sparsity for Efficient Submodular Data Summarization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The facility location problem is widely used for summarizing large datasets and has additional applications in sensor placement, image retrieval, and clustering. One difficulty of this problem is that submodular optimization algorithms require the calculation of pairwise benefits for all items in the dataset. This is infeasible for large problems, so recent work proposed to only calculate nearest neighbor benefits. One limitation is that several strong assumptions were invoked to obtain provable approximation guarantees. In this paper we establish that these extra assumptions are not necessary-solving the sparsified problem will be almost optimal under the standard assumptions of the problem. We then analyze a different method of sparsification that is a better model for methods such as Locality Sensitive Hashing to accelerate the nearest neighbor computations and extend the use of the problem to a broader family of similarities. We validate our approach by demonstrating that it rapidly generates interpretable summaries.	[Lindgren, Erik M.; Wu, Shanshan; Dimakis, Alexandros G.] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Lindgren, EM (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	erikml@utexas.edu; shanshan@utexas.edu; dimakis@austin.utexas.edu	Dimakis, Alexandros G/A-5496-2011; Dimakis, Alexandros G/P-6034-2019	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	National Science Foundation Graduate Research Fellowship [DGE-1110007]; NSF [CCF 1344179, 1344364, 1407278, 1422549]; ARO [YIP W911NF-14-1-0258]	National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); ARO	This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE-1110007 as well as NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258.	Andoni A., 2012, EXACT ALGORITHMS A 2; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; Avrachenkov K., 2007, SIAM J NUMERICAL ANA; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Barbosa R., 2015, ICML; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Beygelzimer A., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Chen J., 2009, JMLR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; DVORETZKY A, 1956, ANN MATH STAT, V27, P642, DOI 10.1214/aoms/1177728174; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Garcia V., 2010, INT C IM PROC; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gomes R., 2010, P 27 INT C MACHINE L, P391; Gupta P., 2013, WWW; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; IMDb, 2016, ALT INT; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Krause A, 2008, J WATER RES PL-ASCE, V134, P516, DOI 10.1061/(ASCE)0733-9496(2008)134:6(516); Kumar R., 2015, ACM T PARALLEL COMPU, V2, P14; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Lin H., 2012, UAI; Meng X., 2016, J MACH LEARN RES, V17, P1235, DOI DOI 10.1145/2882903.2912565; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mirrokni V, 2015, ACM S THEORY COMPUT, P153, DOI 10.1145/2746539.2746624; Mirzasoleiman B, 2016, PR MACH LEARN RES, V48; Mirzasoleiman B, 2015, AAAI CONF ARTIF INTE, P1812; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Neyshabur B., 2015, ICML; Page L., 1999, PAGERANK CITATION RA; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; Wei K, 2014, PR MACH LEARN RES, V32, P1494; Wei K, 2015, PR MACH LEARN RES, V37, P1954	40	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703018
C	Namkoong, H; Duchi, JC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Namkoong, Hongseok; Duchi, John C.			Stochastic Gradient Methods for Distributionally Robust Optimization with f-divergences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We develop efficient solution methods for a robust empirical risk minimization problem designed to give calibrated confidence intervals on performance and provide optimal tradeoffs between bias and variance. Our methods apply to distributionally robust optimization problems proposed by Ben-Tal et al., which put more weight on observations inducing high loss via a worst-case approach over a non-parametric uncertainty set on the underlying data distribution. Our algorithm solves the resulting minimax problems with nearly the same computational cost of stochastic gradient descent through the use of several carefully designed data structures. For a sample of size n, the per-iteration cost of our method scales as O(log n), which allows us to give optimality certificates that distributionally robust optimization provides at little extra cost compared to empirical risk minimization and stochastic gradient methods.	[Namkoong, Hongseok; Duchi, John C.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Namkoong, H (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	hnamk@stanford.edu; jduchi@stanford.edu		Duchi, John/0000-0003-0045-7185	SAIL-Toyota Center for AI Research; National Science Foundation [NSF-CAREER-1553086]; Samsung	SAIL-Toyota Center for AI Research; National Science Foundation(National Science Foundation (NSF)); Samsung(Samsung)	JCD and HN were partially supported by the SAIL-Toyota Center for AI Research and the National Science Foundation award NSF-CAREER-1553086. HN was also partially supported Samsung Fellowship.	[Anonymous], 2015, GUROBI OPTIMIZER REF; Audibert J-Y, 2010, J MACHINE LEARNING R, V11, P2635; Ben-Tal A, 2015, OPER RES, V63, P628, DOI 10.1287/opre.2015.1374; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; BenTal A, 2009, PRINC SER APPL MATH, P1; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Boyd S, 2004, CONVEX OPTIMIZATION; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Clarkson K., 2012, J ASS COMPUTING MACH, V59; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; CRESSIE N, 1984, J ROY STAT SOC B MET, V46, P440; Duchi J., 2016, ARXIV161002581; Duchi J., 2008, P 25 INT C MACH LEAR; Duchi J. C., 2016, ARXIV161003425STATML; Hazan E., 2011, P 24 ANN C COMP LEAR; Hazan E, 2012, OPTIMIZATION FOR MACHINE LEARNING, P287; Hiriart-Urruty J-B, 1993, CONVEX ANAL MINIMIZA, VII; Hiriart-Urruty Jean-Baptiste, 1993, CONVEX ANAL MINIMIZA; Lichman M., 2013, UCI MACHINE LEARNING; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Owen A.B., 2001, EMPIRICAL LIKELIHOOD; Shalev-Shwartz S., 2016, P 32 INT C MACH LEAR; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Zinkevich M., 2003, INT C MACH LEARN ICM	28	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700005
C	Le, T; Nguyen, TD; Nguyen, V; Phung, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Trung Le; Tu Dinh Nguyen; Vu Nguyen; Dinh Phung			Dual Space Gradient Descent for Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PERCEPTRON	One crucial goal in kernel online learning is to bound the model size. Common approaches employ budget maintenance procedures to restrict the model sizes using removal, projection, or merging strategies. Although projection and merging, in the literature, are known to be the most effective strategies, they demand extensive computation whilst removal strategy fails to retain information of the removed vectors. An alternative way to address the model size problem is to apply random features to approximate the kernel function. This allows the model to be maintained directly in the random feature space, hence effectively resolve the curse of kernelization. However, this approach still suffers from a serious shortcoming as it needs to use a high dimensional random feature space to achieve a sufficiently accurate kernel approximation. Consequently, it leads to a significant increase in the computational cost. To address all of these aforementioned challenges, we present in this paper the Dual Space Gradient Descent (DualSGD), a novel framework that utilizes random features as an auxiliary space to maintain information from data points removed during budget maintenance. Consequently, our approach permits the budget to be maintained in a simple, direct and elegant way while simultaneously mitigating the impact of the dimensionality issue on learning performance. We further provide convergence analysis and extensively conduct experiments on five real-world datasets to demonstrate the predictive performance and scalability of our proposed method in comparison with the state-of-the-art baselines.	[Trung Le; Tu Dinh Nguyen; Vu Nguyen; Dinh Phung] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia	Deakin University	Le, T (corresponding author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.	trung.l@deakin.edu.au; tu.nguyen@deakin.edu.au; v.nguyen@deakin.edu.au; dinh.phung@deakin.edu.au	Nguyen, Vu/AAQ-5062-2020; Nguyen, Vu/HGV-1806-2022	Nguyen, Vu/0000-0002-0294-4561; Phung, Dinh/0000-0002-9977-8247	Australian Research Council [DP160109394]	Australian Research Council(Australian Research Council)	This work is partially supported by the Australian Research Council under the Discovery Project DP160109394.	Cavallanti G, 2007, MACH LEARN, V69, P143, DOI 10.1007/s10994-007-5003-0; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Crammer K, 2006, J MACH LEARN RES, V7, P551; Dekel O., 2005, ADV NEURAL INF PROCE; Dredze M., 2008, P 25 INT C MACHINE L, V307, P264, DOI DOI 10.1145/1390156.1390190; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Le T., 2016, 32 C UNC ART INT JUN; Le T., 2016, 19 INT C ART INT STA; Lu J., 2015, J MACH LEARN RES; Ming L., 2014, ACM T KNOWL DISCOV D, V8; Orabona F, 2009, J MACH LEARN RES, V10, P2643; Rahimi A., 2007, ADV NEURAL INFOMRATI; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Wang Z., 2010, P INT C ART INT STAT, V9, P908; Wang Z, 2012, J MACH LEARN RES, V13, P3103; Zhao P., 2012, CORR	21	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704084
C	Frogner, C; Zhang, CY; Mobahi, H; Araya-Polo, M; Poggio, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Frogner, Charlie; Zhang, Chiyuan; Mobahi, Hossein; Araya-Polo, Mauricio; Poggio, Tomaso			Learning with a Wasserstein Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DISTANCE	Learning to predict multi-label outputs is challenging, but in many problems there is a natural metric on the outputs that can be used to improve predictions. In this paper we develop a loss function for multi-label learning, based on the Wasserstein distance. The Wasserstein distance provides a natural notion of dissimilarity for probability measures. Although optimizing with respect to the exact Wasserstein distance is costly, recent work has described a regularized approximation that is efficiently computed. We describe an efficient learning algorithm based on this regularization, as well as a novel extension of the Wasserstein distance from probability measures to unnormalized measures. We also describe a statistical learning bound for the loss. The Wasserstein loss can encourage smoothness of the predictions with respect to a chosen metric on the output space. We demonstrate this property on a real-data tag prediction problem, using the Yahoo Flickr Creative Commons dataset, outperforming a baseline that doesn't use the metric.	[Frogner, Charlie; Zhang, Chiyuan; Poggio, Tomaso] MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA; [Mobahi, Hossein] MIT, CSAIL, Cambridge, MA 02139 USA; [Araya-Polo, Mauricio] Shell Int E&P Inc, The Hague, Netherlands	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Royal Dutch Shell	Frogner, C (corresponding author), MIT, Ctr Brains Minds & Machines, Cambridge, MA 02139 USA.	frogner@mit.edu; chiyuan@mit.edu; hmobahi@csail.mit.edu; Mauricio.Araya@shell.com; tp@ai.mit.edu	zhang, chi/GRX-3610-2022					Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; [Anonymous], 2014, ICML; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Bogachev VI, 2012, RUSS MATH SURV+, V67, P785, DOI 10.1070/RM2012v067n05ABEH004808; Chen LC., ARXIV 2015ABS1412706; Chizat Lenaic, 2015, UNBALANCED OPTIMAL T; Coen Michael H., 2010, P 27 INT C MACH LEAR, P231; Cuturi M., 2013, NEURIPS; Cuturi M., 2015, SMOOTHED DUAL APPROA; Cuturi M., 2014, ICML; Edelsbrunner H., 2012, P EUR C MATH; GIVENS CR, 1984, MICH MATH J, V31, P231; GRAUMAN K, 2004, CVPR; Knight Philip A, 2012, IMA J NUMER ANAL, V33; Ledoux M, 2011, CLASS MATH, P1; Long J., 2015, CVPR IN PRESS; Mikolov T., 2013, ADV NEURAL INF PROCE, V26; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Shirdhonkar S., 2008, CVPR; Thomee B., 2015, ARXIV150301817; Vedaldi A., 2014, ABS14124564 CORR; Villani C., 2008, OPTIMAL TRANSPORT OL	27	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100047
C	Kozdoba, M; Mannor, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kozdoba, Mark; Mannor, Shie			Community Detection via Measure Space Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				COMPLEX NETWORKS; ALGORITHMS	We present a new algorithm for community detection. The algorithm uses random walks to embed the graph in a space of measures, after which a modification of k -means in that space is applied. The algorithm is therefore fast and easily parallelizable. We evaluate the algorithm on standard random graph benchmarks, including some overlapping community benchmarks, and find its performance to be better or at least as good as previously known algorithms. We also prove a linear time (in number of edges) guarantee for the algorithm on a p, q-stochastic block model with where p >= c . N (-1/2 + epsilon) and p - q >= c' root pN(-1/2+epsilon) logN.	[Kozdoba, Mark; Mannor, Shie] The Technion, Haifa, Israel	Technion Israel Institute of Technology	Kozdoba, M (corresponding author), The Technion, Haifa, Israel.	markk@tx.technion.ac.il; shie@ee.technion.ac.il		Mannor, Shie/0000-0003-4439-7647				Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Amini Arash A., 2013, PSEUDO LIKELIHOOD ME, V41; Anandkumar A., 2013, C LEARNING THEORY, P867; Ball B, 2011, PHYS REV E, V84, DOI 10.1103/PhysRevE.84.036103; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22; Chen YD, 2014, IEEE T INFORM THEORY, V60, P6440, DOI 10.1109/TIT.2014.2346205; Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2; Esquivel AV, 2011, PHYS REV X, V1, DOI 10.1103/PhysRevX.1.021025; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Gregory S, 2010, NEW J PHYS, V12, DOI 10.1088/1367-2630/12/10/103018; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jordan Michael I., 2001, ADV NEURAL INFORM PR, V14; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Lancichinetti A, 2009, PHYS REV E; Lancichinetti A, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016118; Lancichinetti A, 2009, NEW J PHYS, V11, DOI 10.1088/1367-2630/11/3/033015; Newman MEJ, 2007, P NATL ACAD SCI USA, V104, P9564, DOI 10.1073/pnas.0610537104; Palla G, 2005, NATURE, V435, P814, DOI 10.1038/nature03607; Pons P, 2005, LECT NOTES COMPUT SC, V3733, P284; Ronhovde P, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.016109; Rosvall M, 2008, P NATL ACAD SCI USA, V105, P1118, DOI 10.1073/pnas.0706851105; Shamir R, 2007, RANDOM STRUCT ALGOR, V31, P418, DOI 10.1002/rsa.20181; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752	28	13	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101068
C	Jiang, AX; Marcolino, LS; Procaccia, AD; Sandholm, T; Shah, N; Tambe, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Jiang, Albert Xin; Marcolino, Leandro Soriano; Procaccia, Ariel D.; Sandholm, Tuomas; Shah, Nisarg; Tambe, Milind			Diverse Randomized Agents Vote to Win	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We investigate the power of voting among diverse, randomized software agents. With teams of computer Go agents in mind, we develop a novel theoretical model of two-stage noisy voting that builds on recent work in machine learning. This model allows us to reason about a collection of agents with different biases (determined by the first-stage noise models), which, furthermore, apply randomized algorithms to evaluate alternatives and produce votes (captured by the second-stage noise models). We analytically demonstrate that a uniform team, consisting of multiple instances of any single agent, must make a significant number of mistakes, whereas a diverse team converges to perfection as the number of agents grows. Our experiments, which pit teams of computer Go agents against strong agents, provide evidence for the effectiveness of voting when agents are diverse.	[Jiang, Albert Xin] Trinity Univ, San Antonio, TX 78212 USA; [Marcolino, Leandro Soriano; Tambe, Milind] USC, Los Angeles, CA USA; [Procaccia, Ariel D.; Sandholm, Tuomas; Shah, Nisarg] CMU, Pittsburgh, PA USA	Trinity University; University of Southern California; Carnegie Mellon University	Jiang, AX (corresponding author), Trinity Univ, San Antonio, TX 78212 USA.	xjiang@trinity.edu; sorianom@usc.edu; arielpro@cs.cmu.edu; sandholm@cs.cmu.edu; nkshah@cs.cmu.edu; tambe@usc.edu			NSF [IIS-1350598, CCF-1215883]; MURI [W911NF-11-1-0332]	NSF(National Science Foundation (NSF)); MURI(MURI)	Procaccia and Shah were partially supported by the NSF under grants IIS-1350598 and CCF-1215883, and Marcolino by MURI grant W911NF-11-1-0332.	Azari H, 2012, NIPS 12, P126; Azari Soufiani H., 2014, P 31 ICML; Baudis Petr, 2012, Advances in Computer Games. 13th International Conference, ACG 2011. Revised Selected Papers, P24, DOI 10.1007/978-3-642-31866-5_3; Boutilier Craig, 2012, P 13 ACM C EL COMM, P197; Braouezec Y, 2010, COMPUT ECON, V35, P245, DOI 10.1007/s10614-009-9194-2; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Caragiannis I, 2013, EC 13, P143, DOI DOI 10.1145/2482540.2482570; Condorcet N, 1785, ESSAI APPL ANAL PROB; Enzenberger M, 2010, IEEE T COMP INTEL AI, V2, P259, DOI 10.1109/TCIAIG.2010.2083662; Hellinger E, 1909, J REINE ANGEW MATH, V136, P210, DOI 10.1515/crll.1909.136.210; Hong L, 2004, P NATL ACAD SCI USA, V101, P16385, DOI 10.1073/pnas.0403723101; Hong L., 2009, COLLECTIVE WISDOM, P56; LiCalzi M, 2012, MANAGE SCI, V58, P1408, DOI 10.1287/mnsc.1110.1495; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Lu T., 2011, ICML, P145; Luce R.D, 1959, INDIVIDUAL CHOICE BE; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Marcolino L. S., 2014, P 28 AAAI; Marcolino Leandro Soriano, 2013, 23 INT JOINT C ART I; MOSTELLER F, 1951, PSYCHOMETRIKA, V16, P3; Parkes D. C., 2013, P 27 C ART INT AAAI, P767; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; Procaccia AD, 2012, UAI 12, P695; Sandholm T, 2010, AI MAG, V31, P13, DOI 10.1609/aimag.v31i4.2311; SOUFIANI HA, 2013, ADV NEURAL INFORM PR, P2706; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288	26	13	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103038
C	Malinowski, M; Fritz, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Malinowski, Mateusz; Fritz, Mario			A Multi-World Approach to Question Answering about Real-World Scenes based on Uncertain Input	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a method for automatically answering questions about images by bringing together recent advances from natural language processing and computer vision. We combine discrete reasoning with uncertain predictions by a multi-world approach that represents uncertainty about the perceived world in a bayesian framework. Our approach can handle human questions of high complexity about realistic scenes and replies with range of answer like counts, object classes, instances and lists of them. The system is directly trained from question-answer pairs. We establish a first benchmark for this task that can be seen as a modern attempt at a visual turing test.	[Malinowski, Mateusz; Fritz, Mario] Max Planck Inst Informat, Saarbrucken, Germany	Max Planck Society	Malinowski, M (corresponding author), Max Planck Inst Informat, Saarbrucken, Germany.	mmalinow@mpi-inf.mpg.de; mfritz@mpi-inf.mpg.de	Malinowski, Mateusz/AAI-8855-2020					Dickerson S., 2011, AAAI; Fellbaum C, 1999, WORDNET; Guadarrama S., 2013, IROS; Guadarrama S., 2013, INT C COMP VIS; Gupta S., 2013, CVPR; Karpathy A., 2014, NIPS; Kong C., 2014, CVPR; Krishnamurthy J., 2013, JOINTLY LEARNING PAR; Kruijff G. J. M., 2007, IJARS; Kwiatkowski T, 2010, EMNLP; LAN T, 2012, ECCV; Levit M., 2007, SYSTEMS MAN CYBERN B; Liang P., 2013, COMPUTATIONAL LINGUI; Matuszek C., 2013, EXPT ROBOTICS; Matuszek C., 2012, ICML; Miller George A., 1995, CACM; Palmer M., 1994, ACL; Regier T., 2001, J EXPT PSYCHOL GEN; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Silberman N., 2012, ECCV; Tukey J. W., 1977, EXPLORATORY DATA ANA; van de Weijer J., 2007, CVPR; Vogel A., 2010, ACL; Wick M., 2010, VLDB; ZADEH LA, 1965, FUZZY SETS INFORM CO; Zettlemoyer L. S., 2007, EMNLPCONLL2007	26	13	13	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101082
C	Su, WJ; Boyd, S; Candes, EJ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Su, Weijie; Boyd, Stephen; Candes, Emmanuel J.			A Differential Equation for Modeling Nesterov's Accelerated Gradient Method: Theory and Insights	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SYSTEMS	We derive a second-order ordinary differential equation (ODE), which is the limit of Nesterov's accelerated gradient method. This ODE exhibits approximate equivalence to Nesterov's scheme and thus can serve as a tool for analysis. We show that the continuous time ODE allows for a better understanding of Nesterov's scheme. As a byproduct, we obtain a family of schemes with similar convergence rates. The ODE interpretation also suggests restarting Nesterov's scheme leading to an algorithm, which can be rigorously proven to converge at a linear rate whenever the objective is strongly convex.	[Su, Weijie; Candes, Emmanuel J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Boyd, Stephen] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Candes, Emmanuel J.] Stanford Univ, Dept Math, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University	Su, WJ (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	wjsu@stanford.edu; boyd@stanford.edu; candes@stanford.edu						Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Becker S, 2011, SIAM J IMAGING SCI, V4, P1, DOI 10.1137/090756855; Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5; BLOCH AM, 1994, HAMILTONIAN GRADIENT, V3; BRANIN FH, 1972, IBM J RES DEV, V16, P504, DOI 10.1147/rd.165.0504; BROWN AA, 1989, J OPTIMIZ THEORY APP, V62, P211, DOI 10.1007/BF00941054; Hauser R, 2005, SIAM J OPTIMIZ, V15, P915, DOI 10.1137/S1052623403432633; Helmke U., 1996, P IEEE, V84, P907; Leader J. J, 2004, NUMERICAL ANAL SCI C, DOI [10.1201/9781003042273/numerical-analysis-scientific-computationjeffery-leader, DOI 10.1201/9781003042273/NUMERICAL-ANALYSIS-SCIENTIFIC-COMPUTATIONJEFFERY-LEADER]; Monteiro R., 2012, ADAPTIVE ACCELERATED; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2007, CORE DISCUSSION PAPE; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; ODonoghue B., 2013, FDN COMPUT MATH; Ou Y., 2014, INT J COMPUT MATH, P1; ROCKAFELLAR R. T., 1997, PRINCETON LANDMARKS; Schropp J, 2000, NUMER FUNC ANAL OPT, V21, P537, DOI 10.1080/01630560008816971; Tseng P., 2008, SIAM J UNPUB; Tseng P, 2010, MATH PROGRAM, V125, P263, DOI 10.1007/s10107-010-0394-2	20	13	13	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100101
C	Eichhorn, J; Tolias, A; Zien, A; Kuss, M; Rasmussen, CE; Weston, J; Logothetis, N; Schlkopf, B		Thrun, S; Saul, K; Scholkopf, B		Eichhorn, J; Tolias, A; Zien, A; Kuss, M; Rasmussen, CE; Weston, J; Logothetis, N; Schlkopf, B			Prediction on spike data using kernel algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We report and compare the performance of different learning algorithms based on data from cortical recordings. The task is to predict the orientation of visual stimuli from the activity of a population of simultaneously recorded neurons. We compare several ways of improving the coding of the input (i.e., the spike data) as well as of the output (i.e., the orientation), and report the results obtained using different kernel algorithms.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Eichhorn, J (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.		Logothetis, Nikos/U-9282-2019; Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925; Rasmussen, Carl Edward/0000-0001-8899-7850				[Anonymous], 2002, LEARNING KERNELS; FOLDIAK P, 1993, COMPUTATIONAL NEURAL; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; NEEDLEMAN SB, 1970, J MOL BIOL, V48, P443, DOI 10.1016/0022-2836(70)90057-4; Sanger TD, 1996, J NEUROPHYSIOL, V76, P2790, DOI 10.1152/jn.1996.76.4.2790; SHPIGELMAN L, 2003, ADV NEURAL INFORMATI, V15; TOLIAS AS, 2002, SOC NEUR ABST 28; Vapnik V.N, 1998, STAT LEARNING THEORY; Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310; WESTON J, 2003, ADV NEURAL INFORMATI, V15; WILLIAMS CKI, 1996, ADV NEURAL INFORMATI, V8; Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017	12	13	13	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1367	1374						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500170
C	Steck, H; Jaakkola, TS		Thrun, S; Saul, K; Scholkopf, B		Steck, H; Jaakkola, TS			Bias-corrected bootstrap and model uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				BAYESIAN NETWORKS	The bootstrap has become a popular method for exploring model (structure) uncertainty. Our experiments with artificial and real-world data demonstrate that the graphs learned from bootstrap samples can be severely biased towards too complex graphical models. Accounting for this bias is hence essential, e.g., when exploring model uncertainty. We find that this bias is intimately tied to (well-known) spurious dependences induced by the bootstrap. The leading-order bias-correction equals one half of Akaike's penalty for model complexity. We demonstrate the effect of this simple bias-correction in our experiments. We also relate this bias to the bias of the plug-in estimator for entropy, as well as to the difference between the expected test and training errors of a graphical model, which asymptotically equals Akaike's penalty (rather than one half).	MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Steck, H (corresponding author), MIT, CSAIL, 200 Technol Sq, Cambridge, MA 02139 USA.							Akaike H, 1973, INT S INF THEOR, P267, DOI DOI 10.2307/2334537; CARLTON AG, 1969, PSYCHOL BULL, V71, P108, DOI 10.1037/h0026857; COOPER GF, 1991, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P86; Davison A., 1997, BOOTSTRAP METHODS TH, V94; Efron B., 1994, MONOGR STAT APPL PRO, DOI DOI 10.1007/978-1-4899-4541-9; Friedman N, 2000, J COMPUT BIOL, V7, P601, DOI 10.1089/106652700750050961; Friedman N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P196; Friedman N, 1999, ARTIFICIAL INTELLIGENCE AND STATISTICS 99, PROCEEDINGS, P197; HARTEMINK AJ, 2002, PAC S BIOC; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Miller GA., 1955, INF THEORY PSYCHOL P, V2, P95; Spiegelhalter DJ, 2002, J R STAT SOC B, V64, P583, DOI 10.1111/1467-9868.00353; STECK H, 2003, 2003002 AI MIT; STONE M, 1977, J R STAT SOC B, V39, P44, DOI 10.1111/j.2517-6161.1977.tb01603.x; Victor JD, 2000, NEURAL COMPUT, V12, P2797, DOI 10.1162/089976600300014728; [No title captured]	16	13	13	2	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						521	528						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500066
C	Parveen, S; Green, PD		Dietterich, TG; Becker, S; Ghahramani, Z		Parveen, S; Green, PD			Speech recognition with missing data using recurrent neural nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In the 'missing data' approach to improving the robustness of automatic speech recognition to added noise, an initial process identifies spectral-temporal regions which are dominated by the speech source. The remaining regions are considered to be 'missing'. In this paper we develop a connectionist approach to the problem of adapting speech recognition to the missing data case, using Recurrent Neural Networks. In contrast to methods based on Hidden Markov Models, RNNs allow us to make use of long-term time constraints and to make the problems of classification with incomplete data and imputing missing values interact. We report encouraging results on an isolated digit recognition task.	Univ Sheffield, Dept Comp Sci, Speech & Hearing Res Grp, Sheffield S14DP, S Yorkshire, England	University of Sheffield	Parveen, S (corresponding author), Univ Sheffield, Dept Comp Sci, Speech & Hearing Res Grp, Sheffield S14DP, S Yorkshire, England.	s.parveen@dcs.shef.ac.uk; p.green@dcs.shef.ac.uk	Green, Phil/AAV-4599-2020					AHMED S, 1993, ADV NEURAL INFORMATI, P393; BARKER J, 2001, WORKSH INN SPEECH PR; BARKER J, 2000, IN PRESS ICSLP 2000; Bourlard H, 1998, LECT NOTES ARTIF INT, V1387, P389, DOI 10.1007/BFb0054006; COOKE M, 1999, UNPUB SPEECH COMMUNI; COOKE MP, 1996, ESCA TUT WORKSH AUD; David P., 2000, P ICSLP, P29; Drygajlo A, 1998, INT CONF ACOUST SPEE, P121, DOI 10.1109/ICASSP.1998.674382; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Furui S., 1997, P ESCA NATO TUT RES, P11; Ghahramani Z., 1994, P ADV NEUR INF PROC, P120; Gingras F, 1998, INT J COMPUTATIONAL, V1, P154; JORDAN MI, 1998, 8827 COINS TR; JOSIFOVSKI L, 1999, P EUR 99 BUD, V6, P2837; LEONARD RG, 1984, P ICASSP 84, V3; Lippmann RP, 1997, SPEECH COMMUN, V22, P1, DOI 10.1016/S0167-6393(97)00021-6; MORRIS A, 2000, ICSLP 200; PEDERSEN MW, 1997, THESIS TU DENMARK; RAJ B, 2000, ICSLP 2000; SEUNG HS, 1997, P NIPS 97, P654; VIZINHO A, 1999, P EUR 1999 BUD HUNG, V5, P2407	21	13	13	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1189	1195						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100148
C	Boyan, JA; Littman, ML		Leen, TK; Dietterich, TG; Tresp, V		Boyan, JA; Littman, ML			Exact solutions to time-dependent MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We describe an extension of the Markov decision process model in which a continuous time dimension is included in the state space. This allows for the representation and exact solution of a wide range of problems in which transitions or rewards vary over time. We examine problems based on route planning with public transportation and telescope observation scheduling.	ITA Software, Cambridge, MA 02139 USA		Boyan, JA (corresponding author), ITA Software, Bldg 400,1 Kendall Sq, Cambridge, MA 02139 USA.							[No title captured]; [No title captured]; [No title captured]	3	13	13	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1026	1032						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800144
C	Elidan, G; Lotner, N; Friedman, N; Koller, D		Leen, TK; Dietterich, TG; Tresp, V		Elidan, G; Lotner, N; Friedman, N; Koller, D			Discovering hidden variables: A structure-based approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				EM ALGORITHM; NETWORKS	A serious problem in learning probabilistic models is the presence of hidden variables. These variables are not observed, yet interact with several of the observed variables. As such, they induce seemingly complex dependencies among the latter. In recent years, much attention has been devoted to the development of algorithms for learning parameters, and in some cases structure, in the presence of hidden variables. In this paper, we address the related problem of detecting hidden variables that interact with the observed variables. This problem is of interest both for improving our understanding of the domain and as a preliminary step that guides the learning procedure towards promising models. A very natural approach is to search for "structural signatures" of hidden variables substructures in the learned network that tend to suggest the presence of a hidden variable. We make this basic idea concrete, and show how to integrate it with structure-search algorithms. We evaluate this method on several synthetic and real-life datasets, and show that it performs surprisingly well.	Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel	Hebrew University of Jerusalem	Elidan, G (corresponding author), Hebrew Univ Jerusalem, IL-91905 Jerusalem, Israel.		Elidan, GAl/A-7380-2009	Elidan, Gal/0000-0001-5365-599X				Beinlich I., 1989, P 2 EUR C AI MED; Binder J, 1997, MACH LEARN, V29, P213, DOI 10.1023/A:1007421730016; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FRIEDMAN N, 1998, UAI; Friedman N., 1999, UAI; Friedman N., 2000, UAI; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; HECKERMAN D, 1998, LEARNING GRAPHICAL M; LAURITZEN SL, 1995, COMPUT STAT DATA AN, V19, P191, DOI 10.1016/0167-9473(93)E0056-A; Spirtes P., 2000, CAUSATION PREDICTION; VanLehn K., 1995, DISCRETE FACTOR ANAL	11	13	13	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						479	485						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800068
C	Schodl, A; Essa, I		Leen, TK; Dietterich, TG; Tresp, V		Schodl, A; Essa, I			Machine learning for video-based rendering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present techniques for rendering and animation of realistic scenes by analyzing and training on short video sequences. This work extends the new paradigm for computer animation, video textures, which uses recorded video to generate novel animations by replaying the video samples in a new order. Here we concentrate on video sprites, which are a special type of video texture. In video sprites, instead of storing whole images, the object of interest is separated from the background and the video samples are stored as a sequence of alpha-matted sprites with associated velocity information. They can be rendered anywhere on the screen to create a novel animation of the object. We present methods to create such animations by finding a sequence of sprite samples that is both visually smooth and follows a desired path. To estimate visual smoothness, we train a linear classifier to estimate visual similarity between video samples. If the motion path is known in advance, we use beam search to find a good sample sequence. We can specify the motion interactively by precomputing the sequence cost function using Q-learning.	Georgia Inst Technol, Coll Comp, GVU Ctr, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Schodl, A (corresponding author), Georgia Inst Technol, Coll Comp, GVU Ctr, Atlanta, GA 30332 USA.							Bishop, 1995, NEURAL NETWORKS PATT; BREGLER C, 1997, ANN C SERIES, P353; DEBEVEC P, 1999, SIGGRAPH 99 COURS 39; GRZESZCZUK R, 1998, ANN C SERIES, P9; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; KAMGARPARSI B, 1999, COMPUTER VISION PATT; SCHODL A, 2000, ANN C SERIES ACM SIG; WITKIN A, 1988, ANN C SERIES ACM SIG, P159	8	13	13	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1002	1008						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800141
C	Bayliss, JD; Ballard, DH		Solla, SA; Leen, TK; Muller, KR		Bayliss, JD; Ballard, DH			Recognizing evoked potentials in a virtual environment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				OCULAR ARTIFACTS; EEG	Virtual reality (VR) provides immersive and controllable experimental environments. It expands the bounds of possible evoked potential (EP) experiments by providing complex, dynamic environments in order to study cognition without sacrificing environmental control. VR also serves as a safe dynamic testbed for brain-computer interface (BCI) research. However, there has been some concern about detecting EP signals in a complex VR environment. This paper shows that EPs exist at red, green, and yellow stop lights in a virtual driving environment. Experimental results show the existence of the P3 EP at "go" and "stop" lights and the contingent negative variation (CNV) EP at "slow down" lights. In order to test the feasibility of on-line recognition in VR, we looked at recognizing the P3 EP at red stop lights and the absence of this signal at yellow slow down lights. Recognition results show that the P3 may successfully be used to control the brakes of a VR car at stop lights.	Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of Rochester	Bayliss, JD (corresponding author), Univ Rochester, Dept Comp Sci, 601 Elmwood Ave, Rochester, NY 14627 USA.							BAYLISS JD, 1998, 685 TR U ROCH NAT RE; CHAPMAN RM, 1964, NATURE, V203, P1155, DOI 10.1038/2031155a0; FARWELL LA, 1988, ELECTROEN CLIN NEURO, V70, P510, DOI 10.1016/0013-4694(88)90149-6; JUNG TP, 1997, IN PRESS ADV NEURAL, V10; Makeig S, 1997, P NATL ACAD SCI USA, V94, P10979, DOI 10.1073/pnas.94.20.10979; MCFARLAND DJ, 1993, PSYCHOBIOLOGY, V21, P77; PFURTSCHELLER G, 1996, MED PROGR TECHNOLOGY, V21, P111; Polich J, 1998, J CLIN NEUROPHYSIOL, V15, P14, DOI 10.1097/00004691-199801000-00004; RAO RPN, 1998, ADV NEURAL INFORMATI, P10; ROSVOLD HE, 1956, J CONSULT PSYCHOL, P20; SEMLITSCH HV, 1986, PSYCHOPHYSIOLOGY, V23, P695, DOI 10.1111/j.1469-8986.1986.tb00696.x; SUTTON S, 1965, SCIENCE, V150, P1187, DOI 10.1126/science.150.3700.1187; Vaughan T M, 1996, IEEE Trans Rehabil Eng, V4, P425, DOI 10.1109/86.547945; Vigario RN, 1997, ELECTROEN CLIN NEURO, V103, P395, DOI 10.1016/S0013-4694(97)00042-8	14	13	13	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						3	9						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700001
C	Hojen-Sorensen, PADFR; Hansen, LK; Rasmussen, CE		Solla, SA; Leen, TK; Muller, KR		Hojen-Sorensen, PADFR; Hansen, LK; Rasmussen, CE			Bayesian modelling of fMRI time series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				FUNCTIONAL MRI; HUMAN BRAIN	We present a Hidden Markov Model (HMM) for inferring the hidden psychological state (or neural activity) during single trial fMRI activation experiments with blocked task paradigms. Inference is based on Bayesian methodology, using a combination of analytical and a variety of Markov Chain Monte Carlo (MCMC) sampling techniques. The advantage of this method is that detection of short time learning effects between repeated trials is possible since inference is based only on single trial experiments.	Tech Univ Denmark, Dept Math Modelling, DK-2800 Lyngby, Denmark	Technical University of Denmark	Hojen-Sorensen, PADFR (corresponding author), Tech Univ Denmark, Dept Math Modelling, Bldg 321, DK-2800 Lyngby, Denmark.		Hansen, Lars/E-3174-2013	Rasmussen, Carl Edward/0000-0001-8899-7850; Hansen, Lars Kai/0000-0003-0442-5877				BANDETTINI PA, 1993, MAGNET RESON MED, V30, P161, DOI 10.1002/mrm.1910300204; DALE A, 1997, NEUROIMAGE, V5; GILKS WR, 1992, APPL STAT, V41, P337, DOI DOI 10.2307/2347565; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HANSEN LK, 1999, IN PRESS NEUROIMAGE; LANGE N, 1997, APPL STAT-J ROY ST C, V46, P1; McKeown MJ, 1998, P NATL ACAD SCI USA, V95, P803, DOI 10.1073/pnas.95.3.803; Worsley KJ, 1997, NEUROIMAGE, V6, P305, DOI 10.1006/nimg.1997.0294	8	13	13	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						754	760						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700107
C	Barber, D; Wiegerinck, W		Kearns, MS; Solla, SA; Cohn, DA		Barber, D; Wiegerinck, W			Tractable variational structures for approximating graphical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BELIEF NETWORKS	Graphical models provide a broad probabilistic framework with applications in speech recognition (Hidden Markov Models), medical diagnosis (Belief networks) and artificial intelligence (Boltzmann Machines). However, the computing time is typically exponential in the number of nodes in the graph. Within the variational framework for approximating these models, we present two classes of distributions, decimatable Boltzmann Machines and Tractable Belief Networks that go beyond the standard factorized approach. We give generalised mean-field equations for both these directed and undirected approximations. Simulation results on a small benchmark problem suggest using these richer approximations compares favorably against others previously reported in the literature.	Catholic Univ Nijmegen, RWCP Theoret Fdn, SNN, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Barber, D (corresponding author), Catholic Univ Nijmegen, RWCP Theoret Fdn, SNN, NL-6525 EZ Nijmegen, Netherlands.							BARBER D, 1998, IN PRESS J ARTIFICIA; BISHOP CM, 1998, APPROXIMATING POSTER; Castillo E., 1997, EXPERT SYSTEMS PROBA; GALLAND CC, 1993, NETWORK-COMP NEURAL, V4, P355, DOI 10.1088/0954-898X/4/3/007; JAAKKOLA TS, 1996, RECURSIVE ALGORITHMS; JAAKKOLA TS, 1997, THESIS MIT; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Peterson C., 1987, Complex Systems, V1, P995; Saul L. K., 1995, Advances in Neural Information Processing Systems 7, P435; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; SAUL LK, 1998, EXPLOITING TRACTABLE; WIEGERNICK W, 1998, MEAN FIELD THEORY BA	12	13	14	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						183	189						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700026
C	Sato, M; Ishii, S		Kearns, MS; Solla, SA; Cohn, DA		Sato, M; Ishii, S			Reinforcement learning based on on-line EM algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In this article, we propose a new reinforcement learning (RL) method based on an actor-critic architecture. The actor and the critic are approximated by Normalized Gaussian Networks (NGnet), which are networks of local linear regression units. The NGnet is trained by the on-line EM algorithm proposed in our previous paper. We apply our RL method to the task of swinging-up and stabilizing a single pendulum and the task of balancing a double pendulum near the upright position. The experimental results show that our RL method can be applied to optimal control problems having continuous state/action spaces and that. thc method achieves good control with a small number of trial-and-errors.	ATR, Human Informat Proc Res Labs, Kyoto 6190288, Japan		Sato, M (corresponding author), ATR, Human Informat Proc Res Labs, Kyoto 6190288, Japan.			Ishii, Shin/0000-0001-9385-8230				BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; BARTO AG, 1990, FDN ADAPTIVE NETWORK, P539; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Doya K, 1996, ADV NEUR IN, V8, P1073; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Moody J, 1989, NEURAL COMPUT, V1, P281, DOI 10.1162/neco.1989.1.2.281; SATO M, 1998, TRH243; Sofge D. A., 1992, HDB INTELLIGENT CONT, P259; Sutton RS, 1996, ADV NEUR IN, V8, P1038; TESAURO G, 1992, MACH LEARN, V8, P257, DOI 10.1007/BF00992697; Werbos P. J., 1990, NEURAL NETWORKS CONT, P67; XU L, 1995, ADV NEURAL INFORM PR, V7, P633	12	13	13	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1052	1058						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700148
C	Manwani, A; Koch, C		Jordan, MI; Kearns, MJ; Solla, SA		Manwani, A; Koch, C			Synaptic transmission: An information-theoretic perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	Advances in Neural Information Processing Systems		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Here we analyze synaptic transmission from an information-theoretic perspective. We derive closed-form expressions for the lower-bounds on the capacity of a simple model of a cortical synapse under two explicit coding paradigms. Under the "signal estimation" paradigm, we assume the signal to be encoded in the mean firing rate of a Poisson neuron. The performance of an optimal linear estimator of the signal then provides a lower bound on the capacity for signal estimation. Under the "signal detection" paradigm, the presence or absence of the signal has to be detected. Performance of the optimal spike detector allows us to compute a lower bound on the capacity for signal detection. We find that single synapses (for empirically measured parameter values) transmit information poorly but significant improvement can be achieved with a small amount of redundancy.	CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA	California Institute of Technology	Manwani, A (corresponding author), CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA.	quixote@klab.caltech.edu; koch@klab.caltech.edu		Koch, Christof/0000-0001-6482-8067					0	13	13	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						201	207						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700029
C	Gray, MS; Movellan, JR; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Gray, MS; Movellan, JR; Sejnowski, TJ			Dynamic features for visual speechreading: A systematic comparison	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Humans use visual as well as auditory speech signals to recognize spoken words. A variety of systems have been investigated for performing this task. The main purpose of this research was to systematically compare the performance of a range of dynamic visual features on a speechreading task. We have found that normalization of images to eliminate variation due to translation, scale, and planar rotation yielded substantial improvements in generalization performance regardless of the visual representation used. In addition, the dynamic information in the difference between successive frames yielded better performance than optical-flow based approaches, and compression by local low-pass filtering worked surprisingly better than global principal components analysis (PCA). These results are examined and possible explanations are explored.			Gray, MS (corresponding author), UNIV CALIF SAN DIEGO,DEPT COGNIT SCI,LA JOLLA,CA 92093, USA.		Sejnowski, Terrence/AAV-5558-2021						0	13	13	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						751	757						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00106
C	Jordan, MI; Ghahramani, Z; Saul, LK		Mozer, MC; Jordan, MI; Petsche, T		Jordan, MI; Ghahramani, Z; Saul, LK			Hidden Markov decision trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We study a time series model that can be viewed as a decision tree with Markov temporal structure. The model is intractable for exact calculations, thus we utilize variational approximations. We consider three different distributions for the approximation: one in which the Markov calculations are performed exactly and the layers of the decision tree are decoupled, one in which the decision tree calculations are performed exactly a,nd the time steps of the Markov chain are decoupled, and one in which a Viterbi-like assumption is made to pick out a single most likely state sequence. We present simulation results for artificial data and the Each chorales.			Jordan, MI (corresponding author), MIT,CTR BIOL & COMPUTAT LEARNING,77 MASSACHUSETTS AVE,CAMBRIDGE,MA 02139, USA.		Jordan, Michael I/C-5253-2013						0	13	13	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						501	507						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00071
C	Handzel, AA; Flash, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Handzel, AA; Flash, T			The geometry of eye rotations and Listing's law	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						WEIZMANN INST SCI,DEPT APPL MATH & COMP SCI,IL-76100 REHOVOT,ISRAEL	Weizmann Institute of Science									0	13	13	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						117	123						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00017
C	Smith, LS		Touretzky, DS; Mozer, MC; Hasselmo, ME		Smith, LS			Onset-based sound segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV STIRLING,DEPT COMP SCI,CCCN,STIRLING FK9 4LA,SCOTLAND	University of Stirling									0	13	13	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						729	735						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00103
C	DOYA, K; YOSHIZAWA, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DOYA, K; YOSHIZAWA, S			ADAPTIVE SYNCHRONIZATION OF NEURAL AND PHYSICAL OSCILLATORS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	13	14	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						109	116						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00014
C	OMOHUNDRO, SM		MOODY, JE; HANSON, SJ; LIPPMANN, RP		OMOHUNDRO, SM			BEST-1ST MODEL MERGING FOR DYNAMIC LEARNING AND RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	13	13	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						958	965						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00118
C	Zhang, ZJ; Zhang, ZR; Zhou, Y; Shen, YL; Jin, RM; Dou, DJ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Zhang, Zijie; Zhang, Zeru; Zhou, Yang; Shen, Yelong; Jin, Ruoming; Dou, Dejing			Adversarial Attacks on Deep Graph Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Despite achieving remarkable performance, deep graph learning models, such as node classification and network embedding, suffer from harassment caused by small adversarial perturbations. However, the vulnerability analysis of graph matching under adversarial attacks has not been fully investigated yet. This paper proposes an adversarial attack model with two novel attack techniques to perturb the graph structure and degrade the quality of deep graph matching: (1) a kernel density estimation approach is utilized to estimate and maximize node densities to derive imperceptible perturbations, by pushing attacked nodes to dense regions in two graphs, such that they are indistinguishable from many neighbors; and (2) a meta learning-based projected gradient descent method is developed to well choose attack starting points and to improve the search performance for producing effective perturbations. We evaluate the effectiveness of the attack model on real datasets and validate that the attacks can be transferable to other graph learning models.	[Zhang, Zijie; Zhang, Zeru; Zhou, Yang] Auburn Univ, Auburn, AL 36849 USA; [Shen, Yelong] Microsoft Dynam 365 AI, Redmond, WA USA; [Jin, Ruoming] Kent State Univ, Kent, OH 44242 USA; [Dou, Dejing] Univ Oregon, Eugene, OR 97403 USA; [Dou, Dejing] Baidu Res, Sunnyvale, CA USA	Auburn University System; Auburn University; University System of Ohio; Kent State University; Kent State University Kent; Kent State University Salem; University of Oregon; Baidu	Zhang, ZJ (corresponding author), Auburn Univ, Auburn, AL 36849 USA.	zzz0092@auburn.edu; zzz0054@auburn.edu; yangzhou@auburn.edu; yeshe@microsoft.com; rjin1@kent.edu; dou@cs.uoregon.edu						Bertinetto L., 2019, 7 INT C LEARN REPR I; Bojchevski A, 2019, PR MACH LEARN RES, V97; Chang Heng, 2020, 34 AAAI C ART INT AA; Chen J., 2018, ABS180902797 CORR; Chen J., 2020, ABS200211320 CORR; Chen JY, 2019, IEEE T COMPUT SOC SY, V6, P491, DOI 10.1109/TCSS.2019.2912801; Chen YZ, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1125, DOI 10.1145/3133956.3134083; Chen Z, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P587, DOI 10.1145/3132847.3132904; Cheng H, 2012, DATA MIN KNOWL DISC, V25, P450, DOI 10.1007/s10618-012-0263-0; Cheng H, 2011, ACM T KNOWL DISCOV D, V5, DOI 10.1145/1921632.1921638; Chu XK, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P273, DOI 10.1145/3308558.3313499; Dai HJ, 2018, PR MACH LEARN RES, V80; Dey P., 2019, P 19 INT C AUT AG MU; Du BX, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1339, DOI 10.1145/3219819.3220002; Du BX, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1447, DOI 10.1145/3097983.3098040; Du XB, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2251; Entezari Negin, 2020, P 13 ACM INT C WEB S; Fallah A, 2020, PR MACH LEARN RES, V108, P1082; Feng J, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P459, DOI 10.1145/3308558.3313424; Fey M., 2020, 8 INT C LEARN REPR I; Finn C, 2017, PR MACH LEARN RES, V70; Fu S, 2020, KNOWL-BASED SYST, V193, DOI 10.1016/j.knosys.2019.105301; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Guzzi PH, 2018, BRIEF BIOINFORM, V19, P472, DOI 10.1093/bib/bbw132; HE W, 2014, P 20 ACM SIGKDD C KN, P1270; Heimann M, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P117, DOI 10.1145/3269206.3271788; HJORT NL, 1995, ANN STAT, V23, P882, DOI 10.1214/aos/1176324627; Hou SF, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P609, DOI 10.1145/3357384.3357875; Huynh T. T., 2020, EXPERT SYST APPL, V140; Jiang H, 2017, PR MACH LEARN RES, V70; Kelley BP, 2003, P NATL ACAD SCI USA, V100, P11394, DOI 10.1073/pnas.1534710100; Kong XN, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P179, DOI 10.1145/2505515.2505531; Lee K, 2015, PROCEEDINGS OF SC15: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/2807591.2807604; Leibrandt R., 2018, P 2018 SIAM INT C DA, P747; Li CZ, 2019, PROCEEDINGS OF THE 28TH ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM '19), P249, DOI 10.1145/3357384.3357904; Li CZ, 2019, AAAI CONF ARTIF INTE, P996; Li CZ, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P447, DOI 10.1145/3269206.3271675; Li G, 2019, IEEE INT CONF BIG DA, P1151, DOI 10.1109/BigData47090.2019.9005701; Li J, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P917, DOI 10.1145/3366423.3380171; Liu H, 2019, PR MACH LEARN RES, V97; Liu Li, 2016, IJCAI, P1774; Liu SY, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P51, DOI 10.1145/2588555.2588559; Liu X., 2019, ADV NEURAL INFORM PR; Liu YW, 2017, AAAI CONF ARTIF INTE, P31; Madry A., 2018, INT C LEARN REPR ICL; Malmi E, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1687, DOI 10.1145/3132847.3132983; Man Tong, 2016, PROC IJCAI, V16, P1823; Masrour F, 2019, PROCEEDINGS OF THE 2019 IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2019), P448, DOI 10.1145/3341161.3342937; Mu X, 2019, 2019 10TH IEEE INTERNATIONAL CONFERENCE ON BIG KNOWLEDGE (ICBK 2019), P173, DOI 10.1109/ICBK.2019.00032; Mu X, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1775, DOI 10.1145/2939672.2939849; Nair V., 2010, ICML, P807; Nassar H, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P619, DOI 10.1145/3178876.3186128; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Raghu A., 2020, 8 INT C LEARN REPR I; Ravindra V., 2019, INT C COMPL NETW THE, P621; Ren JX, 2019, IEEE DATA MINING, P1288, DOI 10.1109/ICDM.2019.00162; Ren YX, 2019, PROC INT CONF DATA, P1690, DOI 10.1109/ICDE.2019.00174; Shi YC, 2019, PROC CVPR IEEE, P6512, DOI 10.1109/CVPR.2019.00668; Shlens Jonathon, 2015, INT C LEARN REPR; Shu Kai, 2017, ACM SIGKDD EXPLORATI, V18, P5, DOI [10.1145/3068777.3068781, DOI 10.1145/3068777.3068781]; Singh R, 2008, P NATL ACAD SCI USA, V105, P12763, DOI 10.1073/pnas.0806627105; Song WZ, 2018, NEUROCOMPUTING, V319, P42, DOI 10.1016/j.neucom.2018.08.072; Su S, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3863; Su ZY, 2015, ACM T WEB, V9, DOI 10.1145/2754934; Sun L, 2019, IEEE INT CONF BIG DA, P1224, DOI 10.1109/BigData47090.2019.9006430; Sun M., 2018, ABS181012881 CORR; Sun Y., 2019, ABS190906543 CORR; Sun YW, 2020, WEB CONFERENCE 2020: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2020), P673, DOI 10.1145/3366423.3380149; Takahashi T, 2019, IEEE INT CONF BIG DA, P1395, DOI 10.1109/BigData47090.2019.9006004; Vijayan V, 2020, IEEE ACCESS, V8, P41961, DOI [10.1109/access.2020.2976487, 10.1109/ACCESS.2020.2976487]; Vijayan V, 2018, IEEE ACM T COMPUT BI, V15, P1669, DOI 10.1109/TCBB.2017.2740381; Wan XY, 2020, J VISION, V20, DOI 10.1167/jov.20.7.16; Wang BH, 2019, PROCEEDINGS OF THE 2019 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY (CCS'19), P2023, DOI 10.1145/3319535.3354206; Wang SK, 2019, ENTROPY-SWITZ, V21, DOI 10.3390/e21030254; Wang X., 2018, ABS181010751 CORR; Wang YQ, 2019, WORLD WIDE WEB, V22, P2611, DOI 10.1007/s11280-018-0572-3; Waniek M, 2018, NAT HUM BEHAV, V2, P139, DOI 10.1038/s41562-017-0290-3; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu HJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4816; Wu S., 2021, P 29 INT JOINT C ART, P3766; Wu S, 2020, P 58 ANN M ASS COMPU, P5811; Xie W, 2018, IEEE DATA MINING, P1338, DOI 10.1109/ICDM.2018.00182; Xu H., 2019, ABS190908072 CORR; Xu H., 2019, ICML, V97, P6932; XU K, 2019, P 28 INT JOINT C ART, P3961; Xu K, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P3156; Yang ZH, 2010, PROCEEDINGS OF INTERNATIONAL CONFERENCE ON RESOURCE ENVIRONMENT AND INFORMATION TECHNOLOGY IN 2010 (REIT' 2010), P689, DOI 10.1109/ICDM.2010.41; Yasar A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2614, DOI 10.1145/3219819.3220079; Zang X., 2020, ABS200204784 CORR; Zhai C, 2019, CHIN CONT DECIS CONF, P4885, DOI 10.1109/CCDC.2019.8832331; Zhang JW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2125; Zhang JW, 2013, IEEE DATA MINING, P1289, DOI 10.1109/ICDM.2013.134; Zhang S, 2019, IEEE INT CONF BIG DA, P998, DOI 10.1109/BigData47090.2019.9005663; Zhang S, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P2344, DOI 10.1145/3308558.3313484; Zhang S, 2019, IEEE T KNOWL DATA EN, V31, P1680, DOI 10.1109/TKDE.2018.2866440; Zhang S, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1345, DOI 10.1145/2939672.2939766; Zhang S, 2017, IEEE DATA MINING, P1189, DOI 10.1109/ICDM.2017.160; Zhang YT, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1485, DOI 10.1145/2783258.2783268; Zheng VW, 2018, IEEE DATA MINING, P1434, DOI 10.1109/ICDM.2018.00198; Zhong ZX, 2018, AAAI CONF ARTIF INTE, P5714; Zhou F, 2019, IEEE INFOCOM SER, P1360, DOI 10.1109/INFOCOM.2019.8737411; Zhou F, 2018, IEEE INFOCOM SER, P1313; Zhou JY, 2019, IEEE INFOCOM SER, P2116, DOI 10.1109/INFOCOM.2019.8737542; Zhou K, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P305; Zhou XP, 2018, IEEE T KNOWL DATA EN, V30, P1178, DOI 10.1109/TKDE.2017.2784430; Zhou Y., 2011, DATA MINING FDN INTE, V1; Zhou Y., 2020, P 2020 IEEE INT C BI; Zhou Y., 2009, PROC VLDB ENDOW, V2, P718, DOI DOI 10.14778/1687627.1687709; Zhou Y, 2020, ACM T INTERNET TECHN, V20, DOI 10.1145/3397505; Zhou Y, 2019, IEEE INT CONF BIG DA, P1018, DOI 10.1109/BigData47090.2019.9006299; Zhou Y, 2019, IEEE INT CONF BIG DA, P1267, DOI 10.1109/BigData47090.2019.9005595; Zhou Y, 2018, IEEE INT CONF BIG DA, P1162, DOI 10.1109/BigData.2018.8621992; Zhou Y, 2018, IEEE DATA MINING, P1464, DOI 10.1109/ICDM.2018.00203; Zhou Y, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1563, DOI 10.1145/2783258.2783328; Zhou Y, 2016, IEEE J SEL AREA COMM, V34, P551, DOI 10.1109/JSAC.2016.2525478; Zhou Y, 2015, PROC VLDB ENDOW, V8, P1262, DOI 10.14778/2809974.2809987; Zhou Y, 2015, ACM T KNOWL DISCOV D, V10, DOI 10.1145/2717314; Zhou Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P338; Zhou Yang, 2015, HPDC, P179; Zhu DY, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1399, DOI 10.1145/3292500.3330851; Zhu QN, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1943; Zugner D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6246, DOI 10.1145/3219819.3220078; Zugner D., 2020, ACM T KNOWLEDGE DISC; Zugner Daniel, 2019, 7 INT C LEARN REPR I	125	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													18	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000052
C	Alaa, AM; van der Schaar, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alaa, Ahmed M.; van der Schaar, Mihaela			Demystifying Black-box Models with Symbolic Metamodels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REPRESENTATION; SUPERPOSITION; NETWORKS; THEOREM	Understanding the predictions of a machine learning model can be as crucial as the model's accuracy in many application domains. However, the black-box nature of most highly-accurate (complex) models is a major hindrance to their interpretability. To address this issue, we introduce the symbolic metamodeling framework - a general methodology for interpreting predictions by converting "black-box" models into "white-box" functions that are understandable to human subjects. A symbolic metamodel is a model of a model, i.e., a surrogate model of a trained (machine learning) model expressed through a succinct symbolic expression that comprises familiar mathematical functions and can be subjected to symbolic manipulation. We parameterize metamodels using Meijer G-functions - a class of complex-valued contour integrals that depend on real-valued parameters, and whose solutions reduce to familiar algebraic, analytic and closed-form functions for different parameter settings. This parameterization enables efficient optimization of metamodels via gradient descent, and allows discovering the functional forms learned by a model with minimal a priori assumptions. We show that symbolic metamodeling provides a generalized framework for model interpretation - many common forms of model explanation can be analytically derived from a symbolic metamodel.	[Alaa, Ahmed M.] Univ Calif Los Angeles, ECE Dept, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [van der Schaar, Mihaela] Univ Cambridge, Cambridge, England; [van der Schaar, Mihaela] Alan Turing Inst, London, England	University of California System; University of California Los Angeles; University of California System; University of California Los Angeles; University of Cambridge	Alaa, AM (corresponding author), Univ Calif Los Angeles, ECE Dept, Los Angeles, CA 90024 USA.	ahmedmalaa@ucla.edu; mv472@cam.ac.uk			National Science Foundation (NSF) [1462245, 1533983]; US Office of Naval Research (ONR)	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); US Office of Naval Research (ONR)(Office of Naval Research)	This work was supported by the National Science Foundation (NSF grants 1462245 and 1533983), and the US Office of Naval Research (ONR). The data for our experiments was provided by the UK national cancer registration and analysis service (NCRAS).	Abell M. L., 2017, MATH EXAMPLE; Alaa A. M., 2019, ADV NEURAL INFORM PR, p11 338; Alvarez-Melis D, 2018, ADV NEUR IN, V31; Beals R., 2013, NOT AMS, V60, P866, DOI [10.1090/noti1016, DOI 10.1090/NOTI1016]; Borwein JM, 2013, NOT AM MATH SOC, V60, P50, DOI [10.1090/NOTI936, DOI 10.1090/NOTI936]; Bouchard K.A., 2017, ADV NEURAL INFORM PR, P1078; dos Reis FJC, 2017, BREAST CANCER RES, V19, DOI 10.1186/s13058-017-0852-3; Chen JB, 2018, PR MACH LEARN RES, V80; Chen JB, 2017, ADV NEUR IN, V30; Chow TY, 1999, AM MATH MON, V106, P440, DOI 10.2307/2589148; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Elenberg ER, 2017, ADV NEUR IN, V30; Girosi F, 1989, NEURAL COMPUT, V1, P465, DOI 10.1162/neco.1989.1.4.465; Gradshteyn I. S., 2014, TABLE INTEGRALS SERI; Hastie T.J., 2017, STAT MODELS S, P249, DOI DOI 10.1201/9780203738535; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Igelnik B, 2003, IEEE T NEURAL NETWOR, V14, P725, DOI 10.1109/TNN.2003.813830; Jordon James, 2018, INT C REPR LEARN ICL; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kattan MW, 2016, CA-CANCER J CLIN, V66, P370, DOI 10.3322/caac.21339; Koh PW, 2017, PR MACH LEARN RES, V70; KOLMOGOROV AN, 1957, DOKL AKAD NAUK SSSR+, V114, P953; KURKOVA V, 1992, NEURAL NETWORKS, V5, P501, DOI 10.1016/0893-6080(92)90012-8; Lipton Zachary C, 2016, INT C MACH LEARN WOR; Lou Y, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P623, DOI 10.1145/2487575.2487579; Lundberg SM, 2017, ADV NEUR IN, V30; Meijer C.S., 1936, NIEUW ARCH WISKD 2 S, V18, P10; Meijer CS, 1946, G FUNCTION; Menezes T, 2014, SCI REP-UK, V4, DOI 10.1038/srep06284; Meurer A, 2017, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.103; Orzechowski P, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1183, DOI 10.1145/3205455.3205539; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Schmidt M, 2009, SCIENCE, V324, P81, DOI 10.1126/science.1165893; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; SPRECHER DA, 1993, NEURAL NETWORKS, V6, P1089, DOI 10.1016/S0893-6080(09)80020-8; Stephens T, 2015, GPLEARN MODEL GENETI; Tsang M, 2018, ADV NEUR IN, V31; Vladislavleva EJ, 2009, IEEE T EVOLUT COMPUT, V13, P333, DOI 10.1109/TEVC.2008.926486; Wang Yiqun, 2019, ARXIV190104136; Wolfram S., 2013, MATHEMATICA, V8, P23; Yoon Jinsung, 2018, INT C REPR LEARN ICL; Zhang X, 2018, ADV NEUR IN, V31	42	12	12	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902088
C	Allen-Zhu, Z; Li, YZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Allen-Zhu, Zeyuan; Li, Yuanzhi			What Can ResNet Learn Efficiently, Going Beyond Kernels?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					How can neural networks such as ResNet efficiently learn CIFAR-10 with test accuracy more than 96%, while other methods, especially kernel methods, fall relatively behind? Can we more provide theoretical justifications for this gap? Recently, there is an influential line of work relating neural networks to kernels in the over-parameterized regime, proving they can learn certain concept class that is also learnable by kernels with similar test error. Yet, can neural networks provably learn some concept class better than kernels? We answer this positively in the distribution-free setting. We prove neural networks can efficiently learn a notable class of functions, including those defined by three-layer residual networks with smooth activations, without any distributional assumption. At the same time, we prove there are simple functions in this class such that with the same number of training examples, the test error obtained by neural networks can be much smaller than any kernel method, including neural tangent kernels (NTK). The main intuition is that multi-layer neural networks can implicitly perform hierarchical learning using different layers, which reduces the sample complexity comparing to "one-shot" learning algorithms such as kernel methods. In a followup work [2], this theory of hierarchical learning is further strengthened to incorporate the "backward feature correction" process when training deep networks. In the end, we also prove a computation complexity advantage of ResNet with respect to other learning methods including linear regression over arbitrary feature mappings.	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA; [Li, Yuanzhi] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; yuanzhil@andrew.cmu.edu	Li, Yuan/GXV-1310-2022					Allen-Zhu Z., 2020, BACKWARD FEATURE COR; Allen-Zhu Zeyuan, 2019, NEURIPS; Allen- Zhu Zeyuan, 2019, CORR; Allen-Zhu Zeyuan, 2019, ICML; Arora Sanjeev, 2019, CORR; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; BAKSHI A., 2018, ARXIV181101885; Boob D., 2017, ARXIV171011241; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Ge R., 2017, ARXIV PREPRINT ARXIV; Ge Rong, 2019, REPRESENTATIONS; Golowich Noah, 2018, LEARNING THEORY; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Li Y., 2018, COLT; Liang Yingyu, 2018, ADV NEURAL INFORM PR; Ma Tengyu, 2017, CS229T STAT231 STAT; Recht B, 2018, ARXIV180600451; Rudelson M, 2010, PROCEEDINGS OF THE INTERNATIONAL CONGRESS OF MATHEMATICIANS, VOL III: INVITED LECTURES, P1576; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Siuly S, 2016, HEALTH INFOR SCI, P99, DOI 10.1007/978-3-319-47653-7_6; Soltanolkotabi M., 2017, ARXIV170704926; Soudry D., 2016, ARXIV PREPRINT ARXIV; Tian Yuandong, 2017, ARXIV170300560; Vempala S., 2018, ARXIV180502677; Wainwright M., 2015, BASIC TAIL CONCENTRA; Wei Colin, 2018, ARXIV181005369; Xie B., 2016, ARXIV161103131; Yang G., 2019, ARXIV190204760; Zhong  Kai, 2017, ARXIV170603175; Zou D, 2018, ARXIV181108888	40	12	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900059
C	Anderson, P; Shrivastava, A; Parikh, D; Batra, D; Lee, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Anderson, Peter; Shrivastava, Ayush; Parikh, Devi; Batra, Dhruv; Lee, Stefan			Chasing Ghosts: Instruction Following as Bayesian State Tracking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A visually-grounded navigation instruction can be interpreted as a sequence of expected observations and actions an agent following the correct trajectory would encounter and perform. Based on this intuition, we formulate the problem of finding the goal location in Vision-and-Language Navigation (VLN) [1] within the framework of Bayesian state tracking - learning observation and motion models conditioned on these expectable events. Together with a mapper that constructs a semantic spatial map on-the-fly during navigation, we formulate an end-to-end differentiable Bayes filter and train it to identify the goal by predicting the most likely trajectory through the map according to the instructions. The resulting navigation policy constitutes a new approach to instruction following that explicitly models a probability distribution over states, encoding strong geometric and algorithmic priors while enabling greater explainability. Our experiments show that our approach outperforms a strong LingUNet [2] baseline when predicting the goal location on the map. On the full VLN task, i.e., navigating to the goal location, our approach achieves promising results with less reliance on navigation constraints.	[Anderson, Peter; Shrivastava, Ayush; Parikh, Devi; Batra, Dhruv; Lee, Stefan] Georgia Inst Technol, Atlanta, GA 30332 USA; [Parikh, Devi; Batra, Dhruv] Facebook AI Res, New York, NY USA; [Lee, Stefan] Oregon State Univ, Corvallis, OR 97331 USA	University System of Georgia; Georgia Institute of Technology; Facebook Inc; Oregon State University	Anderson, P (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	peter.anderson@gatech.edu; ayshrv@gatech.edu; parikh@gatech.edu; dbatra@gatech.edu; leestef@oregonstate.edu						Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Anderson Peter, 2018, EVALUATION EMBODIED, V1; Blukis Valts, 2018, CORL; Chang Angel, 2017, INT C 3D VIS 3DV; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Das Abhishek, 2018, CVPR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fried D, 2018, ADV NEUR IN, V31; Gordon D, 2018, PROC CVPR IEEE, P4089, DOI 10.1109/CVPR.2018.00430; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Haarnoja T., 2016, NIPS; Henriques Joao F, 2018, CVPR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg Max, 2017, ICLR; Jonschkowski R., 2016, WORKSH DEEP LEARN AC; Jonschkowski R, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Karkus Peter, 2018, CORL; Ke LYM, 2019, PROC CVPR IEEE, P6734, DOI 10.1109/CVPR.2019.00690; Koltun, 2018, ICLR, P1; Luong Minh-Thang, 2014, EMNLP; Ma C. Y, 2019, ARXIV190103035; Ma CY, 2019, PROC CVPR IEEE, P6725, DOI 10.1109/CVPR.2019.00689; Mirowski P., 2017, PROC INT C LEARN REP, P1; Misra Dipendra Kumar, 2018, EMNLP; Oh J, 2016, PR MACH LEARN RES, V48; Parisotto Emilio, 2018, INT C LEARN REPR; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Savva Manolis, 2017, ARXIV171203931; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Tan Hao, 2019, NAACL; Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754; Uhrig J, 2017, INT CONF 3D VISION, P11, DOI 10.1109/3DV.2017.00012; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang XG, 2019, PROC CVPR IEEE, P8868, DOI [10.1109/CVPR.2019.00908, 10.1109/CVPR.2019.00267]; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wierstra D, 2007, LECT NOTES COMPUT SC, V4668, P697; Winograd Terry, 1971, TECHNICAL REPORT; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6	38	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300034
C	Bojchevski, A; Gunnemann, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bojchevski, Aleksandar; Guennemann, Stephan			Certifiable Robustness to Graph Perturbations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness. This is even more alarming given recent findings showing that they are extremely vulnerable to adversarial attacks on both the graph structure and the node attributes. We propose the first method for verifying certifiable (non-)robustness to graph perturbations for a general class of models that includes graph neural networks and label/feature propagation. By exploiting connections to PageRank and Markov decision processes our certificates can be efficiently (and under many threat models exactly) computed. Furthermore, we investigate robust training procedures that increase the number of certifiably robust nodes while maintaining or improving the clean predictive accuracy.	[Bojchevski, Aleksandar; Guennemann, Stephan] Tech Univ Munich, Munich, Germany	Technical University of Munich	Bojchevski, A (corresponding author), Tech Univ Munich, Munich, Germany.	a.bojchevski@in.tum.de; guennemann@in.tum.de			German Research Foundation; Emmy Noether grant [GU 1409/2-1]; German Federal Ministry of Education and Research (BMBF) [01IS18036B]	German Research Foundation(German Research Foundation (DFG)); Emmy Noether grant(German Research Foundation (DFG)); German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF))	This research was supported by the German Research Foundation, Emmy Noether grant GU 1409/2-1, and the German Federal Ministry of Education and Research (BMBF), grant no. 01IS18036B. The authors of this work take full responsibilities for its content.	Altman E, 1999, CONSTRAINED MARKOV D, V7; Avrachenkov K, 2006, STOCHASTIC MODELS, V22; Becchetti L, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1839490.1839494; Bojchevski A, 2019, PR MACH LEARN RES, V97; Bojchevski A, 2018, AAAI CONF ARTIF INTE, P2738; Bojchevski A, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P737, DOI 10.1145/3097983.3098156; Bojchevski Aleksandar, 2018, INT C LEARN REPR; Buchnik E, 2018, 2018 ACM INT C MEAS; Cai LZ, 2008, COMPUT J, V51, P102, DOI 10.1093/comjnl/bxm086; Chen J, 2019, ARXIV PREPRINT ARXIV; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Csaji B. C, 2010, ALT 21 INT C; Csaji BC, 2014, DISCRETE APPL MATH, V169, P73, DOI 10.1016/j.dam.2014.01.007; Dai HJ, 2018, PR MACH LEARN RES, V80; de Kerchove C, 2008, LINEAR ALGEBRA ITS A, V429; Feng F., 2019, IEEE T KNOWLEDGE DAT; Fercoq O., 2012, THESIS; Fercoq O, 2013, IEEE T AUTOMAT CONTR, V58, P134, DOI 10.1109/TAC.2012.2226103; Fey M., 2019, ICLR WORKSH REPR LEA; Fout A, 2017, ADV NEUR IN, V30; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Haveliwala Taher H., 2002, P 11 INT C WORLD WID, P517, DOI DOI 10.1145/511446.511513; Hein M, 2017, NEURAL INFORM PROCES; Hoang N, 2019, ARXIV190501591; Hollanders R, 2011, ARXIV11083779; Hooi B., 2016, P 2016 SIAM INT C DA, P495; Jeh G., 2003, P 12 INT C WORLD WID, P271, DOI DOI 10.1145/775152.775191; Ke Sun, 2019, Pattern Recognition and Computer Vision. Second Chinese Conference, PRCV 2019. Proceedings. Lecture Notes in Computer Science (LNCS 11857), P431, DOI 10.1007/978-3-030-31654-9_37; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Klicpera Johannes, 2019, INT C LEARN REPR ICL; McCallum A, 2000, INF RETR, V3; Miller B. A, 2018, IMPROVING ROBUSTNESS; Olsen M, 2010, LECT NOTES COMPUT SC, V6509, P87, DOI 10.1007/978-3-642-17461-2_7; Olsen M, 2010, LECT NOTES COMPUT SC, V6078, P37, DOI 10.1007/978-3-642-13073-1_5; PRADHAN K, 2008, AI MAG, V20; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Raghunathan A, 2018, NEURAL INFORM PROCES; Rhee S, 2018, INT JOINT C ART INT; Sherali H. D, 1995, J GLOBAL OPTIMIZATIO, V7; Sokol M, 2012, SIAM INT C DATA MINI; Tang X, 2019, ARXIV190807558; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Wang J, 2019, COMP 2019 WORLD WID; Wang S, 2019, ARXIV190503679; Wong E, 2018, PR MACH LEARN RES, V80; Wu F, 2019, PR MACH LEARN RES, V97; Wu HJ, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4816; Xu K, INT JOINT C ART INT; Ying R, 2018, INT C KNOWL DISC DAT; Zhang Y, 2019, P REPR LEARN GRAPHS; Zhang Y, 2019, AAAI C ART INT; Zhou D, 2007, INT C MACH LEARN ICM; Zhou D, 2005, INT C MACH LEARN ICM; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhou K, 2019, INT C DAT MIN ICDM; Zhu D, 2019, INT C KNOWL DISC DAT; Zugner D, 2018, INT C KNOWL DISC DAT; Zugner D, 2019, INT C KNOWL DISC DAT; Zugner<spacing Daniel, 2019, ICLR	60	12	12	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308035
C	Cai, Q; Yang, ZR; Lee, JD; Wang, ZR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cai, Qi; Yang, Zhuoran; Lee, Jason D.; Wang, Zhaoran			Neural Temporal-Difference Learning Converges to Global Optima	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	Temporal-difference learning (TD), coupled with neural networks, is among the most fundamental building blocks of deep reinforcement learning. However, due to the nonlinearity in value function approximation, such a coupling leads to non-convexity and even divergence in optimization. As a result, the global convergence of neural TD remains unclear. In this paper, we prove for the first time that neural TD converges at a sublinear rate to the global optimum of the mean-squared projected Bellman error for policy evaluation. In particular, we show how such global convergence is enabled by the overparametrization of neural networks, which also plays a vital role in the empirical success of neural TD.	[Cai, Qi; Wang, Zhaoran] Northwestern Univ, Dept Ind Engn & Management Sci, Evanston, IL 60208 USA; [Yang, Zhuoran] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Lee, Jason D.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA	Northwestern University; Princeton University; Princeton University	Cai, Q (corresponding author), Northwestern Univ, Dept Ind Engn & Management Sci, Evanston, IL 60208 USA.		Wang, Zhaoran/P-7113-2018	Lee, Jason/0000-0003-0064-7800				Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], 2018, ARXIV180601175; [Anonymous], 2019, ARXIV190405526; Arora S, 2019, PR MACH LEARN RES, V97; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Bertsekas DP, 2019, IEEE-CAA J AUTOMATIC, V6, P1, DOI 10.1109/JAS.2018.7511249; Bhandari J., 2018, ARXIV180602450, P1691; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Borkar V. S., 2009, STOCHASTIC APPROXIMA, V48; Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369; Boyan J. A., 1999, INT C MACH LEARN; Chizat L., 2018, NOTE LAZY TRAINING S; Dalal G, 2018, AAAI CONF ARTIF INTE, P6144; Du SS, 2017, PR MACH LEARN RES, V70; Duan Y, 2016, PR MACH LEARN RES, V48; Facchinei F., 2002, FINITE DIMENSIONAL V; Geist M, 2013, IEEE T NEUR NET LEAR, V24, P845, DOI 10.1109/TNNLS.2013.2247418; Ghavamzadeh M., 2010, ADV NEURAL INFORM PR; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; HARKER PT, 1990, MATH PROGRAM, V48, P161, DOI 10.1007/BF01582255; Henderson P, 2018, AAAI CONF ARTIF INTE, P3207; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Jaakkola T., 1994, ADV NEURAL INFORM PR; Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058; Konda VR, 2000, ADV NEUR IN, V12, P1008; Kushner HJ., 2003, STOCHASTIC APPROXIMA; Lakshminarayanan C, 2018, PR MACH LEARN RES, V84; Lazaric A., 2010, INT C MACH LEARN; Lee J., 2019, ARXIV190206720; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Melo E S., 2008, INT C MACH LEARN; Mnih V, 2016, PR MACH LEARN RES, V48; Neyshabur B., 2018, ARXIV180512076; Pfau David, 2016, ARXIV161001945; Rahimi A., 2008, ANN ALL C COMM CONTR; Rahimi A., 2008, ADV NEURAL INFOIRMAT; Scherrer B., 2010, INT C MACH LEARN; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman John, 2017, EQUIVALENCE POLICY; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, PROC ADV NEURAL INF, P1609; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Touati A., 2017, ARXIV170509322; Tsitsiklis JN, 1997, ADV NEUR IN, V9, P1075; Tu Stephen, 2017, ARXIV171208642; Wang YB, 2017, ADV NEUR IN, V30; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhang Chiyuan, 2016, ARXIV161103530; Zou S., 2019, ARXIV190202234CSSTAT	60	12	12	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902089
C	Du, YL; Mordatch, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Du, Yilun; Mordatch, Igor			Implicit Generation and Modeling with Energy-Based Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Energy based models (EBMs) are appealing due to their generality and simplicity in likelihood modeling, but have been traditionally difficult to train. We present techniques to scale MCMC based EBM training on continuous neural networks, and we show its success on the high-dimensional data domains of ImageNet32x32, ImageNet128x128, CIFAR-10, and robotic hand trajectories, achieving better samples than other likelihood models and nearing the performance of contemporary GAN approaches, while covering all modes of the data. We highlight some unique capabilities of implicit generation such as compositionality and corrupt image reconstruction and inpainting. Finally, we show that EBMs are useful models across a wide variety of tasks, achieving state-of-the-art out-of-distribution classification, adversarially robust classification, state-of-the-art continual online class learning, and coherent long term predicted trajectory rollouts.	[Du, Yilun] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Mordatch, Igor] Google Brain, Mountain View, CA USA; [Du, Yilun] OpenAI, San Francisco, CA USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Du, YL (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	yilundu@mit.edu						ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; [Anonymous], 2014, ICLR; Arjovsky M., 2017, ARXIV170107875; Cimpoi M., 2014, P IEEE C COMP VIS PA; DOWSON DC, 1982, J MULTIVARIATE ANAL, V12, P450, DOI 10.1016/0047-259X(82)90077-X; Dumoulin V., LEARNED REPRESENTATI; Farquhar Sebastian, 2018, ARXIV180509733; Finn Chelsea, 2016, NIPS WORKSH; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani Ishaan, 2017, NIPS; Haarnoja T., 2017, ARXIV170208165; He Junxian, 2019, ARXIV190105534; Hendrycks D., 2016, ARXIV161002136, DOI DOI 10.48550/ARXIV.1606.08415; Hendrycks D., 2018, DEEP ANOMALY DETECTI; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Higgins I., 2017, ICLR; Hinton G. E., 1999, INT C ART NEUR NETW; Hinton G. E., 2006, TRAINING, V14; Hsu Yen-Chang, 2018, ARXIV181012488; Kim T., 2016, ARXIV160603439; Kingma D. P., 2018, PROC NEURIPS; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kumar Rithesh, 2019, ARXIV190108508; Kurach K., 2018, ARXIV180704720; LeCun Y., 2006, PREDICTING STRUCTURE; Lim T.Y, SEMANTIC IMAGE INPAI; Madry A., 2018, ARXIV PREPRINT ARXIV; Miyato T., 2018, INT C LEARN REPR, P2; Mnih Andriy, 2004, LEARNING NONLINEAR C; Mnih Volodymyr, 2013, NIPS WORKSH; Nalisnick E., 2019, INT C LEARN REPR; Neal RM, 2011, HDB MARKOV CHAIN MON, V2, P2; Odena A, 2017, PR MACH LEARN RES, V70; OpenAI, 2018, ARXIV180800177; Ostrovski Georg, 2018, ARXIV180605575; Radford A., 2016, ICLR; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Salimans Tim, 2016, ADV NEURAL INFORM PR; Schwarz J, 2018, ARXIV180506370; Teh YW, 2004, J MACH LEARN RES, V4, P1235; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Turner R., 2005, CD NOTES; Van Oord Aaron, 2016, ICML; Wang C., 2018, CVPR; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Xie JW, 2016, PR MACH LEARN RES, V48; Zenke F, 2017, PR MACH LEARN RES, V70; Zhao Junbo, 2016, P INT C LEARN REPR T, DOI DOI 10.48550/ARXIV.1609.03126	49	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303058
C	Gebauer, NWA; Gastegger, M; Schutt, KT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gebauer, Niklas W. A.; Gastegger, Michael; Schuett, Kristof T.			Symmetry-adapted generation of 3d point sets for the targeted discovery of molecules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep learning has proven to yield fast and accurate predictions of quantum-chemical properties to accelerate the discovery of novel molecules and materials. As an exhaustive exploration of the vast chemical space is still infeasible, we require generative models that guide our search towards systems with desired properties. While graph-based models have previously been proposed, they are restricted by a lack of spatial information such that they are unable to recognize spatial isomerism and non-bonded interactions. Here, we introduce a generative neural network for 3d point sets that respects the rotational invariance of the targeted structures. We apply it to the generation of molecules and demonstrate its ability to approximate the distribution of equilibrium structures using spatial metrics as well as established measures from chemoinformatics. As our model is able to capture the complex relationship between 3d geometry and electronic properties, we bias the distribution of the generator towards molecules with a small HOMO-LUMO gap - an important property for the design of organic solar cells.	[Gebauer, Niklas W. A.; Gastegger, Michael; Schuett, Kristof T.] Tech Univ Berlin, Machine Learning Grp, D-10587 Berlin, Germany	Technical University of Berlin	Gebauer, NWA (corresponding author), Tech Univ Berlin, Machine Learning Grp, D-10587 Berlin, Germany.	n.wa.gebauer@gmail.com; michael.gastegger@tu-berlin.de; kristof.schuett@tu-berlin.de	Schütt, Kristof T/Q-2604-2017	Schütt, Kristof T/0000-0001-8342-0964; Gebauer, Niklas/0000-0002-9149-7424	Federal Ministry of Education and Research (BMBF) [01IS18037A]; European Unions Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant [792572]	Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); European Unions Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant	We thank Liu et al. [22] for providing us with generated molecules of their CGVAE model. This work was supported by the Federal Ministry of Education and Research (BMBF) for the Berlin Center for Machine Learning (01IS18037A). MG was provided financial support by the European Unions Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement NO 792572. Correspondence to NWAG and KTS.	Achlioptas P, 2018, PR MACH LEARN RES, V80; [Anonymous], 2015, ARXIV150202072; Bartok AP, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.136403; BECKE AD, 1993, J CHEM PHYS, V98, P5648, DOI 10.1063/1.464913; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Blaschke T, 2018, MOL INFORM, V37, DOI 10.1002/minf.201700123; Chmiela S, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-06169-2; Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015; Dai Hanjun, 2018, INT C LEARN REPR; Duvenaud David K, 2015, P NIPS; Erickenberg M., 2017, ADV NEURAL INFORM PR, P6543; Gastegger M, 2017, CHEM SCI, V8, P6924, DOI 10.1039/c7sc02267k; Gilmer J, 2017, PR MACH LEARN RES, V70; Gomez-Bombarelli R., 2016, ACS CENT SCI; Gupta A, 2018, MOL INFORM, V37, DOI 10.1002/minf.201700111; Larsen AH, 2017, J PHYS-CONDENS MAT, V29, DOI 10.1088/1361-648X/aa680e; Janz Dave, 2018, INT C LEARN REPR; Jha D, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-35934-y; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jin W., 2018, P 35 INT C MACHINE L; Jin W., 2019, INT C LEARN REPR; Jorgensen PB, 2018, J CHEM PHYS, V148, DOI 10.1063/1.5023563; Kingma D.P, P 3 INT C LEARNING R; Kusner MJ, 2017, PR MACH LEARN RES, V70; LEE CT, 1988, PHYS REV B, V37, P785, DOI 10.1103/PhysRevB.37.785; Li Y., 2018, ARXIV; Li YB, 2018, J CHEMINFORMATICS, V10, DOI 10.1186/s13321-018-0287-6; Lim J, 2018, J CHEMINFORMATICS, V10, DOI 10.1186/s13321-018-0286-7; Liu Q, 2018, ADV NEURAL INFORM PR, V31; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Mansimov E., 2019, ARXIV190400314; Neese F, 2012, WIRES COMPUT MOL SCI, V2, P73, DOI 10.1002/wcms.81; O'Boyle NM, 2011, J CHEMINFORMATICS, V3, DOI 10.1186/1758-2946-3-33; Podryabinkin EV, 2019, PHYS REV B, V99, DOI 10.1103/PhysRevB.99.064114; Ramachandran P, 2011, COMPUT SCI ENG, V13, P40, DOI 10.1109/MCSE.2011.35; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; rdkit, RDKIT OPEN SOURCE CH; Reymond JL, 2015, ACCOUNTS CHEM RES, V48, P722, DOI 10.1021/ar500432k; Ruddigkeit L, 2012, J CHEM INF MODEL, V52, P2864, DOI 10.1021/ci300415d; Schutt KT, 2018, J CHEM PHYS, V148, DOI 10.1063/1.5019779; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Schutt KT, 2019, J CHEM THEORY COMPUT, V15, P448, DOI 10.1021/acs.jctc.8b00908; Schutt K.T., 2017, ADV NEURAL INF PROCE, V30, P992; Segler MHS, 2018, ACS CENTRAL SCI, V4, P120, DOI 10.1021/acscentsci.7b00512; Smith JS, 2017, CHEM SCI, V8, P3192, DOI 10.1039/c6sc05720a; Smith JS, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.193; STEPHENS PJ, 1994, J PHYS CHEM-US, V98, P11623, DOI 10.1021/j100096a001; Tkatchenko A, 2012, PHYS REV LETT, V108, DOI [10.1103/PhysRevLett.108.058301, 10.1103/PhysRevLett.108.236402]; VOSKO SH, 1980, CAN J PHYS, V58, P1200, DOI 10.1139/p80-159; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a; You J., 2018, CORR ABS180602473, P6410	58	12	12	3	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307057
C	Gong, MM; Xu, YW; Li, CY; Zhang, K; Batmanghelich, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gong, Mingming; Xu, Yanwu; Li, Chunyuan; Zhang, Kun; Batmanghelich, Kayhan			Twin Auxiliary Classifiers GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Conditional generative models enjoy remarkable progress over the past few years. One of the popular conditional models is Auxiliary Classifier GAN (AC-GAN), which generates highly discriminative images by extending the loss function of GAN with an auxiliary classifier. However, the diversity of the generated samples by AC-GAN tends to decrease as the number of classes increases, hence limiting its power on large-scale data. In this paper, we identify the source of the low diversity issue theoretically and propose a practical solution to solve the problem. We show that the auxiliary classifier in AC-GAN imposes perfect separability, which is disadvantageous when the supports of the class distributions have significant overlap. To address the issue, we propose Twin Auxiliary Classifiers Generative Adversarial Net (TAC-GAN) that further benefits from a new player that interacts with other players (the generator and the discriminator) in GAN. Theoretically, we demonstrate that TAC-GAN can effectively minimize the divergence between the generated and real-data distributions. Extensive experimental results show that our TAC-GAN can successfully replicate the true data distributions on simulated data, and significantly improves the diversity of class-conditional image generation on real datasets.	[Gong, Mingming; Xu, Yanwu; Batmanghelich, Kayhan] Univ Pittsburgh, Dept Biomed Informat, Pittsburgh, PA 15260 USA; [Li, Chunyuan] Microsoft Res, Redmond, WA USA; [Gong, Mingming; Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Microsoft; Carnegie Mellon University	Gong, MM (corresponding author), Univ Pittsburgh, Dept Biomed Informat, Pittsburgh, PA 15260 USA.; Gong, MM (corresponding author), Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA.	mig73@pitt.edu; yanwuxu@pitt.edu; cl319@duke.edu; kunz1@cmu.edu; kayhan@pitt.edu	Li, Chunyuan/AAG-1303-2020	Gong, Mingming/0000-0001-7147-5589	NIH [1R01HL141813-01]; NSF [1839332]; Tripod+X; SAP SE; NVIDIA Corporation; Pittsburgh SuperComputing grant [TG-ASC170024]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); Tripod+X; SAP SE; NVIDIA Corporation; Pittsburgh SuperComputing grant	This work was partially supported by NIH Award Number 1R01HL141813-01, NSF 1839332 Tripod+X, and SAP SE. We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research. We were also grateful for the computational resources provided by Pittsburgh SuperComputing grant number TG-ASC170024.	Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; Arjovsky M, 2017, PR MACH LEARN RES, V70; Binkowski M., 2018, ABS180101401 CORR; Brock A., 2019, INT C LEARNING REPRE; Dash Ayushman, 2017, ARXIV170306412; Denton Emily L, 2015, NEURIPS, V2, P4; Dumoulin Vincent, 2016, ARXIV E PRINTS; Gao WH, 2017, ADV NEUR IN, V30; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton Arthur, 2012, J MACH LEARN RES, V2; Gulrajani I., 2017, INT C NEURAL INF PRO; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Huang L, 2018, PORTL INT CONF MANAG; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Krizhevsky A., CIFAR100; LeCun Yann, MNIST DATABASE HANDW; Li Chunyuan, 2017, NIPS; Lin ZA, 2018, ADV NEUR IN, V31; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mescheder L, 2018, PR MACH LEARN RES, V80; Mirza M., 2014, ARXIV; Nowozin S, 2016, ADV NEUR IN, V29; Odena A, 2017, PR MACH LEARN RES, V70; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Perarnau G, 2016, ARXIV161106355; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Rui Shu, AC GAN LEARNS BIASED; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T., 2016, ADV NEUR IN, P2234; Salimans T, 2016, ADV NEUR IN, V29; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Yang D, 2019, INT C LEARN REPR; Zhang H., 2017, ICCV; Zhang Han, 2018, ARXIV180508318; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068	41	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID	32103879				2022-12-19	WOS:000534424301033
C	Gu, JT; Wang, CH; Junbo, JZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gu, Jiatao; Wang, Changhan; (Junbo), Jake Zhao			Levenshtein Transformer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Modern neural sequence generation models are built to either generate tokens step-by-step from scratch or (iteratively) modify a sequence of tokens bounded by a fixed length. In this work, we develop Levenshtein Transformer, a new partially autoregressive model devised for more flexible and amenable sequence generation. Unlike previous approaches, the basic operations of our model are insertion and deletion. The combination of them facilitates not only generation but also sequence refinement allowing dynamic length changes. We also propose a set of new training techniques dedicated at them, effectively exploiting one as the other's learning signal thanks to their complementary nature. Experiments applying the proposed model achieve comparable or even better performance with much-improved efficiency on both generation (e.g. machine translation, text summarization) and refinement tasks (e.g. automatic post-editing). We further confirm the flexibility of our model by showing a Levenshtein Transformer trained by machine translation can straightforwardly be used for automatic post-editing.	[Gu, Jiatao; Wang, Changhan] Facebook AI Res, Menlo Pk, CA 94025 USA; [(Junbo), Jake Zhao] NYU, New York, NY 10003 USA; [(Junbo), Jake Zhao] Tigerobo Inc, Shanghai, Peoples R China	Facebook Inc; New York University	Gu, JT (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	jgu@fb.com; changhan@fb.com; jakezhao@cs.nyu.edu						[Anonymous], 2017, P 4 WORKSHOP ASIAN T, P1; Cho K., 2015, CORR; Cho Kyunghyun, 2016, ARXIV160503835; Devlin Jacob, 2018, P NAACL HLT, DOI DOI 10.18653/V1/N19-1423; Dong Yue, 2019, ARXIV190608104; Freitag M, 2019, ARXIV190404790; Ghazvininejad Marjan, 2019, ABS190409324 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grangier David, 2017, ARXIV171104805; Gu Jiatao, 2018, 6 INT C LEARN REPR I; Gu Jiatao, 2019, ARXIV190201370; Guu  K., 2018, T ASSOC COMPUT LING, V6, P437, DOI [10.1162/tacl_a_00030, DOI 10.1162/TACL_A_00030]; Kaiser Lukasz, 2018, ARXIV180303382, P2390; Kim Yoon, 2016, EMNLP; Koehn P., 2003, P 2003 C N AM CHAPT, P127, DOI DOI 10.3115/1073445.1073462; LEVENSHT.VI, 1965, DOKL AKAD NAUK SSSR+, V163, P845; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Novak Roman, 2016, ARXIV161006602; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Riloff Ellen, 2018, P 2018 C EMP METH NA; Rush Alexander M, 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044; Sabour Sara, 2018, ARXIV181001398; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Snover Matthew, 2006, P ASS MACH TRANSL AM, P223; Stern M., 2018, ADV NEURAL INFORM PR, P10107; Stern Mitchell, 2019, ARXIV190203249; Vaswani Ashish, 2017, P ANN C NEUR INF PRO; Wang Chunqi, 2018, P 2018 C EMP METH NA, P479, DOI DOI 10.18653/V1/D18-1044; Welleck S., 2019, ARXIV190202192; Xia YC, 2017, ADV NEUR IN, V30	30	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902077
C	Lan, GH; Li, ZZ; Zhou, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lan, Guanghui; Li, Zhize; Zhou, Yi			A unified variance-reduced accelerated gradient method for convex optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC-APPROXIMATION ALGORITHMS; COMPOSITE OPTIMIZATION	We propose a novel randomized incremental gradient algorithm, namely, VAriance-Reduced Accelerated Gradient (Varag), for finite-sum optimization. Equipped with a unified step-size policy that adjusts itself to the value of the condition number, Varag exhibits the unified optimal rates of convergence for solving smooth convex finite-sum problems directly regardless of their strong convexity. Moreover, Varag is the first accelerated randomized incremental gradient method that benefits from the strong convexity of the data-fidelity term to achieve the optimal linear convergence. It also establishes an optimal linear rate of convergence for solving a wide class of problems only satisfying a certain error bound condition rather than strong convexity. Varag can also be extended to solve stochastic finite-sum problems.	[Lan, Guanghui] Georgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Li, Zhize] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing 100084, Peoples R China; [Zhou, Yi] IBM Almaden Res Ctr, San Jose, CA 95120 USA	University System of Georgia; Georgia Institute of Technology; Tsinghua University; International Business Machines (IBM)	Lan, GH (corresponding author), Georgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	george.lan@isye.gatech.edu; zz-li14@mails.tsinghua.edu.cn; yi.zhou@ibm.com						Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; AllenZhu Zeyuan, 2016, KATYUSHA 1 DIRECT AC; Auslender A, 2006, SIAM J OPTIMIZ, V16, P697, DOI 10.1137/S1052623403427823; Bauschke HH, 2003, SIAM J CONTROL OPTIM, V42, P596, DOI 10.1137/S0363012902407120; Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; CENSOR Y, 1981, J OPTIMIZ THEORY APP, V34, P321, DOI 10.1007/BF00934676; Dang CD, 2017, J COMPUT MATH, V35, P452, DOI 10.4208/jcm.1612-m2016-0703; Defazio A., 2014, ADV NEURAL INFORM PR, V27; Dua D., 2017, UCI MACHINE LEARNING; Hazan Elad, 2016, ABS160202101 CORR, P2; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kiwiel KC, 1997, SIAM J CONTROL OPTIM, V35, P1142, DOI 10.1137/S0363012995281742; Kulunchakov A., 2019, ARXIV190108788; Lan GH, 2018, MATH PROGRAM, V171, P167, DOI 10.1007/s10107-017-1173-0; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Necoara I, 2019, MATH PROGRAM, V175, P69, DOI 10.1007/s10107-018-1232-1; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nemirovski A, 1983, WILEY INTERSCIENCE S, V15; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Tang J., 2018, ADV NEURAL INFORM PR, P429; Wang Jialei, 2017, P MACHINE LEARNING R, V70, P3694; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791	27	12	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902013
C	Li, YK; Ma, T; Bai, YQ; Duan, N; Wei, SN; Wang, XG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Yikang; Ma, Tao; Bai, Yeqi; Duan, Nan; Wei, Sining; Wang, Xiaogang			PasteGAN: A Semi-Parametric Method to Generate Image from Scene Graph	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite some exciting progress on high-quality image generation from structured (scene graphs) or free-form (sentences) descriptions, most of them only guarantee the image-level semantical consistency, i.e. the generated image matching the semantic meaning of the description. They still lack the investigations on synthesizing the images in a more controllable way, like finely manipulating the visual appearance of every object. Therefore, to generate the images with preferred objects and rich interactions, we propose a semi-parametric method, PasteGAN, for generating the image from the scene graph and the image crops, where spatial arrangements of the objects and their pair-wise relationships are defined by the scene graph and the object appearances are determined by the given object crops. To enhance the interactions of the objects in the output, we design a Crop Refining Network and an Object-Image Fuser to embed the objects as well as their relationships into one map. Multiple losses work collaboratively to guarantee the generated images highly respecting the crops and complying with the scene graphs while maintaining excellent image quality. A crop selector is also proposed to pick the most-compatible crops from our external object tank by encoding the interactions around the objects in the scene graph if the crops are not provided. Evaluated on Visual Genome and COCO-Stuff dataset, our proposed method significantly outperforms the SOTA methods on Inception Score, Diversity Score and Frechet Inception Distance. Extensive experiments also demonstrate our method's ability to generate complex and diverse images with given objects. The code is available at https://github.com/yikang-li/PasteGAN.	[Li, Yikang; Wang, Xiaogang] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Ma, Tao] Northwestern Polytech Univ, Xian, Peoples R China; [Bai, Yeqi] Nanyang Technol Univ, Singapore, Singapore; [Duan, Nan; Wei, Sining] Microsoft, Redmond, WA USA	Chinese University of Hong Kong; Northwestern Polytechnical University; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Microsoft	Li, YK (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	ykli@ee.cuhk.edu.hk; taoma@mail.nwpu.edu.cn; baiyeqi@gmail.com; nanduan@microsoft.com; sinwei@microsoft.com; xgwang@ee.cuhk.edu.hk			SenseTime Group Limited; Microsoft Research; General Research Fund through the Research Grants Council of Hong Kong [CUHK14202217, CUHK14203118, CUHK14207319]	SenseTime Group Limited; Microsoft Research(Microsoft); General Research Fund through the Research Grants Council of Hong Kong	This work is supported in part by SenseTime Group Limited, in part by Microsoft Research, and in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14207319.	Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265; Gaurav Mittal, 2019, ARXIV190503743; Gauthier J, 2014, CS231N; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Henaff M, 2015, ARXIV150605163; Hong S, 2018, PROC CVPR IEEE, P7986, DOI 10.1109/CVPR.2018.00833; Jin DK, 2017, COMPUT VIS PATT REC, P151, DOI 10.1016/B978-0-08-101291-8.00007-9; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Li YK, 2018, LECT NOTES COMPUT SC, V11205, P346, DOI 10.1007/978-3-030-01246-5_21; Li YK, 2017, IEEE I CONF COMP VIS, P1270, DOI 10.1109/ICCV.2017.142; Li YK, 2017, PROC CVPR IEEE, P7244, DOI 10.1109/CVPR.2017.766; Mirza M., 2014, ARXIV; Newell A., 2017, ADV NEURAL INFORM PR, V30, P1172; Odena A, 2017, PR MACH LEARN RES, V70; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; van den Oord A, 2016, PR MACH LEARN RES, V48; Xu DF, 2017, PROC CVPR IEEE, P3097, DOI 10.1109/CVPR.2017.330; Zhang H., 2017, ARXIV PREPRINT ARXIV; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068; Zhao B, 2019, IEEE COMPUT SOC CONF, P398, DOI [10.1109/CVPRW.2019.00053, 10.1109/CVPR.2019.00878]	32	12	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303089
C	Salehi, F; Abbasi, E; Hassibi, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Salehi, Fariborz; Abbasi, Ehsan; Hassibi, Babak			The Impact of Regularization on High-dimensional Logistic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GENERALIZED LINEAR-MODELS; SELECTION	Logistic regression is commonly used for modeling dichotomous outcomes. In the classical setting, where the number of observations is much larger than the number of parameters, properties of the maximum likelihood estimator in logistic regression are well understood. Recently, Sur and Candes [26] have studied logistic regression in the high-dimensional regime, where the number of observations and parameters are comparable, and show, among other things, that the maximum likelihood estimator is biased. In the high-dimensional regime the underlying parameter vector is often structured (sparse, block-sparse, finite-alphabet, etc.) and so in this paper we study regularized logistic regression (RLR), where a convex regularizer that encourages the desired structure is added to the negative of the log-likelihood function. An advantage of RLR is that it allows parameter recovery even for instances where the (unconstrained) maximum likelihood estimate does not exist. We provide a precise analysis of the performance of RLR via the solution of a system of six nonlinear equations, through which any performance metric of interest (mean, mean-squared error, probability of support recovery, etc.) can be explicitly computed. Our results generalize those of Sur and Candes and we provide a detailed study for the cases of l(2)(2)-RLR and sparse (l(1)-regularized) logistic regression. In both cases, we obtain explicit expressions for various performance metrics and can find the values of the regularizer parameter that optimizes the desired performance. The theory is validated by extensive numerical simulations across a range of parameter values and problem instances.	[Salehi, Fariborz; Abbasi, Ehsan; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Salehi, F (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.				National Science Foundation [CNS-0932428, CCF-1018927, CCF-1423663, CCF-1409204]; Qualcomm Inc.; Futurewei Inc.; NASA's Jet Propulsion Laboratory; King Abdullah University of Science and Technology	National Science Foundation(National Science Foundation (NSF)); Qualcomm Inc.; Futurewei Inc.; NASA's Jet Propulsion Laboratory; King Abdullah University of Science and Technology(King Abdullah University of Science & Technology)	This work was supported in part by the National Science Foundation under grants CNS-0932428, CCF-1018927, CCF-1423663 and CCF-1409204, by a grant from Qualcomm Inc., by a grant from Futurewei Inc., by NASA's Jet Propulsion Laboratory through the President and Director's Fund, and by King Abdullah University of Science and Technology.	Abbasi E, 2019, INT CONF ACOUST SPEE, P4554, DOI 10.1109/ICASSP.2019.8683890; [Anonymous], 2013, ARXIV13037291; Ben Atitallah I, 2017, INT CONF ACOUST SPEE, P4262, DOI 10.1109/ICASSP.2017.7952960; BOYD CR, 1987, J TRAUMA, V27, P370, DOI 10.1097/00005373-198704000-00005; Bunea F, 2008, ELECTRON J STAT, V2, P1153, DOI 10.1214/08-EJS287; Candes E. J, 2018, ARXIV180409753; Dhifallah O., 2018, ARXIV180509555; Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761; Hosmer DW, 2013, APPL LOGISTIC REGRES; James G., 2013, INTRO STAT LEARNING, V112; Kakade Sham, 2010, P 13 INT C ART INT S, P381; KING G, 2001, POLITICAL ANAL, V0009; Koh KM, 2007, J MACH LEARN RES, V8, P1519; Krishnapuram B, 2005, IEEE T PATTERN ANAL, V27, P957, DOI 10.1109/TPAMI.2005.127; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Miolane L., 2018, ARXIV181101212; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; Oymak S, 2018, INF INFERENCE, V7, P337, DOI 10.1093/imaiai/iax011; Panahi Ashkan, 2017, ADV NEURAL INFORM PR, P3381; Salehi F., 2018, ADV NEURAL INFORM PR, P8641; Salehi F, 2018, IEEE INT SYMP INFO, P976; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Sur P., 2017, PROBABILITY THEORY R, P1; Sur Pragya, 2018, ARXIV180306964; Thrampoulidis C., 2015, PROC C LEARN THEORY, P1683; Thrampoulidis C, 2018, IEEE T INFORM THEORY, V64, P5592, DOI 10.1109/TIT.2018.2840720; Thrampoulidis Christos, 2019, ARXIV190303949; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tu JV, 1996, J CLIN EPIDEMIOL, V49, P1225, DOI 10.1016/S0895-4356(96)00002-9; van de Geer SA, 2008, ANN STAT, V36, P614, DOI 10.1214/009053607000000929; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3	34	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903060
C	Staib, M; Jegelka, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Staib, Matthew; Jegelka, Stefanie			Distributionally Robust Optimization and Generalization in Kernel Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONVERGENCE	Distributionally robust optimization (DRO) has attracted attention in machine learning due to its connections to regularization, generalization, and robustness. Existing work has considered uncertainty sets based on phi-divergences and Wasserstein distances, each of which have drawbacks. In this paper, we study DRO with uncertainty sets measured via maximum mean discrepancy (MMD). We show that MMD DRO is roughly equivalent to regularization by the Hilbert norm and, as a byproduct, reveal deep connections to classic results in statistical learning. In particular, we obtain an alternative proof of a generalization bound for Gaussian kernel ridge regression via a DRO lense. The proof also suggests a new regularizer. Our results apply beyond kernel methods: we derive a generically applicable approximation of MMD DRO, and show that it generalizes recent work on variance-based regularization.	[Staib, Matthew; Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Staib, M (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	mstaib@mit.edu; stefje@csail.mit.edu			Defense Advanced Research Projects Agency [YFA17 N66001-17-1-4039]	Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported by The Defense Advanced Research Projects Agency (grant number YFA17 N66001-17-1-4039). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense. We thank Cameron Musco and Joshua Robinson for helpful conversations, and Marwa El Halabi and Sebastian Claici for comments on the draft.	Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; Bertsimas D, 2018, MATH PROGRAM, V167, P235, DOI 10.1007/s10107-017-1125-8; Bietti Alberto, 2019, P 36 INT C MACH LEAR; Bietti Alberto, 2019, J MACHINE LEARNING R, V20, P876; Blanchet J., 2016, ARXIV161005627; Blanchet J., 2017, ARXIV170507152; Blanchet J., 2018, ARXIV181002403; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741; DUCHI J, 2016, ARXIV161003425; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Gao R., 2017, ARXIV171206050; Gao R., 2016, MATH OPER RES; Goh J, 2010, OPER RES, V58, P902, DOI 10.1287/opre.1090.0795; Goodfellow I. J., 2015, P ICLR; GOTOH JY, 2015, ROBUST EMPIRICAL OPT; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hashimoto Tatsunori, 2018, P 35 INT C MACH LEAR, P2; Jitkrittum W, 2017, ADV NEURAL INFORM PR, P262; Lei Jing, 2018, ARXIV180410556; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu Q, 2016, PR MACH LEARN RES, V48; Madry Aleksander, 2018, ICLR; Marti Gautier, 2018, INT C LEARN REPR; MAURER A., 2009, C LEARN THEOR; Mohri M., 2018, FDN MACHINE LEARNING; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Namkoong H, 2017, ADV NEURAL INFORM PR, V30, P2971; Scarf H., 1958, STUDIES MATH THEORY; Singh Shashank, 2018, ARXIV180208855; Sinha A., 2018, ICLR; Staib M, 2019, PR MACH LEARN RES, V89, P506; Staib Matthew, 2017, NIPS MACH LEARN COMP; Sutherland DJ, 2017, INT C LEARN REPR; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Villani C., 2008, OPTIMAL TRANSPORT OL; Xu H, 2009, J MACH LEARN RES, V10, P1485	42	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900069
C	Tsutsui, S; Fu, YW; Crandall, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tsutsui, Satoshi; Fu, Yanwei; Crandall, David			Meta-Reinforced Synthetic Data for One-Shot Fine-Grained Visual Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					One-shot fine-grained visual recognition often suffers from the problem of training data scarcity for new fine-grained classes. To alleviate this problem, an off-the-shelf image generator can be applied to synthesize additional training images, but these synthesized images are often not helpful for actually improving the accuracy of one-shot fine-grained recognition. This paper proposes a meta-learning framework to combine generated images with original images, so that the resulting "hybrid" training images can improve one-shot learning. Specifically, the generic image generator is updated by a few training instances of novel classes, and a Meta Image Reinforcing Network (MetaIRNet) is proposed to conduct one-shot fine-grained recognition as well as image reinforcement. The model is trained in an end-to-end manner, and our experiments demonstrate consistent improvement over baselines on one-shot fine-grained image classification benchmarks.	[Tsutsui, Satoshi; Crandall, David] Indiana Univ, Bloomington, IN 47405 USA; [Fu, Yanwei] Fudan Univ, Sch Data Sci, Shanghai, Peoples R China; [Fu, Yanwei] Fudan Univ, MOE Frontiers Ctr Brain Sci, Shanghai Key Lab Intelligent Informat Proc, Shanghai, Peoples R China	Indiana University System; Indiana University Bloomington; Fudan University; Fudan University	Tsutsui, S (corresponding author), Indiana Univ, Bloomington, IN 47405 USA.	stsutsui@indiana.edu; yanweifu@fudan.edu.cn; djcran@indiana.edu			NSFC project [61572138]; Science and Technology Commission of Shanghai Municipality Project [19511120700]; National Science Foundation [CAREER IIS-1253549]; Indiana University Office of the Vice Provost for Research; College of Arts and Sciences; Luddy School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children"	NSFC project(National Natural Science Foundation of China (NSFC)); Science and Technology Commission of Shanghai Municipality Project(Science & Technology Commission of Shanghai Municipality (STCSM)); National Science Foundation(National Science Foundation (NSF)); Indiana University Office of the Vice Provost for Research; College of Arts and Sciences; Luddy School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children"	We would like to thank Yi Li for drawing Figure 2, and Minjun Li and Atsuhiro Noguchi for helpful discussions. Part of this work was done while Satoshi Tsutsui was an intern at Fudan University. Yanwei Fu was supported in part by the NSFC project (#61572138), and Science and Technology Commission of Shanghai Municipality Project (#19511120700). David Crandall was supported in part by the National Science Foundation (CAREER IIS-1253549), and the Indiana University Office of the Vice Provost for Research, the College of Arts and Sciences, and the Luddy School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children." Yanwei Fu is the corresponding author.	Antoniou Antreas, 2018, INT C ART NEUR NETW; Arjovsky M., 2017, ARXIV170107875; Brock A., 2019, INT C LEARNING REPRE; Chen Wei-Yu, 2019, INT C LEARN REPR, P12; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Chen Zitian, 2019, AAAI C ART INT AAAI; de Vries H, 2017, ADV NEUR IN, V30; Dong XW, 2018, IEEE CONF COMPUT; Dumoulin Vincent, 2017, ICLR; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ADV NEURAL INFORM PR, P9516; Gao Hang, 2018, ADV NEURAL INFORM PR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Noguchi A, 2019, IEEE I CONF COMP VIS, P2750, DOI 10.1109/ICCV.2019.00284; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ravi S., 2017, INT C LEARN REPR, P12; Rusu Andrei A, 2018, ARXIV180705960; Santoro A, 2016, PR MACH LEARN RES, V48; Schwartz Eli, 2018, ADV NEURAL INFORM PR, P2; Shmelkov K, 2018, LECT NOTES COMPUT SC, V11206, P218, DOI 10.1007/978-3-030-01216-8_14; Shrivastava A, 2017, PROC CVPR IEEE, P2242, DOI 10.1109/CVPR.2017.241; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Van Horn Grant, 2015, IEEE C COMP VIS PATT; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wah C., 2011, TECH REP; Wang Y., 2018, P EUR C COMP VIS ECC; Wang YX, 2016, ADV NEUR IN, V29; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang RX, 2018, ADV NEUR IN, V31; Zhu Jun-Yan, 2017, ICCV	38	12	13	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303009
C	Wang, YL; Pan, XR; Song, SJ; Zhang, H; Wu, C; Huang, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Yulin; Pan, Xuran; Song, Shiji; Zhang, Hong; Wu, Cheng; Huang, Gao			Implicit Semantic Data Augmentation for Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we propose a novel implicit semantic data augmentation (ISDA) approach to complement traditional augmentation techniques like flipping, translation or rotation. Our work is motivated by the intriguing property that deep networks are surprisingly good at linearizing features, such that certain directions in the deep feature space correspond to meaningful semantic transformations, e.g., adding sunglasses or changing backgrounds. As a consequence, translating training samples along many semantic directions in the feature space can effectively augment the dataset to improve generalization. To implement this idea effectively and efficiently, we first perform an online estimate of the covariance matrix of deep features for each class, which captures the intra-class semantic variations. Then random vectors are drawn from a zero-mean normal distribution with the estimated covariance to augment the training data in that class. Importantly, instead of augmenting the samples explicitly, we can directly minimize an upper bound of the expected cross-entropy (CE) loss on the augmented training set, leading to a highly efficient algorithm. In fact, we show that the proposed ISDA amounts to minimizing a novel robust CE loss, which adds negligible extra computational cost to a normal training procedure. Although being simple, ISDA consistently improves the generalization performance of popular deep models (ResNets and DenseNets) on a variety of datasets, e.g., CIFAR-10, CIFAR-100 and ImageNet. Code for reproducing our results is available at https://github.com/blackfeather-wang/ISDA-for-Deep-Networks.	[Wang, Yulin; Pan, Xuran; Song, Shiji; Wu, Cheng; Huang, Gao] Tsinghua Univ, Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China; [Zhang, Hong] Baidu Inc, Beijing, Peoples R China	Tsinghua University; Baidu	Huang, G (corresponding author), Tsinghua Univ, Dept Automat, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.	yulin.bh@gmail.com; fykalviny@gmail.com; shijis@tsinghua.edu.cn; pxr18@mails.tsinghua.edu.cn; wuc@tsinghua.edu.cn; gaohuang@tsinghua.edu.cn			Beijing Academy of Artificial Intelligence (BAAI) [BAA2019QN0106]; Tencent Al Lab Rhino-Bird Focused Research Program [JR201914]	Beijing Academy of Artificial Intelligence (BAAI); Tencent Al Lab Rhino-Bird Focused Research Program	Gao Huang is supported in part by Beijing Academy of Artificial Intelligence (BAAI) under grant BAA2019QN0106 and Tencent Al Lab Rhino-Bird Focused Research Program under grant JR201914.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antoniou Antreas, 2018, ARXIV171104340; Arjovsky M., 2017, ARXIV170107875; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bengio Yoshua, 2013, INT C MACHINE LEARNI, P552; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Bowles C., 2018, ARXIV PREPRINT ARXIV; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Cubuk Ekin D., 2018, ARXIV180509501; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gastaldi Xavier, 2017, ARXIV170507485; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Z., 2017, CORR, Vabs/1711.10678; Howard A.G., 2014, CORR; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2022, IEEE T PATTERN ANAL, V44, P8704, DOI 10.1109/TPAMI.2019.2918284; Jaderberg M, 2016, INT J COMPUT VISION, V116, P1, DOI 10.1007/s11263-015-0823-z; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li M., 2016, ARXIV161005586 CORR; Liang X., 2017, ICONIP; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Liu WY, 2016, PR MACH LEARN RES, V48; Maaten L., 2013, P 30 INT C MACH LEAR, P410; Mirza M., 2014, ARXIV; Odena A, 2017, PR MACH LEARN RES, V70; Ratner AJ, 2017, ADV NEUR IN, V30; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Srivastava Rupesh Kumar, 2015, ADV NEURAL INFORM PR, P2377; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Upchurch P, 2017, PROC CVPR IEEE, P6090, DOI 10.1109/CVPR.2017.645; Wang X., 2018, IJCAI; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Xie LX, 2016, PROC CVPR IEEE, P4753, DOI 10.1109/CVPR.2016.514; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zagoruyko Sergey, 2017, BMVC; Zhang ZL, 2018, ADV NEUR IN, V31; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	42	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904030
C	Xie, TY; Ma, YF; Wang, YX		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xie, Tengyang; Ma, Yifei; Wang, Yu-Xiang			Towards Optimal Off-Policy Evaluation for Reinforcement Learning with Marginalized Importance Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Motivated by the many real-world applications of reinforcement learning (RL) that require safe-policy iterations, we consider the problem of off-policy evaluation (OPE) - the problem of evaluating a new policy using the historical data obtained by different behavior policies - under the model of nonstationary episodic Markov Decision Processes (MDP) with a long horizon and a large action space. Existing importance sampling (IS) methods often suffer from large variance that depends exponentially on the RL horizon H. To solve this problem, we consider a marginalized importance sampling (MIS) estimator that recursively estimates the state marginal distribution for the target policy at every step. MIS achieves a mean-squared error of 1/n Sigma(H)(t=1) E-mu [d(t)(pi)(st)(2)/d(t)(mu)(s(t))(2) Var(mu) [pi/t(a(t)vertical bar s(t))/mu(t)(a(t)vertical bar s(t))(V-t+1(pi)(s(t+1)) + r(t)vertical bar s(t)]] + (O) over tilde (n(-1.5)) where mu and pi are the logging and target policies, d(t)(mu)(s(t)) and d(t)(pi)(st) are the marginal distribution of the state at tth step, H is the horizon, n is the sample size and V-t+1(pi) is the value function of the MDP under pi. The result matches the Cramer-Rao lower bound in Jiang and Li [2016] up to a multiplicative factor of H. To the best of our knowledge, this is the first OPE estimation error bound with a polynomial dependence on H. Besides theory, we show empirical superiority of our method in time -varying, partially observable, and long -horizon RL environments.	[Xie, Tengyang] UIUC, Dept Comp Sci, Urbana, IL 61801 USA; [Ma, Yifei] Amazoncom Serv Inc, AWS AI Labs, East Palo Alto, CA 94303 USA; [Wang, Yu-Xiang] UC Santa Barbara, Dept Comp Sci, Santa Barbara, CA 93106 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of California System; University of California Santa Barbara	Xie, TY (corresponding author), UIUC, Dept Comp Sci, Urbana, IL 61801 USA.	tx10@illinois.edu; yifeim@amazon.com; yuxiangw@cs.ucsb.edu		Wang, Yu-Xiang/0000-0002-6403-212X	UCSB CS department [NSF-OAC 1934641]; NSF-OAC [1934641]	UCSB CS department; NSF-OAC	YW was supported by a start-up grant from UCSB CS department, NSF-OAC 1934641 and a gift from AWS ML Research Award.	Bottou L, 2013, J MACH LEARN RES, V14, P3207; Chapelle O, 2015, ACM T INTEL SYST TEC, V5, DOI 10.1145/2532128; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Ernst D, 2006, IEEE DECIS CONTR P, P669; Farajtabar M, 2018, PR MACH LEARN RES, V80; Gelada C, 2019, AAAI CONF ARTIF INTE, P3647; Gottesman O, 2019, PR MACH LEARN RES, V97; Guo ZD, 2017, ADV NEUR IN, V30; Hallak A, 2017, PR MACH LEARN RES, V70; Hirano K, 2003, ECONOMETRICA, V71, P1161, DOI 10.1111/1468-0262.00442; Jiang N, 2016, PR MACH LEARN RES, V48; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Liu Q., 2018, ADV NEURAL INFORM PR, P5356; Liu YH, 2018, IEEE T NEUR NET LEAR, V29, P4983, DOI [10.1109/TSMC.2018.2867061, 10.1109/TNNLS.2017.2785278]; MANDEL T, 2014, INT C AUT AG MULT SY, P1077; Murphy SA, 2001, J AM STAT ASSOC, V96, P1410, DOI 10.1198/016214501753382327; Precup D., 2000, P 17 INT C MACH LEAR; Raghu A, 2017, P MACH LEARN HEALTHC, P147; Singh SP, 1996, MACH LEARN, V22, P123, DOI 10.1007/BF00114726; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tang L, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1587, DOI 10.1145/2505515.2514700; Theocharous G, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1806; Thomas P. S., 2015, THESIS U MASSACHUSET; Thomas PS, 2017, AAAI CONF ARTIF INTE, P4740; Thomas PS, 2015, AAAI CONF ARTIF INTE, P3000	29	12	12	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901031
C	Yang, RZ; Sun, XY; Narasimhan, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Runzhe; Sun, Xingyuan; Narasimhan, Karthik			A Generalized Algorithm for Multi-Objective Reinforcement Learning and Policy Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION	We introduce a new algorithm for multi-objective reinforcement learning (MORL) with linear preferences, with the goal of enabling few-shot adaptation to new tasks. In MORL, the aim is to learn policies over multiple competing objectives whose relative importance (preferences) is unknown to the agent. While this alleviates dependence on scalar reward design, the expected return of a policy can change significantly with varying preferences, making it challenging to learn a single model to produce optimal policies under different preference conditions. We propose a generalized version of the Bellman equation to learn a single parametric representation for optimal policies over the space of all possible preferences. After an initial learning phase, our agent can execute the optimal policy under any given preference, or automatically infer an underlying preference with very few samples. Experiments across four different domains demonstrate the effectiveness of our approach.(1)	[Yang, Runzhe; Sun, Xingyuan; Narasimhan, Karthik] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA	Princeton University	Yang, RZ (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.	runzhey@cs.princeton.edu; xs5@cs.princeton.edu; karthikn@cs.princeton.edu						Abbeel Pieter, 2004, ACM INT C P SERIES, V69; Abels Axel, 2019, P 36 INT C MACH LEAR; Aguilera Castro Adriana, 2012, Pensam. gest., P1; Amodei D., 2016, CONCRETE PROBLEMS AI; Andrychowicz M., 2017, ADV NEURAL INFORM PR; [Anonymous], 2018, ABSTRACT DYNAMIC PRO; Barrett L., 2008, P 25 INT C MACHINE L, P41, DOI DOI 10.1145/1390156.1390162; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bertsekas DP, 2017, SIAM J OPTIMIZ, V27, P1694, DOI 10.1137/16M1090946; Boutilier C, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P239; Castelletti A., 2011, 2011 International Conference on Networking, Sensing and Control (ICNSC 2011), P260, DOI 10.1109/ICNSC.2011.5874921; Chajewska U, 2001, P 18 INT C MACH LEAR, P35; Chen L., 2004, TECHNICAL REPORT; Conen W., 2001, EC'01. Proceedings of the 3rd ACM Conference on Electronic Commerce, P256, DOI 10.1145/501158.501191; Gu SX, 2016, PR MACH LEARN RES, V48; Hiraoka K, 2008, LECT NOTES COMPUT SC, V4984, P487; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Iima H, 2014, IEEE SYS MAN CYBERN, P876, DOI 10.1109/SMC.2014.6974022; Kauten C., 2018, SUPER MARIO BROS OPE; Kim IY, 2006, STRUCT MULTIDISCIP O, V31, P105, DOI 10.1007/s00158-005-0557-6; Kirk WA., 2011, INTRO METRIC SPACES, V53; Konak A, 2006, RELIAB ENG SYST SAFE, V91, P992, DOI 10.1016/j.ress.2005.11.018; Lillicrap T., 2015, CORR; Lin JG, 2005, MATH PROGRAM, V103, P1, DOI 10.1007/s10107-003-0462-y; Liu CM, 2015, IEEE T SYST MAN CY-S, V45, P385, DOI 10.1109/TSMC.2014.2358639; Mannor S., 2001, NEURAL INFORM PROCES, P1563; Metz Luke, 2017, CORR; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mossalam H., 2016, CORR; Nakayama H, 2009, VECTOR OPTIM, P1, DOI 10.1007/978-3-540-88910-6; Natarajan S., 2005, P 22 INT C MACH LEAR, P601; Naylor A., 2000, LINEAR OPERATOR THEO; Parisi S, 2017, NEUROCOMPUTING, V263, P3, DOI 10.1016/j.neucom.2016.11.094; Pirotta M, 2015, AAAI CONF ARTIF INTE, P2928; Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987; Schatzmann J, 2009, IEEE T AUDIO SPEECH, V17, P733, DOI 10.1109/TASL.2008.2012071; Schaul T., 2015, CORR; Tesauro G., 2007, ADV NEURAL INFORM PR, P1497; Ultes S, 2017, 18TH ANNUAL MEETING OF THE SPECIAL INTEREST GROUP ON DISCOURSE AND DIALOGUE (SIGDIAL 2017), P65; Ultes S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P73, DOI 10.18653/v1/P17-4013; Vamplew P, 2011, MACH LEARN, V84, P51, DOI 10.1007/s10994-010-5232-5; van der Maaten Laurens, 2008, J MACHINE LEARNING R; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Van Moffaert K, 2013, IEEE SYMP ADAPT DYNA, P191, DOI 10.1109/ADPRL.2013.6615007; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; WATSON LT, 1989, COMPUT METHOD APPL M, V74, P289, DOI 10.1016/0045-7825(89)90053-4; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	50	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906031
C	Yin, D; Lopes, RG; Shlens, J; Cubuk, ED; Gilmer, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yin, Dong; Lopes, Raphael Gontijo; Shlens, Jonathon; Cubuk, Ekin D.; Gilmer, Justin			A Fourier Perspective on Model Robustness in Computer Vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Achieving robustness to distributional shift is a longstanding and challenging goal of computer vision. Data augmentation is a commonly used approach for improving robustness, however robustness gains are typically not uniform across corruption types. Indeed increasing performance in the presence of random noise is often met with reduced performance on other corruptions such as contrast change. Understanding when and why these sorts of trade-offs occur is a crucial step towards mitigating them. Towards this end, we investigate recently observed trade-offs caused by Gaussian data augmentation and adversarial training. We find that both methods improve robustness to corruptions that are concentrated in the high frequency domain while reducing robustness to corruptions that are concentrated in the low frequency domain. This suggests that one way to mitigate these trade-offs via data augmentation is to use a more diverse set of augmentations. Towards this end we observe that AutoAugment [6], a recently proposed data augmentation policy optimized for clean accuracy, achieves state-of-the-art robustness on the CIFAR-10-C [17] benchmark.	[Yin, Dong] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA; [Yin, Dong; Lopes, Raphael Gontijo; Shlens, Jonathon; Cubuk, Ekin D.; Gilmer, Justin] Google Res, Brain Team, Mountain View, CA 94043 USA; [Lopes, Raphael Gontijo] Google AI Residency Program, Redwood City, CA USA	University of California System; University of California Berkeley; Google Incorporated	Yin, D (corresponding author), Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA.; Yin, D (corresponding author), Google Res, Brain Team, Mountain View, CA 94043 USA.	dongyin@berkeley.edu; iraphael@google.com; shlens@google.com; cubuk@google.com; gilmer@google.com						[Anonymous], 2020, INT C LEARN RE UNPUB; Aydemir A. E., 2018, EFFECTS JPEG JPEG200; Azulay A., 2018, WHY DO DEEP CONVOLUT; Bracewell R., 1986, FOURIER TRANSFORM IT; CARLINI N, 2017, S P, P39, DOI DOI 10.1109/SP.2017.49; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Das N, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P196, DOI 10.1145/3219819.3219910; Dodge Samuel, 2017, 2017 26 INT C COMP C, P1, DOI DOI 10.1109/ICCCN.2017.8038465; Dziedzic A., 2019, P 36 INT C MACH LEAR, P1745; FORD N, 2019, ARXIV190110513; Geirhos R., 2019, P INT C LEARN REPR I; Geirhos R., 2018, ADV NEURAL INFORM PR, P7538; Gilmer Justin, 2018, ARXIV180706732; Goodfellow I. J., 2015, P INT C LEARN REPR I; Guo C., 2019, ARXIV190507121; Hendrycks Dan, 2019, P ICLR; Ilyas A, 2019, ADV NEUR IN, V32; Jacobsen Jorn-Henrik, 2018, ABS181100401 CORR; Jo J., 2017, ARXIV; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Liu ZH, 2019, PROC CVPR IEEE, P860, DOI 10.1109/CVPR.2019.00095; Madry Aleksander, 2018, P INT C LEARN REPR I; Olah C., 2017, DISTILL; Recht B, 2019, PR MACH LEARN RES, V97; Tsipras Dimitris, 2019, P INT C LEARN REPR I; Tsuzuku Y, 2019, PROC CVPR IEEE, P51, DOI 10.1109/CVPR.2019.00014; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	27	12	12	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904086
C	Zhao, Q; Wang, YS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Qi; Wang, Yusu			Learning metrics for persistence-based summaries and applications for graph classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently a new feature representation framework based on a topological tool called persistent homology (and its persistence diagram summary) has gained much momentum. A series of methods have been developed to map a persistence diagram to a vector representation so as to facilitate the downstream use of machine learning tools. In these approaches, the importance (weight) of different persistence features are usually pre-set. However often in practice, the choice of the weight-function should depend on the nature of the specific data at hand. It is thus highly desirable to learn a best weight-function (and thus metric for persistence diagrams) from labelled data. We study this problem and develop a new weighted kernel, called WKPI, for persistence summaries, as well as an optimization framework to learn the weight (and thus kernel). We apply the learned kernel to the challenging task of graph classification, and show that our WKPI-based classification framework obtains similar or (sometimes significantly) better results than the best results from a range of previous graph classification frameworks on benchmark datasets.	[Zhao, Qi; Wang, Yusu] Ohio State Univ, Comp Sci & Engn Dept, Columbus, OH 43221 USA	University System of Ohio; Ohio State University	Zhao, Q (corresponding author), Ohio State Univ, Comp Sci & Engn Dept, Columbus, OH 43221 USA.	zhao.2017@osu.edu; yusu@cse.ohio-state.edu			National Science Foundation [CCF-1740761, CCF-1733798, RI-1815697]; National Health Institute [R01EB022899]	National Science Foundation(National Science Foundation (NSF)); National Health Institute	The authors would like to thank Chao Chen and Justin Eldridge for useful discussions related to this project. We would also like to thank Giorgio Ascoli for helping provide the neuron dataset. This work is partially supported by National Science Foundation via grants CCF-1740761, CCF-1733798, and RI-1815697, as well as by National Health Institute under grant R01EB022899.	Adams H, 2017, J MACH LEARN RES, V18; Bai L, 2015, PATTERN RECOGN, V48, P344, DOI 10.1016/j.patcog.2014.03.028; Bauer U., MATH SOFTWARE ICMS 2, P137; Bhatia S., 2018, ARXIV181104049; Boissonnat Jean-Daniel, 2014, GUDHI LIB SIMPLICIAL; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Bubenik P, 2015, J MACH LEARN RES, V16, P77; Carlsson G, 2010, FOUND COMPUT MATH, V10, P367, DOI 10.1007/s10208-010-9066-0; Carlsson G, 2009, PROCEEDINGS OF THE TWENTY-FIFTH ANNUAL SYMPOSIUM ON COMPUTATIONAL GEOMETRY (SCG'09), P247, DOI 10.1145/1542362.1542408; Carlsson G, 2009, DISCRETE COMPUT GEOM, V42, P71, DOI 10.1007/s00454-009-9176-0; Carriere M., 2019, ARXIV190409378; Carriere M, 2017, PR MACH LEARN RES, V70; Chazal F., 2016, SPRINGERBRIEFS MATH; Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5; Cohen-Steiner D, 2010, FOUND COMPUT MATH, V10, P127, DOI 10.1007/s10208-010-9060-6; Cohen-Steiner D, 2009, FOUND COMPUT MATH, V9, P79, DOI 10.1007/s10208-008-9027-z; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Dey T. K., 2016, 24 ANN EUR S ALG; Dobson Paul D, 2003, J Mol Biol, V330, P771; Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2; Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO; Helma C, 2001, BIOINFORMATICS, V17, P107, DOI 10.1093/bioinformatics/17.1.107; Hido S, 2009, IEEE DATA MINING, P179, DOI 10.1109/ICDM.2009.30; Hofer C., 2017, ADV NEURAL INFORM PR, P1634; Kerber M., 2017, ACM J EXP ALGORITHMI, V22, DOI 10.1145/3064175; Kerber Michael, 2018, HERA SOFTWARE COMPUT; Kerber Michael, 2017, 33 INT S COMP GEOM S, P57; Kokiopoulou E, 2011, NUMER LINEAR ALGEBR, V18, P565, DOI 10.1002/nla.743; Kondor R., 2018, 6 INT C LEARN REPR I; Kriege NM, 2016, ADV NEUR IN, V29; Kusano G, 2018, J MACH LEARN RES, V18; Le T, 2018, ADV NEUR IN, V31; Levie R, 2019, IEEE T SIGNAL PROCES, V67, P97, DOI 10.1109/TSP.2018.2879624; Li YQ, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0182231; Lin Y, 2011, TOHOKU MATH J, V63, P605, DOI 10.2748/tmj/1325886283; Markisic S, 2012, PROCEEDINGS 26TH EUROPEAN CONFERENCE ON MODELLING AND SIMULATION ECMS 2012, P37, DOI 10.7148/2012-0037-0043; Niepert M, 2016, PR MACH LEARN RES, V48; Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106; Rieck B, 2019, PR MACH LEARN RES, V97; Sheehy D., 2012, P 28 ANN S COMP GEOM, P239; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Verma S, 2017, ADV NEUR IN, V30; Xu K, 2019, PROC INT CONF PARAL, DOI 10.1145/3337821.3337923; Xu L, 2015, ACSR ADV COMPUT, P246; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zhang Z, 2018, ADV NEUR IN, V31	49	12	12	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901048
C	Zheng, KC; Zha, ZJ; Wei, W		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zheng, Kecheng; Zha, Zheng-Jun; Wei, Wei			Abstract Reasoning with Distracting Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A Abstraction reasoning is a long-standing challenge in artificial intelligence. Recent studies suggest that many of the deep architectures that have triumphed over other domains failed to work well in abstract reasoning. In this paper, we first illustrate that one of the main challenges in such a reasoning task is the presence of distracting features, which requires the learning algorithm to leverage counter-evidence and to reject any of the false hypotheses in order to learn the true patterns. We later show that carefully designed learning trajectory over different categories of training data can effectively boost learning performance by mitigating the impacts of distracting features. Inspired by this fact, we propose feature robust abstract reasoning (FRAR) model, which consists of a reinforcement learning based teacher network to determine the sequence of training and a student network for predictions. Experimental results demonstrated strong improvements over baseline algorithms and we are able to beat the state-of-the-art models by 18.7% in the RAVEN dataset and 13.3% in the PGM dataset.	[Zheng, Kecheng; Zha, Zheng-Jun] Univ Sci & Technol China, Hefei, Peoples R China; [Wei, Wei] Google Res, Mountain View, CA USA	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Google Incorporated	Zha, ZJ (corresponding author), Univ Sci & Technol China, Hefei, Peoples R China.	zkcys001@mail.ustc.edu.cn; zhazj@ustc.edu.cn; wewei@google.com	Zha, Zheng-Jun/AAF-8667-2020; Zha, Zheng-Jun/AAE-8408-2020	Zha, Zheng-Jun/0000-0003-2510-8993	National Key R&D Program of China [2017YFB1300201]; National Natural Science Foundation of China (NSFC) [61622211, 61620106009]; Fundamental Research Funds for the Central Universities [WK2100100030]	National Key R&D Program of China; National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported by the National Key R&D Program of China under Grant 2017YFB1300201, the National Natural Science Foundation of China (NSFC) under Grants 61622211 and 61620106009 as well as the Fundamental Research Funds for the Central Universities under Grant WK2100100030.	ACHILLE A, 2018, ADV NEURAL INFORM PR, P9873, DOI DOI 10.1364/OPTICA.5.000803; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bringsjord S., 2003, INT JOINT C ART INT, P887; CARPENTER PA, 1990, PSYCHOL REV, V97, P404, DOI 10.1037/0033-295X.97.3.404; Chang H. -S., 2017, ADV NEURAL INFORM PR, V30, P1002; Dehghani Mostafa, 2018, FIDELITY WEIGHTED LE; Eastwood Cian, 2018, INT C LEARN REPR; Esmaeili Babak, 2019, HIERARCHICAL DISENTA; Fan Y., 2018, ARXIV PREPRINT ARXIV; Fan YB, 2017, AAAI CONF ARTIF INTE, P1877; Hernandez-Orallo J, 2016, ARTIF INTELL, V230, P74, DOI 10.1016/j.artint.2015.09.011; Higgins Irina, 2018, SCAN LEARNING ABSTRA; Hill Felix, 2019, INT C LEARN REPR; Hoshen D., 2017, ARXIV PREPRINT ARXIV; Jiang L, 2014, ADV NEUR IN, V27; Jordan Michael I, 2018, INFORM CONSTRAINTS A; Kabra Rishab, 2018, MULTIOBJECT REPRESEN; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kumar A., 2018, VARIATIONAL INFERENC; Kumar A, 2010, ASIA PACIF MICROWAVE, P1189; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; Little Daniel R, 2012, P ANN M COGN SCI SOC, V34; Lovett A, 2017, PSYCHOL REV, V124, P60, DOI 10.1037/rev0000039; Lovett A, 2009, COGNITIVE SCI, V33, P1192, DOI 10.1111/j.1551-6709.2009.01052.x; Lovett Andrew, 2010, P ANN M COGN SCI SOC, V32; Lu J, 2018, PR MACH LEARN RES, V80; Lu Jiang, 2015, 29 AAAI C ART INT; Malisiewicz T, 2011, IEEE I CONF COMP VIS, P89, DOI 10.1109/ICCV.2011.6126229; McGreggor Keith, 2014, 28 AAAI C ART INT; Mekik Can Serif, 2018, P INT JOINT C ART IN, P1576; Raven JC., 1998, RAVENS PROGRESSIVE M, V759; Ren MY, 2018, PR MACH LEARN RES, V80; Santoro A., 2017, ADV NEURAL INFORM PR, P4967; Santoro A., 2018, INT C MACH LEARN, P4477; Steenbrugge Xander, 2018, P 32 C NEUR INF PROC; van Steenkiste Sjoerd, 2019, CORR; Wang K, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P903; Zhang Chi, 2019, P IEEE C COMP VIS PA, P8	39	12	12	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305079
C	Zhou, ZX; Tan, SL; Xu, ZZ; Li, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Zhixin; Tan, Shulong; Xu, Zhaozhuo; Li, Ping			Mobius Transformation for Fast Inner Product Search on Graph	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	We present a fast search on graph algorithm for Maximum Inner Product Search (MIPS). This optimization problem is challenging since traditional Approximate Nearest Neighbor (ANN) search methods may not perform efficiently in the nonmetric similarity measure. Our proposed method is based on the property that Mobius transformation introduces an isomorphism between a subgraph of l(2)-Delaunay graph and Delaunay graph for inner product. Under this observation, we propose a simple but novel graph indexing and searching algorithm to find the optimal solution with the largest inner product with the query. Experiments show our approach leads to significant improvements compared to existing methods.	[Zhou, Zhixin] Cognit Comp Lab, 10900 NE 8th St, Bellevue, WA 98004 USA; Baidu Res, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA	Baidu	Zhou, ZX (corresponding author), Cognit Comp Lab, 10900 NE 8th St, Bellevue, WA 98004 USA.	zhixin0825@gmail.com; shulongtan@baidu.com; zhaozhuoxu@gmail.com; liping11@baidu.com	Zhou, Zhixin/GSD-7016-2022	ZHOU, Zhixin/0000-0003-3737-9248				Bachrach Y, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P257, DOI 10.1145/2645710.2645741; Barber CB, 1996, ACM T MATH SOFTWARE, V22, P469, DOI 10.1145/235815.235821; Beaumont Olivier, 2007, 2007 S, P1; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Cayton L., 2008, P 25 INT C MACHINE L, P112; Curtin R.R., 2013, P 2013 SIAM INT C DA, P1; Curtin RR, 2014, STAT ANAL DATA MIN, V7, P229, DOI 10.1002/sam.11218; Fan M, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2509, DOI 10.1145/3292500.3330651; Fortune S., 2004, HDB DISCRETE COMPUTA, P513; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; FRIEDMAN JH, 1975, IEEE T COMPUT, V24, P1000, DOI 10.1109/T-C.1975.224110; Fu C, 2019, PROC VLDB ENDOW, V12, P461, DOI 10.14778/3303753.3303754; George P.-L., 1998, DELAUNAY TRIANGULATI; Guo RQ, 2016, JMLR WORKSH CONF PRO, V51, P482; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Johnson Jeff, 2017, BILLION SCALE SIMILA; Kleinberg Jon M., P 32 ANN ACM S THEOR, P163; Kuehnel W, 2007, J MATH PURE APPL, V88, P251, DOI 10.1016/j.matpur.2007.06.005; Li Ping, 2012, P 21 INT C WORLD WID, P565; Malkov Y. A., IEEE T PATTERN ANAL; Malkov Y, 2014, INFORM SYST, V45, P61, DOI 10.1016/j.is.2013.10.006; Morozov S., 2018, ADV NEURAL INFORM PR, V31, P4721; Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926; Ram Parikshit, 2012, P 18 ACM SIGKDD INT, P931, DOI DOI 10.1145/2339530.2339677; Shrivastava A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P812; Shrivastava A, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P981, DOI 10.1145/2736277.2741285; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Tan Shulong, 2019, EMNLP IJCNLP, P5235; Tan Shulong, 2020, P 13 ACM INT C WEB S; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Weston J, 2010, MACH LEARN, V81, P21, DOI 10.1007/s10994-010-5198-3; Xue HJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3203; Yan Xiao, 2018, ADV NEURAL INFORM PR, P2956; Yu H.-F., 2014, INT C MACH LEARN, P593; Yu Hsiang-Fu, 2017, ADV NEURAL INFORM PR, P5453; Zhao Weijie, 2020, 35 IEEE INT C DAT EN	39	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308026
C	Blier, L; Ollivier, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Blier, Leonard; Ollivier, Yann			The Description Length of Deep Learning Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Solomonoff's general theory of inference (Solomonoff, 1964) and the Minimum Description Length principle (Grunwald, 2007; Rissanen, 2007) formalize Occam's razor, and hold that a good model of data is a model that is good at losslessly compressing the data, including the cost of describing the model itself. Deep neural networks might seem to go against this principle given the large number of parameters to be encoded. We demonstrate experimentally the ability of deep neural networks to compress the training data even when accounting for parameter encoding. The compression viewpoint originally motivated the use of variational methods in neural networks (Hinton and Van Camp, 1993; Schmidhuber, 1997). Unexpectedly, we found that these variational methods provide surprisingly poor compression bounds, despite being explicitly built to minimize such bounds. This might explain the relatively poor practical performance of variational methods in deep learning. On the other hand, simple incremental encoding methods yield excellent compression values on deep networks, vindicating Solomonoff's approach.	[Blier, Leonard] Ecole Normale Super, Paris, France; [Ollivier, Yann] Facebook Artificial Intelligence Res, Paris, France	UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Facebook Inc	Blier, L (corresponding author), Ecole Normale Super, Paris, France.	leonard.blier@normalesup.org; yol@fb.com						Achille A., 2017, ARXIV170601350; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora S, 2018, PR MACH LEARN RES, V80; Blum A, 2003, LECT NOTES ARTIF INT, V2777, P344, DOI 10.1007/978-3-540-45167-9_26; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Chaitin G. J., 2007, THINKING GODEL TURIN; DAWID AP, 1984, J ROY STAT SOC A STA, V147, P278, DOI 10.2307/2981683; Dziugaite GK, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Gao T., 2016, ARXIV160309260; Graves A., 2011, ADV NEURAL INFORM PR; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762; Hutter M., 2007, THEORETICAL COMPUTER, V384; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li C., 2018, P 6 INT C LEARN REPR; Li M, 2008, INTRO KOLMOGOROV COM; Louizos C, 2017, ADV NEUR IN, V30; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Ollivier Y., 2014, ARXIV14037752; RISSANEN J, 1992, IEEE T INFORM THEORY, V38, P315, DOI 10.1109/18.119689; Rissanen J., 2007, INFORM COMPLEXITY ST; Romero Adriana, 2015, ICLR; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Schmidhuber J, 1997, NEURAL NETWORKS, V10, P857, DOI 10.1016/S0893-6080(96)00127-X; See Abigail, 2016, ARXIV160609274; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; Shwartz-Ziv Ravid, 2017, ARXIV170300810; Solomonoff R., 1964, INFORM CONTROL; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tallec C., 2018, PYVARINF VARIATIONAL; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Ullrich Karen, 2017, ICLR; van Erven T, 2012, J R STAT SOC B, V74, P361, DOI 10.1111/j.1467-9868.2011.01025.x; Xu TB, 2017, LECT NOTES COMPUT SC, V10666, P590, DOI 10.1007/978-3-319-71607-7_52; Yang YH, 1999, ANN STAT, V27, P1564; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414	45	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302024
C	Caterini, AL; Doucet, A; Sejdinovic, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Caterini, Anthony L.; Doucet, Arnaud; Sejdinovic, Dino			Hamiltonian Variational Auto-Encoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Variational Auto-Encoders (VAEs) have become very popular techniques to perform inference and learning in latent variable models: they allow us to leverage the rich representational power of neural networks to obtain flexible approximations of the posterior of latent variables as well as tight evidence lower bounds (ELBOs). Combined with stochastic variational inference, this provides a methodology scaling to large datasets. However, for this methodology to be practically efficient, it is necessary to obtain low-variance unbiased estimators of the ELBO and its gradients with respect to the parameters of interest. While the use of Markov chain Monte Carlo (MCMC) techniques such as Hamiltonian Monte Carlo (HMC) has been previously suggested to achieve this [25, 28], the proposed methods require specifying reverse kernels which have a large impact on performance. Additionally, the resulting unbiased estimator of the ELBO for most MCMC kernels is typically not amenable to the reparameterization trick. We show here how to optimally select reverse kernels in this setting and, by building upon Hamiltonian Importance Sampling (HIS) [19], we obtain a scheme that provides low-variance unbiased estimators of the ELBO and its gradients using the reparameterization trick. This allows us to develop a Hamiltonian Variational Auto-Encoder (HVAE). This method can be re-interpreted as a target-informed normalizing flow [22] which, within our context, only requires a few evaluations of the gradient of the sampled likelihood and trivial Jacobian calculations at each iteration.	[Caterini, Anthony L.; Doucet, Arnaud; Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England; [Doucet, Arnaud; Sejdinovic, Dino] Alan Turing Inst Data Sci, London, England	University of Oxford	Caterini, AL (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	anthony.caterini@stats.ox.ac.uk; doucet@stats.ox.ac.uk; dino.sejdinovic@stats.ox.ac.uk		Doucet, Arnaud/0000-0002-7662-419X; Sejdinovic, Dino/0000-0001-5547-9213	UK government	UK government	Anthony L. Caterini is a Commonwealth Scholar, funded by the UK government.	Abadi M, 2015, P 12 USENIX S OPERAT; Burda Yuri, 2016, 4 INT C LEARN REPR I; Crooks GE, 1998, J STAT PHYS, V90, P1481, DOI 10.1023/A:1023208217925; Cuendet MA, 2006, PHYS REV LETT, V96, DOI 10.1103/PhysRevLett.96.120602; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Dosovitskiy A, 2015, PROC CVPR IEEE, P1538, DOI 10.1109/CVPR.2015.7298761; Glasserman Paul, 1991, GRADIENT ESTIMATION, V116; Heng Jeremy, 2017, ARXIV170808396; Hoffman MD, 2017, PR MACH LEARN RES, V70; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Jarzynski C, 1997, PHYS REV LETT, V78, P2690, DOI 10.1103/PhysRevLett.78.2690; Jarzynski C, 2000, J STAT PHYS, V98, P77, DOI 10.1023/A:1018670721277; Kingma D.P, P 3 INT C LEARNING R; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal Radford M, 2005, BANFF INT RES STAT B; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Procacci P, 2006, J CHEM PHYS, V125, DOI 10.1063/1.2360273; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Scholl-Paschinger E, 2006, J CHEM PHYS, V125, DOI 10.1063/1.2227025; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; van den Berg Rianne, 2018, ARXIV180305649; Wolf Christopher, 2016, ARXIV160908203	27	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002069
C	Deshpande, Y; Montanari, A; Mossel, E; Sen, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Deshpande, Yash; Montanari, Andrea; Mossel, Elchanan; Sen, Subhabrata			Contextual Stochastic Block Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMMUNITY DETECTION; MUTUAL INFORMATION; NETWORKS	We provide the first information theoretic tight analysis for inference of latent community structure given a sparse graph along with high dimensional node covariates, correlated with the same latent communities. Our work bridges recent theoretical breakthroughs in the detection of latent community structure without nodes covariates and a large body of empirical work using diverse heuristics for combining node covariates with graphs for inference. The tightness of our analysis implies in particular, the information theoretical necessity of combining the different sources of information. Our analysis holds for networks of large degrees as well as for a Gaussian version of the model.	[Deshpande, Yash; Mossel, Elchanan; Sen, Subhabrata] MIT, Dept Math, Cambridge, MA 02139 USA; [Montanari, Andrea] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Montanari, Andrea] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Massachusetts Institute of Technology (MIT); Stanford University; Stanford University	Deshpande, Y (corresponding author), MIT, Dept Math, Cambridge, MA 02139 USA.				 [NSF DMS-1613091];  [NSF CCF-1714305];  [NSF IIS-1741162];  [NSF DMS-1737944];  [ONR N00014-17-1-2598]	; ; ; ; 	A.M. was partially supported by grants NSF DMS-1613091, NSF CCF-1714305 and NSF IIS-1741162. E.M was partially supported by grants NSF DMS-1737944 and ONR N00014-17-1-2598. Y.D would like to acknowledge Nilesh Tripuraneni for discussions about this paper.	Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Aicher C, 2015, J COMPLEX NETW, V3, P221, DOI 10.1093/comnet/cnu026; Balasubramanyan R., 2011, SIAM INT C DATA MINI, P450, DOI DOI 10.1137/1.9781611972818.39; Binkiewicz N, 2017, BIOMETRIKA, V104, P361, DOI 10.1093/biomet/asx008; Bothorel C, 2015, NETW SCI, V3, P408, DOI 10.1017/nws.2015.9; Chang J, 2010, ANN APPL STAT, V4, P124, DOI 10.1214/09-AOAS309; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Cheng H, 2011, ACM T KNOWL DISCOV D, V5, DOI 10.1145/1921632.1921638; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Dang T.A., 2012, INT C DIG SOC ICDS, P7; Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y; Eaton E., 2012, P AAAI C ART INT, P900; Gibert J, 2012, PATTERN RECOGN, V45, P3072, DOI 10.1016/j.patcog.2012.01.009; Gunnemann S, 2013, IEEE DATA MINING, P231, DOI 10.1109/ICDM.2013.110; Guo DN, 2005, IEEE T INFORM THEORY, V51, P1261, DOI 10.1109/TIT.2005.844072; Hoff Peter D, 2003, RANDOM EFFECTS MODEL; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Johnstone I.M., 2004, SPARSE PRINCIP UNPUB; Kanade V, 2016, IEEE T INFORM THEORY, V62, P5906, DOI 10.1109/TIT.2016.2516564; Kim Myunghwan, 2012, ARXIV12054546, P1719; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Kumar V, 2012, MAGN RESON IMAGING, V30, P1234, DOI 10.1016/j.mri.2012.06.010; Lelarge M, 2015, IEEE T NETW SCI ENG, V2, P152, DOI 10.1109/TNSE.2015.2490580; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; McAuley Julian, 2012, ADV NEURAL INFORM PR, P539; Mezard M., 2009, INFORM PHYS COMPUTAT; Montanari A., 2012, COMPRESSED SENSING T; Montanari A., 2015, ADV NEURAL INFORM PR, P217; MOSSEL E., 2013, CORR, V38, P1; Mossel E, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P71, DOI 10.1145/2840728.2840749; Neville J, 2003, P TEXT MIN LINK AN W; Newman MEJ, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms11863; Ng AY, 2002, ADV NEUR IN, V14, P849; Onatski A, 2013, ANN STAT, V41, P1204, DOI 10.1214/13-AOS1100; Peel L., 2012, ARXIV12095561; Silva A, 2012, PROC VLDB ENDOW, V5, P466, DOI 10.14778/2140436.2140443; Smith LM, 2016, ACM T KNOWL DISCOV D, V11, DOI 10.1145/2968451; Tramel E. W., 2014, ARXIV14095557; Hoang TA, 2014, LECT NOTES COMPUT SC, V8851, P1, DOI 10.1007/978-3-319-13734-6_1; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Xu Z., 2012, SIGMOD, P505; Yang J, 2013, IEEE DATA MINING, P1151, DOI 10.1109/ICDM.2013.167; Yang TB, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P927; Zanghi H, 2010, PATTERN RECOGN LETT, V31, P830, DOI 10.1016/j.patrec.2010.01.026; Zhang P, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.052802; Zhou Y., 2009, PROC VLDB ENDOW, V2, P718, DOI DOI 10.14778/1687627.1687709	61	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003017
C	Dhurandhar, A; Shanmugam, K; Luss, R; Olsen, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dhurandhar, Amit; Shanmugam, Karthikeyan; Luss, Ronny; Olsen, Peder			Improving Simple Models with Confidence Profiles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we propose a new method called ProfWeight for transferring information from a pre-trained deep neural network that has a high test accuracy to a simpler interpretable model or a very shallow network of low complexity and a priori low test accuracy. We are motivated by applications in interpretability and model deployment in severely memory constrained environments (like sensors). Our method uses linear probes to generate confidence scores through flattened intermediate representations. Our transfer method involves a theoretically justified weighting of samples during the training of the simple model using confidence scores of these intermediate layers. The value of our method is first demonstrated on CIFAR-10, where our weighting method significantly improves (3-4%) networks with only a fraction of the number of Resnet blocks of a complex Resnet model. We further demonstrate operationally significant results on a real manufacturing problem, where we dramatically increase the test accuracy of a CART model (the domain standard) by roughly 13%.	[Dhurandhar, Amit; Shanmugam, Karthikeyan; Luss, Ronny; Olsen, Peder] IBM Res, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Dhurandhar, A (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	adhuran@us.ibm.com; karthikeyan.shanmugam2@ibm.com; rluss@us.ibm.com; pederao@us.ibm.com						Agarwal D., 2011, 14 INT C ART INT STA, P93; Alain Guillaume, 2016, ARXIV161001644; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Bastani Osbert, 2017, PANCREATIC CANC; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Carrier P.-L., 2013, ICML; Chen YH, 2016, CONF PROC INT SYMP C, P367, DOI 10.1109/ISCA.2016.40; Dhurandhar A., 2017, ARXIV PREPRINT ARXIV; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Grippo L, 2000, OPER RES LETT, V26, P127, DOI 10.1016/S0167-6377(99)00074-7; Guo Chuan, 2017, ICML, DOI DOI 10.5555/3305381.3305518; He K., 2015, INT C COMP VIS PATT; Hinton J. D. Geoffrey, 2015, DISTILLING KNOWLEDGE; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lindstrom M., 2016, SMALL DATA TINY CLUE, P35; Lipton Z. C., 2015, KDNUGGETS; Lopez-Paz D., 2016, P INT C LEARN REPR, P1; Lundberg S.-I. L. Scott, 2017, ADV NEURAL INF P SYS; Montavon G, 2018, DIGIT SIGNAL PROCESS, V73, P1, DOI 10.1016/j.dsp.2017.10.011; Reagen B, 2016, CONF PROC INT SYMP C, P267, DOI 10.1109/ISCA.2016.32; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Romero Adriana, 2015, ICLR; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Subramanya R. B. Akshayvarun, 2017, ARXIV170707013; Tan S., 2017, CORR; Weiss S., 2013, ACM SIGKDD C KNOWL D	31	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004081
C	Le, T; Yamada, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Le, Tam; Yamada, Makoto			Persistence Fisher Kernel: A Riemannian Manifold Kernel for Persistence Diagrams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				HOMOLOGY; SHAPES	Algebraic topology methods have recently played an important role for statistical analysis with complicated geometric structured data such as shapes, linked twist maps, and material data. Among them, persistent homology is a well-known tool to extract robust topological features, and outputs as persistence diagrams (PDs). However, PDs are point multi-sets which can not be used in machine learning algorithms for vector data. To deal with it, an emerged approach is to use kernel methods, and an appropriate geometry for PDs is an important factor to measure the similarity of PDs. A popular geometry for PDs is the Wasserstein metric. However, Wasserstein distance is not negative definite. Thus, it is limited to build positive definite kernels upon the Wasserstein distance without approximation. In this work, we rely upon the alternative Fisher information geometry to propose a positive definite kernel for PDs without approximation, namely the Persistence Fisher (PF) kernel. Then, we analyze eigensystem of the integral operator induced by the proposed kernel for kernel machines. Based on that, we derive generalization error bounds via covering numbers and Rademacher averages for kernel machines with the PF kernel. Additionally, we show some nice properties such as stability and infinite divisibility for the proposed kernel. Furthermore, we also propose a linear time complexity over the number of points in PDs for an approximation of our proposed kernel with a bounded error. Throughout experiments with many different tasks on various benchmark datasets, we illustrate that the PF kernel compares favorably with other baseline kernels for PDs.	[Le, Tam; Yamada, Makoto] RIKEN Ctr Adv Intelligence Project, Tokyo, Japan; [Yamada, Makoto] Kyoto Univ, Kyoto, Japan	RIKEN; Kyoto University	Le, T (corresponding author), RIKEN Ctr Adv Intelligence Project, Tokyo, Japan.	tam.le@riken.jp; makoto.yamada@riken.jp			JSPS KAKENHI [17K12745]; JST PRESTO program [JPMJPR165A]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST PRESTO program	We thank Ha Quang Minh, and anonymous reviewers for their comments. TL acknowledges the support of JSPS KAKENHI Grant number 17K12745. MY was supported by the JST PRESTO program JPMJPR165A.	Adams H, 2017, J MACH LEARN RES, V18; [Anonymous], 1972, NATURE, V239, P488; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Berg C., 1984, HARMONIC ANAL SEMIGR; Bubenik P, 2015, J MACH LEARN RES, V16, P77; Cang ZX., 2015, MOL BASED MATH BIOL, V3, P140, DOI 10.1515/mlbmb-2015-0009; Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x; Carriere M, 2017, PR MACH LEARN RES, V70; Carriere M, 2015, COMPUT GRAPH FORUM, V34, P1, DOI 10.1111/cgf.12692; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5; de Silva V, 2007, ALGEBR GEOM TOPOL, V7, P339, DOI 10.2140/agt.2007.7.339; Di Fabio B, 2015, LECT NOTES COMPUT SC, V9279, P294, DOI 10.1007/978-3-319-23231-7_27; Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454; Edelsbrunner H, 2008, CONTEMP MATH, V453, P257; Elliott S.R., 1983, PHYS AMORPHOUS MAT; FERAGEN A, 2015, PROC CVPR IEEE, P3032; Francois N, 2013, PHYS REV LETT, V111, DOI 10.1103/PhysRevLett.111.148001; GREENGARD L, 1991, SIAM J SCI STAT COMP, V12, P79, DOI 10.1137/0912004; Guo Ying, 1999, COLT, P267; Harchaoui Z., 2009, ADV NEURAL INFORM PR, V21, P609; Hertzsch JM, 2007, SMALL, V3, P202, DOI 10.1002/smll.200600361; Hofer C., 2017, ADV NEURAL INFORM PR, P1634; Istas J, 2012, ESAIM-PROBAB STAT, V16, P222, DOI 10.1051/ps/2011106; Jayasumana S, 2015, IEEE T PATTERN ANAL, V37, P2464, DOI 10.1109/TPAMI.2015.2414422; Kasson PM, 2007, BIOINFORMATICS, V23, P1753, DOI 10.1093/bioinformatics/btm250; Kusano G, 2018, J MACH LEARN RES, V18; Kusano G, 2016, PR MACH LEARN RES, V48; Kwitt R., 2015, ADV NEURAL INFORM PR, V28, P3052; Lafferty J, 2005, J MACH LEARN RES, V6, P129; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Le T, 2015, PR MACH LEARN RES, V37, P2002; Le T, 2015, MACH LEARN, V99, P169, DOI 10.1007/s10994-014-5446-z; Lee H, 2011, I S BIOMED IMAGING, P841, DOI 10.1109/ISBI.2011.5872535; Lee J. M., 2006, RIEMANNIAN MANIFOLDS, DOI 10.1007/b98852; Levy P., 1965, PROCESSUS STOCHASTIQ; Mendelson S, 2004, J MACH LEARN RES, V4, P759, DOI 10.1162/1532443041424337; Minh H.Q., 2006, INT C COMP LEARN THE, P154, DOI DOI 10.1007/11776420_14; Morariu V.I., 2009, ADV NEURAL INFORM PR, P1113; Muller Claus, 2012, ANAL SPHERICAL SYMME, V129; Nakamura T, 2015, NANOTECHNOLOGY, V26, DOI 10.1088/0957-4484/26/30/304001; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Petri G, 2014, J R SOC INTERFACE, V11, DOI 10.1098/rsif.2014.0873; Peyre G., 2017, COMPUTATIONAL OPTIMA; Quadriato N., 2016, JMLR, P2732, DOI DOI 10.1016/J.CSDA.2013.01.026; Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106; Schoenberg I., 1942, DUKE MATH J, V9, P96, DOI DOI 10.1215/S0012-7094-42-00908-6; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Singh G, 2008, J VISION, V8, DOI 10.1167/8.8.11; Smola AJ, 2001, ADV NEUR IN, V13, P308; Turaga P., 2016, IEEE C COMP VIS PATT, P68; Turner K, 2014, INF INFERENCE, V3, P310, DOI 10.1093/imaiai/iau011; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Xia KL, 2014, INT J NUMER METH BIO, V30, P814, DOI 10.1002/cnm.2655	56	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004055
C	Liang, SY; Sun, RY; Lee, JD; Srikant, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liang, Shiyu; Sun, Ruoyu; Lee, Jason D.; Srikant, R.			Adding One Neuron Can Eliminate All Bad Local Minima	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					One of the main difficulties in analyzing neural networks is the non-convexity of the loss function which may have many bad local minima. In this paper, we study the landscape of neural networks for binary classification tasks. Under mild assumptions, we prove that after adding one special neuron with a skip connection to the output, or one special neuron per layer, every local minimum is a global minimum.	[Liang, Shiyu; Srikant, R.] Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Champaign, IL 61820 USA; [Sun, Ruoyu] Univ Illinois, Dept ISE, Coordinated Sci Lab, Champaign, IL USA; [Lee, Jason D.] Univ Southern Calif, Marshall Sch Business, Los Angeles, CA 90089 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; University of Southern California	Liang, SY (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Coordinated Sci Lab, Champaign, IL 61820 USA.	sliang26@illinois.edu; ruoyus@illinois.edu; jasonlee@marshall.usc.edu; rsrikant@illinois.edu		Lee, Jason/0000-0003-0064-7800	USDA/NSF CPS grant [AG 2018-67007-2837]; NSF [NeTS 1718203, CPS ECCS 1739189, CCF 1755847]; DTRA Grant [HDTRA1-15-1-0003]; Dept. of ISE, University of Illinois Urbana-Champaign	USDA/NSF CPS grant; NSF(National Science Foundation (NSF)); DTRA Grant; Dept. of ISE, University of Illinois Urbana-Champaign	Research is supported by the following grants: USDA/NSF CPS grant AG 2018-67007-2837, NSF NeTS 1718203, NSF CPS ECCS 1739189, DTRA Grant DTRA grant HDTRA1-15-1-0003, NSF CCF 1755847 and a start-up grant from Dept. of ISE, University of Illinois Urbana-Champaign.	Andoni A, 2014, PR MACH LEARN RES, V32, P1908; [Anonymous], 2017, ARXIV170906010; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Brutzkus A, 2017, PR MACH LEARN RES, V70; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Cortes C., 1995, MACH LEARN, P273, DOI [10.1023/A:1022627411411, DOI 10.1007/BF00994018]; Du SS, 2018, PR MACH LEARN RES, V80; Du Simon S., 2018, INT C LEARN REPR; Freeman C.D., 2016, ARXIV161101540; Gautier A, 2016, ADV NEUR IN, V29; GE R., 2018, P 6 INT C LEARN REPR; Goodfellow I. J., 2013, ARXIV13024389; Haeffele B., 2014, ICML; Haeffele B. D., 2015, ARXIV PREPRINT ARXIV; HARDT M., 2017, P 5 INT C LEARN REPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Janzamin M., 2015, ARXIV150608473; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liang S., 2018, UNDERSTANDING LOSS S; Livni R, 2014, ADV NEUR IN, V27; Mei S., 2018, ARXIV180406561; Nguyen Quynh, 2017, ARXIV171010928; Safran Itay, 2018, ICML; SEDGHI H., 2014, ARXIV14122693; Shamir O., 2018, ARXIV180406739; SOLTANOLKOTABI M., 2017, ARXIV170504591; Soudry D., 2016, ABS160508361 CORR; Soudry Daniel, 2017, ARXIV170205777; Wan L., 2013, P INT C MACHINE LEAR, P1058; YuN C., 2017, ARXIV170702444; Zhang Chiyuan, 2016, ARXIV161103530; Zhang X., 2012, SIAM J MATRIX ANAL A; Zhong K, 2017, PR MACH LEARN RES, V70	37	12	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304037
C	Munteanu, A; Schwiegelshohn, C; Sohler, C; Woodruff, DP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Munteanu, Alexander; Schwiegelshohn, Chris; Sohler, Christian; Woodruff, David P.			On Coresets for Logistic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Coresets are one of the central methods to facilitate the analysis of large data. We continue a recent line of research applying the theory of coresets to logistic regression. First, we show the negative result that no strongly sublinear sized coresets exist for logistic regression. To deal with intractable worst-case instances we introduce a complexity measure mu(X), which quantifies the hardness of compressing a data set for logistic regression. mu(X) has an intuitive statistical interpretation that may be of independent interest. For data sets with bounded mu(X)-complexity, we show that a novel sensitivity sampling scheme produces the first provably sublinear (1 +/- epsilon)-coreset. We illustrate the performance of our method by comparing to uniform sampling as well as to state of the art methods in the area. The experiments are conducted on real world benchmark data for logistic regression.	[Munteanu, Alexander; Sohler, Christian] TU Dortmund Univ, Dept Comp Sci, D-44227 Dortmund, Germany; [Schwiegelshohn, Chris] Sapienza Univ Rome, Dept Comp Sci, I-00185 Rome, Italy; [Woodruff, David P.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Dortmund University of Technology; Sapienza University Rome; Carnegie Mellon University	Munteanu, A (corresponding author), TU Dortmund Univ, Dept Comp Sci, D-44227 Dortmund, Germany.	alexander.munteanu@tu-dortmund.de; schwiegelshohn@diag.uniroma1.it; christian.sohler@tu-dortmund.de; dwoodruf@cs.cmu.edu			German Science Foundation (DFG) Collaborative Research Center [SFB 876]; ERC [788893 AMDROMA]; Office of Naval Research (ONR) [N00014-18-1-2562]	German Science Foundation (DFG) Collaborative Research Center(German Research Foundation (DFG)); ERC(European Research Council (ERC)European Commission); Office of Naval Research (ONR)(Office of Naval Research)	We thank the anonymous reviewers for their valuable comments. We also thank our assistant Moritz Paweletz. This work was supported by the German Science Foundation (DFG) Collaborative Research Center SFB 876, projects A2 and C4. Chris Schwiegelshohn is supported in part by an ERC Advanced Grant 788893 AMDROMA. David P. Woodruff is supported in part by an Office of Naval Research (ONR) grant N00014-18-1-2562, and part of this work was done while he was visiting the Simons Institute for the Theory of Computing.	Agarwal PK, 2004, J ACM, V51, P606, DOI 10.1145/1008731.1008736; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; [Anonymous], 1994, INTRO COMPUTATIONAL; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bachem O, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1119, DOI 10.1145/3219819.3219973; Balcan M.-F., 2015, DAGSTUHL REPORTS, V4, P30; Barger A., 2016, P 2016 SIAM INT C DA; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Braverman Vladimir, 2016, ABS161200889 CORR; CHAO MT, 1982, BIOMETRIKA, V69, P653; Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482; Clarkson KL, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P257; Clarkson KL, 2016, SIAM J COMPUT, V45, P763, DOI 10.1137/140963698; Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson Kenneth L, 2015, P 26 ANN ACM SIAM S, P921; Cohen MB, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P181, DOI 10.1145/2688073.2688113; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Dasgupta A, 2009, SIAM J COMPUT, V38, P2060, DOI 10.1137/070696507; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Feldman D., 2011, ADV NEURAL INFORM PR, V24, P2142; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Feldman D, 2011, ACM S THEORY COMPUT, P569; Geppert LN, 2017, STAT COMPUT, V27, P79, DOI 10.1007/s11222-015-9608-z; Golub G. H., 2013, MATRIX COMPUTATIONS; Heinze G, 2002, STAT MED, V21, P2409, DOI 10.1002/sim.1047; Huggins Jonathan, 2016, P ADV NEUR INF PROC, P4080; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kremer I, 1999, COMPUT COMPLEX, V8, P21, DOI 10.1007/s000370050018; Langberg M, 2010, PROC APPL MATH, V135, P598; Li M, 2013, ANN IEEE SYMP FOUND, P127, DOI 10.1109/FOCS.2013.22; Lucic M, 2016, JMLR WORKSH CONF PRO, V51, P1; MEHTA CR, 1995, STAT MED, V14, P2143, DOI 10.1002/sim.4780141908; Molina A, 2018, P 32 AAAI C ART INT; Musco C, 2017, ADV NEUR IN, V30; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; Reddi SJ, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P752; Roughgarden T., 2017, HIGHL ALG C HALG 201; Sohler C, 2011, ACM S THEORY COMPUT, P755; Tolochinsky E., 2018, ARXIV180207382; Woodruff D., 2013, C LEARN THEOR, P546; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	44	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001013
C	Nair, A; Pong, V; Dalal, M; Bahl, S; Lin, S; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nair, Ashvin; Pong, Vitchyr; Dalal, Murtaza; Bahl, Shikhar; Lin, Steven; Levine, Sergey			Visual Reinforcement Learning with Imagined Goals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					For an autonomous agent to fulfill a wide range of user-specified goals at test time, it must be able to learn broadly applicable and general-purpose skill repertoires. Furthermore, to provide the requisite level of generality, these skills must handle raw sensory input such as images. In this paper, we propose an algorithm that acquires such general-purpose skills by combining unsupervised representation learning and reinforcement learning of goal-conditioned policies. Since the particular goals that might be required at test-time are not known in advance, the agent performs a self-supervised "practice" phase where it imagines goals and attempts to achieve them. We learn a visual representation with three distinct purposes: sampling goals for self-supervised practice, providing a structured transformation of raw sensory inputs, and computing a reward signal for goal reaching. We also propose a retroactive goal relabeling scheme to further improve the sample-efficiency of our method. Our off-policy algorithm is efficient enough to learn policies that operate on raw image observations and goals for a real-world robotic system, and substantially outperforms prior techniques.	[Nair, Ashvin; Pong, Vitchyr; Dalal, Murtaza; Bahl, Shikhar; Lin, Steven; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Nair, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	anair17@berkeley.edu; vitchyr@berkeley.edu; mdalal@berkeley.edu; shikharbahl@berkeley.edu; stevenlin598@berkeley.edu; svlevine@berkeley.edu			National Science Foundation [IIS-1651843, IIS-1614653]; Huawei Fellowship; Berkeley DeepDrive; Siemens; NVIDIA	National Science Foundation(National Science Foundation (NSF)); Huawei Fellowship(Huawei Technologies); Berkeley DeepDrive; Siemens(Siemens AG); NVIDIA	We would like to thank Aravind Srinivas and Pulkit Agrawal for useful discussions, and Alex Lee for helpful feedback on an initial draft of the paper. We would also like to thank Carlos Florensa for making multiple useful suggestions in later version of the draft. This work was supported by the National Science Foundation IIS-1651843 and IIS-1614653, a Huawei Fellowship, Berkeley DeepDrive, Siemens, and support from NVIDIA.	Agrawal P., 2016, ADV NEURAL INFORM PR; Andrychowicz M., 2017, NIPS; Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008; Bellemare M., 2016, NEURIPS; Chen X, 2016, ADV NEUR IN, V29; Cheung B., 2014, P INT C LEARN REPR W; Desjardins G., 2012, CORR; Ebert F, 2017, C ROB LEARN CORL; EYSENBACH B, 2018, 6 INT C LEARN REPR I, P1; Finn C., 2016, ADV NEURAL INFORM PR; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Florensa Carlos, 2017, ARXIV170403012; Fujimoto S., 2018, ARXIV180209477; Ha D., 2018, ARXIV180310122; Higgins Irina, 2017, ICLR 2017 INT C LEAR; Higgins Irina, 2017, INT C MACH LEARN ICM; Jonschkowski R, 2017, ARXIV170509805; KAELBLING LP, 1993, IJCAI-93, VOLS 1 AND 2, P1094; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Lange Sascha, 2010, EUR S ART NEUR NETW	20	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003072
C	Perrone, V; Jenatton, R; Seeger, M; Archambeau, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Perrone, Valerio; Jenatton, Rodolphe; Seeger, Matthias; Archambeau, Cedric			Scalable Hyperparameter Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Bayesian optimization (BO) is a model-based approach for gradient-free black-box function optimization, such as hyperparameter optimization. Typically, BO relies on conventional Gaussian process (GP) regression, whose algorithmic complexity is cubic in the number of evaluations. As a result, GP-based BO cannot leverage large numbers of past function evaluations, for example, to warm-start related BO runs. We propose a multi-task adaptive Bayesian linear regression model for transfer learning in BO, whose complexity is linear in the function evaluations: one Bayesian linear regression model is associated to each black-box function optimization problem (or task), while transfer learning is achieved by coupling the models through a shared deep neural net. Experiments show that the neural net learns a representation suitable for warm-starting the black-box optimization problems and that BO runs can be accelerated when the target black-box function (e.g., validation loss) is learned together with other related signals (e.g., training loss). The proposed method was found to be at least one order of magnitude faster than competing methods recently published in the literature.	[Perrone, Valerio; Jenatton, Rodolphe; Seeger, Matthias; Archambeau, Cedric] Amazon, Berlin, Germany	Amazon.com	Perrone, V (corresponding author), Amazon, Berlin, Germany.	vperrone@amazon.com; jenatton@amazon.com; matthis@amazon.com; cedrica@amazon.com						Bakker B, 2003, J MACHINE LEARNING R, V4, P83; Bardenet Remi, 2013, INT C MACHINE LEARNI, P199; Bishop CM, 2006, PATTERN RECOGNITION; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Dheeru D., 2019, UCI MACHINE LEARNING; Eggensperger K, 2015, AAAI CONF ARTIF INTE, P1114; Feurer M, 2015, AAAI CONF ARTIF INTE, P1128; Feurer Matthias, 2018, TECHNICAL REPORT; Gelbart Michael A, 2014, TECHNICAL REPORT; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; Hernandez-Lobato Jose Miguel, 2017, P INT C MACH LEARN I; Jenatton R, 2017, PR MACH LEARN RES, V70; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kingma D., 2014, INT C REPR LEARN; Kingma Diederik P, 2014, TECHNICAL REPORT; Levesque Julien-Charles, 2017, INT JOINT C NEUR NET; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; McIntire M., 2016, P UAI; Mohammad RM, 2012, INT CONF INTERNET, P492; Neal RM, 1996, LECT NOTES STAT, V118; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Poloczek M, 2017, ADV NEUR IN, V30; Poloczek M, 2016, WINT SIMUL C PROC, P770, DOI 10.1109/WSC.2016.7822140; Quilan J. R., 1987, INT J MAN MACHINE ST; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rasmussen C., 2005, INT C MACH LEARN, V22; Rezende D., 2014, INT C MACH LEARN, V31; Schilling N, 2015, LECT NOTES ARTIF INT, V9285, P87, DOI 10.1007/978-3-319-23525-7_6; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Seeger Matthias, 2017, TECHNICAL REPORT; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Trecate G. F., 1999, NEURAL INFORM PROCES, V11; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; Wilson A.G., 2015, ARXIV151101870; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wistuba M., 2016, PROC JOINT EUR C MAC, P199; Wistuba M., 2015, INT C DAT SCI ADV AN, P1; Yogatama D, 2014, JMLR WORKSH CONF PRO, V33, P1077	47	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001039
C	Reddy, S; Dragan, AD; Levine, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey			Where Do You Think You're Going?: Inferring Beliefs about Dynamics from Behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS	Inferring intent from observed behavior has been studied extensively within the frameworks of Bayesian inverse planning and inverse reinforcement learning. These methods infer a goal or reward function that best explains the actions of the observed agent, typically a human demonstrator. Another agent can use this inferred intent to predict, imitate, or assist the human user. However, a central assumption in inverse reinforcement learning is that the demonstrator is close to optimal. While models of suboptimal behavior exist, they typically assume that suboptimal actions are the result of some type of random noise or a known cognitive bias, like temporal inconsistency. In this paper, we take an alternative approach, and model suboptimal behavior as the result of internal model misspecification: the reason that user actions might deviate from near-optimal actions is that the user has an incorrect set of beliefs about the rules - the dynamics - governing how actions affect the environment. Our insight is that while demonstrated actions may be suboptimal in the real world, they may actually be near-optimal with respect to the user's internal model of the dynamics. By estimating these internal beliefs from observed behavior, we arrive at a new method for inferring intent. We demonstrate in simulation and in a user study with 12 participants that this approach enables us to more accurately model human intent, and can be used in a variety of applications, including offering assistance in a shared autonomy framework and inferring human preferences.	[Reddy, Siddharth; Dragan, Anca D.; Levine, Sergey] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Reddy, S (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	sgr@berkeley.edu; anca@berkeley.edu; svlevine@berkeley.edu		Dragan, Anca/0000-0001-6312-5466	Berkeley EECS Department Fellowship; Berkeley DeepDrive	Berkeley EECS Department Fellowship; Berkeley DeepDrive	We would like to thank Oleg Klimov for open-sourcing his implementation of the Lunar Lander game, which was originally developed by Atari in 1979, and inspired by the lunar modules built in the 1960s and 70s for the Apollo space program. We would also like to thank Eliezer Yudkowsky for the fanfiction novel, Harry Potter and the Methods of Rationality - Harry's misadventure with the rocket-assisted broomstick in chapter 59 inspired us to try to close the gap between intuitive physics and the real world. This work was supported in part by a Berkeley EECS Department Fellowship for first-year Ph.D. students, Berkeley DeepDrive, computational resource donations from Amazon, NSF IIS-1700696, and AFOSR FA9550-17-1-0308.	AKERLOF GA, 1991, AM ECON REV, V81, P1; Armstrong  Stuart, 2017, ARXIV171205812; Baker CL, 2009, COGNITION, V113, P329, DOI 10.1016/j.cognition.2009.07.005; Bergen  Leon, 2010, P ANN M COGN SCI SOC, V32; Bertsekas D. P., 2014, CONSTRAINED OPTIMIZA; Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156; Botvinick Matthew, 2009, Adv Neural Inf Process Syst, V21, P169; Broad  Alexander, 2017, ROB SCI SYST P; Brockman Greg, 2016, PROBABILISTIC RANDOM; Calafiore G., 2006, PROBABILISTIC RANDOM; CARAMAZZA A, 1981, COGNITION, V9, P117, DOI 10.1016/0010-0277(81)90007-X; Carmena JM, 2013, PLOS BIOL, V11, DOI 10.1371/journal.pbio.1001561; Cutler M, 2015, IEEE INT CONF ROBOT, P2605, DOI 10.1109/ICRA.2015.7139550; Desmurget M, 2000, TRENDS COGN SCI, V4, P423, DOI 10.1016/S1364-6613(00)01537-0; Eridovich-Keil David, 2017, ARXIV171004731; Evans O., 2015, NIPS WORKSHOP BOUNDE, V6; Evans O, 2016, AAAI CONF ARTIF INTE, P323; Finn C, 2016, PR MACH LEARN RES, V48; Fodor Jerry A, 1992, COGNITION; Fu J., 2017, ARXIV171011248; Gerstenberg T., 2017, OXFORD HDB CAUSAL RE, P515; Golub M., 2013, P INT C MACH LEARN, P606; Gopnik A., 1994, MAPPING MIND DOMAIN, P257, DOI DOI 10.1017/CBO9780511752902.011; Griffiths TL, 2015, TOP COGN SCI, V7, P217, DOI 10.1111/tops.12142; Haarnoja T., 2017, ARXIV170208165; Hamrick J., 2011, P 33 ANN C COGN SCI; Herbert S. L., 2017, ARXIV170307373; Herman M, 2016, JMLR WORKSH CONF PRO, V51, P102; Javdani S., 2015, ARXIV150307619; Kawato M, 1999, CURR OPIN NEUROBIOL, V9, P718, DOI 10.1016/S0959-4388(99)00028-8; King DB, 2015, ACS SYM SER, V1214, P1; Kleinberg J., 2014, P 15 ACM C EC COMPUT, P547; Majumdar A., 2017, ROBOTICS SCI SYSTEMS; Mehta B, 2002, J NEUROPHYSIOL, V88, P942, DOI 10.1152/jn.2002.88.2.942; Mueller KD, 2018, ARCH CLIN NEUROPSYCH, V33, P993, DOI 10.1093/arclin/acx116; Neu G., 2012, ARXIV12065264; Peng X. B., 2018, ARXIV180402717; PREMACK D, 1978, BEHAV BRAIN SCI, V1, P515, DOI 10.1017/S0140525X00076512; PROFFITT DR, 1989, J EXP PSYCHOL HUMAN, V15, P384, DOI 10.1037/0096-1523.15.2.384; Rafferty A.N., 2016, EDM, V16, P472; Rafferty AN, 2015, COGNITIVE SCI, V39, P584, DOI 10.1111/cogs.12157; Rafferty Anna N, DIAGNOSING ALGEBRA U; Ramachandran D., 2007, URBANA, V51, P61801; Reddy S., 2018, ARXIV180201744; Schwarting Wilko, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1928, DOI 10.1109/ICRA.2017.7989224; Shenoy KV, 2014, NEURON, V84, P665, DOI 10.1016/j.neuron.2014.08.038; Simon HA, 1991, ORGAN SCI, V2, P125, DOI 10.1287/orsc.2.1.125; Todorov E, 2004, NAT NEUROSCI, V7, P907, DOI 10.1038/nn1309; TVERSKY A, 1974, SCIENCE, V185, P1124, DOI 10.1126/science.185.4157.1124; Uchibe  Eiji, 2017, NEURAL PROCESS LETT, P1; Wilkening F., 2010, WILEY BLACKWELL HDB, P473, DOI DOI 10.1002/9781444325485.CH18; WOLPERT DM, 1995, SCIENCE, V269, P1880, DOI 10.1126/science.7569931; Wulfmeier M., 2015, ARXIV150704888; Ziebart B. D., 2010, P 27 INT C INT C MAC, P1255; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart BD, 2009, 2009 IEEE-RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, P3931, DOI 10.1109/IROS.2009.5354147	57	12	12	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301044
C	Xu, HT; Wang, WL; Liu, W; Carin, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Hongteng; Wang, Wenlin; Liu, Wei; Carin, Lawrence			Distilled Wasserstein Learning for Word Embedding and Topic Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a novel Wasserstein method with a distillation mechanism, yielding joint learning of word embeddings and topics. The proposed method is based on the fact that the Euclidean distance between word embeddings may be employed as the underlying distance in the Wasserstein topic model. The word distributions of topics, their optimal transports to the word distributions of documents, and the embeddings of words are learned in a unified framework. When learning the topic model, we leverage a distilled underlying distance matrix to update the topic distributions and smoothly calculate the corresponding optimal transports. Such a strategy provides the updating of word embeddings with robust guidance, improving the algorithmic convergence. As an application, we focus on patient admission records, in which the proposed method embeds the codes of diseases and procedures and learns the topics of admissions, obtaining superior performance on clinically-meaningful disease network construction, mortality prediction as a function of admission codes, and procedure recommendation.	[Xu, Hongteng] Infinia ML Inc, Durham, NC 27705 USA; [Xu, Hongteng; Wang, Wenlin; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA; [Liu, Wei] Tencent AI Lab, Bellevue, WA USA	Duke University	Xu, HT (corresponding author), Infinia ML Inc, Durham, NC 27705 USA.; Xu, HT (corresponding author), Duke Univ, Durham, NC 27706 USA.	hongteng.xu@infiniaml.com	Xu, Hongteng/AAB-1636-2021	Liu, Wei/0000-0002-3865-8145; Carin, Lawrence/0000-0001-6277-7948	DARPA; DOE; NIH; ONR; NSF	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by DARPA, DOE, NIH, ONR and NSF. Morgan A. Schmitz kindly helped us by sharing his Wasserstein dictionary learning code. We also thank Prof. Hongyuan Zha at Georgia Institute of Technology for helpful discussions.	Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Arjovsky M., 2017, ARXIV170107875; Bajor J. M., 2018, ARXIV180204233; Baumel T., 2018, WORKSH 32 AAAI C ART; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boissard E, 2015, BERNOULLI, V21, P740, DOI 10.3150/13-BEJ585; Che Zhengping, 2015, ARXIV151203542; Choi  E., 2016, KDD; Courty  Nicolas, 2017, ARXIV171007457; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Das R, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P795; Ge nevay A., 2017, ARXIV170600292; Gerard S., 1983, INTRO MODERN INFORM; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; Harutyunyan H., 2017, ARXIV170307771; Hinton G., 2015, ARXIV150302531; Huang G., 2016, PROC NEURAL INF PROC, P4869; Inan H., 2016, ARXIV161101462; Joachims  T., 2002, LEARNING CLASSIFY TE, V186; Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Liu Y, 2015, AAAI CONF ARTIF INTE, P2418; Lopez-Paz D., 2015, ARXIV151103643; Mikolov T., 2013, ARXIV; Mullenbach J., 2018, P NAACL HLT, P1101; Muromagi  A., 2017, P 21 NORDIC C COMPUT; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Pereyra Gabriel, 2017, ARXIV170106548; Le Q, 2014, PR MACH LEARN RES, V32, P1188; Rusu A. A., 2016, ARXIV160604671; Schmitz MA, 2018, SIAM J IMAGING SCI, V11, P643, DOI 10.1137/17M1140431; Shen DH, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P440; Shi B, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P375, DOI 10.1145/3077136.3080806; Shi H., 2017, ARXIV PREPRINT ARXIV; Sy L.W., 2018, ARXIV180202311; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang  W., 2017, ARXIV171209783; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647; Zemel  Y., 2017, ARXIV170106876	44	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301068
C	Djolonga, J; Krause, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Djolonga, Josip; Krause, Andreas			Differentiable Learning of Submodular Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SENSITIVITY-ANALYSIS; OPTIMIZATION; ALGORITHMS	Can we incorporate discrete optimization algorithms within modern machine learning models? For example, is it possible to incorporate in deep architectures a layer whose output is the minimal cut of a parametrized graph? Given that these models are trained end-to-end by leveraging gradient information, the introduction of such layers seems very challenging due to their non-continuous output. In this paper we focus on the problem of submodular minimization, for which we show that such layers are indeed possible. The key idea is that we can continuously relax the output without sacrificing guarantees. We provide an easily computable approximation to the Jacobian complemented with a complete theoretical analysis. Finally, these contributions let us experimentally learn probabilistic log-supermodular models via a bi-level variational inference formulation.	[Djolonga, Josip; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Djolonga, J (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	josipd@inf.ethz.ch; krausea@ethz.ch	Jeong, Yongwook/N-7413-2016	Krause, Andreas/0000-0001-7260-9673	ERC [StG 307036]; Google European PhD Fellowship	ERC(European Research Council (ERC)European Commission); Google European PhD Fellowship(Google Incorporated)	The research was partially supported by ERC StG 307036 and a Google European PhD Fellowship.	Amos B., 2017, INT C MACH LEARN ICM; Ba J., 2017, P 3 INT C LEARN REPR; Bach F., 2013, FOUND TRENDS MACH LE, V6, P2; Bach F., 2010, ADV NEURAL INFORM PR; Barbero A lvaro, 2014, MODULAR PROXIMAL OPT; BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873; BOOT JCG, 1963, OPER RES, V11, P771, DOI 10.1287/opre.11.5.771; Borenstein E, 2008, IEEE T PATTERN ANAL, V30, P2109, DOI 10.1109/TPAMI.2007.70840; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; CHAKRAVARTI N, 1993, DISCRETE APPL MATH, V45, P183, DOI 10.1016/0166-218X(93)90008-C; DJOLONGA J., 2015, INT C MACH LEARN ICM; Djolonga  J., 2014, ADV NEURAL INFORM PR; Dolhansky B., 2016, NEURAL INFORM PROCES; Domke J, 2013, IEEE T PATTERN ANAL, V35, P2454, DOI 10.1109/TPAMI.2013.31; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Edmonds Jack, 1970, COMBINATORIAL STRUCT, P69; FUJISHIGE S, 2006, MINIMUM NORM POINT A; Groenevelt H., 1991, EUROPEAN J OPERATION, V54; Kohli P., 2008, COMPUTER VISION PATT; Lovasz L., 1983, MATH PROGRAMMING STA, P235, DOI DOI 10.1007/978-3-642-68874-4_10; Martins A., 2016, INT C MACH LEARN ICM; Moulines E., 2011, ADV NEURAL INF PROCE, V24; Narasimhan M., 2006, ADV NEURAL INFORM PR, P979; Niculae V., 2017, ARXIV170507704; Ochs Peter, 2015, Scale Space and Variational Methods in Computer Vision. 5th International Conference, SSVM 2015. Proceedings: LNCS 9087, P654, DOI 10.1007/978-3-319-18461-6_52; Queyranne M, 1998, MATH PROGRAM, V82, P3, DOI 10.1007/BF01585863; Robertson T., 1988, TECH REP; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; SHAPIRO A, 1988, SIAM J CONTROL OPTIM, V26, P628, DOI 10.1137/0326037; Suehiro Daiki, 2012, INT C ALG LEARN THEO; Tappen M. F., 2007, COMPUTER VISION PATT; Taskar B, 2004, ADV NEUR IN, V16, P25; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Wainwright M. J., 2006, J MACHINE LEARNING R, V7; Yao-Liang Y., 2013, ADV NEURAL INFORM PR, V26, P91; [No title captured]	39	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401006
C	Dong, K; Eriksson, D; Nickisch, H; Bindel, D; Wilson, AG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dong, Kun; Eriksson, David; Nickisch, Hannes; Bindel, David; Wilson, Andrew Gordon			Scalable Log Determinants for Gaussian Process Kernel Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					For applications as varied as Bayesian neural networks, determinantal point processes, elliptical graphical models, and kernel learning for Gaussian processes (GPs), one must compute a log determinant of an n x n positive definite matrix, and its derivatives - leading to prohibitive O(n(3)) computations. We propose novel O(n) approaches to estimating these quantities from only fast matrix vector multiplications (MVMs). These stochastic approximations are based on Chebyshev, Lanczos, and surrogate models, and converge quickly even for kernel matrices that have challenging spectra. We leverage these approximations to develop a scalable Gaussian process approach to kernel learning. We find that Lanczos is generally superior to Chebyshev for kernel learning, and that a surrogate approach can be highly efficient and accurate with popular kernels.	[Dong, Kun; Eriksson, David; Bindel, David; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14850 USA; [Nickisch, Hannes] Phillips Res Hamburg, Hamburg, Germany	Cornell University	Dong, K (corresponding author), Cornell Univ, Ithaca, NY 14850 USA.		Nickisch, Hannes/I-7049-2017; Jeong, Yongwook/N-7413-2016	Nickisch, Hannes/0000-0003-1604-6647; Eriksson, David/0000-0002-3143-0922				Avron H, 2011, J ACM, V58, DOI 10.1145/1944345.1944349; Bai Zhaojun, 1998, TECHNICAL REPORT; Boutsidis C., 2015, ARXIV150300374; Buhmann M. D., 2000, Acta Numerica, V9, P1, DOI 10.1017/S0962492900000015; Cullum J., 2002, LANCZOS ALGORITHMS L, DOI [10.1137/1.9780898719192, DOI 10.1137/1.9780898719192]; Fasshauer G. E., 2007, MESHFREE APPROXIMATI, V6; FIEDLER M, 1984, LINEAR ALGEBRA APPL, V58, P75, DOI 10.1016/0024-3795(84)90205-2; Gil A, 2007, NUMERICAL METHODS FOR SPECIAL FUNCTIONS, P1, DOI 10.1137/1.9780898717822; Golub GH, 2010, PRINC SER APPL MATH, P1; Han I, 2015, PR MACH LEARN RES, V37, P908; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Herlands W, 2016, JMLR WORKSH CONF PRO, V51, P1013; Higham NJ, 2008, FUNCTIONS MATRICES T; HUTCHINSON MF, 1990, COMMUN STAT SIMULAT, V19, P433, DOI 10.1080/03610919008812866; Joaquin QC, 2007, LARGE SCALE KERNEL M, P203, DOI DOI 10.7551/MITPRESS/7496.003.0011; Le Quoc V., 2013, INT C MACH LEARN, P244; MacKay D, 1997, NEURAL COMPUT; Mackay D.J.C., 1992, THESIS CALIFORNIA I; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen CE, 2010, J MACH LEARN RES, V11, P3011; Rasmussen CE, 2001, ADV NEUR IN, V13, P294; RUE H., 2005, GAUSSIAN MARKOV RAND; Saad Y, 1992, NUMERICAL METHODS LA; Schaback R, 2006, ACT NUMERIC, V15, P543, DOI 10.1017/S0962492906270016; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; SILVERMAN BW, 1985, J R STAT SOC B, V47, P1; Snelson E., 2006, ADV NEURAL INFORM PR, V18, P1259; Stein ML, 2013, ANN APPL STAT, V7, P1162, DOI 10.1214/13-AOAS627; Ubaru Shashanka, FAST ESTIMATION TR F; Wendland H., 2004, SCATTERED DATA APPRO, V17; Weyl H, 1912, MATH ANN, V71, P441, DOI 10.1007/BF01456804; Wilson A., 2014, P ADV NEUR INF PROC, V4, P3626; Wilson A.G., 2014, THESIS U CAMBRIDGE C; Wilson AG, 2016, ADV NEUR IN, V29; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	39	12	12	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406039
C	Guo, ZD; Thomas, PS; Brunskill, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Guo, Zhaohan Daniel; Thomas, Philip S.; Brunskill, Emma			Using Options and Covariance Testing for Long Horizon Off-Policy Policy Evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Evaluating a policy by deploying it in the real world can be risky and costly. Off-policy policy evaluation (OPE) algorithms use historical data collected from running a previous policy to evaluate a new policy, which provides a means for evaluating a policy without requiring it to ever be deployed. Importance sampling is a popular OPE method because it is robust to partial observability and works with continuous states and actions. However, the amount of historical data required by importance sampling can scale exponentially with the horizon of the problem: the number of sequential decisions that are made. We propose using policies over temporally extended actions, called options, and show that combining these policies with importance sampling can significantly improve performance for long-horizon problems. In addition, we can take advantage of special cases that arise due to options-based policies to further improve the performance of importance sampling. We further generalize these special cases to a general covariance testing rule that can be used to decide which weights to drop in an IS estimate, and derive a new IS algorithm called Incremental Importance Sampling that can provide significantly more accurate estimates for a broad class of domains.	[Guo, Zhaohan Daniel] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Thomas, Philip S.] Univ Massachusetts, Amherst, MA 01003 USA; [Brunskill, Emma] Stanford Univ, Stanford, CA 94305 USA	Carnegie Mellon University; University of Massachusetts System; University of Massachusetts Amherst; Stanford University	Guo, ZD (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zguo@cs.cmu.edu; pthomas@cs.umass.edu; ebrun@cs.stanford.edu	Jeong, Yongwook/N-7413-2016	Guo, Zhaohan/0000-0002-2497-6441	ONR Young Investigator award; NSF CAREER award; Institute of Education Sciences, U.S. Department of Education	ONR Young Investigator award; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Institute of Education Sciences, U.S. Department of Education(US Department of Education)	The research reported here was supported in part by an ONR Young Investigator award, an NSF CAREER award, and by the Institute of Education Sciences, U.S. Department of Education. The opinions expressed are those of the authors and do not represent views of NSF, IES or the U.S. Dept. of Education.	Bastani M., 2014, THESIS; Brunskill E, 2014, PR MACH LEARN RES, V32, P316; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Hallak A., 2015, ARXIV150905172; Hallak A, 2015, PR MACH LEARN RES, V37, P711; Jiang N, 2016, PR MACH LEARN RES, V48; Li LH, 2015, JMLR WORKSH CONF PRO, V38, P608; Man- nor Shie, 2013, RLDM, P9; Mankowitz DJ, 2014, PR MACH LEARN RES, V32, P1350; Precup D., 2001, P 18 INT C MACH LEAR, P417; Precup D., 2000, P 17 INT C MACH LEAR; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Theocharous G, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1806; Thomas PS, 2017, AAAI CONF ARTIF INTE, P2646; Thomas PS, 2015, AAAI CONF ARTIF INTE, P3000; Thomas Philip S, 2015, ADV NEURAL INFORM PR, P334	19	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402053
C	Ligett, K; Neel, S; Roth, A; Waggoner, B; Wu, ZS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ligett, Katrina; Neel, Seth; Roth, Aaron; Waggoner, Bo; Wu, Zhiwei Steven			Accuracy First: Selecting a Differential Privacy Level for Accuracy-Constrained ERM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Traditional approaches to differential privacy assume a fixed privacy requirement epsilon for a computation, and attempt to maximize the accuracy of the computation subject to the privacy constraint. As differential privacy is increasingly deployed in practical settings, it may often be that there is instead a fixed accuracy requirement for a given computation and the data analyst would like to maximize the privacy of the computation subject to the accuracy constraint. This raises the question of how to find and run a maximally private empirical risk minimizer subject to a given accuracy requirement. We propose a general "noise reduction" framework that can apply to a variety of private empirical risk minimization (ERM) algorithms, using them to "search" the space of privacy levels to find the empirically strongest one that meets the accuracy constraint, and incurring only logarithmic overhead in the number of privacy levels searched. The privacy analysis of our algorithm leads naturally to a version of differential privacy where the privacy parameters are dependent on the data, which we term ex-post privacy, and which is related to the recently introduced notion of privacy odometers. We also give an ex-post privacy analysis of the classical AboveThreshold privacy tool, modifying it to allow for queries chosen depending on the database. Finally, we apply our approach to two common objective functions, regularized linear and logistic regression, and empirically compare our noise reduction methods to (i) inverting the theoretical utility guarantees of standard private ERM algorithms and (ii) a stronger, empirical baseline based on binary search.	[Ligett, Katrina] CALTECH, Pasadena, CA 91125 USA; [Ligett, Katrina] Hebrew Univ Jerusalem, Jerusalem, Israel; [Neel, Seth; Roth, Aaron; Waggoner, Bo] Univ Penn, Philadelphia, PA 19104 USA; [Wu, Zhiwei Steven] Microsoft Res, Redmond, WA USA	California Institute of Technology; Hebrew University of Jerusalem; University of Pennsylvania; Microsoft	Ligett, K (corresponding author), CALTECH, Pasadena, CA 91125 USA.; Ligett, K (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.		Jeong, Yongwook/N-7413-2016	Waggoner, Bo/0000-0002-1366-1065; Ligett, Katrina/0000-0003-2780-6656				[Anonymous], 1999, KDD 99 KDD CUP 1999; Bassily Raef, 2014, CORR; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Chaudhuri K., 2008, PROC 22 ANN C NEURAL, P289; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Duchi JC, 2013, ANN ALLERTON CONF, P1592, DOI 10.1109/Allerton.2013.6736718; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Fanti Giulia, 2016, POPETS, V3, P2016; Greenberg A, 2016, WIRED MAGAZINE; Jain P., 2012, COLT; Kifer D., 2012, COLT; Koufogiannis Fragkiskos, 2017, J PRIVACY CONFIDENTI, V7; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Rogers Ryan M, 2016, ADV NEURAL INFORM PR, P1921; Rubinstein B. I. P., 2009, CORR; Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35; The AMA Team at Laboratoire d'Informatique de Grenoble, 2017, BUZZ PRED ONL SOC ME; Williams O., 2010, P 23 INT C NEURAL IN, P2451	21	12	12	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402060
C	Liu, WW; Shen, XB; Tsang, IW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Weiwei; Shen, Xiaobo; Tsang, Ivor W.			Sparse Embedded k-Means Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DIMENSIONALITY REDUCTION	The k-means clustering algorithm is a ubiquitous tool in data mining and machine learning that shows promising performance. However, its high computational cost has hindered its applications in broad domains. Researchers have successfully addressed these obstacles with dimensionality reduction methods. Recently, [1] develop a state-of-the-art random projection (RP) method for faster k-means clustering. Their method delivers many improvements over other dimensionality reduction methods. For example, compared to the advanced singular value decomposition based feature extraction approach, [1] reduce the running time by a factor of min{n, d}epsilon(2)log(d)/k for data matrix X is an element of R-nxd with n data points and d features, while losing only a factor of one in approximation accuracy. Unfortunately, they still require O(ndk/epsilon(2)log(d)) for matrix multiplication and this cost will be prohibitive for large values of n and d. To break this bottleneck, we carefully build a sparse embedded 1-means clustering algorithm which requires O(nnz (X)) (nnz(X) denotes the number of non-zeros in X) for fast matrix multiplication. Moreover, our proposed algorithm improves on [1]'s results for approximation accuracy by a factor of one. Our empirical studies corroborate our theoretical findings, and demonstrate that our approach is able to significantly accelerate k-means clustering, while achieving satisfactory clustering performance.	[Liu, Weiwei] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia; [Shen, Xiaobo] Nanyang Technol Univ, Sch Comp Sci & Engn, Singapore, Singapore; [Liu, Weiwei; Tsang, Ivor W.] Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia	University of New South Wales Sydney; Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; University of Technology Sydney	Liu, WW (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia.; Liu, WW (corresponding author), Univ Technol Sydney, Ctr Artificial Intelligence, Sydney, NSW, Australia.	liuweiwei863@gmail.com; njust.shenxiaobo@gmail.com; ivor.tsang@uts.edu.au		Tsang, Ivor/0000-0001-8095-4637; Shen, Xiaobo/0000-0001-8655-1265	ARC Future Fellowship [FT130100746]; ARC [LP150100671, DP170101628, DP150102728, DP150103071]; NSFC [61232006, 61672235]	ARC Future Fellowship(Australian Research Council); ARC(Australian Research Council); NSFC(National Natural Science Foundation of China (NSFC))	We would like to thank the area chairs and reviewers for their valuable comments and constructive suggestions on our paper. This project is supported by the ARC Future Fellowship FT130100746, ARC grant LP150100671, DP170101628, DP150102728, DP150103071, NSFC 61232006 and NSFC 61672235.	Boutsidis C, 2015, IEEE T INFORM THEORY, V61, P1045, DOI 10.1109/TIT.2014.2375327; Cohen M. B., 2016, P 43 INT C AUT LANG, DOI DOI 10.4230/LIPICS.ICALP.2016.11; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Drineas, 2009, ADV NEURAL INFORM PR, P153, DOI DOI 10.1016/J.FUTURE.2017.11.043; Drineas P, 1999, PROCEEDINGS OF THE TENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P291; Fan JQ, 2009, J MACH LEARN RES, V10, P2013; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; FOLEY DH, 1975, IEEE T COMPUT, VC 24, P281, DOI 10.1109/T-C.1975.224208; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; He X., 2005, P ADV NEUR INF PROC, P507; Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902; Liu WW, 2017, J MACH LEARN RES, V18; Liu XW, 2017, AAAI CONF ARTIF INTE, P2259; Mirsky L., 1960, Q J MATH, V11, P50, DOI [10.1093/qmath/11.1.50, DOI 10.1093/QMATH/11.1.50]; Mitchell TM, 2004, MACH LEARN, V57, P145, DOI 10.1023/B:MACH.0000035475.85309.1b; Pourkamali-Anaraki F, 2017, IEEE T INFORM THEORY, V63, P2954, DOI 10.1109/TIT.2017.2672725; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sanchez J, 2013, INT J COMPUT VISION, V105, P222, DOI 10.1007/s11263-013-0636-x; Shen XB, 2017, AAAI CONF ARTIF INTE, P2527; Wang R, 2015, IEEE T CYBERNETICS, V45, P1108, DOI 10.1109/TCYB.2014.2341575; Zhai YT, 2014, IEEE COMPUT INTELL M, V9, P14, DOI 10.1109/MCI.2014.2326099	21	12	12	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403038
C	London, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		London, Ben			A PAC-Bayesian Analysis of Randomized Learning with Application to Stochastic Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STABILITY	We study the generalization error of randomized learning algorithms-focusing on stochastic gradient descent (SGD)-using a novel combination of PAC-Bayes and algorithmic stability. Importantly, our generalization bounds hold for all posterior distributions on an algorithm's random hyperparameters, including distributions that depend on the training data. This inspires an adaptive sampling algorithm for SGD that optimizes the posterior at runtime. We analyze this algorithm in the context of our generalization bounds and evaluate it on a benchmark dataset. Our experiments demonstrate that adaptive sampling can reduce empirical risk faster than uniform sampling while also improving out-of-sample accuracy.	[London, Ben] Amazon AI, Seattle, WA 98109 USA		London, B (corresponding author), Amazon AI, Seattle, WA 98109 USA.	blondon@amazon.com	Jeong, Yongwook/N-7413-2016					Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; CATONI O, 2007, I MATH STAT LECT NOT, V56; Collins M, 2008, J MACH LEARN RES, V9, P1775; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Elisseeff A, 2005, J MACH LEARN RES, V6, P55; Feng J., 2016, CORR; Freund Y., 1995, COMPUTATIONAL LEARNI, V904, P23, DOI [10.1007/3-540-59119-2_166, DOI 10.1007/3-540-59119-2_166]; Germain Pascal, 2009, INT C MACH LEARN; Hardt M, 2016, PR MACH LEARN RES, V48; Kontorovich A., 2014, INT C MACH LEARN; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kuzborskij I., 2017, CORR; Langford J., 2002, NEURAL INFORM PROCES; Lin J., 2016, NEURAL INFORM PROCES; Lin J, 2016, INT CONF EUR ENERG; London B, 2016, J MACH LEARN RES, V17; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; Rosasco L., 2015, NEURAL INFORM PROCES; Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386; Shalev-Shwartz S., 2014, CORR; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Wang YQ, 2016, BMC GENET, V17, DOI 10.1186/s12863-016-0364-7; Zhao PL, 2015, PR MACH LEARN RES, V37, P1	25	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402096
C	van Seijen, H; Fatemi, M; Romoff, J; Laroche, R; Barnes, T; Tsang, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		van Seijen, Harm; Fatemi, Mehdi; Romoff, Joshua; Laroche, Romain; Barnes, Tavian; Tsang, Jeffrey			Hybrid Reward Architecture for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					One of the main challenges in reinforcement learning (RL) is generalisation. In typical deep RL methods this is achieved by approximating the optimal value function with a low-dimensional representation using a deep network. While this approach works well in many domains, in domains where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable. This paper contributes towards tackling such challenging domains, by proposing a new method, called Hybrid Reward Architecture (HRA). HRA takes as input a decomposed reward function and learns a separate value function for each component reward function. Because each component typically only depends on a subset of all features, the corresponding value function can be approximated more easily by a low-dimensional representation, enabling more effective learning. We demonstrate HRA on a toy-problem and the Atari game Ms. Pac-Man, where HRA achieves above-human performance.	[van Seijen, Harm; Fatemi, Mehdi; Romoff, Joshua; Laroche, Romain; Barnes, Tavian; Tsang, Jeffrey] Microsoft Maluuba, Montreal, PQ, Canada; [Romoff, Joshua] McGill Univ, Montreal, PQ, Canada	McGill University	van Seijen, H (corresponding author), Microsoft Maluuba, Montreal, PQ, Canada.	harm.vanseijen@microsoft.com; mehdi.fatemi@microsoft.com; joshua.romoff@mail.mcgill.ca; romain.laroche@microsoft.com; tavian.barnes@microsoft.com; tsang.jeffrey@microsoft.com	Jeong, Yongwook/N-7413-2016					[Anonymous], 2017, 5 INT C LEARN REPR I; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bacon P.-L., 2017, P 31 AAAI C ART INT; Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Diuk C., 2008, P 25 INT C MACH LEAR; Fuster J., 2003, CORTEX MIND UNIFYING; Gluck M.A., 2013, LEARNING MEMORY BRAI; Kulkarni TD, 2016, ADV NEUR IN, V29; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A., 2015, DEEP LEARN WORKSH IC; Ng A. Y., 1999, P 16 INT C MACHINE L; Roijers Diederik M., 2013, J ARTIFICIAL INTELLI; Russell S., 2003, P 20 INT C MACH LEAR; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Silver D, 2015, P 32 INT C MACH LEAR; Sprague N., 2003, P 18 INT JOINT C ART, P1445; STOUT A, 2005, AAAI SPRING S DEV RO; Sutton R, 2011, P 10 INT C AUT AG MU; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Szepesvari, 2009, ALGORITHMS REINFORCE; van Seijen H, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P177; Vezhnevets A, 2016, ADV NEUR IN, V29; Wang Z, 2016, PR INT CONGR SOUND V	26	12	12	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405046
C	Abuzaid, F; Bradley, J; Liang, F; Feng, A; Yang, L; Zaharia, M; Talwalkar, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Abuzaid, Firas; Bradley, Joseph; Liang, Feynman; Feng, Andrew; Yang, Lee; Zaharia, Matei; Talwalkar, Ameet			Yggdrasil: An Optimized System for Training Deep Decision Trees at Scale	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep distributed decision trees and tree ensembles have grown in importance due to the need to model increasingly large datasets. However, PLANET, the standard distributed tree learning algorithm implemented in systems such as XGBOOST and Spark MLLIB, scales poorly as data dimensionality and tree depths grow. We present YGGDRASIL, a new distributed tree learning method that outperforms existing methods by up to 24x. Unlike PLANET, YGGDRASIL is based on vertical partitioning of the data (i.e., partitioning by feature), along with a set of optimized data structures to reduce the CPU and communication costs of training. YGGDRASIL (1) trains directly on compressed data for compressible features and labels; (2) introduces efficient data structures for training on uncompressed data; and (3) minimizes communication between nodes by using sparse bitvectors. Moreover, while PLANET approximates split points through feature binning, YGGDRASIL does not require binning, and we analytically characterize the impact of this approximation. We evaluate YGGDRASIL against the MNIST 8M dataset and a high-dimensional dataset at Yahoo; for both, YGGDRASIL is faster by up to an order of magnitude.	[Abuzaid, Firas; Zaharia, Matei] MIL CSAIL, Cambridge, MA 02139 USA; [Bradley, Joseph] Databricks, San Francisco, CA USA; [Liang, Feynman] Univ Cambridge, Cambridge, England; [Feng, Andrew; Yang, Lee] Yahoo, Sunnyvale, CA USA; [Talwalkar, Ameet] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	University of Cambridge; University of California System; University of California Los Angeles	Abuzaid, F (corresponding author), MIL CSAIL, Cambridge, MA 02139 USA.							Ben-Haim Y, 2010, J MACH LEARN RES, V11, P849; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Caragea Doina, 2004, Int J Hybrid Intell Syst, V1, P80; Chambi S., 2015, SOFTWARE PRACTICE EX; Chen T., 2016, P 22 ACM SIGKDD INT, P785; Cortes C., 2014, ICML; FAYYAD UM, 1992, MACH LEARN, V8, P87, DOI 10.1023/A:1022638503176; Lamb A, 2012, PROC VLDB ENDOW, V5, P1790, DOI 10.14778/2367502.2367518; Meng X., 2016, J MACH LEARN RES, V17, P1235, DOI DOI 10.1145/2882903.2912565; Panda B, 2009, INT C VER LARG DAT B; SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458; Stonebraker M., 2005, P 31 INT C VER LARG, P553; Svore K. M., 2011, SCALING MACHINE LEAR, P2; Ye J., 2009, P 18 ACM C INFORM KN, P2061, DOI [10.1145/1645953.1646301, DOI 10.1145/1645953.1646301]	15	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703002
C	Bachman, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bachman, Philip			An Architecture for Deep, Hierarchical Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present an architecture which lets us train deep, directed generative models with many layers of latent variables. We include deterministic paths between all latent variables and the generated output, and provide a richer set of connections between computations for inference and generation, which enables more effective communication of information throughout the model during training. To improve performance on natural images, we incorporate a lightweight autoregressive model in the reconstruction distribution. These techniques permit end-to-end training of models with 10+ layers of latent variables. Experiments show that our approach achieves state-of-the-art performance on standard image modelling benchmarks, can expose latent class structure in the absence of label information, and can provide convincing imputations of occluded regions in natural images.	[Bachman, Philip] Maluuba Res, Montreal, PQ, Canada		Bachman, P (corresponding author), Maluuba Res, Montreal, PQ, Canada.	phil.bachman@maluuba.com						Bachman P, 2015, ADV NEUR IN, V28; Burda Yuri, 2015, ICLR; Denton E. L., 2015, ARXIV150605751CSCV; Eslami SM, 2016, NEURIPS, V1; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Honari S, 2016, PROC CVPR IEEE, P5743, DOI 10.1109/CVPR.2016.619; Kaiser Lukasz, 2016, ICLR; Kingma D. P, 2014, ARXIV13126114; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maaloe L, 2016, PR MACH LEARN RES, V48; Ranganath R, 2016, PR MACH LEARN RES, V48; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rezende DJ, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256; Theis L, 2015, ADV NEURAL INFORM PR, P1927; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Winther O.:, 2016, ARXIV160202282	26	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700105
C	Barbier, J; Dia, M; Macris, N; Krzakala, F; Lesieur, T; Zdeborova, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Barbier, Jean; Dia, Mohamad; Macris, Nicolas; Krzakala, Florent; Lesieur, Thibault; Zdeborova, Lenka			Mutual information for symmetric rank-one matrix estimation: A proof of the replica formula	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Factorizing low-rank matrices has many applications in machine learning and statistics. For probabilistic models in the Bayes optimal setting, a general expression for the mutual information has been proposed using heuristic statistical physics computations, and proven in few specific cases. Here, we show how to rigorously prove the conjectured formula for the symmetric rank-one case. This allows to express the minimal mean-square-error and to characterize the detectability phase transitions in a large set of estimation problems ranging from community detection to sparse PCA. We also show that for a large set of parameters, an iterative algorithm called approximate message-passing is Bayes optimal. There exists, however, a gap between what currently known polynomial algorithms can do and what is expected information theoretically. Additionally, the proof technique has an interest of its own and exploits three essential ingredients: the interpolation method introduced in statistical physics by Guerra, the analysis of the approximate message-passing algorithm and the theory of spatial coupling and threshold saturation in coding. Our approach is generic and applicable to other open problems in statistical estimation where heuristic statistical physics predictions are available.	[Barbier, Jean; Dia, Mohamad; Macris, Nicolas] Ecole Polytech Fed Lausanne, Fac Informat & Commun, Lab Theorie Commun, CH-1015 Lausanne, Switzerland; [Krzakala, Florent] Sorbonne Univ, PSL Univ & Ecole Normale Super, CNRS, Lab Phys Stat, F-75005 Paris, France; [Krzakala, Florent] Univ Paris 06, F-75005 Paris, France; [Lesieur, Thibault; Zdeborova, Lenka] Univ Paris Saclay, CNRS, CEA, Inst Phys Theor, F-91191 Gif Sur Yvette, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite; CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Barbier, J (corresponding author), Ecole Polytech Fed Lausanne, Fac Informat & Commun, Lab Theorie Commun, CH-1015 Lausanne, Switzerland.	jean.barbier@epfl.ch; mohamad.dia@epfl.ch; nicolas.macris@epfl.ch; florent.krzakala@ens.fr; lesieur.thibault@gmail.com; lenka.zdeborova@gmail.com	Barbier, Jean/ABD-8759-2020; Krzakala, Florent/Q-9652-2019	Krzakala, Florent/0000-0003-2313-2578	SNSF [200021-156672]; ERC under the EU [307087-SPARCS]	SNSF(Swiss National Science Foundation (SNSF)); ERC under the EU	J.B and M.D acknowledge funding from the SNSF (grant 200021-156672). Part of this research received funding from the ERC under the EU's 7th Framework Programme (FP/2007-2013/ERC Grant Agreement 307087-SPARCS). F.K and L.Z thank the Simons Institute for its hospitality.	Amini AA, 2008, IEEE INT SYMP INFO, P2454, DOI 10.1109/ISIT.2008.4595432; Barbier J., 2016, CORR; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Deshpande Y., 2015, ARXIV150708685; Deshpande Y, 2015, FOUND COMPUT MATH, V15, P1069, DOI 10.1007/s10208-014-9215-y; Deshpande Y, 2014, IEEE INT SYMP INFO, P2197, DOI 10.1109/ISIT.2014.6875223; Guerra F, 2005, MATH STAT PHYS, V2005, P243; Guo D., 2005, IEEE T INF THEORY, V51; Hassani S. H., 2010, IEEE INF THEOR WORKS; Johnstone I.M., 2012, J AM STAT ASS; Krzakala F., 2016, ARXIV160308447; Kudekar S., 2011, IEEE T INF TH, V57; Lesieur T., 2015, ANN ALL C; Lesieur T, 2015, IEEE INT SYMP INFO, P1635, DOI 10.1109/ISIT.2015.7282733; Rangan S., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1246, DOI 10.1109/ISIT.2012.6283056; Rigollet P., 2013, ARXIV13040828; Yedla A, 2014, IEEE T INFORM THEORY, V60, P6943, DOI 10.1109/TIT.2014.2352296; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	23	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703016
C	Eberhardt, S; Cader, J; Serre, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Eberhardt, Sven; Cader, Jonah; Serre, Thomas			How Deep is the Feature Analysis underlying Rapid Visual Categorization?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Rapid categorization paradigms have a long history in experimental psychology: Characterized by short presentation times and speeded behavioral responses, these tasks highlight the efficiency with which our visual system processes natural object categories. Previous studies have shown that feed-forward hierarchical models of the visual cortex provide a good fit to human visual decisions. At the same time, recent work in computer vision has demonstrated significant gains in object recognition accuracy with increasingly deep hierarchical architectures. But it is unclear how well these models account for human visual decisions and what they may reveal about the underlying brain processes. We have conducted a large-scale psychophysics study to assess the correlation between computational models and human behavioral responses on a rapid animal vs. non-animal categorization task. We considered visual representations of varying complexity by analyzing the output of different stages of processing in three stateof- the-art deep networks. We found that recognition accuracy increases with higher stages of visual processing (higher level stages indeed outperforming human participants on the same task) but that human decisions agree best with predictions from intermediate stages. Overall, these results suggest that human participants may rely on visual features of intermediate complexity and that the complexity of visual representations afforded by modern deep network models may exceed the complexity of those used by human participants during rapid categorization.	[Eberhardt, Sven; Cader, Jonah; Serre, Thomas] Brown Univ, Brown Inst Brain Sci, Dept Cognit Linguist & Psychol Sci, Providence, RI 02818 USA	Brown University	Eberhardt, S (corresponding author), Brown Univ, Brown Inst Brain Sci, Dept Cognit Linguist & Psychol Sci, Providence, RI 02818 USA.	sven2@brown.edu; jonah_cader@brown.edu; thomas_serre@brown.edu			NSF [IIS-1252951]; DARPA [YFA N66001-14-1-4037]; Center for Computation and Visualization (CCV)	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Center for Computation and Visualization (CCV)(Canadian Institutes of Health Research (CIHR))	We would like to thank Matt Ricci for his early contribution to this work and further discussions. This work was supported by NSF early career award [grant number IIS-1252951] and DARPA young faculty award [grant number YFA N66001-14-1-4037]. Additional support was provided by the Center for Computation and Visualization (CCV).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Cauchoix M, 2016, NEUROIMAGE, V125, P280, DOI 10.1016/j.neuroimage.2015.10.012; Crouzet SM, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00326; Crump MJC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0057410; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010; Fabre-Thorpe M, 2011, FRONT PSYCHOL, V2, DOI 10.3389/fpsyg.2011.00243; Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Jia Y., 2014, P 2014 ACM C MULT MM, P10005; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; McDonnell J. V., 2012, PSITURK VERSION 1 02; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Potter MC, 2012, FRONT PSYCHOL, V3, DOI 10.3389/fpsyg.2012.00032; Serre T, 2007, P NATL ACAD SCI USA, V104, P6424, DOI 10.1073/pnas.0700622104; Sofer I, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004456; Vanrullen Rufin, 2008, Adv Cogn Psychol, V3, P167, DOI 10.2478/v10053-008-0022-3; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yu W., 2014, VISUALIZING COMPARIN	21	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701075
C	Reed, S; Akata, Z; Mohan, S; Tenka, S; Schiele, B; Lee, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Reed, Scott; Akata, Zeynep; Mohan, Santosh; Tenka, Samuel; Schiele, Bernt; Lee, Honglak			Learning What and Where to Draw	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Generative Adversarial Networks (GANs) have recently demonstrated the capability to synthesize compelling real-world images, such as room interiors, album covers, manga, faces, birds, and flowers. While existing models can synthesize images based on global constraints such as a class label or caption, they do not provide control over pose or object location. We propose a new model, the Generative Adversarial What-Where Network (GAWWN), that synthesizes images given instructions describing what content to draw in which location. We show high-quality 128 x 128 image synthesis on the Caltech-UCSD Birds dataset, conditioned on both informal text descriptions and also object location. Our system exposes control over both the bounding box around the bird and its constituent parts. By modeling the conditional distributions over part locations, our system also enables conditioning on arbitrary subsets of parts (e.g. only the beak and tail), yielding an efficient interface for picking part locations.	[Reed, Scott; Mohan, Santosh; Tenka, Samuel; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA; [Akata, Zeynep; Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany; [Reed, Scott] DeepMind, London, England	University of Michigan System; University of Michigan; Max Planck Society	Reed, S (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.; Reed, S (corresponding author), DeepMind, London, England.	reedscot@google.com; akata@mpi-inf.mpg.de; santoshm@umich.edu; samtenka@umich.edu; schiele@mpi-inf.mpg.de; honglak@umich.edu			NSF CAREER [IIS-1453651]; ONR [N00014-13-1-0762]; Sloan Research Fellowship	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); Sloan Research Fellowship(Alfred P. Sloan Foundation)	This work was supported in part by NSF CAREER IIS-1453651, ONR N00014-13-1-0762, and a Sloan Research Fellowship.	Akata Z., 2015, CVPR; Andriluka M., 2014, CVPR; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2016, ICML; [Anonymous], 2014, ICLR; Cho K., 2014, P SSST 8 8 WORKSHOP, P103; Denton E. L., 2015, NIPS; Dosovitskiy A., 2015, CVPR; Eslami S. M. A., 2016, NIPS 2016, P3; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, ICML; Kiros R., 2014, ACL; Kulkarni T. D., 2015, NIPS; Larochelle H., 2011, AISTATS; Mansimov E., 2016, P INT C LEARN REPR; Oh J., 2015, NIPS; Oquab Q., 2016, MODULES SPATIAL TRAN; Radford A., 2016, ICLR; Reed S. E., 2015, P ADV NEUR INF PROC, P1; Reed S. E., 2016, CVPR; Rezende D., 2016, ICML; Rezende D.J., 2014, ICML; Salakhutdinov R., 2009, AISTATS; Theis L., 2015, NIPS; Van Den Oord A., 2016, ICML; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Yang J., 2015, P ADV NEUR INF PROC	27	12	12	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700075
C	Wu, J; Frazier, PI		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Jian; Frazier, Peter, I			The Parallel Knowledge Gradient Method for Batch Bayesian Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In many applications of black-box optimization, one can evaluate multiple points simultaneously, e.g. when evaluating the performances of several different neural networks in a parallel computing environment. In this paper, we develop a novel batch Bayesian optimization algorithm - the parallel knowledge gradient method. By construction, this method provides the one-step Bayes optimal batch of points to sample. We provide an efficient strategy for computing this Bayes-optimal batch of points, and we demonstrate that the parallel knowledge gradient method finds global optima significantly faster than previous batch Bayesian optimization algorithms on both synthetic test functions and when tuning hyperparameters of practical machine learning algorithms, especially when function evaluations are noisy.	[Wu, Jian; Frazier, Peter, I] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Wu, J (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	jw926@cornell.edu; pf98@cornell.edu			NSF CAREER [CMMI-1254298]; NSF [CMMI-1536895, IIS-1247696]; AFOSR [FA9550-12-1-0200, FA9550-15-1-0038, FA9550-16-1-0046]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	The authors were partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.	Bingham D., 2015, OPTIMIZATION TEST PR; Chevalier Clement, 2013, Learning and Intelligent Optimization. 7th International Conference, LION 7. Revised Selected Papers: LNCS 7997, P59, DOI 10.1007/978-3-642-44973-4_7; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Deng L, 2013, FOUND TRENDS SIGNAL, V7, pI, DOI 10.1561/2000000039; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Gelbart MA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P250; Ginsbourger D, 2010, ADAPT LEARN OPTIM, V2, P131; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; LECUYER P, 1990, MANAGE SCI, V36, P1364, DOI 10.1287/mnsc.36.11.1364; Marmin Sebastien, 2015, Machine Learning, Optimization and Big Data. First International Workshop, MOD 2015. Revised Selected Papers: LNCS 9432, P37, DOI 10.1007/978-3-319-27926-8_4; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Scott W, 2011, SIAM J OPTIMIZ, V21, P996, DOI 10.1137/100801275; Shah A., 2015, ADV NEURAL INFORM PR, P3312; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Snoek Jasper and authors, 2015, SPEARMINT; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Tu┬rn A., 1989, GLOBAL OPTIMIZATION, V350; Wang J., 2015, PARALLEL BAYESIAN GL; Wang J., 2014, METRICS OPTIMIZATION; Wang YF, 2015, SIAM J SCI COMPUT, V37, pB361, DOI 10.1137/140971117	25	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702054
C	Zhai, SF; Cheng, Y; Lu, WN; Zhang, ZF		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhai, Shuangfei; Cheng, Yu; Lu, Weining; Zhang, Zhongfei (Mark)			Doubly Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Building large models with parameter sharing accounts for most of the success of deep convolutional neural networks (CNNs). In this paper, we propose doubly convolutional neural networks (DCNNs), which significantly improve the performance of CNNs by further exploring this idea. In stead of allocating a set of convolutional filters that are independently learned, a DCNN maintains groups of filters where filters within each group are translated versions of each other. Practically, a DCNN can be easily implemented by a two-step convolution procedure, which is supported by most modern deep learning libraries. We perform extensive experiments on three image classification benchmarks: CIFAR-10, CIFAR-100 and ImageNet, and show that DCNNs consistently outperform other competing architectures. We have also verified that replacing a convolutional layer with a doubly convolutional layer at any depth of a CNN can improve its performance. Moreover, various design choices of DCNNs are demonstrated, which shows that DCNN can serve the dual purpose of building more accurate models and/or reducing the memory footprint without sacrificing the accuracy.	[Zhai, Shuangfei; Zhang, Zhongfei (Mark)] Binghamton Univ, Vestal, NY 13902 USA; [Cheng, Yu] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Lu, Weining] Tsinghua Univ, Beijing 10084, Peoples R China	State University of New York (SUNY) System; State University of New York (SUNY) Binghamton; International Business Machines (IBM); Tsinghua University	Zhai, SF (corresponding author), Binghamton Univ, Vestal, NY 13902 USA.	szhai2@binghamton.edu; chengyu@us.ibm.com; luwn14@mails.tsinghua.edu.cn; zhongfei@cs.binghamton.edu						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baldi P, 2014, ARXIV14126830; Cheng Yu, 2015, INT C COMP VIS ICCV; Cohen TS, 2016, PR MACH LEARN RES, V48; Dieleman S, 2016, PR MACH LEARN RES, V48; Djork-Arn, ICLR 2016; Gens R., 2014, NIPS; Goodfellow I. J., 2013, ARXIV13024389; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kavukcuoglu K, 2009, PROC CVPR IEEE, P1605, DOI 10.1109/CVPRW.2009.5206545; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee C.-Y., 2014, ARXIV14095185; Li HY, 2016, PR MACH LEARN RES, V48; Lin M, 2014, PUBLIC HEALTH NUTR, V17, P2029, DOI [10.1017/S1368980013002176, 10.1109/PLASMA.2013.6634954]; Novikov A, 2015, ADV NEUR IN, V28; Shang WL, 2016, PR MACH LEARN RES, V48; Sindhwani V., 2015, ADV NEURAL INFORM PR, P3088; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173; Yu F., 2016, P ICLR 2016; Zeiler Matthew D, 2012, ARXIV12125701	24	12	15	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702087
C	Abdolmaleki, A; Lioutikov, R; Lau, N; Reis, LP; Peters, J; Neumann, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Abdolmaleki, Abbas; Lioutikov, Rudolf; Lau, Nuno; Reis, Luis Paulo; Peters, Jan; Neumann, Gerhard			Model-Based Relative Entropy Stochastic Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Stochastic search algorithms are general black-box optimizers. Due to their ease of use and their generality, they have recently also gained a lot of attention in operations research, machine learning and policy search. Yet, these algorithms require a lot of evaluations of the objective, scale poorly with the problem dimension, are affected by highly noisy objective functions and may converge prematurely. To alleviate these problems, we introduce a new surrogate-based stochastic search approach. We learn simple, quadratic surrogate models of the objective function. As the quality of such a quadratic approximation is limited, we do not greedily exploit the learned models. The algorithm can be misled by an inaccurate optimum introduced by the surrogate. Instead, we use information theoretic constraints to bound the 'distance' between the new and old data distribution while maximizing the objective function. Additionally the new method is able to sustain the exploration of the search distribution to avoid premature convergence. We compare our method with state of art black-box optimization methods on standard uni-modal and multi-modal optimization functions, on simulated planar robot tasks and a complex robot ball throwing task. The proposed method considerably outperforms the existing approaches.	[Abdolmaleki, Abbas; Lau, Nuno] Univ Aveiro, IEETA, Aveiro, Portugal; [Abdolmaleki, Abbas; Reis, Luis Paulo] Univ Minho, DSI, Braga, Portugal; [Abdolmaleki, Abbas; Reis, Luis Paulo] Univ Porto, LIACC, Porto, Portugal; [Lioutikov, Rudolf; Peters, Jan] Tech Univ Darmstadt, IAS, Darmstadt, Germany; [Neumann, Gerhard] Tech Univ Darmstadt, CLAS, Darmstadt, Germany; [Peters, Jan] Max Planck Inst Intelligent Syst, Stuttgart, Germany	Universidade de Aveiro; Universidade do Minho; Universidade do Porto; Technical University of Darmstadt; Technical University of Darmstadt; Max Planck Society	Abdolmaleki, A (corresponding author), Univ Aveiro, IEETA, Aveiro, Portugal.	abbas.a@ua.pt; Lioutikov@ias.tu-darmstadt.de; nunolau@ua.pt; lpreis@dsi.uminho.pt; peters@ias.tu-darmstadt.de; neumann@ias.tu-darmstadt.de	Reis, Luis Paulo/C-5751-2008; Peters, Jan/P-6027-2019; Peters, Jan R/D-5068-2009	Reis, Luis Paulo/0000-0002-4709-1718; Peters, Jan/0000-0002-5266-8091; Peters, Jan R/0000-0002-5266-8091	European Unions Horizon 2020 research and innovation programme [645582]; FCT [SFRH/BD/81155/2011]	European Unions Horizon 2020 research and innovation programme; FCT(Portuguese Foundation for Science and TechnologyEuropean Commission)	This project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No #645582 (RoMaNS) and the first author is supported by FCT under grant SFRH/BD/81155/2011.	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Furmston T., 2012, NEURAL INFORM PROCES; Hansen N., 2003, REDUCING TIME COMPLE; Ijspeert A., 2003, ADV NEURAL INFORM PR, V15; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Kober J., 2010, MACH LEARN, P1; Kupcsik A., 2013, P NAT C ART INT AAAI; Loshchilov I., 2013, CORR; Loshchilov I., 2013, GECCO; Mannor S., 2003, P 20 INT C MACH LEAR; Mehmet G., 2013, IEEE T CYBERNETICS; Molga M., 2005, TEST FUNCTIONS OPTIM; Murray I., 2010, JMLR W CP, V9; Peters J., 2010, P 24 NAT C ART INT A; Powell M. J. D., 2004, 2004NA05 DAMTP U CAM; Powell M. J. D, 2009, 2009NA06 DAMTP U CAM; Ruckstiess T., 2008, P EUR C MACH LEARN E; Stulp F., 2012, INT C MACH LEARN ICM; Sun Y., 2009, P 11 ANN C GEN EV CO; Theodorou E., 2010, J MACHINE LEARNING R; Wierstra D., 2014, J MACHINE LEARNING R	22	12	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100040
C	Lin, T; Li, J; Chen, W		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lin, Tian; Li, Jian; Chen, Wei			Stochastic Online Greedy Learning with Semi-bandit Feedbacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The greedy algorithm is extensively studied in the field of combinatorial optimization for decades. In this paper, we address the online learning problem when the input to the greedy algorithm is stochastic with unknown parameters that have to be learned over time. We first propose the greedy regret and epsilon-quasi greedy regret as learning metrics comparing with the performance of offline greedy algorithm. We then propose two online greedy learning algorithms with semi-bandit feedbacks, which use multi-armed bandit and pure exploration bandit policies at each level of greedy learning, one for each of the regret metrics respectively. Both algorithms achieve O(log T) problem-dependent regret bound (T being the time horizon) for a general class of combinatorial structures and reward functions that allow greedy solutions. We further show that the bound is tight in T and other problem instance parameters.	[Lin, Tian; Li, Jian] Tsinghua Univ, Beijing, Peoples R China; [Chen, Wei] Microsoft Res, Beijing, Peoples R China	Tsinghua University; Microsoft	Lin, T (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	lintian06@gmail.com; lapordge@gmail.com; weic@microsoft.com		Chen, Wei/0000-0003-0065-3610	National Basic Research Program of China [2015CB358700, 2011CBA00300, 2011CBA00301]; National NSFC [61202009, 61033001, 61361136003]	National Basic Research Program of China(National Basic Research Program of China); National NSFC(National Natural Science Foundation of China (NSFC))	Jian Li was supported in part by the National Basic Research Program of China grants 2015CB358700, 2011CBA00300, 2011CBA00301, and the National NSFC grants 61202009, 61033001, 61361136003.	Audibert J.-Y., 2011, ARXIV11054871; Audibert Jean-Yves, 2010, P 23 ANN C LEARN THE, p2010a; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bjorner A., 1992, MATROID APPL, P284; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen S., 2014, NIPS; Chen W., 2013, ICML 2013, P151; Chvatal V., 1979, Mathematics of Operations Research, V4, P233, DOI 10.1287/moor.4.3.233; Gabillon V., 2013, ADV NEURAL INFORM PR, P2697; Gai Y., 2010, DYSPAN; Garivier A., 2011, ARXIV11022490; HELMAN P, 1993, SIAM J DISCRETE MATH, V6, P274, DOI 10.1137/0406021; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Kempe D., 2003, SIGKDD; KORTE B, 1984, SIAM J ALGEBRA DISCR, V5, P229, DOI 10.1137/0605024; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Kveton B., 2014, ARXIV14100949; Kveton B, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P420; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lin Tsun-Han, 2014, ICML; Mirzasoleiman B, 2015, AAAI CONF ARTIF INTE, P1812; PRIM RC, 1957, AT&T TECH J, V36, P1389, DOI 10.1002/j.1538-7305.1957.tb01515.x; Streeter M., 2009, NIPS	26	12	13	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102078
C	Ralaivola, L; d'Alche-Buc, F		Thrun, S; Saul, K; Scholkopf, B		Ralaivola, L; d'Alche-Buc, F			Dynamical modeling with kernels for nonlinear time series prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We consider the question of predicting nonlinear time series. Kernel Dynamical Modeling (KDM), a new method based on kernels, is proposed as an extension to linear dynamical models. The kernel trick is used twice: first, to learn the parameters of the model, and second, to compute preimages of the time series predicted in the feature space by means of Support Vector Regression. Our model shows strong connection with the classic Kalman Filter model, with the kernel feature space as hidden state space. Kernel Dynamical Modeling is tested against two benchmark time series and achieves high quality predictions.	Univ Paris 06, Lab Informat Paris 6, F-75015 Paris, France	UDICE-French Research Universities; Sorbonne Universite	Ralaivola, L (corresponding author), Univ Paris 06, Lab Informat Paris 6, 8 Rue Capitaine Scott, F-75015 Paris, France.	liva.ralaivola@lip6.fr; florence.dalche@lip6.fr						BOSER B, 1992, P 5 AN WORKSH COMP L, V5; Dorffner G., 1996, Neural Network World, V6, P447; Julier S. J., 1997, INT S AER DEF SENS S; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; MIKA S, 1999, NIPS; MUKHERJEE S, 1997, P IEEE NNSP 97; Muller K.-R., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P999, DOI 10.1007/BFb0020283; RALAVIOLA L, 2003, ACT 19 S GRETSI TRAI; RALAVIOLA L, 2003, THSIS U PARIS 6 FRAN; ROSTI AVI, 2001, CUEDFINFENGTR420; ROWEIS S, 1997, NEURAL COMPUT, V11, P305; Scholkopf B., 2001, LEARNING KERNELS SUP; Smola A., 1998, NEUROCOLT2; Ueda N, 1999, ADV NEUR IN, V11, P599; Vapnik V.N, 1998, STAT LEARNING THEORY	15	12	12	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						129	136						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500017
C	Thomas, PJ; Spencer, DJ; Hampton, SK; Park, P; Zurkus, JP		Thrun, S; Saul, K; Scholkopf, B		Thomas, PJ; Spencer, DJ; Hampton, SK; Park, P; Zurkus, JP			The diffusion mediated biochemical signal relay channel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				INFORMATION CAPACITY	Biochemical signal-transduction networks are the biological information-processing systems by which individual cells, from neurons to amoebae, perceive and respond to their chemical environments. We introduce a simplified model of a single biochemical relay and analyse its capacity as a communications channel. A diffusible ligand is released by a sending cell and received by binding to a transmembrane receptor protein on a receiving cell. This receptor-ligand interaction creates a nonlinear communications channel with non-Gaussian noise. We model this channel numerically and study its response to input signals of different frequencies in order to estimate its channel capacity. Stochastic effects introduced in both the diffusion process and the receptor-ligand interaction give the channel low-pass characteristics. We estimate the channel capacity using a water-filling formula adapted from the additive white-noise Gaussian channel.	Salk Inst Biol Studies, Computat Neurobiol Lab, La Jolla, CA 92037 USA	Salk Institute	Thomas, PJ (corresponding author), Salk Inst Biol Studies, Computat Neurobiol Lab, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.			Thomas, Peter/0000-0001-7533-6770				Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Detwiler PB, 2000, BIOPHYS J, V79, P2801, DOI 10.1016/S0006-3495(00)76519-2; FREY MR, 1991, IEEE T INFORM THEORY, V37, P244, DOI 10.1109/18.75239; Getz WM, 2001, CHEM SENSES, V26, P95, DOI 10.1093/chemse/26.2.95; Mitra PP, 2001, NATURE, V411, P1027, DOI 10.1038/35082518; Rappel WJ, 2002, BIOPHYS J, V83, P1361, DOI 10.1016/S0006-3495(02)73906-4; Ueda M, 2001, SCIENCE, V294, P864, DOI 10.1126/science.1063951; Uteshev VV, 1997, BIOPHYS J, V72, P1127, DOI 10.1016/S0006-3495(97)78761-7	8	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1263	1270						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500157
C	Yanover, C; Weiss, Y		Thrun, S; Saul, K; Scholkopf, B		Yanover, C; Weiss, Y			Finding the M most probable configurations using loopy belief propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Loopy belief propagation (BP) has been successfully used in a number of difficult graphical models to find the most probable configuration of the hidden variables. In applications ranging from protein folding to image analysis one would like to find not just the best configuration but rather the top M. While this problem has been solved using the junction tree formalism, in many real world problems the clique size in the junction tree is prohibitively large. In this work we address the problem of finding the M best configurations when exact inference is impossible. We start by developing a new exact inference algorithm for calculating the best configurations that uses only max-marginals. For approximate inference, we replace the max-marginals with the beliefs, calculated using max-product BP and generalized BP. We show empirically that the algorithm can accurately and rapidly approximate the M best configurations in graphs with hundreds of variables.	Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Yanover, C (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.	cheny@cs.huji.ac.il; yweiss@cs.huji.ac.il	Yanover, Chen/A-3754-2012	Yanover, Chen/0000-0003-3663-4286				CANO A, 2000, J INTELLIGENT SYSTEM, V15, P1010; Cowell R., 1998, LEARNING GRAPHICAL M; Dawid A. P., 1992, Statistics and Computing, V2, P25, DOI 10.1007/BF01890546; DECHTER R, 1997, UNCERTAINTY ARTIFIC; DOUCET A, 2000, P UAI 2000; FREY BJ, 2001, ADV NEURAL INFORMATI, V14; Jaakkola TS, 1999, J ARTIF INTELL RES, V10, P291, DOI 10.1613/jair.583; Leach AR, 1998, PROTEINS, V33, P227, DOI 10.1002/(SICI)1097-0134(19981101)33:2<227::AID-PROT7>3.0.CO;2-F; LEVIN A, 2002, P NIPS 2002; MURPHY KP, 2001, COMPUTING SCI STAT, V33; Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483; PEARL J, 1998, PROBABILISTIC REASON; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; WAINWRIGHT MJ, 2002, P2554 MIT LIDS; WAINWRIGHT MJ, 2002, P NIPS 2002; Weiss Y, 2001, IEEE T INFORM THEORY, V47, P736, DOI 10.1109/18.910585; YANOVER C, 2002, P NIPS 2002; Yedidia J. S., 2003, EXPLORING ARTIFICIAL	18	12	12	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						289	296						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500037
C	Hershey, J; Casey, M		Dietterich, TG; Becker, S; Ghahramani, Z		Hershey, J; Casey, M			Audio-visual sound separation via hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SPEECH; ENHANCEMENT; NOISE	It is well known that under noisy conditions we can hear speech much more clearly when we read the speaker's lips. This suggests the utility of audio-visual information for the task of speech enhancement. We propose a method to exploit audio-visual cues to enable speech separation under non-stationary noise and with a single microphone. We revise and extend HMM-based speech enhancement techniques, in which signal and noise models are factorially combined, to incorporate visual lip information and employ novel signal HMMs in which the dynamics of narrow-band and wide band components are factorial. We avoid the combinatorial explosion in the factorial model by using a simple approximate inference technique to quickly estimate the clean signals in a mixture. We present a preliminary evaluation of this approach using a small-vocabulary audio-visual database, showing promising improvements in machine intelligibility for speech enhanced using audio and visual information.	Univ Calif San Diego, Dept Cognit Sci, San Diego, CA 92103 USA	University of California System; University of California San Diego	Hershey, J (corresponding author), Univ Calif San Diego, Dept Cognit Sci, San Diego, CA 92103 USA.							ATTIAS H, 2001, ADV NEURAL INFORMATI, V13; Brand M, 1999, NEURAL COMPUT, V11, P1155, DOI 10.1162/089976699300016395; Dupont S, 2000, IEEE T MULTIMEDIA, V2, P141, DOI 10.1109/6046.865479; EPHRAIM Y, 1992, P IEEE, V80, P1526, DOI 10.1109/5.168664; FISHER JW, 2001, ADV NEURAL INFORMATI, V13; GALES M, 1996, THESIS CAMBRIDGE U; GHAHRAMANI Z, 1996, ADV NEURAL INFORMATI, V8; Girin L, 2001, J ACOUST SOC AM, V109, P3007, DOI 10.1121/1.1358887; HUANG FJ, 1998, IEEE WORKSH MULT SIN; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; LEGGETTER CJ, 1995, COMPUT SPEECH LANG, V9, P171, DOI 10.1006/csla.1995.0010; Robert-Ribes J, 1998, J ACOUST SOC AM, V103, P3677, DOI 10.1121/1.423069; ROWEIS ST, 2001, ADV NEURAL INFORMATI, V13; SUMBY WH, 1954, J ACOUST SOC AM, V26, P212, DOI 10.1121/1.1907309	14	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1173	1180						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100146
C	Trappenberg, TP; Rolls, ET; Stringer, SM		Dietterich, TG; Becker, S; Ghahramani, Z		Trappenberg, TP; Rolls, ET; Stringer, SM			Effective size of receptive fields of inferior temporal visual cortex neurons in natural scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SINGLE NEURONS; CORTICAL AREAS; ATTENTION; RESPONSES; MACAQUE	Inferior temporal cortex (IT) neurons have large receptive fields when a single effective object stimulus is shown against a blank background, but have much smaller receptive fields when the object is placed in a natural scene. Thus, translation invariant object recognition is reduced in natural scenes, and this may help object selection. We describe a model which accounts for this by competition within an attractor in which the neurons are tuned to different objects in the scene, and the fovea has a higher cortical magnification factor than the peripheral visual field. Furthermore, we show that top-down object bias can increase the receptive field size, facilitating object search in complex visual scenes, and providing a model of object-based attention. The model leads to the prediction that introduction of a second object into a scene with blank background will reduce the receptive field size to values that depend on the closeness of the second object to the target stimulus. We suggest that mechanisms of this type enable the output of IT to be primarily about one object, so that the areas that receive from IT can select the object as a potential target for action.	Dalhousie Univ, Fac Comp Sci, Halifax, NS B3H 1W5, Canada	Dalhousie University	Trappenberg, TP (corresponding author), Dalhousie Univ, Fac Comp Sci, 5060 Univ Ave, Halifax, NS B3H 1W5, Canada.			Rolls, Edmund/0000-0003-3025-1292; Trappenberg, Thomas/0000-0002-6144-8963				[Anonymous], 1998, NEURAL NETWORKS BRAI; Booth MCA, 1998, CEREB CORTEX, V8, P510, DOI 10.1093/cercor/8.6.510; Deco G, 2001, VIS COGN, V8, P119, DOI 10.1080/13506280042000054; DESIMONE R, 1995, ANNU REV NEUROSCI, V18, P193, DOI 10.1146/annurev.neuro.18.1.193; DOW BM, 1981, EXP BRAIN RES, V44, P213; GATTASS R, 1985, EXP BRAIN RES S, V11; Gross C.G., 1985, EXP BRAIN RES, V11, P179; RENART A, 2000, IN PRESS ADV NEURAL; Rolls E., 2000, SOC NEUR ABSTR, V26, P1331; Rolls E.T., 2002, COMPUTATIONAL NEUROS; ROLLS ET, 1995, EXP BRAIN RES, V103, P409; Rolls ET, 2000, NEURON, V27, P205, DOI 10.1016/S0896-6273(00)00030-1; ROLLS ET, 2001, SOC NEUR ABSTR, V27; SATO T, 1989, EXP BRAIN RES, V77, P23; TOVEE MJ, 1994, J NEUROPHYSIOL, V72, P1049, DOI 10.1152/jn.1994.72.3.1049	15	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						293	300						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100037
C	Attias, H		Solla, SA; Leen, TK; Muller, KR		Attias, H			Independent factor analysis with temporally structured sources	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				BLIND SOURCE SEPARATION; COMPONENT ANALYSIS; ALGORITHM	We present a new technique for time series analysis based on dynamic probabilistic networks. Ln this approach, the observed data are modeled in terms of unobserved, mutually independent factors, as in the recently introduced technique of Independent Factor Analysis (IFA). However, unlike in IFA, the factors are not i.i.d.; each factor has its own temporal statistical characteristics. We derive a family of EM algorithms that learn the structure of the underlying factors and their relation to the data. These algorithms perform source separation and noise reduction in an integrated manner, and demonstrate superior performance compared to IFA.	UCL, Gatsby Unit, London WC1N 3AR, England	University of London; University College London	Attias, H (corresponding author), UCL, Gatsby Unit, 17 Queen Sq, London WC1N 3AR, England.							Amari S, 1996, ADV NEUR IN, V8, P757; Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458; Attias H, 1998, NEURAL COMPUT, V10, P1373, DOI 10.1162/neco.1998.10.6.1373; ATTIAS H, 2000, ADV NEUR INFO P SYS, V12; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; LEE DH, COMMUNICATION; LEE DU, 1999, UNPUB; Pearlmutter BA, 1997, ADV NEUR IN, V9, P613; Rabiner L., 1993, FUNDAMENTALS SPEECH; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251	12	12	12	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						386	392						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700055
C	Horn, D; Levy, N; Meilijson, I; Ruppin, E		Solla, SA; Leen, TK; Muller, KR		Horn, D; Levy, N; Meilijson, I; Ruppin, E			Distributed synchrony of spiking neurons in a Hebbian cell assembly	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				TEMPORAL CORTEX	We investigate the behavior of a Hebbian cell assembly of spiking neurons formed via a temporal synaptic learning curve. This learning function is based on recent experimental findings. It includes potentiation for short time delays between pre- and post-synaptic neuronal spiking, and depression for spiking events occuring in the reverse order. The coupling between the dynamics of the synaptic learning and of the neuronal activation leads to interesting results. We find that the cell assembly can fire asynchronously, but may also function in complete synchrony, or in distributed synchrony. The latter implies spontaneous division of the Hebbian cell assembly into groups of cells that fire in a cyclic manner. We invetigate the behavior of distributed synchrony both by simulations and by analytic calculations of the resulting synaptic distributions.	Tel Aviv Univ, Raymond & Beverly Sackler Fac Exact Sci, Sch Phys & Astron, IL-69978 Tel Aviv, Israel	Tel Aviv University	Horn, D (corresponding author), Tel Aviv Univ, Raymond & Beverly Sackler Fac Exact Sci, Sch Phys & Astron, IL-69978 Tel Aviv, Israel.		Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940; Horn, David/0000-0003-2708-186X				Abbott LF, 1999, ADV NEUR IN, V11, P69; Abeles M., 1982, LOCAL CORTICAL CIRCU; BRUNEL N, 1999, J COMPUTATIONAL NEUR; HERRMANN M, 1995, NETWORK-COMP NEURAL, V6, P403, DOI 10.1088/0954-898X/6/3/006; Kempter R, 1999, ADV NEUR IN, V11, P125; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; MIYASHITA Y, 1988, NATURE, V335, P817, DOI 10.1038/335817a0; Yakovlev V, 1998, NAT NEUROSCI, V1, P310, DOI 10.1038/1131; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	9	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						129	135						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700019
C	Kearns, M; Mansour, Y; Ng, AY		Solla, SA; Leen, TK; Muller, KR		Kearns, M; Mansour, Y; Ng, AY			Approximate planning in large POMDPs via reusable trajectories	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We consider the problem of reliably choosing a near-best strategy from a restricted class of strategies IT in a partially observable Markov decision process (POMDP). We assume we are given the ability to simulate the POMDP, and study what might be called the sample complexity that is, the amount of data one must generate in the POMDP in order to choose a good strategy. We prove upper bounds on the sample complexity showing that, even for infinitely large and arbitrarily complex POMDPs, the amount of data needed can be finite, and depends only linearly on the complexity of the restricted strategy class II, and exponentially on the horizon time. This latter dependence can be eased in a variety of ways, including the application of gradient and local search algorithms. Our measure of complexity generalizes the classical supervised learning notion of VC dimension to the settings of reinforcement learning and planning.	AT&T Labs, Murray Hill, NJ 07974 USA	AT&T	Kearns, M (corresponding author), AT&T Labs, Murray Hill, NJ 07974 USA.							BAIRD LC, 1999, ADV NEURAL INFORMATI, V11; BOUTILIER C, 1999, J ARTIFICIAL INTELLI; Boyen X, 1998, P 14 ANN C UNC ART I, P33; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Kaelbling L.P., 1998, ARTIF INTELL, V101; KEARNS M, 1999, APPROXIMATE PLANNING; KOLLER D, 1999, P 16 INT JOINT C ART; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Vapnik V., 1982, ESTIMATION DEPENDENC; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	10	12	12	1	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						1001	1007						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700141
C	Baraduc, P; Guigon, E; Burnod, Y		Kearns, MS; Solla, SA; Cohn, DA		Baraduc, P; Guigon, E; Burnod, Y			Where does the population vector of motor cortical cells point during reaching movements?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				VISUAL TARGETS; ARM MOVEMENTS; CORTEX; MODEL; SPACE	Visually-guided arm reaching movements are produced by distributed neural networks within parietal and frontal regions of the cerebral cortex. Experimental data indicate that (1) single neurons in these regions are broadly tuned to parameters of movement; (2) appropriate commands are elaborated by populations of neurons; (3) the coordinated action of neurons can be visualized using a neuronal population vector (NPV). However, the NPV provides only a rough estimate of movement parameters (direction, velocity) and may even fail to reflect the parameters of movement when arm posture is changed. We designed a model of the cortical motor command to investigate the relation between the desired direction of the movement, the actual direction of movement and the direction of the NPV in motor cortex. The model is a two-layer self-organizing neural network which combines broadly-tuned (muscular) proprioceptive and (cartesian) visual information to calculate (angular) motor commands for the initial part of the movement of a two-link arm. The network was trained by motor babbling in 5 positions. Simulations showed that (1) the network produced appropriate movement direction over a large part of the workspace; (2) small deviations of the actual trajectory from the desired trajectory existed at the extremities of the workspace; (3) these deviations were accompanied by large deviations of the NPV from both trajectories. These results suggest the NPV does not give a faithful image of cortical processing during arm reaching movements.	Univ Paris 06, INSERM U483, F-75252 Paris 05, France	Institut National de la Sante et de la Recherche Medicale (Inserm); UDICE-French Research Universities; Sorbonne Universite	Baraduc, P (corresponding author), Univ Paris 06, INSERM U483, 9 Quai St Bernard, F-75252 Paris 05, France.							BULLOCK D, 1993, J COGNITIVE NEUROSCI, V5, P408, DOI 10.1162/jocn.1993.5.4.408; BURNOD Y, 1992, J NEUROSCI, V12, P1435; CAMINITI R, 1991, J NEUROSCI, V11, P1182; Georgopoulos AP, 1996, COGNITIVE BRAIN RES, V3, P151, DOI 10.1016/0926-6410(95)00040-2; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; GHILARDI MF, 1995, J NEUROPHYSIOL, V73, P2535, DOI 10.1152/jn.1995.73.6.2535; KALASKA JF, 1992, SCIENCE, V255, P1517, DOI 10.1126/science.1549781; LEMON R, 1988, TRENDS NEUROSCI, V11, P501, DOI 10.1016/0166-2236(88)90012-4; SALINAS E, 1995, J NEUROSCI, V15, P6461; SCOTT SH, 1995, J NEUROPHYSIOL, V73, P2563, DOI 10.1152/jn.1995.73.6.2563	10	12	12	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						83	89						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700012
C	Freeman, WT; Pasztor, EC		Kearns, MS; Solla, SA; Cohn, DA		Freeman, WT; Pasztor, EC			Learning to estimate scenes from images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				REGULARIZATION; CODE	We seek the scene interpretation that best explains image data. For example, we may want to infer the projected velocities (scene) which best explain two consecutive image frames (image). From synthetic data, we model the relationship between image and scene patches, and between a scene patch and neighboring scene patches. Given a new image, we propagate likelihoods in a Markov network (ignoring the effect of loops) to infer the underlying scene. This yields an efficient method to form low-level scene interpretations. We demonstrate the technique for motion analysis and estimating high resolution images from low-resolution ones.	MERL, Cambridge, MA 02139 USA		Freeman, WT (corresponding author), MERL, 201 Broadway, Cambridge, MA 02139 USA.							ATICK JJ, 1992, NEURAL COMPUT, V4, P559, DOI 10.1162/neco.1992.4.4.559; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Berger J. O., 1985, STAT DECISION THEORY; Bishop, 1995, NEURAL NETWORKS PATT; Black MJ, 1993, P 4 INT C COMP VIS, P231, DOI DOI 10.1109/ICCV.1993.378214; DEBONET JS, 1998, P IEEE COMPUTER VISI; FREY BJ, 1997, BAYESIAN NETWORKS PA; GEIGER D, 1991, IEEE T PATTERN ANAL, V13, P401, DOI 10.1109/34.134040; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GRAY RM, 1993, DIGITAL IMAGES HUMAN; HEEGER DJ, 1995, ACM SIGGRAPH, P229; HURLBERT AC, 1988, SCIENCE, V239, P482, DOI 10.1126/science.3340834; ISARD M, 1996, P EUR C COMP VIS, P343; JORDAN M, 1998, LEARNING GRAPHICAL M; Ju SX, 1996, PROC CVPR IEEE, P307, DOI 10.1109/CVPR.1996.517090; KERSTEN D, 1987, APPL OPTICS, V26, P4999, DOI 10.1364/AO.26.004999; KERSTEN D, 1991, COMPUTATIONAL MODELS, pCH15; Knill DC, 1996, PERCEPTION BAYESIAN; LUETTGEN MR, 1994, IEEE T IMAGE PROCESS, V3, P41, DOI 10.1109/83.265979; MacKay D.J, 1995, LNCS, V1025; NOWLAN SJ, 1995, J NEUROSCI, V15, P1195; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; PENTLAND A, 1993, DIGITAL IMAGES HUMAN; POGGIO T, 1985, NATURE, V317, P314, DOI 10.1038/317314a0; SAUND E, 1998, CVPR 98 WORKSH PERCE; SCHULTZ RR, 1994, IEEE T IMAGE PROCESS, V3, P233, DOI 10.1109/83.287017; SIMONCELLI EP, 1999, ADV NEURAL INFORMATI, V11; SIMONCELLI EP, 1997, 31 AS C SIG SYS COMP; Weiss Y, 1997, ADV NEUR IN, V9, P908; WEISS Y, 1998, 1616 MIT AI LAB MEM; ZHU SC, 1997, IEEE PATTERN ANAL MA, V19	33	12	12	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						775	781						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700109
C	Moody, J; Saffell, M		Kearns, MS; Solla, SA; Cohn, DA		Moody, J; Saffell, M			Reinforcement learning for trading	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				PERFORMANCE	We propose to train trading systems by optimizing financial objective functions via reinforcement learning. The performance functions that we consider are profit or wealth, the Sharpe ratio and our recently proposed differential Sharpe ratio for online learning. In Moody & Wu (1997), we presented empirical results that demonstrate the advantages of reinforcement learning relative to supervised learning. Here we extend our previous work to compare Q-Learning to our Recurrent Reinforcement Learning (RRL) algorithm. We provide new simulation results that demonstrate the presence of predictability in the monthly S&P 500 Stock Index for the 25 year period 1970 through 1994, as well as a sensitivity analysis that provides economic insight into the trader's structure.	Oregon Grad Inst, CSE Dept, Portland, OR 97291 USA		Moody, J (corresponding author), Oregon Grad Inst, CSE Dept, POB 91000, Portland, OR 97291 USA.	moody@cse.ogi.edu; saffell@cse.ogi.edu						Crites RH, 1996, ADV NEUR IN, V8, P1017; John Christopher, 1989, THESIS; Moody J, 1998, J FORECASTING, V17, P441, DOI 10.1002/(SICI)1099-131X(1998090)17:5/6<441::AID-FOR707>3.0.CO;2-#; MOODY J, 1996, NNCM 96 C REC, P23; Neuneier R, 1996, ADV NEUR IN, V8, P952; SHARPE WF, 1966, J BUS, V39, P119, DOI 10.1086/294846; Tesauro G, 1989, NEURAL COMPUT, V1, P321, DOI 10.1162/neco.1989.1.3.321; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Zhang W, 1996, ADV NEUR IN, V8, P1024	9	12	12	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						917	923						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700129
C	Opper, M; Vivarelli, F		Kearns, MS; Solla, SA; Cohn, DA		Opper, M; Vivarelli, F			General bounds on Bayes errors for regression with Gaussian processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Based on a simple convexity lemma, we develop bounds for different types of Bayesian prediction errors for regression with Gaussian processes. The basic bounds are formulated for a fixed training set. Simpler expressions are obtained for sampling from an input distribution which equals the weight function of the covariance kernel, yielding asymptotically tight results. The results are compared with numerical experiments.	Aston Univ, Neural Comp Res Grp, Dept Elect Engn & Comp Sci, Birmingham B4 7ET, W Midlands, England	Aston University	Opper, M (corresponding author), Aston Univ, Neural Comp Res Grp, Dept Elect Engn & Comp Sci, Birmingham B4 7ET, W Midlands, England.							Barber D, 1997, ADV NEUR IN, V9, P340; GIBBS MN, 1997, VARIATIONAL GAUSSIAN; Haussler D, 1997, ANN STAT, V25, P2451; Neal RM., 1996, BAYESIAN LEARNING NE, P29; NEAL RM, 1997, CRGTR972; Peierls R, 1938, PHYS REV, V54, P918, DOI 10.1103/PhysRev.54.918; Williams CKI, 1996, ADV NEUR IN, V8, P514; WILLIAMS CKI, 1997, BAYESIAN CLASSIFICAT; ZHU H, 1997, NCRG97011	10	12	12	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						302	308						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700043
C	Vivarelli, F; Williams, CKI		Kearns, MS; Solla, SA; Cohn, DA		Vivarelli, F; Williams, CKI			Discovering hidden features with Gaussian processes regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					In Gaussian process regression the covariance between the outputs at input locations x and x' is usually assumed to depend on the distance (x - x')(T) W (x - x'), where W is a positive definite matrix. W is often taken to be diagonal, but if we allow W to be a general positive definite matrix which can be tuned on the basis of training data, then an eigen-analysis of W shows that we are effectively creating hidden features, where the dimensionality of the hidden-feature space is determined by the data. We demonstrate the superiority of predictions using the general matrix over those based on a diagonal matrix on two test problems.	Ctr Ric Ambientali Montecatini, I-48023 Marina di Ravenna, Italy		Vivarelli, F (corresponding author), Ctr Ric Ambientali Montecatini, Via Ciro Menotti 48, I-48023 Marina di Ravenna, Italy.							BREIMAN L, 1993, IEEE T INFORM THEORY, V39, P999, DOI 10.1109/18.256506; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; Neal RM, 1996, LECT NOTES STAT, V118; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Rasmussen C.E., 1996, THESIS U TORONTO; Whittle P, 1963, PREDICTION REGULATIO; Williams CKI, 1996, ADV NEUR IN, V8, P514; WILLIAMS PM, 1996, NEURAL COMPUTATION, V8	9	12	12	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						613	619						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700087
C	Weinshall, D; Jacobs, DW; Gdalyahu, Y		Kearns, MS; Solla, SA; Cohn, DA		Weinshall, D; Jacobs, DW; Gdalyahu, Y			Classification in non-metric spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					A key question in vision is how to represent our knowledge of previously encountered objects to classify new ones. The answer depends on how we determine the similarity of two objects. Similarity tells us how relevant each previously seen object is in determining the category to which a new object belongs. Here a dichotomy emerges. Complex notions of similarity appear necessary for cognitive models and applications, while simple notions of similarity form a tractable basis for current computational approaches to classification. We explore the nature of this dichotomy and why it calls for new approaches to well-studied problems in learning. We begin this process by demonstrating new computational methods for supervised learning that can handle complex notions of similarity. (1) We discuss how to implement parametric methods that represent a class by its mean when using non-metric similarity functions; and (2) We review non-parametric methods that we have developed using nearest neighbor classification in non-metric spaces. Point (2), and some of the background of our work have been described in more detail in [8].	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Weinshall, D (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							COX I, 1996, P ICPR, VC, P361; DASARATHY BV, 1994, IEEE T SYST MAN CYB, V24, P511, DOI 10.1109/21.278999; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; GDALYAHU Y, 1989, P DARPA IM UND WORKS; Haralick R.M., 1993, COMPUTER ROBOT VISIO, V2; HART PE, 1968, IEEE T INFORM THEORY, V14, P515, DOI 10.1109/TIT.1968.1054155; Huber P., 1981, ROBUST STAT; Jacobs DW, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P596, DOI 10.1109/ICCV.1998.710778; MEER P, 1991, INT J COMPUT VISION, V6, P59, DOI 10.1007/BF00127126; ROSCH E, 1975, COGNITIVE PSYCHOL, V7, P532, DOI 10.1016/0010-0285(75)90021-3; TOMEK I, 1976, IEEE T SYST MAN CYB, V6, P769, DOI 10.1109/tsmc.1976.4309452; TVERSKY A, 1977, PSYCHOL REV, V84, P327, DOI 10.1037/h0026750; WILLIAMS P, UNPUB PROTOTYPYES EX	13	12	12	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						838	844						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700118
C	Patel, GN; Holleman, JH; DeWeerth, SP		Jordan, MI; Kearns, MJ; Solla, SA		Patel, GN; Holleman, JH; DeWeerth, SP			Analog VLSI model of intersegmental coordination with nearest-neighbor coupling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	Advances in Neural Information Processing Systems		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We have a developed an analog VLSI system that models the coordination of neurobiological segmental oscillators. We have implemented and tested a system that consists of a chain of eleven pattern generating circuits that are synaptically coupled to their nearest neighbors. Each pattern generating circuit is implemented with two silicon Morris-Lecar neurons that are connected in a reciprocally inhibitory network. We discuss the mechanisms of oscillations in the two-cell network and explore system behavior based on isotropic and anisotropic coupling, and frequency gradients along the chain of oscillators.	Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Patel, GN (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.	girish@ece.gatech.edu; jeremy@ece.gatech.edu; steved@ece.gatech.edu							0	12	12	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						719	725						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700102
C	Bricolo, E; Poggio, T; Logothetis, N		Mozer, MC; Jordan, MI; Petsche, T		Bricolo, E; Poggio, T; Logothetis, N			3D object recognition: A model of view-tuned neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In 1990 Poggio and Edelman proposed a view-based model of object recognition that accounts for several psychophysical properties of certain recognition tasks. The model predicted the existence of view-tuned and view-invariant units, that were later found by Logothetis et al. (Logothetis et al., 1995) in IT cortex of monkeys trained with views of specific paperclip objects. The model, however, does not specify the inputs to the view-tuned units and their internal organization. In this paper we propose a model of these view-tuned units that is consistent with physiological data from single cell responses.			Bricolo, E (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E25-618,CAMBRIDGE,MA 02139, USA.		Logothetis, Nikos/U-9282-2019						0	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						41	47						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00006
C	Caruana, R; deSa, VR		Mozer, MC; Jordan, MI; Petsche, T		Caruana, R; deSa, VR			Promoting poor features to supervisors: Some inputs work better as outputs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				In supervised learning there is usually a clear distinction between inputs and outputs - inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra. outputs instead. This result is surprising since a feature used as an output is not used during testing.			Caruana, R (corresponding author), JPRC,PITTSBURGH,PA 15213, USA.								0	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						389	395						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00055
C	Doya, K		Mozer, MC; Jordan, MI; Petsche, T		Doya, K			Efficient nonlinear control with actor-tutor architecture	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A new reinforcement learning architecture for nonlinear control is proposed. A direct feedback controller, or the actor, is trained by a value-gradient based controller, or the tutor. This architecture enables both efficient use of the value function and simple computation for real-time implementation. Good performance was verified in multi-dimensional nonlinear control tasks using Gaussian soft-max networks.			Doya, K (corresponding author), ATR,HUMAN INFORMAT PROC RES LABS,2-2 HIKARIDAI,KYOTO 61902,JAPAN.								0	12	12	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1012	1018						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00142
C	Frey, BJ		Mozer, MC; Jordan, MI; Petsche, T		Frey, BJ			Continuous sigmoidal belief networks trained using slice sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Real-valued random hidden variables can be useful for modeling latent structure that explains correlations among observed variables. I propose a simple unit that adds zero-mean Gaussian noise to its input before passing it through a sigmoidal squashing function. Such units can produce a variety of useful behaviors, ranging from deterministic to binary stochastic to continuous stochastic. I show how ''slice sampling'' can be used for inference and learning in top-down networks of these units and demonstrate learning on two simple problems.			Frey, BJ (corresponding author), UNIV TORONTO,DEPT COMP SCI,6 KINGS COLL RD,TORONTO,ON M5S 1A4,CANADA.								0	12	14	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						452	458						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00064
C	Juels, A; Wattenberg, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Juels, A; Wattenberg, M			Stochastic hillclimbing as a baseline method for evaluating genetic algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF BERKELEY,DEPT COMP SCI,BERKELEY,CA	University of California System; University of California Berkeley									0	12	12	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						430	436						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00061
C	Koiran, P; Sontag, ED		Touretzky, DS; Mozer, MC; Hasselmo, ME		Koiran, P; Sontag, ED			Neural networks with quadratic VC dimension	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						ECOLE NORMALE SUPER LYON,LAB INFORMAT PARALLELISME,CNRS,F-69364 LYON 07,FRANCE	Centre National de la Recherche Scientifique (CNRS); Ecole Normale Superieure de Lyon (ENS de LYON)			Sontag, Eduardo D/J-4420-2012	Sontag, Eduardo D/0000-0001-8020-5783					0	12	13	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						197	203						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00028
C	Moore, AW; Schneider, J		Touretzky, DS; Mozer, MC; Hasselmo, ME		Moore, AW; Schneider, J			Memory-based stochastic optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,SCH COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	12	12	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1066	1072						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00150
C	ALSPECTOR, J; JAYAKUMAR, A; LUNA, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		ALSPECTOR, J; JAYAKUMAR, A; LUNA, S			EXPERIMENTAL EVALUATION OF LEARNING IN A NEURAL MICROSYSTEM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	12	12	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						871	878						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00107
C	HORN, D; USHER, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HORN, D; USHER, M			OSCILLATORY MODEL OF SHORT-TERM-MEMORY	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Usher, Marius/AFK-8639-2022						0	12	12	0	2	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						125	132						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00016
C	MOODY, J; YARVIN, N		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MOODY, J; YARVIN, N			NETWORKS WITH LEARNED UNIT RESPONSE FUNCTIONS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	12	12	0	3	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1048	1055						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00129
C	Luo, YW; Liu, P; Guan, T; Yu, JQ; Yang, Y		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Luo, Yawei; Liu, Ping; Guan, Tao; Yu, Junqing; Yang, Yi			Adversarial Style Mining for One-Shot Unsupervised Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We aim at the problem named One-Shot Unsupervised Domain Adaptation. Unlike traditional Unsupervised Domain Adaptation, it assumes that only one unlabeled target sample can be available when learning to adapt. This setting is realistic but more challenging, in which conventional adaptation approaches are prone to failure due to the scarce of unlabeled target data. To this end, we propose a novel Adversarial Style Mining approach, which combines the style transfer module and task-specific module into an adversarial manner. Specifically, the style transfer module iteratively searches for harder stylized images around the one-shot target sample according to the current learning state, leading the task model to explore the potential styles that are difficult to solve in the almost unseen target domain, thus boosting the adaptation performance in a data-scarce scenario. The adversarial learning framework makes the style transfer module and task-specific module benefit each other during the competition. Extensive experiments on both cross-domain classification and segmentation benchmarks verify that ASM achieves state-of-the-art adaptation performance under the challenging one-shot setting.	[Luo, Yawei; Guan, Tao; Yu, Junqing] Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China; [Luo, Yawei; Yang, Yi] Zhejiang Univ, CCAI, Hangzhou, Peoples R China; [Luo, Yawei] Baidu Res, Beijing, Peoples R China; [Liu, Ping; Yang, Yi] Univ Technol Sydney, ReLER, Sydney, NSW, Australia; [Liu, Ping] ASTAR, Inst High Performance Comp, Singapore, Singapore	Huazhong University of Science & Technology; Zhejiang University; Baidu; University of Technology Sydney; Agency for Science Technology & Research (A*STAR); A*STAR - Institute of High Performance Computing (IHPC)	Guan, T (corresponding author), Huazhong Univ Sci & Technol, Sch Comp Sci & Technol, Wuhan, Peoples R China.	qd_gt@hust.edu.cn	Yang, Yi/B-9273-2017; yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022; yang, yang/GWB-9426-2022; Luo, Yawei/AFK-9247-2022	Yang, Yi/0000-0002-0512-880X; 	National Key R&D Program of China [2020AAA0108800]; National Key RD program [2016YFB1000204]; CCF-Baidu Open Fund [CCF-BAIDU OF2020016]	National Key R&D Program of China; National Key RD program; CCF-Baidu Open Fund	This work is supported by National Key R&D Program of China under Grant No. 2020AAA0108800 and National Key R&D program under Grant No. 2016YFB1000204. This work is also partially supported by CCF-Baidu Open Fund under Grant No. CCF-BAIDU OF2020016. Most importantly, I'd like to express my deepest gratitude to my wife, Dan Zhou. The paper was written during her pregnancy. Many thanks to her understanding and support. Now I am looking forward to the birth of our lovely baby, just like I was expecting this paper to be accepted.	[Anonymous], 2017, INT C MACH LEARN ICM; [Anonymous], 2018, ECCV; [Anonymous], 2018, ICML; [Anonymous], 2018, IEEE T PATTERN ANAL; [Anonymous], 2017, ICCV; Ben- David S., 2010, MACHINE LEARNING; Benaim S., 2018, NEURIPS; Chen Y., 2018, CVPR; Cohen Tomer, 2019, ARXIV190901595; Cordts M., 2016, CVPR; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Dundar A., 2018, ARXIV PREPRINT ARXIV; Gatys LeonA., 2015, ARXIV, DOI 10.1167/16.12.326; Ghifary M., 2016, ECCV; Gong Rui, 2019, CVPR; Goodfellow Ian, 2014, 27 INT C NEURAL INFO; Hoffman J, 2016, FCNS WILD PIXELLEVEL; Huang X., 2017, ICCV; Huang Xun, 2018, ECCV; HULL JJ, 1994, IEEE T PATTERN ANAL; Jackson PT., 2018, STYLE AUGMENTATION D, V6, P101; Kang G, 2020, IEEE T PATTERN ANAL; Kang Guoliang, 2020, ADV NEURAL INFORM PR, V2, P8; LeCun Y., 1998, P IEEE; Li P., 2020, ADV NEURAL INFORM PR; Li Peike, 2020, P 28 ACM INT C MULT; Li Peilun, 2018, BMVC; Li Y., 2019, CVPR; Liu Ming-Yu, 2016, NEURIPS; Long Mingsheng., 2018, NEURIPS; Luo Y., 2019, CVPR; Luo YW, 2020, PATTERN RECOGN, V106, DOI 10.1016/j.patcog.2020.107451; Luo Yawei, 2019, ICCV; Motiian S., 2017, NEURIPS, P2; Netzer Y., 2011, P NIPS WORKSH DEEP L; Richter S. R., 2016, ECCV; Ros G., 2016, CVPR; Sankaranarayanan S., 2018, CVPR; Tsai Y.H., 2018, CVPR; Tzeng E., 2017, CVPR; van der Maaten Laurens, 2008, J MACHINE LEARNING R; Vu Tuan-Hung, 2019, CVPR; Yue Xiangyu, 2019, ICCV; Zhang Y., 2018, CVPR; Zheng Zhedong, 2020, IJCV; Zheng Zhedong, 2020, IJCAI; Zou Yang, 2019, ICCV	48	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000069
C	Wu, SF; Xiao, X; Ding, QG; Zhao, PL; Wei, Y; Huang, JZ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Wu, Sifan; Xiao, Xi; Ding, Qianggang; Zhao, Peilin; Wei, Ying; Huang, Junzhou			Adversarial Sparse Transformer for Time Series Forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Many approaches have been proposed for time series forecasting, in light of its significance in wide applications including business demand prediction. However, the existing methods suffer from two key limitations. Firstly, most point prediction models only predict an exact value of each time step without flexibility, which can hardly capture the stochasticity of data. Even probabilistic prediction using the likelihood estimation suffers these problems in the same way. Besides, most of them use the auto-regressive generative mode, where ground-truth is provided during training and replaced by the network's own one-step ahead output during inference, causing the error accumulation in inference. Thus they may fail to forecast time series for long time horizon due to the error accumulation. To solve these issues, in this paper, we propose a new time series forecasting model - Adversarial Sparse Transformer (AST), based on Generative Adversarial Networks (GANs). Specifically, AST adopts a Sparse Transformer as the generator to learn a sparse attention map for time series forecasting, and uses a discriminator to improve the prediction performance at a sequence level. Extensive experiments on several real-world datasets show the effectiveness and efficiency of our method.	[Wu, Sifan; Ding, Qianggang] Tsinghua Univ, Beijing, Peoples R China; [Xiao, Xi] Tsinghua Univ, Peng Cheng Lab, Beijing, Peoples R China; [Zhao, Peilin; Wei, Ying] Tencent AI Lab, Bellevue, WA 98004 USA; [Huang, Junzhou] Univ Texas Arlington, Tencent AI Lab, Arlington, TX 76019 USA	Tsinghua University; Tsinghua University; University of Texas System; University of Texas Arlington	Zhao, PL (corresponding author), Tencent AI Lab, Bellevue, WA 98004 USA.	wusf18@mails.tsinghua.edu.cn; xiaox@sz.tsinghua.edu.cn; dqg18@mails.tsinghua.edu.cn; masonzhao@tencent.com; judywei@tencent.com; jzhuang@uta.edu		Wei, Ying/0000-0003-1662-4443	National Natural Science Foundation of China [61972219]; National Natural Science Foundation of Guangdong Province [2018A030313422]; RD Program of Shenzhen [JCYJ20190813174403598, SGDX20190918101201696]; National Key Research and Development Program of China [2018YFB1800600, 2018YFB1800204]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); RD Program of Shenzhen; National Key Research and Development Program of China	This work is supported in part by the National Natural Science Foundation of China (61972219), the National Natural Science Foundation of Guangdong Province (2018A030313422), the RD Program of Shenzhen (JCYJ20190813174403598,SGDX20190918101201696), the National Key Research and Development Program of China (2018YFB1800600, 2018YFB1800204).	Bianchi F.M., 2017, RECURRENT NEURAL NET; Blondel Mathieu, 2018, ARXIV180509717; Box G.E.P., 2015, TIME SERIES ANAL FOR; Cinar YG, 2017, LECT NOTES COMPUT SC, V10638, P533, DOI 10.1007/978-3-319-70139-4_54; Correia GM, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P2174; Gasthaus J., 2017, DEEPAR PROBABILISTIC; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hyndman RJ, 2011, COMPUT STAT DATA AN, V55, P2579, DOI 10.1016/j.csda.2011.03.006; Hyndman RJ, 2008, SPRINGER SER STAT, P3; Jain Sarthak, 2019, ARXIV190210186, DOI [10.18653/v1/N19-1357, DOI 10.18653/V1/N19-1357]; Kingma D.P, P 3 INT C LEARNING R; Lai GK, 2018, ACM/SIGIR PROCEEDINGS 2018, P95, DOI 10.1145/3209978.3210006; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; Li S., 2019, ENHANCING LOCALITY B; Liu CH, 2016, AAAI CONF ARTIF INTE, P1867; Makhzani A., 2015, ARXIV151105644; Makridakis S, 2018, INT J FORECASTING, V34, P802, DOI 10.1016/j.ijforecast.2018.06.001; Martins AFT, 2016, PR MACH LEARN RES, V48; Mogren Olof, 2016, ARXIV161109904; Oord A.V.D., 2016, SSW; Peters Ben, 2019, ARXIV190505702; Qin Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2627; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; Rangapuram SS, 2018, ADV NEUR IN, V31; Song HA, 2018, AAAI CONF ARTIF INTE, P4091; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wen R., 2017, MULTIHORIZON QUANTIL; Xu B., 2015, ARXIV150500853, V1505, P853; Xu Ke, 2020, IJCAI; Yoon J, 2019, ADV NEUR IN, V32; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847	32	11	11	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000068
C	Allen-Zhu, Z; Li, YZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Allen-Zhu, Zeyuan; Li, Yuanzhi			Can SGD Learn Recurrent Neural Networks with Provable Generalization?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recurrent Neural Networks (RNNs) are among the most popular models in sequential data analysis. Yet, in the foundational PAC learning language, what concept class can it learn? Moreover, how can the same recurrent unit simultaneously learn functions from different input tokens to different output tokens, without affecting each other? Existing generalization bounds for RNN scale exponentially with the input length, significantly limiting their practical implications. In this paper, we show using the vanilla stochastic gradient descent (SGD), RNN can actually learn some notable concept class efficiently, meaning that both time and sample complexity scale polynornially in the input length (or almost polynomially, depending on the concept). This concept class at least includes functions where each output token is generated from inputs of earlier tokens using a smooth two-layer neural network.	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA; [Li, Yuanzhi] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; yuanzhil@andrew.cmu.edu	Li, Yuan/GXV-1310-2022					Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Allen-Zhu Zeyuan, 2019, NEURIPS; Arora S., 2017, INT C LEARNING REPRE; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Boob D., 2017, ARXIV171011241; Brutzkus A, 2017, PR MACH LEARN RES, V70; Chen Minshuo, 2019, GENERALIZATION BOUND; Dasgupta B, 1996, ADV NEUR IN, V8, P204; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ge Rong, 2017, ARXIV171100501; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hardt M, 2018, J MACH LEARN RES, V19; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Koiran P, 1998, DISCRETE APPL MATH, V86, P63, DOI 10.1016/S0166-218X(98)00014-6; Li Y., 2018, C LEARN THEOR, V75, P2; Liang Percy, 2016, CS229T STAT231 STAT; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; Ostmeyer Jared, 2018, NEUROCOMPUTING; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Salehinejad H, 2017, RECENT ADV RECURRENT, DOI DOI 10.48550/ARXIV.1801.01078; Soltanolkotabi M., 2017, ARXIV170704926; Soudry D., 2016, ABS160508361 CORR; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tian YD, 2017, PR MACH LEARN RES, V70; Xie Bo, 2016, ARXIV161103131; Zhang Jiong, 2018, ARXIV180309327; Zhong Kai, 2017, ARXIV170603175	31	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902001
C	Bassily, R; Feldman, V; Talwar, K; Thakurta, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bassily, Raef; Feldman, Vitaly; Talwar, Kunal; Thakurta, Abhradeep			Private Stochastic Convex Optimization with Optimal Rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study differentially private (DP) algorithms for stochastic convex optimization (SCO). In this problem the goal is to approximately minimize the population loss given i.i.d. samples from a distribution over convex and Lipschitz loss functions. A long line of existing work on private convex optimization focuses on the empirical loss and derives asymptotically tight bounds on the excess empirical loss. However a significant gap exists in the known bounds for the population loss. We show that, up to logarithmic factors, the optimal excess population loss for DP algorithms is equal to the larger of the optimal non-private excess population loss, and the optimal excess empirical loss of DP algorithms. This implies that, contrary to intuition based on private ERM, private SCO has asymptotically the same rate of 1/root n as non-private SCO in the parameter regime most common in practice. The best previous result in this setting gives rate of 1/n(1/4). Our approach builds on existing differentially private algorithms and relies on the analysis of algorithmic stability to ensure generalization.	[Bassily, Raef] Ohio State Univ, Columbus, OH 43210 USA; [Feldman, Vitaly; Talwar, Kunal; Thakurta, Abhradeep] Google Res Brain Team, Mountain View, CA USA; [Thakurta, Abhradeep] Univ Calif Santa Cruz, Santa Cruz, CA 95064 USA; [Bassily, Raef; Feldman, Vitaly; Talwar, Kunal; Thakurta, Abhradeep] Simons Inst Theory Comp, Berkeley, CA 94720 USA	University System of Ohio; Ohio State University; University of California System; University of California Santa Cruz	Bassily, R (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.; Bassily, R (corresponding author), Simons Inst Theory Comp, Berkeley, CA 94720 USA.	bassily.l@osu.edu; kunal@google.com			NSF [AF-1908281, SHF-1907715, TRIPODS+X-1839317, TRIPODS-1740850]; Google Faculty Research Award; OSU faculty start-up support	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); OSU faculty start-up support	We thank Adam Smith, Thomas Steinke and Jon Ullman for the insightful discussions of the problem at the early stages of this project. We are also grateful to Tomer Koren for bringing the Moreau-Yosida smoothing technique to our attention. R. Bassily's research is supported by NSF Awards AF-1908281, SHF-1907715, Google Faculty Research Award, and OSU faculty start-up support. A. Thakurta's research is supported by NSF Awards TRIPODS+X-1839317, AF-1908281, TRIPODS-1740850, and Google Faculty Research Award.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; [Anonymous], 2013, IEEE GLOB C SIGN INF; Bassily R., 2014, ARXIV14057085; Bassily R., 2019, ARXIV190809970; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; [Canl] Emmanuel Candes, 2011, LEC NOTES MATH, V301; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chaudhuri Kamalika, 2008, NIPS; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2006, EUROCRYPT; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, ACM S THEORY COMPUT, P117, DOI 10.1145/2746539.2746580; Feldman V, 2016, ADV NEURAL INFORM PR, V29, P3576; Feldman Vitaly, 2019, ARXIV190210710; Hardt M, 2015, ARXIV150901240; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Iyengar Roger, 2019, IEEE S P; Jain Prateek, 2012, 25 ANN C LEARN THEOR, P241; Jain Prateek, 2014, ICML; Kakade Sham M., 2008, NIPS; Kifer D., 2012, C LEARN THEOR, P25; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35; Talwar K., 2015, NIPS 2015 P 28 INT C, V2, P3025; Thakurta Abhradeep Guha, 2013, P 26 ANN C LEARN THE, P819; Ullman J, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P303, DOI 10.1145/2745754.2745755; Wang D, 2017, ASIA-PAC INT SYM ELE, P272, DOI 10.1109/APEMC.2017.7975481; Wu Xi, 2017, SIGMOD; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791	31	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902086
C	Crane, R; Roosta, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Crane, Rixon; Roosta, Fred			DINGO: Distributed Newton-Type Method for Gradient-Norm Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC ALGORITHMS	For optimization of a large sum of functions in a distributed computing environment, we present a novel communication efficient Newton-type algorithm that enjoys a variety of advantages over similar existing methods. Our algorithm, DINGO, is derived by optimization of the gradient's norm as a surrogate function. DINGO does not impose any specific form on the underlying functions and its application range extends far beyond convexity and smoothness. The underlying sub-problems of DINGO are simple linear least-squares, for which a plethora of efficient algorithms exist. DINGO involves a few hyper-parameters that are easy to tune and we theoretically show that a strict reduction in the surrogate objective is guaranteed, regardless of the selected hyper-parameters.	[Crane, Rixon; Roosta, Fred] Univ Queensland, Brisbane, Qld, Australia	University of Queensland	Crane, R (corresponding author), Univ Queensland, Brisbane, Qld, Australia.	r.crane@uq.edu.au; fred.roosta@uq.edu.au	Roosta, Fred/U-2926-2018	Roosta, Fred/0000-0002-6920-7072	Australian Research Council (ARC) Centre of Excellence for Mathematical & Statistical Frontiers (ACEMS); DARPA; ARC through a Discovery Early Career Researcher Award [DE180100923]	Australian Research Council (ARC) Centre of Excellence for Mathematical & Statistical Frontiers (ACEMS)(Australian Research Council); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ARC through a Discovery Early Career Researcher Award(Australian Research Council)	Both authors gratefully acknowledge the generous support by the Australian Research Council (ARC) Centre of Excellence for Mathematical & Statistical Frontiers (ACEMS). Fred Roosta was partially supported by DARPA as well as ARC through a Discovery Early Career Researcher Award (DE180100923). Part of this work was done while Fred Roosta was visiting the Simons Institute for the Theory of Computing.	Arjevani Y., 2017, MATH PROGRAM, P1; Beck A., 2017, MOS SIAM SERIES OPTI; Bekkerman R., 2012, SCALING MACHINE LEAR; BENISRAEL A, 1986, J AUST MATH SOC B, V28, P1, DOI 10.1017/S0334270000005142; Choi SCT, 2011, SIAM J SCI COMPUT, V33, P1810, DOI 10.1137/100787921; Fong DCL, 2011, SIAM J SCI COMPUT, V33, P2950, DOI 10.1137/10079687X; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; HOGAN E, 2013, ADV NEURAL INFORM PR, P315; Hubbard J.H., 2015, VECTOR CALCULUS LINE; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Mishra SK, 2008, NONCONVEX OPTIM, P1; Mohri M., 2018, FDN MACHINE LEARNING; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Penrose R., 1955, P CAMBRIDGE PHILOS S, V51, P406, DOI [10.1017/S0305004100030401, DOI 10.1017/S0305004100030401]; Roosta F., 2018, ARXIV181000303; Roosta-Khorasani F, 2014, ELECTRON T NUMER ANA, V42, P177; Roosta-Khorasani F, 2014, SIAM J SCI COMPUT, V36, pS3, DOI 10.1137/130922756; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Tutunov R, 2019, IEEE T AUTOMAT CONTR, V64, P3983, DOI 10.1109/TAC.2019.2907711; Wang S, 2018, ADV NEURAL INFORM PR, P2338; Xing HQ, 2016, INT WORKS EARTH OB; Ye Nan, 2017, MATRIX BOOK SERIES, V2	25	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901016
C	Dai, HJ; Li, CT; Coley, CW; Dai, B; Song, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dai, Hanjun; Li, Chengtao; Coley, Connor W.; Dai, Bo; Song, Le			Retrosynthesis Prediction with Conditional Graph Logic Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SEQUENCE	Retrosynthesis is one of the fundamental problems in organic chemistry. The task is to identify reactants that can be used to synthesize a specified product molecule. Recently, computer-aided retrosynthesis is finding renewed interest from both chemistry and computer science communities. Most existing approaches rely on template-based models that define subgraph matching rules, but whether or not a chemical reaction can proceed is not defined by hard decision rules. In this work, we propose a new approach to this task using the Conditional Graph Logic Network, a conditional graphical model built upon graph neural networks that learns when rules from reaction templates should be applied, implicitly considering whether the resulting reaction would be both chemically feasible and strategic. We also propose an efficient hierarchical sampling to alleviate the computation cost. While achieving a significant improvement of 8.1% over current state-of-the-art methods on the benchmark dataset, our model also offers interpretations for the prediction.	[Dai, Hanjun; Dai, Bo] Google Res, Brain Team, Mountain View, CA 94043 USA; [Li, Chengtao] Galixir Inc, Germantown, MD USA; [Coley, Connor W.] MIT, Cambridge, MA 02139 USA; [Dai, Hanjun; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Song, Le] Ant Financial, Hangzhou, Peoples R China	Google Incorporated; Massachusetts Institute of Technology (MIT); University System of Georgia; Georgia Institute of Technology	Dai, HJ (corresponding author), Google Res, Brain Team, Mountain View, CA 94043 USA.; Dai, HJ (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	hadai@google.com; chengtao.li@galixir.com; ccoley@mit.edu; bodai@google.com; lsong@cc.gatech.edu	Dai, Hanjun/AAQ-8943-2021		NSF [CDSE-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, IIS-1350983]	NSF(National Science Foundation (NSF))	We would like to thank anonymous reviewers for providing constructive feedbacks. This project was supported in part by NSF grants CDS&E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983 to L.S.	Baylon Javier L., ENHANCING RETROSYNTH, V59, P673; Bradshaw J., 2018, GENERATIVE MODEL ELE; Coley CW, 2017, ACS CENTRAL SCI, V3, P1237, DOI 10.1021/acscentsci.7b00355; Coley Connor W, J CHEM INFORM MODELI; Coley Connor W., MACHINE LEARNING COM, V51, P1281, DOI [10.1021/acs.accounts.8b00087., DOI 10.1021/ACS.ACCOUNTS.8B00087.]; Coley Connor W., GRAPH CONVOLUTIONAL, V10, P370, DOI [10.1039/C8SC042, DOI 10.1039/C8SC04228D]; COREY EJ, 1991, ANGEW CHEM INT EDIT, V30, P455, DOI 10.1002/anie.199104553; COREY EJ, 1969, SCIENCE, V166, P178, DOI 10.1126/science.166.3902.178; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Dai HJ, 2016, PR MACH LEARN RES, V48; Duvenaud David K, 2015, P NIPS; Gilmer J, 2017, PR MACH LEARN RES, V70; Guo RQ, 2016, JMLR WORKSH CONF PRO, V51, P482; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoonakker Frank, CONDENSED GRAPH REAC; Jin W., 2017, P 31 INT C NEURAL IN, P2607; Karpov P., 2019, TRANSFORMER MODEL RE; Kingma D.P, P 3 INT C LEARNING R; Lei T, 2017, PR MACH LEARN RES, V70; Li Yujia, 2015, ARXIV151105493; Liu BW, 2017, ACS CENTRAL SCI, V3, P1103, DOI 10.1021/acscentsci.7b00303; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schreck John S, 2019, ARXIV190106569; Schwaller P, 2018, CHEM SCI, V9, P6091, DOI 10.1039/c8sc02339e; Schwaller Philippe, MOL TRANSFORMER CHEM, DOI [10.26434/chemrxiv., DOI 10.26434/CHEMRXIV]; Segler MHS, 2018, NATURE, V555, P604, DOI 10.1038/nature25978; Segler Marwin H. S., NEURAL SYMBOLIC MACH, V23, P5966, DOI [10.1002/chem.201605499, DOI 10.1002/CHEM.201605499]; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szymkuc Sara, COMPUTER ASSISTED SY, V55, P5904, DOI [10.1002/anie.201506101., DOI 10.1002/ANIE.201506101]; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Velickovi Petar, 2017, ARXIV171010903; Xu K, 2018, IEEE INT SYMP CIRC S, DOI 10.1109/ISCAS.2018.8350934; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391	35	11	11	4	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900046
C	Du, SS; Luo, YP; Wang, RS; Zhang, HR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Du, Simon S.; Luo, Yuping; Wang, Ruosong; Zhang, Hanrui			Provably Efficient Q-learning with Function Approximation via Distribution Shift Error Checking Oracle	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Q-learning with function approximation is one of the most popular methods in reinforcement learning. Though the idea of using function approximation was proposed at least 60 years ago [27], even in the simplest setup, i.e, approximating Q-functions with linear functions, it is still an open problem how to design a provably efficient algorithm that learns a near-optimal policy. The key challenges are how to efficiently explore the state space and how to decide when to stop exploring in conjunction with the function approximation scheme. The current paper presents a provably efficient algorithm for Q-learning with linear function approximation. Under certain regularity assumptions, our algorithm, Difference Maximization Q-learning (DMQ), combined with linear function approximation, returns a near-optimal policy using polynomial number of trajectories. Our algorithm introduces a new notion, the Distribution Shift Error Checking (DSEC) oracle. This oracle tests whether there exists a function in the function class that predicts well on a distribution D-1, but predicts poorly on another distribution D-2, where D-1 and D-2 are distributions over states induced by two different exploration policies. For the linear function class, this oracle is equivalent to solving a top eigenvalue problem. We believe our algorithmic insights, especially the DSEC oracle, are also useful in designing and analyzing reinforcement learning algorithms with general function approximation.	[Du, Simon S.] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Luo, Yuping] Princeton Univ, Princeton, NJ 08544 USA; [Wang, Ruosong] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Zhang, Hanrui] Duke Univ, Durham, NC 27706 USA	Institute for Advanced Study - USA; Princeton University; Carnegie Mellon University; Duke University	Du, SS (corresponding author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.	ssdu@ias.edu; yupingl@cs.princeton.edu; ruosongw@andrew.cmu.edu; hrzhang@cs.duke.edu						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Agrawal Shipra, 2017, NIPS; [Anonymous], [No title captured]; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Audibert J.-Y., 2010, COLT 23 C LEARN THEO, P13; Azizzadenesheli  Kamyar, 2018, ARXIV180204412; Bertsekas Dimitri P, 1996, NEURODYNAMIC PROGRAM, V5; Dani Varsha, 2008, C LEARN THEOR; Dann C., 2017, ADV NEURAL INFORM PR, P5717; Dann Christoph, 2018, ARXIV180300606; Du Simon S, 2019, ARXIV190109018; Everitt B.S., 2002, CAMBRIDGE DICT STAT; Fortunato M., 2018, P INT C LEARN REPR, P1; Hsu D., 2012, JMLR WORKSHOP C P, V23; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jiang N, 2017, PR MACH LEARN RES, V70; Jin C., 2018, ADV NEURAL INFORM PR, P4863; Kakade S., 2018, VARIANCE REDUCTION M; Kakade Sham M., 2003, THESIS; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Krishnamurthy A., 2016, ADV NEURAL INFORM PR, P1840; Lipton Zachary Chase, 2018, AAAI; Melo FS, 2007, LECT NOTES COMPUT SC, V4539, P308, DOI 10.1007/978-3-540-72927-3_23; Osband I, 2016, PR MACH LEARN RES, V48; Pazis Jason, 2013, P ANN AAAI C ARTIFIC, P774; Simchowitz Max, 2019, NONASYMPTOTIC GAP DE; Song Z., 2019, ARXIV PREPRINT ARXIV; Strehl A.L., 2006, ICML, P881, DOI [10.1145/1143844.1143955, DOI 10.1145/1143844.1143955]; Sun W., 2018, ARXIV181108540; Sutton Richard S., 1999, EUROCOLT; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Vu VQ, 2013, ADV NEURAL INFORM PR, V26; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Wen Z., 2013, ADV NEURAL INFORM PR; Yang Lin F, 2019, ARXIV190501576; Zanette Andrea, 2019, ARXIV190100210; Zou S., 2019, ARXIV190202234CSSTAT	39	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308012
C	Eom, C; Ham, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Eom, Chanho; Ham, Bumsub			Learning Disentangled Representation for Robust Person Re-identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We address the problem of person re-identification (reID), that is, retrieving person images from a large dataset, given a query image of the person of interest. A key challenge is to learn person representations robust to intra-class variations, as different persons can have the same attribute and the same person's appearance looks different with viewpoint changes. Recent reID methods focus on learning discriminative features but robust to only a particular factor of variations (e.g., human pose), which requires corresponding supervisory signals (e.g., pose annotations). To tackle this problem, we propose to disentangle identity-related and -unrelated features from person images. Identity-related features contain information useful for specifying a particular person (e.g., clothing), while identity-unrelated ones hold other factors (e.g., human pose, scale changes). To this end, we introduce a new generative adversarial network, dubbed identity shuffle GAN (IS-GAN), that factorizes these features using identification labels without any auxiliary information. We also propose an identity-shuffling technique to regularize the disentangled features. Experimental results demonstrate the effectiveness of IS-GAN, significantly outperforming the state of the art on standard reID benchmarks including the Market-1501, CUHK03 and DukeMTMC-reID.	[Eom, Chanho; Ham, Bumsub] Yonsei Univ, Sch Elect & Elect Engn, Seoul, South Korea	Yonsei University	Ham, B (corresponding author), Yonsei Univ, Sch Elect & Elect Engn, Seoul, South Korea.	cheom@yonsei.ac.kr; bumsub.ham@yonsei.ac.kr			RAMP;D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) - Ministry of Science and ICT [NRF-2018M3E3A1057289]	RAMP;D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) - Ministry of Science and ICT(National Research Foundation of Korea)	This research was supported by R&D program for Advanced Integrated-intelligence for Identification (AIID) through the National Research Foundation of KOREA(NRF) funded by Ministry of Science and ICT (NRF-2018M3E3A1057289).	[Anonymous], 2017, NIPS; Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702; Chang XB, 2018, PROC CVPR IEEE, P2109, DOI 10.1109/CVPR.2018.00225; Chen WH, 2017, PROC CVPR IEEE, P1320, DOI 10.1109/CVPR.2017.145; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Fu Y, 2019, AAAI CONF ARTIF INTE, P8287; Ge YX, 2018, ADV NEUR IN, V31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hermans Alexander, 2017, ARXIV170307737; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Huang Xun, 2018, ECCV; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kalayeh MM, 2018, PROC CVPR IEEE, P1062, DOI 10.1109/CVPR.2018.00117; Kingma D.P, P 3 INT C LEARNING R; Kostinger Martin, 2012, CVPR, DOI DOI 10.1109/CVPR.2012.6247939; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee Hsin-Ying, 2018, ECCV; Li DW, 2017, PROC CVPR IEEE, P7398, DOI 10.1109/CVPR.2017.782; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Li YK, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P211, DOI 10.1145/3240508.3240551; Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Liu J., 2018, ARXIV181111510V1; Liu XH, 2017, IEEE I CONF COMP VIS, P350, DOI 10.1109/ICCV.2017.46; Lu Boyu, 2019, CVPR; Maas A.L., 2013, P ICML, V30, P3, DOI DOI 10.1016/0010-0277(84)90022-2; Odena A, 2017, PR MACH LEARN RES, V70; Qian XL, 2018, LECT NOTES COMPUT SC, V11213, P661, DOI 10.1007/978-3-030-01240-3_40; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ristani E, 2016, LECT NOTES COMPUT SC, V9914, P17, DOI 10.1007/978-3-319-48881-3_2; Shen YT, 2018, LECT NOTES COMPUT SC, V11219, P508, DOI 10.1007/978-3-030-01267-0_30; Shen YJ, 2018, PROC CVPR IEEE, P821, DOI 10.1109/CVPR.2018.00092; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Su C, 2017, IEEE I CONF COMP VIS, P3980, DOI 10.1109/ICCV.2017.427; Suh Y, 2018, LECT NOTES COMPUT SC, V11218, P418, DOI 10.1007/978-3-030-01264-9_25; Sun YF, 2018, LECT NOTES COMPUT SC, V11208, P501, DOI 10.1007/978-3-030-01225-0_30; Sun YF, 2017, IEEE I CONF COMP VIS, P3820, DOI 10.1109/ICCV.2017.410; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tran L., 2017, CVPR; Ulyanov D., 2016, ARXIV160708022; Wang GS, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P274, DOI 10.1145/3240508.3240552; Wang YL, 2018, PROC CVPR IEEE, P8906, DOI 10.1109/CVPR.2018.00928; Wei LH, 2018, PROC CVPR IEEE, P79, DOI 10.1109/CVPR.2018.00016; Yu LC, 2018, PROC CVPR IEEE, P1307, DOI 10.1109/CVPR.2018.00142; Zhang Xuan, 2017, ARXIV171108184; Zhao LM, 2017, IEEE I CONF COMP VIS, P3239, DOI 10.1109/ICCV.2017.349; Zheng Liang, 2015, ICCV, V1, P7; Zheng Liang, 2016, ARXIV161002984; Zheng ZD, 2019, PROC CVPR IEEE, P2133, DOI [10.1109/CVPR.2019.00224, 10.1109/CVPR.2019.01247]; Zheng ZD, 2017, IEEE I CONF COMP VIS, P3774, DOI 10.1109/ICCV.2017.405; Zhong Z, 2017, PROC CVPR IEEE, P3652, DOI 10.1109/CVPR.2017.389; Zhu Jun-Yan, 2017, ICCV; Zhun Zhong, 2017, ARXIV PREPRINT ARXIV	56	11	11	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305031
C	Golatkar, A; Achille, A; Soatto, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Golatkar, Aditya; Achille, Alessandro; Soatto, Stefano			Time Matters in Regularizing Deep Networks: Weight Decay and Data Augmentation Affect Early Learning Dynamics, Matter Little Near Convergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Regularization is typically understood as improving generalization by altering the landscape of local extrema to which the model eventually converges. Deep neural networks (DNNs), however, challenge this view: We show that removing regularization after an initial transient period has little effect on generalization, even if the final loss landscape is the same as if there had been no regularization. In some cases, generalization even improves after interrupting regularization. Conversely, if regularization is applied only after the initial transient, it has no effect on the final solution, whose generalization gap is as bad as if regularization never happened. This suggests that what matters for training deep networks is not just whether or how, but when to regularize. The phenomena we observe are manifest in different datasets (CIFAR-10, CIFAR-100, SVHN, ImageNet), different architectures (ResNet-18, All-CNN), different regularization methods (weight decay, data augmentation, mixup), different learning rate schedules (exponential, piece-wise constant). They collectively suggest that there is a "critical period" for regularizing deep networks that is decisive of the final performance. More analysis should, therefore, focus on the transient rather than asymptotic behavior of learning.	[Golatkar, Aditya; Achille, Alessandro; Soatto, Stefano] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Golatkar, A (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.	aditya29@cs.ucla.edu; achille@cs.ucla.edu; soatto@cs.ucla.edu			ARO [W911NF-15-1-0564]; ONR [N00014-19-1-2066]	ARO; ONR(Office of Naval Research)	We would like to thank the anonymous reviewers for their feedback and suggestions. This work is supported by ARO W911NF-15-1-0564 and ONR N00014-19-1-2066.	Achille Alessandro, 2019, INT C LEARN REPR; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Bos S, 1996, IEEE IJCNN, P241, DOI 10.1109/ICNN.1996.548898; Chaudhari P, 2016, ARXIV161101838; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dinh L, 2017, PR MACH LEARN RES, V70; Fisher RA, 1925, P CAMB PHILOS SOC, V22, P700, DOI 10.1017/S0305004100009580; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henaff M, 2016, PR MACH LEARN RES, V48; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Hong B.-W., 2017, ARXIV170503350; Ioffe S, 2016, P INT C MACH LEARN, P448; Jastrzebski Stanislaw, 2017, ARXIV171104623; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Krizhevsky A, 2009, LEARNING MULTIPLE LA; KROGH A, 1992, ADV NEUR IN, V4, P950; Li H, 2018, ADV NEUR IN, V31; Loshchilov I., 2017, P INT C LEARNING REP; Loshchilov Ilya, 2019, INT C LEARN REPR; Martens James, 2014, NEW INSIGHTS PERSPEC; Mobahi H, 2015, AAAI CONF ARTIF INTE, P1205; Neelakantan A., 2015, ARXIV151106807; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Neyshabur Behnam, 2018, INT C LEARN REPR; Smith LN, 2017, IEEE WINT CONF APPL, P464, DOI 10.1109/WACV.2017.58; Springenberg J.T., 2014, ARXIV14126806; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Vapnik, 2000, NATURE STAT LEARNING, P267, DOI [10.1007/978-1-4757-3264-1_9, DOI 10.1007/978-1-4757-3264-1_9]; Wilson AC, 2017, ADV NEUR IN, V30; Zhang Guojun, 2019, INT C LEARN REPR, P8; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31	38	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902032
C	Grnarova, P; Levy, KY; Lucchi, A; Perraudin, N; Goodfellow, I; Hofmann, T; Krause, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Grnarova, Paulina; Levy, Kfir Y.; Lucchi, Aurelien; Perraudin, Nathanael; Goodfellow, Ian; Hofmann, Thomas; Krause, Andreas			A Domain Agnostic Measure for Monitoring and Evaluating GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generative Adversarial Networks (GANs) have shown remarkable results in modeling complex distributions, but their evaluation remains an unsettled issue. Evaluations are essential for: (i) relative assessment of different models and (ii) monitoring the progress of a single model throughout training. The latter cannot be determined by simply inspecting the generator and discriminator loss curves as they behave non-intuitively. We leverage the notion of duality gap from game theory to propose a measure that addresses both (i) and (ii) at a low computational cost. Extensive experiments show the effectiveness of this measure to rank different GAN models and capture the typical GAN failure scenarios, including mode collapse and non-convergent behaviours. This evaluation metric also provides meaningful monitoring on the progression of the loss during training. It highly correlates with FID on natural image datasets, and with domain specific scores for text, sound and cosmology data where FID is not directly suitable. In particular, our proposed metric requires no labels or a pretrained classifier, making it domain agnostic.	[Grnarova, Paulina; Lucchi, Aurelien; Hofmann, Thomas; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland; [Levy, Kfir Y.] Technion Israel Inst Technol, Haifa, Israel; [Perraudin, Nathanael] Swiss Data Sci Ctr, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Technion Israel Institute of Technology	Grnarova, P (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	paulina.grnarova@inf.ethz.ch	Perraudin, Nathanaël/AAC-4007-2019	Perraudin, Nathanaël/0000-0001-8285-1308; Lucchi, Aurelien/0000-0001-7015-2710; Levy, Kfir Yehuda/0000-0003-1236-2626; Krause, Andreas/0000-0001-7260-9673				Arjovsky M, 2017, PR MACH LEARN RES, V70; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Chen X., 2018, P 6 INT C LEARN REPR; Daskalakis C., 2018, P INT C LEARN REPR; Furui S, 2012, IEEE SIGNAL PROC MAG, V29, P16, DOI 10.1109/MSP.2012.2209906; Gemici Mevlana, 2018, ARXIV180509575; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova Paulina, 2018, INT C LEARN REPR ICL; Gulrajani I, 2017, P NIPS 2017; Hensel M, 2017, ADV NEUR IN, V30; Hsieh Y.-P., 2018, ARXIV181102002; Komodakis N, 2015, IEEE SIGNAL PROC MAG, V32, P31, DOI 10.1109/MSP.2014.2377273; Krizhevsky A., 2010, THE CIFAR 10 DATASET; Kurach K., 2018, ARXIV180704720; Li Y., 2017, ADV NEURAL INFORM PR, P5606; LopezPaz D., 2016, ARXIV161006545; Marafioti Andr6s, 2019, ARXIV190204072; Mescheder L, 2018, PR MACH LEARN RES, V80; Metz Luke, 2016, ARXIV161102163; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nowozin S, 2016, ADV NEUR IN, V29; Oliehoek F. A., 2018, LOCAL NASH EQU UNPUB; Olsson C, 2018, ARXIV180804888; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rodriguez Andres C., 2018, ARXIV180109070; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T, 2016, ADV NEUR IN, V29; Theis Lucas, 2015, ARXIV151101844; Vincent P., 2019, ICLR; Yu LQ, 2017, AAAI CONF ARTIF INTE, P66; Zhu YM, 2018, ACM/SIGIR PROCEEDINGS 2018, P1097, DOI 10.1145/3209978.3210080	34	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903068
C	Liu, XQ; Si, S; Zhu, XJ; Li, Y; Hsieh, CJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Xuanqing; Si, Si; Zhu, Xiaojin; Li, Yang; Hsieh, Cho-Jui			A Unified Framework for Data Poisoning Attack to Graph-based Semi-supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we proposed a general framework for data poisoning attacks to graph-based semi-supervised learning (G-SSL). In this framework, we first unify different tasks, goals and constraints into a single formula for data poisoning attack in G-SSL, then we propose two specialized algorithms to efficiently solve two important cases - poisoning regression tasks under l(2)-norm constraint and classification tasks under l(0)-norm constraint. In the former case, we transform it into a non-convex trust region problem and show that our gradient-based algorithm with delicate initialization and update scheme finds the (globally) optimal perturbation. For the latter case, although it is an NP-hard integer programming problem, we propose a probabilistic solver that works much better than the classical greedy method. Lastly, we test our framework on real datasets and evaluate the robustness of G-SSL algorithms. For instance, on the MNIST binary classification problem (50000 training data with 50 labeled), flipping two labeled data is enough to make the model perform like random guess (around 50% error).	[Liu, Xuanqing; Hsieh, Cho-Jui] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA; [Si, Si; Li, Yang] Google Res, Mountain View, CA USA; [Zhu, Xiaojin] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA	University of California System; University of California Los Angeles; Google Incorporated; University of Wisconsin System; University of Wisconsin Madison	Liu, XQ (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA.	xqliu@cs.ucla.edu; sisidaisy@google.com; jerryzhu@cs.wisc.edu; liyang@google.com; chohsieh@cs.ucla.edu			NSF [IIS-1719097, 1545481, 1561512, 1623605, 1704117, 1836978]; Intel faculty award; Google Cloud; MADLab AF COE [FA9550-18-1-0166]; Nvidia	NSF(National Science Foundation (NSF)); Intel faculty award; Google Cloud(Google Incorporated); MADLab AF COE; Nvidia	Xuanqing Liu and Cho-Jui Hsieh acknowledge the support of NSF IIS-1719097, Intel faculty award, Google Cloud and Nvidia. Zhu acknowledges NSF 1545481, 1561512, 1623605, 1704117, 1836978 and the MADLab AF COE FA9550-18-1-0166.	Athalye A., 2018, P 35 INT C MACH LEAR; Ba J., 2017, P 3 INT C LEARN REPR; Belkin M, 2006, J MACH LEARN RES, V7, P2399; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; CARMON Y, 2016, ARXIV161200547; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Chen X., 2017, ARXIV171205526; Cheng M., 2019, P ICLR; Cheng Minhao, 2018, ARXIV180301128; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; Dal Hanjun, 2018, ICML; Figurnov Michael, 2018, ARXIV180508498; Goodfellow I. J., 2015, ICLR; Hazan E, 2016, MATH PROGRAM, V158, P363, DOI 10.1007/s10107-015-0933-y; Koh Pang Wei, 2017, ARXIV170304730; Li B., 2016, ADV NEURAL INFORM PR, P1885; Mei S, 2015, AAAI; Papernot N, 2016, IEEE MILIT COMMUN C, P49, DOI 10.1109/MILCOM.2016.7795300; Sindhwani Vikas, 2005, LINEAR MANIFOLD REGU; Speriosu M, 2011, P 1 WORKSHOP UNSUPER, P53, DOI DOI 10.1017/CB09781107415324.004; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Tsipras D., 2018, ROBUSTNESS MAY BE OD; Tucker G., 2017, NIPS; WANG X., 2018, ARXIV181010751; Wang Y, 2018, ARXIV180808994; Zhang P, 2015, SCI REP-UK, V5, DOI 10.1038/srep12339; Zhao Mengchen, 2018, DATA POISONING ATTAC; Zhu Xiaojin., 2003, P ICLR, P912; Zhu Xiaojin, 2002, CMU CALD TECH REPORT; Ziigner Daniel, 2018, KDD; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	34	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901041
C	Ma, JQ; Tang, WJ; Zhu, J; Mei, QZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Jiaqi; Tang, Weijing; Zhu, Ji; Mei, Qiaozhu			A Flexible Generative Framework for Graph-based Semi-supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider a family of problems that are concerned about making predictions for the majority of unlabeled, graph-structured data samples based on a small proportion of labeled samples. Relational information among the data samples, often encoded in the graph/network structure, is shown to be helpful for these semi-supervised learning tasks. However, conventional graph-based regularization methods and recent graph neural networks do not fully leverage the interrelations between the features, the graph, and the labels. In this work, we propose a flexible generative framework for graph-based semi-supervised learning, which approaches the joint distribution of the node features, labels, and the graph structure. Borrowing insights from random graph models in network science literature, this joint distribution can be instantiated using various distribution families. For the inference of missing labels, we exploit recent advances of scalable variational inference techniques to approximate the Bayesian posterior. We conduct thorough experiments on benchmark datasets for graph-based semi-supervised learning. Results show that the proposed methods outperform the state-of-the-art models in most settings.	[Ma, Jiaqi; Mei, Qiaozhu] Univ Michigan, Sch Informat, Ann Arbor, MI 48109 USA; [Tang, Weijing; Zhu, Ji] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; [Mei, Qiaozhu] Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan; University of Michigan System; University of Michigan; University of Michigan System; University of Michigan	Ma, JQ (corresponding author), Univ Michigan, Sch Informat, Ann Arbor, MI 48109 USA.	jiaqima@umich.edu; weijtang@umich.edu; jizhu@umich.edu; qmei@umich.edu	zhu, ji/HDM-1538-2022	Mei, Qiaozhu/0000-0002-8640-1942	National Science Foundation [1633370, 1620319]	National Science Foundation(National Science Foundation (NSF))	This work was in part supported by the National Science Foundation under grant numbers 1633370 and 1620319.	Belkin M, 2006, J MACH LEARN RES, V7, P2399; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; De Cao Nicola, 2018, ARXIV180511973; Fey M., 2019, ICLR WORKSH REPR LEA; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Kipf TN, 2016, P INT C LEARN REPR; LIU B, 2019, THESIS; Mei Q., 2008, P 31 ANN INT ACM SIG, P611; Mikolov T., 2013, EFFICIENT ESTIMATION, P1, DOI DOI 10.48550/ARXIV.1301.3781; Newman M., 2010, NETWORKS INTRO, DOI [DOI 10.1093/ACPROF:OSO/9780199206650.001.0001, 10.1162/artl_r_00062., 10.1162/artl_r_00062]; Ng Y. C., 2018, ADV NEURAL INFORM PR, P1690; Qu M, 2019, PR MACH LEARN RES, V97; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Velickovic P., 2017, ARXIV MACHINE LEARNI, P1; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Yang Z, 2016, PR MACH LEARN RES, V48; Zhang Yingxue, 2018, ARXIV181111103; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhu X, 2003, INT C MACH LEARN, P912	26	11	11	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303029
C	Oh, C; Tomczak, JM; Gavves, E; Welling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Oh, Changyong; Tomczak, Jakub M.; Gavves, Efstratios; Welling, Max			Combinatorial Bayesian Optimization using the Graph Cartesian Product	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper focuses on Bayesian Optimization (BO) for objectives on combinatorial search spaces, including ordinal and categorical variables. Despite the abundance of potential applications of Combinatorial BO, including chipset configuration search and neural architecture search, only a handful of methods have been proposed. We introduce COMBO, a new Gaussian Process (GP) BO. COMBO quantifies "smoothness" of functions on combinatorial search spaces by utilizing a combinatorial graph. The vertex set of the combinatorial graph consists of all possible joint assignments of the variables, while edges are constructed using the graph Cartesian product of the sub-graphs that represent the individual variables. On this combinatorial graph, we propose an ARD diffusion kernel with which the GP is able to model high-order interactions between variables leading to better performance. Moreover, using the Horseshoe prior for the scale parameter in the ARD diffusion kernel results in an effective variable selection procedure, making COMBO suitable for high dimensional problems. Computationally, in COMBO the graph Cartesian product allows the Graph Fourier Transform calculation to scale linearly instead of exponentially.We validate COMBO in a wide array of realistic benchmarks, including weighted maximum satisfiability problems and neural architecture search. COMBO outperforms consistently the latest state-of-the-art while maintaining computational and statistical efficiency.	[Oh, Changyong; Gavves, Efstratios; Welling, Max] Univ Amsterdam, Amsterdam, Netherlands; [Tomczak, Jakub M.; Welling, Max] Qualcomm AI Res, San Diego, CA USA; [Welling, Max] CIFAR, Toronto, ON, Canada	University of Amsterdam; Canadian Institute for Advanced Research (CIFAR)	Oh, C (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	C.Oh@uva.nl; jtomczak@qti.qualcomm.com; egavves@uva.nl; m.welling@uva.nl	Tomczak, Jakub/ABB-7948-2020	Gavves, Efstratios/0000-0001-8947-1332				[Anonymous], 2017, P ADV NEURAL INFORM; Bergstra J, 2013, 30 INT C MACH LEARN; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Brochu E., 2010, ARXIV10122599; Carvalho C. M., 2009, J MACHINE LEARNING R, V5, P73; Carvalho CM, 2010, BIOMETRIKA, V97, P465, DOI 10.1093/biomet/asq017; Davis L, 1991, HDB GENETIC ALGORITH, DOI DOI 10.1.1.87.3586; Elsken T., 2018, J MACH LEARN RES; Fan R. K. Chung, 1996, SPECTRAL GRAPH THEOR, V92; Frazier P.I., 2018, ARXIV, DOI 10.1287/educ.2018.0188; Freitas AA, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P371, DOI 10.1007/978-0-387-09823-4_19; Garnett R, 2010, PROCEEDINGS OF THE 9TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P209, DOI 10.1145/1791212.1791238; Garrido-Merchan E.., 2018, DEALING CATEGORICAL; Hammack R.H., 2011, HAND BOOK PRODUCT GR, V2; HANSEN P, 1990, COMPUTING, V44, P279, DOI 10.1007/BF02241270; Haussler D, 1999, TECHNICAL REPORT; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hu YJ, 2010, WINT SIMUL C PROC, P2678, DOI 10.1109/WSC.2010.5678963; Ignatiev, RC2 2018 MAXSAT EVAL, P13; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kandasamy K., 2016, ADV NEURAL INFORM PR, P992; Kandasamy K., 2018, 32 C NEURAL INFORM P, P2016; Kingma D.P, P 3 INT C LEARNING R; Kondor R.I., 2002, P 19 INT C MACHINE L, P315; Krizhevsky A., 2009, LEARNING MULTIPLE LA; MacKay DJC, 1994, ASHRAE T, V100, P1053; Malkomes G., 2016, P WORKSH AUT MACH LE, P2900; Mokus J., 1975, LECT NOTES COMPUTER, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55, DOI 10.1007/978-3-662-38527-2_55, 10.1007/3-540-07165-2, DOI 10.1007/3-540-07165-2]; Murray I., 2010, ADV NEURAL INFORM PR, V23, P1732, DOI DOI 10.5555/2997046.2997089; Neal R. M., 2012, BAYESIAN LEARNING NE; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Oh C., 2019, ICML WORKSH LEARN RE; Oh C., 2018, ARXIV180601619; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Real E., 2018, ARXIV180201548; Resende M. G. C., 1997, Satisfiability Problem: Theory and Applications. DIMACS Workshop, P393; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Spears W. M., 1993, CLIQUES COLORING SAT, V26, P533, DOI [10.1090/dimacs/026/26, DOI 10.1090/DIMACS/026/26]; Wang ZY, 2016, J ARTIF INTELL RES, V55, P361, DOI 10.1613/jair.4806; Wilson A, 2014, J MACH LEARN RES, V15, P253; Wistuba M., 2019, ARXIV190501392	49	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302086
C	Ozair, S; Lynch, C; Bengio, Y; van den Oord, A; Levine, S; Sermanet, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ozair, Sherjil; Lynch, Corey; Bengio, Yoshua; van den Oord, Aaron; Levine, Sergey; Sermanet, Pierre			Wasserstein Dependency Measure for Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFORMATION	Mutual information maximization has emerged as a powerful learning objective for unsupervised representation learning obtaining state-of-the-art performance in applications such as object recognition, speech recognition, and reinforcement learning. However, such approaches are fundamentally limited since a tight lower bound on mutual information requires sample size exponential in the mutual information. This limits the applicability of these approaches for prediction tasks with high mutual information, such as in video understanding or reinforcement learning. In these settings, such techniques are prone to overfit, both in theory and in practice, and capture only a few of the relevant factors of variation. This leads to incomplete representations that are not optimal for downstream tasks. In this work, we empirically demonstrate that mutual information-based representation learning approaches do fail to learn complete representations on a number of designed and real-world tasks. To mitigate these problems we introduce the Wasserstein dependency measure, which learns more complete representations by using the Wasserstein distance instead of the KL divergence in the mutual information estimator. We show that a practical approximation to this theoretically motivated solution, constructed using Lipschitz constraint techniques from the GAN literature, achieves substantially improved results on tasks where incomplete representations are a major challenge.	[Ozair, Sherjil; Bengio, Yoshua] Univ Montreal, Mila, Montreal, PQ, Canada; [Lynch, Corey; Levine, Sergey; Sermanet, Pierre] Google Brain, Mountain View, CA USA; [van den Oord, Aaron] Deepmind, London, England	Universite de Montreal; Google Incorporated	Ozair, S (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.							Alain Guillaume, 2016, ARXIV161001644; Anil C, 2018, ARXIV181105381; Arjovsky M., 2017, ARXIV170107875; ATAL BS, 1970, BELL SYST TECH J, V49, P1973, DOI 10.1002/j.1538-7305.1970.tb04297.x; Aytar Y, 2018, ADV NEUR IN, V31; BECKER S, 1992, NATURE, V355, P161, DOI 10.1038/355161a0; Belghazi MI, 2018, PR MACH LEARN RES, V80; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Brock Andrew, 2018, ARXIV180911096; Clark A, 2013, BEHAV BRAIN SCI, V36, P181, DOI 10.1017/S0140525X12000477; Crooks Gavin E, 2017, MEASURES ENTROPY INF; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dinh L, 2016, ARXIV PREPRINT ARXIV; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Doersch Carl, MULTITASK SELF SUPER; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Dwibedi Debidatta, 2018, 2018 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), P1577, DOI 10.1109/IROS.2018.8593951; ELIAS P, 1955, IRE T INFORM THEOR, V1, P16, DOI 10.1109/TIT.1955.1055126; Friston KJ, 2009, PHILOS T R SOC B, V364, P1211, DOI 10.1098/rstb.2008.0300; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Hjelm R Devon, 2019, INT C LEARN REPR; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kim Hyoungseok, 2018, ARXIV181001176; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; McAllester David, 2018, ARXIV181104251; Nemenman I, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.056111; Neyshabur Behnam, 2017, ARXIV170709564; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Odena A, 2017, PR MACH LEARN RES, V70; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Poole B, 2019, PR MACH LEARN RES, V97; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Sermanet P, 2017, IEEE COMPUT SOC CONF, P486, DOI 10.1109/CVPRW.2017.69; Tkacik G, 2016, ANNU REV CONDEN MA P, V7, P89, DOI 10.1146/annurev-conmatphys-031214-014803; van den Oord Aaron, 2018, ARXIV180703748; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wei DL, 2018, PROC CVPR IEEE, P8052, DOI 10.1109/CVPR.2018.00840; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40	49	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907028
C	Park, E; Oliva, JB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Park, Eunbyung; Oliva, Junier B.			Meta-Curvature	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose meta-curvature (MC), a framework to learn curvature information for better generalization and fast model adaptation. MC expands on the model-agnostic meta-learner (MAML) by learning to transform the gradients in the inner optimization such that the transformed gradients achieve better generalization performance to a new task. For training large scale neural networks, we decompose the curvature matrix into smaller matrices in a novel scheme where we capture the dependencies of the model's parameters with a series of tensor products. We demonstrate the effects of our proposed method on several few-shot learning tasks and datasets. Without any task specific techniques and architectures, the proposed method achieves substantial improvement upon previous MAML variants and outperforms the recent state-of-the-art methods. Furthermore, we observe faster convergence rates of the meta-training process. Finally, we present an analysis that explains better generalization performance with the meta-trained curvature.	[Park, Eunbyung; Oliva, Junier B.] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA	University of North Carolina; University of North Carolina Chapel Hill	Park, E (corresponding author), Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA.	eunbyung@cs.unc.edu; joliva@cs.unc.edu		Oliva, Junier/0000-0002-2601-5652				Al-Shedivat M., 2018, INT C LEARN REPR, P1; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Andrychowicz M, 2016, ADV NEUR IN, V29; Antoniou A., 2019, INT C LEARN REPR, P1; Cubuk Ekin D., 2018, ARXIV180509501; Dong XW, 2018, IEEE CONF COMPUT; Du Yunshu, 2018, ARXIV181202224; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, INT C LEARN REPR; Garcia V., 2018, ICLR; Grant Erin, 2018, INT C LEARN REPR; Grosse R, 2016, PR MACH LEARN RES, V48; Kim Taesup, 2018, NEURAL INFORM PROCES; Kingma D.P, P 3 INT C LEARNING R; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kossaif Jean, 2018, ARXIV170708308; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Le Roux Nicolas, 2008, NEURAL INFORM PROCES; Lee K, 2019, PROC CVPR IEEE, P10649, DOI 10.1109/CVPR.2019.01091; Lee Y, 2018, PR MACH LEARN RES, V80; Li Zhenguo, 2017, METASGD LEARNING LEA; Lopez-Paz D, 2017, ADV NEUR IN, V30; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Metz L., 2018, P 2 WORKSH MET METAL; Mishra N., 2018, INT C LEARN REPR, P1; Muandet Krikamol, 2012, NEURAL INFORM PROCES; Munkhdalai T, 2018, PR MACH LEARN RES, V80; Nichol Alex, 2018, ARXIV180302999; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755; Ravi S., 2017, INT C LEARN REPR, P12; Ren M., 2018, ICLR; Riemer Matthew, 2019, INT C LEARN REPR ICL; Rothfuss J., 2019, INT C LEARN REPR ICL; Rusu Andrei A, 2019, ICLR; Schwartz Eli, 2018, NEURAL INFORM PROCES, P2; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wichrowska O, 2017, PR MACH LEARN RES, V70; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	45	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303032
C	Peng, JR; Sun, M; Zhang, ZX; Tan, TN; Yan, JJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Peng, Junran; Sun, Ming; Zhang, Zhaoxiang; Tan, Tieniu; Yan, Junjie			Efficient Neural Architecture Transformation Search in Channel-Level for Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently, Neural Architecture Search has achieved great success in large-scale image classification. In contrast, there have been limited works focusing on architecture search for object detection, mainly because the costly ImageNet pretraining is always required for detectors. Training from scratch, as a substitute, demands more epochs to converge and brings no computation saving. To overcome this obstacle, we introduce a practical neural architecture transformation search(NATS) algorithm for object detection in this paper. Instead of searching and constructing an entire network, NATS explores the architecture space on the base of existing network and reusing its weights. We propose a novel neural architecture search strategy in channel-level instead of path-level and devise a search space specially targeting at object detection. With the combination of these two designs, an architecture transformation scheme could be discovered to adapt a network designed for image classification to task of object detection. Since our method is gradient-based and only searches for a transformation scheme, the weights of models pretrained in ImageNet could be utilized in both searching and retraining stage, which makes the whole process very efficient. The transformed network requires no extra parameters and FLOPs, and is friendly to hardware optimization, which is practical to use in real-time application. In experiments, we demonstrate the effectiveness of NATS on networks like ResNet and ResNeXt. Our transformed networks, combined with various detection frameworks, achieve significant improvements on the COCO dataset while keeping fast.	[Peng, Junran; Zhang, Zhaoxiang; Tan, Tieniu] Univ Chinese Acad Sci, Beijing, Peoples R China; [Peng, Junran; Sun, Ming; Yan, Junjie] SenseTime Grp Ltd, Hong Kong, Peoples R China; [Peng, Junran; Zhang, Zhaoxiang; Tan, Tieniu] CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS	Zhang, ZX (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; Zhang, ZX (corresponding author), CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China.				National Key RAMP;D Program of China [2018YFB-1402605]; Beijing Municipal Natural Science Foundation [Z181100008918010]; National Natural Science Foundation of China [61836014, 61761146004, 61773375, 61602481]; CAS-AIR	National Key RAMP;D Program of China; Beijing Municipal Natural Science Foundation(Beijing Natural Science Foundation); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CAS-AIR	This work was supported in part by the National Key R&D Program of China(No.2018YFB-1402605), the Beijing Municipal Natural Science Foundation (No.Z181100008918010), the National Natural Science Foundation of China(No.61836014, No.61761146004, No.61773375, No.61602481) and CAS-AIR.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baker Bowen, 2017, ICLR; Cai H., 2018, ARXIV PREPRINT ARXIV; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He Kaiming, 2018, ARXIV181108883; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jie H., 2017, P IEEE C COMP VIS PA, P99; Kong T, 2018, LECT NOTES COMPUT SC, V11209, P172, DOI 10.1007/978-3-030-01228-1_11; Kong T, 2017, PROC CVPR IEEE, P5244, DOI 10.1109/CVPR.2017.557; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li ZQ, 2018, LECT NOTES COMPUT SC, V11207, P74, DOI 10.1007/978-3-030-01219-9_5; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C, 2018, LECT NOTES COMPUT SC, V11210, P203, DOI 10.1007/978-3-030-01231-1_13; Liu Hanxiao, 2018, ICLR; Liu Hanxiao, 2018, ARXIV180609055; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Luo WJ, 2016, ADV NEUR IN, V29; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Nair V, 2010, P 27 INT C MACHINE L, P807; Pham H, 2018, PR MACH LEARN RES, V80; Real E., 2018, ARXIV180201548, DOI [DOI 10.1609/AAAI.V33I01.33014780, 10.1609/aaai.v33i01.33014780]; Real E, 2017, PR MACH LEARN RES, V70; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sun S., 2018, NIPS; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zhang S., 2018, PROCEEDINGS OF THE I; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhu X., 2018, ARXIV PREPRINT ARXIV; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	45	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906002
C	Perrone, V; Shen, HB; Seeger, M; Archambeau, C; Jenatton, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Perrone, Valerio; Shen, Huibin; Seeger, Matthias; Archambeau, Cedric; Jenatton, Rodolphe			Learning search spaces for Bayesian optimization: Another view of hyperparameter transfer learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bayesian optimization (BO) is a successful methodology to optimize black-box functions that are expensive to evaluate. While traditional methods optimize each black-box function in isolation, there has been recent interest in speeding up BO by transferring knowledge across multiple related black-box functions. In this work, we introduce a method to automatically design the BO search space by relying on evaluations of previous black-box functions. We depart from the common practice of defining a set of arbitrary search ranges a priori by considering search space geometries that are learned from historical data. This simple, yet effective strategy can be used to endow many existing BO methods with transfer learning properties. Despite its simplicity, we show that our approach considerably boosts BO by reducing the size of the search space, thus accelerating the optimization of a variety of black-box optimization problems. In particular, the proposed approach combined with random search results in a parameter-free, easy-to-implement, robust hyperparameter optimization strategy. We hope it will constitute a natural baseline for further research attempting to warm-start BO.	[Perrone, Valerio; Shen, Huibin; Seeger, Matthias; Archambeau, Cedric; Jenatton, Rodolphe] Amazon, Berlin, Germany; [Jenatton, Rodolphe] Google Brain, Berlin, Germany	Amazon.com	Perrone, V (corresponding author), Amazon, Berlin, Germany.	vperrone@amazon.com; huibishe@amazon.com; matthis@amazon.com; cedrica@amazon.com						[Anonymous], [No title captured]; [Anonymous], [No title captured]; [Anonymous], 2017, ARXIV170509236; Baker B., 2017, ARXIV170510823; Bardenet Remi, 2013, INT C MACHINE LEARNI, P199; Bergstra J. S., 2011, P 24 INT C NEUR INF, P2546, DOI DOI 10.1145/3065386; Boyd S, 2004, CONVEX OPTIMIZATION; Diamond S., 2014, CVXPY PYTHON EMBEDDE; Falkner S, 2018, PR MACH LEARN RES, V80; Feurer M, 2015, AAAI CONF ARTIF INTE, P1128; Fusi N, 2017, ARXIV170505355; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; Gramacy Robert B, 2010, ARXIV10044027; Grtschel M., 2012, GEOMETRIC ALGORITHMS, V2; Hansen N., 2005, CMA EVOLUTION STRATE, DOI DOI 10.1162/106365603321828970; Harman R, 2010, J MULTIVARIATE ANAL, V101, P2297, DOI 10.1016/j.jmva.2010.06.002; John F., 1948, TRACES EMERGENCE NON, P187, DOI DOI 10.1007/978-3-0348-0439-4_9; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kandasamy K., 2019, ARXIV190306694; Klein A., 2019, ARXIV190504970; Knorr E. M., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P126, DOI 10.1145/502512.502532; Knudde N., 2017, ARXIV PREPRINT ARXIV; Mockus J., 1978, APPL BAYESIAN METHOD, V2; Perrone V., 2018, ADV NEURAL INFORM PR; Poloczek M, 2016, WINT SIMUL C PROC, P770, DOI 10.1109/WSC.2016.7822140; Real E., 2018, ARXIV180201548; Schilling N., 2016, JOINT EUROPEAN C MAC, P33; Shahriari B, 2016, JMLR WORKSH CONF PRO, V51, P1168; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J., 2012, P NIPS, V12, P2960; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Sun P, 2004, OPER RES, V52, P690, DOI 10.1287/opre.1040.0115; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; van Rijn JN, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2367, DOI 10.1145/3219819.3220058; Vanschoren J., 2013, ACM SIGKDD EXPLOR NE, V15, P49, DOI [10.1145/2641190.2641198, DOI 10.1145/2641190.2641198]; WANG Z, 2017, ARXIV170301973; Wilson James T., 2018, ADV NEURALINFORMATIO, P9906; Wistuba M, 2015, IEEE DATA MINING, P1033, DOI 10.1109/ICDM.2015.20; Wistuba M, 2015, LECT NOTES ARTIF INT, V9285, P104, DOI 10.1007/978-3-319-23525-7_7; Yogatama D, 2014, JMLR WORKSH CONF PRO, V33, P1077; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	49	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904042
C	Russac, Y; Vernade, C; Cappe, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Russac, Yoan; Vernade, Claire; Cappe, Olivier			Weighted Linear Bandits for Non-Stationary Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider a stochastic linear bandit model in which the available actions correspond to arbitrary context vectors whose associated rewards follow a non-stationary linear regression model. In this setting, the unknown regression parameter is allowed to vary in time. To address this problem, we propose D-LinUCB, a novel optimistic algorithm based on discounted linear regression, where exponential weights are used to smoothly forget the past. This involves studying the deviations of the sequential weighted least-squares estimator under generic assumptions. As a by-product, we obtain novel deviation results that can be used beyond non-stationary environments. We provide theoretical guarantees on the behavior of D-LinUCB in both slowly-varying and abruptly-changing environments. We obtain an upper bound on the dynamic regret that is of order d(2/3)B(T)(1/3)T(2/3), where B-T is a measure of non-stationarity (d and T being, respectively, dimension and horizon). This rate is known to be optimal. We also illustrate the empirical performance of D-LinUCB and compare it with recently proposed alternatives in simulated environments.	[Russac, Yoan; Cappe, Olivier] Univ PSL, CNRS, ENS, INRIA, Paris, France; [Vernade, Claire] Deepmind, London, England	Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Russac, Y (corresponding author), Univ PSL, CNRS, ENS, INRIA, Paris, France.	yoan.russac@ens.fr; vernade@google.com; olivier.cappe@cnrs.fr						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; [Anonymous], 2017, P ADKDD TARGETAD WOR; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Auer P., 2018, EUR WORKSH REINF LEA, V14; Besbes O., 2014, ADV NEURAL INFORM PR, P199; Besbes O., 2018, OPTIMAL EXPLORATION; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; Besson L., 2019, ARXIV190201575; Bleakley K, 2011, ARXIV11064199; Cao Y., 2018, ARXIV180203692; Chen Y., 2019, ARXIV190200980; Cheung Wang Chi, 2018, ARXIV181003024; Cheung Wang Chi, 2019, ARXIV190301461; Garivier A, 2011, LECT NOTES ARTIF INT, V6925, P174, DOI 10.1007/978-3-642-24412-4_16; Goldenshluger A., 2013, STOCHASTIC SYSTEMS, V3, P230, DOI [10.1287/11-SSY032, DOI 10.1287/11-SSY032]; Gupta N., 2011, 2011 10 INT C MACH L, V1; Keskin NB, 2017, MATH OPER RES, V42, P277, DOI 10.1287/moor.2016.0807; Kirschner J, 2018, P MACHINE LEARNING R, V75, P1; Kocsis L., 2006, 2 PASC CHALL WORKSH; Lattimore Tor, 2019, BANDIT ALGORITHMS; Levine N., 2017, ADV NEURAL INFORM PR, P3074; Li L., 2010, WWW; Luo H., 2017, ARXIV170801799; Mintz Y., 2017, ARXIV170708423; Pefia V. H., 2008, SELF NORMALIZED PROC; Raj V., 2017, ARXIV170709727; Seznec Julien, 2018, ARXIV181111043; Wei L, 2018, P AMER CONTR CONF, P6291; Wu QY, 2018, ACM/SIGIR PROCEEDINGS 2018, P495, DOI 10.1145/3209978.3210051; Yu J.Y., 2009, P 26 ANN INT C MACHI, P1177	30	11	11	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903063
C	Salinas, D; Bohlke-Schneider, M; Callot, L; Medico, R; Gasthaus, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Salinas, David; Bohlke-Schneider, Michael; Callot, Laurent; Medico, Roberto; Gasthaus, Jan			High-Dimensional Multivariate Forecasting with Low-Rank Gaussian Copula Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GARCH	Predicting the dependencies between observations from multiple time series is critical for applications such as anomaly detection, financial risk management, causal analysis, or demand forecasting. However, the computational and numerical difficulties of estimating time-varying and high-dimensional covariance matrices often limits existing methods to handling at most a few hundred dimensions or requires making strong assumptions on the dependence between series. We propose to combine an RNN-based time series model with a Gaussian copula process output model with a low-rank covariance structure to reduce the computational complexity and handle non-Gaussian marginal distributions. This permits to drastically reduce the number of parameters and consequently allows the modeling of time-varying correlations of thousands of time series. We show on several real-world datasets that our method provides significant accuracy improvements over state-of-the-art baselines and perform an ablation study analyzing the contributions of the different components of our model.	[Salinas, David] Naverlabs, Seongnam Si, South Korea; [Bohlke-Schneider, Michael; Callot, Laurent; Gasthaus, Jan] Amazon Res, Seattle, WA USA; [Medico, Roberto] Univ Ghent, Ghent, Belgium	Ghent University	Salinas, D (corresponding author), Naverlabs, Seongnam Si, South Korea.	david.salinas@naverlabs.com; bohlkem@amazon.com; lcallot@amazon.com; roberto.medico91@gmail.com; gasthaus@amazon.com						[Anonymous], 2013, PREPRINT ARXIV 1308; Aussenegg W, 2012, WORKING PAPER SERIES, V68, P1; Bauwens L, 2006, J APPL ECONOMET, V21, P79, DOI 10.1002/jae.842; Bernanke BS, 2005, Q J ECON, V120, P387, DOI 10.1162/qjec.2005.120.1.387; BOX GEP, 1964, J ROY STAT SOC B, V26, P211, DOI 10.1111/j.2517-6161.1964.tb00553.x; Callot LA., 2014, ESSAYS NONLINEAR TIM, P238; Dheeru D., 2019, UCI MACHINE LEARNING; Durbin J, 2012, TIME SERIES ANAL STA, V38; Elidan Gal, 2013, COPULAE MATH QUANTIT, P39; Engle R, 2002, J BUS ECON STAT, V20, P339, DOI 10.1198/073500102288618487; Everitt B., 1984, INTRO LATENT VARIABL; Gasthaus Jan, 2019, AISTATS; Ghalanos A., 2019, RMGARCH MULTIVARIATE; Graves A., 2015, ARXIV150204623; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Lai Guokun, 2017, CORR; Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037; Liu H, 2009, J MACH LEARN RES, V10, P2295; Lutkepohl H., 2005, NEW INTRO MULTIPLE T, DOI DOI 10.1007/078-3-540-27752-1; NYC Taxi and Limousine Commission, 2015, TLC TRIP RECORD DATA; Oliva JB, 2018, PR MACH LEARN RES, V80; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Patton AJ, 2012, J MULTIVARIATE ANAL, V110, P4, DOI 10.1016/j.jmva.2012.02.021; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; RUBIN DB, 1982, PSYCHOMETRIKA, V47, P69, DOI 10.1007/BF02293851; Salinas David, 2017, CORR; Sen R., 2019, ARXIV190503806; Sklar A., 1959, PUBLICATIONS I STAT, V8, P229, DOI DOI 10.1007/978-3-642-33590-7; Spearman C, 1904, AM J PSYCHOL, V15, P201, DOI 10.2307/1412107; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Toubeau JF, 2019, IEEE T POWER SYST, V34, P1203, DOI 10.1109/TPWRS.2018.2870041; van den Oord A., 2016, SSW; Van der Weide R, 2002, J APPL ECONOMET, V17, P549, DOI 10.1002/jae.688; Wen R., 2019, ARXIV190710697; Wen R., 2017, TIME SERIES WORKSHOP; Wilson AG., 2010, ADV NEURAL INFORM PR, P2460	37	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306079
C	Steck, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Steck, Harald			Markov Random Fields for Collaborative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LIKELIHOOD-ESTIMATION; MODEL	In this paper, we model the dependencies among the items that are recommended to a user in a collaborative-filtering problem via a Gaussian Markov Random Field (MRF). We build upon Besag's auto-normal parameterization and pseudo-likelihood [7], which not only enables computationally efficient learning, but also connects the areas of MRFs and sparse inverse covariance estimation with autoencoders and neighborhood models, two successful approaches in collaborative filtering. We propose a novel approximation for learning sparse MRFs, where the trade-off between recommendation-accuracy and training-time can be controlled. At only a small fraction of the training-time compared to various baselines, including deep nonlinear models, the proposed approach achieved competitive ranking-accuracy on all three well-known data-sets used in our experiments, and notably a 20% gain in accuracy on the data-set with the largest number of items.	[Steck, Harald] Netflix, Los Gatos, CA 95032 USA	Netflix, Inc.	Steck, H (corresponding author), Netflix, Los Gatos, CA 95032 USA.	hsteck@netflix.com						Aiolli F., 2013, P 7 ACM C REC SYST, P273, DOI [DOI 10.1145/2507157.2507189, 10.1145/2507157.2507189]; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Bennet J., 2007, WORKSH SIGKDD 07 ACM; Bertin-Mahieux Thierry, 2011, P 12 INT C MUS INF R, DOI DOI 10.7916/D8NZ8J07; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; BESAG J, 1977, BIOMETRIKA, V64, P616, DOI 10.1093/biomet/64.3.616; BESAG JE, 1972, J ROY STAT SOC B, V34, P75; Blumensath T., 2008, ARXIV08050510; Blumensath T, 2008, J FOURIER ANAL APPL, V14, P629, DOI 10.1007/s00041-008-9035-z; Cheng H., 2016, DLRS 2016PROCEEDINGS, P7, DOI [10.1145/2988450.2988454, DOI 10.1145/2988450.2988454]; Dinh L, 2017, 5 INT C LEARN REPR I; Fattahi S., 2018, EUR CONTR C ECC; Fattahi S, 2019, J MACH LEARN RES, V20; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Germain M., 2015, INT C MACH LEARN ICM; Guillot D., 2012, ADV NEURAL INFORM PR; Hammersley J.M., 1971, MARKOV FIELDS FINITE; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614; Hidasi B., 2017, CORR; Hsieh C.-J., 2014, J MACHINE LEARNING R; Hsieh C-J, 2013, ADV NEURAL INFORM PR; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Jack, 2013, RECSYS LARG SCAL REC; Jacobsen J orn-Henrik, 2018, P ICLR; Kabbur S., 2013, ACM C KNOWL DISC DAT; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150; Lindsay BG, 1988, CONT MATH, V80, P221, DOI DOI 10.1090/CONM/080/999014; Mazumder R, 2012, J MACH LEARN RES, V13, P781; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Mikolov T., 2013, ARXIV; Pan R, 2008, IEEE DATA MINING, P502, DOI 10.1109/ICDM.2008.16; Papamakarios George, 2017, ARXIV170507057; Paterek A., 2007, P ACM KDD CUP WORKSH, P5, DOI DOI 10.1145/1557019.1557072; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Rendle Steffen, 2009, UAI; Scheinberg K., 2010, P 23 INT C NEUR INF; Schmidt M, 2011, THESIS; Sedhain S., 2016, AAAI C ART INT; Sedhain S, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P111, DOI 10.1145/2740908.2742726; Sojoudi S., 2016, J MACHINE LEARNING R; Steck H., 2011, P 5 ACM C RECOMMENDE, P125; Steck H, 2019, WEB CONFERENCE 2019: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW 2019), P3251, DOI 10.1145/3308558.3313710; Stojkovic I., 2017, INT JOINT C ART INT; Tikk D, 2015, ARXIV151106939; Treister E., 2014, ADV NEURAL INFORM PR; Verstrepen K, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P177, DOI 10.1145/2645710.2645731; Volkovs MN, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P313, DOI 10.1145/2766462.2767716; Wang Huahua, 2013, ADV NEURAL INFORM PR; Wang L., 2016, INT C ART INT STAT A; Witten DM, 2011, J COMPUT GRAPH STAT, V20, P892, DOI 10.1198/jcgs.2011.11051a; Wu Y, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P153, DOI 10.1145/2835776.2835837; Xia Ning, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P497, DOI 10.1109/ICDM.2011.134; Yuan M., 2007, BIOMETRIKA; Yuan M, 2010, J MACH LEARN RES, V11, P2261; Zhang R.Y., 2018, INT C MACH LEARN ICM; Zheng Y, 2016, PR MACH LEARN RES, V48; Zhou S., 2011, J MACHINE LEARNING R, V12	63	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305046
C	Stelmakh, I; Shah, NB; Singh, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Stelmakh, Ivan; Shah, Nihar B.; Singh, Aarti			On Testing for Biases in Peer Review	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COVARIATE MEASUREMENT ERROR; DOUBLE-BLIND; SINGLE-BLIND; EFFECTS MODELS; REGRESSION	We consider the issue of biases in scholarly research, specifically, in peer review. There is a long standing debate on whether exposing author identities to reviewers induces biases against certain groups, and our focus is on designing tests to detect the presence of such biases. Our starting point is a remarkable recent work by Tomkins, Zhang and Heavlin which conducted a controlled, large-scale experiment to investigate existence of biases in the peer reviewing of the WSDM conference. We present two sets of results in this paper. The first set of results is negative, and pertains to the statistical tests and the experimental setup used in the work of Tomkins et al. We show that the test employed therein does not guarantee control over false alarm probability and under correlations between relevant variables, coupled with any of the following conditions, with high probability can declare a presence of bias when it is in fact absent: (a) measurement error, (b) model mismatch, (c) reviewer calibration. Moreover, we show that the setup of their experiment may itself inflate false alarm probability if (d) bidding is performed in non-blind manner or (e) popular reviewer assignment procedure is employed. Our second set of results is positive, in that we present a general framework for testing for biases in (single vs. double blind) peer review. We then present a hypothesis test with guaranteed control over false alarm probability and non-trivial power even under conditions (a)-(c). Conditions (d) and (e) are more fundamental problems that are tied to the experimental setup and not necessarily related to the test.	[Stelmakh, Ivan; Shah, Nihar B.; Singh, Aarti] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Stelmakh, I (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	stiv@cs.cmu.edu; nihars@cs.cmu.edu; aarti@cs.cmu.edu			NSF [CRII: CIF: 1755656, CIF: 1763734]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF grant CRII: CIF: 1755656 and in part by NSF grant CIF: 1763734.	Arnold D, 2018, Q J ECON, V133, P1885, DOI 10.1093/qje/qjy012; BAKANIC V, 1987, AM SOCIOL REV, V52, P631, DOI 10.2307/2095599; Bertrand M, 2004, AM ECON REV, V94, P991, DOI 10.1257/0002828042002561; BLANK RM, 1991, AM ECON REV, V81, P1041; Brunner J, 2009, CAN J STAT, V37, P33, DOI 10.1002/cjs.10004; Budden AE, 2008, TRENDS ECOL EVOL, V23, P4, DOI 10.1016/j.tree.2007.07.008; COHEN J, 1992, PSYCHOL BULL, V112, P155, DOI 10.1037/0033-2909.112.1.155; ERNST E, 1994, J LAB CLIN MED, V124, P178; Fisher RA, 1935, DESIGN EXPT; Gao Y., 2019, N AM CHAPTER ASS COM, P1274; Hill S, 2003, ACM SIGKDD EXPLORATI, V5, P179, DOI DOI 10.1145/980972.981001; Kang D., 2018, NAACL, V1, P1647; KERR S, 1977, ACAD MANAGE J, V20, P132, DOI 10.2307/255467; Kobren A, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1247, DOI 10.1145/3292500.3330899; LAIRD NM, 1982, BIOMETRICS, V38, P963, DOI 10.2307/2529876; Lamont M., 2009, HOW PROFESSORS THINK; Largent E.A., 2016, BLINDING SOLUTION BI, P75, DOI [10.1016/b978-0-12-802460-7.00005-x, DOI 10.1016/B978-0-12-802460-7.00005-X]; Laurent Charlin, 2013, P 30 INT C MACH LEAR; LINDSTROM MJ, 1990, BIOMETRICS, V46, P673, DOI 10.2307/2532087; Mahoney M.J., 1977, COGNITIVE THERAPY RE, V1, P161, DOI [10.1007/bf01173636, DOI 10.1007/BF01173636, 10.1007/BF01173636]; Moss-Racusin CA, 2012, P NATL ACAD SCI USA, V109, P16474, DOI 10.1073/pnas.1211286109; Okike K, 2016, JAMA-J AM MED ASSOC, V316, P1315, DOI 10.1001/jama.2016.11014; Rabe-Hesketh S, 2003, STAT MODEL, V3, P215, DOI 10.1191/1471082X03st056oa; Seeber M, 2017, SCIENTOMETRICS, V113, P567, DOI 10.1007/s11192-017-2264-7; Snodgrass R, 2006, SIGMOD REC, V35, P8; Squazzoni F, 2012, J INFORMETR, V6, P265, DOI 10.1016/j.joi.2011.12.005; STEFANSKI LA, 1985, ANN STAT, V13, P1335, DOI 10.1214/aos/1176349741; Stelmakh I., 2018, ARXIV180606237; Stelmakh I, 2019, ADV NEUR IN, V32; Thorngate W, 2014, ADV INTELL SYST, V229, P177, DOI 10.1007/978-3-642-39829-2_16; Thornhill T, 2019, SOCIOL RACE ETHNIC, V5, P456, DOI 10.1177/2332649218792579; Tomkins A, 2017, P NATL ACAD SCI USA, V114, P12708, DOI 10.1073/pnas.1707323114; Wang J., 2018, CORR; Webb TJ, 2008, TRENDS ECOL EVOL, V23, P351, DOI 10.1016/j.tree.2008.03.003; Weisberg S, 2014, APPL LINEAR REGRESSI, V4, DOI DOI 10.1002/0471704091	35	11	11	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305030
C	Tibshirani, RJ; Barber, RF; Candes, EJ; Ramdas, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tibshirani, Ryan J.; Barber, Rina Foygel; Candes, Emmanuel J.; Ramdas, Aaditya			Conformal Prediction Under Covariate Shift	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We extend conformal prediction methodology beyond the case of exchangeable data. In particular, we show that a weighted version of conformal prediction can be used to compute distribution-free prediction intervals for problems in which the test and training covariate distributions differ, but the likelihood ratio between the two distributions is known-or, in practice, can be estimated accurately from a set of unlabeled data (test covariate points). Our weighted extension of conformal prediction also applies more broadly, to settings in which the data satisfies a certain weighted notion of exchangeability. We discuss other potential applications of our new conformal methodology, including latent variable and missing data problems.	[Tibshirani, Ryan J.; Ramdas, Aaditya] Carnegie Mellon Univ, Machine Learning Dept, Dept Stat, Pittsburgh, PA 15213 USA; [Barber, Rina Foygel] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Candes, Emmanuel J.] Stanford Univ, Dept Math, Dept Stat, Stanford, CA 94305 USA	Carnegie Mellon University; University of Chicago; Stanford University	Tibshirani, RJ (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Dept Stat, Pittsburgh, PA 15213 USA.	ryantibs@cmu.edu; rina@uchicago.edu; candes@stanford.edu; aramdas@cmu.edu			National Science Foundation [DMS-1654076, DMS-1712800, DMS-1554123]; Alfred P. Sloan fellowship; Office of Naval Research [N00014-16-1-2712]	National Science Foundation(National Science Foundation (NSF)); Alfred P. Sloan fellowship(Alfred P. Sloan Foundation); Office of Naval Research(Office of Naval Research)	The authors thank the American Institute of Mathematics for supporting and hosting our collaboration. R.F.B. was partially supported by the National Science Foundation under grant DMS-1654076 and by an Alfred P. Sloan fellowship. E.J.C. was partially supported by the Office of Naval Research under grant N00014-16-1-2712, by the National Science Foundation under grant DMS-1712800, and by a generous gift from TwoSigma. R.J.T. was partially supported by the National Science Foundation under grant DMS-1554123.	Agarwal Deepak, 2011, INT C ART INT STAT; Barber R. F., 2019, ARXIV190304684; Burnaev Evgeny, 2014, ANN C LEARN THEOR; Chen Xiangli, 2016, INT C ART INT STAT; Gretton A, 2009, NEURAL INF PROCESS S, P131; Lei J, 2018, J AM STAT ASSOC, V113, P1094, DOI 10.1080/01621459.2017.1307116; Lei J, 2015, ANN MATH ARTIF INTEL, V74, P29, DOI 10.1007/s10472-013-9366-6; Lei J, 2014, J R STAT SOC B, V76, P71, DOI 10.1111/rssb.12021; Papadopoulos Harris, 2002, EUR C MACH LEARN; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Reddi S, 2015, AAAI C ART INT; Shafer G, 2008, J MACH LEARN RES, V9, P371; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sugiyama M, 2007, J MACH LEARN RES, V8, P985; Sugiyama M, 2005, STATIST RISK MODEL, V23, P249, DOI 10.1524/stnd.2005.23.4.249; Vovk V., 2005, ALGORITHMIC LEARNING, DOI DOI 10.1007/B106715; Vovk V, 2009, ANN STAT, V37, P1566, DOI 10.1214/08-AOS622; Vovk Vladimir, 2012, AS C MACH LEARN; Vovk Vladimir, 2013, S CONF PROB PRED APP; Wen Junfeng, 2014, INT C MACH LEARN	21	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302052
C	Tran, D; Vafa, K; Agrawal, KK; Dinh, L; Poole, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tran, Dustin; Vafa, Keyon; Agrawal, Kumar Krishna; Dinh, Laurent; Poole, Ben			Discrete Flows: Invertible Generative Models of Discrete Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					While normalizing flows have led to significant advances in modeling high-dimensional continuous distributions, their applicability to discrete distributions remains unknown. In this paper, we show that flows can in fact be extended to discrete events-and under a simple change-of-variables formula not requiring log-determinant-Jacobian computations. Discrete flows have numerous applications. We consider two flow architectures: discrete autoregressive flows that enable bidirectionality, allowing, for example, tokens in text to depend on both left-to-right and right-to-left contexts in an exact language model; and discrete bipartite flows that enable efficient non-autoregressive generation as in RealNVP. Empirically, we find that discrete autoregressive flows outperform autoregressive baselines on synthetic discrete distributions, an addition task, and Potts models; and bipartite flows can obtain competitive performance with autoregressive baselines on character-level language modeling for Penn Tree Bank and text8.	[Tran, Dustin; Vafa, Keyon; Agrawal, Kumar Krishna; Dinh, Laurent; Poole, Ben] Google Brain, Mountain View, CA 94043 USA; [Vafa, Keyon] Columbia Univ, New York, NY 10027 USA	Google Incorporated; Columbia University	Tran, D (corresponding author), Google Brain, Mountain View, CA 94043 USA.				NSF [DGE-1644869]	NSF(National Science Foundation (NSF))	Supported by NSF grant DGE-1644869.	Aitchison L., 2018, DISCRETE FLOW POSTER; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Bengio Yoshua, 2013, ARXIV; Bowman S. R., 2016, ARXIV, P10; Britz Denny, 2017, P 2017 C EMP METH NA; Cobo, 2017, ARXIV171110433; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dinh L., 2014, ARXIV; Dinh Laurent., 2017, INT C LEARN REPR; Ford N., 2018, EMPIRICAL METHODS NA; Gu Jiatao, 2018, P ICLR; Honkala M., 2015, ADV NEURAL INFORM PR, P856, DOI DOI 10.1021/ACS.JCIM.9B00943; Hoogeboom E., 2019, ARXIV190507376; Jang E., 2017, ICLR; Jernite Y, 2015, PR MACH LEARN RES, V37, P2209; Kaiser L., 2018, ARXIV180303382; Kingma D. P., 2016, NEURAL INFORM PROCES; Kingma D.P., 2018, P 32 INT C NEUR INF, P10236; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lee J., 2018, ARXIV180206901; Maddison Chris J, 2016, ARXIV161100712; Merity Stephen, 2018, ARXIV180308240; Metz L., 2016, ARXIV161102163; Mikolov T., 2012, PREPRINT, P8; Mnih A., 2012, FAST SIMPLE ALGORITH; Papamakarios George, 2017, ARXIV170507057; Ping W., 2018, INT C LEARN REPR; Prenger R., 2018, ARXIV181100002; Ranganath R, 2016, PR MACH LEARN RES, V48; Reed S, 2017, PR MACH LEARN RES, V70; Rezende D.J., 2015, INT C MACH LEARN; Rippel O., 2013, ARXIV13025125; Salmon J. K., 2011, SC 11 P 2011 INT C H, P1, DOI [10.1145/2063384.2063405, DOI 10.1145/2063384.2063405]; Stern M., 2018, ADV NEURAL INFORM PR, P10107; Tabak EG, 2013, COMMUN PUR APPL MATH, V66, P145, DOI 10.1002/cpa.21423; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Vinyals Oriol, 2015, ARXIV151106391; WU FY, 1982, REV MOD PHYS, V54, P235, DOI 10.1103/RevModPhys.54.235; Xia YC, 2017, ADV NEUR IN, V30; Zhang SZ, 2016, ADV NEUR IN, V29; Ziegler Z. M., 2019, ARXIV190110548	42	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906038
C	Wen, YD; Singh, R; Raj, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wen, Yandong; Singh, Rita; Raj, Bhiksha			Face Reconstruction from Voice using Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IDENTITY	Voice profiling aims at inferring various human parameters from their speech, e.g. gender, age, etc. In this paper, we address the challenge posed by a subtask of voice profiling - reconstructing someone's face from their voice. The task is designed to answer the question: given an audio clip spoken by an unseen person, can we picture a face that has as many common elements, or associations as possible with the speaker, in terms of identity? To address this problem, we propose a simple but effective computational framework based on generative adversarial networks (GANs). The network learns to generate faces from voices by matching the identities of generated faces to those of the speakers, on a training set. We evaluate the performance of the network by leveraging a closely related task - cross-modal matching. The results show that our model is able to generate faces that match several biometric characteristics of the speaker, and results in matching accuracies that are much better than chance.	[Wen, Yandong; Singh, Rita; Raj, Bhiksha] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wen, YD (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	yandongw@andrew.cmu.edu; rsingh@cs.cmu.edu; bhiksha@cs.cmu.edu						Bahari M.H., 2012, AGE ESTIMATION TELEP; Bao JM, 2018, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2018.00702; Belin P, 2004, TRENDS COGN SCI, V8, P129, DOI 10.1016/j.tics.2004.01.008; Belin P, 2011, BRIT J PSYCHOL, V102, P711, DOI 10.1111/j.2044-8295.2011.02041.x; Bulat A, 2017, IEEE I CONF COMP VIS, P3726, DOI 10.1109/ICCV.2017.400; Dehak N, 2011, IEEE T AUDIO SPEECH, V19, P788, DOI 10.1109/TASL.2010.2064307; Ellis A.W., 1989, HDB RES FACE PROCESS, P207; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Jamal AJ, 2019, INFECT CONT HOSP EP, V40, P1006, DOI 10.1017/ice.2019.173; Kamachi M, 2003, CURR BIOL, V13, P1709, DOI 10.1016/j.cub.2003.09.005; Karras T., 2017, PROGR GROWING GANS I; Kim C., 2018, P AS C COMP VIS ACCV, P276; Kim Changil, 2019, P IEEE C COMP VIS PA; Kingma D.P, P 3 INT C LEARNING R; Kotti M, 2008, INT C PATT RECOG, P1734; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lu Yiping, 2017, ARXIV PREPRINT ARXIV; Mavica LW, 2013, J EXP PSYCHOL HUMAN, V39, P307, DOI 10.1037/a0030945; MCALLISTER HA, 1993, BASIC APPL SOC PSYCH, V14, P161, DOI 10.1207/s15324834basp1402_3; McGilloway S, 2000, P ISCA WORKSH SPEECH, P207; MERMELSTEIN P, 1967, J ACOUST SOC AM, V41, P1283, DOI 10.1121/1.1910470; Mirza M., 2014, ARXIV; Nagrani A, 2018, PROC CVPR IEEE, P8427, DOI 10.1109/CVPR.2018.00879; Nagrani Arsha, 2018, ARXIV180500833; Parkhi Omkar M., 2015, BRIT MACH VIS C; Perarnau G, 2016, ARXIV161106355; PTACEK PH, 1966, J SPEECH HEAR RES, V9, P273, DOI 10.1044/jshr.0902.273; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Schweinberger SR, 2007, Q J EXP PSYCHOL, V60, P1446, DOI 10.1080/17470210601063589; Schweinberger SR, 2011, CORTEX, V47, P1026, DOI 10.1016/j.cortex.2010.11.011; Sherwani J, 2007, 2007 INTERNATIONAL CONFERENCE ON INFORMATION AND COMMUNICATION TECHNOLOGIES AND DEVELOPMENT (ICTD), P130; Singh R, 2019, PROFILING HUMANS THE; Singh R, 2016, 2016 39TH INTERNATIONAL CONVENTION ON INFORMATION AND COMMUNICATION TECHNOLOGY, ELECTRONICS AND MICROELECTRONICS (MIPRO), P1375, DOI 10.1109/MIPRO.2016.7522354; Singh Rita, 2020, PROFILING HUMANS THE; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; TEAGER HM, 1990, NATO ADV SCI I D-BEH, V55, P241, DOI DOI 10.1007/978-94-009-2037-8_10; Wen Y., 2018, ARXIV180704836; Xia Xianjun, 2018, IEEE T MULTIMEDIA; Zhou Hang, 2018, ARXIV180707860	41	11	12	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305028
C	Wilder, B; Ewing, E; Dilkina, B; Tambe, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wilder, Bryan; Ewing, Eric; Dilkina, Bistra; Tambe, Milind			End to end learning and optimization on graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Real-world applications often combine learning and optimization problems on graphs. For instance, our objective may be to cluster the graph in order to detect meaningful communities (or solve other common graph optimization problems such as facility location, maxcut, and so on). However, graphs or related attributes are often only partially observed, introducing learning problems such as link prediction which must be solved prior to optimization. Standard approaches treat learning and optimization entirely separately, while recent machine learning work aims to predict the optimal solution directly from the inputs. Here, we propose an alternative decision-focused learning approach that integrates a differentiable proxy for common graph optimization problems as a layer in learned systems. The main idea is to learn a representation that maps the original optimization problem onto a simpler proxy problem that can be efficiently differentiated through. Experimental results show that our CLUSTERNET system outperforms both pure end-to-end approaches (that directly predict the optimal solution) and standard approaches that entirely separate learning and optimization.	[Wilder, Bryan; Tambe, Milind] Harvard Univ, Cambridge, MA 02138 USA; [Ewing, Eric; Dilkina, Bistra] Univ Southern Calif, Los Angeles, CA 90007 USA	Harvard University; University of Southern California	Wilder, B (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	bwilder@g.harvard.edu; ericewin@usc.edu; dilkina@usc.edu; milind_tambe@harvard.edu			Army Research Office [MURI W911NF1810208]; NSF Graduate Research Fellowship; NSF [1914522]; U.S. Department of Homeland Security [2015-ST-061-CIRC01]	Army Research Office; NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); U.S. Department of Homeland Security(United States Department of Homeland Security (DHS))	This work was supported by the Army Research Office (MURI W911NF1810208). Wilder is supported by a NSF Graduate Research Fellowship. Dilkina is supported partially by NSF award #1914522 and by U.S. Department of Homeland Security under Grant Award No. 2015-ST-061-CIRC01. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the official policies, either expressed or implied, of the U.S Department of Homeland Security.	Ageev AA, 2004, J COMB OPTIM, V8, P307, DOI 10.1023/B:JOCO.0000038913.96607.c2; Ahmed N. K., 2018, ARXIV180202896; Amos B., 2017, ICML; Andrychowicz M, 2016, ADV NEUR IN, V29; [Anonymous], 2017, ADOLESCENT HEALTH; [Anonymous], 2017, KOBLENZ NETWORK COLL; [Anonymous], 2017, HUMAN PROTEIN VIDAL; [Anonymous], 2018, 2018 IEEE ACM INT C; [Anonymous], 2011, PHYS REP, DOI DOI 10.1016/j.physrep.2010.11.002; Bello I., 2016, ARXIV PREPRINT ARXIV; Berlusconi G, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0154244; Burgess M, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0153384; Clauset A, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066111; Demirovic Emir, 2019, CPAIOR; Djolonga Josip, 2017, NEURIPS; Domke Justin, 2012, INT C ARTIFICIAL INT; Donti P, 2017, P ADV NEUR INF PROC, V30, P5484; El Balghiti O., 2019, ADV NEURAL INFORM PR, P14412; Elmachtoub A. N., 2017, ARXIV171008005; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Greff Klaus, 2016, NEURLPS2016; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Guo X., 2017, IJCAI; Hamilton William L., 2017, NIPS; Karimi M., 2017, ADV NEURAL INFORM PR; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Khalil E. B., 2017, NIPS; Kipf Thomas N., 2017, INT C LEARNING REPRE; Kool W., 2019, ICLR; Law Marc T, 2017, ICML; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; Mensch Arthur, 2018, ICML; Myers SA, 2012, IEEE DATA MINING, P539, DOI 10.1109/ICDM.2012.159; Nazi Azade, 2019, ARXIV190300614; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Schlichtkrull M, 2018, EUR SEM WEB C; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shaham U, 2018, ICLR; Tan SY, 2016, SCI REP-UK, V6, DOI 10.1038/srep22916; Tian F., 2014, 28 AAAI C ART INT; Titsias Michalis, 2016, NEURIPS; Vinyals O., 2015, NIPS; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wilder 2018b], 2018, AAMAS; Wilder Bryan, 2019, AAAI; Xie JY, 2016, PR MACH LEARN RES, V48; Yan BW, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/09/P09008; Yang L., 2016, INT JOINT C ARTIFICI, P2252, DOI [10.55553060832.3060936////, DOI 10.5555/3060832.3060936]; Ying Z., 2018, ADV NEURAL INFORM PR, P4800; Zhang Muhan, 2018, NIPS; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	54	11	11	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304065
C	Wu, A; Zhu, LC; Han, YH; Yang, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Aming; Zhu, Linchao; Han, Yahong; Yang, Yi			Connective Cognition Network for Directional Visual Commonsense Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Visual commonsense reasoning (VCR) has been introduced to boost research of cognition-level visual understanding, i.e., a thorough understanding of correlated details of the scene plus an inference with related commonsense knowledge. Recent studies on neuroscience have suggested that brain function or cognition can be described as a global and dynamic integration of local neuronal connectivity, which is context-sensitive to specific cognition tasks. Inspired by this idea, towards VCR, we propose a connective cognition network (CCN) to dynamically reorganize the visual neuron connectivity that is contextualized by the meaning of questions and answers. Concretely, we first develop visual neuron connectivity to fully model correlations of visual content. Then, a contextualization process is introduced to fuse the sentence representation with that of visual neurons. Finally, based on the output of contextualized connectivity, we propose directional connectivity to infer answers or rationales. Experimental results on the VCR dataset demonstrate the effectiveness of our method. Particularly, in Q -> AR mode, our method is around 4% higher than the state-of-the-art method.	[Wu, Aming; Han, Yahong] Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China; [Zhu, Linchao; Yang, Yi] Univ Technol Sydney, ReLER, Sydney, NSW, Australia	Tianjin University; University of Technology Sydney	Han, YH (corresponding author), Tianjin Univ, Coll Intelligence & Comp, Tianjin, Peoples R China.	tjwam@tju.edu.cn; Linchao.Zhu@uts.edu.au; yahong@tju.edu.cn; yi.yang@uts.edu.au	yang, yang/GVT-5210-2022; yang, yang/HGT-7999-2022; Zhu, Linchao/AAE-6700-2020; Yang, Yi/B-9273-2017; yang, yang/GWB-9426-2022	Zhu, Linchao/0000-0002-4093-7557; Yang, Yi/0000-0002-0512-880X; 	NSFC [61876130, 61932009, U1509206]	NSFC(National Natural Science Foundation of China (NSFC))	This work is supported by the NSFC (under Grant 61876130, 61932009, U1509206).	Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arandjelovic Relja, 2016, CVPR; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Bhardwaj S, 2019, PROC CVPR IEEE, P354, DOI 10.1109/CVPR.2019.00044; Bola M, 2015, NEUROIMAGE, V114, P398, DOI 10.1016/j.neuroimage.2015.03.057; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Y., 2018, ARXIV181112814; Cord Matthieu, 2019, CVPR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Gao P., 2018, ARXIV180802632; Girshick R., 2015, ICCV; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Hazan Tamir, 2019, CVPR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Kim Jin-Hwa, 2016, ICLR; Kim Kyung-Min, 2018, ECCV; Kipf TN, 2016, P INT C LEARN REPR; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Malinowski M, 2018, LECT NOTES COMPUT SC, V11210, P3, DOI 10.1007/978-3-030-01231-1_1; Miech Antoine, 2017, P IEEE C COMP VIS PA; Mikaela Angelina Uy, 2018, CVPR; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Narasimhan M, 2018, ADV NEUR IN, V31; Norcliffe-Brown Will, 2018, ADV NEURAL INFORM PR; Park HJ, 2013, SCIENCE, V342, P579, DOI 10.1126/science.1238411; Patel Vimla L., 1997, COGNITIVE MODELS DIR, P67; Peng, 2018, ARXIV181205252; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Russell S., 2009, ARTIF INTELL; Tang Yongyi, 2018, P EUR C COMP VIS ECC, P0; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Velickovi Petar, 2017, ARXIV171010903; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Xu K., 2018, P 2018 C EMPIRICAL M, P918; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688; Zhou J., 2020, OPEN, V1, DOI [10.1016/j.aiopen.2021.01.001, DOI 10.1016/J.AIOPEN.2021.01.001]	43	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305064
C	Yu, T; Hu, SY; Guo, C; Chao, WL; Weinberger, KQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yu, Tao; Hu, Shengyuan; Guo, Chuan; Chao, Wei-Lun; Weinberger, Kilian Q.			A New Defense Against Adversarial Images: Turning a Weakness into a Strength	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Natural images are virtually surrounded by low-density misclassified regions that can be efficiently discovered by gradient-guided search - enabling the generation of adversarial images. While many techniques for detecting these attacks have been proposed, they are easily bypassed when the adversary has full knowledge of the detection mechanism and adapts the attack strategy accordingly. In this paper, we adopt a novel perspective and regard the omnipresence of adversarial perturbations as a strength rather than a weakness. We postulate that if an image has been tampered with, these adversarial directions either become harder to find with gradient methods or have substantially higher density than for natural images. We develop a practical test for this signature characteristic to successfully detect adversarial attacks, achieving unprecedented accuracy under the white-box setting where the adversary is given full knowledge of our detection mechanism.	[Yu, Tao; Hu, Shengyuan; Guo, Chuan; Weinberger, Kilian Q.] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Chao, Wei-Lun] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	Cornell University; University System of Ohio; Ohio State University	Yu, T (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	ty367@cornell.edu; sh797@cornell.edu; cg563@cornell.edu; chao.209@osu.edu; kqw4@cornell.edu			NSF [III-1618134, III-1526012, IIS-1149882, IIS-1724282, TRIPODS-1740822]; Bill and Melinda Gates Foundation; Cornell Center for Materials Research; NSF MRSEC program [DMR-1719875]; SAP America Inc.; Zillow; Facebook	NSF(National Science Foundation (NSF)); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Cornell Center for Materials Research; NSF MRSEC program(National Science Foundation (NSF)NSF - Directorate for Mathematical & Physical Sciences (MPS)); SAP America Inc.; Zillow; Facebook(Facebook Inc)	C.G., W-L.C., K.Q.W. are supported by grants from the NSF (III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822), the Bill and Melinda Gates Foundation, and the Cornell Center for Materials Research with funding from the NSF MRSEC program (DMR-1719875); and are also supported by Zillow, SAP America Inc., and Facebook. We thank Pin-Yu Chen (IBM) for constructive discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aravind CV, 2018, SPRINGERBRIEF ENERG, P1, DOI 10.1007/978-981-13-0435-4_1; Athalye A., 2018, ABS180200420 CORR; Behzadan V., 2017, ABS170104143 CORR; Bhagoji AN, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS); Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Brendel W., 2017, ABS171204248 CORR; Buckman J., 2018, 6 INT C LEARN REPR I; Carlini N., 2017, 10 ACM WORKSH ART IN; Carlini N., 2018, ABS180101944 CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Cisse M., 2017, ABS170705373 CORR; Deng J., 2009, CVPR, V2, P4; Dhillon G.S., 2018, 6 INT C LEARN REPR I; Fawzi A., 2018, ADV NEURAL INFORM PR, P1186; Feinman R., 2017, DETECTING ADVERSARIA; Gal Y, 2016, PR MACH LEARN RES, V48; Goodfellow R, 2015, OIL AND GAS PIPELINES: INTEGRITY AND SAFETY HANDBOOK, P3; Grosse K., 2017, ARXIV PREPRINT ARXIV; Guo C., 2018, INT C LEARN REPR ICL; Huang S.H., 2017, ABS170202284 CORR; Ilyas A., 2018, ABS180707978 CORR; Ilyas A., 2018, ICML, P2142; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin A., 2017, INT C LEARN REPR ICL; Li X, 2017, IEEE I CONF COMP VIS, P5775, DOI 10.1109/ICCV.2017.615; Liu XD, 2018, COMPLEXITY, DOI 10.1155/2018/5145348; Liu Y., 2016, ABS161102770 CORR; Ma XM, 2018, WIREL COMMUN MOB COM, DOI 10.1155/2018/3610482; Madry Aleksander, 2018, 6 INT C LEARN REPR I; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Metzen J.H., 2017, 5 INT C LEARN REPR I; Pang Tianyu, 2018, ADV NEURAL INFORM PR, V31, P4579; Prakash A, 2018, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR.2018.00894; Raghunathan NJ, 2018, J ADOLESC YOUNG ADUL, V7, P125, DOI 10.1089/jayao.2017.0050; Roth K., 2019, P 36 INT C MACH LEAR; Samangouei P., 2018, ABS180506605 CORR; Shafahi A., 2018, ABS180902104 CORR; Sinha A., 2018, 6 INT C LEARN REPR I; Song Y., 2018, 6 INT C LEARN REPR I; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2014, INT C MACH LEARN ICM; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tramer F, 2017, ABS170507204 CORR; Tu C., 2018, ABS180511770 CORR; Uesato Jonathan, 2018, P 35 INT C MACH LEAR; Weinberger K.Q., 2019, ABS190507121 CORR; Wong E., 2017, INT C MACH LEARN ICM; Xie C., 2018, 6 INT C LEARN REPR I; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu W., 2018, NETW DISTR SYST SEC	55	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301060
C	Zhou, P; Yuan, XT; Xu, H; Yan, SC; Feng, JS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Pan; Yuan, Xiao-Tong; Xu, Huan; Yan, Shuicheng; Feng, Jiashi			Efficient Meta Learning via Minibatch Proximal Update	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We address the problem of meta-learning which learns a prior over hypothesis from a sample of meta-training tasks for fast adaptation on meta-testing tasks. A particularly simple yet successful paradigm for this research is model-agnostic meta-learning (MAML). Implementation and analysis of MAML, however, can be tricky; first-order approximation is usually adopted to avoid directly computing Hessian matrix but as a result the convergence and generalization guarantees remain largely mysterious for MAML. To remedy this deficiency, in this paper we propose a minibatch proximal update based meta-learning approach for learning to efficient hypothesis transfer. The principle is to learn a prior hypothesis shared across tasks such that the minibatch risk minimization biased regularized by this prior can quickly converge to the optimal hypothesis in each training task. The prior hypothesis training model can be efficiently optimized via SGD with provable convergence guarantees for both convex and non-convex problems. Moreover, we theoretically justify the benefit of the learnt prior hypothesis for fast adaptation to new few-shot learning tasks via minibatch proximal update. Experimental results on several few-shot regression and classification tasks demonstrate the advantages of our method over state-of-the-arts.	[Zhou, Pan; Feng, Jiashi] Natl Univ Singapore, Learning & Vis Lab, Singapore, Singapore; [Yuan, Xiao-Tong] Nanjing Univ Informat Sci & Technol, B DAT Lab, Nanjing, Peoples R China; [Xu, Huan] Alibaba, San Mateo, CA USA; [Xu, Huan] Georgia Inst Technol, Atlanta, GA 30332 USA; [Yan, Shuicheng] YITU Technol, Shanghai, Peoples R China	National University of Singapore; Nanjing University of Information Science & Technology; University System of Georgia; Georgia Institute of Technology	Zhou, P (corresponding author), Natl Univ Singapore, Learning & Vis Lab, Singapore, Singapore.	pzhou@u.nus.edu; xtyuan@nuist.edu.cn; Huan.xu@alibaba-inc.com; eleyans@nus.edu.sg; elefjia@nus.edu.sg	Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022		National Major Project of China for New Generation of AI [2018AAA0100400]; Natural Science Foundation of China (NSFC) [61876090, 61936005]; NUS IDS [R-263- 000-C67-646]; ECRA [R-263-000-C87-133]; MOE Tier-II [R-263-000-D17-112]; AI.SG [R-263-000-D97-490]	National Major Project of China for New Generation of AI; Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); NUS IDS; ECRA; MOE Tier-II; AI.SG	Xiao-Tong Yuan was supported by National Major Project of China for New Generation of AI (No. 2018AAA0100400) and Natural Science Foundation of China (NSFC) under Grant 61876090 and Grant 61936005. Jiashi Feng was partially supported by NUS IDS R-263- 000-C67-646, ECRA R-263-000-C87-133, MOE Tier-II R-263-000-D17-112 and AI.SG R-263-000-D97-490.	Alquier P, 2017, PR MACH LEARN RES, V54, P261; [Anonymous], 2017, INT C LEARN REPR; [Anonymous], 2014, ADV NEURAL INFORM PR; Bengio Y., 1990, IJCNN; Crammer K, 2006, J MACH LEARN RES, V7, P551; Denevi G., 2018, ADV NEURAL INFORM PR, P10169; Denevi G., 2019, INT C MACH LEARN, P1566; Denevi G, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P457; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan Y., 2016, RL2 FAST REINFORCEME; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Finn C., 2019, P INT C MACH LEARN; Finn C, 2017, PR MACH LEARN RES, V70; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Graves A, 2014, NEURAL TURING MACHIN; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Khodak M., 2019, ARXIV190210644; Kingma D., 2014, ADAM METHOD STOCHAST; Koch Gregory, 2015, P ICML DEEP LEARN WO, V2; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuzborskij I, 2017, MACH LEARN, V106, P171, DOI 10.1007/s10994-016-5594-4; Kuzborskij Ilja, 2013, P 30 INT C MACH LEAR, P942; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612; Li Z., 2017, P C NEUR INF PROC SY; Mishra N., 2017, ARXIV170703141, V2; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Naik D. K., 1992, IJCNN International Joint Conference on Neural Networks (Cat. No.92CH3114-6), P437, DOI 10.1109/IJCNN.1992.287172; Nichol Alex, 2018, ABS180302999 ARXIV; Orabona F, 2009, IEEE INT CONF ROBOT, P439; Pentina A, 2014, PR MACH LEARN RES, V32, P991; Reddi SJ, 2016, PR MACH LEARN RES, V48; Ren M., 2018, P 6 INT C LEARN REPR; Rousseeuw P.J., 2005, ROBUST REGRESSION OU, V589; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F., 2017, ARXIV170609529; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Thrun S., 2012, LEARNING LEARN; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang J., 2017, P MACHINE LEARNING R, P1882; Weston J., 2014, ARXIV14103916; Zhou P., 2017, KNOWL-BASED SYST, V9, P1; Zhou P., 2018, P C NEUR INF PROC SY	46	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301051
C	Allen-Zhu, Z; Li, YZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Allen-Zhu, Zeyuan; Li, Yuanzhi			NEON2: Finding Local Minima via First-Order Oracles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a reduction for non-convex optimization that can (1) turn an stationary-point finding algorithm into an local-minimum finding one, and (2) replace the Hessian-vector product computations with only gradient computations. It works both in the stochastic and the deterministic settings, without hurting the algorithm's performance. As applications, our reduction turns Natasha2 into a first-order method without hurting its theoretical performance. It also converts SGD, GD, SCSG, and SVRG into algorithms finding approximate local minima, outperforming some best known results.	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA; [Li, Yuanzhi] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; yuanzhil@stanford.edu	Li, Yuan/GXV-1310-2022					Agarwal N, 2017, STOC; Allen-Zhu Z., 2018, ICML; Allen-Zhu Z., 2017, ICML; Allen-Zhu Zeyuan, 2016, NEURIPS; Allen-Zhu Zeyuan, 2016, ICML; ALLENZHU Z, 2018, NEURIPS; [Anonymous], ICML; [Anonymous], 2013, APPROXIMATION THEORY; Choromanska A., 2015, AISTATS; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Defazio Aaron, 2014, NEURIPS; Garber Dan, 2016, ICML; Goodfellow I.J., 2014, GENERATIVE ADVERSARI; Jin C., 2017, ICML; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lanczos C., 1950, J RES NBS, V45; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Reddi S. J., 2016, ICML; Saad Y, 1992, NUMERICAL METHODS LA; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; SHALEVSHWARTZ S, 2016, ICML; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	30	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303069
C	Cai, RC; Qiao, J; Zhang, K; Zhang, ZJ; Hao, ZF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cai, Ruichu; Qiao, Jie; Zhang, Kun; Zhang, Zhenjie; Hao, Zhifeng			Causal Discovery from Discrete Data using Hidden Compact Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Causal discovery from a set of observations is one of the fundamental problems across several disciplines. For continuous variables, recently a number of causal discovery methods have demonstrated their effectiveness in distinguishing the cause from effect by exploring certain properties of the conditional distribution, but causal discovery on categorical data still remains to be a challenging problem, because it is generally not easy to find a compact description of the causal mechanism for the true causal direction. In this paper we make an attempt to find a way to solve this problem by assuming a two-stage causal process: the first stage maps the cause to a hidden variable of a lower cardinality, and the second stage generates the effect from the hidden representation. In this way, the causal mechanism admits a simple yet compact representation. We show that under this model, the causal direction is identifiable under some weak conditions on the true causal mechanism. We also provide an effective solution to recover the above hidden compact representation within the likelihood framework. Empirical studies verify the effectiveness of the proposed approach on both synthetic and real-world data.	[Cai, Ruichu; Qiao, Jie; Hao, Zhifeng] Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Guangdong, Peoples R China; [Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA; [Zhang, Zhenjie] Yitu Technol Ltd, Singapore R&D, Shanghai, Peoples R China; [Hao, Zhifeng] Foshan Univ, Sch Math & Big Data, Foshan, Peoples R China	Guangdong University of Technology; Carnegie Mellon University; Foshan University	Cai, RC (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Guangdong, Peoples R China.	cairuichu@gdut.edu.cn; qiaojie.chn@gmail.com; kunz1@andrew.cmu.edu; zhenjie.zhang@yitu-inc.com; zfhao@gdut.edu.cn	cai, ruichu/AAX-7200-2021	Qiao, Jie/0000-0002-4581-9656	NSFC-Guangdong Joint Found [U1501254]; Natural Science Foundation of China [61876043, 61472089]; NSF of Guangdong [2014A030306004, 2014A030308008]; Science and Technology Planning Project of Guangdong [2015B010108006, 2015B010131015]; Guangdong High-level Personnel of Special Support Program [2015TQ01X140]; Pearl River S&T Nova Program of Guangzhou [201610010101]; United States Air Force [FA8650-17-C-7715]; National Science Foundation under EAGER [IIS-1829681]; National Institutes of Health [NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, FAIN-U54HG008540]; Department of Defense [FA8702-15-D-0002]; Carnegie Mellon University for the operation of the Software Engineering Institute	NSFC-Guangdong Joint Found; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSF of Guangdong(National Natural Science Foundation of Guangdong Province); Science and Technology Planning Project of Guangdong; Guangdong High-level Personnel of Special Support Program; Pearl River S&T Nova Program of Guangzhou; United States Air Force(United States Department of Defense); National Science Foundation under EAGER; National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Department of Defense(United States Department of Defense); Carnegie Mellon University for the operation of the Software Engineering Institute	This research was supported in part by NSFC-Guangdong Joint Found (U1501254), Natural Science Foundation of China (61876043, 61472089), NSF of Guangdong (2014A030306004, 2014A030308008), Science and Technology Planning Project of Guangdong (2015B010108006, 2015B010131015), Guangdong High-level Personnel of Special Support Program (2015TQ01X140), Pearl River S&T Nova Program of Guangzhou (201610010101). This material is partially based upon work supported by United States Air Force under Contract No. FA8650-17-C-7715, by National Science Foundation under EAGER Grant No. IIS-1829681, and National Institutes of Health under Contract No. NIH-1R01EB022858-01, FAINR01EB022858, NIH-1R01LM012087, NIH-5U54HG008540-02, and FAIN-U54HG008540, and work funded and supported by the Department of Defense under Contract No. FA8702-15-D-0002 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the United States Air Force or the National Institutes of Health or the National Science Foundation. We appreciate the comments from anonymous reviewers, which greatly helped to improve the paper.	Bezdek J. C., 2003, Neural, Parallel & Scientific Computations, V11, P351; Budhathoki K, 2017, IEEE DATA MINING, P751, DOI 10.1109/ICDM.2017.87; Cai R, 2018, AAAI; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; Janzing D, 2012, ARTIF INTELL, V182, P1, DOI 10.1016/j.artint.2012.01.002; Lichman M., 2013, UCI MACHINE LEARNING; Liu FR, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2700477; Liu Furui, 2016, NEURAL COMPUTATION; Pearl J., 1995, STUD LOG FDN MATH, V134, P789, DOI DOI 10.1016/S0049-237X(06)80074-1; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Peters J., 2010, P 13 INT C ART INT S, P597; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Shimizu S, 2011, J MACH LEARN RES, V12, P1225; Spirtes P., 2000, CAUSATION PREDICTION; Zhang K., 2009, P TWENTYFIFTH C UNCE, P647; Zhang K, 2006, P 13 INT C NEUR INF	17	11	12	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302066
C	Chen, JF; Zhu, J; Teh, YW; Zhang, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Jianfei; Zhu, Jun; Teh, Yee Whye; Zhang, Tong			Stochastic Expectation Maximization with Variance Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EM ALGORITHM	Expectation-Maximization (EM) is a popular tool for learning latent variable models, but the vanilla batch EM does not scale to large data sets because the whole data set is needed at every E-step. Stochastic Expectation Maximization (sEM) reduces the cost of E-step by stochastic approximation. However, sEM has a slower asymptotic convergence rate than batch EM, and requires a decreasing sequence of step sizes, which is difficult to tune. In this paper, we propose a variance reduced stochastic EM (sEM-vr) algorithm inspired by variance reduced stochastic gradient descent algorithms. We show that sEM-vr has the same exponential asymptotic convergence rate as batch EM. Moreover, sEM-vr only requires a constant step size to achieve this rate, which alleviates the burden of parameter tuning. We compare sEM-vr with batch EM, sEM and other algorithms on Gaussian mixture models and probabilistic latent semantic analysis, and sEM-vr converges significantly faster than these baselines.	[Chen, Jianfei; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intell Tech & Syst, BNRist Ctr,Inst AI,THBI Lab, Beijing 100084, Peoples R China; [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England; [Zhang, Tong] Tencent AI Lab, Nanjing, Jiangsu, Peoples R China	Tsinghua University; University of Oxford; Tencent	Zhu, J (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intell Tech & Syst, BNRist Ctr,Inst AI,THBI Lab, Beijing 100084, Peoples R China.	chenjian14@mails.tsinghua.edu.cn; dcszj@.tsinghua.edu.cn; y.w.teh@stats.ox.ac.uk; tongzhang@tongzhang-ml.org	Zhang, Tong/HGC-1090-2022		National Key Research and Development Program of China [2017YFA0700904]; NSFC [61620106010, 61621136008, 61332007]; MIIT Grant of Int. Man. Comp. Stan [2016ZXFB00001]; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Siemens; European Research Council under the European Union [617071]; Tencent AI Lab through the Oxford-Tencent Collaboration on Large Scale Machine Learning	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); MIIT Grant of Int. Man. Comp. Stan; Tsinghua Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; Siemens(Siemens AG); European Research Council under the European Union(European Research Council (ERC)); Tencent AI Lab through the Oxford-Tencent Collaboration on Large Scale Machine Learning	We thank Chris Maddison, Adam Foster, and Jin Xu for proofreading. J.C. and J.Z. were supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC projects (Nos. 61620106010, 61621136008, 61332007), the MIIT Grant of Int. Man. Comp. Stan (No. 2016ZXFB00001), Tsinghua Tiangong Institute for Intelligent Computing, the NVIDIA NVAIL Program and a Project from Siemens. YWT was supported by funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071, and from Tencent AI Lab through the Oxford-Tencent Collaboration on Large Scale Machine Learning.	Asuncion A., 2009, P 25 C UNCERTAINTY A, P27, DOI DOI 10.1080/10807030390248483; Asuncion A, 2007, UCI MACHINE LEARNING; Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cappe O, 2011, J COMPUT GRAPH STAT, V20, P728, DOI 10.1198/jcgs.2011.09109; Cappe O, 2009, J ROY STAT SOC B, V71, P593, DOI 10.1111/j.1467-9868.2009.00698.x; Chatterji N., 2018, ARXIV180205431; Chen C., 2017, ARXIV170901180; Chen J., 2015, ADV NEURAL INFORM PR, P1765; Chen JF, 2016, PROC VLDB ENDOW, V9, P744, DOI 10.14778/2977797.2977801; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154; Dupuy Christophe, 2017, J MACHINE LEARNING R, V18, P4581; Durbin R., 1998, BIOL SEQUENCE ANAL P; Foulds J, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P446; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Ishiguro K, 2017, J MACH LEARN RES, V18; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Liang P., 2009, P HUMAN LANGUAGE TEC, P611; Mandt S., 2014, ADV NEURAL INFORM PR, P2438; McLachlan GJ, 2004, FINITE MIXTURE MODEL, DOI [10.1002/0471721182, DOI 10.1002/0471721182]; Mimno D, 2012, ARXIV12066425; Patterson S., 2013, P 26 INT C NEUR INF, P3102; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux N.L., 2012, ADV NEURAL INFORM PR, V25, P2663; Sato Issei, 2012, ICML; Spearman C., 1950, HUMAN ABILITY; TITTERINGTON DM, 1984, J ROY STAT SOC B MET, V46, P257; Wallach H., 2009, P 26 ANN INT C MACHI, V382, P1105, DOI DOI 10.1145/1553374.1553515; Wang Zhaoran, 2014, ARXIV14128729; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Zaheer M, 2016, JMLR WORKSH CONF PRO, V51, P966; Zhang A., 2013, P 22 INT C WORLD WID, P1489; Zhu RD, 2017, PR MACH LEARN RES, V70	39	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002051
C	Dai, B; Fidler, S; Lin, DH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dai, Bo; Fidler, Sanja; Lin, Dahua			A Neural Compositional Paradigm for Image Captioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Mainstream captioning models often follow a sequential structure to generate captions, leading to issues such as introduction of irrelevant semantics, lack of diversity in the generated captions, and inadequate generalization performance. In this paper, we present an alternative paradigm for image captioning, which factorizes the captioning procedure into two stages: (1) extracting an explicit semantic representation from the given image; and (2) constructing the caption based on a recursive compositional procedure in a bottom-up manner. Compared to conventional ones, our paradigm better preserves the semantic content through an explicit factorization of semantics and syntax. By using the compositional generation procedure, caption construction follows a recursive structure, which naturally fits the properties of human language. Moreover, the proposed compositional procedure requires less data to train, generalizes better, and yields more diverse captions.	[Dai, Bo; Lin, Dahua] Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China; [Fidler, Sanja] Univ Toronto, Toronto, ON, Canada; [Fidler, Sanja] Vector Inst, Hyderabad, India; [Fidler, Sanja] NVIDIA, Santa Clara, CA USA	Chinese University of Hong Kong; University of Toronto; Nvidia Corporation	Dai, B (corresponding author), Chinese Univ Hong Kong, CUHK SenseTime Joint Lab, Hong Kong, Peoples R China.	bdai@ie.cuhk.edu.hk; fidler@cs.toronto.edu; dhlin@ie.cuhk.edu.hk	Lin, Dahua/W-6576-2019	Lin, Dahua/0000-0002-8865-7896; Dai, Bo/0000-0003-0777-9232	Big Data Collaboration Research grant from SenseTime Group (CUHK) [TS1610626]; General Research Fund (GRF) of Hong Kong [14236516]	Big Data Collaboration Research grant from SenseTime Group (CUHK); General Research Fund (GRF) of Hong Kong	This work is partially supported by the Big Data Collaboration Research grant from SenseTime Group (CUHK Agreement No. TS1610626), the General Research Fund (GRF) of Hong Kong (No. 14236516).	Anderson P, 2018, PROC CVPR IEEE, P6077, DOI 10.1109/CVPR.2018.00636; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Carnie Andrew., 2013, SYNTAX GENERATIVE IN; Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082; Dai B, 2017, ADV NEUR IN, V30; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Dai Bo, 2018, P EUR C COMP VIS ECC, P282; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Devlin J., 2015, ARXIV150504467; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Klein Dan, 2003, P 41 ANN M ASS COMP; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Kuznetsova Polina, 2014, T ASSOC COMPUT LING, V2, P351, DOI DOI 10.1162/TACL_A_00188; Li S., 2011, P 15 C COMPUTATIONAL, P220; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin Dahua, 2015, BMVC; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Ling Huan, 2017, NIPS; Liu P, 2016, ARXIV160505101, P2873, DOI [10.48550/arXiv.1605.05101, DOI 10.5555/3060832.3061023]; Lu Jiasen, 2016, ABS161201887 CORR; Manning CD, 1999, FDN STAT NATURAL LAN; Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Petrov S., 2007, HUMAN LANGUAGE TECHN, P404; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Socher R., 2013, LONG PAPERS, V1, P455; Tan Y. H., 2016, P AS C COMP VIS, P101; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang YF, 2017, PROC CVPR IEEE, P7378, DOI 10.1109/CVPR.2017.780; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yao T, 2016, ARXIV161101646; Young P., 2014, P TACL, V2, P67, DOI 10.1162/tacl_a_00166; Yu LC, 2015, IEEE I CONF COMP VIS, P2461, DOI 10.1109/ICCV.2015.283	39	11	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300061
C	Dupont, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dupont, Emilien			Learning Disentangled Joint Continuous and Discrete Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a framework for learning disentangled and interpretable jointly continuous and discrete representations in an unsupervised manner. By augmenting the continuous latent distribution of variational autoencoders with a relaxed discrete distribution and controlling the amount of information encoded in each latent unit, we show how continuous and categorical factors of variation can be discovered automatically from data. Experiments show that the framework disentangles continuous and discrete generative factors on various datasets and outperforms current disentangling methods when a discrete generative factor is prominent.	[Dupont, Emilien] Schlumberger Software Technol Innovat Ctr, Menlo Pk, CA 94025 USA		Dupont, E (corresponding author), Schlumberger Software Technol Innovat Ctr, Menlo Pk, CA 94025 USA.	dupont@slb.com						Arjovsky M., 2017, ARXIV170107875; Aubry M, 2014, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2014.487; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Burgess  Christopher, 2017, NIPS 2017 DIS WORKSH; Gao Shuyang, 2018, ARXIV180205822; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Gumbel Emil Julius, 1954, NAT BUR STANDARDS AP, V33; Higgins I, 2017, PR MACH LEARN RES, V70; Higgins  Irina, 2016, ICLR 2017; Higgins  Irina, 2017, ARXIV170703389; Jang E., 2016, ARXIV; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Maddison Chris J, 2016, ARXIV161100712; Makhzani A, 2017, ADV NEUR IN, V30; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Tian Qi Chen, 2018, ARXIV180204942; Whitney W. F., 2016, ARXIV160206822; Xiao H., 2017, ARXIV 170807747; Yang Jimei, 2015, NIPS	26	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300066
C	Gao, YX; Chen, L; Li, BC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gao, Yuanxiang; Chen, Li; Li, Baochun			Post: Device Placement with Cross-Entropy Minimization and Proximal Policy Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Training deep neural networks requires an exorbitant amount of computation resources, including a heterogeneous mix of GPU and CPU devices. It is critical to place operations in a neural network on these devices in an optimal way, so that the training process can complete within the shortest amount of time. The state-of-the-art uses reinforcement learning to learn placement skills by repeatedly performing Monte-Carlo experiments. However, due to its equal treatment of placement samples, we argue that there remains ample room for significant improvements. In this paper, we propose a new joint learning algorithm, called Post, that integrates cross-entropy minimization and proximal policy optimization to achieve theoretically guaranteed optimal efficiency. In order to incorporate the cross-entropy method as a sampling technique, we propose to represent placements using discrete probability distributions, which allows us to estimate an optimal probability mass by maximal likelihood estimation, a powerful tool with the best possible efficiency. We have implemented Post in the Google Cloud platform, and our extensive experiments with several popular neural network training benchmarks have demonstrated clear evidence of superior performance: with the same amount of learning time, it leads to placements that have training times up to 63.7% shorter over the state-of-the-art.	[Gao, Yuanxiang; Li, Baochun] Univ Toronto, Dept Elect & Comp Engn, Toronto, ON, Canada; [Gao, Yuanxiang] Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu, Sichuan, Peoples R China; [Chen, Li] Univ Louisiana Lafayette, Sch Comp & Informat, Lafayette, LA 70504 USA	University of Toronto; University of Electronic Science & Technology of China; University of Louisiana Lafayette	Gao, YX (corresponding author), Univ Toronto, Dept Elect & Comp Engn, Toronto, ON, Canada.; Gao, YX (corresponding author), Univ Elect Sci & Technol China, Sch Informat & Commun Engn, Chengdu, Sichuan, Peoples R China.	yuanxiang@ece.utoronto.ca; li.chen@louisiana.edu; bli@ece.toronto.edu	Chen, Li/ABD-5671-2020					Boyd S, 2004, CONVEX OPTIMIZATION; De Boer Pieter-Tjerk, 2005, ANN OPERATIONS RES; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gao YX, 2018, PR MACH LEARN RES, V80; Goschin S., 2013, INT C MACH LEARN ICM; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Heess N., 2017, ABS170702286 CORR; Jozefowicz R., 2016, ARXIV PREPRINT ARXIV; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Mannor S., 2003, P INT C MACH LEARN I; Mirhoseini A, 2017, PR MACH LEARN RES, V70; Mirhoseini Azalia, 2018, P INT C LEARNING REP; Papoulis A, 2002, PROBABILITY RANDOM V, V4th; Rubinstein R., 2004, CROSS ENTROPY METHOD; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shulman J., 2015, P INT C MACH LEARN I; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szita I., 2006, NEURAL COMPUTATION; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144	21	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004052
C	Imani, E; Graves, E; White, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Imani, Ehsan; Graves, Eric; White, Martha			An Off-policy Policy Gradient Theorem Using Emphatic Weightings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Policy gradient methods are widely used for control in reinforcement learning, particularly for the continuous action setting. There have been a host of theoretically sound algorithms proposed for the on-policy setting, due to the existence of the policy gradient theorem which provides a simplified form for the gradient. In off-policy learning, however, where the behaviour policy is not necessarily attempting to learn and follow the optimal policy for the given task, the existence of such a theorem has been elusive. In this work, we solve this open problem by providing the first off-policy policy gradient theorem. The key to the derivation is the use of emphatic weightings. We develop a new actor-critic algorithm-called Actor Critic with Emphatic weightings (ACE)-that approximates the simplified gradients provided by the theorem. We demonstrate in a simple counterexample that previous off-policy policy gradient methods-particularly OffPAC and DPG-converge to the wrong solution whereas ACE finds the optimal solution.	[Imani, Ehsan; Graves, Eric; White, Martha] Univ Alberta, Reinforcement Learning & Artificial Intelligence, Dept Comp Sci, Edmonton, AB, Canada	University of Alberta	Imani, E (corresponding author), Univ Alberta, Reinforcement Learning & Artificial Intelligence, Dept Comp Sci, Edmonton, AB, Canada.	imani@ualberta.ca; graves@ualberta.ca; whitem@ualberta.ca	White, Martha/AAF-7066-2020	White, Martha/0000-0002-5356-2950	Alberta Innovates; Alberta Machine Intelligence Institute	Alberta Innovates; Alberta Machine Intelligence Institute	The authors would like to thank Alberta Innovates for funding the Alberta Machine Intelligence Institute and by extension this research. We would also like to thank Hamid Maei, Susan Murphy, and Rich Sutton for their helpful discussions and insightful comments.	[Anonymous], 2000, THESIS; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Bhatnagar S., 2009, AUTOMATICA; Bhatnagar S., 2008, ADV NEURAL INFORM PR, P105; Degris T, 2012, P AMER CONTR CONF, P2177; Degris Thomas, 2012, INT C MACH LEARN; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595; Gu, 2017, ADV NEURAL INFORM PR, V30, P3846; Gu Shixiang, 2016, ARXIV161102247; Konda VR, 2000, ADV NEUR IN, V12, P1008; Lillicrap TP, 2016, 4 INT C LEARN REPR; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Maei H. R., 2018, ARXIV180207842; Maei Hamid Reza, 2011, THESIS; Marbach P, 2001, IEEE T AUTOMAT CONTR, V46, P191, DOI 10.1109/9.905687; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Peters J, 2005, LECT NOTES ARTIF INT, V3720, P280, DOI 10.1007/11564096_29; Schaul T, 2016, C TRACK P, P1; Silver D, 2014, P 31; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton R. S., 1988, MACHINE LEARNING; Sutton R. S., 2016, J MACHINE LEARNING R; Sutton R. S., 2000, ADV NEURAL INFORM PR; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Thomas PS, 2014, PR MACH LEARN RES, V32; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Weaver Lex., 2001, P 17 C UNC ART INT U, P538; White Adam, 2015, THESIS; White M, 2017, PR MACH LEARN RES, V70; Williams R. J., 1992, MACHINE LEARNING; WITTEN IH, 1977, INFORM CONTROL, V34, P286, DOI 10.1016/S0019-9958(77)90354-0; Yu Huizhen, 2015, ANN C LEARN THEOR; Ziyu Wang, 2016, SAMPLE EFFICIENT ACT	34	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300010
C	Jiang, P; Gu, FL; Wang, YH; Tu, CH; Chen, BQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Peng; Gu, Fanglin; Wang, Yunhai; Tu, Changhe; Chen, Baoquan			DifNet: Semantic Segmentation by Diffusion Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep Neural Networks (DNNs) have recently shown state of the art performance on semantic segmentation tasks, however, they still suffer from problems of poor boundary localization and spatial fragmented predictions. The difficulties lie in the requirement of making dense predictions from a long path model all at once, since details are hard to keep when data goes through deeper layers. Instead, in this work, we decompose this difficult task into two relative simple sub-tasks: seed detection which is required to predict initial predictions without the need of wholeness and preciseness, and similarity estimation which measures the possibility of any two nodes belong to the same class without the need of knowing which class they are. We use one branch network for one sub-task each, and apply a cascade of random walks base on hierarchical semantics to approximate a complex diffusion process which propagates seed information to the whole image according to the estimated similarities. The proposed DifNet consistently produces improvements over the baseline models with the same depth and with the equivalent number of parameters, and also achieves promising performance on Pascal VOC and Pascal Context dataset. Our DifNet is trained end-to-end without complex loss functions.	[Jiang, Peng; Gu, Fanglin; Wang, Yunhai; Tu, Changhe; Chen, Baoquan] Shandong Univ, Jinan, Shandong, Peoples R China; [Chen, Baoquan] Peking Univ, Beijing, Peoples R China	Shandong University; Peking University	Jiang, P (corresponding author), Shandong Univ, Jinan, Shandong, Peoples R China.	sdujump@gmail.com; fanglin.gu@gmail.com; cloudseawang@gmail.com; chtu@sdu.edu.cn; baoquan.chen@gmail.com			National Natural Science Foundation of China [61702301, 61332015]; China Postdoctoral Science Foundation [2017M612272]; Fundamental Research Funds of Shandong University; National Basic Research grant (973) [2015CB352501]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); China Postdoctoral Science Foundation(China Postdoctoral Science Foundation); Fundamental Research Funds of Shandong University; National Basic Research grant (973)(National Basic Research Program of China)	This work was supported by the grants of National Natural Science Foundation of China (61702301), China Postdoctoral Science Foundation funded project (2017M612272), Fundamental Research Funds of Shandong University, National Natural Science Foundation of China (61332015) and National Basic Research grant (973) (2015CB352501).	Berg A.C., 2015, ARXIV150604579; Bertasius G, 2017, PROC CVPR IEEE, P6137, DOI 10.1109/CVPR.2017.650; Bertasius G, 2016, PROC CVPR IEEE, P3602, DOI 10.1109/CVPR.2016.392; Chandra S., 2016, EUR C COMP VIS ECCV; Chandra S, 2017, IEEE I CONF COMP VIS, P5113, DOI 10.1109/ICCV.2017.546; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Jampani V, 2016, PROC CVPR IEEE, P4452, DOI 10.1109/CVPR.2016.482; Jiang P., 2015, IEEE INT C COMP VIS; Lin GS, 2016, PROC CVPR IEEE, P3194, DOI 10.1109/CVPR.2016.348; Liu SF, 2017, ADV NEUR IN, V30; Liu ST, 2018, LECT NOTES COMPUT SC, V11215, P404, DOI 10.1007/978-3-030-01252-6_24; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Vemulapalli R, 2016, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2016.351; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Xie S., 2016, EUR C COMP VIS ECCV; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	25	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301060
C	Kanai, S; Fujiwara, Y; Yamanaka, Y; Adachi, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kanai, Sekitoshi; Fujiwara, Yasuhiro; Yamanaka, Yuki; Adachi, Shuichi			Sigsoftmax: Reanalysis of the Softmax Bottleneck	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Softmax is an output activation function for modeling categorical probability distributions in many applications of deep learning. However, a recent study revealed that softmax can be a bottleneck of representational capacity of neural networks in language modeling (the softmax bottleneck). In this paper, we propose an output activation function for breaking the softmax bottleneck without additional parameters. We re-analyze the softmax bottleneck from the perspective of the output set of log-softmax and identify the cause of the softmax bottleneck. On the basis of this analysis, we propose sigsoftmax, which is composed of a multiplication of an exponential function and sigmoid function. Sigsoftmax can break the softmax bottleneck. The experiments on language modeling demonstrate that sigsoftmax and mixture of sigsoftmax outperform softmax and mixture of softmax, respectively.	[Kanai, Sekitoshi] Keio Univ, NTT Software Innovat Ctr, Tokyo, Japan; [Fujiwara, Yasuhiro] NTT Software Innovat Ctr, Tokyo, Japan; [Yamanaka, Yuki] NTT Secure Platform Labs, Tokyo, Japan; [Adachi, Shuichi] Keio Univ, Tokyo, Japan	Keio University; Nippon Telegraph & Telephone Corporation; Keio University	Kanai, S (corresponding author), Keio Univ, NTT Software Innovat Ctr, Tokyo, Japan.	kanai.sekitoshi@lab.ntt.co.jp; fujiwara.yasuhiro@lab.ntt.co.jp; yamanaka.yuki@lab.ntt.co.jp; adachi.shuichi@appi.keio.ac.jp						[Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Bishop, 1995, NEURAL NETWORKS PATT; Bishop C.M, 2006, PATTERN RECOGN; Bridle J. S., 1990, PROC 2 INT C NEURAL, P211, DOI [10.5555/2969830, DOI 10.5555/2969830]; Bridle John S, 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28; Chelba Ciprian, 2013, TECHNICAL REPORT; Chen B., 2017, P IEEE C COMP VIS PA, P5372; Cho K., 2014, P 2014 C EMP METH NA, P1724; de Brebisson  Alexandre, 2016, P ICLR; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Joulin A., 2017, P INT C MACH LEARN I, P1302; Kahembwe Emmanuel, 2017, ARXIV170907432; Kanai Sekitoshi, 2017, P INT C ADV NEUR NFI, P435; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Mahoney Matt, 2011, LARGE TEXT COMPRESSI; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Martins AFT, 2016, PR MACH LEARN RES, V48; Memisevic R., 2010, ADV NEURAL INFORM PR, V23, P1603; Merity S., 2018, P ICLR; Merity  Stephen, 2017, P ICLR; Mikolov T.A., 2012, STAT LANGUAGE MODELS; Mohassel P, 2017, P IEEE S SECUR PRIV, P19, DOI 10.1109/SP.2017.12; Nair V., 2010, ICML, P807; Ollivier Y., 2013, ARXIV13030818; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Press W.H., 2007, NUMERICAL RECIPES; Shim K., 2017, ADV NEURAL INFORM PR, P5469; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Sutskever I., 2014, ARXIV; Titsias M. K., 2016, ADV NEURAL INFORM PR, P4161; Yang Z., 2018, P ICLR	34	11	11	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300027
C	Kim, MP; Reingold, O; Rothblum, GN		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kim, Michael P.; Reingold, Omer; Rothblum, Guy N.			Fairness Through Computationally-Bounded Awareness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of fair classification within the versatile framework of Dwork et al. [6], which assumes the existence of a metric that measures similarity between pairs of individuals. Unlike earlier work, we do not assume that the entire metric is known to the learning algorithm; instead, the learner can query this arbitrary metric a bounded number of times. We propose a new notion of fairness called metric multifairness and show how to achieve this notion in our setting. Metric multifairness is parameterized by a similarity metric d on pairs of individuals to classify and a rich collection C of (possibly overlapping) "comparison sets" over pairs of individuals. At a high level, metric multifairness guarantees that similar subpopulations are treated similarly, as long as these subpopulations are identified within the class C.	[Kim, Michael P.; Reingold, Omer] Stanford Univ, Stanford, CA 94305 USA; [Rothblum, Guy N.] Weizmann Inst Sci, Rehovot, Israel	Stanford University; Weizmann Institute of Science	Kim, MP (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	mpk@cs.stanford.edu; reingold@stanford.edu; rothblum@alum.mit.edu						Angwin J., 2016, PROPUBLICA, P254; Bogdanov A, 2017, INFORM SEC CRYPT TEX, P79, DOI 10.1007/978-3-319-57048-8_3; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Chouldechova A., 2017, BIG DATA; Corbett-Davies S., 2017, KDD; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Feldman V, 2012, SIAM J COMPUT, V41, P1558, DOI 10.1137/120865094; Feldman Vitaly, 2010, P 1 S INN COMP SCI 1; Feller Avi, 2016, WASHINGTON POST; Gillen Stephen, 2018, ARXIV180206936; Goldreich O., 1984, 25th Annual Symposium on Foundations of Computer Science (Cat. No. 84CH2085-9), P464, DOI 10.1109/SFCS.1984.715949; Gopalan P, 2008, ACM S THEORY COMPUT, P527; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; HEBERT- JOHNSON U., 2018, ICML; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Kearns M., 2018, ICML; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Kim Michael P, 2018, ARXIV180512317; Kleinberg Jon M., 2017, ITCS; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Pleiss G., 2017, NIPS; ROTHBLUM G. N., 2018, ICML; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Waddell Kaveh, 2016, ATLANTIC; Woodworth B., 2017, COLT	25	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304082
C	Liang, C; Norouzi, M; Berant, J; Le, Q; Lao, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liang, Chen; Norouzi, Mohammad; Berant, Jonathan; Quoc Le; Lao, Ni			Memory Augmented Policy Optimization for Program Synthesis and Semantic Parsing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present Memory Augmented Policy Optimization (MAPO), a simple and novel way to leverage a memory buffer of promising trajectories to reduce the variance of policy gradient estimates. MAPO is applicable to deterministic environments with discrete actions, such as structured prediction and combinatorial optimization. Our key idea is to express the expected return objective as a weighted sum of two terms: an expectation over the high-reward trajectories inside a memory buffer, and a separate expectation over trajectories outside of the buffer. To design an efficient algorithm based on this idea, we propose: (1) memory weight clipping to accelerate and stabilize training; (2) systematic exploration to discover high-reward trajectories; (3) distributed sampling from inside and outside of the memory buffer to speed up training MAPO improves the sample efficiency and robustness of policy gradient, especially on tasks with sparse rewards. We evaluate MAPO on weakly supervised program synthesis from natural language (semantic parsing). On the WIKITABEQUESTIONS benchmark, we improve the state-of-the-art by 2.6%, achieving an accuracy of 46.3%. On the WIKISQL benchmark, MAPO achieves an accuracy of 74.9% with only weak supervision, outperforming several strong baselines with full supervision. Our source code is available at goo.gl/TXBp4e.	[Liang, Chen; Norouzi, Mohammad; Quoc Le] Google Brain, Mountain View, CA 94039 USA; [Berant, Jonathan] Tel Aviv Univ, AI2, Tel Aviv, Israel; [Lao, Ni] SayMosaic Inc, Palo Alto, CA USA	Google Incorporated; Tel Aviv University	Liang, C (corresponding author), Google Brain, Mountain View, CA 94039 USA.	crazydonkey200@gmail.com; mnorouzi@google.com; joberant@cs.tau.ac.il; qvl@google.com; ni.lao@mosaix.ai			Israel Science Foundation [942/16]	Israel Science Foundation(Israel Science Foundation)	We would like to thank Dan Abolafia, Ankur Taly, Thanapon Noraset, Arvind Neelakantan, Wenyun Zuo, Chenchen Pan and Mia Liang for helpful discussions. Jonathan Berant was partially supported by The Israel Science Foundation grant 942/16.	Abadi M., TENSORFLOW LARGE SCA; Abolafia D. A., 2018, ARXIV180103526; Andrychowicz M., 2017, ADV NEURAL INFORM PR; [Anonymous], 2018, ARXIV180201561; [Anonymous], 2015, ARXIV PREPRINT ARXIV; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, ARXIV170306585; [Anonymous], 2015, ACL; Balog M., 2017, ICLR; Bellemare Marc G, 2013, JMLR; Bello I., 2016, ARXIV PREPRINT ARXIV; Berant J., 2013, EMNLP, V2, P6; Brockman G., 2016, OPENAI GYM; Bunel R., 2018, INT C LEARN REPR; Chen Liang, 2017, ACL; Degris Thomas, 2012, ICML; Grathwohl W., 2017, ARXIV171100123; Guu K., 2017, ACL; Haug Till, 2018, ECIR; Hester Todd, 2018, AAAI; Hochreiter S., 1997, NEURAL COMPUT; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Huang Po-Han, 2018, CORR; Konda VR, 2000, ADV NEUR IN, V12, P1008; Krishnamurthy Jayant, 2017, EMNLP; Le Roux Nicolas, 2017, ICLR; Li D., 2018, CORR; Li Jiwei, 2016, P 2016 C EMP METH NA; Liang P., 2011, ACL; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Liu H., 2017, ARXIV171011198; Mania H., 2018, ARXIV PREPRINT ARXIV; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mudrakarta Pramod Kaushik, 2018, ARXIV180304579; Nachum O, 2017, NIPS, P2775; Neelakantan Arvind, 2016, INT C LEARN REPR ICL; Nichol A., 2018, ARXIV180403720; Norouzi M, 2016, ADV NEUR IN, V29; Nowozin S, 2010, FOUND TRENDS COMPUT, V6, pX, DOI 10.1561/0600000033; Oh Junhyuk, 2018, ICML; Pasupat P, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P23; Pasupat Panupong, 2016, ACL; Pathak D., 2017, ICML; Pennington Jeffrey, 2014, EMNLP; Peters Jan, 2006, IROS; Rajeswaran A., 2017, NIPS; Ranzato M., 2016, ICLR; Ross St<prime>ephane, 2011, AISTATS; Schaul T., 2016, ICLR; Schulman J., 2015, ICML; Schulman J., 2017, ABS170706347 CORR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Silver David, 2017, NATURE; Sun Yibo, 2018, ARXIV180408338; Tang H, 2017, NIPS, V30, P2753; Wang Chenglong, 2018, ICLR; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Cathy, 2018, ICLR; Wu Y., 2016, GOOGLES NEURAL MACHI; Xu Xiaojun, 2018, ICLR; Yih Wen-tau, 2016, ACL; Yu Tao, 2018, ARXIV180409769; Zelle JM, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1050; Zettlemoyer Luke S., 2005, P 21 C UNC ART INT, P658, DOI DOI 10.3115/1690219.1690283; Zhang Yuchen, 2017, ACL; Zhong Victor, 2017, CORR; Ziyu Wang, 2017, ICLR; Zoph B., 2016, ICLR; Zoph B., 2017, ARXIV170707012; Zoph Barret, 2016, ICLR	70	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004054
C	Liu, ZH; Xu, JZ; Peng, XL; Xiong, RQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Zhenhua; Xu, Jizheng; Peng, Xiulian; Xiong, Ruiqin			Frequency-Domain Dynamic Pruning for Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep convolutional neural networks have demonstrated their powerfulness in a variety of applications. However, the storage and computational requirements have largely restricted their further extensions on mobile devices. Recently, pruning of unimportant parameters has been used for both network compression and acceleration. Considering that there are spatial redundancy within most filters in a CNN, we propose a frequency-domain dynamic pruning scheme to exploit the spatial correlations. The frequency-domain coefficients are pruned dynamically in each iteration and different frequency bands are pruned discriminatively, given their different importance on accuracy. Experimental results demonstrate that the proposed scheme can outperform previous spatial-domain counterparts by a large margin. Specifically, it can achieve a compression ratio of 8.4x and a theoretical inference speed-up of 9.2 x for ResNet-110, while the accuracy is even better than the reference model on CIFAR-10.	[Liu, Zhenhua; Xiong, Ruiqin] Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing, Peoples R China; [Xu, Jizheng; Peng, Xiulian] Microsoft Res Asia, Beijing, Peoples R China	Peking University; Microsoft; Microsoft Research Asia	Liu, ZH (corresponding author), Peking Univ, Inst Digital Media, Sch Elect Engn & Comp Sci, Beijing, Peoples R China.	liu-zh@pku.edu.cn; jzxu@microsoft.com; xipe@microsoft.com; rqxiong@pku.edu.cn			National Key Research and Development Program of China [2017YFB1002203]; National Natural Science Foundation of China [61772041]; Beijing Natural Science Foundation [4172027]; Cooperative Medianet Innovation Center	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Cooperative Medianet Innovation Center	This work was part supported by the National Key Research and Development Program of China (2017YFB1002203), the National Natural Science Foundation of China (61772041), the Beijing Natural Science Foundation (4172027), and also by the Cooperative Medianet Innovation Center. This work was done when Z. Liu was with Microsoft Research Asia.	Chen WL, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1475, DOI 10.1145/2939672.2939839; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Guo YW, 2016, ADV NEUR IN, V29; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kim Y., 2016, ICLR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li ZF, 2017, IEEE I CONF COMP VIS, P2603, DOI 10.1109/ICCV.2017.282; Louizos C, 2017, ADV NEUR IN, V30; Mellempudi N., 2017, ARXIV170501462; Molchanov D, 2017, PR MACH LEARN RES, V70; Neklyudov Kirill, 2017, ADV NEURAL INFORM PR, V30; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Tai C., 2016, ICLR POST; Ullrich Karen, 2017, ICLR; Wang  P., 2016, COMPUTER VISION PATT, P4012; Wang YH, 2016, ADV NEUR IN, V29, P253; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Zhang XY, 2015, PROC CVPR IEEE, P1984, DOI 10.1109/CVPR.2015.7298809; Zhou A, 2017, INCREMENTAL NETWORK; Zhu Chenzhuo, 2016, INT C LEARN REPR	26	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301007
C	Milios, D; Camoriano, R; Michiardi, P; Rosasco, L; Filippone, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Milios, Dimitrios; Camoriano, Raffaello; Michiardi, Pietro; Rosasco, Lorenzo; Filippone, Maurizio			Dirichlet-based Gaussian Processes for Large-scale Calibrated Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper studies the problem of deriving fast and accurate classification algorithms with uncertainty quantification. Gaussian process classification provides a principled approach, but the corresponding computational burden is hardly sustainable in large-scale problems and devising efficient alternatives is a challenge. In this work, we investigate if and how Gaussian process regression directly applied to classification labels can be used to tackle this question. While in this case training is remarkably faster, predictions need to be calibrated for classification and uncertainty estimation. To this aim, we propose a novel regression approach where the labels are obtained through the interpretation of classification labels as the coefficients of a degenerate Dirichlet distribution. Extensive experimental results show that the proposed approach provides essentially the same accuracy and uncertainty quantification as Gaussian process classification while requiring only a fraction of computational resources.	[Milios, Dimitrios; Michiardi, Pietro; Filippone, Maurizio] EURECOM, Sophia Antipolis, France; [Camoriano, Raffaello; Rosasco, Lorenzo] IIT, LCSL, Genoa, Italy; [Camoriano, Raffaello; Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA; [Rosasco, Lorenzo] Univ Genoa, DIBRIS, Genoa, Italy	IMT - Institut Mines-Telecom; EURECOM; Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT); University of Genoa	Milios, D (corresponding author), EURECOM, Sophia Antipolis, France.	dimitrios.milios@eurecom.fr; raffaello.camoriano@iit.it; pietro.michiardi@eurecom.fr; lrosasco@mit.edu; maurizio.filippone@eurecom.fr	Camoriano, Raffaello/GWN-0743-2022	Filippone, Maurizio/0000-0001-7294-472X	Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]; Italian Institute of Technology; AFOSR [FA9550-17-1-0390, BAA-AFRL-AFOSR-2016-0007]; EU H2020-MSCA-RISE project NoMADS [DLV-777826]; KPMG; AXA Research Fund	Center for Brains, Minds and Machines (CBMM) - NSF STC award; Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); EU H2020-MSCA-RISE project NoMADS; KPMG; AXA Research Fund(AXA Research Fund)	L. R. acknowledges the financial support of the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, the Italian Institute of Technology, the AFOSR projects FA9550-17-1-0390 and BAA-AFRL-AFOSR-2016-0007 (European Office of Aerospace Research and Development), and the EU H2020-MSCA-RISE project NoMADS -DLV-777826. R. C. and L. R. gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research. DM and PM are partially supported by KPMG. MF gratefully acknowledges support from the AXA Research Fund.	Asuncion A, 2007, UCI MACHINE LEARNING; Baldassarre L, 2012, MACH LEARN, V87, P259, DOI 10.1007/s10994-012-5282-y; Bartlett P., 2006, J AM STAT ASS; Camoriano R, 2016, JMLR WORKSH CONF PRO, V51, P1403; Flach P. A., 2016, CLASSIFIER CALIBRATI, P1; Guo CA, 2017, PR MACH LEARN RES, V70; HASTIE T., 2001, MATH INTELL, V27, P83, DOI [10.1198/jasa.2004.s339., DOI 10.1198/JASA.2004.S339]; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Hernandez-Lobato D, 2016, JMLR WORKSH CONF PRO, V51, P168; Huang P., 2014, IEEE INT C AC SPEECH; Kendall A., 2017, WHAT UNCERTAINTIES W; Kumar S., 2009, ADV NEURAL INFORM PR, P1060; Kurakin A., 2017, ARXIV160702533; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Lu Z., 2014, CORR; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Minka T.P., 2001, P 17 C UNC ART INT, P362; Mroueh Y., 2012, ADV NEURAL INFORM PR, P2789; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Platt J., 1999, ADV LARGE MARGIN CLA, V10; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rifkin R, 2003, NATO SCI SERIES 3, V190, P131, DOI DOI 10.1016/S0072-9752(06)80038-2; Rudi A., 2015, ADV NEURAL INFORM PR, V28, P1657, DOI DOI 10.5555/2969239.2969424; Rudi A., 2017, ADV NEURAL INFORM PR, V30, P3888; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Si S, 2014, PR MACH LEARN RES, V32; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Suykens JAK, 1999, NEURAL PROCESS LETT, V9, P293, DOI 10.1023/A:1018628609742; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Wilson A. G., 2016, P 30 INT C NEUR INF, P2594	37	11	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000050
C	Shu, R; Bui, HH; Zhao, SJ; Kochenderfer, MJ; Ermon, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shu, Rui; Bui, Hung H.; Zhao, Shengjia; Kochenderfer, Mykel J.; Ermon, Stefano			Amortized Inference Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				POSTERIOR REGULARIZATION	The variational autoencoder (VAE) is a popular model for density estimation and representation learning. Canonically, the variational principle suggests to prefer an expressive inference model so that the variational approximation is accurate. However, it is often overlooked that an overly-expressive inference model can be detrimental to the test set performance of both the amortized posterior approximator and, more importantly, the generative density estimator. In this paper, we leverage the fact that VAEs rely on amortized inference and propose techniques for amortized inference regularization (AIR) that control the smoothness of the inference model. We demonstrate that, by applying AIR, it is possible to improve VAE generalization on both inference and generative performance. Our paper challenges the belief that amortized inference is simply a mechanism for approximating maximum likelihood training and illustrates that regularization of the amortization family provides a new direction for understanding and improving generalization in VAEs.	[Shu, Rui; Zhao, Shengjia; Kochenderfer, Mykel J.; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA; [Bui, Hung H.] DeepMind, London, England	Stanford University	Shu, R (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ruishu@stanford.edu; buih@google.com; sjzhao@stanford.edu; mykel@stanford.edu; ermon@cs.stanford.edu			NSF [1651565, 1522054, 1733686]; TRI; ONR; Sony; FLI; Toyota Research Institute	NSF(National Science Foundation (NSF)); TRI; ONR(Office of Naval Research); Sony; FLI; Toyota Research Institute	This research was supported by TRI, NSF (#1651565, #1522054, #1733686), ONR, Sony, and FLI. Toyota Research Institute provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity.	Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bousquet O., 2017, TECH REP; Burda Yuri, 2015, ARXIV150900519; Chen T.Q., 2018, NEURIPS, P2610; Cremer C, 2018, PR MACH LEARN RES, V80; Cremer Chris, 2017, ARXIV170402916; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dinh L, 2017, PR MACH LEARN RES, V70; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Hoffman MD, 2017, PR MACH LEARN RES, V70; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Im DJ, 2017, AAAI CONF ARTIF INTE, P2059; Kim H, 2018, PR MACH LEARN RES, V80; Kim Y, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krishnan Rahul G, 2017, ARXIV171006085; Maaloe L, 2016, PR MACH LEARN RES, V48; Masters D., 2018, ARXIV, DOI DOI 10.48550/ARXIV.1804.07612; Rainforth Tom, 2018, ARXIV180204537; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Smith Samuel L., 2018, REPRESENTATIONS; Sonderby CK, 2016, ADV NEUR IN, V29; Tomczak J. M., 2017, ARXIV170507120; Wu Y., 2016, ARXIV161104273; Zhang Chiyuan, 2016, ARXIV161103530; Zhu J, 2014, J MACH LEARN RES, V15, P1799	33	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304041
C	Sun, YT; Gilbert, A; Tewari, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Sun, Yitong; Gilbert, Anna; Tewari, Ambuj			But How Does It Work in Theory? Linear SVM with Random Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We prove that, under low noise assumptions, the support vector machine with N << m random features (RFSVM) can achieve the learning rate faster than O(1/root m) on a training set with m samples when an optimized feature map is used. Our work extends the previous fast rate analysis of random features method from least square loss to 0-1 loss. We also show that the reweighted feature selection method, which approximates the optimized feature map, helps improve the performance of RFSVM in experiments on a synthetic data set.	[Sun, Yitong; Gilbert, Anna] Univ Michigan, Dept Math, Ann Arbor, MI 48109 USA; [Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan; University of Michigan System; University of Michigan	Sun, YT (corresponding author), Univ Michigan, Dept Math, Ann Arbor, MI 48109 USA.	syitong@umich.edu; annacg@umich.edu; tewaria@umich.edu			Sloan Research Fellowship; Simons Foundation Fellowship	Sloan Research Fellowship(Alfred P. Sloan Foundation); Simons Foundation Fellowship	AT acknowledges the support of a Sloan Research Fellowship.; ACG acknowledges the support of a Simons Foundation Fellowship.	Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Cortes C, 2010, P 13 INT C ART INT S, P113; Cucker F, 2002, B AM MATH SOC, V39, P1; Dai B., 2014, NIPS; Harchaoui Z., 2008, ADV NEURAL INFORM PR, P609; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; LAX P. D., 2002, FUNCTIONAL ANAL PURE; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudi Alessandro, 2017, ADV NEURAL INFORM PR, P3218; Scovel C, 2010, J COMPLEXITY, V26, P641, DOI 10.1016/j.jco.2010.03.002; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Steinwart I., 2008, SUPPORT VECTOR MACHI; Sutherland Dougal J., 2015, CORR; Widom Harold, 1963, T AM MATH SOC, P278, DOI 10.1090/S0002-9947-1963-0155161-0; Yang T., 2012, ADV NEURAL INFORM PR, P476; Zhang K., 2012, ARTIF INTELL STAT, P1425	20	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303038
C	Wang, GR; Peng, JF; Luo, P; Wang, XJ; Lin, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Guangrun; Peng, Jiefeng; Luo, Ping; Wang, Xinjiang; Lin, Liang			Kalman Normalization: Normalizing Internal Representations Across Network Layers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					As an indispensable component, Batch Normalization (BN) has successfully improved the training of deep neural networks (DNNs) with mini-batches, by normalizing the distribution of the internal representation for each hidden layer. However, the effectiveness of BN would diminish with the scenario of micro-batch (e.g. less than 4 samples in a mini-batch), since the estimated statistics in a mini-batch are not reliable with insufficient samples. This limits BN's room in training larger models on segmentation, detection, and video-related problems, which require small batches constrained by memory consumption. In this paper, we present a novel normalization method, called Kalman Normalization (KN), for improving and accelerating the training of DNNs, particularly under the context of micro-batches. Specifically, unlike the existing solutions treating each hidden layer as an isolated system, KN treats all the layers in a network as a whole system, and estimates the statistics of a certain layer by considering the distributions of all its preceding layers, mimicking the merits of Kalman Filtering. On ResNet50 trained in ImageNet, KN has 3.4% lower error than its BN counterpart when using a batch size of 4; Even when using typical batch sizes, KN still maintains an advantage over BN while other BN variants suffer a performance degradation. Moreover, KN can be naturally generalized to many existing normalization variants to obtain gains, e.g.equipping Group Normalization [34] with Group Kalman Normalization (GKN). KN can outperform BN and its variants for large scale object detection and segmentation task in COCO 2017.	[Wang, Guangrun; Peng, Jiefeng; Lin, Liang] Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China; [Luo, Ping] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Wang, Xinjiang] SenseTime Grp Ltd, Hong Kong, Peoples R China	Sun Yat Sen University; Chinese University of Hong Kong	Lin, L (corresponding author), Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China.	wanggrun@mail2.sysu.edu.cn; jiefengpeng@gmail.com; pluo.lhi@gmail.com; linliang@ieee.org	Luo, Ping/GPG-2707-2022; Luo, Ping/HGE-7623-2022	Luo, Ping/0000-0002-6685-7950	National Key Research and Development Program of China [2018YFC0830103]; National High Level Talents Special Support Plan (Ten Thousand Talents Program); National Natural Science Foundation of China (NSFC) [61622214, 61503366]	National Key Research and Development Program of China; National High Level Talents Special Support Plan (Ten Thousand Talents Program); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Key Research and Development Program of China under Grant No. 2018YFC0830103, in part by National High Level Talents Special Support Plan (Ten Thousand Talents Program), and in part by National Natural Science Foundation of China (NSFC) under Grant No. 61622214, and 61503366.	Arpit D, 2016, PR MACH LEARN RES, V48; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Desjardins G., 2015, P 28 C NEUR PROC SYS, P2071; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Girshick R., 2015, ICCV; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Huang L., 2017, ARXIV170906079; Huang Lei, 2018, IEEE CVPR; Ioffe Sergey, 2017, NEURIPS; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kuen J, 2017, IEEE INT CONF COMP V, P958, DOI 10.1109/ICCVW.2017.117; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Luo Ping, 2017, ICML; Luo Ping, 2017, IJCAI; Nair V, 2010, P 27 INT C MACHINE L, P807; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Povey D, 2014, PARALLEL TRAINING DE; Raiko T, 2012, P INT C ART INT STAT, V22, P924; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tim Cooijmans, 2016, ARXIV160309025; Ulyanov D., 2016, ARXIV160708022; Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463; Wang GC, 2017, IEEE I CONF COMP VIS, P2831, DOI 10.1109/ICCV.2017.306; Wiesler Simon, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P180, DOI 10.1109/ICASSP.2014.6853582; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1	34	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300003
C	Xu, Y; Jin, R; Yang, TB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Yi; Jin, Rong; Yang, Tianbao			First-order Stochastic Algorithms for Escaping From Saddle Points in Almost Linear Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we consider first-order methods for solving stochastic non-convex optimization problems. The key building block of the proposed algorithms is first-order procedures to extract negative curvature from the Hessian matrix through a principled sequence starting from noise, which are referred to NEgative-curvature-Originated-from-Noise or NEON and are of independent interest. Based on this building block, we design purely first-order stochastic algorithms for escaping from non-degenerate saddle points with a much better time complexity (almost linear time in the problem's dimensionality) under a bounded variance condition of stochastic gradients than previous first-order stochastic algorithms. In particular, we develop a general framework of first-order stochastic algorithms with a second-order convergence guarantee based on our new technique and existing algorithms that may only converge to a first-order stationary point. For finding a nearly second-order stationary point x such that parallel to del F(x)parallel to <= epsilon and del F-2(x) >= -root epsilon I (in high probability), the best time complexity of the presented algorithms is (O) over tilde (d/epsilon(3.5)), where F(.) denotes the objective function and d is the dimensionality of the problem. To the best of our knowledge, this is the first theoretical result of first-order stochastic algorithms with an almost linear time in terms of problem's dimensionality for finding second-order stationary points, which is even competitive with existing stochastic algorithms hinging on the second-order information.	[Xu, Yi; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52246 USA; [Jin, Rong] Alibaba Grp, Machine Intelligence Technol, Bellevue, WA 98004 USA	University of Iowa; Alibaba Group	Xu, Y (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52246 USA.	yi-xu@uiowa.edu; jinrong.jr@alibaba-inc.com; tianbao-yang@uiowa.edu			National Science Foundation [IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	The authors thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1545995).	Allen-Zhu Z., 2017, ABS170808694 CORR; Allen-Zhu  Z., 2017, ABS171106673 CORR; Carmon Y, 2017, PR MACH LEARN RES, V70; Carmon Y, 2018, SIAM J OPTIMIZ, V28, P1751, DOI 10.1137/17M1114296; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hazan E, 2016, PR MACH LEARN RES, V48; Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058; Jin C., 2018, P 31 C LEARNING THEO, P1042; Jin C, 2017, PR MACH LEARN RES, V70; KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066; Lei LH, 2017, ADV NEUR IN, V30; Li H., 2015, PROC 28 INT C NEURAL, P379; Liu  M., 2017, ABS171009447 CORR; Liu  M., 2017, ABS170908571 CORR; Nesterov Y, 2004, INTRO LECT CONVEX OP, P15; O'Neill  M., 2017, ABS170607993 CORR; Reddi SJ, 2018, PR MACH LEARN RES, V84; YAN Y., 2018, IJCAI, P2955, DOI DOI 10.24963/IJCAI.2018/410; Zhang Y., 2017, C LEARN THEOR PMLR, P1980	23	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000007
C	Yan, X; Li, JF; Dai, XY; Chen, HZ; Cheng, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yan, Xiao; Li, Jinfeng; Dai, Xinyan; Chen, Hongzhi; Cheng, James			Norm-Ranging LSH for Maximum Inner Product Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neyshabur and Srebro proposed SIMPLE-LSH [2015], which is the state-of-the-art hashing based algorithm for maximum inner product search (MIPS). We found that the performance of SIMPLE-LSH, in both theory and practice, suffers from long tails in the 2-norm distribution of real datasets. We propose NORM-RANGING LSH, which addresses the excessive normalization problem caused by long tails by partitioning a dataset into sub-datasets and building a hash index for each sub-dataset independently. We prove that NORM-RANGING LSH achieves lower query time complexity than SIMPLE-LSH under mild conditions. We also show that the idea of dataset partitioning can improve another hashing based MIPS algorithm. Experiments show that NORM-RANGING LSH probes much less items than SIMPLE-LSH at the same recall, thus significantly benefiting MIPS based applications.	[Yan, Xiao; Li, Jinfeng; Dai, Xinyan; Chen, Hongzhi; Cheng, James] Chinese Univ Hong Kong, Dept Comp Sci, Shatin, Hong Kong, Peoples R China	Chinese University of Hong Kong	Yan, X (corresponding author), Chinese Univ Hong Kong, Dept Comp Sci, Shatin, Hong Kong, Peoples R China.	xyan@cse.cuhk.edu.hk; jfli@cse.cuhk.edu.hk; xydai@cse.cuhk.edu.hk; hzchen@cse.cuhk.edu.hk; jcheng@cse.cuhk.edu.hk	li, jinfeng/GVS-5425-2022		Hong Kong RGC [CUHK 14222816]	Hong Kong RGC(Hong Kong Research Grants Council)	We thank the reviewers for their valuable comments. This work was supported in part by Grant CUHK 14222816 from the Hong Kong RGC.	Andoni A., 2018, CORR; Andoni A, 2015, ACM S THEORY COMPUT, P793, DOI 10.1145/2746539.2746553; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; Cai D, 2016, CORR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dean T, 2013, PROC CVPR IEEE, P1814, DOI 10.1109/CVPR.2013.237; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; FRIEDMAN JH, 1974, IEEE T COMPUT, VC 23, P881, DOI 10.1109/T-C.1974.224051; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Koenigstein Noam, 2012, P CIKM 12, P535, DOI DOI 10.1145/2396761.2396831; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926; Qin Lv, 2007, P 33 VLDB2007, P950; Ram Parikshit, 2012, P 18 ACM SIGKDD INT, P931, DOI DOI 10.1145/2339530.2339677; Shrivastava A, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P812; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Wang HY, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P1969, DOI 10.1145/2505515.2505765; Weber R., 1998, Proceedings of the Twenty-Fourth International Conference on Very-Large Databases, P194; Yun H., 2013, ARXIV13120193	21	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302093
C	Zhang, JZ; Mokhtari, A; Sra, S; Jadbabaie, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Jingzhao; Mokhtari, Aryan; Sra, Suvrit; Jadbabaie, Ali			Direct Runge-Kutta Discretization Achieves Acceleration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MINIMIZATION; CONVERGENCE; SYSTEM	We study gradient-based optimization methods obtained by directly discretizing a second-order ordinary differential equation (ODE) related to the continuous limit of Nesterov's accelerated gradient method. When the function is smooth enough, we show that acceleration can be achieved by a stable discretization of this ODE using standard Runge-Kutta integrators. Specifically, we prove that under Lipschitz-gradient, convexity and order-(s + 2) differentiability assumptions, the sequence of iterates generated by discretizing the proposed second-order ODE converges to the optimal solution at a rate of O(N-2s/s+1), where s is the order of the Runge-Kutta numerical integrator. Furthermore, we introduce a new local flatness condition on the objective, under which rates even faster than O(N-2) can be achieved with low-order integrators and only gradient information. Notably, this flatness condition is satisfied by several standard loss functions used in machine learning. We provide numerical experiments that verify the theoretical rates predicted by our results.	[Zhang, Jingzhao; Mokhtari, Aryan] MIT, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Sra, Suvrit; Jadbabaie, Ali] MIT, LIDS, IDSS, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Zhang, JZ (corresponding author), MIT, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	jzhzhang@mit.edu; aryanm@mit.edu; suvrit@mit.edu; jadbabai@mit.edu			DARPA FunLoL; DARPA Lagrange; ONR Basic Research Challenge Program;  [NSF-IIS-1409802]	DARPA FunLoL; DARPA Lagrange; ONR Basic Research Challenge Program; 	AJ and SS acknowledge support in part from DARPA FunLoL, DARPA Lagrange; AJ also acknowledges support from an ONR Basic Research Challenge Program, and SS acknowledges support from NSF-IIS-1409802.	Allen-Zhu Z., 2014, ARXIV14071537; Alvarez F, 2000, SIAM J CONTROL OPTIM, V38, P1102, DOI 10.1137/S0363012998335802; Attouch H, 2000, COMMUN CONTEMP MATH, V2, P1, DOI 10.1142/S0219199700000025; Attouch H, 1996, J DIFFER EQUATIONS, V128, P519, DOI 10.1006/jdeq.1996.0104; Attouch H, 2016, SIAM J OPTIMIZ, V26, P1824, DOI 10.1137/15M1046095; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Betancourt M., 2018, ARXIV180203653; BRUCK RE, 1975, J FUNCT ANAL, V18, P15, DOI 10.1016/0022-1236(75)90027-0; Bubeck S., 2015, ARXIV150608187MATHOC; Diakonikolas J., 2017, ARXIV171202485; Fazlyab M., 2017, ARXIV170503615; Hairer E., 2006, SPRINGER SERIES COMP, V31; Hu B., 2017, ARXIV170604381; Isaacson E., 1994, ANAL NUMERICAL METHO; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Lojasiewicz S., 1965, LECT NOTES IHES BURE; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; Scieur D., 2016, ADV NEURAL INFORM PR, P712; SCIEUR D, 2017, ARXIV170206751; Verner JH, 1996, APPL NUMER MATH, V22, P345, DOI 10.1016/S0168-9274(96)00041-4; West M., 2004, THESIS; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113	26	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303086
C	Zhu, ZH; Li, X; Liu, K; Li, QW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhu, Zhihui; Li, Xiao; Liu, Kai; Li, Qiuwei			Dropping Symmetry for Fast Symmetric Nonnegative Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CONVERGENCE ANALYSIS; ALGORITHMS; MINIMIZATION	Symmetric nonnegative matrix factorization (NMF)-a special but important class of the general NMF-is demonstrated to be useful for data analysis and in particular for various clustering tasks. Unfortunately, designing fast algorithms for Symmetric NMF is not as easy as for the nonsymmetric counterpart, the later admitting the splitting property that allows efficient alternating-type algorithms. To overcome this issue, we transfer the symmetric NMF to a nonsymmetric one, then we can adopt the idea from the state-of-the-art algorithms for nonsymmetric NMF to design fast algorithms solving symmetric NMF. We rigorously establish that solving nonsymmetric reformulation returns a solution for symmetric NMF and then apply fast alternating based algorithms for the corresponding reformulated problem. Furthermore, we show these fast algorithms admit strong convergence guarantee in the sense that the generated sequence is convergent at least at a sublinear rate and it converges globally to a critical point of the symmetric NMF. We conduct experiments on both synthetic data and image clustering to support our result.	[Zhu, Zhihui] Johns Hopkins Univ, Math Inst Data Sci, Baltimore, MD 21218 USA; [Li, Xiao] Chinese Univ Hong Kong, Dept Elect Engn, Shatin, Hong Kong, Peoples R China; [Liu, Kai] Colorado Sch Mines, Dept Comp Sci, Golden, CO 80401 USA; [Li, Qiuwei] Colorado Sch Mines, Dept Elect Engn, Golden, CO 80401 USA	Johns Hopkins University; Chinese University of Hong Kong; Colorado School of Mines; Colorado School of Mines	Zhu, ZH (corresponding author), Johns Hopkins Univ, Math Inst Data Sci, Baltimore, MD 21218 USA.	zzhu29@jhu.edu; xli@ee.cuhk.edu.hk; kaliu@mines.edu; qiuli@mines.edu	Li, Xiao/ADO-7061-2022; Zhu, Zhihui/AAR-5029-2020; Li, Qiuwei/AAH-4942-2019	Zhu, Zhihui/0000-0002-3856-0375; Li, Qiuwei/0000-0002-2306-6649				Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Cichocki A, 2009, IEICE T FUND ELECTR, VE92A, P708, DOI 10.1587/transfun.E92.A.708; Di Pillo G, 1994, ALGORITHMS CONTINUOU, P209; Ding C, 2005, SIAM PROC S, P606; Gillis N., 2014, REGULARIZATION OPTIM, V12, P257; Guillamet D, 2002, LECT NOTES ARTIF INT, V2504, P336; He ZS, 2011, IEEE T NEURAL NETWOR, V22, P2117, DOI 10.1109/TNN.2011.2172457; Hsieh C.J., 2011, P 17 ACM SIGKDD INT, P1064; Kim J, 2008, IEEE DATA MINING, P353, DOI 10.1109/ICDM.2008.149; Kuang D, 2015, J GLOBAL OPTIM, V62, P545, DOI 10.1007/s10898-014-0247-2; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 2001, ADV NEUR IN, V13, P556; Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756; Lu ST, 2017, IEEE T SIGNAL PROCES, V65, P3120, DOI 10.1109/TSP.2017.2679687; Ma WK, 2014, IEEE SIGNAL PROC MAG, V31, P67, DOI 10.1109/MSP.2013.2279731; PILLO GD, 1989, SIAM J CONTROL OPTIM, V27, P1333, DOI 10.1137/0327068; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Shahnaz F, 2006, INFORM PROCESS MANAG, V42, P373, DOI 10.1016/j.ipm.2004.11.005; Tu S., 2015, ARXIV150703566; Vandaele A, 2016, IEEE T SIGNAL PROCES, V64, P5571, DOI 10.1109/TSP.2016.2591510; Xu W., 2003, P 26 ANN INT ACM SIG, P267, DOI DOI 10.1145/860435.860485; Zhu Z, 2017, ARXIV170301256	24	11	11	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305019
C	Chen, JB; Stern, M; Wainwright, MJ; Jordan, MI		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Jianbo; Stern, Mitchell; Wainwright, Martin J.; Jordan, Michael, I			Kernel Feature Selection via Conditional Covariance Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DEPENDENCE; REDUCTION	We propose a method for feature selection that employs kernel-based measures of independence to find a subset of covariates that is maximally predictive of the response. Building on past work in kernel dimension reduction, we show how to perform feature selection via a constrained optimization problem involving the trace of the conditional covariance operator. We prove various consistency results for this procedure, and also demonstrate that our method compares favorably with other state-of-the-art algorithms on a variety of synthetic and real data sets.	[Chen, Jianbo; Stern, Mitchell; Wainwright, Martin J.; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Chen, JB (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	jianbochen@berkeley.edu; mitchell@berkeley.edu; wainwrig@berkeley.edu; jordan@berkeley.edu	Jeong, Yongwook/N-7413-2016; Jordan, Michael I/C-5253-2013	Jordan, Michael/0000-0001-8935-817X				Allen GI, 2013, J COMPUT GRAPH STAT, V22, P284, DOI 10.1080/10618600.2012.681213; Bolon-Canedo V, 2015, KNOWL-BASED SYST, V86, P33, DOI 10.1016/j.knosys.2015.05.014; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82; Cao B, 2007, P 24 INT C MACH LEAR, P121, DOI DOI 10.1145/1273496.1273512; Duchi J., 2008, PROC 25 INT C MACH L, P272; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Fukumizu K., 2012, ADV NEURAL INFORM PR, P2114; Gilad-Bachrach R., 2004, P 21 INT C MACHINE L, P43, DOI DOI 10.1145/1015330.1015352; Grandvalet Y., 2002, P ADV NEURAL INFORM, V15, P569; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Guyon I, 2002, MACH LEARN, V46, P389, DOI 10.1023/A:1012487302797; Hastie T, 2009, ELEMENTS STAT LEARNI; Jaganathan P, 2011, COMM COM INF SC, V190, P683; Li JD, 2018, ACM COMPUT SURV, V50, DOI 10.1145/3136625; Masaeli M., 2010, P 27 INT C MACH LEAR, P751; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Rahimi Ali, 2007, NEURAL INFOM PROCESS; Ren SG, 2015, JMLR WORKSH CONF PRO, V38, P781; Song L., 2007, P 24 INT C MACHINE L, P823, DOI [10.1145/1273496.1273600, DOI 10.1145/1273496.1273600]; Song L, 2012, J MACH LEARN RES, V13, P1393; Sun SQ, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0102541; Weston J., 2003, Journal of Machine Learning Research, V3, P1439, DOI 10.1162/153244303322753751; Weston J, 2000, P 13 INT C NEUR INF, V13, P647; Yamada M, 2014, NEURAL COMPUT, V26, P185, DOI 10.1162/NECO_a_00537	28	11	11	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407004
C	Fan, YB; Lyu, SW; Ying, YM; Hu, BG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fan, Yanbo; Lyu, Siwei; Ying, Yiming; Hu, Bao-Gang			Learning with Average Top-k Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SUPPORT; ALGORITHM	In this work, we introduce the average top-k (AT(k)) loss as a new aggregate loss for supervised learning, which is the average over the k largest individual losses over a training dataset. We show that the AT(k) loss is a natural generalization of the two widely used aggregate losses, namely the average loss and the maximum loss, but can combine their advantages and mitigate their drawbacks to better adapt to different data distributions. Furthermore, it remains a convex function over all individual losses, which can lead to convex optimization problems that can be solved effectively with conventional gradient-based methods. We provide an intuitive interpretation of the AT(k) loss based on its equivalent effect on the continuous individual loss functions, suggesting that it can reduce the penalty on correctly classified data. We further give a learning theory analysis of MAT(k) learning on the classification calibration of the AT(k) loss and the error bounds of AT(k)-SVM. We demonstrate the applicability of minimum average top-k learning for binary classification and regression using synthetic and real datasets.	[Fan, Yanbo; Lyu, Siwei] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA; [Ying, Yiming] SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA; [Fan, Yanbo; Hu, Bao-Gang] CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; [Fan, Yanbo; Hu, Bao-Gang] Univ Chinese Acad Sci, Beijing, Peoples R China	State University of New York (SUNY) System; State University of New York (SUNY) Albany; State University of New York (SUNY) System; State University of New York (SUNY) Albany; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS	Lyu, SW (corresponding author), SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA.	yanbolan@nlpr.ia.ac.cn; slyu@albany.edu; yying@albany.edu; hubg@nlpr.ia.ac.cn	Ying, Yiming/AGD-7246-2022; Jeong, Yongwook/N-7413-2016	Ying, Yiming/0000-0001-7345-6672; Hu, Bao-Gang/0000-0002-6916-5394; Lyu, Siwei/0000-0002-0992-685X	University of Chinese Academy of Sciences (UCAS); National Science Foundation (NSF) [IIS-1537257]; Simons Foundation [422504]; Presidential Innovation Fund for Research and Scholarship (PIFRS) program from SUNY Albany; National Science Foundation of China (NSFC) [61620106003]	University of Chinese Academy of Sciences (UCAS); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Simons Foundation; Presidential Innovation Fund for Research and Scholarship (PIFRS) program from SUNY Albany; National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	We thank the anonymous reviewers for their constructive comments. This work was completed when the first author was a visiting student at SUNY Albany, supported by a scholarship from University of Chinese Academy of Sciences (UCAS). Siwei Lyu is supported by the National Science Foundation (NSF, Grant IIS-1537257) and Yiming Ying is supported by the Simons Foundation (#422504) and the 2016-2017 Presidential Innovation Fund for Research and Scholarship (PIFRS) program from SUNY Albany. This work is also partially supported by the National Science Foundation of China (NSFC, Grant 61620106003) for Bao-Gang Hu and Yanbo Fan.	Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; CRAMMER K, 2001, J MACHINE LEARNING R, V2, P265, DOI DOI 10.1162/15324430260185628; De Vito E, 2005, FOUND COMPUT MATH, V5, P59, DOI 10.1007/s10208-004-0134-1; Fan YB, 2017, AAAI CONF ARTIF INTE, P1877; He R, 2011, IEEE T PATTERN ANAL, V33, P1561, DOI 10.1109/TPAMI.2010.220; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lapin M, 2015, ADV NEUR IN, V28; Lapin M, 2016, PROC CVPR IEEE, P1468, DOI 10.1109/CVPR.2016.163; Lin Y, 2004, STAT PROBABIL LETT, V68, P73, DOI 10.1016/j.spl.2004.03.002; Masnadi-Shirazi Hamed, 2009, P 21 INT C NEUR INF; Ogryczak W, 2003, INFORM PROCESS LETT, V85, P117, DOI 10.1016/S0020-0190(02)00370-8; Rudin C, 2009, J MACH LEARN RES, V10, P2233; Scholkopf B., 2001, LEARNING KERNELS SUP; Shalev-Shwartz S, 2016, PR MACH LEARN RES, V48; Srebro N., 2010, ICML TUTORIAL; Steinwart I, 2003, IEEE T PATTERN ANAL, V25, P1274, DOI 10.1109/TPAMI.2003.1233901; Usunier Nicolas, 2009, ICML; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Wu Q, 2006, FOUND COMPUT MATH, V6, P171, DOI 10.1007/s10208-004-0155-9; Wu YC, 2007, J AM STAT ASSOC, V102, P974, DOI 10.1198/016214507000000617; Yang M., 2010, NEURAL INFORM PROCES, P2532	28	11	11	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400048
C	Jung, YH; Goetz, J; Tewari, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jung, Young Hun; Goetz, Jack; Tewari, Ambuj			Online Multiclass Boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recent work has extended the theoretical analysis of boosting algorithms to multiclass problems and to online settings. However, the multiclass extension is in the batch setting and the online extensions only consider binary classification. We fill this gap in the literature by defining, and justifying, a weak learning condition for online multiclass boosting. This condition leads to an optimal boosting algorithm that requires the minimal number of weak learners to achieve a certain accuracy. Additionally, we propose an adaptive algorithm which is near optimal and enjoys an excellent performance on real data due to its adaptive property.	[Jung, Young Hun; Goetz, Jack; Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Jung, YH (corresponding author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	yhjung@umich.edu; jrgoetz@umich.edu; tewaria@umich.edu			NSF [CAREER IIS-1452099, CIF-1422157]	NSF(National Science Foundation (NSF))	We acknowledge the support of NSF under grants CAREER IIS-1452099 and CIF-1422157.	Beygelzimer A, 2015, PR MACH LEARN RES, V37, P2323; Blake C., 1998, UCI MACHINE LEARNING; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Chen S.-T., 2012, ICML; Chen S.-T., 2014, P INT C MACH LEARN, P342; Daniely Amit, 2011, JMLR WORKSHOP C P, P207; Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107; Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Higuera C, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0129126; Hu HZ, 2017, PR MACH LEARN RES, V54, P595; Korytkowski M, 2016, INFORM SCIENCES, V327, P175, DOI 10.1016/j.ins.2015.08.030; LITTLESTONE N, 1989, ANN IEEE SYMP FOUND, P256, DOI 10.1109/SFCS.1989.63487; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Mukherjee I, 2013, J MACH LEARN RES, V14, P437; Oza NC, 2005, IEEE SYS MAN CYBERN, P2340; Schapire RE, 2001, MACH LEARN, V43, P265, DOI 10.1023/A:1010800213066; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; SLUD EV, 1977, ANN PROBAB, V5, P404, DOI 10.1214/aop/1176995801; Ugulino Wallace, 2012, Advances in Artificial Intelligence - SBIA 2012. Proceedings 21th Brazilian Symposium on Artificial Intelligence, P52, DOI 10.1007/978-3-642-34459-6_6; Vovk V. G., 1990, Proceedings of the Third Annual Workshop on Computational Learning Theory, P371; Zhang X.-L., 2014, P INTERSPEECH, P1534; Zinkevich Martin, 2003, P 20 ICML	24	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400088
C	Kang, D; Dhar, D; Chan, AB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kang, Di; Dhar, Debarun; Chan, Antoni B.			Incorporating Side Information by Adaptive Convolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Computer vision tasks often have side information available that is helpful to solve the task. For example, for crowd counting, the camera perspective (e.g., camera angle and height) gives a clue about the appearance and scale of people in the scene. While side information has been shown to be useful for counting systems using traditional hand-crafted features, it has not been fully utilized in counting systems based on deep learning. In order to incorporate the available side information, we propose an adaptive convolutional neural network (ACNN), where the convolution filter weights adapt to the current scene context via the side information. In particular, we model the filter weights as a low-dimensional manifold within the high-dimensional space of filter weights. The filter weights are generated using a learned "filter manifold" sub-network, whose input is the side information. With the help of side information and adaptive weights, the ACNN can disentangle the variations related to the side information, and extract discriminative features related to the current context (e.g. camera perspective, noise level, blur kernel parameters). We demonstrate the effectiveness of ACNN incorporating side information on 3 tasks: crowd counting, corrupted digit recognition, and image deblurring. Our experiments show that ACNN improves the performance compared to a plain CNN with a similar number of parameters. Since existing crowd counting datasets do not contain ground-truth side information, we collect a new dataset with the ground-truth camera angle and height as the side information.	[Kang, Di; Dhar, Debarun; Chan, Antoni B.] City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China	City University of Hong Kong	Kang, D (corresponding author), City Univ Hong Kong, Dept Comp Sci, Hong Kong, Peoples R China.	dkang5-c@my.cityu.edu.hk; ddhar2-c@my.cityu.edu.hk; abchan@cityu.edu.hk	Jeong, Yongwook/N-7413-2016		Research Grants Council of the Hong Kong Special Administrative Region, China [T32-101/15-R]; City University of Hong Kong [7004682]; NVIDIA Corporation	Research Grants Council of the Hong Kong Special Administrative Region, China(Hong Kong Research Grants Council); City University of Hong Kong(City University of Hong Kong); NVIDIA Corporation	The work described in this paper was supported by a grant from the Research Grants Council of the Hong Kong Special Administrative Region, China (Project No. [T32-101/15-R]), and by a Strategic Research Grant from City University of Hong Kong (Project No. 7004682). We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arteta Carlos, 2014, ECCV, P504; BURGER HC, 2012, PROC CVPR IEEE, P2392, DOI DOI 10.1109/CVPR.2012.6247952; Chan A. B., 2012, IEEE T IMAGE PROCESS; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Chan AB, 2009, IEEE I CONF COMP VIS, P545, DOI 10.1109/ICCV.2009.5459191; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; De Brabandere B, 2016, ADV NEUR IN, V29; Eigen D, 2013, IEEE I CONF COMP VIS, P633, DOI 10.1109/ICCV.2013.84; Fiaschi L., 2012, ICPR; Gharbi M, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2982399; Ha David, 2017, ICLR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Idrees H., 2013, CVPR; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Kang D., 2017, ARXIV170510118, VPP, P1; Klein E, 2015, PROC CVPR IEEE, P4437, DOI 10.1109/CVPR.2015.7299073; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lempitsky V., 2010, NIPS, V23, P1324; Li S., 2015, IJCV; Ma Z., 2015, CVPR; Maas Andrew L, 2013, P ICML CIT, V30, P3, DOI DOI 10.21437/INTERSPEECH.2016-1230; Onoro- Rubio D., 2016, ECCV; Rodriguez M., 2011, ICCV; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Xu L., 2014, INT C NEUR INF PROC, V27, P1790; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684; Zhang Y., 2016, CVPR; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7	31	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403090
C	Levine, N; Crammer, K; Mannor, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Levine, Nir; Crammer, Koby; Mannor, Shie			Rotting Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The Multi-Armed Bandits (MAB) framework highlights the trade-off between acquiring new knowledge (Exploration) and leveraging available knowledge (Exploitation). In the classical MAB problem, a decision maker must choose an arm at each time step, upon which she receives a reward. The decision maker's objective is to maximize her cumulative expected reward over the time horizon. The MAB problem has been studied extensively, specifically under the assumption of the arms' rewards distributions being stationary, or quasi-stationary, over time. We consider a variant of the MAB framework, which we termed Rotting Bandits, where each arm's expected reward decays as a function of the number of times it has been pulled. We are motivated by many real-world scenarios such as online advertising, content recommendation, crowdsourcing, and more. We present algorithms, accompanied by simulations, and derive theoretical guarantees.	[Levine, Nir; Crammer, Koby; Mannor, Shie] Technion, Elect Engn Dept, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Levine, N (corresponding author), Technion, Elect Engn Dept, IL-32000 Haifa, Israel.	levin.nir1@gmail.com; koby@ee.technion.ac.il; shie@ee.technion.ac.il	Jeong, Yongwook/N-7413-2016	Mannor, Shie/0000-0003-4439-7647	European Research Council under the European Union's Seventh Framework Program (FP/2007-2013)/ERC [306638]	European Research Council under the European Union's Seventh Framework Program (FP/2007-2013)/ERC	The research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Program (FP/2007-2013)/ERC Grant Agreement n. 306638	Agarwal D., 2009, WWW, P21; Agrawal S., 2013, ARTIF INTELL, P99; Arora Raman, 2012, ARXIV12066400; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Besbes O., 2014, ADV NEURAL INFORM PR, P199; Bouneffouf D, 2016, NEUROCOMPUTING, V205, P16, DOI 10.1016/j.neucom.2016.02.052; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chakrabarti D., 2009, ADV NEURAL INFORM PR, V21; Du S, 2013, IEEE T CIRC SYST VID, V23, P322, DOI 10.1109/TCSVT.2012.2203741; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Garivier A., 2008, ARXIV08053415; GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148; Gopalan A, 2014, PR MACH LEARN RES, V32; Hazan E, 2011, J MACH LEARN RES, V12, P1287; Heidari H., 1972, DYNAMIC ALLOCATION I; Jones D. M., 1972, DYNAMIC ALLOCATION I; Kaspi H, 1998, ANN APPL PROBAB, V8, P1270; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Kocsis L., 2006, 2 PASCAL CHAL WORKSH, P784; Komiyama J, 2014, LECT NOTES COMPUT SC, V8877, P460, DOI 10.1007/978-3-319-13129-0_40; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; MANDELBAUM A, 1987, ANN PROBAB, V15, P1527, DOI 10.1214/aop/1176991992; Pandey S, 2007, PROCEEDINGS OF THE SEVENTH SIAM INTERNATIONAL CONFERENCE ON DATA MINING, P216; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Slivkins Aleksandrs, 2008, COLT, P343; Stoltz G, 2011, P 24 ANN C LEARN THE, P497; Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tran-Thanh L, 2012, FRONT ARTIF INTEL AP, V242, P768, DOI 10.3233/978-1-61499-098-7-768; WHITTLE P, 1981, ANN PROBAB, V9, P284, DOI 10.1214/aop/1176994469; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163; Yu J.Y., 2009, P 26 ANN INT C MACHI, P1177	35	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403014
C	Li, CL; Chang, WC; Cheng, Y; Yang, YM; Poczos, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Chun-Liang; Chang, Wei-Cheng; Cheng, Yu; Yang, Yiming; Poczos, Barnabas			MMD GAN: Towards Deeper Understanding of Moment Matching Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative moment matching network (GMMN) is a deep generative model that differs from Generative Adversarial Network (GAN) by replacing the discriminator in GAN with a two-sample test based on kernel maximum mean discrepancy (MMD).' Although some theoretical guarantees of MMD have been studied, the empirical performance of GMMN is still not as competitive as that of GAN on challenging and large benchmark datasets. The computational efficiency of GMMN is also less desirable in comparison with GAN, partially due to its requirement for a rather large batch size during the training. In this paper, we propose to improve both the model expressiveness of GMMN and its computational efficiency by introducing adversarial kernel learning techniques, as the replacement of a fixed Gaussian kernel in the original GMMN. The new approach combines the key ideas in both GMMN and GAN, hence we name it MMD GAN. The new distance measure in MMD GAN is a meaningful loss that enjoys the advantage of weak* topology and can be optimized via gradient descent with relatively small batch sizes. In our evaluation on multiple benchmark datasets, including MNIST, CIFAR-10, CelebA and LSUN, the performance of MMD GAN significantly outperforms GMMN, and is competitive with other representative GAN works.	[Li, Chun-Liang; Chang, Wei-Cheng; Yang, Yiming; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Cheng, Yu] IBM Res, AI Fdn, Yorktown Hts, NY USA	Carnegie Mellon University; International Business Machines (IBM)	Li, CL (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	chunlia@cs.cmu.edu; wchang2@cs.cmu.edu; chengyu@us.ibm.com; yiming@cs.cmu.edu; bapoczos@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation (NSF) [IIS-1546329, IIS-1563887]	National Science Foundation (NSF)(National Science Foundation (NSF))	We thank the reviewers for their helpful comments. This work is supported in part by the National Science Foundation (NSF) under grants IIS-1546329 and IIS-1563887.	[Anonymous], 2017, ICLR; Bellemare MG, 2017, ARXIV; Berthelot D., 2017, BEGAN BOUNDARY EQUIL; Bottou L., 2017, ICML; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Dziugaite G. K., 2015, UAI; Fukumizu K., 2009, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A., 2012, JMLR; Gretton A., 2012, ADV NEURAL INFORM PR, P1214; Gretton Arthur, 2017, NOTES CRAMER GAN; Gulrajani I., 2017, P ADV NEUR INF PROC, V2017-Decem, P5768; Herbrich R, 1999, SUPPORT VECTOR LEARN; Kingma D.P., 2013, ICLR; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; LeCun Y., 1998, P IEEE; Li Y., 2015, ICML; Liang Y., 2017, ARXIV170300573; Liu Ziwei, 2015, CVPR; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Mroueh Youssef, 2017, 170208398 ARXIV; Muandet K., 2016, ARXIV PREPRINT ARXIV, P133; Nowozin Sebastian, 2016, P ADV NEURAL INFORM; Radford A., 2016, ICLR; Salakhutdinov R., 2009, AISTATS; Salimans Tim, 2016, ADV NEURAL INFORM PR; Sriperumbudur B. K., 2010, JMLR; Sutherland D. J., 2017, ICLR; Tieleman T., 2012, COURSERA NEURAL NETW; Ulyanov Dmitry, 2017, ARXIV170402304; Warde-Farley D., 2017, ICLR; Wasserman L., 2013, ALL STAT CONCISE COU; Wilson A. G., 2016, AISTATS; Xu K., 2015, ICML; Yu F., 2015, ARXIVABS150603365 CO; Zellinger W., 2017, 5 INT C LEARN REPR I; Zhai Shuangfei, 2016, CORR	37	11	11	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402025
C	Liu, WY; Zhang, YM; Li, XG; Yu, ZD; Dai, B; Zhao, T; Song, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Weiyang; Zhang, Yan-Ming; Li, Xingguo; Yu, Zhiding; Dai, Bo; Zhao, Tuo; Song, Le			Deep Hyperspherical Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Convolution as inner product has been the founding basis of convolutional neural networks (CNNs) and the key to end-to-end visual representation learning. Benefiting from deeper architectures, recent CNNs have demonstrated increasingly strong representation abilities. Despite such improvement, the increased depth and larger parameter space have also led to challenges in properly training a network. In light of such challenges, we propose hyperspherical convolution (SphereConv), a novel learning framework that gives angular representations on hyperspheres. We introduce SphereNet, deep hyperspherical convolution networks that are distinct from conventional inner product based convolutional networks. In particular, SphereNet adopts SphereConv as its basic convolution operator and is supervised by generalized angular softmax loss - a natural loss formulation under SphereConv. We show that SphereNet can effectively encode discriminative representation and alleviate training difficulty, leading to easier optimization, faster convergence and comparable (even better) classification accuracy over convolutional counterparts. We also provide some theoretical insights for the advantages of learning on hyperspheres. In addition, we introduce the learnable SphereConv, i.e., a natural improvement over prefixed SphereConv, and SphereNorm, i.e., hyperspherical learning as a normalization method. Experiments have verified our conclusions.	[Liu, Weiyang; Li, Xingguo; Dai, Bo; Zhao, Tuo; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Zhang, Yan-Ming] Chinese Acad Sci, Inst Automat, Beijing, Peoples R China; [Li, Xingguo] Univ Minnesota, Minneapolis, MN 55455 USA; [Yu, Zhiding] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University System of Georgia; Georgia Institute of Technology; Chinese Academy of Sciences; Institute of Automation, CAS; University of Minnesota System; University of Minnesota Twin Cities; Carnegie Mellon University	Liu, WY (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	wyliu@gatech.edu; ymzhang@nlpr.ia.ac.cn; tourzhao@gatech.edu; lsong@cc.gatech.edu			NSF [IIS-1218749, IIS-1639792 EAGER, CNS-1704701]; NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; University of Minnesota; National Natural Science Foundation of China [61773376]	NSF(National Science Foundation (NSF)); NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); Intel ISTC; NVIDIA; Amazon AWS; University of Minnesota(University of Minnesota System); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank Zhen Liu (Georgia Tech) for helping with the experiments and providing suggestions. This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA and Amazon AWS. Xingguo Li is supported by doctoral dissertation fellowship from University of Minnesota. Yan-Ming Zhang is supported by the National Natural Science Foundation of China under Grant 61773376.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, ARXIV161209296; Chen Liang-Chich, 2015, ABS14127062 CORR; Girshick R., 2014, CVPR; Glorot X., 2010, PROC MACH LEARN RES, P249; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2016, PR MACH LEARN RES, V48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mishkin Dmytro, 2015, ICLR; Nakatsukasa Y, 2012, APPL NUMER MATH, V62, P67, DOI 10.1016/j.apnum.2011.09.010; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; Wilber M, 2016, P ADV NEUR INF PROC, V30, P550; Xie Di, 2017, ARXIV170301827	21	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404003
C	Mazumdar, A; Saha, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mazumdar, Arya; Saha, Barna			Clustering with Noisy Queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RECOVERY	In this paper, we provide a rigorous theoretical study of clustering with noisy queries. Given a set of n elements, our goal is to recover the true clustering by asking minimum number of pairwise queries to an oracle. Oracle can answer queries of the form "do elements u and v belong to the same cluster?"-the queries can be asked interactively (adaptive queries), or non-adaptively up-front, but its answer can be erroneous with probability p. In this paper, we provide the first information theoretic lower bound on the number of queries for clustering with noisy oracle in both situations. We design novel algorithms that closely match this query complexity lower bound, even when the number of clusters is unknown. Moreover, we design computationally efficient algorithms both for the adaptive and non-adaptive settings. The problem captures/generalizes multiple application scenarios. It is directly motivated by the growing body of work that use crowdsourcing for entity resolution, a fundamental and challenging data mining task aimed to identify all records in a database referring to the same entity. Here crowd represents the noisy oracle, and the number of queries directly relates to the cost of crowdsourcing. Another application comes from the problem of sign edge prediction in social network, where social interactions can be both positive and negative, and one must identify the sign of all pair-wise interactions by querying a few pairs. Furthermore, clustering with noisy oracle is intimately connected to correlation clustering, leading to improvement therein. Finally, it introduces a new direction of study in the popular stochastic block model where one has an incomplete stochastic block model matrix to recover the clusters.	[Mazumdar, Arya; Saha, Barna] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Mazumdar, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	arya@cs.umass.edu; barna@cs.umass.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303]; Yahoo ACE Award; Google Faculty Research Award	NSF(National Science Foundation (NSF)); Yahoo ACE Award; Google Faculty Research Award(Google Incorporated)	This work is supported in parts by NSF awards CCF 1642658, CCF 1642550, CCF 1464310, CCF 1652303, a Yahoo ACE Award and a Google Faculty Research Award. The authors are thankful to an anonymous reviewer whose comments led to many improvements in the presentation. The authors would also like to thank Sanjay Subramanian for his help with the experiments.	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Ailon Nir, 2013, P 30 INT C MACH LEAR, P995; [Anonymous], 2016, ARXIV160401839; Ashtiani H., 2016, NIPS; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; Braverman M., 2009, CORR; Braverman M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P268; Brzozowski MJ, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P817; Burke M, 2008, CSCW: 2008 ACM CONFERENCE ON COMPUTER SUPPORTED COOPERATIVE WORK, CONFERENCE PROCEEDINGS, P27; CARTWRIGHT D, 1956, PSYCHOL REV, V63, P277, DOI 10.1037/h0046049; Cesa-Bianchi N., 2012, ANN C LEARN THEOR MI, P34; Chaudhuri Kamalika, 2012, J MACHINE LEARNING R, P35; Chen Y., 2012, ADV NEURAL INFORM PR, P2204; CHEN Y., 2016, INT C MACH LEARN, P689; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Chiang KY, 2014, J MACH LEARN RES, V15, P1177; Chin P., 2015, ARXIV150105021; Christen P., 2012, DATA MATCHING CONCEP, DOI 10.1007/978-3-642-31164-2; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; DYER ME, 1989, J ALGORITHM, V10, P451, DOI 10.1016/0196-6774(89)90001-1; Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; FELLEGI IP, 1969, J AM STAT ASSOC, V64, P1183, DOI 10.2307/2286061; Firmani D, 2016, PROC VLDB ENDOW, V9, P384; Getoor L, 2012, PROC VLDB ENDOW, V5, P2018, DOI 10.14778/2367502.2367564; Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599; Gokhale C, 2014, SIGMOD'14: PROCEEDINGS OF THE 2014 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P601; Gruenheid Anja, 2015, FAULT TOLERANT ENTIT; Hajek B, 2016, IEEE T INFORM THEORY, V62, P5918, DOI 10.1109/TIT.2016.2594812; HAN TS, 1994, IEEE T INFORM THEORY, V40, P1247, DOI 10.1109/18.335943; Harary F, 1953, MICH MATH J, V2, P143; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Kleinberg R., 2007, LECT NOTES LEARNING; Lampe C, 2007, CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1 AND 2, P1253; Larsen MD, 2001, J AM STAT ASSOC, V96, P32, DOI 10.1198/016214501750332956; Leskovec J., 2010, P INT C WORLD WID WE, P641, DOI DOI 10.1109/ICDE.2018.00077; Lim Shiau Hong, 2014, ADV NEURAL INFORM PR, P1188; Makarychev Konstantin, 2015, P 28 C LEARN THEOR, P1321; Mathieu C, 2010, PROC APPL MATH, V135, P712; Mazumdar A., 2017, ADV NEURAL INFORM PR, P31; Mazumdar A., 2017, 31 AAAI C ART INT AA; Mitzenmacher M., 2016, CORR; Mossel E, 2015, ACM S THEORY COMPUT, P69, DOI 10.1145/2746539.2746603; Polyanskiy Y., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1327, DOI 10.1109/ALLERTON.2010.5707067; Prelec D, 2017, NATURE, V541, P532, DOI 10.1038/nature21054; Verroios V., 2017, SIGMOD, P219; Verroios V, 2015, PROC INT CONF DATA, P219, DOI 10.1109/ICDE.2015.7113286; Vesdapunt N, 2014, PROC VLDB ENDOW, V7, P1071, DOI 10.14778/2732977.2732982; Vinayak R.K., 2016, ADV NEURAL INFORM PR, P1316; Wang J, 2012, PROC VLDB ENDOW, V5, P1483, DOI 10.14778/2350229.2350263	56	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405084
C	Miller, AC; Foti, NJ; D'Amour, A; Adams, RP		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Miller, Andrew C.; Foti, Nicholas J.; D'Amour, Alexander; Adams, Ryan P.			Reducing Reparameterization Gradient Variance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Optimization with noisy gradients has become ubiquitous in statistics and machine learning. Reparameterization gradients, or gradient estimates computed via the "reparameterization trick," represent a class of noisy gradients often used in Monte Carlo variational inference (MCVI). However, when these gradient estimators are too noisy, the optimization procedure can be slow or fail to converge. One way to reduce noise is to generate more samples for the gradient estimate, but this can be computationally expensive. Instead, we view the noisy gradient as a random variable, and form an inexpensive approximation of the generating procedure for the gradient sample. This approximation has high correlation with the noisy gradient by construction, making it a useful control variate for variance reduction. We demonstrate our approach on a non-conjugate hierarchical model and a Bayesian neural net where our method attained orders of magnitude (20-2,000x) reduction in gradient variance resulting in faster and more stable optimization.	[Miller, Andrew C.] Harvard Univ, Cambridge, MA 02138 USA; [Foti, Nicholas J.] Univ Washington, Seattle, WA 98195 USA; [D'Amour, Alexander] Univ Calif Berkeley, Berkeley, CA USA; [Adams, Ryan P.] Google Brain, Mountain View, CA USA; [Adams, Ryan P.] Princeton Univ, Princeton, NJ 08544 USA	Harvard University; University of Washington; University of Washington Seattle; University of California System; University of California Berkeley; Google Incorporated; Princeton University	Miller, AC (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	acm@seas.harvard.edu; nfoti@uw.edu; alexdamour@berkeley.edu; rpa@princeton.edu	Jeong, Yongwook/N-7413-2016		Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy [DE-AC02-05CH11231]; Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering and Data Science; NSF [IIS-1421780]; Alfred P. Sloan Foundation	Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy(United States Department of Energy (DOE)); Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering and Data Science; NSF(National Science Foundation (NSF)); Alfred P. Sloan Foundation(Alfred P. Sloan Foundation)	The authors would like to thank Finale Doshi-Velez, Mike Hughes, Taylor Killian, Andrew Ross, and Matt Hoffman for helpful conversations and comments on this work. ACM is supported by the Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. NJF is supported by a Washington Research Foundation Innovation Postdoctoral Fellowship in Neuroengineering and Data Science. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation.	Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M., 2017, ARXIV170107875; Bekas C, 2007, APPL NUMER MATH, V57, P1214, DOI 10.1016/j.apnum.2007.01.003; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Gelman A., 2007, DATA ANAL USING REGR, DOI 10.1017/CBO9780511790942; Glasserman P., 2013, MONTE CARLO METHODS, V53; GLASSERMAN P, 2004, APPL MATH, V53, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hoffman Matthew D, 2016, ELBO SURG YET ANOTHE; Kingma D.P, P 3 INT C LEARNING R; Maclaurin D., 2015, AUTOGRAD REVERSE MOD; Martens James, 2012, P INT C MACH LEARN; Mnih A, 2016, PR MACH LEARN RES, V48; Mohamed Shakir, 2016, ARXIV161003483; Paisley J., 2012, ARXIV12066430; Paszke Adam, 2017, PYTORCH TENSORS DYNA, P6; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Ranganath R, 2016, PR MACH LEARN RES, V48; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roeder Geoffrey, 2017, ARXIV170309194; Ruiz Francisco R, 2016, ADV NEURAL INFORM PR, P460; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Tran D., 2017, INT C LEARN REPR ICL; Wang C., 2013, ADV NEURAL INFORM PR, P181; Zhang, 2013, ADV NEURAL INFORM PR, P315	30	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403075
C	Niculae, V; Blondel, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Niculae, Vlad; Blondel, Mathieu			A Regularized Framework for Sparse and Structured Neural Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHM; SMOOTHNESS; SHRINKAGE	Modern neural networks are often augmented with an attention mechanism, which tells the network where to focus within the input. We propose in this paper a new framework for sparse and structured attention, building upon a smoothed max operator. We show that the gradient of this operator defines a mapping from real values to probabilities, suitable as an attention mechanism. Our framework includes softmax and a slight generalization of the recently-proposed sparsemax as special cases. However, we also show how our framework can incorporate modern structured penalties, resulting in more interpretable attention mechanisms, that focus on entire segments or groups of an input. We derive efficient algorithms to compute the forward and backward passes of our attention mechanisms, enabling their use in a neural network trained with backpropagation. To showcase their potential as a drop-in replacement for existing ones, we evaluate our attention mechanisms on three large-scale tasks: textual entailment, machine translation, and sentence summarization. Our attention mechanisms improve interpretability without sacrificing performance; notably, on textual entailment and summarization, we outperform the standard attention mechanisms based on softmax and sparsemax.	[Niculae, Vlad] Cornell Univ, Ithaca, NY 14850 USA; [Blondel, Mathieu] NTT Commun Sci Labs, Kyoto, Japan	Cornell University; Nippon Telegraph & Telephone Corporation	Niculae, V (corresponding author), Cornell Univ, Ithaca, NY 14850 USA.	vlad@cs.cornell.edu; mathieu@mblondel.org	Jeong, Yongwook/N-7413-2016					Amos B., 2017, P ICML; [Anonymous], 2015, P EMNLP; [Anonymous], 2015, P EMNLP; Bahdanau D., 2015, 3 INT C LEARN REPR I; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bengio Y., 2005, P NIPS; Bengio Yoshua, 2013, P NIPS; Bowman S. R, 2015, P EMNLP; Boyd S, 2004, CONVEX OPTIMIZATION; Chorowski J., 2015, P NIPS; Clarke Frank H., 1990, OPTIMIZATION NONSMOO, V5; Cohn Trevor, 2016, P NAACL HLT; Condat L, 2013, IEEE SIGNAL PROC LET, V20, P1054, DOI 10.1109/LSP.2013.2278339; Dagan Ido, 2010, Natural Language Engineering, V16, pi, DOI 10.1017/S1351324909990209; Duchi John C, 2008, P ICML; Dugas C., 2001, P NIPS; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Graves A., 2014, P NIPS; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Irsoy O., 2017, THESIS; Jang E., 2017, P ICLR; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Kim Y., 2017, P ICLR; Klein G., 2017, ARXIV E PRINTS; KOEHN P, 2007, P ACL; Lei T., 2016, P EMNLP; Li Jiwei, 2016, P NAACL HLT; Liu B., 2015, P ICCVPR; Maddison C. J., 2017, P ICLR; Martins A., 2016, P ICML; Martins A. F., 2017, P EMNLP; Meshi Ofer, 2015, P NIPS; MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Rocktaschel T., 2016, P ICLR; Scardapane S, 2017, NEUROCOMPUTING, V241, P81, DOI 10.1016/j.neucom.2017.02.029; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Wen W., 2016, P NIPS; Xu K., 2015, P ICML; Yu Y., 2013, P NIPS; Zalinescu C., 2002, CONVEX ANAL GEN VECT, DOI [10.1142/5021, DOI 10.1142/5021]; Zeng X., 2014, ORDERED WEIGHTED L1; Zeng XR, 2014, DIGIT SIGNAL PROCESS, V31, P124, DOI 10.1016/j.dsp.2014.03.010	48	11	11	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403040
C	Platanios, EA; Poon, H; Mitchell, TM; Horvitz, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Platanios, Emmanouil A.; Poon, Hoifung; Mitchell, Tom M.; Horvitz, Eric			Estimating Accuracy from Unlabeled Data: A Probabilistic Logic Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose an efficient method to estimate the accuracy of classifiers using only unlabeled data. We consider a setting with multiple classification problems where the target classes may be tied together through logical constraints. For example, a set of classes may be mutually exclusive, meaning that a data instance can belong to at most one of them. The proposed method is based on the intuition that: (i) when classifiers agree, they are more likely to be correct, and (ii) when the classifiers make a prediction that violates the constraints, at least one classifier must be making an error. Experiments on four real-world data sets produce accuracy estimates within a few percent of the true accuracy, using solely unlabeled data. Our models also outperform existing state-of-the-art solutions in both estimating accuracies, and combining multiple classifier outputs. The results emphasize the utility of logical constraints in estimating accuracy, thus validating our intuition.	[Platanios, Emmanouil A.; Mitchell, Tom M.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Poon, Hoifung; Horvitz, Eric] Microsoft Res, Redmond, WA USA	Carnegie Mellon University; Microsoft	Platanios, EA (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	e.a.platanios@cs.cmu.edu; hoifung@microsoft.com; tom.mitchell@cs.cmu.edu; horvitz@microsoft.com	Jeong, Yongwook/N-7413-2016		NSF [IIS1250956]; Carnegie Mellon University	NSF(National Science Foundation (NSF)); Carnegie Mellon University	We would like to thank Abulhair Saparov and Otilia Stretcu for the useful feedback they provided in early versions of this paper. This research was performed during an internship at Microsoft Research, and was also supported in part by NSF under award IIS1250956, and in part by a Presidential Fellowship from Carnegie Mellon University.	Bach S., 2015, CORR; Bach S. H., 2013, C UNC ART INT; Balcan N., 2013, P 30 INT C MACH LEAR, V28, P1112; Bengio Y., 2003, Journal of Machine Learning Research, V3, P1209, DOI 10.1162/153244303322753634; Brocheler M., 2010, P 26 C UNCERTAINTY A, P73; Collins J, 2014, STAT MED, V33, P4141, DOI 10.1002/sim.6218; Collins M., 1999, JOINT C EMP METH NAT; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Klir G.J., 1995, FUZZY SETS FUZZY LOG, DOI 10.1109/9780470544341.ch11; Madani O., 2004, NEURAL INFORM PROCES; Mitchell T., 2015, NEVER ENDING LEARNIN; Moreno P. G., 2015, J MACHINE LEARNING R, V16; Niu F, 2011, PROC VLDB ENDOW, V4, P373, DOI 10.14778/1978665.1978669; Parisi F., 2014, P NATL ACAD SCI; Platanios E. A., 2016, INT C MACH LEARN, P1416; Platanios E. A., 2014, C UNC ART INT; Tian T., 2015, NEURAL INFORM PROCES; Wehbe Leila, 2014, PREDICTING BRAIN ACT	20	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404042
C	Rashtchian, C; Makarychev, K; Racz, M; Ang, SD; Jevdjic, D; Yekhanin, S; Ceze, L; Strauss, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rashtchian, Cyrus; Makarychev, Konstantin; Racz, Miklos; Ang, Siena Dumas; Jevdjic, Djordje; Yekhanin, Sergey; Ceze, Luis; Strauss, Karin			Clustering Billions of Reads for DNA Data Storage	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SIMILARITY; ROBUST	Storing data in synthetic DNA offers the possibility of improving information density and durability by several orders of magnitude compared to current storage technologies. However, DNA data storage requires a computationally intensive process to retrieve the data. In particular, a crucial step in the data retrieval pipeline involves clustering billions of strings with respect to edit distance. Datasets in this domain have many notable properties, such as containing a very large number of small clusters that are well-separated in the edit distance metric space. In this regime, existing algorithms are unsuitable because of either their long running time or low accuracy. To address this issue, we present a novel distributed algorithm for approximately computing the underlying clusters. Our algorithm converges efficiently on any dataset that satisfies certain separability properties, such as those coming from DNA data storage systems. We also prove that, under these assumptions, our algorithm is robust to outliers and high levels of noise. We provide empirical justification of the accuracy, scalability, and convergence of our algorithm on real and synthetic data. Compared to the state-of-the-art algorithm for clustering DNA sequences, our algorithm simultaneously achieves higher accuracy and a 1000x speedup on three real datasets.	[Rashtchian, Cyrus; Makarychev, Konstantin; Racz, Miklos; Ang, Siena Dumas; Jevdjic, Djordje; Yekhanin, Sergey; Ceze, Luis; Strauss, Karin] Microsoft Res, Redmond, WA 98052 USA; [Rashtchian, Cyrus; Ceze, Luis] Univ Washington, CSE, Seattle, WA 98195 USA; [Makarychev, Konstantin] Northwestern Univ, EECS, Evanston, IL 60208 USA; [Racz, Miklos] Princeton Univ, ORFE, Princeton, NJ 08544 USA	Microsoft; University of Washington; University of Washington Seattle; Northwestern University; Princeton University	Rashtchian, C (corresponding author), Microsoft Res, Redmond, WA 98052 USA.; Rashtchian, C (corresponding author), Univ Washington, CSE, Seattle, WA 98195 USA.		Makarychev, Konstantin/P-6054-2017; Jeong, Yongwook/N-7413-2016					Abboud A., 2016, STOC; Ackerman M., 2013, AISTATS; Ackerman M, 2014, ADV NEUR IN, V27; Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513; Andoni A., SIAM J COMPUT, V39; Andoni A, 2012, ACM T ALGORITHMS, V8, DOI 10.1145/2344422.2344434; Backurs A., 2015, STOC; Balcan MF, 2014, J MACH LEARN RES, V15, P3831; Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; Batu T., 2003, STOC; Betancourt B., 2016, NIPS; Bornholt J., 2016, ASPLOS; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; Broder AZ, 1997, COMPUT NETWORKS ISDN, V29, P1157, DOI 10.1016/S0169-7552(97)00031-7; Buhler J, 2001, BIOINFORMATICS, V17, P419, DOI 10.1093/bioinformatics/17.5.419; Chakraborty D., 2016, STOC; Charikar M., 2006, THEORY COMPUTING, V2, P207, DOI DOI 10.4086/TOC.2006.V002A011; Chawla S., 2015, STOC; Chen J., 2016, NIPS; Christen P, 2012, DATA CENTRIC SYSTEMS, DOI [10.1007/978-3-642-31164-2, DOI 10.1007/978-3-642-31164-2]; Deng D, 2014, PROC INT CONF DATA, P340, DOI 10.1109/ICDE.2014.6816663; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Elmagarmid AK, 2007, IEEE T KNOWL DATA EN, V19, P1, DOI 10.1109/TKDE.2007.250581; Erlich Y, 2017, SCIENCE, V355, P950, DOI 10.1126/science.aaj2038; Ganguly S, 2016, IEEE INT SYMP INFO, P265, DOI 10.1109/ISIT.2016.7541302; Goldman N, 2013, NATURE, V494, P77, DOI 10.1038/nature11875; Gollapudi S., 2006, CIKM; Gravano L., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P491; Guha S., 2017, ARXIV170301539; Hanada H., 2017, ARXIV170106134; Har-Peled S., 2012, THEORY COMPUT, V8, P321, DOI DOI 10.4086/TOC.2012.V008A014; Hassanzadeh O., 2009, PVLDB, V2, P1282, DOI [10.14778/1687627.1687771, DOI 10.14778/1687627.1687771]; Hennig C., 2015, HDB CLUSTER ANAL, DOI DOI 10.1201/B19706; Jiang Y., 2013, JOINT EDBT ICDT WORK; Jiang Y, 2014, PROC VLDB ENDOW, V7, P625, DOI 10.14778/2732296.2732299; Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572; Kobren A., 2017, KDD; Krauthgamer R, 2009, SIAM J COMPUT, V38, P2487, DOI 10.1137/060660126; Li H, 2009, BIOINFORMATICS, V25, P1094, DOI [10.1093/bioinformatics/btp100, 10.1093/bioinformatics/btp324]; Li P., 2012, NIPS; Li Ping, 2010, P 19 INT C WORLD WID, P671, DOI DOI 10.1145/1772690.1772759; Malkomes G., 2015, NIPS, V28, P1063; Matousek, 2002, LECT DISCRETE GEOMET, V212; Meila M, 2001, MACH LEARN, V42, P9, DOI 10.1023/A:1007648401407; Organick L., 2017, BIORXIV; Ostrovsky R, 2007, J ACM, V54, DOI 10.1145/1284320.1284322; Pan X., 2015, ADV NEURAL INFORM PR, V1, P82; Rasheed Z., 2012, P 2012 SIAM INT C DA, P1023; Sundaram N, 2013, PROC VLDB ENDOW, V6, P1930, DOI 10.14778/2556549.2556574; UKKONEN E, 1992, THEOR COMPUT SCI, V92, P191, DOI 10.1016/0304-3975(92)90143-4; Yan CR, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172526; Yazdi S. H. T., 2016, BIORXIV; Yu MH, 2016, FRONT COMPUT SCI-CHI, V10, P399, DOI 10.1007/s11704-015-5900-5; Yuan PS, 2014, LECT NOTES COMPUT SC, V8505, P217, DOI 10.1007/978-3-662-43984-5_16; Zadeh RB, 2013, J MACH LEARN RES, V14, P1605; Zhang H., 2017, KDD; Zorita E. V., 2015, BIOINFORMATICS	57	11	11	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403042
C	Tropp, JA; Yurtsever, A; Udell, M; Cevher, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tropp, Joel A.; Yurtsever, Alp; Udell, Madeleine; Cevher, Volkan			Fixed-Rank Approximation of a Positive-Semidefinite Matrix from Streaming Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RANDOMIZED ALGORITHM	Several important applications, such as streaming PCA and semidefinite programming, involve a large-scale positive-semidefinite (psd) matrix that is presented as a sequence of linear updates. Because of storage limitations, it may only be possible to retain a sketch of the psd matrix. This paper develops a new algorithm for fixed-rank psd approximation from a sketch. The approach combines the Nystrom approximation with a novel mechanism for rank truncation. Theoretical analysis establishes that the proposed method can achieve any prescribed relative error in the Schatten 1-norm and that it exploits the spectral decay of the input matrix. Computer experiments show that the proposed method dominates alternative techniques for fixed-rank psd matrix approximation across a wide range of examples.	[Tropp, Joel A.] CALTECH, Pasadena, CA 91125 USA; [Yurtsever, Alp; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Udell, Madeleine] Cornell, Ithaca, NY USA	California Institute of Technology; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Tropp, JA (corresponding author), CALTECH, Pasadena, CA 91125 USA.	jtropp@caltech.edu; alp.yurtsever@epfl.ch; mru8@cornell.edu; volkan.cevher@epfl.ch	Jeong, Yongwook/N-7413-2016; Yurtsever, Alp/AAB-9053-2020	Udell, Madeleine/0000-0002-3985-915X	ONR Award [N00014-17-1-2146]; Gordon & Betty Moore Foundation; European Commission under Grant ERC Future Proof; SNF [200021-146750, CRSII2-147633]; DARPA Award [FA8750-17-2-0101]	ONR Award; Gordon & Betty Moore Foundation(Gordon and Betty Moore Foundation); European Commission under Grant ERC Future Proof; SNF; DARPA Award	The authors wish to thank Mark Tygert and Alex Gittens for helpful feedback on preliminary versions of this work. JAT gratefully acknowledges partial support from ONR Award N00014-17-1-2146 and the Gordon & Betty Moore Foundation. VC and AY were supported in part by the European Commission under Grant ERC Future Proof, SNF 200021-146750, and SNF CRSII2-147633. MU was supported in part by DARPA Award FA8750-17-2-0101.	Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Boutsidis C., 2016, P 48 ACM S THEOR COM; Boutsidis C., 2015, P 26 ANN ACM SIAM S, P887, DOI DOI 10.1137/1.9781611973730.61; Boutsidis C, 2013, SIAM J MATRIX ANAL A, V34, P1301, DOI 10.1137/120874540; Chiu JW, 2013, SIAM J MATRIX ANAL A, V34, P1361, DOI 10.1137/110852310; Clarkson K. L., 2009, P 41 ACM S THEOR COM; Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061; Cohen M. B., 2016, P 43 INT C AUT LANG, DOI DOI 10.4230/LIPICS.ICALP.2016.11; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Davis T. A., 2011, ACM T MATH SOFTWARE, V3; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Feldman D, 2016, ADV NEUR IN, V29; Fowlkes C, 2004, IEEE T PATTERN ANAL, V26, P214, DOI 10.1109/TPAMI.2004.1262185; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Gilbert A., 2012, SKETCHED SVD RECOVER; Gittens A., 2013, THESIS; Gittens A., 2011, SPECTRAL NORM ERROR; Gittens A., 2013, REVISITING NYSTROM M; Gittens A, 2016, J MACH LEARN RES, V17; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Gu M, 2015, SIAM J SCI COMPUT, V37, pA1139, DOI 10.1137/130938700; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Halko N, 2011, SIAM J SCI COMPUT, V33, P2580, DOI 10.1137/100804139; Higham N. J., 1988, APPL MATRIX THEORY, P1; Jain Prateek, 2016, C LEARN THEOR, P1147; Kumar S, 2012, J MACH LEARN RES, V13, P981; Li Y, 2014, LECT NOTES COMPUT SC, V8690, P174, DOI 10.1007/978-3-319-10605-2_12; Liberty E., 2009, THESIS; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Martinsson PG, 2011, APPL COMPUT HARMON A, V30, P47, DOI 10.1016/j.acha.2010.02.003; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; Musco C., 2017, SUBLINEAR TIME LOW R; Platt J. C., 2005, P 10 INT WORKSH ART, P261; Pourkamali- Anaraki F., 2016, RANDOMIZED CLUSTERED; Tropp J. A., 2017, 201701 ACM CALT; Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787; Tygert M., 2014, BETA VERSIONS MATLAB; Wang S., 2017, SCALABLE KERNEL K ME; Williams C. K. I., 2000, ADV NEURAL INFORM PR, V13; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Woolfe F, 2008, APPL COMPUT HARMON A, V25, P335, DOI 10.1016/j.acha.2007.12.002; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yurtsever A, 2017, PR MACH LEARN RES, V54, P1188	44	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401026
C	Wu, X; Guo, RQ; Suresh, AT; Kumar, S; Holtmann-Rice, D; Simcha, D; Yu, FX		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Xiang; Guo, Ruiqi; Suresh, Ananda Theertha; Kumar, Sanjiv; Holtmann-Rice, Dan; Simcha, David; Yu, Felix X.			Multiscale Quantization for Fast Similarity Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PRODUCT QUANTIZATION; TREES	We propose a multiscale quantization approach for fast similarity search on large, high-dimensional datasets. The key insight of the approach is that quantization methods, in particular product quantization, perform poorly when there is large variance in the norms of the data points. This is a common scenario for real-world datasets, especially when doing product quantization of residuals obtained from coarse vector quantization. To address this issue, we propose a multiscale formulation where we learn a separate scalar quantizer of the residual norm scales. All parameters are learned jointly in a stochastic gradient descent framework to minimize the overall quantization error. We provide theoretical motivation for the proposed technique and conduct comprehensive experiments on two large-scale public datasets, demonstrating substantial improvements in recall over existing state-of-the-art methods.	[Wu, Xiang; Guo, Ruiqi; Suresh, Ananda Theertha; Kumar, Sanjiv; Holtmann-Rice, Dan; Simcha, David; Yu, Felix X.] Google Res, New York, NY 11021 USA	Google Incorporated	Wu, X (corresponding author), Google Res, New York, NY 11021 USA.	wuxiang@google.com; guorq@google.com; theertha@google.com; sanjivk@google.com; dhr@google.com; dsimcha@google.com; felixyu@google.com	Jeong, Yongwook/N-7413-2016					Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; Andre F, 2015, PROC VLDB ENDOW, V9, P288; Babenko A, 2016, PROC CVPR IEEE, P2055, DOI 10.1109/CVPR.2016.226; Babenko A, 2015, PROC CVPR IEEE, P4240, DOI 10.1109/CVPR.2015.7299052; Babenko A, 2014, PROC CVPR IEEE, P931, DOI 10.1109/CVPR.2014.124; Babenko A, 2012, PROC CVPR IEEE, P3069, DOI 10.1109/CVPR.2012.6248038; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Cayley A., 1846, J REINE ANGEW MATH, V32, P119, DOI DOI 10.1515/CRLL.1846.32.119; Dasgupta S, 2008, ACM S THEORY COMPUT, P537; Douze M, 2016, LECT NOTES COMPUT SC, V9906, P785, DOI 10.1007/978-3-319-46475-6_48; Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240; Gersho A., 1992, VECTOR QUANTIZATION; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Gray R. M., 1984, IEEE ASSP Magazine, V1, P4, DOI 10.1109/MASSP.1984.1162229; Guo RQ, 2016, JMLR WORKSH CONF PRO, V51, P482; Harwood B, 2016, PROC CVPR IEEE, P5713, DOI 10.1109/CVPR.2016.616; He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378; Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson J, 2021, IEEE T BIG DATA, V7, P535, DOI 10.1109/TBDATA.2019.2921572; Kalantidis Y, 2014, PROC CVPR IEEE, P2329, DOI 10.1109/CVPR.2014.298; Kingma D.P, P 3 INT C LEARNING R; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Martinez J, 2016, LECT NOTES COMPUT SC, V9906, P137, DOI 10.1007/978-3-319-46475-6_9; Martinez Julieta, 2014, CORR; Matsui Y, 2015, IEEE I CONF COMP VIS, P1940, DOI 10.1109/ICCV.2015.225; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Norouzi M, 2014, IEEE T PATTERN ANAL, V36, P1107, DOI 10.1109/TPAMI.2013.231; Norouzi M, 2013, PROC CVPR IEEE, P3017, DOI 10.1109/CVPR.2013.388; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wang J., 2014, ARXIV14082927; Wang J, 2016, P IEEE, V104, P34, DOI 10.1109/JPROC.2015.2487976; Weiss Y, 2009, ADV NEURAL INFORM PR, P1753; Xu Zhang, 2015, INT C COMP VIS ICCV; Zhang T., 2014, PR MACH LEARN RES, P838; Zhang T, 2015, PROC CVPR IEEE, P4548, DOI 10.1109/CVPR.2015.7299085	39	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405080
C	Zhang, LJ; Yang, TB; Yi, JF; Jin, R; Zhou, ZH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhang, Lijun; Yang, Tianbao; Yi, Jinfeng; Jin, Rong; Zhou, Zhi-Hua			Improved Dynamic Regret for Non-degenerate Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recently, there has been a growing research interest in the analysis of dynamic regret, which measures the performance of an online learner against a sequence of local minimizers. By exploiting the strong convexity, previous studies have shown that the dynamic regret can be upper bounded by the path-length of the comparator sequence. In this paper, we illustrate that the dynamic regret can be further improved by allowing the learner to query the gradient of the function multiple times, and meanwhile the strong convexity can be weakened to other non-degenerate conditions. Specifically, we introduce the squared path-length, which could be much smaller than the path-length, as a new regularity of the comparator sequence. When multiple gradients are accessible to the learner, we first demonstrate that the dynamic regret of strongly convex functions can be upper bounded by the minimum of the path-length and the squared path-length. We then extend our theoretical guarantee to functions that are semi-strongly convex or self-concordant. To the best of our knowledge, this is the first time that semi-strong convexity and self-concordance are utilized to tighten the dynamic regret.	[Zhang, Lijun; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China; [Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Yi, Jinfeng] IBM Thomas J Watson Res Ctr, AI Fdn Lab, Yorktown Hts, NY USA; [Jin, Rong] Alibaba Grp, Seattle, WA USA	Nanjing University; University of Iowa; International Business Machines (IBM); Alibaba Group	Zhang, LJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing, Jiangsu, Peoples R China.	zhanglj@lamda.nju.edu.cn; tianbao-yang@uiowa.edu; jinfengyi@tencent.com; jinrong.jr@alibaba-inc.com; zhouzh@lamda.nju.edu.cn			NSFC [61603177, 61333014]; JiangsuSF [BK20160658]; NSF [IIS-1545995]; Collaborative Innovation Center of Novel Software Technology and Industrialization; YESS [2017QNRC001]	NSFC(National Natural Science Foundation of China (NSFC)); JiangsuSF; NSF(National Science Foundation (NSF)); Collaborative Innovation Center of Novel Software Technology and Industrialization; YESS	This work was partially supported by the NSFC (61603177, 61333014), JiangsuSF (BK20160658), YESS (2017QNRC001), NSF (IIS-1545995), and the Collaborative Innovation Center of Novel Software Technology and Industrialization. Jinfeng Yi is now at Tencent AI Lab, Bellevue, WA, USA.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Abernethy Jacob D, 2009, COMPETING DARK EFFIC; [Anonymous], 2011, JMLR WORKSHOP C P; [Anonymous], 2012, ADV NEURAL INFORM PR; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; Boyd S, 2004, CONVEX OPTIMIZATION; Buchbinder N., 2012, P 25 ANN C LEARN THE; Chiang C.-K., 2012, P 25 ANN C LEARN THE; Daniely A., 2015, P 32 INT C MACH LEAR; Gong P., 2014, ARXIV PREPRINT ARXIV; Hall, 2013, P 30 INT C MACH LEAR, P579; Hazan E., 2007, EL C COMP COMPL, V88; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Jadbabaie Ali, 2015, P 18 INT C ART INT S; Mokhtari A., 2016, ARXIV160304954; Necoara  Ion, 2015, ARXIV150406298; Nemirovski A., 2004, LECT NOTES; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Wang PW, 2014, J MACH LEARN RES, V15, P1523; Yang T., 2016, P 33 INT C MACH LEAR; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	25	11	11	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400070
C	Zhou, ZY; Mertikopoulos, P; Bambos, N; Boyd, S; Glynn, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhou, Zhengyuan; Mertikopoulos, Panayotis; Bambos, Nicholas; Boyd, Stephen; Glynn, Peter			Stochastic Mirror Descent in Variationally Coherent Optimization Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we examine a class of non-convex stochastic optimization problems which we call variationally coherent, and which properly includes pseudo-/quasi-convex and star-convex optimization problems. To solve such problems, we focus on the widely used stochastic mirror descent (SMD) family of algorithms (which contains stochastic gradient descent as a special case), and we show that the last iterate of SMD converges to the problem's solution set with probability 1. This result contributes to the landscape of non-convex stochastic optimization by clarifying that neither pseudo-/quasi-convexity nor star-convexity is essential for (almost sure) global convergence; rather, variational coherence, a much weaker requirement, suffices. Characterization of convergence rates for the subclass of strongly variationally coherent optimization problems as well as simulation results are also presented.	[Zhou, Zhengyuan; Bambos, Nicholas; Boyd, Stephen; Glynn, Peter] Stanford Univ, Stanford, CA 94305 USA; [Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, INRIA, LIG, Grenoble, France	Stanford University; Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS)	Zhou, ZY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	zyzhou@stanford.edu; panayotis.mertikopoulos@imag.fr; bambos@stanford.edu; boyd@stanford.edu; glynn@stanford.edu	Jeong, Yongwook/N-7413-2016	Bambos, Nicholas/0000-0001-9250-4553; Mertikopoulos, Panayotis/0000-0003-2026-9616	Stanford Graduate Fellowship; Huawei Innovation Research Program ULTRON; ANR JCJC project ORACLESS [ANR-16-CE33-0004-01]	Stanford Graduate Fellowship(Stanford University); Huawei Innovation Research Program ULTRON; ANR JCJC project ORACLESS(French National Research Agency (ANR))	Zhengyuan Zhou is supported by Stanford Graduate Fellowship and would like to thank Yinyu Ye and Jose Blanchet for constructive discussions and feedback. Panayotis Mertikopoulos gratefully acknowledges financial support from the Huawei Innovation Research Program ULTRON and the ANR JCJC project ORACLESS (grant no. ANR-16-CE33-0004-01).	Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Benaim M., 1996, Journal of Dynamics and Differential Equations, V8, P141, DOI 10.1007/BF02218617; Benaim M, 1999, LECT NOTES MATH, V1709, P1; Boyd S, 2004, CONVEX OPTIMIZATION; Facchinei F., 2003, SPRINGER SERIES OPER; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; KRICHENE W., 2015, NIPS 15; KUSHNER H., 2013, STOCHASTIC MODELLING; MERTIKOPOULOS P., 2016, LEARNING GAMES CONTI; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Rockafellar R.T., 1998, SERIES COMPREHENSIVE, V317; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Stoltz G., 2012, P ADV NEUR INF PROC, P471; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Xiao L, 2010, J MACH LEARN RES, V11, P2543	22	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407013
C	Pentina, A; Urner, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Pentina, Anastasia; Urner, Ruth			Lifelong Learning with Weighted Majority Votes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Better understanding of the potential benefits of information transfer and representation learning is an important step towards the goal of building intelligent systems that are able to persist in the world and learn over time. In this work, we consider a setting where the learner encounters a stream of tasks but is able to retain only limited information from each encountered task, such as a learned predictor. In contrast to most previous works analyzing this scenario, we do not make any distributional assumptions on the task generating process. Instead, we formulate a complexity measure that captures the diversity of the observed tasks. We provide a lifelong learning algorithm with error guarantees for every observed task (rather than on average). We show sample complexity reductions in comparison to solving every task in isolation in terms of our task complexity measure. Further, our algorithmic framework can naturally be viewed as learning a representation from encountered tasks with a neural network.	[Pentina, Anastasia] IST Austria, Klosterneuburg, Austria; [Urner, Ruth] Max Planck Inst Intelligent Syst, Tubingen, Germany	Institute of Science & Technology - Austria; Max Planck Society	Pentina, A (corresponding author), IST Austria, Klosterneuburg, Austria.	apentina@ist.ac.at; rurner@tuebingen.mpg.de			European Research Council under the European Union [308036]	European Research Council under the European Union(European Research Council (ERC))	This work was in parts funded by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036.	Abernethy J, 2007, LECT NOTES COMPUT SC, V4539, P484, DOI 10.1007/978-3-540-72927-3_35; Argyriou A., 2007, NIPS, V19, P41, DOI DOI 10.1007/S10994-007-5040-8; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Balcan, 2015, C LEARN THEOR, P191; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Ben-David S., 2003, EXPLOITING TASK RELA; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Boucheron S., 2005, ESAIM-PROBAB STAT, V9, P323, DOI [DOI 10.1051/PS:2005018, 2182250]; Crammer Koby, 2012, PROC 25 INT C NEURAL, P1475; Freund Y, 1996, ICML; Germain P, 2016, PR MACH LEARN RES, V48; Kifer Daniel, 2004, VLDB, DOI DOI 10.1016/B978-012088469-8/50019-X; Kolter JZ, 2007, J MACH LEARN RES, V8, P2755; Kumar A., 2012, INT C MACH LEARN; Kuzborskij Ilja, 2013, P 30 INT C MACH LEAR, P942; Mansour Yishay, 2009, COLT 2009 22 C LEARN; Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pentina A., 2015, INT C ALG LEARN THEO; Pentina A, 2014, PR MACH LEARN RES, V32, P991; Pontil M., 2013, PROC ANN C LEARN THE, P55; Ruvolo P., 2013, P 30 INT C MACHINE L, P507; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Thrun S, 1998, LEARNING TO LEARN, P181; THRUN S, 1995, ROBOT AUTON SYST, V15, P25, DOI 10.1016/0921-8890(95)00004-Y; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	27	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700059
C	Bzdok, D; Eickenberg, M; Grisel, O; Thirion, B; Varoquaux, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bzdok, Danilo; Eickenberg, Michael; Grisel, Olivier; Thirion, Bertrand; Varoquaux, Gael			Semi-Supervised Factored Logistic Regression for High-Dimensional Neuroimaging Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA			Brain Imaging; Cognitive Science; Semi-Supervised Learning; Systems Biology	FMRI	Imaging neuroscience links human behavior to aspects of brain biology in ever-increasing datasets. Existing neuroimaging methods typically perform either discovery of unknown neural structure or testing of neural structure associated with mental tasks. However, testing hypotheses on the neural correlates underlying larger sets of mental tasks necessitates adequate representations for the observations. We therefore propose to blend representation modelling and task classification into a unified statistical learning problem. A multinomial logistic regression is introduced that is constrained by factored coefficients and coupled with an autoencoder. We show that this approach yields more accurate and interpretable neural models of psychological tasks in a reference dataset, as well as better generalization to other datasets.	[Bzdok, Danilo] INRIA, Parietal Team, Saclay, France; CEA, Neurospin, Gif Sur Yvette, France	Inria; CEA	Bzdok, D (corresponding author), INRIA, Parietal Team, Saclay, France.	danilo.bzdok@inria.fr; michael.eickenberg@inria.fr; olivier.grisel@inria.fr; bertrand.thirion@inria.fr; gael.varoquaux@inria.fr		Varoquaux, Gael/0000-0003-1076-5122	European Union [604102]; German National Academic Foundation; MetaMRI associated team	European Union(European Commission); German National Academic Foundation; MetaMRI associated team	The research leading to these results has received funding from the European Union Seventh Framework Programme (FP7/2007-2013) under grant agreement no. 604102 (Human Brain Project). Data were provided by the Human Connectome Project. Further support was received from the German National Academic Foundation (D.B.) and the MetaMRI associated team (B.T., G.V.).	Abraham A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00014; Amunts K, 2013, SCIENCE, V340, P1472, DOI 10.1126/science.1235381; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Barch DM, 2013, NEUROIMAGE, V80, P169, DOI 10.1016/j.neuroimage.2013.05.033; Bastien F., 2012, ARXIV12115590; Beckmann CF, 2005, PHILOS T R SOC B, V360, P1001, DOI 10.1098/rstb.2005.1634; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Biswal BB, 2010, P NATL ACAD SCI USA, V107, P4734, DOI 10.1073/pnas.0911855107; Cole M. W., 2014, NEURON C, V83; Fox MD, 2007, NAT REV NEUROSCI, V8, P700, DOI 10.1038/nrn2201; Frackowiak R, 2015, PHILOS T R SOC B, V370, P20, DOI 10.1098/rstb.2014.0171; Friston KJ, 1997, NEUROIMAGE, V6, P218, DOI 10.1006/nimg.1997.0291; Friston KJ., 1994, HUM BRAIN MAPP, V2, P189, DOI [10.1002/hbm.460020402, DOI 10.1002/HBM.460020402]; Gorgolewski Krzysztof, 2011, Front Neuroinform, V5, P13, DOI 10.3389/fninf.2011.00013; Hertz J., 1991, INTRO THEORY NEURAL, V1; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hipp J.F., 2015, CURR BIOL; Le Q.V., 2011, NEURIPS, P1017; Need Anna C, 2010, Dialogues Clin Neurosci, V12, P37; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pinel P, 2007, BMC NEUROSCI, V8, DOI 10.1186/1471-2202-8-91; Poldrack RA, 2014, NAT NEUROSCI, V17, P1510, DOI 10.1038/nn.3818; Schwartz Y, 2013, ADV NEURAL INFORM PR; Smith SM, 2013, NEUROIMAGE, V80, P144, DOI 10.1016/j.neuroimage.2013.05.039; Smith SM, 2009, P NATL ACAD SCI USA, V106, P13040, DOI 10.1073/pnas.0905267106; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Varoquaux G, 2011, LECT NOTES COMPUT SC, V6801, P562, DOI 10.1007/978-3-642-22092-0_46; 2009, PSYCHOL SCI, V20, P1364	28	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100014
C	Rippel, O; Snoek, J; Adams, RP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rippel, Oren; Snoek, Jasper; Adams, Ryan P.			Spectral Representations for Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Discrete Fourier transforms provide a significant speedup in the computation of convolutions in deep learning. In this work, we demonstrate that, beyond its advantages for efficient computation, the spectral domain also provides a powerful representation in which to model and train convolutional neural networks (CNNs). We employ spectral representations to introduce a number of innovations to CNN design. First, we propose spectral pooling, which performs dimensionality reduction by truncating the representation in the frequency domain. This approach preserves considerably more information per parameter than other pooling strategies and enables flexibility in the choice of pooling output dimensionality. This representation also enables a new form of stochastic regularization by randomized modification of resolution. We show that these methods achieve competitive results on classification and approximation tasks, without using any dropout or max-pooling. Finally, we demonstrate the effectiveness of complex-coefficient spectral parameterization of convolutional filters. While this leaves the underlying model unchanged, it results in a representation that greatly facilitates optimization. We observe on a variety of popular CNN configurations that this leads to significantly faster convergence during training.	[Rippel, Oren] MIT, Dept Math, Cambridge, MA 02139 USA; [Snoek, Jasper; Adams, Ryan P.] Twitter, San Francisco, CA USA; [Snoek, Jasper; Adams, Ryan P.] Harvard SEAS, San Francisco, CA USA	Massachusetts Institute of Technology (MIT); Twitter, Inc.	Rippel, O (corresponding author), MIT, Dept Math, Cambridge, MA 02139 USA.	rippel@math.mit.edu; jsnoek@seas.harvard.edu; rpa@seas.harvard.edu			Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy [DE-AC02-05CH11231]	Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy(United States Department of Energy (DOE))	We would like to thank Prabhat, Michael Gelbart and Matthew Johnson for useful discussions and assistance throughout this project. Jasper Snoek was a fellow in the Harvard Center for Research on Computation and Society. This work is supported by the Applied Mathematics Program within the Office of Science Advanced Scientific Computing Research of the U.S. Department of Energy under contract No. DE-AC02-05CH11231. This work used resources of the National Energy Research Scientific Computing Center (NERSC). We thank Helen He and Doug Jacobsen for providing us with access to the Babbage Xeon-Phi testbed at NERSC.	[Anonymous], 2013, ABS13125851 CORR; Bengio Y., 2007, LARGE SCALE KERNEL M; Goodfellow I. J., 2013, ABS13024389 CORR; Hinton G. E., 2014, MIT BRAIN COGNITIVE; Hinton Geoffrey, 2014, REDDIT MACHINE LEARN; Ioffe S., 2015, ICML, P448; Karpathy A., 2014, COMPUTER VISION PATT; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 1990, ADV NEURAL INFORM PR, V2; Lee C., 2014, ABS14095185 CORR; Lin M., 2013, ABS13124400 CORR; Rippel Oren, 2014, INT C MACH LEARN; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Snoek J., 2012, ADV NEURAL INF PROCE; Snoek Jasper, 2015, INT C MACHINE LEARNI; Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302; Vasilache N, 2014, ABS14127580 CORR; Zeiler M. D., 2013, ABS13013557 CORR	20	11	11	4	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100017
C	Shanmugam, K; Kocaoglu, M; Dimakis, AG; Vishwanath, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shanmugam, Karthikeyan; Kocaoglu, Murat; Dimakis, Alexandros G.; Vishwanath, Sriram			Learning Causal Graphs with Small Interventions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MARKOV EQUIVALENCE CLASSES	We consider the problem of learning causal networks with interventions, when each intervention is limited in size under Pearl's Structural Equation Model with independent errors (SEM-IE). The objective is to minimize the number of experiments to discover the causal directions of all the edges in a causal graph. Previous work has focused on the use of separating systems for complete graphs for this task. We prove that any deterministic adaptive algorithm needs to be a separating system in order to learn complete graphs in the worst case. In addition, we present a novel separating system construction, whose size is close to optimal and is arguably simpler than previous work in combinatorics. We also develop a novel information theoretic lower bound on the number of interventions that applies in full generality, including for randomized adaptive learning algorithms. For general chordal graphs, we derive worst case lower bounds on the number of interventions. Building on observations about induced trees, we give a new deterministic adaptive algorithm to learn directions on any chordal skeleton completely. In the worst case, our achievable scheme is an.-approximation algorithm where. is the independence number of the graph. We also show that there exist graph classes for which the sufficient number of experiments is close to the lower bound. In the other extreme, there are graph classes for which the required number of experiments is multiplicatively sigma away from our lower bound. In simulations, our algorithm almost always performs very close to the lower bound, while the approach based on separating systems for complete graphs is significantly worse for random chordal graphs.	[Shanmugam, Karthikeyan; Kocaoglu, Murat; Dimakis, Alexandros G.; Vishwanath, Sriram] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Shanmugam, K (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	karthiksh@utexas.edu; mkocaoglu@utexas.edu; dimakis@austin.utexas.edu; sriram@ece.utexas.edu	Dimakis, Alexandros G/P-6034-2019; Dimakis, Alexandros G/A-5496-2011	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033; Kocaoglu, Murat/0000-0003-2447-2689	NSF [CCF 1344179, 1344364, 1407278, 1422549]; ARO YIP award [W911NF-14-1-0258]	NSF(National Science Foundation (NSF)); ARO YIP award	Authors acknowledge the support from grants: NSF CCF 1344179, 1344364, 1407278, 1422549 and a ARO YIP award (W911NF-14-1-0258). We also thank Frederick Eberhardt for helpful discussions.	Andersson SA, 1997, ANN STAT, V25, P505; Buhlmann P, 2012, P 6 EUR WORKSH PROB; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt F., P 21 C UNC ART INT U, P178; Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Hoyer P. O, 2008, P NIPS 2008; Hu H., 2014, P NIPS 2014 MONTR CA; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Katona G., 1966, J COMB THEORY, V1, P174; LIPTON RJ, 1979, SIAM J APPL MATH, V36, P177, DOI 10.1137/0209046; MEEK C, 1995, P 11 INT C UNC ART I; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; VERMA TS, 1992, P 8 INT C UNC ART IN; WEGENER I, 1979, DISCRETE MATH, V28, P219, DOI 10.1016/0012-365X(79)90101-8	17	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102058
C	Watter, M; Springenberg, JT; Boedecker, J; Riedmiller, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Watter, Manuel; Springenberg, Jost Tobias; Boedecker, Joschka; Riedmiller, Martin			Embed to Control: A Locally Linear Latent Dynamics Model for Control from Raw Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce Embed to Control (E2C), a method for model learning and control of non-linear dynamical systems from raw pixel images. E2C consists of a deep generative model, belonging to the family of variational autoencoders, that learns to generate image trajectories from a latent space in which the dynamics is constrained to be locally linear. Our model is derived directly from an optimal control formulation in latent space, supports long-term prediction of image sequences and exhibits strong performance on a variety of complex control problems.	[Watter, Manuel; Springenberg, Jost Tobias; Boedecker, Joschka] Univ Freiburg, Freiburg, Germany; [Riedmiller, Martin] Google DeepMind, London, England	University of Freiburg; Google Incorporated	Watter, M (corresponding author), Univ Freiburg, Freiburg, Germany.	watterm@cs.uni-freiburg.de; springj@cs.uni-freiburg.de; jboedeck@cs.uni-freiburg.de; riedmiller@google.com			DFG [SPP1597]; BrainLinks-BrainTools Cluster of Excellence [EXC 1086]; State Graduate Funding Program of Baden-Wurttemberg	DFG(German Research Foundation (DFG)); BrainLinks-BrainTools Cluster of Excellence; State Graduate Funding Program of Baden-Wurttemberg	We thank A. Radford, L. Metz, and T. DeWolf for sharing code, as well as A. Dosovitskiy for useful discussions. This work was partly funded by a DFG grant within the priority program "Autonomous learning" (SPP1597) and the BrainLinks-BrainTools Cluster of Excellence (grant number EXC 1086). M. Watter is funded through the State Graduate Funding Program of Baden-Wurttemberg.	Bayer J., 2014, LEARNING STOCHASTIC; Bohmer W., 2015, KI KUNSTLICHE INTELL; Cohen T., 2015, ICLR; Dinh L., 2015, ABS14108516 CORR; Dosovitskiy A., 2015, P CVPR; Gregor K., 2015, P ICML; Hinton G., 2011, P ICANN; Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC; Jordan M. I., 1999, MACHINE LEARNING; Kingma D., 2014, P ICLR; Kingma Diederik P, 2015, ICLR 2015; Lange S., 2010, P IJCNN; Langford J., 2009, ICML; Levine S., 2013, P NIPS; Levine S., 2015, ABS150400702 CORR; Li W., 2004, P ICINCO; Matsubara T., 2014, UAI; Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pan Y., 2014, P NIPS; Rezende D. J., 2014, P ICML; Stengel R. F., 1994, OPTIMAL CONTROL ESTI; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tanaka K, 1996, IEEE T FUZZY SYST, V4, P1, DOI 10.1109/91.481840; Tassa Y., 2008, P NIPS; Taylor G. W., 2010, P CVPR; Todorov  E., 2005, ACC; Toussaint M., 2009, P ICML; van Hoof H., 2015, P AISTATS; Wahlstrom N., 2015, ABS50202251 CORR; West M., 1997, BAYESIAN FORECASTING, V2nd. ed.; Zeiler Matthew D., 2010, CVPR	32	11	11	4	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103002
C	Zhang, CC; Chaudhuri, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhang, Chicheng; Chaudhuri, Kamalika			Active Learning from Weak and Strong Labelers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					An active learner is given a hypothesis class, a large set of unlabeled examples and the ability to interactively query labels to an oracle of a subset of these examples; the goal of the learner is to learn a hypothesis in the class that fits the data well by making as few label queries as possible. This work addresses active learning with labels obtained from strong and weak labelers, where in addition to the standard active learning setting, we have an extra weak labeler which may occasionally provide incorrect labels. An example is learning to classify medical images where either expensive labels may be obtained from a physician (oracle or strong labeler), or cheaper but occasionally incorrect labels may be obtained from a medical resident (weak labeler). Our goal is to learn a classifier with low error on data labeled by the oracle, while using the weak labeler to reduce the number of label queries made to this labeler. We provide an active learning algorithm for this setting, establish its statistical consistency, and analyze its label complexity to characterize when it can provide label savings over using the strong labeler alone.	[Zhang, Chicheng; Chaudhuri, Kamalika] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Zhang, CC (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	chichengzhang@ucsd.edu; kamalika@eng.ucsd.edu			NSF [IIS 1162581]	NSF(National Science Foundation (NSF))	We thank NSF under IIS 1162581 for research support and Jennifer Dy for introducing us to the problem of active learning from multiple labelers.	Balcan M. F, 2013, COLT; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Beygelzimer A., 2009, ACTIVE LEARNING ERM; Beygelzimer A., 2010, NIPS; Bshouty NH, 2005, MACH LEARN, V59, P99, DOI 10.1007/s10994-005-0464-5; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Crammer K., 2005, NIPS; Dasgupta S., 2007, NIPS; DASGUPTA S, 2005, NIPS; Dekel O, 2012, J MACH LEARN RES, V13, P2655; Donmez P., 2008, CIKM; Fang M, 2012, IEEE DATA MINING, P858, DOI 10.1109/ICDM.2012.64; Hanneke S, 2007, ICML; Hsu D., 2010, THESIS; Ipeirotis PG, 2014, DATA MIN KNOWL DISC, V28, P402, DOI 10.1007/s10618-013-0306-1; Kalai AT, 2012, J COMPUT SYST SCI, V78, P1481, DOI 10.1016/j.jcss.2011.12.026; Kanade Varun, 2014, COLT; Lin C. H., 2015, ICML WORKSH CROWDS M; Lin Christopher H, 2014, HCOMP; Malago L, 2014, NIPS WORKSH LEARN WI; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Simon HU, 2014, ANN MATH ARTIF INTEL, V71, P283, DOI 10.1007/s10472-012-9325-7; Song S., 2015, AISTATS; Urner R., 2012, ARTIFICIAL INTELLIGE, P1252; Vijayanarasimhan S, 2011, INT J COMPUT VISION, V91, P24, DOI 10.1007/s11263-010-0372-4; Vijayanarasimhan S, 2009, PROC CVPR IEEE, P2262, DOI 10.1109/CVPRW.2009.5206705; Yan Y., 2012, P ART INT STAT, P1350; Yan Y., 2011, ICML, V11, P1161; Zhang C., 2014, NIPS	30	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103026
C	Hernandez-Lobato, JM; Hoffman, MW; Ghahramani, Z		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hernandez-Lobato, Jose Miguel; Hoffman, Matthew W.; Ghahramani, Zoubin			Predictive Entropy Search for Efficient Global Optimization of Black-box Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a novel information-theoretic approach for Bayesian optimization called Predictive Entropy Search (PES). At each iteration, PES selects the next evaluation point that maximizes the expected information gained with respect to the global maximum. PES codifies this intractable acquisition function in terms of the expected reduction in the differential entropy of the predictive distribution. This reformulation allows PES to obtain approximations that are both more accurate and efficient than other alternatives such as Entropy Search (ES). Furthermore, PES can easily perform a fully Bayesian treatment of the model hyperparameters while ES cannot. We evaluate PES in both synthetic and real-world applications, including optimization problems in machine learning, finance, biotechnology, and robotics. We show that the increased accuracy of PES leads to significant gains in optimization performance.	[Hernandez-Lobato, Jose Miguel; Hoffman, Matthew W.; Ghahramani, Zoubin] Univ Cambridge, Cambridge, England	University of Cambridge	Hernandez-Lobato, JM (corresponding author), Univ Cambridge, Cambridge, England.	jmh233@cam.ac.uk; mwh30@cam.ac.uk; zoubin@eng.cam.ac.uk			Rafael del Pino Foundation	Rafael del Pino Foundation	J.M.H.L acknowledges support from the Rafael del Pino Foundation.	AHMAD IA, 1976, IEEE T INFORM THEORY, V22, P372, DOI 10.1109/TIT.1976.1055550; Anderson B. S., 2000, ICML, P17; Bochner S., 1959, LECT FOURIER INTEGRA; Brochu E., 2007, P ADV NEUR INF PROC, P409, DOI [10.5555/2981562.2981614, DOI 10.5555/2981562.2981614]; Burrows EH, 2009, BIOTECHNOL PROGR, V25, P1009, DOI 10.1002/btpr.213; Cora V.M., 2009, TR200923 UBC DEP COM; Hennig Philipp, 2012, J MACHINE LEARNING R, P13; Hoffman M., 2011, P 27 C UNCERTAINTY A, P327; Houlsby N., 2012, NIPS, V3, P2096; Jondeau E, 2006, J INT MONEY FINANC, V25, P827, DOI 10.1016/j.jimonfin.2006.04.007; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kushner H. J., 1964, J BASIC ENG, V86; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Lichman M, 2013, UCI MACHINE LEARNING; Lizotte D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P944; Lizotte DJ, 2008, PRACTICAL BAYESIAN O; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Minka T., 2001, THESIS MIT CAMBRIDGE; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Seeger MW, 2008, J MACH LEARN RES, V9, P759; Snoek J., 2012, P NIPS, V12, P2960; Solak E., 2003, ADV NEURAL INFORM PR, P1057; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Vanhatalo J., 2012, ABS12065754 CORR; Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2; Wang Z., 2013, ICML; Westervelt E., 2007, CONTROL AUTOMATION S; Zilinskas A., 1978, GLOBAL OPTIMIZATION, V2	30	11	11	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100103
C	Saade, A; Krzakala, F; Zdeborova, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Saade, Alaa; Krzakala, Florent; Zdeborova, Lenka			Spectral Clustering of Graphs with the Bethe Hessian	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				STOCHASTIC BLOCKMODELS; COMMUNITY STRUCTURE	Spectral clustering is a standard approach to label nodes on a graph by studying the (largest or lowest) eigenvalues of a symmetric real matrix such as e.g. the adjacency or the Laplacian. Recently, it has been argued that using instead a more complicated, non-symmetric and higher dimensional operator, related to the non-backtracking walk on the graph, leads to improved performance in detecting clusters, and even to optimal performance for the stochastic block model. Here, we propose to use instead a simpler object, a symmetric real matrix known as the Bethe Hessian operator, or deformed Laplacian. We show that this approach combines the performances of the non-backtracking operator, thus detecting clusters all the way down to the theoretical limit in the stochastic block model, with the computational, theoretical and memory advantages of real symmetric matrices.	[Saade, Alaa] Ecole Normale Super, CNRS UMR 8550, Lab Phys Stat, 24 Rue Lhomond, F-75005 Paris, France; [Krzakala, Florent] UPMC Univ Paris 06, Sorbonne Univ, Ecole Normale Super, Lab Phys Stat,CNRS UMR 8550, F-75005 Paris, France; [Zdeborova, Lenka] CEA Saclay, Inst Phys Theor, F-91191 Gif Sur Yvette, France; [Zdeborova, Lenka] CNRS URA 2306, F-91191 Gif Sur Yvette, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute of Physics (INP); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); CNRS - Institute of Physics (INP); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Sorbonne Universite; Universite Paris Cite; CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; CEA; Centre National de la Recherche Scientifique (CNRS)	Saade, A (corresponding author), Ecole Normale Super, CNRS UMR 8550, Lab Phys Stat, 24 Rue Lhomond, F-75005 Paris, France.		Krzakala, Florent/Q-9652-2019	Krzakala, Florent/0000-0003-2313-2578	ERC under the European Union [307087-SPARCS]	ERC under the European Union	This work has been supported in part by the ERC under the European Union's 7th Framework Programme Grant Agreement 307087-SPARCS	Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; [Anonymous], [No title captured]; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Bordenave C, 2010, RANDOM STRUCT ALGOR, V37, P332, DOI 10.1002/rsa.20313; Decelle A, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.065701; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Lusseau D, 2003, BEHAV ECOL SOCIOBIOL, V54, P396, DOI 10.1007/s00265-003-0651-y; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Mooij Joris M, 2004, NIPS; Mossel E., 2013, PROOF BLOCK MODEL TH; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Ricci-Tersenghi F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08015; Rogers T, 2008, PHYS REV E, V78, DOI 10.1103/PhysRevE.78.031116; RUHE A, 1973, SIAM J NUMER ANAL, V10, P674, DOI 10.1137/0710059; Saade A, 2014, EPL-EUROPHYS LETT, V107, DOI 10.1209/0295-5075/107/50005; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Watanabe Y., 2009, ADV NEURAL INF PROCE, P2017; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752	23	11	11	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102081
C	Zhu, ZY; Luo, P; Wang, XG; Tang, X		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhu, Zhenyao; Luo, Ping; Wang, Xiaogang; Tang, Xiaoou			Multi-View Perceptron: a Deep Model for Learning Face Identity and View Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Various factors, such as identity, view, and illumination, are coupled in face images. Disentangling the identity and view representations is a major challenge in face recognition. Existing face recognition systems either use handcrafted features or learn features discriminatively to improve recognition accuracy. This is different from the behavior of primate brain. Recent studies [5, 19] discovered that primate brain has a face-processing network, where view and identity are processed by different neurons. Taking into account this instinct, this paper proposes a novel deep neural net, named multi-view perceptron (MVP), which can untangle the identity and view features, and in the meanwhile infer a full spectrum of multi-view images, given a single 2D face image. The identity features of MVP achieve superior performance on the MultiPIE dataset. MVP is also capable to interpolate and predict images under viewpoints that are unobserved in the training data.	[Zhu, Zhenyao; Luo, Ping; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China; [Wang, Xiaogang] Chinese Univ Hong Kong, Dept Elect Engn, Hong Kong, Peoples R China; [Zhu, Zhenyao; Luo, Ping; Wang, Xiaogang; Tang, Xiaoou] Chinese Acad Sci, Shenzhen Inst Adv Technol, Shenzhen Key Lab CVPR, Shenzhen, Peoples R China	Chinese University of Hong Kong; Chinese University of Hong Kong; Chinese Academy of Sciences; Shenzhen Institute of Advanced Technology, CAS	Zhu, ZY (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	zz012@ie.cuhk.edu.hk; lp011@ie.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk; xtang@ie.cuhk.edu.hk	Luo, Ping/HGE-7623-2022; Luo, Ping/GPG-2707-2022	Luo, Ping/0000-0002-6685-7950; 	Natural Science Foundation of China [91320101, 61472410]; Shenzhen Basic Research Program [JCYJ20120903092050890, JCYJ20120617114614438, JCYJ20130402113127496]; Guangdong Innovative Research Team Program [201001D0104648280]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Basic Research Program; Guangdong Innovative Research Team Program	This work is partly supported by Natural Science Foundation of China (91320101, 61472410), Shenzhen Basic Research Program (JCYJ20120903092050890, JCYJ20120617114614438, JCYJ20130402113127496), Guangdong Innovative Research Team Program (201001D0104648280).	Ahonen T, 2006, IEEE T PATTERN ANAL, V28, P2037, DOI 10.1109/TPAMI.2006.244; [Anonymous], 2013, CVPR; Asthana A., 2011, ICCV; Cao Zhimin, 2010, CVPR; Chen D., 2013, CVPR; Freiwald WA, 2010, SCIENCE, V330, P845, DOI 10.1126/science.1194908; Gonzalez-Jimenez D, 2007, IEEE T INF FOREN SEC, V2, P413, DOI 10.1109/TIFS.2007.903543; Gross R., 2010, IMAGE VISION COMPUTI; Hu Y., 2004, AFGR; Huang G. B., 2012, CVPR; Kullback S., 1951, ANN MATH STAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li S., 2012, ECCV; Li Y., 2000, AFGR; Liu CJ, 2002, IEEE T IMAGE PROCESS, V11, P467, DOI 10.1109/TIP.2002.999679; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo P., 2013, ICCV; Luo Ping, 2012, CVPR; Nair V., 2010, ICML, P807; Ohayon S, 2012, NEURON, V74, P567, DOI 10.1016/j.neuron.2012.03.024; Reed S., 2014, ICML; Simonyan K., 2013, BMVC; Sun Y., 2014, NIPS; Sun Y., 2013, ICCV; Sun Y., 2014, CVPR; Taigman Y., 2014, CVPR; Tang C., 2013, P ADV NEUR INF PROC, P503; Zhu Z., 2013, ICCV	28	11	11	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102107
C	Andrews, S		Thrun, S; Saul, K; Scholkopf, B		Andrews, S			Multiple instance learning via disjunctive programming boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				ALGORITHMS	Learning from ambiguous training data is highly relevant in many applications. We present a new learning algorithm for classification problems where labels are associated with sets of pattern instead of individual patterns. This encompasses multiple instance learning as a special case. Our approach is based on a generalization of linear programming boosting and uses results from disjunctive programming to generate successively stronger linear relaxations of a discrete non-convex problem.	Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Brown University	Andrews, S (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.	stu@cs.brown.edu						ANDREWS S, 2003, ADV NEURAL INFORMATI, V15; BALAS E, 1985, SIAM J ALGEBRA DISCR, V6, P466, DOI 10.1137/0606047; DEMIREZ A, 2000, APPL ALGORITHMS COMP; Demiriz A, 2002, MACH LEARN, V46, P225, DOI 10.1023/A:1012470815092; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Gartner T., 2002, ICML, V2, P7; Grove A., 1998, P 15 NAT C ART INT; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Lee S, 2000, COMPUT CHEM ENG, V24, P2125, DOI 10.1016/S0098-1354(00)00581-0; Maron O., 1998, P 15 INT C MACH LEAR, P341; Ramon Jan, 2000, P ICML 2000 WORKSH A; Ratsch G, 2002, IEEE T PATTERN ANAL, V24, P1184, DOI 10.1109/TPAMI.2002.1033211; RATSCH G, 1998, NCTR1998021 U LOND D; ZHANG Q, 2002, ADV NEURAL INFORMATI, V14	14	11	11	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						65	72						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500009
C	Kemp, C; Griffiths, TL; Stromsten, S; Tenenbaum, JB		Thrun, S; Saul, K; Scholkopf, B		Kemp, C; Griffiths, TL; Stromsten, S; Tenenbaum, JB			Semi-supervised learning with trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We describe a nonparametric Bayesian approach to generalizing from few labeled examples, guided by a larger set of unlabeled objects and the assumption of a latent tree-structure to the domain. The tree (or a distribution over trees) may be inferred using the unlabeled data. A prior over concepts generated by a mutation process on the inferred tree(s) allows efficient computation of the optimal Bayesian classification function from the labeled examples. We test our approach on eight real-world datasets.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Kemp, C (corresponding author), MIT, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA.			Kemp, Charles/0000-0001-9683-8737				BELKIN M, 2003, IN PRESS MACHINE LEA; BLUM A, 2001, ICML, V18; CHAPELLE O, 2003, NIPS, V15; HAUSSLER D, 1994, MACHINE LEARNING, V14; JOW H, 2002, MOL BIOL EVOL, V19, P1951; KEMP C, 2003, P25 ANN C COGN SCI S; MITCHELL TOM M., 1997, MACH LEARN, P2; NEAL R, 2001, 0108 U TOR; SHIH L, 2003, UNPUB LEARNING CLASS; SZUMMER M, 2002, NIPS, V14; VERT JP, 2002, BIOINFORMATICS, V1, P1; ZHU X, 2003, ICML, V20	12	11	11	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						257	264						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500033
C	Ramanan, D; Forsyth, DA		Thrun, S; Saul, K; Scholkopf, B		Ramanan, D; Forsyth, DA			Automatic annotation of everyday movements	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MOTION	This paper describes a system that can annotate a video sequence with: a description of the appearance of each actor; when the actor is in view; and a representation of the actor's activity while in view. The system does not require a fixed background, and is automatic. The system works by (1) tracking people in 2D and then, using an annotated motion capture dataset, (2) synthesizing an annotated 3D motion sequence matching the 2D tracks. The 3D motion capture data is manually annotated off-line using a class structure that describes everyday motions and allows motion annotations to be composed - one may jump while running, for example. Descriptions computed from video of real motions show that the method is accurate.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ramanan, D (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.							Aggarwal JK, 1999, COMPUT VIS IMAGE UND, V73, P428, DOI 10.1006/cviu.1998.0744; ARIKAN O, 2003, P ACM SIGGRAPH; Arikan O, 2002, P ACM SIGGRAPH; Bobick AF, 2001, IEEE T PATTERN ANAL, V23, P257, DOI 10.1109/34.910878; Bobick AF, 1997, PHILOS T ROY SOC B, V352, P1257, DOI 10.1098/rstb.1997.0108; CAMPBELL LW, 1995, ICCV, P624; CHANG CC, 2000, LIBSVM INTRO BENCHMA; FELZENSCHWALB P, 2000, P CVPR, P141; Gavrila DM, 1999, COMPUT VIS IMAGE UND, V73, P82, DOI 10.1006/cviu.1998.0716; HODGINS JK, 1997, SIGGRAPH 97; Jordan M. I., 1999, LEARNING GRAPHICAL M; LEVENTON ME, 1998, TR9806 MERL; Ramanan D., 2003, P CVPR; RAMANAN D, 2003, UCBCSD031262	14	11	11	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1547	1554						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500192
C	Cesa-Bianchi, N; Conconi, A; Gentile, C		Dietterich, TG; Becker, S; Ghahramani, Z		Cesa-Bianchi, N; Conconi, A; Gentile, C			On the generalization ability of on-line learning algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In this paper we show that on-line algorithms for classification and regression can be naturally used to obtain hypotheses with good data-dependent tail bounds on their risk. Our results are proven without requiring complicated concentration-of-measure arguments and they hold for arbitrary on-line learning algorithms. Furthermore, when applied to concrete on-line algorithms, our results yield tail bounds that in many cases are comparable or better than the best known bounds.	Univ Milan, DTI, I-26013 Crema, Italy	University of Milan	Cesa-Bianchi, N (corresponding author), Univ Milan, DTI, Via Bramante 65, I-26013 Crema, Italy.	cesa-bianchi@dti.unimi.it; conconi@dti.unimi.it; gentile@dsi.unimi.it	Cesa-Bianchi, Nicolò/C-3721-2013	Cesa-Bianchi, Nicolò/0000-0001-8477-4748				Angluin D., 1988, Machine Learning, V2, P319, DOI 10.1023/A:1022821128753; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; Azuma K., 1967, THOKU MATH J, V19, P357, DOI DOI 10.2748/TMJ/1178243286; Blum A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P203, DOI 10.1145/307400.307439; Boucheron S, 2000, RANDOM STRUCT ALGOR, V16, P277, DOI 10.1002/(SICI)1098-2418(200005)16:3<277::AID-RSA4>3.0.CO;2-1; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; FORSTER J, 2000, 13 COLT, P90; Freund Y, 1999, MACH LEARN, V37, P277, DOI 10.1023/A:1007662407062; GENTILE C, 1999, 12 COLT, P1; Grove AJ, 2001, MACH LEARN, V43, P173, DOI 10.1023/A:1010844028087; HELMBOLD DP, 1995, J COMPUT SYST SCI, V50, P551, DOI 10.1006/jcss.1995.1044; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; LITTLESTONE N, 1989, 2ND P ANN WORKSH COM, P269; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582; SAUNDERS C, 1998, 15 ICML; SHAWETAYLOR J, 2000, 2000082 NEUROCOLT2; Vapnik V.N, 1998, STAT LEARNING THEORY; VOVK V, 1998, NIPS 10; WILLIAMSON RC, IN PRESS IEEE T IT	23	11	11	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						359	366						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100045
C	Even-Dar, E; Mansour, Y		Dietterich, TG; Becker, S; Ghahramani, Z		Even-Dar, E; Mansour, Y			Convergence of optimistic and incremental Q-learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				STOCHASTIC-APPROXIMATION	We show the convergence of two deterministic variants of Q-learning. The first is the widely used optimistic Q-learning, which initializes the Q-values to large initial values and then follows a greedy policy with respect to the Q-values. We show that setting the initial value sufficiently large guarantees the converges to an epsilon-optimal policy. The second is a new and novel algorithm incremental Q-learning, which gradually promotes the values of actions that are not taken. We show that incremental Q-learning converges, in the limit, to the optimal policy. Our incremental Q-learning algorithm can be viewed as derandomization of the epsilon-greedy Q-learning.	Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel	Tel Aviv University	Even-Dar, E (corresponding author), Tel Aviv Univ, Sch Comp Sci, IL-69978 Tel Aviv, Israel.							Brafman R., 2001, IJCAI; EVENDAR E, 2001, COLT; Gittins J. C., 1974, PROGR STAT, P241; JAKKOLA T, 1994, NEURAL COMPUTATION, V6; KEARNS M, 1998, ICML; LITTMAN M, 1996, ICML; Puterman M. L., 1994, MARKOV DECISION PROC, DOI DOI 10.1002/9780470316887; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1023/A:1022689125041; Watkins C.J.C.H., 1989, LEARNING DELAYED REW; WATKINS CJC, 1992, MACH LEARN, V3, P279	13	11	11	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1499	1506						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100186
C	Morimoto, J; Doya, K		Leen, TK; Dietterich, TG; Tresp, V		Morimoto, J; Doya, K			Robust reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					This paper proposes a new reinforcement learning (RL) paradigm that explicitly takes into account input disturbance as well as modeling errors. The use of environmental models in RL is quite popular for both off-line learning by simulations and for on-line action planning. However, the difference between the model and the real environment can lead to unpredictable, often unwanted results. Based on the theory of H-infinity control, we consider a differential game in which a 'disturbing' agent (disturber) tries to make the worst possible disturbance while a 'control' agent (actor) tries to make the best control input. The problem is formulated as finding a minmax solution of a value function that takes into account the norm of the output deviation and the norm of the disturbance. We derive on-line learning algorithms for estimating the value function and for calculating the worst disturbance and the best control in reference to the value function. We tested the paradigm, which we call "Robust Reinforcement Learning (RRL)," in the task of inverted pendulum. In the linear domain, the policy and the value function learned by the on-line algorithms coincided with those derived analytically by the linear H-infinity theory. For a fully nonlinear swing-up task, the control by RRL achieved robust performance against changes in the pendulum weight and friction while a standard RL control could not deal with such environmental changes.	Nara Inst Sci & Technol, Grad Sch Informat Sci, Nara, Japan	Nara Institute of Science & Technology	Morimoto, J (corresponding author), Nara Inst Sci & Technol, Grad Sch Informat Sci, Nara, Japan.	xmorimo@erato.atr.co.jp; doya@isd.atr.co.jp						[Anonymous], ADV NEURAL INFORM PR; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Doya K, 2000, NEURAL COMPUT, V12, P219, DOI 10.1162/089976600300015961; MORIMOTO J, 2000, P INT C MACH LEARN, P623; WEILAND S, 1989, P WORKSH RICC EQ CON, P156; Zhou K., 1996, ROBUST OPTIMAL CONTR	6	11	11	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1061	1067						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800149
C	Saul, LK; Allen, JB		Leen, TK; Dietterich, TG; Tresp, V		Saul, LK; Allen, JB			Periodic component analysis: An eigenvalue method for representing periodic structure in speech	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				SEPARATION	An eigenvalue method is developed for analyzing periodic structure in speech. Signals are analyzed by a matrix diagonalization reminiscent of methods for principal component analysis (PCA) and independent component analysis (ICA). Our method-called periodic component analysis (pi CA) - uses constructive interference to enhance periodic components of the frequency spectrum and destructive interference to cancel noise. The front end emulates important aspects of auditory processing, such as cochlear filtering, nonlinear compression, and insensitivity to phase, with the aim of approaching the robustness of human listeners. The method avoids the inefficiencies of autocorrelation at the pitch period: it does not require long delay lines, and it correlates signals at a clock rate on the order of the actual pitch, as opposed to the original sampling rate. We derive its cost function and present some experimental results.	AT&T Labs Res, Florham Pk, NJ 07932 USA	AT&T	Saul, LK (corresponding author), AT&T Labs Res, 180 Pk Ave, Florham Pk, NJ 07932 USA.							Bregman A.S., 1994, AUDITORY SCENE ANAL; BROKX JPL, 1982, J PHONETICS, V10, P23, DOI 10.1016/S0095-4470(19)30909-X; HALL JW, 1984, J ACOUST SOC AM, V76, P50, DOI 10.1121/1.391005; Hartmann W. M., 1997, SIGNALS SOUND SENSAT; Hess W., 1983, PITCH DETERMINATION, DOI [10.1007/978-3-642-81926-1, DOI 10.1007/978-3-642-81926-1]; MILLER GA, 1955, J ACOUST SOC AM, V27, P338, DOI 10.1121/1.1907526; MOLGEDEY L, 1994, PHYS REV LETT, V72, P3634, DOI 10.1103/PhysRevLett.72.3634; ROWEIS S, 2000, ADV NEURAL INFORMATI, V13; SLANEY M, 1990, P IEEE INT C AC SPEE, V1, P357; Stevens Kenneth N., 1999, ACOUSTIC PHONETICS; Talkin D., 1995, SPEECH CODING SYNTHE, P497	11	11	12	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						807	813						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800114
C	Schneidman, E; Brenner, N; Tishby, N; van Steveninck, RRD; Bialek, W		Leen, TK; Dietterich, TG; Tresp, V		Schneidman, E; Brenner, N; Tishby, N; van Steveninck, RRD; Bialek, W			Universality and individuality in a neural code	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The problem of neural coding is to understand how sequences of action potentials (spikes) are related to sensory stimuli, motor outputs, or (ultimately) thoughts and intentions. One clear question is whether the same coding rules are used by different neurons, or by corresponding neurons in different individuals. We present a quantitative formulation of this problem using ideas from information theory, and apply this approach to the analysis of experiments in the fly visual system. We find significant individual differences in the structure of the code, particularly in the way that temporal patterns of spikes are used to convey information beyond that available from variations in spike rate. On the other hand, all the flies in our ensemble exhibit a high coding efficiency, so that every spike carries the same amount of information in all the individuals. Thus the neural code has a quantifiable mixture of individuality and universality.	Ctr Neural Computat, Sch Comp Sci, Jerusalem, Israel		Schneidman, E (corresponding author), Ctr Neural Computat, Sch Comp Sci, Jerusalem, Israel.	elads@cs.huji.ac.il; naama@research.nj.nec.com; tishby@cs.huji.ac.il; ruyter@research.nj.nec.com; bialek@research.nj.nec.com						Brenner N, 2000, NEURAL COMPUT, V12, P1531, DOI 10.1162/089976600300015259; Bullock T., 1965, STRUCTURE FUNCTION N; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; ELYANIV R, 1997, NIPS, V10, P465; Hausen K., 1984, PHOTORECEPTION VISIO, P523, DOI [10.1007/978-1-4613-2743-1_15, DOI 10.1007/978-1-4613-2743-1_15]; LAND MF, 1974, J COMP PHYSIOL, V89, P331, DOI 10.1007/BF00695351; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Rieke F., 1997, SPIKES EXPLORING NEU; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; vanSteveninck RRD, 1997, SCIENCE, V275, P1805, DOI 10.1126/science.275.5307.1805; Victor JD, 1996, J NEUROPHYSIOL, V76, P1310, DOI 10.1152/jn.1996.76.2.1310	12	11	11	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						159	165						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800023
C	Tanaka, T		Leen, TK; Dietterich, TG; Tresp, V		Tanaka, T			Analysis of bit error probability of direct-sequence CDMA multiuser demodulators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				SPIN	We analyze the bit error probability of multiuser demodulators for direct-sequence binary phase-shift-keying (DS/BPSK) CDMA channel with additive gaussian noise. The problem of multiuser demodulation is cast into the finite-temperature decoding problem, and replica analysis is applied to evaluate the performance of the resulting MPM (Marginal Posterior Mode) demodulators, which include the optimal demodulator and the MAP demodulator as special cases. An approximate implementation of demodulators is proposed using analog-valued Hopfield model as a naive mean-field approximation to the MPM demodulators, and its performance is also evaluated by the replica analysis. Results of the performance evaluation shows effectiveness of the optimal demodulator and the mean-field demodulator compared with the conventional one, especially in the cases of small information bit rate and low noise level.	Tokyo Metropolitan Univ, Dept Elect & Informat Engn, Hachioji, Tokyo 1920397, Japan	Tokyo Metropolitan University	Tanaka, T (corresponding author), Tokyo Metropolitan Univ, Dept Elect & Informat Engn, Hachioji, Tokyo 1920397, Japan.		Tanaka, Toshiyuki/C-2749-2011					BRAY AJ, 1986, J PHYS C SOLID STATE, V19, P6389, DOI 10.1088/0022-3719/19/32/014; DEALMEIDA JRL, 1978, J PHYS A-MATH GEN, V11, P983, DOI 10.1088/0305-4470/11/5/028; Fischer KH., 1991, SPIN GLASSES, DOI [10.1017/CBO9780511628771, DOI 10.1017/CBO9780511628771]; Iba Y, 1999, J PHYS A-MATH GEN, V32, P3875, DOI 10.1088/0305-4470/32/21/302; Kabashima Y, 1999, ADV NEUR IN, V11, P246; Kechriotis GI, 1996, IEEE T NEURAL NETWOR, V7, P131, DOI 10.1109/72.478397; Simon M. K., 1994, SPREAD SPECTRUM COMM; THOULESS DJ, 1977, PHILOS MAG, V35, P593, DOI 10.1080/14786437708235992; VITERBI AJ, 1995, CDMA PRINCIPLES SPRE; WINKLER G., 1995, IMAGE ANAL RANDOM FI	10	11	11	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						315	321						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800045
C	Zhang, T		Leen, TK; Dietterich, TG; Tresp, V		Zhang, T			Convergence of large margin separable linear classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PERCEPTRON ALGORITHM; NEURAL NETWORKS	Large margin linear classification methods have been successfully applied to many applications. For a linearly separable problem, it is known that under appropriate assumptions, the expected misclassification error of the computed "optimal hyperplane" approaches zero at a rate proportional to the inverse training sample size. This rate is usually characterized by the margin and the maximum norm of the input data. In this paper, we argue that another quantity, namely the robustness of the input data distribution, also plays an important role in characterizing the convergence behavior of expected misclassification error. Based on this concept of robustness, we show that for a large margin separable linear classification problem, the expected misclassification error may converge exponentially in the number of training sample size.	IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Zhang, T (corresponding author), IBM Corp, Thomas J Watson Res Ctr, Dept Math Sci, Yorktown Hts, NY 10598 USA.							ANLAUF JK, 1989, EUROPHYS LETT, V10, P687, DOI 10.1209/0295-5075/10/7/014; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cristianini N., 2000, INTRO SUPPORT VECTOR; Heuser HG., 1982, FUNCTIONAL ANAL; KINZEL W, 1990, LECT NOTES PHYS, V368, P175; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; OPPER M, 1989, EUROPHYS LETT, V8, P389, DOI 10.1209/0295-5075/8/4/015; OPPER M, 1988, PHYS REV A, V38, P3824, DOI 10.1103/PhysRevA.38.3824; Rockafellar R. T., 1970, CONVEX ANAL; Schuurmans D, 1997, J COMPUT SYST SCI, V55, P140, DOI 10.1006/jcss.1997.1505; Vapnik V.N, 1998, STAT LEARNING THEORY; WILLIAMSON RC, 2000, COLT 00, P309; Yurinsky, 1995, SUMS GAUSSIAN VECTOR; ZHANG T, 1999, NIPS 99, P370	14	11	11	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						357	363						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800051
C	Duffy, N; Helmbold, D		Solla, SA; Leen, TK; Muller, KR		Duffy, N; Helmbold, D			Potential boosters?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				ALGORITHMS	Recent interpretations of the Adaboost algorithm view it as performing a gradient descent on a potential function. Simply changing the potential function allows one to create new algorithms related to AdaBoost. However, these new algorithms are generally not known to have the formal boosting property. This paper examines the question of which potential functions lead to new algorithms that are boosters. The two main results are general sets of conditions on the potential; one set implies that the resulting algorithm is a booster, while the other implies that the algorithm is not. These conditions are applied to previously studied potential functions, such as those used by LogitBoost and Doom II.	Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA	University of California System; University of California Santa Cruz	Duffy, N (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95064 USA.	nigeduff@cse.ucsc.edu; dph@cse.ucsc.edu						Bauer E, 1999, MACH LEARN, V36, P105, DOI 10.1023/A:1007515423169; Breiman L., 1997, 486 U CAL BERK STAT; DIETTERICH TG, IN PRESS MACHINE LEA; Duffy N, 1999, LECT NOTES ARTIF INT, V1572, P18; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P102, DOI 10.1145/307400.307419; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 1998, ADDITIVE LOGISTIC RE; HAUSSLER D, 1991, INFORM COMPUT, V95, P129, DOI 10.1016/0890-5401(91)90042-Z; KEARNS M, 1994, J ACM, V41, P67, DOI 10.1145/174644.174647; Kivinen J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P134, DOI 10.1145/307400.307424; LAFFERTY J, P 12 ANN C COMP LEAR, P125; MASON L, 2000, IN PRESS NIPS; RATSCH G, 2000, IN PRESS MACHINE LEA; Schapire RE, 1998, ANN STAT, V26, P1651; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Schapire RE, 1992, THESIS; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972	18	11	13	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						258	264						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700037
C	Renart, A; Parga, N; Rolls, ET		Solla, SA; Leen, TK; Muller, KR		Renart, A; Parga, N; Rolls, ET			A recurrent model of the interaction between Prefrontal and Inferotemporal cortex in delay tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				INFERIOR TEMPORAL CORTEX; SHORT-TERM-MEMORY; WORKING-MEMORY; VISUAL-SEARCH; NEURONS; MECHANISMS; RESPONSES	A very simple model of two reciprocally connected attractor neural networks is studied analytically in situations similar to those encountered in delay match-to-sample tasks with intervening stimuli and in tasks of memory guided attention. The model qualitatively reproduces many of the experimental data on these types of tasks and provides a framework for the understanding of the experimental observations in the context of the attractor neural network scenario.	Univ Autonoma Madrid, Dept Fis Teor, E-28049 Madrid, Spain	Autonomous University of Madrid; Consejo Superior de Investigaciones Cientificas (CSIC); CSIC - UAM - Institut de Fisica Teorica (IFT)	Renart, A (corresponding author), Univ Autonoma Madrid, Dept Fis Teor, E-28049 Madrid, Spain.			Rolls, Edmund/0000-0003-3025-1292				AMIT DJ, 1991, NETWORK-COMP NEURAL, V2, P259, DOI 10.1088/0954-898X/2/3/003; AMIT DJ, 1995, BEHAV BRAIN SCI, V18, P617, DOI 10.1017/S0140525X00040164; BAYLIS GC, 1987, EXP BRAIN RES, V65, P614; CHELAZZI L, 1993, NATURE, V363, P345, DOI 10.1038/363345a0; Chelazzi L, 1998, J NEUROPHYSIOL, V80, P2918, DOI 10.1152/jn.1998.80.6.2918; Durstewitz D, 1999, J NEUROSCI, V19, P2807; Fuster JM, 1995, MEMORY CEREBRAL CORT; KUHN R, 1990, LECT NOTES PHYS, V368, P19; MEZARD M, 1987, SPIN GLASS THEORY BE; MILLER EK, 1993, J NEUROSCI, V13, P1460; MILLER EK, 1994, SCIENCE, V263, P520, DOI 10.1126/science.8290960; Miller EK, 1996, J NEUROSCI, V16, P5154; Renart A, 1999, NETWORK-COMP NEURAL, V10, P237, DOI 10.1088/0954-898X/10/3/303; Renart A, 1999, NEURAL COMPUT, V11, P1349, DOI 10.1162/089976699300016278; Tomita H, 1999, NATURE, V401, P699, DOI 10.1038/44372	15	11	12	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						171	177						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700025
C	Watanabe, S		Solla, SA; Leen, TK; Muller, KR		Watanabe, S			Algebraic analysis for non-regular learning machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				B-FUNCTIONS	Hierarchical learning machines are non-regular and non-identifiable statistical models, whose true parameter sets are analytic sets with singularities. Using algebraic analysis, we rigorously prove that the stochastic complexity of a non-identifiable learning machine is asymptotically equal to lambda (1) log n - (m(1) - 1) log log n + const., where n is the number of training samples. Moreover we show that the rational number XI and the integer mi can be algorithmically calculated using resolution of singularities in algebraic geometry. Also we obtain inequalities 0 < <lambda>(1) less than or equal to d/2 and 1 less than or equal to m(1) less than or equal to d, where d is the number of parameters.	Tokyo Inst Technol, Precis & Intelligence Lab, Midori Ku, Yokohama, Kanagawa 223, Japan	Tokyo Institute of Technology	Watanabe, S (corresponding author), Tokyo Inst Technol, Precis & Intelligence Lab, Midori Ku, 4259 Nagatsuta, Yokohama, Kanagawa 223, Japan.	swatanab@pi.titech.ac.jp	Watanabe, Sumio/C-3880-2015; Watanabe, Sumio/M-7370-2019	Watanabe, Sumio/0000-0001-8341-5639; 				AMARI S, 1993, NEURAL COMPUT, V5, P140, DOI 10.1162/neco.1993.5.1.140; ATIYAH MF, 1970, COMMUNICATIONS PURE, V13, P145; Fukumizu K, 1999, LECT NOTES ARTIF INT, V1720, P51; HAGIWARA K, 1993, P INT JOINT C NEUR N, V3, P2263; HIRONAKA H, 1964, ANN MATH, V79, P109, DOI 10.2307/1970486; KASHIWARA M, 1976, INVENT MATH, V38, P33, DOI 10.1007/BF01390168; Oaku T, 1997, DUKE MATH J, V87, P115, DOI 10.1215/S0012-7094-97-08705-6; SATO M, 1974, ANN MATH, V100, P131, DOI 10.2307/1970844; Watanabe S, 1999, LECT NOTES ARTIF INT, V1720, P39; WATANABE S, 1998, IEICE T A, V81, P1442	10	11	11	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						356	362						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700051
C	Brand, M		Kearns, MS; Solla, SA; Cohn, DA		Brand, M			An entropic estimator for structure discovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We introduce a novel framework for simultaneous structure and parameter learning in hidden-variable conditional probability models, based on an entropic prior and a solution for its maximum a postpriori (MAP) estimator. The MAP estimate minimizes uncertainty in all respects: cross-entropy between model and data; entropy of the model; entropy of the data's descriptive statistics. Iterative estimation extinguishes weakly supported parameters, compressing and sparsifying the model. Trimming operators accelerate this process by removing excess parameters and, unlike most pruning schemes, guarantee an increase in posterior probability. Entropic estimation takes a overcomplete random model and simplifies it, inducing the structure of relations between hidden and observed variables. Applied to hidden Markov models (HMMs), it finds a concise finite-state machine representing the hidden structure of a signal. We entropically model music, handwriting, and video time-series, and show that the resulting models are highly concise, structured, predictive, and interpretable: Surviving states tend to be highly correlated with meaningful partitions of the data, while surviving transitions provide a low-perplexity model of the signal dynamics.	Mitsubishi Elect Res Labs, Cambridge, MA 02139 USA		Brand, M (corresponding author), Mitsubishi Elect Res Labs, 201 Broadway, Cambridge, MA 02139 USA.							BRAND M, 1998, IN PRESS P ARTIFICIA; BRAND M, 1997, IN PRESS NEURAL COMP; Merz C, 1998, UCI REPOSITORY MACHI; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; REYNOLDS D, 1992, HANDWRITTEN DIGIT DA; RISSAMEN J, 1989, STOCHASTIC COMPLEXIT; STOLCKE A, 1994, TR94003 INT COMP SCI; TAKAMI JI, 1991, SP9188 TR IEICE; VOVK VG, 1995, P COMPUTATIONAL LEAR, P237; WOLFERTSTETTER F, 1995, INT C AC SPEECH SIGN, V1, P544	10	11	11	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						723	729						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700102
C	Hofmann, R; Tresp, V		Jordan, MI; Kearns, MJ; Solla, SA		Hofmann, R; Tresp, V			Nonlinear Markov networks for continuous variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We address the problem of learning structure in nonlinear Markov networks with continuous variables. This can be viewed as non-Gaussian multidimensional density estimation exploiting certain conditional independencies in the variables. Markov networks are a graphical way of describing conditional independencies well suited to model relationships which do not exhibit a natural causal ordering. We use neural network structures to model the quantitative relationships between variables. The main focus in this paper will be on learning the structure for the purpose of gaining insight into the underlying process. Using two data sets we show that interesting structures can be found using our approach. Inference will be briefly addressed.	Siemens AG, Corp Technol Informat & Commun, D-81730 Munchen, Germany	Siemens AG; Siemens Germany	Hofmann, R (corresponding author), Siemens AG, Corp Technol Informat & Commun, D-81730 Munchen, Germany.								0	11	11	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						521	527						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700074
C	Lin, JK		Jordan, MI; Kearns, MJ; Solla, SA		Lin, JK			Factorizing multivariate function classes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					The mathematical framework for factorizing equivalence classes of multivariate functions is formulated in this paper. Independent component analysis is shown to be a special case of this decomposition. Using only the local geometric structure of a class representative, we derive an analytic solution for the factorization. We demonstrate the factorization solution with numerical experiments and present a preliminary tie to decorrelation.	MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Lin, JK (corresponding author), MIT, E25-201, Cambridge, MA 02139 USA.								0	11	11	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						563	569						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700080
C	Marrs, AD		Jordan, MI; Kearns, MJ; Solla, SA		Marrs, AD			An application of Reversible-Jump MCMC to multivariate spherical Gaussian mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Applications of Gaussian mixture models occur frequently in the fields of statistics and artificial neural networks. One of the key issues arising from any mixture model application is how to estimate the optimum number of mixture components. This paper extends the Reversible-Jump Markov Chain Monte Carlo (MCMC) algorithm to the case of multivariate spherical Gaussian mixtures using a hierarchical prior model. Using this method the number of mixture components is no longer fixed but becomes a parameter of the model which we shall estimate. The Reversible-Jump MCMC algorithm is capable of moving between parameter subspaces which correspond to models with different numbers of mixture components. As a result a sample from the full joint distribution of all unknown model parameters is generated. The technique is then demonstrated on a simulated example and a well known vowel dataset.	Def Evaluat & Res Agcy, Signal & Informat Proc Dept, Gt Malvern WR14 3PS, England		Marrs, AD (corresponding author), Def Evaluat & Res Agcy, Signal & Informat Proc Dept, Gt Malvern WR14 3PS, England.								0	11	13	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						577	583						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700082
C	Schaal, S; Vijayakumar, S; Atkeson, CG		Jordan, MI; Kearns, MJ; Solla, SA		Schaal, S; Vijayakumar, S; Atkeson, CG			Local dimensionality reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					If globally high dimensional data has locally only low dimensional distributions, it is advantageous to perform a local dimensionality reduction before further processing the data. In this paper we examine several techniques for local dimensionality reduction in the context of locally weighted linear regression. As possible candidates, we derive local versions of factor analysis regression, principle component regression, principle component regression on joint distributions, and partial least squares regression. After outlining the statistical bases of these methods, we perform Monte Carlo simulations to evaluate their robustness with respect to violations of their statistical assumptions. One surprising outcome is that locally weighted partial least squares regression offers the best average results, thus outperforming even factor analysis, the theoretically most appealing of our candidate techniques.	ERATO, Kawato Dynam Brain Project, JST, Kyoto 61902, Japan		Schaal, S (corresponding author), ERATO, Kawato Dynam Brain Project, JST, 2-2 Hikaridai, Kyoto 61902, Japan.								0	11	11	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						633	639						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700090
C	Tresp, V; Briegel, T		Jordan, MI; Kearns, MJ; Solla, SA		Tresp, V; Briegel, T			A solution for missing data in recurrent neural networks with an application to blood glucose prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We consider neural network models for stochastic nonlinear dynamical systems where measurements of the variable of interest are only available at irregular intervals i.e. most realizations are missing. Difficulties arise since the solutions for prediction and maximum likelihood learning with missing data lead to complex integrals, which even for simple cases cannot be solved analytically. In this paper we propose a specific combination of a nonlinear recurrent neural predictive model and a linear error model which leads to tractable prediction and maximum likelihood adaptation rules. In particular, the recurrent neural network can be trained using the real-time recurrent learning rule and the linear error model can be trained by an EM adaptation rule, implemented using forward-backward Kalman filter equations. The model is applied to predict the glucose/insulin metabolism of a diabetic patient where blood glucose measurements are only available a few times a day at irregular intervals. The new model shows considerable improvement with respect to both recurrent neural networks trained with teacher forcing or in a free running mode and various linear models.	Siemens AG, Corp Technol, D-81730 Munich, Germany	Siemens AG; Siemens Germany	Tresp, V (corresponding author), Siemens AG, Corp Technol, Otto Hahn Ring 6, D-81730 Munich, Germany.								0	11	12	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						971	977						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700137
C	Bos, S; Opper, M		Mozer, MC; Jordan, MI; Petsche, T		Bos, S; Opper, M			Dynamics of training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				A new method to calculate the full training process of a neural network is introduced. No sophisticated methods like the replica trick are used. The results are directly related to the actual number of training steps. Some results are presented here, like the maximal learning rate, an exact description of early stopping, and the necessary number of training steps. Further problems can be addressed with this approach.			Bos, S (corresponding author), RIKEN,LAB INFORMAT REPRESENTAT,HIROSAWA 2-1,WAKO,SAITAMA 35101,JAPAN.								0	11	11	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						141	147						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00020
C	Platt, JC; Matic, NP		Mozer, MC; Jordan, MI; Petsche, T		Platt, JC; Matic, NP			A constructive RBF network for writer adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper discusses a fairly general adaptation algorithm which augments a standard neural network to increase its recognition accuracy for a specific user. The basis for the algorithm is that the output of a neural network is characteristic of the input, even when the output is incorrect. We exploit this characteristic output by using an Output Adaptation Module (OAM) which maps this output into the correct user-dependent confidence vector. The OAM is a simplified Resource Allocating Network which constructs radial basis functions on-line. We applied the OAM to construct a writer-adaptive character recognition system for on-line hand-printed characters. The OAM decreases the word error rate on a test set by an average of 45%, while creating only 3 to 25 basis functions for each writer in the test set.			Platt, JC (corresponding author), SYNAPT INC,2698 ORCHARD PKWY,SAN JOSE,CA 95134, USA.		Platt, John/GOH-2678-2022	Platt, John/0000-0002-5652-5303					0	11	11	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						765	771						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00108
C	Schneider, JG		Mozer, MC; Jordan, MI; Petsche, T		Schneider, JG			Exploiting model uncertainty estimates for safe dynamic control learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Model learning combined with dynamic programming has been shown to be effective for learning control of continuous state dynamic systems. The simplest method assumes the learned model is correct and applies dynamic programming to it, but many approximators provide uncertainty estimates on the fit. How can they be exploited? This paper addresses the case where the system must be prevented from having catastrophic failures during learning. We propose a new algorithm adapted from the dual control literature and use Bayesian locally weighted regression models with dynamic programming. A common reinforcement learning assumption is that aggressive exploration should be encouraged. This paper addresses the converse case in which the system has to reign in exploration. The algorithm is illustrated on a 4 dimensional simulated control problem.			Schneider, JG (corresponding author), CARNEGIE MELLON UNIV,INST ROBOT,PITTSBURGH,PA 15213, USA.								0	11	11	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1047	1053						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00147
C	Baxter, J		Touretzky, DS; Mozer, MC; Hasselmo, ME		Baxter, J			Learning model bias	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV LONDON ROYAL HOLLOWAY & BEDFORD NEW COLL,DEPT COMP SCI,LONDON NW1 4NS,ENGLAND	University of London; Royal Holloway University London									0	11	12	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						169	175						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00024
C	Blatt, M; Wiseman, S; Domany, E		Touretzky, DS; Mozer, MC; Hasselmo, ME		Blatt, M; Wiseman, S; Domany, E			Clustering data through an analogy to the Potts model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						WEIZMANN INST SCI,DEPT PHYS COMPLEX SYS,IL-76100 REHOVOT,ISRAEL	Weizmann Institute of Science			DOMANY, EYTAN/K-1560-2012						0	11	11	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						416	422						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00059
C	Schaal, S; Atkeson, CC		Touretzky, DS; Mozer, MC; Hasselmo, ME		Schaal, S; Atkeson, CC			From isolation to cooperation: An alternative view of a system of experts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						GEORGIA TECH RES INST,COLL COMP,ATLANTA,GA 30332	University System of Georgia; Georgia Institute of Technology									0	11	12	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						605	611						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00086
C	Softky, WR		Touretzky, DS; Mozer, MC; Hasselmo, ME		Softky, WR			Unsupervised pixel-prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						NIDDK,MATH RES BRANCH,NIH,BETHESDA,MD 20814	National Institutes of Health (NIH) - USA; NIH National Institute of Diabetes & Digestive & Kidney Diseases (NIDDK)									0	11	11	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						809	815						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00114
C	Wang, X; Jagota, A; Botelho, F; Garzon, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Wang, X; Jagota, A; Botelho, F; Garzon, M			Absence of cycles in symmetric neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF LOS ANGELES, DEPT COMP SCI, LOS ANGELES, CA 90024 USA	University of California System; University of California Los Angeles									0	11	11	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						372	378						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00053
C	NOWLAN, SJ; HINTON, GE		MOODY, JE; HANSON, SJ; LIPPMANN, RP		NOWLAN, SJ; HINTON, GE			ADAPTIVE SOFT WEIGHT TYING USING GAUSSIAN MIXTURES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	11	11	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						993	1000						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00122
C	Ablin, P; Moreau, T; Massias, M; Gramfort, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ablin, Pierre; Moreau, Thomas; Massias, Mathurin; Gramfort, Alexandre			Learning step sizes for unfolded sparse coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				THRESHOLDING ALGORITHM; CONVERGENCE; SHRINKAGE	Sparse coding is typically solved by iterative optimization techniques, such as the Iterative Shrinkage-Thresholding Algorithm (ISTA). Unfolding and learning weights of ISTA using neural networks is a practical way to accelerate estimation. In this paper, we study the selection of adapted step sizes for ISTA. We show that a simple step size strategy can improve the convergence rate of ISTA by leveraging the sparsity of the iterates. However, it is impractical in most large-scale applications. Therefore, we propose a network architecture where only the step sizes of ISTA are learned. We demonstrate that for a large class of unfolded algorithms, if the algorithm converges to the solution of the Lasso, its last layers correspond to ISTA with learned step sizes. Experiments show that our method is competitive with state-of-the-art networks when the solutions are sparse enough.	[Ablin, Pierre; Moreau, Thomas; Massias, Mathurin; Gramfort, Alexandre] Univ Paris Saclay, INRIA, CEA, St Aubin, France	CEA; Inria; UDICE-French Research Universities; Universite Paris Saclay	Ablin, P (corresponding author), Univ Paris Saclay, INRIA, CEA, St Aubin, France.	pierre.ablin@inria.fr; thomas.moreau@inria.fr; mathurin.massias@inria.fr; alexandre.gramfort@inria.fr		Moreau, Thomas/0000-0002-1523-3419	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program [676943]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program(European Research Council (ERC))	We would like to thank the anonymous reviewers for their insightful comments which have improved the quality of the paper. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (Grant agreement No. 676943)	Adler J., 2017, ARXIV171010898; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Borgerding M, 2017, IEEE T SIGNAL PROCES, V65, P4293, DOI 10.1109/TSP.2017.2708040; Chen Xiaohan, 2018, ADV NEURAL INFORM PR, P9061; Cho D, 2018, KDI EWC SER ECON POL, P1; Combettes Patrick L, 2011, CONVEX ANAL MONOTONE, DOI [10.1017/CB09781107415324.004., DOI 10.1017/CB09781107415324.004]; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Giryes R, 2018, IEEE T SIGNAL PROCES, V66, P1676, DOI 10.1109/TSP.2018.2791945; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Hale ET, 2008, SIAM J OPTIMIZ, V19, P1107, DOI 10.1137/070698920; Hastie T, 2015, CHAPMAN HALLCRC MONO, DOI [10.1201/b18401, DOI 10.1201/B18401]; Hershey J. R., 2014, ARXIV14092574; Johnson TB, 2015, PR MACH LEARN RES, V37, P1171; Liang Jingwei, ADV NEURAL INFORM PR, P1970; Liu J., 2019, P INT C LEARN REPR; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; Massias M, 2018, PR MACH LEARN RES, V80; Moreau Thomas, 2017, INT C LEARN REPR ICL; Ndiaye E, 2017, J MACH LEARN RES, V18; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Olshausen Bruno A., 1997, SPARSE CODING INCOMP; Paszke Adam, 2017, NIPS AUT WORKSH, DOI DOI 10.1017/CBO9781107707221.009; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Poon Clarice, 2017, INT C MACH LEARN ICM; Rosset S, 2004, J MACH LEARN RES, V5, P941; Sprechmann P., 2012, P 29 INT C MACH LEAR, P615; Sprechmann P., 2013, ADV NEURAL INF PROCE, V26, P908; Sun YF, 2019, PR MACH LEARN RES, V89; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Wang Zhangyang, 2015, AAAI C ART INT, P2194; Xin B, 2016, ADV NEUR IN, V29; Yang Yan, 2017, ADV NEURAL INFORM PR, P10; ZANGWILL WI, 1969, MANAGE SCI, V16, P1, DOI 10.1287/mnsc.16.1.1; Zhang J, 2018, PROC CVPR IEEE, P1828, DOI 10.1109/CVPR.2018.00196	41	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904072
C	Altschuler, J; Bach, F; Rudi, A; Niles-Weed, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Altschuler, Jason; Bach, Francis; Rudi, Alessandro; Niles-Weed, Jonathan			Massively Scalable Sinkhorn Distances via the Nystrom Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EARTH MOVERS DISTANCE; TRANSPORT	The Sinkhorn "distance," a variant of the Wasserstein distance with entropic regularization, is an increasingly popular tool in machine learning and statistical inference. However, the time and memory requirements of standard algorithms for computing this distance grow quadratically with the size of the data, making them prohibitively expensive on massive data sets. In this work, we show that this challenge is surprisingly easy to circumvent: combining two simple techniques-the Nystrom method and Sinkhorn scaling-provably yields an accurate approximation of the Sinkhorn distance with significantly lower time and memory requirements than other approaches. We prove our results via new, explicit analyses of the Nystrom method and of the stability properties of Sinkhorn scaling. We validate our claims experimentally by showing that our approach easily computes Sinkhorn distances on data sets hundreds of times larger than can be handled by other techniques.	[Altschuler, Jason] MIT, Cambridge, MA 02139 USA; [Bach, Francis; Rudi, Alessandro] INRIA, ENS, PSL, Paris, France; [Niles-Weed, Jonathan] NYU, New York, NY 10003 USA	Massachusetts Institute of Technology (MIT); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); New York University	Altschuler, J (corresponding author), MIT, Cambridge, MA 02139 USA.	jasonalt@mit.edu; francis.bach@inria.fr; alessandro.rudi@inria.fr; jnw@cims.nyu.edu			NSF Graduate Research Fellowship [1122374]; European Research Council [SEQUOIA 724063]; Josephine de Karman Fellowship	NSF Graduate Research Fellowship(National Science Foundation (NSF)); European Research Council(European Research Council (ERC)European Commission); Josephine de Karman Fellowship	We thank the reviewers for their helpful comments. We also thank Piotr Indyk, Pablo Parrilo, and Philippe Rigollet for helpful discussions. JA was supported in part by NSF Graduate Research Fellowship 1122374. FB and AR were supported in part by the European Research Council (grant SEQUOIA 724063). JNW was supported in part by the Josephine de Karman Fellowship.	Alaoui A., 2015, P 28 INT C NEURAL IN, P775; ALLENZHU Z, 2017, FOCS, P890, DOI DOI 10.1109/FOCS.2017.87; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Altschuler Jason, 2018, ARXIV181010046; Bach Francis, 2013, C LEARNING THEORY, P185; Belkin M, 2018, ARXIV180103437; Blanchet J., 2018, ARXIV181007717; Bottou L., 2017, ARXIV170107875STATML; Bousquet O., 2017, TECH REP; COHEN MB, 2017, FOCS, P902, DOI DOI 10.1109/FOCS.2017.88; Courty Nicolas, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P274, DOI 10.1007/978-3-662-44848-9_18; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dvurechensky P., 2018, ARXIV180204367; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Forrow A, 2018, ARXIV180607348; FRANKLIN J, 1989, LINEAR ALGEBRA APPL, V114, P717, DOI 10.1016/0024-3795(89)90490-4; Friedman J., 2001, SPRINGER SERIES STAT, V1; Fuselier E, 2012, SIAM J NUMER ANAL, V50, P1753, DOI 10.1137/110821846; Garnett R., 2016, ADV NEURAL INFORM PR, V29, P3718; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Genevay A, 2018, PR MACH LEARN RES, V84; Gittens A., 2011, ARXIV11105305; Hangelbroek T, 2010, SIAM J MATH ANAL, V42, P1732, DOI 10.1137/090769570; Kalantari B, 2008, MATH PROGRAM, V112, P371, DOI 10.1007/s10107-006-0021-4; Kolouri S, 2017, IEEE SIGNAL PROC MAG, V34, P43, DOI 10.1109/MSP.2017.2695801; Li PH, 2013, IEEE I CONF COMP VIS, P1689, DOI 10.1109/ICCV.2013.212; Linial N., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P644, DOI 10.1145/276698.276880; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Musco C., 2017, ADV NEURAL INFORM PR, P3833; Peyre Gabriel, 2017, TECHNICAL REPORT; Quanrud K., 2019, SOSA; Rieger C, 2010, ADV COMPUT MATH, V32, P103, DOI 10.1007/s10444-008-9089-0; Rigollet P, 2018, COMPTES RENDUS MATH; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Schiebinger G, 2019, CELL, V176, DOI 10.1016/j.cell.2019.02.026; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; SINKHORN R, 1967, AM MATH MON, V74, P402, DOI 10.2307/2314570; Smola A. J, 2000, P ICML; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Tenetov E, 2018, SIAM J SCI COMPUT, V40, pA3400, DOI 10.1137/17M1162925; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wendland H., 2004, SCATTERED DATA APPRO, V17; Williams C., 2001, ADV NIPS; WILSON AG, 1969, J TRANSP ECON POLICY, V3, P108; Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008	46	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304043
C	Beckham, C; Honari, S; Verma, V; Lamb, A; Ghadiri, F; Hjelm, RD; Bengio, Y; Pal, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Beckham, Christopher; Honari, Sina; Verma, Vikas; Lamb, Alex; Ghadiri, Farnoosh; Hjelm, R. Devon; Bengio, Yoshua; Pal, Christopher			On Adversarial Mixup Resynthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we explore new approaches to combining information encoded within the learned representations of auto-encoders. We explore models that are capable of combining the attributes of multiple inputs such that a resynthesised output is trained to fool an adversarial discriminator for real versus synthesised data. Furthermore, we explore the use of such an architecture in the context of semi-supervised learning, where we learn a mixing function whose objective is to produce interpolations of hidden states, or masked combinations of latent representations that are consistent with a conditioned class label. We show quantitative and qualitative evidence that such a formulation is an interesting avenue of research.(1)	[Beckham, Christopher; Honari, Sina; Verma, Vikas; Lamb, Alex; Ghadiri, Farnoosh; Hjelm, R. Devon; Bengio, Yoshua; Pal, Christopher] Mila Quebec Artificial Intelligence Inst, Montreal, PQ, Canada; [Lamb, Alex; Hjelm, R. Devon; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada; [Beckham, Christopher; Honari, Sina; Ghadiri, Farnoosh; Pal, Christopher] Polytech Montreal, Montreal, PQ, Canada; [Pal, Christopher] Element AI, Montreal, PQ, Canada; [Hjelm, R. Devon] Microsoft Res, Montreal, PQ, Canada; [Verma, Vikas] Aalto Univ, Espoo, Finland	Universite de Montreal; Universite de Montreal; Polytechnique Montreal; Aalto University	Beckham, C (corresponding author), Mila Quebec Artificial Intelligence Inst, Montreal, PQ, Canada.; Beckham, C (corresponding author), Polytech Montreal, Montreal, PQ, Canada.	christopher.beckham@mila.quebec; sina.honari@mila.quebec; vikas.verma@aalto.fi; alex.lamb@mila.quebec; farnoosh.ghadiri@mila.quebec; r.hjelm@mila.quebec; yoshua.bengio@mila.quebec; christopher.pal@polymtl.ca			Academy of Finland [13312683/Raiko Tapani AT kulut]; Huawei	Academy of Finland(Academy of Finland); Huawei(Huawei Technologies)	We thank Compute Canada for GPU access, and nVidia for donating a DGX-1 used for this research. We also thank Huawei for their support. Vikas Verma was supported by Academy of Finland project 13312683/Raiko Tapani AT kulut.	Alain Guillaume, 2016, ARXIV161001644; Bahdanau Dzmitry, 2018, ABS181112889 CORR; Belghazi MI, 2018, PR MACH LEARN RES, V80; Bengio Y, 1991, IJCNN 91, V2; Bengio Y., 2012, JMLR WORKSHOP C P, V27, P17; Bengio Y., 2017, ABS170801289 CORR; Berthelot David, 2019, INT C LEARN REPR; Clanuwat Tarin, 2018, DEEP LEARNING CLASSI; Deng L., 2012, IEEE SIGNAL PROC MAG, V29, P141, DOI DOI 10.1109/MSP.2012.2211477; Ghiasi Golnaz, 2018, ABS181012890 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; Higgins M, 2017, PALGR COMMUN, V3, DOI 10.1057/s41599-017-0005-4; Hjelm R Devon, 2019, INT C LEARN REPR; Hsu Kyle, 2019, INT C LEARN REPR; Jang E., 2016, ARXIV; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lin, 2015, ARXIV150204156; Liu Ming-Yu, 2017, NIPS; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Odena A, 2017, PR MACH LEARN RES, V70; Raffel C.A., 2019, ADV NEURAL INFORM PR, P5049; Rifai S., 2011, PROC INT C MACH LEAR; Sainburg Tim, 2018, ABS180706650 CORR; Salimans T., 2017, ARXIV170303864; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Such F. P., 2017, ARXIV171206567; van den Oord A, 2017, ADV NEUR IN, V30; Verma V, 2020, P AAAI C ARTIFICIAL, P10024; Verma V, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3635; Verma Vikas, 2018, ARXIV18005236; Vincent E, 2008, INT CONF ACOUST SPEE, P109, DOI 10.1109/ICASSP.2008.4517558; Yaguchi Yoichi, 2019, MIXFEAT MIX FEATURE; Yu A, 2017, IEEE I CONF COMP VIS, P5571, DOI 10.1109/ICCV.2017.594; Yu A, 2014, PROC CVPR IEEE, P192, DOI 10.1109/CVPR.2014.32; Yun S, 2019, IEEE I CONF COMP VIS, P6022, DOI 10.1109/ICCV.2019.00612; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang MZ, 2017, PR IEEE COMP DESIGN, P585, DOI 10.1109/ICCD.2017.101	46	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304036
C	Chen, JF; Wu, X; Rastogi, V; Liang, YY; Jha, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Jiefeng; Wu, Xi; Rastogi, Vaibhav; Liang, Yingyu; Jha, Somesh			Robust Attribution Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					An emerging problem in trustworthy machine learning is to train models that produce robust interpretations for their predictions. We take a step towards solving this problem through the lens of axiomatic attribution of neural networks. Our theory is grounded in the recent work, Integrated Gradients (IG) [STY17], in axiomatically attributing a neural network's output change to its input change. We propose training objectives in classic robust optimization models to achieve robust IG attributions. Our objectives give principled generalizations of previous objectives designed for robust predictions, and they naturally degenerate to classic soft-margin training for one-layer neural networks. We also generalize previous theory and prove that the objectives for different robust optimization models are closely related. Experiments demonstrate the effectiveness of our method, and also point to intriguing problems which hint at the need for better optimization techniques or better neural network architectures for robust attribution training.	[Chen, Jiefeng; Liang, Yingyu; Jha, Somesh] Univ Wisconsin, Madison, WI 53706 USA; [Wu, Xi; Rastogi, Vaibhav] Google, Mountain View, CA USA; [Jha, Somesh] XaiPient, New York, NY USA; [Rastogi, Vaibhav] UW Madison, Madison, WI USA	University of Wisconsin System; University of Wisconsin Madison; Google Incorporated; University of Wisconsin System; University of Wisconsin Madison	Chen, JF (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.				Air Force Grant [FA9550-18-1-0166]; National Science Foundation (NSF) [CCF-FMitF-1836978, SaTC-Frontiers-1804648, CCF-1652140]; ARO [W911NF-17-1-0405]	Air Force Grant(United States Department of DefenseUS Air Force Research Laboratory); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); ARO	This work is partially supported by Air Force Grant FA9550-18-1-0166, the National Science Foundation (NSF) Grants CCF-FMitF-1836978, SaTC-Frontiers-1804648 and CCF-1652140 and ARO grant number W911NF-17-1-0405.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; BenTal A, 2009, PRINC SER APPL MATH, P1; Burges, 1998, MNIST DATABASE HANDW; DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600; Engstrom Logan, 2019, ARXIV190600945; EricWong Zico, 2018, INT C MACH LEARN, P5286; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kendall MG, 1938, BIOMETRIKA, V30, P81, DOI 10.2307/2332226; Kolter Z, 2018, ADVERSARIAL ROBUSTNE; Luenberger D. G., 1969, OPTIMIZATION VECTOR; Madry Aleksander, 2017, ARXIV; Nilsback M-E., 2006, IEEE C COMP VIS PATT, DOI [10.1109/CVPR.2006., DOI 10.1109/CVPR.2006.42]; Rockafellar R., 1998, VARIATIONAL ANAL, V317; Ros AS, 2018, AAAI CONF ARTIF INTE, P1660; Sinha A., 2018, ICLR; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; Sundararajan M, 2017, PR MACH LEARN RES, V70; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Xiao H., 2017, ARXIV 170807747; Zou, 2017, ABS171010547 CORR	22	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906003
C	Choromanski, K; Pacchiano, A; Parker-Holder, J; Tang, YH; Sindhwani, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Choromanski, Krzysztof; Pacchiano, Aldo; Parker-Holder, Jack; Tang, Yunhao; Sindhwani, Vikas			From Complexity to Simplicity: Adaptive ES-Active Subspaces for Blackbox Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a new algorithm (ASEBO) for optimizing high-dimensional blackbox functions. ASEBO adapts to the geometry of the function and learns optimal sets of sensing directions, which are used to probe it, on-the-fly. It addresses the exploration-exploitation trade-off of blackbox optimization with expensive blackbox queries by continuously learning the bias of the lower-dimensional model used to approximate gradients of smoothings of the function via compressed sensing and contextual bandits methods. To obtain this model, it leverages techniques from the emerging theory of active subspaces [8] in a novel ES blackbox optimization context. As a result, ASEBO learns the dynamically changing intrinsic dimensionality of the gradient space and adapts to the hardness of different stages of the optimization without external supervision. Consequently, it leads to more sample-efficient blackbox optimization than state-of-the-art algorithms. We provide theoretical results and test ASEBO advantages over other methods empirically by evaluating it on the set of reinforcement learning policy optimization tasks as well as functions from the recently open-sourced Nevergrad library.	[Choromanski, Krzysztof; Sindhwani, Vikas] Google Brain Robot, Mountain View, CA 94043 USA; [Pacchiano, Aldo] Univ Calif Berkeley, Berkeley, CA USA; [Parker-Holder, Jack] Univ Oxford, Oxford, England; [Tang, Yunhao] Columbia Univ, New York, NY 10027 USA	University of California System; University of California Berkeley; University of Oxford; Columbia University	Choromanski, K (corresponding author), Google Brain Robot, Mountain View, CA 94043 USA.	kchoro@google.com; pacchiano@berkeley.edu; jackph@robots.ox.ac.uk; yt2541@columbia.edu; sindhwani@google.com						Agrawal S, 2015, CORR; Ahmad S., 2013, FLIGHT OPTIMIZATION; Akimoto Y., 2014, GECCO; Akimoto Y., 2016, GECCO; [Anonymous], 2017, CORR; Bridges R. A., 2019, CORR; Bridges R. A., 2019, CORR; Brockman G., 2016, OPENAI GYM; Choromanski K, 2018, PR MACH LEARN RES, V80; Constantine P. G., 2016, SIAM J SCI COMPUTING, V38; Constantine PG, 2015, 2015 IEEE 6TH INTERNATIONAL WORKSHOP ON COMPUTATIONAL ADVANCES IN MULTI-SENSOR ADAPTIVE PROCESSING (CAMSAP), P353, DOI 10.1109/CAMSAP.2015.7383809; Constantine PG, 2015, ACTIVE SUBSPACES EME, V2; Conti E, 2018, ADV NEUR IN, V31; Golub G. H., 1973, SIAM, V15; HA D, 2018, NEURIPS; Hansen M, 1996, IEEE C EVOL COMPUTAT, P312, DOI 10.1109/ICEC.1996.542381; Hesse C., 2017, OPENAI BASELINES; Houthooft Rein, 2018, NEURIPS; Jolliffe I.T, 2002, PRINCIPAL COMPONENT, VXXIX; Lehman J, 2018, GECCO'18: PROCEEDINGS OF THE 2018 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P450, DOI 10.1145/3205455.3205474; Li C., 2018, 6 INT C LEARN REPR I; Liu G., 2019, AAAI; Loshchilov I., 2019, IEEE T EVOLUTIONARY; Loshchilov I., 2014, GECCO; Maheswaranathan N., 2018, CORR; Mania Horia, 2018, CORR; Muller N., 2018, PARALLEL PROBLEM SOL; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Ros R, 2008, LECT NOTES COMPUT SC, V5199, P296, DOI 10.1007/978-3-540-87700-4_30; Rowland M., 2018, NEURIPS; Salimans T., 2017, EVOLUTION STRATEGIES; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, P 32 INT C MACH LEAR; Teytaud O., 2018, NEVERGRAD OPEN SOURC; Varelas K., 2018, PPSN 15 2018 15 INT; Zhang Guanhua, 2019, CORR; Zhou ZP, 2017, ACS CENTRAL SCI, V3, P1337, DOI 10.1021/acscentsci.7b00492	37	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901088
C	Dall'Amico, L; Couillet, R; Tremblay, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dall'Amico, Lorenzo; Couillet, Romain; Tremblay, Nicolas			Revisiting the Bethe-Hessian: Improved Community Detection in Sparse Heterogeneous Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RECONSTRUCTION	Spectral clustering is one of the most popular, yet still incompletely understood, methods for community detection on graphs. This article studies spectral clustering based on the Bethe-Hessian matrix H-r = (r(2) - 1)I-n + D - rA for sparse heterogeneous graphs (following the degree-corrected stochastic block model) in a two-class setting. For a specific value r = zeta, clustering is shown to be insensitive to the degree heterogeneity. We then study the behavior of the informative eigenvector of H zeta and, as a result, predict the clustering accuracy. The article concludes with an overview of the generalization to more than two classes along with extensive simulations on synthetic and real networks corroborating our findings.	[Dall'Amico, Lorenzo; Couillet, Romain; Tremblay, Nicolas] UGA, Grenoble INP, CNRS, GIPSA Lab, Grenoble, France; [Couillet, Romain] Univ Paris Saclay, Cent Supelec, L2S, Paris, France	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay	Dall'Amico, L (corresponding author), UGA, Grenoble INP, CNRS, GIPSA Lab, Grenoble, France.	lorenzo.dall-amico@gipsa-lab.fr	Dall'Amico, Lorenzo/AAE-4319-2022; Dall'Amico, Lorenzo/AAE-5336-2021	Dall'Amico, Lorenzo/0000-0002-7493-6421; 	ANR Project RMT4GRAPH [ANR-14-CE28-0006]; IDEX GSTATS Chair at University Grenoble Alpes; CNRS PEPS 13A [RW4SPEC]	ANR Project RMT4GRAPH(French National Research Agency (ANR)); IDEX GSTATS Chair at University Grenoble Alpes; CNRS PEPS 13A	This work is supported by the ANR Project RMT4GRAPH (ANR-14-CE28-0006), the IDEX GSTATS Chair at University Grenoble Alpes and by CNRS PEPS 13A (Project RW4SPEC). The authors thank Jean-Louis Barrat for fruitful discussions.	Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Ali HT, 2016, CONF REC ASILOMAR C, P1385, DOI 10.1109/ACSSC.2016.7869603; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Bordenave C, 2015, ANN IEEE SYMP FOUND, P1347, DOI 10.1109/FOCS.2015.86; Dembo A, 2010, BRAZ J PROBAB STAT, V24, P137, DOI 10.1214/09-BJPS027; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Gulikers L, 2018, ANN APPL PROBAB, V28, P3002, DOI 10.1214/18-AAP1381; Gulikers L, 2017, ADV APPL PROBAB, V49, P686, DOI 10.1017/apr.2017.18; Gulikers Lennart, 2017, LEIBNIZ INT P, V67; Joseph A., 2013, ARXIV13121733; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Le CM, 2017, RANDOM STRUCT ALGOR, V51, P538, DOI 10.1002/rsa.20713; Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274; Leskovec J, 2014, SNAP DATASETS STANFO; Lusseau D, 2003, BEHAV ECOL SOCIOBIOL, V54, P396, DOI 10.1007/s00265-003-0651-y; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; Mossel E., 2014, P MACHINE LEARNING R, P356; Nadakuditi RR, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.188701; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Newman MEJ, 2004, PHYS REV E, V70, DOI [10.1103/PhysRevE.70.056131, 10.1103/PhysRevE.69.026113]; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; SAADE A., 2014, ADV NEURAL INFORM PR, V27, P406; Terras A, 2010, ZETA FUNCTIONS GRAPH, V128; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752	31	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304008
C	De Palma, G; Kiani, BT; Lloyd, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		De Palma, Giacomo; Kiani, Bobak T.; Lloyd, Seth			Random deep neural networks are biased towards simple functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPLEXITY	We prove that the binary classifiers of bit strings generated by random wide deep neural networks with ReLU activation function are biased towards simple functions. The simplicity is captured by the following two properties. For any given input bit string, the average Hamming distance of the closest input bit string with a different classification is at least root n/(2 pi ln n), where n is the length of the string. Moreover, if the bits of the initial string are flipped randomly, the average number of flips required to change the classification grows linearly with n. These results are confirmed by numerical experiments on deep neural networks with two hidden layers, and settle the conjecture stating that random deep neural networks are biased towards simple functions. This conjecture was proposed and numerically explored in [Valle Perez et al., ICLR 2019] to explain the unreasonably good generalization properties of deep learning algorithms. The probability distribution of the functions generated by random deep neural networks is a good choice for the prior probability distribution in the PAC-Bayesian generalization bounds. Our results constitute a fundamental step forward in the characterization of this distribution, therefore contributing to the understanding of the generalization properties of deep learning algorithms.	[De Palma, Giacomo; Kiani, Bobak T.] MIT, MechE & RLE, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Lloyd, Seth] MIT, MechE Phys & RLE, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	De Palma, G (corresponding author), MIT, MechE & RLE, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	gdepalma@mit.edu; bkiani@mit.edu; slloyd@mit.edu	De Palma, Giacomo/AAA-5599-2022	De Palma, Giacomo/0000-0002-5064-8695	European Research Council (ERC) [337603, 321029]; Danish Council for Independent Research (Sapere Aude); VILLUM FONDEN via the QMATH Centre of Excellence [10059]; AFOSR; ARO under the Blue Sky program; IARPA; NSF; BMW under the MIT Energy Initiative	European Research Council (ERC)(European Research Council (ERC)European Commission); Danish Council for Independent Research (Sapere Aude); VILLUM FONDEN via the QMATH Centre of Excellence(Villum Fonden); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ARO under the Blue Sky program; IARPA; NSF(National Science Foundation (NSF)); BMW under the MIT Energy Initiative	GdP acknowledges financial support from the European Research Council (ERC Grant Agreements Nos. 337603 and 321029), the Danish Council for Independent Research (Sapere Aude), VILLUM FONDEN via the QMATH Centre of Excellence (Grant No. 10059) and AFOSR and ARO under the Blue Sky program. SL and BTK were supported by IARPA, NSF, BMW under the MIT Energy Initiative, and ARO under the Blue Sky program.	Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], 2013, ARXIV13126098; Arora S, 2018, PR MACH LEARN RES, V80; Arpit D, 2017, PR MACH LEARN RES, V70; Baity-Jesi M., 2018, INT C MACH LEARN PML, P314; Bartlett P. L., 2017, ARXIV; BLUMER A, 1987, INFORM PROCESS LETT, V24, P377, DOI 10.1016/0020-0190(87)90114-1; Bovier A., 2005, LECT NOTES TU BERLIN; Burges, 1998, MNIST DATABASE HANDW; Canziani A, 2016, ARXIV; Catoni Olivier, 2007, I MATH STAT LECT NOT; Choromanska A., 2015, COLT, P1756; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Dingle K, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-03101-6; Dziugaite Gintare Karolina, 2018, ARXIV180209583; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Franco L, 2006, NEUROCOMPUTING, V70, P351, DOI 10.1016/j.neucom.2006.01.025; Garriga-Alonso A., 2018, ARXIV180805587; Gilmer Justin, 2016, INT C LEARN REPR; Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Hardt M., 2015, PREPRINT; Hinz Peter, 2018, ARXIV180601918; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kingma D.P, P 3 INT C LEARNING R; Kolmogorov AN, 1998, THEOR COMPUT SCI, V207, P387, DOI 10.1016/S0304-3975(98)00075-9; Lattimore T, 2013, LECT NOTES COMPUT SC, V7070, P223, DOI 10.1007/978-3-642-44958-1_17; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J., 2019, ARXIV190206720; Lee Jaehoon, 2017, ARXIV171100165; LEMPEL A, 1976, IEEE T INFORM THEORY, V22, P75, DOI 10.1109/TIT.1976.1055501; Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013; Li M., 2013, MONOGRAPHS COMPUTER; Lin HW, 2017, J STAT PHYS, V168, P1223, DOI 10.1007/s10955-017-1836-5; Madry Aleksander, 2017, ARXIV; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Mehta D, 2018, PHYS REV E, V97, DOI 10.1103/PhysRevE.97.052307; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Morcos Ari S, 2018, ARXIV180306959; Nakkiran P., 2019, ARXIV190100532; Neyshabur B., 2018, ARXIV180512076; Neyshabur Behnam, 2014, ARXIV14126614; Neyshabur Behnam, 2017, ARXIV170709564; Novak R., 2019, INT C LEARN REPR OPE; Novak R., 2018, INT C LEARN REPR; Peck J, 2017, ADV NEURAL INFORM PR, P804; Perez-Cruz F, 2018, INT C ART INT STAT, V84, P1924; Poole B, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Raghu M., 2016, ARXIV160605336; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Rusk N, 2016, NAT METHODS, V13, P35, DOI 10.1038/nmeth.3707; Schmidhuber J, 1997, NEURAL NETWORKS, V10, P857, DOI 10.1016/S0893-6080(96)00127-X; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Srebro N., 2017, ARXIV171010345; Sun SZ, 2016, AAAI CONF ARTIF INTE, P2066; Tsipras Dimitris, 2018, ARXIV180512152; Valle-Perez G., 2019, INT C LEARN REPR; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vladimirova M, 2019, PR MACH LEARN RES, V97; Wolpert DH, 1995, SFI S SCI C, V20, P117; Wu L., 2017, ARXIV170610239; Xiao LC, 2018, PR MACH LEARN RES, V80; Yang G., 2019, ARXIV190204760; Zhang Chiyuan, 2016, ARXIV161103530; ZIV J, 1977, IEEE T INFORM THEORY, V23, P337, DOI 10.1109/TIT.1977.1055714	79	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302001
C	Defazio, A; Bottou, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Defazio, Aaron; Bottou, Leon			On the Ineffectiveness of Variance Reduced Optimization for Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The application of stochastic variance reduction to optimization has shown remarkable recent theoretical and practical success. The applicability of these techniques to the hard non-convex optimization problems encountered during training of modern deep neural networks is an open problem. We show that naive application of the SVRG technique and related approaches fail, and explore why.	[Defazio, Aaron; Bottou, Leon] Facebook AI Res New York, New York, NY 10003 USA		Defazio, A (corresponding author), Facebook AI Res New York, New York, NY 10003 USA.							Abadi M, 2015, P 12 USENIX S OPERAT; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Balamurugan P, 2016, ADV NEUR IN, V29; Bietti A, 2017, ADV NEUR IN, V30; Clevert Djork-Arne, 2016, INT C LEARN REPR 201; Defazio A, 2014, ADV NEUR IN, V27; Defazio A, 2016, ADV NEUR IN, V29; Defazio AJ, 2014, PR MACH LEARN RES, V32, P1125; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Frostig Roy, 2015, P 28 C LEARN THEOR; Goyal Priya, 2017, ARXIV170602677; Harchaoui, 2018, P INT C ART INT STAT, P613; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kidambi Rahul, 2018, INT C LEARN REPR ICL; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lei LH, 2017, ADV NEUR IN, V30; Lei Lihua, 2017, P 20 INT C ART INT S; Lin H, 2015, ADV NEUR IN, V28; Mairal Julien, 2014, TECHNICAL REPORT; Nguyen LM, 2017, PR MACH LEARN RES, V70; Nguyen Lam M., 2019, ARXIV190107648; Reddi SJ, 2016, PR MACH LEARN RES, V48; Schmidt Mark, 2017, F MATH PROGRAM; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Zeyuan Allen-Zhu, 2017, P 49 ANN ACM SIGACT; Zhang, 2013, ADV NEURAL INFORM PR, P315	31	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301071
C	Goel, G; Lin, YH; Sun, HY; Wierman, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Goel, Gautam; Lin, Yiheng; Sun, Haoyuan; Wierman, Adam			Beyond Online Balanced Descent: An Optimal Algorithm for Smoothed Online Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study online convex optimization in a setting where the learner seeks to minimize the sum of a per-round hitting cost and a movement cost which is incurred when changing decisions between rounds. We prove a new lower bound on the competitive ratio of any online algorithm in the setting where the costs are m-strongly convex and the movement costs are the squared l(2) norm. This lower bound shows that no algorithm can achieve a competitive ratio that is o(m(-1/2)) as m tends to zero. No existing algorithms have competitive ratios matching this bound, and we show that the state-of-the-art algorithm, Online Balanced Decent (OBD), has a competitive ratio that is Omega(m(-2/3)). We additionally propose two new algorithms, Greedy OBD (G-OBD) and Regularized OBD (R-OBD) and prove that both algorithms have an O(m(-1/2)) competitive ratio. The result for G-OBD holds when the hitting costs are quasiconvex and the movement costs are the squared l(2) norm, while the result for R-OBD holds when the hitting costs are m-strongly convex and the movement costs are Bregman Divergences. Further, we show that R-OBD simultaneously achieves constant, dimension-free competitive ratio and sublinear regret when hitting costs are strongly convex.	[Goel, Gautam; Lin, Yiheng; Sun, Haoyuan; Wierman, Adam] CALTECH, Pasadena, CA 91125 USA; [Lin, Yiheng] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China	California Institute of Technology; Tsinghua University	Goel, G; Lin, YH; Sun, HY (corresponding author), CALTECH, Pasadena, CA 91125 USA.; Lin, YH (corresponding author), Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.			Lin, Yiheng/0000-0001-6524-2877	NSF [AitF-1637598, CNS-1518941]; Amazon AWS AI Fellowship	NSF(National Science Foundation (NSF)); Amazon AWS AI Fellowship	This work was supported by NSF grants AitF-1637598 and CNS-1518941, with additional support for Gautam Goel provided by an Amazon AWS AI Fellowship.	Antoniadis Antonios, 2017, P WAOA 2017, P164; Argue CJ, 2019, P 13 ANN ACM SIAM S, P117; Azizan N., 2019, P INT C LEARN REPR I; Badici M, 2015, IEEE DECIS CONTR P, P6730, DOI 10.1109/CDC.2015.7403279; Bansal N., 2015, P WORKSH APPR ALG CO; Bansal N, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1253; Bansal Nikhil, 2017, ARXIV171204581; Bartal Yair, 1997, P ACM S THEOR COMP S, P711, DOI [10.1145/258533.258667, DOI 10.1145/258533.258667]; Bienkowski M., 2018, ARXIV181109233; Blum A, 2000, MACH LEARN, V39, P35, DOI 10.1023/A:1007621832648; BORODIN A, 1992, J ACM, V39, P745, DOI 10.1145/146585.146588; Breitner J, 2018, P ACM PROGRAM LANG, V2, DOI 10.1145/3236784; Bubeck S., 2019, P ACM SIGACT S THEOR; Bubeck S, 2018, ACM S THEORY COMPUT, P3, DOI 10.1145/3188745.3188798; Buchbinder N., 2019, P 30 ANN ACM SIAM S, P98; Chen NJ, 2016, SIGMETRICS/PERFORMANCE 2016: PROCEEDINGS OF THE SIGMETRICS/PERFORMANCE JOINT INTERNATIONAL CONFERENCE ON MEASUREMENT AND MODELING OF COMPUTER SCIENCE, P193, DOI [10.1145/2896377.2901464, 10.1145/2964791.2901464]; Chen Z, 2018, INT C ELECTR MACH SY, P1574, DOI 10.23919/ICEMS.2018.8549154; Daniely A., 2019, ALGORITHMIC LEARNING, P333; FRIEDMAN J, 1993, DISCRETE COMPUT GEOM, V9, P293, DOI 10.1007/BF02189324; Goel G., 2017, P IEEE C DEC CONTR C, P1291; Goel G, 2019, PR MACH LEARN RES, V89; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Joseph V, 2012, IEEE INFOCOM SER, P567, DOI 10.1109/INFCOM.2012.6195799; Kim SJ, 2017, IEEE T SMART GRID, V8, P2784, DOI 10.1109/TSG.2016.2539948; Kim T, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P577, DOI 10.1145/2783258.2783356; Lin M., 2012, GREEN COMPUTING C IG, P1, DOI DOI 10.1109/IGCC.2012.6322266; Lin MH, 2013, IEEE ACM T NETWORK, V21, P1378, DOI 10.1109/TNET.2012.2226216; MANASSE MS, 1990, J ALGORITHM, V11, P208, DOI 10.1016/0196-6774(90)90003-W; Murata N, 2004, NEURAL COMPUT, V16, P1437, DOI 10.1162/089976604323057452; Niangjun Chen, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P191, DOI 10.1145/2745844.2745854; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; strom K. J. A, 2010, FEEDBACK SYSTEMS	32	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301082
C	Gondal, MW; Wuthrich, M; Miladinovic, D; Locatello, F; Breidt, M; Volchkov, V; Akpo, J; Bachem, O; Scholkopf, B; Bauer, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gondal, Muhammad Waleed; Wuethrich, Manuel; Miladinovic, Dorde; Locatello, Francesco; Breidt, Martin; Volchkov, Valentin; Akpo, Joel; Bachem, Olivier; Schoelkopf, Bernhard; Bauer, Stefan			On the Transfer of Inductive Bias from Simulation to the Real World: a New Disentanglement Dataset	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REPRESENTATION	Learning meaningful and compact representations with disentangled semantic aspects is considered to be of key importance in representation learning. Since real-world data is notoriously costly to collect, many recent state-of-the-art disentanglement models have heavily relied on synthetic toy data-sets. In this paper, we propose a novel data-set which consists of over one million images of physical 3D objects with seven factors of variation, such as object color, shape, size and position. In order to be able to control all the factors of variation precisely, we built an experimental platform where the objects are being moved by a robotic arm. In addition, we provide two more datasets which consist of simulations of the experimental setup. These datasets provide for the first time the possibility to systematically investigate how well different disentanglement methods perform on real data in comparison to simulation, and how simulated data can be leveraged to build better representations of the real world. We provide a first experimental study of these questions and our results indicate that learned models transfer poorly, but that model and hyperparameter selection is an effective means of transferring information to the real world.	[Gondal, Muhammad Waleed; Wuethrich, Manuel; Locatello, Francesco; Volchkov, Valentin; Akpo, Joel; Schoelkopf, Bernhard; Bauer, Stefan] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Miladinovic, Dorde; Locatello, Francesco] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Breidt, Martin] Max Planck Inst Biol Cybernet, Tubingen, Germany; [Bachem, Olivier] Google Res, Brain Team, Mountain View, CA USA	Max Planck Society; Swiss Federal Institutes of Technology Domain; ETH Zurich; Max Planck Society; Google Incorporated	Gondal, MW; Bauer, S (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.	waleed.gondal@tue.mpg.de; stefan.bauer@inf.ethz.ch	Locatello, Francesco/GQY-6025-2022	Locatello, Francesco/0000-0002-4850-0683	Max Planck ETH Center for Learning Systems; Google Cloud	Max Planck ETH Center for Learning Systems; Google Cloud(Google Incorporated)	This research was partially supported by the Max Planck ETH Center for Learning Systems and Google Cloud. We thank Alexander Neitz and Arash Mehrjou for useful discussions. We would also like to thank Felix Grimminger, Ludovic Righetti, Stefan Schaal, Julian Viereck and Felix Widmaier whose work served as a starting point for the development of the robotic platform in the present paper.	[Anonymous], 2017, ARXIV170501314; Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI [DOI 10.1038/NATURE14539, 10.1016/j.asoc.2014.05.028, DOI 10.1016/J.ASOC.2014.05.028]; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bouchacourt D., 2018, AAAI; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Chartsias A, 2018, LECT NOTES COMPUT SC, V11071, P490, DOI 10.1007/978-3-030-00934-2_55; Chen T.Q., 2018, NEURIPS, P2610; Cheung Brian, 2015, WORKSH INT C LEARN R; Creager E, 2019, PR MACH LEARN RES, V97; Eastwood Cian, 2018, INT C LEARN REPR; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Esmaeili Babak, 2018, ARXIV180402086; Fidler S., 2012, P ADV NEURAL INFORM, P611; Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646, DOI DOI 10.5555/2984093.2984166; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; Higgins I., 2018, ICLR; Higgins I., 2017, P INT C LEARN REPR T; Higgins I., 2018, ARXIV PREPRINT ARXIV; Higgins I, 2017, PR MACH LEARN RES, V70; Higgins  Irina, 2017, ARXIV170703389; James S., 2018, ABS181207252 CORR; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kulkarni TD, 2015, ADV NEUR IN, V28; Kumar A., 2017, INT C LEARN REPR; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Laversanne-Finot A., 2018, MACHINE LEARNING RES, V87, P487; LeCun Y, 2004, PROC CVPR IEEE, P97; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lesort T, 2018, NEURAL NETWORKS, V108, P379, DOI 10.1016/j.neunet.2018.07.006; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Locatello F, 2019, ARXIV190501258; Locatello F, 2019, PR MACH LEARN RES, V97; Locatello Francesco, 2019, ARXIV190513662; Munch Edvard, 1893, SCREAM; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Peters J, 2017, ADAPT COMPUT MACH LE; Pong V., 2018, PROC 2 WORKSHOP LIFE; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Ridgeway K, 2018, ADV NEUR IN, V31; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Scholkopf Bernhard, 2012, ICML; Spirtes P., 2000, CAUSATION PREDICTION; Steenbrugge Xander, 2018, P 32 C NEUR INF PROC; Suter R, 2019, PR MACH LEARN RES, V97; Tomczak J. M., 2017, ARXIV170507120; vanSteenkiste Sjoerd, 2019, ARXIV190512506; Watters N., 2019, ARXIV190107017; Zhang JW, 2019, IEEE ROBOT AUTOM LET, V4, P1148, DOI 10.1109/LRA.2019.2894216	55	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907040
C	Hoogeboom, E; Peters, JWT; van den Bergt, R; Welling, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hoogeboom, Emiel; Peters, Jorn W. T.; van den Berg, Rianne; Welling, Max			Integer Discrete Flows and Lossless Compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DENSITY-ESTIMATION	Lossless compression methods shorten the expected representation size of data without loss of information, using a statistical model. Flow-based models are attractive in this setting because they admit exact likelihood optimization, which is equivalent to minimizing the expected number of bits per message. However, conventional flows assume continuous data, which may lead to reconstruction errors when quantized for compression. For that reason, we introduce a flow-based generative model for ordinal discrete data called Integer Discrete Flow (IDF): a bijective integer map that can learn rich transformations on high-dimensional data. As building blocks for IDFs, we introduce a flexible transformation layer called integer discrete coupling. Our experiments show that IDFs are competitive with other flow-based generative models. Furthermore, we demonstrate that IDF based compression achieves state-of-the-art lossless compression rates on CIFAR10, ImageNet32, and ImageNet64. To the best of our knowledge, this is the first lossless compression method that uses invertible neural networks.	[Hoogeboom, Emiel; Peters, Jorn W. T.; Welling, Max] Univ Amsterdam, UvA Bosch Delta Lab, Amsterdam, Netherlands; [van den Berg, Rianne] Univ Amsterdam, Amsterdam, Netherlands; [van den Berg, Rianne] Google, Mountain View, CA 94043 USA	University of Amsterdam; University of Amsterdam; Google Incorporated	Hoogeboom, E (corresponding author), Univ Amsterdam, UvA Bosch Delta Lab, Amsterdam, Netherlands.	e.hoogeboom@uva.nl; j.w.t.peters@uva.nl; riannevdberg@gmail.com; m.welling@uva.nl						AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784; [Anonymous], 2017, 5 INT C LEARN REPR I; Bengio Yoshua, 2013, ARXIV; Calderbank AR, 1997, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL I, P596, DOI 10.1109/ICIP.1997.647983; Calderbank AR, 1998, APPL COMPUT HARMON A, V5, P332, DOI 10.1006/acha.1997.0238; Deco G., 1995, Advances in Neural Information Processing Systems 7, P247; Dewitte S, 1997, IEEE SIGNAL PROC LET, V4, P158, DOI 10.1109/97.586035; Dinh L., 2015, 3 INT C LEARN REPR I; Dinh Laurent, 2017, 5 INT C LEARN REPR I; Duda J., 2009, ARXIV09020271; Duda J, 2013, ARXIV13112540; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl Will, 2019, 7 INT C LEARN REPR I; Helin H, 2018, J PATHOLOGY INFORN, V9; Hoogeboom Emiel, 2019, P 36 INT C MACH LEAR; Huang G., 2019, P IEEE C COMP VIS PA, P4700; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; International Organization for Standardization, 2003, 159482003 ISO; International Organization for Standardization, 2003, 1544412016 ISO; Janowczyk A, 2018, COMP M BIO BIO E-IV, V6, P270, DOI 10.1080/21681163.2016.1141063; Kingma D.P., 2015, INT C LEARNING REPRE; Kingma D.P., 2018, P 32 INT C NEUR INF, P10236; Kingma D.P., 2014, P 2 INT C LEARN REPR, DOI DOI 10.1093/BIOINFORMATICS/BTAA169; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kingma Friso H, 2019, 36 INT C MACH LEARN; Kumar M., 2019, ARXIV190301434; Mentzer F, 2019, PROC CVPR IEEE, P10621, DOI 10.1109/CVPR.2019.01088; Moffat A, 1998, ACM T INFORM SYST, V16, P256, DOI 10.1145/290159.290162; Papamakarios George, 2017, ADV NEURAL INFORM PR, P2338; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Prenger R, 2019, INT CONF ACOUST SPEE, P3617, DOI 10.1109/ICASSP.2019.8683143; Rezende D., 2015, ICML, P1530; Rippel O., 2013, ARXIV13025125; RISSANEN J, 1979, IBM J RES DEV, V23, P149, DOI 10.1147/rd.232.0149; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Sneyers J, 2016, IEEE IMAGE PROC, P66, DOI 10.1109/ICIP.2016.7532320; Tabak EG, 2013, COMMUN PUR APPL MATH, V66, P145, DOI 10.1002/cpa.21423; Tabak EG, 2010, COMMUN MATH SCI, V8, P217; Townsend James, 2019, 7 INT C LEARN REPR I; Tran Dustin, 2019, ICLR 2019 WORKSH DEE; van den Berg Rianne, 2018, 34 C UNC ART INT UAI; van den Oord A, 2016, PR MACH LEARN RES, V48	42	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903074
C	Ibrahim, S; Fu, X; Kargas, N; Huang, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ibrahim, Shahana; Fu, Xiao; Kargas, Nikos; Huang, Kejun			Crowdsourcing via Pairwise Co-occurrences: Identifiability and Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NONNEGATIVE MATRIX FACTORIZATION; VOLUME; SIGNAL	The data deluge comes with high demands for data labeling. Crowdsourcing (or, more generally, ensemble learning) techniques aim to produce accurate labels via integrating noisy, non-expert labeling from annotators. The classic Dawid-Skene estimator and its accompanying expectation maximization (EM) algorithm have been widely used, but the theoretical properties are not fully understood. Tensor methods were proposed to guarantee identification of the Dawid-Skene model, but the sample complexity is a hurdle for applying such approaches-since the tensor methods hinge on the availability of third-order statistics that are hard to reliably estimate given limited data. In this paper, we propose a framework using pairwise co-occurrences of the annotator responses, which naturally admits lower sample complexity. We show that the approach can identify the Dawid-Skene model under realistic conditions. We propose an algebraic algorithm reminiscent of convex geometry-based structured matrix factorization to solve the model identification problem efficiently, and an identifiability-enhanced algorithm for handling more challenging and critical scenarios. Experiments show that the proposed algorithms outperform the state-of-art algorithms under a variety of scenarios.	[Ibrahim, Shahana; Fu, Xiao] Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA; [Kargas, Nikos] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA; [Huang, Kejun] Univ Florida, Dept Comp & Info Sci & Engn, Gainesville, FL 32611 USA	Oregon State University; University of Minnesota System; University of Minnesota Twin Cities; State University System of Florida; University of Florida	Ibrahim, S (corresponding author), Oregon State Univ, Sch Elect Engn & Comp Sci, Corvallis, OR 97331 USA.	ibrahish@oregonstate.edu; xiao.fu@oregonstate.edu; kaga005@umn.edu; kejun.huang@ufl.edu	Kargas, Nikos/AAX-7248-2021	Kargas, Nikos/0000-0002-3798-2875	National Science Foundation [ECCS 1808159, NSF ECCS 1608961]; Army Research Office (ARO) [ARO W911NF-19-1-0247, ARO W911NF-19-1-0407]	National Science Foundation(National Science Foundation (NSF)); Army Research Office (ARO)	The work is supported in part by the National Science Foundation under projects ECCS 1808159 and NSF ECCS 1608961, and by the Army Research Office (ARO) under projects ARO W911NF-19-1-0247 and ARO W911NF-19-1-0407.	Anandkumar A, 2014, J MACH LEARN RES, V15, P2239; [Anonymous], 2013, ACM SIGMETRICS PERFO; Arora S., 2013, P ICML; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Chan TH, 2011, IEEE T GEOSCI REMOTE, V49, P4177, DOI 10.1109/TGRS.2011.2141672; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Donoho D., 2003, ADV NEURAL INFORM PR, V16; Fu X, 2019, IEEE SIGNAL PROC MAG, V36, P59, DOI 10.1109/MSP.2018.2877582; Fu X, 2018, IEEE SIGNAL PROC LET, V25, P328, DOI 10.1109/LSP.2018.2789405; Fu X, 2016, IEEE T SIGNAL PROCES, V64, P6254, DOI 10.1109/TSP.2016.2602800; Fu X, 2015, IEEE J-STSP, V9, P1128, DOI 10.1109/JSTSP.2015.2410763; Ghosh A, 2011, EURODYN, P1671; Gillis N., 2014, REGULARIZATION OPTIM, V12, P257; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Huang K, 2018, P ICML 2018; Huang KJ, 2014, IEEE T SIGNAL PROCES, V62, P211, DOI 10.1109/TSP.2013.2285514; JONKER R, 1986, OPER RES LETT, V5, P171, DOI 10.1016/0167-6377(86)90073-8; Karger David R., 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P284; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Kittur A, 2008, CHI 2008: 26TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P453; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Lease M, 2011, OVERVIEW TREC 2011 C; Lin CH, 2015, IEEE T GEOSCI REMOTE, V53, P5530, DOI 10.1109/TGRS.2015.2424719; Liu Q., 2012, NIPS, P692; Nascimento JMP, 2005, IEEE T GEOSCI REMOTE, V43, P898, DOI 10.1109/TGRS.2005.844293; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Razaviyayn M, 2013, SIAM J OPTIMIZ, V23, P1126, DOI 10.1137/120891009; Sidiropoulos ND, 2017, IEEE T SIGNAL PROCES, V65, P3551, DOI 10.1109/TSP.2017.2690524; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; STEIN P, 1966, AM MATH MON, V73, P299, DOI 10.2307/2315353; Stephane Boucheron, 2004, CONCENTRATION INEQUA; Traganitis PA, 2018, IEEE T SIGNAL PROCES, V66, P4737, DOI 10.1109/TSP.2018.2860562; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260; Zhou DY, 2014, PR MACH LEARN RES, V32, P262	40	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307082
C	Lecarpentier, E; Rachelson, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Lecarpentier, Erwan; Rachelson, Emmanuel			Non-Stationary Markov Decision Processes a Worst-Case Approach using Model-Based Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POLICY	This work tackles the problem of robust planning in non-stationary stochastic environments. We study Markov Decision Processes (MDPs) evolving over time and consider Model-Based Reinforcement Learning algorithms in this setting. We make two hypotheses: 1) the environment evolves continuously with a bounded evolution rate; 2) a current model is known at each decision epoch but not its evolution. Our contribution can be presented in four points. 1) we define a specific class of MDPs that we call Non-Stationary MDPs (NSMDPs). We introduce the notion of regular evolution by making an hypothesis of Lipschitz-Continuity on the transition and reward functions w.r.t. time; 2) we consider a planning agent using the current model of the environment but unaware of its future evolution. This leads us to consider a worst-case method where the environment is seen as an adversarial agent; 3) following this approach, we propose the Risk-Averse Tree-Search (RATS) algorithm, a Model-Based method similar to minimax search; 4) we illustrate the benefits brought by RATS empirically and compare its performance with reference Model-Based algorithms.	[Lecarpentier, Erwan] Univ Toulouse, ONERA French Aerosp Lab, Toulouse, France; [Rachelson, Emmanuel] Univ Toulouse, ISAE SUPAERO, Toulouse, France	National Office for Aerospace Studies & Research (ONERA); Universite de Toulouse; Universite de Toulouse; Institut Superieur de l'Aeronautique et de l'Espace (ISAE-SUPAERO)	Lecarpentier, E (corresponding author), Univ Toulouse, ONERA French Aerosp Lab, Toulouse, France.	erwan.lecarpentier@isae-supaero.fr; emmanuel.rachelson@isae-supaero.fr		Rachelson, Emmanuel/0000-0002-8559-1617	Occitanie region, France	Occitanie region, France	This research was supported by the Occitanie region, France.	Abel D, 2018, PR MACH LEARN RES, V80; Abel D, 2018, PR MACH LEARN RES, V80; Asadi K, 2018, PR MACH LEARN RES, V80; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Bubeck S., 2010, 10 C LEARN THEOR; CAMPO L, 1991, IEEE T AUTOMAT CONTR, V36, P238, DOI 10.1109/9.67304; Choi S. P., 2001, P 8 INT WORKSH ART; Choi S. P., 1999, IJCAI WORKSH NEUR SY; Chung JJ, 2015, INT J ROBOT RES, V34, P158, DOI 10.1177/0278364914553683; Csaji BC, 2008, J MACH LEARN RES, V9, P1679; Da Silva Bruno C, 2006, P 23 INT C MACHINE L, P217; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; Dick T, 2014, PR MACH LEARN RES, V32; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Fudenberg D., 1991, GAME THEORY 1991, V393, P12; Hadoux E., 2014, LEARNING MULTIPLE CO, P1; Hadoux E., 2015, THESIS; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Jaulmes R., 2005, ECML WORKSH REINF LE, P26; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Keller Thomas, 2013, ICAPS; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Lecarpentier E., 2017, ARXIV170705668; Lecarpentier E., 2018, IJCAI; Li L., 2013, AAAI SPRING S LIF MA, P49; Lim SH, 2013, ADV NEURAL INFORM PR, V26, P701; Munos R, 2014, FOUND TRENDS MACH LE, V7, P1, DOI 10.1561/2200000038; Pazis Jason, 2013, P ANN AAAI C ARTIFIC, P774; Pirotta M, 2015, MACH LEARN, V100, P255, DOI 10.1007/s10994-015-5484-1; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rachelson E., 2010, LOCALITY ACTION DOMI; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szita I., 2002, J MACHINE LEARNING R, V3, P145; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wiering M. A., 2001, P 8 INT C MACH LEARN, P585; Zhang N.L., 2000, SEQUENCE LEARNING, P264	40	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307025
C	Liu, YK; Yu, R; Zheng, S; Zhan, E; Yue, YS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Yukai; Yu, Rose; Zheng, Stephan; Zhan, Eric; Yue, Yisong			NAOMI: Non-Autoregressive Multiresolution Sequence Imputation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Missing value imputation is a fundamental problem in spatiotemporal modeling, from motion tracking to the dynamics of physical systems. Deep autoregressive models suffer from error propagation which becomes catastrophic for imputing long-range sequences. In this paper, we take a non-autoregressiveapproach and propose a novel deep generative model: Non-AutOregressive Multiresolution Imputation (NAOMI) to impute long-range sequences given arbitrary missing patterns. NAOMI exploits the multiresolution structure of spatiotemporal data and decodes recursively from coarse to fine-grained resolutions using a divide-and-conquer strategy. We further enhance our model with adversarial training. When evaluated extensively on benchmark datasets from systems of both deterministic and stochastic dynamics. In our experiments, NAOMI demonstrates significant improvement in imputation accuracy (reducing average error by 60% compared to autoregressive counterparts) and generalization for long-range sequences.	[Liu, Yukai; Zheng, Stephan; Zhan, Eric; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA; [Yu, Rose] Northeastern Univ, Boston, MA 02115 USA	California Institute of Technology; Northeastern University	Liu, YK (corresponding author), CALTECH, Pasadena, CA 91125 USA.	yukai@caltech.edu; roseyu@northeastern.edu; stephan.zheng@salesforce.com; ezhan@caltech.edu; yyue@caltech.edu			NSF [1850349, 1564330]; DARPA [PAI: HR00111890035]	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported in part by NSF #1564330, NSF #1850349, and DARPA PAI: HR00111890035.	Acuna E, 2004, ST CLASS DAT ANAL, P639; Ansley C. F., 1984, TIME SERIES ANAL IRR, P9, DOI DOI 10.1007/978-1-4684-9403-7_2; Bradbury James, 2018, INT C LEARN REPR ICL; Cao W., 2018, P 32 INT C NEUR INF, P6776; Che ZP, 2018, SCI REP-UK, V8, DOI 10.1038/s41598-018-24271-9; Chung Junyoung, 2017, INT C LEARN REPR ICL; Dua D., 2017, UCI MACHINE LEARNING; Fedus William, 2018, P INT C LEARN REPR; Fragkiadaki K., 2016, INT C LEARN REPR ICL; Friedman S, 2001, IND REL RES, P1, DOI 10.1097/00054725-200102000-00001; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kakade Sham M., 2003, THESIS; Karras Tero, 2018, INT C LEARN REPR ICL; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lee Jason, 2018, P 2018 C EMP METH NA; Libovickt Jindfich, 2018, P 2018 C EMP METH NA; Little Roderick JA, 2019, STAT ANAL MISSING DA, V793; Luo Y., 2018, 32 INT C NEUR INF PR, P1603; Nelwamondo FV, 2007, CURR SCI INDIA, V93, P1514; Reed Scott, 2017, INT C MACH LEARN; Roberts Adam, 2018, INT C MACH LEARN; Rubin DB, 2004, MULTIPLE IMPUTATION, V81; Serban IV, 2017, AAAI CONF ARTIF INTE, P3288; Urtasun R., 2006, 2006 IEEE COMP VIS P, P238, DOI DOI 10.1109/CVPR.2006.15; van Buuren S, 2011, J STAT SOFTW, V45, P1; Van den Oord Aaron, 2018, INT C MACH LEARN; Yoon J, 2018, IEEE T BIOMEDICAL EN; Yoon J., 2018, INT C MACH LEARN; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zhan Eric, 2019, INT C LEARN REPR; Zheng Stephan, 2016, ADV NEURAL INFORM PR, P1543	33	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902082
C	Martinez-Rubio, D; Kanade, V; Rebeschini, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Martinez-Rubio, David; Kanade, Varun; Rebeschini, Patrick			Decentralized Cooperative Stochastic Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTIARMED BANDIT; ALGORITHMS	We study a decentralized cooperative stochastic multi-armed bandit problem with K arms on a network of N agents. In our model, the reward distribution of each arm is the same for each agent and rewards are drawn independently across agents and time steps. In each round, each agent chooses an arm to play and subsequently sends a message to her neighbors. The goal is to minimize the overall regret of the entire network. We design a fully decentralized algorithm that uses an accelerated consensus procedure to compute (delayed) estimates of the average of rewards obtained by all the agents for each arm, and then uses an upper confidence bound (UCB) algorithm that accounts for the delay and error of the estimates. We analyze the regret of our algorithm and also provide a lower bound. The regret is bounded by the optimal centralized regret plus a natural and simple term depending on the spectral gap of the communication matrix. Our algorithm is simpler to analyze than those proposed in prior work and it achieves better regret bounds, while requiring less information about the underlying network. It also performs better empirically.	[Martinez-Rubio, David; Kanade, Varun] Univ Oxford, Dept Comp Sci, Oxford, England; [Rebeschini, Patrick] Univ Oxford, Dept Stat, Oxford, England	University of Oxford; University of Oxford	Martinez-Rubio, D (corresponding author), Univ Oxford, Dept Comp Sci, Oxford, England.	david.martinez@cs.ox.ac.uk; varunk@cs.ox.ac.uk; patrick.rebeschini@stats.ox.ac.uk		Martinez-Rubio, David/0000-0002-4345-5422	EPSRC MPLS division [EP/N509711/1, 2053152]; Alan Turing Institute under the EPSRC [EP/N510129/1]; AWS Cloud Credits for Research program	EPSRC MPLS division(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); AWS Cloud Credits for Research program	The authors thank Raphael Berthier and Francis Bach for helpful exchanges on the problem of averaging. David Martinez-Rubio was supported in part by EP/N509711/1 from the EPSRC MPLS division, grant No 2053152. Varun Kanade and Patrick Rebeschini were supported in part by the Alan Turing Institute under the EPSRC grant EP/N510129/1. The authors acknolwedge support from the AWS Cloud Credits for Research program.	Anandkumar A, 2011, IEEE J SEL AREA COMM, V29, P731, DOI 10.1109/JSAC.2011.110406; [Anonymous], [No title captured]; Arioli M, 2014, NUMER ALGORITHMS, V66, P591, DOI 10.1007/s11075-013-9750-7; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auzinger W., 2011, LECT NOTES; Awerbuch B, 2008, J COMPUT SYST SCI, V74, P1271, DOI 10.1016/j.jcss.2007.08.004; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Braca P, 2008, IEEE T SIGNAL PROCES, V56, P3375, DOI 10.1109/TSP.2008.917855; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Buccapatnam S, 2013, IEEE DECIS CONTR P, P7309, DOI 10.1109/CDC.2013.6761049; Cesa-Bianchi Nicol`o, 2016, C LEARN THEOR, P605; Chakraborty M, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P164; Cross V, 2009, CLIN OPHTHALMOL, V3, P1; Dimakis AG, 2010, P IEEE, V98, P1847, DOI 10.1109/JPROC.2010.2052531; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Gai Y., 2010, P IEEE S NEW FRONT D, P1; Hillel Eshcar, 2013, ADV NEURAL INFORM PR, P854; Horn R.A., 2013, MATRIX ANAL, P321; Joulani P., 2013, ICML, P1453; Kalathil D, 2014, IEEE T INFORM THEORY, V60, P2331, DOI 10.1109/TIT.2014.2302471; Kar S, 2011, IEEE DECIS CONTR P, P1771; Korda N, 2016, PR MACH LEARN RES, V48; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Landgren P, 2016, 2016 EUROPEAN CONTROL CONFERENCE (ECC), P243, DOI 10.1109/ECC.2016.7810293; Landgren P, 2016, IEEE DECIS CONTR P, P167, DOI 10.1109/CDC.2016.7798264; Liu KQ, 2010, IEEE T SIGNAL PROCES, V58, P5667, DOI 10.1109/TSP.2010.2062509; Long TT, 2012, AUTON AGENT MULTI-AG, V25, P352, DOI 10.1007/s10458-011-9179-0; Nayyar Naumaan, 2016, IEEE T CONTROL NETWO; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Perchet V, 2016, ANN STAT, V44, P660, DOI 10.1214/15-AOS1381; Robert Busa-Fekete, 2013, J MACHINE LEARNING R, V2, P1056; Scaman Kevin, 2017, ARXIV170208704; Shahrampour S, 2017, INT CONF ACOUST SPEE, P2786, DOI 10.1109/ICASSP.2017.7952664; Stranders R., 2012, P 11 INT C AUT AG MU, P289; Tekin C., 2012, P IEEE MIL COMM C P IEEE MIL COMM C, P1; Xiao L, 2004, SYST CONTROL LETT, V53, P65, DOI 10.1016/j.sysconle.2004.02.022; Xu J, 2015, IEEE T SIGNAL PROCES, V63, P2225, DOI 10.1109/TSP.2015.2403288	38	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304052
C	Medini, T; Huang, QX; Wang, YQ; Mohan, V; Shrivastava, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Medini, Tharun; Huang, Qixuan; Wang, Yiqiu; Mohan, Vijai; Shrivastava, Anshumali			Extreme Classification in Log Memory using Count-Min Sketch: A Case Study of Amazon Search with 50M Products	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the last decade, it has been shown that many hard AI tasks, especially in NLP, can be naturally modeled as extreme classification problems leading to improved precision. However, such models are prohibitively expensive to train due to the memory blow-up in the last layer. For example, a reasonable softmax layer for the dataset of interest in this paper can easily reach well beyond 100 billion parameters (> 400 GB memory). To alleviate this problem, we present Merged-Average Classifiers via Hashing (MACH), a generic K-classification algorithm where memory provably scales at O(log K) without any strong assumption on the classes. MACH is subtly a count-min sketch structure in disguise, which uses universal hashing to reduce classification with a large number of classes to few embarrassingly parallel and independent classification tasks with a small (constant) number of classes. MACH naturally provides a technique for zero communication model parallelism. We experiment with 6 datasets; some multiclass and some multilabel, and show consistent improvement over respective state-of-the-art baselines. In particular, we train an end-to-end deep classifier on a private product search dataset sampled from Amazon Search Engine with 70 million queries and 49.46 million products. MACH outperforms, by a significant margin, the state-of-the-art extreme classification models deployed on commercial search engines: Parabel and dense embedding models. Our largest model has 6.4 billion parameters and trains in less than 35 hours on a single p3.16x machine. Our training times are 7-10x faster, and our memory footprints are 2-4x smaller than the best baselines. This training time is also significantly lower than the one reported by Google's mixture of experts (MoE) language model on a comparable model size and hardware.	[Medini, Tharun] Rice Univ, Elect & Comp Engn, Houston, TX 77005 USA; [Huang, Qixuan; Shrivastava, Anshumali] Rice Univ, Comp Sci, Houston, TX 77005 USA; [Wang, Yiqiu] MIT, Comp Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Medini, Tharun; Mohan, Vijai] Amazon Search, Palo Alto, CA 94301 USA	Rice University; Rice University; Massachusetts Institute of Technology (MIT)	Medini, T (corresponding author), Rice Univ, Elect & Comp Engn, Houston, TX 77005 USA.; Medini, T (corresponding author), Amazon Search, Palo Alto, CA 94301 USA.	tharun.medini@rice.edu; qh5@rice.edu; yiqiuw@mit.edu; vijaim@amazon.com; anshumali@rice.edu			Amazon Research Award; ONR BRC grant for Randomized Numerical Linear Algebra;  [NSF-1652131];  [NSF-BIGDATA 1838177];  [AFOSR-YIPFA9550-18-10152]	Amazon Research Award; ONR BRC grant for Randomized Numerical Linear Algebra; ; ; 	The work was supported by NSF-1652131, NSF-BIGDATA 1838177, AFOSR-YIPFA9550-18-10152, Amazon Research Award, and ONR BRC grant for Randomized Numerical Linear Algebra.	Abadi M., TENSORFLOW LARGE SCA; Babbar R, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P721, DOI 10.1145/3018661.3018741; Baraniuk RG, 2007, IEEE SIGNAL PROC MAG, V24, P6, DOI 10.1109/MSP.2007.909718; Bhatia Himanshu Jain Yashoteja Prabhu Manik Varma Kush, 2014, EXTREME CLASSIFICATI; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Carter J.L., 1977, P 9 ANN ACM S THEORY, P106, DOI DOI 10.1145/800105.803400; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Choromanska A. E., 2015, ADV NEURAL INFORM PR, P55; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Daume III H., 2016, ARXIV160604988; Dietterich TG, 1994, J ARTIF INTELL RES, V2, P263; GREEN PJ, 1984, J R STAT SOC B, V46, P149; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Kingma D.P, P 3 INT C LEARNING R; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Nigam P, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2876, DOI 10.1145/3292500.3330759; Prabhu Y, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P993, DOI 10.1145/3178876.3185998; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Shazeer N., 2018, ADV NEURAL INFORM PR, P10435; Shazeer N., 2017, ICLR; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tagami Y, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P455, DOI 10.1145/3097983.3097987; Weinberger K., 2009, ANN INT C MACH LEARN, P1113, DOI DOI 10.1145/1553374.1553516; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890	25	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904085
C	Omi, T; Ueda, N; Aihara, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Omi, Takahiro; Ueda, Naonori; Aihara, Kazuyuki			Fully Neural Network based Model for General Temporal Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A temporal point process is a mathematical model for a time series of discrete events, which covers various applications. Recently, recurrent neural network (RNN) based models have been developed for point processes and have been found effective. RNN based models usually assume a specific functional form for the time course of the intensity function of a point process (e.g., exponentially decreasing or increasing with the time since the most recent event). However, such an assumption can restrict the expressive power of the model. We herein propose a novel RNN based model in which the time course of the intensity function is represented in a general manner. In our approach, we first model the integral of the intensity function using a feedforward neural network and then obtain the intensity function as its derivative. This approach enables us to both obtain a flexible model of the intensity function and exactly evaluate the log-likelihood function, which contains the integral of the intensity function, without any numerical approximations. Our model achieves competitive or superior performances compared to the previous state-of-the-art methods for both synthetic and real datasets.	[Omi, Takahiro] Univ Tokyo, RIKEN AIP, Tokyo, Japan; [Ueda, Naonori] RIKEN AIP, NTT Commun Sci Labs, Tokyo, Japan; [Aihara, Kazuyuki] Univ Tokyo, Tokyo, Japan	RIKEN; University of Tokyo; Nippon Telegraph & Telephone Corporation; RIKEN; University of Tokyo	Omi, T (corresponding author), Univ Tokyo, RIKEN AIP, Tokyo, Japan.	takahiro.omi.em@gmail.com; naonori.ueda.fr@hco.ntt.co.jp; aihara@sat.t.u-tokyo.ac.jp			AMED [JP19dm0307009]; Kozo Keikaku Engineering Inc.	AMED; Kozo Keikaku Engineering Inc.	This research was partly supported by AMED under Grant Number JP19dm0307009. T. O. and K. A. are supported by Kozo Keikaku Engineering Inc.	Bacry E, 2015, MARK MICROSTRUCT LIQ, V1, DOI 10.1142/S2382626615500057; Baydin AG, 2018, J MACH LEARN RES, V18; Brown EN, 2002, NEURAL COMPUT, V14, P325, DOI 10.1162/08997660252741149; Chilinski Pawel, 2018, ARXIV181100974; DRUCKER H, 1992, IEEE T NEURAL NETWOR, V3, P991, DOI 10.1109/72.165600; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Goodfellow I., 2016, DEEP LEARNING; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Huang Hengguang, AAAI 2019; Jing H, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P515, DOI 10.1145/3018661.3018719; Kingma D.P, P 3 INT C LEARNING R; Li Shuang, 2018, ADV NEURAL INFORM PR; Lindqvist BH, 2003, TECHNOMETRICS, V45, P31, DOI 10.1198/004017002188618671; Mei H., 2017, ADV NEURAL INFORM PR, P6754; OGATA Y, 1988, J AM STAT ASSOC, V83, P9, DOI 10.1080/01621459.1988.10478560; Omi T, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.012303; Sill J, 1998, ADV NEUR IN, V10, P661; Upadhyay Utkarsh, 2018, ADV NEURAL INFORM PR; Xiao S, 2017, AAAI CONF ARTIF INTE, P1597; Zhou K., 2013, ARTIF INTELL, P641	20	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302015
C	Paternain, S; Chamon, LFO; Calvo-Fullana, M; Ribeiro, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Paternain, Santiago; Chamon, Luiz F. O.; Calvo-Fullana, Miguel; Ribeiro, Alejandro			Constrained Reinforcement Learning Has Zero Duality Gap	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ACTOR-CRITIC ALGORITHM; APPROXIMATION	Autonomous agents must often deal with conflicting requirements, such as completing tasks using the least amount of time/energy, learning multiple tasks, or dealing with multiple opponents. In the context of reinforcement learning (RL), these problems are addressed by (i) designing a reward function that simultaneously describes all requirements or (ii) combining modular value functions that encode them individually. Though effective, these methods have critical downsides. Designing good reward functions that balance different objectives is challenging, especially as the number of objectives grows. Moreover, implicit interference between goals may lead to performance plateaus as they compete for resources, particularly when training on-policy. Similarly, selecting parameters to combine value functions is at least as hard as designing an all-encompassing reward, given that the effect of their values on the overall policy is not straightforward. The later is generally addressed by formulating the conflicting requirements as a constrained RL problem and solved using Primal-Dual methods. These algorithms are in general not guaranteed to converge to the optimal solution since the problem is not convex. This work provides theoretical support to these approaches by establishing that despite its non-convexity, this problem has zero duality gap, i.e., it can be solved exactly in the dual domain, where it becomes convex. Finally, we show this result basically holds if the policy is described by a good parametrization (e.g., neural networks) and we connect this result with primal-dual algorithms present in the literature and we establish the convergence to the optimal solution.	[Paternain, Santiago; Chamon, Luiz F. O.; Calvo-Fullana, Miguel; Ribeiro, Alejandro] Univ Penn, Elect & Syst Engn, Philadelphia, PA 19104 USA	University of Pennsylvania	Paternain, S (corresponding author), Univ Penn, Elect & Syst Engn, Philadelphia, PA 19104 USA.	spater@seas.upenn.edu; luizf@seas.upenn.edu; cfullana@seas.upenn.edu; aribeiro@seas.upenn.edu						Achiam J, 2017, PR MACH LEARN RES, V70; Altman E., 1999, STOCH MODEL SER; [Anonymous], 2018, REINFORCEMENT LEARNI, DOI 10.1016/S1364-6613(99)01331-5; Bertsekas D., 2015, CONVEX OPTIMIZATION; Bhatnagar S, 2012, J OPTIMIZ THEORY APP, V153, P688, DOI 10.1007/s10957-012-9989-5; BORKAR VS, 1988, PROBAB THEORY REL, V78, P583, DOI 10.1007/BF00353877; Borkar VS, 2005, SYST CONTROL LETT, V54, P207, DOI 10.1016/j.sysconle.2004.08.007; Boyd S, 2004, CONVEX OPTIMIZATION; Chow Y., 2015, P 28 INT C NEUR INF, P1522; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dalal G., 2018, ARXIV180108757; Di Castro D., 2012, PREPRINT; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Gu SX, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON MECHATRONICS AND AUTOMATION (ICMA), P387, DOI 10.1109/ICMA.2017.8015848; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Hou C, 2018, IEEE T AUTOM SCI ENG, V15, P1152, DOI 10.1109/TASE.2017.2746348; Konda VR, 2000, ADV NEUR IN, V12, P1008; Koutsopoulos I., 2011, P 2 INT C EN EFF COM, P41; Krokhmal P., 2001, J RISK, V4, P43, DOI [DOI 10.21314/JOR.2002.057, 10.21314/JOR.2002.057]; Leike J, 2017, ARXIV171109883; Lin H., 2018, P ADV NEUR INF PROC, P6169; Lu Z., 2017, ADV NEURAL INFORM PR, P6231; Mania H., 2018, ARXIV PREPRINT ARXIV; Mannor S, 2004, J MACH LEARN RES, V5, P325; Park J, 1991, NEURAL COMPUT, V3, P246, DOI 10.1162/neco.1991.3.2.246; Peng XB, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201311; Rockafellar R. T., 1970, CONVEX ANAL; Schaul Tom, 2019, ARXIV190411455; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sriperumbudur B. K., 2010, P 13 INT C ART INT S, V9, P773; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tamar Aviv, 2013, ARXIV13103697; Tessler C., 2018, ARXIV180511074	35	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307056
C	Schafer, F; Anandkumar, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Schafer, Florian; Anandkumar, Anima			Competitive Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a new algorithm for the numerical computation of Nash equilibria of competitive two-player games. Our method is a natural generalization of gradient descent to the two-player setting where the update is given by the Nash equilibrium of a regularized bilinear local approximation of the underlying game. It avoids oscillatory and divergent behaviors seen in alternating gradient descent. Using numerical experiments and rigorous analysis, we provide a detailed comparison to methods based on optimism and consensus and show that our method avoids making any unnecessary changes to the gradient dynamics while achieving exponential (local) convergence for (locally) convex-concave zero sum games. Convergence and stability properties of our method are robust to strong interactions between the players, without adapting the stepsize, which is not the case with previous methods. In our numerical experiments on non-convex-concave problems, existing methods are prone to divergence and instability due to their sensitivity to interactions among the players, whereas we never observe divergence of our algorithm. The ability to choose larger stepsizes furthermore allows our algorithm to achieve faster convergence, as measured by the number of model evaluations.	[Schafer, Florian; Anandkumar, Anima] CALTECH, Comp & Math Sci, Pasadena, CA 91125 USA	California Institute of Technology	Schafer, F (corresponding author), CALTECH, Comp & Math Sci, Pasadena, CA 91125 USA.	florian.schaefer@caltech.edu; anima@caltech.edu		Schaefer, Florian/0000-0002-4891-0172	Bren endowed chair; Darpa PAI; Raytheon; Microsoft; Google; Adobe; Air Force Office of Scientific Research [FA9550-18-1-0271]; Amazon AWS under the Caltech Amazon Fellows program	Bren endowed chair; Darpa PAI; Raytheon; Microsoft(Microsoft); Google(Google Incorporated); Adobe; Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Amazon AWS under the Caltech Amazon Fellows program	A. Anandkumar is supported in part by Bren endowed chair, Darpa PAI, Raytheon, and Microsoft, Google and Adobe faculty fellowships. F. Schafer gratefully acknowledges support by the Air Force Office of Scientific Research under award number FA9550-18-1-0271 (Games for Computation and Learning) and by Amazon AWS under the Caltech Amazon Fellows program. We thank the reviewers for their constructive feedback, which has helped us improve the paper.	Balduzzi D, 2018, PR MACH LEARN RES, V80; Bertsimas D, 2011, SIAM REV, V53, P464, DOI 10.1137/080734510; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Daskalakis Constantinos, 2017, ARXIV171100141; FACCHINEI F, 2003, SPRINGER SERIES OPER, V2; Foerster J, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P122; Gemp Ian, 2018, ARXIV180801531; Gilpin A, 2007, LECT NOTES COMPUT SC, V4858, P57; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grnarova P., 2017, ARXIV170603269; Hensel M, 2017, ADV NEUR IN, V30; Huber PJ, 2009, WILEY SERIES PROBABI; KORPELEVICH GM, 1977, MATEKON, V13, P35; Lee J.D., 2016, ARXIV160204915, P1246; Li J, 2017, ARXIV170609884; Liang Tengyuan, 2018, ARXIV180206132; Liu B., 2016, P 25 INT JOINT C ART, P4195; Mertikopoulos P., 2019, INT C LEARN REPR; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Metz L., 2016, 161102163 ARXIV; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Pathak Deepak, 2017, P IEEE C COMP VIS PA, P16; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Pfau David, 2016, ARXIV161001945; Rakhlin Alexander, 2013, ONLINE LEARNING PRED; Revels J., 2016, ARXIV; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Shalev-Shwartz S., 2007, ADV NEURAL INFORM PR, P1265; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Wayne G, 2014, NEURAL COMPUT, V26, P2163, DOI 10.1162/NECO_a_00639; Yadav A, 2017, ARXIV PREPRINT ARXIV	35	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307062
C	Shi, B; Du, SS; Su, WJJ; Jordan, MI		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Shi, Bin; Du, Simon S.; Su, Weijie J.; Jordan, Michael I.			Acceleration via Symplectic Discretization of High-Resolution Differential Equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study first-order optimization algorithms obtained by discretizing ordinary differential equations (ODEs) corresponding to Nesterov's accelerated gradient methods (NAGs) and Polyak's heavy-ball method. We consider three discretization schemes: symplectic Euler (S), explicit Euler (E) and implicit Euler (I) schemes. We show that the optimization algorithm generated by applying the symplectic scheme to a high-resolution ODE proposed by Shi et al. [2018] achieves the accelerated rate for minimizing both strongly convex functions and convex functions. On the other hand, the resulting algorithm either fails to achieve acceleration or is impractical when the scheme is implicit, the ODE is low-resolution, or the scheme is explicit.	[Shi, Bin; Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Du, Simon S.] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Su, Weijie J.] Univ Penn, Philadelphia, PA 19104 USA	University of California System; University of California Berkeley; Institute for Advanced Study - USA; University of Pennsylvania	Shi, B (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	binshi@berkeley.edu; ssdu@ias.edu; suw@wharton.upenn.edu; jordan@cs.berkeley.edu						[Anonymous], 1964, COMP MATH MATH PHYS+; Betancourt M., 2018, ARXIV180203653; Bubeck S., 2015, FDN TRENDS MACHINE L; Diakonikolas J., 2017, ARXIV171202485; Hairer E., 2006, SPRINGER SERIES COMP, V31; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Krichene W., 2017, ADV NEURAL INFORM PR, P6796; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Polyak B. T., 1987, INTRO OPTIMIZATION; SHI B, 2018, ARXIV181008907; Su WJ, 2016, J MACH LEARN RES, V17; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wilson A. C., 2016, ARXIV161102635; Zhang J., 2018, ARXIV180500521	17	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305071
C	Vempala, SS; Wibisono, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vempala, Santosh S.; Wibisono, Andre			Rapid Convergence of the Unadjusted Langevin Algorithm: Isoperimetry Suffices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LOGARITHMIC SOBOLEV INEQUALITIES; FUNCTIONAL INEQUALITIES; LOGCONCAVE FUNCTIONS; TRANSPORTATION COST; RENYI DIVERGENCE; ENTROPIES; POINCARE; RATES	We study the Unadjusted Langevin Algorithm (ULA) for sampling from a probability distribution nu = e(-f) on R-n. We prove a convergence guarantee in Kullback-Leibler (KL) divergence assuming nu satisfies log-Sobolev inequality and f has bounded Hessian. Notably, we do not assume convexity or bounds on higher derivatives. We also prove convergence guarantees in Renyi divergence of order q > 1 assuming the limit of ULA satisfies either log-Sobolev or Poincare inequality.	[Vempala, Santosh S.; Wibisono, Andre] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Vempala, SS (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	vempala@gatech.edu; wibisono@gatech.edu			NSF [CCF-1563838, CCF-1717349]	NSF(National Science Foundation (NSF))	The first author was supported in part by NSF awards CCF-1563838 and CCF-1717349. The authors would like to thank Kunal Talwar for explaining the application of Renyi divergence to data privacy. The authors thank Yu Cao, Jianfeng Lu, and Yulong Lu for alerting us to their work [13] on Renyi divergence. The authors also thank Xiang Cheng and Peter Bartlett for helpful comments on an earlier version of this paper.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Applegate David, 1991, P 23 STOC, P156, DOI [10.1145/103418.103439]5, DOI 10.1145/103418.103439]5]; Baez J.C, 2011, RNYI ENTROPY FREE EN; Bai S, 2018, J CRYPTOL, V31, P610, DOI 10.1007/s00145-017-9265-9; Bakry D., 1985, LECT NOTES MATH, P177, DOI DOI 10.1007/BFB0075847; Bakry D, 2008, ELECTRON COMMUN PROB, V13, P60, DOI 10.1214/ECP.v13-1352; Bardet JB, 2018, BERNOULLI, V24, P333, DOI 10.3150/16-BEJ879; Bernton E., 2018, P MACHINE LEARNING R, P1777; Bobkov SG, 2019, ANN PROBAB, V47, P270, DOI 10.1214/18-AOP1261; Bobkov SG, 2001, J MATH PURE APPL, V80, P669, DOI 10.1016/S0021-7824(01)01208-9; Bobkov SG, 1999, J FUNCT ANAL, V163, P1, DOI 10.1006/jfan.1998.3326; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Cao YC, 2018, J APPL TOXICOL, V38, P724, DOI 10.1002/jat.3580; Chafai D, 2004, J MATH KYOTO U, V44, P325, DOI 10.1215/kjm/1250283556; Chafai D, 2010, ANN I H POINCARE-PR, V46, P72, DOI 10.1214/08-AIHP309; Chen Z., 2019, APPROXIMATION RANDOM, V145, P64; Cheng X., 2018, ARXIV180501648; Cheng X., 2018, P 29 INT C ALG LEARN, P186; Courtade Thomas A, 2018, ARXIV180700027; CSISZAR I, 1995, IEEE T INFORM THEORY, V41, P26, DOI 10.1109/18.370121; Dalalyan A., 2019, STOCHASTIC PROCESSES; Dalalyan A. S., 2017, ARXIV170404752; Durmus A., 2017, ARXIV170500166; Durmus  Alain, 2018, ARXIV180209188; Dwivedi R., 2018, C LEARNING THEORY PM, P793; Dwork C., 2016, ARXIV PREPRINT ARXIV; Eberle A, 2019, ANN PROBAB, V47, P1982, DOI 10.1214/18-AOP1299; Gorham J, 2017, PR MACH LEARN RES, V70; GROSS L, 1975, AM J MATH, V97, P1061, DOI 10.2307/2373688; Harremoes P, 2006, PHYSICA A, V365, P57, DOI 10.1016/j.physa.2006.01.012; He Y, 2003, IEEE T SIGNAL PROCES, V51, P1211, DOI 10.1109/TSP.2003.810305; HOLLEY R, 1988, COMMUN MATH PHYS, V115, P553, DOI 10.1007/BF01224127; HOLLEY R, 1987, J STAT PHYS, V46, P1159, DOI 10.1007/BF01011161; Jordan R, 1998, SIAM J MATH ANAL, V29, P1, DOI 10.1137/S0036141096303359; Kannan R, 1997, RANDOM STRUCT ALGOR, V11, P1, DOI 10.1002/(SICI)1098-2418(199708)11:1<1::AID-RSA1>3.0.CO;2-X; Ledoux M, 1999, LECT NOTES MATH, V1709, P120; Lee YT, 2018, ACM S THEORY COMPUT, P1115, DOI 10.1145/3188745.3188774; Li Xuechen, 2019, ARXIV190607868; Lovasz L, 2006, SIAM J COMPUT, V35, P985, DOI 10.1137/S009753970544727X; Lovasz L, 2006, ANN IEEE SYMP FOUND, P57; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; Ma Y, 2019, ARXIV190708990; Mackey MC., 1992, TIMES ARROW ORIGINS; Mangoubi O., 2017, ARXIV170807114; Mangoubi O., 2019, C LEARNING THEORY CO, P2259; Mangoubi Oren, 2018, ADV NEURAL INFORM PR, V31, P6027; Mansour Y., 2009, P 25 C UNC ART INT, P367; Menz G, 2014, ANN PROBAB, V42, P1809, DOI 10.1214/14-AOP908; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Morales D, 2000, STATISTICS, V34, P151, DOI 10.1080/02331880008802324; Otto F, 2000, J FUNCT ANAL, V173, P361, DOI 10.1006/jfan.1999.3557; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Renyi A., 1961, MEASURES ENTROPY INF, VVolume 1; ROTHAUS OS, 1981, J FUNCT ANAL, V42, P102, DOI 10.1016/0022-1236(81)90049-5; Shikata J, 2013, INT C INF THEOR SEC, P103; Talagrand M, 1996, GEOM FUNCT ANAL, V6, P587, DOI 10.1007/BF02249265; van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500; Villani C~edric, 2003, OCLC 908039764; Wang FY, 2016, ANN I H POINCARE-PR, V52, P898, DOI 10.1214/14-AIHP659; Wibisono A., 2018, PROC C LEARNING THEO, Vvol 31, P2093; Zhou Feng-yan, 2016, Chinese Journal of Biologicals, V29, P733	62	10	10	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308015
C	Wang, WY; Xu, QG; Ceylan, D; Mech, R; Neumann, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Weiyue; Xu, Qiangeng; Ceylan, Duygu; Mech, Radomir; Neumann, Ulrich			DISN: Deep Implicit Surface Network for High-quality Single-view 3D Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reconstructing 3D shapes from single-view images has been a long-standing research problem. In this paper, we present DISN, a Deep Implicit Surface Network which can generate a high-quality detail-rich 3D mesh from a 2D image by predicting the underlying signed distance fields. In addition to utilizing global image features, DISN predicts the projected location for each 3D point on the 2D image and extracts local features from the image feature maps. Combining global and local features significantly improves the accuracy of the signed distance field prediction, especially for the detail-rich areas. To the best of our knowledge, DISN is the first method that constantly captures details such as holes and thin structures present in 3D shapes from single-view images. DISN achieves the state-of-the-art single-view reconstruction performance on a variety of shape categories reconstructed from both synthetic and real images. Code is available at https://github.com/laughtervv/DISN. The supplementary can be found at https://xharlie.github.io/images/neurips_2019_supp.pdf.	[Wang, Weiyue; Xu, Qiangeng; Neumann, Ulrich] Univ Southern Calif, Los Angeles, CA 90007 USA; [Ceylan, Duygu; Mech, Radomir] Adobe, San Jose, CA USA	University of Southern California; Adobe Systems Inc.	Wang, WY (corresponding author), Univ Southern Calif, Los Angeles, CA 90007 USA.	weiyuewa@usc.edu; qiangenx@usc.edu; ceylan@adobe.com; rmech@adobe.com; uneumann@usc.edu						[Anonymous], 2018, ARXIV180401654; Chen Z., 2018, ARXIV181202822; Choy C.B., 2016, ECCV; Dai A., 2017, SHAPE COMPLETION USI; Fan H., 2017, CVPR; Girdhar R., 2016, ECCV; Groueix Thibault, 2018, COMPUTER VISION PATT; Hane Christian, 2017, 3DV; Hanrahan P., 2015, SHAPENET INFORM RICH; Insafutdinov Eldar, 2018, NEURIPS; Kato H., 2018, IEEE C COMP 6 VIS PA; Lorensen William E, 1987, ACM SIGGRAPH 87; Mescheder Lars, 2019, CVPR; Niu Chengjie, 2018, CVPR; Park Jeong Joon, 2019, ARXIV190105103; Saito S., 2019, ARXIV190505172; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sin FS, 2013, COMPUT GRAPH FORUM, V32, P36, DOI 10.1111/j.1467-8659.2012.03230.x; Sinha Ayan, 2018, CVPR; Tang Jiapeng, 2019, CVPR; Tatarchenko M., 2017, ICCV; Tatarchenko Maxim, 2019, P IEEE CVF C COMP VI, P3405; Tulsiani S., 2017, IEEE C COMP VIS PATT; Wang P.-S., 2018, ARXIV180907917; Wang W., 2017, ICCV; Wang W., 2019, CVPR; Wu J., 2017, NEURIPS; Wu Jiajun, 2018, NEURIPS; Xu HY, 2014, IEEE SYS MAN CYBERN, P300, DOI 10.1109/SMC.2014.6973924; Yan Xinchen, 2016, NEURIPS; Yang Guorun, 2018, ECCV; Zhong Yiqi, 2019, NEURIPS; Zhou Yi, 2018, ARXIV181207035; Zhu R., 2017, ICCV; Zou C., 2017, ICCV	35	10	10	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300045
C	Xu, XY; Li, SY; Sun, WX; Yin, Q; Yang, MH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Xiangyu; Li Siyao; Sun, Wenxiu; Yin, Qian; Yang, Ming-Hsuan			Quadratic Video Interpolation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Video interpolation is an important problem in computer vision, which helps overcome the temporal limitation of camera sensors. Existing video interpolation methods usually assume uniform motion between consecutive frames and use linear models for interpolation, which cannot well approximate the complex motion in the real world. To address these issues, we propose a quadratic video interpolation method which exploits the acceleration information in videos. This method allows prediction with curvilinear trajectory and variable velocity, and generates more accurate interpolation results. For high-quality frame synthesis, we develop a flow reversal layer to estimate flow fields starting from the unknown target frame to the source frame. In addition, we present techniques for flow refinement. Extensive experiments demonstrate that our approach performs favorably against the existing linear models on a wide variety of video datasets.	[Xu, Xiangyu] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Li Siyao; Sun, Wenxiu] SenseTime Res, Hong Kong, Peoples R China; [Yin, Qian] Beijing Normal Univ, Beijing, Peoples R China; [Yang, Ming-Hsuan] Univ Calif, Merced, CA USA; [Yang, Ming-Hsuan] Google, Mountain View, CA 94043 USA	Carnegie Mellon University; Beijing Normal University; University of California System; University of California Merced; Google Incorporated	Xu, XY (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	xuxiangyu2014@gmail.com; lisiyao1@sensetime.com; sunwenxiu@sensetime.com; yinqian@bnu.edu.cn; mhyang@ucmerced.edu	Yang, Ming-Hsuan/T-9533-2019	Yang, Ming-Hsuan/0000-0003-4848-2304				Anderson R, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980257; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2014, ICLR; Baker S, 2011, INT J COMPUT VISION, V92, P1, DOI 10.1007/s11263-010-0390-2; Bao Wenbo, 2018, ARXIV181008768; BARRON JL, 1994, INT J COMPUT VISION, V12, P43, DOI 10.1007/BF01420984; Brooks Tim, 2019, CVPR; Gan Y., 2018, ECCV; Gonzalez R.C., 2006, DIGITAL IMAGE PROCES; Jiang H., 2018, CVPR; Johnson J, 2016, ECCV; Karargyris A, 2011, IEEE T MED IMAGING, V30, P957, DOI 10.1109/TMI.2010.2098882; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; MCALLISTER DF, 1978, MATH COMPUT, V32, P1154, DOI 10.1090/S0025-5718-1978-0481734-6; Meister Simon, 2018, AAAI; Meyer Simone, 2015, CVPR; Nah S., 2017, CVPR; Niklaus S., 2017, CVPR, V2, P6; Niklaus Simon, 2018, CVPR; Niklaus Simon, 2017, CVPR; Paliwal A., 2018, PYTORCH IMPLEMENTATI; Perazzi F., 2016, CVPR; Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P4364, DOI 10.1109/TIP.2019.2910412; Ren WQ, 2019, IEEE T IMAGE PROCESS, V28, P1895, DOI 10.1109/TIP.2018.2876178; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Shi J., 1994, CVPR; Simonyan Karen, 2015, INT C LEARN REPR; Soomro K., 2012, CRCVTR1201; Su S., 2017, CVPR; Sun Deqing, 2018, CVPR; Wang D., 2017, ICCV; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xu X., 2017, ICCV; Xu Xiangyu, 2017, IEEE T IMAGE PROCESS, V27, P194; Xu Xiangyu, 2019, ARXIV190406903; Xu Xiangyu, 2019, CVPR; Xu XD, 2018, RAMSEY THEORY: UNSOLVED PROBLEMS AND RESULTS, P5, DOI 10.1515/9783110576702-002; Zitnick CL, 2004, ACM T GRAPHIC, V23, P600, DOI 10.1145/1015706.1015766; Zwicker M., 2001, P 28 ANN C COMP GRAP	39	10	10	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301061
C	Yang, C; Ma, XJ; Huang, WB; Sun, FC; Liu, HP; Huang, JZ; Gan, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Chao; Ma, Xiaojian; Huang, Wenbing; Sun, Fuchun; Liu, Huaping; Huang, Junzhou; Gan, Chuang			Imitation Learning from Observations by Minimizing Inverse Dynamics Disagreement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper studies Learning from Observations (LfO) for imitation learning with access to state-only demonstrations. In contrast to Learning from Demonstration (LfD) that involves both action and state supervision, LfO is more practical in leveraging previously inapplicable resources (e.g. videos), yet more challenging due to the incomplete expert guidance. In this paper, we investigate LfO and its difference with LfD in both theoretical and practical perspectives. We first prove that the gap between LfD and LfO actually lies in the disagreement of inverse dynamics models between the imitator and the expert, if following the modeling approach of GAIL [15]. More importantly, the upper bound of this gap is revealed by a negative causal entropy which can be minimized in a model-free way. We term our method as Inverse-Dynamics-Disagreement-Minimization (IDDM) which enhances the conventional LfO method through further bridging the gap to LfD. Considerable empirical results on challenging benchmarks indicate that our method attains consistent improvements over other LfO counterparts.	[Yang, Chao; Ma, Xiaojian; Huang, Wenbing; Sun, Fuchun; Liu, Huaping] Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China; [Ma, Xiaojian] Univ Calif Los Angeles, Dept Comp Sci, Ctr Vis Cognit Learning & Auton, Los Angeles, CA 90024 USA; [Huang, Junzhou] Tencent AI Lab, Bellevue, WA USA; [Gan, Chuang] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA	Tsinghua University; University of California System; University of California Los Angeles; Massachusetts Institute of Technology (MIT)	Sun, FC (corresponding author), Tsinghua Univ, Dept Comp Sci & Technol, State Key Lab Intelligent Technol & Syst, Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.	yangchao18@mails.tsinghua.edu.cn; maxiaojian@ucla.edu; hwenbing@126.com; fcsun@tsinghua.edu.cn	Huang, Wenbing/AAI-7943-2021; Huang, Wenbing/AHB-1846-2022	Huang, Wenbing/0000-0002-2566-4159; Huang, Wenbing/0000-0002-2566-4159	National Science and Technology Major Project of the Ministry of Science and Technology of China [2018AAA0102900]; National Science Foundation of China [91848206]; National Science Foundation of China (NSFC); German Research Foundation (DFG); NSFC [61621136008/DFG TRR-169]	National Science and Technology Major Project of the Ministry of Science and Technology of China; National Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC)); German Research Foundation (DFG)(German Research Foundation (DFG)); NSFC(National Natural Science Foundation of China (NSFC))	This research was funded by National Science and Technology Major Project of the Ministry of Science and Technology of China (No.2018AAA0102900). It was also partially supported by National Science Foundation of China (Grant No.91848206), National Science Foundation of China (NSFC) and the German Research Foundation (DFG) in project Cross Modal Learning, NSFC 61621136008/DFG TRR-169. We would like to thank Mingxuan Jing and Dr. Boqing Gong for the insightful discussions and the anonymous reviewers for the constructive feedback.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Atkeson C. G., 1997, P 14 INT C MACHINE L, V97, P12, DOI [10.1007/springerreference_, DOI 10.1007/978-3-7091-6874-5_4]; Belghazi MI, 2018, PR MACH LEARN RES, V80; Bentivegna DC, 2004, ROBOT AUTON SYST, V47, P163, DOI 10.1016/j.robot.2004.03.010; Bojarski Mariusz, 2016, arXiv; Brockman G., 2016, OPENAI GYM; Duan Y, 2016, PR MACH LEARN RES, V48; Nguyen-Tuong D, 2011, COGN PROCESS, V12, P319, DOI 10.1007/s10339-011-0404-1; Fu Justin, 2018, INT C LEARN REPR; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Haarnoja T, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV; Hjelm R Devon, 2019, INT C LEARN REPR; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Jing Mingxuan, 2019, AAAI C ART INT AAAI; Kang BY, 2018, PR MACH LEARN RES, V80; Kim Beomjoon, 2013, ROBOTICS SCI SYSTEMS; Kim KE, 2018, AAAI CONF ARTIF INTE, P3415; Kumar Rithesh, 2019, ARXIV190108508; Liu Yuxuan, 2018, 2018 IEEE INT C ROB; Mnih V, 2016, PR MACH LEARN RES, V48; Nowozin S, 2016, ADV NEUR IN, V29; PENG XB, 2018, ACM T GRAPHICS TOG; Peng Xue Bin, 2019, INT C LEARN REPR ICL; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Ross S., 2012, ICML; Ross S., 2010, PROC 13 INT C ARTIF, V9, P661; Schaal S, 1997, ADV NEUR IN, V9, P1040; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Siciliano B., 2008, SPRINGER HDB ROBOTIC, DOI [10.1007/978-3-540-30301-5, DOI 10.1007/978-3-540-30301-5]; Spong Mark W, 1990, IEEE T AUTOMATIC CON; Stadie Bradly C., 2017, INT C LEARN REPR ICL; Sun Wen, 2019, INT C MACH LEARN ICM; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Torabi F, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4950; Torabi Faraz, 2018, ARXIV180706158; Torabi Faraz, 2019, INT JOINT C ART INT	41	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300022
C	Yang, ZR; Chen, YX; Hong, MY; Wang, ZR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Zhuoran; Chen, Yongxin; Hong, Mingyi; Wang, Zhaoran			Provably Global Convergence of Actor-Critic: A Case for Linear Quadratic Regulator with Ergodic Cost	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STOCHASTIC-APPROXIMATION; POLICY; ALGORITHMS	Despite the empirical success of the actor-critic algorithm, its theoretical understanding lags behind. In a broader context, actor-critic can be viewed as an online alternating update algorithm for bilevel optimization, whose convergence is known to be fragile. To understand the instability of actor-critic, we focus on its application to linear quadratic regulators, a simple yet fundamental setting of reinforcement learning. We establish a nonasymptotic convergence analysis of actor-critic in this setting. In particular, we prove that actor-critic finds a globally optimal pair of actor (policy) and critic (action-value function) at a linear rate of convergence. Our analysis may serve as a preliminary step towards a complete theoretical understanding of bilevel optimization with nonconvex subproblems, which is NP-hard in the worst case and is often solved using heuristics.	[Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA; [Chen, Yongxin] Georgia Inst Technol, Atlanta, GA 30332 USA; [Hong, Mingyi] Univ Minnesota, Minneapolis, MN 55455 USA; [Wang, Zhaoran] Northwestern Univ, Evanston, IL 60208 USA	Princeton University; University System of Georgia; Georgia Institute of Technology; University of Minnesota System; University of Minnesota Twin Cities; Northwestern University	Yang, ZR (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	zy6@princeton.edu; yongchen@gatech.edu; mhong@umn.edu; zhaoran.wang@northwestern.edu	Hong, Mingyi/H-6274-2013; Wang, Zhaoran/P-7113-2018					Agarwal A., 2019, ARXIV190800261; Alizadeh F, 1998, SIAM J OPTIMIZ, V8, P746, DOI 10.1137/S1052623496304700; Anderson B.D.O., 2007, OPTIMAL CONTROL LINE; [Anonymous], 2019, UNDERSTAND DYNAMICS; Bard J. F., 1998, PRACTICAL BILEVEL OP; Bertsekas D. P., 1995, DYNAMIC PROGRAMMING; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Borkar VS, 1997, SYST CONTROL LETT, V29, P291, DOI 10.1016/S0167-6911(97)90015-3; Borkar VS, 1997, SADHANA-ACAD P ENG S, V22, P525, DOI 10.1007/BF02745577; Bradtke S. J., 1993, ADV NEURAL INFORM PR; Bradtke S. J., 1994, AM CONTR C, V3; Cai Q., 2019, ARXIV190103674; Chen X., 2018, P 6 INT C LEARN REPR; Dai B, 2017, INT C ART INT STAT; Dai BZ, 2018, 2018 11TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING, BIOMEDICAL ENGINEERING AND INFORMATICS (CISP-BMEI 2018); Dean S., 2018, ARXIV180910121; Dean S., 2018, ARXIV180509388; Dempe S., 2002, FDN BILEVEL PROGRAMM; Du S. S., 2018, ARXIV180201504; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grondman I, 2012, IEEE T SYST MAN CY C, V42, P1291, DOI 10.1109/TSMCC.2012.2218595; Hardt M, 2018, J MACH LEARN RES, V19; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Horn R.A., 2013, TOPICS MATRIX ANAL, DOI DOI 10.1017/CBO9780511840371; Islam R., 2017, ICML 2017 REPR MACH; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kirk DE., 1970, OPTIMAL CONTROL THEO; Konda VR, 2000, ADV NEUR IN, V12, P1008; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Q., 2018, ARXIV PREPRINT ARXIV; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Liu BY, 2019, ADV NEUR IN, V32; Magnus JR, 1978, STAT NEERL, V32, P201, DOI 10.1111/j.1467-9574.1978.tb01399.x; Malik D., 2018, ARXIV181208305; Meyn SP, 1997, IEEE T AUTOMAT CONTR, V42, P1663, DOI 10.1109/9.650016; Mnih V, 2016, PR MACH LEARN RES, V48; NAGAR AL, 1959, ECONOMETRICA, V27, P575, DOI 10.2307/1909352; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Olsder T., 1999, DYNAMIC NONCOOPERATI; Pfau David, 2016, ARXIV161001945; Powell Warren B., 2011, Journal of Control Theory and Applications, V9, P336, DOI 10.1007/s11768-011-0313-y; Recht Benjamin, 2018, ARXIV180609460; Rudelson M, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2865; Sanjabi M., 2018, ARXIV181202878; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Shani L., 2019, ARXIV190902769; Silver D, 2014, PR MACH LEARN RES, V32; Simchowitz M., 2018, C LEARNING THEORY, P439; Sinha A, 2017, ARXIV171010571; Sinha A, 2018, IEEE T EVOLUT COMPUT, V22, P276, DOI 10.1109/TEVC.2017.2712906; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, PROC ADV NEURAL INF, P1609; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tu S, 2019, C LEARN THEOR COLT, P3036; Tu Stephen, 2017, ARXIV171208642; Wang L., 2019, ARXIV190901150; Wang YB, 2017, ADV NEUR IN, V30; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yang Z., 2018, CONVERGENT REI UNPUB; Zhang K., 2019, ARXIV190608383; Zhou K., 1996, ROBUST OPTIMAL CONTR	76	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308038
C	Bambach, S; Crandall, DJ; Smith, LB; Yu, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bambach, Sven; Crandall, David J.; Smith, Linda B.; Yu, Chen			Toddler-Inspired Visual Object Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Real-world learning systems have practical limitations on the quality and quantity of the training datasets that they can collect and consider. How should a system go about choosing a subset of the possible training examples that still allows for learning accurate, generalizable models? To help address this question, we draw inspiration from a highly efficient practical learning system: the human child. Using head-mounted cameras, eye gaze trackers, and a model of foveated vision, we collected first-person (egocentric) images that represent a highly accurate approximation of the "training data" that toddler's visual systems collect in everyday, naturalistic learning contexts. We used state-of-the-art computer vision learning models (convolutional neural networks) to help characterize the structure of these data, and found that child data produce significantly better object models than egocentric data experienced by adults in exactly the same environment. By using the CNNs as a modeling tool to investigate the properties of the child data that may enable this rapid learning, we found that child data exhibit a unique combination of quality and diversity, with not only many similar large, high-quality object views but also a greater number and diversity of rare views. This novel methodology of analyzing the visual "training data" used by children may not only reveal insights to improve machine learning, but also may suggest new experimental tools to better understand infant learning in developmental psychology.	[Bambach, Sven; Crandall, David J.] Indiana Univ, Sch Informat Comp & Engn, Bloomington, IN 47405 USA; [Smith, Linda B.; Yu, Chen] Indiana Univ, Dept Psychol & Brain Sci, Bloomington, IN 47405 USA	Indiana University System; Indiana University Bloomington; Indiana University System; Indiana University Bloomington	Bambach, S (corresponding author), Indiana Univ, Sch Informat Comp & Engn, Bloomington, IN 47405 USA.	sbambach@iu.edu; djcran@iu.edu; smith4@iu.edu; chenyu@iu.edu			National Science Foundation [CAREER IIS-1253549]; National Institutes of Health [R01 HD074601, R01 HD093792]; IU Office of the Vice Provost for Research, the College of Arts and Sciences, and the School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children"	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); IU Office of the Vice Provost for Research, the College of Arts and Sciences, and the School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children"	This work was supported by the National Science Foundation (CAREER IIS-1253549) and the National Institutes of Health (R01 HD074601, R01 HD093792), as well as the IU Office of the Vice Provost for Research, the College of Arts and Sciences, and the School of Informatics, Computing, and Engineering through the Emerging Areas of Research Project "Learning: Brains, Machines, and Children." We would like to thank Drew Abney, Esther Chen, Steven Elmlinger, Seth Foster, Lauren Slone, Catalina Suarez, Charlene Tay, and Yayun Zhang for helping with the collection of the first-person toy play dataset.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bambach  Sven, 2016, ANN C COGN SCI SOC C; Cevallos  Alfonso, 2017, P 28 ANN ACM SIAM S; Charikar  Moses, 1999, P 31 ANN ACM S THEOR; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Clerkin EM, 2017, PHILOS T R SOC B, V372, DOI 10.1098/rstb.2016.0055; Elhamifar E, 2016, IEEE T PATTERN ANAL, V38, P2182, DOI 10.1109/TPAMI.2015.2511748; Frank MC, 2017, J CHILD LANG, V44, P677, DOI 10.1017/S0305000916000209; Fu YF, 2013, IEEE T CYBERNETICS, V43, P464, DOI 10.1109/TSMCB.2012.2209177; Gathercole  Chris, 1994, P INT C EV COMP 3 C; Greene MR, 2016, J EXP PSYCHOL GEN, V145, P82, DOI 10.1037/xge0000129; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881; Lin  Hui, 2009, 10 ANN C INT SPEECH; Mahajan D, 2018, LECT NOTES COMPUT SC, V11206, P185, DOI 10.1007/978-3-030-01216-8_12; Nguyen H.T., 2004, P 21 INT C MACH LEAR; Nosofsky Robert M, 2018, PSYCHONOMIC B REV, P1; Perry Jeffrey S., 2002, HUMAN VISION ELECT I; Peterson J. C., 2016, P 38 C COGNITIVE SCI; Plutowski  Mark, 1994, ADV NEURAL INFORM PR, P1135; Prasad  Adarsh, 2014, ARXIV14111752; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ritter S, 2017, PR MACH LEARN RES, V70; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Smith LB, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02124; Torralba A, 2003, INT J COMPUT VISION, V53, P169, DOI 10.1023/A:1023052124951; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang  Panqu, 2013, ANN C COGN SCI SOC C, V35; Wei K, 2015, PR MACH LEARN RES, V37, P1954; Yu C, 2017, CHILD DEV, V88, P2060, DOI 10.1111/cdev.12730; Yu C, 2016, CURR BIOL, V26, P1235, DOI 10.1016/j.cub.2016.03.026; Yu C, 2012, COGNITION, V125, P244, DOI 10.1016/j.cognition.2012.06.016; Zhu XJ, 2015, AAAI CONF ARTIF INTE, P4083	35	10	10	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301021
C	Belkin, M; Hsu, D; Mitra, PP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Belkin, Mikhail; Hsu, Daniel; Mitra, Partha P.			Overfitting or perfect fitting? Risk bounds for classification and regression rules that interpolate	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				REGULARIZATION; RATES	Many modern machine learning models are trained to achieve zero or near-zero training error in order to obtain near-optimal (but non-zero) test error. This phenomenon of strong generalization performance for "overfitted" /interpolated classifiers appears to be ubiquitous in high-dimensional data, having been observed in deep networks, kernel machines, boosting and random forests. Their performance is consistently robust even when the data contain large amounts of label noise. Very little theory is available to explain these observations. The vast majority of theoretical analyses of generalization allows for interpolation only when there is little or no label noise. This paper takes a step toward a theoretical foundation for interpolated classifiers by analyzing local interpolating schemes, including geometric simplicial interpolation algorithm and singularly weighted k-nearest neighbor schemes. Consistency or near-consistency is proved for these schemes in classification and regression problems. Moreover, the nearest neighbor schemes exhibit optimal rates under some standard statistical assumptions. Finally, this paper suggests a way to explain the phenomenon of adversarial examples, which are seemingly ubiquitous in modern machine learning, and also discusses some connections to kernel machines and random forests in the interpolated regime.	[Belkin, Mikhail] Ohio State Univ, Columbus, OH 43210 USA; [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA; [Mitra, Partha P.] Cold Spring Harbor Lab, POB 100, Cold Spring Harbor, NY 11724 USA	University System of Ohio; Ohio State University; Columbia University; Cold Spring Harbor Laboratory	Belkin, M (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.	mbelkin@cse.ohio-state.edu; djhsu@cs.columbia.edu; mitra@cshl.edu			NSF [CCF-1740833, DMR-1534910]; Crick-Clay Professorship (CSHL); H N Mahabala Chair (IITM)	NSF(National Science Foundation (NSF)); Crick-Clay Professorship (CSHL); H N Mahabala Chair (IITM)	We would like to thank Raef Bassily, Luis Rademacher, Sasha Rakhlin, and Yusu Wang for conversations and valuable comments. We acknowledge funding from NSF. DH acknowledges support from NSF grants CCF-1740833 and DMR-1534910. PPM acknowledges support from the Crick-Clay Professorship (CSHL) and H N Mahabala Chair (IITM). This work grew out of discussions originating at the Simons Institute for the Theory of Computing in 2017, and we thank the Institute for the hospitality. PPM and MB thank ICTS (Bangalore) for their hospitality at the 2017 workshop on Statistical Physics Methods in Machine Learning.	AFFENTRANGER F, 1991, DISCRETE COMPUT GEOM, V6, P291, DOI 10.1007/BF02574691; Amenta N, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1106; Anthony M., 1995, Computational Learning Theory. Second European Conference, EuroCOLT '95. Proceedings, P211; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Audibert JY, 2007, ANN STAT, V35, P608, DOI 10.1214/009053606000001217; Bartlett Peter, 2017, NIPS; Bartlett PL, 1996, J COMPUT SYST SCI, V52, P434, DOI 10.1006/jcss.1996.0033; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Belkin M, 2004, LECT NOTES COMPUT SC, V3120, P624, DOI 10.1007/978-3-540-27819-1_43; BELKIN M., 2018, DOES DATA INTERPOLAT; Belkin Mikhail, 2018, ARXIV180605161; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Cutler A., 2001, COMPUTING SCI STAT, V33, P490; Davies S, 1997, ADV NEUR IN, V9, P1005; Devroye L, 1998, J MULTIVARIATE ANAL, V65, P209, DOI 10.1006/jmva.1997.1725; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Fukuda Komei, 2004, TECHNICAL REPORT; Golowich Noah, 2018, 31 ANN C LEARN THEOR; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gyorfi L., 2002, SPRINGER SERIES STAT; Halton John H, 1991, TR91002 U N CAR CHAP; Koltchinskii V, 2002, ANN STAT, V30, P1; Liang T, 2017, ABS171101530 CORR; Madry Aleksander, 2018, ICLR; Mammen E, 1999, ANN STAT, V27, P1808; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Neyshabur Behnam, 2018, INT C LEARN REPR; Salakhutdinov Ruslan, 2017, DEEP LEARNING TUTORI; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Schapire Robert E, 1998, ANN STAT, V26; SCHNEIDER R., 2004, HDB DISCRETE COMPUTA, P255, DOI [10.1201/9781420035315.ch36, DOI 10.1201/9781420035315.CH36]; Scholkopf B., 2001, LEARNING KERNELS SUP; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shepard D., 1968, P 1968 23 ACM NAT C, DOI DOI 10.1016/j.enggeo.2007.05.007; Su J., 2017, ARXIV171008864; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Tsybakov AB, 2004, ANN STAT, V32, P135; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Wang YZ, 2018, PR MACH LEARN RES, V80; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; Wyner AJ, 2017, J MACH LEARN RES, V18, P1; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zeng HQ, 2017, PROC INT CONF RECON; Zhu Xiaojin., 2003, P ICLR, P912	54	10	10	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302032
C	Birdal, T; Simsekli, U; Eken, MO; Ilic, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Birdal, Tolga; Simsekli, Umut; Eken, M. Onur; Ilic, Slobodan			Bayesian Pose Graph Optimization via Bingham Distributions and Tempered Geodesic MCMC	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CAMERA MOTION	We introduce Tempered Geodesic Markov Chain Monte Carlo (TG-MCMC) algorithm for initializing pose graph optimization problems, arising in various scenarios such as SFM (structure from motion) or SLAM (simultaneous localization and mapping). TG-MCMC is first of its kind as it unites global non-convex optimization on the spherical manifold of quaternions with posterior sampling, in order to provide both reliable initial poses and uncertainty estimates that are informative about the quality of solutions. We devise theoretical convergence guarantees and extensively evaluate our method on synthetic and real benchmarks. Besides its elegance in formulation and theory, we show that our method is robust to missing data, noise and the estimated uncertainties capture intuitive properties of the data.	[Birdal, Tolga; Eken, M. Onur; Ilic, Slobodan] Tech Univ Munich, CAMP Chair, D-85748 Munich, Germany; [Birdal, Tolga; Eken, M. Onur; Ilic, Slobodan] Siemens AG, D-81739 Munich, Germany; [Simsekli, Umut] Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France	Technical University of Munich; Siemens AG; Siemens Germany; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Birdal, T (corresponding author), Tech Univ Munich, CAMP Chair, D-85748 Munich, Germany.; Birdal, T (corresponding author), Siemens AG, D-81739 Munich, Germany.		Birdal, Tolga/H-9173-2019	Birdal, Tolga/0000-0001-7915-7964; Ilic, Slobodan/0000-0002-3413-1936	French National Research Agency (ANR) as a part of the FBIMATRIX project [ANR-16-CE23-0014]; industrial chair Machine Learning for Big Data from Telecom ParisTech	French National Research Agency (ANR) as a part of the FBIMATRIX project(French National Research Agency (ANR)); industrial chair Machine Learning for Big Data from Telecom ParisTech	We would like to thank Robert M. Gower and Francois Portier for fruitful discussions and Hans Peschke for his feedback and efforts in verifying the correctness of our descriptions. We thank Antonio Vargas of the Mathematics-StackExchange for providing the reference on inequalities for generalized hypergeometric functions. This work is partly supported by the French National Research Agency (ANR) as a part of the FBIMATRIX project (ANR-16-CE23-0014) and by the industrial chair Machine Learning for Big Data from Telecom ParisTech.	Arrigoni F., 2014, 2014 2nd International Conference on 3D Vision (3DV). Proceedings, P491, DOI 10.1109/3DV.2014.48; Arrigoni F., 2015, ARXIV150608765; Arrigoni F, 2016, INT CONF 3D VISION, P546, DOI 10.1109/3DV.2016.64; BINGHAM C, 1974, ANN STAT, V2, P1201, DOI 10.1214/aos/1176342874; Birdal T., 2016, IEEE WACV 2016, P1; Birdal T, 2017, IEEE I CONF COMP VIS, P133, DOI 10.1109/ICCV.2017.24; Birdal Tolga, 2016, IEEE INT C 3DVISION; Briales Jesus, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P5134, DOI 10.1109/ICRA.2017.7989600; Briales J, 2016, 2016 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS (IROS 2016), P4630, DOI 10.1109/IROS.2016.7759681; Busam B, 2017, IEEE INT CONF COMP V, P2436, DOI 10.1109/ICCVW.2017.287; Byrne S, 2013, SCAND J STAT, V40, P825, DOI 10.1111/sjos.12036; Carlone L, 2018, IEEE ROBOT AUTOM LET, V3, P1160, DOI 10.1109/LRA.2018.2793352; Carlone L, 2015, IEEE INT CONF ROBOT, P4597, DOI 10.1109/ICRA.2015.7139836; Chatterjee A, 2018, IEEE T PATTERN ANAL, V40, P958, DOI 10.1109/TPAMI.2017.2693984; Chatterjee A, 2013, IEEE I CONF COMP VIS, P521, DOI 10.1109/ICCV.2013.70; CHEN C., 2015, ADV NEURAL INFORM PR, P2269; Chen CY, 2016, JMLR WORKSH CONF PRO, V51, P1051; Dalalyan A. S., 2017, ARXIV170404752; Diaconis P., 2013, ADV MODERN STAT THEO, P102; DURMUS A., 2016, ADV NEURAL INFORM PR, P2047; Eriksson A, 2018, PROC CVPR IEEE, P127, DOI 10.1109/CVPR.2018.00021; Fredriksson J., 2012, P AS C COMP VIS, P245; Gao X., 2018, ARXIV180904618; GELFAND SB, 1991, SIAM J CONTROL OPTIM, V29, P999, DOI 10.1137/0329055; Glover J, 2014, IEEE INT CONF ROBOT, P4133, DOI 10.1109/ICRA.2014.6907460; Glover J, 2012, ROBOTICS: SCIENCE AND SYSTEMS VII, P97; Govindu VM, 2004, PROC CVPR IEEE, P684; Govindu VM, 2001, PROC CVPR IEEE, P218; Haarbach A, 2018, INT CONF 3D VISION, P381, DOI 10.1109/3DV.2018.00051; Hartley R., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3041, DOI 10.1109/CVPR.2011.5995745; Hartley R, 2013, INT J COMPUT VISION, V103, P267, DOI 10.1007/s11263-012-0601-0; Huber DF, 2003, IMAGE VISION COMPUT, V21, P637, DOI 10.1016/S0262-8856(03)00060-X; HWANG CR, 1980, ANN PROBAB, V8, P1177, DOI 10.1214/aop/1176994579; Knapitsch Arno, 2017, ACM Transactions on Graphics, V36, DOI 10.1145/3072959.3073599; Kummerle Rainer, 2011, IEEE International Conference on Robotics and Automation, P3607; Kurz G, 2013, 2013 16TH INTERNATIONAL CONFERENCE ON INFORMATION FUSION (FUSION), P1487; Lepetit Vincent, 2005, Foundations and Trends in Computer Graphics and Vision, V1, P1, DOI 10.1561/0600000001; Liu C, 2016, ADV NEURAL INFORM PR, P3009; Liu YY, 2017, ADV NEUR IN, V30; Ma Y.A., 2015, ARXIV150604696, P2917; Matthews Charles, 2015, MOL DYNAMICS DETERMI, V39; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Raginsky M., 2017, P MACHINE LEARNING R, P1674; Rosen D., 2017, MITCSAILTR2017002; Simsekli U, 2018, PR MACH LEARN RES, V80; Simsekli U, 2017, PR MACH LEARN RES, V70; Simsekli U, 2016, PR MACH LEARN RES, V48; Steenrod N. E., 1951, TOPOLOGY FIBRE BUNDL, V14; Strecha C, 2008, PROC CVPR IEEE, P2838; Torsello A., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2441, DOI 10.1109/CVPR.2011.5995565; Triggs B., 2000, Vision Algorithms: Theory and Practice. International Workshop on Vision Algorithms. Proceedings (Lecture Notes in Computer Science Vol. 1883), P298; TRON R., 2016, P IEEE C COMP VIS PA, P77; Tron R, 2014, LECT NOTES COMPUT SC, V8693, P804, DOI 10.1007/978-3-319-10602-1_52; Tzen B., 2018, P 2018 C LEARN THEOR, P857; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wilson K, 2014, LECT NOTES COMPUT SC, V8691, P61, DOI 10.1007/978-3-319-10578-9_5; Wilson K, 2016, LECT NOTES COMPUT SC, V9911, P255, DOI 10.1007/978-3-319-46478-7_16; Wu Changchang, 2011, VISUALSFM VISUAL STR, P1; Ye NY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3019; Zhang Hongyi, 2016, PROC C LEARN THEORY; Zhang Y., 2017, P 2017 C LEARN THEOR, V65, P1	63	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300029
C	Brosse, N; Moulines, E; Durmus, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Brosse, Nicolas; Moulines, Eric; Durmus, Alain			The promises and pitfalls of Stochastic Gradient Langevin Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Stochastic Gradient Langevin Dynamics (SGLD) has emerged as a key MCMC algorithm for Bayesian learning from large scale datasets. While SGLD with decreasing step sizes converges weakly to the posterior distribution, the algorithm is often used with a constant step size in practice and has demonstrated successes in machine learning tasks. The current practice is to set the step size inversely proportional to N where N is the number of training samples. As N becomes large, we show that the SGLD algorithm has an invariant probability measure which significantly departs from the target posterior and behaves like Stochastic Gradient Descent (SGD). This difference is inherently due to the high variance of the stochastic gradients. Several strategies have been suggested to reduce this effect; among them, SGLD Fixed Point (SGLDFP) uses carefully designed control variates to reduce the variance of the stochastic gradients. We show that SGLDFP gives approximate samples from the posterior distribution, with an accuracy comparable to the Langevin Monte Carlo (LMC) algorithm for a computational cost sublinear in the number of data points. We provide a detailed analysis of the Wasserstein distances between LMC, SGLD, SGLDFP and SGD and explicit expressions of the means and covariance matrices of their invariant distributions. Our findings are supported by limited numerical experiments.	[Brosse, Nicolas; Moulines, Eric] Ecole Polytech, UMR 7641, Ctr Math Appl, Palaiseau, France; [Durmus, Alain] Ecole Normale Super CMLA, 61 Av President Wilson, F-94235 Cachan, France	Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Brosse, N (corresponding author), Ecole Polytech, UMR 7641, Ctr Math Appl, Palaiseau, France.	nicolas.brosse@polytechnique.edu; eric.moulines@polytechnique.edu; alain.durmus@cmla.ens-cachan.fr		Brosse, Nicolas/0000-0002-6771-0950				Ahn S., 2012, P 29 INT C MACH LEAR; Ahn S, 2014, PR MACH LEARN RES, V32, P1044; Baker J., 2017, 170605439 ARXIV; Bardenet R, 2017, J MACH LEARN RES, V18, P1; Chatterji N. S., 2018, 180205431 ARXIV; CHEN C., 2015, ADV NEURAL INFORM PR, P2269; Chen C., 2017, 170901180 ARXIV; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Dalalyan A. S., 2017, ARXIV170404752; Dalalyan A. S., 2017, 171000095 ARXIV; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Dubey Avinava, 2016, Adv Neural Inf Process Syst, V29, P1154; Durmus A., 2016, 160501559 ARXIV; Durmus A, 2017, ANN APPL PROBAB, V27, P1551, DOI 10.1214/16-AAP1238; GRENANDER U, 1994, J R STAT SOC B, V56, P549; GRENANDER U, 1983, TUTORIAL PATTERN THE; Hasenclever L, 2017, J MACH LEARN RES, V18; Jones E., 2001, SCIPY OPEN SOURCE SC; Karatzas I., 1987, BROWNIAN MOTION STOC; Korattikara A, 2014, PR MACH LEARN RES, V32; Li WZ, 2016, JMLR WORKSH CONF PRO, V51, P723; Ma Y.A., 2015, ARXIV150604696, P2917; Nagapetyan T., 2017, 170602692 ARXIV; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Neven, 2014, ADV NEURAL INFORM PR, P3203; Patterson S., 2013, P 26 INT C NEUR INF, P3102; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Sato I, 2014, PR MACH LEARN RES, V32, P982; Teh Y. W., 2016, J MACH LEARN RES, V17, P193; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	33	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002078
C	Dennis, DK; Pabbaraju, C; Simhadri, HV; Jain, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dennis, Don Kurian; Pabbaraju, Chirag; Simhadri, Harsha Vardhan; Jain, Prateek			Multiple Instance Learning for Efficient Sequential Data Classification on Resource-constrained Devices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the problem of fast and efficient classification of sequential data (such as time-series) on tiny devices, which is critical for various IoT related applications like audio keyword detection or gesture detection. Such tasks are cast as a standard classification task by sliding windows over the data stream to construct data points. Deploying such classification modules on tiny devices is challenging as predictions over sliding windows of data need to be invoked continuously at a high frequency. Each such predictor instance in itself is expensive as it evaluates large models over long windows of data. In this paper, we address this challenge by exploiting the following two observations about classification tasks arising in typical IoT related applications: (a) the "signature" of a particular class (e.g. an audio keyword) typically occupies a small fraction of the overall data, and (b) class signatures tend to be discernible early on in the data. We propose a method, EMI-RNN, that exploits these observations by using a multiple instance learning formulation along with an early prediction technique to learn a model that achieves better accuracy compared to baseline models, while simultaneously reducing computation by a large fraction. For instance, on a gesture detection benchmark [26], EMI-RNN requires 72x less computation than standard LSTM while improving accuracy by 1%. This enables us to deploy such models for continuous real-time prediction on devices as small as a Raspberry Pi0 and Arduino variants, a task that the baseline LSTM could not achieve. Finally, we also provide an analysis of our multiple instance learning algorithm in a simple setting and show that the proposed algorithm converges to the global optima at a linear rate, one of the first such result in this domain. The code for EMI-RNN is available at [14].	[Dennis, Don Kurian; Pabbaraju, Chirag; Simhadri, Harsha Vardhan; Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India		Dennis, DK (corresponding author), Microsoft Res, Bengaluru, Karnataka, India.	t-dodenn@microsoft.com; t-chpab@microsoft.com; harshasi@microsoft.com; prajain@microsoft.com						Amores J, 2013, ARTIF INTELL, V201, P81, DOI 10.1016/j.artint.2013.06.003; Anguita D., 2013, ESANN, V3, P3, DOI DOI 10.1007/978-3-642-40728-4_54; Arjovsky M, 2016, PR MACH LEARN RES, V48; Auer Peter, 1997, P 29 ANN ACM S THEOR, P314; Bello J.P., 2018, COMPUTATIONAL ANAL S, P373, DOI [10.1007/978-3-319-63450-0, DOI 10.1007/978-3-319-63450-0, 10.1007/978-3-319-63450-0_13, DOI 10.1007/978-3-319-63450-0_13]; Bengio Y., 2014, ARXIV14061078; Chang Shih-Fu, 2018, INT C LEAR REPR; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2014, ARXIV14123555; Collins J., 2016, ARXIV PREPRINT ARXIV; D'Ausilio A, 2012, BEHAV RES METHODS, V44, P305, DOI 10.3758/s13428-011-0163-z; Dachraoui A, 2015, LECT NOTES ARTIF INT, V9284, P433, DOI 10.1007/978-3-319-23528-8_27; Dennis Don Kurian, 2018, EDGEML LIB CODE EMI; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Ghalwash MF, 2012, BMC BIOINFORMATICS, V13, DOI 10.1186/1471-2105-13-195; Gupta C, 2017, PR MACH LEARN RES, V70; Hammerla N. Y., 2016, DEEP CONVOLUTIONAL R; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Kumar A, 2017, PR MACH LEARN RES, V70; MA SG, 2016, PROC CVPR IEEE, P1942, DOI DOI 10.1109/CVPR.2016.214; Mori U., 2017, IEEE T NEURAL NETWOR, P1; Patil Shishir, 2018, TECHNICAL REPORT; Sabato S, 2012, J MACH LEARN RES, V13, P2999; Sainath TN, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1; Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838; Sak H, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1468; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tang Raphael, 2017, ARXIV171100333; Tavenard R., 2016, LNCS LNAI, V9851, P632, DOI [10.1007/978-3-319-46128-140, DOI 10.1007/978-3-319-46128-140]; Upton E., 2012, RASPBERRY PI USER GU; Warden P., 2017, SPEECH COMMANDS PUBL; Xing ZZ, 2012, KNOWL INF SYST, V31, P105, DOI 10.1007/s10115-011-0400-x	36	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005053
C	Joseph, M; Roth, A; Ullman, J; Waggoner, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Joseph, Matthew; Roth, Aaron; Ullman, Jonathan; Waggoner, Bo			Local Differential Privacy for Evolving Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					There are now several large scale deployments of differential privacy used to collect statistical information about users. However, these deployments periodically recollect the data and recompute the statistics using algorithms designed for a single use. As a result, these systems do not provide meaningful privacy guarantees over long time scales. Moreover, existing techniques to mitigate this effect do not apply in the "local model" of differential privacy that these systems use. In this paper, we introduce a new technique for local differential privacy that makes it possible to maintain up-to-date statistics over time, with privacy guarantees that degrade only in the number of changes in the underlying distribution rather than the number of collection periods. We use our technique for tracking a changing statistic in the setting where users are partitioned into an unknown collection of groups, and at every time period each user draws a single bit from a common ( but changing) group-specific distribution. We also provide an application to frequency and heavy-hitter estimation.	[Joseph, Matthew; Roth, Aaron; Waggoner, Bo] Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA; [Ullman, Jonathan] Northeastern Univ, Comp & Informat Sci, Boston, MA 02115 USA	University of Pennsylvania; Northeastern University	Joseph, M (corresponding author), Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.	majos@cis.upenn.edu; aaroth@cis.upenn.edu; jullman@ccs.neu.edu; bowaggoner@gmail.com						Abowd John M., 2016, CHALLENGE SCI REPROD; [Anonymous], 2017, ARXIV170902753; Bassily R., 2014, ARXIV14057085; Bassily R., 2017, ADV NEURAL INFORM PR, P2285; Bassily R, 2015, ACM S THEORY COMPUT, P127, DOI 10.1145/2746539.2746632; Bittau Andrea, 2017, ARXIV171000901; Blum A, 2013, J ACM, V60, DOI 10.1145/2450142.2450148; Bun Mark, 2017, ARXIV171104740; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; DifferentialDifferential Privacy Team Apple, 2017, LEARNING PRIVACY SCA; Ding BL, 2017, ADV NEUR IN, V30; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ACM S THEORY COMPUT, P715; Dwork C, 2009, ACM S THEORY COMPUT, P381; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85; Hsu J, 2012, LECT NOTES COMPUT SC, V7391, P461, DOI 10.1007/978-3-642-31594-7_39; Kasiviswanathan SP, 2008, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2008.27; Ligett K., 2017, ADV NEURAL INFORM PR, P2563; Mishra N., 2006, PODS 06, P143; Rogers Ryan M, 2016, ADV NEURAL INFORM PR, P1921; Roth A, 2010, ACM S THEORY COMPUT, P765; Ullman Jonathan, 2018, TIGHT LOWER BO UNPUB	24	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302039
C	Kallus, N; Puli, AM; Shalit, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kallus, Nathan; Puli, Aahlad Manas; Shalit, Uri			Removing Hidden Confounding by Experimental Grounding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				EXTERNAL VALIDITY; TRANSPORTABILITY	Observational data is increasingly used as a means for making individual-level causal predictions and intervention recommendations. The foremost challenge of causal inference from observational data is hidden confounding, whose presence cannot be tested in data and can invalidate any causal conclusion. Experimental data does not suffer from confounding but is usually limited in both scope and scale. We introduce a novel method of using limited experimental data to correct the hidden confounding in causal effect models trained on larger observational data, even if the observational data does not fully overlap with the experimental data. Our method makes strictly weaker assumptions than existing approaches, and we prove conditions under which it yields a consistent estimator. We demonstrate our method's efficacy using real-world data from a large educational experiment.	[Kallus, Nathan] Cornell Univ, New York, NY 10021 USA; [Kallus, Nathan] Cornell Tech, New York, NY 10044 USA; [Puli, Aahlad Manas] NYU, New York, NY USA; [Shalit, Uri] Technion, Haifa, Israel	Cornell University; New York University	Kallus, N (corresponding author), Cornell Univ, New York, NY 10021 USA.; Kallus, N (corresponding author), Cornell Tech, New York, NY 10044 USA.	kallus@cornell.edu; apm470@nyu.edu; urishalit@technion.ac.il			National Science Foundation [1656996]	National Science Foundation(National Science Foundation (NSF))	We wish to thank the anonymous reviewers for their helpful suggestions and comments. (NK) This material is based upon work supported by the National Science Foundation under Grant No. 1656996.	Andrews I, 2017, TECHNICAL REPORT; Bareinboim E, 2013, J CAUSAL INFERENCE, V1, P107, DOI 10.1515/jci-2012-0004; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Crump RK, 2008, REV ECON STAT, V90, P389, DOI 10.1162/rest.90.3.389; DAmour A., 2017, ARXIV171102582; Hartman E, 2015, J R STAT SOC A STAT, V178, P757, DOI 10.1111/rssa.12094; Hernan MA, 2008, EPIDEMIOLOGY, V19, P766, DOI 10.1097/EDE.0b013e3181875e61; Kang, 2016, ARXIV160309326; Krueger AB, 1999, Q J ECON, V114, P497, DOI 10.1162/003355399556052; McFowland E., 2018, ARXIV180309159; Mooij JM, 2016, ARXIV161110351 ARXIV161110351; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; Pearl J, 2014, STAT SCI, V29, P579, DOI 10.1214/14-STS486; Pearl Judea, 2015, SOCIOLOGICAL METHODS; Rossouw JE, 2002, JAMA-J AM MED ASSOC, V288, P321, DOI 10.1001/jama.288.3.321; Rothwell PM, 2005, LANCET, V365, P82, DOI 10.1016/S0140-6736(04)17670-8; Stuart EA, 2011, J R STAT SOC A STAT, V174, P369, DOI 10.1111/j.1467-985X.2010.00673.x; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; Vandenbroucke JP, 2009, LANCET, V373, P1233, DOI 10.1016/S0140-6736(09)60708-X; Wager Stefan, 2017, J AM STAT ASS; Word Elizabeth, 1990, STATE TENNESSEES STU	21	10	10	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005047
C	Li, YT; Murias, M; Major, S; Dawson, G; Carlson, DE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Yitong; Murias, Michael; Major, Samantha; Dawson, Geraldine; Carlson, David E.			Extracting Relationships by Multi-Domain Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In many biological and medical contexts, we construct a large labeled corpus by aggregating many sources to use in target prediction tasks. Unfortunately, many of the sources may be irrelevant to our target task, so ignoring the structure of the dataset is detrimental. This work proposes a novel approach, the Multiple Domain Matching Network (MDMN), to exploit this structure. MDMN embeds all data into a shared feature space while learning which domains share strong statistical relationships. These relationships are often insightful in their own right, and they allow domains to share strength without interference from irrelevant data. This methodology builds on existing distribution-matching approaches by assuming that source domains are varied and outcomes multi-factorial. Therefore, each domain should only match a relevant subset. Theoretical analysis shows that the proposed approach can have a tighter generalization bound than existing multiple-domain adaptation approaches. Empirically, we show that the proposed methodology handles higher numbers of source domains (up to 21 empirically), and provides state-of-the-art performance on image, text, and multi-channel time series classification, including clinical outcome data in an open label trial evaluating a novel treatment for Autism Spectrum Disorder.	[Li, Yitong; Carlson, David E.] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA; [Murias, Michael] Duke Univ, Duke Inst Brain Sci, Durham, NC 27706 USA; [Major, Samantha; Dawson, Geraldine] Duke Univ, Dept Psychiat, Durham, NC 27706 USA; [Major, Samantha; Dawson, Geraldine] Duke Univ, Dept Behav Sci, Durham, NC 27706 USA; [Carlson, David E.] Duke Univ, Dept Civil & Environm Engn, Durham, NC 27706 USA; [Carlson, David E.] Duke Univ, Dept Biostat & Bioinformat, Durham, NC 27706 USA	Duke University; Duke University; Duke University; Duke University; Duke University; Duke University	Li, YT (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	yitong.li@duke.edu; michael.murias@duke.edu; samantha.major@duke.edu; geraldine.dawson@duke.edu; david.carlson@duke.edu		Dawson, Geraldine/0000-0003-1410-2764; Carlson, David/0000-0003-1005-6385	Stylli Translational Neuroscience Award; Marcus Foundation; NIMH [3R01MH099192-05S2]; NICHD [P50-HD093074]	Stylli Translational Neuroscience Award; Marcus Foundation; NIMH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); NICHD(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH Eunice Kennedy Shriver National Institute of Child Health & Human Development (NICHD))	Funding was provided by the Stylli Translational Neuroscience Award, Marcus Foundation, NICHD P50-HD093074, and NIMH 3R01MH099192-05S2.	[Anonymous], 2017, NIPS; [Anonymous], 2014, ICLR; Ao Shuang, 2017, AAAI; Ash JT., 2016, ARXIV160204889; Ben- David S., 2010, MACHINE LEARNING; Ben- David S., 2014, ANN MATH ARTIFICIAL; Bottou L., 2017, ARXIV170107875STATML; Chen M., 2012, P LEARN WORKSH UT UT, V36; Courty N., 2017, IEEE PAMI; Crammer K, 2008, JMLR; Daume H., 2009, FRUSTRATINGLY EASY D; Dawson G., 2017, STEM CELLS TRANSLATI; Duan L., 2009, INT C MACH LEARN; Fernando B., 2013, ICCV; Ganin Y., 2016, JMLR; Gebru Timnit, 2017, ICCV; Glorot X, 2011, P ICML; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, P ADV NEUR INF PROC, V2017-Decem, P5768; Hou C.-A., 2016, IEEE T IMAGE PROCESS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Li C., 2017, ARXIV170609549; Li Y., 2017, NIPS; Lin Y. - P., 2017, FRONTIERS HUMAN NEUR; Liu H., 2016, ICDM; Liu M.Y., 2016, NIPS; Liu M. -Y., 2017, NIPS; Long M., 2016, ICML; Long M., 2016, NIPS; Long Mingsheng, 2017, ICML; Maaten L. v. d, 2008, JMLR; Mansour Y., 2009, NIPS; Motiian S., 2017, NIPS; Pan S. J., 2011, IEEE T NEURAL NETWOR; Russo P., 2017, ARXIV170508824; Shi Y., 2012, ICML; SUN Q., 2011, ADV NEURAL INFORM PR, P24; Tu W., 2012, NEUROCOMPUTING; Tzeng Eric, 2017, ADVERSARIAL DISCRIMI; Venkateswara Hemanth, 2017, CVPR; Villani C., 2008, OPTIMAL TRANSPORT OL; Vu M. - A. T., 2018, J NEUROSCIENCE; Xie Q, 2017, NIPS; Xu H., 2012, STAT SIGN PROC WORKS; Zhao H., 2017, ARXIV170509684; Zheng W., 2015, IEEE T AUTONOMOUS ME	46	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001035
C	MacKay, M; Vicol, P; Ba, J; Grosse, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		MacKay, Matthew; Vicol, Paul; Ba, Jimmy; Grosse, Roger			Reversible Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recurrent neural networks (RNNs) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of RNN models which can be trained. Reversible RNNs-RNNs for which the hidden-to-hidden transition can be reversed-offer a path to reduce the memory requirements of training, as hidden states need not be stored and instead can be recomputed during backpropagation. We first show that perfectly reversible RNNs, which require no storage of the hidden activations, are fundamentally limited because they cannot forget information from their hidden state. We then provide a scheme for storing a small number of bits in order to allow perfect reversal with forgetting. Our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10-15. We extend our technique to attention-based sequence-to-sequence models, where it maintains performance while reducing activation memory cost by a factor of 5-10 in the encoder, and a factor of 10-15 in the decoder.	[MacKay, Matthew; Vicol, Paul; Ba, Jimmy; Grosse, Roger] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	MacKay, M (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	mmackay@cs.toronto.edu; pvicol@cs.toronto.edu; jba@cs.toronto.edu; rgrosse@cs.toronto.edu			NSERC CGS-M award; NSERC PGS-D award	NSERC CGS-M award; NSERC PGS-D award(Natural Sciences and Engineering Research Council of Canada (NSERC))	We thank Kyunghyun Cho for experimental advice and discussion. We also thank Aidan Gomez, Mengye Ren, Gennady Pekhimenko, and David Duvenaud for helpful discussion. MM is supported by an NSERC CGS-M award, and PV is supported by an NSERC PGS-D award.	Abadi M, 2015, P 12 USENIX S OPERAT; Al-Rfou R, 2016, THEANO PYTHON FRAMEW; Arjovsky M, 2016, PR MACH LEARN RES, V48; Barone A.V.M., 2017, ARXIV PREPRINT ARXIV; Bengio Y., 2014, ARXIV14061078; Cettolo, 2016, P 13 INT WORKSH SPOK; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Courbariaux M., 2014, ARXIV PREPRINT ARXIV; Czarnecki WM, 2017, PR MACH LEARN RES, V70; Dinh L, 2016, ARXIV PREPRINT ARXIV; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Elliott Desmond, 2016, ARXIV160500459; Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]; Gomez Aidan N, 2017, ADV NEURAL INFORM PR, P2214; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924; Gruslys A, 2016, ADV NEUR IN, V29; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg Max, 2017, INT C MACH LEARN ICM; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; ling Li, 2016, ARXIV161205231; Luong M., 2015, ARXIV150804025; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27; Melis G., 2017, ARXIV; Merity Stephen, 2017, ICLR; Papamakarios George, 2017, ARXIV170507057; Pascanu R., 2013, ARXIV13126026; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Wisdom S, 2016, ADV NEUR IN, V29; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Zaremba W, 2014, CORR; Zilly J.G., 2016, ARXIV PREPRINT ARXIV	38	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003057
C	Nayebi, A; Bear, D; Kubilius, J; Kar, K; Ganguli, S; Sussillo, D; DiCarlo, JJ; Yamins, DLK		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nayebi, Aran; Bear, Daniel; Kubilius, Jonas; Kar, Kohitij; Ganguli, Surya; Sussillo, David; DiCarlo, James J.; Yamins, Daniel L. K.			Task-Driven Convolutional Recurrent Models of the Visual System	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL-NETWORKS; OBJECT	Feed-forward convolutional neural networks (CNNs) are currently state-of-the-art for object classification tasks such as ImageNet. Further, they are quantitatively accurate models of temporally-averaged responses of neurons in the primate brain's visual system. However, biological visual systems have two ubiquitous architectural features not shared with typical CNNs: local recurrence within cortical areas, and long-range feedback from downstream areas to upstream areas. Here we explored the role of recurrence in improving classification performance. We found that standard forms of recurrence (vanilla RNNs and LSTMs) do not perform well within deep CNNs on the ImageNet task. In contrast, novel cells that incorporated two structural features, bypassing and gating, were able to boost task accuracy substantially. We extended these design principles in an automated search over thousands of model architectures, which identified novel local recurrent cells and long-range feedback connections useful for object recognition. Moreover, these task-optimized ConvRNNs matched the dynamics of neural activity in the primate visual system better than feedforward networks, suggesting a role for the brain's recurrent connections in performing difficult visual behaviors.	[Nayebi, Aran] Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA; [Bear, Daniel; Yamins, Daniel L. K.] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA; [Yamins, Daniel L. K.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA; [Kubilius, Jonas; Kar, Kohitij; DiCarlo, James J.] MIT, McGovern Inst Brain Res, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [DiCarlo, James J.] MIT, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Kubilius, Jonas] Katholieke Univ Leuven, Brain & Cognit, Leuven, Belgium; [Ganguli, Surya; Sussillo, David] Google Inc, Google Brain, Mountain View, CA 94043 USA; [Yamins, Daniel L. K.] Wu Tsai Neurosci Inst, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University; Stanford University; Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); KU Leuven; Google Incorporated	Nayebi, A (corresponding author), Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA.	anayebi@stanford.edu; dbear@stanford.edu; qbilius@mit.edu	; Kar, Kohitij/J-8264-2015	Ganguli, Surya/0000-0002-9264-7551; Kar, Kohitij/0000-0002-4283-9256	James S. McDonnell foundation [220020469]; Simons Foundation (SCGB grants) [543061, 325500/542965]; US National Science Foundation [IIS-RI1703161]; European Union's Horizon 2020 research and innovation programme [705498]; US National Eye Institute [R01-EY014970]; US Office of Naval Research [MURI-114407]	James S. McDonnell foundation; Simons Foundation (SCGB grants); US National Science Foundation(National Science Foundation (NSF)); European Union's Horizon 2020 research and innovation programme; US National Eye Institute(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); US Office of Naval Research(Office of Naval Research)	This project has received funding from the James S. McDonnell foundation (grant #220020469, D.L.K.Y.), the Simons Foundation (SCGB grants 543061 (D.L.K.Y) and 325500/542965 (J.J.D)), the US National Science Foundation (grant IIS-RI1703161, D.L.K.Y.), the European Union's Horizon 2020 research and innovation programme (agreement #705498, J.K.), the US National Eye Institute (grant R01-EY014970, J. J. D.), and the US Office of Naval Research (MURI-114407, J. J. D). We thank Google (TPUv2 team) and the NVIDIA corporation for generous donation of hardware resources.	Abadi M, 2015, P 12 USENIX S OPERAT; Bergstra James, 2015, Computational Science and Discovery, V8, DOI 10.1088/1749-4699/8/1/014008; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Cadena Santiago A., 2017, BIORXIV, DOI [10.1101/201764, DOI 10.1101/201764]; Collins J., 2017, ICLR; Costa  R., 2017, NIPS; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Gilbert CD, 2013, NAT REV NEUROSCI, V14, P350, DOI 10.1038/nrn3476; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hong H, 2016, NAT NEUROSCI, V19, P613, DOI 10.1038/nn.4247; Hung CP, 2005, SCIENCE, V310, P863, DOI 10.1126/science.1117593; Issa E. B., 2018, NEURAL DYNAMICS SUCC, DOI [10.1101/092551, DOI 10.1101/092551]; James W., 1890, PRINCIPLES PSYCHOL, V1, P474; Kar  K., 2018, BIORXIV, DOI [10.1101/354753, DOI 10.1101/354753]; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Leroux  S., 2018, ICLR WORKSH 2018; Li X, 2018, PATTERN RECOGN, V79, P183, DOI 10.1016/j.patcog.2018.01.015; Liao Q., 2016, CORR; Lindsay GW, 2015, ARXIV PREPRINT ARXIV; Linsley  D., 2018, ARXIV180508315; Lotter W., 2017, ICLR, DOI [DOI 10.48550/ARXIV.1605.08104, 10.48550/arXiv.1605.08104]; Majaj NJ, 2015, J NEUROSCI, V35, P13402, DOI 10.1523/JNEUROSCI.5181-14.2015; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; McIntosh L, 2017, ARXIV171110151; Michaelis C., 2018, ARXIV180309597; Mishkin D, 2017, COMPUT VIS IMAGE UND, V161, P11, DOI 10.1016/j.cviu.2017.05.007; Mizuseki K, 2009, NEURON, V64, P267, DOI 10.1016/j.neuron.2009.08.037; Pinto N., 2008, PLOS COMPUTATIONAL B; Rajaei  K., 2018, BIORXIV, DOI [10.1101/302034, DOI 10.1101/302034]; Rajalingham  R., 2018, BIORXIV, DOI [10.1101/240614, DOI 10.1101/240614]; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Shi JX, 2018, HUM BRAIN MAPP, V39, P2269, DOI 10.1002/hbm.24006; Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	42	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305032
C	Peng, YS; Tang, KF; Lin, HT; Chang, EY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Peng, Yu-Shao; Tang, Kai-Fu; Lin, Hsuan-Tien; Chang, Edward Y.			REFUEL: Exploring Sparse Features in Deep Reinforcement Learning for Fast Disease Diagnosis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper proposes REFUEL, a reinforcement learning method with two techniques: reward shaping and feature rebuilding, to improve the performance of online symptom checking for disease diagnosis. Reward shaping can guide the search of policy towards better directions. Feature rebuilding can guide the agent to learn correlations between features. Together, they can find symptom queries that can yield positive responses from a patient with high probability. Experimental results justify that the two techniques in REFUEL allow the symptom checker to identify the disease more rapidly and accurately.	[Peng, Yu-Shao; Tang, Kai-Fu; Chang, Edward Y.] HTC Res & Healthcare, New Taipei, Taiwan; [Lin, Hsuan-Tien] Natl Taiwan Univ, Dept CSIE, Taipei, Taiwan	National Taiwan University	Peng, YS (corresponding author), HTC Res & Healthcare, New Taipei, Taiwan.	ys_peng@htc.com; keyin_tang@htc.com; htlin@csie.ntu.edu.tw; edward_chang@htc.com	Lin, Hsuan-Tien/AAE-4359-2020		Ministry of Science and Technology of Taiwan under MOST [107-2628-E-002-008-MY3]	Ministry of Science and Technology of Taiwan under MOST	The work was carried out during the first author's internship at HTC Research and became part of the Master's thesis of the first author [10]. We thank Profs. Shou-De Lin, Yun-Nung Chen, Min Sun and the anonymous reviewers for valuable suggestions. We also thank the members at HTC Research: Jin-Fu Lin for the support of operating GPU instances and Yang-En Chen for his efforts in organizing the experiment results. This work was partially supported by the Ministry of Science and Technology of Taiwan under MOST 107-2628-E-002-008-MY3.	Grzes M, 2009, EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P337, DOI 10.1109/ICMLA.2009.33; Janisch J., 2017, CORR; Kao HC, 2018, AAAI CONF ARTIF INTE, P2305; Kingma D.P, P 3 INT C LEARNING R; Kusner MJ, 2014, AAAI CONF ARTIF INTE, P1939; Nan F., 2017, ADV NEURAL INFORM PR, P4730; Nan F, 2015, PR MACH LEARN RES, V37, P1983; Nan F, 2016, ADV NEUR IN, V29; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Peng Y.-S., 2018, THESIS; Randlov J., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P463; Semigran H. L., 2015, BMJ, V351; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Tang Kai-Fu, 2016, P NIPS WORKSHOP DEEP; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Vincent P, 2010, J MACH LEARN RES, V11, P3371; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Xu Z., 2013, ICML, P133	19	10	10	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001084
C	Stadie, BC; Yang, G; Houthooft, R; Chen, X; Duan, Y; Wu, YH; Abbeel, P; Sutskever, I		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Stadie, Bradly C.; Yang, Ge; Houthooft, Rein; Chen, Xi; Duan, Yan; Wu, Yuhuai; Abbeel, Pieter; Sutskever, Ilya			Some Considerations on Learning to Explore via Meta-Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We interpret meta-reinforcement learning as the problem of learning how to quickly find a good sampling distribution in a new environment. This interpretation leads to the development of two new meta-reinforcement learning algorithms: E-MAML and E-RL2. Results are presented on a new environment we call 'Krazy World': a difficult high-dimensional gridworld which is designed to highlight the importance of correctly differentiating through sampling distributions in meta-reinforcement learning. Further results are presented on a set of maze environments. We show E-MAML and E-RL2 deliver better performance than baseline algorithms on both tasks.	[Stadie, Bradly C.; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Yang, Ge] Univ Chicago, Chicago, IL 60637 USA; [Houthooft, Rein; Sutskever, Ilya] OpenAI, San Francisco, CA USA; [Chen, Xi; Duan, Yan] Covariant Ai, Emeryville, CA USA; [Wu, Yuhuai] Univ Toronto, Toronto, ON M5S 1A1, Canada	University of California System; University of California Berkeley; University of Chicago; University of Toronto	Stadie, BC (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Yang, G (corresponding author), Univ Chicago, Chicago, IL 60637 USA.	bstadie@berkeley.edu; ge.yang@berkeley.edu			ONR PECASE [N000141612723]; AWS credit; GCE compute credit	ONR PECASE(Office of Naval Research); AWS credit; GCE compute credit	This work was supported in part by ONR PECASE N000141612723 and by AWS and GCE compute credits.	[Anonymous], 2019, ICLR UNPUB; Bacon P.-L., 2015, NIPS DEEP REINF LEAR; Barto, 2003, DISCRETE EVENT DYNAM; Bellemare M, 2016, NIPS; Brafman R. I., 2002, J MACHINE LEARNING R; Carmel D.., 1999, AUTONOMOUS AGENTS MU; Finn Chelsea, 2017, ICML; Graziano V., 2011, ACTA FUTURA, V4, P41; Houthooft R., 2016, NIPS; Kearns M. J., 2002, MACHINE LEARNING; Kolter J. Z., 2009, ICML; Kompella, 2002, ADV EVOLUTIONARY COM; Koutnik J, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P1061; Lillicrap TP, 2016, 4 INT C LEARN REPR; Mnih V, 2016, ASYNCHRONOUS METHODS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ngo H., 2012, 2012 INT JOINT C NEU, P1, DOI DOI 10.1109/IJCNN.2012.6252824; Osband I., 2016, NIPS; Ostrovski G., 2017, ARXIV170301310; Rusu A. A., 2016, CORR; Schmidhuber, 1987, THESIS; Schmidhuber, 2006, ARTIFICIAL GEN INTEL; Schmidhuber J., 1991, P INT C SIM AD BEH A, P222; Schmidhuber J., 1997, LEARNING LEARN; Schmidhuber J., 2015, ARTIFICIAL INTELLIGE; Schmidhuber Juergen, 2015, LEARNING THINK ALGOR, P1; Schulman J., 2015, ARXIV PREPRINT ARXIV; Schulman J., 2017, ABS170706347 CORR; Schulman John, 2015, 150205477 ARXIV; Silver Yand, 2013, DAAAI SPRING S 2013; Stadie B. C., 2015, ARXIV150700814; Storck J., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P159; Tang H., 2016, EXPLORATION STUDY CO; Taylor, 2013, DAAAI SPRING S 2013; Tessler Chen, 2016, AAAI; Thrun, 1996, NIPS; Vezhnevets A. S., 2017, ARXIV170301161; Wiering, 1997, ADAPTIVE BEHAV; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yi Sun, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P41, DOI 10.1007/978-3-642-22887-2_5	40	10	10	0	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003080
C	Wang, TZ; Wu, Y; Moore, DA; Russell, SJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Tongzhou; Wu, Yi; Moore, David A.; Russell, Stuart J.			Meta-Learning MCMC Proposals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Effective implementations of sampling-based probabilistic inference often require manually constructed, model-specific proposals. Inspired by recent progresses in meta-learning for training learning agents that can generalize to unseen environments, we propose a meta-learning approach to building effective and generalizable MCMC proposals. We parametrize the proposal as a neural network to provide fast approximations to block Gibbs conditionals. The learned neural proposals generalize to occurrences of common structural motifs across different models, allowing for the construction of a library of learned inference primitives that can accelerate inference on unseen models with no model-specific training required. We explore several applications including open-universe Gaussian mixture models, in which our learned proposals outperform a hand-tuned sampler, and a real-world named entity recognition task, in which our sampler yields higher final F1 scores than classical single-site Gibbs sampling.	[Wang, Tongzhou] Facebook AI Res, Menlo Pk, CA 94025 USA; [Wang, Tongzhou; Wu, Yi; Moore, David A.; Russell, Stuart J.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Moore, David A.] Google, Mountain View, CA USA	Facebook Inc; University of California System; University of California Berkeley; Google Incorporated	Wang, TZ (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.; Wang, TZ (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	tongzhou.wang.1994@gmail.com; jxwuyi@gmail.com; davmre@gmail.com; russell@cs.berkeley.edu						Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Andrychowicz M, 2016, ADV NEUR IN, V29; Berg-Kirkpatrick Taylor, 2014, ADV NEURAL INFORM PR, P1538; Bishop C.M., 1994, MIXTURE DENSITY NETW; Burda Yuri, 2015, ARXIV150900519; Djork-Arn, ICLR 2016; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Finn C, 2017, PR MACH LEARN RES, V70; Grosse Roger, 2012, 28 C UNC ART INT, P15; HEESS N, 2013, ADV NEURAL INFORM PR, V26, P3219; Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001; Ke Li, 2017, ARXIV170300441; Kemp C, 2008, P NATL ACAD SCI USA, V105, P10687, DOI 10.1073/pnas.0802631105; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koller D., 2009, PROBABILISTIC GRAPHI; Kulkarni TD, 2015, PROC CVPR IEEE, P4390, DOI 10.1109/CVPR.2015.7299068; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Liang Percy, 2008, P 25 INT C MACH LEAR, P592; Mateescu R, 2010, J ARTIF INTELL RES, V37, P279, DOI 10.1613/jair.2842; Moore David A., 2017, ARTIFICIAL INTELLIGE; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Paige B., 2016, JMLR, V48; Ritchie D., 2016, ADV NEURAL INFORM PR, P622; Ritchie D, 2015, COMPUT GRAPH FORUM, V34, P515, DOI 10.1111/cgf.12580; Ross S., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2737, DOI 10.1109/CVPR.2011.5995724; Song Jiaming, 2017, ARXIV170607561; Spiegelhalter DJ, 1996, BUGS BAYESIAN INFERE; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Tobin J, 2017, IEEE INT C INT ROBOT, P23; Le TA, 2017, PR MACH LEARN RES, V54, P1338; Turek Daniel, 2016, BAYESIAN ANAL; Venugopal Deepak, 2013, P 29 C UNC ART INT, P664; Wang W, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P902; Yan Duan, 2016, ARXIV161102779; Yi Wu, 2018, ARXIV180102209	37	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304018
C	Belinkov, Y; Glass, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Belinkov, Yonatan; Glass, James			Analyzing Hidden Representations in End-to-End Automatic Speech Recognition Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Neural networks have become ubiquitous in automatic speech recognition systems. While neural networks are typically used as acoustic models in more complex systems, recent studies have explored end-to-end speech recognition systems based on neural networks, which can be trained to directly predict text from input acoustic features. Although such systems are conceptually elegant and simpler than traditional systems, it is less obvious how to interpret the trained models. In this work, we analyze the speech representations learned by a deep end-to-end model that is based on convolutional and recurrent layers, and trained with a connectionist temporal classification (CTC) loss. We use a pre-trained model to generate frame-level features which are given to a classifier that is trained on frame classification into phones. We evaluate representations from different layers of the deep model and compare their quality for predicting phone labels. Our experiments shed light on important aspects of the end-to-end model such as layer depth, model complexity, and other design choices.	[Belinkov, Yonatan; Glass, James] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Belinkov, Y (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	belinkov@mit.edu; glass@mit.edu	Jeong, Yongwook/N-7413-2016		Qatar Computing Research Institute (QCRI)	Qatar Computing Research Institute (QCRI)(Qatar Foundation (QF))	We would like to thank members of the MIT spoken language systems group for helpful discussions. This work was supported by the Qatar Computing Research Institute (QCRI).	Adi Yossi, 2017, INT C LEARN REPR; Alishahi Afra, 2017, P 21 C COMP NAT LANG, P368, DOI DOI 10.18653/V1/K17-1037; Amodei D, 2016, PR MACH LEARN RES, V48; Audhkhasi K., 2017, INTERSPEECH 2017; Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618; Belinkov Y, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P861, DOI 10.18653/v1/P17-1080; Chaabouni R, 2017, INTERSPEECH, P2218, DOI 10.21437/Interspeech.2017-1689; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chorowski J., 2014, P NIPS WORKSH DEEP L; Chrupala G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P613, DOI 10.18653/v1/P17-1057; Eyben F, 2009, 2009 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION & UNDERSTANDING (ASRU 2009), P376, DOI 10.1109/ASRU.2009.5373257; Gelderloos Lieke, 2016, P COLING 2016 26 INT, P1309; Graves A., 2006, P 23 INT C MACH LEAR, P369; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Harwath D, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P506, DOI 10.18653/v1/P17-1047; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Kohn A., 2014, P 2015 C EMP METH NA, V2015, P2067, DOI DOI 10.18653/V1/D15-1246; Laurent C, 2016, INT CONF ACOUST SPEE, P2657, DOI 10.1109/ICASSP.2016.7472159; LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546; Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790; Mohamed AR, 2012, INT CONF ACOUST SPEE, P4273, DOI 10.1109/ICASSP.2012.6288863; Nagamine T., 2015, INTERSPEECH 2015; Nagamine T, 2016, INTERSPEECH, P803, DOI 10.21437/Interspeech.2016-1406; Naren S, 2016, DEEPSPEECH TORCH; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Qian P, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1478; Sainath TN, 2015, INT CONF ACOUST SPEE, P4580, DOI 10.1109/ICASSP.2015.7178838; Senior A, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P604, DOI 10.1109/ASRU.2015.7404851; Shi Xing, 2016, P 2016 C EMP METH NA, P1526, DOI DOI 10.18653/V1/D16-1159; Soltau H., 2017, INTERSPEECH 2017; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Toshniwal Shubham, 2017, ARXIV170401631; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang S, 2017, INTERSPEECH, P1497, DOI 10.21437/Interspeech.2017-1125; Wang Y., 2017, INTERSPEECH 2017; Wu ZZ, 2016, INT CONF ACOUST SPEE, P5140, DOI 10.1109/ICASSP.2016.7472657; Yu D., 2013, INT C LEARN REPR ICL	38	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402048
C	Bietti, A; Mairal, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bietti, Alberto; Mairal, Julien			Invariance and Stability of Deep Convolutional Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we study deep signal representations that are near-invariant to groups of transformations and stable to the action of diffeomorphisms without losing signal information. This is achieved by generalizing the multilayer kernel introduced in the context of convolutional kernel networks and by studying the geometry of the corresponding reproducing kernel Hilbert space. We show that the signal representation is stable, and that models from this functional space, such as a large class of convolutional neural networks, may enjoy the same stability.	[Bietti, Alberto; Mairal, Julien] Univ Grenoble Alpes, Inria, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Bietti, A (corresponding author), Univ Grenoble Alpes, Inria, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.	alberto.bietti@inria.fr; julien.mairal@inria.fr	Mairal, Julien/AAL-5611-2021		ANR (MACARON project) [ANR-14-CE23-0003-01]; ERC [714381]; MSR-Inria joint center	ANR (MACARON project)(French National Research Agency (ANR)); ERC(European Research Council (ERC)European Commission); MSR-Inria joint center	This work was supported by a grant from ANR (MACARON project under grant number ANR-14-CE23-0003-01), by the ERC grant number 714381 (SOLARIS project), and by the MSR-Inria joint center.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Anselmi F., 2015, ARXIV150801084; Anselmi F, 2016, INF INFERENCE, V5, P134, DOI 10.1093/imaiai/iaw009; Bietti A., 2017, ARXIV170603078; Bo LF, 2011, PROC CVPR IEEE, P1729, DOI 10.1109/CVPR.2011.5995719; Bruna J., 2013, ARXIV13013537; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Cho Youngmin, 2009, P ADV NEUR INF PROC, V22, P342; Cisse M, 2017, PR MACH LEARN RES, V70; Cohen TS, 2016, PR MACH LEARN RES, V48; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Haasdonk B, 2007, MACH LEARN, V68, P35, DOI 10.1007/s10994-007-5009-7; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627; Mairal J, 2016, ADV NEUR IN, V29; Mallat S, 2012, COMMUN PUR APPL MATH, V65, P1331, DOI 10.1002/cpa.21413; Montavon G, 2011, J MACH LEARN RES, V12, P2563; Mroueh Y, 2015, ADV NEUR IN, V28; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Oyallon E, 2015, PROC CVPR IEEE, P2865, DOI 10.1109/CVPR.2015.7298904; Raj A., 2017, INT C ART INT STAT A; SAITOH S, 1997, INTEGRAL TRANSFORMS, V369; Schoenberg I., 1942, DUKE MATH J, V9, P96, DOI DOI 10.1215/S0012-7094-42-00908-6; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 2001, LEARNING KERNELS SUP; Scholkopf B., 1997, THESIS; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Sifre L, 2013, PROC CVPR IEEE, P1233, DOI 10.1109/CVPR.2013.163; Smola A. J., 2000, P INT C MACH LEARN; Stein E.M., 1993, PRINCETON MATH SERIE, VIII; Torralba A, 2003, NETWORK-COMP NEURAL, V14, P391, DOI 10.1088/0954-898X/14/3/302; Uhl J.J, 1977, VECTOR MEASURES; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhang YC, 2017, PR MACH LEARN RES, V70; Zhang YT, 2016, PR MACH LEARN RES, V48	36	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406028
C	Bietti, A; Mairal, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bietti, Alberto; Mairal, Julien			Stochastic Optimization with Variance Reduction for Infinite Datasets with Finite Sum Structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ONLINE	Stochastic optimization algorithms with variance reduction have proven successful for minimizing large finite sums of functions. Unfortunately, these techniques are unable to deal with stochastic perturbations of input data, induced for example by data augmentation. In such cases, the objective is no longer a finite sum, and the main candidate for optimization is the stochastic gradient descent method (SGD). In this paper, we introduce a variance reduction approach for these settings when the objective is composite and strongly convex. The convergence rate outperforms SGD with a typically much smaller constant factor, which depends on the variance of gradient estimates only due to perturbations on a single example.	[Bietti, Alberto; Mairal, Julien] Univ Grenoble Alpes, Inria, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France	Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Bietti, A (corresponding author), Univ Grenoble Alpes, Inria, CNRS, Grenoble INP,LJK, F-38000 Grenoble, France.	alberto.bietti@inria.fr; julien.mairal@inria.fr	Mairal, Julien/AAL-5611-2021		ANR (MACARON project) [ANR-14-CE23-0003-01]; ERC [714381]; MSR-Inria joint center	ANR (MACARON project)(French National Research Agency (ANR)); ERC(European Research Council (ERC)European Commission); MSR-Inria joint center	This work was supported by a grant from ANR (MACARON project under grant number ANR-14-CE23-0003-01), by the ERC grant number 714381 (SOLARIS project), and by the MSR-Inria joint center.	Achab M., 2015, ARXIV151004822; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Allen-Zhu Zeyuan, 2016, ABS160202151 ARXIV; Bottou L., 2016, ARXIV160604838; Bruna J, 2013, IEEE T PATTERN ANAL, V35, P1872, DOI 10.1109/TPAMI.2012.230; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A., 2014, INT C MACH LEARN ICM; Duchi J. C., 2012, ADV NEURAL INFORM PR; Duchi J, 2009, J MACH LEARN RES, V10, P2899; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA, V306, pxviii+346, DOI 10.1007/978-3-662-06409-2; Hofmann T, 2015, ADV NEURAL INFORM PR; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lacoste-Julien Simon, 2012, ARXIV12122002; Lan G., 2017, MATH PROGRAMMING; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Mairal J, 2016, ADV NEUR IN, V29; Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x; Moulines E., 2011, ADV NEURAL INF PROCE, V24; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y., 2018, APPL OPTIMIZATION; Paulin M., 2014, CVPR; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S., 2016, INT C MACH LEARN ICM; Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239; van de Vijver MJ, 2002, NEW ENGL J MED, V347, P1999, DOI 10.1056/NEJMoa021967; van der Maaten L., 2013, P 30 INT C MACH LEAR, pI; Wager S., 2014, ADV NEURAL INFORM PR; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zheng Stephan, 2016, P CVPR	36	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401064
C	Christiano, PF; Leike, J; Brown, TB; Martic, M; Legg, S; Amodei, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Christiano, Paul F.; Leike, Jan; Brown, Tom B.; Martic, Miljan; Legg, Shane; Amodei, Dario			Deep Reinforcement Learning from Human Preferences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than 1% of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any which have been previously learned from human feedback.	[Christiano, Paul F.; Brown, Tom B.; Amodei, Dario] OpenAI, San Francisco, CA 94110 USA; [Leike, Jan; Martic, Miljan; Legg, Shane] DeepMind, London, England; [Brown, Tom B.] Google Brain, Mountain View, CA USA	Google Incorporated	Christiano, PF (corresponding author), OpenAI, San Francisco, CA 94110 USA.	paul@openai.com; leike@google.com; tombbrown@google.com; miljanm@google.com; legg@google.com; damodei@openai.com	Jeong, Yongwook/N-7413-2016						0	10	10	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404036
C	Downey, C; Hefny, A; Li, BY; Boots, B; Gordon, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Downey, Carlton; Hefny, Ahmed; Li, Boyue; Boots, Byron; Gordon, Geoff			Predictive State Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a new model, Predictive State Recurrent Neural Networks (PSRNNs), for filtering and prediction in dynamical systems. PSRNNs draw on insights from both Recurrent Neural Networks (RNNs) and Predictive State Representations (PSRs), and inherit advantages from both types of models. Like many successful RNN architectures, PSRNNs use (potentially deeply composed) bilinear transfer functions to combine information from multiple sources. We show that such bilinear functions arise naturally from state updates in Bayes filters like PSRs, in which observations can be viewed as gating belief states. We also show that PSRNNs can be learned effectively by combining Backpropogation Through Time (BPTT) with an initialization derived from a statistically consistent learning algorithm for PSRs called two-stage regression (2SR). Finally, we show that PSRNNs can be factorized using tensor decomposition, reducing model size and suggesting interesting connections to existing multiplicative architectures such as LSTMs and GRUs. We apply PSRNNs to 4 datasets, and show that we outperform several popular alternative approaches to modeling dynamical systems in all cases.	[Downey, Carlton; Hefny, Ahmed; Li, Boyue; Gordon, Geoff] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Boots, Byron] Georgia Tech, Atlanta, GA 30332 USA	Carnegie Mellon University; University System of Georgia; Georgia Institute of Technology	Downey, C (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	cmdowney@cs.cmu.edu; ahefny@cs.cmu.edu; boyue@cs.cmu.edu; bboots@cc.gatech.edu; ggordon@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		ONR [N000141512365]; DARPA [FA87501720152]	ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	The authors gratefully acknowledge support from ONR (grant number N000141512365) and DARPA (grant number FA87501720152).	Alpaydin E., 1998, MACH LEARN REPOSITOR, V4; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; BAUM LE, 1966, ANN MATH STAT, V37, P1554, DOI 10.1214/aoms/1177699147; Belanger D, 2015, PR MACH LEARN RES, V37, P833; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Boots B., 2013, ABS13096819 CORR; Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092; Boots Byron, 2011, P 25 NAT C ART INT A; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Downey Carlton, 2017, TECHNICAL REPORT; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Glorot X., 2010, PROC MACH LEARN RES, P249; Haarnoja T, 2016, ADV NEUR IN, V29; Hefny A., 2015, ADV NEURAL INFORM PR, V28, P1963; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsu Daniel J., 2008, ABS08114413 CORR; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Ko J, 2011, AUTON ROBOT, V30, P3, DOI 10.1007/s10514-010-9213-0; Kossaifi J., 2017, ARXIV170708308; Littman M. L., 2001, NIPS; Ljung L, 1999, SYSTEM IDENTIFICATIO, DOI DOI 10.1002/047134608X.W1046.PUB2; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Martens James, 2010, P 27 INT C MACH LEAR, P743; Pasa Luca, 2014, 22 EUR S ART NEUR NE; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Shaban Amirreza, 2015, P INT C UNC ART INT; Song L., 2010, P 27 INT C MACH LEAR, P991; Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754; Van Overschee Peter, 1993, P IFAC WORLD C SYDN, V7, P361; VANOVERSCHEE P, 1994, AUTOMATICA, V30, P75, DOI 10.1016/0005-1098(94)90230-5; Wu Yuhuai, 2016, ABS160606630 CORR; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260	37	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406013
C	Fanti, G; Viswanath, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fanti, Giulia; Viswanath, Pramod			Deanonymization in the Bitcoin P2P Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recent attacks on Bitcoin's peer-to-peer (P2P) network demonstrated that its transaction-flooding protocols, which are used to ensure network consistency, may enable user deanonymization-the linkage of a user's IP address with her pseudonym in the Bitcoin network. In 2015, the Bitcoin community responded to these attacks by changing the network's flooding mechanism to a different protocol, known as diffusion. However, it is unclear if diffusion actually improves the system's anonymity. In this paper, we model the Bitcoin networking stack and analyze its anonymity properties, both pre- and post-2015. The core problem is one of epidemic source inference over graphs, where the observational model and spreading mechanisms are informed by Bitcoin's implementation; notably, these models have not been studied in the epidemic source detection literature before. We identify and analyze near-optimal source estimators. This analysis suggests that Bitcoin's networking protocols (both pre- and post-2015) offer poor anonymity properties on networks with a regular-tree topology. We confirm this claim in simulation on a 2015 snapshot of the real Bitcoin P2P network topology.	[Fanti, Giulia] Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA; [Viswanath, Pramod] Univ Illinois, ECE Dept, Champaign, IL USA	Carnegie Mellon University; University of Illinois System; University of Illinois Urbana-Champaign	Fanti, G (corresponding author), Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA.		Jeong, Yongwook/N-7413-2016		NSF [CCF-1705007]	NSF(National Science Foundation (NSF))	This work was funded by NSF grant CCF-1705007.	Androulaki E., 2013, PROC INT C FINANCIAL, P34, DOI DOI 10.1007/978-3-642-39884-1; [Anonymous], 2012, ARXIV12112333; Athreya K.B., 2012, BRANCHING PROCESSES, V196; Bender C. M., 1999, ADV MATH METHODS SCI, DOI DOI 10.1007/978-1-4757-3069-2; Biryukov A, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P15, DOI 10.1145/2660267.2660379; Biryukov A, 2015, P IEEE S SECUR PRIV, P122, DOI 10.1109/SP.2015.15; Boneh A., 1997, COMMUN STAT STOCHAST, V13, P39, DOI 10.1080/15326349708807412; Chen Z, 2016, IEEE T NETW SCI ENG, V3, P17, DOI 10.1109/TNSE.2016.2523804; Eggenberger F, 1923, Z ANGEW MATH MECH, V3, P279, DOI 10.1002/zamm.19230030407; Fanti G., 2015, ICML; Janson S, 2004, STOCH PROC APPL, V110, P177, DOI 10.1016/j.spa.2003.12.002; Khim J., 2015, ARXIV151005461; Koshy P, 2014, LECT NOTES COMPUT SC, V8437, P469, DOI 10.1007/978-3-662-45472-5_30; Lokhov A.Y., 2013, ARXIV13035315; Mah Paul, 2016, TOP 5 VPN SERVICES P; Mahmoud H., 2008, POLYA URN MODELS; Meiklejohn S., 2013, P 2013 C INTERNET ME, P127, DOI [10.1145/2504730.2504747, DOI 10.1145/2504730.2504747]; Miller A., 2015, DISCOVERING BITCOINS; Milling Chris, 2012, Performance Evaluation Review, V40, P223, DOI 10.1145/2318857.2254784; Morris David Z., 2017, LEGAL SPARRING CONTI; Nakamoto S., 2008, DECENTRALIZED BUS RE, P21260; Ober M, 2013, FUTURE INTERNET, V5, P237, DOI 10.3390/fi5020237; Pinto PC, 2012, PHYS REV LETT, V109, DOI 10.1103/PhysRevLett.109.068702; Prakash BA, 2012, IEEE DATA MINING, P11, DOI 10.1109/ICDM.2012.136; Reid F., 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P1318, DOI 10.1109/PASSAT/SocialCom.2011.79; Ron D., 2013, LNCS, P6, DOI DOI 10.1007/978-3-642-39884-1; Shah Devavrat, 2012, Performance Evaluation Review, V40, P199, DOI 10.1145/2318857.2254782; Shah D, 2010, PERF E R SI, V38, P203, DOI 10.1145/1811099.1811063; Wang Z., 2014, ACM SIGMETRICS; Weisstein E. W., 2002, EULER MASCHERONI CON; ZHU K, 2014, COMPUT SOC NETW, V1, P1; Zhu K., 2013, ARXIV13094846	33	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401039
C	Garimella, K; Gionis, A; Parotsidis, N; Tatti, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Garimella, Kiran; Gionis, Aristides; Parotsidis, Nikos; Tatti, Nikolaj			Balancing information exposure in social networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPETITION; DIFFUSION	Social media has brought a revolution on how people are consuming news. Beyond the undoubtedly large number of advantages brought by social-media platforms, a point of criticism has been the creation of echo chambers and filter bubbles, caused by social homophily and algorithmic personalization. In this paper we address the problem of balancing the information exposure in a social network. We assume that two opposing campaigns (or viewpoints) are present in the network, and that network nodes have different preferences towards these campaigns. Our goal is to find two sets of nodes to employ in the respective campaigns, so that the overall information exposure for the two campaigns is balanced. We formally define the problem, characterize its hardness, develop approximation algorithms, and present experimental evaluation results. Our model is inspired by the literature on influence maximization, but there are significant differences from the standard model. First, balance of information exposure is modeled by a symmetric difference function, which is neither monotone nor submodular, and thus, not amenable to existing approaches. Second, while previous papers consider a setting with selfish agents and provide bounds on best-response strategies (i.e., move of the last player), we consider a setting with a centralized agent and provide bounds for a global objective function.	[Garimella, Kiran; Gionis, Aristides; Tatti, Nikolaj] Aalto Univ, Helsinki, Finland; [Garimella, Kiran; Gionis, Aristides; Tatti, Nikolaj] Aalto Univ, Helsinki, Finland; [Parotsidis, Nikos] Univ Roma Tor Vergata, Rome, Italy	Aalto University; Aalto University; University of Rome Tor Vergata	Garimella, K (corresponding author), Aalto Univ, Helsinki, Finland.; Garimella, K (corresponding author), Aalto Univ, Helsinki, Finland.	kiran.garimella@aalto.fi; aristides.gionis@aalto.fi; nikos.parotsidis@uniroma2.it; nikolaj.tatti@aalto.fi	Jeong, Yongwook/N-7413-2016; Garimella, Kiran/AAC-8114-2022; Gionis, Aristides/G-2225-2013	Gionis, Aristides/0000-0002-5211-112X; Tatti, Nikolaj/0000-0002-2087-5360	Academy of Finland [286211, 313927]; EC H2020 RIA project "SoBigData" [654024]	Academy of Finland(Academy of Finland); EC H2020 RIA project "SoBigData"	This work has been supported by the Academy of Finland projects "Nestor" (286211) and "Agra" (313927), and the EC H2020 RIA project "SoBigData" (654024).	Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Alon N, 2010, INFORM PROCESS LETT, V110, P221, DOI 10.1016/j.ipl.2009.12.009; [Anonymous], 2013, WWW; Bharathi S., 2007, WINE; Borodin A., 2010, WINE; Budak C., 2011, P 20 INT C WORLD WID; Carnes T., 2007, EC; Chen W., 2010, KDD, P1029, DOI [10.1145/1835804.1835934, DOI 10.1145/1835804.1835934]; Conover M., 2011, POLITICAL POLARIZATI; Dubey P., 2006, WINE; Farajtabar M., 2016, ADV NEURAL INFORM PR, P4718; Garimella K., 2017, WSDM; Garimella K, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P33; Garrett RK, 2009, J COMPUT-MEDIAT COMM, V14, P265, DOI 10.1111/j.1083-6101.2009.01440.x; Gottfried J., 2016, NEWS USE SOCIAL MEDI; Goyal S., 2014, GAMES EC BEHAV; Jie RL, 2016, PHYSICA A, V454, P129, DOI 10.1016/j.physa.2016.02.048; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Liao QV, 2014, 32ND ANNUAL ACM CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS (CHI 2014), P2745, DOI 10.1145/2556288.2557240; Lu H., 2015, P 24 ACM INT C INF K, P213, DOI DOI 10.1145/2806416.2806573; Lu W, 2015, PROC VLDB ENDOW, V9, P60; Morales AJ, 2015, CHAOS, V25, DOI 10.1063/1.4913758; Myers SA, 2012, IEEE DATA MINING, P539, DOI 10.1109/ICDM.2012.159; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Pariser E, 2011, FILTER BUBBLE WHAT I; Tzoumas V, 2012, INTERNET NETWORK EC, V7695; Valera I., 2015, ICDM; Zarezade A, 2017, AAAI CONF ARTIF INTE, P238	28	10	10	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404071
C	Gomez, AN; Ren, MY; Urtasun, R; Grosse, RB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gomez, Aidan N.; Ren, Mengye; Urtasun, Raquel; Grosse, Roger B.			The Reversible Residual Network: Backpropagation Without Storing Activations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep residual networks (ResNets) have significantly pushed forward the state-of-the-art on image classification, increasing in performance as networks grow both deeper and wider. However, memory consumption becomes a bottleneck, as one needs to store the activations in order to calculate gradients using backpropagation. We present the Reversible Residual Network (RevNet), a variant of ResNets where each layer's activations can be reconstructed exactly from the next layer's. Therefore, the activations for most layers need not be stored in memory during backpropagation. We demonstrate the effectiveness of RevNets on CIFAR-10, CIFAR-100, and ImageNet, establishing nearly identical classification accuracy to equally-sized ResNets, even though the activation storage requirements are independent of depth.	[Gomez, Aidan N.; Ren, Mengye; Urtasun, Raquel; Grosse, Roger B.] Univ Toronto, Toronto, ON, Canada; [Ren, Mengye; Urtasun, Raquel; Grosse, Roger B.] Vector Inst Artificial Intelligence, Toronto, ON, Canada; [Ren, Mengye; Urtasun, Raquel] Uber Adv Technol Grp, Toronto, ON, Canada	University of Toronto	Gomez, AN (corresponding author), Univ Toronto, Toronto, ON, Canada.	aidan@cs.toronto.edu; mren@cs.toronto.edu; urtasun@cs.toronto.edu; rgrosse@cs.toronto.edu	Jeong, Yongwook/N-7413-2016					Abadi M., TENSORFLOW LARGE SCA; Al-Rfou R., 2016, ARXIV160502688; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ICLR; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Chen J., 2016, ABS160400981 CORR; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Chollet F., 2017, P IEEE C COMPUTER VI, P1251, DOI DOI 10.1109/CVPR.2017.195; Dean J., 2012, ADV NEURAL INFORM PR, V25; Deco G., 1995, Advances in Neural Information Processing Systems 7, P247; Dinh L., 2015, NICE NONLINEAR INDEP; Gruslys A., 2016, NIPS; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jaderberg M., 2016, ARXIV160805343; Kalchbrenner Nal, 2016, ARXIV161010099; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kyrola A., 2017, ABS170602677 ARXIV; LeCun Y., 1990, ADV NEURAL INFORM PR, P396, DOI DOI 10.1111/DSU.12130; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Maclaurin D., 2015, ICML; Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27; Nair V., 2010, ICML, P807; Rall L.B., 1981, SPRINGER LECT NOTES, V120; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sifre L., 2014, THESIS; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; van den Oord A., 2016, GENERATIVE MODEL RAW; van den Oord Aron, 2016, NIPS; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Wu Z, 2016, CORR, P1; Wu Z, 2016, ARXIV160404339; Zhao H, 2017, P IEEE C COMPUTER VI; Zhu J.-Y., 2017, ARXIV170310593	37	10	10	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402026
C	Hazan, E; Singh, K; Zhang, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hazan, Elad; Singh, Karan; Zhang, Cyril			Learning Linear Dynamical Systems via Spectral Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present an efficient and practical algorithm for the online prediction of discrete-time linear dynamical systems with a symmetric transition matrix. We circumvent the non-convex optimization problem using improper learning: carefully overparameterize the class of LDSs by a polylogarithmic factor, in exchange for convexity of the loss functions. From this arises a polynomial-time algorithm with a near-optimal regret guarantee, with an analogous sample complexity bound for agnostic learning. Our algorithm is based on a novel filtering technique, which may be of independent interest: we convolve the time series with the eigenvectors of a certain Hankel matrix.	[Hazan, Elad; Singh, Karan; Zhang, Cyril] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA	Princeton University	Hazan, E (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.	fehazan@cs.princeton.edu; karans@cs.princeton.edu; cyril.zhang@cs.princeton.edu	Jeong, Yongwook/N-7413-2016; Singh, Karan/AAR-1348-2020	Singh, Karan/0000-0002-6992-1655; Hazan, Elad/0000-0002-1566-3216				Audenaert K., 2014, ARXIV14104941; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Beckermann B., 2016, ARXIV160909494; CHOI MD, 1983, AM MATH MON, V90, P301, DOI 10.2307/2975779; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ghahramani Z, 1996, SWITCHING STATE SPAC; GRUNBAUM FA, 1982, LINEAR ALGEBRA APPL, V43, P119, DOI 10.1016/0024-3795(82)90247-6; Hardt M., 2016, ARXIV160905191; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hilbert D, 1894, ACTA MATH, V18, P155, DOI DOI 10.1007/BF02418278; Huang WB, 2016, PROC CVPR IEEE, P3938, DOI 10.1109/CVPR.2016.427; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Ljung L, 2002, CIRC SYST SIGNAL PR, V21, P11, DOI 10.1007/BF01211648; Ljung L., 1998, SYSTEM IDENTIFICATIO; Martens James, 2010, P 27 INT C MACH LEAR, P743; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Schur J, 1911, J REINE ANGEW MATH, V140, P1; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; SLEPIAN D, 1978, BELL SYST TECH J, V57, P1371, DOI 10.1002/j.1538-7305.1978.tb02104.x; Van Overschee P., 2012, THEORY IMPLEMENTATIO; Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	24	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406074
C	Jin, L; Lazarow, J; Tu, ZW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jin, Long; Lazarow, Justin; Tu, Zhuowen			Introspective Classification with Convolutional Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose introspective convolutional networks (ICN) that emphasize the importance of having convolutional neural networks empowered with generative capabilities. We employ a reclassification-by-synthesis algorithm to perform training using a formulation stemmed from the Bayes theory. Our ICN tries to iteratively: (1) synthesize pseudo-negative samples; and (2) enhance itself by improving the classification. The single CNN classifier learned is at the same time generative - being able to directly synthesize new samples within its own discriminative model. We conduct experiments on benchmark datasets including MNIST, CIFAR-10, and SVHN using state-of-the-art CNN architectures, and observe improved classification results.	[Jin, Long; Lazarow, Justin; Tu, Zhuowen] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Jin, L (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	longjin@ucsd.edu; jlazarow@ucsd.edu; ztu@ucsd.edu	Jeong, Yongwook/N-7413-2016; Jin, Long/HHS-5672-2022		NSF [IIS-1618477, IIS-1717431]; Northrop Grumman Contextual Robotics grant	NSF(National Science Foundation (NSF)); Northrop Grumman Contextual Robotics grant	This work is supported by NSF IIS-1618477, NSF IIS-1717431, and a Northrop Grumman Contextual Robotics grant. We thank Saining Xie, Weijian Xu, Fan Fan, Kwonjoon Lee, Shuai Tang, and Sanjoy Dasgupta for helpful discussions.	[Anonymous], 2017, ICLR; Baldi P., 2012, ICML UNSUPERVISED TR, V27, P37; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Brock A., 2017, ICLR; Carreira-Perpinan M.A., 2005, P ARTIFICIAL INTELLI, VR5:, P33; Chen T., 2014, ICML; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 2015, PACKAGE GLASSO; Gatys LeonA., 2015, ARXIV, DOI 10.1167/16.12.326; Goodfellow I. J., 2015, ICLR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2016, P CVPR, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Jebara T., 2012, MACHINE LEARNING DIS, V755; Kim T., DCGAN TENSORFLOW; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Krizhevsky A., 2010, CONVOLUTIONAL DEEP B, P1; Lazarow J., 2017, ICCV; Leake D. B., 2012, ENCY SCI LEARNING, P1638; LeCun Y., 1989, NEURAL COMPUTATION; Lee C., 2015, AISTATS; Lee C.Y., 2016, AISTATS; Liang P., 2008, ICML; Liu J. S., 2008, MONTE CARLO STRATEGI; Maas Andrew L, 2013, P ICML; Mandt S., 2017, ARXIV170404289; Mooney ChristopherZ., 1993, BOOTSTRAPPING NONPAR; Mordvintsev A., GOOGLE RES BLOG; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Quinlan JR, 1996, J ARTIF INTELL RES, V4, P77, DOI 10.1613/jair.279; Radford A., 2016, ICLR; Salimans Tim, 2016, ADV NEURAL INFORM PR; Settles Burr, 2010, ACTIVE LEARNING LIT, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X; Sinha A., 2017, ARXIV170404959; Szegedy C., 2016, CVPR; Tu Z, 2007, CVPR; Tu Z, 2008, IEEE T MED IMAGING, V27, P495, DOI 10.1109/TMI.2007.908121; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Welling M., 2011, ICML; Welling M., 2002, NIPS; Wu Y., TENSORPACK TOOLBOX; Wu Y. N., 2000, INT J COMPUTER VISIO, V38; Xie J., 2016, ICML; Xie J., 2016, ARXIV160909408CSSTAT; Zhu SC, 1997, NEURAL COMPUT, V9, P1627, DOI 10.1162/neco.1997.9.8.1627; Zhu X., 2005, 1530 U WISC	51	10	10	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400079
C	Li, DS; Chen, C; Liu, W; Lu, T; Gu, N; Chu, SM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Dongsheng; Chen, Chao; Liu, Wei; Lu, Tun; Gu, Ning; Chu, Stephen M.			Mixture-Rank Matrix Approximation for Collaborative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Low-rank matrix approximation (LRMA) methods have achieved excellent accuracy among today's collaborative filtering (CF) methods. In existing LRMA methods, the rank of user/item feature matrices is typically fixed, i.e., the same rank is adopted to describe all users/items. However, our studies show that submatrices with different ranks could coexist in the same user-item rating matrix, so that approximations with fixed ranks cannot perfectly describe the internal structures of the rating matrix, therefore leading to inferior recommendation accuracy. In this paper, a mixture-rank matrix approximation (MRMA) method is proposed, in which user-item ratings can be characterized by a mixture of LRMA models with different ranks. Meanwhile, a learning algorithm capitalizing on iterated condition modes is proposed to tackle the non-convex optimization problem pertaining to MRMA. Experimental studies on MovieLens and Netflix datasets demonstrate that MRMA can outperform six state-of-the-art LRMA-based CF methods in terms of recommendation accuracy.	[Li, Dongsheng; Chen, Chao; Chu, Stephen M.] IBM Res, Beijing, Peoples R China; [Liu, Wei] Tencent AI Lab, Shenzhen, Peoples R China; [Lu, Tun; Gu, Ning] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China; [Lu, Tun; Gu, Ning] Fudan Univ, Shanghai Key Lab Data Sci, Shanghai, Peoples R China	Tencent; Fudan University; Fudan University	Li, DS (corresponding author), IBM Res, Beijing, Peoples R China.	ldsli@cn.ibm.com; cshchen@cn.ibm.com; wliu@ee.columbia.edu; lutun@fudan.edu.cn; ninggu@fudan.edu.cn; schu@cn.ibm.com	Jeong, Yongwook/N-7413-2016	Liu, Wei/0000-0002-3865-8145; Li, Dongsheng/0000-0003-3103-8442	National Natural Science Foundation of China [61332008]; NSAF [U1630115]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NSAF	This work was supported in part by the National Natural Science Foundation of China under Grant No. 61332008 and NSAF under Grant No. U1630115.	[Anonymous], 2008, P 25 INT C MACH LEAR; BESAG J, 1986, J R STAT SOC B, V48, P259; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Chen C., 2016, P 25 INT JOINT C ART; Chen C, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P303, DOI 10.1145/2766462.2767718; Dueck D., 2004, PSI200423 U TOR; Hardt M, 2015, ARXIV150901240; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Lee J., 2013, P 30 INT C MACH LEAR, P82; Li DS, 2016, PR MACH LEARN RES, V48; Ma H., 2008, NEUROCOMPUTING, P931; Mackey LW, 2011, ADV NEURAL INFORM PR, P1134, DOI DOI 10.5555/2986459.2986586; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Schmidt M, 2007, LECT NOTES ARTIF INT, V4701, P286; Toh KC, 2010, PAC J OPTIM, V6, P615; Yuan T, 2014, AAAI CONF ARTIF INTE, P222	20	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400046
C	Mescheder, L; Nowozin, S; Geiger, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mescheder, Lars; Nowozin, Sebastian; Geiger, Andreas			The Numerics of GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we analyze the numerics of common algorithms for training Generative Adversarial Networks (GANs). Using the formalism of smooth two-player games we analyze the associated gradient vector field of GAN training objectives. Our findings suggest that the convergence of current algorithms suffers due to two factors: i) presence of eigenvalues of the Jacobian of the gradient vector field with zero real-part, and ii) eigenvalues with big imaginary part. Using these findings, we design a new algorithm that overcomes some of these limitations and has better convergence properties. Experimentally, we demonstrate its superiority on training common GAN architectures and show convergence on GAN architectures that are known to be notoriously hard to train.	[Mescheder, Lars; Geiger, Andreas] MPI Tubingen, Autonomous Vis Grp, Tubingen, Germany; [Nowozin, Sebastian] Microsoft Res, Machine Intelligence & Percept Grp, Redmond, WA USA	Microsoft	Mescheder, L (corresponding author), MPI Tubingen, Autonomous Vis Grp, Tubingen, Germany.	lars.mescheder@tuebingen.mpg.de; sebastian.nowozin@microsoft.com; andreas.geiger@tuebingen.mpg.de	Jeong, Yongwook/N-7413-2016; Mescheder, Lars/ABE-3522-2020		Microsoft Research through its PhD Scholarship Programme	Microsoft Research through its PhD Scholarship Programme(Microsoft)	This work was supported by Microsoft Research through its PhD Scholarship Programme.	Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; [Anonymous], 2016, ABS160304467 CORR; Arjovsky Martin, 2017, ABS170104862 CORR; Bertsekas D. P., 2014, CONSTRAINED OPTIMIZA; Butcher J.C., 2016, NUMERICAL METHODS OR, Vthird, DOI DOI 10.1002/9781119121534; Chintala S., 2017, ABS170107875 CORR; Donahue Jeff, 2016, ABS160509782 CORR; Dumoulin Vincent, 2016, ABS160600704 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, ABS170400028 CORR; Isola P., 2016, ABS161107004 CORR; Ledig Christian, 2016, ABS160904802 CORR; Mescheder L, 2017, PR MACH LEARN RES, V70; Metz L., 2016, ABS161102163 CORR; Odena A, 2017, PR MACH LEARN RES, V70; Pascanu R., 2013, ABS13013584 CORR; Paszke Adam, 2017, PYTORCH; Pfau David, 2016, ABS161001945 CORR; Radford A., 2015, ABS151106434 CORR; Ratliff LJ, 2013, ANN ALLERTON CONF, P917, DOI 10.1109/Allerton.2013.6736623; Salimans T, 2016, ADV NEUR IN, V29; Sonderby Casper Kaae, 2016, ABS161004490 CORR; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Tzeng Eric, 2017, ABS170205464 CORR; Yeh R. A., 2016, ABS160707539 CORR	27	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401083
C	Racaniere, S; Weber, T; Reichert, DP; Buesing, L; Guez, A; Rezende, D; Badia, AP; Vinyals, O; Heess, N; Li, YJ; Pascanu, R; Battaglia, P; Hassabis, D; Silver, D; Wierstra, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Racaniere, Sebastien; Weber, Theophane; Reichert, David P.; Buesing, Lars; Guez, Arthur; Rezende, Danilo; Badia, Adria Puigdomenech; Vinyals, Oriol; Heess, Nicolas; Li, Yujia; Pascanu, Razvan; Battaglia, Peter; Hassabis, Demis; Silver, David; Wierstra, Daan			Imagination-Augmented Agents for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FUTURE	We introduce Imagination-Augmented Agents (I2As), a novel architecture for deep reinforcement learning combining model-free and model-based aspects. In contrast to most existing model-based reinforcement learning and planning methods, which prescribe how a model should be used to arrive at a policy, I2As learn to interpret predictions from a learned environment model to construct implicit plans in arbitrary ways, by using the predictions as additional context in deep policy networks. I2As show improved data efficiency, performance, and robustness to model misspecification compared to several baselines.	[Racaniere, Sebastien; Weber, Theophane; Reichert, David P.; Buesing, Lars; Guez, Arthur; Rezende, Danilo; Badia, Adria Puigdomenech; Vinyals, Oriol; Heess, Nicolas; Li, Yujia; Pascanu, Razvan; Battaglia, Peter; Hassabis, Demis; Silver, David; Wierstra, Daan] DeepMind, London, England		Racaniere, S; Weber, T; Reichert, DP (corresponding author), DeepMind, London, England.	sracaniere@google.com; theophane@google.com; reichert@google.com	Jeong, Yongwook/N-7413-2016					Abbeel P, 2005, ICML ACM INT C PROCE, V119, P1, DOI 10.1145/1102351.1102352; Bagnell, 2016, INT S EXP ROB, P703; Baird III L. C., 1993, WLTR931146; Bansal S., 2017, ARXIV170309260; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Chiappa S., 2017, 5 INT C LEARN REPR; Childs BE, 2008, 2008 IEEE SYMPOSIUM ON COMPUTATIONAL INTELLIGENCE AND GAMES, P389, DOI 10.1109/CIG.2008.5035667; Christiano P., 2016, ARXIV161003518; Cutler M, 2015, IEEE T ROBOT, V31, P655, DOI 10.1109/TRO.2015.2419431; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Dickinson A., 2002, STEVENS HDB EXPT PSY; Finn C., 2017, IEEE INT C ROB AUT I; Graves A., 2016, ADAPTIVE COMPUTATION; Gu SX, 2016, PR MACH LEARN RES, V48; Gupta A., 2017, ARXIV170703374; Hamrick JB, 2017, P 5 INT C LEARN REPR; Hassabis D, 2007, J NEUROSCI, V27, P14365, DOI 10.1523/JNEUROSCI.4549-07.2007; Hassabis D, 2007, P NATL ACAD SCI USA, V104, P1726, DOI 10.1073/pnas.0610561104; Henaff M., 2017, ARXIV170507177; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Junhyuk Oh, 2017, ARXIV170703497; Kansky Ken, 2017, INT C MACH LEARN 201; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Lake B. M., 2016, ARXIV160400289; Legg S, 2007, MIND MACH, V17, P391, DOI 10.1007/s11023-007-9079-x; Leibfried Felix, 2016, ABS161107078 CORR; Lenz I, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Levine S, 2014, ADV NEUR IN, V27; Lillicrap Timothy P, 2016, ICLR; Marco Alonso, 2017, ARXIV170301250; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V., 2013, ARXIV PREPRINT ARXIV; Murase Y., 1996, PRICAI'96: Topics in Artificial Intelligence. 4th Pacific Rim International Conference on Artificial Intelligence. Proceedings, P592; Oh J., 2015, P ADV NEUR INF PROC, P2863; Pascanu R., 2017, LEARNING MODEL BASED; Peng J., 1993, ADAPTIVE BEHAV, V1, P437, DOI DOI 10.1177/105971239300100403; Pfeiffer BE, 2013, NATURE, V497, P74, DOI 10.1038/nature12112; Rosin C.D., 2011, IJCAI, P649, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-115; Schacter DL, 2012, NEURON, V76, P677, DOI 10.1016/j.neuron.2012.11.001; Schmidhuber J., 1990, IJCNN International Joint Conference on Neural Networks (Cat. No.90CH2879-5), P253, DOI 10.1109/IJCNN.1990.137723; Schmidhuber J., 2015, ARXIV151109249; Schulman J., 2015, ARXIV PREPRINT ARXIV; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D., 2016, ARXIV161208810; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sylvain G., 2007, P 24 INT C MACH LEAR, P273; Talvitie E, 2015, AAAI CONF ARTIF INTE, P2986; Talvitie E, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P780; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Taylor J, 2011, GAMEON-NA 2011: 6TH INTERNATIONAL NORTH- AMERICAN CONFERENCE ON INTELLIGENT GAMES AND AND SIMULATION / 3RD INTERNATIONAL NORTH AMERICAN SIMULATION TECHNOLOGY CONFERENCE, NASTEC 2011, P5; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Tesauro G, 1997, ADV NEUR IN, V9, P1068; Tieleman T, 2012, COURSERA NEURAL NETW, V4; TOLMAN EC, 1948, PSYCHOL REV, V55, P189, DOI 10.1037/h0061626; Tzeng E., 2015, ARXIV151107111; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746	59	10	10	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405075
C	Staib, M; Claici, S; Solomon, J; Jegelka, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Staib, Matthew; Claici, Sebastian; Solomon, Justin; Jegelka, Stefanie			Parallel Streaming Wasserstein Barycenters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMAL TRANSPORT; ALGORITHM	Efficiently aggregating data from different sources is a challenging problem, particularly when samples from each source are distributed differently. These differences can be inherent to the inference task or present for other reasons: sensors in a sensor network may be placed far apart, affecting their individual measurements. Conversely, it is computationally advantageous to split Bayesian inference tasks across subsets of data, but data need not be identically distributed across subsets. One principled way to fuse probability distributions is via the lens of optimal transport: the Wasserstein barycenter is a single distribution that summarizes a collection of input measures while respecting their geometry. However, computing the barycenter scales poorly and requires discretization of all input distributions and the barycenter itself. Improving on this situation, we present a scalable, communication-efficient, parallel algorithm for computing the Wasserstein barycenter of arbitrary distributions. Our algorithm can operate directly on continuous input distributions and is optimized for streaming data. Our method is even robust to nonstationary input distributions and produces a barycenter estimate that tracks the input measures over time. The algorithm is semi-discrete, needing to discretize only the barycenter estimate. To the best of our knowledge, we also provide the first bounds on the quality of the approximate barycenter as the discretization becomes finer. Finally, we demonstrate the practical effectiveness of our method, both in tracking moving distributions on a sphere, as well as in a large-scale Bayesian inference task.	[Staib, Matthew; Claici, Sebastian; Solomon, Justin; Jegelka, Stefanie] MIT CS AIL, Cambridge, MA 02139 USA		Staib, M (corresponding author), MIT CS AIL, Cambridge, MA 02139 USA.	mstaib@mit.edu; sclaici@mit.edu; jsolomon@mit.edu; stefje@mit.edu	Jeong, Yongwook/N-7413-2016	Solomon, Justin/0000-0002-7701-7586	DoD; Air Force Office of Scientific Research; National Defense Science and Engineering Graduate (NDSEG) Fellowship [32 CFR 168a]; MIT Research Support Committee ("Structured Optimization for Geometric Problems"); Army Research Office grant [W911NF-12-R-0011]; NSF CAREER award [1553284]; Defense Advanced Research Projects Agency [N66001-17-1-4039]	DoD(United States Department of Defense); Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Defense Science and Engineering Graduate (NDSEG) Fellowship; MIT Research Support Committee ("Structured Optimization for Geometric Problems"); Army Research Office grant; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank the anonymous reviewers for their helpful suggestions. We also thank MIT Supercloud and the Lincoln Laboratory Supercomputing Center for providing computational resources. M. Staib acknowledges Government support under and awarded by DoD, Air Force Office of Scientific Research, National Defense Science and Engineering Graduate (NDSEG) Fellowship, 32 CFR 168a. J. Solomon acknowledges funding from the MIT Research Support Committee ("Structured Optimization for Geometric Problems"), as well as Army Research Office grant W911NF-12-R-0011 ("Smooth Modeling of Flows on Graphs"). This research was supported by NSF CAREER award 1553284 and The Defense Advanced Research Projects Agency (grant number N66001-17-1-4039). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.	Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Anderes E, 2016, MATH METHOD OPER RES, V84, P389, DOI 10.1007/s00186-016-0549-x; Araya M., 2015, P ADV NEUR INF PROC, P2053; Arjovsky M., 2017, P 34 INT C MACH LEAR, P214; Baum M, 2015, IEEE SIGNAL PROC LET, V22, P1511, DOI 10.1109/LSP.2015.2410217; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Bigot Jeremie, 2016, ARXIV160601025MATHST; Boissard E, 2015, BERNOULLI, V21, P740, DOI 10.3150/13-BEJ585; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; Cuturi M., 2014, FAST COMPUTATION WAS, P685; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Duchi J., 2008, PROC 25 INT C MACH L, P272; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Genevay A., 2016, P NEUR INF PROC SYST, P3440; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Held M., 1974, Mathematical Programming, V6, P62, DOI 10.1007/BF01580223; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; JOHNSON M, 2013, ADV NEURAL INFORM PR, P2715; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kim YH, 2017, ADV MATH, V307, P640, DOI 10.1016/j.aim.2016.11.026; Kitagawa J., 2016, ARXIV160305579CSMATH; Kloeckner B, 2012, ESAIM CONTR OPTIM CA, V18, P343, DOI 10.1051/cocv/2010100; Levy B, 2015, ESAIM-MATH MODEL NUM, V49, P1693, DOI 10.1051/m2an/2015055; MICHELOT C, 1986, J OPTIMIZ THEORY APP, V50, P195, DOI 10.1007/BF00938486; Minsker S, 2014, PR MACH LEARN RES, V32, P1656; Montavon G., 2016, ADV NEURAL INF PROCE, P3718; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Neiswanger Willie, P 30 C UNC ART INT U, P623; Nemirovski A., 2005, MODELING UNCERTAINTY, P156; Newman D., 2008, ADV NEURAL INFORM PR, P1081; Peyre G, 2016, PR MACH LEARN RES, V48; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Santambrogio F, 2015, PROG NONLINEAR DIFFE, V87, P1, DOI 10.1007/978-3-319-20828-2_1; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shamir O, 2016, ADV NEUR IN, V29; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Sra Suvrit, 2016, ARXIV160500316; Srivastava S, 2015, JMLR WORKSH CONF PRO, V38, P912; Srivastava Sanvesh, 2015, ARXIV150805880STAT; Villani C., 2009, GRUNDLEHREN MATH WIS; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Ye JB, 2017, IEEE T SIGNAL PROCES, V65, P2317, DOI 10.1109/TSP.2017.2659647; Zhang YC, 2015, J MACH LEARN RES, V16, P3299; Zhang YC, 2013, J MACH LEARN RES, V14, P3321; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	52	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402068
C	Tolstikhin, I; Gelly, S; Bousquet, O; Simon-Gabriel, CJ; Scholkopf, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tolstikhin, Ilya; Gelly, Sylvain; Bousquet, Olivier; Simon-Gabriel, Carl-Johann; Schoelkopf, Bernhard			AdaGAN: Boosting Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative Adversarial Networks (GAN) are an effective method for training generative models of complex data such as natural images. However, they are notoriously hard to train and can suffer from the problem of missing modes where the model is not able to produce examples in certain regions of the space. We propose an iterative procedure, called AdaGAN, where at every step we add a new component into a mixture model by running a GAN algorithm on a re-weighted sample. This is inspired by boosting algorithms, where many potentially weak individual predictors are greedily aggregated to form a strong composite predictor. We prove analytically that such an incremental procedure leads to convergence to the true distribution in a finite number of steps if each step is optimal, and convergence at an exponential rate otherwise. We also illustrate experimentally that this procedure addresses the problem of missing modes.	[Tolstikhin, Ilya; Simon-Gabriel, Carl-Johann; Schoelkopf, Bernhard] MPI Intelligent Syst, Tubingen, Germany; [Gelly, Sylvain; Bousquet, Olivier] Google Brain, Zurich, Switzerland	Max Planck Society	Tolstikhin, I (corresponding author), MPI Intelligent Syst, Tubingen, Germany.	ilya@tue.mpg.de; sylvaingelly@google.com; obousquet@google.com; cjsimon@tue.mpg.de; bs@tue.mpg.de	Schölkopf, Bernhard/A-7570-2013; Jeong, Yongwook/N-7413-2016	Schölkopf, Bernhard/0000-0002-8177-0925; 				[Anonymous], 2014, ICLR; Arjovsky M., 2017, ARXIV170107875; Barron A, 1997, BIOMETRICS, V53, P603; Che Tong, 2016, ARXIV161202136; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Fuglede B, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P31; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover Aditya, 2016, ICLR 2017 C UNPUB; Hein M, 2005, P 10 INT WORKSH ART, P136; Liese F, 2008, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-73194-0_1; Metz L, 2017, ARXIV161102163; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Nowozin S., 2016, ADV NEURAL INFORM PR; Radford A., 2016, ICLR; Reid MD, 2011, J MACH LEARN RES, V12, P731; Rosset S., 2002, ADV NEURAL INFORM PR, V15, P641; Tu Z, 2007, PROC CVPR IEEE, P500; Van De Weijer J, 2016, ARXIV161200991; Welling M, 2002, ADV NEURAL INFORM PR, P665	19	10	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405049
C	Oglic, D; Gartner, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Oglic, Dino; Gartner, Thomas			Greedy Feature Construction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present an effective method for supervised feature construction. The main goal of the approach is to construct a feature representation for which a set of linear hypotheses is of sufficient capacity - large enough to contain a satisfactory solution to the considered problem and small enough to allow good generalization from a small number of training examples. We achieve this goal with a greedy procedure that constructs features by empirically fitting squared error residuals. The proposed constructive procedure is consistent and can output a rich set of features. The effectiveness of the approach is evaluated empirically by fitting a linear ridge regression model in the constructed feature space and our empirical results indicate a superior performance of our approach over competing methods.	[Oglic, Dino] Univ Bonn, Inst Informat 3, Bonn, Germany; [Oglic, Dino; Gartner, Thomas] Univ Nottingham, Sch Comp Sci, Nottingham, England	University of Bonn; University of Nottingham	Oglic, D (corresponding author), Univ Bonn, Inst Informat 3, Bonn, Germany.; Oglic, D (corresponding author), Univ Nottingham, Sch Comp Sci, Nottingham, England.	dino.oglic@uni-bonn.de; thomas.gaertner@nottingham.ac.uk			German Science Foundation [GA 1615/1-1]	German Science Foundation(German Research Foundation (DFG))	We are grateful for access to the University of Nottingham High Performance Computing Facility. A part of this work was also supported by the German Science Foundation (grant number GA 1615/1-1).	Aronszajn N., 1950, T AM MATH SOC; Bach Francis R., P 22 INT C MACH LEAR; Barron A.R., 1993, IEEE T INFORM THEORY, V39, P3; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Bertinet A., 2004, REPRODUCING KERNEL H; Bochner S., 1932, VORLESUNGEN FOURIERS; Carl B., 1990, ENTROPY COMPACTNESS; Cucker F, 2002, B AM MATH SOC, V39, P1; Dai Bo, ADV NEURAL INFORM PR, V27; Demsar J, 2006, J MACH LEARN RES, V7, P1; Donahue Michael J., 1997, CONSTRUCTIVE APPROXI, V13; Fine Shai, 2002, J MACHINE LEARNING R, V2; Friedman Jerome, 2000, ANN STAT, V29; Gelfand I.M., 1963, CALCULUS VARIATIONS; Genton Marc G., 2002, J MACHINE LEARNING R, V2; Kakade S., 2011, ADV NEURAL INFORM PR; Kalai Adam T., 2009, P C LEARN THEOR; Keerthi Sathiya, 2006, ADV NEURAL INFORM PR, V19; Kolmogorov Andrey N., 1959, USPEHI MATEMATICHESK, V14; Kulis B., 2006, P 23 INT C MACH LEAR, P505; Mason L, 1999, ADV LARGE MARGIN CLA; Mayer Sebastian, 2015, CONSTRUCTIVE APPROXI, V42; MITCHELL T, 1989, ANNU REV COMPUT SCI, V4, P417; Rahimi Ali, ADV NEURAL INFORM PR, V20; WELCH BL, 1947, BIOMETRIKA, V34, P28, DOI 10.1093/biomet/34.1-2.28; WILCOXON F, 1946, J ECON ENTOMOL, V39, P269, DOI 10.1093/jee/39.2.269; Yang Z., 2015, P 18 INT C ART INT S; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	31	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704081
C	Papamakarios, G; Murray, I		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Papamakarios, George; Murray, Iain			Fast epsilon-free Inference of Simulation Models with Bayesian Conditional Density Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MONTE-CARLO; COMPUTATION	Many statistical models can be simulated forwards but have intractable likelihoods. Approximate Bayesian Computation (ABC) methods are used to infer properties of these models from data. Traditionally these methods approximate the posterior over parameters by conditioning on data being inside an epsilon-ball around the observed data, which is only correct in the limit epsilon -> 0. Monte Carlo methods can then draw samples from the approximate posterior to approximate predictions or error bars on parameters. These algorithms critically slow down as epsilon -> 0, and in practice draw samples from a broader distribution than the posterior. We propose a new approach to likelihood-free inference based on Bayesian conditional density estimation. Preliminary inferences based on limited simulation data are used to guide later simulations. In some cases, learning an accurate parametric representation of the entire true posterior distribution requires fewer model simulations than Monte Carlo ABC methods need to produce a single sample from an approximate posterior.	[Papamakarios, George; Murray, Iain] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Papamakarios, G (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	g.papamakarios@ed.ac.uk; i.murray@ed.ac.uk			Centre for Doctoral Training in Data Science - EPSRC [EP/L016427/1]; University of Edinburgh; Microsoft Research; EPSRC [EP/M507258/1] Funding Source: UKRI	Centre for Doctoral Training in Data Science - EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh; Microsoft Research(Microsoft); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank Amos Storkey for useful comments. George Papamakarios is supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh, and by Microsoft Research through its PhD Scholarship Programme.	Beaumont MA, 2002, GENETICS, V162, P2025; Bishop C.M., 1994, MIXTURE DENSITY NETW; Blum MGB, 2010, STAT COMPUT, V20, P63, DOI 10.1007/s11222-009-9116-0; Bonassi FV, 2015, BAYESIAN ANAL, V10, P171, DOI 10.1214/14-BA891; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Fan Y, 2013, STAT-US, V2, P34, DOI 10.1002/sta4.15; GOURIEROUX C, 1993, J APPL ECONOM, V8, pS85, DOI 10.1002/jae.3950080507; Gu S. S., 2015, ADV NEURAL INFORM PR, P2629; Gutmann M. U., 2015, ABS150103291V3 ARXIV; Kingma D. P., 2014, ADAM METHOD STOCHAST; Kingma DP, 2013, P 2 INT C LEARN REPR; Marjoram P, 2003, P NATL ACAD SCI USA, V100, P15324, DOI 10.1073/pnas.0306899100; Meeds E., 2014, P 30 C UNC ART INT, V30; Meeds E, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P582; Meeds T., 2015, ADV NEURAL INFORM PR, V28, P2071; Morris Q., 2001, P 17 C UNC ART INT, P370; Nair V, 2008, LECT NOTES COMPUT SC, V5163, P971, DOI 10.1007/978-3-540-87536-9_99; Paige B., 2016, P 33 INT C MACH LEAR; Papamakarios G., 2015, PROBABILISTIC INTEGR; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Schafer C. M., 2012, STAT CHALLENGES MODE, VV, P3; Wood SN, 2010, NATURE, V466, P1102, DOI 10.1038/nature09319; Wu J., 2015, ADV NEURAL INFORM PR	25	10	10	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700048
C	Zhao, Y; Park, IM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhao, Yuan; Park, Il Memming			Interpretable Nonlinear Dynamic Modeling of Neural Trajectories	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIMENSIONAL DYNAMICS; COMPUTATION; CHAOS	A central challenge in neuroscience is understanding how neural system implements computation through its dynamics. We propose a nonlinear time series model aimed at characterizing interpretable dynamics from neural trajectories. Our model assumes low-dimensional continuous dynamics in a finite volume. It incorporates a prior assumption about globally contractional dynamics to avoid overly enthusiastic extrapolation outside of the support of observed trajectories. We show that our model can recover qualitative features of the phase portrait such as attractors, slow points, and bifurcations, while also producing reliable long-term future predictions in a variety of dynamical models and in real neural data.	[Zhao, Yuan; Park, Il Memming] SUNY Stony Brook, Dept Appl Math & Stat, Inst Adv Computat Sci, Dept Neurobiol & Behav, Stony Brook, NY 11794 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Zhao, Y (corresponding author), SUNY Stony Brook, Dept Appl Math & Stat, Inst Adv Computat Sci, Dept Neurobiol & Behav, Stony Brook, NY 11794 USA.	yuan.zhao@stonybrook.edu; memming.park@stonybrook.edu	Zhao, Yuan/AAV-8723-2020	Zhao, Yuan/0000-0002-6123-8579	Thomas Hartman Foundation for Parkinson's Research	Thomas Hartman Foundation for Parkinson's Research	We thank the reviewers for their constructive feedback. This work was partially supported by the Thomas Hartman Foundation for Parkinson's Research.	Abadi M, 2015, P 12 USENIX S OPERAT; Barak O, 2013, PROG NEUROBIOL, V103, P214, DOI 10.1016/j.pneurobio.2013.02.002; CHEN S, 1990, INT J CONTROL, V52, P1327, DOI 10.1080/00207179008953599; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Curto C, 2009, J NEUROSCI, V29, P10600, DOI 10.1523/JNEUROSCI.2053-09.2009; Eikenberry SE, 2013, J COMPUT NEUROSCI, V34, P163, DOI 10.1007/s10827-012-0412-x; Gan M, 2010, INFORM SCIENCES, V180, P4370, DOI 10.1016/j.ins.2010.07.012; Ganguli S, 2008, NEURON, V58, P15, DOI 10.1016/j.neuron.2008.01.038; Goldman MS, 2009, NEURON, V61, P621, DOI 10.1016/j.neuron.2008.12.012; HANSEL D, 1992, PHYS REV LETT, V68, P718, DOI 10.1103/PhysRevLett.68.718; Izhikevich E. M., 2007, DYNAMICAL SYSTEMS NE, P226, DOI 10.7551/mitpress/2526.001.0001; Kantz H., 2003, NONLINEAR TIME SERIE; Kingma D.P, P 3 INT C LEARNING R; Laje R, 2013, NAT NEUROSCI, V16, P925, DOI 10.1038/nn.3405; Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955; Machens CK, 2005, SCIENCE, V307, P1121, DOI 10.1126/science.1104171; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Mazurek ME, 2003, CEREB CORTEX, V13, P1257, DOI 10.1093/cercor/bhg097; Ozaki T., 2012, TIME SERIES MODELING; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Peyrache A, 2015, NAT NEUROSCI, V18, P569, DOI 10.1038/nn.3968; Rabinovich MI, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000072; Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Wong KF, 2006, J NEUROSCI, V26, P1314, DOI 10.1523/JNEUROSCI.3733-05.2006; Zhao Y., 2016, ARXIV E PRINTS	27	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704067
C	Fan, K; Wang, ZT; Beck, J; Kwok, JT; Heller, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Fan, Kai; Wang, Ziteng; Beck, Jeffrey; Kwok, James T.; Heller, Katherine			Fast Second-Order Stochastic Backpropagation for Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a second-order (Hessian or Hessian-free) based optimization method for variational inference inspired by Gaussian backpropagation, and argue that quasi-Newton optimization can be developed as well. This is accomplished by generalizing the gradient computation in stochastic backpropagation via a reparametrization trick with lower complexity. As an illustrative example, we apply this approach to the problems of Bayesian logistic regression and variational auto-encoder (VAE). Additionally, we compute bounds on the estimator variance of intractable expectations for the family of Lipschitz continuous function. Our method is practical, scalable and model free. We demonstrate our method on several real-world datasets and provide comparisons with other stochastic gradient methods to show substantial enhancement in convergence rates.	[Fan, Kai; Beck, Jeffrey; Heller, Katherine] Duke Univ, Durham, NC 27706 USA; [Wang, Ziteng; Kwok, James T.] HKUST, Hong Kong, Hong Kong, Peoples R China	Duke University; Hong Kong University of Science & Technology	Fan, K (corresponding author), Duke Univ, Durham, NC 27706 USA.	kai.fan@stat.duke.edu; wangzt2012@gmail.com; jeff.beck@duke.edu; jamesk@cse.ust.hk; kheller@gmail.com			Research Grants Council of the Hong Kong Special Administrative Region [614513]	Research Grants Council of the Hong Kong Special Administrative Region(Hong Kong Research Grants Council)	This research was supported in part by the Research Grants Council of the Hong Kong Special Administrative Region (Grant No. 614513).	Ba J., 2017, P 3 INT C LEARN REPR; Beal M.J, 2003, THESIS; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bonnans J, 2006, NUMERICAL OPTIMIZATI; BONNET G, 1964, ANNALES TELECOMMUN, V19, P203; Dahl G. E., 2013, ICASSP; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Erhan D, 2010, J MACH LEARN RES, V11, P625; FERGUSON TS, 1962, ANN MATH STAT, V33, P986, DOI 10.1214/aoms/1177704466; Gan Z., 2015, NIPS; Gregor K., 2015, ICML; Hensman J., 2012, NIPS; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Khan Mohammad E, 2014, NIPS; Kingma D. P., 2014, NIPS; Martens J., 2010, ICML; Mnih A., 2014, ICML; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ngiam J., 2011, ICML; Pascanu Razvan, 2013, ARXIV13013584; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; PRICE R, 1958, IRE T INFORM THEOR, V4, P69, DOI 10.1109/TIT.1958.1057444; Rezende D.J., 2014, PROC INT CONFER ENCE; Salimans T., 2015, ICML; Sutskever I., 2014, NEURIPS; Titsias M., 2014, ICML	28	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100038
C	Narasimhan, H; Parkes, DC; Singer, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Narasimhan, Harikrishna; Parkes, David C.; Singer, Yaron			Learnability of Influence in Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We show PAC learnability of influence functions for three common influence models, namely, the Linear Threshold (LT), Independent Cascade (IC) and Voter models, and present concrete sample complexity results in each case. Our results for the LT model are based on interesting connections with neural networks; those for the IC model are based an interpretation of the influence function as an expectation over random draw of a subgraph and use covering number arguments; and those for the Voter model are based on a reduction to linear regression. We show these results for the case in which the cascades are only partially observed and we do not see the time steps in which a node has been influenced. We also provide efficient polynomial time learning algorithms for a setting with full observation, i.e. where the cascades also contain the time steps in which nodes are influenced.	[Narasimhan, Harikrishna; Parkes, David C.; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA	Harvard University	Narasimhan, H (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	hnarasimhan@seas.harvard.edu; parkes@seas.harvard.edu; yaron@seas.harvard.edu			Indo-US Science & Technology Forum; NSF [CCF-1301976]; Google Faculty Research Award; NSF CAREER [CCF-1452961]	Indo-US Science & Technology Forum; NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	Part of this work was carried out while HN was visiting Harvard as a part of a student visit under the Indo-US Joint Center for Advanced Research in Machine Learning, Game Theory & Optimization supported by the Indo-US Science & Technology Forum. HN thanks Kevin Murphy, Shivani Agarwal and Harish G. Ramaswamy for helpful discussions. YS and DP were supported by NSF grant CCF-1301976 and YS by CAREER CCF-1452961 and a Google Faculty Research Award.	Abrahao Bruno D., 2013, KDD; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Balcan M., 2011, STOC; Bartlett Peter L., 1995, HDB BRAIN THEORY NEU, P1188; Daneshmand Hadi, 2014, ICML; De Abir, 2014, CIKM 2014; Domingos P., 2001, KDD; Du N., 2012, NIPS; Du Nan, 2013, NIPS; Du Nan, 2014, ICML; Even-Dar E, 2011, INFORM PROCESS LETT, V111, P184, DOI 10.1016/j.ipl.2010.11.015; Feldman Vitaly, 2014, COLT; Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741; Gomez-Rodriguez Manuel, 2011, ICML; Goyal Amit, 2010, KDD; Honorio J, 2015, J MACH LEARN RES, V16, P1157; Kempe D., 2003, KDD; MOSSEL E, 2007, STOC; Netrapalli Praneeth, 2012, SIGMETRICS; Pouget-Abadie Jean, 2015, ICML; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Zhang T, 2004, ANN STAT, V32, P56	22	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103027
C	Shafieezadeh-Abadeh, S; Esfahani, PM; Kuhn, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shafieezadeh-Abadeh, Soroosh; Esfahani, Peyman Mohajerin; Kuhn, Daniel			Distributionally Robust Logistic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				OPTIMIZATION; ALGORITHM	This paper proposes a distributionally robust approach to logistic regression. We use theWasserstein distance to construct a ball in the space of probability distributions centered at the uniform distribution on the training samples. If the radius of this ball is chosen judiciously, we can guarantee that it contains the unknown data-generating distribution with high confidence. We then formulate a distributionally robust logistic regression model that minimizes a worst-case expected logloss function, where the worst case is taken over all distributions in the Wasserstein ball. We prove that this optimization problem admits a tractable reformulation and encapsulates the classical as well as the popular regularized logistic regression problems as special cases. We further propose a distributionally robust approach based on Wasserstein balls to compute upper and lower confidence bounds on the misclassification probability of the resulting classifier. These bounds are given by the optimal values of two highly tractable linear programs. We validate our theoretical out-of-sample guarantees through simulated and empirical experiments.	[Shafieezadeh-Abadeh, Soroosh; Esfahani, Peyman Mohajerin; Kuhn, Daniel] Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Shafieezadeh-Abadeh, S (corresponding author), Ecole Polytech Fed Lausanne, CH-1015 Lausanne, Switzerland.	soroosh.shafiee@epfl.ch; peyman.mohajerin@epfl.ch; daniel.kuhn@epfl.ch			Swiss National Science Foundation [BSCGI0_157733]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This research was supported by the Swiss National Science Foundation under grant BSCGI0_157733.	Ben-Tal A, 2002, MATH PROGRAM, V92, P453, DOI 10.1007/s101070100286; BenTal A, 2009, PRINC SER APPL MATH, P1; Bertsimas D, 2004, OPER RES, V52, P35, DOI 10.1287/opre.1030.0065; Birge JR, 2011, SPRINGER SER OPER RE, P3, DOI 10.1007/978-1-4614-0237-4; Delage E, 2010, OPER RES, V58, P595, DOI 10.1287/opre.1090.0741; Ding N., 2013, J MACHINE LEARNING R, V5, P1; Erdogan E, 2006, MATH PROGRAM, V107, P37, DOI 10.1007/s10107-005-0678-0; Fournier Nicolas, 2014, PROBAB THEORY REL, P1; Goh J, 2010, OPER RES, V58, P902, DOI 10.1287/opre.1090.0795; Hosmer D, 2004, APPL LOGISTIC REGRES; Hu Z., 2013, OPTIMIZATION; Koh KM, 2007, J MACH LEARN RES, V8, P1519; Kuhn D., DATA DRIVEN DISTRIBU; Lichman M, 2013, UCI MACHINE LEARNING; Liu C., 2005, ROBIT REGRESSION SIM, P227; Lofberg J., 2004, 2004 IEEE International Symposium on Computer Aided Control Systems Design (IEEE Cat. No.04TH8770), P284, DOI 10.1109/CACSD.2004.1393890; Ng Andrew Y, 2004, ICML, DOI [10.1145/1015330.1015435, DOI 10.1145/1015330.1015435]; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; Rousseeuw PJ, 2003, COMPUT STAT DATA AN, V43, P315, DOI 10.1016/S0167-9473(02)00304-3; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Shi JN, 2010, J MACH LEARN RES, V11, P713; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y; Wiesemann W, 2014, OPER RES, V62, P1358, DOI 10.1287/opre.2014.1314; Xu HA, 2010, IEEE T INFORM THEORY, V56, P3561, DOI 10.1109/TIT.2010.2048503; Xu H, 2009, J MACH LEARN RES, V10, P1485; Yan S., 2014, ADV NEURAL INFORM PR, P253; Yun S, 2011, COMPUT OPTIM APPL, V48, P273, DOI 10.1007/s10589-009-9251-8	30	10	10	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101005
C	Dai, B; Xie, B; He, N; Liang, YY; Raj, A; Balcan, MF; Song, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Dai, Bo; Xie, Bo; He, Niao; Liang, Yingyu; Raj, Anant; Balcan, Maria-Florina; Song, Le			Scalable Kernel Methods via Doubly Stochastic Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The general perception is that kernel methods are not scalable, so neural nets become the choice for large-scale nonlinear learning problems. Have we tried hard enough for kernel methods? In this paper, we propose an approach that scales up kernel methods using a novel concept called "doubly stochastic functional gradients". Based on the fact that many kernel methods can be expressed as convex optimization problems, our approach solves the optimization problems by making two unbiased stochastic approximations to the functional gradient-one using random training points and another using random features associated with the kernel-and performing descent steps with this noisy functional gradient. Our algorithm is simple, need no commit to a preset number of random features, and allows the flexibility of the function class to grow as we see more incoming data in the streaming setting. We demonstrate that a function learned by this procedure after t iterations converges to the optimal function in the reproducing kernel Hilbert space in rate O(1/t), and achieves a generalization bound of O(1/root t). Our approach can readily scale kernel methods up to the regimes which are dominated by neural nets. We show competitive performances of our approach as compared to neural nets in datasets such as 2.3 million energy materials from MolecularSpace, 8 million handwritten digits from MNIST, and 1 million photos from ImageNet using convolution features.	[Dai, Bo; Xie, Bo; He, Niao; Raj, Anant; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Liang, Yingyu] Princeton Univ, Princeton, NJ 08544 USA; [Balcan, Maria-Florina] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University System of Georgia; Georgia Institute of Technology; Princeton University; Carnegie Mellon University	Dai, B (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	bodai@gatech.edu; bxie33@gatech.edu; nhe6@gatech.edu; yingyul@cs.princeton.edu; araj34@gatech.edu; ninamf@cs.cmu.edu; lsong@cc.gatech.edu	He, Niao/L-9453-2017	He, Niao/0000-0003-4225-7536	NSF [CCF-0953192, CCF-1451177, CCF-1101283, CCF-1422910, IIS-1116886]; ONR [N00014-09-1-0751]; AFOSR [FA9550-09-1-0538]; NSF/NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; Raytheon Faculty Fellowship	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); NSF/NIH BIGDATA; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Raytheon Faculty Fellowship	M.B. is suppoerted in part by NSF CCF-0953192, CCF-1451177, CCF-1101283, and CCF-1422910, ONR N00014-09-1-0751, and AFOSR FA9550-09-1-0538. L.S. is supported in part by NSF IIS-1116886, NSF/NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, and a Raytheon Faculty Fellowship.	[Anonymous], 2002, LEARNING KERNELS; Cortes Corinna, 2010, AISTATS; Cotter Andrew, 2013, ICML; Dang C.D., 2013, TECHNICAL REPORT; DEVINATZ A, 1953, T AM MATH SOC, V74, P56, DOI 10.2307/1990848; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Hein Matthias, 2004, 127 MAX PLANCK I BIO; Joachims T, 1999, ADVANCES IN KERNEL METHODS, P169; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Le Q., 2013, ICML; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Lopez-Paz D., 2014, ICML; Montavon G., 2012, NIPS; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Pham N., 2013, KDD; Platt J, 1998, MSRTR9814; Rahimi A., 2008, NIPS; Rahimi A., 2009, NIPS; Rakhlin A., 2012, P 29 INT C MACH LEAR, P449; Ratliff N., 2007, IJCAI; Shalev-Shwartz S., 2007, ICML; Smola A. J., 2000, ICML; Wendland H., 2005, SCATTERED DATA APPRO; Williams C. K. I., 2000, NIPS	30	10	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100017
C	Hernandez-Lobato, D; Sharmanska, V; Kersting, K; Lampert, CH; Quadrianto, N		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hernandez-Lobato, Daniel; Sharmanska, Viktoriia; Kersting, Kristian; Lampert, Christoph H.; Quadrianto, Novi			Mind the Nuisance: Gaussian Process Classification using Privileged Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The learning with privileged information setting has recently attracted a lot of attention within the machine learning community, as it allows the integration of additional knowledge into the training process of a classifier, even when this comes in the form of a data modality that is not available at test time. Here, we show that privileged information can naturally be treated as noise in the latent function of a Gaussian process classifier (GPC). That is, in contrast to the standard GPC setting, the latent function is not just a nuisance but a feature: it becomes a natural measure of confidence about the training data by modulating the slope of the GPC probit likelihood function. Extensive experiments on public datasets show that the proposed GPC method using privileged noise, called GPC+, improves over a standard GPC without privileged knowledge, and also over the current state-of-the-art SVM-based method, SVM+. Moreover, we show that advanced neural networks and deep learning methods can be compressed as privileged information.	[Hernandez-Lobato, Daniel] Univ Autonoma Madrid, Madrid, Spain; [Sharmanska, Viktoriia; Lampert, Christoph H.] IST Austria, Klosterneuburg, Austria; [Kersting, Kristian] TU Dortmund, Dortmund, Germany; [Quadrianto, Novi] Univ Sussex, SMiLe CLiN, Brighton, E Sussex, England	Autonomous University of Madrid; Institute of Science & Technology - Austria; Dortmund University of Technology; University of Sussex	Hernandez-Lobato, D (corresponding author), Univ Autonoma Madrid, Madrid, Spain.	daniel.hernandez@uam.es; vsharman@ist.ac.at; kristian.kersting@cs.tu-dortmund.de; chl@ist.ac.at; n.quadrianto@sussex.ac.uk		Quadrianto, Novi/0000-0001-8819-306X; Sharmanska, Viktoriia/0000-0003-0192-9308	Direccion General de Investigacion MCyT; Consejeria de Educacion CAM [TIN2010-21575-C02-02, TIN2013-42351-P, S2013/ICE-2845]; European Research Council under the ERC [308036]	Direccion General de Investigacion MCyT; Consejeria de Educacion CAM; European Research Council under the ERC(European Research Council (ERC))	D. Hernandez-Lobato is supported by Direccion General de Investigacion MCyT and by Consejeria de Educacion CAM (projects TIN2010-21575-C02-02, TIN2013-42351-P and S2013/ICE-2845). V. Sharmanska is funded by the European Research Council under the ERC grant agreement no 308036.	Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Berg TL, 2010, LECT NOTES COMPUT SC, V6311, P663, DOI 10.1007/978-3-642-15549-9_48; Demsar J, 2006, J MACH LEARN RES, V7, P1; Donahue J, 2014, PR MACH LEARN RES, V32; Feyereisl J, 2012, INFORM SCIENCES, V194, P4, DOI 10.1016/j.ins.2011.04.025; Fouad S, 2013, IEEE T NEUR NET LEAR, V24, P1086, DOI 10.1109/TNNLS.2013.2251470; Goldberg PW, 1998, ADV NEUR IN, V10, P493; Hernandez-Lobato D., 2011, ADV NEURAL INFORM PR, P280; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Lapin M, 2014, NEURAL NETWORKS, V53, P95, DOI 10.1016/j.neunet.2014.02.002; Lazaro-Gredilla M., 2011, P INT C MACH LEARN M, P841; Mikolov T., 2013, ARXIV; Minka T., 2001, THESIS MIT CAMBRIDGE; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Pechyony D., 2010, ADV NEURAL INFORM PR, P1894; Pechyony D., 2011, STAT LEARNING DATA S, V1; Quadrianto N, 2009, IEEE DATA MINING, P938, DOI 10.1109/ICDM.2009.82; Quoc Le, 2014, P 31 INT C MACHINE L, V32, P1188; Ribeiro B, 2010, IEEE IJCNN; Riihimaki J, 2013, J MACH LEARN RES, V14, P75; Scholkopf B., 2001, LEARNING KERNELS SUP; Seeger M., 2006, TECHNICAL REPORT; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Sharmanska  Viktoriia, 2014, ARXIV14100389, P2; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042	28	10	12	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101044
C	Lemonnier, R; Scaman, K; Vayatis, N		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lemonnier, Remi; Scaman, Kevin; Vayatis, Nicolas			Tight Bounds for Influence in Diffusion Networks and Application to Bond Percolation and Epidemiology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper, we derive theoretical bounds for the long-term influence of a node in an Independent Cascade Model (ICM). We relate these bounds to the spectral radius of a particular matrix and show that the behavior is sub-critical when this spectral radius is lower than 1. More specifically, we point out that, in general networks, the sub-critical regime behaves in O(root n) where n is the size of the network, and that this upper bound is met for star-shaped networks. We apply our results to epidemiology and percolation on arbitrary networks, and derive a bound for the critical value beyond which a giant connected component arises. Finally, we show empirically the tightness of our bounds for a large family of networks.	[Lemonnier, Remi; Scaman, Kevin; Vayatis, Nicolas] CNRS, CMLA ENS Cachan, Paris, France; [Lemonnier, Remi] 1000mercis, Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Lemonnier, R (corresponding author), CNRS, CMLA ENS Cachan, Paris, France.	lemonnier@cmla.ens-cachan.fr; scaman@cmla.ens-cachan.fr; vayatis@cmla.ens-cachan.fr			French Government within the program of "Investments for the Future - Big Data"	French Government within the program of "Investments for the Future - Big Data"	This research is part of the SODATECH project funded by the French Government within the program of "Investments for the Future - Big Data".	[Anonymous], 2006, CONNECTED MARKETING; [Anonymous], 2003, OXFORD STUDIES PROBA; Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168; Chen W., 2010, KDD, P1029, DOI [10.1145/1835804.1835934, DOI 10.1145/1835804.1835934]; Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047; Chung F., 2002, ANN COMB, V6, P125, DOI [DOI 10.1007/PL00012580.PDF, 10.1007/PL00012580]; Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57, DOI 10.1145/502512.502525; Draief M., 2006, PROC 1 INT C PERFORM, P51; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; Gomez-Rodriguez M., 2012, ICML; Gomez-Rodriguez Manuel, 2011, ICML, P561; Goyal A., 2011, PROC 20 INT C COMPAN, P47, DOI DOI 10.1145/1963192.1963217; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Kermack WO, 1932, P R SOC LOND A-CONTA, V138, P55, DOI 10.1098/rspa.1932.0171; MOLLOY M, 1995, RANDOM STRUCT ALGOR, V6, P161, DOI 10.1002/rsa.3240060204; Molloy M, 1998, COMB PROBAB COMPUT, V7, P295, DOI 10.1017/S0963548398003526; Myers Seth, 2010, P INT C ADV NEURAL I, P1741; Nelson Kenrad E, 2007, INFECT DIS EPIDEMIOL, P17; Newman M., 2010, NETWORKS INTRO, DOI [DOI 10.1093/ACPROF:OSO/9780199206650.001.0001, 10.1162/artl_r_00062., 10.1162/artl_r_00062]; Newman MEJ, 2002, PHYS REV E, V66, DOI 10.1103/PhysRevE.66.016128; Ohara K., 2013, P 5 ASIAN C MACH LEA, V29, P149; Rodriguez Manuel Gomez, 2010, P 16 ACM SIGKDD INT, P1019; Rucinski A., 2011, RANDOM GRAPHS, V45	23	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101035
C	Mahmood, AR; Van Hasselt, H; Sutton, RS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mahmood, A. Rupam; Van Hasselt, Hado; Sutton, Richard S.			Weighted importance sampling for off-policy learning with linear function approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				COVARIATE SHIFT	Importance sampling is an essential component of off-policy model-free reinforcement learning algorithms. However, its most effective variant, weighted importance sampling, does not carry over easily to function approximation and, because of this, it is not utilized in existing off-policy learning algorithms. In this paper, we take two steps toward bridging this gap. First, we show that weighted importance sampling can be viewed as a special case of weighting the error of individual training samples, and that this weighting has theoretical and empirical benefits similar to those of weighted importance sampling. Second, we show that these benefits extend to a new weighted-importance-sampling version of off-policy LSTD(lambda). We show empirically that our new WIS-LSTD(lambda) algorithm can result in much more rapid and reliable convergence than conventional off-policy LSTD(lambda) (Yu 2010, Bertsekas & Yu 2009).	[Mahmood, A. Rupam; Van Hasselt, Hado; Sutton, Richard S.] Univ Alberta, Reinforcement Learning & Artificial Intelligence, Edmonton, AB T6G 1S2, Canada	University of Alberta	Mahmood, AR (corresponding author), Univ Alberta, Reinforcement Learning & Artificial Intelligence, Edmonton, AB T6G 1S2, Canada.	ashique@cs.ualberta.ca; vanhasse@cs.ualberta.ca; sutton@cs.ualberta.ca			National Science and Engineering Research Council; Alberta Innovates Centre for Machine Learning; Alberta Innovates Technology Futures	National Science and Engineering Research Council(Natural Sciences and Engineering Research Council of Canada (NSERC)); Alberta Innovates Centre for Machine Learning; Alberta Innovates Technology Futures	This work was supported by grants from Alberta Innovates Technology Futures, National Science and Engineering Research Council, and Alberta Innovates Centre for Machine Learning.	ANDRADOTTIR S, 1995, OPER RES, V43, P509, DOI 10.1287/opre.43.3.509; [Anonymous], 2001, THESIS; Bertsekas DP, 2009, J COMPUT APPL MATH, V227, P27, DOI 10.1016/j.cam.2008.07.037; Boyan JA, 1999, MACHINE LEARNING, PROCEEDINGS, P49; Casella G, 1998, J COMPUT GRAPH STAT, V7, P139, DOI 10.2307/1390810; Dasgupta S., 2001, P 18 INT C MACH LEAR; Geist M, 2014, J MACH LEARN RES, V15, P289; Hachiya H, 2012, NEUROCOMPUTING, V80, P93, DOI 10.1016/j.neucom.2011.09.016; Hachiya H, 2009, NEURAL NETWORKS, V22, P1399, DOI 10.1016/j.neunet.2009.01.002; Hesterberg T.C., 1988, THESIS; KAHN H, 1953, J OPER RES SOC AM, V1, P263, DOI 10.1287/opre.1.5.263; Koller D., 2009, PROBABILISTIC GRAPHI; Liu JS., 2001, MONTE CARLO STRATEGI, DOI DOI 10.1007/978-0-387-76371-2; Maei Hamid Reza, 2011, THESIS; MAEI HR, 2010, P 3 C ART GEN INT, P91; Precup D., 2000, INT C MACH LEARN, P759; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Rubinstein R. Y., 2017, SIMULATION MONTE CAR, V3rd, DOI 10.1002/9781118631980; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Sutton R. S., 2014, P 31 INT C MACH LEAR; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Yu H., 2010, P 27 INT C MACH LEAR, V27, P1207	23	10	10	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100028
C	Zaremba, W; Kurach, K; Fergus, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zaremba, Wojciech; Kurach, Karol; Fergus, Rob			Learning to Discover Efficient Mathematical Identities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In this paper we explore how machine learning techniques can be applied to the discovery of efficient mathematical identities. We introduce an attribute grammar framework for representing symbolic expressions. Given a grammar of math operators, we build trees that combine them in different ways, looking for compositions that are analytically equivalent to a target expression but of lower computational complexity. However, as the space of trees grows exponentially with the complexity of the target expression, brute force search is impractical for all but the simplest of expressions. Consequently, we introduce two novel learning approaches that are able to learn from simpler expressions to guide the tree search. The first of these is a simple n-gram model, the other being a recursive neural-network. We show how these approaches enable us to derive complex identities, beyond reach of brute-force search, or human derivation.	[Zaremba, Wojciech; Fergus, Rob] NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA; [Kurach, Karol] Google Zurich, Zurich, Switzerland; [Kurach, Karol] Univ Warsaw, Dept Comp Sci, Warsaw, Poland	New York University; Google Incorporated; University of Warsaw	Zaremba, W (corresponding author), NYU, Courant Inst, Dept Comp Sci, New York, NY 10003 USA.				Microsoft Research; Facebook	Microsoft Research(Microsoft); Facebook(Facebook Inc)	The authors would like to thank Facebook and Microsoft Research for their support.	[Anonymous], 1973, SYMBOLIC LOGIC MECH; [Anonymous], P 3 GRAMM EV WORKSH; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Bottou L, 2014, MACH LEARN, V94, P133, DOI 10.1007/s10994-013-5335-x; Bowman S.R., 2013, ARXIV13126192; Bridge JP, 2014, J AUTOM REASONING, V53, P141, DOI 10.1007/s10817-014-9301-5; Char B. W., 1991, MAPLE V LIB REFERENC, V199; Cheung G., 1999, Proceedings 1999 International Conference on Image Processing (Cat. 99CH36348), P797, DOI 10.1109/ICIP.1999.823006; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; Cook S.A., 1971, P 3 ANN ACM S THEORY, P151, DOI [10.1145/800157.805047, DOI 10.1145/800157.805047]; DESAINTECATHERINE M, 1994, SIGPLAN NOTICES, V29, P56, DOI 10.1145/185009.185021; Fitting M., 1996, 1 ORDER LOGIC AUTOMA, DOI [10.1007/978-1-4612-2360-3, DOI 10.1007/978-1-4612-2360-3]; Goodman N, 2012, ARXIV PREPRINT ARXIV; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Knuth D. E., 1968, Mathematical Systems Theory, V2, P127, DOI 10.1007/BF01692511; Luong Thang, 2013, P 17 C COMP NAT LANG, P104, DOI DOI 10.1007/BF02579642; Mikolov T., 2013, ARXIV; Mnih Andriy, 2009, ADV NEURAL INFORM PR, P1081; Nordin P., 1997, EVOLUTIONARY PROGRAM; Pfeffer A, 2011, LECT NOTES ARTIF INT, V6489, P2, DOI 10.1007/978-3-642-21295-6_2; Saxe Andrew M, 2013, ARXIV13126120; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Socher Richard, 2010, P NIPS 2010 DEEP LEA; Stanley RP., 2011, ENUMERATIVE COMBINAT, DOI 10.1017/CBO9781139058520; Turian J, 2010, ACL 2010: 48TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P384; Waldispuhl J, 2002, BIOINFORMATICS, V18, pS250, DOI 10.1093/bioinformatics/18.suppl_2.S250; Wolfram S., 1996, MATH BOOK, V221; Wong ML, 1997, EVOL COMPUT, V5, P143, DOI 10.1162/evco.1997.5.2.143; Zaremba W., 2014, ARXIV14061584	29	10	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101021
C	Bousquet, O; Chapelle, O; Hein, M		Thrun, S; Saul, K; Scholkopf, B		Bousquet, O; Chapelle, O; Hein, M			Measure based regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We address in this paper the question of how the knowledge of the marginal distribution P(x) can be incorporated in a learning algorithm. We suggest three theoretical methods for taking into account this distribution for regularization and provide links to existing graph-based semi-supervised learning algorithms. We, also propose practical implementations.	Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany	Max Planck Society	Bousquet, O (corresponding author), Max Planck Inst Biol Cybernet, D-72076 Tubingen, Germany.	oliver.bousquet@tuebingen.mpg.de; olivier.chapelle@tuebingen.mpg.de; matthias.hein@tuebingen.mpg.de						[Anonymous], 2002, LEARNING KERNELS; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; BELKIN M, 2003, IN PRESS MACHINE LEA; GIROSI F, 1993, 1430 MIT; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; LALOUDOUANA D, 2002, ADV NEURAL INFORMATI, V15; NG A, 2001, ADV NEURAL INFORMATI, V14; Smola AJ, 1998, ALGORITHMICA, V22, P211, DOI 10.1007/PL00013831; SZUMMER M, 2002, ADV NEURAL INFORMATI, V15; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Vincent P., 2003, SNOWB LEARN WORKSH; VONLUXBURG U, 2003, P 16 ANN C COMP LEAR	13	10	10	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1221	1228						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500152
C	Hennig, MH; Wogotter, F		Thrun, S; Saul, K; Scholkopf, B		Hennig, MH; Wogotter, F			Eye micro-movements improve stimulus detection beyond the Nyquist limit in the peripheral retina	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				RECEPTIVE-FIELDS; GANGLION-CELLS; MORPHOLOGY; VISION; MONKEY; CAT	Even under perfect fixation the human eye is under steady motion (tremor, microsaccades, slow drift). The "dynamic" theory of vision [ 1, 2] states that eye-movements can improve hyperacuity. According to this theory, eye movements are thought to create variable spatial excitation patterns on the photoreceptor grid, which will allow for better spatiotemporal summation at later stages. We reexamine this theory using a realistic model of the vertebrate retina by comparing responses of a resting and a moving eye. The performance of simulated ganglion cells in a hyperacuity task is evaluated by ideal observer analysis. We find that in the central retina eye-micromovements have no effect on the performance. Here optical blurring limits vernier acuity. In the retinal periphery however, eye-micromovements clearly improve performance. Based on ROC analysis, our predictions are quantitatively testable in electrophysiological and psychophysical experiments.	Univ Stirling, Stirling FK9 4LR, Scotland	University of Stirling	Hennig, MH (corresponding author), Univ Stirling, Stirling FK9 4LR, Scotland.			Hennig, Matthias/0000-0001-7270-5817				Averill HL, 1925, J COMP PSYCHOL, V5, P147, DOI 10.1037/h0072373; CRONER LJ, 1995, VISION RES, V35, P7, DOI 10.1016/0042-6989(94)E0066-T; DACEY DM, 1992, P NATL ACAD SCI USA, V89, P9666, DOI 10.1073/pnas.89.20.9666; EIZENMAN M, 1985, VISION RES, V25, P1635, DOI 10.1016/0042-6989(85)90134-8; Goodchild AK, 1996, J COMP NEUROL, V366, P55, DOI 10.1002/(SICI)1096-9861(19960226)366:1<55::AID-CNE5>3.0.CO;2-J; Hennig MH, 2002, J NEUROSCI, V22, P8726; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Landolt O, 2001, AUTON ROBOT, V11, P233, DOI 10.1023/A:1012482822516; Marshall W. H., 1942, BIOL S VISUAL MECHAN, V7, P117; RODIECK RW, 1965, J NEUROPHYSIOL, V28, P833, DOI 10.1152/jn.1965.28.5.833; SCHNAPF JL, 1990, J PHYSIOL-LONDON, V427, P681, DOI 10.1113/jphysiol.1990.sp018193; Schneeweis DM, 1999, J NEUROSCI, V19, P1203; Sjostrand J, 1999, VISION RES, V39, P2987, DOI 10.1016/S0042-6989(99)00030-9; Steinman R M, 1990, Rev Oculomot Res, V4, P115; Thibos LN, 1996, VISION RES, V36, P249, DOI 10.1016/0042-6989(95)00109-D; THIBOS LN, 1987, VISION RES, V27, P2193, DOI 10.1016/0042-6989(87)90134-9; Watson A. B., 1986, HDB PERCEPTION HUMAN, V1; Westheimer G., 1986, HDB PERCEPTION HUMAN, V1	19	10	10	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1475	1482						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500183
C	Wang, XR; Hutchinson, R; Mitchell, TM		Thrun, S; Saul, K; Scholkopf, B		Wang, XR; Hutchinson, R; Mitchell, TM			Training fMRI classifiers to discriminate cognitive states across multiple subjects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We consider learning to classify cognitive states of human subjects, based on their brain activity observed via functional Magnetic Resonance Imaging (fMRI). This problem is important because such classifiers constitute "virtual sensors" of hidden cognitive states, which may be useful in cognitive science research and clinical applications. In recent work, Mitchell, et al. [6,7,9] have demonstrated the feasibility of training such classifiers for individual human subjects (e.g., to distinguish whether the subject is reading an ambiguous or unambiguous sentence, or whether they are reading a noun or a verb). Here we extend that line of research, exploring how to train classifiers that can be applied across multiple human subjects, including subjects who were not involved in training the classifier. We describe the design of several machine learning approaches to training multiple-subject classifiers, and report experimental results demonstrating the success of these methods in learning cross-subject classifiers for two different fMRI data sets.	Carnegie Mellon Univ, Ctr Automated Learning & Discovery, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, XR (corresponding author), Carnegie Mellon Univ, Ctr Automated Learning & Discovery, 5000 Forbes Ave, Pittsburgh, PA 15213 USA.	xuerui.wang@cs.cmu.edu; rebecca.hutchinson@cs.cmu.edu; tom.mitchell@cs.cmu.edu						Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736; Keller TA, 2001, ANN CONV PSYCH SOC O; MASON R, 2003, IN PRESS J EXPT PSYC; MITCHELL TM, 2003, IN PRESS MACHINE LEA; MITCHELL TM, 2003, IN PRESS AM MED INF; MITCHELL TOM M., 1997, MACH LEARN, P2; PEREIRA F, 2001, PKDD 2001 FREIB GERM; Talairach J., 1988, COPLANAR STEREOTAXIC; Wagner AD, 1998, SCIENCE, V281, P1188, DOI 10.1126/science.281.5380.1188; 2001, NIPS 2001 BRAIN COMP	11	10	10	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						709	716						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500089
C	Amari, S; Park, H; Ozeki, T		Dietterich, TG; Becker, S; Ghahramani, Z		Amari, S; Park, H; Ozeki, T			Geometrical singularities in the neuromanifold of multilayer Perceptrons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				MACHINES; ERROR	Singularities are ubiquitous in the parameter space of hierarchical models such as multilayer perceptrons. At singularities, the Fisher information matrix degenerates, and the Cramer-Rao paradigm does no more hold, implying that the classical model selection theory such as AIC and MDL cannot be applied. It is important to study the relation between the generalization error and the training error at singularities. The present paper demonstrates a method of analyzing these errors both for the maximum likelihood estimator and the Bayesian predictive distribution in terms of Gaussian random fields, by using simple models.	RIKEN, Brain Sci Inst, Wako, Saitama 3510198, Japan	RIKEN	Amari, S (corresponding author), RIKEN, Brain Sci Inst, Hirosawa 2-1, Wako, Saitama 3510198, Japan.	amari@brain.riken.go.jp; hypark@brain.riken.go.jp; tomoko@brain.riken.go.jp						Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; AMARI S, 2000, METHODS INFORMATION; CHEN AM, 1993, NEURAL COMPUT, V5, P910, DOI 10.1162/neco.1993.5.6.910; DACUNHACASTELLE D, 1997, PROBABILITY STAT, V1, P285; FUKUMIZU K, 2001, 780 I STAT MATH; Hagiwara K, 2001, NEURAL NETWORKS, V14, P1419, DOI 10.1016/S0893-6080(01)00122-8; Hartigan J.A., 1985, P BERKELEY C HONOR J, V2, P807; Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4; RUGGER SM, 1997, NEURAL PROCESS LETT, V5, P63; Watanabe S, 2001, NEURAL COMPUT, V13, P899, DOI 10.1162/089976601300014402	13	10	10	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						343	350						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100043
C	Bi, JB; Bennett, KP		Dietterich, TG; Becker, S; Ghahramani, Z		Bi, JB; Bennett, KP			Duality, geometry, and support vector regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We develop an intuitive geometric framework for support vector regression (SVR). By examining when c-tubes exist, we show that SVR can be regarded as a classification problem in the dual space. Hard and soft E-tubes are constructed by separating the convex or reduced convex hulls respectively of the training data with the response variable shifted up and down by E. A novel SVR model is proposed based on choosing the max-margin plane between the two shifted datasets. Maximizing the margin corresponds to shrinking the effective epsilon-tube. In the proposed approach the effects of the choices of all parameters become clear geometrically.	Rensselaer Polytech Inst, Dept Math Sci, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Bi, JB (corresponding author), Rensselaer Polytech Inst, Dept Math Sci, Troy, NY 12180 USA.	bij2@rpi.edu; bennek@rpi.edu						Bennett K.P., 2000, ICML, P57; Crisp DJ, 2000, ADV NEUR IN, V12, P244; Keerthi SS, 2000, IEEE T NEURAL NETWOR, V11, P124, DOI 10.1109/72.822516; Mangasarian O.L., 1994, NONLINEAR PROGRAMMIN; SCHOLKOPF B, 1999, ADV NEURAL INFO P SY, V12; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	6	10	10	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						593	600						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100074
C	Dayan, P		Dietterich, TG; Becker, S; Ghahramani, Z		Dayan, P			Motivated reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				DOPAMINE; AMYGDALA	The standard reinforcement learning view of the involvement of neuromodulatory systems in instrumental conditioning includes a rather straightforward conception of motivation as prediction of sum future reward. Competition between actions is based on the motivating characteristics of their consequent states in this sense. Substantial, careful, experiments reviewed in Dickinson & Balleine,(12,13) into the neurobiology and psychology of motivation shows that this view is incomplete. In many cases, animals are faced with the choice not between many different actions at a given state, but rather whether a single response is worth executing at all. Evidence suggests that the motivational process underlying this choice has different psychological and neural properties from that underlying action choice. We describe and model these motivational systems, and consider the way they interact.	Gatsby Comutat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Dayan, P (corresponding author), Gatsby Comutat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.	dayan@gatsby.ucl.ac.uk						ADAMS CD, 1982, Q J EXP PSYCHOL-B, V34, P77, DOI 10.1080/14640748208400878; Baird III L. C., 1993, WLTR931146; BALKENIUS C, 1995, THESIS LUND U SWEDEN; BALLEINE BW, 1995, J EXP PSYCHOL ANIM B, V21, P203, DOI 10.1037/0097-7403.21.3.203; Balleine BW, 1998, NEUROPHARMACOLOGY, V37, P407, DOI 10.1016/S0028-3908(98)00033-1; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Berridge KC, 2001, PSYCHOL LEARN MOTIV, V40, P223, DOI 10.1016/s0079-7421(00)80022-5; BERRIDGE KC, 1989, Q J EXP PSYCHOL-B, V41, P121; Braver TS, 1999, BIOL PSYCHIAT, V46, P312, DOI 10.1016/S0006-3223(99)00116-X; Corbit LH, 2001, J NEUROSCI, V21, P3251, DOI 10.1523/JNEUROSCI.21-09-03251.2001; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; DICKINSON A, 1995, ANIM LEARN BEHAV, V23, P197, DOI 10.3758/BF03199935; DICKINSON A, 1994, ANIM LEARN BEHAV, V22, P1, DOI 10.3758/BF03199951; DICKINSON A, 2001, LEARNING MOTIVATION, V3; Estes WK, 1943, J EXP PSYCHOL, V32, P150, DOI 10.1037/h0058316; Holland PC, 1999, TRENDS COGN SCI, V3, P65, DOI 10.1016/S1364-6613(98)01271-6; HOLLAND PC, 1975, J EXP PSYCHOL ANIM B, V1, P355, DOI 10.1037/0097-7403.1.4.355; Konorski J., 1948, CONDITIONED REFLEXES; Konorski J., 1967, INTEGRATIVE ACTIVITY; Mackintosh N.J., 1974, PSYCHOL ANIMAL LEARN; MONTAGUE PR, 1995, NATURE, V377, P725, DOI 10.1038/377725a0; Montague PR, 1996, J NEUROSCI, V16, P1936, DOI 10.1523/jneurosci.16-05-01936.1996; Schoenbaum G, 1999, J NEUROSCI, V19, P1876; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; SPIER E, 1997, THESIS BALLIOL COLL; Sutton R. S., 1985, 7 ANN C COGN SCI SOC, P54; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; SUTTON RS, 1981, COGN BRAIN THEORY, V4, P217; SUTTON RS, 1995, P 12 INT C MACH LEAR, P531; Tolman EC, 1938, PSYCHOL REV, V45, P1, DOI 10.1037/h0062733; Watkins C.J.C.H., 1989, LEARNING DELAYED REW	31	10	10	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						11	18						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100002
C	Domingos, P; Hulten, G		Dietterich, TG; Becker, S; Ghahramani, Z		Domingos, P; Hulten, G			Learning from infinite data in finite time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We propose the following general method for scaling learning algorithms to arbitrarily large data sets. Consider the model M-(n) over right arrow learned by the algorithm using n(i) examples in step i ((n) over right arrow = (n(1),..., n(m))), and the model M-infinity that would be learned using infinite examples. Upper-bound the loss L(M-(n) over right arrow, M-infinity) between them as a function of (n) over right arrow, and then minimize the algorithm's time complexity f((n) over right arrow) subject to the constraint that L(M-infinity, M-(n) over right arrow) be at most E with probability at most delta. We apply this method to the EM algorithm for mixtures of Gaussians. Preliminary experiments on a series of large data sets provide evidence of the potential of this approach.	Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98185 USA	University of Washington; University of Washington Seattle	Domingos, P (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98185 USA.	pedrod@cs.washington.edu; ghulten@cs.washington.edu						DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Domingos P., 2000, Proceedings. KDD-2000. Sixth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P71, DOI 10.1145/347090.347107; Domingos P., 2001, ICML, P106; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Meek C., 2000, MSRTR0134; THIESSON B, 2001, MSRTR9931; Wolman A, 1999, USENIX ASSOCIATION PROCEEDINGS OF THE 2ND USENIX SYMPOSIUM ON INTERNET TECHNOLOGIES AND SYSTEMS (USITS'99), P25; Zhang T., 1996, P 1996 ACM SIGMOD IN, P103	8	10	11	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						673	680						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100084
C	Harmeling, S; Ziehe, A; Kawanabe, M; Muller, KR		Dietterich, TG; Becker, S; Ghahramani, Z		Harmeling, S; Ziehe, A; Kawanabe, M; Muller, KR			Kernel feature spaces and nonlinear blind source separation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In kernel based learning the data is mapped to a kernel feature space of a dimension that corresponds to the number of training data points. In practice, however, the data forms a smaller submanifold in feature space, a fact that has been used e.g. by reduced set techniques for SVMs. We propose a new mathematical construction that permits to adapt to the intrinsic dimension and to find an orthonormal basis of this submanifold. In doing so, computations get much simpler and more important our theoretical framework allows to derive elegant kernelized blind source separation (BSS) algorithms for arbitrary invertible nonlinear mixings. Experiments demonstrate the good performance and high computational efficiency of our kTDSEP algorithm for the problem of nonlinear BSS.	Fraunhofer FIRST IDA, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Harmeling, S (corresponding author), Fraunhofer FIRST IDA, Kekulestr 7, D-12489 Berlin, Germany.	harmeli@first.fhg.de; ziehe@first.fhg.de; kawanabe@first.fhg.de; klaus@first.fhg.de	Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685				Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307; BUREL G, 1992, NEURAL NETWORKS, V5, P937, DOI 10.1016/S0893-6080(05)80090-5; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546; Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250; Cristianini N., 2000, INTRO SUPPORT VECTOR; FYFE C, 2000, P INT WORKSH IND COM, P279; HYVARINEN A, 1910, INDEPENDENT COMPONEN; LEE TW, 1997, NEURAL NETWORKS SIGN, V7, P406; Lin JK, 1997, NEURAL COMPUT, V9, P1305, DOI 10.1162/neco.1997.9.6.1305; Marques G., 1999, P 1 INT WORKSH IND C, P277; Muller KR, 2001, IEEE T NEURAL NETWOR, V12, P181, DOI 10.1109/72.914517; Pajunen P., 1996, Progress in Neural Information Processing. Proceedings of the International Conference on Neural Information Processing, P1207; PAJUNEN P, 1997, P 1997 INT C ART NEU, P541; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Taleb A, 1999, IEEE T SIGNAL PROCES, V47, P2807, DOI 10.1109/78.790661; VALPOLA H, 2000, P INT WORKSH IND COM, P351; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Yang HH, 1998, SIGNAL PROCESS, V64, P291, DOI 10.1016/S0165-1684(97)00196-5; ZIEHE A, 1998, P INT C ART NEUR NET, P675	21	10	10	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						761	768						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100095
C	Sollich, P		Dietterich, TG; Becker, S; Ghahramani, Z		Sollich, P			Gaussian process regression with mismatched models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Learning curves for Gaussian process regression are well understood when the 'student' model happens to match the 'teacher' (true data generation process). I derive approximations to the learning curves for the more generic case of mismatched models, and find very rich behaviour: For large input space dimensionality, where the results become exact, there are universal (student-independent) plateaux in the learning curve, with transitions in between that can exhibit arbitrarily many over-fitting maxima; over-fitting can occur even if the student estimates the teacher noise level correctly. In lower dimensions, plateaux also appear, and the learning curve remains dependent on the mismatch between student and teacher even in the asymptotic limit of a large number of training examples. Learning with excessively strong smoothness assumptions can be particularly dangerous: For example, a student with a standard radial basis function covariance function will learn a rougher teacher function only logarithmically slowly. All predictions are confirmed by simulations.	Kings Coll London, Dept Math, London WC2R 2LS, England	University of London; King's College London	Sollich, P (corresponding author), Kings Coll London, Dept Math, Strand, London WC2R 2LS, England.	peter.sollich@kcl.ac.uk	Sollich, Peter/ABC-2993-2020; Sollich, Peter/H-2174-2011	Sollich, Peter/0000-0003-0169-7893				MACKAY DJC, GAUSSIAN PROCESSES; MALZAHN D, NIPS, V13, P273; MICHELLI CA, 1981, APPROXIMATION THEORY, P329; OPPER M, 1997, THEORETICAL ASPECTS, P17; SOLLICH P, NIPS, V11, P344; Williams CKI, 2000, MACH LEARN, V40, P77, DOI 10.1023/A:1007601601278; Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599	10	10	10	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						519	526						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100065
C	Szummer, M; Jaakkola, T		Leen, TK; Dietterich, TG; Tresp, V		Szummer, M; Jaakkola, T			Kernel expansions with unlabeled examples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Modern classification applications necessitate supplementing the few available labeled examples with unlabeled examples to improve classification performance. We present a new tractable algorithm for exploiting unlabeled examples in discriminative classification. This is achieved essentially by expanding the input vectors into longer feature vectors via both labeled and unlabeled examples. The resulting classification method can be interpreted as a discriminative kernel density estimate and is readily trained via the EM algorithm, which in this case is both discriminative and achieves the optimal solution. We provide, in addition, a purely discriminative formulation of the estimation problem by appealing to the maximum entropy framework. We demonstrate that the proposed approach requires very few labeled examples for high classification accuracy.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Szummer, M (corresponding author), MIT, AI Lab, Cambridge, MA 02139 USA.	szummer@ai.mit.edu; tommi@ai.mit.edu						Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Castelli V, 1996, IEEE T INFORM THEORY, V42, P2102, DOI 10.1109/18.556600; HOFMANN T, 1998, TR98042 INT COMP SCI; JAAKKOLA T, 1999, ADV NEURAL INFORMATI, V12; Joachims T., 1999, INT C MACH LEARN; MILLER D, 1996, ADV NEURAL INFORMATI, V9, P571; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; TONG S, 2000, P AAAI; Vapnik V.N, 1998, STAT LEARNING THEORY	9	10	10	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						626	632						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800089
C	Bartlett, MS; Donato, G; Movellan, JR; Hager, JC; Ekman, P; Sejnowski, TJ		Solla, SA; Leen, TK; Muller, KR		Bartlett, MS; Donato, G; Movellan, JR; Hager, JC; Ekman, P; Sejnowski, TJ			Image representations for facial expression coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				FACE RECOGNITION; EIGENFACES	The Facial Action Coding System (FACS) (9) is an objective method for quantifying facial movement in terms of component actions. This system is widely used in behavioral investigations of emotion, cognitive processes, and social interaction. The coding is presently performed by highly trained human experts. This paper explores and compares techniques for automatically recognizing facial actions in sequences of images. These methods include unsupervised learning techniques for finding basis images such as principal component analysis, independent component analysis and local feature analysis, and supervised learning techniques such as Fisher's linear discriminants. These data-driven bases are compared to Gabor wavelets, in which the basis images are predefined. Best performances were obtained using the Gabor wavelet representation and the independent component representation, both of which achieved 96% accuracy for classifying 12 facial actions. The ICA representation employs 2 orders of magnitude fewer basis images than the Gabor representation and takes 90% less CPU time to compute for new images. The results provide converging support for using local basis images, high spatial frequencies, and statistical independence for classifying facial actions.	Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Bartlett, MS (corresponding author), Univ Calif San Diego, 0523, La Jolla, CA 92093 USA.		Sejnowski, Terrence/AAV-5558-2021					Bartlett M., 1998, THESIS U CALIFORNIA; Bartlett MS, 1998, P SOC PHOTO-OPT INS, V3299, P528, DOI 10.1117/12.320144; Belhumeur PN, 1997, IEEE T PATTERN ANAL, V19, P711, DOI 10.1109/34.598228; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; BRUNELLI R, 1993, IEEE T PATTERN ANAL, V15, P1042, DOI 10.1109/34.254061; Cohn JF, 1999, PSYCHOPHYSIOLOGY, V36, P35, DOI 10.1017/S0048577299971184; Donato G, 1999, IEEE T PATTERN ANAL, V21, P974, DOI 10.1109/34.799905; Ekman P., 2002, FACIAL ACTION CODING; Gray MS, 1997, PROCEEDINGS OF THE 4TH JOINT SYMPOSIUM ON NEURAL COMPUTATION, VOL 7, P92; LADES M, 1993, IEEE T COMPUT, V42, P300, DOI 10.1109/12.210173; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; MOVELLAN JR, 1995, ADV NEURAL INFORMATI, V7, P851; PADGETT C, 1997, ADV NEURAL INFORMATI, V9; Penev PS, 1996, NETWORK-COMP NEURAL, V7, P477, DOI 10.1088/0954-898X/7/3/002; SIMONCELLI EP, 1997, 31 AS C SIGN SYST CO; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71	17	10	10	0	39	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						886	892						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700125
C	Downs, OB; MacKay, DJC; Lee, DD		Solla, SA; Leen, TK; Muller, KR		Downs, OB; MacKay, DJC; Lee, DD			The nonnegative Boltzmann machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The nonnegative Boltzmann machine (NNBM) is a recurrent neural network model that can describe multimodal nonnegative data. Application of maximum likelihood estimation to this model gives a learning rule that is analogous to the binary Boltzmann machine. We examine the utility of the mean field approximation for the NNBM, and describe how Monte Carlo sampling techniques can be used to learn its parameters. Reflective slice sampling is particularly well-suited for this distribution, and can efficiently be implemented to sample the distribution. We illustrate learning of the NNBM on a translationally invariant distribution, as well as on a generative model for images of human faces.	Princeton Univ, Hopfield Grp, Princeton, NJ 08544 USA	Princeton University	Downs, OB (corresponding author), Princeton Univ, Hopfield Grp, Schultz Bldg, Princeton, NJ 08544 USA.	obdowns@princeton.edu; mackay@mrao.cam.ac.uk; ddlee@bell-labs.com	Lee, Daniel D./B-5753-2013	Lee, Daniel/0000-0003-4239-8777				ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; GALLAND CC, 1993, NETWORK-COMP NEURAL, V4, P355, DOI 10.1088/0954-898X/4/3/007; Hinton G. E., 1983, Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition, P448; KAPPEN HJ, 1997, PATTERN RECOGNTION P, V5; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Mackay DJC, 1998, NATO ADV SCI I D-BEH, V89, P175; Neal R.M., 1997, 9722 U TOR DEP STAT; Neal RM., 1995, 9508 U TOR DEP STAT; Socci ND, 1998, ADV NEUR IN, V10, P350	10	10	12	0	8	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						428	434						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700061
C	Manwani, A; Steinmetz, PN; Koch, C		Solla, SA; Leen, TK; Muller, KR		Manwani, A; Steinmetz, PN; Koch, C			Channel noise in excitable neuronal membranes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				RELIABILITY	Stochastic fluctuations of voltage-gated ion channels generate current and voltage noise in neuronal membranes. This noise may be a critical determinant of the efficacy of information processing within neural systems. Using Monte-Carlo simulations, we carry out a systematic investigation of the relationship between channel kinetics and the resulting membrane voltage noise using a stochastic Markov version of the Mainen-Sejnowski model of dendritic excitability in cortical neurons. Our simulations show that kinetic parameters which lead to an increase in membrane excitability (increasing channel densities, decreasing temperature) also lead to an increase in the magnitude of the sub-threshold voltage noise. Noise also increases as the membrane is depolarized from rest towards threshold. This suggests that channel fluctuations may interfere with a neuron's ability to function as an integrator of its synaptic inputs and may limit the reliability and precision of neural information processing.	CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA	California Institute of Technology	Manwani, A (corresponding author), CALTECH, Computat & Neural Syst Program, MS 139-74, Pasadena, CA 91125 USA.		London, Michael/B-4175-2010	London, Michael/0000-0001-5137-1707; Koch, Christof/0000-0001-6482-8067				Chow CC, 1996, BIOPHYS J, V71, P3013, DOI 10.1016/S0006-3495(96)79494-8; DeFelice L. J., 1981, INTRO MEMBRANE NOISE; DEFELICE LJ, 1992, J STAT PHYS, V70, P339; HODGKIN AL, 1952, J PHYSIOL-LONDON, V117, P500, DOI 10.1113/jphysiol.1952.sp004764; Johnston D., 1995, FDN CELLULAR NEUROPH; KOCH C, 1984, BIOL CYBERN, V50, P15, DOI 10.1007/BF00317936; MAINEN ZF, 1995, SCIENCE, V268, P1503, DOI 10.1126/science.7770778; Manwani A, 1998, ADV NEUR IN, V10, P201; MANWANI A, 1999, IN PRESS NEURAL COMP; MAURO A, 1970, J GEN PHYSIOL, V55, P497, DOI 10.1085/jgp.55.4.497; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; SABAH NH, 1972, BIOPHYS J, V12, P1132, DOI 10.1016/S0006-3495(72)86150-2; Schneidman E, 1998, NEURAL COMPUT, V10, P1679, DOI 10.1162/089976698300017089; Shadlen MN, 1998, J NEUROSCI, V18, P3870; SKAUGEN E, 1979, ACTA PHYSIOL SCAND, V107, P343, DOI 10.1111/j.1748-1716.1979.tb06486.x; STEINMETZ PN, 1999, UNPUB SUBTHESHOLD VO; STRASSBERG AF, 1993, NEURAL COMPUT, V5, P843, DOI 10.1162/neco.1993.5.6.843	17	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						143	149						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700021
C	Bartlett, PL; Maiorov, V; Meir, R		Kearns, MS; Solla, SA; Cohn, DA		Bartlett, PL; Maiorov, V; Meir, R			Almost linear VC dimension bounds for piecewise polynomial networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				VAPNIK-CHERVONENKIS DIMENSION	We compute upper and lower bounds on the VC dimension of feedforward networks of units with piecewise polynomial activation functions. We show that if the number of layers is fixed, then the VC dimension grows as W log W, where W is the number of parameters in the network. This result stands in opposition to the case where the number of layers is unbounded, in which case the VC dimension grows as W-2.	Australian Natl Univ, Dept Syst Engn, Canberra, ACT 0200, Australia	Australian National University	Bartlett, PL (corresponding author), Australian Natl Univ, Dept Syst Engn, Canberra, ACT 0200, Australia.			Bartlett, Peter/0000-0002-8760-3140				ANTHONY M, 1999, IN PRESS NEURAL NETW; Bartlett PL, 1998, NEURAL COMPUT, V10, P2159, DOI 10.1162/089976698300017016; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Koiran P, 1997, J COMPUT SYST SCI, V54, P190, DOI 10.1006/jcss.1997.1479; MAASS W, 1994, NEURAL COMPUT, V6, P877, DOI 10.1162/neco.1994.6.5.877; MAIOROV V, 1997, UNPUB NEAR OPTIMALIT; SAKURAI A, 1993, WORLD C NEUAL NETWOR, V3, P540; SAKURAI A, 1999, ADV NEURAL INFORMATI, V11; Vapnik V., 1982, ESTIMATION DEPENDENC; VIDYASAGAR M, 1996, THEORY LEARNING GEN	11	10	10	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						190	196						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700027
C	Boyen, X; Koller, D		Kearns, MS; Solla, SA; Cohn, DA		Boyen, X; Koller, D			Approximate learning of dynamic models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Inference is a key component in learning probabilistic models from partially observable data. When learning temporal models, each of the many inference phases requires a traversal over an entire long data sequence; furthermore, the data structures manipulated are exponentially large, making this process computationally expensive. In [2], we describe an approximate inference algorithm for monitoring stochastic processes, and prove bounds on its approximation error. In this paper, we apply this algorithm as an approximate forward propagation step in an EM algorithm for learning temporal Bayesian networks. We provide a related approximation for the backward step, and prove error bounds for the combined algorithm. We show empirically that, for a real-life domain, EM using our inference algorithm is much faster than EM using exact inference, with almost no degradation in quality of the learned model. We extend our analysis to the online learning task, showing a bound on the error resulting from restricting attention to a small window of observations. We present an online EM learning algorithm for dynamic systems, and show that it learns much faster than standard oft-line EM.	Comp Sci Dept 1A, Stanford, CA 94305 USA		Boyen, X (corresponding author), Comp Sci Dept 1A, Stanford, CA 94305 USA.	xb@cs.stanford.edu; koller@cs.stanford.edu						ARTZROUNI M, 1995, LINEAR ALGEBRA APPL, V214, P93, DOI 10.1016/0024-3795(93)00058-8; Boyen X, 1998, P 14 ANN C UNC ART I, P33; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DEAN T, 1989, COMP INT, V5; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FRIEDMAN N, 1998, P 14 C UNC ART INT, P139; GHAHRAMANI Z, 1996, NIPS 8; Ka1man R., 1960, J BASIC ENG-T ASME, V82, P34, DOI DOI 10.1115/1.3662552; LAURITZEN SL, 1988, J ROY STAT SOC B, V50; Neal R. M., 1998, LEARNING GRAPHICAL M; Rabiner L. R., 1986, IEEE ACOUSTICS SPEEC; Zweig G, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P173	12	10	10	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						396	402						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700056
C	Gat, I; Tishby, N		Kearns, MS; Solla, SA; Cohn, DA		Gat, I; Tishby, N			Synergy and redundancy among brain cells of behaving monkeys	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NEURONS	Determining the relationship between the activity of a single nerve cell to that of an entire population is a fundamental question that bears on the basic neural computation paradigms. In this paper we apply an information theoretic approach to quantify the level of cooperative activity among cells in a behavioral context. It is possible to discriminate between synergetic activity of the cells vs. redundant activity, depending on the difference between the information they provide when measured jointly and the information they provide independently. We define a synergy value that is positive in the first case and negative in the second and show that the synergy value can be measured by detecting the behavioral mode of the animal from simultaneously recorded activity of the cells. We observe that among cortical cells positive synergy fan be found, while cells from the basal ganglia, active during the same task, do not exhibit similar synergetic activity.	Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Gat, I (corresponding author), Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel.							ABELES M, 1990, NETWORK, V1; Abeles M., 1991, CORTICONICS; AHISSAR E, 1992, SCIENCE, V257, P1412, DOI 10.1126/science.1529342; Amit DJ, 1989, MODELING BRAIN FUNCT, DOI 10.1017/CBO9780511623257; BRENNER N, 1998, EC IMPULSES STIFFNES; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Gat I, 1997, NETWORK-COMP NEURAL, V8, P297, DOI 10.1088/0954-898X/8/3/005; GAT I, UNPUB COMP STUDY DIF; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; Raz A, 1996, J NEUROPHYSIOL, V76, P2083, DOI 10.1152/jn.1996.76.3.2083	10	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						111	117						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700016
C	Ratsch, G; Onoda, T; Muller, KR		Kearns, MS; Solla, SA; Cohn, DA		Ratsch, G; Onoda, T; Muller, KR			Regularizing AdaBoost	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Boosting methods maximize a hard classification margin and are known as powerful techniques that do not exhibit overfitting for low noise cases. Also for noisy data boosting will try to enforce a hard margin and thereby give too much weight to outliers, which then leads to the dilemma of non-smooth fits and overfitting. Therefore we propose three algorithms to allow for soft margin classification by introducing regularization with slack variables into the boosting concept: (1) AdaBoost(reg) and regularized versions of (2) linear and (3) quadratic programming AdaBoost. Experiments show the usefulness of the proposed algorithms in comparison to another soft margin classifier: the support vector machine.	GMD First, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Ratsch, G (corresponding author), GMD First, Rudower Chaussee 5, D-12489 Berlin, Germany.		Rätsch, Gunnar/B-8182-2009; Mueller, Klaus-Robert/Y-3547-2019; Rätsch, Gunnar/O-5914-2017	Mueller, Klaus-Robert/0000-0002-3861-7685; Rätsch, Gunnar/0000-0001-5486-8532				Bishop, 1995, NEURAL NETWORKS PATT; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; BREIMAN L, 1997, 504 BERK STAT DEPT; BREIMAN L, 1997, 460 BERK STAT DEP; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; GROVE AJ, 1998, IN PRESS P 15 NAT C; LeCun Y., 1995, Neural Networks: The Statistical Mechanics Perspective. Proceedings of the CTP-PBSRI. Joint Workshop on Theoretical Physics, P261; ONODA T, 1998, P ICANN 98 APR; QUINLAN J, LNAI, V1160, P143; RATSCH G, 1998, NCTR1998021; SCHAPIRE R, 1998, MACH LEARN, P148; SCHAPIRE R, P COLT 98; SCHWENK H, 1997, LNCS, V1327, P967; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	14	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						564	570						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700080
C	Sakurai, A		Kearns, MS; Solla, SA; Cohn, DA		Sakurai, A			Tight bounds for the VC-dimension of piecewise polynomial networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					O(ws(s log d + log(dqh/s))) and O(ws((h/s) log q)+ log(dqh/s)) are upper bounds for the VC-dimension of a set of neural networks of units with piecewise polynomial activation functions, where s is the depth of the network, h is the number of hidden units, w is the number of adjustable parameters, q is the maximum of the number of polynomial segments of the activation function, and d is the maximum degree of the polynomials; also Omega(ws log(dqh/s)) is a lower bound for the VC-dimension of such a network set, which are tight for the cases s = Theta(h) and a is constant. For the special case q = 1, the VC-dimension is Theta(ws log d).										ANTHONY M, 1995, NCTR95011; Goldberg P., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P361, DOI 10.1145/168304.168377; KARPINSKI M, 1995, P 27 ACM S THEOR COM, P200; Koiran P, 1997, J COMPUT SYST SCI, V54, P190, DOI 10.1006/jcss.1997.1479; Maass W., 1993, Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, P335, DOI 10.1145/167088.167193; MAASS W, 1994, NEURAL COMPUT, V6, P877, DOI 10.1162/neco.1994.6.5.877; Milnor J., 1964, P AM MATH SOC, V15, P275, DOI DOI 10.1090/S0002-9939-1964-0161339-9; SAKURAI A, 1995, THEOR COMPUT SCI, V137, P109, DOI 10.1016/0304-3975(94)00163-D; SAKURAI A, 1993, P NOLTA 93, P239; SAKURAI A, 1993, P WORLD C NEUR NETW, V3, P540; WARREN HE, 1968, T AM MATH SOC, V133, P167, DOI 10.2307/1994937	11	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						323	329						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700046
C	Deneve, S; Pouget, A		Jordan, MI; Kearns, MJ; Solla, SA		Deneve, S; Pouget, A			Neural basis of object-centered representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We present a neural model that can perform eye movements to a particular side of an object regardless of the position and orientation of the object in space, a generalization of a task which has been recently used by Olson and Gettner [4] to investigate the neural structure of object-centered representations. Our model uses an intermediate representation in which units have oculocentric receptive fields- just like collicular neurons- whose gain is modulated by the side of the object to which the movement is directed, as well as the orientation of the object. We show that these gain modulations are consistent with Olson and Gettner's single cell recordings in the supplementary eye field. This demonstrates that it is possible to perform an object-centered task without a representation involving an object-centered map, viz., without neurons whose receptive fields are defined in object-centered coordinates. We also show that the same approach can account for object-centered neglect, a situation in which patients with a right parietal lesion neglect the left side of objects regardless of the orientation of the objects.	Georgetown Univ, Georgetown Inst Computat & Cognit Sci, Washington, DC 20007 USA	Georgetown University	Deneve, S (corresponding author), Georgetown Univ, Georgetown Inst Computat & Cognit Sci, Washington, DC 20007 USA.								0	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						24	30						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700004
C	El-Yaniv, R; Fine, S; Tishby, N		Jordan, MI; Kearns, MJ; Solla, SA		El-Yaniv, R; Fine, S; Tishby, N			Agnostic classification of Markovian sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Classification of finite sequences without explicit knowledge of their statistical nature is a fundamental problem with many important applications. We propose a new information theoretic approach to this problem which is based on the following ingredients: (i) sequences are similar when they are likely to be generated by the same source; (ii) cross entropies can be estimated via "universal compression"; (iii) Markovian sequences can be asymptotically-optimally merged. With these ingredients we design a method for the classification of discrete sequences whenever they can be compressed. We introduce the method and illustrate its application for hierarchical clustering of languages and for estimating similarities of protein sequences.	Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Tishby, N (corresponding author), Hebrew Univ Jerusalem, Inst Comp Sci, IL-91904 Jerusalem, Israel.								0	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						465	471						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700066
C	Maass, W; Zador, AM		Jordan, MI; Kearns, MJ; Solla, SA		Maass, W; Zador, AM			Dynamic stochastic synapses as computational units	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In most neural network models, synapses are treated as static weights that change only on the slow time scales of learning. In fact, however, synapses are highly dynamic, and show use-dependent plasticity over a wide range of time scales. Moreover, synaptic transmission is an inherently stochastic process: a spike arriving at a presynaptic terminal triggers release of a vesicle of neurotransmitter from a release site with a probability that can be much less than one. Changes in release probability represent one of the main mechanisms by which synaptic efficacy is modulated in neural circuits. We propose and investigate a simple model for dynamic stochastic synapses that can easily be integrated into common models for neural computation. We show through computer simulations and rigorous theoretical analysis that this model for a dynamic stochastic synapse increases computational power in a nontrivial way. Our results may have implications for the processing of time-varying signals by both biological and artificial neural networks.	Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Maass, W (corresponding author), Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria.	maass@igi.tu-graz.ac.at; zador@salk.edu		Zador, Anthony/0000-0002-8431-9136					0	10	11	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						194	200						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700028
C	Riesenhuber, M; Poggio, T		Jordan, MI; Kearns, MJ; Solla, SA		Riesenhuber, M; Poggio, T			Just one view: Invariances in inferotemporal cell tuning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In macaque inferotemporal cortex (IT), neurons have been found to respond selectively to complex shapes while showing broad tuning ("invariance") with respect to stimulus transformations such as translation and scale changes and a limited tuning to rotation in depth. Training monkeys with novel, paperclip-like objects, Logothetis et al.(9) could investigate whether these invariance properties are due to experience with exhaustively many transformed instances of an object or if there are mechanisms that allow the cells to show response invariance also to previously unseen instances of that object. They found object-selective cells in anterior IT which exhibited limited invariance to various transformations after training with single object views. While previous models accounted for the tuning of the cells for rotations in depth and for their selectivity to a specific object relative to a population of distractor objects,(14,1) the model described here attempts to explain in a biologically plausible way the additional properties of translation and size invariance. Using the same stimuli as in the experiment, we find that model IT neurons exhibit invariance properties which closely parallel those of real neurons. Simulations show that the model is capable of unsupervised learning of view-tuned neurons.	MIT, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Riesenhuber, M (corresponding author), MIT, Ctr Biol & Computat Learning, E25-201, Cambridge, MA 02139 USA.			Riesenhuber, Maximilian/0000-0002-5744-0408					0	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						215	221						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700031
C	Bair, W; Cavanaugh, JR; Movshon, JA		Mozer, MC; Jordan, MI; Petsche, T		Bair, W; Cavanaugh, JR; Movshon, JA			Reconstructing stimulus velocity from neuronal responses in area MT	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We employed a white-noise velocity signal to study the dynamics of the response of single neurons in the cortical area MT to visual motion. Responses were quantified using reverse correlation, optimal linear reconstruction filters, and reconstruction signal-to-noise ratio (SNR). The SNR and lower bound estimates of information rate were lower than are expected. Ninety percent of the information was transmitted below 18 Hz, and the highest lower bound on bit rate was 12 bits/s. A simulated opponent motion energy subunit with Poisson spike statistics was able to out-perform the MT neurons. The temporal integration window, measured from the reverse correlation half-width, ranged from 30-90 ms. The window was narrower when a stimulus moved faster, but did not change when temporal frequency was held constant.			Bair, W (corresponding author), NYU,HOWARD HUGHES MED INST,4 WASHINGTON PL,ROOM 809,NEW YORK,NY 10003, USA.								0	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						34	40						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00005
C	Halkjaer, S; Winther, O		Mozer, MC; Jordan, MI; Petsche, T		Halkjaer, S; Winther, O			The effect of correlated input data on the dynamics of learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The convergence properties of the gradient descent algorithm in the case of the linear perceptron may be obtained from the response function. We derive a general expression for the response function and apply it to the case of data with simple input correlations. It is found that correlations severely may slow down learning. This explains the success of PCA as a method for reducing training time. Motivated by this finding we furthermore propose to transform the input data by removing the mean across input variables as well as examples to decrease correlations. Numerical findings for a medical classification problem are in fine agreement with the theoretical results.			Halkjaer, S (corresponding author), NIELS BOHR INST,CONNECT,BLEGDAMSVEJ 17,DK-2100 COPENHAGEN,DENMARK.								0	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						169	175						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00024
C	Saad, D; Solla, SA		Mozer, MC; Jordan, MI; Petsche, T		Saad, D; Solla, SA			Learning with noise and regularizers in multilayer neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We study the effect of noise and regularization in an on-line gradient-descent learning scenario for a general two-layer student network with an arbitrary number of hidden units. Training examples are randomly drawn input vectors labeled by a two-layer teacher network with an arbitrary number of hidden units; the examples are corrupted by Gaussian noise affecting either the output or the model itself. We examine the effect of both types of noise and that of weight-decay regularization on the dynamical evolution of the order parameters and the generalization error in various phases of the learning process.			Saad, D (corresponding author), ASTON UNIV,DEPT COMP SCI & APPL MATH,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	10	10	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						260	266						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00037
C	vanSchaik, A; Fragniere, E; Vittoz, E		Mozer, MC; Jordan, MI; Petsche, T		vanSchaik, A; Fragniere, E; Vittoz, E			A silicon model of amplitude modulation detection in the auditory brainstem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Detection of the periodicity of amplitude modulation is a major step in the determination of the pitch of a sound. In this article we will present a silicon model that uses synchronicity of spiking neurons to extract the fundamental frequency of a sound. It is based on the observation that the so called 'Choppers' in the mammalian Cochlear Nucleus synchronize well for certain rates of amplitude modulation, depending on the cell's intrinsic chopping frequency. Our silicon model uses three different circuits, i.e., an artificial cochlea, an Inner Hair Cell circuit, and a spiking neuron circuit.			vanSchaik, A (corresponding author), SWISS FED INST TECHNOL,MANTRA CTR NEUROMIMET SYST,CH-1015 LAUSANNE,SWITZERLAND.			van Schaik, Andre/0000-0001-6140-017X					0	10	10	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						741	747						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00105
C	Barto, AG; Buckingham, JT; Houk, JC		Touretzky, DS; Mozer, MC; Hasselmo, ME		Barto, AG; Buckingham, JT; Houk, JC			A predictive switching model of cerebellar movement control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MASSACHUSETTS,DEPT COMP SCI,AMHERST,MA 01003	University of Massachusetts System; University of Massachusetts Amherst									0	10	10	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						138	144						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00020
C	Grunewald, A		Touretzky, DS; Mozer, MC; Hasselmo, ME		Grunewald, A			A model of transparent motion and non-transparent motion aftereffects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MAX PLANCK INST BIOL CYBERNET,D-72076 TUBINGEN,GERMANY	Max Planck Society									0	10	10	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						837	843						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00118
C	Rasmussen, CE		Touretzky, DS; Mozer, MC; Hasselmo, ME		Rasmussen, CE			A practical Monte Carlo implementation of Bayesian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TORONTO,DEPT COMP SCI,TORONTO,ON M5S 1A4,CANADA	University of Toronto				Rasmussen, Carl Edward/0000-0001-8899-7850					0	10	10	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						598	604						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00085
C	Saad, D; Solla, SA		Touretzky, DS; Mozer, MC; Hasselmo, ME		Saad, D; Solla, SA			Dynamics of on-line gradient descent learning for multilayer neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						ASTON UNIV,DEPT COMP SCI & APPL MATH,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND	Aston University				Saad, David/0000-0001-9821-2623					0	10	10	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						302	308						5	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00043
C	BELL, AJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BELL, AJ			SELF-ORGANIZATION IN REAL NEURONS - ANTI-HEBB IN CHANNEL SPACE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	10	10	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						59	66						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00008
C	HAFFNER, P; WAIBEL, A		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HAFFNER, P; WAIBEL, A			MULTISTATE TIME-DELAY NEURAL NETWORKS FOR CONTINUOUS SPEECH RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	10	10	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						135	142						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00017
C	KOISTINEN, P; HOLMSTROM, L		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KOISTINEN, P; HOLMSTROM, L			KERNEL REGRESSION AND BACKPROPAGATION TRAINING WITH NOISE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Koistinen, Petri/G-2334-2012						0	10	10	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1033	1039						7	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00127
C	SCHMIDHUBER, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SCHMIDHUBER, J			LEARNING UNAMBIGUOUS REDUCED SEQUENCE DESCRIPTIONS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Peters, Jan/P-6027-2019	Peters, Jan/0000-0002-5266-8091					0	10	10	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						291	298						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00036
C	SCHRAUDOLPH, NN; SEJNOWSKI, TJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SCHRAUDOLPH, NN; SEJNOWSKI, TJ			COMPETITIVE ANTI-HEBBIAN LEARNING OF INVARIANTS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Sejnowski, Terrence/AAV-5558-2021						0	10	10	1	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1017	1024						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00125
C	Ruis, L; Andreas, J; Baroni, M; Bouchacourt, D; Lake, BM		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Ruis, Laura; Andreas, Jacob; Baroni, Marco; Bouchacourt, Diane; Lake, Brenden M.			A Benchmark for Systematic Generalization in Grounded Language Understanding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Humans easily interpret expressions that describe unfamiliar situations composed from familiar parts ("greet the pink brontosaurus by the ferris wheel"). Modern neural networks, by contrast, struggle to interpret novel compositions. In this paper, we introduce a new benchmark, gSCAN, for evaluating compositional generalization in situated language understanding. Going beyond a related benchmark that focused on syntactic aspects of generalization, gSCAN defines a language grounded in the states of a grid world, facilitating novel evaluations of acquiring linguistically motivated rules. For example, agents must understand how adjectives such as 'small' are interpreted relative to the current world state or how adverbs such as 'cautiously' combine with new verbs. We test a strong multi-modal baseline model and a state-of-the-art compositional method finding that, in most cases, they fail dramatically when generalization requires systematic compositional rules.	[Ruis, Laura] Univ Amsterdam, Amsterdam, Netherlands; [Andreas, Jacob] MIT, Cambridge, MA 02139 USA; [Baroni, Marco] ICREA, Barcelona, Spain; [Ruis, Laura; Baroni, Marco; Bouchacourt, Diane; Lake, Brenden M.] Facebook AI Res, Menlo Pk, CA USA; [Lake, Brenden M.] NYU, New York, NY 10003 USA	University of Amsterdam; Massachusetts Institute of Technology (MIT); ICREA; Facebook Inc; New York University	Ruis, L (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	laura.ruis@student.uva.nl; jda@mit.edu; mbaroni@fb.com; dianeb@fb.com; brenden@nyu.edu			NSF [1922658]	NSF(National Science Foundation (NSF))	We are grateful to Adina Williams and Ev Fedorenko for very helpful discussions, to Joao Loula who did important initial work to explore compositional learning in a grid world, to Robin Vaaler for comments on an earlier version of this paper, and to Esther Vecht for important design advice and support. Through B. Lake's position at NYU, this research was partially funded by NSF Award 1922658 NRT-HDR: FUTURE Foundations, Translation, and Responsibility for Data Science.	Andreas J., 2019, P ICLR NEW ORL LA; Andreas Jacob, 2020, P ACL; Bahdanau D., 2019, CLOSURE ASSESSING SY; Bahdanau D., 2019, INT C LEARN REPR ICL, P1; Bastings Jasmijn, 2018, P 2018 EMNLP WORKSH, P47, DOI [10.18653/v1/W18-5407, DOI 10.18653/V1/W18-5407]; Bisk Yonatan, 2016, P 2016 C N AM CHAPT, P751, DOI DOI 10.18653/V1/N16-1089.URL; Bowman Samuel R., 2015, P 3 WORKSH CONT VECT, P12; Chevalier-Boisvert M., 2019, P ICLR NEW ORL LA; Chomsky N., 1957, SYNTACTIC STRUCTURES; Dessi R., 2019, P ACL FIR IT; Devlin J., 2017, INT C MACH LEARN ICM, V3, P1641; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Geiger A., 2019, P 2019 C EMP METH NA, P4485; Gordon Jonathan, 2020, INT C LEARNING REPRE; Graves A., 2014, ARXIV14105401; Hill F., 2020, INT C LEARN REPR ICL; Hill F., 2017, UNDERSTANDING GROUND, P1; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hupkes D., 2019, COMPOSITIONALITY NEU; Joao~ Loula Marco, 2018, P 2018 EMNLP WORKSH, P108, DOI [10.18653/v1/W18-5413, DOI 10.18653/V1/W18-5413]; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Johnson R, 2017, INT CONF PERVAS COMP; Keysers D., 2019, INT C LEARN REPR ICL; Kingma D.P, P 3 INT C LEARNING R; Kuhnle A., 2017, CORR; LAKE BM, 2019, ADV NEURAL INFORM PR; Lake B, 2018, PR MACH LEARN RES, V80; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lake Brenden M., 2019, HUMAN FEW SHOT LEARN; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Matuszek Cynthia, 2012, P 29 INT C MACH LEAR, P1671; Mei HY, 2016, AAAI CONF ARTIF INTE, P2772; Nye M. I., 2020, LEARNING COMPOSITION; Pustejovsky J., 1991, Computational Linguistics, V17, P409; Russin J., 2019, CORR; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Talmor Alon, 2018, NAACL; Wang ZS, 2020, MINIM INVASIV THER, V29, P353, DOI 10.1080/13645706.2019.1653925	40	9	9	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000007
C	Xu, D; Ruan, CW; Korpeoglu, E; Kumar, S; Achan, K		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Xu, Da; Ruan, Chuanwei; Korpeoglu, Evren; Kumar, Sushant; Achan, Kannan			Adversarial Counterfactual Learning and Evaluation for Recommender System	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				PROPENSITY SCORE	The feedback data of recommender systems are often subject to what was exposed to the users; however, most learning and evaluation methods do not account for the underlying exposure mechanism. We first show in theory that applying supervised learning to detect user preferences may end up with inconsistent results in the absence of exposure information. The counterfactual propensity-weighting approach from causal inference can account for the exposure mechanism; nevertheless, the partial-observation nature of the feedback data can cause identifiability issues. We propose a principled solution by introducing a minimax empirical risk formulation. We show that the relaxation of the dual problem can be converted to an adversarial game between two recommendation models, where the opponent of the candidate model characterizes the underlying exposure mechanism. We provide learning bounds and conduct extensive simulation studies to illustrate and justify the proposed approach over a broad range of recommendation settings, which shed insights on the various benefits of the proposed approach.	[Xu, Da; Ruan, Chuanwei; Korpeoglu, Evren; Kumar, Sushant; Achan, Kannan] Walmart Labs, Sunnyvale, CA 94086 USA	Wal-Mart Stores Inc	Xu, D (corresponding author), Walmart Labs, Sunnyvale, CA 94086 USA.	Da.Xu@walmartlabs.com; Chuanwei.Ruan@walmartlabs.com; EKorpeoglu@walmartlabs.com; SKumar4@walmartlabs.com; KAchan@walmartlabs.com			Walmart U.S. eCommerce	Walmart U.S. eCommerce	The work is supported by the Walmart U.S. eCommerce. The authors declare that there is no conflict of interest.	Agarwal Aman, 2018, ARXIV180500065; Ai QY, 2018, ACM/SIGIR PROCEEDINGS 2018, P385, DOI 10.1145/3209978.3209986; Austin PC, 2015, STAT MED, V34, P3661, DOI 10.1002/sim.6607; Austin PC, 2011, MULTIVAR BEHAV RES, V46, P119, DOI 10.1080/00273171.2011.540480; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; Franks A. M., 2016, ARXIV160306045; Franzini A, 2020, J NEUROSURG, V133, P830, DOI 10.3171/2019.5.JNS19227; Gao R, 2017, WASSERSTEIN DISTRIBU; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Haidong Chen, 2016, 2016 IEEE International Workshop on Electromagnetics (iWEM): Applications and Student Innovation Competition, P1, DOI [10.1109/PESGM.2016.7741231, 10.1109/iWEM.2016.7504980]; HE J, 2010, P 19 INT C WORLD WID, P1061, DOI DOI 10.1145/1772690.1772758; He XN, 2018, ACM/SIGIR PROCEEDINGS 2018, P355, DOI 10.1145/3209978.3209981; Hensel M, 2017, ADV NEUR IN, V30; Hernandez-Lobato JM, 2014, PR MACH LEARN RES, V32, P1512; Hirano K., 2001, HEALTH SERV OUTCOME, V2, P259, DOI [10.1023/A:1020371312283, DOI 10.1023/A:1020371312283]; Jin C., 2019, ARXIV190200618; Joachims T, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P781, DOI 10.1145/3018661.3018699; Kallus Nathan, 2018, ARXIV180205664; Liang DW, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P951, DOI 10.1145/2872427.2883090; Liang Dawen, 2016, UAI WORKSH CAUS FDN; Morgan SL, 2015, ANAL METHOD SOC RES, P1; Panaretos VM, 2019, ANNU REV STAT APPL, V6, P405, DOI 10.1146/annurev-statistics-030718-104938; Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057; Prasad HL, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1371; Rahimian H., 2019, ARXIV PREPRINT ARXIV; Ratliff LJ, 2013, ANN ALLERTON CONF, P917, DOI 10.1109/Allerton.2013.6736623; Rendle S, 2020, RECSYS 2020: 14TH ACM CONFERENCE ON RECOMMENDER SYSTEMS, P240, DOI 10.1145/3383313.3412488; Rosenbaum PR, 2010, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4419-1213-8; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; Saito Y, 2020, PROCEEDINGS OF THE 13TH INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM '20), P501, DOI 10.1145/3336191.3371783; Schnabel T, 2016, PR MACH LEARN RES, V48; Tramer F., 2017, ARXIV; Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786; WANG M, 2018, ADV NEURAL INFORM PR, P666, DOI DOI 10.1145/3241539.3267745; Xie CH, 2019, PROC CVPR IEEE, P501, DOI 10.1109/CVPR.2019.00059; Yang LQ, 2018, 12TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS), P279, DOI 10.1145/3240323.3240355; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; Yoon J., 2018, GANITE ESTIMATION IN; Zhang SA, 2019, ACM COMPUT SURV, V52, DOI 10.1145/3285029; Zheng GJ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P167, DOI 10.1145/3178876.3185994; Zhou GR, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1059, DOI 10.1145/3219819.3219823	44	9	9	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000056
C	Addanki, R; Venkatakrishnan, SB; Gupta, S; Mao, HZ; Alizadeh, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Addanki, Ravichandra; Venkatakrishnan, Shaileshh Bojja; Gupta, Shreyan; Mao, Hongzi; Alizadeh, Mohammad			Placeto: Learning Generalizable Device Placement Algorithms for Distributed Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present Placeto, a reinforcement learning (RL) approach to efficiently find device placements for distributed neural network training. Unlike prior approaches that only find a device placement for a specific computation graph, Placeto can learn generalizable device placement policies that can be applied to any graph. We propose two key ideas in our approach: (1) we represent the policy as performing iterative placement improvements, rather than outputting a placement in one shot; (2) we use graph embeddings to capture relevant information about the structure of the computation graph, without relying on node labels for indexing. These ideas allow Placeto to train efficiently and generalize to unseen graphs. Our experiments show that Placeto requires up to 6.1 x fewer training steps to find placements that are on par with or better than the best placements found by prior approaches. Moreover, Placeto is able to learn a generalizable placement policy for any given family of graphs, which can then be used without any retraining to predict optimized placements for unseen graphs from the same family. This eliminates the large overhead incurred by prior RL approaches whose lack of generalizability necessitates re-training from scratch every time a new graph is to be placed.	[Addanki, Ravichandra; Venkatakrishnan, Shaileshh Bojja; Gupta, Shreyan; Mao, Hongzi; Alizadeh, Mohammad] MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Addanki, R (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, Cambridge, MA 02139 USA.	addanki@mit.edu; bjjvnkt@mit.edu; shreyang@mit.edu; hongzi@mit.edu; alizadeh@mit.edu			NSF [CNS-1751009, CNS-1617702]; Google Faculty Research Award; AWS Machine Learning Research Award; Cisco Research Center Award; Alfred P. Sloan Research Fellowship; MIT Data Systems and AI Lab	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); AWS Machine Learning Research Award; Cisco Research Center Award; Alfred P. Sloan Research Fellowship(Alfred P. Sloan Foundation); MIT Data Systems and AI Lab	We thank the anonymous NeurIPS reviewers for their feedback. This work was funded in part by NSF grants CNS-1751009, CNS-1617702, a Google Faculty Research Award, an AWS Machine Learning Research Award, a Cisco Research Center Award, an Alfred P. Sloan Research Fellowship, and the sponsors of MIT Data Systems and AI Lab. We also gratefully acknowledge Cloudlab [5] and Chameleon testbeds [10] for providing us with compute environments for some of the experiments.	[Anonymous], 2018, EC2 INSTANCE TYPES; [Anonymous], 2020, REINFORCEMENT LEARNI; Battaglia Peter W, 2018, ARXIV 180601261; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Duplyakin D, 2019, PROCEEDINGS OF THE 2019 USENIX ANNUAL TECHNICAL CONFERENCE, P1; Gao Y., 2018, P MACHINE LEARNING R, V80, P1676; Gimeno F., 2019, ARXIV190502494; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Jia Z., 2018, DATA MODEL PARALLELI; Keahey K., 2017, CONT HIGH PERFORMANC, V3; Mao H., 2018, ARXIV181001963CSSTAT; Microsoft, 2021, ONNX MODEL ZOO; Mirhoseini A., 2017, DEVICE PLACEMENT OPT; Mirhoseini Azalia, 2018, INT C LEARN REPR; Nair A., 2015, ARXIV150704296; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Pellegrini F, 2007, LECT NOTES COMPUT SC, V4641, P195; Pham H, 2018, 35 INT C MACH LEARN; Shazeer N., 2018, ADV NEURAL INFORM PR, P10435; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Vinyals O., 2016, GOOGLES NEURAL MACHI; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Zoph B., 2017, ARXIV17070701226, V2	27	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304003
C	Ardakani, A; Ji, ZY; Ardakani, A; Gross, WJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ardakani, Arash; Ji, Zhengyun; Ardakani, Amir; Gross, Warren J.			The Synthesis of XNOR Recurrent Neural Networks with Stochastic Logic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The emergence of XNOR networks seek to reduce the model size and computational cost of neural networks for their deployment on specialized hardware requiring real-time processes with limited hardware resources. In XNOR networks, both weights and activations are binary, bringing great benefits to specialized hardware by replacing expensive multiplications with simple XNOR operations. Although XNOR convolutional and fully-connected neural networks have been successfully developed during the past few years, there is no XNOR network implementing commonly-used variants of recurrent neural networks such as long short-term memories (LSTMs). The main computational core of LSTMs involves vector-matrix multiplications followed by a set of non-linear functions and element-wise multiplications to obtain the gate activations and state vectors, respectively. Several previous attempts on quantization of LSTMs only focused on quantization of the vector-matrix multiplications in LSTMs while retaining the element-wise multiplications in full precision. In this paper, we propose a method that converts all the multiplications in LSTMs to XNOR operations using stochastic computing. To this end, we introduce a weighted finite-state machine and its synthesis method to approximate the non-linear functions used in LSTMs on stochastic bit streams. Experimental results show that the proposed XNOR LSTMs reduce the computational complexity of their quantized counterparts by a factor of 86x without any sacrifice on latency while achieving a better accuracy across various temporal tasks.	[Ardakani, Arash; Ji, Zhengyun; Ardakani, Amir; Gross, Warren J.] McGill Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada	McGill University	Ardakani, A (corresponding author), McGill Univ, Dept Elect & Comp Engn, Montreal, PQ, Canada.	arash.ardakani@mail.mcgill.ca; zhengyun.ji@mail.mcgill.ca; amir.ardakani@mail.mcgill.ca; warren.gross@mcgill.ca						Alaghi A, 2013, ACM T EMBED COMPUT S, V12, DOI 10.1145/2465787.2465794; Ardakani A., 2016, 5 INT C LEARN REPR I; Ardakani A., 2019, INT C LEARN REPR; Ardakani A, 2019, DES AUT TEST EUROPE, P1427, DOI 10.23919/DATE.2019.8714765; Ardakani A, 2017, IEEE T VLSI SYST, V25, P2688, DOI 10.1109/TVLSI.2017.2654298; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2014, ARXIV14061078; Brown BD, 2001, IEEE T COMPUT, V50, P891, DOI 10.1109/12.954505; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123; Gaines B. R., 1969, ADV INFORM SYSTEMS S, P37, DOI 10.1007/978-1-4899-5841-9_2; Graves A, 2005, IEEE IJCNN, P2047; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hou L., 2018, INT C LEARN REPR; Hou L., 2016, CORR; Hubara I, 2016, ADV NEUR IN, V29; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; [李凡杰 Li Fanjie], 2016, [低温工程, Cryogenics], P1; Lin Z, 2015, CORR; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Onizawa N, 2016, IEEE T NANOTECHNOL, V15, P705, DOI 10.1109/TNANO.2015.2511151; Pascanu R., 2013, ICML 13, V28; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Wang PQ, 2018, ADV NEUR IN, V31; Wen W, 2016, ADV NEUR IN, V29; Xu C., 2018, INT C LEARN REPR; Zhou S., 2016, ARXIV160606160; Zhu Chenzhuo, 2016, ARXIV161201064	33	9	9	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900008
C	Bepler, T; Zhong, ED; Kelley, K; Brignole, E; Berger, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bepler, Tristan; Zhong, Ellen D.; Kelley, Kotaro; Brignole, Edward; Berger, Bonnie			Explicitly disentangling image content from translation and rotation with spatial-VAE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CLASSIFICATION; NETWORKS	Given an image dataset, we are often interested in finding data generative factors that encode semantic content independently from pose variables such as rotation and translation. However, current disentanglement approaches do not impose any specific structure on the learned latent representations. We propose a method for explicitly disentangling image rotation and translation from other unstructured latent factors in a variational autoencoder (VAE) framework. By formulating the generative model as a function of the spatial coordinate, we make the reconstruction error differentiable with respect to latent translation and rotation parameters. This formulation allows us to train a neural network to perform approximate inference on these latent variables while explicitly constraining them to only represent rotation and translation. We demonstrate that this framework, termed spatial-VAE, effectively learns latent representations that disentangle image rotation and translation from content and improves reconstruction over standard VAEs on several benchmark datasets, including applications to modeling continuous 2-D views of proteins from single particle electron microscopy and galaxies in astronomical images.	[Bepler, Tristan; Zhong, Ellen D.; Brignole, Edward; Berger, Bonnie] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Kelley, Kotaro] New York Struct Biol Ctr, New York, NY USA	Massachusetts Institute of Technology (MIT)	Berger, B (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	tbepler@mit.edu; zhonge@mit.edu; kkelley@nysbc.org; brignole@mit.edu; bab@mit.edu		Bepler, Tristan/0000-0001-5595-9954	NIH [GM103310, R01-GM081871, OD019994-01]; Simons Foundation [SF349247]; NYSTAR; Agouron Institute [F00316]; National Institutes of Health [R35 GM126982]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Simons Foundation; NYSTAR; Agouron Institute(Pfizer); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was supported by NIH R01-GM081871.; We would like to thank Bridget Carragher and Clint Potter at NYSBC for their support in providing the antibody dataset. The NYSBC portion of this work was supported by Simons Foundation SF349247, NYSTAR, and NIH GM103310 with additional support from Agouron Institute F00316 and NIH OD019994-01.; We would also like to thank the laboratory of HHMI investigator Catherine L. Drennan, MIT, for providing the CODH/ACS dataset that was collected with support from the National Institutes of Health (R35 GM126982).	Bricman PA, 2018, LECT NOTES COMPUT SC, V11302, P64, DOI 10.1007/978-3-030-04179-3_6; Burgess C. P., 2018, UNDERSTANDING DISENT, Patent No. [1804.03599, 180403599]; Chen Xi, 2016, INFOGAN INTERPRETABL; Dashti A, 2014, P NATL ACAD SCI USA, V111, P17492, DOI 10.1073/pnas.1419276111; Davidson TR, 2018, HYPERSPHERICAL VARIA; Denton E., 2015, NEURIPS; Dieleman S, 2015, MON NOT R ASTRON SOC, V450, P1441, DOI 10.1093/mnras/stv632; Eslami SM, 2016, NEURIPS, V1; Falorsi Luca, 2018, EXPLORATIONS HOMEOMO; Frey BJ, 2003, IEEE T PATTERN ANAL, V25, P1, DOI 10.1109/TPAMI.2003.1159942; Goodfellow I., 2014, P 27 INT C NEURAL IN, VVolume 27; Gulrajani I., 2016, PIXELVAE LATENT VARI; Jammalamadaka S., 2001, TOPICS CIRCULAR STAT, V5, DOI DOI 10.1142/4031; Kingma D.P, P 3 INT C LEARNING R; Kipf T., 2018, ARXIV180400891; Larsen A B L, 2015, AUTOENCODING PIXELS; Lin FY, 2016, J BIOL CHEM, V291, P4537, DOI 10.1074/jbc.M115.705624; Liu R, 2018, ADV NEURAL INFORM PR, P9605; Lyumkis D, 2013, J STRUCT BIOL, V183, P377, DOI 10.1016/j.jsb.2013.07.005; Nakane T, 2018, ELIFE, V7, DOI [10.7554/eLife.36861, 10.7554/elife.36861]; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Radford A., 2015, CVPR; Scheres SHW, 2005, J MOL BIOL, V348, P139, DOI 10.1016/j.jmb.2005.02.031; Scheres SHW, 2010, METHOD ENZYMOL, V482, P295, DOI 10.1016/S0076-6879(10)82012-9; Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8; Watters Nicholas, 2019, ARXIV190107017; Welling M., 2013, P ICLR	28	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907013
C	Carmon, Y; Jin, YJ; Sidford, A; Tian, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Carmon, Yair; Jin, Yujia; Sidford, Aaron; Tian, Kevin			Variance Reduction for Matrix Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VARIATIONAL-INEQUALITIES; ALGORITHM	We present a randomized primal -dual algorithm that solves the problem minx maxy yTAx to additive error E in time nnz(A) \/nnz(A)77/6, for matrix A with larger dimension n and nnz(A) nonzero entries. This improves the best known exact gradient methods by a factor of /nnz(A)/n and is faster than fully stochastic gradient methods in the accurate and/or sparse regime E < /n/nnz(A). Our results hold for x, y in the simplex (matrix games, linear programming) and for x in an 2 ball and y in the simplex (perceptron / SVM, minimum enclosing ball). Our algorithm combines the Nemirovski's "conceptual prox-method" and a novel reduced -variance gradient estimator based on "sampling from the difference" between the current iterate and a reference point.	[Carmon, Yair; Jin, Yujia; Sidford, Aaron; Tian, Kevin] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Carmon, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yairc@stanford.edu; yujiajin@stanford.edu; sidford@stanford.edu; kjtian@stanford.edu			Stanford Graduate Fellowships; NSF CAREER Award [CCF-1844855]; NSF Graduate Fellowship [DGE1656518]	Stanford Graduate Fellowships; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF Graduate Fellowship(National Science Foundation (NSF))	YC and YJ were supported by Stanford Graduate Fellowships. AS was supported by the NSF CAREER Award CCF-1844855. KT was supported by the NSF Graduate Fellowship DGE1656518.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Balamurugan P., 2016, ADV NEURAL INFORM PR; Bottou L, 2018, SIAM REV, V60, P223, DOI 10.1137/16M1080173; Chavdarova T., 2019, ADV NEURAL INFORM PR; Clarkson KL, 2010, ANN IEEE SYMP FOUND, P449, DOI 10.1109/FOCS.2010.50; Cohen M. B., 2018, ARXIV181007896; Dantzig G. B, 1953, LINEAR PROGRAMMING E; Daskalakis C., 2019, INT C LEARN REPR; Drori Y, 2015, OPER RES LETT, V43, P209, DOI 10.1016/j.orl.2015.02.001; ECKSTEIN J, 1993, MATH OPER RES, V18, P202, DOI 10.1287/moor.18.1.202; Fang C., 2018, ADV NEURAL INFORM PR; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Gidel G., 2017, P 20 INT C ART INT S; Goodfellow J., 2014, ADV NEURAL INFORM PR; GRIGORIADIS MD, 1995, OPER RES LETT, V18, P53, DOI 10.1016/0167-6377(95)00032-0; Hiriart-Urruty J.-B., 1993, CONVEX ANAL MINIMIZA, V306, pxviii+346, DOI 10.1007/978-3-662-06409-2; Jin C., 2019, ARXIV190200618; Johoson R., 2013, ADV NEURAL INFORM PR; Kolossoski O, 2017, OPTIM METHOD SOFTW, V32, P1244, DOI 10.1080/10556788.2016.1266355; Lee YT, 2015, ANN IEEE SYMP FOUND, P230, DOI 10.1109/FOCS.2015.23; Lin H., 2015, ADV NEURAL INFORM PR; Madry Aleksander, 2018, ICLR; Mertikopoulos P., 2019, INT C LEARN REPR; Minsky M., 1987, PERCEPTRONS ANINTROD; Mishchenko K., 2019, ARXIV190511373; Mokhtari A., 2019, ARXIV190108511; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Reddi SJ, 2016, PR MACH LEARN RES, V48; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Sherman J, 2017, ACM S THEORY COMPUT, P452, DOI 10.1145/3055399.3055501; Shi Z., 2017, ADV NEURAL INFORM PR; Sidford A, 2018, ANN IEEE SYMP FOUND, P922, DOI 10.1109/FOCS.2018.00091; Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4; VOSE MD, 1991, IEEE T SOFTWARE ENG, V17, P972, DOI 10.1109/32.92917; Woodworth B. E., 2016, ADV NEURAL INFORM PR; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhou Dongruo, 2018, ADV NEURAL INFORM PR	45	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903006
C	Chang, SY; Zhang, Y; Yu, M; Jaakkola, TS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chang, Shiyu; Zhang, Yang; Yu, Mo; Jaakkola, Tommi S.			A Game Theoretic Approach to Class-wise Selective Rationalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Selection of input features such as relevant pieces of text has become a common technique of highlighting how complex neural predictors operate. The selection can be optimized post-hoc for trained models or incorporated directly into the method itself (self-explaining). However, an overall selection does not properly capture the multi-faceted nature of useful rationales such as pros and cons for decisions. To this end, we propose a new game theoretic approach to class-dependent rationalization, where the method is specifically trained to highlight evidence supporting alternative conclusions. Each class involves three players set up competitively to find evidence for factual and counterfactual scenarios. We show theoretically in a simplified scenario how the game drives the solution towards meaningful class-dependent rationales. We evaluate the method in single- and multi-aspect sentiment classification tasks and demonstrate that the proposed method is able to identify both factual (justifying the ground truth label) and counterfactual (countering the ground truth label) rationales consistent with human rationalization. The code for our method is publicly available(2).	[Chang, Shiyu; Zhang, Yang] MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA; [Chang, Shiyu; Zhang, Yang; Yu, Mo] IBM Res, Yorktown Hts, NY 10598 USA; [Jaakkola, Tommi S.] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); International Business Machines (IBM); Massachusetts Institute of Technology (MIT)	Chang, SY (corresponding author), MIT, IBM Watson AI Lab, Cambridge, MA 02139 USA.; Chang, SY (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	shiyu.chang@ibm.com; yang.zhang2@ibm.com; yum@us.ibm.com; tommi@csail.mit.edu						ALI SM, 1966, J ROY STAT SOC B, V28, P131; Alvarez-Melis D, 2018, ADV NEUR IN, V31; Alvarez-Melis David, 2017, P 2017 C EMP METH NA, P412, DOI DOI 10.18653/V1/D17-1042; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2016, ARXIV160101705, P1545, DOI [DOI 10.18653/V1/N16-1181, 10.18653/v1/N16-1181]; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Bengio Yoshua, 2013, ARXIV13083432; Blitzer J., P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441; Chen JB, 2018, PR MACH LEARN RES, V80; Chen Jianbo, 2018, ARXIV180802610; Datta A, 2016, P IEEE S SECUR PRIV, P598, DOI 10.1109/SP.2016.42; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Johnson O, 2017, IEEE INT SYMP INFO, P898, DOI 10.1109/ISIT.2017.8006658; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Lee G.-H., 2019, ARXIV190703207; Lee GH, 2019, PR MACH LEARN RES, V97; Li Jiwei, 2016, ARXIV161208220; Li Jiwei, 2016, P NAACL, DOI DOI 10.18653/V1/N16-1082; Lundberg SM, 2017, ADV NEUR IN, V30; McAuley J, 2012, IEEE DATA MINING, P1020, DOI 10.1109/ICDM.2012.110; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Springenberg J.T., 2014, ARXIV14126806; Strumbelj E, 2010, J MACH LEARN RES, V11, P1; Sundararajan M, 2017, PR MACH LEARN RES, V70; Wang Hongning, 2010, P 16 ACM SIGKDD INT, P783, DOI DOI 10.1145/1835804.1835903; Yala A, 2019, RADIOLOGY, V292, P60, DOI 10.1148/radiol.2019182716; Yu Mo, 2019, EMPIRICAL METHODS NA; Yu Mo, 2018, LEARNING CORRES RATI	32	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901066
C	de Haan, P; Jayaraman, D; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		de Haan, Pim; Jayaraman, Dinesh; Levine, Sergey			Causal Confusion in Imitation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NETWORKS	Behavioral cloning reduces policy learning to supervised learning by training a discriminative model to predict expert actions given observations. Such discriminative models are non-causal: the training procedure is unaware of the causal structure of the interaction between the expert and the environment. We point out that ignoring causality is particularly damaging because of the distributional shift in imitation learning. In particular, it leads to a counter-intuitive "causal misidentification" phenomenon: access to more information can yield worse performance. We investigate how this problem arises, and propose a solution to combat it through targeted interventions-either environment interaction or expert queries-to determine the correct causal model. We show that causal misidentification occurs in several benchmark control domains as well as realistic driving settings, and validate our solution against DAgger and other baselines and ablations.	[de Haan, Pim] Univ Amsterdam, Qualcomm AI Res, Amsterdam, Netherlands; [Jayaraman, Dinesh; Levine, Sergey] Berkeley AI Res, Berkeley, CA USA; [Jayaraman, Dinesh] Facebook AI Res, Menlo Pk, CA USA	University of Amsterdam; Facebook Inc	de Haan, P (corresponding author), Univ Amsterdam, Qualcomm AI Res, Amsterdam, Netherlands.		Jayaraman, Dinesh/AAI-2527-2021	Jayaraman, Dinesh/0000-0002-6888-3095	Berkeley DeepDrive; NVIDIA; Google	Berkeley DeepDrive; NVIDIA; Google(Google Incorporated)	We would like to thank Karthikeyan Shanmugam and Shane Gu for pointers to prior work early in the project, and Yang Gao, Abhishek Gupta, Marvin Zhang, Alyosha Efros, and Roberto Calandra for helpful discussions in various stages of the project. We are also grateful to Drew Bagnell and Katerina Fragkiadaki for helpful feedback on an earlier draft of this paper. This project was supported in part by Berkeley DeepDrive, NVIDIA, and Google.	Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Bagnell Drew, 2016, TALK FEEDBACK MACHIN; Bansal M, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV; Bojarski Mariusz, 2016, arXiv; Burgess CP, 2018, ARXIV180403599; Chen T.Q., 2018, NEURIPS, P2610; Codevilla Felipe, 2019, INT C COMP VIS ICCV; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Eberhardt F., 2017, INT J DATA SCI ANAL, V3, P81, DOI [DOI 10.1007/S41060-016-0038-6, https://doi.org/10.1007/s41060-016-0038-6]; Eberhardt F, 2007, PHILOS SCI, V74, P981, DOI 10.1086/525638; Gao WH, 2017, ADV NEUR IN, V30; Giusti A, 2016, IEEE ROBOT AUTOM LET, V1, P661, DOI 10.1109/LRA.2015.2509024; Goudet O., 2017, ARXIV170905321; Guyon Isabelle, 2008, J MACH LEARN RES P T, P1; Haarnoja T, 2017, PR MACH LEARN RES, V70; Heckerman David, 2006, BAYESIAN APPROACH CA, P1, DOI [10.1007/3-540-33486-6_1, 10.1007/3-540-33486-61, DOI 10.1007/3-540-33486-6_1]; Higgins I, 2016, BETA VAE LEARNING BA; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hussein A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054912; Jang E., 2016, ARXIV; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krahenbuhl P, 2018, PROC CVPR IEEE, P2955, DOI 10.1109/CVPR.2018.00312; Lalonde V, 2010, PROCEEDINGS OF THE ASME FLUIDS ENGINEERING DIVISION SUMMER MEETING - 2010 - VOL 3, PTS A AND B, P661; LASKEY M, 2017, P C ROBOT LEARNING, P143; Le T., 2016, IEEE ACM T COMPUTATI; Lecun Y., 2006, ADV NEURAL INFORM PR, V18, P739; Levine Sergey, 2018, ARXIV180500909; Liu R, 2018, ADV NEURAL INFORM PR, P9605; Lopez-Paz D, 2017, PROC CVPR IEEE, P58, DOI 10.1109/CVPR.2017.14; Louizos C, 2017, ADV NEUR IN, V30; Maathuis MH, 2010, NAT METHODS, V7, P247, DOI 10.1038/nmeth0410-247; Maddison Chris J, 2016, ARXIV161100712; Mahler J., 2017, PROC C ROBOT LEARN, P515; Mitrovic J, 2018, ADV NEUR IN, V31; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mulling K, 2013, INT J ROBOT RES, V32, P263, DOI 10.1177/0278364912472380; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Peters C, 2017, TLS-TIMES LIT SUPPL, P6; Peters J, 2014, J MACH LEARN RES, V15, P2009; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Ross St<prime>ephane, 2011, AISTATS; Schaal S, 1999, TRENDS COGN SCI, V3, P233, DOI 10.1016/S1364-6613(99)01327-3; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Sculley D., 2015, P C ADV NEURAL INFOR, V2, P2503, DOI DOI 10.5555/2969442.2969519; Sen R, 2017, PR MACH LEARN RES, V70; Shanmugam K, 2015, ADV NEUR IN, V28; Spirtes P., 2000, CAUSATION PREDICTION; Spirtes P, 2010, J MACH LEARN RES, V11, P1643; Spirtes Peter, 2016, APPL INFORM; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Steyvers M, 2003, COGNITIVE SCI, V27, P453, DOI 10.1016/S0364-0213(03)00010-7; Tong S., 2001, IJCAI; Wang Dequan, 2019, IROS; Wang Yixin, 2018, ARXIV180506826; Widrow Bernard, 1964, PATTERN RECOGNIZING	58	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903034
C	Ellis, K; Nye, M; Pu, YW; Sosa, F; Tenenbaum, JB; Solar-Lezama, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ellis, Kevin; Nye, Maxwell; Pu, Yewen; Sosa, Felix; Tenenbaum, Joshua B.; Solar-Lezama, Armando			Write, Execute, Assess: Program Synthesis with a REPL	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a neural program synthesis approach integrating components which write, execute, and assess code to navigate the search space of possible programs. We equip the search process with an interpreter or a read-eval-print-loop (REPL), which immediately executes partially written programs, exposing their semantics. The REPL addresses a basic challenge of program synthesis: tiny changes in syntax can lead to huge changes in semantics. We train a pair of models, a policy that proposes the new piece of code to write, and a value function that assesses the prospects of the code written so-far. At test time we can combine these models with a Sequential Monte Carlo algorithm. We apply our approach to two domains: synthesizing text editing programs and inferring 2D and 3D graphics programs.	[Ellis, Kevin; Nye, Maxwell; Pu, Yewen; Tenenbaum, Joshua B.; Solar-Lezama, Armando] MIT, Cambridge, MA 02139 USA; [Sosa, Felix] Harvard Univ, Cambridge, MA 02138 USA	Massachusetts Institute of Technology (MIT); Harvard University	Ellis, K (corresponding author), MIT, Cambridge, MA 02139 USA.			Ellis, Kevin/0000-0001-6586-0632; Solar Lezama, Armando/0000-0001-7604-8252	AFOSR [FA9550-16-1-0012]; MIT-IBM Watson AI Lab; NSF; Siemens research	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); MIT-IBM Watson AI Lab(International Business Machines (IBM)); NSF(National Science Foundation (NSF)); Siemens research	We gratefully acknowledge many extended and productive conversations with Tao Du, Wojciech Matusik, and Siemens research. In addition to these conversations, Tao Du assisted by providing 3D ray tracing code, which we used when rerendering 3D programs. Work was supported by Siemens research, AFOSR award FA9550-16-1-0012 and the MIT-IBM Watson AI Lab. K. E. and M. N. are additionally supported by NSF graduate fellowships.	Alur R, 2016, ELECTRON P THEOR COM, P178, DOI 10.4204/EPTCS.229.13; Balog M., 2017, ICLR; Chen Xinyun, 2018, ICLR; Devlin J, 2017, PR MACH LEARN RES, V70; Doucet A, 2001, STAT ENG IN, P3; Du T, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275006; Ellis K, 2018, ADV NEUR IN, V31; Ellis Kevin, 2018, ADV NEURAL INFORM PR, P7805; Ganin Y, 2018, PR MACH LEARN RES, V80; Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423; Kalyan A., 2018, ICLR; Lau T, 2001, THESIS; Mnih V, 2016, PR MACH LEARN RES, V48; Nye Maxwell, 2019, ARXIV190206349; Pu Yewen, 2018, INT C MACH LEARN, V80, P4161; RATHER ED, 1993, SIGPLAN NOTICES, V28, P177, DOI 10.1145/155360.155369; Schmidhuber J, 2004, MACH LEARN, V54, P211, DOI 10.1023/B:MACH.0000015880.99707.b2; Sharma G, 2018, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2018.00578; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simmons-Edler R., 2018, ARXIV180602932; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Yang TY, 2019, IEEE WIREL COMMUNN; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; Zohar A., 2018, C NEURAL INFORM PROC, P2094	24	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900072
C	Goldt, S; Advani, MS; Saxe, AM; Krzakala, F; Zdeborova, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Goldt, Sebastian; Advani, Madhu S.; Saxe, Andrew M.; Krzakala, Florent; Zdeborova, Lenka			Dynamics of stochastic gradient descent for two-layer neural networks in the teacher-student setup	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STATISTICAL-MECHANICS	Deep neural networks achieve stellar generalisation even when they have enough parameters to easily fit all their training data. We study this phenomenon by analysing the dynamics and the performance of over-parameterised two-layer neural networks in the teacher-student setup, where one network, the student, is trained on data generated by another network, called the teacher. We show how the dynamics of stochastic gradient descent (SGD) is captured by a set of differential equations and prove that this description is asymptotically exact in the limit of large inputs. Using this framework, we calculate the final generalisation error of student networks that have more parameters than their teachers. We find that the final generalisation error of the student increases with network size when training only the first layer, but stays constant or even decreases with size when training both layers. We show that these different behaviours have their root in the different solutions SGD finds for different activation functions. Our results indicate that achieving good generalisation in neural networks goes beyond the properties of SGD alone and depends on the interplay of at least the algorithm, the model architecture, and the data set.	[Goldt, Sebastian; Zdeborova, Lenka] Univ Paris Saclay, CNRS, Inst Phys Theor, CEA, Saclay, France; [Advani, Madhu S.] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA; [Saxe, Andrew M.] Univ Oxford, Dept Expt Psychol, Oxford, England; [Krzakala, Florent] Univ Pierre & Marie Curie Paris 6, Sorbonne Univ, Ecole Normale Super, Lab Phys Stat, F-75005 Paris, France	CEA; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Saclay; Harvard University; University of Oxford; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Sorbonne Universite; Universite Paris Cite	Goldt, S (corresponding author), Univ Paris Saclay, CNRS, Inst Phys Theor, CEA, Saclay, France.		Goldt, Sebastian/AAQ-9486-2021	Goldt, Sebastian/0000-0002-5799-7644	ERC under the European Union's Horizon 2020 Research and Innovation Programme [714608-SMiLe]; Swartz Program in Theoretical Neuroscience at Harvard University; European Research Council [725937]; "Chaire de recherche sur les modeles et sciences des donnees", Fondation CFM pour la RechercheENS; French National Research Agency (ANR) grant PAIL	ERC under the European Union's Horizon 2020 Research and Innovation Programme; Swartz Program in Theoretical Neuroscience at Harvard University; European Research Council(European Research Council (ERC)European Commission); "Chaire de recherche sur les modeles et sciences des donnees", Fondation CFM pour la RechercheENS; French National Research Agency (ANR) grant PAIL(French National Research Agency (ANR))	SG and LZ acknowledge funding from the ERC under the European Union's Horizon 2020 Research and Innovation Programme Grant Agreement 714608-SMiLe. MA thanks the Swartz Program in Theoretical Neuroscience at Harvard University for support. AS acknowledges funding by the European Research Council, grant 725937 NEUROABSTRACTION. FK acknowledges support from "Chaire de recherche sur les modeles et sciences des donnees", Fondation CFM pour la RechercheENS, and from the French National Research Agency (ANR) grant PAIL.	Advani M.S., 2016, PHYS REV X, V6, P1; Advani M. S., 2017, ARXIV171003667; Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], ARXIV181103962; Arora S., 2018, 35 INT C MACH LEARN, P390; Arpit, 2017, 7TH INTERNATIONAL CONFERENCE ON COMPUTER AND COMMUNICATION TECHNOLOGY (ICCCT - 2017), P34, DOI 10.1145/3154979.3155004; Aubin B., 2018, ADV NEURAL INFORM PR, P3227; Baity-Jesi M., 2018, P 35 INT C MACH LEAR; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BIEHL M, 1995, J PHYS A-MATH GEN, V28, P643, DOI 10.1088/0305-4470/28/3/018; Chaudhari P., 2017, ICLR; Chaudhari P, 2018, INT C LEARN REPR; Chizat L, 2019, ADV NEUR IN, V32; Du S.S., 2019, ARXIV191003016; Dziugaite G. K., 2017, C UNC ART INT; GARDNER E, 1989, J PHYS A-MATH GEN, V22, P1983, DOI 10.1088/0305-4470/22/12/004; Golowich N., 2019, INFORM INFERENCE J I; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; KINZEL W, 1990, EUROPHYS LETT, V13, P473, DOI 10.1209/0295-5075/13/5/016; Lampinen AK., 2019, INT C LEARN REPR ICL; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Li Y., 2018, C LEARN THEOR, V75, P2; Li YG, 2018, ASIA PACIF MICROWAVE, P31, DOI 10.23919/APMC.2018.8617636; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Mohri M., 2018, FDN MACHINE LEARNING; Neyshabur B, 2015, ICLR; Neyshabur B, 2015, C LEARN THEOR; Riegler P., 1995, J PHYS A, V28; Rotskof G, 2018, ADV NEURAL INFORM PR, V31, P7146; Saad D, 1997, ADV NEUR IN, V9, P260; SAAD D, 1995, PHYS REV LETT, V74, P4337, DOI 10.1103/PhysRevLett.74.4337; Saxe Andrew M., 2014, ICLR; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sirignano J., 2019, STOCHASTIC PROCESSES; Soudry D, 2018, INT C LEARN REPR; Theodor Misiakiewicz and, 2019, ARXIV190206015; Vapnik V., 1998, STAT LEARNING THEORY, P156; Wang C, 2018, ARXIV180508349; Zhang C., 2017, ICLR; Zou D., 2019, MOBILE NETW APPL, P1	48	9	8	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307004
C	Gregor, K; Rezende, DJ; Besse, F; Wu, Y; Merzic, H; van den Oord, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gregor, Karol; Rezende, Danilo Jimenez; Besse, Frederic; Wu, Yan; Merzic, Hamza; van den Oord, Aaron			Shaping Belief States with Generative Environment Models for RL	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					When agents interact with a complex environment, they must form and maintain beliefs about the relevant aspects of that environment. We propose a way to efficiently train expressive generative models in complex environments. We show that a predictive algorithm with an expressive generative model can form stable belief-states in visually rich and dynamic 3D environments. More precisely, we show that the learned representation captures the layout of the environment as well as the position and orientation of the agent. Our experiments show that the model substantially improves data-efficiency on a number of reinforcement learning (RL) tasks compared with strong model-free baseline agents. We find that predicting multiple steps into the future (overshooting), in combination with an expressive generative model, is critical for stable representations to emerge. In practice, using expressive generative models in RL is computationally expensive and we propose a scheme to reduce this computational burden, allowing us to build agents that are competitive with model-free baselines.	[Gregor, Karol; Rezende, Danilo Jimenez; Besse, Frederic; Wu, Yan; Merzic, Hamza; van den Oord, Aaron] Google DeepMind, London, England	Google Incorporated	Gregor, K (corresponding author), Google DeepMind, London, England.	karolg@google.com; danilor@google.com; fbesse@google.com; yanwu@google.com; hamzamerzic@google.com; avdnoord@google.com						Alemi AA, 2017, ARXIV171100464; Amos B., 2018, INT C LEARN REPR; ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X; Beattie C., 2016, ARXIV161203801; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Chiappa Silvia, 2017, CORR; Co-Reyes JD, 2018, PR MACH LEARN RES, V80; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Dinh L, 2016, ARXIV PREPRINT ARXIV; Diuk C., 2008, ICML, DOI [10.1145/1390156.1390187, DOI 10.1145/1390156.1390187]; Dosovitskiy A., 2016, ARXIV161101779; ELIAS P, 1955, IRE T INFORM THEOR, V1, P16, DOI 10.1109/TIT.1955.1055126; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Espeholt L, 2018, PR MACH LEARN RES, V80; Fraccaro Marco, 2018, ARXIV180409401; Gregor Karol, 2018, CORR; Guo Z., 2018, CORR; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Ha David, 2018, NEURIPS, DOI [10.5281/zenodo.1207631, DOI 10.5281/ZENODO.1207631]; Hafner D., 2018, ARXIV181104551; Hassabis Demis, 2018, ARXIV180203006; Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678; Higgins I, 2017, PR MACH LEARN RES, V70; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hung C-C, 2018, ARXIV181006721; Igl M, 2018, PR MACH LEARN RES, V80; Iyer Ganesh, 2018, P IEEE C COMP VIS PA, P267; Jaakkola T., 1995, Advances in Neural Information Processing Systems 7, P345; Jaderberg Max, 2017, ABS171109846 CORR; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kalchbrenner N, 2017, PR MACH LEARN RES, V70; Kayalibay Baris, 2018, NAVIGATION PLANNING; Ke N. R., 2019, CORR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni T. D., 2016, ARXIV160602396; Lample G, 2017, AAAI CONF ARTIF INTE, P2140; Li X., 2015, ARXIV150903044; Lin L., 1992, MEMORY APPROACHES RE; Liu Rui, 2019, ARXIV190401782; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Moreno Pol, 2018, NEURIPS 2018 WORKSH; Munro Paul, 1987, 9TH P ANN C COGN SCI, P165; NGUYEN D, 1990, ADVANCED NEURAL COMPUTERS, P11, DOI 10.1117/12.21108; Parisotto Emilio, 2018, INT C LEARN REPR; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Razavi Ali, 2019, INT C LEARN REPR ICL; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rezende Danilo Jimenez, 2018, GEN ELBO CONSTRAINED; Salimans Tim, 2017, ARXIV170105517; SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Silver D, 2017, PR MACH LEARN RES, V70; Sodhani Shagun, 2019, ARXIV190604355; van den Oord Aaron, 2016, ARXIV160605328; van den Oord Aaron, 2018, ARXIV180703748; Werbos Paul J, 1987, P IEEE INT C SYST MA; Wu Yan, 2018, ADV NEURAL INFORM PR, P9379; Wu Yan, 2018, ARXIV180401756; Xie C, 2016, IEEE INT CONF ROBOT, P504, DOI 10.1109/ICRA.2016.7487172; Zhang Jingwei, 2017, ARXIV170609520; Zhou TH, 2017, PROC CVPR IEEE, P6612, DOI 10.1109/CVPR.2017.700	67	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905018
C	Izacard, G; Mohan, S; Fernandez-Granda, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Izacard, Gautier; Mohan, Sreyas; Fernandez-Granda, Carlos			Data-driven Estimation of Sinusoid Frequencies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIGNALS; PARAMETERS; NUMBER	Frequency estimation is a fundamental problem in signal processing, with applications in radar imaging, underwater acoustics, seismic imaging, and spectroscopy. The goal is to estimate the frequency of each component in a multisinusoidal signal from a finite number of noisy samples. A recent machine-learning approach uses a neural network to output a learned representation with local maxima at the position of the frequency estimates. In this work, we propose a novel neural-network architecture that produces a significantly more accurate representation, and combine it with an additional neural-network module trained to detect the number of frequencies. This yields a fast, fully-automatic method for frequency estimation that achieves state-of-the-art results. In particular, it outperforms existing techniques by a substantial margin at medium-to-high noise levels.	[Izacard, Gautier] Ecole Polytech, Palaiseau, France; [Mohan, Sreyas; Fernandez-Granda, Carlos] NYU, Ctr Data Sci, New York, NY 10003 USA; [Fernandez-Granda, Carlos] NYU, Courant Inst Math Sci, New York, NY 10003 USA	Institut Polytechnique de Paris; New York University; New York University	Izacard, G (corresponding author), Ecole Polytech, Palaiseau, France.	gautier.izacard@polytechnique.edu; sm7582@nyu.edu; cfgranda@cims.nyu.edu			NSF [DMS-1616340]	NSF(National Science Foundation (NSF))	C.F. was supported by NSF award DMS-1616340.	Adavanne Sharath, 2017, ARXIV171010059; BEATTY LG, 1978, J ACOUST SOC AM, V63, P1782, DOI 10.1121/1.381916; BERNI AJ, 1975, IEEE T AERO ELEC SYS, VAE11, P147, DOI 10.1109/TAES.1975.308051; Bienvenu G., 1979, ICASSP 79. 1979 IEEE International Conference on Acoustics, Speech and Signal Processing, P306; Borcea L, 2002, INVERSE PROBL, V18, P1247, DOI 10.1088/0266-5611/18/5/303; BORGEFORS G, 1984, COMPUT VISION GRAPH, V27, P321, DOI 10.1016/0734-189X(84)90035-5; Boyd N., 2018, 267096 BIORXIV, DOI [10.1101/267096, DOI 10.1101/267096]; Boyer C, 2017, INF INFERENCE, V6, P310, DOI 10.1093/imaiai/iaw024; Candes EJ, 2014, COMMUN PUR APPL MATH, V67, P906, DOI 10.1002/cpa.21455; CARRIERE R, 1992, IEEE T ANTENN PROPAG, V40, P13, DOI 10.1109/8.123348; Chakrabarty S, 2017, IEEE WORK APPL SIG, P136; Coulangeon P, 2015, CULT ECON SOC, P1; Fernandez-Granda C., 2013, P 10 INT C SAMPLING, P145; Fernandez-Granda C, 2018, INF INFERENCE, V7, P105, DOI 10.1093/imaiai/iax005; Fernandez-Granda C, 2016, INF INFERENCE, V5, P251, DOI 10.1093/imaiai/iaw005; Han KY, 2013, IEEE T SIGNAL PROCES, V61, P6118, DOI 10.1109/TSP.2013.2283462; HARRIS FJ, 1978, P IEEE, V66, P51, DOI 10.1109/PROC.1978.10837; He H, 2017, ADV NEUR IN, V30; He ZS, 2010, IEEE T PATTERN ANAL, V32, P2006, DOI 10.1109/TPAMI.2010.15; HUA Y, 1990, IEEE T ACOUST SPEECH, V38, P814, DOI 10.1109/29.56027; IZACARD G., 2018, CORR; Kingma D.P, P 3 INT C LEARNING R; KOTELNIKOV V. A., 1933, MAT 1 ALL UN C QUEST, V1933; Krim H, 1996, IEEE SIGNAL PROC MAG, V13, P67, DOI 10.1109/79.526899; Leonowicz Z, 2003, IEEE T IND ELECTRON, V50, P514, DOI 10.1109/TIE.2003.812361; MOITRA A., 2015, P 47 ANN ACM S THEOR; Nyquist H., 1928, T AM I ELECT ENG, V47, P617, DOI [10.1109/T-AIEE.1928.5055024, DOI 10.1109/T-AIEE.1928.5055024]; Oppenheim A., 1997, SIGNALS SYSTEMS; ROY R, 1989, IEEE T ACOUST SPEECH, V37, P984, DOI 10.1109/29.32276; SCHMIDT RO, 1986, IEEE T ANTENN PROPAG, V34, P276, DOI 10.1109/TAP.1986.1143830; SHANNON CE, 1949, P IRE, V37, P10, DOI 10.1109/JRPROC.1949.232969; SLEPIAN D, 1978, BELL SYST TECH J, V57, P1371, DOI 10.1002/j.1538-7305.1978.tb02104.x; SMITH J. O., 2008, INTRO DIGITAL FILTER, V2; STOICA P, 1993, SIGNAL PROCESS, V31, P329, DOI 10.1016/0165-1684(93)90090-W; Stoica P., 2005, SPECTRAL ANAL SIGNAL; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang GG, 2015, IEEE T INFORM THEORY, V61, P499, DOI 10.1109/TIT.2014.2368122; Tang GG, 2013, IEEE T INFORM THEORY, V59, P7465, DOI 10.1109/TIT.2013.2277451; Vetterli M, 2002, IEEE T SIGNAL PROCES, V50, P1417, DOI 10.1109/TSP.2002.1003065; Viti V, 1997, INT J IMAG SYST TECH, V8, P565, DOI 10.1002/(SICI)1098-1098(1997)8:6<565::AID-IMA9>3.0.CO;2-8; Wang JQ, 2015, IEEE INT ULTRA SYM, DOI 10.1109/ULTSYM.2015.0506; WAX M, 1989, IEEE T ACOUST SPEECH, V37, P1190, DOI 10.1109/29.31267; WAX M, 1985, IEEE T ACOUST SPEECH, V33, P387, DOI 10.1109/TASSP.1985.1164557; Xin B, 2016, ADV NEUR IN, V29	47	9	9	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305016
C	Klein, A; Dai, ZW; Hutter, F; Lawrence, N; Gonzalez, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Klein, Aaron; Dai, Zhenwen; Hutter, Frank; Lawrence, Neil; Gonzalez, Javier			Meta-Surrogate Benchmarking for Hyperparameter Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Despite the recent progress in hyperparameter optimization (HPO), available benchmarks that resemble real-world scenarios consist of a few and very large problem instances that are expensive to solve. This blocks researchers and practitioners not only from systematically running large-scale comparisons that are needed to draw statistically significant results but also from reproducing experiments that were conducted before. This work proposes a method to alleviate these issues by means of a meta-surrogate model for HPO tasks trained on off-line generated data. The model combines a probabilistic encoder with a multi-task model such that it can generate inexpensive and realistic tasks of the class of problems of interest. We demonstrate that benchmarking HPO methods on samples of the generative model allows us to draw more coherent and statistically significant conclusions that can be reached orders of magnitude faster than using the original tasks. We provide evidence of our findings for various HPO methods on a wide class of problems.	[Klein, Aaron; Hutter, Frank] Univ Freiburg, Freiburg, Germany; [Dai, Zhenwen; Gonzalez, Javier] Amazon Cambridge, Cambridge, England; [Lawrence, Neil] Univ Cambridge, Cambridge, England	University of Freiburg; University of Cambridge	Klein, A (corresponding author), Univ Freiburg, Freiburg, Germany.	kleinaa@cs.uni-freiburg.de; zhenwend@amazon.com; fh@cs.uni-freiburg.de; ndl21@cam.ac.uk; gojav@amazon.com						Bardenet R., 2013, P 30 INT C MACH LEAR; Bergstra J., 2011, P 24 INT C ADV NEUR; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Chen P, 2017, STUD APPL PHILOS, V34, P17, DOI 10.1007/978-3-319-49872-0_2; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Chen W, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON REAL-TIME COMPUTING AND ROBOTICS (IEEE RCAR), P22, DOI 10.1109/RCAR.2016.7783995; Chen Y., 2018, ARXIV181206855CSLG; Dai Z., 2017, P 30 INT C ADV NEUR; Eggensperger C, 2013, ELECTRON PROC THEOR, P13, DOI 10.4204/EPTCS.114.2; Eggensperger K., 2015, P 29 NAT C ART INT A; Feurer M., 2015, P 29 NAT C ART INT A; Feurer M., 2015, P 28 INT C ADV NEUR; Feurer M., 2018, AUTOMATIC MACHINE LE; Fusi N., 2018, P 31 INT C ADV NEUR; Hansen N, 2006, STUD FUZZ SOFT COMP, V192, P75; Hansen N., 2016, ARXIV160308785CSAI; Hansen N., 2016, ARXIV160503560CSNE; Hutter F., 2018, AUTOMATIC MACHINE LE; Jaderberg Max, 2017, ABS171109846 CORR; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kandasamy K, 2017, PR MACH LEARN RES, V70; KLEIN A, 2017, INT C LEARN REPR ICL; Klein A., 2017, NIPS WORKSH BAYES OP; Klein A., 2019, ARXIV190504970CSLG; KLEIN A, 2017, ELECT J STAT; Komer B., 2014, ICML 2014 AUTOML WOR; Li L., 2017, INT C LEARN REPR ICL; Lichman M, 2013, UCI MACHINE LEARNING; More J. J., 2009, SIAM J OPTIMIZATION; Perrone V., 2018, P 31 INT C ADV NEUR; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Sobol I. M., 1967, USSR COMPUTATIONAL M; Springenberg J. T., 2016, P 29 INT C ADV NEUR; Storn R., 1997, J GLOBAL OPTIM, V11, P341, DOI [10.1023/A:1008202821328, DOI 10.1023/A:1008202821328]; Swersky K., 2013, P 26 INT C ADV NEUR; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; van Rijn J.N., 2018, P 24 ACM SIGKDD INT; Vanschoren J., 2014, SIGKDD EXPLORATIONS; Volpp M., 2019, ARXIV190402642STATML; Ying C., 2019, ARXIV190209635CSLG; [No title captured]	43	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306029
C	Kuznetsov, M; Polykovskiy, D; Vetrov, D; Zhebrak, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kuznetsov, Maksim; Polykovskiy, Daniil; Vetrov, Dmitry; Zhebrak, Alexander			A Prior of a Googol Gaussians: a Tensor Ring Induced Prior for Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Generative models produce realistic objects in many domains, including text, image, video, and audio synthesis. Most popular models-Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs)-usually employ a standard Gaussian distribution as a prior. Previous works show that the richer family of prior distributions may help to avoid the mode collapse problem in GANs and to improve the evidence lower bound in VAEs. We propose a new family of prior distributions-Tensor Ring Induced Prior (TRIP)-that packs an exponential number of Gaussians into a high-dimensional lattice with a relatively small number of parameters. We show that these priors improve Frchet Inception Distance for GANs and Evidence Lower Bound for VAEs. We also study generative models with TRIP in the conditional generation setup with missing conditions. Altogether, we propose a novel plug-and-play framework for generative models that can be utilized in any GAN and VAE-like architectures.	[Kuznetsov, Maksim; Polykovskiy, Daniil; Zhebrak, Alexander] Insilico Med, Rochester, MD 20850 USA; [Vetrov, Dmitry] Natl Res Univ Higher Sch Econ, Moscow, Russia	HSE University (National Research University Higher School of Economics)	Kuznetsov, M (corresponding author), Insilico Med, Rochester, MD 20850 USA.	kuznetsov@insilico.com; daniil@insilico.com; vetrovd@yandex.ru; zhebrak@insilico.com			Russian Science Foundation [17-7120072]	Russian Science Foundation(Russian Science Foundation (RSF))	Image generation for Section 6.3 was supported by the Russian Science Foundation grant no. 17-7120072.	Arjovsky M, 2017, PR MACH LEARN RES, V70; Bauer M, 2019, PR MACH LEARN RES, V89, P66; Ben-Yosef M., 2018, ARXIV180810356; Burda Yuri, 2015, ARXIV150900519; Chen X., 2017, P ICLR; Dinh L, 2017, 5 INT C LEARN REPR I; Dinh Laurent, 2015, ICLR WORKSH; Garipov Timur, 2016, ADV NEURAL INFORM PR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Gurumurthy Swaminathan, 2017, PROC CVPR IEEE, P166, DOI DOI 10.1109/CVPR.2017.525; Hensel M, 2017, ADV NEUR IN, V30; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Levine Y., 2018, INT C LEARN REPR; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; Pan Lili, 2018, ARXIV181206571; Ping W., 2018, INT C LEARN REPR; Polykovskiy Daniil, 2018, MOL PHARM; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Suzuki M., 2017, INT C LEARN REPR WOR; Tjandra Andros, 2017, INT JOINT C NEUR NET; Tomczak JM, 2018, PR MACH LEARN RES, V84; van den Oord A, 2017, ADV NEUR IN, V30; van den Oord Aaron, 2018, PARALLEL WAVENET FAS; Vedantam R., 2018, 6 INT C LEARN REPR I; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhao Qibin, 2016, ARXIV160605535; Zhavoronkov A, 2019, NAT BIOTECHNOL, V37, P1038, DOI 10.1038/s41587-019-0224-x	34	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304014
C	Le, T; Yamada, M; Fukumizu, K; Cuturi, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Le, Tam; Yamada, Makoto; Fukumizu, Kenji; Cuturi, Marco			Tree-Sliced Variants of Wasserstein Distances	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PERSISTENT HOMOLOGY; SPACES; SHAPES	Optimal transport (OT) theory defines a powerful set of tools to compare probability distributions. OT suffers however from a few drawbacks, computational and statistical, which have encouraged the proposal of several regularized variants of OT in the recent literature, one of the most notable being the sliced formulation, which exploits the closed-form formula between univariate distributions by projecting high-dimensional measures onto random lines. We consider in this work a more general family of ground metrics, namely tree metrics, which also yield fast closed-form computations and negative definite, and of which the sliced-Wasserstein distance is a particular case (the tree is a chain). We propose the tree-sliced Wasserstein distance, computed by averaging the Wasserstein distance between these measures using random tree metrics, built adaptively in either low or high-dimensional spaces. Exploiting the negative definiteness of that distance, we also propose a positive definite kernel, and test it against other baselines on a few benchmark tasks.	[Le, Tam; Yamada, Makoto; Fukumizu, Kenji] RIKEN AIP, Tokyo, Japan; [Yamada, Makoto] Kyoto Univ, Kyoto, Japan; [Fukumizu, Kenji] ISM, Tachikawa, Tokyo, Japan; [Cuturi, Marco] Google Brain, Paris, France; [Cuturi, Marco] CREST ENSAE, Palaiseau, France	RIKEN; Kyoto University; Institut Polytechnique de Paris	Le, T (corresponding author), RIKEN AIP, Tokyo, Japan.	tam.le@riken.jp; makoto.yamada@riken.jp; fukumizu@ism.ac.jp; cuturi@google.com		Fukumizu, Kenji/0000-0002-3488-2625	JSPS KAKENHI [17K12745]; JST PRESTO program [JPMJPR165A]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST PRESTO program	We thank anonymous reviewers for their comments. TL acknowledges the support of JSPS KAKENHI Grant number 17K12745. MY was supported by the JST PRESTO program JPMJPR165A.	Adams H, 2017, J MACH LEARN RES, V18; Altschuler J., 2017, ADV NEURAL INFORM PR, P1961; Altschuler J., 2018, ARXIV181205189; Altschuler Jason, 2018, ARXIV181010046; [Anonymous], 1972, NATURE, V239, P488; Backurs A, 2019, PR MACH LEARN RES, V97; Bartal Y., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P161, DOI 10.1145/276698.276725; Bartal Y, 1996, AN S FDN CO, P184, DOI 10.1109/SFCS.1996.548477; Berg C., 1984, HARMONIC ANAL SEMIGR; Burkard Rainer E, 1999, HDB COMBINATORIAL OP, P75, DOI DOI 10.1007/978-1-4757-3023-4_2.; Carriere M, 2017, PR MACH LEARN RES, V70; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M, 1998, ANN IEEE SYMP FOUND, P379, DOI 10.1109/SFCS.1998.743488; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Do Ba K, 2011, THEOR COMPUT SYST, V48, P428, DOI 10.1007/s00224-010-9265-8; Dong Yihe, 2019, ARXIV191004126; Dvurechensky P, 2018, PR MACH LEARN RES, V80; Ebert Johannes, 2017, ARXIV170303658; Edelsbrunner H, 2008, CONTEMP MATH, V453, P257; Elliott S.R., 1983, PHYS AMORPHOUS MAT; Evans SN, 2012, J R STAT SOC B, V74, P569, DOI 10.1111/j.1467-9868.2011.01018.x; Fakcharoenphol J, 2004, J COMPUT SYST SCI, V69, P485, DOI 10.1016/j.jcss.2004.04.01; Feder T., 1988, Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, P434, DOI 10.1145/62212.62255; Francois N, 2013, PHYS REV LETT, V111, DOI 10.1103/PhysRevLett.111.148001; FRANKLIN J, 1989, LINEAR ALGEBRA APPL, V114, P717, DOI 10.1016/0024-3795(89)90490-4; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Harchaoui Z., 2009, ADV NEURAL INFORM PR, V21, P609; Hertzsch JM, 2007, SMALL, V3, P202, DOI 10.1002/smll.200600361; Indyk P, 2001, ANN IEEE SYMP FOUND, P10, DOI 10.1109/SFCS.2001.959878; Indyk P., 2003, P 3 INT WORKSH STAT, P5; Indyk P, 2017, ADV NEUR IN, V30; JOHNSON SC, 1967, PSYCHOMETRIKA, V32, P241, DOI 10.1007/BF02289588; Kantorovich L., 1942, DOKL AKAD NAUK+, V37, P227; Kloeckner BR, 2015, MATHEMATIKA, V61, P162, DOI 10.1112/S0025579314000059; Kolouri S, 2016, PROC CVPR IEEE, P5258, DOI 10.1109/CVPR.2016.568; Kriege NM, 2016, ADV NEUR IN, V29; Kusano G, 2018, J MACH LEARN RES, V18; Kusner MJ, 2015, PR MACH LEARN RES, V37, P957; Latecki LJ, 2000, PROC CVPR IEEE, P424, DOI 10.1109/CVPR.2000.855850; Lavenant H, 2018, SIGGRAPH ASIA'18: SIGGRAPH ASIA 2018 TECHNICAL PAPERS, DOI 10.1145/3272127.3275064; Le T, 2018, ADV NEUR IN, V31; Lee Jaeho, 2018, ADV NEURAL INFORM PR, P2692; Liutkus Antoine, 2018, ARXIV180608141; Lozupone C, 2005, APPL ENVIRON MICROB, V71, P8228, DOI 10.1128/AEM.71.12.8228-8235.2005; Lozupone CA, 2007, APPL ENVIRON MICROB, V73, P1576, DOI 10.1128/AEM.01996-06; McGregor Andrew, 2013, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Algorithms and Techniques. 16th International Workshop, APPROX 2013 and 17th International Workshop, RANDOM 2013. Proceedings: LNCS 8096, P274, DOI 10.1007/978-3-642-40328-6_20; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Morariu V.I., 2009, ADV NEURAL INFORM PR, P1113; Nakamura T, 2015, NANOTECHNOLOGY, V26, DOI 10.1088/0957-4484/26/30/304001; Panaretos VM, 2016, ANN STAT, V44, P771, DOI 10.1214/15-AOS1387; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; SAMET H, 1984, COMPUT SURV, V16, P187, DOI 10.1145/356924.356930; Semple C., 2003, OXFORD LECT SERIES M; Shkarin SA, 2004, TOPOL APPL, V142, P13, DOI 10.1016/j.topol.2003.12.002; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Sommerfeld M, 2018, J R STAT SOC B, V80, P219, DOI 10.1111/rssb.12236; Turner K, 2014, INF INFERENCE, V3, P310, DOI 10.1093/imaiai/iau011; Villani C., 2003, TOPICS OPTIMAL TRANS, V58; Villani Cedric, 2008, OPTIMAL TRANSPORT OL, V338; Yang C., 2005, ADV NEURAL INFORM PR, V17, P1561	66	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903087
C	Li, QY; Haque, S; Anil, C; Lucas, J; Grosse, R; Jacobsen, JH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Qiyang; Haque, Saminul; Anil, Cem; Lucas, James; Grosse, Roger; Jacobsen, Jorn-Henrik			Preventing Gradient Attenuation in Lipschitz Constrained Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Lipschitz constraints under L-2 norm on deep neural networks are useful for provable adversarial robustness bounds, stable training, and Wasserstein distance estimation. While heuristic approaches such as the gradient penalty have seen much practical success, it is challenging to achieve similar practical performance while provably enforcing a Lipschitz constraint. In principle, one can design Lipschitz constrained architectures using the composition property of Lipschitz functions, but Anil et al. [2] recently identified a key obstacle to this approach: gradient norm attenuation. They showed how to circumvent this problem in the case of fully connected networks by designing each layer to be gradient norm preserving. We extend their approach to train scalable, expressive, provably Lipschitz convolutional networks. In particular, we present the Block Convolution Orthogonal Parameterization (BCOP), an expressive parameterization of orthogonal convolution operations. We show that even though the space of orthogonal convolutions is disconnected, the largest connected component of BCOP with 2n channels can represent arbitrary BCOP convolutions over n channels. Our BCOP parameterization allows us to train large convolutional networks with provable Lipschitz bounds. Empirically, we find that it is competitive with existing approaches to provable adversarial robustness and Wasserstein distance estimation.	[Li, Qiyang; Haque, Saminul; Anil, Cem; Lucas, James; Grosse, Roger; Jacobsen, Jorn-Henrik] Univ Toronto, Vector Inst, Toronto, ON, Canada	University of Toronto	Li, QY (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.	qiyang.li@mail.utoronto.ca; saminul.haque@mail.utoronto.ca; cem.anil@mail.utoronto.ca; jlucas@cs.toronto.edu; rgrosse@cs.toronto.edu; j.jacobsen@vectorinstitute.ai			CIFAR Canadian AI Chairs program	CIFAR Canadian AI Chairs program	We would like to thank Lechao Xiao, Arthur Rabinovich, Matt Koster, and Siqi Zhou for their valuable insights and feedback. We would like to thank Sherjil Ozair for spotting a bug in our code, whose fix improved our results. RG acknowledges support from the CIFAR Canadian AI Chairs program.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Anil C, 2019, PR MACH LEARN RES, V97; Arjovsky M., 2017, ARXIV170107875; Bertsekas D. P., 1997, J OPER RES SOC, V48, P334, DOI DOI 10.1057/PALGRAVE.JORS.2600425; BJORCK A, 1971, SIAM J NUMER ANAL, V8, P358; Brendel W., 2018, PROC 6 INT C LEARN R; Burges, 1998, MNIST DATABASE HANDW; Carlini N., 2019, CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chernodub Artem, 2016, ARXIV160402313; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cohen J, 2019, PR MACH LEARN RES, V97; Cohen Jeremy EJ, 2019, ARXIV190404861; Cox D., 2011, Proceedings 2011 IEEE International Conference on Automatic Face & Gesture Recognition (FG 2011), P8, DOI 10.1109/FG.2011.5771385; Croce F, 2019, PR MACH LEARN RES, V89; Croce Francesco, 2019, ARXIV190511213; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Gemici Mevlana, 2018, ARXIV180509575; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gouk H., 2018, ARXIV180404368; Gulrajani I, 2017, P NIPS 2017; Hein M, 2017, NIPS 17; Huster Todd, 2018, JOINT EUR C MACH LEA, P16; Jacobsen Jorn-Henrik, 2018, INT C LEARN REPR; Kang Hyeonwoo, 2016, PYTORCH GENERATIVE M; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Kautsky Jaroslav, 1994, WAVELETS THEORY ALGO, V5, P117; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky Alex, CIFAR 10; Lefkimmiatis S, 2013, IEEE T IMAGE PROCESS, V22, P1873, DOI 10.1109/TIP.2013.2237919; Lezcano-Casado M, 2019, PR MACH LEARN RES, V97; Madry Aleksander, 2017, ARXIV; Milnor John, 1974, CHARACTERISTIC CLASS, V76, P55; Pennington Jeffrey, 2018, P MACHINE LEARNING R, V84, P1924; Peyre G., 2018, ARXIV180300567; Qian Haifeng, 2019, INT C LEARN REPR; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rauber Jonas, 2017, REL MACH LEARN WILD, P6, DOI DOI 10.21105/JOSS.02607; Razenshteyn Ilya, 2019, ADV NEURAL INFORM PR; Schott Lukas, 2019, INT C LEARN REPR; Sedghi Hanie, 2019, INT C LEARN REPR, P2; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Tsuzuku Y, 2018, ADV NEUR IN, V31; Wong E, 2018, ADV NEUR IN, V31; Xiao K. Y., 2019, INT C LEARN REPR; Xiao LC, 2018, PR MACH LEARN RES, V80; Yoshida Y, 2017, ARXIV PREPRINT ARXIV	52	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907009
C	Liu, BY; Cai, Q; Yang, ZR; Wang, ZR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Boyi; Cai, Qi; Yang, Zhuoran; Wang, Zhaoran			Neural Proximal/Trust Region Policy Optimization Attains Globally Optimal Policy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Proximal policy optimization and trust region policy optimization (PPO and TRPO) with actor and critic parametrized by neural networks achieve significant empirical success in deep reinforcement learning. However, due to nonconvexity, the global convergence of PPO and TRPO remains less understood, which separates theory from practice. In this paper, we prove that a variant of PPO and TRPO equipped with overparametrized neural networks converges to the globally optimal policy at a sublinear rate. The key to our analysis is the global convergence of infinite-dimensional mirror descent under a notion of one-point monotonicity, where the gradient and iterate are instantiated by neural networks. In particular, the desirable representation power and optimization geometry induced by the overparametrization of such neural networks allow them to accurately approximate the infinite-dimensional gradient and iterate.	[Liu, Boyi; Cai, Qi; Wang, Zhaoran] Northwestern Univ, Evanston, IL 60208 USA; [Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA	Northwestern University; Princeton University	Liu, BY (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.	boyiliu2018@u.northwestern.edu; qicai2022@u.northwestern.edu; zy6@princeton.edu; zhaoranwang@gmail.com	Wang, Zhaoran/P-7113-2018					Abdolmaleki A., 2018, INT C LEARN REPR; Agarwal A., 2019, ARXIV190800261; Allen-Zhu Zeyuan, 2018, ARXIV181104918; Antos A., 2008, ADV NEURAL INFORM PR, P9; Arora S, 2019, PR MACH LEARN RES, V97; Azar MG, 2012, J MACH LEARN RES, V13, P3207; CAO Y., 2019, ARXIV190201384; Cao Yuan, 2019, ADV NEURAL INFORM PR, P21; Chizat L., 2018, NOTE LAZY TRAINING S; Cho W. S., 2017, ARXIV171202467; Culotta A., 2010, ADV NEURAL INFORM PR, V23, P568; Dai B., 2017, ARXIV171210285; Duan Y, 2016, PR MACH LEARN RES, V48; Facchinei F., 2002, FINITE DIMENSIONAL V; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja T, 2018, PR MACH LEARN RES, V80; Henderson P, 2018, AAAI CONF ARTIF INTE, P3207; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Ilyas A., 2018, ARXIV PREPRINT ARXIV; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kakade S., 2002, P 19 INT C MACH LEAR; Konda VR, 2000, ADV NEUR IN, V12, P1008; Lee J., 2019, ARXIV190206720; Ling Y., 2017, MACH LEARN HEALTHC C; Mnih V, 2016, PR MACH LEARN RES, V48; Munos R, 2008, J MACH LEARN RES, V9, P815; Nesterov Y., 2018, APPL OPTIMIZATION; OpenAI, 2019, OPENAI 5; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Rajeswaran A., 2017, ADV NEURAL INFORM PR, P6550; Sallab A. E.L., 2017, ELECT IMAG, P70; Schulman J., 2015, TRUSTREGIONPOLICYOPT; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Wagner P., 2011, ADV NEURAL INFORM PR; Wagner P., 2013, ADV NEURAL INFORM PR; Wang L., 2019, ARXIV190901150; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yang LF, 2019, PR MACH LEARN RES, V97; Yang Zhuora, 2019, ARXIV190100137; Zhang K., 2019, ARXIV190608383; Zhou M, 2019, PR MACH LEARN RES, V97; Zou D, 2018, ARXIV181108888	54	9	10	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902022
C	Ma, QL; Zheng, JW; Li, S; Cottrell, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Qianli; Zheng, Jiawei; Li, Sen; Cottrell, GarrisonW.			Learning Representations for Time Series Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MODELS	Time series clustering is an essential unsupervised technique in cases when category information is not available. It has been widely applied to genome data, anomaly detection, and in general, in any domain where pattern detection is important. Although feature-based time series clustering methods are robust to noise and outliers, and can reduce the dimensionality of the data, they typically rely on domain knowledge to manually construct high-quality features. Sequence to sequence (seq2seq) models can learn representations from sequence data in an unsupervised manner by designing appropriate learning objectives, such as reconstruction and context prediction. When applying seq2seq to time series clustering, obtaining a representation that effectively represents the temporal dynamics of the sequence, multi-scale features, and good clustering properties remains a challenge. How to best improve the ability of the encoder is still an open question. Here we propose a novel unsupervised temporal representation learning model, named Deep Temporal Clustering Representation (DTCR), which integrates the temporal reconstruction and K-means objective into the seq2seq model. This approach leads to improved cluster structures and thus obtains cluster-specific temporal representations. Also, to enhance the ability of encoder, we propose a fake-sample generation strategy and auxiliary classification task. Experiments conducted on extensive time series datasets show that DTCR is state-of-the-art compared to existing methods. The visualization analysis not only shows the effectiveness of cluster-specific representation but also shows the learning process is robust, even if K-means makes mistakes.	[Ma, Qianli; Zheng, Jiawei; Li, Sen] South China Univ Technol, Guangzhou, Peoples R China; [Cottrell, GarrisonW.] Univ Calif San Diego, San Diego, CA 92103 USA	South China University of Technology; University of California System; University of California San Diego	Ma, QL (corresponding author), South China Univ Technol, Guangzhou, Peoples R China.	qianlima@scut.edu.cn; csjwzheng@foxmail.com; awslee@foxmail.com; gary@ucsd.edu			National Natural Science Foundation of China [61502174, 61872148]; Natural Science Foundation of Guangdong Province [2017A030313355, 2017A030313358, 2019A1515010768]; Guangzhou Science and Technology Planning Project [201704030051, 201902010020]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong Province(National Natural Science Foundation of Guangdong Province); Guangzhou Science and Technology Planning Project	We thank the anonymous reviewers for their helpful feedbacks. The work described in this paper was partially funded by the National Natural Science Foundation of China (Grant Nos. 61502174, 61872148), the Natural Science Foundation of Guangdong Province (Grant Nos. 2017A030313355, 2017A030313358, 2019A1515010768), the Guangzhou Science and Technology Planning Project (Grant Nos. 201704030051, 201902010020).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Aghabozorgi S, 2015, INFORM SYST, V53, P16, DOI 10.1016/j.is.2015.04.007; Cai ZW, 2000, J AM STAT ASSOC, V95, P941, DOI 10.2307/2669476; Chan PK, 2005, MASSIVE COMP, V5, P81, DOI 10.1007/0-387-24230-9_3; Chang SY, 2017, ADV NEUR IN, V30; Chen Y, 2015, UCR TIME SERIES CLAS; Cho K., 2014, P 2014 C EMP METH NA, P1724; Demsar J, 2006, J MACH LEARN RES, V7, P1; Ding R, 2015, PROC VLDB ENDOW, V8, P473, DOI 10.14778/2735479.2735481; Ferreira LN, 2016, INFORM SCIENCES, V326, P227, DOI 10.1016/j.ins.2015.07.046; Fujita A, 2012, BMC SYST BIOL, V6, DOI 10.1186/1752-0509-6-137; Gan Z., 2016, ARXIV161107897; Guo C., 2008, P CVPR, P1, DOI DOI 10.1109/CVPR.2008.4587715; Guo XF, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1753; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INF PROCE, P3294; Lei H, 2016, ANN STAT, V44, P1618, DOI 10.1214/15-AOS1430; Li Z, 2012, 26 AAAI C ART INT; Logeswaran Lajanugen, 2018, ARXIV180302893; Ma Qianli, 2019, IEEE T CYBERNETICS; Madiraju N. S., 2018, ARXIV180201059; Paparrizos J, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1855, DOI 10.1145/2723372.2737793; Qian M., 2013, IJCAI; Rabiner L R, 1993, FUNDAMENTALS SPEECH, V14; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Ranjan V., 2018, ARXIV180803840; Shi L, 2014, IEEE DATA MINING, P977, DOI 10.1109/ICDM.2014.58; TJOSTHEIM D, 1994, J AM STAT ASSOC, V89, P1398, DOI 10.2307/2291002; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xie JY, 2016, PR MACH LEARN RES, V48; Yang J., 2011, P 4 ACM INT C WEB SE, P177; Yang Y, 2011, P INT JOINT C ART IN; Zakaria J, 2012, IEEE DATA MINING, P785, DOI 10.1109/ICDM.2012.26; Zha HY, 2002, ADV NEUR IN, V14, P1057; Zhang H, 2006, INFORM-J COMPUT INFO, V30, P305; Zhang Q., 2018, IEEE T PATTERN ANAL	38	9	9	1	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303073
C	Ma, YZ; Zhang, XZ; Sun, W; Zhu, XJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Yuzhe; Zhang, Xuezhou; Sun, Wen; Zhu, Xiaojin			Policy Poisoning in Batch Reinforcement Learning and Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a security threat to batch reinforcement learning and control where the attacker aims to poison the learned policy. The victim is a reinforcement learner / controller which first estimates the dynamics and the rewards from a batch data set, and then solves for the optimal policy with respect to the estimates. The attacker can modify the data set slightly before learning happens, and wants to force the learner into learning a target policy chosen by the attacker. We present a unified framework for solving batch policy poisoning attacks, and instantiate the attack on two standard victims: tabular certainty equivalence learner in reinforcement learning and linear quadratic regulator in control. We show that both instantiation result in a convex optimization problem on which global optimality is guaranteed, and provide analysis on attack feasibility and attack cost. Experiments show the effectiveness of policy poisoning attacks.	[Ma, Yuzhe; Zhang, Xuezhou; Zhu, Xiaojin] Univ Wisconsin, Madison, WI 53706 USA; [Sun, Wen] Microsoft Res New York, New York, NY USA	University of Wisconsin System; University of Wisconsin Madison	Ma, YZ (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	yzm234@cs.wisc.edu; zhangxz1123@cs.wisc.edu; Sun.Wen@microsoft.com; jerryzhu@cs.wisc.edu	Ma, Yuzhe/CAF-7203-2022; Zhang, Xuezhou/ABD-8993-2021		NSF [1545481, 1561512, 1623605, 1704117, 1836978]; MADLab AF Center of Excellence [FA9550-18-1-0166]	NSF(National Science Foundation (NSF)); MADLab AF Center of Excellence	This work is supported in part by NSF 1545481, 1561512, 1623605, 1704117, 1836978 and the MADLab AF Center of Excellence FA9550-18-1-0166.	Asmuth J, 2008, P 23 AAAI C ARTIFICI, P604; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; Brown DS, 2019, AAAI CONF ARTIF INTE, P7749; Cakmak Maya, 2012, 26 AAAI C ART INT; Dean Sarah, 2017, ARXIV171001688; Devlin SM, 2012, P 11 INT C AUTONOMOU, V1, P433; Diamond S, 2016, J MACH LEARN RES, V17; Huang L., 2011, PROC AISEC, P43, DOI DOI 10.1145/2046684.2046692; Huang S., 2017, 5 INT C LEARN REPR I; Jun Kwang-Sung, 2018, ADV NEURAL INFORM PR, P3640; Kamalaruban P, 2019, 28 INT JOINT C ART I, P604; Koh Pang Wei, 2018, ARXIV, DOI 10.1007/s10994-021-06119-y; Li B., 2016, ADV NEURAL INFORM PR, P1885; Liu Fang, 2019, INT C MACH LEARN, P4042; Ma YZ, 2018, LECT NOTES COMPUT SC, V11199, P186, DOI 10.1007/978-3-030-01554-1_11; Mei S., 2015, 29 AAAI C ART INT; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Wang Y, 2018, ARXIV180808994; Wiewiora E, 2003, J ARTIF INTELL RES, V19, P205, DOI 10.1613/jair.1190; Xiao H, 2015, PR MACH LEARN RES, V37, P1689; Zhang X., 2019, ARXIV190301666	22	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906025
C	Maheswaranathan, N; Williams, AH; Golub, MD; Ganguli, S; Sussillo, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Maheswaranathan, Niru; Williams, Alex H.; Golub, Matthew D.; Ganguli, Surya; Sussillo, David			Reverse engineering recurrent networks for sentiment classification reveals line attractor dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recurrent neural networks (RNNs) are a widely used tool for modeling sequential data, yet they are often treated as inscrutable black boxes. Given a trained recurrent network, we would like to reverse engineer it-to obtain a quantitative, interpretable description of how it solves a particular task. Even for simple tasks, a detailed understanding of how recurrent networks work, or a prescription for how to develop such an understanding, remains elusive. In this work, we use tools from dynamical systems analysis to reverse engineer recurrent networks trained to perform sentiment classification, a foundational natural language processing task. Given a trained network, we find fixed points of the recurrent dynamics and linearize the nonlinear system around these fixed points. Despite their theoretical capacity to implement complex, high-dimensional computations, we find that trained networks converge to highly interpretable, low-dimensional representations. In particular, the topological structure of the fixed points and corresponding linearized dynamics reveal an approximate line attractor within the RNN, which we can use to quantitatively understand how the RNN solves the sentiment analysis task. Finally, we find this mechanism present across RNN architectures (including LSTMs, GRUs, and vanilla RNNs) trained on multiple datasets, suggesting that our findings are not unique to a particular architecture or dataset. Overall, these results demonstrate that surprisingly universal and human interpretable computations can arise across a range of recurrent networks.	[Maheswaranathan, Niru; Sussillo, David] Google Inc, Google Brain, Mountain View, CA 94043 USA; [Williams, Alex H.; Golub, Matthew D.] Stanford Univ, Stanford, CA 94305 USA; [Ganguli, Surya] Google Inc, Stanford & Google Brain, Stanford, CA USA	Google Incorporated; Stanford University; Google Incorporated	Maheswaranathan, N (corresponding author), Google Inc, Google Brain, Mountain View, CA 94043 USA.	nirum@google.com; ahwillia@stanford.edu; mgolub@stanford.edu; sganguli@stanford.edu; sussillo@google.com		Ganguli, Surya/0000-0002-9264-7551				Bengio Y., 2014, ARXIV14061078; Collins J., 2016, ARXIV PREPRINT ARXIV; Golub M., 2018, J OPEN SOURCE SOFTW, V3, P1003, DOI [10.21105/joss.01003, DOI 10.21105/JOSS.01003]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karpathy A., 2015, ARXIV150602078; Khalil H., 2001, NONLINEAR SYSTEMS, V3rd; Liu B, 2015, SENTIMENT ANAL, DOI [10.1017/CBO9781139084789, DOI 10.1017/CBO9781139084789]; Lundberg SM, 2017, ADV NEUR IN, V30; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Morcos Ari S, 2018, ARXIV180306959; Murdoch W James, 2018, INT C LEARN REPR; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Strobelt Hendrik, 2016, CORR; Sussillo D, 2013, NEURAL COMPUT, V25, P626, DOI 10.1162/NECO_a_00409; Wang S., 2012, P 50 ANN M ASS COMP, V2, P90, DOI DOI 10.5555/2390665.2390688; Wiegand Michael, 2010, P WORKSH NEG SPEC NA, P60; Zhang L, 2018, WIRES DATA MIN KNOWL, V8, DOI 10.1002/widm.1253; Zhang Xiang, 2015, ADV NEURAL INFORM PR, P649, DOI DOI 10.5555/2969239.2969312	20	9	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE	32782423				2022-12-19	WOS:000535866907036
C	Pinot, R; Meunier, L; Araujo, A; Kashima, H; Yger, F; Gouy-Pailler, C; Atif, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Pinot, Rafael; Meunier, Laurent; Araujo, Alexandre; Kashima, Hisashi; Yger, Florian; Gouy-Pailler, Cedric; Atif, Jamal			Theoretical evidence for adversarial robustness through randomization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper investigates the theory of robustness against adversarial attacks. It focuses on the family of randomization techniques that consist in injecting noise in the network at inference time. These techniques have proven effective in many contexts, but lack theoretical arguments. We close this gap by presenting a theoretical analysis of these approaches, hence explaining why they perform well in practice. More precisely, we make two new contributions. The first one relates the randomization rate to robustness to adversarial attacks. This result applies for the general family of exponential distributions, and thus extends and unifies the previous approaches. The second contribution consists in devising a new upper bound on the adversarial risk gap of randomized neural networks. We support our theoretical claims with a set of experiments.	[Pinot, Rafael; Meunier, Laurent; Araujo, Alexandre; Yger, Florian; Atif, Jamal] PSL Res Univ, Univ Paris Dauphine, CNRS, LAMSADE, Paris, France; [Pinot, Rafael; Gouy-Pailler, Cedric] Univ Paris Saclay, CEA, Inst LIST, St Aubin, France; [Meunier, Laurent] Facebook AI Res, Paris, France; [Araujo, Alexandre] Wavestone, Paris, France; [Kashima, Hisashi] Kyoto Univ, Kyoto, Japan; [Kashima, Hisashi] RIKEN Ctr AIP, Tokyo, Japan	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; CEA; UDICE-French Research Universities; Universite Paris Saclay; Facebook Inc; Kyoto University; RIKEN	Pinot, R (corresponding author), PSL Res Univ, Univ Paris Dauphine, CNRS, LAMSADE, Paris, France.; Pinot, R (corresponding author), Univ Paris Saclay, CEA, Inst LIST, St Aubin, France.		Gouy-Pailler, Cédric/ABF-8297-2021	Gouy-Pailler, Cédric/0000-0003-1298-7845; ARAUJO, Alexandre/0000-0003-2220-5739	JSPS [SP18218]	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	This work was granted access to the OpenPOWER prototype from GENCI-IDRIS under the Preparatory Access AP010610510 made by GENCI. R. Pinot benefited from a JSPS Summer Program Fellowship during this work (Grant number SP18218). L. Meunier and J. Atif would also like to thank Adrien Balp from Societe Generale for his support.	[Anonymous], 2018, INT C LEARN REPR; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Athalye A, 2018, PR MACH LEARN RES, V80; Athalye A, 2018, PR MACH LEARN RES, V80; Beaudry NJ, 2012, QUANTUM INF COMPUT, V12, P432; Ben-Tal A, 2009, PRINC SER APPL MATH, P27; Carlini N., 2019, CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chapeau-Blondeau F, 2004, IEEE T SIGNAL PROCES, V52, P1327, DOI 10.1109/TSP.2004.826176; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Cohen J. M., 2019, ABS190202918 CORR; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dhillon G. S., 2018, P INT C LEARN REPR I; Diochnos D. I., 2018, P ADV NEU INF PRO SY, P10380; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Fawzi A., 2018, ADV NEURAL INFORM PR, P1186; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Fawzi Alhussein, 2018, IEEE CVPR; Goodfellow I. J., 2015, P ICLR; Gouk H., 2018, ARXIV180404368; Guo Chuan, 2018, COUNTERING ADVERSARI, V2, P5; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin Alexey, 2016, WORKSHOP TRACK P, DOI DOI 10.48550/ARXIV.1607.02533; Lecuyer M., 2018, 2019 IEEE S SEC PRIV, P727; Li B., 2018, ABS180903113 CORR; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Madry Aleksander, 2018, ICLR; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Mitaim S, 1998, P IEEE, V86, P2152, DOI 10.1109/5.726785; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Perez L., 2017, ARXIV; Rakin A. Siraj, 2018, ARXIV181109310; Rrnyi A., 1961, TECHNICAL REPORT; Simon-Gabriel C.-J., 2018, ARXIV180201421; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Xie Cihang, 2018, INT C LEARNING REPRE; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zozor S, 1999, IEEE T SIGNAL PROCES, V47, P108, DOI 10.1109/78.738244	45	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903047
C	Qu, C; Mannor, S; Xu, H; Qi, Y; Song, L; Xiong, JW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qu, Chao; Mannor, Shie; Xu, Huan; Qi, Yuan; Song, Le; Xiong, Junwu			Value Propagation for Decentralized Networked Deep Multi-agent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the networked multi-agent reinforcement learning (MARL) problem in a fully decentralized setting, where agents learn to coordinate to achieve joint success. This problem is widely encountered in many areas including traffic control, distributed control, and smart grids. We assume each agent is located at a node of a communication network and can exchange information only with its neighbors. Using softmax temporal consistency, we derive a primal-dual decentralized optimization method and obtain a principled and data-efficient iterative algorithm named value propagation. We prove a non-asymptotic convergence rate of O(1/T) with nonlinear function approximation. To the best of our knowledge, it is the first MARL algorithm with a convergence guarantee in the control, off-policy, non-linear function approximation, fully decentralized setting.	[Qu, Chao; Qi, Yuan; Song, Le; Xiong, Junwu] Ant Financial Serv Grp, Hangzhou, Peoples R China; [Mannor, Shie] Technion, Haifa, Israel; [Xu, Huan] Alibaba Grp, Hangzhou, Peoples R China; [Xu, Huan; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA	Alibaba Group; University System of Georgia; Georgia Institute of Technology	Qu, C (corresponding author), Ant Financial Serv Grp, Hangzhou, Peoples R China.	luoji.qc@antfin.com		Mannor, Shie/0000-0003-4439-7647				Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Adler JL, 2002, TRANSPORT RES C-EMER, V10, P433, DOI 10.1016/S0968-090X(02)00030-X; Alexander Fax J, 2002, IFAC WORLD C, V22; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Brendan McMahan H., 2016, ARXIV160205629; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Callaway DS, 2011, P IEEE, V99, P184, DOI 10.1109/JPROC.2010.2081652; Cattivelli FS, 2008, IEEE T SIGNAL PROCES, V56, P1865, DOI 10.1109/TSP.2007.913164; Cortes J, 2004, IEEE T ROBOTIC AUTOM, V20, P243, DOI 10.1109/TRA.2004.824698; De Witt Christian Schroeder, 2018, ARXIV180311485; DEGROOT MH, 1974, J AM STAT ASSOC, V69, P118, DOI 10.2307/2285509; Duchi J, 2011, J MACH LEARN RES, V12, P2121; El-Tantawy S, 2013, IEEE T INTELL TRANSP, V14, P1140, DOI 10.1109/TITS.2013.2255286; Foerster JN, 2016, ADV NEUR IN, V29; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Hong M.:, 2016, ARXIV160400543; Hong MY, 2017, PR MACH LEARN RES, V70; Hu J., 2003, J MACHINE LEARNING R, V4, P1039, DOI DOI 10.5555/945365.964288; Jiang J., 2018, ARXIV181009202; Kingma D.P, P 3 INT C LEARNING R; Kurakin A, 2016, INT C LEARN REPR SAN; Lauer Martin, 2000, P 17 INT C MACHINE L; Littman ML, 1994, ICML 1994, P157; Lowe R., 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295385; Mo YL, 2017, IEEE T AUTOMAT CONTR, V62, P753, DOI 10.1109/TAC.2016.2564339; Rabbat M, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P20; Raileanu R, 2018, PR MACH LEARN RES, V80; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Tieleman T, TECHNICAL REPORT; Xiao L, 2005, 2005 FOURTH INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P63; Zhang KQ, 2018, PR MACH LEARN RES, V80	38	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301021
C	Song, CJ; Wu, ZJ; Zhou, Y; Gong, ML; Huang, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Chunjin; Wu, Zhijie; Zhou, Yang; Gong, Minglun; Huang, Hui			ETNet: Error Transition Network for Arbitrary Style Transfer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Numerous valuable efforts have been devoted to achieving arbitrary style transfer since the seminal work of Gatys et al. However, existing state-of-the-art approaches often generate insufficiently stylized results under challenging cases. We believe a fundamental reason is that these approaches try to generate the stylized result in a single shot and hence fail to fully satisfy the constraints on semantic structures in the content images and style patterns in the style images. Inspired by the works on error-correction, instead, we propose a self-correcting model to predict what is wrong with the current stylization and refine it accordingly in an iterative manner. For each refinement, we transit the error features across both the spatial and scale domain and invert the processed features into a residual image, with a network we call Error Transition Network (ETNet). The proposed model improves over the state-of-the-art methods with better semantic structures and more adaptive style pattern details. Various qualitative and quantitative experiments show that the key concept of both progressive strategy and error-correction leads to better results. Code and models are available at https://github.com/zhijieW94/ETNet.	[Song, Chunjin; Wu, Zhijie; Zhou, Yang; Huang, Hui] Shenzhen Univ, Shenzhen, Peoples R China; [Gong, Minglun] Univ Guelph, Guelph, ON, Canada	Shenzhen University; University of Guelph	Zhou, Y; Huang, H (corresponding author), Shenzhen Univ, Shenzhen, Peoples R China.	songchunjin1990@gmail.com; wzj.micker@gmail.com; zhouyangvcc@gmail.com; minglun@uoguelph.ca; hhzhiyan@gmail.com		Huang, Hui/0000-0003-3212-0544	NSFC [61861130365, 61761146002, 61602461]; GD Higher Education Innovation Key Program [2018KZDXM058]; GD Science and Technology Program [2015A030312015]; Shenzhen Innovation Program [KQJSCX20170727101233642]; LHTD [20170003]; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)	NSFC(National Natural Science Foundation of China (NSFC)); GD Higher Education Innovation Key Program; GD Science and Technology Program; Shenzhen Innovation Program; LHTD; Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ)	We thank the anonymous reviewers for their constructive comments. This work was supported in parts by NSFC (61861130365, 61761146002, 61602461), GD Higher Education Innovation Key Program (2018KZDXM058), GD Science and Technology Program (2015A030312015), Shenzhen Innovation Program (KQJSCX20170727101233642), LHTD (20170003), and Guangdong Laboratory of Artificial Intelligence and Digital Economy (SZ).	Cao J., 2018, P MACHINE LEARNING R, P707; Cao JZ, 2019, ADV NEUR IN, V32; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Chen DD, 2017, PROC CVPR IEEE, P2770, DOI 10.1109/CVPR.2017.296; Chen TC, 2017, AGEING SOC, V37, P1798, DOI 10.1017/S0144686X16000623; Denton Emily L, 2015, NEURIPS, V2, P4; Dumoulin Vincent, 2016, ARXIV161007629; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Haris M, 2018, PROC CVPR IEEE, P1664, DOI 10.1109/CVPR.2018.00179; Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Jiang P, 2018, ADV NEUR IN, V31; Jing YH, 2021, IEEE T CYBERNETICS, V51, P568, DOI 10.1109/TCYB.2019.2904768; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li X, 2019, PROC CVPR IEEE, P510, DOI 10.1109/CVPR.2019.00060; Li YJ, 2017, ADV NEUR IN, V30; Li YJ, 2017, PROC CVPR IEEE, P266, DOI 10.1109/CVPR.2017.36; Lotter William, 2016, ARXIV160508104; Shen F., 2017, IEEE C COMP VIS PATT; Sheng L, 2018, PROC CVPR IEEE, P8242, DOI 10.1109/CVPR.2018.00860; Tao Yunzhe, 2018, ADV NEURAL INF PROCE, P496; Ulyanov D, 2016, PR MACH LEARN RES, V48; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Yao Y, 2019, PROC CVPR IEEE, P5520, DOI 10.1109/CVPR.2019.00567; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196; Zhang H., 2017, P EUR C COMP VIS ECC; Zhang R, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P183, DOI 10.1145/3240508.3240524; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262	33	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300061
C	Stretcu, O; Viswanathan, K; Movshovitz-Attias, D; Platanios, EA; Tomkins, A; Ravi, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Stretcu, Otilia; Viswanathan, Krishnamurthy; Movshovitz-Attias, Dana; Platanios, Emmanouil Antonios; Tomkins, Andrew; Ravi, Sujith			Graph Agreement Models for Semi-Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Graph-based algorithms are among the most successful paradigms for solving semi-supervised learning tasks. Recent work on graph convolutional networks and neural graph learning methods has successfully combined the expressiveness of neural networks with graph structures. We propose a technique that, when applied to these methods, achieves state-of-the-art results on semi-supervised learning datasets. Traditional graph-based algorithms, such as label propagation, were designed with the underlying assumption that the label of a node can be imputed from that of the neighboring nodes. However, real-world graphs are either noisy or have edges that do not correspond to label agreement. To address this, we propose Graph Agreement Models (GAM), which introduces an auxiliary model that predicts the probability of two nodes sharing the same label as a learned function of their features. The agreement model is used when training a node classification model by encouraging agreement only for the pairs of nodes it deems likely to have the same label, thus guiding its parameters to better local optima. The classification and agreement models are trained jointly in a co-training fashion. Moreover, GAM can also be applied to any semi-supervised classification problem, by inducing a graph whenever one is not provided. We demonstrate that our method achieves a relative improvement of up to 72% for various node classification models, and obtains state-of-the-art results on multiple established datasets.	[Viswanathan, Krishnamurthy; Movshovitz-Attias, Dana; Tomkins, Andrew; Ravi, Sujith] Google Res, Mountain View, CA USA; [Stretcu, Otilia; Platanios, Emmanouil Antonios] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Google Incorporated; Carnegie Mellon University	Stretcu, O (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ostretcu@cs.cmu.edu; kvis@google.com; danama@google.com; e.a.platanios@cs.cmu.edu; tomkins@google.com; sravi@google.com						[Anonymous], 2016, ARXIV E PRINTS; [Anonymous], 2018, IEEE T PAMI; Athiwaratkun B., 2019, P ICLR; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bengio Y., 2018, INT C LEARN REPR; Bhattacharya Indrajit, 2007, ACM T KNOWL DISCOV D, V1, P5, DOI [DOI 10.1145/1217299.1217304, 10.1145/1217299.1217304]; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bui TD, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P64, DOI 10.1145/3159652.3159731; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Deng Z., 2019, ABS190209192 CORR; Devlin J., 2019, C N AM CHAPTER ASS C; Emmanouil Antonios Platanios, 2018, ARXIV180601258; Faerman E., 2017, ARXIV E PRINTS; Grandvalet Y, 2004, ADV NEURAL INFORM PR, V17; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Laine Samuli, 2017, ABS161002242 CORR; Liu W, 2009, PROC CVPR IEEE, P381, DOI 10.1109/CVPRW.2009.5206871; Lu Q., 2003, P 20 INT C MACH LEAR, P496, DOI DOI 10.5555/3041838.3041901; Luo D., 2012, ADV NEURAL INFORM PR, V25, P2960; Luo YC, 2018, PROC CVPR IEEE, P8896, DOI 10.1109/CVPR.2018.00927; Mitchell T, 2018, COMMUN ACM, V61, P103, DOI 10.1145/3191513; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Namata G., 2012, P INT WORKSH MIN LEA; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; OLIVER A., 2018, NEURIPS; Perozzi Bryan, 2014, KDD, P701, DOI DOI 10.1145/2623330.2623732; Sajjadi M., 2016, NIPS; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; Tarvainen Antti, 2017, ICLR; Thekumparampil K. K., 2018, ARXIV E PRINTS; Thomas P., 2009, IEEE T NEURAL NETWOR, V20, P542, DOI DOI 10.1109/TNN.2009.2015974; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Weston J., 2008, P 25 INT C MACHINE L, P1168, DOI [DOI 10.1145/1390156.1390303, 10.1145/1390156.1390303]; Yang Z., 2016, ICML; Yang Zhilin, 2016, PLANETOID GITHUB REP; Ying R., 2018, KDD; Zhou DY, 2004, ADV NEUR IN, V16, P321; Zhou J., 2018, ABS181208434 CORR ABS181208434 CORR; Zhu Xiaojin., 2003, P ICLR, P912	43	9	9	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900032
C	Wu, J; Frazier, PI		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wu, Jian; Frazier, Peter I.			Practical Two-Step Look-Ahead Bayesian Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GLOBAL OPTIMIZATION	Expected improvement and other acquisition functions widely used in Bayesian optimization use a "one-step" assumption: they value objective function evaluations assuming no future evaluations will be performed. Because we usually evaluate over multiple steps, this assumption may leave substantial room for improvement. Existing theory gives acquisition functions looking multiple steps in the future but calculating them requires solving a high-dimensional continuous-state continuous-action Markov decision process (MDP). Fast exact solutions of this MDP remain out of reach of today's methods. As a result, previous two- and multi-step look-ahead Bayesian optimization algorithms are either too expensive to implement in most practical settings or resort to heuristics that may fail to fully realize the promise of two-step lookahead. This paper proposes a computationally efficient algorithm that provides an accurate solution to the two-step lookahead Bayesian optimization problem in seconds to at most several minutes of computation per batch of evaluations. The resulting acquisition function provides increased query efficiency and robustness compared with previous two- and multi-step lookahead methods in both single-threaded and batch experiments. This unlocks the value of two-step lookahead in practice. We demonstrate the value of our algorithm with extensive experiments on synthetic test functions and real-world problems.	[Wu, Jian; Frazier, Peter I.] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA	Cornell University	Frazier, PI (corresponding author), Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA.	wujian046@gmail.com; pf98@cornell.edu						Asmussen S., 2007, STOCHASTIC MODELLING, V57, DOI [10.1007/978-0-387-69033-9, DOI 10.1007/978-0-387-69033-9]; Bai Y, 2016, IEEE IJCNN, P3124, DOI 10.1109/IJCNN.2016.7727597; Bingham D., 2015, OPTIMIZATION TEST PR; Brochu E., 2010, TUTORIAL BAYESIAN OP; Eggensperger K., 2013, NIPS WORK BAYESIAN O; Foreman-Mackey D, 2013, PUBL ASTRON SOC PAC, V125, P306, DOI 10.1086/670067; Forrester A., 2008, ENG DESIGN VIA SURRO, P168; Frazier, 2018, ARXIV180702811; Ginsbourger D, 2010, ADAPT LEARN OPTIM, V2, P131; Ginsburg DH, 2010, COMPET POLICY INT, V6, P89; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P790, DOI 10.1016/j.jpedsurg.2016.02.024; HEIDELBERGER P, 1988, MANAGE SCI, V34, P1281, DOI 10.1287/mnsc.34.11.1281; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hong LJ, 2006, OPER RES, V54, P115, DOI 10.1287/opre.1050.0237; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Kushner HaroldJ., 1964, J BASIC ENG-T ASME, V86, P97, DOI [10.1115/1.3653121, DOI 10.1115/1.3653121]; Lam R., 2018, COMMUNICATION; Lam RR, 2016, ADV NEUR IN, V29; LECUYER P, 1990, MANAGE SCI, V36, P1364, DOI 10.1287/mnsc.36.11.1364; LIU Q, 1994, BIOMETRIKA, V81, P624; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; Osborne M, 2009, LEARNING INTELLIGENT, P1; Poloczek M, 2017, ADV NEUR IN, V30; Powell W. B., 2007, APPROXIMATE DYNAMIC, V703; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Smith SP, 1995, J COMPUT GRAPH STAT, V4, P134, DOI DOI 10.2307/1390762; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Wang J., 2016, OPERATIONS RES; Wang Z, 2017, PR MACH LEARN RES, V70; Wu Jiajun, 2017, ADV NEURAL INFORM PR, V3	35	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901044
C	Xu, D; Ruan, CW; Kumar, S; Korpeoglu, E; Achan, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Da; Ruan, Chuanwei; Kumar, Sushant; Korpeoglu, Evren; Achan, Kannan			Self-attention with Functional Time Representation Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Sequential modelling with self-attention has achieved cutting edge performances in natural language processing. With advantages in model flexibility, computation complexity and interpretability, self-attention is gradually becoming a key component in event sequence models. However, like most other sequence models, self-attention does not account for the time span between events and thus captures sequential signals rather than temporal patterns. Without relying on recurrent network structures, self-attention recognizes event orderings via positional encoding. To bridge the gap between modelling time-independent and time-dependent event sequence, we introduce a functional feature map that embeds time span into high-dimensional spaces. By constructing the associated translation-invariant time kernel function, we reveal the functional forms of the feature map under classic functional function analysis results, namely Bochner's Theorem and Mercer's Theorem. We propose several models to learn the functional time representation and the interactions with event representation. These methods are evaluated on real-world datasets under various continuous-time event sequence prediction tasks. The experiments reveal that the proposed methods compare favorably to baseline models while also capturing useful time-event interactions.	[Xu, Da; Ruan, Chuanwei; Kumar, Sushant; Korpeoglu, Evren; Achan, Kannan] Walmart Labs, Sunnyvale, CA 94086 USA	Wal-Mart Stores Inc	Xu, D (corresponding author), Walmart Labs, Sunnyvale, CA 94086 USA.	Da.Xu@walmartlabs.com; Chuanwei.Ruan@walmartlabs.com; SKumar4@walmartlabs.com; EKorpeoglu@walmartlabs.com; KAchan@walmartlabs.com	Kumar, Sushant/ABH-9893-2020					Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Chen L, 2017, PROC CVPR IEEE, P6298, DOI 10.1109/CVPR.2017.667; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Dinh L, 2016, ARXIV PREPRINT ARXIV; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Jackson D., 1930, THEORY APPROXIMATION, V11; Kang WC, 2018, IEEE DATA MINING, P197, DOI 10.1109/ICDM.2018.00035; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li Y., 2017, ARXIV170800065; Loomis L. H., 2013, INTRO ABSTRACT HARMO; Mei HY, 2017, ADV NEUR IN, V30; Papamakarios George, 2017, ARXIV170507057; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Tang JX, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P565, DOI 10.1145/3159652.3159656; Tikk D, 2015, ARXIV151106939; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; WIDOM H, 1964, ARCH RATION MECH AN, V17, P215; Xiao S., 2017, OINT MODELING EVENT; Xiao Shuai, 2017, AAAI; Xu D., 2019, ARXIV190412574V2; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zhao QY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1513, DOI 10.1145/2783258.2783401; Zhu Y, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3602	27	9	9	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907056
C	Xu, YL; Cao, P; Kong, YQ; Wang, YZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Yilun; Cao, Peng; Kong, Yuqing; Wang, Yizhou			L-DMI: A Novel Information-theoretic Loss Function for Training Deep Nets Robust to Label Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Accurately annotating large scale dataset is notoriously expensive both in time and in money. Although acquiring low-quality-annotated dataset can be much cheaper, it often badly damages the performance of trained models when using such dataset without particular treatment. Various methods have been proposed for learning with noisy labels. However, most methods only handle limited kinds of noise patterns, require auxiliary information or steps (e.g., knowing or estimating the noise transition matrix), or lack theoretical justification. In this paper, we propose a novel information-theoretic loss function, LDMI, for training deep neural networks robust to label noise. The core of LDMI is a generalized version of mutual information, termed Determinant based Mutual Information (DMI), which is not only information-monotone but also relatively invariant. To the best of our knowledge, LDMI is the first loss function that is provably robust to instance-independent label noise, regardless of noise pattern, and it can be applied to any existing classification neural networks straightforwardly without any auxiliary information. In addition to theoretical justification, we also empirically show that using LDMI outperforms all other counterparts in the classification task on both image dataset and natural language dataset include Fashion-MNIST, CIFAR-10, Dogs vs. Cats, MR with a variety of synthesized noise patterns and noise amounts, as well as a real-world dataset Clothing1M.	[Xu, Yilun; Cao, Peng] Peking Univ, Sch Elect Engn & Comp Sci, Beijing, Peoples R China; [Kong, Yuqing] Peking Univ, Comp Sci Dept, Ctr Frontiers Comp Studies, Beijing, Peoples R China; [Wang, Yizhou] Peking Univ, Deepwise AI Lab, Comp Sci Dept, Beijing, Peoples R China	Peking University; Peking University; Peking University	Xu, YL (corresponding author), Peking Univ, Sch Elect Engn & Comp Sci, Beijing, Peoples R China.	xuyilun@pku.edu.cn; caopeng2016@pku.edu.cn; yuqing.kong@pku.edu.cn; Yizhou.Wang@pku.edu.cn			 [2018AAA0102004];  [NSFC-61625201];  [NSFC-61527804]	; ; 	We would like to express our thanks for support from the following research grants: 2018AAA0102004, NSFC-61625201, NSFC-61527804.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brooks JP, 2011, OPER RES, V59, P467, DOI 10.1287/opre.1100.0854; Cao Peng, 2018, MAX MIG INFORM THEOR; Cheng J., 2017, ARXIV170903768; Ghosh A., 2017, 31 AAAI C ART INT; Goldberger Jacob, 2016, TRAINING DEEP NEURAL; Han B., 2018, ARXIV180508193, P5836; Han B, 2018, ADV NEUR IN, V31, DOI 10.5555/3327757.3327944; Hendrycks D., 2018, ADV NEURAL INFORM PR, P10456; Hjelm R.D., 2018, P INT C LEARN REPR; Jiang L., 2017, ARXIV171205055, V4; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kim Y., 2014, P 2014 C EMP METH NA; Kong YQ, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P177, DOI 10.1145/3219166.3219194; Kong Yuqing, ACM SIAM S DISCR ALG; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee KH, 2018, PROC CVPR IEEE, P5447, DOI 10.1109/CVPR.2018.00571; Liu TL, 2016, IEEE T PATTERN ANAL, V38, P447, DOI 10.1109/TPAMI.2015.2456899; Ma Xingjun, 2018, ARXIV180602612; Manwani N, 2013, IEEE T CYBERNETICS, V43, P1146, DOI 10.1109/TSMCB.2012.2223460; Masnadi-Shirazi Hamed, 2009, P 21 INT C NEUR INF; Menon A., 2016, ARXIV160500751; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Pang B., 2005, P 43 ANN M ASS COMP, V43, P115, DOI DOI 10.3115/1219840.1219855; Paszke Adam, 2017, PYTORCH TENSORS DYNA; Patrini G., 2017, P IEEE C COMP VIS PA, P1944; Rabinovich S. E, 2014, P 3 INT C LEARN REPR; Ramaswamy HG, 2016, PR MACH LEARN RES, V48; Scott C, 2015, JMLR WORKSH CONF PRO, V38, P838; Seneta E, 2006, SPRINGER SER STAT, P1, DOI 10.1007/0-387-32792-4; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI DOI 10.1002/J.1538-7305.1948.TB01338.X; Sukhbaatar Sainbayar, 2014, ICLR; Tanaka D, 2018, PROC CVPR IEEE, P5552, DOI 10.1109/CVPR.2018.00582; Vahdat A, 2017, ADV NEURAL INFORM PR, P5596; Van Rooyen B., 2015, P ADV NEURAL INFORM, P10; Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696; Xiao H., 2017, ARXIV 170807747; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Yao Jiangchao, 2019, SAFEGUARDED DYNAMIC; Yi Kun, 2019, ARXIV190307788; Zeng W., 2018, ARXIV180309050; Zhang Chiyuan, 2016, ARXIV161103530; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31; Zhang Z., 2018, ADV NEURAL INFORM PR, P8778	46	9	9	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306025
C	Zhang, LF; Tan, ZH; Song, JB; Chen, JW; Bao, CL; Ma, KS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Linfeng; Tan, Zhanhong; Song, Jiebo; Chen, Jingwei; Bao, Chenglong; Ma, Kaisheng			SCAN: A Scalable Neural Networks Framework Towards Compact and Efficient Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Remarkable achievements have been attained by deep neural networks in various applications. However, the increasing depth and width of such models also lead to explosive growth in both storage and computation, which has restricted the deployment of deep neural networks on resource-limited edge devices. To address this problem, we propose the so-called SCAN framework for networks training and inference, which is orthogonal and complementary to existing acceleration and compression methods. The proposed SCAN firstly divides neural networks into multiple sections according to their depth and constructs shallow classifiers upon the intermediate features of different sections. Moreover, attention modules and knowledge distillation are utilized to enhance the accuracy of shallow classifiers. Based on this architecture, we further propose a threshold controlled scalable inference mechanism to approach human-like sample-specific inference. Experimental results show that SCAN can be easily equipped on various neural networks without any adjustment on hyper-parameters or neural networks architectures, yielding significant performance gain on CIFAR100 and ImageNet.	[Zhang, Linfeng; Tan, Zhanhong; Ma, Kaisheng] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China; [Song, Jiebo] Tsinghua Univ, Yau Math Sci Ctr, Beijing, Peoples R China; [Chen, Jingwei] Inst Interdisciplinary Informat Core Technol, Beijing, Peoples R China; [Bao, Chenglong] HiSilicon, Shenzhen, Peoples R China	Tsinghua University; Tsinghua University	Ma, KS (corresponding author), Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China.; Bao, CL (corresponding author), HiSilicon, Shenzhen, Peoples R China.	zhang-lf19@mails.tsinghua.edu.cn; tanzh19@mails.tsinghua.edu.cn; songjb@iiisct.com; jean.chenjingwei@hisilicon.com; clbao@mail.tsinghua.edu.cn; kaisheng@mail.tsinghua.edu.cn		Zhang, Linfeng/0000-0002-3341-183X	Institute for Interdisciplinary Information Core Technology, Beijing Academy of Artificial Intelligence and Zhongguancun Haihua Institute for Frontier Information Technology	Institute for Interdisciplinary Information Core Technology, Beijing Academy of Artificial Intelligence and Zhongguancun Haihua Institute for Frontier Information Technology	This paper is supported by Institute for Interdisciplinary Information Core Technology, Beijing Academy of Artificial Intelligence and Zhongguancun Haihua Institute for Frontier Information Technology.	Bahdanau D., 2015, ICLR; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Courbariaux M., 2015, ADV NEUR IN, P3123; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Fu J., 2017, P CVPR, V2, P3; Han S., 2016, ICLR; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Hinton Geoffrey, 2014, NEURIPS; Howard A. G., 2017, CVPR; Huang G., 2018, P INT C LEARNING REP, P1, DOI 10.48550/arXiv.1703.09844; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Jandola Forrest N, 2016, ICLR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; KIM E, 2018, CVPR, P8669, DOI DOI 10.1109/CVPR.2018.00904; Kim E, 2018, PROC CVPR IEEE, P8669, DOI 10.1109/CVPR.2018.00904; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kumar A, 2017, PR MACH LEARN RES, V70; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Liu W., 2016, SINGLE SHOT MULTIBOX; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Romero A., 2015, ICLR; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; WANG B, 2018, ECCV, P409, DOI DOI 10.1007/978-981-10-6077-9_15; Wang F, 2017, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2017.683; Wang XJ, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2051, DOI 10.1145/3097983.3098096; Wu ZX, 2018, PROC CVPR IEEE, P8817, DOI 10.1109/CVPR.2018.00919; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; XuK BaJ, 2015, ICML, P20148; Yu Jiahui, 2019, ICLR; Zagoruyko S., 2017, ICLR; Zagoruyko S., 2016, BMVC; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196; Zhang Linfeng, 2019, 190508094 ARXIV; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454	40	9	9	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304007
C	Zhang, SQ; Zhang, Q; Lin, JY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Sai Qian; Zhang, Qi; Lin, Jieyu			Efficient Communication in Multi-Agent Reinforcement Learning via Variance Based Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Multi-agent reinforcement learning (MARL) has recently received considerable attention due to its applicability to a wide range of real-world applications. However, achieving efficient communication among agents has always been an overarching problem in MARL. In this work, we propose Variance Based Control (VBC), a simple yet efficient technique to improve communication efficiency in MARL. By limiting the variance of the exchanged messages between agents during the training phase, the noisy component in the messages can be eliminated effectively, while the useful part can be preserved and utilized by the agents for better performance. Our evaluation using multiple MARL benchmarks indicates that our method achieves 2 - 10X lower in communication overhead than state-of-the-art MARL algorithms, while allowing agents to achieve better overall performance.	[Zhang, Sai Qian] Harvard Univ, Cambridge, MA 02138 USA; [Zhang, Qi] Amazon Inc, Seattle, WA USA; [Lin, Jieyu] Univ Toronto, Toronto, ON, Canada	Harvard University; Amazon.com; University of Toronto	Zhang, SQ (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.							De Witt Christian Schroeder, 2018, ARXIV180311485; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Foerster J. N., 2018, ARXIV181011702; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Hausknecht M., 2015, P 2015 AAAI FALL S S, V9, P29; Kim D., 2019, INT C LEARN REPR; Kober J., 2013, INT J ROBOTICS RES; Lowe R., 2017, P INT C NEUR INF PRO, P6379; Ma JY, 2018, IEEE INT CONF ROBOT, P7254; Marc L., 2017, ADV NEURAL INFORM PR; Mnih V., 2013, ARXIV PREPRINT ARXIV; Peng P., 2017, ARXIV170310069; Samvelyan M., 2019, ABS190204043 CORR; Shai S.-S., 2016, ARXIV161003295; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sunehag P., 2017, ARXIV170605296; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; Tan M., 1993, ICML	18	9	9	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303025
C	Zhao, S; Wang, Y; Yang, Z; Cai, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Shuai; Wang, Yang; Yang, Zheng; Cai, Deng			Region Mutual Information Loss for Semantic Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Semantic segmentation is a fundamental problem in computer vision. It is considered as a pixel-wise classification problem in practice, and most segmentation models use a pixel-wise loss as their optimization criterion. However, the pixelwise loss ignores the dependencies between pixels in an image. Several ways to exploit the relationship between pixels have been investigated, e.g., conditional random fields (CRF) and pixel affinity based methods. Nevertheless, these methods usually require additional model branches, large extra memories, or more inference time. In this paper, we develop a region mutual information (RMI) loss to model the dependencies among pixels more simply and efficiently. In contrast to the pixel-wise loss which treats the pixels as independent samples, RMI uses one pixel and its neighbour pixels to represent this pixel. Then for each pixel in an image, we get a multi-dimensional point that encodes the relationship between pixels, and the image is cast into a multi-dimensional distribution of these high-dimensional points. The prediction and ground truth thus can achieve high order consistency through maximizing the mutual information (MI) between their multi-dimensional distributions. Moreover, as the actual value of the MI is hard to calculate, we derive a lower bound of the MI and maximize the lower bound to maximize the real value of the MI. RMI only requires a few extra computational resources in the training stage, and there is no overhead during testing. Experimental results demonstrate that RMI can achieve substantial and consistent improvements in performance on PASCAL VOC 2012 and CamVid datasets.	[Zhao, Shuai; Cai, Deng] Zhejiang Univ, Coll Comp Sci, State Key Lab CAD&CG, Hangzhou, Peoples R China; [Wang, Yang] Huazhong Univ Sci & Technol, Sch Artificial Intelligence & Automat, Wuhan, Peoples R China; [Yang, Zheng] Fabu Inc, Hangzhou, Peoples R China; [Cai, Deng] Alibaba Zhejiang Univ Joint Inst Frontier Technol, Hangzhou, Peoples R China	Zhejiang University; Huazhong University of Science & Technology	Cai, D (corresponding author), Zhejiang Univ, Coll Comp Sci, State Key Lab CAD&CG, Hangzhou, Peoples R China.; Cai, D (corresponding author), Alibaba Zhejiang Univ Joint Inst Frontier Technol, Hangzhou, Peoples R China.	zhaoshuaimcc@gmail.com; wangyang_sky@hust.edu.cn; yangzheng@fabu.ai; dcai@zju.edu.cn		Zhao, Shuai/0000-0003-1320-4283	National Key Research and Development Program of China [2018AAA0101400]; National Nature Science Foundation of China [61936006]	National Key Research and Development Program of China; National Nature Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by The National Key Research and Development Program of China (Grant Nos: 2018AAA0101400), in part by The National Nature Science Foundation of China (Grant Nos: 61936006).	Abadi Martin, 2016, arXiv; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen LB, 2017, IEEE INT SYMP NANO, P1, DOI 10.1109/NANOARCH.2017.8053709; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI 10.1002/0471200611; Everingham Mark, 2010, IJCV; Fu Justin., 2017, ARXIV PREPRINT ARXIV; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Tong, 2018, ABS181201187 CORR; Hoeffding Wassily, 1948, DUKE MATH J; Hyvdrinen Aapo, 2000, NEURAL NETWORKS; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Lin D, 2018, LECT NOTES COMPUT SC, V11207, P622, DOI 10.1007/978-3-030-01219-9_37; Liu SF, 2017, ADV NEUR IN, V30; Liu Ziwei, 2018, PAMI; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maes Frederik, 1997, IEEE T MED IMAGING; Maire Michael, 2016, CVPR; Paszke Adam, 2017, NEURIPS W; Petersen K. B., 2012, MATRIX COOKBOOK; Pluim Josien P. W., 2003, IEEE T MED IMAGING; Prasad Sudhakar, 2010, ABS10101508 CORR; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen FL, 2017, PROC CVPR IEEE, P5178, DOI 10.1109/CVPR.2017.550; Tomasi C., 2004, ECCV; Triantafyllopoulos K., 2008, STATISTICS; Viergever Max A., 2016, MED IMAGE ANAL; Viola Paul A., 1997, IJCV; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	41	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902071
C	Zhussip, M; Soltanayev, S; Chun, SY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhussip, Magauiya; Soltanayev, Shakarim; Chun, Se Young			Extending Stein's unbiased risk estimator to train deep denoisers with correlated pairs of noisy images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SURE	Recently, Stein's unbiased risk estimator (SURE) has been applied to unsupervised training of deep neural network Gaussian denoisers that outperformed classical non-deep learning based denoisers and yielded comparable performance to those trained with ground truth. While SURE requires only one noise realization per image for training, it does not take advantage of having multiple noise realizations per image when they are available (e.g., two uncorrelated noise realizations per image for Noise2Noise). Here, we propose an extended SURE (eSURE) to train deep denoisers with correlated pairs of noise realizations per image and applied it to the case with two uncorrelated realizations per image to achieve better performance than SURE based method and comparable results to Noise2Noise. Then, we further investigated the case with imperfect ground truth (i.e., mild noise in ground truth) that may be obtained considering painstaking, time-consuming, and even expensive processes of collecting ground truth images with multiple noisy images. For the case of generating noisy training data by adding synthetic noise to imperfect ground truth to yield correlated pairs of images, our proposed eSURE based training method outperformed conventional SURE based method as well as Noise2Noise. Code is available at https.//github.com/Magauiya/Extended_SURE	[Zhussip, Magauiya; Soltanayev, Shakarim; Chun, Se Young] Ulsan Natl Inst Sci & Technol UNIST, Ulsan, South Korea	Ulsan National Institute of Science & Technology (UNIST)	Zhussip, M (corresponding author), Ulsan Natl Inst Sci & Technol UNIST, Ulsan, South Korea.	mzhussip@unist.ac.kr; shakarim@unist.ac.kr; sychun@unist.ac.kr		Chun, Se Young/0000-0001-8739-8960	Basic Science Research Program through the National Research Foundation of Korea(NRF) - Ministry of Education [NRF-2017R1D1A1B05035810]; Technology Innovation Program or Industrial Strategic Technology Development Program - Ministry of Trade, Industry & Energy (MOTIE, Korea) [10077533]; Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI); Ministry of Health & Welfare, Republic of Korea [HI18C0316]	Basic Science Research Program through the National Research Foundation of Korea(NRF) - Ministry of Education(National Research Foundation of Korea); Technology Innovation Program or Industrial Strategic Technology Development Program - Ministry of Trade, Industry & Energy (MOTIE, Korea); Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI)(Korea Health Industry Development Institute (KHIDI)); Ministry of Health & Welfare, Republic of Korea(Ministry of Health & Welfare (MOHW), Republic of Korea)	This work was supported partly by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education(NRF-2017R1D1A1B05035810), the Technology Innovation Program or Industrial Strategic Technology Development Program (10077533, Development of robotic manipulation algorithm for grasping/assembling with the machine learning using visual and tactile sensing information) funded by the Ministry of Trade, Industry & Energy (MOTIE, Korea), and a grant of the Korea Health Technology R&D Project through the Korea Health Industry Development Institute (KHIDI), funded by the Ministry of Health & Welfare, Republic of Korea (grant number: HI18C0316).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Batson J., 2019, P 36 INT C MACH LEAR, P524; Blu T, 2007, IEEE T IMAGE PROCESS, V16, P2778, DOI 10.1109/TIP.2007.906002; Brooks Tim, 2019, IEEE C COMP VIS PATT; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Cha E., 2019, ARXIV190607330; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Deledalle CA, 2014, SIAM J IMAGING SCI, V7, P2448, DOI 10.1137/140968045; Eldar YC, 2009, IEEE T SIGNAL PROCES, V57, P471, DOI 10.1109/TSP.2008.2008212; Hoffman T., 2007, ADV NEURAL INFORM PR, V19, P1145; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma DP, 2015, INT C LEARN REPR ICL; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krull A, 2019, PROC CVPR IEEE, P2124, DOI 10.1109/CVPR.2019.00223; Krull Alexander, 2019, ARXIV190600651; Laine Samuli, 2019, INT C LEARN REPR ICL; Le Montagner Y, 2014, IEEE T IMAGE PROCESS, V23, P1255, DOI 10.1109/TIP.2014.2300821; Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623; Lehtinen Jaakko, 2018, P 35 INT C MACH LEAR, P2965; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Metzler C., 2019, INT BIOM ASTR SIGN P; Plotz Tobias, 2017, P IEEE C COMP VIS PA, P1586; Ramani S, 2008, IEEE T IMAGE PROCESS, V17, P1540, DOI 10.1109/TIP.2008.2001404; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Soltanayev Shakarim, 2018, ADV NEURAL INFORM PR, V31, P3261; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Van de Ville D, 2011, IEEE T IMAGE PROCESS, V20, P2683, DOI 10.1109/TIP.2011.2121083; Van De Ville D, 2009, IEEE SIGNAL PROC LET, V16, P973, DOI 10.1109/LSP.2009.2027669; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhou Y., 2019, ARXIV190403485; Zhussip M, 2019, PROC CVPR IEEE, P10247, DOI 10.1109/CVPR.2019.01050	35	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301045
C	Arik, SO; Chen, JT; Peng, KN; Ping, W; Zhou, YQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Arik, Sercan O.; Chen, Jitong; Peng, Kainan; Ping, Wei; Zhou, Yanqi			Neural Voice Cloning with a Few Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Voice cloning is a highly desired feature for personalized speech interfaces. We introduce a neural voice cloning system that learns to synthesize a person's voice from only a few audio samples. We study two approaches: speaker adaptation and speaker encoding. Speaker adaptation is based on fine-tuning a multi-speaker generative model. Speaker encoding is based on training a separate model to directly infer a new speaker embedding, which will be applied to a multi-speaker generative model. In terms of naturalness of the speech and similarity to the original speaker, both approaches can achieve good performance, even with a few cloning - audios.(2) While speaker adaptation can achieve slightly better naturalness and similarity, cloning time and required memory for the speaker encoding approach are significantly less, making it more favorable for low-resource deployment.	[Arik, Sercan O.; Chen, Jitong; Peng, Kainan; Ping, Wei; Zhou, Yanqi] Baidu Res, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA	Baidu	Arik, SO (corresponding author), Baidu Res, 1195 Bordeaux Dr, Sunnyvale, CA 94089 USA.	sercanarik@baidu.com; chenjitong01@baidu.com; pengkainan@baidu.com; pingwei01@baidu.com; yanqiz@baidu.com	Peng, Kainan/AAF-5799-2020					Abdel-Hamid O., 2013, IEEE ICASSP; Agiomyrgiannakis Y., 2016, IEEE ICASSP; Amodei D, 2016, PR MACH LEARN RES, V48; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2018, ICLR; Arik S. O., 2017, ICML; Azadi S., 2017, CORR; Chen L. H., 2014, IEEE ACM T AUDIO SPE; Cui Xiaodong, 2017, ARXIV171006937; Desai S., 2010, IEEE T AUDIO SPEECH; Diamos G., 2017, ADV NEURAL INFORM PR, V30, P2966; Hwang H. T., 2015, 2015 AS PAC SIGN INF; Jozefowicz Rafal, 2016, ARXIV160202410; Karras T., 2017, CORR; Lake B. M., 2013, NIPS; Lake B.M., 2014, COGSCI; Lake B. M., 2015, SCIENCE; Li X., 2015, INTERSPEECH; Mehri S., 2016, ARXIV161207837; Miao Y., 2015, 16 ANN C INT SPEECH; Miao Y., 2015, IEEE ACM T AUDIO SPE; Oord A. v. d., 2016, ARXIV160903499; Panayotov V., 2015, IEEE ICASSP; Prince S., 2007, ICCV; Reed S. E., 2017, CORR; Rezende D., 2016, ICML; Shen J., 2017, ARXIV171205884; Snyder D, 2016, IEEE W SP LANG TECH, P165, DOI 10.1109/SLT.2016.7846260; Sotelo J., 2017, CHAR2WAV END TO END; Taigman Yaniv, 2018, ICLR; vander Oord A., 2016, ADV NEURAL INFORM PR; Veaux C., 2017, CSTR VCTK CORPUS ENG; Wang Yandan, 2017, CORR; Wester M, 2016, INTERSPEECH, P1637, DOI 10.21437/Interspeech.2016-1331; Wu YC, 2016, INTERSPEECH, P1652, DOI 10.21437/Interspeech.2016-567; Wu Z., 2015, INTERSPEECH; Xue S., 2014, IEEE ACM T AUDIO SPE; Yamagishi J., 2009, IEEE T AUDIO SPEECH; Yu D., 2013, IEEE ICASSP	39	9	9	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004056
C	Bhattacharjya, D; Subramanian, D; Gao, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bhattacharjya, Debarun; Subramanian, Dharmashankar; Gao, Tian			Proximal Graphical Event Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Event datasets involve irregular occurrences of events over the timeline and are prevalent in numerous domains. We introduce proximal graphical event models (PGEMs) as a representation of such datasets. PGEMs belong to a broader family of graphical models that characterize relationships between various types of events; in a PGEM, the rate of occurrence of an event type depends only on whether or not its parents have occurred in the most recent history. The main advantage over state-of-the-art models is that learning is entirely data driven and without the need for additional inputs from the user, which can require knowledge of the domain such as choice of basis functions and hyper-parameters. We theoretically justify our learning of parental sets and their optimal windows, proposing sound and complete algorithms in terms of parent structure learning. We present efficient heuristics for learning PGEMs from data, demonstrating their effectiveness on synthetic and real datasets.	[Bhattacharjya, Debarun; Subramanian, Dharmashankar; Gao, Tian] IBM Res, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Bhattacharjya, D (corresponding author), IBM Res, Thomas J Watson Res Ctr, Yorktown Hts, NY 10598 USA.	debarunb@us.ibm.com; dharmash@us.ibm.com; tgao@us.ibm.com						Dahlhaus R, 2000, METRIKA, V51, P157, DOI 10.1007/s001840000055; de Campos CP, 2011, J MACH LEARN RES, V12, P663; Dean T., 1989, Computational Intelligence, V5, P142, DOI 10.1111/j.1467-8640.1989.tb00324.x; Didelez V, 2008, J ROY STAT SOC B, V70, P245, DOI 10.1111/j.1467-9868.2007.00634.x; Eichler M., 1999, THESIS; Fournier-Viger P, 2014, J MACH LEARN RES, V15, P3389; Gao Tian, 2018, P INT C MACH LEARN I, P1671; Gerner D. J., 2002, INT STUD ASS ISA ANN; Goulding J, 2016, IEEE DATA MINING, P161, DOI [10.1109/ICDM.2016.0027, 10.1109/ICDM.2016.150]; Gunawardana A., 2011, ADV NEURAL INFORM PR, P1962; Gunawardana A, 2016, JMLR WORKSH CONF PRO, V51, P556; Meek C., 2014, P UAI WORKSH CAUS IN, P43; Murphy KP., 2002, DYNAMIC BAYESIAN NET; Nodelman U., 2002, P 18 C UNCERTAINTY A, P378; O'Brien SP, 2010, INT STUD REV, V12, P87, DOI 10.1111/j.1468-2486.2009.00914.x; Parikh Ankur P, 2012, P UNC ART INT WORKSH; Rajaram S, 2005, P INT WORKSH ART INT; SIMMA A., 2010, PROC UAI 10, P546; Simma A., 2008, P 24 C UAI AUAI, P484; Teyssier M., 2005, P 21 C UNC ART INT, P584; Weiss Jeremy C., 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8190, P547, DOI 10.1007/978-3-642-40994-3_35; Zhou K, 2013, ICML	22	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002066
C	Chung, YA; Weng, WH; Tong, S; Glass, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chung, Yu-An; Weng, Wei-Hung; Tong, Schrasing; Glass, James			Unsupervised Cross-Modal Alignment of Speech and Text Embedding Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISCOVERY	Recent research has shown that word embedding spaces learned from text corpora of different languages can be aligned without any parallel data supervision. Inspired by the success in unsupervised cross-lingual word embeddings, in this paper we target learning a cross-modal alignment between the embedding spaces of speech and text learned from corpora of their respective modalities in an unsupervised fashion. The proposed framework learns the individual speech and text embedding spaces, and attempts to align the two spaces via adversarial training, followed by a refinement procedure. We show how our framework could be used to perform spoken word classification and translation, and the experimental results on these two tasks demonstrate that the performance of our unsupervised alignment approach is comparable to its supervised counterpart. Our framework is especially useful for developing automatic speech recognition (ASR) and speech-to-text translation systems for low- or zero-resource languages, which have little parallel audio-text data for training modern supervised ASR and speech-to-text translation models, but account for the majority of the languages spoken across the world.	[Chung, Yu-An; Weng, Wei-Hung; Tong, Schrasing; Glass, James] MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Chung, YA (corresponding author), MIT, Comp Sci & Artificial Intelligence Lab, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	andyyuan@mit.edu; ckbjimmy@mit.edu; st9@mit.edu; glass@mit.edu						Amodei D, 2016, PR MACH LEARN RES, V48; Artetxe M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P451, DOI 10.18653/v1/P17-1042; Artetxe Mikel, 2016, EMNLP; Artetxe Mikel, 2018, ICLR; Bengio S, 2014, INTERSPEECH, P1053; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Cao Hailong, 2016, COLING; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chen Y.-C., 2018, ARXIV180310952; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chung  Y.-A., 2017, NIPS ML4AUDIO WORKSH; Chung YA, 2018, INTERSPEECH, P811, DOI 10.21437/Interspeech.2018-2341; Chung YA, 2016, INTERSPEECH, P765, DOI 10.21437/Interspeech.2016-82; Conneau Alexis, 2018, INT C LEARN REPR; Dinu Georgiana, 2015, ICLR WORKSH TRACK; Duong Long, 2016, EMNLP; Faruqui Manaal, 2014, P EACL, DOI [10.3115/v1/E14-1049, DOI 10.3115/V1/E14-1049]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; He  W., 2017, ICLR; Jansen Aren, 2011, ASRU; Kamper H., 2016, ICASSP; Kamper  H., 2017, ASRU; Kamper H, 2017, COMPUT SPEECH LANG, V46, P154, DOI 10.1016/j.csl.2017.04.008; Kamper H, 2016, IEEE-ACM T AUDIO SPE, V24, P669, DOI 10.1109/TASLP.2016.2517567; Kocabiyikoglu Ali, 2018, LREC; Kohn A., 2016, LREC; Lample Guillaume, 2018, ICLR; Lee C. y., 2015, T ASSOC COMPUT LING, V3, P389, DOI 10.1162/tacl_a_00146; Levin K., 2013, ASRU; Mikolov T., 2013, ARXIV; Mikolov T., 2013, ARXIV PREPRINT ARXIV; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Park AS, 2008, IEEE T AUDIO SPEECH, V16, P186, DOI 10.1109/TASL.2007.909282; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ra<spacing O. J., 2015, INTERSPEECH; Settle  S., 2016, SLT; Smith S. L., 2016, ICLR; Sun M, 2013, COMPUT SPEECH LANG, V27, P969, DOI 10.1016/j.csl.2012.09.006; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Waibel A, 2008, IEEE SIGNAL PROC MAG, V25, P70, DOI 10.1109/MSP.2008.918415; Walter O., 2013, ASRU; Xing Chao, 2015, NAACL HLT; Zhang Meng, 2017, EMNLP; Zhang Meng, 2017, ACL	48	9	10	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001087
C	Farnia, F; Tse, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Farnia, Farzan; Tse, David			A Convex Duality Framework for GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generative adversarial network (GAN) is a minimax game between a generator mimicking the true model and a discriminator distinguishing the samples produced by the generator from the real training samples. Given an unconstrained discriminator able to approximate any function, this game reduces to finding the generative model minimizing a divergence score, e.g. the Jensen-Shannon (JS) divergence, to the data distribution. However, in practice the discriminator is constrained to be in a smaller class such as convolutional neural nets. Then, a natural question is how the divergence minimization interpretation will change as we constrain F. In this work, we address this question by developing a convex duality framework for analyzing GAN minimax problems. For a convex set F, this duality framework interprets the original vanilla GAN problem as finding the generative model with the minimum JS-divergence to the distributions penalized to match the moments of the data distribution, with the moments specified by the discriminators in F. We show that this interpretation more generally holds for f-GAN and Wasserstein GAN. We further apply the convex duality framework to explain why regularizing the discriminator's Lipschitz constant, e.g. via spectral normalization or gradient penalty, can greatly improve the training performance in a general f-GAN problem including the vanilla GAN formulation. We prove that Lipschitz regularization can be interpreted as convolving the original divergence score with the first-order Wasserstein distance, which results in a continuously-behaving target divergence measure. We numerically explore the power of Lipschitz regularization for improving the continuity behavior and training performance in GAN problems.	[Farnia, Farzan; Tse, David] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University	Farnia, F (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	farnia@stanford.edu; dntse@stanford.edu		Farnia, Farzan/0000-0002-6049-9232	Stanford Graduate Fellowship; National Science Foundation [CCF-1563098]; Center for Science of Information (CSoI), an NSF Science and Technology Center [CCF-0939370]	Stanford Graduate Fellowship(Stanford University); National Science Foundation(National Science Foundation (NSF)); Center for Science of Information (CSoI), an NSF Science and Technology Center	We are grateful for support under a Stanford Graduate Fellowship, the National Science Foundation grant under CCF-1563098, and the Center for Science of Information (CSoI), an NSF Science and Technology Center under grant agreement CCF-0939370.	Altun Y, 2006, LECT NOTES ARTIF INT, V4005, P139, DOI 10.1007/11776420_13; Arjovsky M., 2017, PRINCIPLED METHODS T; Arora S., 2017, ARXIV170608224; Ba J., 2017, P 3 INT C LEARN REPR; Bottou L., 2017, ARXIV170107875; Boyd S, 2004, CONVEX OPTIMIZATION; Daskalakis Constantinos, 2017, ARXIV171100141; Dudik M, 2007, J MACH LEARN RES, V8, P1217; Dziugaite G.K., 2015, ARXIV150503906; Fathony R., 2017, ADV NEURAL INFORM PR, P563; Fathony Rizal, 2016, ADV NEURAL INFORM PR, P559; Feizi S., 2017, ARXIV PREPRINT ARXIV; Goodfellow I., 2016, ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Li C.-L., 2017, ADV NEURAL INFORM PR, P2200; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liang Y., 2017, ARXIV170300573; Liu S., 2017, ADV NEURAL INFORM PR, V30, P5551; Liu Ziwei, 2015, P INT C COMP VIS ICC; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Nagarajan V, 2017, ADV NEUR IN, V30; Nock R., 2017, ADV NEURAL INFORM PR, P456; Radford A., 2015, P COMP C; Razaviyayn M., 2015, P ADV NEURAL INF PRO, P3276; Roth K., 2017, ADV NEURAL INFORM PR, P2015; Sanjabi M., 2018, ARXIV180208249; Santurkar Shibani, 2017, ARXIV171100970; Sinha A., 2018, ICLR; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Xu C., 2018, INT C LEARN REPR; Yu F., 2015, ARXIVABS150603365 CO; Zhang Pengchuan, 2018, P INT C LEARN REPR; Zhao Junbo, 2016, P INT C LEARN REPR T, DOI DOI 10.48550/ARXIV.1609.03126; Zhao S., 2018, ARXIV180606514	41	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305028
C	Feng, ZL; Wang, XC; Ke, CL; Zeng, AX; Tao, DC; Song, ML		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Feng, Zunlei; Wang, Xinchao; Ke, Chenglong; Zeng, Anxiang; Tao, Dacheng; Song, Mingli			Dual Swap Disentangling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning interpretable disentangled representations is a crucial yet challenging task. In this paper, we propose a weakly semi-supervised method, termed as Dual Swap Disentangling (DSD), for disentangling using both labeled and unlabeled data. Unlike conventional weakly supervised methods that rely on full annotations on the group of samples, we require only limited annotations on paired samples that indicate their shared attribute like the color. Our model takes the form of a dual autoencoder structure. To achieve disentangling using the labeled pairs, we follow a "encoding-swap-decoding" process, where we first swap the parts of their encodings corresponding to the shared attribute, and then decode the obtained hybrid codes to reconstruct the original input pairs. For unlabeled pairs, we follow the "encoding-swap-decoding" process twice on designated encoding parts and enforce the final outputs to approximate the input pairs. By isolating parts of the encoding and swapping them back and forth, we impose the dimension-wise modularity and portability of the encodings of the unlabeled samples, which implicitly encourages disentangling under the guidance of labeled pairs. This dual swap mechanism, tailored for semi-supervised setting, turns out to be very effective. Experiments on image datasets from a wide domain show that our model yields state-of-the-art disentangling performances.	[Feng, Zunlei; Ke, Chenglong; Song, Mingli] Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China; [Wang, Xinchao] Stevens Inst Technol, Hoboken, NJ 07030 USA; [Zeng, Anxiang] Alibaba Grp, Hangzhou, Zhejiang, Peoples R China; [Tao, Dacheng] Univ Sydney, Sydney, NSW, Australia	Zhejiang University; Stevens Institute of Technology; Alibaba Group; University of Sydney	Song, ML (corresponding author), Zhejiang Univ, Hangzhou, Zhejiang, Peoples R China.	zunleifeng@zju.edu.cn; xinchao.wang@stevens.edu; chenglongke@zju.edu.cn; renzhong@taobao.com; dctao@sydney.edu.au; brooksong@zju.edu.cn	Wang, Xinchao/L-7655-2018	Wang, Xinchao/0000-0003-0057-1404	Natonal Basic Research Program of China [2015CB352400]; National Natural Science Foundation of China [61572428, U1509206]; Fundamental Research Funds for the Central Universities [2017FZA5014]; Key Research and Development Program of Zhejiang Province [2018C01004]; Australian Research Council [FL-170100117, DP-140102164]	Natonal Basic Research Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); Key Research and Development Program of Zhejiang Province; Australian Research Council(Australian Research Council)	This work is supported by Natonal Basic Research Program of China under Grant No. 2015CB352400, National Natural Science Foundation of China (61572428,U1509206), Fundamental Research Funds for the Central Universities (2017FZA5014), Key Research and Development Program of Zhejiang Province (2018C01004), and Australian Research Council Projects (FL-170100117, DP-140102164).	Banijamali Ershad, 2017, JADE JOINT AUTOENCOD; Bouchacourt D., 2017, MULTILEVEL VARIATION; Burgess  Christopher, 2017, NIPS 2017 DIS WORKSH; Chen T.Q., 2018, NIPS, DOI 10.1007/978-3-030-04167-0; Chen X., 2016, INFOGAN INTERPRETABL; Dupont Emilien, 2018, ARXIV180400104; Eastwood Cian, 2018, INT C LEARN REPR; Feng Zunlei, 2018, INTERPRETABLE PARTIT; Gao Shuyang, 2018, ARXIV180205822; Gao W, 2008, IEEE T SYST MAN CY A, V38, P149, DOI 10.1109/TSMCA.2007.909557; Gershman S.J., 2017, BEHAV BRAIN SCI, V40; Gulrajani I., 2017, NEURALPS, P5769, DOI 10.5555/3295222.3295327; Haykin S., 2009, GRADIENTBASED LEARNI, P306; Higgins I., 2017, ARXIV170708475; Higgins I., 2016, INT C LEARNING REPRE; Higgins Irina, 2017, SCAN LEARNING ABSTRA; Kim H, 2018, ARXIV180205983; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kulkarni T.D., 2015, DEEP CONVOLUTIONAL I, V71, P2539; Moreno P, 2016, LECT NOTES COMPUT SC, V9915, P170, DOI 10.1007/978-3-319-49409-8_16; Perarnau G., 2016, NIPS WORKSH ADV TRAI; Shen XY, 2016, COMPUT GRAPH FORUM, V35, P93, DOI 10.1111/cgf.12814; Siddharth N., 2017, LEARNING DISENTANGLE; Wang CY, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2901; Xia Y, 2016, DUAL LEARNING MACHIN; Xiao T., 2017, DNA GAN LEARNING DIS; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	28	9	9	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000040
C	Foster, DJ; Sekhari, A; Sridharan, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Foster, Dylan J.; Sekhari, Ayush; Sridharan, Karthik			Uniform Convergence of Gradients for Non-Convex Learning and Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We investigate 1) the rate at which refined properties of the empirical risk-in particular, gradients-converge to their population counterparts in standard non-convex learning tasks, and 2) the consequences of this convergence for optimization. Our analysis follows the tradition of norm-based capacity control. We propose vector-valued Rademacher complexities as a simple, composable, and user-friendly tool to derive dimension-free uniform convergence bounds for gradients in non-convex learning problems. As an application of our techniques, we give a new analysis of batch gradient descent methods for non-convex generalized linear models and non-convex robust regression, showing how to use any algorithm that finds approximate stationary points to obtain optimal sample complexity, even when dimension is high or possibly infinite and multiple passes over the dataset are allowed. Moving to non-smooth models we show--in contrast to the smooth case-that even for a single ReLU it is not possible to obtain dimension-independent convergence rates for gradients in the worst case. On the positive side, it is still possible to obtain dimension-independent rates under a new type of distributional assumption.	[Foster, Dylan J.; Sekhari, Ayush; Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Foster, DJ (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	djfoster@cornell.edu; sekhari@cs.cornell.edu; sridharan@cs.cornell.edu			NSF [CDSE-MSS 1521544]; Alfred P. Sloan Fellowship; NDSEG PhD fellowship; Facebook PhD fellowship; NSF CAREER [1750575]	NSF(National Science Foundation (NSF)); Alfred P. Sloan Fellowship(Alfred P. Sloan Foundation); NDSEG PhD fellowship; Facebook PhD fellowship(Facebook Inc); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	K.S acknowledges support from the NSF under grants CDS&E-MSS 1521544 and NSF CAREER Award 1750575, and the support of an Alfred P. Sloan Fellowship. D.F. acknowledges support from the NDSEG PhD fellowship and Facebook PhD fellowship.	Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; [Anonymous], 2018, APPL COMPUTATIONAL H; Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Borwein J., 2010, CONVEX ANAL NONLINEA; Chi Jin, 2018, ADV NEURAL INFORM PR; Clarke Frank H., 1990, OPTIMIZATION NONSMOO, V5; Davis D, 2018, ARXIV181007590; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Golowich Noah, 2018, C LEARN THEOR; Gonen Alon, 2017, C LEARN THEOR; Hardt Moritz, 2018, J MACHINE LEARNING R; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Jain P, 2017, FOUND TRENDS MACH LE, V10, P142, DOI 10.1561/2200000058; Kakade S., 2011, ADV NEURAL INFORM PR; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Lei LH, 2017, ADV NEUR IN, V30; Li Y., 2017, ADV NEURAL INFORM PR, P597; Liu HK, 2016, PR MACH LEARN RES, V48; Ma T, 2017, P 5 INT C LEARN REPR; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; Mohri M., 2018, FDN MACHINE LEARNING; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; PISIER G, 1975, ISRAEL J MATH, V20, P326, DOI 10.1007/BF02760337; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Reddi SJ, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Song Mei, 2016, ANN STAT; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Tibshirani R., 2015, STAT LEARNING SPARSI; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Zeng HQ, 2017, PROC INT CONF RECON; Zeyuan Allen-Zhu, 2018, ADV NEURAL INFORM PR; Zhang Huishuai, 2016, INT C MACH LEARN; Zhou Dongruo, 2018, ADV NEURAL INFORM PR	43	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003031
C	Hanzely, F; Mishchenko, K; Richtarik, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hanzely, Filip; Mishchenko, Konstantin; Richtarik, Peter			SEGA: Variance Reduction via Gradient Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COORDINATE DESCENT; OPTIMIZATION	We propose a randomized first order optimization method-SEGA (SkEtched GrAdient)-which progressively throughout its iterations builds a variance-reduced estimate of the gradient from random linear measurements (sketches) of the gradient. In each iteration, SEGA updates the current estimate of the gradient through a sketch-and-project operation using the information provided by the latest sketch, and this is subsequently used to compute an unbiased estimate of the true gradient through a random relaxation procedure. This unbiased estimate is then used to perform a gradient step. Unlike standard subspace descent methods, such as coordinate descent, SEGA can be used for optimization problems with a non-separable proximal term. We provide a general convergence analysis and prove linear convergence for strongly convex objectives. In the special case of coordinate sketches, SEGA can be enhanced with various techniques such as importance sampling, minibatching and acceleration, and its rate is up to a small constant factor identical to the best-known rate of coordinate descent.	[Hanzely, Filip; Mishchenko, Konstantin; Richtarik, Peter] King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia; [Richtarik, Peter] Univ Edinburgh, Edinburgh, Midlothian, Scotland; [Richtarik, Peter] Moscow Inst Phys & Technol, Dolgoprudnyi, Russia	King Abdullah University of Science & Technology; University of Edinburgh; Moscow Institute of Physics & Technology	Hanzely, F (corresponding author), King Abdullah Univ Sci & Technol, Thuwal, Saudi Arabia.		Richtarik, Peter/O-5797-2018; Mishchenko, Konstantin/AAZ-1982-2020	Mishchenko, Konstantin/0000-0002-5241-7292; Hanzely, Filip/0000-0003-0203-4004				Allen-Zhu Z., 2017, INNOVATIONS THEORETI; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bergou El Houcine, 2018, RANDOM DIRECT UNPUB; Chambolle Antonin, 2018, SIAM J OPTIMIZ, V28; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Conn A. R., 2009, MPS SIAM SERIES OPTI; D'Aspremont A, 2008, SIAM J OPTIMIZ, V19, P1171, DOI 10.1137/060676386; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Gower R., 2016, ARXIV161206255; Gower R.M., 2015, ARXIV151206890; Gower RM, 2016, PR MACH LEARN RES, V48; Gower RM, 2017, SIAM J MATRIX ANAL A, V38, P1380, DOI 10.1137/16M1062053; Gower RM, 2015, SIAM J MATRIX ANAL A, V36, P1660, DOI 10.1137/15M1025487; Gower Robert M, 2018, ARXIV180502632; Gower Robert M, 2018, ARXIV180204079; Hanzely F., 2018, ARXIV180909354; HOOKE R, 1961, J ACM, V8, P212, DOI 10.1145/321062.321069; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Konecny Jakub, 2014, ARXIV14100390; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Loizou N., 2017, ARXIV171209677; Loizou Nicolas, 2017, NIPS WORKSH OPT MACH; NECOARA I, 2018, ARXIV180104873; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2004, APPL OPTIM; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nguyen LM, 2017, PR MACH LEARN RES, V70; QU Z, 2015, ADV NEURAL INFORM PR, P865; Qu Z, 2016, PR MACH LEARN RES, V48; Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360; Qu Z, 2016, OPTIM METHOD SOFTW, V31, P858, DOI 10.1080/10556788.2016.1190361; RICHTARIK P, 2017, ARXIV170601108; Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Schmidt M., 2011, ADV NEURAL INFORM PR, P1458; Shalev-Shwartz S., 2012, ARXIV12112717; Tu S, 2017, PR MACH LEARN RES, V70; Zhang, 2013, ADV NEURAL INFORM PR, P315	46	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302012
C	Hazan, E; Lee, H; Singh, K; Zhang, C; Zhang, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hazan, Elad; Lee, Holden; Singh, Karan; Zhang, Cyril; Zhang, Yi			Spectral Filtering for General Linear Dynamical Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We give a polynomial-time algorithm for learning latent-state linear dynamical systems without system identification, and without assumptions on the spectral radius of the system's transition matrix. The algorithm extends the recently introduced technique of spectral filtering, previously applied only to systems with a symmetric transition matrix, using a novel convex relaxation to allow for the efficient identification of phases.	[Hazan, Elad; Lee, Holden; Singh, Karan; Zhang, Cyril; Zhang, Yi] Princeton Univ, Princeton, NJ 08544 USA; [Hazan, Elad; Singh, Karan; Zhang, Cyril; Zhang, Yi] Google AI Princeton, Princeton, NJ USA	Princeton University	Hazan, E (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.; Hazan, E (corresponding author), Google AI Princeton, Princeton, NJ USA.	ehazan@cs.princeton.edu; holdenl@princeton.edu; karans@cs.princeton.edu; cyril.zhang@cs.princeton.edu; y.zhang@cs.princeton.edu	Singh, Karan/AAR-1348-2020	Singh, Karan/0000-0002-6992-1655; Hazan, Elad/0000-0002-1566-3216				Abbasi-Yadkori Y., 2011, P 24 ANN C LEARNING, P1; Anava O., 2013, P 26 ANN C LEARNING, P172; [Anonymous], 1996, ROBUST OPTIMAL CONTR; Beckermann B, 2017, SIAM J MATRIX ANAL A, V38, P1227, DOI 10.1137/16M1096426; Belanger D, 2015, PR MACH LEARN RES, V37, P833; Boley DL, 1998, LINEAR ALGEBRA APPL, V284, P41, DOI 10.1016/S0024-3795(98)10101-5; Box G.E.P., 1994, J TIME, DOI [10.1002/9781118619193, DOI 10.1002/9781118619193]; BROCKWELL P. J., 2009, TIME SERIES THEORY M; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; CHOI MD, 1983, AM MATH MON, V90, P301, DOI 10.2307/2975779; Dean S., 2017, ARXIV171001688; Hamilton J.D., 1994, TIME SERIES ANAL, DOI 10.2307/j.ctv14jx6sm; Hardt M., 2016, ARXIV160905191; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hazan E., 2017, P ADV NEUR INF PROC, P6705; Hilbert D, 1894, ACTA MATH, V18, P155, DOI DOI 10.1007/BF02418278; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kuznetsov Vitaly, 2016, P MACHINE LEARNING R, P1190; Kuznetsov Vitaly, 2017, ADV NEURAL INFORM PR, V30, P5671; Ljung L., 1998, SYSTEM IDENTIFICATIO; Moon T, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P1126, DOI 10.1109/ISIT.2007.4557121; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Van Overschee P., 2012, THEORY IMPLEMENTATIO	25	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304063
C	He, XX; Zhou, ZM; Thiele, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		He, Xiaoxi; Zhou, Zimu; Thiele, Lothar			Multi-Task Zipping via Layer-wise Neuron Sharing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Future mobile devices are anticipated to perceive, understand and react to the world on their own by running multiple correlated deep neural networks on-device. Yet the complexity of these neural networks needs to be trimmed down both within-model and cross-model to fit in mobile storage and memory. Previous studies squeeze the redundancy within a single model. In this work, we aim to reduce the redundancy across multiple models. We propose Multi-Task Zipping (MTZ), a framework to automatically merge correlated, pre-trained deep neural networks for cross-model compression. Central in MTZ is a layer-wise neuron sharing and incoming weight updating scheme that induces a minimal change in the error function. MTZ inherits information from each model and demands light retraining to re-boost the accuracy of individual tasks. Evaluations show that MTZ is able to fully merge the hidden layers of two VGG-16 networks with a 3.18% increase in the test error averaged on ImageNet and CelebA, or share 39.61% parameters between the two networks with < 0.5% increase in the test errors for both tasks. The number of iterations to retrain the combined network is at least 17.8x lower than that of training a single VGG-16 network. Moreover, experiments show that MTZ is also able to effectively merge multiple residual networks.	[He, Xiaoxi; Zhou, Zimu; Thiele, Lothar] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Zhou, ZM (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	hex@ethz.ch; zzhou@tik.ee.ethz.ch; thiele@ethz.ch						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Denil M., 2013, ADV NEURAL INFORM PR, P2148, DOI DOI 10.5555/2999792.2999852; Dong X, 2017, ADV NEUR IN, V30; Georgiev Petko, 2017, P ACM INT MOB WEAR U, V1; Guo YW, 2016, ADV NEUR IN, V29; Han S., 2016, P 4 INT C LEARN REPR, P1; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hu H., 2016, ARXIV PREPRINT ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu Yongxi, 2017, P IEEE C COMP VIS PA, P5334; Mathur A, 2017, MOBISYS'17: PROCEEDINGS OF THE 15TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE SYSTEMS, APPLICATIONS, AND SERVICES, P68, DOI 10.1145/3081333.3081359; Misra I, 2016, PROC CVPR IEEE, P3994, DOI 10.1109/CVPR.2016.433; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Romero Adriana, 2014, ARXIV14126550; Rothe R, 2018, INT J COMPUT VISION, V126, P144, DOI 10.1007/s11263-016-0940-3; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Soomro K., 2012, ARXIV; Stallkamp J, 2012, NEURAL NETWORKS, V32, P323, DOI 10.1016/j.neunet.2012.02.016; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Yang Yongxin, 2016, P INT C LEARN REPR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang Yu, 2017, ARXIV170708114, DOI DOI 10.1109/TKDE.2021.3070203	31	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000051
C	Ishida, T; Niu, G; Sugiyama, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ishida, Takashi; Niu, Gang; Sugiyama, Masashi			Binary Classification from Positive-Confidence Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SUPPORT	Can we learn a binary classifier from only positive data, without any negative data or unlabeled data? We show that if one can equip positive data with confidence (positive-confidence), one can successfully learn a binary classifier, which we name positive-confidence (Pconf) classification. Our work is related to one-class classification which is aimed at "describing" the positive class by clustering-related methods, but one-class classification does not have the ability to tune hyper-parameters and their aim is not on "discriminating" positive and negative classes. For the Pconf classification problem, we provide a simple empirical risk minimization framework that is model-independent and optimization-independent. We theoretically establish the consistency and an estimation error bound, and demonstrate the usefulness of the proposed method for training deep neural networks through experiments.	[Ishida, Takashi; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan; [Ishida, Takashi; Niu, Gang; Sugiyama, Masashi] RIKEN, Tokyo, Japan	University of Tokyo; RIKEN	Ishida, T (corresponding author), Univ Tokyo, Tokyo, Japan.; Ishida, T (corresponding author), RIKEN, Tokyo, Japan.	ishida@msk.u-tokyo.ac.jp; gang.niu@riken.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	Sumitomo Mitsui Asset Management; JST CREST [JPMJCR1403]	Sumitomo Mitsui Asset Management; JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	TI was supported by Sumitomo Mitsui Asset Management. MS was supported by JST CREST JPMJCR1403. We thank Ikko Yamane and Tomoya Sakai for the helpful discussions. We also thank anonymous reviewers for pointing out numerical issues in our experiments, and for pointing out the necessary condition in Theorem 1 in our earlier work of this paper.	Bao H, 2018, PR MACH LEARN RES, V80; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Breunig M. M., 2000, ACM SIGMOD; Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; du Plessis M. C., 2013, TAAI; du Plessis MC, 2017, MACH LEARN, V106, P463, DOI 10.1007/s10994-016-5604-6; du Plessis MC, 2014, ADV NEUR IN, V27; du Plessis MC, 2015, PR MACH LEARN RES, V37, P1386; Du Plessis MC, 2014, IEICE T INF SYST, VE97D, P1358, DOI 10.1587/transinf.E97.D.1358; Elkan Charles, 2008, P 14 ACM SIGKDD INT, P213, DOI DOI 10.1145/1401890.1401920; Fishman, 1996, MONTE CARLO CONCEPTS; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hastie T, 2009, ELEMENTS STAT LEARNI; Hido S, 2008, IEEE DATA MINING, P223, DOI 10.1109/ICDM.2008.49; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Ishida  T., 2018, ARXIV181004327; Ishida T., 2017, NIPS; Johansson F., 2013, MPMATH PYTHON LIB AR; Khan S. S., 2009, IR C ART INT COGN SC; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kiryo R, 2017, ADV NEUR IN, V30; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Lu  N., 2018, ARXIV180810585V2; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Mendelson S, 2008, IEEE T INFORM THEORY, V54, P3797, DOI 10.1109/TIT.2008.926323; Menon AK, 2015, PR MACH LEARN RES, V37, P125; Miyato T., 2016, ICLR; Mohri M., 2018, FDN MACHINE LEARNING; Nair V, 2010, P 27 INT C MACHINE L, P807; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Niu  G., 2016, NIPS; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Oliver A, 2018, ADV NEUR IN, V31; Paszke A., 2017, AUT WORKSH NIPS; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Quadrianto  N., 2008, ICML; Sakai T, 2017, PR MACH LEARN RES, V70; Sakai T, 2018, MACH LEARN, V107, P767, DOI 10.1007/s10994-017-5678-9; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; Scholkopf B., 2001, LEARNING KERNELS SUP; Scott  C., 2009, AISTATS; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Smola  A., 2009, AISTATS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sugiyama M, 2014, NEURAL COMPUT, V26, P84, DOI 10.1162/NECO_a_00534; Tax  D., 1999, PATTERN RECOGNITION; Tax DMJ, 2004, MACH LEARN, V54, P45, DOI 10.1023/B:MACH.0000008084.60811.49; Vapnik V., 1998, STAT LEARNING THEORY; Yang Z, 2016, PR MACH LEARN RES, V48; Yu FX, 2013, ICML; Yu X, 2018, LECT NOTES COMPUT SC, V11213, P219, DOI 10.1007/978-3-030-01240-3_14	55	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000042
C	Josz, C; Ouyang, Y; Zhang, RY; Lavaei, J; Sojoudi, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Josz, C.; Ouyang, Y.; Zhang, R. Y.; Lavaei, J.; Sojoudi, S.			A theory on the absence of spurious solutions for nonconvex and nonsmooth optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We study the set of continuous functions that admit no spurious local optima (i.e. local minima that are not global minima) which we term global functions. They satisfy various powerful properties for analyzing nonconvex and nonsmooth optimization problems. For instance, they satisfy a theorem akin to the fundamental uniform limit theorem in the analysis regarding continuous functions. Global functions are also endowed with useful properties regarding the composition of functions and change of variables. Using these new results, we show that a class of nonconvex and nonsmooth optimization problems arising in tensor decomposition applications are global functions. This is the first result concerning nonconvex methods for nonsmooth objective functions. Our result provides a theoretical guarantee for the widely-used l(1) norm to avoid outliers in nonconvex optimization.	[Josz, C.; Sojoudi, S.] Univ Calif Berkeley, Berkeley, CA USA; [Ouyang, Y.; Zhang, R. Y.; Lavaei, J.] Univ Calif Berkeley, IEOR, Berkeley, CA USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	Josz, C (corresponding author), Univ Calif Berkeley, Berkeley, CA USA.	cedric.josz@gmail.com; ouyangyii@gmail.com; ryz@berkeley.edu; lavaei@berkeley.edu; sojoudi@berkeley.edu			ONR [N00014-17-1-2933 ONR, N00014-18-1-2526]; NSF Award [1808859]; DARPA [D16AP00002]; AFOSR Award [FA9550- 17-1-0163]	ONR(Office of Naval Research); NSF Award(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); AFOSR Award(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported by the ONR Awards N00014-17-1-2933 ONR and N00014-18-1-2526, NSF Award 1808859, DARPA Award D16AP00002, and AFOSR Award FA9550- 17-1-0163. We wish to thank the anonymous reviewers for their valuable feedback, as well as Chris Dock for fruitful discussions.		0	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302045
C	Kakade, SM; Lee, JD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kakade, Sham M.; Lee, Jason D.			Provably Correct Automatic Subdifferentiation for Qualified Programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DIFFERENTIATION	The Cheap Gradient Principle [Griewank and Walther, 2008] - the computational cost of computing the gradient of a scalar-valued function is nearly the same (often within a factor of 5) as that of simply computing the function itself - is of central importance in optimization; it allows us to quickly obtain (high dimensional) gradients of scalar loss functions which are subsequently used in black box gradient-based optimization procedures. The current state of affairs is markedly different with regards to computing subderivatives: widely used ML libraries, including TensorFlow and PyTorch, do not correctly compute (generalized) subderivatives even on simple examples. This work considers the question: is there a Cheap Sub gradient Principle? Our main result shows that, under certain restrictions on our library of nonsmooth functions (standard in nonlinear programming), provably correct generalized subderivatives can be computed at a computational cost that is within a (dimension-free) factor of 6 of the cost of computing the scalar function itself.	[Kakade, Sham M.] Univ Washington, Seattle, WA 98195 USA; [Lee, Jason D.] Univ Southern Calif, Los Angeles, CA 90089 USA	University of Washington; University of Washington Seattle; University of Southern California	Kakade, SM (corresponding author), Univ Washington, Seattle, WA 98195 USA.	sham@cs.washington.edu; jasonlee@marshall.usc.edu		Lee, Jason/0000-0003-0064-7800	Washington Research Foundation Fund for Innovation in Data-Intensive Discovery; NSF [CCF-1740551]; ONR [N00014-18-1-2247]; ARO under MURI [W911NF-11-1-0303]	Washington Research Foundation Fund for Innovation in Data-Intensive Discovery; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO under MURI(MURI)	We thank Dima Drusvyatskiy for many helpful discussions. Sham Kakade acknowledges funding from Washington Research Foundation Fund for Innovation in Data-Intensive Discovery, the NSF through award CCF-1740551, and ONR award N00014-18-1-2247. Jason D. Lee acknowledges support of the ARO under MURI Award W911NF-11-1-0303. This is part of the collaboration between US DOD, UK MOD and UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative.	Abadi M, 2015, P 12 USENIX S OPERAT; Abadie J., 1967, NONLINEAR PROGRAMMIN, P21; Bau III D, 1997, NUMERICAL LINEAR ALG; BAUR W, 1983, THEOR COMPUT SCI, V22, P317, DOI 10.1016/0304-3975(83)90110-X; Baydin Atilim Gunes, 2015, ABS150205767 CORR; Blum L., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P387, DOI 10.1109/SFCS.1988.21955; CLARKE FH, 1975, T AM MATH SOC, V205, P247, DOI 10.1090/s0002-9947-1975-0367131-6; Clarke FH., 2008, NONSMOOTH ANAL CONTR, V178; Coste M., 2000, INTRO O MINIMAL GEOM; Demmel JW, 1997, APPL NUMERICAL LINEA, V56; Fiege Sabrina, 2017, ALGORITHMIC DIFFEREN, P1; Gould F. J., 1971, NECESSARY SUFFICIENT, V20; Griewank A, 1995, LECT NOTES ECON MATH, V429, P155; Griewank A., 1989, MATH PROGRAMMING REC, V6, P83; GRIEWANK A, 2012, DOCUMENTA MATH, P389; Griewank Andreas, 2014, Pesqui. Oper., V34, P621, DOI 10.1590/0101-7438.2014.034.03.0621; Griewank A, 2013, OPTIM METHOD SOFTW, V28, P1139, DOI 10.1080/10556788.2013.796683; Khan KA, 2015, OPTIM METHOD SOFTW, V30, P1185, DOI 10.1080/10556788.2015.1025400; Khan KA, 2013, ACM T MATH SOFTWARE, V39, DOI 10.1145/2491491.2491493; Khan Kamil A., 2017, OPTIMIZATION METHODS, P1; KLATTE D, 2002, NONCON OPTIM ITS APP, V60, pR11; Maclaurin Dougal, 2015, AUTOGRAD REVERSEMODE; Mordukhovich BS., 2006, VARIATIONAL ANAL GEN, DOI 10.1007/3-540-31246-3; Morgenstern Jacques, 1985, SIGA; Nesterov Y, 2005, MATH PROGRAM, V104, P669, DOI 10.1007/s10107-005-0633-0; Paszke A., 2017, AUTOMATIC DIFFERENTI; PETERSON DW, 1973, SIAM REV, V15, P639, DOI 10.1137/1015075; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Seeger Matthias W., 2017, ABS171008717 CORR; SHAPIRO A, 1990, J OPTIMIZ THEORY APP, V66, P477, DOI 10.1007/BF00940933	31	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001065
C	Moreno-Munoz, P; Artes-Rodriguez, A; Alvarez, MA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Moreno-Munoz, Pablo; Artes-Rodriguez, Antonio; Alvarez, Mauricio A.			Heterogeneous Multi-output Gaussian Process Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS	We present a novel extension of multi-output Gaussian processes for handling heterogeneous outputs. We assume that each output has its own likelihood function and use a vector-valued Gaussian process prior to jointly model the parameters in all likelihoods as latent functions. Our multi-output Gaussian process uses a covariance function with a linear model of coregionalisation form. Assuming conditional independence across the underlying latent functions together with an inducing variable framework, we are able to obtain tractable variational bounds amenable to stochastic variational inference. We illustrate the performance of the model on synthetic data and two real datasets: a human behavioral study and a demographic high-dimensional dataset.	[Moreno-Munoz, Pablo; Artes-Rodriguez, Antonio] Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid, Spain; [Alvarez, Mauricio A.] Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England	Universidad Carlos III de Madrid; University of Sheffield	Moreno-Munoz, P (corresponding author), Univ Carlos III Madrid, Dept Signal Theory & Commun, Madrid, Spain.	pmoreno@tsc.uc3m.es; antonio@tsc.uc3m.es; mauricio.alvarez@sheffield.ac.uk	Artes, Antonio/E-4842-2018	Artes, Antonio/0000-0001-6540-7109; Alvarez, Mauricio A./0000-0002-8980-4472	FPI [BES2016-077626]; Ministerio de Economia of Spain under the project Macro-ADOBE [TEC2015-67719-P]; project ADVENTURE [TEC2015-69868-C2-1-R]; project AID [TEC2014-62194-EXP]; project CASI-CAM-CM [S2013/ICE-2845]; Engineering and Physical Research Council (EPSRC) [EP/N014162/1, EP/R034303/1]	FPI(Spanish Government); Ministerio de Economia of Spain under the project Macro-ADOBE; project ADVENTURE; project AID; project CASI-CAM-CM; Engineering and Physical Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors want to thank Wil Ward for his constructive comments and Juan Jose Giraldo for his useful advice about SVI experiments and simulations. We also thank Alan Saul and David Ramirez for their recommendations about scalable inference and feedback on the equations. We are grateful to Eero Siivola and Marcelo Hartmann for sharing their Python module for heterogeneous likelihoods and to Francisco J. R. Ruiz for his illuminating help about the stochastic version of the VEM algorithm. Also, we would like to thank Juan Jose Campafia for his assistance on the London House Price dataset. Pablo Moreno-Munoz acknowledges the support of his doctoral FPI grant BES2016-077626 and was also supported by Ministerio de Economia of Spain under the project Macro-ADOBE (TEC2015-67719-P), Antonio Artes-Rodriguez acknowledges the support of projects ADVENTURE (TEC2015-69868-C2-1-R), AID (TEC2014-62194-EXP) and CASI-CAM-CM (S2013/ICE-2845). Mauricio A. Alvarez has been partially financed by the Engineering and Physical Research Council (EPSRC) Research Projects EP/N014162/1 and EP/R034303/1.	Alvarez M. A., 2009, ADV NEURAL INFORM PR, P57; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; [Anonymous], [No title captured]; Beal M.J, 2003, THESIS; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Chai KMA, 2012, J MACH LEARN RES, V13, P1745; Dai Z., 2017, ADV NEURAL INFORM PR, P5131; Dezfouli A., 2015, ADV NEURAL INFORM PR, P1414; Hadfield JD, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i02; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Higdon D, 2002, QUANTITATIVE METHODS, P37, DOI DOI 10.1007/978-1-4471-0657-9_2; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Journel A.G., 1978, MINING GEOSTATISTICS; Kim H, 2017, IEEE INT WORK SIGN P; Lazaro-Gredilla M., 2011, P INT C MACH LEARN M, P841; Li SJ, 2014, IEEE COMPUT SOC CONF, P488, DOI 10.1109/CVPRW.2014.78; Ngufor C, 2015, LECT NOTES ARTIF INT, V9105, P287, DOI 10.1007/978-3-319-19551-3_37; Nguyen T. V., 2014, UAI; Nguyen Trung V., 2014, NIPS; Parra G., 2017, NIPS 30; Pourmohamad T, 2016, BAYESIAN ANAL, V11, P797, DOI 10.1214/15-BA976; Saul AD, 2016, JMLR WORKSH CONF PRO, V51, P1431; Skolidis G, 2011, IEEE T NEURAL NETWOR, V22, P2011, DOI 10.1109/TNN.2011.2168568; Soleimani H, 2018, IEEE T PATTERN ANAL, V40, P1948, DOI 10.1109/TPAMI.2017.2742504; Teh Y.W., 2005, WORKSH ART INT STAT, V2005, P333; Ulrich K. R., 2015, NIPS 28; Valera I., 2017, ARXIV170603779; Valera I, 2017, PR MACH LEARN RES, V70; Vanhatalo J, 2013, J MACH LEARN RES, V14, P1175; Wilson A. G., 2011, P 27 C UNC ART INT U, P736; Yang E., 2014, ARXIV14110288; Yang Xiaolin, 2009, ADV NEURAL INFORM PR, P2151; Zhang DQ, 2012, NEUROIMAGE, V59, P895, DOI 10.1016/j.neuroimage.2011.09.069	35	9	9	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001027
C	Morozov, S; Babenko, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Morozov, Stanislav; Babenko, Artem			Non-metric Similarity Graphs for Maximum Inner Product Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper we address the problem of Maximum Inner Product Search (MIPS) that is currently the computational bottleneck in a large number of machine learning applications. While being similar to the nearest neighbor search (NNS), the MIPS problem was shown to be more challenging, as the inner product is not a proper metric function. We propose to solve the MIPS problem with the usage of similarity graphs, i.e., graphs where each vertex is connected to the vertices that are the most similar in terms of some similarity function. Originally, the framework of similarity graphs was proposed for metric spaces and in this paper we naturally extend it to the non-metric MIPS scenario. We demonstrate that, unlike existing approaches, similarity graphs do not require any data transformation to reduce MIPS to the NNS problem and should be used for the original data. Moreover, we explain why such a reduction is detrimental for similarity graphs. By an extensive comparison to the existing approaches, we show that the proposed method is a game-changer in terms of the runtime/accuracy trade-off for the MIPS problem.	[Morozov, Stanislav] Lomonosov Moscow State Univ, Yandex, Moscow, Russia; [Babenko, Artem] Natl Res Univ, Higher Sch Econ, Yandex, Moscow, Russia	Lomonosov Moscow State University; HSE University (National Research University Higher School of Economics)	Morozov, S (corresponding author), Lomonosov Moscow State Univ, Yandex, Moscow, Russia.	stanis-morozov@yandex.ru; artem.babenko@phystech.edu	Morozov, Stanislav/AAO-3283-2020					[Anonymous], CORR; Bachrach Yoram, 2014, SPEEDING XBOX RECOMM; Boissonnat J.D., 1998, ALGORITHMIC GEOMETRY; Boytsov Leonid, 2016, CIKM; Boytsov Leonid, 2017, TECHNICAL REPORT; Chandar, 2016, ARXIV PREPRINT ARXIV; Dong Wei, 2011, P 20 INT C WORLD WID, P577, DOI DOI 10.1145/1963405.1963487; Fu Cong, 2016, CORR; Harwood B, 2016, PROC CVPR IEEE, P5713, DOI 10.1109/CVPR.2016.616; Henderson Matthew, 2017, ABS170500652 CORR; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Jun Kwang- Sung, 2017, ADV NEURAL INFORM PR, V30, P98; Keivani O, 2017, IEEE IJCNN, P2927, DOI 10.1109/IJCNN.2017.7966218; Li Hui, 2017, SIGMOD C; Li W., 2016, CORR; Malkov Y., 2016, CORR; Mussmann S, 2016, PR MACH LEARN RES, V48; Mussmann Stephen, 2017, P 33 C UNC ART INT U; Naidan Bilegsaikhan, 2015, VLDB; Navarro G, 2002, VLDB J, V11, P28, DOI 10.1007/s007780200060; Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926; Ponomarenko Alexander, 2014, DATA ANAL, P125; Shapiro S. C., 1987, ENCY ARTIFICIAL INTE; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Teflioudi C, 2017, ACM T DATABASE SYST, V42, DOI 10.1145/2996452; Yu H., 2017, NIPS	26	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304071
C	Mujika, A; Meier, F; Steger, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mujika, Asier; Meier, Florian; Steger, Angelika			Approximating Real-Time Recurrent Learning with Random Kronecker Factors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ALGORITHM	Despite all the impressive advances of recurrent neural networks, sequential data is still in need of better modelling. Truncated backpropagation through time (TBPTT), the learning algorithm most widely used in practice, suffers from the truncation bias, which drastically limits its ability to learn long-term dependencies.The Real-Time Recurrent Learning algorithm (RTRL) addresses this issue, but its high computational requirements make it infeasible in practice. The Unbiased Online Recurrent Optimization algorithm (UORO) approximates RTRL with a smaller runtime and memory cost, but with the disadvantage of obtaining noisy gradients that also limit its practical applicability. In this paper we propose the Kronecker Factored RTRL (KF-RTRL) algorithm that uses a Kronecker product decomposition to approximate the gradients for a large class of RNNs. We show that KF-RTRL is an unbiased and memory efficient online learning algorithm. Our theoretical analysis shows that, under reasonable assumptions, the noise introduced by our algorithm is not only stable over time but also asymptotically much smaller than the one of the UORO algorithm. We also confirm these theoretical results experimentally. Further, we show empirically that the KF-RTRL algorithm captures long-term dependencies and almost matches the performance of TBPTT on real world tasks by training Recurrent Highway Networks on a synthetic string memorization task and on the Penn TreeBank task, respectively. These results indicate that RTRL based approaches might be a promising future alternative to TBPTT.	[Mujika, Asier; Meier, Florian; Steger, Angelika] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Mujika, A (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	asierm@inf.ethz.ch; meierflo@inf.ethz.ch; steger@inf.ethz.ch			Swiss National Science Foundation [CRSII5_173721]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	Author was supported by grant no. CRSII5_173721 of the Swiss National Science Foundation.	Abadi M, 2015, P 12 USENIX S OPERAT; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; CATFOLIS T, 1993, NEURAL NETWORKS, V6, P807, DOI 10.1016/S0893-6080(05)80126-1; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jaderberg M., 2016, ARXIV160805343; Jaeger H., 2001, 148 GMD GERM NAT RES, V148, P13; Kingma D.P, P 3 INT C LEARNING R; Lukosevicius M, 2009, COMPUT SCI REV, V3, P127, DOI 10.1016/j.cosrev.2009.03.005; Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Melis G., 2017, ARXIV; Merity Stephen, 2018, ARXIV180308240; Ollivier Y., 2015, TRAINING RECURRENT N; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Tallec C., 2017, ARXIV170205043; Tallec C., 2017, ARXIV170508209; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Williams RJ, 1990, NEURAL COMPUT, V2, P490, DOI 10.1162/neco.1990.2.4.490; Zilly J.G., 2016, ARXIV PREPRINT ARXIV	20	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001016
C	Mun, J; Lee, K; Shin, J; Han, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mun, Jonghwan; Lee, Kimin; Shin, Jinwoo; Han, Bohyung			Learning to Specialize with Knowledge Distillation for Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Visual Question Answering (VQA) is a notoriously challenging problem because it involves various heterogeneous tasks defined by questions within a unified framework. Learning specialized models for individual types of tasks is intuitively attracting but surprisingly difficult; it is not straightforward to outperform naive independent ensemble approach. We present a principled algorithm to learn specialized models with knowledge distillation under a multiple choice learning (MCL) framework, where training examples are assigned dynamically to a subset of models for updating network parameters. The assigned and non-assigned models are learned to predict ground-truth answers and imitate their own base models before specialization, respectively. Our approach alleviates the limitation of data deficiency in existing MCL frameworks, and allows each model to learn its own specialized expertise without forgetting general knowledge. The proposed framework is model-agnostic and applicable to any tasks other than VQA, e.g., image classification with a large number of labels but few per-class examples, which is known to be difficult under existing MCL schemes. Our experimental results indeed demonstrate that our method outperforms other baselines for VQA and image classification.	[Mun, Jonghwan] POSTECH, Comp Vis Lab, Pohang, South Korea; [Lee, Kimin; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Algorithm Intelligence Lab, Daejeon, South Korea; [Mun, Jonghwan; Han, Bohyung] Seoul Natl Univ, Comp Vis Lab, ASRI, Seoul, South Korea	Pohang University of Science & Technology (POSTECH); Korea Advanced Institute of Science & Technology (KAIST); Seoul National University (SNU)	Mun, J (corresponding author), POSTECH, Comp Vis Lab, Pohang, South Korea.; Mun, J (corresponding author), Seoul Natl Univ, Comp Vis Lab, ASRI, Seoul, South Korea.	choco1916@postech.ac.kr; kiminlee@kaist.ac.kr; jinwoos@kaist.ac.kr; bhhan@snu.ac.kr			ICT R&D program of the MSIP/IITP grant [2016-0-00563, 2017-0-01778]; Kakao corporation; Kakao Brain corporation	ICT R&D program of the MSIP/IITP grant; Kakao corporation; Kakao Brain corporation	This work was partly supported by ICT R&D program of the MSIP/IITP grant [2016-0-00563; Research on Adaptive Machine Learning Technology Development for Intelligent Autonomous Digital Companion, 2017-0-01778; Development of Explainable Human-level Deep Machine Learning Inference Framework] and Kakao and Kakao Brain corporations.	ANDERSON P, 2018, CVPR, V3, P6, DOI DOI 10.1109/CVPR.2018.00636; Andreas J., 2016, ANN C N AM CHAPT ASS; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Chen Guobin, 2017, NEURIPS; Chen Tianqi, 2016, ICLR; Ciregan Dan, 2012, CVPR; Fukui Akira, 2016, ARXIV160601847; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Guzm<prime>an-rivera Abner, 2012, ADV NEURAL INFORM PR; Han B., 2016, P CVPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hu R., 2017, P ICCV; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kazemi Vahid, 2017, ARXIV170403162; Kim J.-H., 2017, ICLR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lee K., 2017, ICML; Lee Kimin, 2018, INT C LEARN REPR; Lee S., 2016, NIPS; Li Quanquan, 2017, CVPR; Li ZZ, 2016, LECT NOTES COMPUT SC, V9908, P614, DOI 10.1007/978-3-319-46493-0_37; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Luo P, 2016, AAAI CONF ARTIF INTE, P3560; Nam H, 2017, PROC CVPR IEEE, P2156, DOI 10.1109/CVPR.2017.232; Noh Hyeonwoo, 2016, ARXIV160603647; Noroozi Mehdi, 2018, C COMP VIS PATT REC; Romero Adriana, 2015, ICLR POSTER; Shih KJ, 2016, PROC CVPR IEEE, P4613, DOI 10.1109/CVPR.2016.499; Xu H., 2016, P EUR C COMP VIS ECC; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Zagoruyko S., 2017, ICLR	36	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002061
C	Riemer, M; Liu, M; Tesauro, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Riemer, Matthew; Liu, Miao; Tesauro, Gerald			Learning Abstract Options	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Building systems that autonomously create temporal abstractions from data is a key challenge in scaling learning and planning in reinforcement learning. One popular approach for addressing this challenge is the options framework [29]. However, only recently in [1] was a policy gradient theorem derived for online learning of general purpose options in an end to end fashion. In this work, we extend previous work on this topic that only focuses on learning a two-level hierarchy including options and primitive actions to enable learning simultaneously at multiple resolutions in time. We achieve this by considering an arbitrarily deep hierarchy of options where high level temporally extended options are composed of lower level options with finer resolutions in time. We extend results from [1] and derive policy gradient theorems for a deep hierarchy of options. Our proposed hierarchical option-critic architecture is capable of learning internal policies, termination conditions, and hierarchical compositions over options without the need for any intrinsic rewards or subgoals. Our empirical results in both discrete and continuous environments demonstrate the efficiency of our framework.	[Riemer, Matthew; Liu, Miao; Tesauro, Gerald] IBM Res, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Riemer, M (corresponding author), IBM Res, TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	mdriemer@us.ibm.com; miao.liu1@us.ibm.com; gtesauro@us.ibm.com						[Anonymous], 2011, AAAI; [Anonymous], 2016, P ADV NEUR INF PROC; [Anonymous], 2001, P 18 INT C MACHINE L; Bacon P.-L., 2017, OPTION CRITIC ARCHIT; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Fox R., 2017, ARXIV170308294; Harb Jean, 2017, ARXIV170904571; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Klissarov M., 2017, ARXIV171200004; Konda VR, 2000, ADV NEUR IN, V12, P1008; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Levy A, 2017, ARXIV PREPRINT ARXIV; Levy Kfir Y, 2011, RECENT ADV REINFORCE, P153, DOI [10.1007/978-3-642-29946-9_17, DOI 10.1007/978-3-642-29946-9_17]; Mann TA, 2015, J ARTIF INTELL RES, V53, P375, DOI 10.1613/jair.4676; Menache I, 2002, LECT NOTES ARTIF INT, V2430, P295; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Niekum Scott D., 2013, SEMANTICALLY GROUNDE; Precup D., 2000, TEMPORAL ABSTRACTION; Puterman M. L., 1994, MARKOV DECISION PROC, P92; Sahni H., 2017, ARXIV171111289; Sharma S., 2017, ARXIV170206053; Shu Tianmin, 2017, ARXIV171207294; Silver David, 2012, ARXIV12066473; Simsek O., 2009, P ADV NEURAL INFORM, P1497; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Veness J., 2017, ARXIV170906009; Vezhnevets A. S., 2017, ARXIV170301161	32	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005004
C	Schlag, I; Schmidhuber, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Schlag, Imanol; Schmidhuber, Jurgen			Learning to Reason with Third-Order Tensor Products	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BACKPROPAGATION; CONNECTIONISM; SYSTEMATICITY	We combine Recurrent Neural Networks with Tensor Product Representations to learn combinatorial representations of sequential data. This improves symbolic interpretation and systematic generalisation. Our architecture is trained end-to-end through gradient descent on a variety of simple natural language reasoning tasks, significantly outperforming the latest state-of-the-art models in single-task and all-tasks settings. We also augment a subset of the data such that training and test data exhibit large systematic differences and show that our approach generalises better than the previous state-of-the-art.	[Schlag, Imanol; Schmidhuber, Jurgen] Swiss Lab IDSIA USI SUPSI, Ticino, Switzerland	Universita della Svizzera Italiana	Schlag, I (corresponding author), Swiss Lab IDSIA USI SUPSI, Ticino, Switzerland.	imanol@idsia.ch; juergen@idsia.ch			European Research Council Advanced Grant [742870]	European Research Council Advanced Grant(European Research Council (ERC))	We thank Paulo Rauber, Klaus Greff, and Filipe Mutz for helpful comments and helping hands. We are also grateful to NVIDIA Corporation for donating a DGX-1 as part of the Pioneers of AI Research Award and to IBM for donating a Minsky machine. This research was supported by an European Research Council Advanced Grant (no: 742870).	Abadi M, 2015, P 12 USENIX S OPERAT; Anandkumar A, 2015, LECT NOTES ARTIF INT, V9355, P19, DOI 10.1007/978-3-319-24486-0_2; Atzmon Y., 2016, ARXIV160807639; Ba J, 2016, ADV NEUR IN, V29; Brousse Olivier J., 1992, THESIS; Dozat T., 2016, INT C LEARN REPR ICL; Dupoux Emmanuel, 2015, DECONSTRUCTING AI CO; FELDMAN JA, 1982, BIOL CYBERN, V46, P27, DOI 10.1007/BF00335349; FODOR J, 1990, COGNITION, V35, P183, DOI 10.1016/0010-0277(90)90014-B; FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5; Gers FA, 2000, NEURAL COMPUT, V12, P2451, DOI 10.1162/089976600300015015; Glorot X., 2010, PROC MACH LEARN RES, P249; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; HADLEY RF, 1994, MIND LANG, V9, P431, DOI 10.1111/j.1468-0017.1994.tb00225.x; Hebb D., 1949, ORG BEHAV; Henaff Mikael, INT C LEARN REPR ICL; Hinton, 2016, ARXIV PREPRINT ARXIV; Hinton G.E., 1987, P 9 ANN C COGNITIVE, P177; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Huang Q., 2018, ARXIV180207089; Huang Qiuyuan, 2017, CORR; KELLEY HJ, 1960, ARSJ-AM ROCKET SOC J, V30, P947, DOI 10.2514/8.5282; Kumar Ankit, 2015, ABS150607285 CORR; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Lake Brenden M., 2017, CORR; Linnainmaa S., 1970, REPRESENTATION CUMUL; Melis G., 2017, ARXIV; Miconi T, 2018, INT C MACH LEARN PML, P3559; Mikolov T., 2015, ARXIV150205698, V1502, P05698; Palangi Hamid, 2017, CORR; Perez Julien, 2016, CORR; Phillips Steven Andrew, 1995, THESIS; Pino J. M., 2017, FACEBOOK RES BLOG; Robinson A.J, 1987, CUEDFINFENGTR1; Sak H., 2015, GOOGLE RES BLOG; Santurkar S, 2018, ADV NEUR IN, V31; Schlag Imanol, 2017, NIPS MET WORKSH; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J., 1991, FKI14791 TU MUNCH I; Schmidhuber J., 1993, P INT C ART NEUR NET, P460; SIEGELMANN HT, 1991, APPL MATH LETT, V4, P77, DOI 10.1016/0893-9659(91)90080-F; SMOLENSKY P, 1990, ARTIF INTELL, V46, P159, DOI 10.1016/0004-3702(90)90007-M; Smolensky P, 2012, PHILOS T R SOC A, V370, P3543, DOI 10.1098/rsta.2011.0334; Smolensky Paul, 2016, CORR; Sukhbaatar S, 2015, ADV NEUR IN, V28; von der Malsburg, 1981, CORRELATION THEORY B; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Weston J., 2014, CORR; Williams R. J., 1994, BACK PROPAGATION THE; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xiong C., 2016, CORR; Yang YC, 2017, PR MACH LEARN RES, V70; Yu R., 2017, LONG TERM FORECASTIN	58	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004053
C	Shah, H; Barber, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shah, Harshil; Barber, David			Generative Neural Machine Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce Generative Neural Machine Translation (GNMT), a latent variable architecture which is designed to model the semantics of the source and target sentences. We modify an encoder-decoder translation model by adding a latent variable as a language agnostic representation which is encouraged to learn the meaning of the sentence. GNMT achieves competitive BLEU scores on pure translation tasks, and is superior when there are missing words in the source sentence. We augment the model to facilitate multilingual translation and semi-supervised learning without adding parameters. This framework significantly reduces over-fitting when there is limited paired data available, and is effective for translating between pairs of languages not seen during training.	[Shah, Harshil; Barber, David] UCL, London, England; [Barber, David] Alan Turing Inst, London, England; [Barber, David] Reinfer Io, London, England	University of London; University College London	Shah, H (corresponding author), UCL, London, England.				Alan Turing Institute under the EPSRC [EP/N510129/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1.	[Anonymous], 2016, ARXIV160502688 THEAN; Bahdanau D., 2015, INT C LEARN REPR ICL; Bowman S. R., 2016, C COMP NAT LANG LEAR; Dieleman S., 2015, LASAGNE 1 RELEASE, V1; Dieng AB, 2017, INT C LEARN REPR; Johnson M., 2017, T ASS COMPUT LINGUIS, V5, P339; Kingma D., 2014, ADAM METHOD STOCHAST; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P86; Shu R., 2017, P 34 INT C MACH LEAR, P3164; Sonderby CK, 2016, ADV NEUR IN, V29; Tiedemann J., 2012, P 8 INT C LANG RES E; Tu ZP, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P76; Yang Z., 2017, INT C MACH LEARN PML, P3881; Zhang Biao, 2016, P 2016 C EMP METH NA, P521, DOI [DOI 10.18653/V1/D16-1050, 10.18653/v1/D16-1050]; Zhang J., 2016, P 2016 C EMP METH NA, P1535	17	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301034
C	Ting, D; Brochu, E		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ting, Daniel; Brochu, Eric			Optimal Subsampling with Influence Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Subsampling is a common and often effective method to deal with the computational challenges of large datasets. However, for most statistical models, there is no well-motivated approach for drawing a non-uniform subsample. We show that the concept of an asymptotically linear estimator and the associated influence function leads to asymptotically optimal sampling probabilities for a wide class of popular models. This is the only tight optimality result for subsampling we are aware of as other methods only provide probabilistic error bounds or optimal rates. We also show that these optimal weights can differ depending on whether the task is parameter estimation or prediction. Furthermore, for linear regression models, which have well-studied procedures for non-uniform subsampling, we empirically show our optimal influence function based method outperforms previous approaches even when using approximations to the optimal probabilities.	[Ting, Daniel] Tableau Software, Seattle, WA 98103 USA; [Brochu, Eric] Tableau Software, Vancouver, BC, Canada		Ting, D (corresponding author), Tableau Software, Seattle, WA 98103 USA.	dting@tableau.com; ebrochu@tableau.com						Bordes A, 2009, J MACH LEARN RES, V10, P1737; Bottou L., 2010, COMPSTAT; Clarkson K. L., 2005, SODA; Clarkson KL, 2016, SIAM J COMPUT, V45, P763, DOI 10.1137/140963698; Cohen E., 2015, HOTWEB; Cohen E., 2013, RANDOM; Dasgupta A, 2009, SIAM J COMPUT, V38, P2060, DOI 10.1137/070696507; Dhillon P., 2013, NIPS; Drineas P, 2012, J MACH LEARN RES, V13, P3475; DuMouchel W., 1999, KDD; Fithian W, 2014, ANN STAT, V42, P1693, DOI 10.1214/14-AOS1220; HAMPEL FR, 1974, J AM STAT ASSOC, V69, P383, DOI 10.2307/2285666; Huggins J., 2016, NIPS; Koenker R., 2005, QUANTILE REGRESSION, DOI DOI 10.1017/CBO9780511754098; Koh P. W., 2017, ICML; Lichman M., 2013, UCI MACHINE LEARNING; Ma P, 2015, J MACH LEARN RES, V16, P861; Madigan D, 2002, DATA MIN KNOWL DISC, V6, P173, DOI 10.1023/A:1014095614948; Meng X., 2013, STOC; Meng XR, 2014, SIAM J SCI COMPUT, V36, pC95, DOI 10.1137/120866580; Raskutti G., 2016, J MACHINE LEARNING R, V17, P7508; Ting Daniel, 2017, ARXIV170804970; Tumanov A., 2017, ABS171205889 CORR; U. S. EPA, 2015, US EPA RISK SCREEN E; Van der Vaart A. W., 2000, ASYMPTOTIC STAT; Wang HY, 2018, J AM STAT ASSOC, V113, P829, DOI 10.1080/01621459.2017.1292914; Zaharia M., 2010, HOTCLOUD USENIX WORK; Zhu R., 2016, NIPS	28	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303063
C	Tran, D; Hoffman, MD; Moore, D; Suter, C; Vasudevan, S; Radul, A; Johnson, M; Saurous, RA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tran, Dustin; Hoffman, Matthew D.; Moore, Dave; Suter, Christopher; Vasudevan, Srinivas; Radul, Alexey; Johnson, Matthew; Saurous, Rif A.			Simple, Distributed, and Accelerated Probabilistic Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We describe a simple, low-level approach for embedding probabilistic programming in a deep learning ecosystem. In particular, we distill probabilistic programming down to a single abstraction-the random variable. Our lightweight implementation in TensorFlow enables numerous applications: a model-parallel variational auto-encoder (vAE) with 2nd-generation tensor processing units (TPUv2s); a data-parallel autoregressive model (Image Transformer) with TPUv2s; and multi-GPU No-U-Turn Sampler (NUTS). For both a state-of-the-art VAE on 64x64 ImageNet and Image Transformer on 256x256 CelebA-HQ, our approach achieves an optimal linear speedup from 1 to 256 TPUv2 chips. With NUTS, we see a 100x speedup on GPUs over Stan and 37x over PyMC3.(1)	[Tran, Dustin; Johnson, Matthew] Google Brain, Mountain View, CA USA; [Hoffman, Matthew D.; Moore, Dave; Suter, Christopher; Vasudevan, Srinivas; Radul, Alexey; Saurous, Rif A.] Google, Mountain View, CA USA	Google Incorporated; Google Incorporated	Tran, D (corresponding author), Google Brain, Mountain View, CA USA.							Amos B, 2017, PR MACH LEARN RES, V70; Andrychowicz M, 2016, ADV NEUR IN, V29; Baydin Atilim Gunes, 2015, ABS150205767 CORR; Baylor D., 2017, KNOWLEDGE DISCOVERY; Bengio Emmanuel, 2015, CORR; Bingham E, 2019, J MACH LEARN RES, V20; Carpenter B., 2017, J STAT SOFTW, DOI [10.18637/jss.v076.i01, DOI 10.18637/JSS.V076.I01]; Cusumano-Towner M. F., 2018, POPL WORKSH; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ge H., 2018, ARTIFICIAL INTELLIGE; Giles C. L., 1990, NEURAL INFORM PROCES; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2014, ARXIV14105401; Graves A., 2016, ABS160308983 CORR; Hafner Danijar, 2018, RELIABLE UNCERTAINTY; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Hoffman MD, 2017, PR MACH LEARN RES, V70; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Kingma D.P, P 3 INT C LEARNING R; Kucukelbir A, 2017, J MACH LEARN RES, V18, P1; Kusner MJ, 2017, PR MACH LEARN RES, V70; Maclaurin D., 2015, AUTOGRAD REVERSE MOD; Mansinghka V., 2014, ARXIV14040099; Papamakarios George, 2017, ARXIV170507057; Parmar Niki, 2018, PR MACH LEARN RES, P4055; Pearl J., 2003, ECONOMET THEOR, V19; Pfeffer A., 2007, INTRO STAT RELATIONA, P399; Probtorch Developers, 2017, PROBT; Ranganath R, 2016, ADV NEUR IN, V29; Ritchie D., 2016, ARXIV161005735; Roy A.G., 2018, ARXIV PREPRINT ARXIV; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Salvatier J, 2016, PEERJ COMPUT SCI, DOI 10.7717/peerj-cs.55; Shazeer N, 2018, ADV NEUR IN, V31; Shi Jiaxin, 2017, ARXIV170905870; Sonderby Casper Kaae, 2016, NEURAL INFORM PROCES; Spiegelhalter D.J., 1995, BUGS BAYESIAN INFERE; Tolpin D., 2016, IFL; Tomczak J. M., 2018, ARTIFICIAL INTELLIGE; Tran D., 2018, INT C LEARN REPR; Tran DA, 2017, 2017 IEEE 28TH ANNUAL INTERNATIONAL SYMPOSIUM ON PERSONAL, INDOOR, AND MOBILE RADIO COMMUNICATIONS (PIMRC), DOI 10.1109/PIMRC.2017.8292622; Tran Dustin, 2016, ARXIV161009787; van den Oord A, 2016, PR MACH LEARN RES, V48; Vaswani A., 2018, MT RES TRACK; Wen Yeming, 2018, ARXIV180304386; Zoph B., 2017, P1	52	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002017
C	Wang, B; Luo, XY; Li, Z; Zhu, W; Shi, ZQ; Osher, SJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Bao; Luo, Xiyang; Li, Zhen; Zhu, Wei; Shi, Zuoqiang; Osher, Stanley J.			Deep Neural Nets with Interpolating Function as Output Activation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We replace the output layer of deep neural nets, typically the softmax function, by a novel interpolating function. And we propose end-to-end training and testing algorithms for this new architecture. Compared to classical neural nets with softmax function as output activation, the surrogate with interpolating function as output activation combines advantages of both deep and manifold learning. The new framework demonstrates the following major advantages: First, it is better applicable to the case with insufficient training data. Second, it significantly improves the generalization accuracy on a wide variety of networks. The algorithm is implemented in PyTorch, and the code is available at https://github.com/BaoWangMath/DNN-DataDependentActivation.	[Wang, Bao; Luo, Xiyang; Osher, Stanley J.] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA; [Li, Zhen] HKUST, Dept Math, Hong Kong, Peoples R China; [Zhu, Wei] Duke Univ, Dept Math, Durham, NC 27706 USA; [Shi, Zuoqiang] Tsinghua Univ, Dept Math, Beijing, Peoples R China	University of California System; University of California Los Angeles; Hong Kong University of Science & Technology; Duke University; Tsinghua University	Wang, B (corresponding author), Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90024 USA.	wangbaonj@gmail.com; xylmath@gmail.com; lishen03@gmail.com; zhu@math.duke.edu; zqshi@mail.tsinghua.edu.cn; sjo@math.ucla.edu			Air Force Research Laboratory; DARPA [FA8750-18-2-0066]; U.S. Department of Energy, Office of Science; National Science Foundation [DOE-SC0013838, DMS-1554564]; NSF [DMS-1737770]; Simons foundation	Air Force Research Laboratory; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); U.S. Department of Energy, Office of Science(United States Department of Energy (DOE)); National Science Foundation(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Simons foundation	This material is based on research sponsored by the Air Force Research Laboratory and DARPA under agreement number FA8750-18-2-0066. And by the U.S. Department of Energy, Office of Science and by National Science Foundation, under Grant Numbers DOE-SC0013838 and DMS-1554564, (STROBE). And by the NSF DMS-1737770 and the Simons foundation. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Baldi P, 2014, ARXIV14126830; Bengio Y., 2007, P ADV NEUR INF PROC, V19, P153, DOI DOI 10.7551/MITPRESS/7503.003.0024; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Burges, 1998, MNIST DATABASE HANDW; Calder J., 2018, ARXIV171110144; Chang B, 2017, ARXIV171010348; Chen Y., 2017, NIPS; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I. J., 2013, ARXIV13024389; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Z, 2017, ARXIV170805115; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Nair V, 2010, P 27 INT C MACHINE L, P807; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Papernot N, 2015, ARXIV151107528; Shi ZQ, 2017, J SCI COMPUT, V73, P1164, DOI 10.1007/s10915-017-0421-z; Tang Yichuan, 2013, ARXIV13060239; Wan L., 2013, P INT C MACHINE LEAR, P1058; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhu W., 2017, 1766 UCLA CAM	32	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300069
C	Wang, NG; Choi, J; Brand, D; Chen, CY; Gopalakrishnan, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Naigang; Choi, Jungwook; Brand, Daniel; Chen, Chia-Yu; Gopalakrishnan, Kailash			Training Deep Neural Networks with 8-bit Floating Point Numbers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The state-of-the-art hardware platforms for training Deep Neural Networks (DNNs) are moving from traditional single precision (32-bit) computations towards 16 bits of precision - in large part due to the high energy efficiency and smaller bit storage associated with using reduced-precision representations. However, unlike inference, training with numbers represented with less than 16 bits has been challenging due to the need to maintain fidelity of the gradient computations during back-propagation. Here we demonstrate, for the first time, the successful training of DNNs using 8-bit floating point numbers while fully maintaining the accuracy on a spectrum of Deep Learning models and datasets. In addition to reducing the data and computation precision to 8 bits, we also successfully reduce the arithmetic precision for additions (used in partial product accumulation and weight updates) from 32 bits to 16 bits through the introduction of a number of key ideas including chunk-based accumulation and floating point stochastic rounding. The use of these novel techniques lays the foundation for a new generation of hardware training platforms with the potential for 2 - 4 x improved throughput over today's systems.	[Wang, Naigang; Choi, Jungwook; Brand, Daniel; Chen, Chia-Yu; Gopalakrishnan, Kailash] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Wang, NG (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	nwang@us.ibm.com; choij@us.ibm.com; danbrand@us.ibm.com; cchen@us.ibm.com; kailash@us.ibm.com	Wang, Naigang/T-2147-2019; Choi, Jungwook/AAA-2088-2020		IBM Research	IBM Research(International Business Machines (IBM))	The authors would like to thank I-Hsin Chung, Ming-Hung Chen, Ankur Agrawal, Silvia Melitta Mueller, Vijayalakshmi Srinivasan, Dongsoo Lee and Jinseok Kim for helpful discussions and supports. This research was supported by IBM Research, IBM SoftLayer, and IBM Congnitive Computing Cluster (CCC).	Castaldo AM, 2008, SIAM J SCI COMPUT, V31, P1156, DOI 10.1137/070679946; Chen CY, 2018, DES AUT TEST EUROPE, P821; Choi Jungwook, 2018, P INT C LEARN REPR I; Das D., 2018, CORR; Fleischer Bruce, 2018, VLSI CIRC 2018 S; Gupta S., 2015, P 32 INT C MACH LEAR, V37, P1737; Gupta S, 2016, IEEE DATA MINING, P171, DOI [10.1109/ICDM.2016.122, 10.1109/ICDM.2016.0028]; Harris Mark, 2016, MIXED PRECISION PROG, V8; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; HIGHAM NJ, 1993, SIAM J SCI COMPUT, V14, P783, DOI 10.1137/0914050; Hubara I, 2016, ADV NEUR IN, V29; Kingma DP, 2015, INT C LEARN REPR ICL; Koster U., 2017, ADV NEURAL INFORM PR, P1742; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex, 2010, UNPUB, V40; Micikevicius Paulius, 2017, ARXIV171003740; ROBERTAZZI TG, 1988, ACM T MATH SOFTWARE, V14, P101, DOI 10.1145/42288.42343; van den Berg E, 2017, INT CONF ACOUST SPEE, P2287, DOI 10.1109/ICASSP.2017.7952564; Wu S., 2018, ARXIV180204680; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xiong W, 2017, INT CONF ACOUST SPEE, P5255, DOI 10.1109/ICASSP.2017.7953159; Zhou S, 2016, CORR	22	9	9	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002024
C	Wen, M; Topcu, U		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wen, Min; Topcu, Ufuk			Constrained Cross-Entropy Method for Safe Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GAME; GO	We study a safe reinforcement learning problem in which the constraints are defined as the expected cost over finite-length trajectories. We propose a constrained cross-entropy-based method to solve this problem. The method explicitly tracks its performance with respect to constraint satisfaction and thus is well-suited for safety-critical applications. We show that the asymptotic behavior of the proposed algorithm can be almost-surely described by that of an ordinary differential equation. Then we give sufficient conditions on the properties of this differential equation for the convergence of the proposed algorithm. At last, we show with simulation experiments that the proposed algorithm can effectively learn feasible policies without assumptions on the feasibility of initial policies, even with non-Markovian objective functions and constraint functions.	[Wen, Min] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA; [Topcu, Ufuk] Univ Texas Austin, Dept Aerosp Engn & Engn Mech, Austin, TX 78712 USA	University of Pennsylvania; University of Texas System; University of Texas Austin	Wen, M (corresponding author), Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA.	wenm@seas.upenn.edu; utopcu@utexas.edu		Topcu, Ufuk/0000-0003-0819-9985	ONR [N000141712623]; ARO [W911NF-15-1-0592]; DARPA [W911NF-16-1-0001]	ONR(Office of Naval Research); ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported in part by ONR N000141712623, ARO W911NF-15-1-0592 and DARPA W911NF-16-1-0001.	Achiam J, 2017, PR MACH LEARN RES, V70; Altman E., 1993, ZOR, Methods and Models of Operations Research, V37, P151, DOI 10.1007/BF01414154; [Anonymous], 2003, P 20 INT C MACH LEAR; Bertsekas D. P., 2007, DYNAMIC PROGRAMMING, V2; Chow Y., 2015, ARXIV151201629; Duan Y, 2016, INT C MACH LEARN, P1329; Fu J, 2017, PROCEEDINGS OF THE 20TH INTERNATIONAL CONFERENCE ON HYBRID SYSTEMS: COMPUTATION AND CONTROL (PART OF CPS WEEK) (HSCC' 17), P227; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Gu SX, 2016, PR MACH LEARN RES, V48; Hanna JP, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P538; Homem-De-Mello T, 2007, INFORMS J COMPUT, V19, P381, DOI 10.1287/ijoc.1060.0176; Hu JQ, 2012, IEEE T AUTOMAT CONTR, V57, P165, DOI 10.1109/TAC.2011.2158128; Hu JQ, 2008, COMMUN INF SYST, V8, P245; Jiang N, 2016, PR MACH LEARN RES, V48; Kobilarov M., 2011, ROBOTICS SCI SYSTEMS; Lening Li, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P1328, DOI 10.1109/ICRA.2017.7989157; Levine S, 2016, J MACH LEARN RES, V17; Livingston S. C., 2015, P 18 INT C HYBR SYST, P269; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V., 2013, PLAYING ATARI DEEP R, P1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Montgomery William, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3373, DOI 10.1109/ICRA.2017.7989383; Papusha I, 2016, IEEE DECIS CONTR P, P434, DOI 10.1109/CDC.2016.7798307; Popov I., 2017, ARXIV170403073; Rubinstein R.Y., 1998, MODERN SIMULATION MO, V7; Salimans T., 2017, ARXIV170303864; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Silver D, 2014, ICML ICML 14, P387; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Szita Istvan, 2006, LEARNING, V18; Uchibe Eiji, 2007, 2007 IEEE 6 INT C DE, P163, DOI [10. 1109/DEVLRN.2007.4354030, DOI 10.1109/DEVLRN.2007.4354030]; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]; Zamora I., 2016, ARXIV160805742; Zhang TH, 2016, IEEE INT CONF ROBOT, P528, DOI 10.1109/ICRA.2016.7487175	37	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002004
C	Yao, ZW; Gholami, A; Lei, Q; Keutzer, K; Mahoney, MW		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yao, Zhewei; Gholami, Amir; Lei, Qi; Keutzer, Kurt; Mahoney, Michael W.			Hessian-based Analysis of Large Batch Training and Robustness to Adversaries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Large batch size training of Neural Networks has been shown to incur accuracy loss when trained with the current methods. The exact underlying reasons for this are still not completely understood. Here, we study large batch size training through the lens of the Hessian operator and robust optimization. In particular, we perform a Hessian based study to analyze exactly how the landscape of the loss function changes when training with large batch size. We compute the true Hessian spectrum, without approximation, by back-propagating the second derivative. Extensive experiments on multiple networks show that saddle-points are not the cause for generalization gap of large batch size training, and the results consistently show that large batch converges to points with noticeably higher Hessian spectrum. Furthermore, we show that robust training allows one to favors flat areas, as points with large Hessian spectrum show poor robustness to adversarial perturbation. We further study this relationship, and provide empirical and theoretical proof that the inner loop for robust training is a saddle-free optimization problem. We present detailed experiments with five different network architectures, including a residual network, tested on MNIST, CIFAR-10, and CIFAR-100 datasets.	[Yao, Zhewei; Gholami, Amir; Keutzer, Kurt; Mahoney, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Lei, Qi] Univ Texas Austin, Austin, TX 78712 USA	University of California System; University of California Berkeley; University of Texas System; University of Texas Austin	Yao, ZW (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	zheweiy@berkeley.edu; amirgh@berkeley.edu; leiqi@ices.utexas.edu; keutzer@berkeley.edu; mahoneymw@berkeley.edu	Yao, Zhewei/ABE-1531-2021; Lei, Qi/T-2146-2019	Yao, Zhewei/0000-0001-7678-4321; 				Bhagoji AN, 2017, ARXIV PREPRINT ARXIV; BOTTOU L, 1994, INT C PATT RECOG, P77, DOI 10.1109/ICPR.1994.576879; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chaudhari P, 2016, ARXIV161101838; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Desjardins G., 2015, ADV NEURAL INFORM PR, P2071; Dinh L, 2017, PR MACH LEARN RES, V70; Eisenstat SC, 1996, SIAM J SCI COMPUT, V17, P16, DOI 10.1137/0917003; Feinman R., 2017, ARXIV PREPRINT ARXIV; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Gholami A., 2018, SPAA 8; Gong Zhitao, 2017, ARXIV170404960; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Goyal Priya, 2017, ARXIV170602677; Grosse Kathrin, 2017, ARXIV170206280; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Iandola FN, 2016, PROC CVPR IEEE, P2592, DOI 10.1109/CVPR.2016.284; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081; Lee JD, 2019, MATH PROGRAM, V176, P311, DOI 10.1007/s10107-019-01374-3; Madry Aleksander, 2017, ARXIV; Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Smith Samuel L, 2017, ARXIV171100489; Smith Samuel L, 2017, 2 WORKSH BAYES DEEP; Yao Z., 2018, ARXIV181001021; You Yang, 2017, ARXIV170803888, V6, P12; Zhang Chiyuan, 2016, ARXIV161103530	35	9	10	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304092
C	Zhao, SJ; Ren, HY; Yuan, A; Song, JM; Goodman, N; Ermon, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhao, Shengjia; Ren, Hongyu; Yuan, Arianna; Song, Jiaming; Goodman, Noah; Ermon, Stefano			Bias and Generalization in Deep Generative Models: An Empirical Study	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In high dimensional settings, density estimation algorithms rely crucially on their inductive bias. Despite recent empirical success, the inductive bias of deep generative models is not well understood. In this paper we propose a framework to systematically investigate bias and generalization in deep generative models of images. Inspired by experimental methods from cognitive psychology, we probe each learning algorithm with carefully designed training datasets to characterize when and how existing models generate novel attributes and their combinations. We identify similarities to human psychology and verify that these patterns are consistent across commonly used models and architectures.	[Zhao, Shengjia; Ren, Hongyu; Yuan, Arianna; Song, Jiaming; Goodman, Noah; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Zhao, SJ (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	sjzhao@stanford.edu; hyren@stanford.edu; xfyuan@stanford.edu; tsong@stanford.edu; ngoodman@stanford.edu; ermon@stanford.edu			Intel; NSF [1651565, 1522054, 1733686]; ONR; TRI	Intel(Intel Corporation); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); TRI	This research was supported by Intel, TRI, NSF (#1651565, #1522054, #1733686), and ONR.	Alvarez GA, 2011, TRENDS COGN SCI, V15, P122, DOI 10.1016/j.tics.2011.01.003; Arora S., 2017, ARXIV170608224; Borji A, 2019, COMPUT VIS IMAGE UND, V179, P41, DOI 10.1016/j.cviu.2018.10.009; Che Tong, 2016, ARXIV161202136; Efromovich S, 2010, WIRES COMPUT STAT, V2, P467, DOI 10.1002/wics.97; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Higgins I, 2016, BETA VAE LEARNING BA; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Keskar Nitish Shirish, 2016, LARGE BATCH TRAINING, P4; Kingma D. P., 2013, AUTO ENCODING VARIAT; Li Yuxi, 2017, ARXIV170107274; Minda J.P., 2011, FORMAL APPROACHES CA, P40, DOI [10.1017/CBO9780511921322.003, DOI 10.1017/CBO9780511921322.003, 10.1017/cbo9780511921322.003]; Mitchell Tom M, 1980, NEED BIASES LEARNING, P3; Nieder A, 2003, NEURON, V37, P149, DOI 10.1016/S0896-6273(02)01144-3; Nieder A, 2002, SCIENCE, V297, P1708, DOI 10.1126/science.1072493; Nieder A, 2007, J NEUROSCI, V27, P5986, DOI 10.1523/JNEUROSCI.1056-07.2007; Piazza M, 2004, NEURON, V44, P547, DOI 10.1016/j.neuron.2004.10.014; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; ROSENBLATT M, 1956, P NATL ACAD SCI USA, V42, P43, DOI 10.1073/pnas.42.1.43; ROSENBLATT M, 1956, ANN MATH STAT, V27, P832, DOI 10.1214/aoms/1177728190; Salimans T, 2016, ADV NEUR IN, V29; Smith JD, 2000, J EXP PSYCHOL LEARN, V26, P3, DOI 10.1037/0278-7393.26.1.3; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Stevens S. S., 2017, PSYCHOPHYSICS INTRO; Theis Lucas, 2015, ARXIV151101844; Zhang Chiyuan, 2016, ARXIV161103530; Zhao SJ, 2017, PR MACH LEARN RES, V70; Zhao Shengjia, 2018, P 34 C UNC ART INT	33	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005038
C	Bui, TD; Nguyen, CV; Turner, RE		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bui, Thang D.; Nguyen, Cuong, V; Turner, Richard E.			Streaming Sparse Gaussian Process Approximations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Sparse pseudo-point approximations for Gaussian process (GP) models provide a suite of methods that support deployment of GPs in the large data regime and enable analytic intractabilities to be sidestepped. However, the field lacks a principled method to handle streaming data in which both the posterior distribution over function values and the hyperparameter estimates are updated in an online fashion. The small number of existing approaches either use suboptimal hand-crafted heuristics for hyperparameter learning, or suffer from catastrophic forgetting or slow updating when new data arrive. This paper develops a new principled framework for deploying Gaussian process probabilistic models in the streaming setting, providing methods for learning hyperparameters and optimising pseu do-input locations. The proposed framework is assessed using synthetic and real-world datasets.	[Bui, Thang D.; Nguyen, Cuong, V; Turner, Richard E.] Univ Cambridge, Dept Engn, Cambridge, England	University of Cambridge	Bui, TD (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	tdb40@cam.ac.uk; vcn22@cam.ac.uk; ret26@cam.ac.uk	Bui, Thang/AAZ-5360-2021; Jeong, Yongwook/N-7413-2016	Bui, Thang/0000-0002-7878-9748; 	Google European Doctoral Fellowship; EPSRC [EP/M0269571, EP/L000776/1]; Google	Google European Doctoral Fellowship(Google Incorporated); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google(Google Incorporated)	The authors would like to thank Mark Rowland, John Bradshaw, and Yingzhen Li for insightful comments and discussion. Thang D. Bui is supported by the Google European Doctoral Fellowship. Cuong V. Nguyen is supported by EPSRC grant EP/M0269571. Richard E. Turner is supported by Google as well as EPSRC grants EP/M0269571 and EP/L000776/1.	Bauer M., 2016, ADV NEURAL INFORM PR; Broderick T., 2013, ADV NEURAL INFORM PR; Bui T. D., 2014, ADV NEURAL INFORM PR; Bui T. D., 2017, J MACHINE LEARNING R; Bui T. D., 2016, INT C MACH LEARN ICM; Cheng C.-A., 2016, ADV NEURAL INFORM PR; CSATO L, 2002, NEURAL COMPUTATION; Csato Lehel, 2002, THESIS; Dezfouli A., 2015, ADV NEURAL INFORM PR; Garofolo JS, 1993, TIMIT ACOUSTIC PHONE; Ghahramani Z., 2000, NIPS WORKSH ONL LEAR; Hensman J., 2013, C UNC ART INT UAI; Hensman J., 2015, INT C ART INT STAT A; Hernandez-Lobato D., 2016, INT C ART INT STAT A; Keogh E. J., 1999, INT C SCI STAT DAT M; Matthews A. G. d. G., 2017, J MACHINE LEARNING R; Matthews A. G. D. G., 2016, INT C ART INT STAT A; Minka Thomas, 2004, TECH REP; Opper Manfred, 1999, ON LINE LEARNING NEU; QUINONEROCANDEL.J, 2005, J MACHINE LEARNING R; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sato M.-A., 2001, NEURAL COMPUTATION; Snelson E., 2006, ADV NEURAL INFORM PR, V18; Titsias M.K., 2009, J MACH LEARN RES	24	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403036
C	Ding, N; Soricut, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ding, Nan; Soricut, Radu			Cold-Start Reinforcement Learning with Softmax Policy Gradient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Policy-gradient approaches to reinforcement learning have two common and undesirable overhead procedures, namely warm-start training and sample variance reduction. In this paper, we describe a reinforcement learning method based on a softmax value function that requires neither of these procedures. Our method combines the advantages of policy-gradient methods with the efficiency and simplicity of maximum-likelihood approaches. We apply this new cold-start reinforcement learning method in training sequence generation models for structured output prediction problems. Empirical evidence validates this method on automatic summarization and image captioning tasks.	[Ding, Nan; Soricut, Radu] Google Inc, Venice, CA 90291 USA	Google Incorporated	Ding, N (corresponding author), Google Inc, Venice, CA 90291 USA.	dingnan@google.com; rsoricut@google.com	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Bengio Y., 2014, ARXIV14061078; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Evans L. C., PREPRINT; Graff David, 2003, ENGLISH GIGAWORD FIF; Huszar Ferenc, 2015, CORR; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Koehn Philipp, 2004, EMNLP; LIN C, 2004, P ACL; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Siqi, 2017, INT C COMP VIS ICCV; Neu G., 2017, CORR; Norouzi M, 2016, ADV NEUR IN, V29; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato MarcAurelio, 2015, ARXIV151106732; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Vedantam Ramakrishna, 2015, IEEE C COMP VIS PATT; Venkatraman A, 2015, AAAI CONF ARTIF INTE, P3024; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu YH, 2018, VIS COMPUT IND BIOME, V1, DOI 10.1186/s42492-018-0008-z	26	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402084
C	Dobbe, R; Fridovich-Keil, D; Tomlin, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dobbe, Roel; Fridovich-Keil, David; Tomlin, Claire			Fully Decentralized Policies for Multi-Agent Systems: An Information Theoretic Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Learning cooperative policies for multi-agent systems is often challenged by partial observability and a lack of coordination. In some settings, the structure of a problem allows a distributed solution with limited communication. Here, we consider a scenario where no communication is available, and instead we learn local policies for all agents that collectively mimic the solution to a centralized multi-agent static optimization problem. Our main contribution is an information theoretic framework based on rate distortion theory which facilitates analysis of how well the resulting fully decentralized policies are able to reconstruct the optimal solution. Moreover, this framework provides a natural extension that addresses which nodes an agent should communicate with to improve the performance of its individual policy.	[Dobbe, Roel; Fridovich-Keil, David; Tomlin, Claire] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Dobbe, R (corresponding author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	dobbe@eecs.berkeley.edu; dfk@eecs.berkeley.edu; tomlin@eecs.berkeley.edu	Jeong, Yongwook/N-7413-2016; Dobbe, Roel/GYU-6935-2022; Fridovich-Keil, David/AAO-6781-2020	Fridovich-Keil, David/0000-0002-5866-6441	NSF under the CPS Frontiers VehiCal project [1545126]; UC-Philippine-California Advanced Research Institute [IIID-2016-005, IIID-2015-10]; ONR MURI Embedded Humans [N00014-16-1-2206]; NSF GRFP	NSF under the CPS Frontiers VehiCal project; UC-Philippine-California Advanced Research Institute; ONR MURI Embedded Humans; NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD))	The authors would like to acknowledge Roberto Calandra for his insightful suggestions and feedback on the manuscript. This research is supported by NSF under the CPS Frontiers VehiCal project (1545126), by the UC-Philippine-California Advanced Research Institute under projects IIID-2016-005 and IIID-2015-10, and by the ONR MURI Embedded Humans (N00014-16-1-2206). David Fridovich-Keil was also supported by the NSF GRFP.	Abbeel P., 2004, INT C MACH LEARN; Aumann R. J., 1974, International Journal of Game Theory, V3, P217, DOI 10.1007/BF01766876; BARAN ME, 1989, IEEE T POWER DELIVER, V4, P725, DOI 10.1109/61.19265; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Brambilla M, 2013, SWARM INTELL-US, V7, P1, DOI 10.1007/s11721-012-0075-2; Christofides PD, 2013, COMPUT CHEM ENG, V51, P21, DOI 10.1016/j.compchemeng.2012.05.011; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dall'Anese E, 2014, IEEE T SUSTAIN ENERG, V5, P487, DOI 10.1109/TSTE.2013.2292828; DAVISON EJ, 1990, IEEE T AUTOMAT CONTR, V35, P652, DOI 10.1109/9.53544; Farivar M, 2013, IEEE DECIS CONTR P, P4329, DOI 10.1109/CDC.2013.6760555; Friedman J., 2015, PACKAGE GLASSO; Goldman CV, 2004, J ARTIF INTELL RES, V22, P143, DOI 10.1613/jair.1427; IEEE PES, 2017, IEEE DISTR TEST FEED; Jiao J., 2014, ARXIV14066956; Low SH, 2014, IEEE T CONTROL NETW, V1, P15, DOI 10.1109/TCNS.2014.2309732; Lunze J., 1992, FEEDBACK CONTROL LAR; Modi PJ, 2005, ARTIF INTELL, V161, P149, DOI 10.1016/j.artint.2004.09.003; Nair R., 2005, AAAI, P133; Oliehoek F.A., 2016, CONCISE INTRO DECENT; Ortega P. A., 2015, ARXIV151206789; Pecan Street Inc, 2017, DAT 2017; Peshkin L, 2000, P 16 C UNCERTAINTY A, P489; Pu Y., 2014, C DEC CONTR LOS ANG; Raffard R. L., 2004, C DEC CONTR NASS BAH; Sammut C, 1996, KNOWL ENG REV, V11, P27, DOI 10.1017/S0269888900007669; Siljak D. D., 2011, DECENTRALIZED CONTRO; Sondermeijer O., 2016, POW EN SOC GEN M BOS; Sun AX, 2013, IEEE POW ENER SOC GE; Xu Y, 2017, IEEE T POWER SYST, V32, P4398, DOI 10.1109/TPWRS.2017.2669343; Zeilinger M. N., 2013, C DEC CONTR FLOR IT; Zhang BS, 2015, IEEE T POWER SYST, V30, P1714, DOI 10.1109/TPWRS.2014.2347281	31	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403001
C	Nguyen, DT; Kumar, A; Lau, HC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin			Policy Gradient With Value Function Approximation For Collective Multiagent Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPLEXITY	Decentralized (PO) MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity, recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDec-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing C Dec-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this, we show how a particular decomposition of the approximate action-value function over agents leads to effective updates, and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches.	[Duc Thien Nguyen; Kumar, Akshat; Lau, Hoong Chuin] Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore	Singapore Management University	Nguyen, DT (corresponding author), Singapore Management Univ, Sch Informat Syst, 80 Stamford Rd, Singapore 178902, Singapore.	dtnguyen.2014@smu.edu.sg; akshatkumar@smu.edu.sg; hclau@smu.edu.sg	LAU, Hoong Chuin/E-8556-2012; Jeong, Yongwook/N-7413-2016	LAU, Hoong Chuin/0000-0002-5326-411X; 	National Research Foundation Singapore under its Corp Lab @ University scheme; Fujitsu Limited; A*STAR graduate scholarship	National Research Foundation Singapore under its Corp Lab @ University scheme(National Research Foundation, Singapore); Fujitsu Limited; A*STAR graduate scholarship(Agency for Science Technology & Research (A*STAR))	This research project is supported by National Research Foundation Singapore under its Corp Lab @ University scheme and Fujitsu Limited. First author is also supported by A*STAR graduate scholarship.	Aberdeen D., 2006, ADV NEURAL INFORM PR, P9; Amato C, 2015, IEEE INT CONF ROBOT, P1241, DOI 10.1109/ICRA.2015.7139350; Bagnell J. A., 2005, INT C NEUR INF PROC, P91; Becker R, 2004, J ARTIF INTELL RES, V22, P423, DOI 10.1613/jair.1497; Becker Raphen, 2004, P 3 INT C AUT AG MUL, P302; Bernstein DS, 2002, MATH OPER RES, V27, P819, DOI 10.1287/moor.27.4.819.297; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Nguyen DT, 2017, AAAI CONF ARTIF INTE, P3036; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Guestrin C., 2002, ICML, P227; Konda VR, 2003, SIAM J CONTROL OPTIM, V42, P1143, DOI 10.1137/S0363012901385691; Kumar A, 2015, J ARTIF INTELL RES, V53, P223, DOI 10.1613/jair.4649; Kumar Akshat, 2011, P 22 INT JOINT C ART, P2140; Leibo J. Z., 2017, INT C AUT AG MULT SY; Meyers CA, 2012, NETWORKS, V59, P252, DOI 10.1002/net.20439; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair R., 2005, AAAI, P133; Pajarinen J, 2014, IEEE T MOBILE COMPUT, V13, P866, DOI 10.1109/TMC.2013.39; Peshkin L, 2000, P 16 C UNCERTAINTY A, P489; Robbel P, 2016, AAAI CONF ARTIF INTE, P2537; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sonu E, 2015, P I C AUTOMAT PLAN S, P202; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Varakantham P. R., 2012, AAAI C ART INT, P1471; Varakantham P, 2014, AAAI CONF ARTIF INTE, P2505; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Winstein K, 2013, ACM SIGCOMM COMP COM, V43, P123, DOI 10.1145/2534169.2486020; Witwicki Stefan J., 2010, INT C AUT PLANN SCHE, P185	29	9	9	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404038
C	Ge, R; Ma, TY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ge, Rong; Ma, Tengyu			On the Optimization Landscape of Tensor Decompositions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COMPLEXITY	Non-convex optimization with local search heuristics has been widely used in machine learning, achieving many state-of-art results. It becomes increasingly important to understand why they can work for these NP-hard problems on typical data. The landscape of many objective functions in learning has been conjectured to have the geometric property that "all local optima are (approximately) global optima", and thus they can be solved efficiently by local search algorithms. However, establishing such property can be very difficult. In this paper, we analyze the optimization landscape of the random over-complete tensor decomposition problem, which has many applications in unsupervised leaning, especially in learning latent variable models. In practice, it can be efficiently solved by gradient ascent on a non-convex objective. We show that for any small constant epsilon > 0, among the set of points with function values (1 + epsilon)-factor larger than the expectation of the function, all the local maxima are approximate global maxima. Previously, the best-known result only characterizes the geometry in small neighborhoods around the true components. Our result implies that even with an initialization that is barely better than the random guess, the gradient ascent algorithm is guaranteed to solve this problem. Our main technique uses Kac-Rice formula and random matrix theory. To our best knowledge, this is the first time when Kac-Rice formula is successfully applied to counting the number of local optima of a highly-structured random polynomial with dependent coefficients.	[Ge, Rong] Duke Univ, Durham, NC 27706 USA; [Ma, Tengyu] Facebook AI Res, Menlo Pk, CA USA	Duke University; Facebook Inc	Ge, R (corresponding author), Duke Univ, Durham, NC 27706 USA.	rongge@cs.duke.edu; tengyuma@cs.stanford.edu	Jeong, Yongwook/N-7413-2016					Abo H., 2015, ARXIV E PRINTS; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Adler RJ., 2009, RANDOM FIELDS GEOMET; Anandkumar A., 2016, JMLR; Anandkumar A., 2012, COLT; Anandkumar Anima, 2012, ADV NEURAL INFORM PR, V25; Arora S., 2015, P 28 C LEARN THEOR; Auffinger A, 2013, ANN PROBAB, V41, P4214, DOI 10.1214/13-AOP862; Auffinger A, 2013, COMMUN PUR APPL MATH, V66, P165, DOI 10.1002/cpa.21422; Bandeira A. S., 2016, JMLR WORKSHOP C P; Barak B, 2015, ACM S THEORY COMPUT, P143, DOI 10.1145/2746539.2746605; Bhaskara A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P594, DOI 10.1145/2591796.2591881; Bhojanapalli Srinadh, 2016, ARXIV160507221; Boumal N., 2016, ARXIV E PRINTS; Cartwright D, 2013, LINEAR ALGEBRA APPL, V438, P942, DOI 10.1016/j.laa.2011.05.040; Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2; Choromanska A., 2015, AISTATS; Cohen N., 2016, CORR; Comon P, 2009, J CHEMOMETR, V23, P393, DOI 10.1002/cem.1236; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; De Lathauwer L, 2007, IEEE T SIGNAL PROCES, V55, P2965, DOI 10.1109/TSP.2007.893943; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ge R., 2017, ARXIV E PRINTS; Ge R., 2015, ARXIV150405287; Ge Rong, 2016, ARXIV160507272; Goyal N., 2013, ARXIV13065825; Hardt M., 2016, CORR; HASTAD J, 1990, J ALGORITHMS, V11, P644, DOI 10.1016/0196-6774(90)90014-6; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hopkins SB, 2016, ACM S THEORY COMPUT, P178, DOI 10.1145/2897518.2897529; Hsu D, 2013, 4 INNOVATIONS THEORE; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Janzamin M., 2015, ARXIV E PRINTS; JANZAMIN M., 2015, P C LEARN THEOR COLT; Kawaguchi K, 2016, ARXIV E PRINTS; Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482; Lee J. D., 2016, C LEARN THEOR, P1246; Ma T., 2016, FOCS 2016; Mossel E, 2006, ANN APPL PROBAB, V16, P583, DOI 10.1214/105051606000000024; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Novikov A., 2015, ARXIV E PRINTS; Sun J., 2015, ARXIV151006096	42	9	9	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403070
C	Pu, YC; Gan, Z; Henao, R; Li, CY; Han, SB; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pu, Yunchen; Gan, Zhe; Henao, Ricardo; Li, Chunyuan; Han, Shaobo; Carin, Lawrence			VAE Learning via Stein Variational Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A new method for learning variational autoencoders (VAEs) is developed, based on Stein variational gradient descent. A key advantage of this approach is that one need not make parametric assumptions about the form of the encoder distribution. Performance is further enhanced by integrating the proposed encoder with importance sampling. Excellent performance is demonstrated across multiple unsupervised and semi-supervised problems, including semi-supervised analysis of the ImageNet data, demonstrating the scalability of the model to large datasets.	[Pu, Yunchen; Gan, Zhe; Henao, Ricardo; Li, Chunyuan; Han, Shaobo; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA	Duke University	Pu, YC (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	yp42@duke.edu; zg27@duke.edu; r.henao@duke.edu; cl319@duke.edu; shaobo.han@duke.edu; lcarin@duke.edu	Jeong, Yongwook/N-7413-2016; Li, Chunyuan/AAG-1303-2020	Henao, Ricardo/0000-0003-4980-845X; Carin, Lawrence/0000-0001-6277-7948	ARO; DARPA; DOE; NGA; ONR; NSF	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.	Burda Y., 2016, ICLR; Chen Liqun, 2017, ARXIV170901846; Feng Y., 2017, UAI; Gan Z., 2015, ICML; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Han J, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Han S., 2016, ARTIFICIAL INTELLIGE; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lauly S., 2012, NIPS; Liu Q., 2016, NEURIPS; Maas Andrew L, 2013, P ICML CIT, V30, P3, DOI DOI 10.21437/INTERSPEECH.2016-1230; Miao YS, 2016, PR MACH LEARN RES, V48; Mnih Andriy, 2014, INT C MACH LEARN; Mnih Andriy, 2016, P ICML; Oord A. v. d., 2016, ICML; Pu  Y., 2015, ICLR WORKSH; Pu Y., 2016, ADV NEURAL INF PROCE, P2352; Pu Y., 2017, P ADV NEUR INF PROC, P4330; Pu Y, 2016, ARTIFICIAL INTELLIGE; Ranganath R., 2015, AISTATS; Ranganath R, 2016, PR MACH LEARN RES, V48; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen D., 2017, ARXIV170907109; Springenberg J. T, 2015, ARXIV PREPRINT ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Zhang Y., 2017, ADV NEURAL INFORM PR; Zhou M., 2012, AISTATS	36	9	9	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404030
C	Shim, K; Lee, M; Choi, I; Boo, Y; Sung, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shim, Kyuhong; Lee, Minjae; Choi, Iksoo; Boo, Yoonho; Sung, Wonyong			SVD-Softmax: Fast Softmax Approximation on Large Vocabulary Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a fast approximation method of a softmax function with a very large vocabulary using singular value decomposition (SVD). SVD-softmax targets fast and accurate probability estimation of the topmost probable words during inference of neural network language models. The proposed method transforms the weight matrix used in the calculation of the output vector by using SVD. The approximate probability of each word can be estimated with only a small part of the weight matrix by using a few large singular values and the corresponding elements for most of the words. We applied the technique to language modeling and neural machine translation and present a guideline for good approximation. The algorithm requires only approximately 20% of arithmetic operations for an 800K vocabulary case and shows more than a three-fold speedup on a GPU.	[Shim, Kyuhong; Lee, Minjae; Choi, Iksoo; Boo, Yoonho; Sung, Wonyong] Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea	Seoul National University (SNU)	Shim, K (corresponding author), Seoul Natl Univ, Dept Elect & Comp Engn, Seoul, South Korea.	skhu20@snu.ac.kr; mjlee@dsp.snu.ac.kr; ischoi@dsp.snu.ac.kr; yhboo@dsp.snu.ac.kr; wysung@snu.ac.kr	Jeong, Yongwook/N-7413-2016		Brain Korea 21 Plus Project; National Research Foundation of Korea (NRF) - Korea government (MSIP) [2015R1A2A1A10056051]	Brain Korea 21 Plus Project; National Research Foundation of Korea (NRF) - Korea government (MSIP)(National Research Foundation of KoreaMinistry of Science, ICT & Future Planning, Republic of Korea)	This work was supported in part by the Brain Korea 21 Plus Project and the National Research Foundation of Korea (NRF) grant funded by the Korea government (MSIP) (No.2015R1A2A1A10056051).	Andreas Jacob, 2015, ADV NEURAL INFORM PR, P1783; Bengio Y., 2014, ARXIV14061078; Bengio Yoshua, 2003, P C ART INT STAT AIS; Bojar Ond.rej, 2015, P 10 WORKSH STAT MAC, P1, DOI DOI 10.18653/V1/W15-3001; Chan W, 2016, INT CONF ACOUST SPEE, P4960, DOI 10.1109/ICASSP.2016.7472621; Chelba Ciprian, 2013, ARXIV13123005; Chen W., 2015, ARXIV151204906; Dauphin Yann N., 2016, CORR; Devlin J, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1370; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; GOLUB GH, 1970, NUMER MATH, V14, P403, DOI 10.1007/BF02163027; Grave E., 2016, ARXIV160904309; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hwang K, 2016, INT CONF ACOUST SPEE, P5335, DOI 10.1109/ICASSP.2016.7472696; Jean Sebastien, 2014, ARXIV14122007; Ji S., 2015, ARXIV151106909; Klein G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P67, DOI 10.18653/v1/P17-4012; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Le HS, 2011, INT CONF ACOUST SPEE, P5524; Liu X., 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4908, DOI 10.1109/ICASSP.2014.6854535; Luong M., 2015, ARXIV150804025; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov T, 2011, INT CONF ACOUST SPEE, P5528; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mnih Andriy, 2012, ARXIV12066426, P419; Mnih Andriy, 2009, ADV NEURAL INFORM PR, P1081; Morin F., 2005, PROC INT WORKSHOP AR, P246; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tiedemann J, 2012, LREC 2012 - EIGHTH INTERNATIONAL CONFERENCE ON LANGUAGE RESOURCES AND EVALUATION, P2214	34	9	9	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405053
C	Yang, Y; Etesami, J; He, N; Kiyavash, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yang, Yingxiang; Etesami, Jalal; He, Niao; Kiyavash, Negar			Online Learning for Multivariate Hawkes Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				STABILITY	We develop a nonparametric and online learning algorithm that estimates the triggering functions of a multivariate Hawkes process (MHP). The approach we take approximates the triggering function f(i,j) (t) by functions in a reproducing kernel Hilbert space (RKHS), and maximizes a time-discretized version of the log-likelihood, with Tikhonov regularization. Theoretically, our algorithm achieves an O (log T) regret bound. Numerical results show that our algorithm offers a competing performance to that of the nonparametric batch learning algorithm, with a run time comparable to parametric online learning algorithms.	[Yang, Yingxiang; Kiyavash, Negar] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Etesami, Jalal; He, Niao; Kiyavash, Negar] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Yang, Y (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	yyang172@illinois.edu; etesami2@illinois.edu; niaohe@illinois.edu; kiyavash@illinois.edu	Jeong, Yongwook/N-7413-2016		MURI grant ARMY [W911NF-15-1-0479]; ONR [W911NF-15-1-0479]	MURI grant ARMY(MURI); ONR(Office of Naval Research)	This work was supported in part by MURI grant ARMY W911NF-15-1-0479 and ONR grant W911NF-15-1-0479.	Bacry E, 2012, EUR PHYS J B, V85, DOI 10.1140/epjb/e2012-21005-8; Bacry E., 2015, GEN ERROR BOUND SPAR; Bacry E, 2015, MARK MICROSTRUCT LIQ, V1, DOI 10.1142/S2382626615500057; Bacry E, 2016, IEEE T INFORM THEORY, V62, P2184, DOI 10.1109/TIT.2016.2533397; Bagnell J Andrew, 2015, LEARNING POSITIVE FU; Bhojanapalli Srinadh, 2016, P C LEARN THEOR, P530; Bochnak J., 2013, REAL ALGEBRAIC GEOME, V36; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bremaud P, 1996, ANN PROBAB, V24, P1563; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Eichler M, 2017, J TIME SER ANAL, V38, P225, DOI 10.1111/jtsa.12213; Etesami J, 2014, P AMER CONTR CONF, P2563, DOI 10.1109/ACC.2014.6859362; Etesami Jalal, 2016, C UNC ART INT; Flaxman S., 2017, INT C ART INT STAT; Hall EC, 2016, IEEE T INFORM THEORY, V62, P4327, DOI 10.1109/TIT.2016.2568202; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Kim S, 2014, P IEEE, V102, P683, DOI 10.1109/JPROC.2014.2307888; Kivinen J, 2004, IEEE T SIGNAL PROCES, V52, P2165, DOI 10.1109/TSP.2004.830991; Krumin M, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00147; Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497; Liniger T, 2009, THESIS; OZAKI T, 1979, ANN I STAT MATH, V31, P145, DOI 10.1007/BF02480272; Reynaud-Bouret P, 2010, ANN STAT, V38, P2781, DOI 10.1214/10-AOS806; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003; Xu H, 2016, INT C MACH LEARN, P1717; Yang Shuang-Hong, 2013, P 30 INT C MACH LEAR, V28; Zhou K, 2013, ICML	29	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405002
C	Bachem, O; Lucic, M; Hassani, SH; Krause, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bachem, Olivier; Lucic, Mario; Hassani, S. Hamed; Krause, Andreas			Fast and Provably Good Seedings for k-Means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PTAS	Seeding - the task of finding initial cluster centers - is critical in obtaining high-quality clusterings for k-Means. However, k-means++ seeding, the state of the art algorithm, does not scale well to massive datasets as it is inherently sequential and requires k full passes through the data. It was recently shown that Markov chain Monte Carlo sampling can be used to efficiently approximate the seeding step of k-means++. However, this result requires assumptions on the data generating distribution. We propose a simple yet fast seeding algorithm that produces provably good clusterings even without assumptions on the data. Our analysis shows that the algorithm allows for a favourable trade-off between solution quality and computational cost, speeding up k-means++ seeding by up to several orders of magnitude. We validate our theoretical results in extensive experiments on a variety of real-world data sets.	[Bachem, Olivier; Lucic, Mario; Hassani, S. Hamed; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Bachem, O (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	olivier.bachem@inf.ethz.ch; lucic@inf.ethz.ch; hamed@inf.ethz.ch; krausea@ethz.ch			ERC [StG 307036]; Google Ph.D. Fellowship; IBM Ph.D. Fellowship	ERC(European Research Council (ERC)European Commission); Google Ph.D. Fellowship(Google Incorporated); IBM Ph.D. Fellowship(International Business Machines (IBM))	This research was partially supported by ERC StG 307036, a Google Ph.D. Fellowship and an IBM Ph.D. Fellowship.	ACKERMANN MR, 2010, SWAT, V6139, P212; Aggarwal A, 2009, LECT NOTES COMPUT SC, V5687, P15, DOI 10.1007/978-3-642-03685-9_2; Ailon N., 2009, PROC 22 ADV NEURAL I, P10; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bachem Olivier, 2016, C ART INT AAAI FEBR; Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915; Boley D., 2013, P 13 SIAM INT C DAT, P476; Bottou L., 1994, ADV NEURAL INFORM PR, V7, P585; Brunsch T, 2011, LECT NOTES COMPUT SC, V6648, P344; Cai HY, 2000, STOCH ANAL APPL, V18, P63, DOI 10.1080/07362990008809654; Celebi ME, 2013, EXPERT SYST APPL, V40, P200, DOI 10.1016/j.eswa.2012.07.021; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Jaiswal R, 2015, INFORM PROCESS LETT, V115, P100, DOI 10.1016/j.ipl.2014.07.009; Jaiswal R, 2014, ALGORITHMICA, V70, P22, DOI 10.1007/s00453-013-9833-9; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; OSTROVSKY R, 2006, FOCS, P165; Sculley D., 2010, P 19 INT C WORLD WID, P1177, DOI [10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]; Zhao WZ, 2009, LECT NOTES COMPUT SC, V5931, P674, DOI 10.1007/978-3-642-10665-1_71	18	9	9	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704003
C	Chen, EYJ; Shen, Y; Choi, A; Darwiche, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Eunice Yuh-Jie; Shen, Yujia; Choi, Arthur; Darwiche, Adnan			Learning Bayesian networks with ancestral constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of learning Bayesian networks optimally, when subject to background knowledge in the form of ancestral constraints. Our approach is based on a recently proposed framework for optimal structure learning based on non-decomposable scores, which is general enough to accommodate ancestral constraints. The proposed framework exploits oracles for learning structures using decomposable scores, which cannot accommodate ancestral constraints since they are non-decomposable. We show how to empower these oracles by passing them decomposable constraints that they can handle, which are inferred from ancestral constraints that they cannot handle. Empirically, we demonstrate that our approach can be orders-of-magnitude more efficient than alternative frameworks, such as those based on integer linear programming.	[Chen, Eunice Yuh-Jie; Shen, Yujia; Choi, Arthur; Darwiche, Adnan] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Chen, EYJ (corresponding author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.	eyjchen@cs.ucla.edu; yujias@cs.ucla.edu; aychoi@cs.ucla.edu; darwiche@cs.ucla.edu			NSF [IIS-1514253]; ONR [N00014-15-1-2339]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This work was partially supported by NSF grant #IIS-1514253 and ONR grant #N00014-15-1-2339.	Bartlett M., 2015, ARTIFICIAL INTELLIGE; Borboudakis G., 2013, P 29 C UNC ART INT; Borboudakis G., 2012, P 29 INT C MACH LEAR; Chen E., 2015, P 4 IJCAI WORKSH GRA; Chen E. Y.-J., 2016, P 19 INT C ART INT S; Chen Y., 2014, P 28 C ART INT; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Cussens J., 2008, P 24 C UNC ART INT, P105; Cussens J, 2013, GENET EPIDEMIOL, V37, P69, DOI 10.1002/gepi.21686; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Jaakkola T, 2010, P 13 INT C ART INT S, P358; Koivisto M, 2004, J MACH LEARN RES, V5, P549; Koller D., 2009, PROBABILISTIC GRAPHI; Li CM, 2009, FRONT ARTIF INTEL AP, V185, P613, DOI 10.3233/978-1-58603-929-5-613; Malone B., 2014, P 28 C ART INT; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Parviainen P, 2013, J MACH LEARN RES, V14, P1387; Silander T., 2006, P 22 C UNC ART INT, P445; Singh A. P., 2005, FINDING OPTIMAL BAYE; Tian J., 2010, P 26 C ANN C UNC ART, P589; van Beek P, 2015, LECT NOTES COMPUT SC, V9255, P429, DOI 10.1007/978-3-319-23219-5_31; Yuan C., 2011, INT JOINT C ARTIFICI, P2186; Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039	23	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703022
C	Cutkosky, A; Boahen, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cutkosky, Ashok; Boahen, Kwabena			Online Convex Optimization with Unconstrained Domains and Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose an online convex optimization algorithm (RESCALEDEXP) that achieves optimal regret in the unconstrained setting without prior knowledge of any bounds on the loss functions. We prove a lower bound showing an exponential separation between the regret of existing algorithms that require a known bound on the loss functions and any algorithm that does not require such knowledge. RESCALEDEXP matches this lower bound asymptotically in the number of iterations. RESCALEDEXP is naturally hyperparameter-free and we demonstrate empirically that it matches prior optimization algorithms that require hyperparameter optimization.	[Cutkosky, Ashok] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Boahen, Kwabena] Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA	Stanford University; Stanford University	Cutkosky, A (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	ashokc@cs.stanford.edu; boahen@stanford.edu						[Anonymous], THESIS; Ben-Hur A., 2005, ADV NEURAL INFORM PR, V17, P545; Brendan McMahan H, 2014, ARXIV14033465; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chang Chih-Chung, 2001, P IJCNN IEEE CIT; Duarte MF, 2004, J PARALLEL DISTR COM, V64, P826, DOI 10.1016/j.jpdc.2004.03.020; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Kingma D.P, P 3 INT C LEARNING R; Kogan S, 2009, PROC HUMAN LANGUAGE, P272; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lichman M, 2013, UCI MACHINE LEARNING; Littlestone Nick, 2014, P 2 ANN WORKSH COMP, P269; McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724; Orabona F., 2013, ADV NEURAL INFORM PR, P1806; Orabona F, 2014, ADV NEUR IN, V27; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Orabona Francesco, 2016, C LEARN THEOR; Orabona Francesco, 2016, ARXIV160101974; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Streeter M., 2012, ADV NEURAL INFORM PR; Tewari, 2008, P 19 ANN C COMP LEAR, P1, DOI DOI 10.1007/11776420; Zeiler Matthew D, 2012, ARXIV12125701; Zinkevich, 2003, P 20 INT C MACH LEAR, P928	28	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703007
C	Yurtsever, A; Vu, BC; Cevher, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yurtsever, Alp; Bang Cong Vu; Cevher, Volkan			Stochastic Three-Composite Convex Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONVERGENCE	We propose a stochastic optimization method for the minimization of the sum of three convex functions, one of which has Lipschitz continuous gradient as well as restricted strong convexity. Our approach is most suitable in the setting where it is computationally advantageous to process smooth term in the decomposition with its stochastic gradient estimate and the other two functions separately with their proximal operators, such as doubly regularized empirical risk minimization problems. We prove the convergence characterization of the proposed algorithm in expectation under the standard assumptions for the stochastic gradient estimate of the smooth term. Our method operates in the primal space and can be considered as a stochastic extension of the three-operator splitting method. Numerical evidence supports the effectiveness of our method in real-world problems.	[Yurtsever, Alp; Bang Cong Vu; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Yurtsever, A (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.	alp.yurtsever@epfl.ch; bang.vu@epfl.ch; cevher@epfl.ch	Yurtsever, Alp/AAB-9053-2020		ERC; SNF [200021-146750, CRSII2-147633]; NCCR-Marvel	ERC(European Research Council (ERC)European Commission); SNF; NCCR-Marvel	This work was supported in part by ERC Future Proof, SNF 200021-146750, SNF CRSII2-147633, and NCCR-Marvel.	Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; Atchade Y. F., 2014, ARXIV14022365V2; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Borodin A, 2004, ADV NEUR IN, V16, P345; Briceno-Arias LM, 2015, OPTIMIZATION, V64, P1239, DOI 10.1080/02331934.2013.855210; Brodie J, 2009, P NATL ACAD SCI USA, V106, P12267, DOI 10.1073/pnas.0904287106; Cevher V., 2016, 215759 EPFL; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Combettes P. L., 2015, ARXIV150707095V1; Combettes PL, 2014, OPTIMIZATION, V63, P1289, DOI 10.1080/02331934.2012.733883; Davis D., 2015, ARXIV150401032V1; Devolder O., 2011, TECHNICAL REPORT; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Fama EF, 1996, J FINANC, V51, P55, DOI 10.2307/2329302; Hu C., 2009, ADV NEURAL INF PROCE, P781; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Lichman M, 2013, UCI MACHINE LEARNING; Lin QH, 2014, OPTIM METHOD SOFTW, V29, P1281, DOI 10.1080/10556788.2014.891592; Mosci S, 2010, LECT NOTES ARTIF INT, V6322, P418, DOI 10.1007/978-3-642-15883-4_27; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Raguet H, 2013, SIAM J IMAGING SCI, V6, P1199, DOI 10.1137/120872802; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rosasco L., 2014, ARXIV14035074V3; Vu BC, 2016, OPTIM LETT, V10, P781, DOI 10.1007/s11590-015-0904-5; Wang M., 2015, ARXIV151103760V1; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Zhong LW, 2014, JMLR WORKSH CONF PRO, V33, P1086	29	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700091
C	Asteris, M; Papailiopoulos, D; Dimakis, AG		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Asteris, Megasthenis; Papailiopoulos, Dimitris; Dimakis, Alexandros G.			Orthogonal NMF through Subspace Exploration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NONNEGATIVE MATRIX FACTORIZATION	Orthogonal Nonnegative Matrix Factorization (ONMF) aims to approximate a nonnegative matrix as the product of two k-dimensional nonnegative factors, one of which has orthonormal columns. It yields potentially useful data representations as superposition of disjoint parts, while it has been shown to work well for clustering tasks where traditional methods underperform. Existing algorithms rely mostly on heuristics, which despite their good empirical performance, lack provable performance guarantees. We present a new ONMF algorithm with provable approximation guarantees. For any constant dimension k, we obtain an additive EPTAS without any assumptions on the input. Our algorithm relies on a novel approximation to the related Nonnegative Principal Component Analysis (NNPCA) problem; given an arbitrary data matrix, NNPCA seeks k nonnegative components that jointly capture most of the variance. Our NNPCA algorithm is of independent interest and generalizes previous work that could only obtain guarantees for a single component. We evaluate our algorithms on several real and synthetic datasets and show that their performance matches or outperforms the state of the art.	[Asteris, Megasthenis; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA; [Papailiopoulos, Dimitris] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of Texas System; University of Texas Austin; University of California System; University of California Berkeley	Asteris, M (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	megas@utexas.edu; dimitrisp@berkeley.edu; dimakis@austin.utexas.edu	Dimakis, Alexandros G/A-5496-2011; Dimakis, Alexandros G/P-6034-2019	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	NSF [CCF-1217058, CCF-1116404, CCF 1344179, 1344364, 1407278, 1422549]; MURI AFOSR [556016]; ARO YIP [W911NF-14-1-0258]	NSF(National Science Foundation (NSF)); MURI AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI); ARO YIP	DP is generously supported by NSF awards CCF-1217058 and CCF-1116404 and MURI AFOSR grant 556016. This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549 and ARO YIP W911NF-14-1-0258.	[Anonymous], 2012, NIPS; [Anonymous], THESIS; Arora S., 2012, ARXIV12124777; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Asteris M, 2014, PR MACH LEARN RES, V32, P1728; BITTORF V., 2012, P ADV NEURAL INFORM, V25, P1223; Buchsbaum G, 2002, VISION RES, V42, P559, DOI 10.1016/S0042-6989(01)00303-0; Cai Liming, 2003, J COMPUTER SYSTEM SC; CAO B, 2007, IJCAI, P2689; Cesati M, 1997, INFORM PROCESS LETT, V64, P165, DOI 10.1016/S0020-0190(97)00164-6; Chen G, 2009, INFORM PROCESS MANAG, V45, P368, DOI 10.1016/j.ipm.2008.12.004; Choi S, 2008, IEEE IJCNN, P1828, DOI 10.1109/IJCNN.2008.4634046; Cichocki Andrzej, 2009, NONNEGATIVE MATRIX T, P2; Ding Chris, 2006, P 12 ACM SIGKDD INT, V2006, P126; Gillis N., 2013, IEEE T PATTERN ANAL; Gillis N., 2012, ARXIV12081237; Huang K, 2013, INT CONF ACOUST SPEE, P4524, DOI 10.1109/ICASSP.2013.6638516; Kuang D., 2012, PROC 2012 SIAM INT C, P106, DOI DOI 10.1137/1.9781611972825.10; Kumar A., 2013, P INT C MACH LEARN, P231; Lee DD, 2001, ADV NEUR IN, V13, P556; Li HL, 2007, J VLSI SIG PROC SYST, V48, P83, DOI 10.1007/s11265-006-0039-0; Li T, 2006, IEEE DATA MINING, P362; Li X., 2007, P 24 INT C MACH LEAR, P537; Lichman M., 2013, UCI MACHINE LEARNING; Lichman M, 2013, UCI MACHINE LEARNING; Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756; Pompili Filippo, 2012, ARXIV12010901; Pompili Filippo, 2013, ESANN 2013; Shahnaz F, 2006, INFORM PROCESS MANAG, V42, P373, DOI 10.1016/j.ipm.2004.11.005; Sigg C.D., 2008, P 25 INT C MACH LEAR, P960, DOI [DOI 10.1145/1390156.1390277, 10.1145/1390156.1390277]; Yang ZR, 2010, IEEE T NEURAL NETWOR, V21, P734, DOI 10.1109/TNN.2010.2041361; Yuan ZJ, 2005, LECT NOTES COMPUT SC, V3540, P333; Zass R., 2007, ADV NEURAL INFORM PR, P1561	33	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103036
C	El Alaoui, A; Mahoney, MW		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		El Alaoui, Ahmed; Mahoney, Michael W.			Fast Randomized Kernel Ridge Regression with Statistical Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MONTE-CARLO ALGORITHMS; APPROXIMATION	One approach to improving the running time of kernel-based methods is to build a small sketch of the kernel matrix and use it in lieu of the full matrix in the machine learning task of interest. Here, we describe a version of this approach that comes with running time guarantees as well as improved guarantees on its statistical performance. By extending the notion of statistical leverage scores to the setting of kernel ridge regression, we are able to identify a sampling distribution that reduces the size of the sketch (i.e., the required number of columns to be sampled) to the effective dimensionality of the problem. This latter quantity is often much smaller than previous bounds that depend on the maximal degrees of freedom. We give an empirical evidence supporting this fact. Our second contribution is to present a fast algorithm to quickly compute coarse approximations to these scores in time linear in the number of samples. More precisely, the running time of the algorithm is O (np(2)) with p only depending on the trace of the kernel matrix and the regularization parameter. This is obtained via a variant of squared length sampling that we adapt to the kernel setting. Lastly, we discuss how this new notion of the leverage of a data point captures a fine notion of the difficulty of the learning problem.	[El Alaoui, Ahmed] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Mahoney, Michael W.] Univ Calif Berkeley, Stat & Int Comp Sci Inst, Berkeley, CA 94720 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	El Alaoui, A (corresponding author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	elalaoui@eecs.berkeley.edu; mmahoney@stat.berkeley.edu						Bach F, 2010, ELECTRON J STAT, V4, P384, DOI 10.1214/09-EJS521; Bach Francis, 2013, C LEARNING THEORY, P185; Bach Francis, 2015, COMMUNICATION; Chatterjee S., 1986, STAT SCI, V1, P379, DOI [10.1214/ss/1177013622, DOI 10.1214/SS/1177013622]; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684; Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; El Alaoui Ahmed, 2014, ARXIV14110306; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Friedman J., 2001, ELEMENTS STAT LEARNI, V1; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Gittens A., 2013, INT C MACHINE LEARNI, P567; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Kumar S, 2009, P INT C ART INT STAT, P304; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Williams CKI, 2001, ADV NEUR IN, V13, P682; Zhang Yuchen, 2013, C LEARN THEOR, P592617	22	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100084
C	Kim, B; Shah, J; Doshi-Velez, F		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kim, Been; Shah, Julie; Doshi-Velez, Finale			Mind the Gap: A Generative Approach to Interpretable Feature Selection and Extraction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present the Mind the Gap Model (MGM), an approach for interpretable feature extraction and selection. By placing interpretability criteria directly into the model, we allow for the model to both optimize parameters related to interpretability and to directly report a global set of distinguishable dimensions to assist with further data exploration and hypothesis generation. MGM extracts distinguishing features on real-world datasets of animal features, recipes ingredients, and disease co-occurrence. It also maintains or improves performance when compared to related approaches. We perform a user study with domain experts to show the MGM's ability to help with dataset exploration.	[Kim, Been; Shah, Julie] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA	Massachusetts Institute of Technology (MIT); Harvard University	Kim, B (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	beenkim@csail.mit.edu; julie_a_shah@csail.mit.edu; finale@seas.harvard.edu						Agrawal R., 1998, SIGMOD Record, V27, P94, DOI 10.1145/276305.276314; Alelyani S., 2013, DATA CLUSTERING ALGO, V29; [Anonymous], 2012, JAMA-J AM MED ASSOC, V307, P1533; Batmanghelich Nematollah Kayhan, 2014, CORR; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Dash M, 2000, LECT NOTES ARTIF INT, V1805, P110; De'ath G, 2000, ECOLOGY, V81, P3178, DOI 10.1890/0012-9658(2000)081[3178:CARTAP]2.0.CO;2; Doshi-Velez F, 2014, PEDIATRICS, V133, pE54, DOI 10.1542/peds.2013-0819; Dy JG, 2004, J MACH LEARN RES, V5, P845; Eisenstein J., 2011, ICML; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fan W., 2012, P JMLR WORKSHOP C P, VVolume 25, P113; Freitas, 2014, ACM SIGKDD EXPLORATI; Guan Y., 2011, P 28 INT C MACH LEAR, P1073; Guerif S., 2008, FSDM; Kemp C., 2008, PNAS; Khoat Than, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P490, DOI 10.1007/978-3-642-33460-3_37; Kim B., 2014, NIPS; Kulesza A., 2010, NIPS; Kulesza A., 2012, THESIS; Lichman M., 2013, UCI MACHINE LEARNING; MILLER GA, 1956, PSYCHOL REV, V63, P81, DOI 10.1037/h0043158; Mitra P, 2002, IEEE T PATTERN ANAL, V24, P301, DOI 10.1109/34.990133; Tsuda K., 2003, NIPS; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Williamson S., 2010, ICML; Yu G, 2010, P 16 ACM SIGKDD INT, P763, DOI 10.1145/1835804.1835901; Zhang J, 1998, JAMA-J AM MED ASSOC, V280, P1690, DOI 10.1001/jama.280.19.1690; Zhu J, 2009, P 26 ANN INT C MACH, P1257; Zou H., 2004, J COMPUT GRAPH STAT, V15, P2006; Zou J., 2013, NIPS; Zou J. Y., 2012, NIPS	32	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102105
C	Ackerman, M; Dasgupta, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ackerman, Margareta; Dasgupta, Sanjoy			Incremental Clustering: The Case for Extra Clusters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The explosion in the amount of data available for analysis often necessitates a transition from batch to incremental clustering methods, which process one element at a time and typically store only a small subset of the data. In this paper, we initiate the formal analysis of incremental clustering methods focusing on the types of cluster structure that they are able to detect. We find that the incremental setting is strictly weaker than the batch model, proving that a fundamental class of cluster structures that can readily be detected in the batch setting is impossible to identify using any incremental method. Furthermore, we show how the limitations of incremental clustering can be overcome by allowing additional clusters.	[Ackerman, Margareta] Florida State Univ, 600 W Coll Ave, Tallahassee, FL 32306 USA; [Dasgupta, Sanjoy] Univ Calif San Diego, La Jolla, CA 92093 USA	State University System of Florida; Florida State University; University of California System; University of California San Diego	Ackerman, M (corresponding author), Florida State Univ, 600 W Coll Ave, Tallahassee, FL 32306 USA.	mackerman@fsu.edu; dasgupta@eng.ucsd.edu		Ackerman, Margareta/0000-0002-2804-4721				Ackerman M., 2013, P AISTATS 09 JMLR W, V31; Ackerman M., 2010, COLT; Ackerman M., 2009, P AISTATS 09 JMLR W, V5, P53; Ackerman M., 2010, NIPS; Ackerman M., 2012, P 26 AAAI C ART INT; Aggarwal C.C., 2013, SURVEY STREAM CLUSTE; Balcan M.-F., 2010, P 23 C LEARN THEOR C, P282; Balcan MF, 2008, ACM S THEORY COMPUT, P671; Epter S., 1999, INT C KNOWL DISC DAT, V7; Guha S, 2000, ANN IEEE SYMP FOUND, P359, DOI 10.1109/SFCS.2000.892124; HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840; Jardine N., 1971, MATH TAXONOMY; Kleinberg J., 2003, ADV NEURAL INFORM PR, P463; Knuth D. E., 1981, ART COMPUTER PROGRAM, V2; Kohonen T, 2001, SELF ORGANIZING MAPS; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; WARD JH, 1963, J AM STAT ASSOC, V58, P236, DOI 10.2307/2282967; Zadeh R.B., 2009, P 25 C UNCERTAINTY A, P639	19	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103059
C	Dasgupta, S; Kpotufe, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Dasgupta, Sanjoy; Kpotufe, Samory			Optimal rates for k-NN density and mode estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MEAN SHIFT	We present two related contributions of independent interest: (1) high-probability finite sample rates for k-NN density estimation, and (2) practical mode estimators-based on k-NN - which attain minimax-optimal rates under surprisingly general distributional conditions.	[Dasgupta, Sanjoy] Univ Calif San Diego, CSE, La Jolla, CA 92093 USA; [Kpotufe, Samory] Princeton Univ, ORFE, Princeton, NJ 08544 USA; [Kpotufe, Samory] TTI Chicago, Chicago, IL USA	University of California System; University of California San Diego; Princeton University	Dasgupta, S (corresponding author), Univ Calif San Diego, CSE, La Jolla, CA 92093 USA.	dasgupta@eng.ucsd.edu; samory@princeton.edu						Abraham C., 2004, ESAIM-PROBAB STAT, V8, P1; Arias-Castro Ery, 2013, ESTIMATION GRA UNPUB; Balakrishnan S., 2013, ADV NEURAL INFORM PR, P2679; Biau G, 2011, ELECTRON J STAT, V5, P204, DOI 10.1214/11-EJS606; Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169; Chaudhuri K., 2010, ADV NEURAL INFORM PR; Chaudhuri K., 2014, CONSISTENT PROCEDURE; Chazal F, 2013, J ACM, V60, DOI 10.1145/2535927; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; CHERNOFF H, 1964, ANN I STAT MATH, V16, P31, DOI 10.1007/BF02868560; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Devroye L., 1979, CAN J STAT, V7, P159, DOI [10.2307/3315115, DOI 10.2307/3315115]; DEVROYE LP, 1977, ANN STAT, V5, P536, DOI 10.1214/aos/1176343851; DONOHO DL, 1991, ANN STAT, V19, P668, DOI 10.1214/aos/1176348115; EDDY WF, 1980, ANN STAT, V8, P870, DOI 10.1214/aos/1176345080; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Genovese Christopher, 2013, ARXIV13127567; GRENANDER U, 1965, ANN MATH STAT, V36, P131, DOI 10.1214/aoms/1177700277; Grund B, 1995, ANN STAT, V23, P2264; Klemela J, 2005, J NONPARAMETR STAT, V17, P83, DOI 10.1080/10485250410001723151; Kpotufe S., 2011, INT C MACH LEARN; Li Jia, 2007, J MACHINE LEARNING R, V8; LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079; Moore D.S., 1976, TECHNICAL REPORT; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Tsybakov A. B., 1990, PROBL PEREDACHI INF, V26, P38	27	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101058
C	Patil, KR; Zhu, XJ; Kopec, L; Love, BC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Patil, Kaustubh Raosaheb; Zhu, Xiaojin; Kopec, Lukasz; Love, Bradley C.			Optimal Teaching for Limited-Capacity Human Learners	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLASSIFICATION; CATEGORIZATION; SIMILARITY	Basic decisions, such as judging a person as a friend or foe, involve categorizing novel stimuli. Recent work finds that people's category judgments are guided by a small set of examples that are retrieved from memory at decision time. This limited and stochastic retrieval places limits on human performance for probabilistic classification decisions. In light of this capacity limitation, recent work finds that idealizing training items, such that the saliency of ambiguous cases is reduced, improves human performance on novel test items. One shortcoming of previous work in idealization is that category distributions were idealized in an ad hoc or heuristic fashion. In this contribution, we take a first principles approach to constructing idealized training sets. We apply a machine teaching procedure to a cognitive model that is either limited capacity (as humans are) or unlimited capacity (as most machine learning systems are). As predicted, we find that the machine teacher recommends idealized training sets. We also find that human learners perform best when training recommendations from the machine teacher are based on a limited-capacity model. As predicted, to the extent that the learning model used by the machine teacher conforms to the true nature of human learners, the recommendations of the machine teacher prove effective. Our results provide a normative basis (given capacity constraints) for idealization procedures and offer a novel selection procedure for models of human learning.	[Patil, Kaustubh Raosaheb] UCL, Affect Brain Lab, London, England; [Patil, Kaustubh Raosaheb] MIT, Sloan Neuroecon Lab, Cambridge, MA 02139 USA; [Zhu, Xiaojin] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA; [Kopec, Lukasz; Love, Bradley C.] UCL, Expt Psychol, London, England	University of London; University College London; Massachusetts Institute of Technology (MIT); University of Wisconsin System; University of Wisconsin Madison; University of London; University College London	Patil, KR (corresponding author), UCL, Affect Brain Lab, London, England.	kaustubh.patil@gmail.com; jerryzhu@cs.wisc.edu; l.kopec.12@ucl.ac.uk; b.love@ucl.ac.uk			Leverhulme Trust [RPG-2014-075]; National Science Foundation [IIS-0953219]; WT-MIT fellowship [103811AIA]	Leverhulme Trust(Leverhulme Trust); National Science Foundation(National Science Foundation (NSF)); WT-MIT fellowship	The authors are thankful to the anonymous reviewers for their comments. This work is partly supported by the Leverhulme Trust grant RPG-2014-075 to BCL, National Science Foundation grant IIS-0953219 to XZ and WT-MIT fellowship 103811AIA to KRP.	Balbach FJ, 2009, LECT NOTES COMPUT SC, V5457, P1, DOI 10.1007/978-3-642-00982-2_1; Bengio Yoshua., 2009, P 26 ANN INT C MACHI, P41, DOI 10.1145/ 1553374.1553380; Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980; Cakmak M., 2012, AAAI C ART INT AAAI; Candela J., 2009, DATASET SHIFT MACHIN; Chen YH, 2009, J MACH LEARN RES, V10, P747; Crump MJC, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0057410; Giguere G, 2013, P NATL ACAD SCI USA, V110, P7613, DOI 10.1073/pnas.1219674110; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Hornsby AN, 2014, J APPL RES MEM COGN, V3, P72, DOI 10.1016/j.jarmac.2014.04.009; Jakel F, 2009, TRENDS COGN SCI, V13, P381, DOI 10.1016/j.tics.2009.06.002; Khan Faisal, 2011, ADV NEURAL INFORM PR, P1449; Lindsey R., 2013, PROC ADV NEUR PROCES, P2778; Love B.C., 2013, OXFORD HDB COGNITIVE, P342; Mack ML, 2013, CURR BIOL, V23, P2023, DOI 10.1016/j.cub.2013.08.035; MEDIN DL, 1978, PSYCHOL REV, V85, P207, DOI 10.1037/0033-295X.85.3.207; Nosofsky RM, 1997, PSYCHOL REV, V104, P266, DOI 10.1037/0033-295X.104.2.266; NOSOFSKY RM, 1986, J EXP PSYCHOL GEN, V115, P39, DOI 10.1037/0096-3445.115.1.39; Pashler H, 2013, J EXP PSYCHOL LEARN, V39, P1162, DOI 10.1037/a0031679; Shafto P, 2008, AAAI FALL S NAT INSP, P101; Zhu X., 2013, ADV NEURAL INFORM PR, P1905	21	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102102
C	Sun, X		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sun, Xu			Structure Regularization for Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					While there are many studies on weight regularization, the study on structure regularization is rare. Many existing systems on structured prediction focus on increasing the level of structural dependencies within the model. However, this trend could have been misdirected, because our study suggests that complex structures are actually harmful to generalization ability in structured prediction. To control structure-based over fitting, we propose a structure regularization framework via structure decomposition, which decomposes training samples into mini-samples with simpler structures, deriving a model with better generalization power. We show both theoretically and empirically that structure regularization can effectively control over fitting risk and lead to better accuracy. As a by-product, the proposed method can also substantially accelerate the training speed. The method and the theoretical results can apply to general graphical models with arbitrary structures. Experiments on well-known tasks demonstrate that our method can easily beat the benchmark systems on those highly-competitive tasks, achieving record-breaking accuracies yet with substantially faster training speed.	[Sun, Xu] Peking Univ, MOE Key Lab Computat Linguist, Beijing, Peoples R China; [Sun, Xu] Peking Univ, Sch Elect Engn & Comp Sci, Beijing, Peoples R China	Peking University; Peking University	Sun, X (corresponding author), Peking Univ, MOE Key Lab Computat Linguist, Beijing, Peoples R China.	xusun@pku.edu.cn			NSFC [61300063]	NSFC(National Natural Science Foundation of China (NSFC))	This work was supported in part by NSFC (No.61300063).	Argyriou A., 2007, P NIPS 07; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1; Gao Jianfeng, 2007, P 45 ANN M ASS COMP, P824; Graca J., 2009, ADV NEURAL INFORM PR, V22, P664; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; London B., 2007, NIPS WORKSH PERT OPT; Martins Andre<prime>F. T., 2011, P EMNLP; Quattoni A, 2009, P ICML 09, P108; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Schmidt  M., 2010, P 13 INT C ART INT S, P709; Shalev-Shwartz S., 2009, P COLT 09; Shen L., 2007, P ACL 07; Sun X., 2014, TECHNICAL REPORT ARX; Sun X, 2014, COMPUT LINGUIST, V40, P563, DOI [10.1162/COLI_a_00193, 10.1162/coli_a_00193]; Sun X, 2013, IEEE T KNOWL DATA EN, V25, P2551, DOI 10.1109/TKDE.2012.246; Sutton C., 2007, P 24 INT C MACH LEAN, P863; Taskar B., 2003, NIPS 03; Tsuruoka Y., 2011, C COMP NAT LANG LEAR; Xue H, 2011, IEEE T NEURAL NETWOR, V22, P573, DOI 10.1109/TNN.2011.2108315; Yoshida K., 2007, ACL WORKSH BIONLP; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	22	9	9	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103014
C	Bartlett, PL; Jordan, MI; McAuliffe, JD		Thrun, S; Saul, K; Scholkopf, B		Bartlett, PL; Jordan, MI; McAuliffe, JD			Large margin classifiers: convex loss, low noise, and convergence rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Many classification algorithms, including the support vector machine, boosting and logistic regression, can be viewed as minimum contrast methods that minimize a convex surrogate of the 0-1 loss function. We characterize the statistical consequences of using such a surrogate by providing a general quantitative relationship between the risk as assessed using the 0-1 loss and the risk as assessed using any nonnegative surrogate loss function. We show that this relationship gives nontrivial bounds under the weakest possible condition on the loss function-that it satisfy a pointwise form of Fisher consistency for classification. The relationship is based on a variational transformation of the loss function that is easy to compute in many applications. We also present a refined version of this result in the case of low noise. Finally, we present applications of our results to the estimation of convergence rates in the general setting of function classes that are scaled hulls of a finite-dimensional base class.	Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Bartlett, PL (corresponding author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.		Jordan, Michael I/C-5253-2013	Bartlett, Peter/0000-0002-8760-3140				BARLETT PL, 2003, 638 UC BERK DEP STAT; Boyd S., 2003, CONVEX OPTIMIZATION; JIANG W, 2003, IN PRESS ANN STAT; Lee WS, 1996, IEEE T INFORM THEORY, V42, P2118, DOI 10.1109/18.556601; Lin Y., 2001, 1044R U WISC DEP STA; LUGOSI G, 2003, IN PRESS ANN STAT; MANNOR S, 2002, P ANN C COMP LEARN T, P319; STENWART I, 2002, 0203 U JEN DEP MACH; TSYBAKOV A, 2001, PMA682 U PAR 6; ZHANG T, 2003, IN PRESS ANN STAT	11	9	9	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1173	1180						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500146
C	Garcez, ASD; Lamba, LC		Thrun, S; Saul, K; Scholkopf, B		Garcez, ASD; Lamba, LC			Reasoning about time and knowledge in neural-symbolic learning systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				LOGIC; SEMANTICS	We show that temporal logic and combinations of temporal logics and modal logics of knowledge can be effectively represented in artificial neural networks. We present a Translation Algorithm from temporal rules to neural networks, and show that the networks compute a fixed-point semantics of the rules. We also apply the translation to the muddy children puzzle, which has been used as a testbed for distributed multi-agent systems. We provide a complete solution to the puzzle with the use of simple neural networks, capable of reasoning about time and of knowledge acquisition through inductive learning.	City Univ London, Dept Comp, London EC1V 0HB, England	City University London	Garcez, ASD (corresponding author), City Univ London, Dept Comp, London EC1V 0HB, England.	aag@soi.city.ac.uk; lamb@inf.ufrgs.br	Lamb, Luis C./AAG-9576-2021	Lamb, Luis C./0000-0003-1571-165X				CLOETE L, 2000, KNOWLEDGE BASED NEUR; Fagin Ronald, 1995, REASONING KNOWLEDGE; Garcez Artur S., 2002, PERSP NEURAL COMP; Garcez ASA, 1999, APPL INTELL, V11, P59, DOI 10.1023/A:1008328630915; Garcez ASD, 2002, ICONIP'02: PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON NEURAL INFORMATION PROCESSING, P1992, DOI 10.1109/ICONIP.2002.1199022; Garcez ASD, 2001, ARTIF INTELL, V125, P155, DOI 10.1016/S0004-3702(00)00077-1; GARCEZ ASD, 2003, IN PRESS P 16 INT FL; GARCEZ ASD, 2002, 20026 DEP COMP IMP C; HALPERN JY, 1986, J COMPUTER SYSTEM SC, V38, P195; HALPERN JY, 2003, IN PRESS SIAM J COMP; Holldobler, 1993, THESIS TH DARMSTADT; Holldobler S, 1999, APPL INTELL, V11, P45, DOI 10.1023/A:1008376514077; Holldobler S, 1994, P ECAI94 WORKSH COMB, P68; HUTH MRA, 2000, LOGIC COMPUTER SCI M; Lloyd J. W., 1987, FDN LOGIC PROGRAMMIN, V2nd; PAZZANI M, 1992, MACH LEARN, V9, P57, DOI 10.1023/A:1022628829777; Rao AS, 1998, J LOGIC COMPUT, V8, P293, DOI 10.1093/logcom/8.3.293; TOWELL GG, 1994, ARTIF INTELL, V70, P119, DOI 10.1016/0004-3702(94)90105-8; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; VANEMDEN MH, 1976, J ACM, V23, P733, DOI 10.1145/321978.321991	20	9	9	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						921	928						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500115
C	Girolami, M; Kaban, A		Thrun, S; Saul, K; Scholkopf, B		Girolami, M; Kaban, A			Simplicial mixtures of Markov chains: Distributed modelling of dynamic user profiles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					To provide a compact generative representation of the sequential activity of a number of individuals within a group there is a tradeoff between the definition of individual specific and global models. This paper proposes a linear-time distributed model for finite state symbolic sequences representing traces of individual user activity by making the assumption that heterogeneous user behavior may be 'explained' by a relatively small number of common structurally simple behavioral patterns which may interleave randomly in a user-specific proportion. The results of an empirical study on three different sources of user traces indicates that this modelling approach provides an efficient representation scheme, reflected by improved prediction performance as well as providing low-complexity and intuitively interpretable representations.	Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland	University of Glasgow	Girolami, M (corresponding author), Univ Glasgow, Dept Comp Sci, Glasgow G12 8QQ, Lanark, Scotland.	girolami@dcs.gla.ac.uk; a.kaban@cs.bham.ac.uk		Girolami, Mark/0000-0003-3008-253X				Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; CADEZ I, IN PRESS J DATA MINI; FRYDMAN H, 1984, J AM STAT ASSOC, V79, P632, DOI 10.2307/2288410; GIROLAMI M, 2003, P 26 ANN INT ACM SIG, P433; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; Lappalainen H, 2000, PERSP NEURAL COMP, P75; Lee DD, 2001, ADV NEUR IN, V13, P556; Minka T. P., 2002, P 18 C UNC ART INT U; Ronning G., 1989, J STAT COMPUT SIMUL, V32, P215, DOI DOI 10.1080/00949658908811178; ROSS DA, 2003, ADV NEURAL INFORMATI, P15	10	9	9	0	4	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						9	16						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500002
C	Heskes, T; Zoeter, O; Wiegerinck, W		Thrun, S; Saul, K; Scholkopf, B		Heskes, T; Zoeter, O; Wiegerinck, W			Approximate expectation maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We discuss the integration of the expectation-maximization (EM) algorithm for maximum likelihood learning of Bayesian networks with belief propagation algorithms for approximate inference. Specifically we propose to combine the outer-loop step of convergent belief propagation algorithms with the M-step of the EM algorithm. This then yields an approximate EM algorithm that is essentially still double loop, with the important advantage of an inner loop that is guaranteed to converge. Simulations illustrate the merits of such an approach.	Univ Nijmegen, SNN, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Heskes, T (corresponding author), Univ Nijmegen, SNN, Geert Grooteplein 21, NL-6525 EZ Nijmegen, Netherlands.		Heskes, Tom/A-1443-2010	Heskes, Tom/0000-0002-3398-5235				DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Frey BJ, 2001, ADV NEUR IN, V13, P486; HESKES T, 2003, UAI, P313; Minka T., 2002, P 18 C UNCERTAINTY A, P352; Minka T.P., 2001, P 17 C UNC ART INT, P362; Murphy K., 2001, P 17 C UNC ART INT, P378; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; SALAKHUTDINOV R, 2003, INT C MACH LEARN, P664; TEH Y, 2003, AISTATS 2003; WAINWRIGHT M, 2003, AISTATS 2003; YEDIDIA J, 2002, CONSTRUCTING FREE EN; YEH Y, 2002, NIPS, V14; Yuille AL, 2002, NEURAL COMPUT, V14, P1691, DOI 10.1162/08997660260028674	14	9	9	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						353	360						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500045
C	Minka, T; Qi, Y		Thrun, S; Saul, K; Scholkopf, B		Minka, T; Qi, Y			Tree-structured approximations by expectation propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Approximation structure plays an important role in inference on loopy graphs. As a tractable structure, tree approximations have been utilized in the variational method of Ghahramani & Jordan (1997) and the sequential projection method of Frey et al. (2000). However, belief propagation represents each factor of the graph with a product of single-node messages. In this paper, belief propagation is extended to represent factors with tree approximations, by way of the expectation propagation framework. That is, each factor sends a "message" to all pairs of nodes in a tree structure. The result is more accurate inferences and more frequent convergence than ordinary belief propagation, at a lower cost than variational trees or double-loop algorithms.	Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Minka, T (corresponding author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.							CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; FREY BJ, 2000, NIPS, V13; HESKES T, 2002, P UAI; Jensen F. V., 1990, Computational Statistics Quarterly, V5, P269; KAPPEN HJ, 2001, NIPS 14; Minka T.P., 2001, P 17 C UNC ART INT, P362; MINKA TP, 2001, THESIS LIT; MURPHY KP, 2001, COMPUTING SCI STAT, V33; TEH YW, 2001, NIPS, V14; WAINWRIGHT M, 2001, NIPS, V14; WAINWRIGHT MJ, 2002, P UAI; WELLING M, 2000, UAI; WIEGERINCK W, 2000, P UAI; YEDIDIA J, 2002, CONSTRUCTING FREE EN; YEDIDIA J, 2000, NIPS, V13; YUILLE A, 2002, IN PRESS NEURAL COMP	17	9	9	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						193	200						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500025
