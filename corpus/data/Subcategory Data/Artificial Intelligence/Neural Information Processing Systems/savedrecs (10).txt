PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Cesa-Bianchi, N; Gentile, C; Lugosi, G; Neu, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cesa-Bianchi, Nicole; Gentile, Claudio; Lugosi, Gabor; Neu, Gergely			Boltzmann Exploration Done Right	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				BOUNDS	Boltzmann exploration is a classic strategy for sequential decision-making under uncertainty, and is one of the most standard tools in Reinforcement Learning (RL). Despite its widespread use, there is virtually no theoretical understanding about the limitations or the actual benefits of this exploration scheme. Does it drive exploration in a meaningful way? Is it prone to misidentifying the optimal actions or spending too much time exploring the suboptimal ones? What is the right tuning for the learning rate? In this paper, we address several of these questions for the classic setup of stochastic multi-armed bandits. One of our main results is showing that the Boltzmann exploration strategy with any monotone learning-rate sequence will induce suboptimal behavior. As a remedy, we offer a simple non-monotone schedule that guarantees near-optimal performance, albeit only when given prior access to key problem parameters that are typically not available in practical situations (like the time horizon T and the suboptimality gap Delta). More importantly, we propose a novel variant that uses different learning rates for different arms, and achieves a distribution-dependent regret bound of order K log(2) T/Delta and a distribution- independent bound of order root KT log K without requiring such prior knowledge. To demonstrate the flexibility of our technique, we also propose a variant that guarantees the same performance bounds even if the rewards are heavy-tailed.	[Cesa-Bianchi, Nicole] Univ Milan, Milan, Italy; [Gentile, Claudio] INRIA Lille Nord Europe, Villeneuve Dascq, France; [Lugosi, Gabor] ICREA, Barcelona, Spain; [Lugosi, Gabor; Neu, Gergely] Univ Pompeu Fabra, Barcelona, Spain	University of Milan; ICREA; Pompeu Fabra University	Cesa-Bianchi, N (corresponding author), Univ Milan, Milan, Italy.	nicolo.cesa-bianchi@unimi.it; cla.gentile@gmail.com; gabor.lugosi@gmail.com; gergely.neu@gmail.com	Cesa-Bianchi, Nicolò/C-3721-2013; Jeong, Yongwook/N-7413-2016	Cesa-Bianchi, Nicolò/0000-0001-8477-4748; 	Spanish Ministry of Economy and Competitiveness [MTM2015-67304-P]; FEDER, EU; UPFellows Fellowship (Marie Curie COFUND program) [600387]	Spanish Ministry of Economy and Competitiveness(Spanish Government); FEDER, EU(European Commission); UPFellows Fellowship (Marie Curie COFUND program)	Gabor Lugosi was supported by the Spanish Ministry of Economy and Competitiveness, Grant MTM2015-67304-P and FEDER, EU. Gergely Neu was supported by the UPFellows Fellowship (Marie Curie COFUND program no 600387).	Abernethy J. D., 2014, COLT, P807; Agrawal S., 2013, ARTIF INTELL, P99; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Audibert J. -Y., 2009, COLT 2009; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6; Bubeck S, 2012, REGRET ANAL STOCHAST; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Cesa-Bianchi N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P100; Garivier A., 2016, NIPS; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Kuleshov V., 2014, ARXIV14026028; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Osband I., 2016, GEN EXPLORATION VIA; Perkins T. J., 2003, ADV NEURAL INFORM PR, P1595; Seldin Y, 2014, PR MACH LEARN RES, V32, P1287; Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szepesvari C., 2010, SYNTHESIS LECT ARTIF; Vermorel J, 2005, LECT NOTES ARTIF INT, V3720, P437, DOI 10.1007/11564096_42	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406035
C	Chao, P; Zhu, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chao, Pan; Zhu, Michael			Group Additive Structure Identification for Kernel Nonparametric Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The additive model is one of the most popularly used models for high dimensional nonparametric regression analysis. However, its main drawback is that it neglects possible interactions between predictor variables. In this paper, we reexamine the group additive model proposed in the literature, and rigorously define the intrinsic group additive structure for the relationship between the response variable Y and the predictor vector X, and further develop an effective structure-penalized kernel method for simultaneous identification of the intrinsic group additive structure and nonparametric function estimation. The method utilizes a novel complexity measure we derive for group additive structures. We show that the proposed method is consistent in identifying the intrinsic group additive structure. Simulation study and real data applications demonstrate the effectiveness of the proposed method as a general tool for high dimensional nonparametric regression.	[Chao, Pan; Zhu, Michael] Purdue Univ, Dept Stat, W Lafayette, IN 47906 USA; [Zhu, Michael] Tsinghua Univ, Dept Ind Engn, Ctr Stat Sci, Beijing, Peoples R China	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Tsinghua University	Chao, P (corresponding author), Purdue Univ, Dept Stat, W Lafayette, IN 47906 USA.	panchao25@gmail.com; yuzhu@purdue.edu	Jeong, Yongwook/N-7413-2016					Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bishop C.M, 2006, PATTERN RECOGN; Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503; Chao P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P905, DOI 10.1145/2783258.2783327; Cucker F, 2002, B AM MATH SOC, V39, P1; Gu C, 2013, SPRINGER SER STAT, V297, P1, DOI 10.1007/978-1-4614-5369-7; Hastie T.J., 1990, GEN ADDITIVE MODELS; Kandasamy K, 2016, PR MACH LEARN RES, V48; Koller D., 2009, PROBABILISTIC GRAPHI; Kuhn T, 2011, J COMPLEXITY, V27, P489, DOI 10.1016/j.jco.2011.01.005; Marlin B.M., 2009, P 26 INT C MACH LEAR, P705; Ramsay JO, 2002, APPL FUNCTIONAL DATA, V77; Smola Alex J, 1998, LEARNING KERNELS, V4; Steinwart I., 2008, SUPPORT VECTOR MACHI; STONE CJ, 1982, ANN STAT, V10, P1040, DOI 10.1214/aos/1176345969; Vapnik V., 2013, NATURE STAT LEARNING; Zhou DX, 2002, J COMPLEXITY, V18, P739, DOI 10.1006/jcom.2002.0635	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404095
C	Chatterji, NS; Bartlett, PL		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chatterji, Niladri S.; Bartlett, Peter L.			Alternating minimization for dictionary learning with random initialization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SPARSE; REPRESENTATIONS; ALGORITHM; SELECTION	We present theoretical guarantees for an alternating minimization algorithm for the dictionary learning/sparse coding problem. The dictionary learning problem is to factorize vector samples y(1), y(2),..., y(n) into an appropriate basis (dictionary) A* and sparse vectors x(1)*,..., x(n)*. Our algorithm is a simple alternating minimization procedure that switches between l(1 )minimization and gradient descent in alternate steps. Dictionary learning and specifically alternating minimization algorithms for dictionary learning are well studied both theoretically and empirically. However, in contrast to previous theoretical analyses for this problem, we replace a condition on the operator norm (that is, the largest magnitude singular value) of the true underlying dictionary A* with a condition on the matrix infinity norm (that is, the largest magnitude term). This not only allows us to get convergence rates for the error of the estimated dictionary measured in the matrix infinity norm, but also ensures that a random initialization will provably converge to the global optimum. Our guarantees are under a reasonable generative model that allows for dictionaries with growing operator norms, and can handle an arbitrary level of over completeness, while having sparsity that is information theoretically optimal We also establish upper bounds on the sample complexity of our algorithm.	[Chatterji, Niladri S.; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Chatterji, NS (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	niladri.chatterji@berkeley.edu; peter@berkeley.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1619362]; Australian Research Council [FL110100281]; Australian Research Council through ARC Centre of Excellence for Mathematical and Statistical Frontiers	NSF(National Science Foundation (NSF)); Australian Research Council(Australian Research Council); Australian Research Council through ARC Centre of Excellence for Mathematical and Statistical Frontiers(Australian Research Council)	We gratefully acknowledge the support of the NSF through grant IIS-1619362, and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the ARC Centre of Excellence for Mathematical and Statistical Frontiers. Thanks also to the Simons Institute for the Theory of Computing Spring 2017 Program on Foundations of Machine Learning. The authors would like to thank Sahand Negahban for pointing out an error in the mu-incoherence assumption in an earlier version.	Agarwal A., 2013, STAT-US, V1050, P839; Agarwal A., 2014, C LEARN THEOR, P123; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; [Anonymous], 2015, PROC C LEARN THEORY; Arora S., 2013, NEW ALGORITHMS LEARN; Balakrishnan S, 2017, ANN STAT, V45, P77, DOI 10.1214/16-AOS1435; Barak B, 2015, ACM S THEORY COMPUT, P143, DOI 10.1145/2746539.2746605; Belloni A., 2014, ARXIV14127216; Belloni Alexandre, 2016, J ROYAL STAT SOC B; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Chen Y., 2013, NOISY MISSING DATA R; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; Engan K, 1999, INT CONF ACOUST SPEE, P2443, DOI 10.1109/ICASSP.1999.760624; Fuchs J.-J., 2004, AC SPEECH SIGN PROC, V2, pii; Gautier E., 2011, ARXIV11052454; Gribonval R, 2003, IEEE T INFORM THEORY, V49, P3320, DOI 10.1109/TIT.2003.820031; Gribonval R, 2015, IEEE T INFORM THEORY, V61, P6298, DOI 10.1109/TIT.2015.2472522; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Hazan E, 2016, ADV NEURAL INFORM PR, P3306; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Lewicki MS, 2000, NEURAL COMPUT, V12, P337, DOI 10.1162/089976600300015826; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Netrapalli P., 2014, P ADV NEUR INF PROC, P1107; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Rosenbaum M., 2013, PROBABILITY STAT BAC, P276; Rosenbaum M, 2010, ANN STAT, V38, P2620, DOI 10.1214/10-AOS793; Spielman D. A, 2012, C LEARN THEOR, P37; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Tropp JA, 2006, IEEE T INFORM THEORY, V52, P1030, DOI 10.1109/TIT.2005.864420; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1; Wu S., 2015, ARXIV150504363; Yu B., 1997, FESTSCHRIFT L LECAM, P423; Zhao P, 2006, J MACH LEARN RES, V7, P2541	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402005
C	Chen, JS; Wang, C; Xiao, L; He, J; Li, LH; Deng, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Jianshu; Wang, Chong; Xiao, Lin; He, Ji; Li, Lihong; Deng, Li			Q-LDA: Uncovering Latent Patterns in Text-based Sequential Decision Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In sequential decision making, it is often important and useful for end users to understand the underlying patterns or causes that lead to the corresponding decisions. However, typical deep reinforcement learning algorithms seldom provide such information due to their black-box nature. In this paper, we present a probabilistic model, Q-LDA, to uncover latent patterns in text-based sequential decision processes. The model can be understood as a variant of latent topic models that are tailored to maximize total rewards; we further draw an interesting connection between an approximate maximum-likelihood estimation of Q-LDA and the celebrated Q-learning algorithm. We demonstrate in the text-game domain that our proposed method not only provides a viable mechanism to uncover latent patterns in decision processes, but also obtains state-of-the-art rewards in these games.	[Chen, Jianshu; Wang, Chong; Xiao, Lin; He, Ji; Li, Lihong; Deng, Li] Microsoft Res, Redmond, WA 98052 USA; [Wang, Chong; Li, Lihong] Google Inc, Kirkland, WA USA; [He, Ji; Deng, Li] Citadel LLC, Seattle, WA USA; [He, Ji; Deng, Li] Citadel LLC, Chicago, IL USA	Microsoft; Google Incorporated	Chen, JS (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	jianshuc@microsoft.com; chongw@google.com; lin.xiao@microsoft.com; Ji.He@citadel.com; lihong@google.com; Li.Deng@citadel.com						Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Bernardo J. M., 2007, BAYESIAN STAT, V8, P3, DOI DOI 10.1007/978-3-642-93220-5_6; Blei D.M., 2007, P 20 INT C NEURAL IN, P121; Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blei DM, 2006, INT C MACH LEARN ICM, V148, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859]; Bouchard Guillaume, 2004, 16 IASC INT S COMP S, P721; Chen J., 2015, ADV NEURAL INFORM PR, P1765; Engel Y., 2005, P 22 INT C MACH LEAR, P201, DOI DOI 10.1145/1102351.1102377; Hausknecht Matthew, 2015, P AAAI SDMIA NOV; He Ji, 2016, P ACL; Holub A, 2005, PROC CVPR IEEE, P664; Jordan MI, 1998, NATO ADV SCI I D-BEH, V89, P105; Kapadia S., 1998, THESIS; Levine S, 2016, J MACH LEARN RES, V17; Lin L.J, 1993, P TECHN REP DTIC DOC, DOI 10.5555/168871; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Narasimhan Karthik, 2015, P EMNLP; Saul L, 1998, NATO ADV SCI I D-BEH, V89, P541; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Yakhnenko Oksana, 2005, P IEEE ICDM	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405006
C	Chen, L; Krause, A; Karbasi, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Lin; Krause, Andreas; Karbasi, Amin			Interactive Submodular Bandit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In many machine learning applications, submodular functions have been used as a model for evaluating the utility or payoff of a set such as news items to recommend, sensors to deploy in a terrain, nodes to influence in a social network, to name a few. At the heart of all these applications is the assumption that the underlying utility/payoff function is known a priori, hence maximizing it is in principle possible. In many real life situations, however, the utility function is not fully known in advance and can only be estimated via interactions. For instance, whether a user likes a movie or not can be reliably evaluated only after it was shown to her. Or, the range of influence of a user in a social network can be estimated only after she is selected to advertise the product. We model such problems as an interactive submodular bandit optimization, where in each round we receive a context (e.g., previously selected movies) and have to choose an action (e.g., propose a new movie). We then receive a noisy feedback about the utility of the action (e.g., ratings) which we model as a submodular function over the context-action space. We develop SM-UCB that efficiently trades off exploration (collecting more data) and exploration (proposing a good action given gathered data) and achieves a O(root T) regret bound after T rounds of interaction. More specifically, given a bounded-RKHS norm kernel over the context-action-payoff space that governs the smoothness of the utility function, SM-UCB keeps an upper-confidence bound on the payoff function that allows it to asymptotically achieve no-regret. Finally, we evaluate our results on four concrete applications, including movie recommendation (on the MovieLense data set), news recommendation (on Yahoo! Webscope dataset), interactive influence maximization (on a subset of the Facebook network), and personalized data summarization (on Reuters Corpus). In all these applications, we observe that SM-UCB consistently outperforms the prior art.	[Chen, Lin; Karbasi, Amin] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA; [Chen, Lin; Karbasi, Amin] Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA; [Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Yale University; Yale University; Swiss Federal Institutes of Technology Domain; ETH Zurich	Chen, L (corresponding author), Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.; Chen, L (corresponding author), Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA.	lin.chen@yale.edu; krausea@ethz.ch; amin.karbasi@yale.edu	Jeong, Yongwook/N-7413-2016; Chen, Lin/CAH-1961-2022	Chen, Lin/0000-0003-0349-6577	DARPA Young Faculty Award [D16AP00046]; ERC StG; SCADAPT	DARPA Young Faculty Award; ERC StG; SCADAPT	This research was supported by DARPA Young Faculty Award (D16AP00046), grant SCADAPT and ERC StG.	Ashtiani H., 2016, ADV NEURAL INFORM PR, P3216; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Awasthi P, 2014, PR MACH LEARN RES, V32, P550; Badanidiyuru A, 2016, P ANN ACM SIAM S DIS, P414; Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Barrenetxea G, 2008, SENSYS'08: PROCEEDINGS OF THE 6TH ACM CONFERENCE ON EMBEDDED NETWORKED SENSOR SYSTEMS, P43; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chen L, 2017, AAAI CONF ARTIF INTE, P1798; Chen W., 2016, JMLR, V17, P1746; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Cohn D. A., 1995, Advances in Neural Information Processing Systems 7, P705; Dasgupta S., 2008, ADV NEURAL INFORM PR, P353; Dasgupta S, 2005, ADV NEURAL INFORM PR, P337; El-Arini K, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P289; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Gartner T, 2008, SER MACH PERCEPT ART, V72, P1; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gomez-Rodriguez Manuel, 2016, ACM TOIS; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Guillory A., 2010, ARXIV10023345; Hassidim A., 2017, COLT, P1069; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Huang RZ, 2009, DATA KNOWL ENG, V68, P49, DOI 10.1016/j.datak.2008.08.008; Javdani S, 2014, JMLR WORKSH CONF PRO, V33, P430; Kahraman HT, 2013, KNOWL-BASED SYST, V37, P283, DOI 10.1016/j.knosys.2012.08.009; Karbasi A., 2012, P 29 INT C MACH LEAR, P855; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Krause A., 2012, TRACTABILITY PRACT A, V3, P8; Krause A, 2008, J MACH LEARN RES, V9, P235; Krause A, 2006, IPSN 2006: THE FIFTH INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P2; Lei SY, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P645, DOI 10.1145/2783258.2783271; Leskovec J., 2012, P 25 INT C NEUR INF, P539, DOI DOI 10.1109/ICDM.2012.159; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Lin T, 2015, ADV NEUR IN, V28; LU T., 2010, PROC 30 INT C ARTIF, P485; Mirzasoleiman B., 2016, ADV NEURAL INFORM PR, P3601; Mirzasoleiman B, 2016, PR MACH LEARN RES, V48; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Seeman L, 2013, ANN IEEE SYMP FOUND, P459, DOI 10.1109/FOCS.2013.56; Settles Burr, 2010, ACTIVE LEARNING LIT, P11, DOI DOI 10.1111/J.1467-7687.2012.01135.X; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Streeter M., 2009, P ADV NEUR INF PROC, P1577; Tong S, 2002, J MACH LEARN RES, V2, P45, DOI 10.1162/153244302760185243; Vanchinathan Hastagiri, 2015, ACM SIGKDD; Wang S I, 2016, ASS COMPUTATIONAL LI; Wang Y., 2009, ADV NEURAL INFORM PR, P1729; Yue Y., 2011, ADV NEURAL INFORM PR, P2483; Zhang Y, 2016, PROCEEDINGS OF THE 2016 IEEE INTERNATIONAL SYMPOSIUM ON HARDWARE ORIENTED SECURITY AND TRUST (HOST), P1, DOI 10.1109/HST.2016.7495547; Zhou Jiaji, 2013, INF WORKSH ICML CIT	52	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400014
C	Chen, SX; Ma, SQ; Liu, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Shixiang; Ma, Shiqian; Liu, Wei			Geometric Descent Method for Convex Composite Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMIZATION	In this paper, we extend the geometric descent method recently proposed by Bubeck, Lee and Singh [1] to tackle nonsmooth and strongly convex composite problems. We prove that our proposed algorithm, dubbed geometric proximal gradient method (GeoPG), converges with a linear rate (1 - 1 root kappa) and thus achieves the optimal rate among first-order methods, where i is the condition number of the problem. Numerical results on linear regression and logistic regression with elastic net regularization show that GeoPG compares favorably with Nesterov's accelerated proximal gradient method, especially when the problem is ill-conditioned.	[Chen, Shixiang] Chinese Univ Hong Kong, Dept SEEM, Hong Kong, Peoples R China; [Ma, Shiqian] Univ Calif Davis, Dept Math, Davis, CA USA; [Liu, Wei] Tencent AI Lab, Beijing, Peoples R China	Chinese University of Hong Kong; University of California System; University of California Davis; Tencent	Chen, SX (corresponding author), Chinese Univ Hong Kong, Dept SEEM, Hong Kong, Peoples R China.		Jeong, Yongwook/N-7413-2016; Chen, Shixiang/AGY-6455-2022; Chen, Shixiang/ABA-8539-2020	Chen, Shixiang/0000-0002-3261-0714; Liu, Wei/0000-0002-3865-8145	CUHK Research Postgraduate Student Grant for Overseas Academic Activities	CUHK Research Postgraduate Student Grant for Overseas Academic Activities	Shixiang Chen is supported by CUHK Research Postgraduate Student Grant for Overseas Academic Activities. Shiqian Ma is supported by a startup funding in UC Davis.	Attouch H., 2016, MATH PROGRAMMING; Beck A, 2007, J GLOBAL OPTIM, V39, P113, DOI 10.1007/s10898-006-9127-8; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; BLAND RG, 1981, OPER RES, V29, P1039, DOI 10.1287/opre.29.6.1039; Brent R.P., 1973, ALGORITHMS MINIMIZAT; Bubeck S., 2015, ARXIV150608187MATHOC; Bubeck S., 2016, ICML; Dekker TJ., 1969, CONSTRUCTIVE ASPECTS; Drusvyatskiy D., 2016, SIAM J OPTIMIZATION; Eldar YC, 2008, IEEE T SIGNAL PROCES, V56, P1388, DOI 10.1109/TSP.2007.908945; Gerdts M, 2017, J IND MANAG OPTIM, V13, P47, DOI 10.3934/jimo.2016003; Hans E, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/2/025005; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Scheinberg K, 2014, FOUND COMPUT MATH, V14, P389, DOI 10.1007/s10208-014-9189-9; Su Weijie, 2014, NIPS; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400061
C	Choi, A; Shen, YJ; Darwiche, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Choi, Arthur; Shen, Yujia; Darwiche, Adnan			Tractability in Structured Probability Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recently, the Probabilistic Sentential Decision Diagram (PSDD) has been proposed as a framework for systematically inducing and learning distributions over structured objects, including combinatorial objects such as permutations and rankings, paths and matchings on a graph, etc. In this paper, we study the scalability of such models in the context of representing and learning distributions over routes on a map. In particular, we introduce the notion of a hierarchical route distribution and show how they can be leveraged to construct tractable PSDDs over route distributions, allowing them to scale to larger maps. We illustrate the utility of our model empirically, in a route prediction task, showing how accuracy can be increased significantly compared to Markov models.	[Choi, Arthur; Shen, Yujia; Darwiche, Adnan] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Choi, A (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.	aychoi@cs.ucla.edu; yujias@cs.ucla.edu; darwiche@cs.ucla.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1514253]; ONR [N00014-15-1-2339]; DARPA XAI [N66001-17-2-4032]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA XAI	We greatly thank Noah Hadfield-Menell and Andy Shih for their contributions, and Eunice Chen for helpful discussions. This work has been partially supported by NSF grant #IIS-1514253, ONR grant #N00014-15-1-2339 and DARPA XAI grant #N66001-17-2-4032.	Choi A., 2015, P IJCAI; Choi A., 2016, P 30 AAAI C ART INT; Darwiche A., 2011, P 22 INT JOINT C ART, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143; Froehlich Jon, 2008, P WORLD C SOC AUT EN; Inoue T., 2014, INT J SOFTW TOOLS TE, P1; Kisa D., 2014, KR; Kisa D., 2014, ICML WORKSH LEARN TR; Knuth D.E., 2009, FASCICLE 1 BITWISE T, V4; Krumm J.A., 2008, MARKOV MODEL DRIVER; Letchner J., 2006, AAAI, P1795; Liang YT, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Lu T., 2011, ICML, P145; Mallows C. L., 1957, BIOMETRIKA; Minato S, 2013, IEICE T INF SYST, VE96D, P1419, DOI 10.1587/transinf.E96.D.1419; Nishino M., 2017, P 31 C ART INT AAAI; Nishino M, 2016, AAAI CONF ARTIF INTE, P1058; Shen Yujia, 2016, ADV NEURAL INFORM PR; Simmons R., 2006, ITSC, P127, DOI 10.1109/ITSC.2006.1706730; VALIANT LG, 1979, SIAM J COMPUT, V8, P410, DOI 10.1137/0208032; Xue Y, 2012, AAAI, P842	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403053
C	Choromanski, K; Sindhwani, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Choromanski, Krzysztof; Sindhwani, Vikas			On Blackbox Backpropagation and Jacobian Sensing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					From a small number of calls to a given "blackbox" on random input perturbations, we show how to efficiently recover its unknown Jacobian, or estimate the left action of its Jacobian on a given vector. Our methods are based on a novel combination of compressed sensing and graph coloring techniques, and provably exploit structural prior knowledge about the Jacobian such as sparsity and symmetry while being noise robust. We demonstrate efficient backpropagation through noisy blackbox layers in a deep neural net, improved data-efficiency in the task of linearizing the dynamics of a rigid body system, and the generic ability to handle a rich class of input-output dependency structures in Jacobian estimation problems.	[Choromanski, Krzysztof; Sindhwani, Vikas] Google Brain, New York, NY 10011 USA	Google Incorporated	Choromanski, K (corresponding author), Google Brain, New York, NY 10011 USA.	kchoro@google.com; sindhwani@google.com	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Abdel-Khali H. S., 2008, ADV AUTOMATIC DIFFER; Bajwa WU, 2007, 2007 IEEE/SP 14TH WORKSHOP ON STATISTICAL SIGNAL PROCESSING, VOLS 1 AND 2, P294, DOI 10.1109/SSP.2007.4301266; Bandeira A. S., 2012, MATH PROGRAMMING, V134; Candes E. J., 2006, COMMUNICATIONS PURE, V59; Candes EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731; Conn AR, 2009, MOS-SIAM SER OPTIMIZ, V8, P1; Donoho David L., 2006, IEEE T INFORM THEORY, V52; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Gebremedhin AH, 2005, SIAM REV, V47, P629, DOI 10.1137/S0036144504444711; Golub G. H., 2012, MATRIX COMPUTATIONS; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; Jacobson D. H., 1970, DIFFERENTIAL DYNAMIC; Jensen T. R., 1995, GRAPH COLORING PROBL; Levine S., 2016, JMLR, V17; Lin W., 2010, P SPIE INT SOC OPTIC; Newsam G. N., 1983, SIAM J ALG DISCR MET; Rauhutk H., 2010, SPARS 09 SIGNAL PROC; Song J, 2004, I C CONT AUTOMAT ROB, P2223; Tedrake R., 2016, DRAKE PLANNING CONTR; Toft B., 1996, HDB COMBINATORICS	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406057
C	Choromanski, K; Rowland, M; Weller, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Choromanski, Krzysztof; Rowland, Mark; Weller, Adrian			The Unreasonable Effectiveness of Structured Random Orthogonal Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We examine a class of embeddings based on structured random matrices with orthogonal rows which can be applied in many machine learning applications including dimensionality reduction and kernel approximation. For both the Johnson-Lindenstrauss transform and the angular kernel, we show that we can select matrices yielding guaranteed improved performance in accuracy and/or speed compared to earlier methods. We introduce matrices with complex entries which give significant further accuracy improvement. We provide geometric and Markov chain-based perspectives to help understand the benefits, and empirical results which suggest that the approach is helpful in a wider range of applications.	[Choromanski, Krzysztof] Google Brain Robot, Mountain View, CA 94043 USA; [Rowland, Mark; Weller, Adrian] Univ Cambridge, Cambridge, England; [Weller, Adrian] Alan Turing Inst, London, England	University of Cambridge	Choromanski, K (corresponding author), Google Brain Robot, Mountain View, CA 94043 USA.	kchoro@google.com; mr504@cam.ac.uk; aw665@cam.ac.uk	Jeong, Yongwook/N-7413-2016		UK Engineering and Physical Sciences Research Council (EPSRC) [EP/L016516/1]; Alan Turing Institute under the EPSRC grant [EP/N510129/1]; Leverhulme Trust via the CFI	UK Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under the EPSRC grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Leverhulme Trust via the CFI	We thank Vikas Sindhwani at Google Brain Robotics and Tamas Sarlos at Google Research for inspiring conversations that led to this work. We thank Matej Balog, Maria Lomeli, Jiri Hron and Dave Janz for helpful comments. MR acknowledges support by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, the Cambridge Centre for Analysis. AW acknowledges support by the Alan Turing Institute under the EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI.	Ailon N., 2006, STOC; Andoni A., 2015, PRACTICAL OPTIMAL LS; Bojarski M., 2017, AISTATS; Cho Youngmin, 2009, NIPS; Choromanska Anna, 2016, ICML; Choromanski K., 2016, ICML; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Le Q., 2013, ICML; Rahimi A., 2007, NIPS; Samo Y. -L. K., 2015, CORR; Schmidt Ludwig, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P1650, DOI 10.1109/ICASSP.2014.6853878; Sidorov G., 2014, COMPUTACION SISTEMAS, V18; Sundaram N, 2013, PROC VLDB ENDOW, V6, P1930, DOI 10.14778/2556549.2556574; Vybiral J, 2011, J FUNCT ANAL, V260, P1096, DOI 10.1016/j.jfa.2010.11.014; Williams CKI, 1998, NEURAL COMPUT, V10, P1203, DOI 10.1162/089976698300017412; Yu Felix X, 2016, ADV NEURAL INFORM PR, V29, P1975; Zhang H., 2013, CORR; Zhang X, 2015, IEEE I CONF COMP VIS, P2929, DOI 10.1109/ICCV.2015.335	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400021
C	Clemencon, S; Achab, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Clemencon, Stephan; Achab, Mastane			Ranking Data with Continuous Labels through Oriented Recursive Partitions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We formulate a supervised learning problem, referred to as continuous ranking, where a continuous real-valued label Y is assigned to an observable r.v. X taking its values in a feature space chi and the goal is to order all possible observations x in chi by means of a scoring function s : chi -> R so that s(X) and Y tend to increase or decrease together with highest probability. This problem generalizes bi/multi-partite ranking to a certain extent and the task of finding optimal scoring functions s(x) can be naturally cast as optimization of a dedicated functional criterion, called the IROC curve here, or as maximization of the Kendall tau related to the pair (s(X), Y). From the theoretical side, we describe the optimal elements of this problem and provide statistical guarantees for empirical Kendall tau maximization under appropriate conditions for the class of scoring function candidates. We also propose a recursive statistical learning algorithm tailored to empirical IROC curve optimization and producing a piecewise constant scoring function that is fully described by an oriented binary tree. Preliminary numerical experiments highlight the difference in nature between regression and continuous ranking and provide strong empirical evidence of the performance of empirical optimizers of the criteria proposed.	[Clemencon, Stephan; Achab, Mastane] Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France	IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay	Clemencon, S (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, F-75013 Paris, France.	stephan.clemencon@telecom-paristech.fr; mastane.achab@telecom-paristech.fr	Jeong, Yongwook/N-7413-2016; Achab, Mastane/AAB-6545-2021	Achab, Mastane/0000-0002-4202-6802	industrial chair Machine Learning for Big Data from Telecom ParisTech; Investissement d'avenir project [ANR-11-LABX-0056-LMH]	industrial chair Machine Learning for Big Data from Telecom ParisTech; Investissement d'avenir project(French National Research Agency (ANR))	This work was supported by the industrial chair Machine Learning for Big Data from Telecom ParisTech and by a public grant (Investissement d'avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH).	Agarwal S, 2005, J MACH LEARN RES, V6, P393; Clemencon S, 2005, LECT NOTES COMPUT SC, V3559, P1, DOI 10.1007/11503415_1; Clemencon S., 2014, J NONPARAMETR STAT, V25, P107; Clemencon S., 2010, CONSTR APPR, V32, P619; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Clemencon S, 2013, MACH LEARN, V91, P67, DOI 10.1007/s10994-012-5325-4; Clemencon S, 2013, J MACH LEARN RES, V14, P39; Clemencon S, 2009, IEEE T INFORM THEORY, V55, P4316, DOI 10.1109/TIT.2009.2025558; Cortes C, 2004, ADV NEUR IN, V16, P313; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; Menon Aditya Krishna, 2016, J MACHINE LEARNING R, V17, P1; Olshen R., 1984, CLASSIFICATION REGRE; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Rajaram S., 2005, NIPS 2005 WORKSH LEA; Rakotomamonjy A., 2004, 1 INT WORKSHOP	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404065
C	Cowley, BR; Williamson, RC; Acar, K; Smith, MA; Yu, BM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cowley, Benjamin R.; Williamson, Ryan C.; Acar, Katerina; Smith, Matthew A.; Yu, Byron M.			Adaptive stimulus selection for optimizing neural population responses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMIZATION; SHAPE	Adaptive stimulus selection methods in neuroscience have primarily focused on maximizing the firing rate of a single recorded neuron. When recording from a population of neurons, it is usually not possible to find a single stimulus that maximizes the firing rates of all neurons. This motivates optimizing an objective function that takes into account the responses of all recorded neurons together. We propose "Adept," an adaptive stimulus selection method that can optimize population objective functions. In simulations, we first confirmed that population objective functions elicited more diverse stimulus responses than single-neuron objective functions. Then, we tested Adept in a closed-loop electrophysiological experiment in which population activity was recorded from macaque V4, a cortical area known for mid-level visual processing. To predict neural responses, we used the outputs of a deep convolutional neural network model as feature embeddings. Natural images chosen by Adept elicited mean neural responses that were 20% larger than those for randomly-chosen natural images, and also evoked a larger diversity of neural responses. Such adaptive stimulus selection methods can facilitate experiments that involve neurons far from the sensory periphery, for which it is often unclear which stimuli to present.	[Cowley, Benjamin R.; Williamson, Ryan C.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Cowley, Benjamin R.; Williamson, Ryan C.; Acar, Katerina; Smith, Matthew A.; Yu, Byron M.] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA; [Yu, Byron M.] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA; [Yu, Byron M.] Carnegie Mellon Univ, Dept Biomed Engn, Pittsburgh, PA 15213 USA; [Williamson, Ryan C.] Univ Pittsburgh, Sch Med, Pittsburgh, PA 15260 USA; [Acar, Katerina] Univ Pittsburgh, Dept Neurosci, Pittsburgh, PA 15260 USA; [Smith, Matthew A.] Univ Pittsburgh, Dept Ophthalmol, Pittsburgh, PA 15260 USA	Carnegie Mellon University; Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Carnegie Mellon University; Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Cowley, BR (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.; Cowley, BR (corresponding author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.	bcowley@cs.cmu.edu; rcw30@pitt.edu; kac216@pitt.edu; smithma@pitt.edu; byronyu@cmu.edu	Jeong, Yongwook/N-7413-2016		BrainHub Richard K. Mellon Fellowship; NIH [R01 EY022928, P30 EY008098, T32 GM008208, T90 DA022762, R01 HD071686, R01 NS105318]; Richard K. Mellon Foundation; NSF [GRFP 1747452]; NSF-NCS [BCS-1533672, BCS-1734901/1734916]; Simons Foundation [364994]	BrainHub Richard K. Mellon Fellowship; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Richard K. Mellon Foundation; NSF(National Science Foundation (NSF)); NSF-NCS; Simons Foundation	B.R.C. was supported by a BrainHub Richard K. Mellon Fellowship. R.C.W. was supported by NIH T32 GM008208, T90 DA022762, and the Richard K. Mellon Foundation. K. A. was supported by NSF GRFP 1747452. M.A.S. and B.M.Y. were supported by NSF-NCS BCS-1734901/1734916. M.A.S. was supported by NIH R01 EY022928 and NIH P30 EY008098. B.M.Y. was supported by NSF-NCS BCS-1533672, NIH R01 HD071686, NIH R01 NS105318, and Simons Foundation 364994.	Benda J, 2007, CURR OPIN NEUROBIOL, V17, P430, DOI 10.1016/j.conb.2007.07.009; Carlson ET, 2011, CURR BIOL, V21, P288, DOI 10.1016/j.cub.2011.01.013; Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842; Cohen MR, 2009, NAT NEUROSCI, V12, P1594, DOI 10.1038/nn.2439; DiMattina C., 2014, CLOSING LOOP NEURAL, P258; Felsen G, 2005, PLOS BIOL, V3, P1819, DOI 10.1371/journal.pbio.0030342; Foldiak P, 2001, NEUROCOMPUTING, V38, P1217, DOI 10.1016/S0925-2312(01)00570-7; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hung CC, 2012, NEURON, V74, P1099, DOI 10.1016/j.neuron.2012.04.029; Kohn A, 2016, ANNU REV NEUROSCI, V39, P237, DOI 10.1146/annurev-neuro-070815-013851; Lewi J, 2009, NEURAL COMPUT, V21, P619, DOI 10.1162/neco.2008.08-07-594; Lin IC, 2015, NEURON, V87, P644, DOI 10.1016/j.neuron.2015.06.035; Machens CK, 2005, NEURON, V47, P447, DOI 10.1016/j.neuron.2005.06.015; Machens CK, 2002, PHYS REV LETT, V88, DOI 10.1103/PhysRevLett.88.228104; O'Connor KN, 2005, J NEUROPHYSIOL, V94, P4051, DOI 10.1152/jn.00046.2005; Okun M, 2015, NATURE, V521, P511, DOI 10.1038/nature14273; Olmos A, 2004, PERCEPTION, V33, P1463, DOI 10.1068/p5321; Paninski L, 2005, NEURAL COMPUT, V17, P1480, DOI 10.1162/0899766053723032; Park M, 2014, NEURAL COMPUT, V26, P1519, DOI 10.1162/NECO_a_00615; Pillow JW, 2016, CLOSED LOOP NEUROSCIENCE, P3, DOI 10.1016/B978-0-12-802452-2.00001-9; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ringach D, 2004, COGNITIVE SCI, V28, P147, DOI 10.1016/j.cogsci.2003.11.003; Roe AW, 2012, NEURON, V74, P12, DOI 10.1016/j.neuron.2012.03.011; Rust NC, 2005, NAT NEUROSCI, V8, P1647, DOI 10.1038/nn1606; Schwartz O, 2006, J VISION, V6, P484, DOI 10.1167/6.4.13; Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444; Stevenson IH, 2011, NAT NEUROSCI, V14, P139, DOI 10.1038/nn.2731; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; Xiao J., 2013, PRINCETON VISION ROB; Yamane Y, 2008, NAT NEUROSCI, V11, P1352, DOI 10.1038/nn.2202; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244	33	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401042
C	Cutkosky, A; Boahen, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cutkosky, Ashok; Boahen, Kwabena			Stochastic and Adversarial Online Learning without Hyperparameters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Most online optimization algorithms focus on one of two things: performing well in adversarial settings by adapting to unknown data parameters (such as Lipschitz constants), typically achieving O (root T) regret, or performing well in stochastic settings where they can leverage some structure in the losses (such as strong convexity), typically achieving O (log(T)) regret. Algorithms that focus on the former problem hitherto achieved O (root T) in the stochastic setting rather than O (log(T)). Here we introduce an online optimization algorithm that achieves O (log(4)(T)) regret in a wide class of stochastic settings while gracefully degrading to the optimal O (root T) regret in adversarial settings (up to logarithmic factors). Our algorithm does not require any prior knowledge about the data or tuning of parameters to achieve superior performance.	[Cutkosky, Ashok] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Boahen, Kwabena] Stanford Univ, Dept Bioengn, Stanford, CA 94305 USA	Stanford University; Stanford University	Cutkosky, A (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	ashokc@cs.stanford.edu; boahen@stanford.edu	Jeong, Yongwook/N-7413-2016	Cutkosky, Ashok/0000-0002-3822-3029				Abernethy J., 2008, OPTIMAL STRATEGIES M; Bartlett PL, 2007, P 21 ANN C ADV NEUR, P65; Cutkosky A, 2016, ADV NEUR IN, V29; Cutkosky Ashok, 2017, ARXIV170302629; Duchi J., 2010, C LEARN THEOR COLT; Koolen Wouter M., 2016, ADV NEURAL INFORM PR, V29, P4457; McMahan H. Brendan, 2010, P 23 ANN C LEARN THE; Orabona F., 2013, ADV NEURAL INFORM PR, P1806; Orabona F., 2016, ADV NEURAL INFORM PR, V29, P577; Sani A., 2014, ADV NEURAL INFORM PR, V27, P810; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Streeter M., 2012, ADV NEURAL INFORM PR; van Erven T, 2016, ADV NEUR IN, V29; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405014
C	Dahlgaard, S; Knudsen, MBT; Thorup, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dahlgaard, Soren; Knudsen, Mathias Baek Tejs; Thorup, Mikkel			Practical Hash Functions for Similarity Estimation and Dimensionality Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Hashing is a basic tool for dimensionality reduction employed in several aspects of machine learning. However, the perfomance analysis is often carried out under the abstract assumption that a truly random unit cost hash function is used, without concern for which concrete hash function is employed. The concrete hash function may work fine on sufficiently random input.The question is if they can be trusted in the real world where they may be faced with more structured input. In this paper we focus on two prominent applications of hashing, namely similarity estimation with the one permutation hashing (OPH) scheme of Li et al. [NIPS'12] and feature hashing (FH) of Weinberger et al. [ICML'09], both of which have found numerous applications, i.e. in approximate near-neighbour search with LSH and large-scale classification with SVM. We consider the recent mixed tabulation hash function of Dahlgaard et al. [FOCS' 15] which was proved theoretically to perform like a truly random hash function in many applications, including the above OPH. Here we first show improved concentration bounds for FH with truly random hashing and then argue that mixed tabulation performs similar when the input vectors are not too dense. Our main contribution, however, is an experimental comparison of different hashing schemes when used inside FH, OPH, and LSH. We find that mixed tabulation hashing is almost as fast as the classic multiply-mod-prime scheme (ax + b) mod p. Mutiply-mod-prime is guaranteed to work well on sufficiently random data, but here we demonstrate that in the above applications, it can lead to bias and poor concentration on both real-world and synthetic data. We also compare with the very popular MurmurHash3, which has no proven guarantees. Mixed tabulation and MurmurHash3 both perform similar to truly random hashing in our experiments. However, mixed tabulation was 40% faster than MurmurHash3, and it has the proven guarantee of good performance (like fully random) on all possible input making it more reliable.	[Dahlgaard, Soren; Knudsen, Mathias Baek Tejs; Thorup, Mikkel] Univ Copenhagen, SupWiz, Copenhagen, Denmark; [Thorup, Mikkel] Univ Copenhagen, Copenhagen, Denmark	University of Copenhagen; University of Copenhagen	Dahlgaard, S (corresponding author), Univ Copenhagen, SupWiz, Copenhagen, Denmark.	s.dahlgaard@supwiz.com; m.knudsen@supwiz.com; mthorup@di.ku.dk	Jeong, Yongwook/N-7413-2016	Thorup, Mikkel/0000-0001-5237-1709	Mikkel Thorup's Advanced Grant from the Danish Council for Independent Research [DFF-0602-02499B]; DABAI project; FNU project AlgoDisc	Mikkel Thorup's Advanced Grant from the Danish Council for Independent Research; DABAI project; FNU project AlgoDisc	The authors gratefully acknowledge support from Mikkel Thorup's Advanced Grant DFF-0602-02499B from the Danish Council for Independent Research as well as the DABAI project. Mathias Baek Tejs Knudsen gratefully acknowledges support from the FNU project AlgoDisc.	Andoni A., 2014, P 25 ANN ACM SIAM S, P1018, DOI DOI 10.1137/1.9781611973402.76; Andoni A, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P47; Andoni A, 2015, ACM S THEORY COMPUT, P793, DOI 10.1145/2746539.2746553; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; [Anonymous], 2012, BREAKING MURMUR HASH; Appleby A., 2016, MURMURHASH3; Aumasson J.-P., 2013, LNCS, V7954, P119, DOI 10.1007; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; Broder AZ, 1997, COMPUT NETWORKS ISDN, V29, P1157, DOI 10.1016/S0169-7552(97)00031-7; Burges, 1998, MNIST DATABASE HANDW; CARTER JL, 1979, J COMPUT SYST SCI, V18, P143, DOI 10.1016/0022-0000(79)90044-8; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; Christiani T, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P31; Dahlgaard Soren, 2013, 2013 IEEE International Conference on Big Data, P28, DOI 10.1109/BigData.2013.6691730; Dahlgaard S, 2015, ANN IEEE SYMP FOUND, P1292, DOI 10.1109/FOCS.2015.83; Dasgupta A, 2010, ACM S THEORY COMPUT, P341; Dietzfelbinger M, 1997, J ALGORITHM, V25, P19, DOI 10.1006/jagm.1997.0873; HUFFMAN DA, 1952, P IRE, V40, P1098, DOI 10.1109/JRPROC.1952.273898; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kennedy Christopher, 2016, ABS160206922 CORR; Li P., 2012, 26 ANN C NEUR INF PR, V25, P3113; Li Ping, 2012, ABS12052958 CORR; Li Ping, 2011, ADV NEURAL INFORM PR, P2672; Manku G.S., 2007, P 16 INT C WORLD WID, P141; Mitzenmacher M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P746; NISAN N, 1992, COMBINATORICA, V12, P449, DOI 10.1007/BF01305237; Patrascu M, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2716317; Pike G., 2011, INTRO CITYHASH; Shakhnarovich G., 2008, IEEE T NEURAL NETWOR, V19, P377; Shrivastava A, 2014, PR MACH LEARN RES, V32; Shrivastava A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P732; Terasawa K, 2007, LECT NOTES COMPUT SC, V4619, P27; Thorup M, 2012, SIAM J COMPUT, V41, P293, DOI 10.1137/100800774; Thorup Mikkel, 2013, P 45 ACM S THEOR COM; Tong Simon, 2010, LESSONS LEARNED DEV; Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406066
C	Dai, ZW; Alvarez, MA; Lawrence, ND		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dai, Zhenwen; Alvarez, Mauricio A.; Lawrence, Neil D.			Efficient Modeling of Latent Information in Supervised Learning using Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Often in machine learning, data are collected as a combination of multiple conditions, e.g., the voice recordings of multiple persons, each labeled with an ID. How could we build a model that captures the latent information related to these conditions and generalize to a new one with few data? We present a new model called Latent Variable Multiple Output Gaussian Processes (LVMOGP) that allows to jointly model multiple conditions for regression and generalize to a new condition with a few data points at test time. LVMOGP infers the posteriors of Gaussian processes together with a latent space representing the information about different conditions. We derive an efficient variational inference method for LVMOGP for which the computational complexity is as low as sparse Gaussian processes. We show that LVMOGP significantly outperforms related Gaussian process methods on various tasks with both synthetic and real data.	[Dai, Zhenwen] Inferentia Ltd, Chesterfield, England; [Alvarez, Mauricio A.; Lawrence, Neil D.] Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England; [Dai, Zhenwen; Lawrence, Neil D.] Amazon Com, Seattle, WA 98109 USA	University of Sheffield; Amazon.com	Dai, ZW (corresponding author), Inferentia Ltd, Chesterfield, England.; Dai, ZW (corresponding author), Amazon Com, Seattle, WA 98109 USA.	zhenwend@amazon.com; mauricio.alvarez@sheffield.ac.uk; lawrennd@amazon.com	Jeong, Yongwook/N-7413-2016		Engineering and Physical Research Council (EPSRC) [EP/N014162/1]	Engineering and Physical Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	MAA has been financed by the Engineering and Physical Research Council (EPSRC) Research Project EP/N014162/1.	Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Alvarez MA, 2011, J MACH LEARN RES, V12, P1459; Bonilla E., 2008, NIPS, V20; Bussas Matthias, 2017, MACH LEARN, P1; Goovaerts P., 1997, GEOSTATISTICS NATURA; Hensman J., 2013, UAI; Journel A.G., 1978, MINING GEOSTATISTICS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Matthews A. G. de G, 2016, AISTATS; Qian PZG, 2008, TECHNOMETRICS, V50, P383, DOI 10.1198/004017008000000262; Quinlan J. R., 1992, Proceedings of the 5th Australian Joint Conference on Artificial Intelligence. AI '92, P343; Stegle Oliver, 2011, ADV NEURAL INFORM PR, P630; Sutskever I., 2014, NIPS, V27; Tenenbaum JB, 2000, NEURAL COMPUT, V12, P1247, DOI 10.1162/089976600300015349; Titsias Michalis K., 2009, AISTATS; Titsias Michalis K., 2010, AISTATS; Zamora-Martinez F, 2014, ENERG BUILDINGS, V83, P162, DOI 10.1016/j.enbuild.2014.04.034	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405021
C	Dai, ZH; Yang, ZL; Yang, F; Cohen, WW; Salakhutdinov, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dai, Zihang; Yang, Zhilin; Yang, Fan; Cohen, William W.; Salakhutdinov, Ruslan			Good Semi-supervised Learning That Requires a Bad GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically we show that given the discriminator objective, good semi-supervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets(2).	[Dai, Zihang; Yang, Zhilin; Yang, Fan; Cohen, William W.; Salakhutdinov, Ruslan] Carnegie Melon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Dai, ZH (corresponding author), Carnegie Melon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	dzihang@cs.cmu.edu; zhiliny@cs.cmu.edu; fanyang1@cs.cmu.edu; wcohen@cs.cmu.edu; rsalakhu@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		DARPA [D17AP00001]; Google focused award; Nvidia NVAIL award	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Google focused award(Google Incorporated); Nvidia NVAIL award	This work was supported by the DARPA award D17AP00001, the Google focused award, and the Nvidia NVAIL award. The authors would also like to thank Han Zhao for his insightful feedback.	[Anonymous], 2017, ARXIV170202206; [Anonymous], 2017, ARXIV170302291; [Anonymous], 2016, ARXIV160308861; Arjovsky M., 2016, NIPS 2016 WORKSH ADV, V2017; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Dai Zihang, 2017, ARXIV170201691; Donahue Jeff, 2016, ARXIV160509782; Dumoulin V., 2016, ARXIV160600704; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kipf TN, 2016, P INT C LEARN REPR; Laine S., 2016, INT C LEARN REPR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Maaloe L., 2016, ARXIV160205473; Miyato T., 2015, ARXIV PREPRINT ARXIV; Miyato T., 2017, ARXIV170403976; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Salimans T., 2017, INT C LEARNING REPRE; Salimans Tim, 2016, ADV NEURAL INFORM PR; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Springenberg Jost Tobias, 2015, ARXIV151106390; Tompson J, 2015, PROC CVPR IEEE, P648, DOI 10.1109/CVPR.2015.7298664; Ulyanov Dmitry, 2017, ARXIV170402304; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Zhao Junbo, 2016, P INT C LEARN REPR T, DOI DOI 10.48550/ARXIV.1609.03126; Zhu Xiaojin., 2003, P ICLR, P912	26	0	0	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406056
C	Dao, T; De Sa, C; Re, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dao, Tri; De Sa, Christopher; Re, Christopher			Gaussian Quadrature for Kernel Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Kernel methods have recently attracted resurgent interest, showing performance competitive with deep neural networks in tasks such as speech recognition. The random Fourier features map is a technique commonly used to scale up kernel machines, but employing the randomized feature map means that O(epsilon(-2)) samples are required to achieve an approximation error of at most E. We investigate some alternative schemes for constructing feature maps that are deterministic, rather than random, by approximating the kernel in the frequency domain using Gaussian quadrature. We show that deterministic feature maps can be constructed, for any gamma > 0, to achieve error epsilon with O(e(e gamma) + epsilon-(1/gamma)) samples as epsilon goes to 0. Our method works particularly well with sparse ANOVA kernels, which are inspired by the convolutional layer of CNNs. We validate our methods on datasets in different domains, such as MNIST and TIMIT, showing that deterministic features are faster to generate and achieve accuracy comparable to the state-of-the-art kernel methods based on random Fourier features.	[Dao, Tri; Re, Christopher] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [De Sa, Christopher] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Stanford University; Cornell University	Dao, T (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	trid@stanford.edu; cdesa@cs.cornell.edu; chrismre@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Defense Advanced Research Projects Agency (DARPA) [FA8750-17-2-0095]; DARPA SIMPLEX program [N66001-15-C-4043]; DARPA [FA8750-12-2-0335, FA8750-13-2-0039]; National Institute of Health (NIH) [U54EB020405]; National Science Foundation (NSF) [CCF-1563078]; Office of Naval Research (ONR) [N000141210041, N000141310129]; Moore Foundation; Okawa Research Grant; American Family Insurance; Intel; Stanford DAWN project: Intel; Microsoft; VMware; DOE [108845]; Accenture; Toshiba; Teradata	Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA SIMPLEX program; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Institute of Health (NIH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); Office of Naval Research (ONR)(Office of Naval Research); Moore Foundation(Gordon and Betty Moore Foundation); Okawa Research Grant; American Family Insurance; Intel(Intel Corporation); Stanford DAWN project: Intel; Microsoft(Microsoft); VMware; DOE(United States Department of Energy (DOE)); Accenture; Toshiba; Teradata	This material is based on research sponsored by Defense Advanced Research Projects Agency (DARPA) under agreement number FA8750-17-2-0095. We gratefully acknowledge the support of the DARPA SIMPLEX program under No. N66001-15-C-4043, DARPA FA8750-12-2-0335 and FA8750-13-2-0039, DOE 108845, National Institute of Health (NIH) U54EB020405, the National Science Foundation (NSF) under award No. CCF-1563078, the Office of Naval Research (ONR) under awards No. N000141210041 and No. N000141310129, the Moore Foundation, the Okawa Research Grant, American Family Insurance, Accenture, Toshiba, and Intel. This research was supported in part by affiliate members and other supporters of the Stanford DAWN project: Intel, Microsoft, Teradata, and VMware. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA or the U.S. Government. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, AFRL, NSF, NIH, ONR, or the U.S. government.	[Anonymous], 1990, FOURIER ANAL GROUPS; Bach F., 2015, ARXIV150206800; Bungartz HJ, 2004, ACT NUMERIC, V13, P147, DOI 10.1017/S0962492904000182; Fan J, 2007, DES AUT TEST EUROPE, P1508, DOI 10.1007/BF01386223; Gales MJF, 1998, COMPUT SPEECH LANG, V12, P75, DOI 10.1006/csla.1998.0043; Garofolo J.S., 1993, TIMIT ACOUSTIC PHONE, DOI 10.35111/17gk-bn40; Gauss C. F., 1815, METHODUS NOVA INTEGR; Gehring J., 2017, ARXIV PREPRINT ARXIV; Gunn SR, 2002, MACH LEARN, V48, P137, DOI 10.1023/A:1013903804720; Hale N, 2013, SIAM J SCI COMPUT, V35, pA652, DOI 10.1137/120889873; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Holtz Markus, 2010, SPARSE GRID QUADRATU, V77; Isaacson E., 1994, ANAL NUMERICAL METHO; Kim Y., P 2014 C EMP METH NA, P1746; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin M, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2611378; Lu ZY, 2016, INT CONF ACOUST SPEE, P5070, DOI 10.1109/ICASSP.2016.7472643; Lu Zhiyun, 2014, ARXIV14114000CSSTAT; Maji S., 2009, UCBEECS2009159; May A., 2017, ARXIV170103577; May A, 2016, INT CONF ACOUST SPEE, P2424, DOI 10.1109/ICASSP.2016.7472112; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Scholkopf B., 2001, LEARNING KERNELS SUP; Simard PY, 2003, PROC INT CONF DOC, P958; SMOLIAK SA, 1963, DOKL AKAD NAUK SSSR+, V148, P1042; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Stitson MO, 1999, ADVANCES IN KERNEL METHODS, P285; Sutherland Dougal J., 2015, P 31 ANN C UNC ART I; Townsend Alex, 2015, IMA J NUMER ANAL; Trefethen LN, 2008, SIAM REV, V50, P67, DOI 10.1137/060659831; Yang JY, 2014, PR MACH LEARN RES, V32; Yang T., 2012, ADV NEURAL INFORM PR, P476	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406018
C	Devlin, J; Bunel, R; Singh, R; Hausknecht, M; Kohli, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Devlin, Jacob; Bunel, Rudy; Singh, Rishabh; Hausknecht, Matthew; Kohli, Pushmeet			Neural Program Meta-Induction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Most recently proposed methods for Neural Program Induction work under the assumption of having a large set of input/output (I/O) examples for learning any underlying input-output mapping. This paper aims to address the problem of data and computation efficiency of program induction by leveraging information from related tasks. Specifically, we propose two approaches for cross-task knowledge transfer to improve program induction in limited-data scenarios. In our first proposal, portfolio adaptation, a set of induction models is pretrained on a set of related tasks, and the best model is adapted towards the new task using transfer learning. In our second approach, meta program induction, a k-shot learning approach is used to make a model generalize to new tasks without additional training. To test the efficacy of our methods, we constructed a new benchmark of programs written in the Karel programming language [17]. Using an extensive experimental evaluation on the Karel benchmark, we demonstrate that our proposals dramatically outperform the baseline induction method that does not use knowledge transfer. We also analyze the relative performance of the two approaches and study conditions in which they perform best. In particular, meta induction outperforms all existing approaches under extreme data sparsity (when a very small number of examples are available), i.e., fewer than ten. As the number of available I/O examples increase (i.e. a thousand or more), portfolio adapted program induction becomes the best approach. For intermediate data sizes, we demonstrate that the combined method of adapted meta program induction has the strongest performance.	[Devlin, Jacob] Google, Mountain View, CA 94043 USA; [Bunel, Rudy] Univ Oxford, Oxford, England; [Singh, Rishabh; Hausknecht, Matthew] Microsoft Res, Redmond, WA USA; [Kohli, Pushmeet] DeepMind, London, England	Google Incorporated; University of Oxford; Microsoft	Devlin, J (corresponding author), Google, Mountain View, CA 94043 USA.	jacobdevlin@google.com; rudy@robots.ox.ac.uk; risin@microsoft.com; mahauskn@microsoft.com; pushmeet@google.com	Jeong, Yongwook/N-7413-2016					Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrychowicz Marcin, 2016, CORR; [Anonymous], 2017, ICLR; [Anonymous], 2015, NIPS; Devlin J., 2017, CORR; Duan Y., 2017, CORR; Graves A, 2014, NEURAL TURING MACHIN; Grefenstette Edward, 2015, NIPS; Gulwani Sumit, 2012, COMMUNICATIONS ACM; Huh Mi-Young, 2016, CORR; Joulin A, 2015, ADV NEUR IN, V28; Kurach Karol, 2016, ICLR; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li Chengtao, 2017, ICLR; Luong Minh-Thang, 2015, STANFORD NEURAL MACH; Neelakantan Arvind, 2016, 4 INT C LEARN REPR I; Pattis Richard E, 1981, KAREL ROBOT GENTLE I; Reed Scott, 2016, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Wayne G., 2016, ICML; Zaremba W., 2015, CORR	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402013
C	Drouin, A; Hocking, TD; Laviolette, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Drouin, Alexandre; Hocking, Toby Dylan; Laviolette, Francois			Maximum Margin Interval Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REGRESSION	Learning a regression function using censored or interval-valued output data is an important problem in fields such as genomics and medicine. The goal is to learn a real-valued prediction function, and the training output labels indicate an interval of possible values. Whereas most existing algorithms for this task are linear models, in this paper we investigate learning nonlinear tree models. We propose to learn a tree by minimizing a margin-based discriminative objective function, and we provide a dynamic programming algorithm for computing the optimal solution in log-linear time. We show empirically that this algorithm achieves state-of-the-art speed and prediction accuracy in a benchmark of several data sets.	[Drouin, Alexandre; Laviolette, Francois] Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ, Canada; [Hocking, Toby Dylan] McGill Univ, McGill Genome Ctr, Montreal, PQ, Canada	Laval University; McGill University	Drouin, A (corresponding author), Univ Laval, Dept Informat & Genie Logiciel, Quebec City, PQ, Canada.	alexandre.drouin.8@ulaval.ca; toby.hocking@r-project.org; francois.laviolette@ift.ulaval.ca	Jeong, Yongwook/N-7413-2016		National Sciences and Engineering Research Council of Canada through an Alexander Graham Bell Canada Graduate Scholarship Doctoral Award [262067]	National Sciences and Engineering Research Council of Canada through an Alexander Graham Bell Canada Graduate Scholarship Doctoral Award	We are grateful to Ulysse Cote-Allard, Mathieu Blanchette, Pascal Germain, Sebastien Giguere, Gael Letarte, Mario Marchand, and Pier-Luc Plante for their insightful comments and suggestions. This work was supported by the National Sciences and Engineering Research Council of Canada, through an Alexander Graham Bell Canada Graduate Scholarship Doctoral Award awarded to AD and a Discovery Grant awarded to FL (#262067).	Basak D, 2007, NEURAL INFORM PROCES, V11, P203, DOI DOI 10.1007/978-1-4302-5990-9_4; Cai T, 2009, BIOMETRICS, V65, P394, DOI 10.1111/j.1541-0420.2008.01074.x; Hocking T., 2013, P 30 ICML, P172; Hocking TD, 2013, BMC BIOINFORMATICS, V14, DOI 10.1186/1471-2105-14-164; Hothorn T., 2017, ARXIV170102110; Hothorn T, 2006, BIOSTATISTICS, V7, P355, DOI 10.1093/biostatistics/kxj011; Huang J., 2005, 349 U IOW DEP STAT A; Klein JP., 2005, SURVIVAL ANAL TECHNI; Lichman M., 2013, UCI MACHINE LEARNING; Molinaro AM, 2004, J MULTIVARIATE ANAL, V90, P154, DOI 10.1016/j.jmva.2004.02.003; Olshen R., 1984, CLASSIFICATION REGRE; Polsterl S., 2016, ARXIV161107054; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; SEGAL MR, 1988, BIOMETRICS, V44, P35, DOI 10.2307/2531894; WEI LJ, 1992, STAT MED, V11, P1871, DOI 10.1002/sim.4780111409	15	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405003
C	Du, SS; Koushik, J; Singh, A; Poczos, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Du, Simon S.; Koushik, Jayanth; Singh, Aarti; Poczos, Barnabas			Hypothesis Transfer Learning via Transformation Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We consider the Hypothesis Transfer Learning (HTL) problem where one incorporates a hypothesis trained on the source domain into the learning procedure of the target domain. Existing theoretical analysis either only studies specific algorithms or only presents upper bounds on the generalization error but not on the excess risk. In this paper, we propose a unified algorithm-dependent framework for HTL through a novel notion of transformation function, which characterizes the relation between the source and the target domains. We conduct a general risk analysis of this framework and in particular, we show for the first time, if two domains are related, HTL enjoys faster convergence rates of excess risks for Kernel Smoothing and Kernel Ridge Regression than those of the classical non-transfer learning settings. Experiments on real world data demonstrate the effectiveness of our framework.	[Du, Simon S.; Koushik, Jayanth; Singh, Aarti; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Du, SS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ssdu@cs.cmu.edu; jayanthkoushik@cmu.edu; aartisingh@cmu.edu; bapoczos@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS1563887]; ARPA-E Terra program; AFRL grant [FA8750-17-2-0212]	NSF(National Science Foundation (NSF)); ARPA-E Terra program; AFRL grant	S.S.D. and B.P. were supported by NSF grant IIS1563887 and ARPA-E Terra program. A.S. was supported by AFRL grant FA8750-17-2-0212.	[Anonymous], 1996, DELVE DATA EVALUATIN; [Anonymous], NEW DIR TRANSF MULT; [Anonymous], 2007, P 15 ACM INT C MULTI; [Anonymous], 2014, ADV NEURAL INFORM PR; Ben-David Shai, 2007, NEURIPS, P7; Blitzer J., 2007, ADV NEURAL INFORM PR, V20, P129; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Carroll RJ, 2006, MEASUREMENT ERROR NO; Cortes C, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P169, DOI 10.1145/2783258.2783368; Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027; Cortes C, 2011, LECT NOTES ARTIF INT, V6925, P308, DOI 10.1007/978-3-642-24412-4_25; Craig CC, 1933, ANN MATH STAT, V4, P94, DOI 10.1214/aoms/1177732803; Huang J., 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0080; Kpotufe S., 2013, ADAPTIVITY LOCAL SMO, V26, P3075; Kuzborskij I., 2016, MACH LEARN, P1; Kuzborskij I, 2013, PROC CVPR IEEE, P3358, DOI 10.1109/CVPR.2013.431; Kuzborskij Ilja, 2013, P 30 INT C MACH LEAR, P942; Kuzborskij Ilja, 2016, COMPUTER VISION IMAG; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Liu TL, 2017, IEEE T PATTERN ANAL, V39, P227, DOI 10.1109/TPAMI.2016.2544314; Mansour Yishay, 2009, P COLT; Mohri Mehryar, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P124, DOI 10.1007/978-3-642-34106-9_13; Nuske S., 2014, FIELD SERVICE ROBOTI, P343, DOI [DOI 10.1007/978-3-642-40686-7_23, 10.1007/978-3-642-40686-7_23]; Orabona F, 2009, IEEE INT CONF ROBOT, P439; Steinwart I., 2009, P C LEARN THEOR COLT; Stroop JR, 1935, J EXP PSYCHOL, V18, P643, DOI 10.1037/h0054651; Sugiyama M., 2008, NIPS, P1433; Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064; Verstynen TD, 2014, J NEUROPHYSIOL, V112, P2457, DOI 10.1152/jn.00221.2014; Wang Xuezhi, 2015, GEN BOUNDS TRANSFER; Wang Xuezhi, 2016, 25 INT JOINT C ART I, V1, P2; Yu Yaoliang, 2012, ARXIV12064650; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Zhang Yu, 2015, AAAI, V2, P6; Zhou DX, 2008, J COMPUT APPL MATH, V220, P456, DOI 10.1016/j.cam.2007.08.023	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400055
C	Du, SS; Wang, YN; Singh, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Du, Simon S.; Wang, Yining; Singh, Aarti			On the Power of Truncated SVD for General High-rank Matrix Estimation Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				OPTIMAL RATES; COVARIANCE; COMPLETION; CONVERGENCE; BOUNDS	We show that given an estimate (A) over cap that is close to a general high-rank positive semi- definite (PSD) matrix A in spectral norm (i.e., parallel to(A) over cap -A parallel to(2) <= delta), the simple truncated Singular Value Decomposition of (A) over cap produces a multiplicative approximation of A in Frobenius norm. This observation leads to many interesting results on general high-rank matrix estimation problems: 1. High-rank matrix completion: we show that it is possible to recover a general high-rank matrix A up to (1 + epsilon) relative error in Frobenius norm from partial observations, with sample complexity independent of the spectral gap of A. 2. High-rank matrix denoising: we design an algorithm that recovers a matrix A with error in Frobenius norm from its noise-perturbed observations, without assuming A is exactly low-rank. 3. Low-dimensional approximation of high-dimensional covariance: given N i.i.d. samples of dimension n from N-n(0, A), we show that it is possible to approximate the covariance matrix A with relative error in Frobenius norm with N approximate to n, improving over classical covariance estimation results which requires N approximate to n(2).	[Du, Simon S.; Wang, Yining; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Du, SS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ssdu@cs.cmu.edu; yiningwa@cs.cmu.edu; aartisingh@cmu.edu	Jeong, Yongwook/N-7413-2016	Wang, Yining/0000-0001-9410-0392	ARPA-E Terra program; NSF CAREER [IIS-1252412]	ARPA-E Terra program; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	S.S.D. was supported by ARPA-E Terra program. Y.W. and A.S. were supported by the NSF CAREER grant IIS-1252412.	Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097; Anderson DG, 2015, JMLR WORKSH CONF PRO, V38, P19; Balcan M-F., 2016, C LEARNING THEORY, P284; Bunea F, 2015, BERNOULLI, V21, P1200, DOI 10.3150/14-BEJ602; Cai TT, 2016, ELECTRON J STAT, V10, P1, DOI 10.1214/15-EJS1081; Cai TT, 2012, ANN STAT, V40, P2389, DOI 10.1214/12-AOS998; Cai TT, 2013, PROBAB THEORY REL, V156, P101, DOI 10.1007/s00440-012-0422-7; Cai TT, 2010, ANN STAT, V38, P2118, DOI 10.1214/09-AOS752; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Chen YD, 2016, IEEE T INFORM THEORY, V62, P503, DOI 10.1109/TIT.2015.2499247; Donoho D, 2014, ANN STAT, V42, P2413, DOI 10.1214/14-AOS1257; Donoho DL, 2013, P NATL ACAD SCI USA, V110, P8405, DOI 10.1073/pnas.1306110110; Eriksson B., 2012, J MACH LEARN RES, V22, P373; Gavish M, 2014, IEEE T INFORM THEORY, V60, P5040, DOI 10.1109/TIT.2014.2323359; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kneip A, 2011, ANN STAT, V39, P2410, DOI 10.1214/11-AOS905; Koltchinskii V, 2011, ANN STAT, V39, P2302, DOI 10.1214/11-AOS894; Liu Z., 2015, P 9 ACM C RECOMMENDE, V15, P171, DOI DOI 10.1145/2792838.2800191; Mackey L, 2014, ANN PROBAB, V42, P906, DOI 10.1214/13-AOP892; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Tu S., 2015, ARXIV150703566; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Wang L., 2016, ARXIV161005275; Yi X., 2016, NIPS, P4152; Zhang L., 2015, ARXIV150406817; Zhu Z. Allen, 2016, ADV NEURAL INFORM PR, P974	32	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400043
C	Dudik, M; Lahaie, S; Rogers, R; Vaughan, JW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dudik, Miroslav; Lahaie, Sebastien; Rogers, Ryan; Vaughan, Jennifer Wortman			A Decomposition of Forecast Error in Prediction Markets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We analyze sources of error in prediction market forecasts in order to bound the difference between a security's price and the ground truth it estimates. We consider cost-function-based prediction markets in which an automated market maker adjusts security prices according to the history of trade. We decompose the forecasting error into three components: sampling error, arising because traders only possess noisy estimates of ground truth; market-maker bias, resulting from the use of a particular market maker (i.e., cost function) to facilitate trade; and convergence error, arising because, at any point in time, market prices may still be in flux. Our goal is to make explicit the tradeoffs between these error components, influenced by design decisions such as the functional form of the cost function and the amount of liquidity in the market. We consider a specific model in which traders have exponential utility and exponential-family beliefs representing noisy estimates of ground truth. In this setting, sampling error vanishes as the number of traders grows, but there is a tradeoff between the other two components. We provide both upper and lower bounds on market-maker bias and convergence error, and demonstrate via numerical simulations that these bounds are tight. Our results yield new insights into the question of how to set the market's liquidity parameter and into the forecasting benefits of enforcing coherent prices across securities.	[Dudik, Miroslav; Vaughan, Jennifer Wortman] Microsoft Res, New York, NY 10011 USA; [Lahaie, Sebastien] Google, New York, NY USA; [Rogers, Ryan] Univ Penn, Philadelphia, PA 19104 USA	Microsoft; Google Incorporated; University of Pennsylvania	Dudik, M (corresponding author), Microsoft Res, New York, NY 10011 USA.	mdudik@microsoft.com; slahaie@google.com; rrogers386@gmail.com; jenn@microsoft.com	Jeong, Yongwook/N-7413-2016					Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12; Abernethy Jacob, 2014, P 15 ACM C EC COMP E; Barndorff-Nielsen Ole, 1982, EXPONENTIAL FAMILIES; Berg J., 2008, HDB EXPT EC RESULTS, P742, DOI DOI 10.1016/S1574-0722(07)00080-7; Bousquet O., 2008, ADV NEURAL INFORM PR; Chen Yiling, 2010, P 11 ACM C EL COMM E; Chen Yiling, 2007, P 23 C UNC ART INT U; Dudik Miroslav, 2013, P 14 ACM C EL COMM E; Frongillo R., 2015, ADV NEURAL INFORM PR; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Olson Kenneth C., 2015, COLL INT; Othman A., 2013, ACM T EC COMPUTATION, V1, P14; Petersen Kaare Brandt, 2012, TECHNICAL REPORT; Rockafellar R. T., 2009, VARIATIONAL ANAL, DOI DOI 10.1007/978-3-030-63416-2_683; Rockafellar R. T., 1970, CONVEX ANAL; Rothschild D, 2009, PUBLIC OPIN QUART, V73, P895, DOI 10.1093/poq/nfp082; Slamka C, 2013, IEEE T ENG MANAGE, V60, P169, DOI 10.1109/TEM.2012.2191618; Wolfers J, 2004, J ECON PERSPECT, V18, P107, DOI 10.1257/0895330041371321	19	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404043
C	Dunner, C; Parnell, T; Jaggi, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dunner, Celestine; Parnell, Thomas; Jaggi, Martin			Efficient Use of Limited-Memory Accelerators for Linear Learning on Heterogeneous Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a generic algorithmic building block to accelerate training of machine learning models on heterogeneous compute systems. Our scheme allows to efficiently employ compute accelerators such as GPUs and FPGAs for the training of large-scale machine learning models, when the training data exceeds their memory capacity. Also, it provides adaptivity to any system's memory hierarchy in terms of size and processing speed. Our technique is built upon novel theoretical insights regarding primal-dual coordinate methods, and uses duality gap information to dynamically decide which part of the data should be made available for fast processing. To illustrate the power of our approach we demonstrate its performance for training of generalized linear models on a large-scale dataset exceeding the memory size of a modern GPU, showing an order-of-magnitude speedup over existing approaches.	[Dunner, Celestine; Parnell, Thomas] IBM Res Zurich, Zurich, Switzerland; [Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	International Business Machines (IBM); Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Dunner, C (corresponding author), IBM Res Zurich, Zurich, Switzerland.	cdu@zurich.ibm.com; tpa@zurich.ibm.com; martin.jaggi@epfl.ch	Jeong, Yongwook/N-7413-2016	Jaggi, Martin/0000-0003-1579-5558				Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Chang K. -W., 2011, P 17 ACM SIGKDD INT, P699; Csiba Dominik, 2015, ICML 2015 P 32 INT C; Dunner C., 2016, ICML, P783; Fercoq O, 2016, SIAM REV, V58, P739, DOI 10.1137/16M1085905; Heinze C, 2016, JMLR WORKSH CONF PRO, V51, P875; Matsushima S., 2012, P 18 ACM SIGKDD INT, P177; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Osokin A, 2016, PR MACH LEARN RES, V48; Parnell T., 2017, P 6 INT WORKSH PAR D; Perekrestenko D, 2017, PR MACH LEARN RES, V54, P869; Qu Z, 2016, OPTIM METHOD SOFTW, V31, P829, DOI 10.1080/10556788.2016.1190360; Smith V., 2016, COCOA GEN FRAMEWORK; Yen Ian En-Hsu, 2015, ADV NEURAL INFORM PR, V28, P3582; Yu H.-F., 2012, ACM T KNOWL DISCOV D, V5, P1; Zhao PL, 2015, PR MACH LEARN RES, V37, P1	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404032
C	Dutta, A; Vijayaraghavan, A; Wang, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dutta, Abhratanu; Vijayaraghavan, Aravindan; Wang, Alex			Clustering Stable Instances of Euclidean k-means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The Euclidean k-means problem is arguably the most widely-studied clustering problem in machine learning. While the k-means objective is NP-hard in the worst-case, practitioners have enjoyed remarkable success in applying heuristics like Lloyd's algorithm for this problem. To address this disconnect, we study the following question: what properties of real-world instances will enable us to design efficient algorithms and prove guarantees for finding the optimal clustering? We consider a natural notion called additive perturbation stability that we believe captures many practical instances of Euclidean k-means clustering. Stable instances have unique optimal k-means solutions that does not change even when each point is perturbed a little (in Euclidean distance). This captures the property that k-means optimal solution should be tolerant to measurement errors and uncertainty in the points. We design efficient algorithms that provably recover the optimal clustering for instances that are additive perturbation stable. When the instance has some additional separation, we can design a simple, efficient algorithm with provable guarantees that is also robust to outliers. We also complement these results by studying the amount of stability in real datasets, and demonstrating that our algorithm performs well on these benchmark datasets.	[Dutta, Abhratanu; Vijayaraghavan, Aravindan] Northwestern Univ, Evanston, IL 60208 USA; [Wang, Alex] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Northwestern University; Carnegie Mellon University	Dutta, A (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.	adutta@u.northwestern.edu; aravindv@northwestern.edu; alexwang@u.northwestern.edu	Vijayaraghavan, Aravindan/I-2257-2015; Jeong, Yongwook/N-7413-2016		National Science Foundation (NSF) [CCF-1637585]	National Science Foundation (NSF)(National Science Foundation (NSF))	Supported by the National Science Foundation (NSF) under Grant No. CCF-1637585.	Ackerman M., 2009, J MACH LEARN RES, V5, P1; Angelidakis Haris, 2017, S THEOR COMP STOC; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4; Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006; Awasthi Pranjal, 2015, ARXIV150203316, V34, P754; Balcan MF, 2016, SIAM J COMPUT, V45, P102, DOI 10.1137/140981575; Balcan MF, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1068; Ben-David Shai, 2015, ABS150100437 CORR; Ben-David S, 2014, THEOR COMPUT SCI, V558, P51, DOI 10.1016/j.tcs.2014.09.025; Bilu Yonatan, 2010, INNOVATIONS COMPUTER, P332; BLOCK HD, 1962, REV MOD PHYS, V34, P123, DOI 10.1103/RevModPhys.34.123; Blum Avrim, 2002, P S DISCR ALG SODA; Dasgupta S, 2008, HARDNESS K MEANS CLU; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; Makarychev Konstantin, 2014, P 22 S DISCR ALG SOD; Novikoff, 1962, P S MATH THEOR AUT, P615; Williams DE, 2011, PHARM BIOL, V49, P296, DOI 10.3109/13880209.2010.517540	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406055
C	El Housni, O; Goyal, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		El Housni, Omar; Goyal, Vineet			Beyond Worst-case: A Probabilistic Analysis of Affine Policies in Dynamic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ROBUST SOLUTIONS; INEQUALITIES	Affine policies (or control) are widely used as a solution approach in dynamic optimization where computing an optimal adjustable solution is usually intractable. While the worst case performance of affine policies can be significantly bad, the empirical performance is observed to be near-optimal for a large class of problem instances. For instance, in the two-stage dynamic robust optimization problem with linear covering constraints and uncertain right hand side, the worst-case approximation bound for affine policies is O(root m) that is also tight (see Bertsimas and Goyal [8]), whereas observed empirical performance is near-optimal. In this paper, we aim to address this stark-contrast between the worst-case and the empirical performance of affine policies. In particular, we show that affine policies give a good approximation for the two-stage adjustable robust optimization problem with high probability on random instances where the constraint coefficients are generated i.i.d. from a large class of distributions; thereby, providing a theoretical justification of the observed empirical performance. On the other hand, we also present a distribution such that the performance bound for affine policies on instances generated according to that distribution is Omega(root m) with high probability; however, the constraint coefficients are not i.i.d.. This demonstrates that the empirical performance of affine policies can depend on the generative model for instances.	[El Housni, Omar; Goyal, Vineet] Columbia Univ, IEOR Dept, New York, NY 10027 USA	Columbia University	El Housni, O (corresponding author), Columbia Univ, IEOR Dept, New York, NY 10027 USA.	oe2148@columbia.edu; vg2277@columbia.edu	Jeong, Yongwook/N-7413-2016					Ben-Tal A, 1998, MATH OPER RES, V23, P769, DOI 10.1287/moor.23.4.769; Ben-Tal A, 2004, MATH PROGRAM, V99, P351, DOI 10.1007/s10107-003-0454-y; Ben-Tal A, 2002, MATH PROGRAM, V92, P453, DOI 10.1007/s101070100286; Ben-Tal A, 1999, OPER RES LETT, V25, P1, DOI 10.1016/S0167-6377(99)00016-4; BenTal A, 2009, PRINC SER APPL MATH, P1; Bertsimas D, 2004, OPER RES, V52, P35, DOI 10.1287/opre.1030.0065; Bertsimas D, 2003, MATH PROGRAM, V98, P49, DOI 10.1007/s10107-003-0396-4; Bertsimas D, 2016, INFORMS J COMPUT, V28, P500, DOI 10.1287/ijoc.2016.0689; Bertsimas D, 2011, SIAM REV, V53, P464, DOI 10.1137/080734510; Bertsimas D, 2011, MATH OPER RES, V36, P24, DOI 10.1287/moor.1110.0482; Bertsimas D, 2012, MATH PROGRAM, V134, P491, DOI 10.1007/s10107-011-0444-4; Dantzig GB, 1955, MANAGE SCI, V1, P197, DOI 10.1287/mnsc.1.3-4.197; El Housni O., 2017, MATH PROGRAM, P1; ElGhaoui L, 1997, SIAM J MATRIX ANAL A, V18, P1035, DOI 10.1137/S0895479896298130; Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115; Feige U, 2007, LECT NOTES COMPUT SC, V4513, P439; Goldfarb D, 2003, MATH OPER RES, V28, P1, DOI 10.1287/moor.28.1.1.14260; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Kall P, 1994, STOCHASTIC PROGRAMMI; Shapiro A, 2008, MATH PROGRAM, V112, P183, DOI 10.1007/s10107-006-0090-4; SOYSTER AL, 1973, OPER RES, V21, P1154, DOI 10.1287/opre.21.5.1154	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404080
C	El Mhamdi, EM; Guerraoui, R; Hendrikx, H; Maurer, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		El Mhamdi, El Mandi; Guerraoui, Rachid; Hendrikx, Hadrien; Maurer, Alexandre			Dynamic Safe Interruptibility for Decentralized Multi-Agent Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In reinforcement learning, agents learn by performing actions and observing their outcomes. Sometimes, it is desirable for a human operator to interrupt an agent in order to prevent dangerous situations from happening. Yet, as part of their learning process, agents may link these interruptions, that impact their reward, to specific states and deliberately avoid them. The situation is particularly challenging in a multi-agent context because agents might not only learn from their own past interruptions, but also from those of other agents. Orseau and Armstrong [16] defined safe interruptibility for one learner, but their work does not naturally extend to multi-agent systems. This paper introduces dynamic safe interruptibility, an alternative definition more suited to decentralized learning problems, and studies this notion in two learning frameworks: joint action learners and independent learners. We give realistic sufficient conditions on the learning algorithm to enable dynamic safe interruptibility in the case of joint action learners, yet show that these conditions are not sufficient for independent learners. We show however that if agents can detect interruptions, it is possible to prune the observations to ensure dynamic safe interruptibility even for independent learners.	[El Mhamdi, El Mandi; Guerraoui, Rachid; Maurer, Alexandre] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Hendrikx, Hadrien] Ecole Polytech, Palaiseau, France	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Institut Polytechnique de Paris	Hendrikx, H (corresponding author), Ecole Polytech, Palaiseau, France.	elmandi.elmhamdi@epfl.ch; rachid.guerraoui@epfl.ch; hadrien.hendrikx@gmail.com; alexandre.maurer@epfl.ch	Jeong, Yongwook/N-7413-2016	El-Mhamdi, El-Mahdi/0000-0001-5041-1260	European ERC [339539 - AOC]; Swiss National Science Foundation [200021_169588]	European ERC(European Research Council (ERC)); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This work has been supported in part by the European ERC (Grant 339539 - AOC) and by the Swiss National Science Foundation (Grant 200021_169588 TARBDA).	Armstrong S., 2016, P 32 C UNC ART INT, P557; Boutilier C, 1996, THEORETICAL ASPECTS OF RATIONALITY AND KNOWLEDGE, P195; Claus C., 1998, P 15 NATIONALTENTH C, P752; Crites RH, 1998, MACH LEARN, V33, P235, DOI 10.1023/A:1007518724497; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Goertzel B., 2007, ARTIFICIAL GEN INTEL, V2; LAMPORT L, 1982, ACM T PROGR LANG SYS, V4, P382, DOI 10.1145/357172.357176; Lattimore T, 2011, LECT NOTES ARTIF INT, V6925, P368, DOI 10.1007/978-3-642-24412-4_29; Littman M. L., 2001, Cognitive Systems Research, V2, P55, DOI 10.1016/S1389-0417(01)00015-8; Littman ML, 1994, ICML 1994, P157; Matignon L, 2012, KNOWL ENG REV, V27, P1, DOI 10.1017/S0269888912000057; Mnih V., 2013, ARXIV PREPRINT ARXIV; Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2; Rodrigues Gomes E., 2009, P 26 ANN INT C MACHI, P369; Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tampuu A., 2015, ARXIV151108779; Tesauro G, 2004, ADV NEUR IN, V16, P871; Tesauro G, 2002, AUTON AGENT MULTI-AG, V5, P289, DOI 10.1023/A:1015504423309; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; Wang X., 2002, ADV NEURAL INFORM PR, P1603; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Wunder M., 2010, P 27 INT C MACH LEAR	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400013
C	Elhamifar, E; Kaluza, MCD		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Elhamifar, Ehsan; Kaluza, M. Clara De Paolis			Subset Selection and Summarization in Sequential Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Subset selection, which is the task of finding a small subset of representative items from a large ground set, finds numerous applications in different areas. Sequential data, including time-series and ordered data, contain important structural relationships among items, imposed by underlying dynamic models of data, that should play a vital role in the selection of representatives. However, nearly all existing subset selection techniques ignore underlying dynamics of data and treat items independently, leading to incompatible sets of representatives. In this paper, we develop a new framework for sequential subset selection that finds a set of representatives compatible with the dynamic models of data. To do so, we equip items with transition dynamic models and pose the problem as an integer binary optimization over assignments of sequential items to representatives, that leads to high encoding, diversity and transition potentials. Our formulation generalizes the well-known facility location objective to deal with sequential data, incorporating transition dynamics among facilities. As the proposed formulation is non-convex, we derive a max-sum message passing algorithm to solve the problem efficiently. Experiments on synthetic and real data, including instructional video summarization, show that our sequential subset selection framework not only achieves better encoding and diversity than the state of the art, but also successfully incorporates dynamics of data, leading to compatible representatives.	[Elhamifar, Ehsan; Kaluza, M. Clara De Paolis] Northeastern Univ, Comp & Informat Sci Coll, Boston, MA 02115 USA	Northeastern University	Elhamifar, E (corresponding author), Northeastern Univ, Comp & Informat Sci Coll, Boston, MA 02115 USA.	eelhami@ccs.neu.edu; clara@ccs.neu.edu			NSF [IIS-1657197]; Northeastern University, College of Computer and Information Science	NSF(National Science Foundation (NSF)); Northeastern University, College of Computer and Information Science	This work is supported by NSF IIS-1657197 award and startup funds from the Northeastern University, College of Computer and Information Science.	Affandi R. H., 2012, C UNC ART INT; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Alayrac J. - B., 2016, COMPUTER VISION PATT; Awasthi P., 2015, C INN THEOR COMP SCI; Bishop C. M., 2006, J ELECT IMAG, V16, P140; Borodin A., 2000, COMMUNICATIONS MATH, V211; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Carbonell J., 1998, SIGIR; Civril A., 2009, THEORETICAL COMPUTER, V410; Duda RO., 2004, PATTERN CLASSIFICATI; Elhamifar E., 2016, IEEE T PATTERN ANAL; Elhamifar E., 2015, AAAI C ART INT; Elhamifar E., 2014, WORLD C INT FED AUT; Elhamifar E., 2012, NEURAL INFORM PROCES; Elhamifar E, 2011, PROC CVPR IEEE; Elhamifar Ehsan, 2017, IEEE C COMP VIS PATT; Esser E, 2012, IEEE T IMAGE PROCESS, V21, P3239, DOI 10.1109/TIP.2012.2190081; Feige U., 1998, J ACM; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Garcia S, 2012, IEEE T PATTERN ANAL, V34, P417, DOI 10.1109/TPAMI.2011.142; Ghahramani Z., 1997, MACHINE LEARNING, V29; Ghahramani Z., 2008, NIPS; Gong B., 2014, NEURAL INFORM PROCES; GONZALEZ T. F., 1985, THEORETICAL COMPUTER, V38; Guyon I., 2003, J MACHINE LEARNING R; Gygli M., 2014, EUR C COMP VIS; Hadlock F., 1975, SIAM J COMPUTING, V4; Hartline J., 2008, WORLD WID WEB C; Joshi S, 2009, IEEE T SIGNAL PROCES, V57, P451, DOI 10.1109/TSP.2008.2007095; Kim G., 2011, INT C COMP VIS; Krause A., 2008, J MACHINE LEARNING R, V9; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Kulesza A., 2011, INT C MACH LEARN; Kulesza A., 2012, FDN TRENDS MACHINE L, V5; Lin H., 2012, C UNC ART INT; McSherry D., 2002, ADV CASE BASED REASO; Mirchandani P.B., 1990, DISCRETE LOCATION TH; Misra I., 2014, WINT C APPL COMP VIS; Motwani R., 1995, RANDOMIZED ALGORITHM; Nellore A., 2015, INFORM COMPUTATION; Nemhauser G. L., 1978, MATH PROGRAMMING, V14; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Reichart R., 2013, C ASS COMP LING; Shah A., 2013, C UNC ART INT; Simon I, 2007, IEEE I CONF COMP VIS, P274; Tschiatschek Sebastian, 2017, AAAI	46	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401008
C	Ene, A; Nguyen, HL; Vegh, LA		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ene, Alina; Nguyen, Huy L.; Vegh, Laszlo A.			Decomposable Submodular Function Minimization Discrete and Continuous	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHM	This paper investigates connections between discrete and continuous approaches for decomposable submodular function minimization We provide improved running time estimates for the state-of-the-art continuous algorithms for the problem using combinatorial arguments. We also provide a systematic experimental comparison of the two types of methods, based on a clear distinction between level-0 and level-1 algorithms.	[Ene, Alina] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA; [Nguyen, Huy L.] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA; [Vegh, Laszlo A.] London Sch Econ, Dept Math, London, England	Boston University; Northeastern University; University of London; London School Economics & Political Science	Ene, A (corresponding author), Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.	aene@bu.edu; hu.nguyen@northeastern.edu; L.Vegh@lse.ac.uk	Nguyen, Huy/AFN-7027-2022; Nguyen, Huy/GWQ-6433-2022					Arora C, 2012, LECT NOTES COMPUT SC, V7576, P17, DOI 10.1007/978-3-642-33715-4_2; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Chakrabarty D., 2014, ADV NEURAL INFORM PR, V1, P802, DOI 10.48550/arXiv.1411.0095; Edmonds Jack, 1970, COMBINATORIAL STRUCT, P69; Ene AR., 2015, P 32 INT C MACH LEAR; Fercoq O, 2015, SIAM J OPTIMIZ, V25, P1997, DOI 10.1137/130949993; Fix A, 2014, PROC CVPR IEEE, P1138, DOI 10.1109/CVPR.2014.149; Fix A, 2013, IEEE I CONF COMP VIS, P3104, DOI 10.1109/ICCV.2013.385; Fleischer L, 2003, DISCRETE APPL MATH, V131, P311, DOI 10.1016/S0166-218X(02)00458-4; FUJISHIGE S, 1980, MATH OPER RES, V5, P186, DOI 10.1287/moor.5.2.186; Fujishige S., 1992, JPN J IND APPL MATH, V9, P369, DOI 10.1007/BF03167272; Fujishige S, 2011, PAC J OPTIM, V7, P3; GOLDBERG AV, 1988, J ACM, V35, P921, DOI 10.1145/48014.61051; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Iwata S, 2003, SIAM J COMPUT, V32, P833, DOI 10.1137/S0097539701397813; Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096; Iwata S., 2009, ACM SIAM S DISCR ALG; Jegelka S.., 2011, P 28 INT C MACHINE L, P345; Jegelka S., 2011, ADV NEURAL INFORM PR, V24, P460; Jegelka S., 2013, ADV NEURAL INFORM PR; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025; Lee Y. T., 2015, IEEE FDN COMPUTER SC; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nishihara R., 2014, ADV NEURAL INFO PROC, P640; Orlin JB, 2009, MATH PROGRAM, V118, P237, DOI 10.1007/s10107-007-0189-2; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; SCHRIJVER A., 2003, COMBINATORIAL OPTIMI, V24; Shanu I, 2016, PROC CVPR IEEE, P5365, DOI 10.1109/CVPR.2016.579; Stobbe P., 2010, ADV NEURAL INFORM PR; WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402090
C	Fai, K; Wei, Q; Carin, L; Heller, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fai, Kai; Wei, Qi; Carin, Lawrence; Heller, Katherine			An Inner-loop Free Solution to Inverse Problems using Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FUSION	We propose a new method that uses deep learning techniques to accelerate the popular alternating direction method of multipliers (ADMM) solution for inverse problems. The ADMM updates consist of a proximity operator, a least squares regression that includes a big matrix inversion, and an explicit solution for updating the dual variables. Typically, inner loops are required to solve the first two sub-minimization problems due to the intractability of the prior and the matrix inversion. To avoid such drawbacks or limitations, we propose an inner-loop free update rule with two pre-trained deep convolutional architectures. More specifically, we learn a conditional denoising auto-encoder which imposes an implicit data-dependent prior/regularization on ground-truth in the first sub-minimization problem. This design follows an empirical Bayesian strategy, leading to so-called amortized inference. For matrix inversion in the second sub-problem, we learn a convolutional neural network to approximate the matrix inversion, i.e., the inverse mapping is learned by feeding the input through the learned forward network. Note that training this neural network does not require ground-truth or measurements, i.e., data-independent. Extensive experiments on both synthetic data and real datasets demonstrate the efficiency and accuracy of the proposed method compared with the conventional ADMM solution using inner loops for solving inverse problems.	[Fai, Kai; Wei, Qi; Carin, Lawrence; Heller, Katherine] Duke Univ, Durham, NC 27706 USA	Duke University	Fai, K (corresponding author), Duke Univ, Durham, NC 27706 USA.	kai.fan@stat.duke.edu; qi.wei@duke.edu; lcarin@duke.edu; kheller@stat.duke.edu	Jeong, Yongwook/N-7413-2016	Carin, Lawrence/0000-0001-6277-7948	Siemens Corporate Research	Siemens Corporate Research	The authors would like to thank Siemens Corporate Research for supporting this work and thank NVIDIA for the GPU donations.	Adler J., 2017, ARXIV170404058; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], [No title captured]; [Anonymous], 2010, P INT C MACH LEARN; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Bruna J., 2015, ARXIV151105666; Csaji B. C., 2001, THESIS; Deng L., 2014, FOUND TRENDS SIGNAL, V7, P197, DOI DOI 10.1561/2000000039; Dosovitskiy Alexey, 2016, NEURIPS; Elad M, 2006, IEEE T IMAGE PROCESS, V15, P3736, DOI 10.1109/TIP.2006.881969; GATYS LA, 2016, PROC CVPR IEEE, P2414, DOI DOI 10.1109/CVPR.2016.265; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lu ST, 2017, IEEE T SIGNAL PROCES, V65, P3120, DOI 10.1109/TSP.2017.2679687; MAURER H, 1979, MATH PROGRAM, V16, P98, DOI 10.1007/BF01582096; Nguyen Anh, 2016, ARXIV161200005; Schlemper J., 2017, ARXIV170300555; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Simoes M, 2015, IEEE T GEOSCI REMOTE, V53, P3373, DOI 10.1109/TGRS.2014.2375320; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Tarantola A., 2005, INVERSE PROBLEM THEO, p72; Tikhonov A. N, 1977, SCRIPTA SERIES MATH; Venkatakrishnan S, 2013, IEEE GLOB CONF SIG, P945, DOI 10.1109/GlobalSIP.2013.6737048; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wei Q, 2016, IEEE SIGNAL PROC LET, V23, P1632, DOI 10.1109/LSP.2016.2608858; Wei Q, 2015, IEEE T IMAGE PROCESS, V24, P4109, DOI 10.1109/TIP.2015.2458572; Wei Q, 2015, IEEE J-STSP, V9, P1117, DOI 10.1109/JSTSP.2015.2407855; Zhao NN, 2016, IEEE T IMAGE PROCESS, V25, P3683, DOI 10.1109/TIP.2016.2567075	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402041
C	Falahatgar, M; Hao, Y; Orlitsky, A; Pichapati, V; Ravindrakumar, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Falahatgar, Moein; Hao, Yi; Orlitsky, Alon; Pichapati, Venkatadheeraj; Ravindrakumar, Vaishakh			Maxing and Ranking with Few Assumptions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					PAC maximum selection (maxing) and ranking of n elements via random pairwise comparisons have diverse applications and have been studied under many models and assumptions. With just one simple natural assumption: strong stochastic transitivity, we show that maxing can be performed with linearly many comparisons yet ranking requires quadratically many. With no assumptions at all, we show that for the Borda-score metric, maximum selection can be performed with linearly many comparisons and ranking can be performed with O(n log n) comparisons.	[Falahatgar, Moein; Hao, Yi; Orlitsky, Alon; Pichapati, Venkatadheeraj; Ravindrakumar, Vaishakh] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Falahatgar, M (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	moein@ucsd.edu; yih179@ucsd.edu; alon@ucsd.edu; dheerajpv7@ucsd.edu; vaishakhr@ucsd.edu	Jeong, Yongwook/N-7413-2016		NSF [CIF-1564355, CIF-1619448]	NSF(National Science Foundation (NSF))	We thank NSF for supporting this work through grants CIF-1564355 and CIF-1619448.	Acharya J, 2014, IEEE INT SYMP INFO, P1682, DOI 10.1109/ISIT.2014.6875120; Acharya Jayadev, 2014, NIPS; Acharya Jayadev, 2016, ARXIV160602786; Ajtai M, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2701427; [Anonymous], 2017, NONTR DIC; Busa-Fekete Robert, 2014, INT C MACH LEARN, P1071; Busa-Fekete Robert, 2014, AAAI; Falahatgar M, 2017, PR MACH LEARN RES, V70; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; Heckel R., 2016, ARXIV160608842; Herbrich R., 2006, ADV NEURAL INFORM PR, P569; Jamieson K, 2015, JMLR WORKSH CONF PRO, V38, P416; Jang M., 2016, ARXIV160304153; Lee D. T., 2014, 2 AAAI C HUM COMP CR; Luce R. D., 2005, INDIVIDUAL CHOICE BE; Negahban S., 2016, OPERATIONS RES; Negahban S., 2012, ADV NEURAL INFORM PR, P2474; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; Radlinski F., 2008, P 17 ACM C INF KNOWL, P43, DOI DOI 10.1145/1458082.1458092; Radlinski F, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P570; Rajkumar A, 2014, PR MACH LEARN RES, V32; Szorenyi B., 2015, ADV NEURAL INFORM PR, V28, P604; Urvoy T., 2013, INT C MACH LEARN, V28, P91; Wang JL, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P502, DOI 10.1145/2623330.2623730; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Zhou Y, 2014, PR MACH LEARN RES, V32, P217; Zhou Yuan, 2014, OPTIMAL PAC MULTIPLE	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407015
C	Fang, C; Cheng, F; Lin, ZC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fang, Cong; Cheng, Feng; Lin, Zhouchen			Faster and Non-ergodic O(1/K) Stochastic Alternating Direction Method of Multipliers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE RATE	We study stochastic convex optimization subjected to linear equality constraints. Traditional Stochastic Alternating Direction Method of Multipliers [1] and its Nesterov's acceleration scheme [2] can only achieve ergodic O(1/ root K) convergence rates, where K is the number of iteration. By introducing Variance Reduction (VR) techniques, the convergence rates improve to ergodic O(1/ root K) [3, 4]. In this paper, we propose a new stochastic ADMM which elaborately integrates Nesterov's extrapolation and VR techniques. With Nesterov's extrapolation, our algorithm can achieve a non-ergodic O(1/K) convergence rate which is optimal for separable linearly constrained non-smooth convex problems, while the convergence rates of VR based ADMM methods are actually tight O(1/ root K) in non-ergodic sense. To the best of our knowledge, this is the first work that achieves a truly accelerated, stochastic convergence rate for constrained convex problems. The experimental results demonstrate that our algorithm is faster than the existing state-of-the-art stochastic ADMM methods.	[Lin, Zhouchen] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China; Shanghai Jiao Tong Univ, Cooperat Medianet Innovat Ctr, Shanghai, Peoples R China	Peking University; Shanghai Jiao Tong University	Lin, ZC (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.	fangcong@pku.edu.cn; fengcheng@pku.edu.cn; zlin@pku.edu.cn	Jeong, Yongwook/N-7413-2016		National Basic Research Program of China (973 Program) [2015CB352502]; National Natural Science Foundation (NSF) of China [61625301, 61731018, 61231002]	National Basic Research Program of China (973 Program)(National Basic Research Program of China); National Natural Science Foundation (NSF) of China(National Natural Science Foundation of China (NSFC))	Zhouchen Lin is supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502) and National Natural Science Foundation (NSF) of China (grant no.s 61625301, 61731018, and 61231002).	Argyriou Andreas, 2007, P C ADV NEUR INF PRO; AzadiSra Samaneh, 2014, P INT C MACH LEARN; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bottou L, 2004, LECT NOTES ARTIF INT, V3176, P146; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Chen CH, 2015, SIAM J IMAGING SCI, V8, P2239, DOI 10.1137/15100463X; Davis D, 2016, SCI COMPUT, P115, DOI 10.1007/978-3-319-41589-5_4; Defazio A, 2014, ADV NEUR IN, V27; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; He BS, 2012, SIAM J NUMER ANAL, V50, P700, DOI 10.1137/110836936; Hua Ouyang, 2013, P INT C MACH LEARN; Johnson Rie, 2013, P C ADV NEUR INF PRO; Li Huan, 2016, ARXIV160806366; Li Shen, 2015, P INT JOINT C ART IN; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Lin ZC, 2015, MACH LEARN, V99, P287, DOI 10.1007/s10994-014-5469-5; Lin Zhouchen, 2011, P C ADV NEUR INF PRO; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y., 1988, MAT METODY RESHENIYA, V24, P509; Schmidt MA, 2013, MAKING IN AMERICA: FROM INNOVATION TO MARKET, P1, DOI 10.1007/s10107-016-1030-6; Suzuki T, 2014, PR MACH LEARN RES, V32; Suzuki Taiji, 2013, P INT C MACH LEARN; Wang KY, 2016, IEEE T PATTERN ANAL, V38, P2010, DOI 10.1109/TPAMI.2015.2505311; Zeyuan Allen-Zhu, 2017, ANN S THEOR COMP; Zhang XQ, 2011, J SCI COMPUT, V46, P20, DOI 10.1007/s10915-010-9408-8; Zheng Shuai, 2016, P INT JOINT C ART IN; Zhong Wenliang, 2014, P INT C MACH LEARN; Zuo WM, 2011, IEEE T IMAGE PROCESS, V20, P2748, DOI 10.1109/TIP.2011.2131665	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404053
C	Farhan, M; Tariq, J; Zaman, A; Shabbir, M; Khan, IU		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Farhan, Muhammad; Tariq, Juvaria; Zaman, Arif; Shabbir, Mudassir; Khan, Imdad Ullah			Efficient Approximation Algorithms for String Kernel Based Sequence Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Sequence classification algorithms, such as SVM, require a definition of distance (similarity) measure between two sequences. A commonly used notion of similarity is the number of matches between k-mers (k-length subsequences) in the two sequences. Extending this definition, by considering two k-mers to match if their distance is at most m, yields better classification performance. This, however, makes the problem computationally much more complex. Known algorithms to compute this similarity have computational complexity that render them applicable only for small values of k and m. In this work, we develop novel techniques to efficiently and accurately estimate the pairwise similarity score, which enables us to use much larger values of k and m, and get higher predictive accuracy. This opens up a broad avenue of applying this classification approach to audio, images, and text sequences. Our algorithm achieves excellent approximation performance with theoretical guarantees. In the process we solve an open combinatorial problem, which was posed as a major hindrance to the scalability of existing solutions. We give analytical bounds on quality and runtime of our algorithm and report its empirical performance on real world biological and music sequences datasets.	[Farhan, Muhammad; Zaman, Arif; Khan, Imdad Ullah] Lahore Univ Management Sci, Sch Sci & Engn, Dept Comp Sci, Lahore, Pakistan; [Tariq, Juvaria] Lahore Univ Management Sci Lahore, Sch Sci & Engn, Dept Math, Lahore, Pakistan; [Shabbir, Mudassir] Informat Technol Univ, Dept Comp Sci, Lahore, Pakistan	Lahore University of Management Sciences	Farhan, M (corresponding author), Lahore Univ Management Sci, Sch Sci & Engn, Dept Comp Sci, Lahore, Pakistan.	14030031@lums.edu.pk; jtariq@emory.edu; arifz@lums.edu.pk; mudassir.shabbir@itu.edu.pk; imdad.khan@lums.edu.pk	Khan, Imdadullah/G-1037-2016; Jeong, Yongwook/N-7413-2016; Farhan, Muhammad/F-8071-2011	Khan, Imdadullah/0000-0002-6955-6168; Farhan, Muhammad/0000-0002-3649-5717				Bach F.R., 2005, P 22 INT C MACH LEAR, P33, DOI DOI 10.1145/1102351.1102356; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cheng JL, 2006, BIOINFORMATICS, V22, P1456, DOI 10.1093/bioinformatics/btl102; Cristianini Nello, 2000, INTRO SUPPORT VECTOR, DOI DOI 10.1017/CBO9780511801389; Ding CHQ, 2001, BIOINFORMATICS, V17, P349, DOI 10.1093/bioinformatics/17.4.349; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Ellis DPW., 2007, ISMIR, V7, P339; Haussler D, 1999, CONVOLUTION KERNELS, P95; Kuang Rui, 2005, Journal of Bioinformatics and Computational Biology, V3, P527, DOI 10.1142/S021972000500120X; Kuksa P., 2012, SIAM INT C DATA MINI, P873; Kuksa P., 2008, P 19 INT C PATT REC, P1; Kuksa P., 2011, THESIS; Kuksa P. P., 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P3320, DOI 10.1109/ICPR.2010.1159; Kuksa P. P., 2013, CORR; Kuksa P. P, 2009, ADV NEURAL INFORM PR, P881; Kuksa P, 2010, LECT NOTES ARTIF INT, V6322, P128, DOI 10.1007/978-3-642-15883-4_9; Leslie C., 2003, ADV NEURAL INFORM PR, P1441; Leslie C., 2002, P PSB, V575, P564; Li T., 2003, P 26 ANN INT ACM SIG, P282, DOI 10.1109/ISSPA.2003.1224828; Lo Conte L, 2000, NUCLEIC ACIDS RES, V28, P257, DOI 10.1093/nar/28.1.257; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Smola Alexander J., 2002, NIPS, P569; Sonnenburg S., 2005, P 22 INT C MACH LEAR, P848, DOI DOI 10.1145/1102351.1102458; Tzanetakis G, 2002, IEEE T SPEECH AUDI P, V10, P293, DOI 10.1109/TSA.2002.800560; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Waterman M., PHYLOGENETIC ANAL DN, P59; Watkins C, 2000, ADV NEUR IN, P39; Weston J, 2004, ADV NEUR IN, V16, P595; Weston J, 2005, BIOINFORMATICS, V21, P3241, DOI 10.1093/bioinformatics/bti497; Yang T., 2012, ADV NEURAL INFORM PR, P476	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407003
C	Fruit, R; Pirotta, M; Lazaric, A; Brunskill, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fruit, Ronan; Pirotta, Matteo; Lazaric, Alessandro; Brunskill, Emma			Regret Minimization in MDPs with Options without Prior Knowledge	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MARKOV; BOUNDS	The option framework integrates temporal abstraction into the reinforcement learning model through the introduction of macro-actions (i.e., options). Recent works leveraged the mapping of Markov decision processes (MDPs) with options to semi-MDPs (SMDPs) and introduced SMDP-versions of exploration-exploitation algorithms (e.g., RMAx - SMDP and UCRL - SMDP) to analyze the impact of options on the learning performance. Nonetheless, the PAC-SMDP sample complexity of RMAx-SMDP can hardly be translated into equivalent PAC-MDP theoretical guarantees, while the regret analysis of UCRL SMDP requires prior knowledge of the distributions of the cumulative reward and duration of each option, which are hardly available in practice. In this paper, we remove this limitation by combining the SMDP view together with the inner Markov structure of options into a novel algorithm whose regret performance matches UCRL-SMDP's up to an additive regret term. We show scenarios where this term is negligible and the advantage of temporal abstraction is preserved. We also report preliminary empirical results supporting the theoretical findings.	[Fruit, Ronan; Pirotta, Matteo; Lazaric, Alessandro] Inria Lille, Sequel Team, Lille, France; [Brunskill, Emma] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Fruit, R (corresponding author), Inria Lille, Sequel Team, Lille, France.	ronan.fruit@inria.fr; matteo.pirotta@inria.fr; alessandro.lazaric@inria.fr; ebrun@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		French Ministry of Higher Education and Research; French National Research Agency (ANR) [ANR-14-CE24-0010-01]; Nord-Pas-de-Calais Regional Council	French Ministry of Higher Education and Research; French National Research Agency (ANR)(French National Research Agency (ANR)); Nord-Pas-de-Calais Regional Council(Region Hauts-de-France)	This research was supported in part by French Ministry of Higher Education and Research, Nord-Pas-de-Calais Regional Council and French National Research Agency (ANR) under project ExTra-Learn (n. ANR-14-CE24-0010-01).	[Anonymous], ICML; [Anonymous], 2001, P 18 INT C MACHINE L; BARTLETT PL, 2009, P 25 C UNC ART INT, P35; Bremaud Pierre, 1999, APPL PROBABILITY MOD; Bremaud Pierre, 1999, APPL PROBABILITY MOD; Brunskill E, 2014, PR MACH LEARN RES, V32, P316; Castro Pablo Samuel, 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P140, DOI 10.1007/978-3-642-29946-9_16; Cho GE, 2001, LINEAR ALGEBRA APPL, V335, P137, DOI 10.1016/S0024-3795(01)00320-2; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; FEDERGRUEN A, 1983, MATH OPER RES, V8, P298, DOI 10.1287/moor.8.2.298; Fruit R, 2017, PR MACH LEARN RES, V54, P576; Hsu D. J., 2015, ADV NEURAL INFORM PR, P1459; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jong Nicholas K., 2008, 7 INT JOINT C AUT AG; Kirkland SJ, 2008, NUMER MATH, V110, P521, DOI 10.1007/s00211-008-0172-8; Levy Kfir Y, 2011, RECENT ADV REINFORCE, P153, DOI [10.1007/978-3-642-29946-9_17, DOI 10.1007/978-3-642-29946-9_17]; Mann TA, 2014, PR MACH LEARN RES, V32; Menache I, 2002, LECT NOTES ARTIF INT, V2430, P295; Ortner R, 2008, MIND MACH, V18, P521, DOI 10.1007/s11023-008-9115-5; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Sairamesh Munu, 2012, Recent Advances in Reinforcement Learning. 9th European Workshop (EWRL 2011). Revised Selected Papers, P165, DOI 10.1007/978-3-642-29946-9_18; SENETA E, 1993, STAT PROBABIL LETT, V17, P163, DOI 10.1016/0167-7152(93)90011-7; Simsek O, 2004, P ICML; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tessler C, 2017, AAAI CONF ARTIF INTE, P1553; Wainwright Martin, 2015, COURSE MATH STAT	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403023
C	Garber, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Garber, Dan			Efficient Online Linear Optimization with Approximation Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We revisit the problem of online linear optimization in case the set of feasible actions is accessible through an approximated linear optimization oracle with a factor alpha multiplicative approximation guarantee. This setting is in particular interesting since it captures natural online extensions of well-studied offline linear optimization problems which are NP-hard, yet admit efficient approximation algorithms. The goal here is to minimize the alpha-regret which is the natural extension of the standard regret in online learning to this setting. We present new algorithms with significantly improved oracle complexity for both the full information and bandit variants of the problem. Mainly, for both variants, we present alpha-regret bounds of O(T-1/3), were T is the number of prediction rounds, using only O(log(T)) calls to the approximation oracle per iteration, on average. These are the first results to obtain both average oracle complexity of O(log(T)) (or even poly-logarithmic in T) and alpha-regret bound O(T-c) for a constant c > 0, for both variants.	[Garber, Dan] Technion Israel Inst Technol, Haifa, Israel	Technion Israel Institute of Technology	Garber, D (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	dangar@technion.ac.il	Jeong, Yongwook/N-7413-2016					Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Balcan M.-F., 2006, P 7 ACM C EL COMM, P29; BARYEHUDA R, 1981, J ALGORITHM, V2, P198, DOI 10.1016/0196-6774(81)90020-1; Bubeck S., 2015, FDN TRENDS MACHINE L; Christofides N., 1976, 388 CARN MELL U PITT; Chvatal V., 1979, Mathematics of Operations Research, V4, P233, DOI 10.1287/moor.4.3.233; Fujita T, 2013, LECT NOTES ARTIF INT, V8139, P68; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Hazan E, 2016, PR MACH LEARN RES, V48; Hazan Elad, 2014, J MACHINE LEARNING R, V35, P408; Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Papadimitriou CH, 2008, J ACM, V55, DOI 10.1145/1379759.1379762; Weinberg S. M., 2014, THESIS; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400060
C	Garg, VK; Jaakkola, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Garg, Vikas K.; Jaakkola, Tommi			Local Aggregative Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Aggregative games provide a rich abstraction to model strategic multi-agent interactions. We introduce local aggregative games, where the payoff of each player is a function of its own action and the aggregate behavior of its neighbors in a connected digraph. We show the existence of a pure strategy epsilon-Nash equilibrium in such games when the payoff functions are convex or sub-modular. We prove an information theoretic lower bound, in a value oracle model, on approximating the structure of the digraph with non-negative monotone sub-modular cost functions on the edge set cardinality. We also define a new notion of structural stability, and introduce gamma-aggregative games that generalize local aggregative games and admit epsilon-Nash equilibrium that is stable with respect to small changes in some specified graph property. Moreover, we provide algorithms for our models that can meaningfully estimate the game structure and the parameters of the aggregator function from real voting data.	[Garg, Vikas K.; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Garg, VK (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	vgarg@csail.mit.edu; tommi@csail.mit.edu						Azrieli Y, 2013, MATH OPER RES, V38, P350, DOI 10.1287/moor.1120.0557; Bach F., 2010, NIPS; Balcan M., 2011, STOC; Bonami P., 2012, IMA VOLUMES MATH ITS, V154; Bonami P, 2008, DISCRETE OPTIM, V5, P186, DOI 10.1016/j.disopt.2006.10.011; Bowers Jeremy, 2014, NY TIMES; Boyd S., 2011, FDN TRENDS MACHINE L, V3; Cummings R., 2015, WINE; Daskalakis C, 2015, J ECON THEORY, V156, P207, DOI 10.1016/j.jet.2014.02.002; Defazio  A., 2012, NIPS; Feige U., 2007, FOCS; Feller W., 1957, INTRO PROBABILITY TH, V1; Garg V. K., 2016, NIPS; GILBERT EN, 1959, ANN MATH STAT, V30, P1141, DOI 10.1214/aoms/1177706098; Goel G., 2009, FOCS; Goemans M., 2009, SODA; Honorio J, 2015, J MACH LEARN RES, V16, P1157; Jensen MK, 2010, ECON THEOR, V43, P45, DOI 10.1007/s00199-008-0419-8; Kalai E, 2004, ECONOMETRICA, V72, P1631, DOI 10.1111/j.1468-0262.2004.00549.x; KEARNS MJ, 2002, UAI; LAFFERTY J, 2001, ICML; Lasry JM, 2007, JAP J MATH, V2, P229, DOI 10.1007/s11537-007-0657-8; NASH J, 1951, ANN MATH, V54, P286, DOI 10.2307/1969529; NOVSHEK W, 1985, REV ECON STUD, V52, P85, DOI 10.2307/2297471; Selten R, 1970, PREISPOLITIK MEHRPRO, P113; Svitkina Z., 2008, FOCS; Taskar Ben, 2003, NIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Yao A. C.-C., 1977, FOCS; Zaheer Manzil, 2017, ARXIV170306114	31	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405041
C	Geumlek, J; Song, S; Chaudhuri, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Geumlek, Joseph; Song, Shuang; Chaudhuri, Kamalika			Renyi Differential Privacy Mechanisms for Posterior Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					With the newly proposed privacy definition of Renyi Differential Privacy (RDP) in [14], we re-examine the inherent privacy of releasing a single sample from a posterior distribution. We exploit the impact of the prior distribution in mitigating the influence of individual data points. In particular, we focus on sampling from an exponential family and specific generalized linear models, such as logistic regression. We propose novel RDP mechanisms as well as offering a new RDP analysis for an existing method in order to add value to the RDP framework. Each method is capable of achieving arbitrary RDP privacy guarantees, and we offer experimental results of their efficacy.	[Geumlek, Joseph; Song, Shuang; Chaudhuri, Kamalika] Univ Calif San Diego, San Diego, CA 92103 USA	University of California System; University of California San Diego	Geumlek, J (corresponding author), Univ Calif San Diego, San Diego, CA 92103 USA.	jgeumlek@cs.ucsd.edu; shs037@eng.ucsd.edu; kamalika@cs.ucsd.edu	song, song/GWN-2626-2022		NSF [IIS 1253942]; ONR [N00014-16-1-2616]; Google Faculty Research Award	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Google Faculty Research Award(Google Incorporated)	This work was partially supported by NSF under IIS 1253942, ONR under N00014-16-1-2616, and a Google Faculty Research Award.	Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chaudhuri K., 2014, NEURAL INF PROCESSIN; Chaudhuri K, 2006, LECT NOTES COMPUT SC, V4117, P198; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork C., 2016, ARXIV PREPRINT ARXIV; Dwork C, 2014, ALGORITHMIC FDN DIFF, V9; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2009, ACM S THEORY COMPUT, P371; Foulds J. R., 2016, P 32 C UNC ART INT U; Machanavajjhala A, 2008, PROC INT CONF DATA, P277, DOI 10.1109/ICDE.2008.4497436; McSherry F., 2017, MANY SECRETS YOU HAV; Minami K., 2016, ADV NEURAL INFORM PR, P956; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Sarwate AD, 2013, IEEE SIGNAL PROC MAG, V30, P86, DOI 10.1109/MSP.2013.2259911; Thakurta Abhradeep Guha, 2013, P 26 ANN C LEARN THE, P819; Vempala S., 2005, COMBINATORIAL COMPUT, V52, P573; Wang YX, 2016, LECT NOTES COMPUT SC, V9867, P121, DOI 10.1007/978-3-319-45381-1_10; Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Zhang Z., 2016, P 13 AAAI C ART INT	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405036
C	Gioyannucci, A; Friedrich, J; Kaufman, M; Churchland, AK; Chkloyskii, D; Paninski, L; Pneymatikakis, EA		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gioyannucci, Andrea; Friedrich, Johannes; Kaufman, Matthew; Churchland, Anne K.; Chkloyskii, Dmitri; Paninski, Liam; Pneymatikakis, Eftychios A.			OnACID: Online Analysis of Calcium Imaging Data in Real Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CELLULAR RESOLUTION; SCALE; DECONVOLUTION; SIGNALS; AWAKE	Optical imaging methods using calcium indicators are critical for monitoring the activity of large neuronal populations in vivo. Imaging experiments typically generate a large amount of data that needs to be processed to extract the activity of the imaged neuronal sources. While deriving such processing algorithms is an active area of research, most existing methods require the processing of large amounts of data at a time, rendering them vulnerable to the volume of the recorded data, and preventing real-time experimental interrogation. Here we introduce OnACID, an Online framework for the Analysis of streaming Calcium Imaging Data, including i) motion artifact correction, ii) neuronal source extraction, and iii) activity denoising and deconvolution. Our approach combines and extends previous work on online dictionary learning and calcium imaging data analysis, to deliver an automated pipeline that can discover and track the activity of hundreds of cells in real time, thereby enabling new types of closed-loop experiments. We apply our algorithm on two large scale experimental datasets, benchmark its performance on manually annotated data, and show that it outperforms a popular offline approach.	[Gioyannucci, Andrea; Friedrich, Johannes; Chkloyskii, Dmitri; Pneymatikakis, Eftychios A.] Flatiron Inst, New York, NY 10010 USA; [Kaufman, Matthew; Churchland, Anne K.] Cold Spring Harbor Lab, POB 100, Cold Spring Harbor, NY 11724 USA; [Friedrich, Johannes; Paninski, Liam] Columbia Univ, New York, NY 10027 USA	Cold Spring Harbor Laboratory; Columbia University	Pneymatikakis, EA (corresponding author), Flatiron Inst, New York, NY 10010 USA.	agiovannucci@flatironinstitute.org; jfriedrich@flatironinstitute.org; mkaufman@cshl.edu; churchland@cshl.edu; dchklovskii@flatironinstitute.org; liam@stat.columbia.edu; epnevmatikakis@flatironinstitute.org	Giovannucci, Andrea/AAH-8084-2021; Jeong, Yongwook/N-7413-2016; Kaufman, Matthew/AAK-2860-2020; Friedrich, Johannes/GXH-9356-2022	Giovannucci, Andrea/0000-0002-7850-444X; Kaufman, Matthew/0000-0002-8072-023X; Friedrich, Johannes/0000-0002-1321-5866	Simons Foundation; SNSF [P300P2_ 158428]; NIH BRAIN Initiative [R01EB22913]; DARPA [N66001-15-C-4032]; IARPA MICRONS [D16PC00003]	Simons Foundation; SNSF(Swiss National Science Foundation (SNSF)); NIH BRAIN Initiative; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); IARPA MICRONS	We thank Sue Ann Koay, Jeff Gauthier and David Tank (Princeton University) for sharing their cortex and hippocampal data with us. We thank Lindsey Myers, Sonia Villani and Natalia Roumelioti for providing manual annotations. We thank Daniel Barabasi (Cold Spring Harbor Laboratory) for useful discussions. AG, DC, and EAP were internally funded by the Simons Foundation. Additional support was provided by SNSF P300P2_ 158428 (JF), and NIH BRAIN Initiative R01EB22913, DARPA N66001-15-C-4032, IARPA MICRONS D16PC00003 (LP).	Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/NMETH.2434, 10.1038/nmeth.2434]; Apthorpe N., 2016, ADV NEURAL INFORM PR, P3270; Asin-Acha R., 2017, ARXIV170301999; Barlow R. E., 1972, STAT INFERENCE ORDER; Bouchard MB, 2015, NAT PHOTONICS, V9, P113, DOI [10.1038/nphoton.2014.323, 10.1038/NPHOTON.2014.323]; Boyden ES, 2005, NAT NEUROSCI, V8, P1263, DOI 10.1038/nn1525; Clancy KB, 2014, NAT NEUROSCI, V17, P807, DOI 10.1038/nn.3712; Dombeck DA, 2007, NEURON, V56, P43, DOI 10.1016/j.neuron.2007.08.003; Emiliani V, 2015, J NEUROSCI, V35, P13917, DOI 10.1523/JNEUROSCI.2916-15.2015; Freeman J, 2014, NAT METHODS, V11, P941, DOI [10.1038/nmeth.3041, 10.1038/NMETH.3041]; Friedrich J, 2017, PLOS COMPUT BIOL, V13, DOI 10.1371/journal.pcbi.1005423; Friedrich Johannes, 2016, BIORXIV; Garg Sahil, 2017, ARXIV170106106; Giovannucci A., 2017, COSYNE ABSTRACTS; Greenberg DS, 2009, J NEUROSCI METH, V176, P1, DOI 10.1016/j.jneumeth.2008.08.020; Grosenick L, 2015, NEURON, V86, P106, DOI 10.1016/j.neuron.2015.03.034; Guizar-Sicairos M, 2008, OPT LETT, V33, P156, DOI 10.1364/OL.33.000156; Hu T, 2014, CONF REC ASILOMAR C, P613, DOI 10.1109/ACSSC.2014.7094519; Kaifosh P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00080; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mukamel EA, 2009, NEURON, V63, P747, DOI 10.1016/j.neuron.2009.08.009; O'Shea DJ, 2017, EXP NEUROL, V287, P437, DOI 10.1016/j.expneurol.2016.08.003; Pachitariu M., 2017, BIORXIV; Pachitariu M., 2013, ADV NEURAL INFORM PR; Packer AM, 2015, NAT METHODS, V12, P140, DOI 10.1038/nmeth.3217; Pnevmatikakis EA, 2017, J NEUROSCI METH, V291, P83, DOI 10.1016/j.jneumeth.2017.07.031; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; Reynolds S, 2017, ENEURO, V4, DOI 10.1523/ENEURO.0012-17.2017; Smith SL, 2010, NAT NEUROSCI, V13, P1144, DOI 10.1038/nn.2620; Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009; Walker T, 2014, CELL MAGIC WAND TOOL	31	0	0	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402042
C	Greenewald, K; Tewari, A; Klasnja, P; Murphy, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Greenewald, Kristjan; Tewari, Ambuj; Klasnja, Predrag; Murphy, Susan			Action Centered Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Contextual bandits have become popular as they offer a middle ground between very simple approaches based on multi-armed bandits and very complex approaches using the full power of reinforcement learning. They have demonstrated success in web applications and have a rich body of associated theoretical guarantees. Linear models are well understood theoretically and preferred by practitioners because they are not only easily interpretable but also simple to implement and debug. Furthermore, if the linear model is true, we get very strong performance guarantees. Unfortunately, in emerging applications in mobile health, the time-invariant linear model assumption is untenable. We provide an extension of the linear model for contextual bandits that has two parts: baseline reward and treatment effect. We allow the former to be complex but keep the latter simple. We argue that this model is plausible for mobile health applications. At the same time, it leads to algorithms with strong performance guarantees as in the linear model setting, while still allowing for complex nonlinear baseline modeling. Our theory is supported by experiments on data gathered in a recently concluded mobile health study.	[Greenewald, Kristjan; Murphy, Susan] Harvard Univ, Dept Stat, Cambridge, MA 02138 USA; [Tewari, Ambuj] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; [Klasnja, Predrag] Univ Michigan, Sch Informat, Ann Arbor, MI 48109 USA; [Murphy, Susan] Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA	Harvard University; University of Michigan System; University of Michigan; University of Michigan System; University of Michigan; Harvard University	Greenewald, K (corresponding author), Harvard Univ, Dept Stat, Cambridge, MA 02138 USA.	kgreenewald@fas.harvard.edu; tewaria@umich.edu; klasnja@umich.edu; samurphy@fas.harvard.edu	Jeong, Yongwook/N-7413-2016		NHLBI/NIA [R01 AA023187, P50 DA039838, U54EB020404, R01 HL125440]; NSF CAREER [IIS-1452099]; Sloan Research Fellowship	NHLBI/NIA; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	This work was supported in part by grants R01 AA023187, P50 DA039838, U54EB020404, R01 HL125440 NHLBI/NIA, NSF CAREER IIS-1452099, and a Sloan Research Fellowship.	Abe N, 1999, MACHINE LEARNING, PROCEEDINGS, P12; Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; Bastani H, 2015, ONLINE DECISION MAKI; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Dudik M., 2011, UNCERTAINTY ARTIFICI; Klasnja P, 2015, HEALTH PSYCHOL, V34, P1220, DOI 10.1037/hea0000305; Li L., 2011, PROC 4 ACM INT C WEB, P297, DOI DOI 10.1145/1935826.1935878; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Liao P, 2015, STAT MED; May BC, 2012, J MACH LEARN RES, V13, P2069; Murphy S. A., 2017, MOBILE HLTH SENSORS; Puterman M. L., 1994, MARKOV DECISION PROC; Seldin Y., 2011, NIPS, V24, P1683; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Valko M., 2013, P 20 9 C UNCERTAINTY, P654	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406006
C	Greff, K; van Steenkiste, S; Schmidhuber, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Greff, Klaus; van Steenkiste, Sjoerd; Schmidhuber, Juergen			Neural Expectation Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				THRESHOLDING ALGORITHM; BINDING; MODEL; SPARSE	Many real world tasks such as reasoning and physical interaction require identification and manipulation of conceptual entities. A first step towards solving these tasks is the automated discovery of distributed symbol-like representations. In this paper, we explicitly formalize this problem as inference in a spatial mixture model where each component is parametrized by a neural network. Based on the Expectation Maximization framework we then derive a differentiable clustering method that simultaneously learns how to group and represent individual entities. We evaluate our method on the (sequential) perceptual grouping task and find that it is able to accurately recover the constituent objects. We demonstrate that the learned representations are useful for next-step prediction.	[Greff, Klaus; van Steenkiste, Sjoerd; Schmidhuber, Juergen] IDSIA, Manno, Switzerland	Universita della Svizzera Italiana	Greff, K (corresponding author), IDSIA, Manno, Switzerland.	klaus@idsia.ch; sjoerd@idsia.ch; juergen@idsia.ch	Peters, Jan/P-6027-2019	Peters, Jan/0000-0002-5266-8091	Swiss National Science Foundation [200021_165675/1]; EU [687795]; NVIDIA Corporation	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); EU(European Commission); NVIDIA Corporation	The authors wish to thank Paulo Rauber and the anonymous reviewers for their constructive feedback. This research was supported by the Swiss National Science Foundation grant 200021_165675/1 and the EU project "INPUT" (H2020-ICT-2015 grant no. 687795). We are grateful to NVIDIA Corporation for donating us a DGX-1 as part of the Pioneers of AI Research award, and to IBM for donating a "Minsky" machine.	[Anonymous], 2010, P INT C MACH LEARN; [Anonymous], 2016, DECONVOLUTION CHECKE; Barlow HB, 1989, NEURAL COMPUT, V1, P412, DOI 10.1162/neco.1989.1.3.412; Beck A, 2009, INT CONF ACOUST SPEE, P693, DOI 10.1109/ICASSP.2009.4959678; Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1; Chen X., 2016, ARXIV160603657, P2172; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Greff K., 2015, ARXIV151106418CS; Greff Klaus, 2016, ARXIV160606724CS; Guerrero-Colon JA, 2008, IEEE IMAGE PROC, P565, DOI 10.1109/ICIP.2008.4711817; Hershey J. R., 2014, ARXIV14092574; Higgins Irina, 2017, 5 INT C LEARN REPR I; Hyvarinen A, 2006, IEEE IJCNN, P4167; Ilin Alexander, 2017, ARXIV170709219CS STA; Isola Phillip, 2015, ARXIV151106811CS; Jojic Nebojsa, 2001, COMP VIS PATT REC 20, V1; Kannan Anitha, 2007, 19 C ADV NEUR INF PR, P657; Kingma D.P, P 3 INT C LEARNING R; Le Roux N, 2011, NEURAL COMPUT, V23, P593, DOI 10.1162/NECO_a_00086; MILNER PM, 1974, PSYCHOL REV, V81, P521, DOI 10.1037/h0037149; Pathak Deepak, 2016, ARXIV161206370CSSTAT; Rao AR, 2008, IEEE T NEURAL NETWOR, V19, P168, DOI 10.1109/TNN.2007.905852; Rao AR, 2010, INT J INTELL COMPUT, V3, P173, DOI 10.1108/17563781011049160; Reichert D. P., 2013, ARXIV13126115CSQBIOS; Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486; SAUND E, 1995, NEURAL COMPUT, V7, P51, DOI 10.1162/neco.1995.7.1.51; Schmid C., 2017, ARXIV170407804CS; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779; Treisman A, 1996, CURR OPIN NEUROBIOL, V6, P171, DOI 10.1016/S0959-4388(96)80070-5; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; von der Malsburg C, 1981, CORRELATION THEORY B; VONDERMALSBURG C, 1995, CURR OPIN NEUROBIOL, V5, P520, DOI 10.1016/0959-4388(95)80014-X; WANG DL, 1995, IEEE T NEURAL NETWOR, V6, P283, DOI 10.1109/72.363423; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; Wersing H, 2001, NEURAL COMPUT, V13, P357, DOI 10.1162/089976601300014574; Williams R, 1989, NUCCS8927; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406073
C	Hafner, D; Irpan, A; Davidson, J; Heess, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hafner, Danijar; Irpan, Alex; Davidson, James; Heess, Nicolas			Learning Hierarchical Information Flow with Recurrent Neural Modules	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose ThalNet, a deep learning model inspired by neocortical communication via the thalamus. Our model consists of recurrent neural modules that send features through a routing center, endowing the modules with the flexibility to share features over multiple time steps. We show that our model learns to route information hierarchically, processing input data by a chain of modules. We observe common architectures, such as feed forward neural networks and skip connections, emerging as special cases of our architecture, while novel connectivity patterns are learned for the text8 compression task. Our model outperforms standard recurrent neural networks on several sequential benchmarks.	[Hafner, Danijar; Irpan, Alex; Davidson, James] Google Brain, Mountain View, CA 94043 USA; [Heess, Nicolas] Google DeepMind, London, England	Google Incorporated; Google Incorporated	Hafner, D (corresponding author), Google Brain, Mountain View, CA 94043 USA.	mail@danijar.com; alexirpan@google.com; jcdavidson@google.com; heess@google.com	Jeong, Yongwook/N-7413-2016					Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; [Anonymous], 2017, P 30 IEEE C COMP VIS; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Cooijmans T., 2016, P 5 INT C LEARN REPR; Devin C., 2016, ARXIV160907088; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Gilbert CD, 2007, NEURON, V54, P677, DOI 10.1016/j.neuron.2007.05.019; Graves A, 2014, NEURAL TURING MACHIN; Graves A., 2016, ADAPTIVE COMPUTATION; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Hawkins J, 2006, TECHNICAL REPORT; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Ionescu C., 2016, ADV NEURAL INFORM PR, P4331; Kingma D.P., 2015, INT C LEARN REPR, P1; Kirkpatrick J., 2017, P NATL ACAD SCI USA; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Maharaj T., 2016, ARXIV160601305; Mahoney Matt, 2011, ABOUT THE TEST DATA; McCulloch W., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]; Pham T., 2017, ARXIV170207021; Reed S. E., 2015, INT C LEARN REPR; Rusu A. A., 2016, PROGR NEURAL NETWORK; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Sherman SM, 2016, NAT NEUROSCI, V19, P533, DOI 10.1038/nn.4269; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Zenke F., 2017, ARXIV170304200	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406076
C	Halloran, JT; Rocke, DM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Halloran, John T.; Rocke, David M.			Gradients of Generative Models for Improved Discriminative Analysis of Tandem Mass Spectra	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FALSE DISCOVERY RATE; PEPTIDE IDENTIFICATION; SHOTGUN	Tandem mass spectrometry (MS/MS) is a high-throughput technology used to identify the proteins in a complex biological sample, such as a drop of blood. A collection of spectra is generated at the output of the process, each spectrum of which is representative of a peptide (protein subsequence) present in the original complex sample. In this work, we leverage the log-likelihood gradients of generative models to improve the identification of such spectra. In particular, we show that the gradient of a recently proposed dynamic Bayesian network (DBN) [7] may be naturally employed by a kernel-based discriminative classifier. The resulting Fisher kernel substantially improves upon recent attempts to combine generative and discriminative models for post-processing analysis, outperforming all other methods on the evaluated datasets. We extend the improved accuracy offered by the Fisher kernel framework to other search algorithms by introducing Theseus, a DBN representing a large number of widely used MS/MS scoring functions. Furthermore, with gradient ascent and max-product inference at hand, we use Theseus to learn model parameters without any supervision.	[Halloran, John T.; Rocke, David M.] Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA	University of California System; University of California Davis	Halloran, JT (corresponding author), Univ Calif Davis, Dept Publ Hlth Sci, Davis, CA 95616 USA.	jthalloran@ucdavis.edu; dmrocke@ucdavis.edu	Jeong, Yongwook/N-7413-2016		National Center for Advancing Translational Sciences (NCATS), National Institutes of Health [UL1 TR001860]	National Center for Advancing Translational Sciences (NCATS), National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Center for Advancing Translational Sciences (NCATS))	This work was supported by the National Center for Advancing Translational Sciences (NCATS), National Institutes of Health, through grant UL1 TR001860.	BENJAMINI Y, 1995, J R STAT SOC B, V57, P289, DOI 10.1111/j.2517-6161.1995.tb02031.x; Craig R, 2004, BIOINFORMATICS, V20, P1466, DOI 10.1093/bioinformatics/bth092; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Elkan C, 2005, LECT NOTES COMPUT SC, V3772, P295; Eng JK, 2013, PROTEOMICS, V13, P22, DOI 10.1002/pmic.201200439; ENG JK, 1994, J AM SOC MASS SPECTR, V5, P976, DOI 10.1016/1044-0305(94)80016-2; Halloran JT, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P320; Halloran JT, 2016, J PROTEOME RES, V15, P2749, DOI 10.1021/acs.jproteome.6b00290; Halloran John T., 2017, GRADIENTS GENERATIVE; Jaakkola T, 1999, Proc Int Conf Intell Syst Mol Biol, P149; Jaakkola T., 1998, ADV NEURAL INFORM PR; Jeffry Howbert  J, 2014, MOL CELLULAR PROTEOM, pmcp; Kall L, 2007, NAT METHODS, V4, P923, DOI 10.1038/NMETH1113; Keich U, 2015, J PROTEOME RES, V14, P3148, DOI 10.1021/acs.jproteome.5b00081; Keich U, 2015, J PROTEOME RES, V14, P1147, DOI 10.1021/pr5010983; Kim S., 2014, NATURE COMMUNICATION, V5; McIlwain  Sean, 2014, J PROTEOME RES; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Spivak M, 2012, MOL CELLULAR PROTEOM, V11; Spivak M, 2009, J PROTEOME RES, V8, P3737, DOI 10.1021/pr801109k; Wang SJ, 2016, BIOINFORMATICS, V32, P322, DOI 10.1093/bioinformatics/btw269; Wenger C. D, 2013, J PROTEOME RES	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405078
C	Hashimoto, TB; Duchi, JC; Liang, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hashimoto, Tatsunori B.; Duchi, John C.; Liang, Percy			Unsupervised Transformation Learning via Convex Relaxations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Our goal is to extract meaningful transformations from raw images, such as varying the thickness of lines in handwriting or the lighting in a portrait. We propose an unsupervised approach to learn such transformations by attempting to reconstruct an image from a linear combination of transformations of its nearest neighbors. On handwritten digits and celebrity portraits, we show that even with linear transformations, our method generates visually high-quality modified images. Moreover, since our method is semiparametric and does not model the data distribution, the learned transformations extrapolate off the training data and can be applied to new types of images.	[Hashimoto, Tatsunori B.; Duchi, John C.; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Hashimoto, TB (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	thashim@cs.stanford.edu; jduchi@cs.stanford.edu; pliang@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		NSF-CAREER award [1553086]; DARPA [N66001-14-2-4055]; DAIS ITA program [W911NF-16-3-0001]	NSF-CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DAIS ITA program	We thank Arun Chaganty for helpful discussions and comments. This work was supported by NSF-CAREER award 1553086, DARPA (Grant N66001-14-2-4055), and the DAIS ITA program (W911NF-16-3-0001).		0	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406090
C	Hayashi, K; Yoshida, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hayashi, Kohei; Yoshida, Yuichi			Fitting Low-Rank Tensors in Constant Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we develop an algorithm that approximates the residual error of Tucker decomposition, one of the most popular tensor decomposition methods, with a provable guarantee. Given an order-K tensor X is an element of R-N1x...xNK, our algorithm randomly samples a constant number s of indices for each mode and creates a "mini" tensor (X) over tilde is an element of R-sx...xs, whose elements are given by the intersection of the sampled indices on X. Then, we show that the residual error of the Tucker decomposition of (X) over tilde is sufficiently close to that of X with high probability. This result implies that we can figure out how much we can fit a low-rank tensor to X in constant time, regardless of the size of X. This is useful for guessing the favorable rank of Tucker decomposition. Finally, we demonstrate how the sampling method works quickly and accurately using multiple real datasets.	[Hayashi, Kohei] RIKEN AIP, Natl Inst Adv Ind Sci & Technol, Tokyo, Japan; [Yoshida, Yuichi] Natl Inst Informat, Tokyo, Japan	National Institute of Advanced Industrial Science & Technology (AIST); RIKEN; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan	Hayashi, K (corresponding author), RIKEN AIP, Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.	hayashi.kohei@gmail.com; yyoshida@nii.ac.jp	Jeong, Yongwook/N-7413-2016		ONR [N62909-17-1-2138]; JSPS KAKENHI [JP17H04676]; JST ERATO, Japan [JPMJER1305]	ONR(Office of Naval Research); JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST ERATO, Japan(Japan Science & Technology Agency (JST))	Supported by ONR N62909-17-1-2138.; Supported by JSPS KAKENHI Grant Number JP17H04676 and JST ERATO Grant Number JPMJER1305, Japan.	Acar E, 2008, J CHEMOMETR, V22, P91, DOI 10.1002/cem.1106; Acar E, 2014, BMC BIOINFORMATICS, V15, DOI 10.1186/1471-2105-15-239; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Blankertz B, 2007, NEUROIMAGE, V37, P539, DOI 10.1016/j.neuroimage.2007.01.051; Caiafa CF, 2010, LINEAR ALGEBRA APPL, V433, P557, DOI 10.1016/j.laa.2010.03.020; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1324, DOI 10.1137/S0895479898346995; Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023; Frieze A, 1996, AN S FDN CO, P12, DOI 10.1109/SFCS.1996.548459; Hayashi K., 2016, NIPS, P2217; Lawaetz AJ, 2012, METABOLOMICS, V8, pS111, DOI 10.1007/s11306-011-0310-7; Lovasz L., 2012, LARGE NETWORKS GRAPH; Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002; Schuldt C, 2004, INT C PATT RECOG, P32, DOI 10.1109/ICPR.2004.1334462; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; STEELE R.J., 2010, FRONTIERS STAT DECIS, V2, P113; Tsourakakis C. E., 2010, P 2010 SIAM INT C DA, P689, DOI DOI 10.1137/1.9781611972801.60; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Vezzani R, 2010, MULTIMED TOOLS APPL, V50, P359, DOI 10.1007/s11042-009-0402-9; Watanabe S., 2009, ALGEBRAIC GEOMETRY S, V25; Zhou Guoxu, 1885, ARXIV14121885	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402051
C	Heikkila, M; Lagerspetz, E; Kaski, S; Shimizu, K; Tarkoma, S; Honkela, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Heikkila, Mikko; Lagerspetz, Eemil; Kaski, Samuel; Shimizu, Kana; Tarkoma, Sasu; Honkela, Antti			Differentially private Bayesian learning on distributed data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NOISE	Many applications of machine learning, for example in health care, would benefit from methods that can guarantee privacy of data subjects. Differential privacy (DP) has become established as a standard for protecting learning results. The standard DP algorithms require a single trusted party to have access to the entire data, which is a clear weakness, or add prohibitive amounts of noise. We consider DP Bayesian learning in a distributed setting, where each party only holds a single sample or a few samples of the data. We propose a learning strategy based on a secure multi-party sum function for aggregating summaries from data holders and the Gaussian mechanism for DP. Our method builds on an asymptotically optimal and practically efficient DP Bayesian inference with rapidly diminishing extra cost.	[Heikkila, Mikko; Honkela, Antti] Univ Helsinki, Helsinki Inst Informat Technol, Dept Math & Stat, Helsinki, Finland; [Lagerspetz, Eemil; Tarkoma, Sasu] Univ Helsinki, HIIT, Dept Comp Sci, Helsinki, Finland; [Kaski, Samuel] Aalto Univ, Helsinki Inst Informat Technol, Dept Comp Sci, Helsinki, Finland; [Shimizu, Kana] Waseda Univ, Dept Comp Sci & Engn, Tokyo, Japan; [Honkela, Antti] Univ Helsinki, Dept Publ Hlth, Helsinki, Finland	University of Helsinki; Aalto University; University of Helsinki; Aalto University; University of Helsinki; Waseda University; University of Helsinki	Heikkila, M (corresponding author), Univ Helsinki, Helsinki Inst Informat Technol, Dept Math & Stat, Helsinki, Finland.	mikko.a.heikkila@helsinki.fi; eemil.lagerspetz@helsinki.fi; samuel.kaski@aalto.fi; shimizu.kana.g@gmail.com; sasu.tarkoma@helsinki.fi; antti.honkela@helsinki.fi	Jeong, Yongwook/N-7413-2016; Kaski, Samuel/B-6684-2008; Tarkoma, Sasu/V-2345-2019	Kaski, Samuel/0000-0003-1925-9154; Tarkoma, Sasu/0000-0003-4220-3650; Heikkila, Mikko/0000-0001-5753-8643; Lagerspetz, Eemil/0000-0003-3875-8135	Academy of Finland [Centre of Excellence COIN] [259440, 278300, 292334, 294238, 297741, 303815, 303816]; Japan Agency for Medical Research and Development (AMED); JST CREST [JPMJCR1688]	Academy of Finland [Centre of Excellence COIN](Academy of Finland); Japan Agency for Medical Research and Development (AMED)(Japan Agency for Medical Research and Development (AMED)); JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	This work was funded by the Academy of Finland [Centre of Excellence COIN and projects 259440, 278300, 292334, 294238, 297741, 303815, 303816], the Japan Agency for Medical Research and Development (AMED), and JST CREST [JPMJCR1688].	Abadi M., 2016, P CCS 2016; Acs Gergely, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P118, DOI 10.1007/978-3-642-24178-9_9; [Anonymous], 2012, P INT C FIN CRYPT DA; Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016; Dimitrakakis C, 2017, J MACH LEARN RES, V18; Dimitrakakis C, 2014, LECT NOTES ARTIF INT, V8776, P291, DOI 10.1007/978-3-319-11662-4_21; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Eigner F., 2014, P 30 ANN COMP SEC AP, P316; Foulds J., 2016, P 32 C UNC ART INT U, P192; Goryczka S., 2015, IEEE T DEPENDABLE SE; Hamm J., 2016, ICML; Honkela A, 2016, ARXIV160602109STATML; Jalko J, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Lichman M., 2013, UCI MACHINE LEARNING; Park James, 2016, ARXIV161100340; Pathak M., 2010, ADV NEURAL INFORM PR, P1876; Rajkumar A., 2012, ARTIF INTELL, P933; Rastogi Vibhor, 2010, P 2010 ACM SIGMOD IN, P735; Shi E., 2011, P NDSS; Smith A., 2008, ARXIV08094794CSCR; Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Williams O., 2010, ADV NEURAL INF PROCE, V23; Wu GQ, 2016, IEEE TRUST BIG, P921, DOI [10.1109/TrustCom.2016.0157, 10.1109/TrustCom.2016.351]; Zhang J, 2012, PROC VLDB ENDOW, V5, P1364, DOI 10.14778/2350229.2350253; Zhang Z., 2016, P AAAI 2016	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403029
C	Hoy, D; Nekipelov, D; Syrgkanis, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hoy, Darrell; Nekipelov, Denis; Syrgkanis, Vasilis			Welfare Guarantees from Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Analysis of efficiency of outcomes in game theoretic settings has been a main item of study at the intersection of economics and computer science. The notion of the price of anarchy takes a worst-case stance to efficiency analysis, considering instance independent guarantees of efficiency. We propose a data-dependent analog of the price of anarchy that refines this worst-case assuming access to samples of strategic behavior. We focus on auction settings, where the latter is non-trivial due to the private information held by participants. Our approach to bounding the efficiency from data is robust to statistical errors and mis-specification. Unlike traditional econometrics, which seek to learn the private information of players from observed behavior and then analyze properties of the outcome, we directly quantify the inefficiency without going through the private information. We apply our approach to datasets from a sponsored search auction system and find empirical results that are a significant improvement over bounds from worst-case analysis.	[Hoy, Darrell] Univ Maryland, College Pk, MD 20742 USA; [Nekipelov, Denis] Univ Virginia, Charlottesville, VA 22903 USA; [Syrgkanis, Vasilis] Microsoft Res, Redmond, WA USA	University System of Maryland; University of Maryland College Park; University of Virginia; Microsoft	Hoy, D (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	darrell.hoy@gmail.com; denis@virginia.edu; vasy@microsoft.com						Caragiannis Ioannis, 2014, BOUNDING INEFFICIENC, P1; Edelman B, 2007, AM ECON REV, V97, P242, DOI 10.1257/aer.97.1.242; Guerre E, 2000, ECONOMETRICA, V68, P525, DOI 10.1111/1468-0262.00123; Hartline J., 2014, ACM C EC COMPUTATION, P693; Koutsoupias E, 1999, LECT NOTES COMPUT SC, V1563, P404; Krishna Vijay., 2002, AUCTION THEORY; Paarsch H. J., 2006, INTRO STRUCTURAL ECO; Pollard David, 1984, CONVERGENCE STOCHAST; Roughgarden T, 2002, J ACM, V49, P236, DOI 10.1145/506147.506153; Roughgarden Tim, 2016, ABS160707684 CORR; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Syrgkanis V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P211; Varian HR, 2009, AM ECON REV, V99, P430, DOI 10.1257/aer.99.2.430; VICKREY W, 1961, J FINANC, V16, P8, DOI 10.2307/2977633	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403081
C	Huang, XR; Liang, ZX; Bajaj, C; Huang, QX		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Huang, Xiangru; Liang, Zhenxiao; Bajaj, Chandrajit; Huang, Qixing			Translation Synchronization via Truncated Least Squares	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we introduce a robust algorithm, TranSync, for the 1D translation synchronization problem, in which the aim is to recover the global coordinates of a set of nodes from noisy measurements of relative coordinates along an observation graph. The basic idea of TranSync is to apply truncated least squares, where the solution at each step is used to gradually prune out noisy measurements. We analyze TranSync under both deterministic and randomized noisy models, demonstrating its robustness and stability. Experimental results on synthetic and real datasets show that TranSync is superior to state-of-the-art convex formulations in terms of both efficiency and accuracy.	[Huang, Xiangru; Bajaj, Chandrajit; Huang, Qixing] Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA; [Liang, Zhenxiao] Tsinghua Univ, Beijing 100084, Peoples R China	University of Texas System; University of Texas Austin; Tsinghua University	Huang, XR (corresponding author), Univ Texas Austin, 2317 Speedway, Austin, TX 78712 USA.	xrhuang@cs.utexas.edu; liangzx14@mails.tsinghua.edu.cn; bajaj@cs.utexas.edu; huangqx@cs.utexas.edu	Huang, Xiangru/ABG-5316-2021; Jeong, Yongwook/N-7413-2016		NSF [DMS-1700234]; National Institute of Health [R41 GM116300, R01 GM117594]	NSF(National Science Foundation (NSF)); National Institute of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Qixing Huang would like to acknowledge support this research from NSF DMS-1700234. Chandrajit Bajaj would like to acknowledge support for this research from the National Institute of Health grants #R41 GM116300 and #R01 GM117594.	Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x; Arrigoni Federica, 2015, CORR; Chatterjee A., 2013, 2013 IEEE INT C COMP; Chen Yun-Nung, 2016, CORR; Cho T. S., 2010, IEEE C COMP VIS PATT; Chung F, 2007, INTERNET MATH, V4, P225, DOI 10.1080/15427951.2007.10129296; Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626; Cruz R, 2016, IEEE IJCNN, P2182, DOI 10.1109/IJCNN.2016.7727469; Daubechies I., COMM PURE APPL MATH; Gelfand N., 2005, P 3 EUR S GEOM PROC, P197; Goldberg D., 2002, P 18 ANN S COMP GEOM, P82; Heiberger R. M., 1992, DESIGN S FUNCTION RO, P112; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184; HUBER D, 2002, THESIS; Jiang XY, 2011, MATH PROGRAM, V127, P203, DOI 10.1007/s10107-010-0419-x; Kim V. G., 2012, T GRAPHICS, V31; Liu HL, 2014, INT J OPT, V2014, DOI 10.1155/2014/693807; Marande W, 2007, SCIENCE, V318, P415, DOI 10.1126/science.1148033; Mellado N, 2014, COMPUT GRAPH FORUM, V33, P205, DOI 10.1111/cgf.12446; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Roberts R., 2011, STRUCTURE MOTION SCE, P3137; Shen Y., 2016, ADV NEURAL INFORM PR, V29, P4925; Snavely N, 2006, ACM T GRAPHIC, V25, P835, DOI 10.1145/1141911.1141964; Wang L., 2012, CORR; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801; Zhou XW, 2015, IEEE I CONF COMP VIS, P4032, DOI 10.1109/ICCV.2015.459	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401048
C	Huang, ZB; Geng, SN; Page, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Huang, Zhaobin; Geng, Sinong; Page, David			A Screening Rule for l(1)-Regularized Ising Model Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INVERSE COVARIANCE ESTIMATION; MARKOV NETWORKS; SELECTION; LASSO	We discover a screening rule for l(1)-regularized Ising model estimation. The simple closed-form screening rule is a necessary and sufficient condition for exactly recovering the blockwise structure of a solution under any given regularization parameters. With enough sparsity, the screening rule can be combined with various optimization procedures to deliver solutions efficiently in practice. The screening rule is especially suitable for large-scale exploratory data analysis, where the number of variables in the dataset can be thousands while we are only interested in the relationship among a handful of variables within moderate-size clusters for interpretability. Experimental results on various datasets demonstrate the efficiency and insights gained from the introduction of the screening rule.	[Huang, Zhaobin; Geng, Sinong; Page, David] Univ Wisconsin, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Huang, ZB (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	zkuang@wisc.edu; sgeng2@wisc.edu; page@biostat.wisc.edu			NIH BD2K Initiative grant [U54 AI117924]; NIGMS [2RO1 GM097618]	NIH BD2K Initiative grant; NIGMS(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of General Medical Sciences (NIGMS))	The authors would like to gratefully acknowledge the NIH BD2K Initiative grant U54 AI117924 and the NIGMS grant 2RO1 GM097618.	[Anonymous], 2013, NIPS; [Anonymous], [No title captured]; [Anonymous], 2019, STAT LEARNING SPARSI; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Barber RF, 2015, ELECTRON J STAT, V9, P567, DOI 10.1214/15-EJS1012; Chen H, 2004, BMC BIOINFORMATICS, V5, DOI 10.1186/1471-2105-5-147; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; Fercoq O, 2015, PR MACH LEARN RES, V37, P333; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Geng S., 2017, EFFICIENT PSEUDO LIK; Ghaoui L., 2010, SAFE FEATURE ELIMINA; Hofling H, 2009, J MACH LEARN RES, V10, P883; Honorio J., 2010, PROC INT C MACH LEAR, P447; Karger D, 2001, SIAM PROC S, P392; Koller D., 2009, PROBABILISTIC GRAPHI; Lee S., 2017, IEEE T PATTERN ANAL; Liu Han, 2010, Adv Neural Inf Process Syst, V24, P1432; Liu J., 2014, THESIS; Liu J., 2014, ADV NEURAL INFORM PR, V27, P1053; Liu J., 2013, ICML 2013 WORKSH STR; Liu J., 2013, ARXIV13077577; Liu J, 2016, ANN APPL STAT, V10, P1699, DOI 10.1214/16-AOAS956; Liu J, 2014, JMLR WORKSH CONF PRO, V33, P576; Liu J, 2014, PR MACH LEARN RES, V32, P955; Liu Jie, 2012, Uncertain Artif Intell, V2012, P511; Loh PL, 2013, ANN STAT, V41, P3022, DOI 10.1214/13-AOS1162; Luo S., 2014, ARXIV14077819; Mazumder R, 2012, J MACH LEARN RES, V13, P781; Ndiaye E, 2015, ADV NEUR IN, V28; Pena Javier, 2016, LECT NOTES MACHINE L; Peng J, 2009, J AM STAT ASSOC, V104, P735, DOI 10.1198/jasa.2009.0126; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Uhlen M, 2015, SCIENCE, V347, DOI 10.1126/science.1260419; Viallon V, 2014, BIOMETRICAL J, V56, P307, DOI 10.1002/bimj.201200253; Vuffray M., 2016, ADV NEURAL INFORM PR, P2595; Wainwright M. J., 2006, ADV NEURAL INFORM PR, P1465; Wan YW, 2016, BMC SYST BIOL, V10, DOI 10.1186/s12918-016-0313-0; Wan YW, 2015, BIOINFORMATICS; Wang Jie, 2013, ADV NEURAL INFORM PR; Weinstein JN, 2013, NAT GENET, V45, P1113, DOI 10.1038/ng.2764; Witten DM, 2011, J COMPUT GRAPH STAT, V20, P892, DOI 10.1198/jcgs.2011.11051a; Xiang ZJ, 2017, IEEE T PATTERN ANAL, V39, P1008, DOI 10.1109/TPAMI.2016.2568185; Yang S, 2015, SIAM J OPTIMIZ, V25, P916, DOI 10.1137/130936397	46	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400069
C	Inan, H; Erdogdu, MA; Schnitzer, MJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Inan, Hakan; Erdogdu, Murat A.; Schnitzer, Mark J.			Robust Estimation of Neural Signals in Calcium Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Calcium imaging is a prominent technology in neuroscience research which allows for simultaneous recording of large numbers of neurons in awake animals. Automated extraction of neurons and their temporal activity from imaging datasets is an important step in the path to producing neuroscience results. However, nearly all imaging datasets contain gross contaminating sources which could originate from the technology used, or the underlying biological tissue. Although past work has considered the effects of contamination under limited circumstances, there has not been a general framework treating contamination and its effects on the statistical estimation of calcium signals. In this work, we proceed in a new direction and propose to extract cells and their activity using robust statistical estimation. Using the theory of M-estimation, we derive a minimax optimal robust loss, and also find a simple and practical optimization routine for this loss with provably fast convergence. We use our proposed robust loss in a matrix factorization framework to extract the neurons and their temporal activity in calcium imaging datasets. We demonstrate the superiority of our robust estimation approach over existing methods on both simulated and real datasets.	[Inan, Hakan; Schnitzer, Mark J.] Stanford Univ, Stanford, CA 94305 USA; [Erdogdu, Murat A.] Microsoft Res, Redmond, WA USA; [Erdogdu, Murat A.] Vector Inst, Toronto, ON, Canada; [Schnitzer, Mark J.] Howard Hughes Med Inst, Chevy Chase, MD USA	Stanford University; Microsoft; Howard Hughes Medical Institute	Inan, H (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	inanh@stanford.edu; erdogdu@cs.toronto.edu; mschnitz@stanford.edu	Jeong, Yongwook/N-7413-2016		DARPA	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We gratefully acknowledge support from DARPA and technical assistance from Biafra Ahanonu, Lacey Kitch, Yaniv Ziv, Elizabeth Otto and Margaret Carr.	Apthorpe N. J., 2016, ARXIV160607372; COLLINS JR, 1976, ANN STAT, V4, P68, DOI 10.1214/aos/1176343348; DENK W, 1990, SCIENCE, V248, P73, DOI 10.1126/science.2321027; Flusberg BA, 2008, NAT METHODS, V5, P935, DOI 10.1038/nmeth.1256; Ghosh KK, 2011, NAT METHODS, V8, P871, DOI [10.1038/NMETH.1694, 10.1038/nmeth.1694]; Helmchen F, 2005, NAT METHODS, V2, P932, DOI 10.1038/NMETH818; HUBER PJ, 1973, ANN STAT, V1, P799, DOI 10.1214/aos/1176342503; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; JAECKEL LA, 1971, ANN MATH STAT, V42, P1020, DOI 10.1214/aoms/1177693330; Kaifosh P, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00080; Kokic P.N., 1994, J OFF STAT, V10, P419; MARTIN RD, 1993, ANN STAT, V21, P338, DOI 10.1214/aos/1176349029; Mukamel EA, 2009, NEURON, V63, P747, DOI 10.1016/j.neuron.2009.08.009; Pachitariu M., 2013, ADV NEURAL INFORM PR; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; Zhou P., 2016, ARXIV160507266; Ziv Y, 2013, NAT NEUROSCI, V16, P264, DOI 10.1038/nn.3329	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402093
C	Ito, S; Hatano, D; Sumita, H; Yabe, A; Fukunaga, T; Kakimura, N; Kawarabayashi, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ito, Shinji; Hatano, Daisuke; Sumita, Hanna; Yabe, Akihiro; Fukunaga, Takuro; Kakimura, Naonori; Kawarabayashi, Ken-ichi			Efficient Sublinear-Regret Algorithms for Online Sparse Linear Regression with Limited Observation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Online sparse linear regression is the task of applying linear regression analysis to examples arriving sequentially subject to a resource constraint that a limited number of features of examples can be observed. Despite its importance in many practical applications, it has been recently shown that there is no polynomial-time sublinear-regret algorithm unless NP subset of BPP, and only an exponential-time sublinear-regret algorithm has been found. In this paper, we introduce mild assumptions to solve the problem. Under these assumptions, we present polynomial-time sublinear-regret algorithms for the online sparse linear regression. In addition, thorough experiments with publicly available data demonstrate that our algorithms outperform other known algorithms.	[Ito, Shinji; Yabe, Akihiro] NEC Corp Ltd, Tokyo, Japan; [Hatano, Daisuke; Sumita, Hanna; Kawarabayashi, Ken-ichi] Natl Inst Informat, Tokyo, Japan; [Fukunaga, Takuro] JST, PRESTO, Tokyo, Japan; [Kakimura, Naonori] Keio Univ, Tokyo, Japan	NEC Corporation; Research Organization of Information & Systems (ROIS); National Institute of Informatics (NII) - Japan; Japan Science & Technology Agency (JST); Keio University	Ito, S (corresponding author), NEC Corp Ltd, Tokyo, Japan.	s-ito@me.jp.nec.com; hatano@nii.ac.jp; sumita@nii.ac.jp; a-yabe@cq.jp.nec.com; takuro@nii.ac.jp; kakimura@math.keio.ac.jp; k-keniti@nii.ac.jp			JST ERATO Grant, Japan [JPMJER1201]	JST ERATO Grant, Japan(Japan Science & Technology Agency (JST))	This work was supported by JST ERATO Grant Number JPMJER1201, Japan.	[Anonymous], 2012, ADV NEURAL INFORM PR; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cesa-Bianchi N., 2010, JOINT ICML COLT WORK; Cesa-Bianchi N, 2011, J MACH LEARN RES, V12, P2857; Foster D. P., 2016, P 29 C LEARNING THEO, P960; Hazan E., 2012, P 29 INT C MACH LEAR, P807; Kale  S., 2017, P 34 INT C MACH LEAR, V70, P1780; Kale S., 2014, P 27 C LEARN THEOR, P1299; Lichman M., 2013, UCI MACHINE LEARNING; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zinkevich Martin, 2003, P INT C MACH LEARN, P928; ZOLGHADR N, 2013, ADV NEURAL INFORM PR, P1241	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404017
C	Jain, L; Mason, B; Nowak, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jain, Lalit; Mason, Blake; Nowak, Robert			Learning Low-Dimensional Metrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper investigates the theoretical foundations of metric learning, focused on three key questions that are not fully addressed in prior work: 1) we consider learning general low-dimensional (low-rank) metrics as well as sparse metrics; 2) we develop upper and lower (minimax) bounds on the generalization error; 3) we quantify the sample complexity of metric learning in terms of the dimension of the feature space and the dimension/rank of the underlying metric; 4) we also bound the accuracy of the learned metric relative to the underlying true generative metric. All the results involve novel mathematical approaches to the metric learning problem, and also shed new light on the special case of ordinal embedding (aka non-metric multidimensional scaling).	[Jain, Lalit] Univ Michigan, Ann Arbor, MI 48109 USA; [Mason, Blake; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA	University of Michigan System; University of Michigan; University of Wisconsin System; University of Wisconsin Madison	Jain, L (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	lalitj@umich.edu; bmason3@wisc.edu; rdnowak@wisc.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1218189, IIS-1623605]	NSF(National Science Foundation (NSF))	This work was partially supported by the NSF grants CCF-1218189 and IIS-1623605	Abramovich F, 2016, IEEE T INFORM THEORY, V62, P3721, DOI 10.1109/TIT.2016.2555812; Bellet A., 2015, SYNTH LECT ARTIF INT, V9, P1, DOI DOI 10.2200/S00626ED1V01Y201501AIM030; Bellet A, 2015, NEUROCOMPUTING, V151, P259, DOI 10.1016/j.neucom.2014.09.044; Bian W, 2012, IEEE T NEUR NET LEAR, V23, P1194, DOI 10.1109/TNNLS.2012.2198075; Bunea F, 2007, ANN STAT, V35, P1674, DOI 10.1214/009053606000001587; Dattorro J., 2011, CONVEX OPTIMIZATION; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; DAVIDSON K. R, 2001, HDB GEOMETRY BANACH, V1, P131; Guo ZC, 2014, NEURAL COMPUT, V26, P497, DOI 10.1162/NECO_a_00556; Heim E., 2015, ARXIV151102254; Jain L., 2016, ADV NEURAL INFORM PR, P2703; Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574; Rau Martina A, 2016, P 9 INT C ED DAT MIN, P199, DOI DOI 10.1080/00461520.2011.611369; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Shi Y., 2014, ARXIV14044105; Tropp Joel A., 2015, INTRO MATRIX CONCENT; Ying Y., 2009, ADV NEURAL INFORM PR, P2214	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404021
C	Jalali, A; Willett, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jalali, Amin; Willett, Rebecca			Subspace Clustering via Tangent Cones	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Given samples lying on any of a number of subspaces, subspace clustering is the task of grouping the samples based on the their corresponding subspaces. Many subspace clustering methods operate by assigning a measure of affinity to each pair of points and feeding these affinities into a graph clustering algorithm. This paper proposes a new paradigm for subspace clustering that computes affinities based on the corresponding conic geometry. The proposed conic subspace clustering (CSC) approach considers the convex hull of a collection of normalized data points and the corresponding tangent cones. The union of subspaces underlying the data imposes a strong association between the tangent cone at a sample x and the original subspace containing x. In addition to describing this novel geometric perspective, this paper provides a practical algorithm for subspace clustering that leverages this perspective, where a tangent cone membership test is used to estimate the affinities. This algorithm is accompanied with deterministic and stochastic guarantees on the properties of the learned affinity matrix, on the true and false positive rates and spread, which directly translate into the overall clustering accuracy.	[Jalali, Amin] Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53715 USA; [Willett, Rebecca] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison	Jalali, A (corresponding author), Univ Wisconsin, Wisconsin Inst Discovery, Madison, WI 53715 USA.	amin.jalali@wisc.edu; willett@discovery.wisc.edu	Jeong, Yongwook/N-7413-2016					[Anonymous], 2016, JMLR WORKSH CONF PRO; [Anonymous], 2008, CONFIDENCE BOUNDS IN; Blair D. E, 2000, STUDENT MATH LIB, V9; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Elhamifar E, 2010, INT CONF ACOUST SPEE, P1926, DOI 10.1109/ICASSP.2010.5495317; GORDON Y, 1988, LECT NOTES MATH, V1317, P84; Heckel R, 2015, IEEE T INFORM THEORY, V61, P6320, DOI 10.1109/TIT.2015.2472520; Lu CY, 2012, LECT NOTES COMPUT SC, V7578, P347, DOI 10.1007/978-3-642-33786-4_26; MOREAU JJ, 1962, CR HEBD ACAD SCI, V255, P238; Nasihatkon B., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2137, DOI 10.1109/CVPR.2011.5995679; Park D., 2014, ADV NEURAL INFORM PR, P2753; Renegar J, 2016, SIAM J OPTIMIZ, V26, P2649, DOI 10.1137/15M1027371; Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang YX, 2016, J MACH LEARN RES, V17; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406078
C	Jitkrittum, W; Xu, WK; Szabo, Z; Fukumizu, K; Gretton, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jitkrittum, Wittawat; Xu, Wenkai; Szabo, Zoltan; Fukumizu, Kenji; Gretton, Arthur			A Linear-Time Kernel Goodness-of-Fit Test	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a novel adaptive test of goodness-of-fit, with computational cost linear in the number of samples. We learn the test features that best indicate the differences between observed samples and a reference model, by minimizing the false negative rate. These features are constructed via Stein's method, meaning that it is not necessary to compute the normalising constant of the model. We analyse the asymptotic Bahadur efficiency of the new test, and prove that under a mean-shift alternative, our test always has greater relative efficiency than a previous linear-time kernel test, regardless of the choice of parameters for that test. In experiments, the performance of our method exceeds that of the earlier linear-time test, and matches or exceeds the power of a quadratic-time kernel test. In high dimensions and where model structure may be exploited, our goodness of fit test performs far better than a quadratic-time two-sample test based on the Maximum Mean Discrepancy, with samples drawn from the model.	[Jitkrittum, Wittawat; Xu, Wenkai; Gretton, Arthur] UCL, Gatsby Unit, London, England; [Szabo, Zoltan] Ecole Polytech, CMAP, Palaiseau, France; [Fukumizu, Kenji] Inst Stat Math, Tachikawa, Tokyo, Japan	University of London; University College London; Institut Polytechnique de Paris; Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan	Jitkrittum, W (corresponding author), UCL, Gatsby Unit, London, England.	wittawatj@gmail.com; wenkaix@gatsby.ucl.ac.uk; zoltan.szabo@polytechnique.edu; fukumizu@ism.ac.jp; arthur.gretton@gmail.com	Jeong, Yongwook/N-7413-2016; Jitkrittum, Wittawat/U-6881-2019	Gretton, Arthur/0000-0003-3169-7624	Gatsby Charitable Foundation; Data Science Initiative; KAKENHI Innovative Areas [25120012]	Gatsby Charitable Foundation; Data Science Initiative; KAKENHI Innovative Areas	WJ, WX, and AG thank the Gatsby Charitable Foundation for the financial support. ZSz was financially supported by the Data Science Initiative. KF has been supported by KAKENHI Innovative Areas 25120012.	BAHADUR RR, 1960, ANN MATH STAT, V31, P276, DOI 10.1214/aoms/1177705894; Baringhaus L., 1988, METRIKA, V35, P339, DOI DOI 10.1007/BF02613322; BEIRLANT J, 1994, CAN J STAT, V22, P309, DOI 10.2307/3315594; Bhatia R., 2013, MATRIX ANAL, V169; BOWMAN AW, 1993, J AM STAT ASSOC, V88, P529, DOI 10.2307/2290333; Carmeli C, 2010, ANAL APPL, V8, P19, DOI 10.1142/S0219530510001503; Chwialkowski K. P., 2015, ADV NEURAL INFORM PR, P1981; Chwialkowski K.P., 2014, ADV NEURAL INFORM PR, V27, P3608; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Epps TW., 1986, J STAT COMPUT SIM, V26, P177, DOI [10.1080/00949658608810963, DOI 10.1080/00949658608810963]; Gleser L. J., 1964, MEASURE TEST EFFICIE, V35, P1537; Gleser L. J., 1966, COMP MULTIVARIATE TE, V28, P157; Gorham J, 2017, PR MACH LEARN RES, V70; Gorham J, 2015, ADV NEUR IN, V28; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gyorfi L., 1990, NONPARAMETRIC FUNCTI, P631; Jitkrittum W., 2017, INT C MACH LEARN ICM, V70, P1742; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Ley C, 2017, PROBAB SURV, V14, P1, DOI 10.1214/16-PS278; Liu Q, 2016, PR MACH LEARN RES, V48; MASSEY FJ, 1951, J AM STAT ASSOC, V46, P68, DOI 10.2307/2280095; Mityagin B., 2015, ARXIV151207276; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Rizzo ML, 2009, ASTIN BULL, V39, P691, DOI 10.2143/AST.39.2.2044654; Serfling R., 2009, APPROXIMATION THEORE; Steinwart I., 2008, SUPPORT VECTOR MACHI; Strathmann H., 2012, P ADV NEUR INF PROC, P1205, DOI DOI 10.5555/2999134.2999269; Sutherland D. J., 2016, ICLR; Szekely GJ, 2005, J MULTIVARIATE ANAL, V93, P58, DOI 10.1016/j.jmva.2003.12.002; Van der Vaart A. W., 2000, ASYMPTOTIC STAT; Zhang QY, 2018, STAT COMPUT, V28, P113, DOI [10.1007/s11222-016-9721-7, 10.1007/s11222-016-9694-6]	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400025
C	Joshi, B; Amini, MR; Partalas, I; Iutzeler, F; Maximov, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Joshi, Bikash; Amini, Massih-Reza; Partalas, Ioannis; Iutzeler, Franck; Maximov, Yury			Aggressive Sampling for Multi-class to Binary Reduction with Applications to Text Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We address the problem of multi-class classification in the case where the number of classes is very large. We propose a double sampling strategy on top of a multi-class to binary reduction strategy, which transforms the original multi-class problem into a binary classification problem over pairs of examples. The aim of the sampling strategy is to overcome the curse of long-tailed class distributions exhibited in majority of large-scale multi-class classification problems and to reduce the number of pairs of examples in the expanded data. We show that this strategy does not alter the consistency of the empirical risk minimization principle defined over the double sample reduction. Experiments are carried out on DMOZ and Wikipedia collections with 10,000 to 100,000 classes where we show the efficiency of the proposed approach in terms of training and prediction time, memory consumption, and predictive performance with respect to state-of-the-art approaches.	[Joshi, Bikash; Amini, Massih-Reza] Univ Grenoble Alps, LIG, Grenoble, France; [Partalas, Ioannis] Expedia EWE, Geneva, Switzerland; [Iutzeler, Franck] Univ Grenoble Alps, LJK, Grenoble, France; [Maximov, Yury] Los Alamos Natl Lab, Los Alamos, NM USA; [Maximov, Yury] Skolkovo IST, Moscow, Russia	UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); United States Department of Energy (DOE); Los Alamos National Laboratory	Joshi, B (corresponding author), Univ Grenoble Alps, LIG, Grenoble, France.	bikash.joshi@imag.fr; massih-reza.amini@imag.fr; ipartalas@expedia.com; franck.iutzeler@imag.fr; yury@lanl.gov	Jeong, Yongwook/N-7413-2016	Maximov, Yury/0000-0002-8135-4622	LabEx PERSYVAL-Lab - French program Investissement d'avenir [ANR-11-LABX-0025-01]; U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative	LabEx PERSYVAL-Lab - French program Investissement d'avenir(French National Research Agency (ANR)); U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative(United States Department of Energy (DOE))	This work has been partially supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01) funded by the French program Investissement d'avenir, and by the U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative.	Babbar Rohit, 2014, SIGKDD EXPLORATIONS, V16; Bengio Samy, 2010, ADV NEURAL INFORM PR, V1, P163, DOI [10.5555/2997189.2997208, DOI 10.5555/2997189.2997208]; Beygelzimer A, 2009, LECT NOTES ARTIF INT, V5809, P247, DOI 10.1007/978-3-642-04414-4_22; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Choromanska A., 2013, NIPS WORKSH EX UNPUB; Choromanska Anna, 2014, CORR; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Daume III H., 2016, ARXIV160604988; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Hullermeier E, 2007, LECT NOTES ARTIF INT, V4701, P583; Jain H, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P935, DOI 10.1145/2939672.2939756; Janson S, 2004, RANDOM STRUCT ALGOR, V24, P234, DOI 10.1002/rsa.20008; Jasinska Kalina, 2016, ARXIV161101964; Joshi B, 2015, LECT NOTES COMPUT SC, V9385, P132, DOI 10.1007/978-3-319-24465-5_12; Liu T-Y, 2007, P SIGIR 2007 WORKSH, P3; Lorena AC, 2008, ARTIF INTELL REV, V30, P19, DOI 10.1007/s10462-009-9114-9; Mineiro P, 2015, LECT NOTES ARTIF INT, V9284, P37, DOI 10.1007/978-3-319-23528-8_3; Partalas I., 2015, ARXIV E PRINTS; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Ralaivola L, 2015, PR MACH LEARN RES, V37, P2436; SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Usunier N, 2005, ADV NEURAL INFORM PR, V18, P1369; Vapnik V.N, 1998, STAT LEARNING THEORY; Volkovs M., 2012, ADV NEURAL INFORM PR, V25, P2294; Watkins C., 1998, CSDTR9804 U LOND ROY; Yen Ian EH, 2016, P 33 INT C MACH LEAR; Yu H.-F., 2014, INT C MACH LEARN, P593	31	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404023
C	Kazemitabar, SJ; Amini, AA; Bloniarz, A; Talwalkar, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kazemitabar, S. Jalil; Amini, Arash A.; Bloniarz, Adam; Talwalkar, Ameet			Variable Importance using Decision Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SPARSITY RECOVERY	Decision trees and random forests are well established models that not only offer good predictive performance, but also provide rich feature importance information. While practitioners often employ variable importance methods that rely on this impurity-based information, these methods remain poorly characterized from a theoretical perspective. We provide novel insights into the performance of these methods by deriving finite sample performance guarantees in a high-dimensional setting under various modeling assumptions. We further demonstrate the effectiveness of these impurity-based methods via an extensive set of simulations.	[Kazemitabar, S. Jalil; Amini, Arash A.] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Bloniarz, Adam] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Talwalkar, Ameet] CMU, Mt Pleasant, MI USA; [Bloniarz, Adam] Google, Mountain View, CA USA	University of California System; University of California Los Angeles; University of California System; University of California Berkeley; Google Incorporated	Kazemitabar, SJ (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.	sjalilk@ucla.edu; aaamini@ucla.edu; adam@stat.berkeley.edu; talwalkar@cmu.edu	Jeong, Yongwook/N-7413-2016					[Anonymous], 2013, ADV NEURAL INFORM PR; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Fan J., 2008, J ROYAL STAT SOC B, V70; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Ishwaran H, 2007, ELECTRON J STAT, V1, P519, DOI 10.1214/07-EJS039; Lafferty J, 2008, ANN STAT, V36, P28, DOI 10.1214/009053607000000811; Olshen R., 1984, CLASSIFICATION REGRE; Ravikumar P, 2009, J R STAT SOC B, V71, P1009, DOI 10.1111/j.1467-9868.2009.00718.x; Rudelson M, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2865; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P5728, DOI 10.1109/TIT.2009.2032816; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018	12	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400041
C	Kendall, A; Gal, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kendall, Alex; Gal, Yarin			What Uncertainties Do We Need in Bayesian Deep Learning for Computer Vision?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model - uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.	[Kendall, Alex; Gal, Yarin] Univ Cambridge, Cambridge, England	University of Cambridge	Kendall, A (corresponding author), Univ Cambridge, Cambridge, England.	agk34@cam.ac.uk; yg279@cam.ac.uk	Jeong, Yongwook/N-7413-2016					Abadi Martin, 2016, 12 USENIX S OP SYST, P265, DOI DOI 10.5555/3026877.3026899; [Anonymous], 2017, P 30 IEEE C COMP VIS; Badrinarayanan V., 2017, IEEE T PATTERN ANAL; BLAKE A, 1993, INT J COMPUT VISION, V11, P127, DOI 10.1007/BF01469225; Blundell C., 2015, ICML; Brostow GJ, 2009, PATTERN RECOGN LETT, V30, P88, DOI 10.1016/j.patrec.2008.04.005; Chen L.C, 2014, ARXIV14127062; Denker John, 1991, ADV NEURAL INFORM PR, V3; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Gal Y., 2016, ICLR WORKSH TRACK; Gal Y., 2016, U CAMBRIDGE; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guynn J., 2015, USA TODAY; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He XM, 2004, PROC CVPR IEEE, P695; Hernandez-Lobato JM, 2016, PR MACH LEARN RES, V48; Jegou S., 2016, ARXIV161109326; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Karsch K, 2012, LECT NOTES COMPUT SC, V7576, P775, DOI 10.1007/978-3-642-33715-4_56; Kendall A, 2015, P BRIT MACH VIS C 20; Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020; Kundu A, 2016, PROC CVPR IEEE, P3168, DOI 10.1109/CVPR.2016.345; Ladicky L, 2014, PROC CVPR IEEE, P89, DOI 10.1109/CVPR.2014.19; Laina I, 2016, INT CONF 3D VISION, P239, DOI 10.1109/3DV.2016.32; Le Q.V., 2005, P 22 INT C MACHINE L, P489, DOI [10.1145/1102351.1102413, DOI 10.1145/1102351.1102413]; Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715; Liu MM, 2014, PROC CVPR IEEE, P716, DOI 10.1109/CVPR.2014.97; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Neal R. M., 2012, BAYESIAN LEARNING NE; NHTSA, 2017, 16007 NHTSA PE US DE; NIX DA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P55, DOI 10.1109/ICNN.1994.374138; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Shelhamer E., 2016, IEEE TPAMI; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Yu Fisher, 2016, MULTISCALE CONTEXT A	36	0	0	12	39	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405064
C	Khetan, A; Oh, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Khetan, Ashish; Oh, Sewoong			Matrix Norm Estimation from a Few Entries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Singular values of a data in a matrix form provide insights on the structure of the data, the effective dimensionality, and the choice of hyper-parameters on higher-level data analysis tools. However, in many practical applications such as collaborative filtering and network analysis, we only get a partial observation. Under such scenarios, we consider the fundamental problem of recovering various spectral properties of the underlying matrix from a sampling of its entries. We propose a framework of first estimating the Schatten k-norms of a matrix for several values of k, and using these as surrogates for estimating spectral properties of interest, such as the spectrum itself or the rank. This paper focuses on the technical challenges in accurately estimating the Schatten norms from a sampling of a matrix. We introduce a novel unbiased estimator based on counting small structures in a graph and provide guarantees that match its empirical performances. Our theoretical analysis shows that Schatten norms can be recovered accurately from strictly smaller number of samples compared to what is needed to recover the underlying low-rank matrix. Numerical experiments suggest that we significantly improve upon a competing approach of using matrix completion methods.	[Khetan, Ashish; Oh, Sewoong] Univ Illinois, Dept ISE, Champaign, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Khetan, A (corresponding author), Univ Illinois, Dept ISE, Champaign, IL 61801 USA.	khetan2@illinois.edu; swoh@illinois.edu			NSF [CNS-1527754, CCF-1553452, CCF-1705007]; GOOGLE Faculty Research Award	NSF(National Science Foundation (NSF)); GOOGLE Faculty Research Award(Google Incorporated)	This work was partially supported by NSF grants CNS-1527754, CCF-1553452, CCF-1705007 and GOOGLE Faculty Research Award.	Achlioptas D., 2001, P 33 ANN ACM S THEOR, P611; Alon N, 1997, ALGORITHMICA, V17, P209, DOI 10.1007/BF02523189; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Di Napoli E., 2016, NUMERICAL LINEAR ALG; ELBASSIONI K., 2015, J GRAPH ALGORITHMS A, V19, P273; Feige U, 2005, RANDOM STRUCT ALGOR, V27, P251, DOI 10.1002/rsa.20089; Friedman J., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P587, DOI 10.1145/73007.73063; Han I., 2016, ARXIV160600942; Han I, 2015, PR MACH LEARN RES, V37, P908; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Kloks T, 2000, INFORM PROCESS LETT, V74, P115, DOI 10.1016/S0020-0190(00)00047-8; Kong W., 2016, ARXIV160200061; Le C. M., 2015, ARXIV150203049; Li Y., 2016, ARXIV160408679; Li Y., 2014, P 25 ANN ACM SIAM S, P1562, DOI 10.1137/1.9781611973402.114; Mason J.C., 2002, CHEBYSHEV POLYNOMIAL; Schudy W., 2012, P 23 ANN ACM SIAM S; Uehara Ryuhei, 1999, NUMBER CONNECT UNPUB; Zhang Y., 2015, ARXIV150201403	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406048
C	Killian, T; Daulton, S; Konidaris, G; Doshi-Velez, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Killian, Taylor; Daulton, Samuel; Konidaris, George; Doshi-Velez, Finale			Robust and Efficient Transfer Learning with Hidden Parameter Markov Decision Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a new formulation of the Hidden Parameter Markov Decision Process (HiP-MDP), a framework for modeling families of related tasks using low-dimensional latent embeddings. Our new framework correctly models the joint uncertainty in the latent parameters and the state space. We also replace the original Gaussian Process-based model with a Bayesian Neural Network, enabling more scalable inference. Thus, we expand the scope of the HiP-MDP to applications with higher dimensions and more complex dynamics.	[Killian, Taylor; Daulton, Samuel; Doshi-Velez, Finale] Harvard Univ, Cambridge, MA 02138 USA; [Daulton, Samuel] Facebook, Menlo Pk, CA USA; [Konidaris, George] Brown Univ, Providence, RI 02912 USA	Harvard University; Facebook Inc; Brown University	Killian, T (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	taylorkillian@g.harvard.edu; sdaulton@g.harvard.edu; gdk@cs.brown.edu; finale@seas.harvard.edu			MIT Lincoln Laboratory Lincoln Scholars Program; NIH [R01MH109177]	MIT Lincoln Laboratory Lincoln Scholars Program; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We thank Mike Hughes, Andrew Miller, Jessica Forde, and Andrew Ross for their helpful conversations. TWK was supported by the MIT Lincoln Laboratory Lincoln Scholars Program. GDK is supported in part by the NIH R01MH109177. The content of this work is solely the responsibility of the authors and does not necessarily represent the official views of the NIH.	Adams BM, 2004, MATH BIOSCI ENG, V1, P223; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Bai HY, 2013, IEEE INT CONF ROBOT, P2853, DOI 10.1109/ICRA.2013.6630972; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Bonilla EV., 2008, ADV NEURAL INF PROCE, V20, P153, DOI DOI 10.5555/2981562.2981582; Brunskill E., 2013, C UNC ART INT; Caruana R, 1998, LEARNING TO LEARN, P95, DOI 10.1007/978-1-4615-5529-2_5; Chen M, 2016, IEEE INT CONF ROBOT, P5427, DOI 10.1109/ICRA.2016.7487754; Deisenroth MP, 2011, P P 28 INT C MACH LE; Delhaisse B, 2017, INT JOINT C NEUR NET; Depeweg S., 2017, ARXIV170608495; Depeweg S, 2017, INT C LEARN REPR; Dietrich CR, 1997, SIAM J SCI COMPUT, V18, P1088, DOI 10.1137/S1064827592240555; Doshi-Velez Finale, 2016, IJCAI (U S), V2016, P1432; Ernst D., 2006, P 45 IEEE C DEC CONT; Fern A., 2010, ADV NEURAL INFORM PR, P577; Gal Y., 2016, P 33 INT C MACH LEAR; Gal Y., 2016, DAT EFF MACH LEARN W; Genton MG, 2015, STAT SCI, V30, P147, DOI 10.1214/14-STS487; Gupta A., 2017, INT C LEARN REPR; Hernandez-Lobato J. M., 2016, P 33 INT C MACH LEAR; Jaques  N., 2015, P NIPS WORKSH MULT M; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kendall A., 2017, P ADV NEUR INF PROC, V30; Kingma D.P., 2015, INT C LEARN REPR ICL; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Marivate VN, 2014, WORKSH 28 AAAI C ART; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Moerland T. M., 2017, LEARNING MULTIMODAL; MOORE AW, 1993, MACH LEARN, V13, P103, DOI 10.1007/BF00993104; Moore BL, 2014, J MACH LEARN RES, V15, P655; Neal RM, 1992, TECHNICAL REPORT; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rasmussen C. E., 2003, ADV NEURAL INFORM PR, V15; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rosman B, 2016, MACH LEARN, V104, P99, DOI 10.1007/s10994-016-5547-y; Schaul T., 2016, INT C LEARN REPR ICL; Sehulam P, 2016, J MACH LEARN RES, V17, P1; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Taylor ME, 2009, J MACH LEARN RES, V10, P1633; Tenenbaum M, 2010, WORKSH NIPS; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Williams Jason D., 2006, AAAI WORKSH STAT EMP, P37	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406032
C	Kim, H; Gao, WH; Kannan, S; Oh, S; Viswanath, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kim, Hyeji; Gao, Weihao; Kannan, Sreeram; Oh, Sewoong; Viswanath, Pramod			Discovering Potential Correlations via Hypercontractivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INEQUALITIES; INFORMATION	Discovering a correlation from one variable to another variable is of fundamental scientific and practical interest. While existing correlation measures are suitable for discovering average correlation, they fail to discover hidden or potential correlations. To bridge this gap, (i) we postulate a set of natural axioms that we expect a measure of potential correlation to satisfy; (ii) we show that the rate of information bottleneck, i.e., the hypercontractivity coefficient, satisfies all the proposed axioms; (iii) we provide a novel estimator to estimate the hypercontractivity coefficient from samples; and (iv) we provide numerical experiments demonstrating that this proposed estimator discovers potential correlations among various indicators of WHO datasets, is robust in discovering gene interactions from gene expression time series data, and is statistically more powerful than the estimators for other correlation measures in binary hypothesis testing of canonical examples of potential correlations.	[Kim, Hyeji; Gao, Weihao; Oh, Sewoong; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Champaign, IL 61820 USA; [Kim, Hyeji; Gao, Weihao; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA; [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Champaign, IL 61820 USA; [Kannan, Sreeram] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; University of Washington; University of Washington Seattle	Kim, H (corresponding author), Univ Illinois, Coordinated Sci Lab, Champaign, IL 61820 USA.; Kim, H (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Champaign, IL 61820 USA.	hyejikim@illinois.edu; wgao9@illinois.edu; ksreeram@uw.edu; swoh@illinois.edu; pramodv@illinois.edu			NSF [CNS-1527754, CNS-1718270, CCF-1553452, CCF-1617745, CCF-1651236, CCF-1705007]; GOOGLE Faculty Research Award	NSF(National Science Foundation (NSF)); GOOGLE Faculty Research Award(Google Incorporated)	This work was partially supported by NSF grants CNS-1527754, CNS-1718270, CCF-1553452, CCF-1617745, CCF-1651236, CCF-1705007, and GOOGLE Faculty Research Award.	Achille A., 2016, 161101353 ARXIV; AHLSWEDE R, 1976, ANN PROBAB, V4, P925, DOI 10.1214/aop/1176995937; Alemi A. A., 2017, ICLR; Andrew G., 2013, INT C MACH LEARN, p1247?1255; [Anonymous], 2013, CORR; BECKNER W, 1975, ANN MATH, V102, P159, DOI 10.2307/1970980; Bekkerman R., 2003, Journal of Machine Learning Research, V3, P1183, DOI 10.1162/153244303322753625; BELL CB, 1962, ANN MATH STAT, V33, P587, DOI 10.1214/aoms/1177704583; BONAMI A, 1970, ANN I FOURIER, V20, P335, DOI 10.5802/aif.357; Davies E. B, 1992, IDEAS METHODS QUANTU, P370; Dhillon I. S., 2003, J MACHINE LEARNING R, V3; Gao W., 2016, P 33 INT C MACH LEAR, P2780; Gebelein H, 1941, Z ANGEW MATH MECH, V21, P364, DOI 10.1002/zamm.19410210604; Gorfine M., 2012, COMMENT DETECT UNPUB; GROSS L, 1975, DUKE MATH J, V42, P383, DOI 10.1215/S0012-7094-75-04237-4; Hirschfeld H, 1935, P CAMB PHILOS SOC, V31, P520, DOI 10.1017/S0305004100013517; Kahn J., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P68, DOI 10.1109/SFCS.1988.21923; Kim H., DISCOVERING PO UNPUB; Krishnaswamy S., 2014, SCIENCE; Michaeli T., 2016, INT C MACH LEARN, P1967; Mossel E, 2013, GEOM FUNCT ANAL, V23, P1062, DOI 10.1007/s00039-013-0229-4; Nair C., 2014, INF THEOR APPL WORKS; Nair C., 2016, COMMUNICATION; Nelson E., 1973, J FUNCT ANAL, V12, P97, DOI 10.1016/0022-1236(73)90091-8; Ngiam J, 2011, P 28 INT C MACH LEAR, V28, P689, DOI DOI 10.5555/3104482.3104569; O'Donnell R, 2014, ANAL BOOLEAN FUNCTIO; Pearson K., 1895, P R SOC LONDON, P240, DOI DOI 10.1098/RSPL.1895.0041; Renyi A., 1959, ACTA MATH ACAD SCI H, V10, DOI [DOI 10.1007/BF02024507, 10.1007/BF02024507]; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Simon N., 2014, ARXIV E PRINTS; Srivastava N., 2012, ADV NEURAL INFORM PR, P2222, DOI [DOI 10.1162/NEC0_A_00311, DOI 10.1109/CVPR.2013.49]; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Tishby Naftali, 1999, ALL C COMM CONTR COM	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404063
C	Kocaoglu, M; Shanmugam, K; Bareinboim, E		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kocaoglu, Murat; Shanmugam, Karthikeyan; Bareinboim, Elias			Experimental Design for Learning Causal Graphs with Latent Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INTERVENTIONS; DISCOVERY; INFERENCE; SELECTION	We consider the problem of learning causal structures with latent variables using interventions. Our objective is not only to learn the causal graph between the observed variables, but to locate unobserved variables that could confound the relationship between observables. Our approach is stage-wise: We first learn the observable graph, i.e., the induced graph between observable variables. Next we learn the existence and location of the latent variables given the observable graph. We propose an efficient randomized algorithm that can learn the observable graph using O(dlog(2) n) interventions where d is the degree of the graph. We further propose an efficient deterministic variant which uses O(log n + l) interventions, where l is the longest directed path in the graph. Next, we propose an algorithm that uses only O(d(2)log n) interventions that can learn the latents between both non-adjacent and adjacent variables. While a naive baseline approach would require O(n(2)) interventions, our combined algorithm can learn the causal graph with latents using O(dlog(2) n + d(2) log (n)) interventions.	[Kocaoglu, Murat] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA; [Shanmugam, Karthikeyan] IBM Res NY, New York, NY USA; [Bareinboim, Elias] Purdue Univ, Dept Comp Sci & Stat, W Lafayette, IN 47907 USA	University of Texas System; University of Texas Austin; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Kocaoglu, M (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	mkocaoglu@utexas.edu; karthikeyan.shanmugam2@ibm.com; eb@purdue.edu						Aho A. V., 1972, SIAM Journal on Computing, V1, P131, DOI 10.1137/0201008; Ali Ayesha R., 2005, P UNC ART INT; ALON N, 1986, COMBINATORICA, V6, P201, DOI 10.1007/BF02579381; Bareinboim E., 2012, P 28 C UNC ART INT, P113; Bareinboim E, 2016, P NATL ACAD SCI USA, V113, P7345, DOI 10.1073/pnas.1510507113; Bensmail J., 2015, ELECT NOTE DISCRETE, V49, P773; Borboudakis Sofia, 2012, 6 EUR WORKSH PROB GR; Buhlmann P, 2012, P 6 EUR WORKSH PROB; Claassen T., 2010, ADV NEURAL INFORM PR, P415; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt F, 2007, PHILOS SCI, V74, P981, DOI 10.1086/525638; Hauser A, 2012, J MACH LEARN RES, V13, P2409; Heinze-Deml Christina, 2017, ANN REV STAT ITS APP; Hoyer P. O, 2008, P NIPS 2008; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Hyttinen Antti, 2013, ARXIV13096836; Katona G., 1966, J COMB THEORY, V1, P174; Kocaoglu Murat, 2017, ICML 17; Kocaoglu Murat, 2017, AAAI 17; Kocaoglu Murat, 2017, R28 PURD U AI LAB; Loh PL, 2014, J MACH LEARN RES, V15, P3065; Magliacane S., 2016, ARXIV161110351; Meganck Stijn, 2006, P 3 EUR WORKSH PROB; Parviainen Pekka, 2011, JOINT EUR C MACH LEA; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021; Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Scholkopf B., 2015, P 32 INT C MACH LEAR; Shanmugam K., 2015, NIPS 2015; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; VERMA TS, 1992, P 8 INT C UNC ART IN; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001; Zhang JJ, 2008, J MACH LEARN RES, V9, P1437	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407011
C	Koren, T; Livni, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Koren, Tomer; Livni, Roi			Affine-Invariant Online Optimization and the Low-rank Experts Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS; BOUNDS	We present a new affine-invariant optimization algorithm called Online Lazy Newton. The regret of Online Lazy Newton is independent of conditioning: the algorithm's performance depends on the best possible preconditioning of the problem in retrospect and on its intrinsic dimensionality. As an application, we show how Online Lazy Newton can be used to achieve an optimal regret of order root rT for the low-rank experts problem, improving by a root r factor over the previously best known bound and resolving an open problem posed by Hazan et al. [15].	[Koren, Tomer] Google Brain, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA; [Livni, Roi] Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA	Google Incorporated; Princeton University	Koren, T (corresponding author), Google Brain, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA.	tkoren@google.com; rlivni@cs.princeton.edu	Jeong, Yongwook/N-7413-2016		Eric and Wendy Schmidt Fund for Strategic Innovations	Eric and Wendy Schmidt Fund for Strategic Innovations	The authors would like to thank Elad Hazan for helpful discussions. RL is supported by the Eric and Wendy Schmidt Fund for Strategic Innovations.	[Anonymous], 2012, C LEARN THEOR; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; Barman S., 2017, ARXIV170604125; Cesa-Bianchi N, 2005, SIAM J COMPUT, V34, P640, DOI 10.1137/S0097539703432542; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cutkosky Ashok, 2017, ARXIV170302629; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Foster D. J., 2017, ARXIV170404010; Hazan E., 2016, 29 ANN C LEARN THEOR, P1096; Hazan E., 2009, P ANN C NEUR INF PRO, P709; Hazan E, 2006, LECT NOTES ARTIF INT, V4005, P499, DOI 10.1007/11776420_37; Hazan E, 2011, J MACH LEARN RES, V12, P1287; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Koren Tomer, 2015, ADV NEURAL INFORM PR, P19; LUO H, 2016, ADV NEURAL INFORM PR, P902; Mannor S., 2017, ARXIV170207870; Rakhlin A., 2013, P 16 INT C ART INT S, P516; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Streeter M., 2010, TECHNICAL REPORT; van Erven T, 2016, ADV NEUR IN, V29; Vovk V, 2001, INT STAT REV, V69, P213; Zinkevich M., 2003, ONLINE CONVEX PROGRA	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404079
C	Koster, U; Webb, TJ; Wang, X; Nassar, M; Bansal, AK; Constable, WH; Elibol, OH; Gray, S; Hall, S; Hornof, L; Khosrowshahi, A; Kloss, C; Pai, RJ; Rao, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Koster, Urs; Webb, Tristan J.; Wang, Xin; Nassar, Marcel; Bansal, Arjun K.; Constable, William H.; Elibol, Oguz H.; Gray, Scott; Hall, Stewart; Hornof, Luke; Khosrowshahi, Amir; Kloss, Carey; Pai, Ruby J.; Rao, Naveen			Flexpoint: An Adaptive Numerical Format for Efficient Training of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep neural networks are commonly developed and trained in 32-bit floating point format. Significant gains in performance and energy efficiency could be realized by training and inference in numerical formats optimized for deep learning. Despite advances in limited precision inference in recent years, training of neural networks in low bit-width remains a challenging problem. Here we present the Flexpoint data format, aiming at a complete replacement of 32-bit floating point format training and inference, designed to support modern deep network topologies without modifications. Flexpoint tensors have a shared exponent that is dynamically adjusted to minimize overflows and maximize available dynamic range. We validate Flexpoint by training AlexNet [1], a deep residual network [2, 3] and a generative adversarial network [4], using a simulator implemented with the neon deep learning framework. We demonstrate that 16-bit Flexpoint closely matches 32-bit floating point in training all three models, without any need for tuning of model hyperparameters. Our results suggest Flexpoint as a promising numerical format for future hardware for training and inference.	[Koster, Urs; Webb, Tristan J.; Wang, Xin; Nassar, Marcel; Bansal, Arjun K.; Constable, William H.; Elibol, Oguz H.; Gray, Scott; Hall, Stewart; Hornof, Luke; Khosrowshahi, Amir; Kloss, Carey; Pai, Ruby J.; Rao, Naveen] Intel Corp, Artificial Intelligence Prod Grp, Santa Clara, CA 95051 USA; [Koster, Urs; Hall, Stewart] Nervana Syst, Cerebras Syst, San Diego, CA 92121 USA; [Koster, Urs; Hall, Stewart] Intel Corp, Santa Clara, CA 95051 USA; [Gray, Scott] Nervana Syst, OpenAI, San Diego, CA 92121 USA	Intel Corporation; Intel Corporation	Koster, U (corresponding author), Intel Corp, Artificial Intelligence Prod Grp, Santa Clara, CA 95051 USA.; Koster, U (corresponding author), Nervana Syst, Cerebras Syst, San Diego, CA 92121 USA.; Koster, U (corresponding author), Intel Corp, Santa Clara, CA 95051 USA.		Jeong, Yongwook/N-7413-2016					Abadi M., TENSORFLOW LARGE SCA; [Anonymous], 2017, ARXIV170404760; Bottou L., 2017, ARXIV170107875STATML; Courbariaux M., 2015, ADV NEUR IN, P3123; Courbariaux Matthieu, 2014, INT C LEARN REPR WOR; Gupta S., 2015, ARXIV150202551; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heusel M., 2017, ABS170608500 CORR; Hubara I., 2016, ARXIV160907061; Hwang K, 2014, IEEE WRK SIG PRO SYS, P174; Kim Minje, 2016, ARXIV160106071; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lin DD, 2016, PR MACH LEARN RES, V48; Lin Z., 2015, ARXIV151003009; Mellempudi N., 2017, ARXIV170108978; Miyashita D., 2016, ARXIV160301025; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Seide F, 2014, INTERSPEECH, P1058; Vanhoucke Vincent, 2011, ADV NEURAL INFORM PR; Venkatesh G., 2016, ARXIV161000324; WILLIAMSON D, 1991, IEEE PACIF, P315, DOI 10.1109/PACRIM.1991.160742; Yu F., 2015, ARXIVABS150603365 CO; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	25	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401075
C	Kotlowski, W; Koolen, WM; Malek, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kotlowski, Wojciech; Koolen, Wouter M.; Malek, Alan			Random Permutation Online Isotonic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				BOUNDS	We revisit isotonic regression on linear orders, the problem of fitting monotonic functions to best explain the data, in an online setting. It was previously shown that online isotonic regression is unlearnable in a fully adversarial model, which lead to its study in the fixed design model. Here, we instead develop the more practical random permutation model. We show that the regret is bounded above by the excess leave-one-out loss for which we develop efficient algorithms and matching lower bounds. We also analyze the class of simple and popular forward algorithms and recommend where to look for algorithms for online isotonic regression on partial orders.	[Kotlowski, Wojciech] Poznan Univ Tech, Poznan, Poland; [Koolen, Wouter M.] Ctr Wiskunde & Informat, Amsterdam, Netherlands; [Malek, Alan] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Poznan University of Technology; Massachusetts Institute of Technology (MIT)	Kotlowski, W (corresponding author), Poznan Univ Tech, Poznan, Poland.	wkotlowski@cs.put.poznan.pl; wmkoolen@cwi.nl; amalek@mit.edu	Kotlowski, Wojciech/O-4730-2014		Polish National Science Centre [2016/22/E/ST6/00299]; Netherlands Organization for Scientific Research (NWO) [639.021.439]	Polish National Science Centre; Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	Wojciech Kotlowski acknowledges support from the Polish National Science Centre (grant no. 2016/22/E/ST6/00299). Wouter Koolen acknowledges support from the Netherlands Organization for Scientific Research (NWO) under Veni grant 639.021.439. This work was done in part while Koolen was visiting the Simons Institute for the Theory of Computing.	AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; BIRGE L, 1993, PROBAB THEORY REL, V97, P113, DOI 10.1007/BF01199316; BRUNK HD, 1955, ANN MATH STAT, V26, P607, DOI 10.1214/aoms/1177728420; Cesa-Bianchi N, 2001, MACH LEARN, V43, P247, DOI 10.1023/A:1010848128995; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; de Leeuw J, 2009, J STAT SOFTW, V32, P1; Fawcett T, 2007, MACH LEARN, V68, P97, DOI 10.1007/s10994-007-5011-0; Forster J, 2002, J COMPUT SYST SCI, V64, P76, DOI 10.1006/jcss.2001.1798; Gaillard P., 2015, C LEARN THEOR, P764; Kakade S., 2011, ADV NEURAL INFORM PR; KALAI A. T., 2009, COLT; Kotlowski W., 2009, P 26 ANN INT C MACH, P537; Kotlowski Wojciech, 2016, P 29 C LEARN THEOR C, V49, P1165; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Kyng Rasmus, 2015, NEURAL INFORM PROCES; Luss R, 2012, ANN APPL STAT, V6, P253, DOI 10.1214/11-AOAS504; Menon Aditya Krishna, 2012, INT C MACH LEARN ICM; Moon T., 2010, P 3 ACM INT C WEB SE, P151, DOI DOI 10.1145/1718487.1718507; Narasimhan Harikrishna, 2013, ADV NEURAL INFORM PR, P2913; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Obozinski G, 2008, GENOME BIOL, V9, DOI 10.1186/gb-2008-9-s1-s6; Rakhlin Alexander, 2014, C LEARN THEOR, P1232; Robertson T., 1998, ORDER RESTRICTED STA; Stylianou M, 2002, BIOMETRICS, V58, P171, DOI 10.1111/j.0006-341X.2002.00171.x; VANDEGEER S, 1990, ANN STAT, V18, P907, DOI 10.1214/aos/1176347632; Vovk V., 2015, ADV NEURAL INFORM PR, P892	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404025
C	Krichene, W; Bartlett, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Krichene, Walid; Bartlett, Peter			Acceleration and Averaging In Stochastic Descent Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MIRROR DESCENT; OPTIMIZATION	We formulate and study a general family of (continuous-time) stochastic dynamics for accelerated first-order minimization of smooth convex functions. Building on an averaging formulation of accelerated mirror descent, we propose a stochastic variant in which the gradient is contaminated by noise, and study the resulting stochastic differential equation. We prove a bound on the rate of change of an energy function associated with the problem, then use it to derive estimates of convergence rates of the function values (almost surely and in expectation), both for persistent and asymptotically vanishing noise. We discuss the interaction between the parameters of the dynamics (learning rate and averaging rates) and the covariation of the noise process. In particular, we show how the asymptotic rate of covariation affects the choice of parameters and, ultimately, the convergence rate.	[Krichene, Walid] Google Inc, Mountain View, CA 94043 USA; [Bartlett, Peter] Univ Calif Berkeley, Berkeley, CA USA	Google Incorporated; University of California System; University of California Berkeley	Krichene, W (corresponding author), Google Inc, Mountain View, CA 94043 USA.	walidk@google.com; bartlett@cs.berkeley.edu			NSF [IIS-1619362]; Australian Research Council through an Australian Laureate Fellowship [FL110100281]; Australian Research Council through Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS)	NSF(National Science Foundation (NSF)); Australian Research Council through an Australian Laureate Fellowship(Australian Research Council); Australian Research Council through Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS)(Australian Research Council)	We gratefully acknowledge the support of the NSF through grant IIS-1619362 and of the Australian Research Council through an Australian Laureate Fellowship (FL110100281) and through the Australian Research Council Centre of Excellence for Mathematical and Statistical Frontiers (ACEMS). We thank the anonymous reviewers for their insightful comments and suggestions.	Attouch H., 2015, ABS150704782 CORR; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Benaim M., 1996, Journal of Dynamics and Differential Equations, V8, P141, DOI 10.1007/BF02218617; Benaim M, 1999, LECT NOTES MATH, V1709, P1; BLACK F, 1973, J POLIT ECON, V81, P637, DOI 10.1086/260062; Bloch A., 1994, HAMILTONIAN GRADIENT; Bottou Leon, 2016, ABS160604838 CORR; Bubeck S., 2015, ADV NEURAL INFORM PR, V28, P1243; Cabot A, 2009, T AM MATH SOC, V361, P5983, DOI 10.1090/S0002-9947-09-04785-0; Cheng X., 2017, ABS170703663 CORR; Cheng X., 2017, ABS170509048 CORR; CHIANG TS, 1987, SIAM J CONTROL OPTIM, V25, P737, DOI 10.1137/0325042; Dalalyan AS, 2017, J R STAT SOC B, V79, P651, DOI 10.1111/rssb.12183; Duchi John C., 2010, SIAM J OPTIMIZ, V22, P1549; Durmus A., 2016, CORR; Eberle A., 2017, CORR; Errami M, 2002, PROBAB THEORY REL, V122, P191, DOI 10.1007/s004400100168; HELMKE U, 1994, COMMUNICATIONS CONTR; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Krichene W., 2016, NIPS; Krichene W., 2015, NIPS; Lyapunov AM, 1892, THESIS; Mertikopoulos P., 2016, ABS161106730 CORR; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; NEMIROVSKY A. S., 1983, WILEY INTERSCIENCE S; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Oksendal B., 2013, STOCHASTIC DIFFERENT; PAVLIOTIS GA, 2014, TEXTS APPL MATH; Raginsky M., 2017, ABS170203849 CORR; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; Rockafellar R. T., 1970, CONVEX ANAL; Su Weijie, 2014, NIPS; Wibisono A., 2016, ABS160304245 CORR; Wilson A. C., 2016, ABS161102635 CORR; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406083
C	Kuznetsov, V; Mohri, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kuznetsov, Vitaly; Mohri, Mehryar			Discriminative State-Space Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce and analyze Discriminative State-Space Models for forecasting non-stationary time series. We provide data-dependent generalization guarantees for learning these models based on the recently introduced notion of discrepancy. We provide an in-depth analysis of the complexity of such models. We also study the generalization guarantees for several structural risk minimization approaches to this problem and provide an efficient implementation for one of them which is based on a convex objective.	[Kuznetsov, Vitaly; Mohri, Mehryar] Google Res, New York, NY 10011 USA; [Mohri, Mehryar] Courant Inst, New York, NY 10011 USA	Google Incorporated	Kuznetsov, V (corresponding author), Google Res, New York, NY 10011 USA.	vitaly@cims.nyu.edu; mohri@cims.nyu.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1618662, CCF-1535987]; Google Research Award	NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated)	This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662, as well as a Google Research Award.	Barve Rakesh D., 1996, COLT; Bollerslev T, 1986, J ECONOMETRICS; Box G. E. P., 1990, TIME SERIES ANAL FOR; Brockwell P. J., 1986, TIME SERIES THEORY M; Commandeur J. J., 2007, INTRO STATE SPACE TI; Cortes Corinna, 2017, ABS171010657 CORR; de la Pena V. H., 1999, PROBABILITY ITS APPL; Durbin J., 2012, TIME SERIES ANAL STA, DOI DOI 10.1093/ACPROF:OSO/9780199641178.001.0001; ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773; Hamilton J.D., 1994, TIME SERIES ANAL, DOI 10.2307/j.ctv14jx6sm; Kalman Rudolph Emil, 1960, T ASME J BASIC ENG, V82; Kuznetsov V., 2015, ADV NEURAL INFORM PR, P541; Kuznetsov Vitaly, 2016, P 29 C LEARN THEOR C; Kuznetsov Vitaly, 2014, ALT; Littlestone N, 1987, MACHINE LEARNING; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Rakhlin A., 2010, NIPS; Rakhlin Alexander, 2011, NIPS; Rakhlin Alexander, 2015, PROBABILITY THEORY R; Rakhlin Alexander, 2015, JMLR, V16; Zimin A., 2017, AISTAT; [No title captured]	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405073
C	Lattimore, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lattimore, Tor			A Scale Free Algorithm for Stochastic Bandits with Bounded Kurtosis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Existing strategies for finite-armed stochastic bandits mostly depend on a parameter of scale that must be known in advance. Sometimes this is in the form of a bound on the payoffs, or the knowledge of a variance or subgaussian parameter. The notable exceptions are the analysis of Gaussian bandits with unknown mean and variance by Cowan et al. [2015] and of uniform distributions with unknown support [Cowan and Katehakis, 2015]. The results derived in these specialised cases are generalised here to the non-parametric setup, where the learner knows only a bound on the kurtosis of the noise, which is a scale free measure of the extremity of outliers.	[Lattimore, Tor] DeepMind, London, England		Lattimore, T (corresponding author), DeepMind, London, England.	tor.lattimore@gmail.com	Jeong, Yongwook/N-7413-2016					Alon N., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P20, DOI 10.1145/237814.237823; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Bubeck S, 2012, REGRET ANAL STOCHAST; Burnetas AN, 1996, ADV APPL MATH, V17, P122, DOI 10.1006/aama.1996.0007; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Cowan W., 2015, ARXIV150501918; Cowan Wesley, 2015, ARXIV150405823V2; DeCarlo LT, 1997, PSYCHOL METHODS, V2, P292, DOI 10.1037/1082-989X.2.3.292; Honda J, 2015, J MACH LEARN RES, V16, P3721; Honda Junya, 2010, COLT, P67; KATEHAKIS MN, 1995, P NATL ACAD SCI USA, V92, P8584, DOI 10.1073/pnas.92.19.8584; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T., 2015, ARXIV150707880; Pena V. H., 2008, SELF NORMALIZED PROC	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401060
C	Law, HCL; Yau, C; Sejdinovic, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Law, Ho Chung Leon; Yau, Christopher; Sejdinovic, Dino			Testing and Learning on Distributions with Symmetric Noise Invariance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				EMBEDDINGS	Kernel embeddings of distributions and the Maximum Mean Discrepancy (MMD), the resulting distance between distributions, are useful tools for fully nonparametric two-sample testing and learning on distributions. However, it is rare that all possible differences between samples are of interest - discovered differences can be due to different types of measurement noise, data collection artefacts or other irrelevant sources of variability. We propose distances between distributions which encode invariance to additive symmetric noise, aimed at testing whether the assumed true underlying processes differ. Moreover, we construct invariant features of distributions, leading to learning algorithms robust to the impairment of the input distributions with symmetric additive noise.	[Law, Ho Chung Leon; Sejdinovic, Dino] Univ Oxford, Dept Stat, Oxford, England; [Yau, Christopher] Univ Birmingham, Ctr Computat Biol, Birmingham, W Midlands, England	University of Oxford; University of Birmingham	Law, HCL (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	hlaw@statsox.ac.uk; c.yau@bham.ac.uk; dino.sejdinovic@stats.ox.ac.uk	Jeong, Yongwook/N-7413-2016	Yau, Christopher/0000-0001-7615-8523; Sejdinovic, Dino/0000-0001-5547-9213	EPSRC; MRC through the OxWaSP CDT programme [EP/L016710/1]; MRC [MR/L001411/1]; Spanish MultiDark Consolider Project [CSD2009-00064]; Gauss Centre for Supercomputing e.V.; Partnership for Advanced Supercomputing in Europe (PRACE)	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); MRC through the OxWaSP CDT programme(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); MRC(UK Research & Innovation (UKRI)Medical Research Council UK (MRC)); Spanish MultiDark Consolider Project(Spanish Government); Gauss Centre for Supercomputing e.V.; Partnership for Advanced Supercomputing in Europe (PRACE)	We thank Dougal Sutherland for suggesting the use of of the dark matter dataset, Michelle Ntampaka for providing the catalog, as well as Ricardo Silva, Hyunjik Kim and Kaspar Martens for useful discussions. This work was supported by the EPSRC and MRC through the OxWaSP CDT programme (EP/L016710/1). C.Y. and H.C.L.L. also acknowledge the support of the MRC Grant No. MR/L001411/1.; The CosmoSim database used in this paper is a service by the Leibniz-Institute for Astrophysics Potsdam (AIP). The MultiDark database was developed in cooperation with the Spanish MultiDark Consolider Project CSD2009-00064. The authors gratefully acknowledge the Gauss Centre for Supercomputing e.V. (www.gauss-centre.eu) and the Partnership for Advanced Supercomputing in Europe (PRACE, www.prace-ri.eu) for funding the MultiDark simulation project by providing computing time on the GCS Supercomputer SuperMUC at Leibniz Supercomputing Centre (LRZ, www.lrz.de).	Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Chwialkowski K. P., 2015, ADV NEURAL INFORM PR, P1981; Delaigle A, 2016, J R STAT SOC B, V78, P231, DOI 10.1111/rssb.12109; Fearnhead P, 2012, J R STAT SOC B, V74, P419, DOI 10.1111/j.1467-9868.2011.01010.x; Gretton A, 2012, J MACH LEARN RES, V13, P723; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jitkrittum W., 2016, ADV NEURAL INFORM PR, P181; Kingma D.P, P 3 INT C LEARNING R; Klypin A., 2014, ARXIV14114001; Law H.C.L., 2017, ARXIV170504293; Lichman M, 2013, UCI MACHINE LEARNING; LINNIK Y, 1977, DECOMPOSITION RANDOM; Mitrovic J, 2016, PR MACH LEARN RES, V48; Muandet K., 2012, PROC 25 INT C NEURAL, P10; Muandet K., 2016, ARXIV PREPRINT ARXIV, P133; Ntampaka M, 2016, ASTROPHYS J, V831, DOI 10.3847/0004-637X/831/2/135; Ntampaka M, 2015, ASTROPHYS J, V803, DOI 10.1088/0004-637X/803/2/50; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rossberg H.-J., 1995, J MATH SCI, V76, P2181, DOI DOI 10.1007/BF02363232; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Sutherland Dougal J., 2016, 30 AAAI C ART INT, P2073; Szabo Zoltan, 2015, P INT C ART INT STAT; Wang Z, 2012, IEEE T GEOSCI REMOTE, V50, P2226, DOI 10.1109/TGRS.2011.2171691; Wendland H, 2004, SCATTERED DATA APPRO, V17	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401037
C	Lehrmann, AM; Sigal, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lehrmann, Andreas M.; Sigal, Leonid			Non-parametric Structured Output Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep neural networks (DNNs) and probabilistic graphical models (PGMs) are the two main tools for statistical modeling. While DNNs provide the ability to model rich and complex relationships between input and output variables, PGMs provide the ability to encode dependencies among the output variables themselves. End-to-end training methods for models with structured graphical dependencies on top of neural predictions have recently emerged as a principled way of combining these two paradigms. While these models have proven to be powerful in discriminative settings with discrete outputs, extensions to structured continuous spaces, as well as performing efficient inference in these spaces, are lacking. We propose non-parametric structured output networks (NSON), a modular approach that cleanly separates a non-parametric, structured posterior representation from a discriminative inference scheme but allows joint end-to-end training of both components. Our experiments evaluate the ability of NSONs to capture structured posterior densities (modeling) and to compute complex statistics of those densities (inference). We compare our model to output spaces of varying expressiveness and popular variational and sampling-based inference algorithms.	[Lehrmann, Andreas M.; Sigal, Leonid] Disney Res, Pittsburgh, PA 15213 USA		Lehrmann, AM (corresponding author), Disney Res, Pittsburgh, PA 15213 USA.	andreas.lehrmann@disneyresearch.com; lsigal@disneyresearch.com	Jeong, Yongwook/N-7413-2016					Adams A., 2010, COMPUTER GRAPHICS FO; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bishop Christopher M, 1994, TECH REP; Campbell N., 2013, CVPR; Chen Liang-Chich, 2015, ABS14127062 CORR; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Deng Z., 2015, CVPR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; GHAHRAMANI Z, 2001, NIPS; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Ihler A., 2009, P AISTATS, V5, P256; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Isard M., 2003, CVPR; Jain A., 2016, CVPR; Kingma D.P, P 3 INT C LEARNING R; Koller D., 2009, PROBABILISTIC GRAPHI; Koller D., 1999, UAI; Kothapa R., 2011, TECHNICAL REPORT; Kraehenbuehl P., 2012, NIPS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lehrmann A., 2013, ICCV; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Pacheco J, 2014, PR MACH LEARN RES, V32, P1152; PARK M, 2008, CVPR; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rena S., 2015, ARXIV150601497CSCV; Ross S., 2011, CVPR; Schwing A. G., 2015, ARXIV PREPRINT ARXIV; Scott D. W., 1992, MULTIVARIATE DENSITY, DOI 10.1002/9780470316849; Shelhamer E., 2016, IEEE TPAMI; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sudderth EB, 2003, PROC CVPR IEEE, P605; Wang Weiran, 2013, ARXIV13091541; Weiss Y, 2001, NEURAL COMPUT, V13, P2173, DOI 10.1162/089976601750541769; Yedidia J S, 2001, TECHNICAL REPORT, V13; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404028
C	Li, C; Wong, FMF; Liu, ZM; Kanade, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Cheng; Wong, Felix M. F.; Liu, Zhenming; Kanade, Varun			From which world is your graph?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NETWORKS; MODELS	Discovering statistical structure from links is a fundamental problem in the analysis of social networks. Choosing a misspecified model, or equivalently, an incorrect inference algorithm will result in an invalid analysis or even falsely uncover patterns that are in fact artifacts of the model. This work focuses on unifying two of the most widely used link-formation models: the stochastic blockmodel (SBM) and the small world (or latent space) model (SWM). Integrating techniques from kernel learning, spectral graph theory, and nonlinear dimensionality reduction, we develop the first statistically sound polynomial-time algorithm to discover latent patterns in sparse graphs for both models. When the network comes from an SBM, the algorithm outputs a block structure. When it is from an SWM, the algorithm outputs estimates of each node's latent position.	[Li, Cheng; Liu, Zhenming] Coll William & Mary, Williamsburg, VA 23187 USA; [Kanade, Varun] Univ Oxford, Oxford, England; [Wong, Felix M. F.] Google, Menlo Pk, CA USA	William & Mary; University of Oxford; Google Incorporated	Li, C (corresponding author), Coll William & Mary, Williamsburg, VA 23187 USA.		Jeong, Yongwook/N-7413-2016; Kanade, Varun/AAS-3434-2020	Kanade, Varun/0000-0002-2300-4819				Abbe E., 2015, ARXIV151209080; Abbe Emmanuel, 2015, P 56 ANN IEEE FDN CO, P18; Abbe Emmanuel, 2016, OMMUNITY DETECTION S; Abraham I, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1853; Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Airoldi EM., 2013, ADV NEURAL INFORM PR, V26, P692; Badoiu Mihai, 2005, P 37 ANN ACM S THEOR, P225; Barbera P, 2015, PSYCHOL SCI, V26, P1531, DOI 10.1177/0956797615594620; Barbera Pablo, 2012, BIRDS SAME FEATHER T; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; BORG I., 2005, MODERN MULTIDIMENSIO, P207; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Choi D, 2014, ANN STAT, V42, P29, DOI 10.1214/13-AOS1173; Clinton J, 2004, AM POLIT SCI REV, V98, P355, DOI 10.1017/S0003055404001194; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Dhillon I.S., 2001, P 7 ACM SIGKDD INT C, P269, DOI DOI 10.1145/502512.502550; Gerrish S., 2012, P NIPS; Gerrish S. M., 2011, P ICML; Grimmer Justin, 2013, POLITICAL ANAL; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Indyk P., 2004, HDB DISCRETE COMPUTA, P177, DOI DOI 10.1201/9781420035315.CH8; Kanade V, 2016, IEEE T INFORM THEORY, V62, P5906, DOI 10.1109/TIT.2016.2516564; Kleinberg J., 2000, Proceedings of the Thirty Second Annual ACM Symposium on Theory of Computing, P163, DOI 10.1145/335305.335325; Konig H., 1986, OPERATOR THEORY ADV, V16; Laver Michael, 2003, AM POLITICAL SCI REV, V97; Leskovec J., 2010, P 19 INT C WORLD WID, P631; Leskovec J., 2008, P 17 INT C WORLD WID, P695, DOI DOI 10.1145/1367497.1367591; Li Cheng, 2017, WHICH WORLD IS YOUR; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Mihail M., 2002, Randomization and Approximation Techniques in Computer Science. 6th International Workshop, RANDOM 2002. Proceedings (Lecture Notes in Computer Science Vol.2483), P254; Mossel E., 2013, PROOF BLOCK MODEL TH; Newman MEJ, 2006, PHYS REV E, V74, DOI 10.1103/PhysRevE.74.036104; Newman MEJ, 2004, PHYS REV E, V70, DOI [10.1103/PhysRevE.70.056131, 10.1103/PhysRevE.69.026113]; Newman MEJ, 2002, P NATL ACAD SCI USA, V99, P2566, DOI 10.1073/pnas.012582999; POOLE KT, 1985, AM J POLIT SCI, V29, P357, DOI 10.2307/2111172; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Raghavan UN, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.036106; Rohe K, 2016, P NATL ACAD SCI USA, V113, P12679, DOI 10.1073/pnas.1525793113; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Rosasco L, 2010, J MACH LEARN RES, V11, P905; Scholkopf B., 2001, LEARNING KERNELS SUP; Silva V.D., 2003, NIPS, P721; Sofia C., 2013, NONPARAMETRIC GRAPHO; Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112; Tauberer Joshua, 2012, LAW VIA THE INTERNET; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Wong FMF, 2016, IEEE T KNOWL DATA EN, V28, P2158, DOI 10.1109/TKDE.2016.2553667; Xu J., 2014, C LEARNING THEORY, P903; Yun S.Y., 2016, ADV NEURAL INFORM PR, V29, P965; Zhao YP, 2012, ANN STAT, V40, P2266, DOI 10.1214/12-AOS1036; Zhou T, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.046115	56	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401049
C	Li, CJ; Wang, MD; Liu, H; Zhang, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Chris Junchi; Wang, Mengdi; Liu, Han; Zhang, Tong			Diffusion Approximations for Online Principal Component Estimation and Global Convergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				EIGENVALUE; POWER; PCA	In this paper, we propose to adopt the diffusion approximation tools to study the dynamics of Oja's iteration which is an online stochastic gradient descent method for the principal component analysis. Oja's iteration maintains a running estimate of the true principal component from streaming data and enjoys less temporal and spatial complexities. We show that the Oja's iteration for the top eigenvector generates a continuous-state discrete-time Markov chain over the unit sphere. We characterize the Oja's iteration in three phases using diffusion approximation and weak convergence tools. Our three-phase analysis further provides a finite-sample error bound for the running estimate, which matches the minimax information lower bound for principal component analysis under the additional assumption of bounded samples.	[Li, Chris Junchi; Wang, Mengdi; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA; [Zhang, Tong] Tencent AI Lab, Shennan Ave, Shenzhen 518057, Guangdong, Peoples R China	Princeton University; Tencent	Li, CJ (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	junchil@princeton.edu; mengdiw@princeton.edu; hanliu@princeton.edu; tongzhang@tongzhang-ml.org	Zhang, Tong/HGC-1090-2022; LI, chris/HDO-6232-2022; Jeong, Yongwook/N-7413-2016					Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Anandkumar A., 2016, ARXIV160205908; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; [Anonymous], 1989, PROBABILITY APPROXIM; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; Arora Raman, 2013, ADV NEURAL INFORM PR, P1815; BALSUBRAMANI A., 2013, ADV NEURAL INFORM PR, V26, P3174, DOI 10.1016/j.compbiomed.2021.104502; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; Darken C, 1991, ADV NEURAL INFORM PR, P1009; De Sa C, 2015, PR MACH LEARN RES, V37, P2332; Ethier S. N., 2005, MARKOV PROCESSES CHA, V282; Garber D., 2015, ARXIV150905647; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Helmke U, 1994, OPTIMIZATION DYNAMIC; Jain Prateek, 2016, ARXIV160206929; Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121; Krasulina T.P., 1969, USSR COMP MATH MATH, V9, P189, DOI DOI 10.1016/0041-5553(69)90135-9; KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066; Lee J. D., 2016, C LEARN THEOR, P1246; Levin D. A., 2009, MARKOV CHAINS MIXING; Li Chris J, 2016, ARXIV PREPRINT ARXIV; LI Q., 2015, ARXIV151106251; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; Mandt Stephan, 2016, ARXIV160202666; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; Musco Cameron, 2015, ARXIV150405477; Nadler B, 2008, ANN STAT, V36, P2791, DOI 10.1214/08-AOS618; OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Oksendal B., 2013, STOCHASTIC DIFFERENT; Panageas I., 2016, ARXIV160500405; Shamir O., 2015, ARXIV150909002; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Shamir Ohad, 2015, ARXIV150708788; Su WJ, 2016, J MACH LEARN RES, V17; Sun J., 2015, ARXIV151006096; Sun J., 2015, ARXIV151104777; Sun Ju, 2015, ARXIV151103607; Vu V., 2012, INT C ARTIFICIAL INT, P1278; Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151; Wang Z., 2014, ARXIV14085352; WRIGHT J., 2016, ARXIV160206664; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zou H, 2006, J AM STAT ASSOC, V101, P1418, DOI 10.1198/016214506000000735	45	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400062
C	Li, CY; Liu, H; Chen, CY; Pu, YC; Chen, LQ; Henao, R; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Chunyuan; Liu, Hao; Chen, Changyou; Pu, Yunchen; Chen, Liqun; Henao, Ricardo; Carin, Lawrence			ALICE: Towards Understanding Adversarial Learning for Joint Distribution Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We investigate the non-identifiability issues associated with bidirectional adversarial training for joint distribution matching. Within a framework of conditional entropy, we propose both adversarial and non-adversarial approaches to learn desirable matched joint distributions for unsupervised and supervised tasks. We unify a broad family of adversarial models as joint distribution matching problems. Our approach stabilizes learning of unsupervised bidirectional adversarial learning methods. Further, we introduce an extension for semi-supervised learning tasks. Theoretical results are validated in synthetic data and real-world applications.	[Li, Chunyuan; Pu, Yunchen; Chen, Liqun; Henao, Ricardo; Carin, Lawrence] Duke Univ, Durham, NC 27708 USA; [Liu, Hao] Nanjing Univ, Nanjing, Jiangsu, Peoples R China; [Chen, Changyou] Univ Buffalo, Buffalo, NY USA	Duke University; Nanjing University; State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Li, CY (corresponding author), Duke Univ, Durham, NC 27708 USA.	cl319@duke.edu	Jeong, Yongwook/N-7413-2016; Li, Chunyuan/AAG-1303-2020		ARO; DARPA; ONR; NSF; DOE; NGA	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE)); NGA	We acknowledge Shuyang Dai, Chenyang Tao and Zihang Dai for helpful feedback/editing. This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.	[Anonymous], 2017, ICLR; [Anonymous], 2016, ICML; [Anonymous], 2017, ICCV; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Ba J., 2017, P 3 INT C LEARN REPR; Chen X, 2016, ADV NEUR IN, V29; Darrell T, 2017, ICLR; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Fidler S, 2012, NIPS, P620; Gan Zhe, 2017, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Isola P., 2017, CVPR; Kim T, 2017, ICML; Larsen A., 2016, ICML; Li C., 2017, NIPS; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Mescheder L., 2017, ICML; Mirza M, 2014, COMPUT SCI, V2014, P2672; Pu Y., 2016, NIPS; Pu Y., 2017, NIPS; Salimans Tim, 2016, ADV NEURAL INFORM PR; Vincent Pascal., 2008, ICML; Wang Z., 2004, IEEE T IMAGE PROCESS; Yi Z., 2017, ICCV; Yu A., 2014, CVPR	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405056
C	Li, Q; Chen, W; Sun, XM; Zhang, JL		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Qiang; Chen, Wei; Sun, Xiaoming; Zhang, Jialin			Influence Maximization with epsilon-Almost Submodular Threshold Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Influence maximization is the problem of selecting k nodes in a social network to maximize their influence spread. The problem has been extensively studied but most works focus on the submodular influence diffusion models. In this paper, motivated by empirical evidences, we explore influence maximization in the non-submodular regime. In particular, we study the general threshold model in which a fraction of nodes have non-submodular threshold functions, but their threshold functions are closely upper- and lower-bounded by some submodular functions (we call them epsilon-almost submodular). We first show a strong hardness result: there is no 1/n(gamma/c) approximation for influence maximization (unless P = NP) for all networks with up to n(gamma) epsilon-almost submodular nodes, where gamma is in (0,1) and c is a parameter depending on epsilon. This indicates that influence maximization is still hard to approximate even though threshold functions are close to submodular. We then provide (1 - epsilon)(l) (1 - 1/e) approximation algorithms when the number of epsilon-almost submodular nodes is f. Finally, we conduct experiments on a number of real-world datasets, and the results demonstrate that our approximation algorithms outperform other baseline algorithms.	[Li, Qiang; Sun, Xiaoming; Zhang, Jialin] Chinese Acad Sci, Inst Comp Technol, CAS Key Lab Network Data Sci & Technol, Beijing, Peoples R China; [Li, Qiang; Sun, Xiaoming; Zhang, Jialin] Univ Chinese Acad Sci, Beijing, Peoples R China; [Chen, Wei] Microsoft Res, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Microsoft	Li, Q (corresponding author), Chinese Acad Sci, Inst Comp Technol, CAS Key Lab Network Data Sci & Technol, Beijing, Peoples R China.; Li, Q (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.	liqiang01@ict.ac.cn; weic@microsoft.com; sunxiaoming@ict.ac.cn; zhangjialin@ict.ac.cn			National Natural Science Foundation of China [61433014, 61502449, 61602440]; 973 Program of China [2016YFB1000201]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); 973 Program of China(National Basic Research Program of China)	This work was supported in part by the National Natural Science Foundation of China Grant 61433014, 61502449, 61602440, the 973 Program of China Grants No. 2016YFB1000201.	Aslay C, 2015, PROC VLDB ENDOW, V8, P814, DOI 10.14778/2752939.2752950; Backstrom L., 2006, ACM SIGKDD, P44, DOI DOI 10.1145/1150402.1150412; Borgs C., 2014, SODA, P946; Cheng W, 2016, LECT NOTES COMPUT SC, V9977, P307, DOI 10.1007/978-3-319-50011-9_24; Du DZ, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P167; Ebrahimi Roozbeh, 2015, ITCS 15; Gao Jie, 2016, ACM C EC COMP; Ghasemiesfeh Golnaz, 2013, ACM C EL COMM; Goyal A., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P211, DOI 10.1109/ICDM.2011.132; Goyal A, 2013, SOC NETW ANAL MIN, V3, P179, DOI 10.1007/s13278-012-0062-z; Harathi S, 2007, LECT NOTES COMPUT SC, V4858, P306; Horel T., 2016, P NIPS BARC, P3045; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Lu W, 2015, PROC VLDB ENDOW, V9, P60; Mossel E, 2007, ACM S THEORY COMPUT, P128; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nguyen HT, 2016, SIGMOD'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P695, DOI 10.1145/2882903.2915207; Ning Chen, 2008, SODA 08; Subramani MR, 2003, COMMUN ACM, V46, P300, DOI 10.1145/953460.953514; Tang YZ, 2015, SIGMOD'15: PROCEEDINGS OF THE 2015 ACM SIGMOD INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1539, DOI 10.1145/2723372.2723734; Tang Youze, 2014, SIGMOD 14; Wang B, 2016, AAAI CONF ARTIF INTE, P791; Wei Chen, 2010, KDD 10; Wei Chen, 2015, ACM C EC COMP; Wei Chen, 2009, P 15 ACM SIGKDD; Yang Y, 2016, AAAI CONF ARTIF INTE, P65; Zhang P, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1306, DOI 10.1145/2623330.2623684	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403084
C	Li, YJ; Schwing, A; Wang, KC; Zemel, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Yujia; Schwing, Alexander; Wang, Kuan-Chieh; Zemel, Richard			Dualing GANs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative adversarial nets (GANs) are a promising technique for modeling a distribution from samples. It is however well known that GAN training suffers from instability due to the nature of its saddle point formulation. In this paper, we explore ways to tackle the instability problem by dualizing the discriminator. We start from linear discriminators in which case conjugate duality provides a mechanism to reformulate the saddle point objective into a maximization problem, such that both the generator and the discriminator of this 'dualing GAN' act in concert. We then demonstrate how to extend this intuition to non-linear formulations. For GANs with linear discriminators our approach is able to remove the instability in training, while for GANs with nonlinear discriminators our approach provides an alternative to the commonly used GAN training algorithm.	[Li, Yujia; Wang, Kuan-Chieh; Zemel, Richard] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada; [Wang, Kuan-Chieh; Zemel, Richard] Vector Inst, Toronto, ON, Canada; [Schwing, Alexander] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA; [Li, Yujia] DeepMind, London, England	University of Toronto; University of Illinois System; University of Illinois Urbana-Champaign	Li, YJ (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.; Li, YJ (corresponding author), DeepMind, London, England.	yujiali@cs.toronto.edu; aschwing@illinois.edu; wangkua1@cs.toronto.edu; zemel@cs.toronto.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [1718221]; NSERC; Samsung; CIFAR	National Science Foundation(National Science Foundation (NSF)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Samsung(Samsung); CIFAR(Canadian Institute for Advanced Research (CIFAR))	This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221, and grants from NSERC, Samsung and CIFAR.	Arjovsky M., 2017, ARXIV170107875; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Huang X., 2016, IEEE C COMP VIS PATT; Im D., 2016, GENERATING IMAGES RE; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Y., 2015, ABS150202761; London B., 2016, P NIPS WORKSH ADV TR; Nowozin S, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Sohl-Dickstein J, 2016, UNROLLED GENERATIVE; van den Oord Aaron, 2016, ARXIV160605328; Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zhang H., 2016, STACKGAN TEXT PHOTOR; Zhao JJ, 2018, IEEE T CIRC SYST VID, V28, P2679, DOI 10.1109/TCSVT.2017.2710120	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405067
C	Liu, LP; Ruiz, FJR; Athey, S; Blei, DM		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Li-Ping; Ruiz, Francisco J. R.; Athey, Susan; Blei, David M.			Context Selection for Embedding Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Word embeddings are an effective tool to analyze language. They have been recently extended to model other types of data beyond text, such as items in recommendation systems. Embedding models consider the probability of a target observation (a word or an item) conditioned on the elements in the context (other words or items). In this paper, we show that conditioning on all the elements in the context is not optimal. Instead, we model the probability of the target conditioned on a learned subset of the elements in the context. We use amortized variational inference to automatically choose this subset. Compared to standard embedding models, this method improves predictions and the quality of the embeddings.	[Liu, Li-Ping] Tufts Univ, Medford, MA 02155 USA; [Ruiz, Francisco J. R.; Blei, David M.] Columbia Univ, New York, NY 10027 USA; [Ruiz, Francisco J. R.] Univ Cambridge, Cambridge, England; [Athey, Susan] Stanford Univ, Stanford, CA 94305 USA	Tufts University; Columbia University; University of Cambridge; Stanford University	Liu, LP (corresponding author), Tufts Univ, Medford, MA 02155 USA.		Jeong, Yongwook/N-7413-2016	Athey, Susan/0000-0001-6934-562X	NSF [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA [PPAML FA8750-14-2-0009]; DARPA SIMPLEX [N66001-15-C-4032]; Alfred P. Sloan Foundation; John Simon Guggenheim Foundation; EU [706760]; NVIDIA Corporation	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DARPA SIMPLEX; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); John Simon Guggenheim Foundation; EU(European Commission); NVIDIA Corporation	This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-4032, the Alfred P. Sloan Foundation, and the John Simon Guggenheim Foundation. Francisco J. R. Ruiz is supported by the EU H2020 programme (Marie Sklodowska-Curie grant agreement 706760). We also acknowledge the support of NVIDIA Corporation with the donation of two GPUs used for this research.	[Anonymous], 2014, INT C MACH LEARN; [Anonymous], 2014, INT C MACH LEARN; Arora S, 2016, T ASS COMPUTATIONAL, V4; BAMLER R., 2017, INT C MACH LEARN; BARKAN O., 2016, IEEE INT WORKSH MACH; Bengio Y., 2006, INNOVATIONS MACHINE; Bishop C.M, 2006, PATTERN RECOGN; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Firth J. R., 1957, STUDIES LINGUISTIC A, V1952-1959; Gershman S., 2014, P ANN M COGN SCI SOC, V36; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma D.P., 2015, INT C LEARN REPR ICL; Korattikara A., 2015, ADV NEURAL INFORM PR; Levy Omer, 2014, NEURIPS; Liang D., 2016, ACM C REC SYST; MIKOLOV T., 2013, C N AM CHAPT ASS COM; Mikolov T., 2013, P INT C LEARN REPR I; Mikolov T., 2013, 27 ANN C NEUR INF PR, P3111; Mnih A., 2013, ADV NEURAL INFORM PR, V26; Munson M. A., 2015, EBIRD REFERENCE DATA; Paisley J. W., 2012, INT C MACH LEARN; Pennington J, 2014, P 2014 C EMP METH NA, DOI DOI 10.3115/V1/D14-1162; Ranganath R., 2014, ARTIFICIAL INTELLIGE; Ranganath R., 2015, ARTIFICIAL INTELLIGE; Rudolph M., 2016, ADV NEURAL INFORM PR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Sullivan BL, 2009, BIOL CONSERV, V142, P2282, DOI 10.1016/j.biocon.2009.05.006; VILNIS L., 2015, INT C LEARN REPR; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	33	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404086
C	Liu, LX; Li, DN; Wong, WH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Linxi; Li, Dangna; Wong, Wing Hung			Convergence rates of a partition based Bayesian multivariate density estimation method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISTRIBUTIONS	We study a class of non-parametric density estimators under Bayesian settings. The estimators are obtained by adaptively partitioning the sample space. Under a suitable prior, we analyze the concentration rate of the posterior distribution, and demonstrate that the rate does not directly depend on the dimension of the problem in several special cases. Another advantage of this class of Bayesian density estimators is that it can adapt to the unknown smoothness of the true density function, thus achieving the optimal convergence rate without artificial conditions on the density. We also validate the theoretical results on a variety of simulated data sets.	[Liu, Linxi] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Li, Dangna] Stanford Univ, ICME, Stanford, CA 94305 USA; [Wong, Wing Hung] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Liu, Linxi] Stanford Univ, Stanford, CA 94305 USA	Columbia University; Stanford University; Stanford University; Stanford University	Liu, LX (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.	ll3098@columbia.edu; dangna@stanford.edu; whwong@stanford.edu	Jeong, Yongwook/N-7413-2016	Liu, Linxi/0000-0001-7494-8417				Abramovich F, 2006, ANN STAT, V34, P584, DOI 10.1214/009053606000000074; Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; de Jonge R, 2012, ELECTRON J STAT, V6, P1984, DOI 10.1214/12-EJS735; DEVORE RA, 1992, IEEE T INFORM THEORY, V38, P719, DOI 10.1109/18.119733; FARRELL RH, 1967, ANN MATH STAT, V38, P471, DOI 10.1214/aoms/1177698962; FERGUSON TS, 1974, ANN STAT, V2, P615, DOI 10.1214/aos/1176342752; FOSTER DP, 1994, ANN STAT, V22, P1947, DOI 10.1214/aos/1176325766; Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228; Grenander U., 1981, PROBABILITY STAT SER; Kruijer W, 2010, ELECTRON J STAT, V4, P1225, DOI 10.1214/10-EJS584; Li Dangna, 2016, 30 C NEUR INF PROC S; Lu L, 2013, J AM STAT ASSOC, V108, P1402, DOI 10.1080/01621459.2013.813389; Ma L, 2011, J AM STAT ASSOC, V106, P1553, DOI 10.1198/jasa.2011.tm10003; Rivoirard V, 2012, BAYESIAN ANAL, V7, P311, DOI 10.1214/12-BA710; Rousseau J, 2010, ANN STAT, V38, P146, DOI 10.1214/09-AOS703; Shen WN, 2015, SCAND J STAT, V42, P1194, DOI 10.1111/sjos.12159; Shen WN, 2013, BIOMETRIKA, V100, P623, DOI 10.1093/biomet/ast015; SHEN XT, 1994, ANN STAT, V22, P580, DOI 10.1214/aos/1176325486; Shen XT, 2001, ANN STAT, V29, P687; Soriano J, 2017, J R STAT SOC B, V79, P547; Wong WH, 2010, ANN STAT, V38, P1433, DOI 10.1214/09-AOS755	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404078
C	Liu, MR; Yang, TB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Mingrui; Yang, Tianbao			Adaptive Accelerated Gradient Converging Method under Holderian Error Bound Condition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DESCENT METHODS; CONVEX	Recent studies have shown that proximal gradient (PG) method and accelerated gradient method (APG) with restarting can enjoy a linear convergence under a weaker condition than strong convexity, namely a quadratic growth condition (QGC). However, the faster convergence of restarting APG method relies on the potentially unknown constant in QGC to appropriately restart APG, which restricts its applicability. We address this issue by developing a novel adaptive gradient converging methods, i.e., leveraging the magnitude of proximal gradient as a criterion for restart and termination. Our analysis extends to a much more general condition beyond the QGC, namely the Holderian error bound (HEB) condition. The key technique for our development is a novel synthesis of adaptive regularization and a conditional restarting scheme, which extends previous work focusing on strongly convex problems to a much broader family of problems. Furthermore, we demonstrate that our results have important implication and applications in machine learning: (i) if the objective function is coercive and semi-algebraic, PG's convergence speed is essentially o(1/t), where t is the total number of iterations; (ii) if the objective function consists of an l(1), l(infinity), l(1,infinity) or huber norm regularization and a convex smooth piecewise quadratic loss (e.g., square loss, squared hinge loss and huber loss), the proposed algorithm is parameter-free and enjoys a faster linear convergence than PG without any other assumptions (e.g., restricted eigen-value condition). It is notable that our linear convergence results for the aforementioned problems are global instead of local. To the best of our knowledge, these improved results are first shown in this work.	[Liu, Mingrui; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA	University of Iowa	Liu, MR (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	mingrui-liu@uiowa.edu; tianbao-yang@uiowa.edu		Liu, Mingrui/0000-0002-5181-3429	National Science Foundation [IIS-1463988, IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their helpful comments. M. Liu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995).	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; BIERSTONE E, 1988, PUBL MATH-PARIS, P5; Bolte Trong Phong, 2015, ABS151008234 CORR; DRUSVYATSKIY D., 2016, ARXIV160206661; Fan R. E., 2011, LIBSVM DATA CLASSIFI; Fercoq O., 2016, ARXIV160907358; Gong Pinghua, 2014, ABS14061102 CORR; Hou K., 2013, ADV NEURAL INFORM PR, P710; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Li GY, 2013, MATH PROGRAM, V137, P37, DOI 10.1007/s10107-011-0481-z; LIN Q., 2014, ICML, V32, P73; LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Necoara I., 2015, ABS150406298 CORR; Nesterov Y., 2012, OPTIMA, V88, P10; Nesterov Y, 2004, INTRO LECT CONVEX OP, P15; Nesterov Y, 2007, GRADIENT METHODS MIN; NYQUIST H, 1983, COMMUN STAT-THEOR M, V12, P2511, DOI 10.1080/03610928308828618; Rockafellar R. T., 1976, SIAM J CONTROL OPTIM, V14; So Anthony Man-Cho, 2013, ABS13090113 CORR; Tseng P., 2008, SIAM J OPTIMIZATION; Wang PW, 2014, J MACH LEARN RES, V15, P1523; Xu Y, 2017, PR MACH LEARN RES, V70; Xu Yi, 2016, ADV NEURAL INFORM PR, P1208; Yang T., 2016, ABS151203107 CORR; Yang WH, 2009, SIAM J OPTIMIZ, V19, P1633, DOI 10.1137/070689838; Zadorozhnyi Oleksandr, 2016, JOINT EUR C MACH LEA, P714; Zhang H., 2016, OPTIMIZATION LETT, P1; Zhang Hui, 2016, ABS160600269 CORR; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157; Zhou Z., 2015, ABS151203518 CORR; Zhou ZR, 2015, PR MACH LEARN RES, V37, P1501	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403017
C	Liu, S; Takeda, A; Suzuki, T; Fukumizu, K		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Song; Takeda, Akiko; Suzuki, Taiji; Fukumizu, Kenji			Trimmed Density Ratio Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				EXPONENTIAL-FAMILIES; REGRESSION	Density ratio estimation is a vital tool in both machine learning and statistical community. However, due to the unbounded nature of density ratio, the estimation procedure can be vulnerable to corrupted data points, which often pushes the estimated ratio toward infinity. In this paper, we present a robust estimator which automatically identifies and trims outliers. The proposed estimator has a convex formulation, and the global optimum can be obtained via subgradient descent. We analyze the parameter estimation error of this estimator under high-dimensional settings. Experiments are conducted to verify the effectiveness of the estimator.	[Liu, Song] Univ Bristol, Bristol, Avon, England; [Takeda, Akiko] RIKEN, AIP, Inst Stat Math, Tokyo, Japan; [Suzuki, Taiji] Univ Tokyo, Sakigake PRESTO, JST, AIP,RIKEN, Tokyo, Japan; [Liu, Song; Fukumizu, Kenji] Inst Stat Math, Tokyo, Japan	University of Bristol; Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan; RIKEN; Japan Science & Technology Agency (JST); RIKEN; University of Tokyo; Research Organization of Information & Systems (ROIS); Institute of Statistical Mathematics (ISM) - Japan	Liu, S (corresponding author), Univ Bristol, Bristol, Avon, England.	song.liu@bristol.ac.uk; atakeda@ism.ac.jp; taiji@mist.i.u-tokyo.ac.jp; fukumizu@ism.ac.jp	Jeong, Yongwook/N-7413-2016	Fukumizu, Kenji/0000-0002-3488-2625	MEXT KAKENHI [25730013, 25120012, 26280009, 15H05707]; JST-PRESTO; JST-CREST; MEXT [25120012];  [15K00031]	MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST-PRESTO(Japan Science & Technology Agency (JST)); JST-CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); MEXT(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)); 	We thank three anonymous reviewers for their detailed and helpful comments. Akiko Takeda thanks Grant-in-Aid for Scientific Research (C), 15K00031. Taiji Suzuki was partially supported by MEXT KAKENHI (25730013, 25120012, 26280009 and 15H05707), JST-PRESTO and JST-CREST. Song Liu and Kenji Fukumizu have been supported in part by MEXT Grant-in-Aid for Scientific Research on Innovative Areas (25120012).	Azmandian Fatemeh, 2012, P 4 AS C MACH LEARN, P49; Boyd S., 2014, TECHNICAL REPORT; CLEVELAND WS, 1979, J AM STAT ASSOC, V74, P829, DOI 10.2307/2286407; Cristianini N., 2000, INTRO SUPPORT VECTOR; Efron B, 1996, ANN STAT, V24, P2431; Fazayeli F, 2016, PR MACH LEARN RES, V48; Fithian W, 2015, BIOMETRIKA, V102, P486, DOI 10.1093/biomet/asu065; Fokianos K, 2004, J R STAT SOC B, V66, P941, DOI 10.1111/j.1467-9868.2004.05480.x; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hadi AS, 1997, COMPUT STAT DATA AN, V25, P251, DOI 10.1016/S0167-9473(97)00011-X; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Kawahara Yoshinobu, 2012, Statistical Analysis and Data Mining, V5, P114, DOI 10.1002/sam.10124; Liu S, 2017, ANN STAT, V45, P959, DOI 10.1214/16-AOS1470; Loh PL, 2015, J MACH LEARN RES, V16, P559; Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7; Neykov N., 1990, COMPSTAT 90 SHORT CO, P99; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Pitman EJG, 1936, P CAMB PHILOS SOC, V32, P567; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201; Scholkopf B, 2000, ADV NEUR IN, V12, P582; Scholkopf B., 2001, LEARNING KERNELS SUP; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Smola A., 2009, 12 INT C ART INT STA, V5, P536; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Sugiyama M, 2008, ANN I STAT MATH, V60, P699, DOI 10.1007/s10463-008-0197-x; Suykens JAK, 2002, NEUROCOMPUTING, V48, P85, DOI 10.1016/S0925-2312(01)00644-0; Tsuboi Y., 2009, J INF PROCESS, V17, P138, DOI DOI 10.2197/IPSJJIP.17.138; Vandev DL, 1998, STATISTICS, V32, P111, DOI 10.1080/02331889808802657; Wornowizki M, 2016, COMPUTATION STAT, V31, P291, DOI 10.1007/s00180-015-0633-3; Yamada M, 2013, NEURAL COMPUT, V25, P1324, DOI 10.1162/NECO_a_00442; Yang E., 2015, ADV NEURAL INFORM PR, P2602; Yang E, 2016, ARXIV160508299	35	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404057
C	Mensch, A; Mairal, J; Bzdok, D; Thirion, B; Varoquaux, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mensch, Arthur; Mairal, Julien; Bzdok, Danilo; Thirion, Bertrand; Varoquaux, Gael			Learning Neural Representations of Human Cognition across Many fMRI Studies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PARCELLATION; FUTURE; BRAIN	Cognitive neuroscience is enjoying rapid increase in extensive public brain-imaging datasets. It opens the door to large-scale statistical models. Finding a unified perspective for all available data calls for scalable and automated solutions to an old challenge: how to aggregate heterogeneous information on brain function into a universal cognitive system that relates mental operations/cognitive processes/psychological tasks to brain networks? We cast this challenge in a machine-learning approach to predict conditions from statistical brain maps across different studies. For this, we leverage multi-task learning and multi-scale dimension reduction to learn low-dimensional representations of brain images that carry cognitive information and can be robustly associated with psychological stimuli. Our multi-dataset classification model achieves the best prediction performance on several large reference datasets, compared to models without cognitive-aware low-dimension representations; it brings a substantial performance boost to the analysis of small datasets, and can be introspected to identify universal template cognitive concepts.	[Mensch, Arthur; Mairal, Julien; Thirion, Bertrand; Varoquaux, Gael] INRIA, Rocquencourt, France; [Bzdok, Danilo] Rhein Westfal TH Aachen, Dept Psychiat, Aachen, Germany; [Mensch, Arthur; Thirion, Bertrand; Varoquaux, Gael] Univ Paris Saclay, CEA, INRIA, F-91191 Gif Sur Yvette, France; [Mairal, Julien] Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LJK, F-38000 Grenoble, France	Inria; RWTH Aachen University; CEA; Inria; UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); Inria	Mensch, A (corresponding author), INRIA, Rocquencourt, France.; Mensch, A (corresponding author), Univ Paris Saclay, CEA, INRIA, F-91191 Gif Sur Yvette, France.	arthur.mensch@m4x.org; julien.mairal@inria.fr; danilo.bzdok@rwth-aachen.de; bertrand.thirion@inria.fr; gael.varoquaux@inria.fr	Jeong, Yongwook/N-7413-2016; Mairal, Julien/AAL-5611-2021		European Union's Horizon 2020 Framework Programme for Research and Innovation [720270]; ERC grant SOLARIS [714381]; ANR [ANR-14-CE23-0003-01]	European Union's Horizon 2020 Framework Programme for Research and Innovation; ERC grant SOLARIS; ANR(French National Research Agency (ANR))	This project has received funding from the European Union's Horizon 2020 Framework Programme for Research and Innovation under grant agreement No 720270 (Human Brain Project SGA1). Julien Mairal was supported by the ERC grant SOLARIS (No 714381) and a grant from ANR (MACARON project ANR-14-CE23-0003-01). We thank Olivier Grisel for his most helpful insights.	Abraham A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00014; Ando RK, 2005, J MACH LEARN RES, V6, P1817; [Anonymous], 2013, ADV NEURAL INFORM PR; Barrett LF, 2009, PERSPECT PSYCHOL SCI, V4, P326, DOI 10.1111/j.1745-6924.2009.01134.x; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Blumensath T, 2013, NEUROIMAGE, V76, P313, DOI 10.1016/j.neuroimage.2013.03.024; Bugden S, 2012, DEV COGN NEUROS-NETH, V2, P448, DOI 10.1016/j.dcn.2012.04.001; Bzdok D, 2015, ADV NEUR IN, V28; Bzdok D, 2017, NEUROIMAGE, V155, P549, DOI 10.1016/j.neuroimage.2017.04.061; Collobert R., 2008, P 25 ICML, V25, P160, DOI DOI 10.1145/1390156.1390177; Donahue J, 2014, PR MACH LEARN RES, V32; Eickhoff SB, 2015, HUM BRAIN MAPP, V36, P4771, DOI 10.1002/hbm.22933; Gramfort A, 2013, INT WORKSHOP PATTERN, P17, DOI 10.1109/PRNI.2013.14; Kingma D.P, P 3 INT C LEARNING R; Koyejo Oluwasanmi, 2013, NIPS WORKSH MACH LEA, P5; Laird AR, 2005, NEUROINFORMATICS, V3, P65, DOI 10.1385/NI:3:1:065; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liang PH., 2013, ADV NEURAL INFORM PR, V26; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; Medaglia JD, 2015, J COGNITIVE NEUROSCI, V27, P1471, DOI 10.1162/jocn_a_00810; Mensch A, 2016, PR MACH LEARN RES, V48; Mensch Arthur, 2017, IEEE T SIGNAL PROCES, V99; Newell A., 1973, YOU CANT PLAY 20 QUE, DOI 10.1016/B978-0-12-170150-5.50012-3; Orfanos DP, 2017, NEUROIMAGE, V144, P309, DOI 10.1016/j.neuroimage.2015.09.052; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pinel P, 2007, BMC NEUROSCI, V8, DOI 10.1186/1471-2202-8-91; Poldrack RA, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.110; Poldrack RA, 2017, NAT REV NEUROSCI, V18, P115, DOI 10.1038/nrn.2016.167; Rubin Timothy, 2016, ADV NEURAL INF PROCE, P1118; Salimi-Khorshidi G, 2009, NEUROIMAGE, V45, P810, DOI 10.1016/j.neuroimage.2008.12.039; Shafto MA, 2014, BMC NEUROL, V14, DOI 10.1186/s12883-014-0204-1; Srebro N., 2005, P ADV NEURAL INFORM; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Thirion B, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00167; Turner JA, 2012, NEUROINFORMATICS, V10, P57, DOI 10.1007/s12021-011-9126-x; Van Essen DC, 2012, NEUROIMAGE, V62, P2222, DOI 10.1016/j.neuroimage.2012.02.018; Wager TD, 2013, NEW ENGL J MED, V368, P1388, DOI 10.1056/NEJMoa1204471; Xue Y, 2007, J MACH LEARN RES, V8, P35; Yarkoni T, 2011, NAT METHODS, V8, P665, DOI [10.1038/NMETH.1635, 10.1038/nmeth.1635]; 2009, PSYCHOL SCI, V20, P1364	40	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405093
C	Meshi, O; Schwing, AG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Meshi, Ofer; Schwing, Alexander G.			Asynchronous Parallel Coordinate Minimization for MAP Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE	Finding the maximum a-posteriori (MAP) assignment is a central task for structured prediction. Since modern applications give rise to very large structured problem instances, there is increasing need for efficient solvers. In this work we propose to improve the efficiency of coordinate-minimization-based dual-decomposition solvers by running their updates asynchronously in parallel. In this case message-passing inference is performed by multiple processing units simultaneously without coordination, all reading and writing to shared memory. We analyze the convergence properties of the resulting algorithms and identify settings where speedup gains can be expected. Our numerical evaluations show that this approach indeed achieves significant speedups in common computer vision tasks.	[Meshi, Ofer] Google, Mountain View, CA 94043 USA; [Schwing, Alexander G.] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA	Google Incorporated; University of Illinois System; University of Illinois Urbana-Champaign	Meshi, O (corresponding author), Google, Mountain View, CA 94043 USA.	meshi@google.com; aschwing@illinois.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [1718221]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221. This work utilized computing resources provided by the Innovative Systems Lab (ISL) at NCSA.	Asuncion A., 2011, DISTRIBUTED GIBBS SA; Avron H, 2015, J ACM, V62, DOI 10.1145/2814566; Barriuso A., 2016, ARXIV160805442; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Chen L.-C., 2015, P ICML; Choi J., 2012, FIELD PROGRAMMABLE L; DAVIS D, 2016, ADV NEURAL INFORM PR, P226; Desmaison A, 2016, LECT NOTES COMPUT SC, V9906, P818, DOI 10.1007/978-3-319-46475-6_50; Globerson A., 2008, NIPS; Gonzalez J., 2011, PARALLEL INFERENCE L; Hazan T, 2010, IEEE T INFORM THEORY, V56, P6294, DOI 10.1109/TIT.2010.2079014; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2370; Hurkat Skand, 2015, 2015 25th International Conference on Field Programmable Logic and Applications (FPL), P1, DOI 10.1109/FPL.2015.7293934; Johnson J. K., 2008, THESIS; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Koller D., 2009, PROBABILISTIC GRAPHI; Kolmogorov V, 2006, IEEE T PATTERN ANAL, V28, P1568, DOI 10.1109/TPAMI.2006.200; Komodakis N., 2007, MRF OPTIMIZATION VIA; Krahenbuhl P., 2011, ADV NEURAL INF PROCE, V24, P109; Kwok J., 2014, P 31 INT C MACH LEAR, P1701; Liu J, 2015, J MACH LEARN RES, V16, P285; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Meshi O., 2014, ADV STRUCTURED PREDI; Meshi O., 2015, NEURAL INFORM PROCES; Nam Ma, 2011, 2011 Proceedings of 23rd International Symposium on Computer Architecture and High Performance Computing (SBAC-PAD 2011), P56, DOI 10.1109/SBAC-PAD.2011.34; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950; Piatkowski N., 2011, INT WORKSH ECML PKDD; Recht B., 2011, ADV NEURAL INFORM PR, V24; Savchynskyy B., 2011, CVPR; Schwing A., 2011, P CVPR; Schwing A. G., 2014, P ICML; Schwing A. G., 2012, P NIPS; Shimony Y., 1994, ARITIFICAL INTELLIGE, V68, P399; Singh S., 2010, NEUR INF PROC SYST N; Sontag D, 2012, OPTIMIZATION FOR MACHINE LEARNING, P219; Tseng P, 1991, SIAM J OPTIMIZ, V1, P603, DOI 10.1137/0801036; Wainwright M, 2008, GRAPHICAL MODELS EXP; Wang YX, 2016, PR MACH LEARN RES, V48; Werner T., 2009, CTUCMP200906; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036; Werner T, 2010, IEEE T PATTERN ANAL, V32, P1474, DOI 10.1109/TPAMI.2009.134; Wick M, 2010, PROC VLDB ENDOW, V3, P794, DOI 10.14778/1920841.1920942; You Y., 2016, ADV NEURAL INFORM PR, P4682; Zhang J., 2014, P NIPS	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405079
C	Metelli, AM; Pirotta, M; Restelli, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Metelli, Alberto Maria; Pirotta, Matteo; Restelli, Marcello			Compatible Reward Inverse Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Inverse Reinforcement Learning (IRL) is an effective approach to recover a reward function that explains the behavior of an expert by observing a set of demonstrations. This paper is about a novel model-free IRL approach that, differently from most of the existing IRL algorithms, does not require to specify a function space where to search for the expert's reward function. Leveraging on the fact that the policy gradient needs to be zero for any optimal policy, the algorithm generates a set of basis functions that span the subspace of reward functions that make the policy gradient vanish. Within this subspace, using a second-order criterion, we search for the reward function that penalizes the most a deviation from the expert's policy. After introducing our approach for finite domains, we extend it to continuous ones. The proposed approach is empirically compared to other IRL methods both in the (finite) Taxi domain and in the (continuous) Linear Quadratic Gaussian (LQG) and Car on the Hill environments.	[Metelli, Alberto Maria; Restelli, Marcello] Politecn Milan, DEIB, Milan, Italy; [Pirotta, Matteo] Inria Lille, SequeL Team, Lille, France	Polytechnic University of Milan	Metelli, AM (corresponding author), Politecn Milan, DEIB, Milan, Italy.	albertomaria.metelli@polimi.it; matteo.pirotta@inria.fr; marcello.restelli@polimi.it	Jeong, Yongwook/N-7413-2016; Metelli, Alberto Maria/AAY-5206-2020	Metelli, Alberto Maria/0000-0002-3424-5212; Restelli, Marcello/0000-0002-6322-1076	French Ministry of Higher Education and Research; Nord-Pasde-Calais Regional Council; French National Research Agency (ANR) [ANR-14-CE24-0010-01]	French Ministry of Higher Education and Research; Nord-Pasde-Calais Regional Council(Region Hauts-de-France); French National Research Agency (ANR)(French National Research Agency (ANR))	This research was supported in part by French Ministry of Higher Education and Research, Nord-Pasde-Calais Regional Council and French National Research Agency (ANR) under project ExTra-Learn (n.ANR-14-CE24-0010-01).	Abbeel P., 2004, P 21 INT C MACHINE L, P1; [Anonymous], 2006, AAAI; [Anonymous], 2010, NIPS; [Anonymous], 2005, P 22 INT C MACH LEAR, DOI DOI 10.1145/1102351.1102421; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Audiffren J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3315; Bohmer W, 2013, J MACH LEARN RES, V14, P2067; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Dorato P., 2000, LINEAR QUADRATIC CON; Englert P, 2015, P INT S ROB RES; Ernst D, 2005, J MACH LEARN RES, V6, P503; Farahmand A. - m., 2012, ADV NEURAL INFORM PR, P1349; Finn C, 2016, PR MACH LEARN RES, V48; Furmston T., 2012, ADV NEURAL INFORM PR, P2717; Hester T., 2017, ABS170403732 CORR; Ho J, 2016, PR MACH LEARN RES, V48; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hwang C.-L., 2012, MULTIPLE OBJECTIVE D, V164; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kingma D.P, P 3 INT C LEARNING R; Klein Edouard, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P1, DOI 10.1007/978-3-642-40988-2_1; Mahadevan S, 2007, J MACH LEARN RES, V8, P2169; Manganini G., 2015, P IJCNN, P1; Mengi E, 2014, SIAM J MATRIX ANAL A, V35, P699, DOI 10.1137/130933472; Neu G, 2009, MACH LEARN, V77, P303, DOI 10.1007/s10994-009-5110-1; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Parisi S, 2016, J ARTIF INTELL RES, V57, P187, DOI 10.1613/jair.4961; Parr R., 2007, P 24 INT C MACH LEAR, P737; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Piot B, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1249; Pirotta M, 2016, AAAI CONF ARTIF INTE, P1993; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Ratliff ND, 2009, AUTON ROBOT, V27, P25, DOI 10.1007/s10514-009-9121-3; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Vidal, 2006, BOTNICA ORGANOGRAFIA; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Ziebart B. D., 2008, AAAI, V8, P1433	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402010
C	Mohri, M; Yang, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mohri, Mehryar; Yang, Scott			Online Learning with Transductive Regret	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study online learning with the general notion of transductive regret, that is regret with modification rules applying to expert sequences (as opposed to single experts) that are representable by weighted finite-state transducers. We show how transductive regret generalizes existing notions of regret, including: (1) external regret; (2) internal regret; (3) swap regret; and (4) conditional swap regret. We present a general and efficient online learning algorithm for minimizing transductive regret. We further extend that to design efficient algorithms for the time-selection and sleeping expert settings. A by-product of our study is an algorithm for swap regret, which, under mild assumptions, is more efficient than existing ones, and a substantially more efficient algorithm for time selection swap regret.	[Mohri, Mehryar] Courant Inst, New York, NY 10012 USA; [Mohri, Mehryar] Google Res, New York, NY 10011 USA; [Yang, Scott] DE Shaw & Co, New York, NY USA; [Yang, Scott] Courant Inst Math Sci, New York, NY USA	Google Incorporated	Mohri, M (corresponding author), Courant Inst, New York, NY 10012 USA.; Mohri, M (corresponding author), Google Res, New York, NY 10011 USA.	mohri@cims.nyu.edu; yangs@cims.nyu.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF-1535987, IIS-1618662]	NSF(National Science Foundation (NSF))	We thank Avrim Blum for informing us of an existing lower bound for swap regret proven by Auer [2017]. This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662.	Adamskiy Dmitry, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P290, DOI 10.1007/978-3-642-34106-9_24; [Anonymous], 2012, ADV NEURAL INFORM PR; Auer P., 2017, COMMUNICATION; Blum A, 1997, MACH LEARN, V26, P5, DOI 10.1023/A:1007335615132; Blum A, 2007, J MACH LEARN RES, V8, P1307; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; COHEN W, 1996, AAAI WORKSH INT BAS; Cohen WW, 1999, ACM T INFORM SYST, V17, P141, DOI 10.1145/306686.306688; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; Freund Y., 1997, P 20 9 ANN ACM S THE, P334, DOI [10.1145/258533.258616, DOI 10.1145/258533.258616]; Greenwald A.R., 2008, COLT, P239; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hazan E., 2009, P 26 ANN INT C MACHI, P393; Hazan Elad, 2008, NIPS, P625; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Khot S., 2008, 21 ANN C LEARN THEOR, V2008; Koolen WM, 2013, IEEE T INFORM THEORY, V59, P7168, DOI 10.1109/TIT.2013.2273353; Lehrer E, 2003, GAME ECON BEHAV, V42, P101, DOI 10.1016/S0899-8256(03)00032-0; Maillard O.-A., 2011, P 14 INT C ART INT S, P570; Mohri M., 2017, 170500132 ARXIV; Mohri M., 2018, AISTATS; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, V27, P1314; Monteleoni C., 2003, NIPS; Nesterov Y, 2015, APPL MATH COMPUT, V255, P58, DOI 10.1016/j.amc.2014.04.053; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Stoltz G, 2005, MACH LEARN, V59, P125, DOI 10.1007/s10994-005-0465-4; Vovk V, 1999, MACH LEARN, V35, P247, DOI 10.1023/A:1007595032382	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405029
C	Mokhtari, A; Ribeiro, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mokhtari, Aryan; Ribeiro, Alejandro			First-Order Adaptive Sample Size Methods to Reduce Complexity of Empirical Risk Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper studies empirical risk minimization (ERM) problems for large-scale datasets and incorporates the idea of adaptive sample size methods to improve the guaranteed convergence bounds for first-order stochastic and deterministic methods. In contrast to traditional methods that attempt to solve the ERM problem corresponding to the full dataset directly, adaptive sample size schemes start with a small number of samples and solve the corresponding ERM problem to its statistical accuracy. The sample size is then grown geometrically - e.g., scaling by a factor of two - and use the solution of the previous ERM as a warm start for the new ERM. Theoretical analyses show that the use of adaptive sample size methods reduces the overall computational cost of achieving the statistical accuracy of the whole dataset for a broad range of deterministic and stochastic first-order methods. The gains are specific to the choice of method. When particularized to, e.g., accelerated gradient descent and stochastic variance reduce gradient, the computational cost advantage is a logarithm of the number of training samples. Numerical experiments on various datasets confirm theoretical claims and showcase the gains of using the proposed adaptive sample size scheme.	[Mokhtari, Aryan; Ribeiro, Alejandro] Univ Penn, Philadelphia, PA 19104 USA	University of Pennsylvania	Mokhtari, A (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	aryanm@seas.upenn.edu; aribeiro@seas.upenn.edu	Jeong, Yongwook/N-7413-2016	Ribeiro, Alejandro/0000-0003-4230-9906	NSF [CCF 1717120]; ARO [W911NF1710438]	NSF(National Science Foundation (NSF)); ARO	This research was supported by NSF CCF 1717120 and ARO W911NF1710438.	Allen-Zhu Z., 2017, STOC; [Anonymous], 2002, THESIS; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bottou L., 2007, P NEURIPS, P161; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Daneshmand H., 2016, P 33 INT C MACH LEAR, P1463; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Defazio A, 2016, ADV NEUR IN, V29; Frostig R., 2015, P C LEARNING THEORY, P728; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Mokhtari A., 2016, ADV NEURAL INFORM PR, P4062; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2007, GRADIENT METHODS MIN; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Vapnik V., 2013, NATURE STAT LEARNING	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402011
C	Morrison, RE; Baptista, R; Marzouk, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Morrison, Rebecca E.; Baptista, Ricardo; Marzouk, Youssef			Beyond normality: Learning sparse probabilistic graphical models in the non-Gaussian setting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				INFERENCE; SELECTION	We present an algorithm to identify sparse dependence structure in continuous and non-Gaussian probability distributions, given a corresponding set of data. The conditional independence structure of an arbitrary distribution can be represented as an undirected graph (or Markov random field), but most algorithms for learning this structure are restricted to the discrete or Gaussian cases. Our new approach allows for more realistic and accurate descriptions of the distribution in question, and in turn better estimates of its sparse Markov structure. Sparsity in the graph is of interest as it can accelerate inference, improve sampling methods, and reveal important dependencies between variables. The algorithm relies on exploiting the connection between the sparsity of the graph and the sparsity of transport maps, which deterministically couple one probability measure to another.	[Morrison, Rebecca E.; Baptista, Ricardo; Marzouk, Youssef] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Morrison, RE (corresponding author), MIT, Cambridge, MA 02139 USA.	rmorriso@mit.edu; rsb@mit.edu; ymarz@mit.edu	Jeong, Yongwook/N-7413-2016	Morrison, Rebecca/0000-0002-5180-7088	AFOSR MURI [FA9550-15-1-0038]	AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI)	This work has been supported in part by the AFOSR MURI on "Managing multiple information sources of multi-physics systems," program officer Jean-Luc Cambier, award FA9550-15-1-0038. We would also like to thank Daniele Bigoni for generous help with code implementation and execution.	[Anonymous], [No title captured]; Bigoni D., COMPUTATION MO UNPUB; Bogachev VI, 2005, SB MATH+, V196, P309, DOI 10.1070/SM2005v196n03ABEH000882; Cai T, 2011, J AM STAT ASSOC, V106, P672, DOI 10.1198/jasa.2011.tm10560; El Moselhy TA, 2012, J COMPUT PHYS, V231, P7815, DOI 10.1016/j.jcp.2012.07.022; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Ghosh SK, 2016, NEW J PHYS, V18, DOI 10.1088/1367-2630/18/1/013027; Kim S, 1998, REV ECON STUD, V65, P361, DOI 10.1111/1467-937X.00050; Knothe H., 1957, MICHIGAN MATH J, V1957; Koller D., 2009, PROBABILISTIC GRAPHI; Liu CL, 2004, MAGN RESON MED, V51, P924, DOI 10.1002/mrm.20071; Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037; Liu H, 2009, J MACH LEARN RES, V10, P2295; Marzouk Y.M., 2016, HDB UNCERTAINTY QUAN; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; OEHLERT GW, 1992, AM STAT, V46, P27, DOI 10.2307/2684406; Parno M., 2014, ARXIV14125492; Parno M, 2016, SIAM-ASA J UNCERTAIN, V4, P1160, DOI 10.1137/15M1032478; PENG CK, 1993, PHYS REV LETT, V70, P1343, DOI 10.1103/PhysRevLett.70.1343; Perron M, 2013, J CLIMATE, V26, P1063, DOI 10.1175/JCLI-D-11-00504.1; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; ROSENBLATT M, 1952, ANN MATH STAT, V23, P470, DOI 10.1214/aoms/1177729394; RUE H., 2005, GAUSSIAN MARKOV RAND; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Sengupta A, 2016, AUST NZ J STAT, V58, P15, DOI 10.1111/anzs.12148; Spantini Alessio, 2017, ARXIV170306131; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402040
C	Murugesan, K; Carbonell, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Murugesan, Keerthiram; Carbonell, Jaime			Active Learning from Peers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CLASSIFICATION	This paper addresses the challenge of learning from peers in an online multitask setting. Instead of always requesting a label from a human oracle, the proposed method first determines if the learner for each task can acquire that label with sufficient confidence from its peers either as a task-similarity weighted sum, or from the single most similar task. If so, it saves the oracle query for later use in more difficult cases, and if not it queries the human oracle. The paper develops the new algorithm to exhibit this behavior and proves a theoretical mistake bound for the method compared to the best linear predictor in hindsight. Experiments over three multitask learning benchmark datasets show clearly superior performance over baselines such as assuming task independence, learning only from the oracle and not learning from peer tasks.	[Murugesan, Keerthiram; Carbonell, Jaime] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Murugesan, K (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	kmuruges@cs.cmu.edu; jgc@cs.cmu.edu	Jeong, Yongwook/N-7413-2016					Abernethy J, 2007, LECT NOTES COMPUT SC, V4539, P484, DOI 10.1007/978-3-540-72927-3_35; Agarwal A., 2013, INT C MACHINE LEARNI, P1220; Agarwal A., 2008, UCBEECS2008138; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Cavallanti G., 2008, ADV NEURAL INF PROCE, V21, P249; Cavallanti G, 2010, J MACH LEARN RES, V11, P2901; Cesa-Bianchi N., 2011, INT C MACHINE LEARNI, P433; Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P1205; Cohen H., 2014, ADV NEURAL INFORM PR, P1170; Cortes C, 2016, LECT NOTES ARTIF INT, V9925, P67, DOI 10.1007/978-3-319-46379-7_5; Crammer Koby, 2012, PROC 25 INT C NEURAL, P1475; Dekel O, 2007, J MACH LEARN RES, V8, P2233; Dekel O, 2012, J MACH LEARN RES, V13, P2655; Donmez P, 2008, P 17 ACM C INF KNOWL, P619, DOI DOI 10.1145/1458082.1458165; Donmez P, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P259; Garnett R., 2016, ADV NEURAL INFORM PR, P4296; Lugosi G., 2009, ARXIV09023526; Saha A., 2011, PROC INT C ARTIF INT, P643; Urner Ruth, 2012, AISTATS, P4; Weinberger K., 2009, ANN INT C MACH LEARN, P1113, DOI DOI 10.1145/1553374.1553516; Xue Y, 2007, J MACH LEARN RES, V8, P35; Yan Y., 2011, ICML, V11, P1161; Zhang C, 2015, ADV NEURAL INFORM PR, P703; Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407010
C	Neklyudov, K; Molchanov, D; Ashukha, A; Vetrov, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Neklyudov, Kirill; Molchanov, Dmitry; Ashukha, Arsenii; Vetrov, Dmitry			Structured Bayesian Pruning via Log-Normal Multiplicative Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Dropout-based regularization methods can be regarded as injecting random noise with pre-defined magnitude to different parts of the neural network during training. It was recently shown that Bayesian dropout procedure not only improves generalization but also leads to extremely sparse neural architectures by automatically setting the individual noise magnitude per weight. However, this sparsity can hardly be used for acceleration since it is unstructured. In the paper, we propose a new Bayesian model that takes into account the computational structure of neural networks and provides structured sparsity, e.g. removes neurons and/or convolutional channels in CNNs. To do this we inject noise to the neurons outputs while keeping the weights unregularized. We establish the probabilistic model with a proper truncated log-uniform prior over the noise and truncated log-normal variational approximation that ensures that the KL-term in the evidence lower bound is computed in closed-form. The model leads to structured sparsity by removing elements with a low SNR from the computation graph and provides significant acceleration on a number of deep neural architectures. The model is easy to implement as it can be formulated as a separate dropout-like layer.	[Neklyudov, Kirill; Molchanov, Dmitry; Ashukha, Arsenii; Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia; [Neklyudov, Kirill; Ashukha, Arsenii; Vetrov, Dmitry] Yandex, Moscow, Russia; [Molchanov, Dmitry] Skolkovo Inst Sci & Technol, Moscow, Russia	HSE University (National Research University Higher School of Economics); Skolkovo Institute of Science & Technology	Neklyudov, K (corresponding author), Natl Res Univ, Higher Sch Econ, Moscow, Russia.; Neklyudov, K (corresponding author), Yandex, Moscow, Russia.	k.necludov@gmail.com; dmolchanov@hse.ru; aashukha@hse.ru; dvetrov@hse.ru	Jeong, Yongwook/N-7413-2016; Molchanov, Dmitry/Y-6008-2018; Ashukha, Arsenii/AAE-8329-2019	Ashukha, Arsenii/0000-0001-9428-374X	HSE International lab of Deep Learning and Bayesian Methods - Russian Academic Excellence Project '5-100'; Ministry of Education and Science of the Russian Federation [14.756.31.0001]; Russian Science Foundation [17-11-01027]	HSE International lab of Deep Learning and Bayesian Methods - Russian Academic Excellence Project '5-100'; Ministry of Education and Science of the Russian Federation(Ministry of Education and Science, Russian Federation); Russian Science Foundation(Russian Science Foundation (RSF))	We would like to thank Christos Louizos and Max Welling for valuable discussions. Kirill Neklyudov and Arsenii Ashukha were supported by HSE International lab of Deep Learning and Bayesian Methods which is funded by the Russian Academic Excellence Project '5-100'. Dmitry Molchanov was supported by the Ministry of Education and Science of the Russian Federation (grant 14.756.31.0001). Dmitry Vetrov was supported by the Russian Science Foundation grant 17-11-01027.	Abadi M., TENSORFLOW LARGE SCA; Figurnov Michael, 2016, ARXIV161202297; Figurnov Mikhail, 2016, NEURIPS; Garipov Timur, 2016, ARXIV161103214; HAN S., 2015, ARXIV151000149; Jaderberg M., 2014, ARXIV14053866; James B, P PYTH SCI COMP C SC; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P9; Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lobacheva Ekaterina, 2017, ARXIV170800077; Louizos C., 2015, THESIS U AMSTERDAM A; Mnih V., 2013, ARXIV PREPRINT ARXIV; Molchanov D., 2017, ARXIV170105369; Molchanov Dmitry, 2016, BAYES DEEP LEARN WOR; Novikov A., 2015, ADV NEURAL INFORM PR, V28, P442, DOI DOI 10.5555/2969239.2969289; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Ullrich K., 2017, 5 INT C LEARN REPPR; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wen W., 2016, ADV NEURAL INFORM PR, P2074; Zhang Chiyuan, 2016, ARXIV161103530	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406081
C	Newling, J; Fleuret, F		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Newling, James; Fleuret, Francois			K-Medoids for K-Means Seeding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We show experimentally that the algorithm clarans of Ng and Han (1994) finds better K-medoids solutions than the Voronoi iteration algorithm of Hastie et al. (2001). This finding, along with the similarity between the Voronoi iteration algorithm and Lloyd's K-means algorithm, motivates us to use clarans as a K-means initializer. We show that clarans outperforms other algorithms on 23/23 datasets with a mean decrease over k-means-++ (Arthur and Vassilvitskii, 2007) of 30% for initialization mean squared error (MSE) and 3% for final MSE. We introduce algorithmic improvements to clarans which improve its complexity and runtime, making it a viable initialization scheme for large datasets.	[Newling, James; Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland; [Newling, James; Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Newling, J (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Newling, J (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	james.newling@idiap.ch; francois.fleuret@idiap.ch	Jeong, Yongwook/N-7413-2016		Hasler Foundation [13018 MASH2]	Hasler Foundation	James Newling was funded by the Hasler Foundation under the grant 13018 MASH2.	Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Bachem Olivier, 2016, NEURAL INFORM PROCES; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P91; Celebi ME, 2013, EXPERT SYST APPL, V40, P200, DOI 10.1016/j.eswa.2012.07.021; Elkan C., 2003, P 20 INT C MACHINE L, V20, P147, DOI DOI 10.1016/0026-2714(92)90278-S; Hartigan J.A., 1975, CLUSTERING ALGORITHM; Hastie TJ, 2001, ELEMENTS STAT LEARNI; Kanungo T., 2002, P 18 ANN S COMP GEOM, P10, DOI [DOI 10.1145/513400.513402, 10.1016/j.comgeo.2004.03.003]; Kaufman L., 1990, FINDING GROUPS DATA; Li YJ, 2007, IEEE T PATTERN ANAL, V29, P1091, DOI 10.1109/TPAMI.2007.1070; Newling J, 2016, PR MACH LEARN RES, V48; Ng R. T., 1994, P VLDB, P144; Ng RT, 2002, IEEE T KNOWL DATA EN, V14, P1003, DOI 10.1109/TKDE.2002.1033770; Park HS, 2009, EXPERT SYST APPL, V36, P3336, DOI 10.1016/j.eswa.2008.01.039; Telgarsky M., 2010, P 13 INT C ARTIFICIA, P820; Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405027
C	Nock, R; Cranko, Z; Menon, AK; Qu, LZ; Williamson, RC		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nock, Richard; Cranko, Zac; Menon, Aditya Krishna; Qu, Lizhen; Williamson, Robert C.			f-GANs in an Information Geometric Nutshell	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DIVERGENCE; RISK	Nowozin et al showed last year how to extend the GAN principle to all f - divergences. The approach is elegant but falls short of a full description of the supervised game, and says little about the key player, the generator: for example, what does the generator actually converge to if solving the GAN game means convergence in some space of parameters? How does that provide hints on the generator's design and compare to the flourishing but almost exclusively experimental literature on the subject? In this paper, we unveil a broad class of distributions for which such convergence happens - namely, deformed exponential families, a wide superset of exponential families -. We show that current deep architectures are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep architectures and their concinnity in the f-GAN game. This result holds given a sufficient condition on activation functions - which turns out to be satisfied by popular choices. The key to our results is a variational generalization of an old theorem that relates the KL divergence between regular exponential families and divergences between their natural parameters. We complete this picture with additional results and experimental insights on how these results may be used to ground further improvements of GAN architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses' link function in the discriminator.	[Nock, Richard; Cranko, Zac; Menon, Aditya Krishna; Qu, Lizhen; Williamson, Robert C.] Data61, Eveleigh, NSW, Australia; [Nock, Richard; Cranko, Zac; Menon, Aditya Krishna; Qu, Lizhen; Williamson, Robert C.] Australian Natl Univ, Canberra, ACT, Australia; [Nock, Richard] Univ Sydney, Sydney, NSW, Australia	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; University of Sydney	Nock, R (corresponding author), Data61, Eveleigh, NSW, Australia.	richard.nock@data61.csiro.au; zac.cranko@data61.csiro.au; aditya.menon@data61.csiro.au; lizhen.qu@data61.csiro.au; bob.williamson@data61.csiro.au	Jeong, Yongwook/N-7413-2016					ALI SM, 1966, J ROY STAT SOC B, V28, P131; Amari S.-i., 1985, DIFFERENTIAL GEOMETR, V28; Amari S.-I., 2016, INFORM GEOMETRY ITS; Amari S, 2012, PHYSICA A, V391, P4308, DOI 10.1016/j.physa.2012.04.016; Arjovsky M., 2017, ARXIV170107875; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; BENTAL A, 1991, J MATH ANAL APPL, V157, P211, DOI 10.1016/0022-247X(91)90145-P; Boyd S, 2004, CONVEX OPTIMIZATION; Clevert D.-A., 2016, 4 ICLR; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Frongillo R, 2014, AIP CONF PROC, V1636, P11, DOI 10.1063/1.4903703; Genevay A., 2017, ABS170600292 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, ABS170400028 CORR; Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072; Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee H., 2017, ABS170207028 CORR; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu S., 2017, ABS170508991 CORR; Maas A.-L., 2013, 30 ICML; Nair V., 2010, ICML, P807; Naudts J, 2011, GENERALISED THERMOSTATISTICS, P1, DOI 10.1007/978-0-85729-355-8; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nock R., 2008, NIPS 21, P1201; Nock R., 2017, ABS170704385 CORR; Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225; Radford A., 2016, 4 ICLR; Reid M.-D., 2010, JMLR, V11; Reid MD, 2011, J MACH LEARN RES, V12, P731; Salimans T, 2016, ADV NEUR IN, V29; Telgarsky M., 2012, 29 ICML; Vigelis R., 2011, J THEORET PROBAB, V21, P1; Wolf L., 2017, ABS170405693 CORR; Yu F., 2015, ARXIVABS150603365 CO	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400044
C	Oates, CJ; Niederer, S; Lee, A; Briol, FX; Girolami, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Oates, Chris J.; Niederer, Steven; Lee, Angela; Briol, Francois-Xavier; Girolami, Mark			Probabilistic Models for Integration Error in the Assessment of Functional Cardiac Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper studies the numerical computation of integrals, representing estimates or predictions, over the output f (x) of a computational model with respect to a distribution p(dx) over uncertain inputs x to the model. For the functional cardiac models that motivate this work, neither f nor p possess a closed-form expression and evaluation of either requires approximate to 100 CPU hours, precluding standard numerical integration methods. Our proposal is to treat integration as an estimation problem, with a joint model for both the a priori unknown function f and the a priori unknown distribution p. The result is a posterior distribution over the integral that explicitly accounts for dual sources of numerical approximation error due to a severely limited computational budget. This construction is applied to account, in a statistically principled manner, for the impact of numerical errors that (at present) are confounding factors in functional cardiac model assessment.	[Oates, Chris J.] Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England; [Niederer, Steven; Lee, Angela] Kings Coll London, London, England; [Briol, Francois-Xavier] Univ Warwick, Coventry, W Midlands, England; [Girolami, Mark] Imperial Coll London, London, England; [Oates, Chris J.; Girolami, Mark] Alan Turing Inst, London, England	Newcastle University - UK; University of London; King's College London; University of Warwick; Imperial College London	Oates, CJ (corresponding author), Newcastle Univ, Newcastle Upon Tyne, Tyne & Wear, England.; Oates, CJ (corresponding author), Alan Turing Inst, London, England.		Jeong, Yongwook/N-7413-2016	Girolami, Mark/0000-0003-3008-253X; Briol, Francois-Xavier/0000-0002-0181-2559	Lloyds Register Foundation Programme on Data-Centric Engineering; EPSRC Intermediate Career Fellowship; EPSRC [EP/K034154/1, EP/R018413/1, EP/P020720/1, EP/L014165/1]; EPSRC Established Career Fellowship [EP/J016934/1]; National Science Foundation (NSF) [DMS-1127914]	Lloyds Register Foundation Programme on Data-Centric Engineering; EPSRC Intermediate Career Fellowship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Established Career Fellowship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); National Science Foundation (NSF)(National Science Foundation (NSF))	CJO and MG were supported by the Lloyds Register Foundation Programme on Data-Centric Engineering. SN was supported by an EPSRC Intermediate Career Fellowship. FXB was supported by the EPSRC grant [EP/L016710/1]. MG was supported by the EPSRC grants [EP/K034154/1, EP/R018413/1, EP/P020720/1, EP/L014165/1], and an EPSRC Established Career Fellowship, [EP/J016934/1]. This material was based upon work partially supported by the National Science Foundation (NSF) under Grant DMS-1127914 to the Statistical and Applied Mathematical Sciences Institute. Opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the NSF.	Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Briol F-X, 2015, ARXIV 1512 00933; Briol FX, 2015, ADV NEUR IN, V28; Briol FX, 2017, PR MACH LEARN RES, V70; Cockayne J., 2017, ARXIV170203673; Cohen SN, 2016, ARXIV160906545; Craig PS, 2001, J AM STAT ASSOC, V96, P717, DOI 10.1198/016214501753168370; Delyon B, 2016, BERNOULLI, V22, P2177, DOI 10.3150/15-BEJ725; DIACONIS P, 1986, ANN STAT, V14, P1, DOI 10.1214/aos/1176349830; Diaconis P., 1988, STAT DECISION THEORY, V1, P163, DOI DOI 10.1007/978-1-4613-8768-8_20; Dick J, 2013, ACTA NUMER, V22, P133, DOI 10.1017/S0962492913000044; Ferguson T. S, 1983, RECENT ADV STAT, P287; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Ghosal S, 2001, ANN STAT, V29, P1233; Gunter T., 2014, ADV NEURAL INFORM PR, P2789; Hennig P, 2015, P ROY SOC A-MATH PHY, V471, DOI 10.1098/rspa.2015.0142; Husz Ferenc, 2012, P 28 C UNCERTAINTY A, P377; Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758; Ishwaran H, 2002, CAN J STAT, V30, P269, DOI 10.2307/3315951; KADANE J. B., 1985, BAYESIAN STAT, V2, P361; Kanagawa M, 2016, ADV NEUR IN, V29; Karvonen T., 2017, ARXIV170306359; Kennedy MC, 2001, J R STAT SOC B, V63, P425, DOI 10.1111/1467-9868.00294; Lee AWC, 2017, J CARDIOVASC ELECTR, V28, P208, DOI 10.1111/jce.13134; Mirams GR, 2016, J PHYSIOL-LONDON, V594, P6833, DOI 10.1113/JP271671; Novak E, 2010, EMS TRACTS MATH, V12, P1; OHAGAN A, 1987, J ROY STAT SOC D-STA, V36, P247; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Osborne M. A., 2012, ADV NEURAL INFORM PR; Osborne Michael A, 2012, ARTIF INTELL, P832; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robert C., 2013, MONTE CARLO STAT MET; Sarkka S., 2016, J ADV INFORM FUSION, V11, P31; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Von Mises R, 1974, MATH THEORY PROBABIL; Wand MP, 1994, KERNEL SMOOTHING	37	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400011
C	Ohama, I; Sato, I; Kida, T; Arimura, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ohama, Iku; Sato, Issei; Kida, Takuya; Arimura, Hiroki			On the Model Shrinkage Effect of Gamma Process Edge Partition Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The edge partition model (EPM) is a fundamental Bayesian nonparametric model for extracting an overlapping structure from binary matrix. The EPM adopts a gamma process (Gamma P) prior to automatically shrink the number of active atoms. However, we empirically found that the model shrinkage of the EPM does not typically work appropriately and leads to an overfitted solution. An analysis of the expectation of the EPM's intensity function suggested that the gamma priors for the EPM hyperparameters disturb the model shrinkage effect of the internal Gamma P. In order to ensure that the model shrinkage effect of the EPM works in an appropriate manner, we proposed two novel generative constructions of the EPM: CEPM incorporating constrained gamma priors, and DEPM. incorporating Dirichlet priors instead of the gamma priors. Furthermore, all DEPM's model parameters including the infinite atoms of the Gamma P prior could be marginalized out, and thus it was possible to derive a truly infinite DEPM. (IDEPM) that can be efficiently inferred using a collapsed Gibbs sampler. We experimentally confirmed that the model shrinkage of the proposed models works well and that the IDEPM indicated state-of-the-art performance in generalization ability, link prediction accuracy, mixing efficiency, and convergence speed.	[Ohama, Iku] Panasonic Corp, Kadoma, Osaka, Japan; [Sato, Issei] Univ Tokyo, Tokyo, Japan; [Ohama, Iku; Kida, Takuya; Arimura, Hiroki] Hokkaido Univ, Sapporo, Hokkaido, Japan	Panasonic; University of Tokyo; Hokkaido University	Ohama, I (corresponding author), Panasonic Corp, Kadoma, Osaka, Japan.; Ohama, I (corresponding author), Hokkaido Univ, Sapporo, Hokkaido, Japan.	ohama.iku@jp.panasonic.com; sato@k.u-tokyo.ac.jp; kida@ist.hokudai.ac.jp; arim@ist.hokudai.ac.jp	Jeong, Yongwook/N-7413-2016					Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; BLACKWELL D, 1973, ANN STAT, V1, P353, DOI 10.1214/aos/1176342372; Davis J., 2006, 23 INT C MACH LEARN, P233, DOI [10.1145/1143844.1143874, DOI 10.1145/1143844.1143874]; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Geyer C.J., 2007, LOWER TRUNCATED POIS; Griffiths T.L., 2005, ADV NEURAL INFORM PR; Griffiths TL, 2011, J MACH LEARN RES, V12, P1185; Hu CW, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P375; Kemp Charles, 2006, AAAI, DOI DOI 10.1145/1837026.1837061; LIU JS, 1994, J AM STAT ASSOC, V89, P958, DOI 10.2307/2290921; Morup M., 2011, P IEEE INT WORKSH MA, P1; Newman D, 2009, J MACH LEARN RES, V10, P1801; Palla K., 2012, P 29 INT C MACH LEAR, P1607; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Zhou M., 2014, NIPS, P3455; Zhou MY, 2015, ADV NEUR IN, V28; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400038
C	Pal, DK; Kannan, AA; Arakalgud, G; Savvides, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pal, Dipan K.; Kannan, Ashwin A.; Arakalgud, Gautam; Savvides, Marios			Max-Margin Invariant Features from Transformed Unlabeled Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The study of representations invariant to common transformations of the data is important to learning. Most techniques have focused on local approximate invariance implemented within expensive optimization frameworks lacking explicit theoretical guarantees. In this paper, we study kernels that are invariant to a unitary group while having theoretical guarantees in addressing the important practical issue of unavailability of transformed versions of labelled data. A problem we call the Unlabeled Transformation Problem which is a special form of semi-supervised learning and one-shot learning. We present a theoretically motivated alternate approach to the invariant kernel SVM based on which we propose Max-Margin Invariant Features (MMIF) to solve this problem. As an illustration, we design an framework for face recognition and demonstrate the efficacy of our approach on a large scale semi-synthetic dataset with 153,000 images and a new challenging protocol on Labelled Faces in the Wild (LFW) while out-performing strong baselines.	[Pal, Dipan K.; Kannan, Ashwin A.; Arakalgud, Gautam; Savvides, Marios] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Pal, DK (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.	dipanp@cmu.edu; aalapakk@cmu.edu; garakalgud@cmu.edu; marioss@cmu.edu	Jeong, Yongwook/N-7413-2016; Arputharaj, Kannan/AAN-4912-2020					Anselmi F., 2013, CORR; Anselmi F., 2013, MAGIC MAT THEORY DEE; Decoste D, 2002, MACH LEARN, V46, P161, DOI 10.1023/A:1012454411458; Haasdonk B, 2002, INT C PATT RECOG, P864, DOI 10.1109/ICPR.2002.1048439; Haasdonk B, 2007, MACH LEARN, V68, P35, DOI 10.1007/s10994-007-5009-7; HINTON GE, 1987, LECT NOTES COMPUT SC, V258, P1; Leibo Joel Z., 2014, INT JOINT C COMP VIS; Liao Q., 2013, ADV NEURAL INFORM PR; Pal DK, 2016, PROC CVPR IEEE, P5590, DOI 10.1109/CVPR.2016.603; Park SW, 2010, PROC CVPR IEEE, P2645, DOI 10.1109/CVPR.2010.5539980; Parkhi O. M., 2015, DEEP FACE RECOGNITIO; Poggio T., 1992, RECOGNITION STRUCTUR; Raj A, 2017, PR MACH LEARN RES, V54, P1225; Reisert Marco, 2008, THESIS; Sanderson C, 2009, LECT NOTES COMPUT SC, V5558, P199, DOI 10.1007/978-3-642-01793-3_21; SCHOLKOPF B, 1998, ADV NEURAL INFORM PR; Scholkopf B., 1996, INT C ART NEUR NETW, P47; Scholkopf B., 2001, LEARNING KERNELS SUP; WALDER C, 2007, ADV NEURAL INFORM PR, P1561; Zhang X., 2013, ADV NEURAL INFORM PR, P2031	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401046
C	Palaiopanos, G; Panageas, I; Piliouras, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Palaiopanos, Gerasimos; Panageas, Ioannis; Piliouras, Georgios			Multiplicative Weights Update with Constant Step-Size in Congestion Games: Convergence, Limit Cycles and Chaos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The Multiplicative Weights Update (MWU) method is a ubiquitous meta-algorithm that works as follows: A distribution is maintained on a certain set, and at each step the probability assigned to action gamma is multiplied by (1 - epsilon C(gamma)) > 0 where C(gamma) is the "cost" of action gamma and then rescaled to ensure that the new values form a distribution. We analyze MWU in congestion games where agents use arbitrary admissible constants as learning rates epsilon and prove convergence to exact Nash equilibria. Interestingly, this convergence result does not carry over to the nearly homologous MWU variant where at each step the probability assigned to action gamma is multiplied by (1 - epsilon)(C(gamma)) even for the simplest case of two-agent, two-strategy load balancing games, where such dynamics can provably lead to limit cycles or even chaotic behavior.	[Palaiopanos, Gerasimos; Piliouras, Georgios] SUTD, Singapore, Singapore; [Panageas, Ioannis] MIT, Cambridge, MA 02139 USA; [Panageas, Ioannis] Georgia Inst Technol, Atlanta, GA 30332 USA; [Panageas, Ioannis; Piliouras, Georgios] Simons Inst Theory Comp, Berkeley, CA USA	Singapore University of Technology & Design; Massachusetts Institute of Technology (MIT); University System of Georgia; Georgia Institute of Technology	Palaiopanos, G (corresponding author), SUTD, Singapore, Singapore.	gerasimosath@yahoo.com; ioannis@csail.mit.edu; georgios@sutd.edu.sg	Jeong, Yongwook/N-7413-2016		SUTD Presidential fellowship; MIT-SUTD postdoctoral fellowship; SUTD grant [SRG ESD 2015 097]; MOE AcRF Tier 2 Grant [2016-T2-1-170]; NRF Fellowship	SUTD Presidential fellowship(Singapore University of Technology & Design); MIT-SUTD postdoctoral fellowship; SUTD grant(Singapore University of Technology & Design); MOE AcRF Tier 2 Grant; NRF Fellowship	Gerasimos Palaiopanos would like to acknowledge a SUTD Presidential fellowship.; Ioannis Panageas would like to acknowledge a MIT-SUTD postdoctoral fellowship. Part of this work was completed while Ioannis Panageas was a PhD student at Georgia Institute of Technology and a visiting scientist at the Simons Institute for the Theory of Computing.; Georgios Piliouras would like to acknowledge SUTD grant SRG ESD 2015 097, MOE AcRF Tier 2 Grant 2016-T2-1-170 and a NRF Fellowship. Part of this work was completed while Georgios Piliouras was a visiting scientist at the Simons Institute for the Theory of Computing.	Ackermann H, 2009, PODC'09: PROCEEDINGS OF THE 2009 ACM SYMPOSIUM ON PRINCIPLES OF DISTRIBUTED COMPUTING, P63, DOI 10.1145/1582716.1582732; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Avramopoulos I., 2016, CORR; Balcan Maria-Florina, 2012, ICML WORKSH MARK MEC; BAUM LE, 1967, B AM MATH SOC, V73, P360, DOI 10.1090/S0002-9904-1967-11751-8; Berenbrink P, 2007, SIAM J COMPUT, V37, P1163, DOI 10.1137/060660345; Berenbrink P, 2014, ACM T ALGORITHMS, V11, DOI 10.1145/2629671; Bilmes Jeff A, 1998, TECHNICAL REPORT, DOI DOI 10.1080/0042098032000136147; Blum A, 2008, ACM S THEORY COMPUT, P373; Caragiannis I., 2011, FOCS; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen PA, 2016, ARTIF INTELL, V241, P217, DOI 10.1016/j.artint.2016.09.002; Chien S, 2011, GAME ECON BEHAV, V71, P315, DOI 10.1016/j.geb.2009.05.004; Cohen J., 2017, P 31 INT C NEUR INF; Daskalakis C., 2017, ARXIV E PRINTS; Daskalakis C., 2006, COMPLEXITY COMPUTING, P71; Daskalakis C, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P790; Daskalakis C, 2010, LECT NOTES COMPUT SC, V6386, P114, DOI 10.1007/978-3-642-16170-4_11; Engelberg R., 2013, P 14 ACM C EL COMM, P379; Fabrikant A., 2004, PROC 36 ANN ACM S TH, P604; Fearnley J., 2017, ARXIV E PRINTS; Foster D. J., 2016, ADV NEURAL INFORM PR, P4727; Fotakis D, 2008, LECT NOTES COMPUT SC, V4997, P121, DOI 10.1007/978-3-540-79309-0_12; Fudenberg Drew, 1998, THEORY LEARNING GAME; Jaggard A. D, 2011, ICS; Jaggard AD, 2017, ACM T ECON COMPUT, V5, DOI 10.1145/3107182; KLEINBERG R., 2011, S INN COMP SCI ICS; Kleinberg R, 2011, DISTRIB COMPUT, V24, P21, DOI 10.1007/s00446-011-0129-5; Kleinberg Robert, 2009, ACM S THEOR COMP STO; LI TY, 1975, AM MATH MON, V82, P985, DOI 10.2307/2318254; Mertikopoulos P, 2010, ANN APPL PROBAB, V20, P1359, DOI 10.1214/09-AAP651; Monderer D, 1996, J ECON THEORY, V68, P258, DOI 10.1006/jeth.1996.0014; Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044; Nisan N, 2008, LECT NOTES COMPUT SC, V5385, P531, DOI 10.1007/978-3-540-92185-1_59; Piliouras G., 2014, SODA; Rosenthal R. W., 1973, International Journal of Game Theory, V2, P65, DOI 10.1007/BF01737559; Roughgarden T, 2009, ACM S THEORY COMPUT, P513; Sarkovskii AN., 1964, UKR MATH J+, V16, P61; Strogatz S., 2000, NONLINEAR DYNAMICS C; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; Welch L.R., 2003, IEEE INFORM THEORY S, V53, P1	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405092
C	Pan, JW; Zhang, BQ; Rao, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pan, Jiangwei; Zhang, Boqian; Rao, Vinayak			Collapsed variational Bayes for Markov jump processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SIMULATION; INFERENCE; CHAINS	Markov jump processes are continuous-time stochastic processes widely used in statistical applications in the natural sciences, and more recently in machine learning. Inference for these models typically proceeds via Markov chain Monte Carlo, and can suffer from various computational challenges. In this work, we propose a novel collapsed variational inference algorithm to address this issue. Our work leverages ideas from discrete-time Markov chains, and exploits a connection between these two through an idea called uniformization. Our algorithm proceeds by marginalizing out the parameters of the Markov jump process, and then approximating the distribution over the trajectory with a factored distribution over segments of a piecewise-constant function. Unlike MCMC schemes that marginalize out transition times of a piecewise-constant process, our scheme optimizes the discretization of time, resulting in significant computational savings. We apply our ideas to synthetic data as well as a dataset of check-in recordings, where we demonstrate superior performance over state-of-the-art MCMC methods.	[Pan, Jiangwei] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA; [Zhang, Boqian; Rao, Vinayak] Purdue Univ, Dept Stat, W Lafayette, IN 47907 USA; [Pan, Jiangwei] Facebook, Menlo Pk, CA 94025 USA	Duke University; Purdue University System; Purdue University; Purdue University West Lafayette Campus; Facebook Inc	Pan, JW (corresponding author), Duke Univ, Dept Comp Sci, Durham, NC 27706 USA.; Pan, JW (corresponding author), Facebook, Menlo Pk, CA 94025 USA.	panjiangwei@gmail.com; zhan1977@purdue.edu; varao@purdue.edu	Jeong, Yongwook/N-7413-2016					Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Bladt M, 2005, J R STAT SOC B, V67, P395, DOI 10.1111/j.1467-9868.2005.00508.x; Boys RJ, 2008, STAT COMPUT, V18, P125, DOI 10.1007/s11222-007-9043-x; Cinlar E., 2013, INTRO STOCHASTIC PRO; Fearnhead P, 2006, J ROY STAT SOC B, V68, P767, DOI 10.1111/j.1467-9868.2006.00566.x; Gao H., 2012, P 21 ACM C INF KNOWL; GILLESPIE DT, 1977, J PHYS CHEM-US, V81, P2340, DOI 10.1021/j100540a008; Hajiaghayi M, 2014, PR MACH LEARN RES, V32; Hobolth A, 2009, ANN APPL STAT, V3, P1204, DOI 10.1214/09-AOAS247; Huggins JH, 2015, PR MACH LEARN RES, V37, P693; Hughes M. C., 2015, ADV NEURAL INFORM PR, P1198; Jensen A, 1953, SKAND AKTUARIETIDSKR, V36, P87, DOI 10.1080/03461238.1953.10419459; Metzner P, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066702; Opper M., 2007, NIPS 20; Pan JW, 2016, PR MACH LEARN RES, V48; Rao V., 2014, J MACHINE LEARNING R, V13; Rao V. A., 2012, ADV NEURAL INFORM PR, V25, P710; Saeedi A., 2011, NIPS 24; Wang P., 2013, AISTATS; Xu J, 2010, J ARTIF INTELL RES, V39, P745, DOI 10.1613/jair.3050	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403079
C	Pang, HT; Vanderbei, R; Liu, H; Zhao, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pang, Haotian; Vanderbei, Robert; Liu, Han; Zhao, Tuo			Parametric Simplex Method for Sparse Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIABLE SELECTION; REGRESSION	High dimensional sparse learning has imposed a great computational challenge to large scale data analysis. In this paper, we are interested in a broad class of sparse learning approaches formulated as linear programs parametrized by a regularization factor, and solve them by the parametric simplex method (PSM). Our parametric simplex method offers significant advantages over other competing methods: (1) PSM naturally obtains the complete solution path for all values of the regularization parameter; (2) PSM provides a high precision dual certificate stopping criterion; (3) PSM yields sparse solutions through very few iterations, and the solution sparsity significantly reduces the computational cost per iteration. Particularly, we demonstrate the superiority of PSM over various sparse learning approaches, including Dantzig selector for sparse linear regression, LAD-Lasso for sparse robust linear regression, CLIME for sparse precision matrix estimation, sparse differential network estimation, and sparse Linear Programming Discriminant (LPD) analysis. We then provide sufficient conditions under which PSM always outputs sparse solutions such that its computational performance can be significantly boosted. Thorough numerical experiments are provided to demonstrate the outstanding performance of the PSM method.	[Pang, Haotian; Vanderbei, Robert; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA; [Liu, Han] Tencent AI Lab, Bellevue, WA USA; [Pang, Haotian; Vanderbei, Robert; Liu, Han] Northwestern Univ, Evanston, IL 60208 USA; [Zhao, Tuo] Georgia Tech, Atlanta, GA 30332 USA	Princeton University; Northwestern University; University System of Georgia; Georgia Institute of Technology	Zhao, T (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	tuo.zhao@isye.gatech.edu	Vanderbei, Robert J/A-9779-2009					[Anonymous], 1997, LINEAR COMPLEMENTARI; Bandyopadhyay S, 2010, SCIENCE, V330, P1385, DOI 10.1126/science.1195618; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cai T, 2011, J AM STAT ASSOC, V106, P1566, DOI 10.1198/jasa.2011.tm11199; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; DANTZIG G., 1951, LINEAR PROGRAMMING E; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Gai YJ, 2013, STAT SINICA, V23, P615, DOI 10.5705/ss.2012.061; Hudson NJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000382; Ideker T, 2012, MOL SYST BIOL, V8, DOI 10.1038/msb.2011.99; Li XG, 2015, J MACH LEARN RES, V16, P553; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; VANDERBEI R., 1995, LINEAR PROGRAMMING F; Wang HS, 2007, J BUS ECON STAT, V25, P347, DOI 10.1198/073500106000000251; Yao YG, 2014, STAT COMPUT, V24, P885, DOI 10.1007/s11222-013-9408-2; ZHAO S. D., 2013, BIOMETRIKA, V58, P253; Zhu J., 2004, ADV NEURAL INFORM PR, V16; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	20	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400018
C	Parthasarathy, N; Batty, E; Falcon, W; Rutten, T; Rajpal, M; Chichilnisky, EJ; Paninski, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Parthasarathy, Nikhil; Batty, Eleanor; Falcon, William; Rutten, Thomas; Rajpal, Mohit; Chichilnisky, E. J.; Paninski, Liam			Neural Networks for Efficient Bayesian Decoding of Natural Images from Retinal Neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RECONSTRUCTION; RESPONSES	Decoding sensory stimuli from neural signals can be used to reveal how we sense our physical environment, and is valuable for the design of brain-machine interfaces. However, existing linear techniques for neural decoding may not fully reveal or exploit the fidelity of the neural signal. Here we develop a new approximate Bayesian method for decoding natural images from the spiking activity of populations of retinal ganglion cells (RGCs). We sidestep known computational challenges with Bayesian inference by exploiting artificial neural networks developed for computer vision, enabling fast nonlinear decoding that incorporates natural scene statistics implicitly. We use a decoder architecture that first linearly reconstructs an image from RGC spikes, then applies a convolutional autoencoder to enhance the image The resulting decoder, trained on natural images and simulated neural responses, significantly outperforms linear decoding, as well as simple point-wise nonlinear decoding. These results provide a tool for the assessment and optimization of retinal prosthesis technologies, and reveal that the retina may provide a more accurate representation of the visual scene than previously appreciated.	[Parthasarathy, Nikhil; Chichilnisky, E. J.] Stanford Univ, Stanford, CA 94305 USA; [Batty, Eleanor; Falcon, William; Rutten, Thomas; Rajpal, Mohit; Paninski, Liam] Columbia Univ, New York, NY 10027 USA	Stanford University; Columbia University	Parthasarathy, N (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	nikparth@gmail.com; erb2180@columbia.edu; waf2107@columbia.edu; tkr2112@columbia.edu; mr3522@columbia.edu; ej@stanford.edu; liam@stat.columbia.edu	Jeong, Yongwook/N-7413-2016	Chichilnisky, E.J./0000-0002-5613-0248	NSF GRFP [DGE-16-44869]; NSF/NIH Collaborative Research in Computational Neuroscience Grant [IIS-1430348/1430239]; DARPA [N66001-17-C-4002, FA8650-16-1-7657]; Simons Foundation [SF-SCGB-365002]; IARPA MICRONS [D16PC00003]	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF/NIH Collaborative Research in Computational Neuroscience Grant; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Simons Foundation; IARPA MICRONS	NSF GRFP DGE-16-44869 (EB), NSF/NIH Collaborative Research in Computational Neuroscience Grant IIS-1430348/1430239 (EJC & LP), DARPA Contract FA8650-16-1-7657 (EJC), Simons Foundation SF-SCGB-365002 (LP); IARPA MICRONS D16PC00003 (LP); DARPA N66001-17-C-4002 (LP).	Anderson AG, 2016, CONF REC ASILOMAR C, P588, DOI 10.1109/ACSSC.2016.7869110; Arsenault E, 2011, J VISION, V11, DOI 10.1167/11.10.14; Batty E., 2017, P 5 INT C LEARN REPR; Brouwer GJ, 2009, J NEUROSCI, V29, P13992, DOI 10.1523/JNEUROSCI.3577-09.2009; Brown EN, 1998, J NEUROSCI, V18, P7411; Burak Y, 2010, P NATL ACAD SCI USA, V107, P19525, DOI 10.1073/pnas.1006076107; Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Diaz-Tahoces Ariadna R., 2015, 6 INT WORK C INT NAT, V9107; Frechette ES, 2005, J NEUROPHYSIOL, V94, P119, DOI 10.1152/jn.01175.2004; Gondara Lovedeep, 2016, 160804667 ARXIV; Heitman A., 2016, TESTING PSEUDO LINEA, DOI [10.1101/045336, DOI 10.1101/045336]; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Junyuan X, 2012, ADV NEURAL INF PROCE, P341; Kamitani Y, 2005, NAT NEUROSCI, V8, P679, DOI 10.1038/nn1444; Kingma D.P, P 3 INT C LEARNING R; Koyama S, 2010, J AM STAT ASSOC, V105, P170, DOI 10.1198/jasa.2009.tm08326; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Lalor EC, 2009, J OPT SOC AM A, V26, pB25, DOI 10.1364/JOSAA.26.000B25; Laparra V., 2017, ARXIV170106641; Ledig C., 2016, IEEE COMPUTER SOC; Liu Ziwei, 2015, P INT C COMP VIS ICC; Marre O, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004304; McIntosh Lane T., 2016, ADV NEURAL INFORM PR; Naselaris T, 2009, NEURON, V63, P902, DOI 10.1016/j.neuron.2009.09.006; Nirenberg Sheila, 2012, PNAS, V109; Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Pasley BN, 2012, PLOS BIOL, V10, DOI 10.1371/journal.pbio.1001251; Ramirez AD, 2011, J NEUROSCI, V31, P3828, DOI 10.1523/JNEUROSCI.3256-10.2011; Rieke F., 1999, SPIKES EXPLORING NEU; Shpigelman L, 2009, ADV NEURAL INFORM PR, P1489; Stanley GB, 1999, J NEUROSCI, V19, P8036, DOI 10.1523/jneurosci.19-18-08036.1999; Sugiyama M., 2016, ADV NEURAL INFORM PR; Sussillo D, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13749; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Wang ZY, 2015, IEEE COMPUT SOC CONF; Warland DK, 1997, J NEUROPHYSIOL, V78, P2336, DOI 10.1152/jn.1997.78.5.2336; Wen Haiguang, 2016, 160803425 ARXIV; Xu Kai, 2011, ENG MED BIOL SOC EMB; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zhao H., 2015, ARXIV151108861	42	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406049
C	Peck, J; Roels, J; Goossens, B; Saeys, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Peck, Jonathan; Roels, Joris; Goossens, Bart; Saeys, Yvan			Lower bounds on the robustness to adversarial perturbations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The input-output mappings learned by state-of-the-art neural networks are significantly discontinuous. It is possible to cause a neural network used for image recognition to misclassify its input by applying very specific, hardly perceptible perturbations to the input, called adversarial perturbations. Many hypotheses have been proposed to explain the existence of these peculiar samples as well as several methods to mitigate them, but a proven explanation remains elusive. In this work, we take steps towards a formal characterization of adversarial perturbations by deriving lower bounds on the magnitudes of perturbations necessary to change the classification of neural networks. The proposed bounds can be computed efficiently, requiring time at most linear in the number of parameters and hyperparameters of the model for any given sample. This makes them suitable for use in model selection, when one wishes to find out which of several proposed classifiers is most robust to adversarial perturbations. They may also be used as a basis for developing techniques to increase the robustness of classifiers, since they enjoy the theoretical guarantee that no adversarial perturbation could possibly be any smaller than the quantities provided by the bounds. We experimentally verify the bounds on the MNIST and CIFAR-10 data sets and find no violations. Additionally, the experimental results suggest that very small adversarial perturbations may occur with non-zero probability on natural samples.	[Peck, Jonathan; Saeys, Yvan] Univ Ghent, Dept Appl Math Comp Sci & Stat, B-9000 Ghent, Belgium; [Peck, Jonathan; Roels, Joris; Saeys, Yvan] VIB Inflammat Res Ctr, Data Min & Modeling Biomed, B-9052 Ghent, Belgium; [Roels, Joris; Goossens, Bart] Univ Ghent, Dept Telecommun & Informat Proc, B-9000 Ghent, Belgium	Ghent University; Flanders Institute for Biotechnology (VIB); Ghent University	Peck, J (corresponding author), Univ Ghent, Dept Appl Math Comp Sci & Stat, B-9000 Ghent, Belgium.; Peck, J (corresponding author), VIB Inflammat Res Ctr, Data Min & Modeling Biomed, B-9052 Ghent, Belgium.		Goossens, Bart/H-4772-2018; Saeys, Yvan/C-1311-2009; Jeong, Yongwook/N-7413-2016	Goossens, Bart/0000-0002-1666-5483; Saeys, Yvan/0000-0002-0415-1506; Peck, Jonathan/0000-0003-2929-4164				Bai Y, 2014, PROC INT CONF RECON; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I., 2015, P 3 INT C LEARN REPR, V3; Gu S., 2014, NIPS WORKSH DEEP LEA; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lou Y., 2016, ARXIV151106292; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Reed Scott, 2015, 2015 IEEE C COMPUTE, P1, DOI [10.1109/CVPR.2015.7298594, DOI 10.1109/CVPR.2015.7298594]; Rozsa A., 2016, ARXIV161200138; Simonyan K., 2015, P 3 INT C LEARN REPR, V3; Szegedy C., 2014, P 2 INT C LEARN REPR, V2	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400077
C	Pennington, J; Schoenholz, SS; Ganguli, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pennington, Jeffrey; Schoenholz, Samuel S.; Ganguli, Surya			Resurrecting the sigmoid in deep learning through dynamical isometry: theory and practice	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					It is well known that weight initialization in deep networks can have a dramatic impact on learning speed. For example, ensuring the mean squared singular value of a network's input-output Jacobian is O(1) is essential for avoiding exponentially vanishing or exploding gradients. Moreover, in deep linear networks, ensuring that all singular values of the Jacobian are concentrated near 1 can yield a dramatic additional speed-up in learning; this is a property known as dynamical isometry. However, it is unclear how to achieve dynamical isometry in nonlinear deep networks. We address this question by employing powerful tools from free probability theory to analytically compute the entire singular value distribution of a deep network's input-output Jacobian. We explore the dependence of the singular value distribution on the depth of the network, the weight initialization, and the choice of nonlinearity. Intriguingly, we find that ReLU networks are incapable of dynamical isometry. On the other hand, sigmoidal networks can achieve isometry, but only with orthogonal weight initialization. Moreover, we demonstrate empirically that deep nonlinear networks achieving dynamical isometry learn orders of magnitude faster than networks that do not. Indeed, we show that properly-initialized deep sigmoidal networks consistently outperform deep ReLU networks. Overall, our analysis reveals that controlling the entire distribution of Jacobian singular values is an important design consideration in deep learning.	[Pennington, Jeffrey; Schoenholz, Samuel S.; Ganguli, Surya] Google Brain, Mountain View, CA 94043 USA; [Ganguli, Surya] Stanford Univ, Appl Phys, Stanford, CA 94305 USA	Google Incorporated; Stanford University	Pennington, J (corresponding author), Google Brain, Mountain View, CA 94043 USA.		Jeong, Yongwook/N-7413-2016		Simons Foundation; McKnight Foundation; James S. McDonnell Foundation; Burroughs Wellcome Foundation; Office of Naval Research	Simons Foundation; McKnight Foundation; James S. McDonnell Foundation; Burroughs Wellcome Foundation(Burroughs Wellcome Fund); Office of Naval Research(Office of Naval Research)	S.G. thanks the Simons, McKnight, James S. McDonnell, and Burroughs Wellcome Foundations and the Office of Naval Research for support.	[Anonymous], 2016, CORR; Ba J., 2017, P 3 INT C LEARN REPR; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Hinton G., NEURAL NETWORKS MACH; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lagrange Joseph Louis, 1770, NOUVELLE METHODE RES; McIntosh LT, 2016, ADV NEUR IN, V29; Mishkin D., 2015, CORR; Neuschel T, 2014, RANDOM MATRICES-THEO, V3, DOI 10.1142/S2010326314500038; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Piech C., 2015, ADV NEURAL INFORM PR, P505; Poole B., 2016, NEURAL INFORM PROCES; Saxe Andrew M, 2013, ICLR 2014; Schoenholz S. S., 2017, INT C LEARN REPR ICL; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; SPEICHER R, 1994, MATH ANN, V298, P611, DOI 10.1007/BF01459754; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; Voiculescu D. V., 1992, CRM MONOGRAPH SERIES; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111	22	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404083
C	Peter, S; Diego, F; Hamprecht, FA; Nadler, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Peter, Sven; Diego, Ferran; Hamprecht, Fred A.; Nadler, Boaz			Cost efficient gradient boosting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ARTIFICIAL NEURAL-NETWORKS; CLASSIFICATION; TREES	Many applications require learning classifiers or regressors that are both accurate and cheap to evaluate. Prediction cost can be drastically reduced if the learned predictor is constructed such that on the majority of the inputs, it uses cheap features and fast evaluations. The main challenge is to do so with little loss in accuracy. In this work we propose a budget-aware strategy based on deep boosted regression trees. In contrast to previous approaches to learning with cost penalties, our method can grow very deep trees that on average are nonetheless cheap to compute. We evaluate our method on a number of datasets and find that it outperforms the current state of the art by a large margin. Our algorithm is easy to implement and its learning time is comparable to that of the original gradient boosting. Source code is made available at http://github.com/svenpeter42/LightGBM-CEGB.	[Peter, Sven; Hamprecht, Fred A.] Heidelberg Univ, Interdisciplinary Ctr Sci Comp, Heidelberg Collab Image Proc, D-69115 Heidelberg, Germany; [Diego, Ferran] Robert Bosch GmbH, Robert Bosch Str 200, D-31139 Hildesheim, Germany; [Nadler, Boaz] Weizmann Inst Sci, Dept Comp Sci, IL-76100 Rehovot, Israel	Ruprecht Karls University Heidelberg; Bosch; Weizmann Institute of Science	Peter, S (corresponding author), Heidelberg Univ, Interdisciplinary Ctr Sci Comp, Heidelberg Collab Image Proc, D-69115 Heidelberg, Germany.	sven.peter@iwr.ani-heidelberg.de; ferran.diegoandilla@de.bosch.com; fred.hamprecht@iwr.uni-heidelberg.de; boaz.nadler@weizmann.ac.ir	Jeong, Yongwook/N-7413-2016					Amayeh G, 2009, LECT NOTES COMPUT SC, V5875, P243, DOI 10.1007/978-3-642-10331-5_23; [Anonymous], 2010, P BMVC; Baldi Pierre, 2016, ARXIV160107913; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Castelli V, 1996, INT CONF ACOUST SPEE, P2199, DOI 10.1109/ICASSP.1996.545857; Chapelle O., 2011, P LEARN RANK CHALL, P1; Chen T., 2016, KDD16 P 22 ACM, P785, DOI [10.1145/2939672.2939785, DOI 10.1145/2939672.2939785]; DeSalvo G, 2015, LECT NOTES ARTIF INT, V9355, P254, DOI 10.1007/978-3-319-24486-0_17; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Getreuer P, 2013, IMAGE PROCESS ON LIN, V3, P286, DOI 10.5201/ipol.2013.87; Han Song, 2016, INT C LEARN REPR ICL; Hancock T, 1996, INFORM COMPUT, V126, P114, DOI 10.1006/inco.1996.0040; Hubara I, 2016, ADV NEUR IN, V29; Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8; Ke G., 2017, P ADV NEURAL INFORM, V30, P3146; Kusner MJ, 2014, AAAI CONF ARTIF INTE, P1939; Lefakis L, 2010, ADV NEURAL INFORM PR, P1315; Lichman M., 2013, UCI MACHINE LEARNING; Maska M, 2014, BIOINFORMATICS, V30, P1609, DOI 10.1093/bioinformatics/btu080; Nan F., 2015, INT C MACH LEARN, P1983; Nan F, 2016, ADV NEUR IN, V29; Naumov G. E., 1991, Soviet Physics - Doklady, V36, P270; Pedersoli M, 2015, PATTERN RECOGN, V48, P1844, DOI 10.1016/j.patcog.2014.11.006; Roe BP, 2005, NUCL INSTRUM METH A, V543, P577, DOI 10.1016/j.nima.2004.12.018; Scherer D, 2010, LECT NOTES COMPUT SC, V6354, P92, DOI 10.1007/978-3-642-15825-4_10; Shi H., 2007, THESIS; Trapeznikov K, 2013, ARTIF INTELL, P581; Viola P., 2001, P IEEE COMP SOC C CO, P1; WANG J, 2015, ADV NEURAL INFORM PR, P2152; Xu Z.E., 2013, ICML 2013, P133; Xu Zhixiang, 2012, ICML, P1175; Xu Z, 2014, J MACH LEARN RES, V15, P2113; Zantema H., 2000, International Journal of Foundations of Computer Science, V11, P343, DOI 10.1142/S0129054100000193	36	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401057
C	Pica, G; Piasini, E; Safaai, H; Runyan, CA; Diamond, ME; Fellin, T; Kayser, C; Harvey, CD; Panzeri, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pica, Giuseppe; Piasini, Eugenio; Safaai, Houman; Runyan, Caroline A.; Diamond, Mathew E.; Fellin, Tommaso; Kayser, Christoph; Harvey, Christopher D.; Panzeri, Stefano			Quantifying how much sensory information in a neural code is relevant for behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISCRIMINATION; NEURONS; CHOICE; PERCEPTION	Determining how much of the sensory information carried by a neural code contributes to behavioral performance is key to understand sensory function and neural information flow. However, there are as yet no analytical tools to compute this information that lies at the intersection between sensory coding and behavioral readout. Here we develop a novel measure, termed the information-theoretic intersection information I-II(S; R; C), that quantifies how much of the sensory information carried by a neural response R is used for behavior during perceptual discrimination tasks. Building on the Partial Information Decomposition framework, we define I-II(S; R; C) as the part of the mutual information between the stimulus S and the response R that also informs the consequent behavioral choice C. We compute I-II(S; R; C) in the analysis of two experimental cortical datasets, to show how this measure can be used to compare quantitatively the contributions of spike timing and spike rates to task performance, and to identify brain areas or neural populations that specifically transform sensory information into choice.	[Pica, Giuseppe; Piasini, Eugenio; Safaai, Houman; Panzeri, Stefano] Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Computat Lab, I-38068 Rovereto, TN, Italy; [Pica, Giuseppe; Fellin, Tommaso; Panzeri, Stefano] Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Coding Lab, I-38068 Rovereto, TN, Italy; [Safaai, Houman; Runyan, Caroline A.; Harvey, Christopher D.] Harvard Med Sch, Dept Neurobiol, Boston, MA 02115 USA; [Runyan, Caroline A.] Univ Pittsburgh, Dept Neurosci, Ctr Neural Basis Cognit, Pittsburgh, PA USA; [Diamond, Mathew E.] Int Sch Adv Studies SISSA, Tactile Percept & Learning Lab, Trieste, Italy; [Fellin, Tommaso] Ist Italiano Tecnol, Opt Approaches Brain Funct Lab, I-16163 Genoa, Italy; [Kayser, Christoph] Univ Glasgow, Inst Neurosci & Psychol, Glasgow, Lanark, Scotland; [Kayser, Christoph] Bielefeld Univ, Fac Biol, Dept Cognit Neurosci, Universitatsstr 25, D-33615 Bielefeld, Germany	Istituto Italiano di Tecnologia - IIT; Istituto Italiano di Tecnologia - IIT; Harvard University; Harvard Medical School; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; International School for Advanced Studies (SISSA); Istituto Italiano di Tecnologia - IIT; University of Glasgow; University of Bielefeld	Pica, G (corresponding author), Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Computat Lab, I-38068 Rovereto, TN, Italy.; Pica, G (corresponding author), Ist Italiano Tecnol, Ctr Neurosci & Cognit Syst UniTn, Neural Coding Lab, I-38068 Rovereto, TN, Italy.	giuseppe.pica@iit.it; eugenio.piasini@iit.it; houman_safaai@hms.haryard.edu; runyan@pitt.edu; diamond@sissa.it; tommaso.fellin@iit.it; christoph.kayser@uni-bielefeld.de; Christopher_Haryey@hms.haryard.edu; stefano.panzeri@iit.it	Piasini, Eugenio/AAX-1566-2020; Jeong, Yongwook/N-7413-2016	Piasini, Eugenio/0000-0003-0384-7699; 	Seal of Excellence Fellowship CONISC; Fondation Bertarelli; NIH [1U01NS090576-01, MH107620, NS089521]; ERC (NEURO-PATTERNS); European Research Council (ERC-2014-CoG) [646657]	Seal of Excellence Fellowship CONISC; Fondation Bertarelli; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ERC (NEURO-PATTERNS); European Research Council (ERC-2014-CoG)	GP was supported by a Seal of Excellence Fellowship CONISC. SP was supported by Fondation Bertarelli. CDH was supported by grants from the NIH (MH107620 and NS089521). CDH is a New York Stem Cell Foundation Robertson Neuroscience Investigator. TF was supported by the grants ERC (NEURO-PATTERNS) and NIH (1U01NS090576-01). CK was supported by the European Research Council (ERC-2014-CoG; grant No 646657).	Barrett AB, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.052802; Beer, 2010, ARXIV10042515, DOI DOI 10.HTTPS://ARXIV.0RG/ABS/1004.2515; Bertschinger N., 2012, P ECCS 2012 BRUSS BE; Bertschinger N, 2014, ENTROPY-SWITZ, V16, P2161, DOI 10.3390/e16042161; BIALEK W, 1991, SCIENCE, V252, P1854, DOI 10.1126/science.2063199; Borst A, 1999, NAT NEUROSCI, V2, P947, DOI 10.1038/14731; Britten KH, 1996, VISUAL NEUROSCI, V13, P87, DOI 10.1017/S095252380000715X; Buonomano DV, 2009, NAT REV NEUROSCI, V10, P113, DOI 10.1038/nrn2558; Chicharro D, 2017, ARXIV170803845; Engineer CT, 2008, NAT NEUROSCI, V11, P603, DOI 10.1038/nn.2109; Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038; Griffith V, 2014, EMERGENCE COMPLEX CO, V9, P159, DOI 10.1007/978-3-642-53734-9_6; Haefner RM, 2013, NAT NEUROSCI, V16, P235, DOI 10.1038/nn.3309; Harder M, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.012130; Harvey CD, 2012, NATURE, V484, P62, DOI 10.1038/nature10918; Harvey MA, 2013, PLOS BIOL, V11, DOI 10.1371/journal.pbio.1001558; Jacobs AL, 2009, P NATL ACAD SCI USA, V106, P5936, DOI 10.1073/pnas.0900573106; Kayser C, 2009, NEURON, V61, P597, DOI 10.1016/j.neuron.2009.01.008; Luczak A, 2015, NAT REV NEUROSCI, V16, P745, DOI 10.1038/nrn4026; Luna R, 2005, NAT NEUROSCI, V8, P1210, DOI 10.1038/nn1513; Nakamura K, 1999, J NEUROPHYSIOL, V82, P2503, DOI 10.1152/jn.1999.82.5.2503; NEWSOME WT, 1989, NATURE, V341, P52, DOI 10.1038/341052a0; O'Connor DH, 2013, NAT NEUROSCI, V16, P958, DOI 10.1038/nn.3419; Panzeri S, 2017, NEURON, V93, P491, DOI 10.1016/j.neuron.2016.12.036; Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002; Panzeri S, 2014, PHILOS T R SOC B, V369, DOI 10.1098/rstb.2012.0467; Panzeri S, 2010, TRENDS NEUROSCI, V33, P111, DOI 10.1016/j.tins.2009.12.001; Pica G, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19090451; Pitkow X, 2015, NEURON, V87, P411, DOI 10.1016/j.neuron.2015.06.033; Quiroga RQ, 2009, NAT REV NEUROSCI, V10, P173, DOI 10.1038/nrn2578; Raposo D, 2014, NAT NEUROSCI, V17, P1784, DOI 10.1038/nn.3865; Rauschecker JP, 2000, P NATL ACAD SCI USA, V97, P11800, DOI 10.1073/pnas.97.22.11800; Romo R, 2003, NAT REV NEUROSCI, V4, P203, DOI 10.1038/nrn1058; Rossi-Pool R, 2016, P NATL ACAD SCI USA, V113, pE7966, DOI 10.1073/pnas.1618196113; Runyan CA, 2017, NATURE, V548, P92, DOI 10.1038/nature23020; Shadlen MN, 1998, J NEUROSCI, V18, P3870; Shamir M, 2014, CURR OPIN NEUROBIOL, V25, P140, DOI 10.1016/j.conb.2014.01.002; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; Victor JD, 2008, NEURAL COMPUT, V20, P2895, DOI 10.1162/neco.2008.10-07-633; Zuo YF, 2015, CURR BIOL, V25, P357, DOI 10.1016/j.cub.2014.11.065	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403073
C	Prasad, A; Niculescu-Mizil, A; Ravikumar, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Prasad, Adarsh; Niculescu-Mizil, Alexandru; Ravikumar, Pradeep			On Separability of Loss Functions, and Revisiting Discriminative Vs Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We revisit the classical analysis of generative vs discriminative models for general exponential families, and high-dimensional settings. Towards this, we develop novel technical machinery, including a notion of separability of general loss functions, which allow us to provide a general framework to obtain l(infinity )convergence rates for general M-estimators. We use this machinery to analyze l(infinity) and l(2) convergence rates of generative and discriminative models, and provide insights into their nuanced behaviors in high-dimensions. Our results are also applicable to differential parameter estimation, where the quantity of interest is the difference between generative model parameters.	[Prasad, Adarsh; Ravikumar, Pradeep] CMU, Machine Learning Dept, Mt Pleasant, MI 48859 USA; [Niculescu-Mizil, Alexandru] NEC Labs Amer, Princeton, NJ USA	NEC Corporation	Prasad, A (corresponding author), CMU, Machine Learning Dept, Mt Pleasant, MI 48859 USA.	adarshp@andrew.cmu.edu; pradeepr@cs.cmu.edu			ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1447574, DMS-1264033]; NIH, Joint DMS/NIGMS Initiative [R01 GM117594-01]	ARO; NSF(National Science Foundation (NSF)); NIH, Joint DMS/NIGMS Initiative	A.P. and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1447574, DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.	de la Fuente A, 2010, TRENDS GENET, V26, P326, DOI 10.1016/j.tig.2010.05.001; Giraud C., 2014, INTRO HIGH DIMENSION, V138; Li TY, 2017, PR MACH LEARN RES, V54, P1; Li Tianyang, 2015, ADV NEURAL INFORM PR, P1054; Li Y.-H., 2015, AISTATS; Liu S, 2014, NEURAL COMPUT, V26, P1169, DOI 10.1162/NECO_a_00589; Ortega JM., 2000, ITERATIVE SOLUTION N, DOI [10.1137/1.9780898719468, DOI 10.1137/1.9780898719468]; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; WAINWRIGHT J. M., 2015, HIGH DIMENSION UNPUB; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Zhao Sihai Dave, 2014, BIOMETRIKA	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407014
C	Pu, YC; Wang, WY; Henao, R; Chen, LQ; Gan, Z; Li, CY; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pu, Yunchen; Wang, Weiyao; Henao, Ricardo; Chen, Liqun; Gan, Zhe; Li, Chunyuan; Carin, Lawrence			Adversarial Symmetric Variational Autoencoder	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A new form of variational autoencoder (VAE) is developed, in which the joint distribution of data and codes is considered in two (symmetric) forms: (i) from observed data fed through the encoder to yield codes, and (ii) from latent codes drawn from a simple prior and propagated through the decoder to manifest data. Lower bounds are learned for marginal log-likelihood fits observed data and latent codes. When learning with the variational bound, one seeks to minimize the symmetric Kullback-Leibler divergence of joint density functions from (i) and (ii), while simultaneously seeking to maximize the two marginal log-likelihoods. To facilitate learning, a new form of adversarial training is developed. An extensive set of experiments is performed, in which we demonstrate state-of-the-art data reconstruction and generation on several image benchmark datasets.	[Pu, Yunchen; Wang, Weiyao; Henao, Ricardo; Chen, Liqun; Gan, Zhe; Li, Chunyuan; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA	Duke University	Pu, YC (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA.	yp42@duke.edu; ww109@duke.edu; r.henao@duke.edu; lc267@duke.edu; zg27@duke.edu; cl319@duke.edu; lcarin@duke.edu	Jeong, Yongwook/N-7413-2016; Li, Chunyuan/AAG-1303-2020		ARO; DARPA; DOE; NGA; ONR; NSF	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.	[Anonymous], 2017, ICLR; [Anonymous], 2016, ICML; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Arjovsky M., 2017, P 34 INT C MACH LEAR, P214; Arora S., 2017, GEN EQUILIBRIUM GENE; Ba J., 2017, P 3 INT C LEARN REPR; Burda Y., 2016, ICLR; Chen Liqun, 2017, ARXIV170901846; Chen X, 2016, ADV NEUR IN, V29; Darrell T, 2017, ICLR; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Gan Zhe, 2017, NIPS; Glorot X., 2010, P 13 INT C ART INT S, VVolume 9, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hornik K., 1989, NEURAL NETWORKS; Kim T., 2017, LEARNING DISCOVER CR; Kingma D. P., 2016, NIPS; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2014, NIPS; Larsen A., 2016, ICML; Li C., 2017, NIPS; Li Chongxuan, 2017, TRIPLE GENERATIVE AD; Liu Q., 2016, NIPS; Makhzani A., 2015, ABS151105644 CORR; Mescheder L., 2016, ADVERSARIAL VARIATIO; Oord A. v. d., 2016, ICML; Pu  Y., 2015, ICLR WORKSH; Pu Y., 2016, NIPS; Pu Y, 2016, ARTIFICIAL INTELLIGE; Pu Y., 2017, NIPS; Radford A., 2016, ICLR; Rezende D.J., 2014, PROC INT CONFER ENCE; REZENDE DJ, 2015, ICML; Salimans Tim, 2017, ICLR; Salimans Tim, 2016, ADV NEURAL INFORM PR; Shen D., 2017, ARXIV170907109; Srivastava N., 2014, JMLR; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Thei L., 2016, ICLR; Zhang Y., 2017, ADV NEURAL INFORM PR; Zhang Y., 2017, ICML; Zhang Y., 2016, NIPS; Zhu J., 2017, UNPAIRED IMAGE TO IM	43	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404039
C	Raghunathan, A; Jain, P; Krishnaswamy, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Raghunathan, Aditi; Jain, Prateek; Krishnaswamy, Ravishankar			Learning Mixture of Gaussians with Streaming Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper, we study the problem of learning a mixture of Gaussians with streaming data: given a stream of N points in d dimensions generated by an unknown mixture of k spherical Gaussians, the goal is to estimate the model parameters using a single pass over the data stream. We analyze a streaming version of the popular Lloyd's heuristic and show that the algorithm estimates all the unknown centers of the component Gaussians accurately if they are sufficiently separated. Assuming each pair of centers are C sigma distant with C = Omega((k log k)(1/4)sigma) and where sigma(2) is the maximum variance of any Gaussian component, we show that asymptotically the algorithm estimates the centers optimally (up to certain constants); our center separation requirement matches the best known result for spherical Gaussians [18]. For finite samples, we show that a bias term based on the initial estimate decreases at O(1/poly(N)) rate while variance decreases at nearly optimal rate of sigma(2) d/N. Our analysis requires seeding the algorithm with a good initial estimate of the true cluster centers for which we provide an online PCA based clustering algorithm. Indeed, the asymptotic per-step time complexity of our algorithm is the optimal d . k while space complexity of our algorithm is O(dk log k). In addition to the bias and variance terms which tend to 0, the hard-thresholding based updates of streaming Lloyd's algorithm is agnostic to the data distribution and hence incurs an approximation error that cannot be avoided. However, by using a streaming version of the classical (soft-thresholding-based) EM method that exploits the Gaussian distribution explicitly, we show that for a mixture of two Gaussians the true means can be estimated consistently, with estimation error decreasing at nearly optimal rate, and tending to 0 for N -> infinity.	[Raghunathan, Aditi] Stanford Univ, Stanford, CA 94305 USA; [Jain, Prateek; Krishnaswamy, Ravishankar] Microsoft Res, Bengaluru, Karnataka, India	Stanford University	Raghunathan, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	aditir@stanford.edu; prajain@microsoft.com; rakri@microsoft.com	Jeong, Yongwook/N-7413-2016					Anandkumar A, 2015, LECT NOTES ARTIF INT, V9355, P19, DOI 10.1007/978-3-319-24486-0_2; [Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; Ashtiani Hassan, 2017, ARXIV170601596; Balakrishnan Sivaraman, 2014, ANN STATS, V45, P77; Balcan MF, 2013, J ACM, V60, DOI 10.1145/2450142.2450144; Dasgupta A, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1036; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duda R.O., 2000, PATTERN CLASSIFICATI; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Jain Prateek, 2016, C LEARN THEOR, P1147; Kumar A, 2010, ANN IEEE SYMP FOUND, P299, DOI 10.1109/FOCS.2010.35; Mitliagkas Ioannis, 2013, ADV NEURAL INFORM PR, P2886; Raghunathan Aditi, 2017, CORR; Shamir O., 2011, ARXIV11102392; Tang C, 2016, JMLR WORKSH CONF PRO, V51, P1280; Tang Cheng, 2017, P AISTATS; Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jcss.2003.11.008; Xu Ji, 2016, ADV NEURAL INFORM PR, P2676; ZAMPETAKIS M., 2016, ARXIV160900368	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406065
C	Rahmanian, H; Warmuth, MK		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rahmanian, Holakou; Warmuth, Manfred K.			Online Dynamic Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS	We consider the problem of repeatedly solving a variant of the same dynamic programming problem in successive trials. An instance of the type of problems we consider is to find a good binary search tree in a changing environment. At the beginning of each trial, the learner probabilistically chooses a tree with the n keys at the internal nodes and the n + 1 gaps between keys at the leaves. The learner is then told the frequencies of the keys and gaps and is charged by the average search cost for the chosen tree. The problem is online because the frequencies can change between trials. The goal is to develop algorithms with the property that their total average search cost (loss) in all trials is close to the total loss of the best tree chosen in hindsight for all trials. The challenge, of course, is that the algorithm has to deal with exponential number of trees. We develop a general methodology for tackling such problems for a wide class of dynamic programming algorithms. Our framework allows us to extend online learning algorithms like Hedge [16] and Component Hedge [25] to a significantly wider class of combinatorial objects than was possible before.	[Rahmanian, Holakou; Warmuth, Manfred K.] Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95060 USA	University of California System; University of California Santa Cruz	Rahmanian, H (corresponding author), Univ Calif Santa Cruz, Dept Comp Sci, Santa Cruz, CA 95060 USA.	holakou@ucsc.edu; manfred@ucsc.edu			National Science Foundation (NSF) [IIS-1619271]	National Science Foundation (NSF)(National Science Foundation (NSF))	We thank S.V.N. Vishwanathan for initiating and guiding much of this research. We also thank Michael Collins for helpful discussions and pointers to the literature on hypergraphs and PCFGs. This research was supported by the National Science Foundation (NSF grant IIS-1619271).	Adamskiy D., 2012, P 25 INT C NEUR INF, P135; Ailon N, 2014, JMLR WORKSH CONF PRO, V33, P29; Audibert J-Y, 2011, P 24 ANN C LEARN THE, P107; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Awerbuch B, 2008, J COMPUT SYST SCI, V74, P97, DOI 10.1016/j.jcss.2007.04.016; Bauschke H. H., 1997, J CONVEX ANAL, V4, P27; Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Cormen T.H., 2009, INTRO ALGORITHMS; Cortes Corinna, 2015, C LEARN THEOR, P424; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; DEUTSCH F, 1995, NATO ADV SCI INST SE, V454, P87; Dick T, 2014, PR MACH LEARN RES, V32; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gupta  Swati, 2016, ARXIV160300522; Gyorgy A, 2007, J MACH LEARN RES, V8, P2369; Helmbold DP, 2009, J MACH LEARN RES, V10, P1705; Herbster M, 1998, MACH LEARN, V32, P151, DOI 10.1023/A:1007424614876; Kaibel V., 2011, ARXIV11041023; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kleinberg J., 2006, ALGORITHM DESIGN; Knight PA, 2008, SIAM J MATRIX ANAL A, V30, P261, DOI 10.1137/060659624; Koolen W.M., 2010, C LEARN THEOR, P239; Kuzmin D, 2005, LECT NOTES COMPUT SC, V3559, P684, DOI 10.1007/11503415_46; Kveton B, 2015, JMLR WORKSH CONF PRO, V38, P535; Loday Jean-Louis, 2005, P 2005 ACAD COLL SER; MARTIN RK, 1990, OPER RES, V38, P127, DOI 10.1287/opre.38.1.127; Mohri M, 2009, MONOGR THEOR COMPUT, P213, DOI 10.1007/978-3-642-01492-5_6; Rahmanian Holakou, 2017, ARXIV160905374; Rajkumar Arun, 2014, ADV NEURAL INFORM PR, P3482; Suehiro Daiki, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P260, DOI 10.1007/978-3-642-34106-9_22; Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328; Warmuth MK, 2008, J MACH LEARN RES, V9, P2287; Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402085
C	Rantanen, K; Hyttinen, A; Jarvisalo, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rantanen, Kari; Hyttinen, Antti; Jarvisalo, Matti			Learning Chordal Markov Networks via Branch and Bound	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				BAYESIAN GRAPHICAL MODELS; CHAIN	We present a new algorithmic approach for the task of finding a chordal Markov network structure that maximizes a given scoring function. The algorithm is based on branch and bound and integrates dynamic programming for both domain pruning and for obtaining strong bounds for search-space pruning. Empirically, we show that the approach dominates in terms of running times a recent integer programming approach (and thereby also a recent constraint optimization approach) for the problem. Furthermore, our algorithm scales at times further with respect to the number of variables than a state-of-the-art dynamic programming algorithm for the problem, with the potential of reaching 20 variables and at the same time circumventing the tight exponential lower bounds on memory consumption of the pure dynamic programming approach.	[Rantanen, Kari; Hyttinen, Antti; Jarvisalo, Matti] Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland	Aalto University; University of Helsinki	Rantanen, K (corresponding author), Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland.		Jeong, Yongwook/N-7413-2016	Hyttinen, Antti/0000-0002-6649-3229	Academy of Finland [251170 COIN]; Centre of Excellence in Computational Inference Research [276412, 284591, 295673, 312662]; Research Funds of the University of Helsinki	Academy of Finland(Academy of Finland); Centre of Excellence in Computational Inference Research; Research Funds of the University of Helsinki	The authors gratefully acknowledge financial support from the Academy of Finland under grants 251170 COIN Centre of Excellence in Computational Inference Research, 276412, 284591, 295673, and 312662; and the Research Funds of the University of Helsinki.	Abel HJ, 2011, STAT APPL GENET MOL, V10, DOI 10.2202/1544-6115.1615; Bartlett M, 2017, ARTIF INTELL, V244, P258, DOI 10.1016/j.artint.2015.03.003; Chickering D. M., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P87; Chickering DM, 2002, J MACH LEARN RES, V2, P445, DOI 10.1162/153244302760200696; Corander J., 2013, ADV NEURAL INFORM PR, P1349; DAWID AP, 1993, ANN STAT, V21, P1272, DOI 10.1214/aos/1176349260; de Campos CP, 2010, AAAI CONF ARTIF INTE, P431; de Campos CP, 2011, J MACH LEARN RES, V12, P663; Dellaportas P, 1999, BIOMETRIKA, V86, P615, DOI 10.1093/biomet/86.3.615; Giudici P, 1999, BIOMETRIKA, V86, P785, DOI 10.1093/biomet/86.4.785; Green PJ, 2013, BIOMETRIKA, V100, P91, DOI 10.1093/biomet/ass052; Janhunen T, 2017, STAT COMPUT, V27, P115, DOI 10.1007/s11222-015-9611-4; Kangas K., 2014, P NIPS, P2357; Kangas K, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P415; Koller D., 2009, PROBABILISTIC GRAPHI; Lauritzen S. L., 1990, READINGS UNCERTAIN R, P415; Letac G, 2007, ANN STAT, V35, P1278, DOI 10.1214/009053606000001235; MADIGAN D, 1995, INT STAT REV, V63, P215, DOI 10.2307/1403615; Malone B, 2014, LECT NOTES COMPUT SC, V8323, P111, DOI 10.1007/978-3-319-04534-4_8; Rantanen Kari, 2017, THESIS; Sesh Kumar K. S., 2013, JMLR WORKSHOP C P, P525; Silander T., 2006, P 22 C UNC ART INT, P445; Srebro N, 2003, ARTIF INTELL, V143, P123, DOI 10.1016/S0004-3702(02)00360-0; Studeny M, 2017, INT J APPROX REASON, V88, P259, DOI 10.1016/j.ijar.2017.06.001; Suzuki J., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P462; Suzuki Joe, 2017, P UAI; Tarantola C, 2004, STAT MODEL, V4, P39, DOI 10.1191/1471082X04st063oa; Tian J., 2000, UAI 00; van Beek P, 2015, LECT NOTES COMPUT SC, V9255, P429, DOI 10.1007/978-3-319-23219-5_31; Verzilli CJ, 2006, AM J HUM GENET, V79, P100, DOI 10.1086/505313; Wiesel A, 2010, IEEE T SIGNAL PROCES, V58, P1482, DOI 10.1109/TSP.2009.2037350; Yuan CH, 2013, J ARTIF INTELL RES, V48, P23, DOI 10.1613/jair.4039	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401085
C	Rebeschini, P; Tatikonda, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rebeschini, Patrick; Tatikonda, Sekhar			Accelerated consensus via Min-Sum Splitting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISTRIBUTED OPTIMIZATION; ALGORITHMS; CONVERGENCE; WALK	We apply the Min-Sum message-passing protocol to solve the consensus problem in distributed optimization. We show that while the ordinary Min-Sum algorithm does not converge, a modified version of it known as Splitting yields convergence to the problem solution. We prove that a proper choice of the tuning parameters allows Min-Sum Splitting to yield subdiffusive accelerated convergence rates, matching the rates obtained by shift-register methods. The acceleration scheme embodied by Min-Sum Splitting for the consensus problem bears similarities with lifted Markov chains techniques and with multi-step first order methods in convex optimization.	[Rebeschini, Patrick] Univ Oxford, Dept Stat, Oxford, England; [Tatikonda, Sekhar] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA	University of Oxford; Yale University	Rebeschini, P (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	patrick.rebeschini@stats.ox.ac.uk; sekhar.tatikonda@yale.edu	Jeong, Yongwook/N-7413-2016	Rebeschini, Patrick/0000-0001-7772-4160	NSF [EECS-1609484]	NSF(National Science Foundation (NSF))	This work was partially supported by the NSF under Grant EECS-1609484.	[Anonymous], 2002, REVERSIBLE MARKOV CH; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Cao M., 2006, 44 ANN ALLERTON C CO, P952; Chen JS, 2012, IEEE T SIGNAL PROCES, V60, P4289, DOI 10.1109/TSP.2012.2198470; Diaconis P, 2000, ANN APPL PROBAB, V10, P726; DIACONIS P, 1994, GEOM FUNCT ANAL, V4, P1, DOI 10.1007/BF01898359; Dimakis AG, 2010, P IEEE, V98, P1847, DOI 10.1109/JPROC.2010.2052531; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Feng Chen, 1999, Proceedings of the Thirty-First Annual ACM Symposium on Theory of Computing, P275, DOI 10.1145/301250.301315; Forero PA, 2010, J MACH LEARN RES, V11, P1663; Franca G, 2017, IEEE SIGNAL PROC LET, V24, P294, DOI 10.1109/LSP.2017.2654860; Ghadimi E, 2013, IEEE T SIGNAL PROCES, V61, P5417, DOI 10.1109/TSP.2013.2278149; Ghosh B., 1996, SPAA '96. 8th Annual ACM Symposium on Parallel Algorithms and Architectures, P72, DOI 10.1145/237502.237509; Golub GH., 1961, NUMER MATH, V3, P147, DOI DOI 10.1007/BF01386013; Jakovetic D, 2014, IEEE T AUTOMAT CONTR, V59, P1131, DOI 10.1109/TAC.2014.2298712; Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X; Jung K, 2010, IEEE T INFORM THEORY, V56, P634, DOI 10.1109/TIT.2009.2034777; Kar S, 2008, IEEE T SIGNAL PROCES, V56, P2609, DOI 10.1109/TSP.2008.923536; Lesser V, 2003, MU S ART SOC SIM ORG, V9, P1; Li D, 2002, IEEE SIGNAL PROC MAG, V19, P17, DOI 10.1109/79.985674; Li WJ, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY PROCEEDINGS, VOLS 1-7, P2881, DOI 10.1109/ISIT.2007.4557655; Li WJ, 2010, IEEE T INFORM THEORY, V56, P6208, DOI 10.1109/TIT.2010.2081030; Liu J, 2013, AUTOMATICA, V49, P873, DOI 10.1016/j.automatica.2013.01.001; Malioutov DM, 2006, J MACH LEARN RES, V7, P2031; Mateos G, 2010, IEEE T SIGNAL PROCES, V58, P5262, DOI 10.1109/TSP.2010.2055862; Moallemi CC, 2006, IEEE T INFORM THEORY, V52, P4753, DOI 10.1109/TIT.2006.883539; Moallemi CC, 2010, IEEE T INFORM THEORY, V56, P2041, DOI 10.1109/TIT.2010.2040863; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Olshevsky A., 2014, 14114186 ARXIV; Predd JB, 2009, IEEE T INFORM THEORY, V55, P1856, DOI 10.1109/TIT.2009.2012992; Rabbat MG, 2005, 2005 IEEE 6TH WORKSHOP ON SIGNAL PROCESSING ADVANCES IN WIRELESS COMMUNICATIONS, P1088; Ram SS, 2010, J OPTIMIZ THEORY APP, V147, P516, DOI 10.1007/s10957-010-9737-7; Roch S, 2005, ELECTRON COMMUN PROB, V10, P282, DOI 10.1214/ECP.v10-1169; Ruozzi N, 2013, J MACH LEARN RES, V14, P2287; Ruozzi N, 2013, IEEE T INFORM THEORY, V59, P5860, DOI 10.1109/TIT.2013.2259576; Scaman K, 2017, PR MACH LEARN RES, V70; Shah D, 2008, FOUND TRENDS NETW, V3, P1, DOI 10.1561/1300000014; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Tsitsiklis J., 1984, THESIS; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Wei E, 2012, IEEE DECIS CONTR P, P5445, DOI 10.1109/CDC.2012.6425904; Xiao L, 2004, SYST CONTROL LETT, V53, P65, DOI 10.1016/j.sysconle.2004.02.022; Young D.M., 1972, J APPROXIMATION THEO, V5, P137, DOI [10.1016/0021-9045(72)90036-6, DOI 10.1016/0021-9045(72)90036-6]	45	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401040
C	Remes, S; Heinonen, M; Kaski, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Remes, Sami; Heinonen, Markus; Kaski, Samuel			Non-Stationary Spectral Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose non-stationary spectral kernels for Gaussian process regression by modelling the spectral density of a non-stationary kernel function as a mixture of input-dependent Gaussian process frequency density surfaces. We solve the generalised Fourier transform with such a model, and present a family of non-stationary and non-monotonic kernels that can learn input-dependent and potentially longrange, non-monotonic covariances between inputs. We derive efficient inference using model whitening and marginalized posterior, and show with case studies that these kernels are necessary when modelling even rather simple time series, image or geospatial data with non-stationary characteristics.	[Remes, Sami; Heinonen, Markus; Kaski, Samuel] Aalto Univ, HIIT, Dept Comp Sci, Espoo, Finland	Aalto University	Remes, S (corresponding author), Aalto Univ, HIIT, Dept Comp Sci, Espoo, Finland.	sami.remes@aalto.fi; markus.o.heinonen@aalto.fi; samuel.kaski@aalto.fi	Kaski, Samuel/B-6684-2008; Heinonen, Markus/Q-1079-2016; Jeong, Yongwook/N-7413-2016	Kaski, Samuel/0000-0003-1925-9154; 	Finnish Funding Agency for Innovation; Academy of Finland [299915, 294238, 292334]	Finnish Funding Agency for Innovation; Academy of Finland(Academy of Finland)	This work has been partly supported by the Finnish Funding Agency for Innovation (project Re: Know) and Academy of Finland (COIN CoE, and grants 299915, 294238 and 292334). We acknowledge the computational resources provided by the Aalto Science-IT project.	Flaxman S., 2015, ICML, V2015; Gibbs MN., 1997, THESIS U CAMBRIDGE; Gramacy RB, 2008, J AM STAT ASSOC, V103, P1119, DOI 10.1198/016214508000000689; Grzegorczyk M, 2008, BIOINFORMATICS, V24, P2071, DOI 10.1093/bioinformatics/btn367; Heinonen M, 2016, JMLR WORKSH CONF PRO, V51, P732; Higdon D, 1999, BAYESIAN STATISTICS 6, P761; Huang NE, 1998, P ROY SOC A-MATH PHY, V454, P903, DOI 10.1098/rspa.1998.0193; Huang NE, 2008, REV GEOPHYS, V46, DOI 10.1029/2007RG000228; KAKIHARA Y, 1985, J MULTIVARIATE ANAL, V16, P140, DOI 10.1016/0047-259X(85)90055-7; Kom Samo Y.-L, 2015, TECHNICAL REPORT; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Loeve M, 1978, GRADUATE TEXTS MATH, V46; Paciorek CJ, 2006, ENVIRONMETRICS, V17, P483, DOI 10.1002/env.785; Paciorek CJ, 2004, ADV NEUR IN, V16, P273; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rioul O, 1991, IEEE SIGNAL PROC MAG, V8, P14, DOI 10.1109/79.91217; Robinson JW, 2009, ADV NEURAL INFORM PR, P1369; Saatci Y, 2011, THESIS; Sampson P., 1992, J AM STAT ASS, V87; SILVERMAN RA, 1957, IRE T INFORM THEOR, V3, P182, DOI 10.1109/TIT.1957.1057413; Sinha A., 2016, NIPS; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Tolvanen V., 2014, 2014 IEEE INT WORKSH, P1; Wilson A., 2013, ICML; Wilson A. G., 2014, NIPS; Wilson A.G., 2014, COVARIANCE KERNELS F; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; YAGLOM AM, 1987, SPRINGER SERIES STAT; Yang Zichao, 2015, AISTATS	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404069
C	Rowland, M; Weller, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rowland, Mark; Weller, Adrian			Uprooting and Rerooting Higher-Order Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The idea of uprooting and rerooting graphical models was introduced specifically for binary pairwise models by Weller [19] as a way to transform a model to any of a whole equivalence class of related models, such that inference on any one model yields inference results for all others. This is very helpful since inference, or relevant bounds, may be much easier to obtain or more accurate for some model in the class. Here we introduce methods to extend the approach to models with higher-order potentials and develop theoretical insights. In particular, we show that the triplet-consistent polytope TRI is unique in being 'universally rooted'. We demonstrate empirically that rerooting can significantly improve accuracy of methods of inference for higher-order models at negligible computational cost.	[Rowland, Mark; Weller, Adrian] Univ Cambridge, Cambridge, England; [Weller, Adrian] Alan Turing Inst, London, England	University of Cambridge	Rowland, M (corresponding author), Univ Cambridge, Cambridge, England.	mr504@cam.ac.uk; aw665@cam.ac.uk			UK Engineering and Physical Sciences Research Council (EPSRC) [EP/L016516/1]; Cambridge Centre for Analysis; Alan Turing Institute under the EPSRC [EP/N510129/1]; Leverhulme Trust via the CFI	UK Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Cambridge Centre for Analysis; Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Leverhulme Trust via the CFI	We thank Aldo Pacchiano for helpful discussions, and the anonymous reviewers for helpful comments. MR acknowledges support by the UK Engineering and Physical Sciences Research Council (EPSRC) grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, the Cambridge Centre for Analysis. AW acknowledges support by the Alan Turing Institute under the EPSRC grant EP/N510129/1, and by the Leverhulme Trust via the CFI.	BARAHONA F, 1988, OPER RES, V36, P493, DOI 10.1287/opre.36.3.493; Deza M., 1997, GEOMETRY CUTS METRIC; Djolonga J., 2015, ICML, P1804; Heskes T., 2003, P 19 ANN C UNC ART I, P313; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kolmogorov V, 2015, SIAM J COMPUT, V44, P1, DOI 10.1137/130945648; Mooij JM, 2010, J MACH LEARN RES, V11, P2169; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Rowland M., 2017, ARTIFICAL INTELLIGEN; SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036; SONTAG D, 2007, NIPS; Sontag D. A., 2007, THESIS; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright Martin J, 2006, IEEE T SIGNAL PROCES; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Weller A., 2015, AISTATS; Weller A., 2014, NIPS; Weller A., 2014, UNCERTAINTY ARTIFICI; Weller A, 2016, UAI; Weller A., 2016, INT C MACH LEARN ICM; Weller A., 2016, ARTIFICIAL INTELLIGE; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400020
C	Roy, A; Xu, H; Pokutta, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Roy, Aurko; Xu, Huan; Pokutta, Sebastian			Reinforcement Learning under Model Mismatch	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS	We study reinforcement learning under model misspecification, where we do not have access to the true environment but only to a reasonably close approximation to it. We address this problem by extending the framework of robust MDPs of [1, 15, 11] to the model free Reinforcement Learning setting, where we do not have access to the model parameters, but can only sample states from it. We define robust versions of Q-learning, SARSA, and TD-learning and prove convergence to an approximately optimal robust policy and approximate value function respectively. We scale up the robust algorithms to large MDPs via function approximation and prove convergence under two different settings. We prove convergence of robust approximate policy iteration and robust approximate value iteration for linear architectures (under mild assumptions). We also define a robust loss function, the mean squared robust projected Bellman error and give stochastic gradient descent algorithms that are guaranteed to converge to a local minimum.	[Roy, Aurko] Google, Mountain View, CA 94043 USA; [Xu, Huan; Pokutta, Sebastian] Georgia Inst Technol, ISyE, Atlanta, GA 30332 USA; [Roy, Aurko] Georgia Tech, Atlanta, GA USA	Google Incorporated; University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Roy, A (corresponding author), Google, Mountain View, CA 94043 USA.	aurkor@google.com; huan.xu@isye.gatech.edu; sebastian.pokutta@isye.gatech.edu	Jeong, Yongwook/N-7413-2016					Bagnell J.A., 2001, SOLVING UNCERTAIN MA; Bertsekas D. P., 1996, LIDSP2349 MIT LAB IN; Bertsekas DP, 2009, J COMPUT APPL MATH, V227, P27, DOI 10.1016/j.cam.2008.07.037; Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Boyan JA, 2002, MACH LEARN, V49, P233, DOI 10.1023/A:1017936530646; Brockman G., 2016, OPENAI GYM; Delage E, 2010, OPER RES, V58, P203, DOI 10.1287/opre.1080.0685; Iyengar GN, 2005, MATH OPER RES, V30, P257, DOI 10.1287/moor.1040.0129; Lim SH, 2013, ADV NEURAL INFORM PR, V26, P701; Morimoto J, 2005, NEURAL COMPUT, V17, P335, DOI 10.1162/0899766053011528; Nedic A, 2003, DISCRETE EVENT DYN S, V13, P79, DOI 10.1023/A:1022192903948; Nilim A., 2003, NIPS, P839; Pinto Lerrel, 2017, ARXIV170302702; Powell W. B., 2007, APPROXIMATE DYNAMIC, V703; Puterman M.L., 2014, MARKOV DECISION PROC; Shapiro A, 2002, OPTIM METHOD SOFTW, V17, P523, DOI 10.1080/1055678021000034008; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton R. S., 2009, ADV NEURAL INFORM PR, V21, P1609; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tamar A, 2014, ARXIV14043862; Tamar A., 2014, ICML, V32, P2014; Wiesemann W, 2013, MATH OPER RES, V38, P153, DOI 10.1287/moor.1120.0566	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403011
C	Roychowdhury, A; Parthasarathy, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Roychowdhury, Anirban; Parthasarathy, Srinivasan			Adaptive Bayesian Sampling with Monte Carlo EM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MAXIMUM-LIKELIHOOD; CONVERGENCE; ALGORITHMS	We present a novel technique for learning the mass matrices in samplers obtained from discretized dynamics that preserve some energy function. Existing adaptive samplers use Riemannian preconditioning techniques, where the mass matrices are functions of the parameters being sampled. This leads to significant complexities in the energy reformulations and resultant dynamics, often leading to implicit systems of equations and requiring inversion of high-dimensional matrices in the leapfrog steps. Our approach provides a simpler alternative, by using existing dynamics in the sampling step of a Monte Carlo EM framework, and learning the mass matrices in the M step with a novel online technique. We also propose a way to adaptively set the number of samples gathered in the E step, using sampling error estimates from the leapfrog dynamics. Along with a novel stochastic sampler based on Nose-Poincare dynamics, we use this framework with standard Hamiltonian Monte Carlo (HMC) as well as newer stochastic algorithms such as SGHMC and SGNHT, and show strong performance on synthetic and real high-dimensional sampling scenarios; we achieve sampling accuracies comparable to Riemannian samplers while being significantly faster.	[Roychowdhury, Anirban; Parthasarathy, Srinivasan] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Roychowdhury, A (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	roychowdhury.7@osu.edu; srini@cse.ohio-state.edu	Jeong, Yongwook/N-7413-2016	Parthasarathy, Srinivasan/0000-0002-6062-6449	National Science Foundation [DMS-1418265]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their insightful comments and suggestions. This material is based upon work supported by the National Science Foundation under Grant No. DMS-1418265. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	Bond SD, 1999, J COMPUT PHYS, V151, P114, DOI 10.1006/jcph.1998.6171; Booth JG, 1999, J ROY STAT SOC B, V61, P265, DOI 10.1111/1467-9868.00176; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; CHAN KS, 1995, J AM STAT ASSOC, V90, P242, DOI 10.2307/2291149; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; DIXON JD, 1982, NUMER MATH, V40, P137, DOI 10.1007/BF01459082; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Eberly Wayne, 2006, P 2006 INT S SYMB AL, P63, DOI [10.1145/1145768.1145785, DOI 10.1145/1145768.1145785]; Fort G, 2003, ANN STAT, V31, P1220; Frenkel D., 2001, UNDERSTANDING MOL SI; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; HOOVER WG, 1985, PHYS REV A, V31, P1695, DOI 10.1103/PhysRevA.31.1695; Jones A, 2011, J CHEM PHYS, V135, DOI 10.1063/1.3626941; Leimkuhler B., 2004, SIMULATING HAMILTONI; Leimkuhler B., 2015, INTERDISCIP APPL MAT; Levine RA, 2001, J COMPUT GRAPH STAT, V10, P422, DOI 10.1198/106186001317115045; McCulloch CE, 1997, J AM STAT ASSOC, V92, P162, DOI 10.2307/2291460; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Patterson S., 2013, P 26 INT C NEUR INF, P3102; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Robert CP, 1999, J STAT COMPUT SIM, V64, P327, DOI 10.1080/00949659908811984; Roychowdhury A., 2016, P 33 INT C MACH LEAR, P2673; Roychowdhury A., 2015, P 18 INT C ART INT S, P800; Sherman RP., 1999, ECONOMET J, V2, P248; Srivastava N., 2013, UAI 2013, P616; Uhler C, 2012, ANN STAT, V40, P238, DOI 10.1214/11-AOS957; WEI GCG, 1990, J AM STAT ASSOC, V85, P699, DOI 10.2307/2290005; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Yin L, 2006, J PHYS A-MATH GEN, V39, P8593, DOI 10.1088/0305-4470/39/27/003; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401029
C	Ruffini, M; Rabusseau, G; Balle, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ruffini, Matteo; Rabusseau, Guillaume; Balle, Borja			Hierarchical Methods of Moments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				TENSOR DECOMPOSITIONS	Spectral methods of moments provide a powerful tool for learning the parameters of latent variable models. Despite their theoretical appeal, the applicability of these methods to real data is still limited due to a lack of robustness to model misspecification. In this paper we present a hierarchical approach to methods of moments to circumvent such limitations. Our method is based on replacing the tensor decomposition step used in previous algorithms with approximate joint diagonalization. Experiments on topic modeling show that our method outperforms previous tensor decomposition methods in terms of speed and model quality.	[Ruffini, Matteo] Univ Politecn Cataluna, Barcelona, Spain; [Rabusseau, Guillaume] McGill Univ, Montreal, PQ, Canada; [Balle, Borja] Amazon Res, Cambridge, England	Universitat Politecnica de Catalunya; McGill University	Ruffini, M (corresponding author), Univ Politecn Cataluna, Barcelona, Spain.	mruffini@cs.upc.edu; guillaume.rabusseau@mail.mcgill.ca; pigem@amazon.co.uk			IVADO postdoctoral fellowship	IVADO postdoctoral fellowship	Guillaume Rabusseau acknowledges support of an IVADO postdoctoral fellowship. Borja Balle completed this work while at Lancaster University.	Anandkumar A., 2015, COLT, P36; Anandkumar Anima, 2012, NIPS; Anandkumar A, 2017, J MACH LEARN RES, V18, P1; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Anandkumar Animashree, 2012, COLT, V1, P4; Bailly Raphael, 2011, J MACHINE LEARNING R, P147; Balle B, 2014, PR MACH LEARN RES, V32, P1386; Balle Borja, 2012, NIPS, P2159; Balle Borja, 2012, ICML, P1819; BUNSEGERSTNER A, 1993, SIAM J MATRIX ANAL A, V14, P927, DOI 10.1137/0614062; Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Duchi J., 2008, PROC 25 INT C MACH L, P272; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Jain P., 2014, J MACH LEARN RES, V35, P824; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kulesza A, 2015, JMLR WORKSH CONF PRO, V38, P517; Kulesza A, 2014, JMLR WORKSH CONF PRO, V33, P522; Parikh A. P., 2011, P 28 INT C MACH LEAR, P1065; Perrone V, 2016, ARXIV161107460; Quattoni A, 2014, PR MACH LEARN RES, V32, P1710; Robeva Elina Mihaylova, 2016, THESIS; Ruffini Matteo, 2016, ARXIV161203409; Savaresi S.M., 2001, P 2001 SIAM INT C DA, P1, DOI DOI 10.1137/1.9781611972719.5; Sievert C., 2014, ACL WORKSH INT LANG; Steinbach M, 2000, P KDD WORKSH TEXT MI, V400, P525; van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401090
C	Ryabko, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ryabko, Daniil			Independence clustering (without a matrix)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The independence clustering problem is considered in the following formulation: given a set S of random variables, it is required to find the finest partitioning {U-1,...,U-k} of S into clusters such that the clusters U-1,...,U-k are mutually independent. Since mutual independence is the target, pairwise similarity measurements are of no use, and thus traditional clustering algorithms are inapplicable. The distribution of the random variables in S is, in general, unknown, but a sample is available. Thus, the problem is cast in terms of time series. Two forms of sampling are considered: i.i.d. and stationary time series, with the main emphasis being on the latter, more general, case. A consistent, computationally tractable algorithm for each of the settings is proposed, and a number of fascinating open directions for further research are outlined.	[Ryabko, Daniil] INRIA Lillle, 40 Ave Halley, Villeneuve Dascq, France		Ryabko, D (corresponding author), INRIA Lillle, 40 Ave Halley, Villeneuve Dascq, France.	daniil@ryabko.net	Jeong, Yongwook/N-7413-2016					Bach F.R., 2003, J MACHINE LEARNING R, V4, P1205; Balcan MF, 2014, J MACH LEARN RES, V15, P3831; Beirlant J., 1997, INT J MATH STAT SCI, V6, P17; Benjaminsson S, 2010, FRONT SYST NEUROSCI, V4, DOI 10.3389/fnsys.2010.00034; Cover TM, 2006, ELEMENTS INFORM THEO; Gray R., 1988, PROBABILITY RANDOM P; Gyorfi Laszlo, 2011, COMMUNICATION; JIROUSEK R, 1991, KYBERNETIKA, V27, P403; Kandasamy K., 2014, ARXIV14114342; Khaleghi A, 2016, J MACH LEARN RES, V17; Kolchinsky A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00066; Kraskov A, 2005, EUROPHYS LETT, V70, P278, DOI 10.1209/epl/i2004-10483-y; Mantegna RN, 1999, EUR PHYS J B, V11, P193, DOI 10.1007/s100510050929; Marrelec G, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137278; Marti Gautier, 2016, IJCAI 16; Meek C, 2001, J ARTIF INTELL RES, V15, P383, DOI 10.1613/jair.914; Priness I, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-111; Ryabko D., 2010, P 27 INT C MACH LEAR, P919; Ryabko D., 2017, INT C ALG LEARN THEO, P400; Ryabko D, 2012, TEST-SPAIN, V21, P317, DOI 10.1007/s11749-011-0245-3; Ryabko D, 2010, J THEOR PROBAB, V23, P565, DOI 10.1007/s10959-009-0263-1; Ryabko D, 2010, IEEE T INFORM THEORY, V56, P1430, DOI 10.1109/TIT.2009.2039169; Shields PC, 1998, IEEE T INFORM THEORY, V44, P2079, DOI 10.1109/18.720532; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Zhang K., 2011, P 27 ANN C UNC ART I; Zhou XB, 2004, J COMPUT BIOL, V11, P147, DOI 10.1089/106652704773416939	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404009
C	Schutt, KT; Kindermans, PJ; Sauceda, HE; Chmiela, S; Tkatchenko, A; Muller, KR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Schuett, K. T.; Kindermans, P-J; Sauceda, H. E.; Chmiela, S.; Tkatchenko, A.; Mueller, K-R			SchNet: A continuous-filter convolutional neural network for modeling quantum interactions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep learning has the potential to revolutionize quantum chemistry as it is ideally suited to learn representations for structured data and speed up the exploration of chemical space. While convolutional neural networks have proven to be the first choice for images, audio and video data, the atoms in molecules are not restricted to a grid. Instead, their precise locations contain essential physical information, that would get lost if discretized. Thus, we propose to use continuous-filter convolutional layers to be able to model local correlations without requiring the data to lie on a grid. We apply those layers in SchNet: a novel deep learning architecture modeling quantum interactions in molecules. We obtain a joint model for the total energy and interatomic forces that follows fundamental quantumchemical principles. Our architecture achieves state-of-the-art performance for benchmarks of equilibrium molecules and molecular dynamics trajectories. Finally, we introduce a more challenging benchmark with chemical and structural variations that suggests the path for further work.	[Schuett, K. T.; Kindermans, P-J; Chmiela, S.; Mueller, K-R] Tech Univ Berlin, Machine Learning Grp, Berlin, Germany; [Sauceda, H. E.] Max Planck Gesell, Fritz Haber Inst, Theory Dept, Berlin, Germany; [Tkatchenko, A.] Univ Luxembourg, Phys & Mat Sci Res Unit, Luxembourg, Luxembourg; [Mueller, K-R] Max Planck Inst Informat, Saarbrucken, Germany; [Mueller, K-R] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea	Technical University of Berlin; Max Planck Society; Fritz Haber Institute of the Max Planck Society; University of Luxembourg; Max Planck Society; Korea University	Schutt, KT (corresponding author), Tech Univ Berlin, Machine Learning Grp, Berlin, Germany.	kristof.schuett@tu-berlin.de; klaus-robert.mueller@tu-berlin.de	Mueller, Klaus-Robert/Y-3547-2019; Schütt, Kristof T/Q-2604-2017; Tkatchenko, Alexandre/E-7148-2011	Mueller, Klaus-Robert/0000-0002-3861-7685; Schütt, Kristof T/0000-0001-8342-0964; Tkatchenko, Alexandre/0000-0002-1012-4854	Federal Ministry of Education and Research (BMBF) [01IS14013A]; DFG [MU 987/20-1]; European Union [657679]; Korean National Research Foundation [2012-005741]; Korea government [2017-0-00451]	Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); DFG(German Research Foundation (DFG)); European Union(European Commission); Korean National Research Foundation(National Research Foundation of Korea); Korea government(Korean Government)	This work was supported by the Federal Ministry of Education and Research (BMBF) for the Berlin Big Data Center BBDC (01IS14013A). Additional support was provided by the DFG (MU 987/20-1) and from the European Union's Horizon 2020 research and innovation program under the Marie Sklodowska-Curie grant agreement NO 657679. K.R.M. gratefully acknowledges the BK21 program funded by Korean National Research Foundation grant (No. 2012-005741) and the Institute for Information & Communications Technology Promotion (IITP) grant funded by the Korea government (no. 2017-0-00451).	[Anonymous], 2015, DEEP CONVOLUTIONAL N; [Anonymous], 2014, ICLR; Bartok AP, 2013, PHYS REV B, V87, DOI 10.1103/PhysRevB.87.184115; Bartok AP, 2010, PHYS REV LETT, V104, DOI 10.1103/PhysRevLett.104.136403; Behler J, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.146401; Behler J, 2011, J CHEM PHYS, V134, DOI 10.1063/1.3553717; Boomsma W., 2017, ADV NEURAL INFORM PR, V30, P3433; Brabandere B.D., 2016, ADV NEURAL INFORM PR, P667; Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015; Chollet F., 2017, P IEEE C COMPUTER VI, P1251, DOI DOI 10.1109/CVPR.2017.195; Duvenaud David K, 2015, P NIPS; Erickenberg M., 2017, ADV NEURAL INFORM PR, P6543; Faber F. A., 2017, ARXIV170205532; Gilmer J, 2017, PR MACH LEARN RES, V70; Hansen K, 2015, J PHYS CHEM LETT, V6, P2326, DOI 10.1021/acs.jpclett.5b00831; Hansen K, 2013, J CHEM THEORY COMPUT, V9, P3404, DOI 10.1021/ct400195d; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hirn M, 2017, MULTISCALE MODEL SIM, V15, P827, DOI 10.1137/16M1075454; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Malshe R., 2009, J CHEM PHYS, V130; Manzhos S, 2006, J CHEM PHYS, V125, DOI 10.1063/1.2336223; Masci J., 2015, P IEEE INT C COMP VI, P37; Max-Moerbeck W, 2014, MON NOT R ASTRON SOC, V445, P437, DOI 10.1093/mnras/stu1707; Nieto-Barajas LE, 2015, STOCH ENV RES RISK A, V29, P577, DOI 10.1007/s00477-014-0894-3; Olafsdottir KB, 2016, COMPUT GEOSCI-UK, V91, P11, DOI 10.1016/j.cageo.2016.03.001; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Reymond JL, 2015, ACCOUNTS CHEM RES, V48, P722, DOI 10.1021/ar500432k; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Snyder JC, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.253002; Tkatchenko A, 2012, PHYS REV LETT, V108, DOI [10.1103/PhysRevLett.108.058301, 10.1103/PhysRevLett.108.236402]; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289	41	0	0	8	26	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401004
C	Scieur, D; Roulet, V; Bach, F; d'Aspremont, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Scieur, Damien; Roulet, Vincent; Bach, Francis; d'Aspremont, Alexandre			Integration Methods and Optimization Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We show that accelerated optimization methods can be seen as particular instances of multi-step integration schemes from numerical analysis, applied to the gradient flow equation. Compared with recent advances in this vein, the differential equation considered here is the basic gradient flow, and we derive a class of multi-step schemes which includes accelerated algorithms, using classical conditions from numerical analysis. Multi-step schemes integrate the differential equation using larger step sizes, which intuitively explains the acceleration phenomenon.	[Scieur, Damien; Roulet, Vincent; Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France; [d'Aspremont, Alexandre] PSL Res Univ, CNRS, ENS, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Scieur, D (corresponding author), PSL Res Univ, ENS, INRIA, Paris, France.	damien.scieur@inria.fr; vincent.roulet@inria.fr; francis.bach@inria.fr; aspremon@ens.fr			European Research Council (ERC project SIPA); European Union [607290 SpaRTaN]; AMX fellowship; chaire Economie des nouvelles donnees; fonds AXA pour la recherche	European Research Council (ERC project SIPA); European Union(European Commission); AMX fellowship; chaire Economie des nouvelles donnees; fonds AXA pour la recherche(AXA Research Fund)	The authors would like to acknowledge support from a starting grant from the European Research Council (ERC project SIPA), from the European Union's Seventh Framework Programme (FP7-PEOPLE-2013-ITN) under grant agreement number 607290 SpaRTaN, an AMX fellowship, as well as support from the chaire Economie des nouvelles donnees with the data science joint research initiative with the fonds AXA pour la recherche and a gift from Societe Generale Cross Asset Quantitative Research.	Allen Zhu Z., 2017, P 8 INN THEOR COMP S; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bubeck S., 2015, ARXIV E PRINTS; Diakonikolas J., 2017, ARXIV170604680; Duchi J. C., 2010, COLT, P14; Gautschi W., 2011, NUMERICAL ANAL; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Lessard L, 2016, SIAM J OPTIMIZ, V26, P57, DOI 10.1137/15M1009597; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2007, GRADIENT METHODS MIN; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Suli E., 2003, INTRO NUMERICAL ANAL; Taylor A. B, 2017, THESIS; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Wilson A. C., 2016, ARXIV161102635	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401015
C	Sharan, V; Kakade, S; Liang, P; Valiant, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sharan, Vatsal; Kakade, Sham; Liang, Percy; Valiant, Gregory			Learning Overcomplete HMMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				IDENTIFIABILITY; PARAMETERS; MODELS	We study the problem of learning overcomplete HMMs-those that have many hidden states but a small output alphabet. Despite having significant practical importance, such HMMs are poorly understood with no known positive or negative results for efficient learning. In this paper, we present several new results-both positive and negative-which help define the boundaries between the tractable and intractable settings. Specifically, we show positive results for a large subclass of HMMs whose transition matrices are sparse, well-conditioned, and have small probability mass on short cycles. On the other hand, we show that learning is impossible given only a polynomial number of samples for HMMs with a small output alphabet and whose transition matrices are random regular graphs with large degree. We also discuss these results in the context of learning HMMs which can capture long-term dependencies.	[Sharan, Vatsal; Liang, Percy; Valiant, Gregory] Stanford Univ, Stanford, CA 94305 USA; [Kakade, Sham] Univ Washington, Seattle, WA 98195 USA	Stanford University; University of Washington; University of Washington Seattle	Sharan, V (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	vsharan@stanford.edu; sham@cs.washington.edu; pliang@cs.stanford.edu; valiant@stanford.edu						Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689; Anandkumar A., 2015, COLT, P36; Anandkumar A., 2013, TENSOR DECOMPOSITION; Anandkumar Animashree, 2012, COLT, V1, P4; Bhaskara A., 2013, CORR; Bhaskara A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P594, DOI 10.1145/2591796.2591881; BLACKWELL D, 1957, ANN MATH STAT, V28, P1011, DOI 10.1214/aoms/1177706802; BLISCHKE WR, 1964, J AM STAT ASSOC, V59, P510, DOI 10.2307/2283005; Chang JT, 1996, MATH BIOSCI, V137, P51, DOI 10.1016/S0025-5564(96)00075-2; Flaxman A, 2004, ELECTRON J COMB, V11; Friedman J., 2003, P 35 ACM S THEOR COM, P720; Ghahramani Z., 1997, MACH LEARN, V1, P31; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Huang QQ, 2016, IEEE T SIGNAL PROCES, V64, P1896, DOI 10.1109/TSP.2015.2510969; ITO H, 1992, IEEE T INFORM THEORY, V38, P324, DOI 10.1109/18.119690; Kakade Sham, 2016, ARXIV161202526; Krivelevich M, 2001, RANDOM STRUCT ALGOR, V18, P346, DOI 10.1002/rsa.1013; Kruskal J. B., 1977, LINEAR ALGEBRA ITS A, V18; LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071; Mossel Elchanan, 2005, P 37 ANN ACM S THEOR, P366; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Shamir E., 1984, N HOLLAND MATH STUD, V87, P271; Weiss R, 2015, PR MACH LEARN RES, V37, P635	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400090
C	Shen, J; Li, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shen, Jie; Li, Ping			Partial Hard Thresholding: Towards A Principled Analysis of Support Recovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ORTHOGONAL MATCHING PURSUIT; SIGNAL RECOVERY; SPARSE RECOVERY; SELECTION; CONSISTENCY; REGRESSION; ALGORITHM	In machine learning and compressed sensing, it is of central importance to understand when a tractable algorithm recovers the support of a sparse signal from its compressed measurements. In this paper, we present a principled analysis on the support recovery performance for a family of hard thresholding algorithms. To this end, we appeal to the partial hard thresholding (PHT) operator proposed recently by Jain et al. [IEEE Trans. Information Theory, 2017]. We show that under proper conditions, PHT recovers an arbitrary s-sparse signal within O(s kappa log kappa) iterations where kappa is an appropriate condition number. Specifying the PHT operator, we obtain the best known results for hard thresholding pursuit and orthogonal matching pursuit with replacement. Experiments on the simulated data complement our theoretical findings and also illustrate the effectiveness of PHT.	[Shen, Jie] Rutgers State Univ, Sch Arts & Sci, Dept Comp Sci, Piscataway, NJ 08854 USA; [Li, Ping] Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, Piscataway, NJ USA	Rutgers State University New Brunswick; Rutgers State University New Brunswick	Shen, J (corresponding author), Rutgers State Univ, Sch Arts & Sci, Dept Comp Sci, Piscataway, NJ 08854 USA.	js2007@rutgers.edu; pingli@stat.rutgers.edu			 [NSF-Bigdata-1419210];  [NSF-III-1360971]	; 	The work is supported in part by NSF-Bigdata-1419210 and NSF-III-1360971. We thank the anonymous reviewers for valuable comments.	Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; Bahmani S, 2013, J MACH LEARN RES, V14, P807; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Bouchot JL, 2016, APPL COMPUT HARMON A, V41, P412, DOI 10.1016/j.acha.2016.03.002; Cai TT, 2011, IEEE T INFORM THEORY, V57, P4680, DOI 10.1109/TIT.2011.2146090; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006; Daubechies I, 2004, COMMUN PUR APPL MATH, V57, P1413, DOI 10.1002/cpa.20042; Foucart S., 2013, APPL NUMERICAL HARMO; Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278; Jain P., 2011, ADV NEURAL INF PROCE, P1215; Jain P, 2017, IEEE T INFORM THEORY, V63, P3029, DOI 10.1109/TIT.2017.2686880; Kar P., 2014, ADV NEURAL INFORM PR, P685; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Osher S, 2016, APPL COMPUT HARMON A, V41, P436, DOI 10.1016/j.acha.2016.01.002; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201; Shen J., 2016, ARXIV160501656; Shen J, 2017, PR MACH LEARN RES, V70; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Tropp JA, 2010, P IEEE, V98, P948, DOI 10.1109/JPROC.2010.2044010; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Wang J, 2016, IEEE T SIGNAL PROCES, V64, P1076, DOI 10.1109/TSP.2015.2498132; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Yuan M, 2007, J R STAT SOC B, V69, P143, DOI 10.1111/j.1467-9868.2007.00581.x; Yuan X., 2016, P 30 ANN C NEUR INF, P3558; Yuan XT, 2014, PR MACH LEARN RES, V32, P127; Zhang T, 2011, IEEE T INFORM THEORY, V57, P6215, DOI 10.1109/TIT.2011.2162263; Zhang T, 2009, J MACH LEARN RES, V10, P555; Zhang T, 2009, ANN STAT, V37, P2109, DOI 10.1214/08-AOS659; Zhao P, 2006, J MACH LEARN RES, V7, P2541	35	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403019
C	Srinivasa, C; Givoni, I; Ravanbakhsh, S; Frey, BJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Srinivasa, Christopher; Givoni, Inmar; Ravanbakhsh, Siamak; Frey, Brendan J.			Min-Max Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the application of min-max propagation, a variation of belief propagation, for approximate min-max inference in factor graphs. We show that for "any" high-order function that can be minimized in O (omega), the min-max message update can be obtained using an efficient O (K (omega + log (K)) procedure, where K is the number of variables. We demonstrate how this generic procedure, in combination with efficient updates for a family of high-order constraints, enables the application of min-max propagation to efficiently approximate the NP-hard problem of makespan minimization, which seeks to distribute a set of tasks on machines, such that the worst case load is minimized.	[Srinivasa, Christopher] Univ Toronto, Borealis, Toronto, ON, Canada; [Givoni, Inmar] Univ Toronto, Toronto, ON, Canada; [Ravanbakhsh, Siamak] Univ British Columbia, Vancouver, BC, Canada; [Frey, Brendan J.] Univ Toronto, Vector Inst, Deep Genom, Toronto, ON, Canada	University of Toronto; University of Toronto; University of British Columbia; University of Toronto	Srinivasa, C (corresponding author), Univ Toronto, Borealis, Toronto, ON, Canada.	christopher.srinivasa@gmail.com; inmar.givoni@gmail.com; siamakx@cs.ubc.ca; frey@psi.toronto.edu						Aji SM, 2000, IEEE T INFORM THEORY, V46, P325, DOI 10.1109/18.825794; Behera D.K., 2012, ADV MATER RES-KR, V488-489, P1708; Behera DK, 2012, LECT NOTES MECH ENG, P373, DOI DOI 10.1007/978-81-322-1007-8_34; Bishop C.M, 2006, PATTERN RECOGN; Edmonds J., 1970, J COMB THEORY, V8, P299; GAIL MH, 1981, BIOMETRIKA, V68, P703, DOI 10.1093/biomet/68.3.703; Garey M., 1979, COMPUTERS INTRACTABI, V174; GRAHAM RL, 1966, AT&T TECH J, V45, P1563, DOI 10.1002/j.1538-7305.1966.tb01709.x; Gupta JND, 2001, PROD PLAN CONTROL, V12, P28, DOI 10.1080/09537280150203951; Gupta R., 2007, P 24 INT C MACH LEAR, P329; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Pinedo ML, 2012, SCHEDULING: THEORY, ALGORITHMS, AND SYSTEMS, FOURTH EDITION, P1, DOI 10.1007/978-1-4614-2361-4; Potetz B, 2008, COMPUT VIS IMAGE UND, V112, P39, DOI 10.1016/j.cviu.2008.05.007; Ravanbakhsh S., 2014, P 31 INT C MACH LEAR; Ravanbakhsh S, 2015, J MACH LEARN RES, V16, P1249; Tarlow Daniel, 2010, INT C ART INT STAT, P812; Vinyals M., 2013, COMPUTER J	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405063
C	Stich, SU; Raj, A; Jaggi, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Stich, Sebastian U.; Raj, Anant; Jaggi, Martin			Safe Adaptive Importance Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DESCENT METHOD; COORDINATE; EFFICIENCY	Importance sampling has become an indispensable strategy to speed up optimization algorithms for large-scale applications. Improved adaptive variants-using importance values defined by the complete gradient information which changes during optimization-enjoy favorable theoretical properties, but are typically computationally infeasible. In this paper we propose an efficient approximation of gradient-based sampling, which is based on safe bounds on the gradient. The proposed sampling distribution is (i) provably the best sampling with respect to the given bounds, (ii) always better than uniform sampling and fixed importance sampling and (iii) can efficiently be computed-in many applications at negligible extra cost. The proposed sampling scheme is generic and can easily be integrated into existing algorithms. In particular, we show that coordinate-descent (CD) and stochastic gradient descent (SGD) can enjoy significant a speed-up under the novel scheme. The proven efficiency of the proposed sampling is verified by extensive numerical testing.	[Stich, Sebastian U.; Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Raj, Anant] Max Planck Inst Intelligent Syst, Stuttgart, Germany	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Max Planck Society	Stich, SU (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	sebastian.stich@epfl.ch; anant.raj@tuebingen.mpg.de; martin.jaggi@epfl.ch	Jeong, Yongwook/N-7413-2016					Alain Guillaume, 2015, VARIANCE REDUCTION S; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Boyd S, 2004, CONVEX OPTIMIZATION; Csiba D., 2016, IMPORTANCE SAMPLING; Csiba Dominik, 2015, ICML 2015; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; He Xi, 2015, DUAL FREE ADAPTIVE M; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; Komiya H., 1988, KODAI MATH J, P5; Lacoste-Julien S., 2012, SIMPLER APPROACH OBT; Liu J, 2014, P 31 INT C MACH LEAR, P289, DOI DOI 10.1109/TSP.2015.2447503; Ndiaye Eugene, 2017, JMLR; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Y, 2017, SIAM J OPTIMIZ, V27, P110, DOI 10.1137/16M1060182; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Osokin A, 2016, PR MACH LEARN RES, V48; Papa G, 2015, LECT NOTES ARTIF INT, V9355, P317, DOI 10.1007/978-3-319-24486-0_21; Perekrestenko D, 2017, PR MACH LEARN RES, V54, P869; Qu Z, 2014, RANDOMIZED DUAL COOR; Richtarik P, 2016, OPTIM LETT, V10, P1233, DOI 10.1007/s11590-015-0916-1; Schmidt M, 2015, JMLR WORKSH CONF PRO, V38, P819; Shalev-Shwartz S, 2011, J MACH LEARN RES, V12, P1865; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shibagaki Ichiro Takeuchi Atsushi, 2017, STOCHASTIC PRIMAL DU; Sion M., 1958, PAC J MATH, V8, P171, DOI [10.2140/pjm.1958.8.171, DOI 10.2140/PJM.1958.8.171]; Stich SU, 2016, MATH PROGRAM, V156, P549, DOI 10.1007/s10107-015-0908-z; Stich SU, 2017, PR MACH LEARN RES, V70; Strohmer T, 2009, J FOURIER ANAL APPL, V15, P262, DOI 10.1007/s00041-008-9030-4; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Zhao PL, 2015, PR MACH LEARN RES, V37, P1; Zhu R., 2016, ADV NEURAL INFORM PR, P406	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404044
C	Su, QL; Liao, XJ; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Su, Qinliang; Liao, Xuejun; Carin, Lawrence			A Probabilistic Framework for Nonlinearities in Stochastic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				LEARNING ALGORITHM; BELIEF; SIMULATION	We present a probabilistic framework for nonlinearities, based on doubly truncated Gaussian distributions. By setting the truncation points appropriately, we are able to generate various types of nonlinearities within a unified framework, including sigmoid, tanh and ReLU, the most commonly used nonlinearities in neural networks. The framework readily integrates into existing stochastic neural networks (with hidden units characterized as random variables), allowing one for the first time to learn the nonlinearities alongside model weights in these networks. Extensive experiments demonstrate the performance improvements brought about by the proposed framework when integrated with the restricted Boltzmann machine (RBM), temporal RBM and the truncated Gaussian graphical model (TGGM).	[Su, Qinliang; Liao, Xuejun; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Duke University	Su, QL (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	qs15@duke.edu; xjliao@duke.edu; lcarin@duke.edu	Jeong, Yongwook/N-7413-2016	Carin, Lawrence/0000-0001-6277-7948	DOE; NGA; NSF; ONR; Accenture	DOE(United States Department of Energy (DOE)); NGA; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Accenture	The research reported here was supported by the DOE, NGA, NSF, ONR and by Accenture.	ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Adams R., 2015, P 32 INT C MACH LEAR; Agostinelli F., 2014, CORR; [Anonymous], 1994, CONTINUOUS UNIVARIAT; Carlson D.E., 2015, ADV NEURAL INFORM PR, P2971; Chopin N, 2011, STAT COMPUT, V21, P275, DOI 10.1007/s11222-009-9168-1; Eisenach Carson, 2017, ICLR; Frey BJ, 1997, ADV NEUR IN, V9, P452; Frey BJ, 1999, NEURAL COMPUT, V11, P193, DOI 10.1162/089976699300016872; Ghosh S, 2016, AAAI CONF ARTIF INTE, P1589; Goodfellow Ian, 2013, INT C MACH LEARN ICM, P4; Gulcehre Caglar, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P530, DOI 10.1007/978-3-662-44848-9_34; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Marlin BM., 2010, P 13 INT C ART INT S, V9, P509; Mittelman R, 2014, PR MACH LEARN RES, V32, P1647; Nair V., 2010, ICML, P807; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Ravanbakhsh Siamak, 2016, AISTATS, V1050, P14; ROBERT CP, 1995, STAT COMPUT, V5, P121, DOI 10.1007/BF00143942; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Soudry D., 2014, PROC 27 INT C NEURAL, P963, DOI DOI 10.5555/2968826.2968934; Su Q., 2016, P 33 INT C MACH LEAR; Su QL, 2015, IEEE T SIGNAL PROCES, V63, P6258, DOI 10.1109/TSP.2015.2465303; Su QL, 2015, IEEE T SIGNAL PROCES, V63, P1144, DOI 10.1109/TSP.2015.2389755; Su QL, 2014, IEEE T SIGNAL PROCES, V62, P5119, DOI 10.1109/TSP.2014.2345635; Su Qinliang, 2016, 31 NAT C ART INT AAA; Sutskever Ilya, 2009, ADV NEURAL INFORM PR, P2; Sutskever Ilya, 2007, AISTATS; Welling M., 2004, ADV NEURAL INFORM PR, V17, P1481	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404054
C	Suarez, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Suarez, Joseph			Character-Level Language Modeling with Recurrent Highway Hypernetworks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present extensive experimental and theoretical support for the efficacy of recurrent highway networks (RHNs) and recurrent hypernetworks complimentary to the original works. Where the original RHN work primarily provides theoretical treatment of the subject, we demonstrate experimentally that RHNs benefit from far better gradient flow than LSTMs in addition to their improved task accuracy. The original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved-we consider these in depth and frame several feasible solutions that we believe will yield further gains in the future. We demonstrate that these approaches are complementary: by combining RHNs and hypernetworks, we make a significant improvement over current state-of-the-art character-level language modeling performance on Penn Treebank while relying on much simpler regularization. Finally, we argue for RHNs as a drop-in replacement for LSTMs (analogous to LSTMs for vanilla RNNs) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.	[Suarez, Joseph] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Suarez, J (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	joseph15@stanford.edu	Jeong, Yongwook/N-7413-2016					Britz Denny, 2017, ARXIV170303906; Chung J., 2014, ARXIV14123555; Cooijmans T., 2016, ARXIV160309025; Dauphin Yann N., 2016, CORR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Greff K., 2015, ARXIV150304069, DOI 10.1109/TNNLS.2016.2582924; Ha David, 2016, ARXIV160909106; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jozefowicz Rafal, 2016, ARXIV160202410; Kalchbrenner Nal, 2016, ARXIV161010099; Kingma D.P, P 3 INT C LEARNING R; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Radford A., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1704.01444; Semeniuta S., 2016, P COLING 2016 26 INT, P1757; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Zilly J.G., 2016, ARXIV PREPRINT ARXIV	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403033
C	Sun, T; Hannah, R; Yin, WT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sun, Tao; Hannah, Robert; Yin, Wotao			Asynchronous Coordinate Descent under More Realistic Assumption	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MODELS	Asynchronous-parallel algorithms have the potential to vastly speed up algorithms by eliminating costly synchronization. However, our understanding of these algorithms is limited because the current convergence theory of asynchronous block coordinate descent algorithms is based on somewhat unrealistic assumptions. In particular, the age of the shared optimization variables being used to update blocks is assumed to be independent of the block being updated. Additionally, it is assumed that the updates are applied to randomly chosen blocks. In this paper, we argue that these assumptions either fail to hold or will imply less efficient implementations. We then prove the convergence of asynchronous-parallel block coordinate descent under more realistic assumptions, in particular, always without the independence assumption. The analysis permits both the deterministic (essentially) cyclic and random rules for block choices. Because a bound on the asynchronous delays may or may not be available, we establish convergence for both bounded delays and unbounded delays. The analysis also covers nonconvex, weakly convex, and strongly convex functions. The convergence theory involves a Lyapunov function that directly incorporates both objective progress and delays. A continuous-time ODE is provided to motivate the construction at a high level.	[Sun, Tao] Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China; [Hannah, Robert; Yin, Wotao] Univ Calif Los Angeles, Los Angeles, CA 90095 USA	National University of Defense Technology - China; University of California System; University of California Los Angeles	Sun, T (corresponding author), Natl Univ Def Technol, Changsha 410073, Hunan, Peoples R China.	nudtsuntao@163.com; RobertHannah89@math.ucla.edu; wotaoyin@math.ucla.edu			National Key R&D Program of China [2017YFB0202902]; China Scholarship Council; NSF [DMS-1720237]; ONR [N000141712162]	National Key R&D Program of China; China Scholarship Council(China Scholarship Council); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	The work is supported in part by the National Key R&D Program of China 2017YFB0202902, China Scholarship Council, NSF DMS-1720237, and ONR N000141712162	Cannelli L., 2016, ARXIV160704818; Cannelli L., 2017, ARXIV170104900; Chow Yat Tin, 2017, SIAM J SCI COMPUTING; Davis D., 2016, ARXIV160400526; Davis D, 2016, SCI COMPUT, P115, DOI 10.1007/978-3-319-41589-5_4; De Sa C., 2015, NIPS, P2674; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Hannah R., 2017, ARXIV170805136; Hannah R., 2016, ARXIV160904746; Lai MJ, 2013, SIAM J IMAGING SCI, V6, P1059, DOI 10.1137/120863290; Leblond R, 2017, PR MACH LEARN RES, V54, P46; Liu J, 2015, J MACH LEARN RES, V16, P285; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Mania H., 2015, ARXIV150706970; Peng Z., 2016, ARXIV161204425; Peng ZM, 2016, SIAM J SCI COMPUT, V38, pA2851, DOI 10.1137/15M1024950; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Sun Ruoyu, 2017, ARXIV160407130; Xu Yangyang, 2017, ARXIV170506391	20	0	0	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406025
C	Sundaresan, M; Nabeel, A; Sridharan, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Sundaresan, Mali; Nabeel, Arshed; Sridharan, Devarajan			Mapping distinct timescales of functional interactions among brain networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRANGER CAUSALITY; LINEAR-DEPENDENCE; FEEDBACK; FMRI; DYNAMICS; TOOLBOX	Brain processes occur at various timescales, ranging from milliseconds (neurons) to minutes and hours (behavior). Characterizing functional coupling among brain regions at these diverse timescales is key to understanding how the brain produces behavior. Here, we apply instantaneous and lag-based measures of conditional linear dependence, based on Granger-Geweke causality (GC), to infer network connections at distinct timescales from functional magnetic resonance imaging (fMRI) data. Due to the slow sampling rate of fMRI, it is widely held that GC produces spurious and unreliable estimates of functional connectivity when applied to fMRI data. We challenge this claim with simulations and a novel machine learning approach. First, we show, with simulated fMRI data, that instantaneous and lag-based GC identify distinct timescales and complementary patterns of functional connectivity. Next, we analyze fMRI scans from 500 subjects and show that a linear classifier trained on either instantaneous or lag-based GC connectivity reliably distinguishes task versus rest brain states, with similar to 80-85% cross-validation accuracy. Importantly, instantaneous and lag-based GC exploit markedly different spatial and temporal patterns of connectivity to achieve robust classification. Our approach enables identifying functionally connected networks that operate at distinct timescales in the brain.	[Sundaresan, Mali; Sridharan, Devarajan] Indian Inst Sci, Ctr Neurosci, Bangalore, Karnataka, India; [Nabeel, Arshed; Sridharan, Devarajan] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore, Karnataka, India	Indian Institute of Science (IISC) - Bangalore; Indian Institute of Science (IISC) - Bangalore	Sridharan, D (corresponding author), Indian Inst Sci, Ctr Neurosci, Bangalore, Karnataka, India.; Sridharan, D (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore, Karnataka, India.	s.malisundar@gmail.com; arshed@iisc.ac.in; sridhar@iisc.ac.in	Jeong, Yongwook/N-7413-2016	Devarajan, Sridharan/0000-0003-1998-9018	Wellcome Trust DBT-India Alliance Intermediate Fellowship, a SERB Early Career Research award; Wellcome Trust DBT-India Alliance Intermediate Fellowship; SERB Early Career Research award; Pratiksha Trust Young Investigator award; DBT-IISc Partnership program; Tata Trusts grant	Wellcome Trust DBT-India Alliance Intermediate Fellowship, a SERB Early Career Research award; Wellcome Trust DBT-India Alliance Intermediate Fellowship(Wellcome Trust DBT India Alliance); SERB Early Career Research award(Department of Science & Technology (India)Science Engineering Research Board (SERB), India); Pratiksha Trust Young Investigator award; DBT-IISc Partnership program; Tata Trusts grant	This research was supported by a Wellcome Trust DBT-India Alliance Intermediate Fellowship, a SERB Early Career Research award, a Pratiksha Trust Young Investigator award, a DBT-IISc Partnership program grant, and a Tata Trusts grant (all to DS). We would like to thank Hritik Jain for help with data analysis.	Barnett L, 2017, J NEUROSCI METH, V275, P93, DOI 10.1016/j.jneumeth.2016.10.016; Barnett L, 2014, J NEUROSCI METH, V223, P50, DOI 10.1016/j.jneumeth.2013.10.018; Bastos AM, 2015, NEURON, V85, P390, DOI 10.1016/j.neuron.2014.12.018; Chang C, 2008, NEUROIMAGE, V43, P90, DOI 10.1016/j.neuroimage.2008.06.030; De Martino F, 2008, NEUROIMAGE, V43, P44, DOI 10.1016/j.neuroimage.2008.06.037; Dhamala M, 2008, NEUROIMAGE, V41, P354, DOI 10.1016/j.neuroimage.2008.02.020; Friston KJ, 2000, NEUROIMAGE, V12, P466, DOI 10.1006/nimg.2000.0630; Ganguli S, 2008, NEURON, V58, P15, DOI 10.1016/j.neuron.2008.01.038; Gel'fand I. M., 1959, AM MATH SOC TRANSL 2, V12, P199; GEWEKE J, 1982, J AM STAT ASSOC, V77, P304, DOI 10.2307/2287238; Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127; Murray JD, 2014, NAT NEUROSCI, V17, P1661, DOI 10.1038/nn.3862; Ojala M, 2010, J MACH LEARN RES, V11, P1833; Rajan K, 2006, PHYS REV LETT, V97, DOI 10.1103/PhysRevLett.97.188104; Roebroeck A, 2005, NEUROIMAGE, V25, P230, DOI 10.1016/j.neuroimage.2004.11.017; Runyan CA, 2017, NATURE, V548, P92, DOI 10.1038/nature23020; Ryali S, 2011, NEUROIMAGE, V54, P807, DOI 10.1016/j.neuroimage.2010.09.052; Seth AK, 2013, NEUROIMAGE, V65, P540, DOI 10.1016/j.neuroimage.2012.09.049; Seth AK, 2010, J NEUROSCI METH, V186, P262, DOI 10.1016/j.jneumeth.2009.11.020; Shirer WR, 2012, CEREB CORTEX, V22, P158, DOI 10.1093/cercor/bhr099; Smith SM, 2011, NEUROIMAGE, V54, P875, DOI 10.1016/j.neuroimage.2010.08.063; Sridharan D, 2008, P NATL ACAD SCI USA, V105, P12569, DOI 10.1073/pnas.0800005105; Vidaurre D, 2017, P NATL ACAD SCI USA, V114, P12827, DOI 10.1073/pnas.1705120114	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404018
C	Taghvaei, A; Kim, JW; Mehta, PG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Taghvaei, Amirhossein; Kim, Jin W.; Mehta, Prashant G.			How regularization affects the critical points in linear networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORKS	This paper is concerned with the problem of representing and learning a linear transformation using a linear neural network. In recent years, there is a growing interest in the study of such networks, in part due to the successes of deep learning. The main question of this body of research (and also of our paper) is related to the existence and optimality properties of the critical points of the mean-squared loss function. An additional primary concern of our paper pertains to the robustness of these critical points in the face of (a small amount of) regularization. An optimal control model is introduced for this purpose and a learning algorithm (backprop with weight decay) derived for the same using the Hamilton's formulation of optimal control. The formulation is used to provide a complete characterization of the critical points in terms of the solutions of a nonlinear matrix-valued equation, referred to as the characteristic equation. Analytical and numerical tools from bifurcation theory are used to compute the critical points via the solutions of the characteristic equation.	[Taghvaei, Amirhossein; Kim, Jin W.; Mehta, Prashant G.] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Taghvaei, A (corresponding author), Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.	taghvae2@illinois.edu; kim684@illinois.edu; mehtapg@illinois.edu	Jeong, Yongwook/N-7413-2016		NSF CMMI grant [1462773]	NSF CMMI grant	Financial support from the NSF CMMI grant 1462773 is gratefully acknowledged.	BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; BALDI PF, 1995, IEEE T NEURAL NETWOR, V6, P837, DOI 10.1109/72.392248; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Choromanska A., 2015, COLT, P1756; Choromanska A., 2015, AISTATS; Clewley R H, 2007, PYDSTOOL SOFTWARE EN; CULVER WJ, 1966, P AM MATH SOC, V17, P1146, DOI 10.2307/2036109; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; FAROTIMI O, 1991, IEEE T NEURAL NETWOR, V2, P378, DOI 10.1109/72.97914; GE R, 2015, ARXIV150302101; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gunasekar S., 2017, ARXIV170509280; Hardt M., 2016, ARXIV161104231; Higham N. J., 2014, FUNCTIONS MATRICES; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; LeCun Y., 1988, P 1988 CONNECTIONIST, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7; Lee J. D., 2016, ARXIV160204915; Neyshabur Behnam, 2014, ARXIV14126614; Saxe A., 2014, INT C LEARNING REPRE; Soudry D., 2016, ARXIV PREPRINT ARXIV; Wibisono A, 2016, P NATL ACAD SCI USA, V113, pE7351, DOI 10.1073/pnas.1614734113; Zhang Chiyuan, 2016, ARXIV161103530	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402054
C	Tanczos, E; Nowak, R; Mankoff, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tanczos, Ervin; Nowak, Robert; Mankoff, Bob			A KL-LUCB Bandit Algorithm for Large-Scale Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper focuses on best-arm identification in multi-armed bandits with bounded rewards. We develop an algorithm that is a fusion of lil-UCB and KL-LUCB, offering the best qualities of the two algorithms in one method. This is achieved by proving a novel anytime confidence bound for the mean of bounded distributions, which is the analogue of the LIL-type bounds recently developed for sub-Gaussian distributions. We corroborate our theoretical results with numerical experiments based on the New Yorker Cartoon Caption Contest.	[Tanczos, Ervin; Nowak, Robert] Univ Wisconsin, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Tanczos, E (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.	tanczos@wisc.edu; rdnowak@wisc.edu; bmankoff@hearst.com	Jeong, Yongwook/N-7413-2016		NSF [IIS-1447449]; AFSOR [FA9550-13-1-0138]	NSF(National Science Foundation (NSF)); AFSOR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was partially supported by the NSF grant IIS-1447449 and the AFSOR grant FA9550-13-1-0138.	Audibert J.-Y., 2010, COLT 23 C LEARN THEO, P13; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Fox Rubin B., 2016, CNET NEWS; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Jamieson K., 2014, C LEARN THEOR, P423; Jamieson KG., 2015, ADV NEURAL INFORM PR, V28, P2656; Kaufmann E., 2013, C LEARNING THEORY, P228; Kaufmann Emilie, 2016, J MACHINE LEARNING R; Simchowitz M., 2017, ARXIV170205186; Stoltz G, 2011, P 24 ANN C LEARN THE, P497	11	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405094
C	Uziel, G; El-Yaniv, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Uziel, Guy; El-Yaniv, Ran			Multi-Objective Non-parametric Sequential Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Online-learning research has mainly been focusing on minimizing one objective function. In many real-world applications, however, several objective functions have to be considered simultaneously. Recently, an algorithm for dealing with several objective functions in the i.i.d. case has been presented. In this paper, we extend the multi-objective framework to the case of stationary and ergodic processes, thus allowing dependencies among observations. We first identify an asymptomatic lower bound for any prediction strategy and then present an algorithm whose predictions achieve the optimal solution while fulfilling any continuous and convex constraining criterion.	[Uziel, Guy; El-Yaniv, Ran] Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel	Technion Israel Institute of Technology	Uziel, G (corresponding author), Technion Israel Inst Technol, Comp Sci Dept, Haifa, Israel.	guziel@cs.technion.ac.il; rani@cs.technion.ac.il	Jeong, Yongwook/N-7413-2016		Israel Science Foundation [1890/14]	Israel Science Foundation(Israel Science Foundation)	We would like to thank the anonymous reviewers for providing helpful comments. This research was supported by The Israel Science Foundation (grant No. 1890/14)	ALGOET PH, 1994, IEEE T INFORM THEORY, V40, P609, DOI 10.1109/18.335876; Biau G, 2011, IEEE T INFORM THEORY, V57, P1664, DOI 10.1109/TIT.2011.2104610; Biau G, 2010, J NONPARAMETR STAT, V22, P297, DOI 10.1080/10485250802680730; Borodin A., 2005, ONLINE COMPUTATION C; BREIMAN L, 1957, ANN MATH STAT, V28, P809, DOI 10.1214/aoms/1177706899; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Devroye L, 2013, PROBABILISTIC THEORY, V31; Gyorfi L, 2008, STATIST RISK MODEL, V26, P145, DOI 10.1524/stnd.2008.0917; Gyorfi L, 2006, MATH FINANC, V16, P337, DOI 10.1111/j.1467-9965.2006.00274.x; Gyorfi L., 2003, ADV LEARNING THEORY, V339, P354; Gyorfi L, 2007, INT J THEOR APPL FIN, V10, P505, DOI 10.1142/S0219024907004251; Kalnishkan Y, 2005, LECT NOTES COMPUT SC, V3559, P188, DOI 10.1007/11503415_13; Luenberger D., 1997, OPTIMIZATION VECTOR; Lugosi G., 2005, MODELING UNCERTAINTY, P225; Luxburg U.V., 2008, ARXIV08104752; Mahdavi M, 2013, ADV NEURAL INFORM PR, V26, P1115; Mannor S, 2009, J MACH LEARN RES, V10, P569; Rigollet P, 2011, J MACH LEARN RES, V12, P2831; Stout W., 1974, ALMOST SURE CONVERGE, V24; Vovk V, 2007, LECT NOTES COMPUT SC, V4539, P439, DOI 10.1007/978-3-540-72927-3_32	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403043
C	van der Pas, S; Roekova, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		van der Pas, Stephanie; Roekova, Veronika			Bayesian Dyadic Trees and Histograms for Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CONVERGENCE-RATES; CART	Many machine learning tools for regression are based on recursive partitioning of the covariate space into smaller regions, where the regression function can be estimated locally. Among these, regression trees and their ensembles have demonstrated impressive empirical performance. In this work, we shed light on the machinery behind Bayesian variants of these methods. In particular, we study Bayesian regression histograms, such as Bayesian dyadic trees, in the simple regression case with just one predictor. We focus on the reconstruction of regression surfaces that are piecewise constant, where the number of jumps is unknown. We show that with suitably designed priors, posterior distributions concentrate around the true step regression function at a near-minimax rate. These results do not require the knowledge of the true number of steps, nor the width of the true partitioning cells. Thus, Bayesian dyadic regression trees are fully adaptive and can recover the true piecewise regression function nearly as well as if we knew the exact number and location of jumps. Our results constitute the first step towards understanding why Bayesian trees and their ensembles have worked so well in practice. As an aside, we discuss prior distributions on balanced interval partitions and how they relate to an old problem in geometric probability. Namely, we relate the probability of covering the circumference of a circle with random arcs whose endpoints are confined to a grid, a new variant of the original problem.	[van der Pas, Stephanie] Leiden Univ, Math Inst, Leiden, Netherlands; [Roekova, Veronika] Univ Chicago, Booth Sch Business, Chicago, IL 60637 USA	Leiden University; Leiden University - Excl LUMC; University of Chicago	van der Pas, S (corresponding author), Leiden Univ, Math Inst, Leiden, Netherlands.	svdpas@math.leidenuniv.nl; Veronika.Rockova@ChicagoBooth.edu	Jeong, Yongwook/N-7413-2016	van der Pas, Stephanie/0000-0002-2448-5378	James S. Kemper Foundation Faculty Research Fund at the University of Chicago Booth School of Business	James S. Kemper Foundation Faculty Research Fund at the University of Chicago Booth School of Business	This work was supported by the James S. Kemper Foundation Faculty Research Fund at the University of Chicago Booth School of Business.	Abu-Nimeh S, 2007, P ANTIPHISHING WORKI, P60; [Anonymous], 1984, STAT PROBABILITY SER; Berchuck A, 2005, CLIN CANCER RES, V11, P3686, DOI 10.1158/1078-0432.CCR-04-2398; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Briol FX, 2015, ADV NEUR IN, V28; Castillo I., 2016, PREPRINT; Castillo I, 2014, ANN STAT, V42, P1941, DOI 10.1214/14-AOS1246; Chen MJ, 2016, BAYESIAN ANAL, V11, P477, DOI 10.1214/15-BA958; Chipman HA, 1998, J AM STAT ASSOC, V93, P935, DOI 10.2307/2669832; Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285; Coram M, 2006, ANN STAT, V34, P1233, DOI 10.1214/009053606000000236; Denison DGT, 1998, BIOMETRIKA, V85, P363; Donoho DL, 1997, ANN STAT, V25, P1870, DOI 10.1214/aos/1069362377; Feller W., 1968, INTRO PROBABILITY TH, V2; FLATTO L, 1962, SIAM REV, V4, P211, DOI 10.1137/1004058; Gao C., 2017, MINIMAX RISK B UNPUN, P1; Ghosal S, 2000, ANN STAT, V28, P500, DOI 10.1214/aos/1016218228; Ghosal S, 2007, ANN STAT, V35, P192, DOI 10.1214/009053606000001172; Green DP, 2012, PUBLIC OPIN QUART, V76, P491, DOI 10.1093/poq/nfs036; Korda N, 2013, ADV NEURAL INFORM PR, P1448; Liu L., 2015, ARXIV150804812V1; Nobel A, 1996, ANN STAT, V24, P1084; Polly E. C., 2010, SUPER LEARNER PREDIC; Razi MA, 2005, EXPERT SYST APPL, V29, P65, DOI 10.1016/j.eswa.2005.01.006; Rockova V., 2017, ARXIV170808734; Rousseau J., 2016, ARXIV E PRINTS; Roy D. M., 2009, ADV NEURAL INFORM PR; Scricciolo C, 2007, SCAND J STAT, V34, P626, DOI 10.1111/j.1467-9469.2006.00540.x; SHEPP LA, 1972, ISRAEL J MATH, V11, P328, DOI 10.1007/BF02789327; Tang J, 2014, PR MACH LEARN RES, V32; Zhang T, 2004, ADV NEUR IN, V16, P1149; [No title captured]	32	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402014
C	Vartak, M; Thiagarajan, A; Miranda, C; Bratman, J; Larochelle, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Vartak, Manasi; Thiagarajan, Arvind; Miranda, Conrado; Bratman, Jeshua; Larochelle, Hugo			A Meta-Learning Perspective on Cold-Start Recommendations for Items	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Matrix factorization (MF) is one of the most popular techniques for product recommendation, but is known to suffer from serious cold-start problems. Item cold-start problems are particularly acute in settings such as Tweet recommendation where new items arrive continuously. In this paper, we present a meta-learning strategy to address item cold-start when new items arrive continuously. We propose two deep neural network architectures that implement our meta-learning strategy. The first architecture learns a linear classifier whose weights are determined by the item history while the second architecture learns a neural network whose biases are instead adjusted. We evaluate our techniques on the real-world problem of Tweet recommendation. On production data at Twitter, we demonstrate that our proposed techniques significantly beat the MF baseline and also outperform production models for Tweet recommendation.	[Vartak, Manasi] MIT, Cambridge, MA 02139 USA; [Thiagarajan, Arvind; Miranda, Conrado; Bratman, Jeshua] Twitter Inc, San Francisco, CA USA; [Larochelle, Hugo] Google Brain, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); Twitter, Inc.; Google Incorporated	Vartak, M (corresponding author), MIT, Cambridge, MA 02139 USA.	mvartak@csail.mit.edu; arvindt@twitter.com; cmiranda@twitter.com; jbratman@twitter.com; hugolarochelle@google.com	Jeong, Yongwook/N-7413-2016					Agarwal D, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P19; Bachman P, 2017, PR MACH LEARN RES, V70; Blei, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480; Charlin L, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P173, DOI 10.1145/2623330.2623663; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; DAS Abhinandan, 2007, P 16 INT C WORLD WID, V16, P271, DOI DOI 10.1145/1242572.1242610; He X, 2014, DMOA, P1; Hidasi B, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P241, DOI 10.1145/2959100.2959167; HONG L., 2013, P 6 ACM INT C WEB SE, P557, DOI DOI 10.1145/2433396.2433467; Kim D, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P233, DOI 10.1145/2959100.2959165; Koch Gregory, 2015, ICML DEEP LEARN WORK; Koren Y., 2008, P 14 ACM SIGKDD INT, P426, DOI DOI 10.1145/1401890.1401944; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lam X.N., 2008, P 2 INT C UBIQUITOUS, P208, DOI 10.1145/1352793.1352837; Lemke C, 2015, ARTIF INTELL REV, V44, P117, DOI 10.1007/s10462-013-9406-y; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Liu J, 2010, IUI 2010, P31; Lops P, 2011, RECOMMENDER SYSTEMS HANDBOOK, P73, DOI 10.1007/978-0-387-85820-3_3; Mensink T, 2012, LECT NOTES COMPUT SC, V7573, P488, DOI 10.1007/978-3-642-33709-3_35; Ravi Sachin, 2017, ICLR; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J., 2017, ABS170305175 CORR; Tikk D., 2015, ARXIV151106939; Vilalta R, 2002, ARTIF INTELL REV, V18, P77, DOI 10.1023/A:1019956318069; Vinyals Oriol, 2016, ARXIV160604080, P3630; Zaheer Manzil, 2017, ABS170306114 CORR	29	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406093
C	Vellanki, P; Rana, S; Gupta, S; Rubin, D; Sutti, A; Dorin, T; Height, M; Sandars, P; Venkatesh, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Vellanki, Pratibha; Rana, Santu; Gupta, Sunil; Rubin, David; Sutti, Alessandra; Dorin, Thomas; Height, Murray; Sandars, Paul; Venkatesh, Svetha			Process-constrained batch Bayesian Optimisation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PRECIPITATION	Prevailing batch Bayesian optimisation methods allow all control variables to be freely altered at each iteration. Real-world experiments, however, often have physical limitations making it time-consuming to alter all settings for each recommendation in a batch. This gives rise to a unique problem in BO: in a recommended batch, a set of variables that are expensive to experimentally change need to be fixed, while the remaining control variables can be varied. We formulate this as a process-constrained batch Bayesian optimisation problem. We propose two algorithms, pc-BO(basic) and pc-BO(nested). pc-BO(basic) is simpler but lacks convergence guarantee. In contrast pc-BO(nested) is slightly more complex, but admits convergence analysis. We show that the regret of pc-BO(nested) is sublinear. We demonstrate the performance of both pc-BO(basic) and pc-BO(nested) by optimising benchmark test functions, tuning hyper-parameters of the SVM classifier, optimising the heat-treatment process for an Al-Sc alloy to achieve target hardness, and optimising the short polymer fibre production process.	[Vellanki, Pratibha; Rana, Santu; Gupta, Sunil; Venkatesh, Svetha] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia; [Rubin, David; Sutti, Alessandra; Dorin, Thomas; Height, Murray] Deakin Univ, Inst Frontier Mat, GTP Res, Geelong, Vic, Australia; [Sandars, Paul] Michigan Technol Univ, Mat Sci & Engn, Houghton, MI 49931 USA	Deakin University; Deakin University; Michigan Technological University	Vellanki, P (corresponding author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.	pratibha.vellanki@deakin.edu.au; santu.rana@deakin.edu.au; sunil.gupta@deakin.edu.au; d.rubindecelisleal@deakin.edu.au; alessandra.sutti@deakin.edu.au; thomas.dorin@deakin.edu.au; murray.height@deakin.edu.au; sanders@mtu.edu; svetha.venkatesh@deakin.edu.au	Jeong, Yongwook/N-7413-2016; Rana, Santu/R-2992-2019	Rana, Santu/0000-0003-2247-850X	Australian Government through the Australian Research Council (ARC); Telstra-Deakin Centre of Excellence in Big Data and Machine Learning; ARC Australian Laureate Fellowship [FL170100006]	Australian Government through the Australian Research Council (ARC)(Australian Research Council); Telstra-Deakin Centre of Excellence in Big Data and Machine Learning; ARC Australian Laureate Fellowship(Australian Research Council)	This research was partially funded by the Australian Government through the Australian Research Council (ARC) and the Telstra-Deakin Centre of Excellence in Big Data and Machine Learning. Prof Venkatesh is the recipient of an ARC Australian Laureate Fellowship (FL170100006).	[Anonymous], P ART INT STAT; Azimi J, 2010, ADV NEURAL INFORM PR, V23, P109; Azimi J, 2016, J ARTIF INTELL RES, V56, P119, DOI 10.1613/jair.4896; Bergstra J. S., 2011, P 24 INT C NEUR INF, P2546, DOI DOI 10.1145/3065386; Brochu E, 2010, ARXIV PREPRINT ARXIV; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Gelbart MA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P250; Ginsbourger D., 2008, TECHNICAL REPORT; Hernandez-Lobato D, 2016, J MACH LEARN RES, V17; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robson JD, 2003, ACTA MATER, V51, P1453, DOI 10.1016/S1359-6454(02)00540-2; Sacks J., 1989, STAT SCI, V4, P409, DOI [10.1214/ss/1177012413, DOI 10.1214/SS/1177012413]; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J., 2012, P NIPS, V12, P2960; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sutti A, 2011, J NANOSCI NANOTECHNO, V11, P8947, DOI 10.1166/jnn.2011.3489; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Thornton C, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P847, DOI 10.1145/2487575.2487629; Wagner R., 1991, HOMOGENEOUS 2 PHASE; Wang Z., 2013, INT JOINT C ART INT	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403047
C	Wald, Y; Globerson, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wald, Yoav; Globerson, Amir			Robust Conditional Probabilities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REGULARIZATION; INEQUALITIES; OPTIMIZATION; ALGORITHM; BOUNDS	Conditional probabilities are a core concept in machine learning. For example, optimal prediction of a label Y given an input X corresponds to maximizing the conditional probability of Y given X . A common approach to inference tasks is learning a model of conditional probabilities. However, these models are often based on strong assumptions (e.g., log-linear models), and hence their estimate of conditional probabilities is not robust and is highly dependent on the validity of their assumptions. Here we propose a framework for reasoning about conditional probabilities without assuming anything about the underlying distributions, except knowledge of their second order marginals, which can be estimated from data. We show how this setting leads to guaranteed bounds on conditional probabilities, which can be calculated efficiently in a variety of settings, including structured-prediction. Finally, we apply them to semi-supervised deep learning, obtaining results competitive with variational autoencoders.	[Wald, Yoav] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel; [Globerson, Amir] Tel Aviv Univ, Balvatnik Sch Comp Sci, Tel Aviv, Israel	Hebrew University of Jerusalem; Tel Aviv University	Wald, Y (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.	yoay.wald@mail.huji.ac.il; gamir@mail.tau.ac.il		Globerson, Amir/0000-0003-2557-1742	ISF Centers of Excellence [2180/15]; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	ISF Centers of Excellence; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	This work was supported by the ISF Centers of Excellence grant 2180/15, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).	AHUJA RK, 1995, NETWORKS, V25, P89, DOI 10.1002/net.3230250207; Akhiezer N. I, 1965, CLASSICAL MOMENT PRO, V5; Benavoli A., 2017, P 10 INT S IMPR PROB; Bertsimas D, 2005, SIAM J OPTIMIZ, V15, P780, DOI 10.1137/S1052623401399903; Cowell R.G., 2006, PROBABILISTIC NETWOR; Dudik M, 2007, J MACH LEARN RES, V8, P1217; Eban E, 2014, PR MACH LEARN RES, V32, P1233; Fromer M., 2009, ADV NEURAL INFORM PR, V22, P567; Grandvalet Y., 2005, CAP, P529; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Kingma D. P, 2014, ARXIV13126114; Koller D., 2009, PROBABILISTIC GRAPHI; Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Livni R., 2012, JMLR W CP, P722; McClosky David, 2006, P MAIN C HUM LANG TE, P3, DOI DOI 10.3115/1220835.1220855; Miranda E, 2007, J THEOR PROBAB, V20, P663, DOI 10.1007/s10959-007-0055-4; Muller AC, 2014, J MACH LEARN RES, V15, P2055; Parrilo PA, 2003, MATH PROGRAM, V96, P293, DOI 10.1007/s10107-003-0387-5; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; SMITH JE, 1995, OPER RES, V43, P807, DOI 10.1287/opre.43.5.807; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Tsoumakas G, 2011, J MACH LEARN RES, V12, P2411; Vandenberghe L, 2007, SIAM REV, V49, P52, DOI 10.1137/S0036144504440543; Wainwright M, 2004, STAT COMPUT, V14, P143, DOI 10.1023/B:STCO.0000021412.33763.d5; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weiss D, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P323, DOI 10.3115/v1/p15-1032; Xu H, 2009, J MACH LEARN RES, V10, P1485	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406042
C	Wang, QS; Chen, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Qinshi; Chen, Wei			Improving Regret Bounds for Combinatorial Semi-Bandits with Probabilistically Triggered Arms and Its Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study combinatorial multi-armed bandit with probabilistically triggered arms and semi-bandit feedback (CMAB-T). We resolve a serious issue in the prior CMAB-T studies where the regret bounds contain a possibly exponentially large factor of 1/p* where p* is the minimum positive probability that an arm is triggered by any action. We address this issue by introducing a triggering probability modulated (TPM) bounded smoothness condition into the general CMAB-T framework, and show that many applications such as influence maximization bandit and combinatorial cascading bandit satisfy this TPM condition. As a result, we completely remove the factor of 1/p* from the regret bounds, achieving significantly better regret bounds for influence maximization and cascading bandits than before. Finally, we provide lower bound results showing that the factor 1/p* is unavoidable for general CMAB-T problems, suggesting that the TPM condition is crucial in removing this factor.	[Wang, Qinshi] Princeton Univ, Princeton, NJ 08544 USA; [Chen, Wei] Microsoft Res, Beijing, Peoples R China	Princeton University; Microsoft	Wang, QS (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	qinshiw@princeton.edu; weic@microsoft.com			National Natural Science Foundation of China [61433014]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	Wei Chen is partially supported by the National Natural Science Foundation of China (Grant No. 61433014).	Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berry D.A., 1985, BANDIT PROBLEMS SEQU; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chen W., 2016, J MACHINE LEARNING R, V17, P1; Chen W., 2013, INFORM INFLUENCE PRO; Chen W., 2016, SINGLE IMAGE DEPTH P; Combes R., 2015, NIPS; Gai Yi, 2012, IEEE ACM T NETWORKIN, V20; Gopalan Aditya, 2014, P 31 INT C MACH LEAR; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Kveton B., 2015, P 32 INT C MACH LEAR; Kveton Branislav, 2015, P 18 INT C ART INT S; Kveton Branislav, 2015, ADV NEURAL INFORM PR; Kveton Branislav, 2014, P 30 C UNC ART INT U; Lagree Paul, 2016, ADV NEURAL INFORM PR, P1597; Lei Siyu, 2015, KDD; Mitzenmacher M., 2005, PROBABILITY COMPUTIN; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Vaswani Sharan, 2017, P 34 INT C MACH LEAR; Vaswani Sharan, 2015, NIPS WORKSH NETW SOC; Wen Zheng, 2016, CORR; Yang Yu, 2016, P 2016 INT C MAN DAT	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401020
C	Wang, SN; Shroff, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Sinong; Shroff, Ness			A New Alternating Direction Method for Linear Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS; CONVERGENCE	It is well known that, for a linear program (LP) with constraint matrix A is an element of R-mxn, the Alternating Direction Method of Multiplier converges globally and linearly at a rate O((parallel to A parallel to(2)(F) + mn) log(1/epsilon)). However, such a rate is related to the problem dimension and the algorithm exhibits a slow and fluctuating "tail convergence" in practice. In this paper, we propose a new variable splitting method of LP and prove that our method has a convergence rate of O(parallel to A parallel to(2) log(1/epsilon)). The proof is based on simultaneously estimating the distance from a pair of primal dual iterates to the optimal primal and dual solution set by certain residuals. In practice, we result in a new first-order LP solver that can exploit both the sparsity and the specific structure of matrix A and a significant speedup for important problems such as basis pursuit, inverse covariance matrix estimation, L1 SVM and nonnegative matrix factorization problem compared with the current fastest LP solvers.	[Wang, Sinong] Ohio State Univ, Dept ECE, Columbus, OH 43210 USA; [Shroff, Ness] Ohio State Univ, Dept ECE & CSE, Columbus, OH 43210 USA	University System of Ohio; Ohio State University; University System of Ohio; Ohio State University	Wang, SN (corresponding author), Ohio State Univ, Dept ECE, Columbus, OH 43210 USA.	wang.7691@osu.edu; shroff.11@osu.edu	Jeong, Yongwook/N-7413-2016		ONR [N00014-17-1-2417, N00014-15-1-2166]; ARO [W911NF-1-0277]; NSF [CNS-1719371]	ONR(Office of Naval Research); ARO; NSF(National Science Foundation (NSF))	This work is supported by ONR N00014-17-1-2417, N00014-15-1-2166, NSF CNS-1719371 and ARO W911NF-1-0277.	Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Boley D, 2013, SIAM J OPTIMIZ, V23, P2183, DOI 10.1137/120878951; Deng W, 2016, J SCI COMPUT, V66, P889, DOI 10.1007/s10915-015-0048-x; ECKSTEIN J, 1992, MATH PROGRAM, V55, P293, DOI 10.1007/BF01581204; Eckstein J., 1990, ALTERNATING DIRECTIO; Eleuterio Vania Lucia Dos Santos, 2009, THESIS; GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41; GULER O, 1992, J OPTIMIZ THEORY APP, V75, P445, DOI 10.1007/BF00940486; Hoffman A. J., 1952, J RES NBS, V49; Hong M., 2012, MATH PROGRAM, V162, P1; Lee YT, 2013, ANN IEEE SYMP FOUND, P147, DOI 10.1109/FOCS.2013.24; LI W, 1994, SIAM J CONTROL OPTIM, V32, P140, DOI 10.1137/S036301299222723X; Lin Q., 2014, ADV NEURAL INFORM PR, P3059; Lin TY, 2015, SIAM J OPTIMIZ, V25, P1478, DOI 10.1137/140971178; Meshi O, 2011, LECT NOTES ARTIF INT, V6912, P470, DOI 10.1007/978-3-642-23783-6_30; Nishihara R, 2015, PR MACH LEARN RES, V37, P343; Recht B., 2012, ADV NEURAL INFORM PR, V25, P1214; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Yang JF, 2011, SIAM J SCI COMPUT, V33, P250, DOI 10.1137/090777761; Yen Ian En-Hsu, 2015, NIPS, P2368; Yin WT, 2010, SIAM J IMAGING SCI, V3, P856, DOI 10.1137/090760350; Yuan M, 2010, J MACH LEARN RES, V11, P2261; Zhu J, 2004, ADV NEUR IN, V16, P49	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401050
C	Wang, XQ; Chen, H; Cai, WD; Shen, DG; Huang, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Xiaoqian; Chen, Hong; Cai, Weidong; Shen, Dinggang; Huang, Heng			Regularized Modal Regression with Applications in Cognitive Impairment Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIABLE SELECTION; WHITE-MATTER; DISEASE; ROBUST; MRI	Linear regression models have been successfully used to function estimation and model selection in high-dimensional data analysis. However, most existing methods are built on least squares with the mean square error (MSE) criterion, which are sensitive to outliers and their performance may be degraded for heavy-tailed noise. In this paper, we go beyond this criterion by investigating the regularized modal regression from a statistical learning viewpoint. A new regularized modal regression model is proposed for estimation and variable selection, which is robust to outliers, heavy-tailed noise, and skewed noise. On the theoretical side, we establish the approximation estimate for learning the conditional mode function, the sparsity analysis for variable selection, and the robustness characterization. On the application side, we applied our model to successfully improve the cognitive impairment prediction using the Alzheimer's Disease Neuroimaging Initiative (ADNI) cohort data.	[Wang, Xiaoqian; Chen, Hong; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA; [Cai, Weidong] Univ Sydney, Sch Informat Technol, Sydney, NSW, Australia; [Shen, Dinggang] Univ North Carolina Chapel Hill, Dept Radiol, Chapel Hill, NC USA; [Shen, Dinggang] Univ North Carolina Chapel Hill, BRIC, Chapel Hill, NC USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; University of Sydney; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina School of Medicine; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina School of Medicine	Huang, H (corresponding author), Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA.	xqwang1991@gmail.com; chenh@mail.hzau.edu.cn; tom.cai@sydney.edu.au; dinggang_shen@med.unc.edu; heng.huang@pitt.edu	Shen, Dinggang/ABF-6812-2020; Jeong, Yongwook/N-7413-2016	Shen, Dinggang/0000-0002-7934-5698; 	U.S. NSF-IIS [1302675]; NSF-IIS [1633753, 1344152, 1619308]; NSF-DBI [1356628]; NIH [AG049371]; National Natural Science Foundation of China (NSFC) [11671161]	U.S. NSF-IIS; NSF-IIS(National Science Foundation (NSF)); NSF-DBI(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH AG049371. Hong Chen was partially supported by National Natural Science Foundation of China (NSFC) 11671161. We are grateful to the anonymous NIPS reviewers for the insightful comments.	Armitage SG, 1946, PSYCHOL MONOGR, V60, P1; Bozzali M, 2002, J NEUROL NEUROSUR PS, V72, P742, DOI 10.1136/jnnp.72.6.742; Chambers CD, 2004, NAT NEUROSCI, V7, P217, DOI 10.1038/nn1203; Chen YC, 2016, ANN STAT, V44, P489, DOI 10.1214/15-AOS1373; COLLOMB G, 1987, J STAT PLAN INFER, V15, P227, DOI 10.1016/0378-3758(86)90099-6; Feng Y., 2017, ARXIV170205960; Feng YL, 2015, J MACH LEARN RES, V16, P993; FOLSTEIN MF, 1975, J PSYCHIAT RES, V12, P189, DOI 10.1016/0022-3956(75)90026-6; He R, 2011, IEEE T PATTERN ANAL, V33, P1561, DOI 10.1109/TPAMI.2010.220; Huang J, 2007, AM J NEURORADIOL, V28, P1943, DOI 10.3174/ajnr.A0700; Huber P., 1981, ROBUST STAT; HUBER PJ, 1984, ANN STAT, V12, P119, DOI 10.1214/aos/1176346396; Jack CR, 2008, J MAGN RESON IMAGING, V27, P685, DOI 10.1002/jmri.21049; Karas G, 2008, AM J NEURORADIOL, V29, P944, DOI 10.3174/ajnr.A0949; Lichman M., 2013, UCI MACHINE LEARNING; Moradi E, 2017, NEUROIMAGE-CLIN, V13, P415, DOI 10.1016/j.nicl.2016.12.011; Nickel J, 2003, EPILEPSIA, V44, P1551, DOI 10.1111/j.0013-9580.2003.13603.x; Nikolova M, 2005, SIAM J SCI COMPUT, V27, P937, DOI 10.1137/030600862; Principe JC, 2010, INFORM SCI STAT, P1, DOI 10.1007/978-1-4419-1570-2; Rockafellar R. T., 1970, CONVEX ANAL; SAGER TW, 1982, ANN STAT, V10, P690, DOI 10.1214/aos/1176345865; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang H., NEUR INF PROC SYST C, P1286; Wang H, 2011, LECT NOTES COMPUT SC, V6893, P115, DOI 10.1007/978-3-642-23626-6_15; Wang H, 2011, IEEE I CONF COMP VIS, P557, DOI 10.1109/ICCV.2011.6126288; Wang X., 19 INT C MED IM COMP, P273; Yang H, 2014, J MULTIVARIATE ANAL, V129, P227, DOI 10.1016/j.jmva.2014.04.024; Yao WX, 2014, SCAND J STAT, V41, P656, DOI 10.1111/sjos.12054; Yao WX, 2012, J NONPARAMETR STAT, V24, P647, DOI 10.1080/10485252.2012.678848; Zhao WH, 2014, ANN I STAT MATH, V66, P165, DOI 10.1007/s10463-013-0410-4	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401047
C	Wei, YT; Yang, F; Wainwright, MJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wei, Yuting; Yang, Fanny; Wainwright, Martin J.			Early stopping for kernel boosting algorithms: A general analysis with localized complexities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				GRADIENT; REGRESSION; PREDICTION	Early stopping of iterative algorithms is a widely-used form of regularization in statistics, commonly used in conjunction with boosting and related gradient type algorithms. Although consistency results have been established in some settings, such estimators are less well-understood than their analogues based on penalized regularization. In this paper, for a relatively broad class of loss functions and boosting algorithms (including L-2 -boost, LogitBoost and AdaBoost, among others), we exhibit a direct connection between the performance of a stopped iterate and the localized Gaussian complexity of the associated function class. This connection allows us to show that local fixed point analysis of Gaussian or Rademacher complexities, now standard in the analysis of penalized estimators, can be used to derive optimal stopping rules. We derive such stopping rules in detail for various kernel classes, and illustrate the correspondence of our theory with practice for Sobolev kernel classes.	[Wei, Yuting; Wainwright, Martin J.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Yang, Fanny; Wainwright, Martin J.] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley	Yang, F (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	ytwei@berkeley.edu; fanny-yang@berkeley.edu; wainwrig@berkeley.edu	Jeong, Yongwook/N-7413-2016		DOD Advanced Research Projects Agency [W911NF-16-1-0552]; National Science Foundation [NSF-DMS-1612948]; Office of Naval Research [DOD-ONR-N00014]	DOD Advanced Research Projects Agency; National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research)	This work was partially supported by DOD Advanced Research Projects Agency W911NF-16-1-0552, National Science Foundation grant NSF-DMS-1612948, and Office of Naval Research Grant DOD-ONR-N00014.	ANDERSSEN RS, 1981, J AUST MATH SOC B, V22, P488, DOI 10.1017/S0334270000002824; [Anonymous], 2001, MATH SURVEYS MONOGRA; [Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], 2002, LEARNING KERNELS; Bartlett PL, 2007, J MACH LEARN RES, V8, P2347; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; Breiman L, 1998, ANN STAT, V26, P801; Buhlmann P, 2007, STAT SCI, V22, P477, DOI 10.1214/07-STS242; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Camoriano R, 2016, JMLR WORKSH CONF PRO, V51, P1403; Caponetto A., 2006, 265AI CBCL MIT; Caponneto A., 2006, 264AI CBCL MIT; Caruana R, 2001, ADV NEUR IN, V13, P402; De Vito E, 2010, FOUND COMPUT MATH, V10, P455, DOI 10.1007/s10208-010-9064-2; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gu C., 2002, SPR S STAT, DOI 10.1007/978-1-4757-3683-0; Gyorfi L., 2002, SPRINGER SERIES STAT; Jiang WX, 2004, ANN STAT, V32, P13; KIMELDORF G, 1971, J MATH ANAL APPL, V33, P82, DOI 10.1016/0022-247X(71)90184-3; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Mason L, 2000, ADV NEUR IN, V12, P512; Mendelson S., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P29; Prechelt L, 1998, LECT NOTES COMPUT SC, V1524, P55; Raskutti G, 2014, J MACH LEARN RES, V15, P335; Schapire RE, 2003, LECT NOTES STAT, V171, P149, DOI 10.1007/978-0-387-21579-2_9; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; STRAND ON, 1974, SIAM J NUMER ANAL, V11, P798, DOI 10.1137/0711066; Van de Geer S., 2000, APPL EMPIRICAL PROCE; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP; Wahba G., 1990, CBMS NSF REGIONAL C; Wahba G., 1987, INVERSE ILL POSED PR, P37; Wainwright Martin J., 2017, HIGH DIMENSIONAL STA; Yang Y., 2017, ANN STAT; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zhang T, 2005, ANN STAT, V33, P1538, DOI 10.1214/009053605000000255	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406014
C	Wu, AQ; Roy, NA; Keeley, S; Pillow, JW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Anqi; Roy, Nicholas A.; Keeley, Stephen; Pillow, Jonathan W.			Gaussian process based nonlinear latent structure discovery in multivariate spike train data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A large body of recent work focuses on methods for extracting low-dimensional latent structure from multi-neuron spike train data. Most such methods employ either linear latent dynamics or linear mappings from latent space to log spike rates. Here we propose a doubly nonlinear latent variable model that can identify low-dimensional structure underlying apparently high-dimensional spike train data. We introduce the Poisson Gaussian-Process Latent Variable Model (P-GPLVM), which consists of Poisson spiking observations and two underlying Gaussian processes-one governing a temporal latent variable and another governing a set of nonlinear tuning curves. The use of nonlinear tuning curves enables discovery of low-dimensional latent structure even when spike responses exhibit high linear dimensionality (e.g., as found in hippocampal place cell codes). To learn the model from data, we introduce the decoupled Laplace approximation, a fast approximate inference method that allows us to efficiently optimize the latent path while marginalizing over tuning curves. We show that this method outperforms previous Laplace-approximation-based inference methods in both the speed of convergence and accuracy. We apply the model to spike trains recorded from hippocampal place cells and show that it compares favorably to a variety of previous methods for latent structure discovery, including variational auto-encoder (VAE) based methods that parametrize the nonlinear mapping from latent space to spike rates with a deep neural network.	[Wu, Anqi; Roy, Nicholas A.; Keeley, Stephen; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA	Princeton University	Wu, AQ (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.		Jeong, Yongwook/N-7413-2016					Archer E., 2015, ARXIV PREPRINT ARXIV; Archer E. W., 2014, ADV NEURAL INFORM PR, V27, P343; Buesing Lars, 2012, ADV NEURAL INFORM PR, P1682; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Damianou A. C., 2014, ARXIV14092287; Gao YJ, 2016, ADV NEUR IN, V29; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; Kao JC, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8759; Karlsson M, 2005, SIMULTANEOUS EXTRACE, DOI [10.6080/K0NK3BZJ, DOI 10.6080/K0NK3BZJ]; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Linderman SW, 2016, J NEUROSCI METH, V263, P36, DOI 10.1016/j.jneumeth.2016.01.022; Macke J.H., 2015, ADV STATE SPACE METH, P137; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Nam H, 2015, THESIS, P8; Paninski L., 2013, ADV NEURAL INF PROCE, V26, P2391; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sussillo D., 2016, ARXIV160806315; Yu Byron M, 2009, ADV NEURAL INFORM PR, P1881, DOI DOI 10.1152/JN.90941; Zhao Y., 2016, ARXIV160403053	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403055
C	Wu, JJ; Wang, YF; Xue, TF; Sun, XY; Freeman, WT; Tenenbaum, JB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Jiajun; Wang, Yifan; Xue, Tianfan; Sun, Xingyuan; Freeman, William T.; Tenenbaum, Joshua B.			MarrNet: 3D Shape Reconstruction via 2.5D Sketches	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					3D object reconstruction from a single image is a highly under-determined problem, requiring strong prior knowledge of plausible 3D shapes. This introduces challenges for learning-based approaches, as 3D object annotations are scarce in real images. Previous work chose to train on synthetic data with ground truth 3D information, but suffered from domain adaptation when tested on real data. In this work, we propose MarrNet, an end-to-end trainable model that sequentially estimates 2.5D sketches and 3D object shape. Our disentangled, two-step formulation has three advantages. First, compared to full 3D shape, 2.5D sketches are much easier to be recovered from a 2D image; models that recover 2.5D sketches are also more likely to transfer from synthetic to real data. Second, for 3D reconstruction from 2.5D sketches, systems can learn purely from synthetic data. This is because we can easily render realistic 2.5D sketches without modeling object appearance variations in real images, including lighting, texture, etc. This further relieves the domain adaptation problem. Third, we derive differentiable projective functions from 3D shape to 2.5D sketches; the framework is therefore end-to-end trainable on real images, requiring no human annotations. Our model achieves state-of-the-art performance on 3D shape reconstruction.	[Wu, Jiajun; Xue, Tianfan; Freeman, William T.; Tenenbaum, Joshua B.] MIT CSAIL, Cambridge, MA 02139 USA; [Wang, Yifan] ShanghaiTech Univ, Shanghai, Peoples R China; [Sun, Xingyuan] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Freeman, William T.] Google Res, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); ShanghaiTech University; Shanghai Jiao Tong University; Google Incorporated	Wu, JJ (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022; Jeong, Yongwook/N-7413-2016; Xue, Tianfan/AAG-5546-2019		NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Center for Brain, Minds and Machines (NSF) [1231216]; Toyota Research Institute; Shell; Samsung	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Center for Brain, Minds and Machines (NSF); Toyota Research Institute; Shell(Royal Dutch Shell); Samsung(Samsung)	We thank Shubham Tulsiani for sharing the DRC results, and Chengkai Zhang for the help on shape visualization. This work is supported by NSF #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF #1231216), Toyota Research Institute, Samsung, and Shell.	[Anonymous], 2016, CVPR; Bansal Aayush, 2016, CVPR; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow H., 1978, COMPUTER VISION SYST; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chen W., 2016, SINGLE IMAGE DEPTH P; Choy C.B., 2016, ECCV; Collobert R., 2011, BIGLEARN NIPS WORKSH; Dai A., 2017, CVPR; Eigen D., 2015, ICCV; Firman M., 2016, CVPR; Girdhar R., 2016, ECCV; Heess N, 2016, NIPS; Hoiem D, 2005, ACM T GRAPHIC, V24, P577, DOI 10.1145/1073204.1073232; Horn B.K.P., 1989, SHAPE SHADING; Izadi S., 2011, UIST; Jakob Wenzel, 2010, MITSUBA RENDERER; Janner M., 2017, NIPS; Kar A., 2015, CVPR; Kemelmacher-Shlizerman I, 2011, IEEE T PATTERN ANAL, V33, P394, DOI 10.1109/TPAMI.2010.63; Lim Joseph J., 2013, ICCV; LOWE DG, 1987, ARTIF INTELL, V31, P355, DOI 10.1016/0004-3702(87)90070-1; Marr D., 1982, VISION COMPUTATIONAL; McCormac J., 2017, ICCV; Rock J., 2015, P CVPR; Saxena A, 2009, IEEE T PATTERN ANAL, V31, P824, DOI 10.1109/TPAMI.2008.132; Shi J., 2017, CVPR; Silberman N., 2012, ECCV; Soltani A. A., 2017, COMPUTER VISION PATT; Song S., 2017, CVPR; Sun J, 2015, CVPR; Tappen M.F., 2003, NIPS; Tulsiani S., 2017, IEEE C COMP VIS PATT; Wang X., 2015, CVPR; Weiss Y., 2001, ICCV; WU J., 2016, NIPS; Wu Jiajun, 2016, ECCV; Xiang Yu, 2014, WACV; Xiao J., 2010, CVPR; Yan X., 2016, NIPS; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284	42	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400052
C	Wu, YB; Lan, M; Sun, SL; Zhang, Q; Huang, XJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wu, Yuanbin; Lan, Man; Sun, Shiliang; Zhang, Qi; Huang, Xuanjing			A Learning Error Analysis for Structured Prediction with Approximate Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this work, we try to understand the differences between exact and approximate inference algorithms in structured prediction. We compare the estimation and approximation error of both underestimate (e.g., greedy search) and overestimate (e.g., linear relaxation of integer programming) models. The result shows that, from the perspective of learning errors, performances of approximate inference could be as good as exact inference. The error analyses also suggest a new margin for existing learning algorithms Empirical evaluations on text classification, sequential labelling and dependency parsing witness the success of approximate inference and the benefit of the proposed margin.	[Wu, Yuanbin; Lan, Man; Sun, Shiliang] East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China; [Wu, Yuanbin; Lan, Man] Shanghai Key Lab Multidimens Informat Proc, Shanghai, Peoples R China; [Zhang, Qi; Huang, Xuanjing] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China	East China Normal University; Fudan University	Wu, YB (corresponding author), East China Normal Univ, Sch Comp Sci & Software Engn, Shanghai, Peoples R China.; Wu, YB (corresponding author), Shanghai Key Lab Multidimens Informat Proc, Shanghai, Peoples R China.	ybwu@cs.ecnu.edu.cn; mlan@cs.ecnu.edu.cn; slsun@cs.ecnu.edu.cn; qz@fudan.edu.cn; xjhuang@fudan.edu.cn	Jeong, Yongwook/N-7413-2016		NSFC [61402175, 61532011]; STCSM [15ZR1410700]; Shanghai Key Laboratory of Trustworthy Computing [07dz22304201604]; Microsoft Research Asia Collaborative Research Program	NSFC(National Natural Science Foundation of China (NSFC)); STCSM(Science & Technology Commission of Shanghai Municipality (STCSM)); Shanghai Key Laboratory of Trustworthy Computing; Microsoft Research Asia Collaborative Research Program(Microsoft)	The authors wish to thank all reviewers for their helpful comments and suggestions. The corresponding authors are Man Lan and Shiliang Sun. This research is (partially) supported by NSFC (61402175, 61532011), STCSM (15ZR1410700) and Shanghai Key Laboratory of Trustworthy Computing (07dz22304201604). Yuanbin Wu is supported by a Microsoft Research Asia Collaborative Research Program.	Besag Julian, 1986, J ROYAL STAT SOC B, V48, P48; Catoni Olivier, 2007, LECT NOTES MONOGRAPH, V56; Chandrasekaran Venkat, 2013, P NATL ACAD SCI, V110; Chang KW, 2015, AAAI CONF ARTIF INTE, P2525; Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1; Collins Michael, 2001, P 7 INT WORKSH PARS; Cortes Corinna, 2016, ADV NEURAL INFORM PR, P2514; Crammer K, 2006, J MACH LEARN RES, V7, P551; Daniely Amit, 2012, NIPS, P494; Eisner Jason M., 1996, P COLING; Emerson Thomas, 2005, P 4 SIGHAN WORKSH CH; Finley T., 2008, INT C MACHINE LEARNI, P304, DOI DOI 10.1145/1390156.1390195; Germain Pascal, 2009, INT C MACH LEARN; Globerson A, 2015, PR MACH LEARN RES, V37, P2181; Honorio Jean, 2016, P UAI; Huang Liang, 2012, P C N AM CHAPT ASS C, P142; Keshet J., 2011, P 24 INT C NEUR INF, P2205; Kulesza A., 2007, ADV NEURAL INFORM PR, P785; Kundu Gourab, 2013, P ACL, P905; Langford J., 2002, P ANN C NEUR INF PRO, P423; London B., 2013, INT C MACH LEARN, P828; Martins Andre F. T., 2009, P ICML, P713; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; McAllester David, 2007, GEN BOUNDS CONSISTEN; McDonald Ryan T, 2006, P EACL; Meshi O., 2010, P 27 INT C MACH LEAR, P783; Meshi O, 2016, PR MACH LEARN RES, V48; Nivre Joakim, 2007, P CONLL SHAR TASK SE, V2007, P915; RENEGAR J, 1994, MATH PROGRAM, V65, P73, DOI 10.1007/BF01581690; RENEGAR J, 1995, SIAM J OPTIMIZ, V5, P506, DOI 10.1137/0805026; Samdani Rajhans, 2012, P ICML; Seeger M., 2002, J MACHINE LEARNING R, P233; Sontag David, 2010, ADV NEURAL INFORM PR, P2181; Taskar B, 2004, ADV NEUR IN, V16, P25; Tjong Erik F., 2000, P CONLL LLL; Wang Z., 2009, P 12 INT C ART INT S, P599	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406020
C	Xu, HT; Zha, HY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Hongteng; Zha, Hongyuan			A Dirichlet Mixture Model of Hawkes Processes for Event Sequence Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				IDENTIFICATION	How to cluster event sequences generated via different point processes is an interesting and important problem in statistical machine learning. To solve this problem, we propose and discuss an effective model-based clustering method based on a novel Dirichlet mixture model of a special but significant type of point processes-Hawkes process. The proposed model generates the event sequences with different clusters from the Hawkes processes with different parameters, and uses a Dirichlet distribution as the prior distribution of the clusters. We prove the identifiability of our mixture model and propose an effective variational Bayesian inference algorithm to learn our model. An adaptive inner iteration allocation strategy is designed to accelerate the convergence of our algorithm. Moreover, we investigate the sample complexity and the computational complexity of our learning algorithm in depth. Experiments on both synthetic and real-world data show that the clustering method based on our model can learn structural triggering patterns hidden in asynchronous event sequences robustly and achieve superior performance on clustering purity and consistency compared to existing methods.	[Xu, Hongteng] Georgia Inst Technol, Sch ECE, Atlanta, GA 30332 USA; [Zha, Hongyuan] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Xu, HT (corresponding author), Georgia Inst Technol, Sch ECE, Atlanta, GA 30332 USA.	hongtengxu313@gmail.com; zha@cc.gatech.edu	Xu, Hongteng/AAB-1636-2021; Jeong, Yongwook/N-7413-2016		NSF [IIS-1639792, IIS-1717916, CMMI-1745382]	NSF(National Science Foundation (NSF))	This work is supported in part by NSF IIS-1639792, IIS-1717916, and CMMI-1745382.	Bacry E, 2012, EUR PHYS J B, V85, DOI 10.1140/epjb/e2012-21005-8; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blundell Charles, 2012, NIPS; Daley D. J., 2007, INTRO THEORY POINT P, V2; Du N., 2015, KDD; Du N., 2012, NIPS; Eichler Michael, 2016, J TIME SER ANAL; Farajtabar M., 2014, NIPS; Gorur D, 2010, J COMPUT SCI TECH-CH, V25, P653, DOI [10.1007/s11390-010-1051-1, 10.1007/s11390-010-9355-8]; Golub GH, 2000, LINEAR ALGEBRA APPL, V309, P289, DOI 10.1016/S0024-3795(99)00204-9; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; Han F., 2013, ICML; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Kim D., 2008, THESIS; Lemonnier Remi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P161, DOI 10.1007/978-3-662-44851-9_11; Lewis E., 2011, J NONPARAMETR STAT, V1, P1; Li L., 2013, CIKM; Lian W., 2015, ICML; Liao TW, 2005, PATTERN RECOGN, V38, P1857, DOI 10.1016/j.patcog.2005.01.025; Luo DX, 2016, IEEE T KNOWL DATA EN, V28, P1518, DOI 10.1109/TKDE.2016.2522426; Luo DX, 2014, IEEE T BROADCAST, V60, P61, DOI 10.1109/TBC.2013.2295894; Luo Dixin, 2015, IJCAI; Maharaj EA, 2000, J CLASSIF, V17, P297, DOI 10.1007/s003570000023; Manning C. D., 2008, INTRO INFORM RETRIEV, V1; Maugis C, 2009, BIOMETRICS, V65, P701, DOI 10.1111/j.1541-0420.2008.01160.x; Meijer E, 2008, J CLASSIF, V25, P113, DOI 10.1007/s00357-008-9008-6; Ogunnaike B.A., 1994, PROCESS DYNAMICS MOD; Rasmussen C. E., 1999, NIPS; Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5; Reynaud-Bouret P, 2010, ANN STAT, V38, P2781, DOI 10.1214/10-AOS806; ROTHENBERG TJ, 1971, ECONOMETRICA, V39, P577, DOI 10.2307/1913267; Saeed M, 2002, COMPUT CARDIOL, V29, P641, DOI 10.1109/CIC.2002.1166854; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Simma A, 2010, UAI; Snoek J., 2012, NIPS; Socher R., 2011, AISTATS; Teh Y. W., 2006, NIPS; Tibshirani R, 2005, J COMPUT GRAPH STAT, V14, P511, DOI 10.1198/106186005X59243; Van Wijk J. J., 1999, IEEE S INF VIS; Von Luxburg U., 2010, CLUSTERING STABILITY; Xu H., 2016, ICML; Xu H., 2017, ICML; Xu Haifeng, 2015, IJCAI; Xu HT, 2017, IEEE T KNOWL DATA EN, V29, P157, DOI 10.1109/TKDE.2016.2618925; Xu Y., 2016, BIOMETRICS; YAKOWITZ SJ, 1968, ANN MATH STAT, V39, P209, DOI 10.1214/aoms/1177698520; Yang S.-H., 2013, INT C MACHINE; Zhang ZH, 2004, STAT COMPUT, V14, P343, DOI 10.1023/B:STCO.0000039484.36470.41; ZHAO Q, 2015, KDD; Zhou K., 2013, AISTATS; Zhou Ke, 2013, ICML	51	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401038
C	Xu, P; Ma, J; Gu, QQ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Pan; Ma, Jian; Gu, Quanquan			Speeding Up Latent Variable Gaussian Graphical Model Estimation via Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				COVARIANCE ESTIMATION; MATRIX COMPLETION; SELECTION; RANK	We study the estimation of the latent variable Gaussian graphical model (LVGGM), where the precision matrix is the superposition of a sparse matrix and a low-rank matrix. In order to speed up the estimation of the sparse plus low-rank components, we propose a sparsity constrained maximum likelihood estimator based on matrix factorization, and an efficient alternating gradient descent algorithm with hard thresholding to solve it. Our algorithm is orders of magnitude faster than the convex relaxation based methods for LVGGM. In addition, we prove that our algorithm is guaranteed to linearly converge to the unknown sparse and low-rank components up to the optimal statistical precision. Experiments on both synthetic and genomic data demonstrate the superiority of our algorithm over the state-of-the-art algorithms and corroborate our theory.	[Xu, Pan; Gu, Quanquan] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA; [Ma, Jian] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	University of Virginia; Carnegie Mellon University	Xu, P (corresponding author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22904 USA.	px3ds@virginia.edu; jianma@cs.cmu.edu; qg5w@virginia.edu	X, Pan/GVS-4402-2022; Ma, Jian/A-9838-2008; Xu, Pan/AAH-3620-2019	Ma, Jian/0000-0002-4202-5834; Xu, Pan/0000-0002-2559-8622	National Science Foundation [IIS-1652539, IIS-1717205, IIS-1717206]	National Science Foundation(National Science Foundation (NSF))	We would like to thank the anonymous reviewers for their helpful comments. This research was sponsored in part by the National Science Foundation IIS-1652539, IIS-1717205 and IIS-1717206. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	[Anonymous], 2015, ARXIV150903025; Balakrishnan Sivaraman, 2014, ARXIV14082156; Bhojanapalli S., 2015, DROPPING CONVEXITY F; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Cai T Tony, 2012, BIOMETRIKA; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P1610, DOI 10.1109/ALLERTON.2010.5707106; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Huang JZ, 2006, BIOMETRIKA, V93, P85, DOI 10.1093/biomet/93.1.85; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Lauritzen Steffen L., 1996, OXFORD STAT SCI SERI, V17; Liu H, 2009, J MACH LEARN RES, V10, P2295; Ma SQ, 2013, NEURAL COMPUT, V25, P2172, DOI 10.1162/NECO_a_00379; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Meng Zhaoshi, 2014, ARXIV14062721; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Tu S., 2015, ARXIV150703566; Wang CJ, 2010, SIAM J OPTIMIZ, V20, P2994, DOI 10.1137/090772514; Wang L., 2016, ARXIV161005275; Wang LX, 2017, PR MACH LEARN RES, V70; Wang LX, 2016, JMLR WORKSH CONF PRO, V51, P177; Wang Zhaoran, 2014, ARXIV14128729; Xu P., 2016, ADV NEURAL INFORM PR, P1064; Xu P, 2017, PR MACH LEARN RES, V54, P923; Xu Pan, 2016, ARXIV161209297; Yi X., 2016, ARXIV160507784; Yin JX, 2011, ANN APPL STAT, V5, P2630, DOI 10.1214/11-AOAS494; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Yuan XT, 2014, IEEE T INFORM THEORY, V60, P1673, DOI 10.1109/TIT.2013.2296784; Zhang Xiao, 2017, ARXIV170206525; Zheng Q., 2015, ADV NEURAL INFORM PR, P109	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401093
C	Xu, Z; Modayil, J; van Hasselt, H; Barreto, A; Silver, D; Schaul, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Zhongwen; Modayil, Joseph; van Hasselt, Hado; Barreto, Andre; Silver, David; Schaul, Tom			Natural Value Approximators: Learning when to Trust Past Estimates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Neural networks have a smooth initial inductive bias, such that small changes in input do not lead to large changes in output. However, in reinforcement learning domains with sparse rewards, value functions have non-smooth structure with a characteristic asymmetric discontinuity whenever rewards arrive. We propose a mechanism that learns an interpolation between a direct value estimate and a projected value estimate computed from the encountered reward and the previous estimate. This reduces the need to learn about discontinuities, and thus improves the value function approximation. Furthermore, as the interpolation is learned and state-dependent, our method can deal with heterogeneous observability. We demonstrate that this one change leads to significant improvements on multiple Atari games, when applied to the state-of-the-art A3C algorithm.	[Xu, Zhongwen; Modayil, Joseph; van Hasselt, Hado; Barreto, Andre; Silver, David; Schaul, Tom] DeepMind, London, England		Xu, Z (corresponding author), DeepMind, London, England.	zhongwen@google.com; modayil@google.com; hado@google.com; andrebarreto@google.com; davidsilver@google.com; schaul@google.com	Barreto, André M S/J-5063-2013					Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellman Richard, 1957, TECHNICAL REPORT; Gyorfi L., 2002, DISTRIBUTION FREE TH; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; John Christopher, 1989, THESIS; Kingma D.P, P 3 INT C LEARNING R; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton Richard Stuart, 1984, THESIS, P4; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; van Hasselt H, 2015, ARXIV150804582; Van Hasselt Hado, 2016, P AAAI C ART INT, V30; Wang Z., 2016, SER P MACHINE LEARNI, DOI DOI https://doi.org/10.1016/j.molstruc.2016.06.044; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402017
C	Yang, YQ; Grover, P; Kar, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yang, Yaoqing; Grover, Pulkit; Kar, Soummya			Coded Distributed Computing for Inverse Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Computationally intensive distributed and parallel computing is often bottlenecked by a small set of slow workers known as stragglers. In this paper, we utilize the emerging idea of "coded computation" to design a novel error-correcting-code inspired technique for solving linear inverse problems under specific iterative methods in a parallelized implementation affected by stragglers. Example machine-learning applications include inverse problems such as personalized PageRank and sampling on graphs. We provably show that our coded-computation technique can reduce the mean-squared error under a computational deadline constraint. In fact, the ratio of mean-squared error of replication-based and coded techniques diverges to infinity as the deadline increases. Our experiments for personalized PageRank performed on real systems and real social networks show that this ratio can be as large as 10(4). Further, unlike coded-computation techniques proposed thus far, our strategy combines outputs of all workers, including the stragglers, to produce more accurate estimates at the computational deadline. This also ensures that the accuracy degrades "gracefully" in the event that the number of stragglers is large.	[Yang, Yaoqing; Grover, Pulkit; Kar, Soummya] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Yang, YQ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	yyaoqing@andrew.cmu.edu; pgrover@andrew.cmu.edu; soummyak@andrew.cmu.edu	Jeong, Yongwook/N-7413-2016					Chen S., 2016, IEEE 2016 GLOBALSIP; Chen SH, 2015, IEEE T SIGNAL PROCES, V63, P6510, DOI 10.1109/TSP.2015.2469645; Da Wang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P7, DOI 10.1145/2847220.2847223; Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794; Dimakis A. G., 2016, GRADIENT CODING; Dimakis AG, 2010, IEEE T INFORM THEORY, V56, P4539, DOI 10.1109/TIT.2010.2054295; Dutta S., 2016, P ADV NEUR INF PROC; Ferdinand NS, 2016, ANN ALLERTON CONF, P954, DOI 10.1109/ALLERTON.2016.7852337; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Haikin M, 2016, IEEE INT SYMP INFO, P2074, DOI 10.1109/ISIT.2016.7541664; Haveliwala Taher H., 2002, P 11 INT C WORLD WID, P517, DOI DOI 10.1145/511446.511513; HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475; Joshi G, 2014, IEEE J SEL AREA COMM, V32, P989, DOI 10.1109/JSAC.2014.140518; Lee K, 2017, IEEE INT SYMP INFO, P2413, DOI 10.1109/ISIT.2017.8006962; Lee K, 2017, IEEE INT SYMP INFO, P2418, DOI 10.1109/ISIT.2017.8006963; Lee K, 2016, IEEE INT SYMP INFO, P1143, DOI 10.1109/ISIT.2016.7541478; Leskovec J., 2012, P 25 INT C NEUR INF, P539, DOI DOI 10.1109/ICDM.2012.159; Li SY, 2016, 2016 IEEE POWER & ENERGY SOCIETY INNOVATIVE SMART GRID TECHNOLOGIES CONFERENCE (ISGT), DOI 10.1109/PESGM.2016.7741282; Li S, 2017, IEEE COMMUN MAG, V55, P34, DOI 10.1109/MCOM.2017.1600894; Li SZ, 2015, ANN ALLERTON CONF, P964, DOI 10.1109/ALLERTON.2015.7447112; Longbo Huang, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2766, DOI 10.1109/ISIT.2012.6284026; Maddah-Ali MA, 2015, IEEE ACM T NETWORK, V23, P1029, DOI 10.1109/TNET.2014.2317316; Mood A., 1974, INTRO THEORY STAT; Narang SK, 2013, IEEE GLOB CONF SIG, P491, DOI 10.1109/GlobalSIP.2013.6736922; Page Lawrence, 1999, TECHNICAL REPORT; Reisizadeh A, 2017, IEEE INT SYMP INFO, P2408, DOI 10.1109/ISIT.2017.8006961; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Sandryhaila A, 2013, IEEE T SIGNAL PROCES, V61, P1644, DOI 10.1109/TSP.2013.2238935; Sathiamoorthy M, 2013, PROC VLDB ENDOW, V6, P325, DOI 10.14778/2535573.2488339; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Wang XH, 2015, IEEE T SIGNAL PROCES, V63, P2432, DOI 10.1109/TSP.2015.2411217; Yang Y., 2017, IEEE T INFORM THEORY; Zhang H., 2013, J APPL MATH; Zhang RW, 2017, NEURAL PLAST, V2017, DOI 10.1155/2017/6809745	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400068
C	Yousefnezhad, M; Zhang, DQ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yousefnezhad, Muhammad; Zhang, Daoqiang			Deep Hyperalignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper proposes Deep Hyperalignment (DHA) as a regularized, deep extension, scalable Hyperalignment (HA) method, which is well-suited for applying functional alignment to fMRI datasets with nonlinearity, high-dimensionality (broad ROI), and a large number of subjects. Unlink previous methods, DHA is not limited by a restricted fixed kernel function. Further, it uses a parametric approach, rank-m Singular Value Decomposition (SVD), and stochastic gradient descent for optimization. Therefore, DHA has a suitable time complexity for large datasets, and DHA does not require the training data when it computes the functional alignment for a new subject. Experimental studies on multi-subject fMRI analysis confirm that the DHA method achieves superior performance to other state-of-the-art HA algorithms.	[Yousefnezhad, Muhammad; Zhang, Daoqiang] Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China	Nanjing University of Aeronautics & Astronautics	Yousefnezhad, M (corresponding author), Nanjing Univ Aeronaut & Astronaut, Coll Comp Sci & Technol, Nanjing, Jiangsu, Peoples R China.	myousefnezhad@nuaa.edu.cn; dgzhang@nuaa.edu.cn	Jeong, Yongwook/N-7413-2016		National Natural Science Foundation of China [61422204, 61473149, 61732006]; NUAA Fundamental Research Funds [NE2013105]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); NUAA Fundamental Research Funds	This work was supported in part by the National Natural Science Foundation of China (61422204, 61473149, and 61732006), and NUAA Fundamental Research Funds (NE2013105).	Andrew G., 2012, 30 INT C MACH LEARN, P1247; Benton A., 2017, 5 INT C LEARN REPR I; Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707; Chen P.C., 2014, P 2014 POW SYST COMP, P1; Chen P. H, 2016, 29 WORKSH REPR LEARN; Chen P-H., 2015, NIPS, V28, P460, DOI DOI 10.5555/2969239.2969291; Duncan KJ, 2009, NEUROIMAGE, V46, P1018, DOI 10.1016/j.neuroimage.2009.03.014; Guntupalli J. S., 2016, CEREBRAL CORTEX; Haxby JV, 2014, ANNU REV NEUROSCI, V37, P435, DOI 10.1146/annurev-neuro-062012-170325; Langs G., 2010, 23 ADV NEURAL INFORM; Lorbert A., 2012, P ADV NEUR INF PROC, P1790; Rastogi P., 2015, HLT NAACL, P556; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Smola AJ, 2004, STAT COMPUT, V14, P199, DOI 10.1023/B:STCO.0000035301.49549.88; Tom SM, 2007, SCIENCE, V315, P515, DOI 10.1126/science.1134239; Wakeman DG, 2015, SCI DATA, V2, DOI 10.1038/sdata.2015.1; Walz JM, 2013, J NEUROSCI, V33, P19212, DOI 10.1523/JNEUROSCI.2649-13.2013; Wang W., 53 ANN ALL C COMM CO, P688; Xu H, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P229, DOI 10.1109/SSP.2012.6319668; Yousefnezhad M, 2017, AAAI CONF ARTIF INTE, P59	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401062
C	Yu, HZ; Li, TX; Varshney, LR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yu, Haizi; Li, Tianxi; Varshney, Lay R.			Probabilistic Rule Realization and Selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				EXPERT-SYSTEM; REGRESSION	A Abstraction and realization are bilateral processes that are key in deriving intelligence and creativity. In many domains, the two processes are approached through rules: high-level principles that reveal invariances within similar yet diverse examples. Under a probabilistic setting for discrete input spaces, we focus on the rule realization problem which generates input sample distributions that follow the given rules. More ambitiously, we go beyond a mechanical realization that takes whatever is given, but instead ask for proactively selecting reasonable rules to realize. This goal is demanding in practice, since the initial rule set may not always be consistent and thus intelligent compromises are needed. We formulate both rule realization and selection as two strongly connected components within a single and symmetric bi-convex problem, and derive an efficient algorithm that works at large scale. Taking music compositional rules as the main example throughout the paper, we demonstrate our model's efficiency in not only music realization (composition) but also music interpretation and understanding (analysis).	[Yu, Haizi] Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA; [Li, Tianxi] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; [Varshney, Lay R.] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Michigan System; University of Michigan; University of Illinois System; University of Illinois Urbana-Champaign	Yu, HZ (corresponding author), Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA.	haiziyu7@illinois.edu; tianxili@umich.edu; varshney@illinois.edu	Jeong, Yongwook/N-7413-2016		IBM-Illinois Center for Cognitive Computing Systems Research (C3SR), as part of the IBM Cognitive Horizons Network	IBM-Illinois Center for Cognitive Computing Systems Research (C3SR), as part of the IBM Cognitive Horizons Network	Supported in part by the IBM-Illinois Center for Cognitive Computing Systems Research (C3SR), a research collaboration as part of the IBM Cognitive Horizons Network.	Barry AnneMarieSeward., 1997, VISUAL INTELLIGENCE; Bengio Yoshua, 2013, Statistical Language and Speech Processing. First International Conference, SLSP 2013. Proceedings: LNCS 7978, P1, DOI 10.1007/978-3-642-39593-2_1; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; COPE D, 1987, COMPUT MUSIC J, V11, P30, DOI 10.2307/3680238; EBCIOGLU K, 1988, COMPUT MUSIC J, V12, P43, DOI 10.2307/3680335; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Friedman J, 2010, STATISTICS-ABINGDON; Fux J. J, 1725, GRADUS AD PARNASSUM; Haase K., 1987, 293 MIT AI LAB; Hong M., 2012, MATH PROGRAM, V162, P1; JACKSON DD, 1972, GEOPHYS J ROY ASTR S, V28, P97, DOI 10.1111/j.1365-246X.1972.tb06115.x; Lewin K., 1951, FIELD THEORY SOCIAL; Pierce J. R., 1949, MM4915029 BEL TEL LA; Schenker H., 1922, KONTRAPUNKT; Skorstad J., 1988, P 10 ANN C COGN SCI; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Wang J., 2014, ADV NEURAL INFORM PR, P2132; Wang W., 2013, ARXIV13091541CSLG; Yu H., 2017, P 5 INT C LEARN REPR; Yu H., 2016, P 2016 ICML WORKSH I; Yu H., 2016, P 4 INT WORKSH MUS M; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401058
C	Yurochkin, M; Nguyen, X; Vasiloglou, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yurochkin, Mikhail; Nguyen, XuanLong; Vasiloglou, Nikolaos			Multi-way Interacting Regression via Factorization Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIABLE SELECTION	We propose a Bayesian regression method that accounts for multi-way interactions of arbitrary orders among the predictor variables. Our model makes use of a factorization mechanism for representing the regression coefficients of interactions among the predictors, while the interaction selection is guided by a prior distribution on random hypergraphs, a construction which generalizes the Finite Feature Model. We present a posterior inference algorithm based on Gibbs sampling, and establish posterior consistency of our regression model. Our method is evaluated with extensive experiments on simulated data and demonstrated to be able to identify meaningful interactions in applications in genetics and retail demand forecasting.(1)	[Yurochkin, Mikhail; Nguyen, XuanLong] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA; [Vasiloglou, Nikolaos] LogicBlox, Atlanta, GA USA	University of Michigan System; University of Michigan	Yurochkin, M (corresponding author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	moonfolk@umich.edu; xuanlong@umich.edu; nikolaos.vasiloglou@logicblox.com	Jeong, Yongwook/N-7413-2016		NSF CAREER [DMS-1351362]; NSF [CNS-1409303]; Margaret and Herman Sokol Faculty Award	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); Margaret and Herman Sokol Faculty Award	This research is supported in part by grants NSF CAREER DMS-1351362, NSF CNS-1409303, a research gift from Adobe Research and a Margaret and Herman Sokol Faculty Award.	Ai CR, 2003, ECON LETT, V80, P123, DOI 10.1016/S0165-1765(03)00032-6; Brambor T, 2006, POLIT ANAL, V14, P63, DOI 10.1093/pan/mpi014; Cheng C, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P265, DOI 10.1145/2645710.2645730; Cordell HJ, 2009, NAT REV GENET, V10, P392, DOI 10.1038/nrg2579; Cristianini N., 2000, INTRO SUPPORT VECTOR; Fan JQ, 2010, STAT SINICA, V20, P101; Freudenthaler C., 2011, BAYESIAN FACTORIZATI; Ghosal S, 1999, ANN STAT, V27, P143; Griffiths T.L., 2005, ADV NEURAL INFORM PR; Griffiths TL, 2011, J MACH LEARN RES, V12, P1185; Harshman Richard A., 1970, FDN PARAFAC PROCEDUR; Himmelstein DS, 2011, BIODATA MIN, V4, DOI 10.1186/1756-0381-4-21; Nguyen TV, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P63, DOI 10.1145/2600428.2609623; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Rendle S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P635; Templeton A. R., 2000, EPISTASIS EVOLUTIONA, P41; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Zhu J, 2004, ADV NEUR IN, V16, P49; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402063
C	Zhang, LP; Tang, K; Yao, X		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhang, Liangpeng; Tang, Ke; Yao, Xin			Log-normality and Skewness of Estimated State/Action Values in Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				APPROXIMATION	Under/overestimation of state/action values are harmful for reinforcement learning agents. In this paper, we show that a state/action value estimated using the Bellman equation can be decomposed to a weighted sum of path-wise values that follow log-normal distributions. Since log-normal distributions are skewed, the distribution of estimated state/action values can also be skewed, leading to an imbalanced likelihood of under/overestimation. The degree of such imbalance can vary greatly among actions and policies within a single problem instance, making the agent prone to select actions/policies that have inferior expected return and higher likelihood of overestimation. We present a comprehensive analysis to such skewness, examine its factors and impacts through both theoretical and empirical results, and discuss the possible ways to reduce its undesirable effects.	[Zhang, Liangpeng; Tang, Ke] Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China; [Zhang, Liangpeng; Yao, Xin] Univ Birmingham, Birmingham, W Midlands, England; [Tang, Ke; Yao, Xin] Southern Univ Sci & Technol, Shenzhen Key Lab Computat Intelligence, Dept Comp Sci & Engn, Shenzhen, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; University of Birmingham; Southern University of Science & Technology	Zhang, LP (corresponding author), Univ Sci & Technol China, Sch Comp Sci & Technol, Hefei, Anhui, Peoples R China.; Zhang, LP (corresponding author), Univ Birmingham, Birmingham, W Midlands, England.	lxz472@cs.bham.ac.uk; tangk3@sustc.edu.cn; xiny@sustc.edu.cn	Jeong, Yongwook/N-7413-2016; YAO, XIN/W-2158-2018	YAO, XIN/0000-0001-8837-4442	Ministry of Science and Technology of China [2017YFB1003102]; National Natural Science Foundation of China [61672478, 61329302]; Science and Technology Innovation Committee Foundation of Shenzhen [ZDSYS201703031748284]; EPSRC [J017515/1]; Royal Society Newton Advanced Fellowship [NA150123]	Ministry of Science and Technology of China(Ministry of Science and Technology, China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Science and Technology Innovation Committee Foundation of Shenzhen; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Royal Society Newton Advanced Fellowship	This paper was supported by Ministry of Science and Technology of China (Grant No. 2017YFB1003102), the National Natural Science Foundation of China (Grant Nos. 61672478 and 61329302), the Science and Technology Innovation Committee Foundation of Shenzhen (Grant No. ZDSYS201703031748284), EPSRC (Grant No. J017515/1), and in part by the Royal Society Newton Advanced Fellowship (Reference No. NA150123).	Asmuth J, 2008, P 23 AAAI C ARTIFICI, P604; Beaulieu NC, 2004, IEEE T VEH TECHNOL, V53, P479, DOI 10.1109/TVT.2004.823494; Bellemare MG, 2016, AAAI CONF ARTIF INTE, P1476; Bertsekas DP, 2012, MATH OPER RES, V37, P66, DOI 10.1287/moor.1110.0532; Browne CB, 2012, IEEE T COMP INTEL AI, V4, P1, DOI 10.1109/TCIAIG.2012.2186810; Casella G, 2002, DUXBURY; D'Eramo C, 2017, AAAI CONF ARTIF INTE, P1840; DAYAN P, 1992, MACH LEARN, V8, P341, DOI 10.1007/BF00992701; Doane DP, 2011, J STAT EDUC, V19, DOI 10.1080/10691898.2011.11889611; Hasselt H, 2010, ADV NEURAL INFORM PR, V23, P2613, DOI DOI 10.5555/2997046.2997187; Hotelling H, 1932, ANN MATH STAT, V3, P141, DOI 10.1214/aoms/1177732911; Jiang N, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P1181; Kakade S, 2002, ADV NEUR IN, V14, P1531; Kocsis L, 2006, LECT NOTES COMPUT SC, V4212, P282, DOI 10.1007/11871842_29; Lee D, 2013, IEEE SYMP ADAPT DYNA, P93, DOI 10.1109/ADPRL.2013.6614994; Littman M. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P394; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; OEHLERT GW, 1992, AM STAT, V46, P27, DOI 10.2307/2684406; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Szepesvari C., 1997, NEURAL INFORM PROCES, V10, P1064; Thrun Sebastian, 1993, P 1993 CONN MOD SUMM, P255; TSITSIKLIS JN, 1994, MACH LEARN, V16, P185, DOI 10.1023/A:1022689125041; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; van Seijen H, 2016, J MACH LEARN RES, V17; Wagner P, 2014, NEURAL NETWORKS, V52, P43, DOI 10.1016/j.neunet.2014.01.002; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Zhang LP, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4033	32	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401081
C	Zhao, H; Gordon, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhao, Han; Gordon, Geoff			Linear Time Computation of Moments in Sum-Product Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Bayesian online algorithms for Sum-Product Networks (SPNs) need to update their posterior distribution after seeing one single additional instance. To do so, they must compute moments of the model parameters under this distribution. The best existing method for computing such moments scales quadratically in the size of the SPN, although it scales linearly for trees. This unfortunate scaling makes Bayesian online algorithms prohibitively expensive, except for small or tree-structured SPNs. We propose an optimal linear-time algorithm that works even when the SPN is a general directed acyclic graph (DAG), which significantly broadens the applicability of Bayesian online algorithms for SPNs. There are three key ingredients in the design and analysis of our algorithm: 1). For each edge in the graph, we construct a linear time reduction from the moment computation problem to a joint inference problem in SPNs. 2). Using the property that each SPN computes a multilinear polynomial, we give an efficient procedure for polynomial evaluation by differentiation without expanding the network that may contain exponentially many monomials. 3). We propose a dynamic programming method to further reduce the computation of the moments of all the edges in the graph from quadratic to linear. We demonstrate the usefulness of our linear time algorithm by applying it to develop a linear time assume density filter (ADF) for SPNs.	[Zhao, Han; Gordon, Geoff] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Zhao, H (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	han.zhao@cs.cmu.edu; ggordon@cs.cmu.edu	Jeong, Yongwook/N-7413-2016		ONR [N000141512365]	ONR(Office of Naval Research)	HZ thanks Pascal Poupart for providing insightful comments. HZ and GG are supported in part by ONR award N000141512365.	Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115; Csiszar I, 2003, IEEE T INFORM THEORY, V49, P1474, DOI 10.1109/TIT.2003.810633; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Dennis A., 2015, INT JOINT C ART INT, V24; Gens R., 2012, 26 ADV NEURAL INFORM, P3239; Gens R., 2013, 30 INT C MACHINE LEA, P873; Jaini P., 2016, C PROB GRAPH MOD, P228; Park JD, 2004, ARTIF INTELL, V156, P197, DOI 10.1016/j.artint.2003.04.004; Peharz R., 2015, AISTATS; Peharz R, 2017, IEEE T PATTERN ANAL, V39, P2030, DOI 10.1109/TPAMI.2016.2618381; Poon H., 2011, P 12 C UNC ART INT, P2551; Rashwan A, 2016, JMLR WORKSH CONF PRO, V51, P1469; Rooshenas A, 2014, ICML; SORENSON HW, 1968, INT J CONTROL, V8, P33, DOI 10.1080/00207176808905650; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6; Zhao H., 2015, ICML; Zhao H., 2016, NIPS; Zhao Han, 2016, ICML	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406092
C	Abbe, E; Sandon, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Abbe, Emmanuel; Sandon, Colin			Achieving the KS threshold in the general stochastic block model with linearized acyclic belief propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS	The stochastic block model (SBM) has long been studied in machine learning and network science as a canonical model for clustering and community detection. In the recent years, new developments have demonstrated the presence of threshold phenomena for this model, which have set new challenges for algorithms. For the detection problem in symmetric SBMs, Decelle et al. conjectured that the so-called Kesten-Stigum (KS) threshold can be achieved efficiently. This was proved for two communities, but remained open for three and more communities. We prove this conjecture here, obtaining a general result that applies to arbitrary SBMs with linear size communities. The developed algorithm is a linearized acyclic belief propagation (ABP) algorithm, which mitigates the effects of cycles while provably achieving the KS threshold in O(n ln n) time. This extends prior methods by achieving universally the KS threshold while reducing or preserving the computational complexity. ABP is also connected to a power iteration method on a generalized nonbacktracking operator, formalizing the spectral-message passing interplay described in Krzakala et al., and extending results from Bordenave et al.	[Abbe, Emmanuel] Princeton Univ, Appl & Computat Math & EE Dept, Princeton, NJ 08544 USA; [Sandon, Colin] Princeton Univ, Dept Math, Princeton, NJ 08544 USA	Princeton University; Princeton University	Abbe, E (corresponding author), Princeton Univ, Appl & Computat Math & EE Dept, Princeton, NJ 08544 USA.	eabbe@princeton.edu; sandon@princeton.edu			NSF CAREER Award [CCF-1552131]; ARO [W911NF-16-1-0051]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); ARO	This research was supported by NSF CAREER Award CCF-1552131 and ARO grant W911NF-16-1-0051.	Abbe E., 2015, ARXIV151209080; [Anonymous], 2014, ARXIV14043918; Banks J., 2016, ARXIV160102658; Bickel P. J., 2014, ARXIV14013915; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168; Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22; Bordenave C, 2015, ANN IEEE SYMP FOUND, P1347, DOI 10.1109/FOCS.2015.86; BUI TN, 1987, COMBINATORICA, V7, P171, DOI 10.1007/BF02579448; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Gao C., 2015, ARXIV E PRINTS; Guedon O, 2016, PROBAB THEORY REL, V165, P1025, DOI 10.1007/s00440-015-0659-z; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Montanari A, 2016, ACM S THEORY COMPUT, P814, DOI 10.1145/2897518.2897548; Mossel E., 2014, PROOF BLOCK MODEL TH; Murphy KP, 1999, P UAI 99, P467	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703001
C	Ahn, S; Chertkov, M; Shin, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ahn, Sungsoo; Chertkov, Michael; Shin, Jinwoo			Synthesis of MCMC and Belief Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				TIME; ALGORITHMS; BETHE	Markov Chain Monte Carlo (MCMC) and Belief Propagation (BP) are the most popular algorithms for computational inference in Graphical Models (GM). In principle, MCMC is an exact probabilistic method which, however, often suffers from exponentially slow mixing. In contrast, BP is a deterministic method, which is typically fast, empirically very successful, however in general lacking control of accuracy over loopy graphs. In this paper, we introduce MCMC algorithms correcting the approximation error of BP, i.e., we provide a way to compensate for BP errors via a consecutive BP-aware MCMC. Our framework is based on the Loop Calculus approach which allows to express the BP error as a sum of weighted generalized loops. Although the full series is computationally intractable, it is known that a truncated series, summing up all 2-regular loops, is computable in polynomial-time for planar pair-wise binary GMs and it also provides a highly accurate approximation empirically. Motivated by this, we first propose a polynomial-time approximation MCMC scheme for the truncated series of general (non-planar) pair-wise binary models. Our main idea here is to use the Worm algorithm, known to provide fast mixing in other (related) problems, and then design an appropriate rejection scheme to sample 2-regular loops. Furthermore, we also design an efficient rejection-free MCMC scheme for approximating the full series. The main novelty underlying our design is in utilizing the concept of cycle basis, which provides an efficient decomposition of the generalized loops. In essence, the proposed MCMC schemes run on transformed GM built upon the non-trivial BP solution, and our experiments show that this synthesis of BP and MCMC outperforms both direct MCMC and bare BP schemes.	[Ahn, Sungsoo; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Chertkov, Michael] Los Alamos Natl Lab, Theoret Div, T-4, Los Alamos, NM 87545 USA; [Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA; [Chertkov, Michael] Skolkovo Inst Sci & Technol, Moscow 143026, Russia	Korea Advanced Institute of Science & Technology (KAIST); United States Department of Energy (DOE); Los Alamos National Laboratory; United States Department of Energy (DOE); Los Alamos National Laboratory; Skolkovo Institute of Science & Technology	Ahn, S (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	sungsoo.ahn@kaist.ac.kr; chertkov@lan1.gov; jinwoos@kaist.ac.kr	Chertkov, Michael/O-8828-2015	Chertkov, Michael/0000-0002-6758-515X	National Research Council of Science & Technology (NST) grant by the Korea government (MSIP) [CRC-15-05-ETRI]; U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative	National Research Council of Science & Technology (NST) grant by the Korea government (MSIP); U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative(United States Department of Energy (DOE))	This work was supported by the National Research Council of Science & Technology (NST) grant by the Korea government (MSIP) (No. CRC-15-05-ETRI), and funding from the U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative.	Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Bang J, 2008, DIGRAPHS THEORY ALGO; Baxter R. J., 2007, EXACTLY SOLVED MODEL; Chandrasekaran V., 2008, ASS UNCERTAINTY ARTI; Chertkov M, 2006, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2006/06/P06009; Chertkov M, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/05/P05003; Collevecchio A., 2015, ARXIV150903201; Dyer M, 2002, SIAM J COMPUT, V31, P1527, DOI 10.1137/S0097539701383844; Freeman WT, 2000, INT J COMPUT VISION, V40, P25, DOI 10.1023/A:1026501619075; GALLAGER RG, 1962, IRE T INFORM THEOR, V8, P21, DOI 10.1109/tit.1962.1057683; Gomez V, 2010, J MACH LEARN RES, V11, P1273; HORTON JD, 1987, SIAM J COMPUT, V16, P358, DOI 10.1137/0216026; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Jordan M. I., 1998, SPRINGER SCI BUSINES, V89; Kasteleyn P. W., 2009, CLASSIC PAPERS COMBI, P281; KIRKPATRICK S, 1984, J STAT PHYS, V34, P975, DOI 10.1007/BF01009452; Kramers HA, 1941, PHYS REV, V60, P263, DOI 10.1103/PhysRev.60.263; Kschischang FR, 1998, IEEE J SEL AREA COMM, V16, P219, DOI 10.1109/49.661110; Nicholas R., 2012, ADV NEURAL INFORM PR; Pearl J., 2014, MORGAN KAUFMANN; Pearl J., 1982, ASS ADV ARTIFICIAL I; Schweinsberg J, 2002, RANDOM STRUCT ALGOR, V20, P59, DOI 10.1002/rsa.10000; Shin J, 2014, IEEE T INFORM THEORY, V60, P3959, DOI 10.1109/TIT.2014.2317487; Teh Y. W., 2001, P 18 C UNC ART INT, P493; Yuille AL, 2002, NEURAL COMPUT, V14, P1691, DOI 10.1162/08997660260028674	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702065
C	Alaa, AM; van der Schaar, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Alaa, Ahmed M.; van der Schaar, Mihaela			Balancing Suspense and Surprise: Timely Decision Making with Endogenous Information Acquisition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					[We develop a Bayesian model for decision-making under time pressure with endogenous information acquisition. In our model, the decision-maker decides when to observe (costly) information by sampling an underlying continuous-time stochastic process (time series) that conveys information about the potential occurrence/non-occurrence of an adverse event which will terminate the decision-making process. In her attempt to predict the occurrence of the adverse event, the decision-maker follows a policy that determines when to acquire information from the time series (continuation), and when to stop acquiring information and make a final prediction (stopping). We show that the optimal policy has a "rendezvous" structure, i.e. a structure in which whenever a new information sample is gathered from the time series, the optimal "date" for acquiring the next sample becomes computable. The optimal interval between two information samples balances a trade-off between the decision maker's "surprise", i.e. the drift in her posterior belief after observing new information, and "suspense", i.e. the probability that the adverse event occurs in the time interval between two information samples. Moreover, we characterize the continuation and stopping regions in the decision-maker's state-space, and show that they depend not only on the decision-maker's beliefs, but also on the "context", i.e. the current realization of the time series.	[Alaa, Ahmed M.; van der Schaar, Mihaela] Univ Calif Los Angeles, Elect Engn Dept, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Alaa, AM (corresponding author), Univ Calif Los Angeles, Elect Engn Dept, Los Angeles, CA 90095 USA.				ONR; NSF [ECCS 1462245]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This work was supported by the ONR and the NSF (Grant number: ECCS 1462245).	[Anonymous], 2012, ADV NEURAL INFORM PR; [Anonymous], 1978, STOCHASTIC OPTIMAL C; Banerjee T, 2012, SEQUENTIAL ANAL, V31, P40, DOI 10.1080/07474946.2012.651981; Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700; Bortfeld T, 2015, INFORMS J COMPUT, V27, P788, DOI 10.1287/ijoc.2015.0659; Chalfin DB, 2007, CRIT CARE MED, V35, P1477, DOI 10.1097/01.CCM.0000266585.74905.5A; Dayanik S, 2013, SIAM J CONTROL OPTIM, V51, P2922, DOI 10.1137/100818005; Drugowitsch J., 2014, ADV NEURAL INFORM PR, V27, P748; Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012; Ely J, 2015, J POLIT ECON, V123, P215, DOI 10.1086/677350; Frazier P., 2007, ADV NEURAL INF PROCE, P465; ITTI L, 2009, VISION RES, V49, P1295, DOI DOI 10.1016/J.VISRES.2008.09.007; Khalvati K., 2015, ADV NEURAL INFORM PR, P2404; Peskir G., 2006, LEC MATH; Schulam P., 2015, NIPS; Shapiro S, 1998, INT J EPIDEMIOL, V27, P735, DOI 10.1093/ije/27.5.735; Shenoy P., 2012, ADV NEURAL INFORM PR, V25, P2123, DOI DOI 10.5555/2999325.2999372; Shiryaev A.N., 2007, OPTIMAL STOPPING RUL, V8; Shreve S.E., 2004, STOCHASTIC CALCULUS, V11; Shvartsman M., 2015, ADV NEURAL INF PROCE, P2476; Simen P, 2011, FRONT INTEGR NEUROSC, V5, DOI [10.3389/fnint.2011.00028, 10.3389/fnint.2011.00056]; Wald A., 1973, SEQUENTIAL ANAL; Yu A. J., 2013, ADV NEURAL INFORM PR, V26, P2607; Yu AJ, 2009, J EXP PSYCHOL HUMAN, V35, P700, DOI 10.1037/a0013553	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700026
C	Allen-Zhu, Z; Yuan, Y; Sridharan, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Allen-Zhu, Zeyuan; Yuan, Yang; Sridharan, Karthik			Exploiting the Structure: Stochastic Gradient Methods Using Raw Clusters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The amount of data available in the world is growing faster than our ability to deal with it. However, if we take advantage of the internal structure, data may become much smaller for machine learning purposes. In this paper we focus on one of the fundamental machine learning tasks, empirical risk minimization (ERM), and provide faster algorithms with the help from the clustering structure of the data. We introduce a simple notion of raw clustering that can be efficiently computed from the data, and propose two algorithms based on clustering information. Our accelerated algorithm ClusterACDM is built on a novel Haar transformation applied to the dual space of the ERM problem, and our variance-reduction based algorithm ClusterSVRG introduces a new gradient estimator using clustering. Our algorithms outperform their classical counterparts ACDM and SVRG respectively.	[Allen-Zhu, Zeyuan] Princeton Univ, IAS, Princeton, NJ 08544 USA; [Yuan, Yang; Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA	Institute for Advanced Study - USA; Princeton University; Cornell University	Allen-Zhu, Z (corresponding author), Princeton Univ, IAS, Princeton, NJ 08544 USA.	zeyuan@csail.mit.edu; yangyuan@cs.cornell.edu; sridharan@cs.cornell.edu						Allen-Zhu Zeyuan, 2016, ICML; Andoni Alexandr, 2015, ADV NEURAL INFORM PR, P1225; [Anonymous], 2015, NIPS; Bottou L., STOCHASTIC GRADIENT; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; DEFAZIO A, 2014, NIPS; Fan Rong-En, LIBSVM DATA CLASSIFI; Frostig Roy, 2015, ICML, V37, P1; Ge TZ, 2014, IEEE T PATTERN ANAL, V36, P744, DOI 10.1109/TPAMI.2013.240; Hofmann T., 2015, ADV NEURAL INFORM PR, V28, P2305; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Schmidt M. W., 2013, MINIMIZING FINITE SU; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; Shalev-Shwartz Shai, 2012, ARXIV12112717, P1; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yang TT, 2016, DEEP-SEA RES PT II, V129, P282, DOI 10.1016/j.dsr2.2014.01.014; Zhang T., 2004, ICML; ZHANG Y, 2015, ICML; Zhao PL, 2015, PR MACH LEARN RES, V37, P1; [No title captured]; [No title captured]; [No title captured]; [No title captured]	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703039
C	Allen-Zhu, Z; Hazan, E		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Allen-Zhu, Zeyuan; Hazan, Elad			Optimal Black-Box Reductions Between Optimization Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MINIMIZATION	The diverse world of machine learning applications has given rise to a plethora of algorithms and optimization methods, finely tuned to the specific regression or classification task at hand. We reduce the complexity of algorithm design for machine learning by reductions: we develop reductions that take a method developed for one setting and apply it to the entire spectrum of smoothness and strong-convexity in applications. Furthermore, unlike existing results, our new reductions are optimal and more practical. We show how these new reductions give rise to new and faster running times on training linear classifiers for various families of loss functions, and conclude with experiments showing their successes also in practice.	[Allen-Zhu, Zeyuan] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Allen-Zhu, Zeyuan; Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA	Institute for Advanced Study - USA; Princeton University	Allen-Zhu, Z (corresponding author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.; Allen-Zhu, Z (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	zeyuan@csail.mit.edu; ehazan@cs.princeton.edu			NSF [1523815]; Microsoft Research Grant [0518584]	NSF(National Science Foundation (NSF)); Microsoft Research Grant(Microsoft)	This paper is partially supported by an NSF Grant, no. 1523815, and a Microsoft Research Grant, no. 0518584.	Allen-Zhu Zeyuan, 2016, ICML; [Anonymous], ARXIV E PRINTS; [Anonymous], 2012, ICML; [Anonymous], 2016, NIPS; [Anonymous], 2015, NIPS; Bot RI, 2015, TOP, V23, P124, DOI 10.1007/s11750-014-0326-z; Bubeck S., 2015, ARXIV E PRINTS; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; DEFAZIO A, 2014, NIPS; Fan Rong-En, LIBSVM DATA CLASSIFI; Frostig Roy, 2015, ICML, V37, P1; Hazan Elad, 2015, FDN TRENDS MACHINE L, VXX, P1; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; LACOSTEJULIEN S, 2012, ARXIV E PRINTS; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Orabona F., 2012, ARXIV12062372; Schmidt M. W., 2013, MINIMIZING FINITE SU; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shalev-Shwartz Shai, 2012, ARXIV12112717, P1; Tran-Dinh Q, 2015, ARXIV150900106; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997; ZHANG Y, 2015, ICML; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702111
C	Allen-Zhu, Z; Li, YZ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Allen-Zhu, Zeyuan; Li, Yuanzhi			LazySVD: Even Faster SVD Decomposition Yet Without Agonizing Pain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study k-SVD that is to obtain the first k singular vectors of a matrix A. Recently, a few breakthroughs have been discovered on k-SVD: Musco and Musco [19] proved the first gap-free convergence result using the block Krylov method, Shamir [21] discovered the first variance-reduction stochastic method, and Bhojanapalli et al. [7] provided the fastest O(nnz(A) + poly(1/epsilon))-time algorithm using alternating minimization. In this paper, we put forward a new and simple LazySVD framework to improve the above breakthroughs. This framework leads to a faster gap-free method outperforming [19], and the first accelerated and stochastic method outperforming [21]. In the O(nnz(A) + poly(1=epsilon)) running-time regime, LazySVD outperforms [7] in certain parameter regimes without even using alternating minimization.	[Allen-Zhu, Zeyuan] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Allen-Zhu, Zeyuan; Li, Yuanzhi] Princeton Univ, Princeton, NJ 08544 USA	Institute for Advanced Study - USA; Princeton University	Allen-Zhu, Z (corresponding author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.; Allen-Zhu, Z (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	zeyuan@csail.mit.edu; yuanzhil@cs.princeton.edu	Li, Yuan/GXV-1310-2022		Microsoft Research Award [0518584]; NSF [CCF-1412958]	Microsoft Research Award(Microsoft); NSF(National Science Foundation (NSF))	The full version of this paper can be found on https://arxiv.org/abs/1607.03463.This paper is partially supported by a Microsoft Research Award, no. 0518584, and an NSF grant, no. CCF-1412958.	Allen-Zhu Zeyuan, 2017, ARXIV E PRINTS; Allen-Zhu Zeyuan, 2016, ICML; [Anonymous], ARXIV E PRINTS; Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794; Bhojanapalli S., 2015, P 2015 ANN ACM SIAM, P902, DOI DOI 10.1137/1.9781611973730.62; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Drineas Petros, 2011, ARXIV E PRINTS; Fan Rong-En, LIBSVM DATA CLASSIFI; Garber D., 2015, ARXIV150905647; Garber Dan, 2016, ICML; Golub G. H., 2012, MATRIX COMPUTATIONS; Jin C., 2016, COLT; Leskovec J, 2014, SNAP DATASETS STANFO; Li Chris J., 2016, ARXIV E PRINTS; Li RC, 2015, NUMER MATH, V131, P83, DOI 10.1007/s00211-014-0681-6; Musco C, 2015, ADV NEUR IN, V28; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Shamir Ohad, 2016, ICML; Tropp Joel A., 2015, ARXIV E PRINTS	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704032
C	Anagnostopoulos, A; Lacki, J; Lattanzi, S; Leonardi, S; Mahdian, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Anagnostopoulos, Aris; Lacki, Jakub; Lattanzi, Silvio; Leonardi, Stefano; Mahdian, Mohammad			Community Detection on Evolving Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				STOCHASTIC BLOCKMODELS	Clustering is a fundamental step in many information-retrieval and data-mining applications. Detecting clusters in graphs is also a key tool for finding the community structure in social and behavioral networks. In many of these applications, the input graph evolves over time in a continual and decentralized manner, and, to maintain a good clustering, the clustering algorithm needs to repeatedly probe the graph. Furthermore, there are often limitations on the frequency of such probes, either imposed explicitly by the online platform (e.g., in the case of crawling proprietary social networks like twitter) or implicitly because of resource limitations (e.g., in the case of crawling the web). In this paper, we study a model of clustering on evolving graphs that captures this aspect of the problem. Our model is based on the classical stochastic block model, which has been used to assess rigorously the quality of various static clustering methods. In our model, the algorithm is supposed to reconstruct the planted clustering, given the ability to query for small pieces of local information about the graph, at a limited rate. We design and analyze clustering algorithms that work in this model, and show asymptotically tight upper and lower bounds on their accuracy. Finally, we perform simulations, which demonstrate that our main asymptotic results hold true also in practice.	[Anagnostopoulos, Aris; Lacki, Jakub; Leonardi, Stefano] Sapienza Univ Rome, Rome, Italy; [Lattanzi, Silvio; Mahdian, Mohammad] Google, Mountain View, CA USA	Sapienza University Rome; Google Incorporated	Anagnostopoulos, A (corresponding author), Sapienza Univ Rome, Rome, Italy.	aris@dis.uniroma1.it; j.lacki@mimuw.edu.pl; silviol@google.com; leonardi@dis.uniroma1.it; mahdian@google.com			EU FET project MULTIPLEX [317532]; Google Focused Award on "Algorithms for Large-scale Data Analysis"	EU FET project MULTIPLEX; Google Focused Award on "Algorithms for Large-scale Data Analysis"(Google Incorporated)	We would like to thank Marek Adamczyk for helping in some mathematical derivations. This work is partly supported by the EU FET project MULTIPLEX no. 317532 and the Google Focused Award on "Algorithms for Large-scale Data Analysis."	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Anagnostopoulos A., 2012, ITCS, P149; [Anonymous], 2010, NETWORKS INTRO, DOI DOI 10.1093/ACPROF:OSO/9780199206650.001.0001; Bahmani B., 2012, P 18 ACM SIGKDD INT, P24, DOI DOI 10.1145/2339530.2339539; Bshouty N, 2010, ICML, P135; Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Hartmann Tanja, 2014, ABS14013516 CORR; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113; Kumar R., 2006, P 12 ACM SIGKDD INT, P611; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Massoulie L, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P694, DOI 10.1145/2591796.2591857; Mossel E, 2015, ACM S THEORY COMPUT, P69, DOI 10.1145/2746539.2746603; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Xu Kevin S., 2013, Social Computing, Behavioral-Cultural Modeling and Prediction. 6th International Conference, SBP 2013. Proceedings, P201, DOI 10.1007/978-3-642-37210-0_22; Xu Kevin S., 2014, ABS14115404 CORR; Zreik Rawya, 2016, COMPUTATION STAT, P1	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701030
C	Anava, O; Karnin, Z		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Anava, Oren; Karnin, Zohar			Multi-armed Bandits: Competing with Optimal Sequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS	We consider sequential decision making problem in the adversarial setting, where regret is measured with respect to the optimal sequence of actions and the feedback adheres the bandit setting. It is well-known that obtaining sublinear regret in this setting is impossible in general, which arises the question of when can we do better than linear regret? Previous works show that when the environment is guaranteed to vary slowly and furthermore we are given prior knowledge regarding its variation (i.e., a limit on the amount of changes suffered by the environment), then this task is feasible. The caveat however is that such prior knowledge is not likely to be available in practice, which causes the obtained regret bounds to be somewhat irrelevant. Our main result is a regret guarantee that scales with the variation parameter of the environment, without requiring any prior knowledge about it whatsoever. By that, we also resolve an open problem posted by Gur, Zeevi and Besbes [8]. An important key component in our result is a statistical test for identifying non-stationarity in a sequence of independent random variables. This test either identifies non-stationarity or upper-bounds the absolute deviation of the corresponding sequence of mean values in terms of its total variation. This test is interesting on its own right and has the potential to be found useful in additional settings.	[Anava, Oren] Voleon Grp, Berkeley, CA 94704 USA; [Karnin, Zohar] Yahoo Res, New York, NY USA		Anava, O (corresponding author), Voleon Grp, Berkeley, CA 94704 USA.	oren@voleon.com; zkarnin@yahoo-inc.com						Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Bertsimas D, 2000, OPER RES, V48, P80, DOI 10.1287/opre.48.1.80.12444; Besbes O., 2014, ADV NEURAL INFORM PR, P199; Bousquet O., 2003, Journal of Machine Learning Research, V3, P363, DOI 10.1162/153244303321897654; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Gheshlaghi-Azar M., 2014, P 31 INT C MACH LEAR, V32, P1557; Guha S, 2007, ANN IEEE SYMP FOUND, P483, DOI 10.1109/FOCS.2007.23; Hazan E., 2009, P 26 ANN INT C MACHI, P393; Hazan E, 2011, J MACH LEARN RES, V12, P1287; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Jadbabaie Ali, 2015, JMLR WORKSHOP C P, V38; Ortner R, 2014, THEOR COMPUT SCI, V558, P62, DOI 10.1016/j.tcs.2014.09.026; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702088
C	Anava, O; Levy, KY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Anava, Oren; Levy, Kfir Y.			k*-Nearest Neighbors: From Global to Local	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CLASSIFICATION	The weighted k-nearest neighbors algorithm is one of the most fundamental non parametric methods in pattern recognition and machine learning. The question of setting the optimal number of neighbors as well as the optimal weights has received much attention throughout the years, nevertheless this problem seems to have remained unsettled. In this paper we offer a simple approach to locally weighted regression/classification, where we make the bias-variance tradeoff explicit. Our formulation enables us to phrase a notion of optimal weights, and to efficiently find these weights as well as the optimal number of neighbors efficiently and adaptively, for each data point whose value we wish to estimate. The applicability of our approach is demonstrated on several datasets, showing superior performance over standard locally weighted methods.	[Anava, Oren] Voleon Grp, Berkeley, CA 94704 USA; [Levy, Kfir Y.] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Anava, O (corresponding author), Voleon Grp, Berkeley, CA 94704 USA.	oren@voleon.com; yehuda.levy@inf.ethz.ch						ABRAMSON IS, 1982, ANN STAT, V10, P1217, DOI 10.1214/aos/1176345986; Adeniyi D. A., 2016, Applied Computing and Informatics, V12, P90, DOI 10.1016/j.aci.2014.10.001; Biau Gerard, 2015, LECT NEAREST NEIGHBO, V1; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Demir S., 2010, HACETTEPE J MATH STA, V39; Devroye L, 2013, PROBABILISTIC THEORY, V31; DEVROYE LP, 1980, ANN STAT, V8, P231, DOI 10.1214/aos/1176344949; Fix E, 1951, TECH REP; Ghosh AK, 2007, J COMPUT GRAPH STAT, V16, P482, DOI 10.1198/106186007x208380; Imandoust S. B., 2013, INT J ENG RES APPL, V3, P605; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jabbar MA, 2013, PROC TECH, V10, P85, DOI 10.1016/j.protcy.2013.12.340; Jianqing Fan, 1996, LOCAL POLYNOMIAL MOD, V66; Khulood H. A., 2014, SCI RES ESSAYS, V9, P966, DOI [10.5897/SRE2014.6121, DOI 10.5897/SRE2014.6121]; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Li Baoli, 2004, ACM TRANS ASIAN LANG, V3, P215, DOI [DOI 10.1145/1039621.1039623, 10.1145/1039621.1039623]; Nadaraya E.A., 1964, THEOR PROBAB APPL+, V9, P141, DOI [DOI 10.1137/1109020, 10.1137/1109020]; Silverman B. W., 1986, DENSITY ESTIMATION S, V26; Sun Shiliang, 2010 7 INT C FUZZ SY; Trstenjak B, 2014, PROCEDIA ENGINEER, V69, P1356, DOI 10.1016/j.proeng.2014.03.129; Watson G.S., 1964, SANKHYA SER A, V26, P359, DOI DOI 10.2307/25049340; WETTSCHERECK D, 1994, P ADV NEURAL INF PRO, P184; Wu XD, 2008, KNOWL INF SYST, V14, P1, DOI 10.1007/s10115-007-0114-2	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703009
C	Nguyen, AT; Xu, J; Yang, Z		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Anh Tuan Nguyen; Xu, Jian; Yang, Zhi			A Bio-inspired Redundant Sensing Architecture	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ACUITY	Sensing is the process of deriving signals from the environment that allows artificial systems to interact with the physical world. The Shannon theorem specifies the maximum rate at which information can be acquired [1]. However, this upper bound is hard to achieve in many man-made systems. The biological visual systems, on the other hand, have highly efficient signal representation and processing mechanisms that allow precise sensing. In this work, we argue that redundancy is one of the critical characteristics for such superior performance. We show architectural advantages by utilizing redundant sensing, including correction of mismatch error and significant precision enhancement. For a proof-of-concept demonstration, we have designed a heuristic-based analog-to-digital converter - a zero-dimensional quantizer. Through Monte Carlo simulation with the error probabilistic distribution as a priori, the performance approaching the Shannon limit is feasible. In actual measurements without knowing the error distribution, we observe at least 2-bit extra precision. The results may also help explain biological processes including the dominance of binocular vision, the functional roles of the fixational eye movements, and the structural mechanisms allowing hyperacuity.	[Anh Tuan Nguyen; Xu, Jian; Yang, Zhi] Univ Minnesota, Dept Biomed Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Yang, Z (corresponding author), Univ Minnesota, Dept Biomed Engn, Minneapolis, MN 55455 USA.	yang5029@umn.edu	Nguyen, Anh Tuan/AAO-4860-2020	Nguyen, Anh Tuan/0000-0001-6201-4529				Analog Devices, 2016, 24 BIT DELT SIGM ADC; BECK J, 1979, VISION RES, V19, P313, DOI 10.1016/0042-6989(79)90176-7; Biveroni Jonas, 2008, 2008 Information Theory and Applications Workshop Conference, P185, DOI 10.1109/ITA.2008.4601045; CAGENELLO R, 1993, J OPT SOC AM A, V10, P1841, DOI 10.1364/JOSAA.10.001841; CRICK FHC, 1980, ORG CEREBRAL CORTEX, P505; CURCIO CA, 1990, J COMP NEUROL, V292, P497, DOI 10.1002/cne.902920402; CURCIO CA, 1990, J COMP NEUROL, V300, P5, DOI 10.1002/cne.903000103; Frey M, 2007, IEEE T CIRCUITS-I, V54, P229, DOI 10.1109/TCSI.2006.887453; Hennig M. H, 2004, ADV NEURAL INFORM PR; Hicheur H, 2013, J VISION, V13, DOI 10.1167/13.13.18; Land M.F., 1985, P53; Martinez-Conde S, 2013, NAT REV NEUROSCI, V14, P83, DOI 10.1038/nrn3405; Murmann B, 2008, IEEE CUST INTEGR CIR, P105, DOI 10.1109/CICC.2008.4672032; Nguyen A T, 2015, P IEEE CUST INT CIRC, P1; Read JCA, 2015, PROC SPIE, V9391, DOI 10.1117/12.2184988; Reece JB., 2010, CAMPBELL BIOL; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Um JY, 2013, IEEE T CIRCUITS-I, V60, P2845, DOI 10.1109/TCSI.2013.2252475; WESTHEIMER G, 1978, J OPT SOC AM, V68, P450, DOI 10.1364/JOSA.68.000450; WESTHEIMER G, 1977, J OPT SOC AM, V67, P207, DOI 10.1364/JOSA.67.000207; Xu RY, 2012, IEEE J SOLID-ST CIRC, V47, P2129, DOI 10.1109/JSSC.2012.2198350	21	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704088
C	Arjevani, Y; Shamir, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Arjevani, Yossi; Shamir, Ohad			Dimension-Free Iteration Complexity of Finite Sum Optimization Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many canonical machine learning problems boil down to a convex optimization problem with a finite sum structure. However, whereas much progress has been made in developing faster algorithms for this setting, the inherent limitations of these problems are not satisfactorily addressed by existing lower bounds. Indeed, current bounds focus on first-order optimization algorithms, and only apply in the often unrealistic regime where the number of iterations is less than O (d=n) (where d is the dimension and n is the number of samples). In this work, we extend the framework of Arjevani et al. [3, 5] to provide new lower bounds, which are dimension-free, and go beyond the assumptions of current bounds, thereby covering standard finite sum optimization methods, e.g., SAG, SAGA, SVRG, SDCA without duality, as well as stochastic coordinate-descent methods, such as SDCA and accelerated proximal SDCA.	[Arjevani, Yossi; Shamir, Ohad] Weizmann Inst Sci, IL-7610001 Rehovot, Israel	Weizmann Institute of Science	Arjevani, Y (corresponding author), Weizmann Inst Sci, IL-7610001 Rehovot, Israel.	yossi.arjevani@weizmann.ac.il; ohad.shamir@weizmann.ac.il							0	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700051
C	Ashtiani, H; Kushagra, S; Ben-David, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ashtiani, Hassan; Kushagra, Shrinu; Ben-David, Shai			Clustering with Same-Cluster Queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a framework for Semi-Supervised Active Clustering framework (SSAC), where the learner is allowed to interact with a domain expert, asking whether two given instances belong to the same cluster or not. We study the query and computational complexity of clustering in this framework. We consider a setting where the expert conforms to a center-based clustering with a notion of margin. We show that there is a trade off between computational complexity and query complexity; We prove that for the case of k-means clustering (i.e., when the expert conforms to a solution of k-means), having access to relatively few such queries allows efficient solutions to otherwise NP hard problems. In particular, we provide a probabilistic polynomial-time (BPP) algorithm for clustering in this setting that asks O k(2) log k + k log n) same-cluster queries and runs with time complexity O kn log n) (where k is the number of clusters and n is the number of instances). The algorithm succeeds with high probability for data satisfying margin conditions under which, without queries, we show that the problem is NP hard. We also prove a lower bound on the number of queries needed to have a computationally efficient clustering algorithm in this setting.	[Ashtiani, Hassan; Kushagra, Shrinu; Ben-David, Shai] Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON, Canada	University of Waterloo	Ashtiani, H (corresponding author), Univ Waterloo, David R Cheriton Sch Comp Sci, Waterloo, ON, Canada.	mhzokaei@uwaterloo.ca; skushagr@uwaterloo.ca; shai@uwaterloo.ca						[Anonymous], 2002, P 19 INT C MACH LEAR; Ashtiani Hassan, 2015, UNCERTAINTY AI UAI; Awasthi P, 2012, INFORM PROCESS LETT, V112, P49, DOI 10.1016/j.ipl.2011.10.006; Balcan MF, 2012, LECT NOTES COMPUT SC, V7391, P63, DOI 10.1007/978-3-642-31594-7_6; Balcan MF, 2008, LECT NOTES ARTIF INT, V5254, P316, DOI 10.1007/978-3-540-87987-9_27; Basu Sugato, 2004, KDD, P59, DOI DOI 10.1145/1014052.1014062; Ben-David S, 2014, THEOR COMPUT SCI, V558, P51, DOI 10.1016/j.tcs.2014.09.025; Dasgupta S, 2008, HARDNESS K MEANS CLU; Garey M.R., 2002, COMPUTERS INTRACTABI, V29; Kulis B, 2009, MACH LEARN, V74, P1, DOI 10.1007/s10994-008-5084-4; Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274, DOI 10.1007/978-3-642-00202-1_24; Shai Ben-David, 2015, CORR; Vattani A., 2009, HARDNESS K MEA UNPUB	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703085
C	Aybat, NS; Hamedani, EY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Aybat, Necdet Serhat; Hamedani, Erfan Yazdandoost			A primal-dual method for conic constrained distributed optimization problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONSENSUS	We consider cooperative multi-agent consensus optimization problems over an undirected network of agents, where only those agents connected by an edge can directly communicate. The objective is to minimize the sum of agent specific composite convex functions over agent-specific private conic constraint sets; hence, the optimal consensus decision should lie in the intersection of these private sets. We provide convergence rates in sub-optimality; infeasibility and consensus violation; examine the effect of underlying network topology on the convergence rates of the proposed decentralized algorithms; and show how to extend these methods to handle time-varying communication networks.	[Aybat, Necdet Serhat; Hamedani, Erfan Yazdandoost] Penn State Univ, Dept Ind Engn, University Pk, PA 16802 USA	Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Aybat, NS (corresponding author), Penn State Univ, Dept Ind Engn, University Pk, PA 16802 USA.	nsa10@psu.edu; evy5047@psu.edu	Hamedani, Erfan Yazdandoost/ABG-7110-2020					Bach F.R., 2004, P 21 INT C MACH LEAR, P6, DOI DOI 10.1145/1015330.1015424; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chang TH, 2014, IEEE T AUTOMAT CONTR, V59, P1524, DOI 10.1109/TAC.2014.2308612; Chen AI, 2012, ANN ALLERTON CONF, P601, DOI 10.1109/Allerton.2012.6483273; Chen YM, 2014, SIAM J OPTIMIZ, V24, P1779, DOI 10.1137/130919362; Forero PA, 2010, J MACH LEARN RES, V11, P1663; He BS, 2012, SIAM J IMAGING SCI, V5, P119, DOI 10.1137/100814494; Ling Q, 2010, IEEE T SIGNAL PROCES, V58, P3816, DOI 10.1109/TSP.2010.2047721; Mateos G, 2010, IEEE T SIGNAL PROCES, V58, P5262, DOI 10.1109/TSP.2010.2055862; Mateos-Nunez D, 2015, IEEE DECIS CONTR P, P5462, DOI 10.1109/CDC.2015.7403075; MCDONALD R, 2010, HUMAN LANGUAGE TECHN, V2010, P456; Nedic A, 2009, J OPTIMIZ THEORY APP, V142, P205, DOI 10.1007/s10957-009-9522-7; Nedic A., 2010, CONVEX OPTIMIZATION, P340; Nedic A., 2014, ENCY SYSTEMS CONTROL, P1; Nedic A, 2010, IEEE T AUTOMAT CONTR, V55, P922, DOI 10.1109/TAC.2010.2041686; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Robbins H., 1971, P S OH STAT U JUN 14, P233; Rockafellar R.T., 2015, CONVEX ANAL; Schizas ID, 2008, IEEE T SIGNAL PROCES, V56, P350, DOI 10.1109/TSP.2007.906734; Srivastava K, 2010, IEEE DECIS CONTR P, P1945, DOI 10.1109/CDC.2010.5717947; Yan F, 2013, IEEE T KNOWL DATA EN, V25, P2483, DOI 10.1109/TKDE.2012.191; Yuan DM, 2011, IEEE T SYST MAN CY B, V41, P1715, DOI 10.1109/TSMCB.2011.2160394	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701099
C	Bak, JH; Choi, JY; Akrami, A; Witten, I; Pillow, JW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bak, Ji Hyun; Choi, Jung Yoon; Akrami, Athena; Witten, Ilana; Pillow, Jonathan W.			Adaptive optimal training of animal behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Neuroscience experiments often require training animals to perform tasks designed to elicit various sensory, cognitive, and motor behaviors. Training typically involves a series of gradual adjustments of stimulus conditions and rewards in order to bring about learning. However, training protocols are usually hand-designed, relying on a combination of intuition, guesswork, and trial-and-error, and often require weeks or months to achieve a desired level of task performance. Here we combine ideas from reinforcement learning and adaptive optimal experimental design to formulate methods for adaptive optimal training of animal behavior. Our work addresses two intriguing problems at once: first, it seeks to infer the learning rules underlying an animal's behavioral changes during training; second, it seeks to exploit these rules to select stimuli that will maximize the rate of learning toward a desired objective. We develop and test these methods using data collected from rats during training on a two-interval sensory discrimination task. We show that we can accurately infer the parameters of a policy-gradient-based learning algorithm that describes how the animal's internal model of the task evolves over the course of training. We then formulate a theory for optimal training, which involves selecting sequences of stimuli that will drive the animal's internal policy toward a desired location in the parameter space. Simulations show that our method can in theory provide a substantial speedup over standard training methods. We feel these results will hold considerable theoretical and practical implications both for researchers in reinforcement learning and for experimentalists seeking to train animals.	[Bak, Ji Hyun] Princeton Univ, Dept Phys, Princeton, NJ 08544 USA; [Choi, Jung Yoon; Witten, Ilana; Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA; [Choi, Jung Yoon; Akrami, Athena; Witten, Ilana; Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Bak, Ji Hyun] Korea Inst Adv Study, Sch Computat Sci, Seoul, South Korea; [Akrami, Athena] Howard Hughes Med Inst, Chevy Chase, MA USA	Princeton University; Princeton University; Princeton University; Korea Institute for Advanced Study (KIAS); Howard Hughes Medical Institute	Bak, JH (corresponding author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.	jhbak@kias.re.kr; jungchoi@princeton.edu; aakrami@princeton.edu; iwitten@princeton.edu; pillow@princeton.edu			Samsung Scholarship; NSF PoLS program; McKnight Foundation; Simons Collaboration on the Global Brain [SCGB AWD1004351]; NSF CAREER Award [IIS-1150186]	Samsung Scholarship(Samsung); NSF PoLS program; McKnight Foundation; Simons Collaboration on the Global Brain; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	JHB was supported by the Samsung Scholarship and the NSF PoLS program. JWP was supported by grants from the McKnight Foundation, Simons Collaboration on the Global Brain (SCGB AWD1004351) and the NSF CAREER Award (IIS-1150186). We thank Nicholas Roy for the careful reading of the manuscript.	Abrahamyan A, 2016, P NATL ACAD SCI USA, V113, pE3548, DOI 10.1073/pnas.1518786113; Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059; Akrami A., 2016, SOC NEUR ABSTR; Bishop C.M, 2006, PATTERN RECOGN; Busse L, 2011, J NEUROSCI, V31, P11351, DOI 10.1523/JNEUROSCI.6689-10.2011; Fassihi A, 2014, P NATL ACAD SCI USA, V111, P2331, DOI 10.1073/pnas.1315171111; Frund I, 2014, J VISION, V14, DOI 10.1167/14.7.9; Green DM, 1966, SIGNAL DETECTION THE; Hernandez A, 1997, J NEUROSCI, V17, P6391; Li JA, 2011, J NEUROSCI, V31, P5504, DOI 10.1523/JNEUROSCI.6316-10.2011; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Pillow JW, 2011, NEURAL COMPUT, V23, P1, DOI 10.1162/NECO_a_00058; Sahani M, 2002, ADV NEURAL INFORM PR, V15, P317; Smith AC, 2004, J NEUROSCI, V24, P447, DOI 10.1523/JNEUROSCI.2908-03.2004; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tyler CW, 2000, VISION RES, V40, P3121, DOI 10.1016/S0042-6989(00)00157-7	16	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702091
C	Balandat, M; Krichene, W; Tomlin, C; Bayen, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Balandat, Maximilian; Krichene, Walid; Tomlin, Claire; Bayen, Alexandre			Minimizing Regret on Reflexive Banach Spaces and Nash Equilibria in Continuous Zero-Sum Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study a general adversarial online learning problem, in which we are given a decision set X in a reflexive Banach space X and a sequence of reward vectors in the dual space of X. At each iteration, we choose an action from X, based on the observed sequence of previous rewards. Our goal is to minimize regret. Using results from infinite dimensional convex analysis, we generalize the method of Dual Averaging to our setting and obtain upper bounds on the worst-case regret that generalize many previous results. Under the assumption of uniformly continuous rewards, we obtain explicit regret bounds in a setting where the decision set is the set of probability distributions on a compact metric space S. Importantly, we make no convexity assumptions on either S or the reward functions. We also prove a general lower bound on the worst-case regret for any online algorithm. We then apply these results to the problem of learning in repeated two-player zero-sum games on compact metric spaces. In doing so, we first prove that if both players play a Hannan-consistent strategy, then with probability 1 the empirical distributions of play weakly converge to the set of Nash equilibria of the game. We then show that, under mild assumptions, Dual Averaging on the (infinite-dimensional) space of probability distributions indeed achieves Hannan-consistency.	[Balandat, Maximilian; Krichene, Walid; Tomlin, Claire; Bayen, Alexandre] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Balandat, M (corresponding author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	balandat@eecs.berkeley.edu; walid@eecs.berkeley.edu; tomlin@eecs.berkeley.edu; bayen@berkeley.edu						Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Bubeck S, 2014, ARXIV E PRINTS; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cover T.M., 1991, MATH FINANC, V1, P1, DOI DOI 10.1111/J.1467-9965.1991.TB00002.X; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; GLICKSBERG IL, 1950, RM478 RAND CORP; Hannan James, 1957, ANN MATH STUDIES 39, VIII; Hart S, 2001, J ECON THEORY, V98, P26, DOI 10.1006/jeth.2000.2746; Heinonen J., 2015, NEW MATH MONOGRAPHS; Krichene W, 2015, PR MACH LEARN RES, V37, P824; Krichene Walid, 2015, ABS150407720 CORR; Kwon Joon, 2014, ARXIV E PRINTS; Lehrer E, 2003, INT J GAME THEORY, V31, P253, DOI 10.1007/s001820200115; Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Srebro N., 2011, ADV NEURAL INFORM PR, V24, P2645; Sridharan K., 2010, P 23 C LEARN THEOR, P1; Stromberg T, 2011, POSITIVITY, V15, P527, DOI 10.1007/s11117-010-0105-5; Xiao L, 2010, J MACH LEARN RES, V11, P2543	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701073
C	Balcan, MF; Sandholm, T; Vitercik, E		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Balcan, Maria-Florina; Sandholm, Tuomas; Vitercik, Ellen			Sample Complexity of Automated Mechanism Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The design of revenue-maximizing combinatorial auctions, i.e. multi-item auctions over bundles of goods, is one of the most fundamental problems in computational economics, unsolved even for two bidders and two items for sale. In the traditional economic models, it is assumed that the bidders' valuations are drawn from an underlying distribution and that the auction designer has perfect knowledge of this distribution. Despite this strong and oftentimes unrealistic assumption, it is remarkable that the revenue-maximizing combinatorial auction remains unknown. In recent years, automated mechanism design has emerged as one of the most practical and promising approaches to designing high-revenue combinatorial auctions. The most scalable automated mechanism design algorithms take as input samples from the bidders' valuation distribution and then search for a high-revenue auction in a rich auction class. In this work, we provide the first sample complexity analysis for the standard hierarchy of deterministic combinatorial auction classes used in automated mechanism design. In particular, we provide tight sample complexity bounds on the number of samples needed to guarantee that the empirical revenue of the designed mechanism on the samples is close to its expected revenue on the underlying, unknown distribution over bidder valuations, for each of the auction classes in the hierarchy. In addition to helping set automated mechanism design on firm foundations, our results also push the boundaries of learning theory. In particular, the hypothesis functions used in our contexts are defined through multi-stage combinatorial optimization procedures, rather than simple decision boundaries, as are common in machine learning.	[Balcan, Maria-Florina; Sandholm, Tuomas; Vitercik, Ellen] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Balcan, MF (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; sandholm@cs.cmu.edu; vitercik@cs.cmu.edu			NSF [CCF-1535967, CCF-1451177, CCF-1422910, IIS-1618714, IIS-1617590, IIS-1320620, IIS-1546752]; ARO award [W911NF-16-1-0061]; Sloan Research Fellowship; Microsoft Research Faculty Fellowship; NSF Graduate Research Fellowship; Microsoft Research Women's Fellowship	NSF(National Science Foundation (NSF)); ARO award; Sloan Research Fellowship(Alfred P. Sloan Foundation); Microsoft Research Faculty Fellowship(Microsoft); NSF Graduate Research Fellowship(National Science Foundation (NSF)); Microsoft Research Women's Fellowship(Microsoft)	This work was supported in part by NSF grants CCF-1535967, CCF-1451177, CCF-1422910, IIS-1618714, IIS-1617590, IIS-1320620, IIS-1546752, ARO award W911NF-16-1-0061, a Sloan Research Fellowship, a Microsoft Research Faculty Fellowship, an NSF Graduate Research Fellowship, and a Microsoft Research Women's Fellowship.	Balcan Maria- Florina, 2008, J COMPUT SYST SCI, V74, P78; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Conitzer V., 2004, P 5 ACM C ELECT COMM, P132; Conitzer Vincent, 2002, P 18 ANN C UNC ART I, P103; Devanur Nikhil R, 2016, P ANN S THEOR COMP S; Dughmi S, 2014, LECT NOTES COMPUT SC, V8877, P277, DOI 10.1007/978-3-319-13129-0_22; Elkind E, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P736; Feldman Michal, 2015, ANN ACM SIAM S DISCR; Hsu Justin, 2016, P ANN S THEOR COMP S; Huang Zhiyi, 2015, 16 ACM C EC COMP EC, P45; Jehiel P, 2007, J ECON THEORY, V134, P494, DOI 10.1016/j.jet.2006.02.001; Likhodedov A, 2004, PROCEEDING OF THE NINETEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE SIXTEENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE, P232; Likhodedov A., 2005, P NAT C ART INT AAAI; Mohri M., 2018, FDN MACHINE LEARNING; Mohri M, 2014, PR MACH LEARN RES, V32; Morgenstern Jamie, 2016, C LEARN THEORY COLT; Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Roberts Kevin, 1979, AGGREGATION REVELATI; Roughgarden Tim, 2016, P ACM C EC COMP EC; Sandholm T, 2003, LECT NOTES COMPUT SC, V2833, P19; Sandholm T, 2015, OPER RES, V63, P1000, DOI 10.1287/opre.2015.1398; Tang Pingzhong, 2012, INT C AUT AG MULT SY	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702098
C	Balcan, MF; Zhang, HY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Balcan, Maria-Florina; Zhang, Hongyang			Noise-Tolerant Life-Long Matrix Completion via Adaptive Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SUBSPACE; PCA	We study the problem of recovering an incomplete m x n matrix of rank r with columns arriving online over time. This is known as the problem of life-long matrix completion, and is widely applied to recommendation system, computer vision, system identification, etc. The challenge is to design provable algorithms tolerant to a large amount of noises, with small sample complexity. In this work, we give algorithms achieving strong guarantee under two realistic noise models. In bounded deterministic noise, an adversary can add any bounded yet unstructured noise to each column. For this problem, we present an algorithm that returns a matrix of a small error, with sample complexity almost as small as the best prior results in the noiseless case. For sparse random noise, where the corrupted columns are sparse and drawn randomly, we give an algorithm that exactly recovers an mu(0)-incoherent matrix by probability at least 1 - delta with sample complexity as small as O (mu(0)rn log(r/delta)). This result advances the state-of-the-art work and matches the lower bound in a worst case. We also study the scenario where the hidden matrix lies on a mixture of subspaces and show that the sample complexity can be even smaller. Our proposed algorithms perform well experimentally in both synthetic and real-world datasets.	[Balcan, Maria-Florina; Zhang, Hongyang] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Balcan, MF (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; hongyanz@cs.cmu.edu			Sloan Fellowship; Microsoft Research Fellowship;  [NSF-CCF 1535967];  [NSF CCF-1422910];  [NSF CCF-1451177]	Sloan Fellowship(Alfred P. Sloan Foundation); Microsoft Research Fellowship(Microsoft); ; ; 	This work was supported in part by grants NSF-CCF 1535967, NSF CCF-1422910, NSF CCF-1451177, a Sloan Fellowship, and a Microsoft Research Fellowship.	Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Balcan M.-F., 2015, ANN C LEARN THEOR; Balcan M.-F.F., 2013, ADV NEURAL INFORM PR, V26, P1295; Balzano L., 2010, 2010 48th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P704, DOI 10.1109/ALLERTON.2010.5706976; Balzano L, 2010, IEEE INT SYMP INFO, P1638, DOI 10.1109/ISIT.2010.5513344; Basri R, 2003, IEEE T PATTERN ANAL, V25, P218, DOI 10.1109/TPAMI.2003.1177153; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Carlson Andrew, 2010, AAAI C ART INT; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; Dhanjal C., 2014, P 2014 SIAM INT C DA, P623; Gittens A., 2011, ARXIV11105305; Gopnik A., 2001, BABIES THINK SCI CHI; Hastie T, 1998, STAT SCI, V13, P54; Kennedy R, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P507, DOI 10.1109/GlobalSIP.2014.7032169; Krishnamurthy Akshay, 2014, ARXIV14073619; Lerman G, 2014, CONSTR APPROX, V40, P329, DOI 10.1007/s00365-014-9242-6; Lois B, 2015, IEEE INT SYMP INFO, P1826, DOI 10.1109/ISIT.2015.7282771; SCHARF LL, 1994, IEEE T SIGNAL PROCES, V42, P2146, DOI 10.1109/78.301849; Warmuth MK, 2008, J MACH LEARN RES, V9, P2287; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Zhang HY, 2016, IEEE T INFORM THEORY, V62, P4748, DOI 10.1109/TIT.2016.2573311; Zhang HY, 2015, AAAI CONF ARTIF INTE, P3143; Zhang HY, 2015, NEURAL COMPUT, V27, P1915, DOI 10.1162/NECO_a_00762	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704104
C	Balsubramani, A; Freund, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Balsubramani, Akshay; Freund, Yoav			Optimal Binary Classifier Aggregation for General Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We address the problem of aggregating an ensemble of predictors with known loss bounds in a semi-supervised binary classification setting, to minimize prediction loss incurred on the unlabeled data. We find the minimax optimal predictions for a very general class of loss functions including all convex and many non-convex losses, extending a recent analysis of the problem for misclassification error. The result is a family of semi-supervised ensemble aggregation algorithms which are as efficient as linear learning by convex optimization, but are minimax optimal without any relaxations. Their decision rules take a form familiar in decision theory - applying sigmoid functions to a notion of ensemble margin - without the assumptions typically made in margin-based learning.	[Balsubramani, Akshay; Freund, Yoav] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Balsubramani, A (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	abalsubr@ucsd.edu; yfreund@ucsd.edu			NSF [IIS-1162581]	NSF(National Science Foundation (NSF))	AB is grateful to Chris "Ceej" Tosh for feedback that made the manuscript clearer. This work was supported by the NSF (grant IIS-1162581).		0	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705011
C	Beatson, A; Wang, ZR; Liu, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Beatson, Alex; Wang, Zhaoran; Liu, Han			Blind Attacks on Machine Learners	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The importance of studying the robustness of learners to malicious data is well established. While much work has been done establishing both robust estimators and effective data injection attacks when the attacker is omniscient, the ability of an attacker to provably harm learning while having access to little information is largely unstudied. We study the potential of a "blind attacker" to provably limit a learner's performance by data injection attack without observing the learner's training set or any parameter of the distribution from which it is drawn. We provide examples of simple yet effective attacks in two settings: firstly, where an "informed learner" knows the strategy chosen by the attacker, and secondly, where a "blind learner" knows only the proportion of malicious data and some family to which the malicious distribution chosen by the attacker belongs. For each attack, we analyze minimax rates of convergence and establish lower bounds on the learner's minimax risk, exhibiting limits on a learner's ability to learn under data injection attack even when the attacker is "blind".	[Beatson, Alex] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA; [Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Princeton University; Princeton University	Beatson, A (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.	abeatson@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018					Al Hasan M, 2006, SDM06 WORKSH LINK AN; Azizyan M., 2013, NEURAL INFORM PROCES; Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5; Biggio B., 2012, 29 INT C MACH LEARN, P1807; Bolton RJ, 2002, STAT SCI, V17, P235; Bruckner M., 2011, ACM SIGKDD; Chen M., 2015, ARXIV151104144; Chen Y., 2013, ARXIV13127006; Duchi J., 2014, ARXIV13023203V4; Duchi J., 2013, NEURAL INFORM PROCES; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; KEARNS M, 1993, SIAM J COMPUT, V22, P807, DOI 10.1137/0222052; Laskov P., 2009, ACM WORKSH SEC ART I; Laskov P, 2010, MACH LEARN, V81, P115, DOI 10.1007/s10994-010-5207-6; Rubinstein B. I., 2008, UCBEECS200873; Shi YA, 2009, IEEE DATA MINING, P483, DOI 10.1109/ICDM.2009.75; Sommer R, 2010, P IEEE S SECUR PRIV, P305, DOI 10.1109/SP.2010.25; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Xiao H, 2012, FRONT ARTIF INTEL AP, V242, P870, DOI 10.3233/978-1-61499-098-7-870	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704007
C	Berahas, AS; Nocedal, J; Takac, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Berahas, Albert S.; Nocedal, Jorge; Takac, Martin			A Multi-Batch L-BFGS Method for Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The question of how to parallelize the stochastic gradient descent (SGD) method has received much attention in the literature. In this paper, we focus instead on batch methods that use a sizeable fraction of the training set at each iteration to facilitate parallelism, and that employ second-order information. In order to improve the learning process, we follow a multi-batch approach in which the batch changes at each iteration. This can cause difficulties because L-BFGS employs gradient differences to update the Hessian approximations, and when these gradients are computed using different data points the process can be unstable. This paper shows how to perform stable quasi-Newton updating in the multi-batch setting, illustrates the behavior of the algorithm in a distributed computing platform, and studies its convergence properties for both the convex and nonconvex cases.	[Berahas, Albert S.; Nocedal, Jorge] Northwestern Univ, Evanston, IL 60208 USA; [Takac, Martin] Lehigh Univ, Bethlehem, PA 18015 USA	Northwestern University; Lehigh University	Berahas, AS (corresponding author), Northwestern Univ, Evanston, IL 60208 USA.	albertberahas@u.northwestern.edu; j-nocedal@northwestern.edu; takac.mt@gmail.com	Takac, Martin/AAA-8564-2022; Nocedal, Jorge/B-7255-2009	Takac, Martin/0000-0001-7455-2025; 	Office of Naval Research [N000141410313]; Department of Energy [DE-FG02-87ER25047]; National Science Foundation [CCF-1618717, DMS-1620022]	Office of Naval Research(Office of Naval Research); Department of Energy(United States Department of Energy (DOE)); National Science Foundation(National Science Foundation (NSF))	The first two authors were supported by the Office of Naval Research award N000141410313, the Department of Energy grant DE-FG02-87ER25047 and the National Science Foundation grant DMS-1620022. Martin Takac was supported by National Science Foundation grant CCF-1618717.	Agarwal A, 2014, J MACH LEARN RES, V15, P1111; [Anonymous], 1999, NUMERICAL OPTIMIZATI; [Anonymous], 2014, PROC NEURIPS; Bertsekas D.P., 1989, PARALLEL DISTRIBUTED, V23; Bollapragada R., 2016, ARXIV160908502; Bottou L., 2016, ARXIV160604838; Bottou U, 2004, ADV NEUR IN, V16, P217; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362; Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X; Dai YH, 2003, SIAM J OPTIMIZ, V13, P693; Dean J., 2012, NIPS 12, V1, P1223; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Le Q.V., 2011, P ICML JAN, P265; Li DH, 2001, SIAM J OPTIMIZ, V11, P1054, DOI 10.1137/S1052623499354242; Mania H., 2015, ARXIV150706970; Mascarenhas WF, 2004, MATH PROGRAM, V99, P49, DOI 10.1007/s10107-003-0421-7; Nedic A, 2001, APPL OPTIMIZAT, V54, P223; Powell M.J.D., 1976, NONLINEAR PROGRAMMIN, V9, P53; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schmidt M., 2016, MATH PROGRAM, P1; Schraudolph N. N., 2007, PROC 11 INT C ARTIF, P436; Taka c. M, 2013, P 30 INT C MACH LEAR, V28, P1022	27	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701002
C	Bhaskara, A; Ghadiri, M; Mirrokni, V; Svensson, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bhaskara, Aditya; Ghadiri, Mehrdad; Mirrokni, Vahab; Svensson, Ola			Linear Relaxations for Finding Diverse Elements in Metric Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION ALGORITHMS; FEATURE-SELECTION; RECOGNITION; DATABASE	Choosing a diverse subset of a large collection of points in a metric space is a fundamental problem, with applications in feature selection, recommender systems, web search, data summarization, etc. Various notions of diversity have been proposed, tailored to different applications. The general algorithmic goal is to find a subset of points that maximize diversity, while obeying a cardinality (or more generally, matroid) constraint. The goal of this paper is to develop a novel linear programming (LP) framework that allows us to design approximation algorithms for such problems. We study an objective known as sum-min diversity, which is known to be effective in many applications, and give the first constant factor approximation algorithm. Our LP framework allows us to easily incorporate additional constraints, as well as secondary objectives. We also prove a hardness result for two natural diversity objectives, under the so-called planted clique assumption. Finally, we study the empirical performance of our algorithm on several standard datasets. We first study the approximation quality of the algorithm by comparing with the LP objective. Then, we compare the quality of the solutions produced by our method with other popular diversity maximization algorithms.	[Bhaskara, Aditya] Univ Utah, Salt Lake City, UT 84112 USA; [Ghadiri, Mehrdad] Sharif Univ Technol, Tehran, Iran; [Mirrokni, Vahab] Google Res, New York, NY USA; [Svensson, Ola] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Utah System of Higher Education; University of Utah; Sharif University of Technology; Google Incorporated; Ecole Polytechnique Federale de Lausanne	Bhaskara, A (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.	bhaskara@cs.utah.edu; ghadiri@ce.sharif.edu; mirrokni@google.com; ola.svensson@epfl.ch	Ghadiri, Mehrdad/Y-4598-2019					Abbassi Z, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P32; Bhattacharya S., 2011, P 20 INT C WORLD WID, P317; Borodin A., 2012, P 31 ACM SIGMOD SIGA, P155; Cai D, 2011, IEEE T PATTERN ANAL, V33, P1548, DOI 10.1109/TPAMI.2010.231; Chandra B, 2001, J ALGORITHM, V38, P438, DOI 10.1006/jagm.2000.1145; Chekuri C, 2010, ANN IEEE SYMP FOUND, P575, DOI 10.1109/FOCS.2010.60; Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97; Feldman V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P655; Gollapudi S, 2009, P 18 INT C WORLD WID, P381; Hassin R, 1997, OPER RES LETT, V21, P133, DOI 10.1016/S0167-6377(97)00034-5; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Karp R.M., 1976, ALGORITHMS COMPLEXIT, P1; Lichman M., 2013, UCI MACHINE LEARNING; Liu TQ, 2007, NUCLEIC ACIDS RES, V35, pD198, DOI 10.1093/nar/gkl999; Meinl T, 2011, J CHEM INF MODEL, V51, P237, DOI 10.1021/ci100426r; Nayar S, 1996, CUCS00696; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; PLOTKIN SA, 1991, PROCEEDINGS - 32ND ANNUAL SYMPOSIUM ON FOUNDATIONS OF COMPUTER SCIENCE, P495, DOI 10.1109/SFCS.1991.185411; Qin L, 2012, PROC VLDB ENDOW, V5, P1124, DOI 10.14778/2350229.2350233; Radlinski F., 2006, SIGIR; SCHRIJVER A., 2003, COMBINATORIAL OPTIMI, V24; Vasconcelos N, 2003, PROC CVPR IEEE, P762; Vieira MR, 2011, PROC INT CONF DATA, P1163, DOI 10.1109/ICDE.2011.5767846	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704025
C	Bhojanapalli, S; Neyshabur, B; Srebro, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bhojanapalli, Srinadh; Neyshabur, Behnam; Srebro, Nathan			Global Optimality of Local Search for Low Rank Matrix Recovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We show that there are no spurious local minima in the non-convex factorized parametrization of low-rank matrix recovery from incoherent linear measurements. With noisy measurements we show all local minima are very close to a global optimum. Together with a curvature bound at saddle points, this yields a polynomial time global convergence guarantee for stochastic gradient descent from random initialization.	[Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Srebro, N (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	srinadh@ttic.edu; bneyshabur@ttic.edu; nati@ttic.edu			NSF RI/AF grant [1302662]	NSF RI/AF grant	Authors would like to thank Afonso Bandeira for discussions, Jason Lee and Tengyu Ma for sharing and discussing their work. This research was supported in part by an NSF RI/AF grant 1302662.	[Anonymous], 2015, ARXIV150903025; Bandeira A. S., 2016, JMLR WORKSHOP C P; Bhojanapalli S., 2015, ARXIV150903917; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; De Sa C, 2015, PR MACH LEARN RES, V37, P2332; Flammia ST, 2012, NEW J PHYS, V14, DOI 10.1088/1367-2630/14/9/095022; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ge Rong, 2016, ARXIV160507272; Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401; Hardt Moritz, 2014, JMLR WORKSHOP C P, P703; Jain P, 2010, P ADV NEUR INF PROC, P937; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Keshavan R. H., 2012, THESIS; Montanari A, 2016, ARXIV160304064; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rennie J. D., 2005, P 22 INT C MACHINE L, P713; Srebro N., 2003, P 20 INT C MACHINE L, P720; Sun J, 2015, PR MACH LEARN RES, V37, P2351; Tu S., 2015, ARXIV150703566; WRIGHT J., 2016, ARXIV160206664; Yu H.-F., 2014, INT C MACH LEARN, P593; Zhang D., 2015, ARXIV150607405; Zheng Q., 2015, ADV NEURAL INFORM PR, P109	30	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702018
C	Bogolubsky, L; Gusev, G; Raigorodskii, A; Tikhonov, A; Zhukovskii, M; Dvurechensky, P; Gasnikov, A; Nesterov, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bogolubsky, Lev; Gusev, Gleb; Raigorodskii, Andrei; Tikhonov, Aleksey; Zhukovskii, Maksim; Dvurechensky, Pavel; Gasnikov, Alexander; Nesterov, Yurii			Learning Supervised PageRank with Gradient-Based and Gradient-Free Optimization Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DERIVATIVES	In this paper, we consider a non-convex loss-minimization problem of learning Supervised PageRank models, which can account for features of nodes and edges. We propose gradient-based and random gradient-free methods to solve this problem. Our algorithms are based on the concept of an inexact oracle and unlike the state-of-the-art gradient-based method we manage to provide theoretically the convergence rate guarantees for both of them. Finally, we compare the performance of the proposed optimization methods with the state of the art applied to a ranking task.	[Bogolubsky, Lev; Gusev, Gleb; Raigorodskii, Andrei; Tikhonov, Aleksey; Zhukovskii, Maksim] Yandex, Moscow, Russia; [Bogolubsky, Lev; Raigorodskii, Andrei] Moscow MV Lomonosov State Univ, Moscow, Russia; [Raigorodskii, Andrei] Buryat State Univ, Ulan Ude, Russia; [Dvurechensky, Pavel] Weierstrass Inst, Berlin, Germany; [Dvurechensky, Pavel; Gasnikov, Alexander] Inst Informat Transmiss Problems RAS, Moscow, Russia; [Gusev, Gleb; Raigorodskii, Andrei; Zhukovskii, Maksim; Gasnikov, Alexander] Moscow Inst Phys & Technol, Moscow, Russia; [Nesterov, Yurii] Ctr Operat Res & Econometr, Louvain La Neuve, Belgium; [Nesterov, Yurii] Higher Sch Econ, Moscow, Russia	Lomonosov Moscow State University; Buryat State University; Weierstrass Institute for Applied Analysis & Stochastics; Russian Academy of Sciences; Moscow Institute of Physics & Technology; HSE University (National Research University Higher School of Economics)	Bogolubsky, L (corresponding author), Yandex, Moscow, Russia.; Bogolubsky, L (corresponding author), Moscow MV Lomonosov State Univ, Moscow, Russia.	bogolubsky@yandex-team.ru; gleb57@yandex-team.ru; raigorodsky@yandex-team.ru; altsoph@yandex-team.ru; zhukmax@yandex-team.ru; pavel.dvurechensky@wias-berlin.de; gasnikov@yandex.ru; yurii.nesterov@uclouvain.be	Dvurechensky, Pavel E./P-7295-2015; Gusev, Gleb G./C-8263-2014; Raigorodskii, Andrei M/L-5095-2017	Dvurechensky, Pavel E./0000-0003-1201-2343; 	Russian Science Foundation [14-50-00150]; RFBR	Russian Science Foundation(Russian Science Foundation (RSF)); RFBR(Russian Foundation for Basic Research (RFBR))	The research by P. Dvurechensky and A. Gasnikov presented in Section 4 of this paper was conducted in IITP RAS and supported by the Russian Science Foundation grant (project 14-50-00150), the research presented in Section 5 was supported by RFBR.	Agarwal A., 2010, 23 ANN C LEARN THEOR; ANDREW AL, 1979, J I MATH APPL, V24, P209; ANDREW AL, 1978, J COMPUT PHYS, V26, P107, DOI 10.1016/0021-9991(78)90102-X; Backstrom L, 2011, SUPERVISED RANDOM WA; Dai Na, 2010, FLOWERS FOOD WEB AUT; Eiron N., 2004, RANKING WEB FRONTIER; Gao B., 2011, SEMISUPERVISED RANKI; Gasnikov A., 2015, COMP MATH MATH PHYS, V55, P1; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Haveliwala T., 1999, EFFICIENT COMPUTATIO; Haveliwala TH, 2002, TOPIC SENSITIVE PAGE; Jeh G., 2003, SCALING PERSONALIZED, DOI 10.1145/775152.775191; KLEINBERG JM, 1998, AUTHORITATIVE SOURCE; Liu Y., 2008, BROWSERANK LETTING W; MATYAS J, 1965, AUTOMAT REM CONTR+, V26, P244; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nesterov Yu., 2015, RANDOM GRADIENT FREE, P1; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nesterov Y, 2015, APPL MATH COMPUT, V255, P58, DOI 10.1016/j.amc.2014.04.053; Page L., 1999, STANFORD INFOLAB; Richardson M., 2002, INTELLIGENT SURFER P; Zhukovskii M., 2014, SUPERVISED NESTED PA; Zhukovskii M., 2013, URL REDIRECTION ACCO; Zhukovskii M., 2013, FRESH BROWSERANK	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704089
C	Bogunovic, I; Scarlett, J; Krause, A; Cevher, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bogunovic, Ilija; Scarlett, Jonathan; Krause, Andreas; Cevher, Volkan			Truncated Variance Reduction: A Unified Approach to Bayesian Optimization and Level-Set Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a new algorithm, truncated variance reduction (TRUVAR), that treats Bayesian optimization (BO) and level-set estimation (LSE) with Gaussian processes in a unified fashion. The algorithm greedily shrinks a sum of truncated variances within a set of potential maximizers (BO) or unclassified points (LSE), which is updated based on confidence bounds. TRUVAR is effective in several important settings that are typically non-trivial to incorporate into myopic algorithms, including pointwise costs and heteroscedastic noise. We provide a general theoretical guarantee for TRUVAR covering these aspects, and use it to recover and strengthen existing results on BO and LSE. Moreover, we provide a new result for a setting where one can select from a number of noise levels having associated costs. We demonstrate the effectiveness of the algorithm on both synthetic and real-world data sets.	[Bogunovic, Ilija; Scarlett, Jonathan; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland; [Krause, Andreas] Swiss Fed Inst Technol, Learning & Adapt Syst Grp, Zurich, Switzerland	Ecole Polytechnique Federale de Lausanne; ETH Zurich	Bogunovic, I (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland.	ilija.bogunovic@epfl.ch; jonathan.scarlett@epfl.ch; krausea@ethz.ch; volkan.cevher@epfl.ch	Scarlett, Jonathan/AGK-0892-2022		European Commission under Grant ERC Future Proof; SNF Sinergia project [CRSII2-147633]; SNF [200021-146750]; EPFL Fellows Horizon2020 grant [665667]	European Commission under Grant ERC Future Proof; SNF Sinergia project; SNF; EPFL Fellows Horizon2020 grant	This work was supported in part by the European Commission under Grant ERC Future Proof, SNF Sinergia project CRSII2-147633, SNF 200021-146750, and EPFL Fellows Horizon2020 grant 665667.	Bryan B., 2008, INT C MACH LEARN ICM; Bubeck S., 2012, FDN TREND MACH LEARN; Contal Emile, 2013, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2013. Proceedings: LNCS 8188, P225, DOI 10.1007/978-3-642-40988-2_15; Das A, 2008, ACM S THEORY COMPUT, P45; Goldberg PW, 1998, ADV NEUR IN, V10, P493; Gotovos A., 2013, THESIS ETH ZURICH SW; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hitz G, 2012, IEEE ROBOT AUTOM MAG, V19, P62, DOI 10.1109/MRA.2011.2181771; JONES DR, 1993, J OPTIMIZ THEORY APP, V79, P157, DOI 10.1007/BF00941892; Kleinberg R., 2008, P ACM S THEOR COMP; Krause A., 2005, TECHNICAL REPORT; Krause A., 2012, TRACTABILITY PRACTIC, V3; Metzen J. H., 2016, INT C MACH LEARN ICM; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Swersky K, 2014, FREEZE THAW BAYESIAN; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Wang Z., BAYESIAN MULTISCALE	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700044
C	Bouchacourt, D; Kumar, MP; Nowozin, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bouchacourt, Diane; Kumar, M. Pawan; Nowozin, Sebastian			DISCO Nets: DISsimilarity COefficient Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a new type of probabilistic model which we call DISsimilarity COefficient Networks (DISCO Nets). DISCO Nets allow us to efficiently sample from a posterior distribution parametrised by a neural network. During training, DISCO Nets are learned by minimising the dissimilarity coefficient between the true distribution and the estimated distribution. This allows us to tailor the training to the loss related to the task at hand. We empirically show that (i) by modeling uncertainty on the output value, DISCO Nets outperform equivalent non-probabilistic predictive networks and (ii) DISCO Nets accurately model the uncertainty of the output, outperforming existing probabilistic models based on deep neural networks.	[Bouchacourt, Diane; Kumar, M. Pawan] Univ Oxford, Oxford, England; [Nowozin, Sebastian] Microsoft Res Cambridge, Cambridge, England	University of Oxford; Microsoft	Bouchacourt, D (corresponding author), Univ Oxford, Oxford, England.	diane@robots.ox.ac.uk; pawan@robots.ox.ac.uk; sebastian.nowozin@microsoft.com			Microsoft Research PhD Scholarship Programme	Microsoft Research PhD Scholarship Programme(Microsoft)	This work is funded by the Microsoft Research PhD Scholarship Programme. We would like to thank Pankaj Pansari, Leonard Berrada and Ondra Miksik for their useful discussions and insights.	[Anonymous], 2016, ICML; [Anonymous], 2014, ICLR; Denton E. L., 2015, NIPS; Dziugaite G. K., 2015, UAI; Fukumizu K., 2013, JMLR; Gauthier J., 2014, CONDITIONAL GENERATI; Gneiting T., 2007, J AM STAT ASS; Gneiting Tilmann, 2008, TEST; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A., 2012, JMLR; Gretton A., 2007, NIPS; Kumar M. P., 2012, ICML; Lacoste-Julien S., 2011, AISTATS; Li Y., 2015, ICML; Makhzani A., 2015, ICLR WORKSH; Mirza M., 2014, NIPS DEEP LEARN WORK; Oberweger M., 2015, COMP VIS WINT WORKSH; Oberweger M., 2015, ICCV; Pinson P., 2013, DISCRIMINATION ABILI; Polyak B. T., 1964, SOME METHODS SPEEDIN; Premachandran V., 2014, CVPR; Radford A., 2015, ICLR; RAO CR, 1982, THEOR POPUL BIOL, V21, P24, DOI 10.1016/0040-5809(82)90004-1; Springenberg J. T., 2016, ICLR; Taylor J., 2012, CVPR; Tompson J, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2629500; Yan X, 2016, ATTRIBUTE2IMAGE COND; Zawadzki E., 2015, AAAI C ART INT	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700107
C	Bresson, X; Laurent, T; Szlam, A; von Brecht, JH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bresson, Xavier; Laurent, Thomas; Szlam, Arthur; von Brecht, James H.			The Product Cut	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce a theoretical and algorithmic framework for multi-way graph partitioning that relies on a multiplicative cut-based objective. We refer to this objective as the Product Cut. We provide a detailed investigation of the mathematical properties of this objective and an effective algorithm for its optimization. The proposed model has strong mathematical underpinnings, and the corresponding algorithm achieves state-of-the-art performance on benchmark data sets.	[Bresson, Xavier] Nanyang Technol Univ, Singapore, Singapore; [Laurent, Thomas] Loyola Marymount Univ, Los Angeles, CA 90045 USA; [Szlam, Arthur] Facebook AI Res, New York, NY USA; [von Brecht, James H.] Calif State Univ Long Beach, Long Beach, CA 90840 USA	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University; Loyola Marymount University; Facebook Inc; California State University System; California State University Long Beach	Bresson, X (corresponding author), Nanyang Technol Univ, Singapore, Singapore.	xavier.bresson@ntu.edu.sg; tlaurent@lmu.edu; aszlam@fb.com; james.vonbrecht@csulb.edu			NSF [DMS-1414396]	NSF(National Science Foundation (NSF))	TL was supported by NSF DMS-1414396.	Andersen R, 2006, ANN IEEE SYMP FOUND, P475; [Anonymous], 2012, NIPS; Arora R., 2011, P 28 INT C MACH LEAR, P761; Bresson X., 2013, ADV NEURAL INFORM PR; Dhillon IS, 2007, IEEE T PATTERN ANAL, V29, P1944, DOI [10.1109/TPAMI.2007.1115, 10.1109/TP'AMI.2007.1115]; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Krishnan D, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461992; Livne OE, 2012, SIAM J SCI COMPUT, V34, pB499, DOI 10.1137/110843563; LOVASZ L, 1993, RANDOM STRUCT ALGOR, V4, P359, DOI 10.1002/rsa.3240040402; Rangapuram Syama Sundar, 2014, ADV NEURAL INFORM PR, V27, P3131; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Spielman D., 2003, C P ANN ACM S THEOR, DOI 10.1145/1007352.1007372; Spielman DA, 2013, SIAM J COMPUT, V42, P1, DOI 10.1137/080744888; Stella X. Yu, 2003, INT C COMP VIS; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701083
C	Bunel, R; Desmaison, A; Kohli, P; Torr, PHS; Kumar, MP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bunel, Rudy; Desmaison, Alban; Kohli, Pushmeet; Torr, Philip H. S.; Kumar, M. Pawan			Adaptive Neural Compilation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper proposes an adaptive neural-compilation framework to address the problem of learning efficient programs. Traditional code optimisation strategies used in compilers are based on applying pre-specified set of transformations that make the code faster to execute without changing its semantics. In contrast, our work involves adapting programs to make them more efficient while considering correctness only on a target input distribution. Our approach is inspired by the recent works on differentiable representations of programs. We show that it is possible to compile programs written in a low-level language to a differentiable representation. We also show how programs in this representation can be optimised to make them efficient on a target input distribution. Experimental results demonstrate that our approach enables learning specifically-tuned algorithms for given data distributions with a high success rate.	[Bunel, Rudy; Desmaison, Alban; Torr, Philip H. S.; Kumar, M. Pawan] Univ Oxford, Oxford, England; [Kohli, Pushmeet] Microsoft Res, Bengaluru, India	University of Oxford	Bunel, R (corresponding author), Univ Oxford, Oxford, England.	rudy@robots.ox.ac.uk; alban@robots.ox.ac.uk; pkohli@microsoft.com; philip.torr@eng.ox.ac.uk; pawan@robots.ox.ac.uk			EPSRC; Leverhulme Trust; Clarendon Fund; ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC/MURI grant [EP/N019474/1]; EPSRC grant [EP/M013774/1]; EPSRC Programme Grant [Seebibyte EP/M013774/1]; Microsoft Research PhD Scolarship Program	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Leverhulme Trust(Leverhulme Trust); Clarendon Fund; ERC(European Research Council (ERC)European Commission); EPSRC/MURI grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Programme Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Microsoft Research PhD Scolarship Program(Microsoft)	We would like to thank Siddharth Narayanaswamy and Diane Bouchacourt for helpful discussions and proofreading the paper. This work was supported by the EPSRC, Leverhulme Trust, Clarendon Fund and the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC/MURI grant ref EP/N019474/1, EPSRC grant EP/M013774/1, EPSRC Programme Grant Seebibyte EP/M013774/1 and Microsoft Research PhD Scolarship Program.	Andrychowicz Marcin, 2016, CORR; [Anonymous], 2015, ARXIV PREPRINT ARXIV; [Anonymous], 2014, ARXIV14105401; Grefenstette Edward, 2015, NIPS; Gruau Frederic, 1995, THEORETICAL COMPUTER; Joulin Armand, 2015, NIPS; Kaiser Lukasz, 2016, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kurach Karol, 2016, ICLR; Massalin H., 1987, Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS II) (Cat. No.87CH2440-6), P122; Neelakantan Arvind, 2016, 4 INT C LEARN REPR I; Neto Joao Pedro, 2003, J BRAZILIAN COMPUTER; Reed Scott, 2016, ICLR; Schkufza Eric, 2013, ACM SIGARCH COMPUTER; Sharma Rahul, 2015, OOPSLA; Siegelmann Hava, 1994, AAAI; Williams R. J., 1992, MACHINE LEARNING; Zaremba W., 2015, CORR	18	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703047
C	Cai, MB; Schuck, NW; Pillow, JW; Niv, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cai, Ming Bo; Schuck, Nicolas W.; Pillow, Jonathan W.; Niv, Yael			A Bayesian method for reducing bias in neural representational similarity analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				OBJECT REPRESENTATIONS; STATISTICS; CORTEX	In neuroscience, the similarity matrix of neural activity patterns in response to different sensory stimuli or under different cognitive states reflects the structure of neural representational space. Existing methods derive point estimations of neural activity patterns from noisy neural imaging data, and the similarity is calculated from these point estimations. We show that this approach translates structured noise from estimated patterns into spurious bias structure in the resulting similarity matrix, which is especially severe when signal-to-noise ratio is low and experimental conditions cannot be fully randomized in a cognitive task. We propose an alternative Bayesian framework for computing representational similarity in which we treat the covariance structure of neural activity patterns as a hyperparameter in a generative model of the neural data, and directly estimate this covariance structure from imaging data while marginalizing over the unknown activity patterns. Converting the estimated covariance structure into a correlation matrix offers a much less biased estimate of neural representational similarity. Our method can also simultaneously estimate a signal-to-noise map that informs where the learned representational structure is supported more strongly, and the learned covariance matrix can be used as a structured prior to constrain Bayesian estimation of neural activity patterns. Our code is freely available in Brain Imaging Analysis Kit (Brainiak) (https://github.com/IntelPNI/brainiak).	[Cai, Ming Bo; Schuck, Nicolas W.; Pillow, Jonathan W.; Niv, Yael] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA	Princeton University	Cai, MB (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	mcai@princeton.edu; nschuck@princeton.edu; pillow@princeton.edu; yael@princeton.edu	Schuck, Nicolas/E-1042-2017	Schuck, Nicolas/0000-0002-0150-8776	John Templeton Foundation; Intel Corporation; McKnight Foundation; NSF CAREER Award [IIS-1150186]; Simons Collaboration on the Global Brain [SCGB AWD1004351]	John Templeton Foundation; Intel Corporation(Intel Corporation); McKnight Foundation; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Simons Collaboration on the Global Brain	This publication was made possible through the support of grants from the John Templeton Foundation and the Intel Corporation. The opinions expressed in this publication are those of the authors and do not necessarily reflect the views of the John Templeton Foundation. JWP was supported by grants from the McKnight Foundation, Simons Collaboration on the Global Brain (SCGB AWD1004351) and the NSF CAREER Award (IIS-1150186). We thank Andrew C. Connolly etc. for sharing of the data used in 4.2. Data used in the supplementary material were obtained from the MGH-USC Human Connectome Project (HCP) database.	Alink A, 2015, BIORXIV; Connolly AC, 2012, J NEUROSCI, V32, P2608, DOI 10.1523/JNEUROSCI.5547-11.2012; Cox RW, 1996, COMPUT BIOMED RES, V29, P162, DOI 10.1006/cbmr.1996.0014; Davis T, 2013, ANN NY ACAD SCI, V1296, P108, DOI 10.1111/nyas.12156; deCharms RC, 2000, ANNU REV NEUROSCI, V23, P613, DOI 10.1146/annurev.neuro.23.1.613; Diedrichsen J, 2011, NEUROIMAGE, V55, P1665, DOI 10.1016/j.neuroimage.2011.01.044; Haxby JV, 2001, SCIENCE, V293, P2425, DOI 10.1126/science.1063736; Henriksson L, 2015, NEUROIMAGE, V114, P275, DOI 10.1016/j.neuroimage.2015.04.026; Jazzard P, 2003, FUNCTIONAL MAGNETIC, P404; Kravitz DJ, 2011, J NEUROSCI, V31, P7322, DOI 10.1523/JNEUROSCI.4588-10.2011; Kriegeskorte N, 2008, FRONT SYST NEUROSCI, V2, DOI 10.3389/neuro.06.004.2008; Kriegeskorte N, 2008, NEURON, V60, P1126, DOI 10.1016/j.neuron.2008.10.043; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Nishimoto S, 2011, CURR BIOL, V21, P1641, DOI 10.1016/j.cub.2011.08.031; Norman KA, 2006, TRENDS COGN SCI, V10, P424, DOI 10.1016/j.tics.2006.07.005; Peelen MV, 2012, J NEUROSCI, V32, P15728, DOI 10.1523/JNEUROSCI.1953-12.2012; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ritchey M., 2012, CEREB CORTEX; Schuck NW, 2016, NEURON, V91, P1402, DOI 10.1016/j.neuron.2016.08.019; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Walther A, 2015, NEUROIMAGE; Woolrich MW, 2001, NEUROIMAGE, V14, P1370, DOI 10.1006/nimg.2001.0931; Xue G, 2010, SCIENCE, V330, P97, DOI 10.1126/science.1193125; Zarahn E, 1997, NEUROIMAGE, V5, P179, DOI 10.1006/nimg.1997.0263	24	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700095
C	Carreira-Perpinan, MA; Raziperchikolaei, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Carreira-Perpinan, Miguel A.; Raziperchikolaei, Ramin			An Ensemble Diversity Approach to Supervised Binary Hashing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIMENSIONALITY REDUCTION	Binary hashing is a well-known approach for fast approximate nearest-neighbor search in information retrieval. Much work has focused on affinity-based objective functions involving the hash functions or binary codes. These objective functions encode neighborhood information between data points and are often inspired by manifold learning algorithms. They ensure that the hash functions differ from each other through constraints or penalty terms that encourage codes to be orthogonal or dissimilar across bits, but this couples the binary variables and complicates the already difficult optimization. We propose a much simpler approach: we train each hash function (or bit) independently from each other, but introduce diversity among them using techniques from classifier ensembles. Surprisingly, we find that not only is this faster and trivially parallelizable, but it also improves over the more complex, coupled objective function, and achieves state-of-the-art precision and recall in experiments with image retrieval.	[Carreira-Perpinan, Miguel A.; Raziperchikolaei, Ramin] Univ Calif, EECS, Merced, CA 95343 USA	University of California System; University of California Merced	Carreira-Perpinan, MA (corresponding author), Univ Calif, EECS, Merced, CA 95343 USA.	mcarreira-perpinan@ucmerced.edu; rraziperchikolaei@ucmerced.edu			NSF [IIS-1423515]	NSF(National Science Foundation (NSF))	Work supported by NSF award IIS-1423515.	Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Carreira-Perpinan M. A., 2015, CVPR; Carreira-Perpinan M. A, 2010, ICML; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Dietterich T. G., 1995, J ARTIFICIAL INTELLI, V2, P253; Dietterich T.G., 2000, ENSEMBLE METHODS MAC; Garey M. R., 1979, Computers and intractability. A guide to the theory of NP-completeness; Ge T., 2014, ECCV; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Gong Y., 2013, PAMI; Grauman K., 2013, MACHINE LEARNING COM, P49, DOI [DOI 10.1007/978-3-642-28661-2_3, 10.1007/978-3-642-28661-2_3]; Kolmogorov V., 2003, PAMI; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krogh A., 1995, Advances in Neural Information Processing Systems 7, P231; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Kuncheva L., 2014, COMBINING PATTERN CL; Leng C., 2014, ECML; Lin B., 2014, ICML; Lin G., 2013, ICCV; Lin GS, 2014, PROC CVPR IEEE, P1971, DOI 10.1109/CVPR.2014.253; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Liu W, 2011, SER INF MANAGE SCI, V10, P1; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Raziperchikolaei R., 2016, NIPS; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Shakhnarovich G., 2006, NEURAL INFORM PROCES; Wang J., 2012, PAMI; Weiss Y., 2009, NIPS; Zhang D, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P18	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702036
C	Chakrabarti, A; Shao, JY; Shakhnarovich, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chakrabarti, Ayan; Shao, Jingyu; Shakhnarovich, Gregory			Depth from a Single Image by Harmonizing Overcomplete Local Network Predictions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A single color image can contain many cues informative towards different aspects of local geometric structure. We approach the problem of monocular depth estimation by using a neural network to produce a mid-level representation that summarizes these cues. This network is trained to characterize local scene geometry by predicting, at every image location, depth derivatives of different orders, orientations and scales. However, instead of a single estimate for each derivative, the network outputs probability distributions that allow it to express confidence about some coefficients, and ambiguity about others. Scene depth is then estimated by harmonizing this overcomplete set of network predictions, using a globalization procedure that finds a single consistent depth map that best matches all the local derivative distributions. We demonstrate the efficacy of this approach through evaluation on the NYU v2 depth data set.	[Chakrabarti, Ayan; Shao, Jingyu; Shakhnarovich, Gregory] TTI Chicago, Chicago, IL 60637 USA; [Shao, Jingyu] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA USA	University of California System; University of California Los Angeles	Chakrabarti, A (corresponding author), TTI Chicago, Chicago, IL 60637 USA.	ayanc@ttic.edu; shaojy15@ucla.edu; gregory@ttic.edu			National Science Foundation [IIS-1618021]	National Science Foundation(National Science Foundation (NSF))	AC acknowledges support for this work from the National Science Foundation under award no. IIS-1618021, and from a gift by Adobe Systems. AC and GS thank NVIDIA Corporation for donations of Titan X GPUs used in this research.	[Anonymous], 2015, P CVPR; Baig  M., 2016, P WACV; Chakrabarti A., 2015, P CVPR; CHATFIELD K, 2014, P BMVC; Clowes M. B., 1971, ARTIFICIAL INTELLIGE; Eigen D., 2014, NIPS; Eigen David, 2015, P ICCV; Horn B. K., 1986, SHAPE SHADING; Karsch K., 2012, P ECCV; Ladicky  L., 2014, P CVPR; Liu F., 2015, P CVPR; Saxena A., 2005, NIPS; Silberman Nathan, 2012, P ECCV; Sugihara K., 1986, MACHINE INTERPRETATI; Wang  P., 2015, P CVPR; Wang X., 2015, P CVPR; Zoran D., 2015, P ICCV	17	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704035
C	Chakrabarti, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chakrabarti, Ayan			Learning Sensor Multiplexing Design through Back-propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recent progress on many imaging and vision tasks has been driven by the use of deep feed-forward neural networks, which are trained by propagating gradients of a loss defined on the final output, back through the network up to the first layer that operates directly on the image. We propose back-propagating one step further to learn camera sensor designs jointly with networks that carry out inference on the images they capture. In this paper, we specifically consider the design and inference problems in a typical color camera-where the sensor is able to measure only one color channel at each pixel location, and computational inference is required to reconstruct a full color image. We learn the camera sensor's color multiplexing pattern by encoding it as layer whose learnable weights determine which color channel, from among a fixed set, will be measured at each location. These weights are jointly trained with those of a reconstruction network that operates on the corresponding sensor measurements to produce a full color image. Our network achieves significant improvements in accuracy over the traditional Bayer pattern used in most color cameras. It automatically learns to employ a sparse color measurement approach similar to that of a recent design, and moreover, improves upon that design by learning an optimal layout for these measurements.	[Chakrabarti, Ayan] Toyota Technol Inst Chicago, 6045 S Kenwood Ave, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Chakrabarti, A (corresponding author), Toyota Technol Inst Chicago, 6045 S Kenwood Ave, Chicago, IL 60637 USA.	ayanc@ttic.edu						[Anonymous], 2015, P CVPR; [Anonymous], NIPS; Baraniuk RG, 2007, IEEE SIGNAL PROCESSI; Bayer B. E., 1976, U.S. Patent, Patent No. [3,971,065, 3971065, 3 971 065]; Burger H., 2012, P CVPR; Chakrabarti A., 2014, P ICCP; Elad M., 2007, IEEE T SIG P; Gehler P., 2008, P CVPR; HAN S., 2015, ARXIV151000149; Holloway J., 2012, P ICCP; Kaltzer T., 2016, P ICCP; Kapah O., 2000, ELECT IMAGING; Khashabi D., 2014, IEEE T IM PROC; Krizhevsky A., 2012, ADV NEURAL INF PROCE; LeCun Y., 1998, NEURAL NETWORKS TRIC; Levin A, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239521; Li X., 2008, P SPIE; Mairal J., 2009, P ICCV; Raskar R., 2006, ACM T GRAPHICS TOG; Schuler C. J., 2013, P CVPR; Sermanet P., 2013, COMPUT VIS PATTERN R; SHI L, 2010, REPROCESSED VERSION; Sun J., 2013, IEEE T IM PROC; VEERARAGHAVAN A, 2007, DAPPLED PHOTOGRAPHY; Wang X., 2015, P CVPR; Xu L., 2014, NIPS; Zhang L., 2005, IEEE T IMAG PROC; Zoran D., 2011, P ICCV	28	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701108
C	Chang, KW; He, H; Daume, H; Langford, J; Ross, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chang, Kai-Wei; He, He; Daume, Hal, III; Langford, John; Ross, Stephane			A Credit Assignment Compiler for Joint Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many machine learning applications involve jointly predicting multiple mutually dependent output variables. Learning to search is a family of methods where the complex decision problem is cast into a sequence of decisions via a search space. Although these methods have shown promise both in theory and in practice, implementing them has been burdensomely awkward. In this paper, we show the search space can be defined by an arbitrary imperative program, turning learning to search into a credit assignment compiler. Altogether with the algorithmic improvements for the compiler, we radically reduce the complexity of programming and the running time. We demonstrate the feasibility of our approach on multiple joint prediction tasks. In all cases, we obtain accuracies as high as alternative approaches, at drastically reduced execution and programming time.	[Chang, Kai-Wei] Univ Virginia, Charlottesville, VA 22903 USA; [He, He; Daume, Hal, III] Univ Maryland, College Pk, MD 20742 USA; [Langford, John] Microsoft Res, Redmond, WA USA; [Ross, Stephane] Google, Mountain View, CA USA	University of Virginia; University System of Maryland; University of Maryland College Park; Microsoft; Google Incorporated	Chang, KW (corresponding author), Univ Virginia, Charlottesville, VA 22903 USA.	kw@kwchang.net; hhe@cs.umd.edu; me@ha13.name; jc1@microsoft.com; stephaneross@google.com			NSF [IIS-1320538]	NSF(National Science Foundation (NSF))	Part of this work was carried out while Kai-Wei, Hal and Stephane were visiting Microsoft Research. Hal and He are also supported by NSF grant IIS-1320538. Any opinions, findings, conclusions, or recommendations expressed here are those of the authors and do not necessarily reflect the view of the sponsor. The authors thank anonymous reviewers for their comments.	Agarwal Alekh, 2011, ARXIV11104198; Andor D, 2016, GLOBALLY NORMALIZED; [Anonymous], 2000, ICML; Beygelzimer A, 2005, P 22 INT C MACH LEAR, P49; Bottou L., 2011, CRFSGD PROJECT; Chang K.-W., 2015, ILLINOISSL JAVA LIB; Chang K.-W., 2013, ECML; Chang Kai-Wei, 2015, ICML; Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082; Collins M., 2002, EMNLP; Collins Michael, 2004, ACL; Daume III H., 2005, ICML; Doppa J. R., 2014, JAIR, V50; Doppa J. R, 2012, ICML; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Dyer C., 2015, ACL; Eisner J., 2005, EMNLP; Germann U, 2004, ARTIF INTELL, V154, P127, DOI 10.1016/j.artint.2003.06.001; Goldberg Y., 2013, T ACL, V1; Goodman N., 2008, UAI; Hal Daume III, 2009, MACHINE LEARNING J; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Huang L., 2012, NAACL; Joachims T., 2009, MACHINE LEARNING J; Karampatziakis N., 2011, UAI; Kimmig A., 2012, NIPS WORKSH PROB PRO; Kordjamshidi P., 2015, IJCAI; Kudo T., 2005, CRF PROJECT; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Langford J., 2007, VOWPAL WABBIT; McCallum A., 2009, NIPS; Milch B, 2007, STAT RELATIONAL LEAR; Minka T., 2010, INFER NET 2 4 2010; Ng AY, 2000, P 16 C UNCERTAINTY A, P406; Nivre J., 2003, P 8 INT WORKSH PARS, P149; Pfeffer Avi, 2001, IJCAI; Rajamani K., 2014, INT C SOFTW ENG ICSE; Ratinov L., 2009, CONLL; Ratliff N., 2007, NIPS; Richardson M., 2006, MACHINE LEARNING, V62; Ross S., 2013, UAI; Ross S., 2014, ARXIV14065979; Ross Stephane, 2011, AI STATS; Roth D., 2007, INTRO STAT RELATIONA; Soon WM, 2001, COMPUT LINGUIST, V27, P521, DOI 10.1162/089120101753342653; Syed U., 2011, NIPS; Taskar Ben, 2003, NIPS; Xu YH, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2041; Xu Yuehua, 2007, INT C MACH LEARN COR, P1047	49	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702003
C	Chaudhuri, S; Tewari, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chaudhuri, Sougata; Tewari, Ambuj			Phased Exploration with Greedy Exploitation in Stochastic Combinatorial Partial Monitoring Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Partial monitoring games are repeated games where the learner receives feedback that might be different from adversary's move or even the reward gained by the learner. Recently, a general model of combinatorial partial monitoring (CPM) games was proposed [1], where the learner's action space can be exponentially large and adversary samples its moves from a bounded, continuous space, according to a fixed distribution. The paper gave a confidence bound based algorithm (GCB) that achieves O (T-2./3 log T) distribution independent and O (log T) distribution dependent regret bounds. The implementation of their algorithm depends on two separate offline oracles and the distribution dependent regret additionally requires existence of a unique optimal action for the learner. Adopting their CPM model, our first contribution is a Phased Exploration with Greedy Exploitation (PEGE) algorithmic framework for the problem. Different algorithms within the framework achieve O (T-2./3 root log T) distribution independent and O (log(2) T) distribution dependent regret respectively. Crucially, our framework needs only the simpler "argmax" oracle from GCB and the distribution dependent regret does not require existence of a unique optimal action. Our second contribution is another algorithm, PEGE2, which combines gap estimation with a PEGE algorithm, to achieve an O (log T) regret bound, matching the GCB guarantee but removing the dependence on size of the learner's action space. However, like GCB, PEGE2 requires access to both offline oracles and the existence of a unique optimal action. Finally, we discuss how our algorithm can be efficiently applied to a CPM problem of practical interest: namely, online ranking with feedback at the top.	[Chaudhuri, Sougata; Tewari, Ambuj] Univ Michigan Ann Arbor, Dept Stat, Ann Arbor, MI 48109 USA; [Tewari, Ambuj] Univ Michigan Ann Arbor, Dept EECS, Ann Arbor, MI USA	University of Michigan System; University of Michigan; University of Michigan System; University of Michigan	Chaudhuri, S (corresponding author), Univ Michigan Ann Arbor, Dept Stat, Ann Arbor, MI 48109 USA.	sougata@umich.edu; tewaria@umich.edu			NSF [IIS 1452099, CCF 1422157]	NSF(National Science Foundation (NSF))	We acknowledge the support of NSF via grants IIS 1452099 and CCF 1422157.	AGRAWAL R, 1989, SYST CONTROL LETT, V13, P405, DOI 10.1016/0167-6911(89)90107-2; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663; Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206; Chaudhuri S, 2015, JMLR WORKSH CONF PRO, V38, P129; Chen W., 2013, ICML 2013, P151; Hayes Thomas P., 2005, COMBINATORICS PROBAB; Komiyama Junpei, 2015, ADV NEURAL INFORM PR, P1783; Kveton B, 2015, JMLR WORKSH CONF PRO, V38, P535; Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701055
C	Chazal, F; Giulini, I; Michel, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chazal, Frederic; Giulini, Ilaria; Michel, Bertrand			Data driven estimation of Laplace-Beltrami operator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				THEORETICAL FOUNDATION; PENALTIES	Approximations of Laplace-Beltrami operators on manifolds through graph Laplacians have become popular tools in data analysis and machine learning. These discretized operators usually depend on bandwidth parameters whose tuning remains a theoretical and practical problem. In this paper, we address this problem for the unnormalized graph Laplacian by establishing an oracle inequality that opens the door to a well-founded data-driven procedure for the bandwidth selection. Our approach relies on recent results by Lacour and Massart [LM15] on the so-called Lepski's method.	[Chazal, Frederic; Giulini, Ilaria] Inria Saclay, Palaiseau, France; [Michel, Bertrand] Ecole Cent Nantes, Lab Math Jean Leray, UMR 6629, CNRS, Nantes, France	Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); Nantes Universite; Ecole Centrale de Nantes	Chazal, F (corresponding author), Inria Saclay, Palaiseau, France.	frederic.chazal@inria.fr; ilaria.giulini@me.com; bertrand.michel@ec-nantes.fr			ANR [ANR-13-BS01-0008]; ERC Gudhi [339025]	ANR(French National Research Agency (ANR)); ERC Gudhi	The authors are grateful to Pascal Massart for helpful discussions on Lepski's method. This work was supported by the ANR project TopData ANR-13-BS01-0008 and ERC Gudhi No. 339025	Arlot S, 2010, STAT SURV, V4, P40, DOI 10.1214/09-SS054; Arlot S, 2009, J MACH LEARN RES, V10, P245; Baudry JP, 2012, STAT COMPUT, V22, P455, DOI 10.1007/s11222-011-9236-1; Belkin M, 2005, LECT NOTES COMPUT SC, V3559, P486, DOI 10.1007/11503415_33; Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belkin M., 2007, NIPS PROCESSING, V19, P129; Belkin M, 2008, J COMPUT SYST SCI, V74, P1289, DOI 10.1016/j.jcss.2007.08.006; Birge L, 2007, PROBAB THEORY REL, V138, P33, DOI 10.1007/s00440-006-0011-8; Gine E., 2006, HIGH DIMENSIONAL PRO, V51, P238; Goldenshluger A, 2009, PROBAB THEORY REL, V143, P41, DOI 10.1007/s00440-007-0119-5; Goldenshluger A, 2008, BERNOULLI, V14, P1150, DOI 10.3150/08-BEJ144; Grigoryan A., 2009, HEAT KERNEL ANAL MAN, V47; Hein M., 2007, J MACHINE LEARNING R, V8; Lacour Claire, 2015, ARXIV150300946; Lacour Claire, 2016, ARXIV160705091; Lepski OV, 1997, ANN STAT, V25, P929; Lepskii O.V., 1992, TOPICS NONPARAMETRIC, V12, P87; LEPSKII OV, 1991, THEOR PROBAB APPL+, V36, P682, DOI 10.1137/1136085; LEPSKII OV, 1992, THEOR PROBAB APPL+, V37, P433; Nadler B, 2006, APPL COMPUT HARMON A, V21, P113, DOI 10.1016/j.acha.2005.07.004; Rieser Antonio, 2015, ARXIV150602633; Rosenberg S, 1997, LAPLACIAN RIEMANNIAN, V31; Ting D., 2011, ARXIV11015435; von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701067
C	Chen, JC; Sun, H; Woodruff, DP; Zhang, Q		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Jiecao; Sun, He; Woodruff, David P.; Zhang, Qin			Communication-Optimal Distributed Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Clustering large datasets is a fundamental problem with a number of applications in machine learning. Data is often collected on different sites and clustering needs to be performed in a distributed manner with low communication. We would like the quality of the clustering in the distributed setting to match that in the centralized setting for which all the data resides on a single site. In this work, we study both graph and geometric clustering problems in two distributed models: (1) a point-to-point model, and (2) a model with a broadcast channel. We give protocols in both models which we show are nearly optimal by proving almost matching communication lower bounds. Our work highlights the surprising power of a broadcast channel for clustering problems; roughly speaking, to spectrally cluster n points or n vertices in a graph distributed across s servers, for a worst-case partitioning the communication complexity in a point-to-point model is n . s, while in the broadcast model it is n + s. A similar phenomenon holds for the geometric setting as well. We implement our algorithms and demonstrate this phenomenon on real life datasets, showing that our algorithms are also very efficient in practice.	[Chen, Jiecao; Zhang, Qin] Indiana Univ, Bloomington, IN 47401 USA; [Sun, He] Univ Bristol, Bristol BS8 1UB, Avon, England; [Woodruff, David P.] IBM Res Almaden, San Jose, CA 95120 USA	Indiana University System; Indiana University Bloomington; University of Bristol; International Business Machines (IBM)	Chen, JC (corresponding author), Indiana Univ, Bloomington, IN 47401 USA.	jiecchen@indiana.edu; h.sun@bristol.ac.uk; dpwoodru@us.ibm.com; qzhangcs@indiana.edu			NSF [CCF-1525024, IIS-1633215]; XDATA program of the Defense Advanced Research Projects Agency (DARPA), Air Force Research Laboratory [FA8750-12-C-0323]	NSF(National Science Foundation (NSF)); XDATA program of the Defense Advanced Research Projects Agency (DARPA), Air Force Research Laboratory	Jiecao Chen and Qin Zhang are supported in part by NSF CCF-1525024 and IIS-1633215. D.W. thanks support from the XDATA program of the Defense Advanced Research Projects Agency (DARPA), Air Force Research Laboratory contract FA8750-12-C-0323.	Andoni A, 2016, ITCS'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON INNOVATIONS IN THEORETICAL COMPUTER SCIENCE, P311, DOI 10.1145/2840728.2840753; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Balcan Maria-Florina, 2014, CORR; Balcan Maria-Florina, 2013, NIPS, P1995; Braverman M, 2013, ANN IEEE SYMP FOUND, P668, DOI 10.1109/FOCS.2013.77; Charikar M, 2001, SIAM PROC S, P642; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Cormode Graham, 2007, ICDE, P1036; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Guha Sudipto, 2017, DISTRIBUTED PA UNPUB; KORUPOLU MR, 1998, P 9 ANN ACM SIAM S D, P1; Lee J.R., 2012, STOC, P1117; LEE YT, 2015, FDN COMP SCI FOCS 20, P250, DOI DOI 10.1109/FOCS.2015.24; Lotker Z., 2003, P S PAR ALG ARCH, P94; Miller G. L., 2012, CORR; Ng AY, 2002, ADV NEUR IN, V14, P849; Peng R., 2015, ARXIV 14112021, P1423; Phillips JM, 2016, SIAM J COMPUT, V45, P174, DOI 10.1137/15M1007525; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Woodruff DP, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P941	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704086
C	Chen, L; Karbasi, A; Crawford, FW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Lin; Karbasi, Amin; Crawford, Forrest W.			Estimating the Size of a Large Network and its Communities from a Random Sample	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SCALE-UP; RISK	Most real-world networks are too large to be measured or studied directly and there is substantial interest in estimating global network properties from smaller sub-samples. One of the most important global properties is the number of vertices/nodes in the network. Estimating the number of vertices in a large network is a major challenge in computer science, epidemiology, demography, and intelligence analysis. In this paper we consider a population random graph G = (V, E) from the stochastic block model (SBM) with K communities/blocks. A sample is obtained by randomly choosing a subset W subset of V and letting G (W) be the induced subgraph in G of the vertices in W. In addition to G (W), we observe the total degree of each sampled vertex and its block membership. Given this partial information, we propose an efficient PopULation Size Estimation algorithm, called PULSE, that accurately estimates the size of the whole population as well as the size of each community. To support our theoretical analysis, we perform an exhaustive set of experiments to study the effects of sample size, K, and SBM model parameters on the accuracy of the estimates. The experimental results also demonstrate that PULSE significantly outperforms a widely-used method called the network scale-up estimator in a wide variety of scenarios.	[Chen, Lin; Karbasi, Amin] Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA; [Chen, Lin; Karbasi, Amin; Crawford, Forrest W.] Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA; [Crawford, Forrest W.] Yale Univ, Dept Biostat, New Haven, CT USA	Yale University; Yale University; Yale University	Chen, L (corresponding author), Yale Univ, Dept Elect Engn, New Haven, CT 06520 USA.; Chen, L (corresponding author), Yale Univ, Yale Inst Network Sci, New Haven, CT 06520 USA.	lin.chen@yale.edu; amin.karbasi@yale.edu; forrest.crawford@yale.edu	Chen, Lin/CAH-1961-2022	Chen, Lin/0000-0003-0349-6577; Crawford, Forrest/0000-0002-0046-0547	Google; DARPA [D16AP00046]; NIH from NICHD [DP2HD091799]; NIH from NCATS [KL2 TR000140]; NIH from NIMH [P30 MH062294]; Yale Center for Clinical Investigation; Yale Center for Interdisciplinary Research on AIDS	Google(Google Incorporated); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NIH from NICHD; NIH from NCATS; NIH from NIMH; Yale Center for Clinical Investigation; Yale Center for Interdisciplinary Research on AIDS	This research was supported by Google Faculty Research Award, DARPA Young Faculty Award (D16AP00046), NIH grants from NICHD DP2HD091799, NCATS KL2 TR000140, and NIMH P30 MH062294, the Yale Center for Clinical Investigation, and the Yale Center for Interdisciplinary Research on AIDS. LC thanks Zheng Wei for his consistent support.	Aldous D.J., 1985, EXCHANGEABILITY RELA; Bernard H., 1988, ESTIMATING NUMBER PE; Bernard HRussell., 2001, CONNECTIONS, V24, P18; Bernstein, 2013, P SIGCHI C HUM FACT, P21, DOI [DOI 10.1145/2470654.2470658, 10.1145/2470654.2470658]; Chen L., 2016, ARXIV161008473; Chen L, 2016, AAAI CONF ARTIF INTE, P1174; Crawford FW, 2016, SOCIOL METHODOL, V46, P187, DOI 10.1177/0081175016641713; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Feehan DM, 2016, SOCIOL METHODOL, V46, P153, DOI 10.1177/0081175016665425; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Guo WW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0070718; Kadushin C., 2006, J DRUG ISSUES; Katzir L., 2011, WWW, P597, DOI DOI 10.1145/1963405.1963489; Maiya Arun S, 2011, P 17 ACM SIGKDD INT, P105, DOI DOI 10.1145/2020408.2020431; Massoulie A L, 2006, P 25 ANN ACM S PRINC, P123, DOI DOI 10.1145/1146381.1146402; Murray B. H., 2000, SIZING THE INTERNET, P3; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Papagelis M, 2013, IEEE T KNOWL DATA EN, V25, P662, DOI 10.1109/TKDE.2011.254; Ribeiro B., 2010, P 10 ACM SIGCOMM C I, P390, DOI [DOI 10.1145/1879141.1879192, 10.1145/1879141]; Salganik MJ, 2011, AM J EPIDEMIOL, V174, P1190, DOI 10.1093/aje/kwr246; Shokoohi M, 2012, INT J PREVENTIVE MED, V3, P471; Xing S, 2003, IEEE J SEL AREA COMM, V21, P922, DOI 10.1109/JSAC.2003.814510	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701086
C	Chen, S; Banerjee, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Sheng; Banerjee, Arindam			Structured Matrix Recovery via the Generalized Dantzig Selector	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INEQUALITIES	In recent years, structured matrix recovery problems have gained considerable attention for its real world applications, such as recommender systems and computer vision. Much of the existing work has focused on matrices with low-rank structure, and limited progress has been made on matrices with other types of structure. In this paper we present non-asymptotic analysis for estimation of generally structured matrices via the generalized Dantzig selector based on sub-Gaussian measurements. We show that the estimation error can always be succinctly expressed in terms of a few geometric measures such as Gaussian widths of suitable sets associated with the structure of the underlying true matrix. Further, we derive general bounds on these geometric measures for structures characterized by unitarily invariant norms, a large family covering most matrix norms of practical interest. Examples are provided to illustrate the utility of our theoretical development.	[Chen, Sheng; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Chen, S (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	shengc@cs.umn.edu; banerjee@cs.umn.edu			NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	The research was supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; [Anonymous], [No title captured]; Argyriou A., 2012, NIPS; Banerjee A., 2014, NIPS; Bhatia Rajendra, 1997, MATRIX ANAL, DOI 10.1007/978-1-4612-0653-8; Bourgain J, 2015, GEOM FUNCT ANAL, V25, P1009, DOI 10.1007/s00039-015-0332-9; Cai T. T., 2014, ARXIV14044408; Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chatterjee S., 2014, ADV NEURAL INFORM PR; Chen S., 2015, NIPS, P2908; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Dirksen S., 2014, ARXIV14023973; Dirksen S., 2015, ELECTRON J PROBAB, P20; Figueiredo M. A. T., 2016, AISTATS; GORDON Y, 1988, LECT NOTES MATH, V1317, P84; GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761; Gunasekar S., 2015, P ADV NEUR INF PROC, P1180; Gunasekar Suriya, 2014, INT C MACH LEARN ICM; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; LEWIS A. S., 1995, J CONVEX ANAL, V2, P173; McDonald A. M., 2014, NIPS; Mendelson S, 2007, GEOM FUNCT ANAL, V17, P1248, DOI 10.1007/s00039-007-0618-7; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Talagrand M., 2014, UPPER LOWER BOUNDS S; Zhang X., 2013, NIPS; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x; Zuk O., 2015, INT C MACH LEARN ICM; [No title captured], DOI [10.1007/BF01895708, DOI 10.1007/BF01895708]	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703030
C	Chen, WF; Fu, Z; Yang, DW; Deng, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Weifeng; Fu, Zhao; Yang, Dawei; Deng, Jia			Single-Image Depth Perception in the Wild	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper studies single-image depth perception in the wild, i.e., recovering depth from a single image taken in unconstrained settings. We introduce a new dataset "Depth in the Wild" consisting of images in the wild annotated with relative depth between pairs of random points. We also propose a new algorithm that learns to estimate metric depth using annotations of relative depth. Compared to the state of the art, our algorithm is simpler and performs better. Experiments show that our algorithm, combined with existing RGB-D data and our new relative depth annotations, significantly improves single-image depth perception in the wild. [GRAPHICS] .	[Chen, Weifeng; Fu, Zhao; Yang, Dawei; Deng, Jia] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Chen, WF (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	wfchen@umich.edu; zhaofu@umich.edu; ydawei@umich.edu; jiadeng@umich.edu	Chen, Wei/GZK-7348-2022		National Science Foundation [1617767]	National Science Foundation(National Science Foundation (NSF))	This work is partially supported by the National Science Foundation under Grant No. 1617767.	[Anonymous], 2015, CVPR; Baig M. H., 2015, ARXIV150104537, P5; Baig M. H., 2014, WACV; Barron Jonathan T, 2015, TPAMI; Bell Sean, 2014, TOG; Cao Zhe, 2007, P 24 INT C MACH LEAR; Chiu W. W.-C., 2011, BMVC; Choi S., 2016, ARXIV160202481; Eigen D., 2015, ICCV; Eigen D., 2014, NIPS; Geiger A., 2013, INT J ROBOT RES; Hane C., 2015, CVPR; Hoiem D., 2005, TOG; Janoch A., 2013, CONSUMER DEPTH CAMER; Joachims T., 2002, P 8 ACM SIGKDD INT C; Karsch K., 2014, TPAMI; Ladicky L., 2014, CVPR; Li C., 2015, CVPR; Liu B., 2010, CVPR; Liu F., 2015, CVPR; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Narihira Takuya, 2015, CVPR; Newell A., 2016, ARXIV160306937; Parikh D., 2011, ICCV; Saxena A., 2005, NIPS; Saxena A., 2008, IJCV; Saxena Ashutosh, 2009, TPAMI; Shelhamer Evan, 2015, ICCV WORKSH; Shi J., 2015, TOG; Silberman N., 2012, ECCV; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Song S., 2015, CVPR; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Todd JT, 2003, PERCEPT PSYCHOPHYS, V65, P31, DOI 10.3758/BF03194781; Xie S. M., 2015, CORR; Xiong Y., 2015, TPAMI; Zhang X, 2013, ARXIV PREPRINT ARXIV; Zhang Z., 2015, ICCV; Zhou T., 2015, ICCV; Zhuo W., 2015, CVPR; Zoran Daniel, 2015, ICCV	41	0	0	6	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704014
C	Chen, X; Cheng, Y; Tang, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Xi; Cheng, Yu; Tang, Bo			On the Recursive Teaching Dimension of VC Classes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The recursive teaching dimension (RTD) of a concept class C subset of {0, 1}(n), introduced by Zilles et al. [ZLHZ11], is a complexity parameter measured by the worst-case number of labeled examples needed to learn any target concept of C in the recursive teaching model. In this paper, we study the quantitative relation between RTD and the well-known learning complexity measure VC dimension (VCD), and improve the best known upper and (worst-case) lower bounds on the recursive teaching dimension with respect to the VC dimension. Given a concept class C subset of {0,1}(n) with VCD(C) = d, we first show that RTD(C) is at most d . 2(d+1). This is the first upper bound for RTD(C) that depends only on VCD(C), independent of the size of the concept class vertical bar C vertical bar and its domain size n. Before our work, the best known upper bound for RTD(C) is 0(d2(d) log log vertical bar C vertical bar), obtained by Moran et al. [MSWY15]. We remove the log log vertical bar C vertical bar factor. We also improve the lower bound on the worst-case ratio of RTD(C) to VCD(C). We present a family of classes {C-k}(k >= 1) with VCD(C-k) = 3k and RTD (C-k) = 5k, which implies that the ratio of RTD(C) to VCD(C) in the worst case can be as large as 5/3. Before our work, the largest ratio known was 3/2 as obtained by Kuhlmann [Kuh99]. Since then, no finite concept class C has been known to satisfy RTD(C) > (3/2) . VCD(C).	[Chen, Xi] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Cheng, Yu] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA; [Tang, Bo] Univ Oxford, Dept Comp Sci, Oxford, England	Columbia University; University of Southern California; University of Oxford	Chen, X (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10027 USA.	xichen@cs.columbia.edu; yu.cheng.1@usc.edu; tangbonk1@gmail.com			NSF [CCF-1149257, CCF-1423100]; Shang-Hua Teng's Simons Investigator Award; ERC [321171]	NSF(National Science Foundation (NSF)); Shang-Hua Teng's Simons Investigator Award; ERC(European Research Council (ERC)European Commission)	We thank the anonymous reviewers for their helpful comments and suggestions. We also thank Joseph Bebel for pointing us to the SAT solvers. This work was done in part while the authors were visiting the Simons Institute for the Theory of Computing. Xi Chen is supported by NSF grants CCF-1149257 and CCF-1423100. Yu Cheng is supported in part by Shang-Hua Teng's Simons Investigator Award. Bo Tang is supported by ERC grant 321171.	[Anonymous], [No title captured]; Audemard G., 2014, GLUCOSE 4 0; Biere A., 2015, LINGELING PLINGELING; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Darnstadt M, 2016, THEOR COMPUT SCI, V620, P73, DOI 10.1016/j.tcs.2015.10.038; Doliwa T, 2014, J MACH LEARN RES, V15, P3107; Doliwa T, 2010, LECT NOTES ARTIF INT, V6331, P209, DOI 10.1007/978-3-642-16108-7_19; Een N, 2004, LECT NOTES COMPUT SC, V2919, P502, DOI 10.1007/978-3-540-24605-3_37; GOLDMAN SA, 1995, J COMPUT SYST SCI, V50, P20, DOI 10.1006/jcss.1995.1003; Kuhlmann C, 1999, LECT NOTES ARTIF INT, V1572, P168; Littlestone N., 1986, RELATING DATA COMPRE; Moran S, 2015, ANN IEEE SYMP FOUND, P40, DOI 10.1109/FOCS.2015.12; Samei R, 2014, THEOR COMPUT SCI, V558, P35, DOI 10.1016/j.tcs.2014.09.024; Shinohara A., 1990, Algorithmic Learning Theory, P247; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Warmuth MK, 2003, LECT NOTES ARTIF INT, V2777, P743, DOI 10.1007/978-3-540-45167-9_60; Wigderson A, 2012, ANN IEEE SYMP FOUND, P390, DOI 10.1109/FOCS.2012.14; Zilles S, 2011, J MACH LEARN RES, V12, P349	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703048
C	Cheng, TY; Lin, KH; Gong, XY; Liu, KJ; Wu, SH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cheng, Ting-Yu; Lin, Kuan-Hua; Gong, Xinyang; Liu, Kang-Jun; Wu, Shan-Hung			Learning User Perceived Clusters with Feature-Level Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Semi-supervised clustering algorithms have been proposed to identify data clusters that align with user perceived ones via the aid of side information such as seeds or pairwise constrains. However, traditional side information is mostly at the instance level and subject to the sampling bias, where non-randomly sampled instances in the supervision can mislead the algorithms to wrong clusters. In this paper, we propose learning from the feature-level supervision. We show that this kind of supervision can be easily obtained in the form of perception vectors in many applications. Then we present novel algorithms, called Perception Embedded (PE) clustering, that exploit the perception vectors as well as traditional side information to find clusters perceived by the user. Extensive experiments are conducted on real datasets and the results demonstrate the effectiveness of PE empirically.				tycheng@datalab.cs.nthu.edu.tw; khlin@datalab.cs.nthu.edu.tw; xygong@datalab.cs.nthu.edu.tw; kjliu@datalab.cs.nthu.edu.tw; shwu@cs.nthu.edu.tw						Banerjee A, 2005, P 11 ACM SIGKDD INT, P532, DOI DOI 10.1145/1081870.1081932; Basu Sugato, 2004, KDD, P59, DOI DOI 10.1145/1014052.1014062; Bhatia SK, 1998, IEEE T SYST MAN CY B, V28, P427, DOI 10.1109/3477.678640; Bilenko M., 2003, P ACM INTCONF KNOWLE, P39; Chua T.-S., 2009, P ACM INT C IM VID R, P1, DOI 10.1145/1646396.1646452; Cleuziou G, 2008, INT C PATT RECOG, P563; Davidson I, 2012, P 18 ACM SIGKDD INT, P1312; Demiriz A., 1999, ARTIFICIAL NEURAL NE, V9, P809; Ding Chris, 2006, P 12 ACM SIGKDD INT, V2006, P126; Finley T., 2005, INT C MACHINE LEARNI, P217, DOI DOI 10.1145/1102351.1102379; Frigui H, 1999, IEEE T PATTERN ANAL, V21, P450, DOI 10.1109/34.765656; He XN, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P771, DOI 10.1145/2566486.2567975; Hillel A., 2003, P 20 INT C MACH LEAR, P11; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Klein D., 2002, P 19 INT C MACH LEAR, P307; Li Q, 2003, IEEE/WIC INTERNATIONAL CONFERENCE ON WEB INTELLIGENCE, PROCEEDINGS, P33; Li Z, 2008, PROC 25 INT C MACH L; Li ZG, 2009, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2009.5459157; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Long Mingsheng, 2012, P 2012 SIAM INT C DA, P540, DOI DOI 10.1137/1.9781611972825.47.SIAM; Lu Z., 2004, ADV NEURAL INFORM PR, P849; Nie FP, 2009, 21ST INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-09), PROCEEDINGS, P1181; Poon L., 2010, P 27 INT C MACH LEAR, P887; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Sadikov E., 2010, P 19 INT C WORLD WID, P841; Schedl M., 2013, P 4 ACM MULT SYST C, P78; Schultz  M., 2004, P NIPS; Sugato Basu R. J. M., 2002, P ICML, P27; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; WAGSTAFF K, 2000, P 17 INT C MACH LEAR, P1103; Xiaoyong Liu, 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186; Yi J., 2013, P 30 INT C MACH LEAR, P1400; Yue YS, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P75, DOI 10.1145/2566486.2567991	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702007
C	Chowdhury, S; Memoli, F; Smith, Z		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chowdhury, Samir; Memoli, Facundo; Smith, Zane			Improved Error Bounds for Tree Representations of Metric Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Estimating optimal phylogenetic trees or hierarchical clustering trees from metric data is an important problem in evolutionary biology and data analysis. Intuitively, the goodness-of-fit of a metric space to a tree depends on its inherent treeness, as well as other metric properties such as intrinsic dimension. Existing algorithms for embedding metric spaces into tree metrics provide distortion bounds depending on cardinality. Because cardinality is a simple property of any set, we argue that such bounds do not fully capture the rich structure endowed by the metric. We consider an embedding of a metric space into a tree proposed by Gromov. By proving a stability result, we obtain an improved additive distortion bound depending only on the hyperbolicity and doubling dimension of the metric. We observe that Gromov's method is dual to the well-known single linkage hierarchical clustering (SLHC) method. By means of this duality, we are able to transport our results to the setting of SLHC, where such additive distortion bounds were previously unknown.	[Chowdhury, Samir; Memoli, Facundo] Ohio State Univ, Dept Math, 231 W 18th Ave, Columbus, OH 43210 USA; [Memoli, Facundo; Smith, Zane] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	Ohio State University; Ohio State University	Chowdhury, S (corresponding author), Ohio State Univ, Dept Math, 231 W 18th Ave, Columbus, OH 43210 USA.	chowdhury.57@osu.edu; memoli@math.osu.edu; smith.9911@osu.edu						Abraham Ittai, 2007, P 26 ANN ACM S PRINC; Abu-Ata Muad, 2014, ARXIV14023364; Agarwala R, 1999, SIAM J COMPUT, V28, P1073, DOI 10.1137/S0097539795296334; Bartal Y., 1996, FDN COMPUTER SCI; Barthelemy JP, 1991, TREES PROXIMITY REPR; Carlsson G. E., 2010, J MACHINE LEARNING R; Chakerian John, 2012, J COMPUTATIONAL GRAP; Chepoi V, 2000, J MATH PSYCHOL, V44, P600, DOI 10.1006/jmps.1999.1270; Deza M. M., 2009, ENCY DISTANCES; Fakcharoenphol J., 2003, P 35 ANN ACM S THEOR, P448, DOI DOI 10.1145/780542.780608; FARACH M, 1995, ALGORITHMICA, V13, P155, DOI 10.1007/BF01188585; Gromov M., 1987, HYPERBOLIC GROUPS; Indyk P., 2004, HDB DISCRETE COMPUTA, P177, DOI DOI 10.1201/9781420035315.CH8; Kleindessner M., 2014, 27 C LEARNING THEORY, V35, P40; Krauthgamer R., 2004, P 15 ANN ACM SIAM S, P798; KRIVANEK M, 1988, INFORM PROCESS LETT, V27, P265, DOI 10.1016/0020-0190(88)90090-7; Li YL, 2006, LECT NOTES COMPUT SC, V3971, P889; Mardia K.V, 1980, MULTIVARIATE ANAL; Semple C, 2003, PHYLOGENETICS, V24; Shieh AD, 2011, P NATL ACAD SCI USA, V108, P16916, DOI 10.1073/pnas.1018393108; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703067
C	Chu, X; Ouyang, WL; Li, HS; Wang, XG		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chu, Xiao; Ouyang, Wanli; Li, Hongsheng; Wang, Xiaogang			CRF-CNN: Modeling Structured Information in Human Pose Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep convolutional neural networks (CNN) have achieved great success. On the other hand, modeling structural information has been proved critical in many vision problems. It is of great interest to integrate them effectively. In a classical neural network, there is no message passing between neurons in the same layer. In this paper, we propose a CRF-CNN framework which can simultaneously model structural information in both output and hidden feature layers in a probabilistic way, and it is applied to human pose estimation. A message passing scheme is proposed, so that in various layers each body joint receives messages from all the others in an efficient way. Such message passing can be implemented with convolution between features maps in the same layer, and it is also integrated with feed-forward propagation in neural networks. Finally, a neural network implementation of endto-end learning CRF-CNN is provided. Its effectiveness is demonstrated through experiments on two benchmark datasets.	[Chu, Xiao; Ouyang, Wanli; Li, Hongsheng; Wang, Xiaogang] Chinese Univ Hong Kong, Hong Kong, Peoples R China	Chinese University of Hong Kong	Chu, X (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	xchu@ee.cuhk.edu.hk; wlouyang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk; xgwang@ee.cuhk.edu.hk			SenseTime Group Limited; Research Grants Council of Hong Kong [CUHK14206114, CUHK14205615, CUHK14207814, CUHK14203015, CUHK417011]; National Natural Science Foundation of China [61371192, 61301269]	SenseTime Group Limited; Research Grants Council of Hong Kong(Hong Kong Research Grants Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by SenseTime Group Limited, Research Grants Council of Hong Kong (Project Number CUHK14206114, CUHK14205615, CUHK14207814, CUHK14203015, and CUHK417011) and National Natural Science Foundation of China (Number 61371192 and 61301269). W. Ouyang and X. Wang are the corresponding authors.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, CVPR; [Anonymous], 2014, ADV NEURAL INF PROCE; Chu X., 2015, ICCV; Chu X., 2016, CVPR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J., 2014, ECCV; Eslami S., 2016, ARXIV160308575; Felzenszwalb PF, 2005, INT J COMPUT VISION, V61, P55, DOI 10.1023/B:VISI.0000042934.15159.49; Gal Y., 2015, ARXIV150602142; He K., 2015, ARXIV150201852; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Johnson S., 2010, P BRIT MACH VIS C; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Li W, 2014, CVPR; Lin G., 2015, NIPS; Ouyang W., 2014, CVPR; Pishchulin L., 2015, DEEPCUT JOINT SUBSET; Sapp B., 2013, CVPR; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Tompson J., 2015, CVPR; Tompson J., 2014, NIPS; WAN L, 2014, ARXIV14115309; Wang Yang, 2011, CVPR; Xiao T., 2016, CVPR; Yang W., 2016, CVPR; Yang Y, 2013, IEEE T PATTERN ANAL, V35, P2878, DOI 10.1109/TPAMI.2012.261; Zheng S., 2015, ICCV	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702025
C	Cormier, Q; Fard, MM; Canini, K; Gupta, MR		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cormier, Q.; Fard, M. Milani; Canini, K.; Gupta, M. R.			Launch and Iterate: Reducing Prediction Churn	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				STABILITY	Practical applications of machine learning often involve successive training iterations with changes to features and training examples. Ideally, changes in the output of any new model should only be improvements (wins) over the previous iteration, but in practice the predictions may change neutrally for many examples, resulting in extra net-zero wins and losses, referred to as unnecessary churn. These changes in the predictions are problematic for usability for some applications, and make it harder and more expensive to measure if a change is statistically significant positive. In this paper, we formulate the problem and present a stabilization operator to regularize a classifier towards a previous classifier. We use a Markov chain Monte Carlo stabilization operator to produce a model with more consistent predictions without adversely affecting accuracy. We investigate the properties of the proposal with theoretical analysis. Experiments on benchmark datasets for different classification algorithms demonstrate the method and the resulting reduction in churn.	[Cormier, Q.] ENS Lyon, 15 Parvis Rene Descartes, Lyon, France; [Fard, M. Milani; Canini, K.; Gupta, M. R.] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA	Ecole Normale Superieure de Lyon (ENS de LYON); Google Incorporated	Cormier, Q (corresponding author), ENS Lyon, 15 Parvis Rene Descartes, Lyon, France.	quentin.cormier@ens-lyon.fr; mmilanifard@google.com; canini@google.com; mayagupta@google.com						Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bousquet O, 2001, ADV NEUR IN, V13, P196; Candillier L., 2012, P ALRA ACT LEARN REA; Cherkassky V, 1997, IEEE Trans Neural Netw, V8, P1564, DOI 10.1109/TNN.1997.641482; DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P601, DOI 10.1109/TIT.1979.1056087; Fernandes K, 2015, LECT NOTES ARTIF INT, V9273, P535, DOI 10.1007/978-3-319-23485-4_53; Kawala F., 2014, CORIA 2014, P1; Niculescu-Mizil Alexandru, 2005, P 22 INT C MACHINE L, P625, DOI 10.1145/1102351.1102430; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Platt JC, 2000, ADV NEUR IN, P61; Reinart A., 2015, STAT DONE WRONG WOEF; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Zhang L., 2016, INFORM SCI	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700017
C	Cortes, C; DeSalvo, G; Mohri, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cortes, Corinna; DeSalvo, Giulia; Mohri, Mehryar			Boosting with Abstention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a new boosting algorithm for the key scenario of binary classification with abstention where the algorithm can abstain from predicting the label of a point, at the price of a fixed cost. At each round, our algorithm selects a pair of functions, a base predictor and a base abstention function. We define convex upper bounds for the natural loss function associated to this problem, which we prove to be calibrated with respect to the Bayes solution. Our algorithm benefits from general margin-based learning guarantees which we derive for ensembles of pairs of base predictor and abstention functions, in terms of the Rademacher complexities of the corresponding function classes. We give convergence guarantees for our algorithm along with a linear-time weak-learning algorithm for abstention stumps. We also report the results of several experiments suggesting that our algorithm provides a significant improvement in practice over two confidence-based algorithms.	[Cortes, Corinna] Google Res, New York, NY 10011 USA; [DeSalvo, Giulia] Courant Inst, New York, NY 10012 USA; [Mohri, Mehryar] Google, New York, NY 10012 USA	Google Incorporated; Google Incorporated	Cortes, C (corresponding author), Google Res, New York, NY 10011 USA.	corinna@google.com; desalvo@cims.nyu.edu; mohri@cims.nyu.edu			NSF [CCF-1535987, IIS-1618662]	NSF(National Science Foundation (NSF))	This work was partly funded by NSF CCF-1535987 and IIS-1618662.	Bartlett Peter L., 2008, JMLR; Bounsiar A., 2007, WASET; Capitaine H. L., 2010, ICPR; Chaudhuri Kamalika, 2014, NIPS; Chow C., 1957, IEEE T COMPUT; Chow C., 1970, IEEE T COMPUT; Cortes C., 2016, ALT; Dubuisson B., 1993, PR; El-Yaniv R., 2011, NIPS; El-Yaniv R., 2010, JMLR; Elkan C., 2001, IJCAI; Fumera G., 2000, ICAPR; Fumera G., 2002, ICPR; Grandvalet Y., 2008, NIPS; I. CVX Research, 2012, CVX MATLAB SOFTWARE; Koltchinskii V, 2002, ANN STAT, V30, P1; Landgrebe T., 2005, PRL; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Littman M., 2008, ICML; Luo Z.-Q., 1992, J OPTIMIZATION THEOR; Melvin I, 2008, BMC BIOINFORMATICS, V9, DOI 10.1186/1471-2105-9-389; Mohri M., 2018, FDN MACHINE LEARNING; Pietraszek T., 2005, ICML; Santos-Pereira C., 2005, PRL; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; Tax D., 2008, PATTERN RECOGNITION; Tortorella F., 2001, ICAPR; Trapeznikov Kirill, 2013, AISTATS; Wang J., 2014, JMLR; Yuan M., 2010, JMLR; Yuang M., 2011, BERNOULLI; Zhang C., 2016, COLT	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702083
C	Cortes, C; Kuznetsov, V; Mohrii, M; Yang, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cortes, Corinna; Kuznetsov, Vitaly; Mohrii, Mehryar; Yang, Scott			Structured Prediction Theory Based on Factor Graph Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a general theoretical analysis of structured prediction with a series of new results. We give new data-dependent margin guarantees for structured prediction for a very wide family of loss functions and a general family of hypotheses, with an arbitrary factor graph decomposition. These are the tightest margin bounds known for both standard multi-class and general structured prediction problems. Our guarantees are expressed in terms of a data-dependent complexity measure, factor graph complexity, which we show can be estimated from data and bounded in terms of familiar quantities for several commonly used hypothesis sets along with a sparsity measure for features and graphs. Our proof techniques include generalizations of Talagrand's contraction lemma that can be of independent interest. We further extend our theory by leveraging the principle of Voted Risk Minimization (VRM) and show that learning is possible even with complex factor graphs. We present new learning bounds for this advanced setting, which we use to design two new algorithms, Voted Conditional Random Field (VCRF) and Voted Structured Boosting (StructBoost). These algorithms can make use of complex features and factor graphs and yet benefit from favorable learning guarantees. We also report the results of experiments with VCRF on several datasets to validate our theory.	[Cortes, Corinna; Kuznetsov, Vitaly] Google Res, New York, NY 10011 USA; [Mohrii, Mehryar; Yang, Scott] Courant Inst, New York, NY 10012 USA; [Mohrii, Mehryar] Google, New York, NY 10012 USA	Google Incorporated; Google Incorporated	Cortes, C (corresponding author), Google Res, New York, NY 10011 USA.	corinna@google.com; vitaly@cims.nyu.edu; mohri@cims.nyu.edu; yangs@cims.nyu.edu			NSF [CCF-1535987, IIS-1618662, GRFP DGE-1342536]	NSF(National Science Foundation (NSF))	This work was partly funded by NSF CCF-1535987 and IIS-1618662 and NSF GRFP DGE-1342536.	BakIr G., 2007, PREDICTING STRUCTURE; Chang Kai-Wei, 2015, ICML; Collins  M., 2001, P IWPT; Cortes C., 2014, ICML; Cortes C., 2007, PREDICTING STRUCTURE; Cortes  C., 2015, JMLR; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Doppa JR, 2014, J MACH LEARN RES, V15, P1317; Kuznetsov V., 2014, NIPS; LAFFERTY J, 2001, ICML; Lam M., 2015, CVPR; Lei  Y., 2015, NIPS; Lucchi A., 2013, CVPR; Maurer  A., 2016, ALT; McAllester D., 2007, PREDICTING STRUCTURE; Mohri M., 2018, FDN MACHINE LEARNING; Nadeau D, 2007, LINGUIST INVESTIG, V30, P3; Ross S., 2011, P INT C ARTIFICIAL I, P627; Taskar Ben, 2003, NIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vinyals O., 2015, NIPS; Vinyals O., 2015, CVPR; Zhang  D., 2008, IJCNLP	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704010
C	David, O; Moran, S; Yehudayoff, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		David, Ofir; Moran, Shay; Yehudayoff, Amir			On statistical learning via the lens of compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SAMPLE COMPRESSION; LEARNABILITY; BOUNDS	This work continues the study of the relationship between sample compression schemes and statistical learning, which has been mostly investigated within the framework of binary classification. The central theme of this work is establishing equivalences between learnability and compressibility, and utilizing these equivalences in the study of statistical learning theory. We begin with the setting of multiclass categorization (zero/one loss). We prove that in this case learnability is equivalent to compression of logarithmic sample size, and that uniform convergence implies compression of constant size. We then consider Vapnik's general learning setting: we show that in order to extend the compressibility-learnability equivalence to this case, it is necessary to consider an approximate variant of compression. Finally, we provide some applications of the compressibility-learnability equivalences.	[David, Ofir; Yehudayoff, Amir] Technion Israel Inst Technol, Dept Math, Haifa, Israel; [Moran, Shay] Technion Israel Inst Technol, Dept Comp Sci, Haifa, Israel	Technion Israel Institute of Technology; Technion Israel Institute of Technology	David, O (corresponding author), Technion Israel Inst Technol, Dept Math, Haifa, Israel.	ofirdav@tx.technion.ac.il; shaymrn@cs.technion.ac.il; amir.yehudayoff@gmail.com						Ben-David S, 1998, DISCRETE APPL MATH, V86, P3, DOI 10.1016/S0166-218X(98)00000-6; BENDAVID S, 1995, J COMPUT SYST SCI, V50, P74, DOI 10.1006/jcss.1995.1008; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Chernikov A, 2013, ISR J MATH, V194, P409, DOI 10.1007/s11856-012-0061-9; Cummings R., 2016, P 29 C LEARN THEOR C, P772; Daniely A., 2014, COLT, P287; Floyd S., 1989, Proceedings of the Second Annual Workshop on Computational Learning Theory, P349; Floyd S, 1995, MACH LEARN, V21, P269; FREUND Y, 1995, INFORM COMPUT, V121, P256, DOI 10.1006/inco.1995.1136; Freund Y., 2012, BOOSTING FDN ALGORIT; Gottlieb  Lee-Ad, 2015, ABS150206208 CORR, P2; Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7; HELMBOLD D, 1992, SIAM J COMPUT, V21, P240, DOI 10.1137/0221019; Kuzmin D, 2007, J MACH LEARN RES, V8, P2047; Littlestone N., 1986, RELATING DATA UNPUB; Livni Roi, 2013, COLT 2013, V30, P77; Moran S, 2016, J ACM, V63, DOI 10.1145/2890490; Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1023/A:1022605311895; Rubinstein BIP, 2012, J MACH LEARN RES, V13, P1221; Rubinstein BIP, 2009, J COMPUT SYST SCI, V75, P37, DOI 10.1016/j.jcss.2008.07.005; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Vapnik V.N, 1998, STAT LEARNING THEORY; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Warmuth MK, 2003, LECT NOTES ARTIF INT, V2777, P743, DOI 10.1007/978-3-540-45167-9_60	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704015
C	Degenne, R; Perchet, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Degenne, Remy; Perchet, Vianney			Combinatorial semi-bandit with known covariance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The combinatorial stochastic semi-bandit problem is an extension of the classical multi-armed bandit problem in which an algorithm pulls more than one arm at each stage and the rewards of all pulled arms are revealed. One difference with the single arm variant is that the dependency structure of the arms is crucial. Previous works on this setting either used a worst-case approach or imposed independence of the arms. We introduce a way to quantify the dependency structure of the problem and design an algorithm that adapts to it. The algorithm is based on linear regression and the analysis develops techniques from the linear bandit literature. By comparing its performance to a new lower bound, we prove that it is optimal, up to a poly-logarithmic factor in the number of pulled arms.	[Degenne, Remy] Univ Paris Diderot, LMPA, CMLA, ENS Paris Saclay, Paris, France; [Perchet, Vianney] ENS Paris Saclay, CMLA, CRITEO Res, Paris, France	UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay; UDICE-French Research Universities; Universite Paris Saclay	Degenne, R (corresponding author), Univ Paris Diderot, LMPA, CMLA, ENS Paris Saclay, Paris, France.	degenne@cmla.ens-cachan.fr; perchet@normalesup.org			ANR [ANR-13-JS01-0004]; Fondation Mathematiques Jacques Hadamard; EDF through the Program Gaspard Monge for Optimization; Irsdi project Tecolere	ANR(French National Research Agency (ANR)); Fondation Mathematiques Jacques Hadamard; EDF through the Program Gaspard Monge for Optimization; Irsdi project Tecolere	The authors would like to acknowledge funding from the ANR under grant number ANR-13-JS01-0004 as well as the Fondation Mathematiques Jacques Hadamard and EDF through the Program Gaspard Monge for Optimization and the Irsdi project Tecolere.	Abbasi-Yadkori Y., 2011, P 24 ANN C LEARNING, P1; Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Carpentier A., 2012, NIPS, P251; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen W., 2013, ICML 2013, P151; Combes Richard, 2015, NEURAL INFORM PROCES, P1; Filippi Sarah, 2010, NEURAL INFORM PROCES, P1; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Garivier A, 2013, 2013 IEEE INFORMATION THEORY WORKSHOP (ITW); Komiyama J., 2015, P 32 INT C MACH LEAR; Kveton Branislav, 2015, P 18 INT C ART INT S; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Pena V. H., 2008, SELF NORMALIZED PROC; Robbins Herbert, 1985, H ROBBINS SELECTED P, P169; Rusmevichientong Paat, 1985, MATH OPER RES, P1	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700101
C	Degraux, K; Peyre, G; Fadili, JM; Jacques, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Degraux, Kevin; Peyre, Gabriel; Fadili, Jalal M.; Jacques, Laurent			Sparse Support Recovery with Non-smooth Loss Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONSISTENCY; SELECTION; LASSO	In this paper, we study the support recovery guarantees of underdetermined sparse regression using the l(1)-norm as a regularizer and a non-smooth loss function for data fidelity. More precisely, we focus in detail on the cases of l(1) and l(infinity) losses, and contrast them with the usual l(2) loss. While these losses are routinely used to account for either sparse (l(1) loss) or uniform (l(infinity) loss) noise models, a theoretical analysis of their performance is still lacking. In this article, we extend the existing theory from the smooth l(2) case to these non-smooth cases. We derive a sharp condition which ensures that the support of the vector to recover is stable to small additive noise in the observations, as long as the loss constraint size is tuned proportionally to the noise level. A distinctive feature of our theory is that it also explains what happens when the support is unstable. While the support is not stable anymore, we identify an "extended support" and show that this extended support is stable to small additive noise. To exemplify the usefulness of our theory, we give a detailed numerical analysis of the support stability/instability of compressed sensing recovery with these different losses. This highlights different parameter regimes, ranging from total support stability to progressively increasing support instability.	[Degraux, Kevin; Jacques, Laurent] Catholic Univ Louvain, ISPGrp, ICTEAM, FNRS, B-1348 Louvain La Neuve, Belgium; [Peyre, Gabriel] Ecole Normale Super, CNRS, DMA, F-75775 Paris, France; [Fadili, Jalal M.] Normandie Univ, ENSICAEN, CNRS, GREYC, F-14050 Caen, France	Universite Catholique Louvain; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Centre National de la Recherche Scientifique (CNRS); Universite de Caen Normandie	Degraux, K (corresponding author), Catholic Univ Louvain, ISPGrp, ICTEAM, FNRS, B-1348 Louvain La Neuve, Belgium.	kevin.degraux@uclouvain.be; gabriel.peyre@ens.fr; Jalal.Fadili@ensicaen.fr; laurent.jacques@uclouvain.be			Belgian F.R.S.-FNRS; Institut Universitaire de France; European Research Council (ERC project SIGMA-Vision)	Belgian F.R.S.-FNRS(Fonds de la Recherche Scientifique - FNRS); Institut Universitaire de France; European Research Council (ERC project SIGMA-Vision)(European Research Council (ERC))	KD and LJ are funded by the Belgian F.R.S.-FNRS. JF is partly supported by Institut Universitaire de France. GP is supported by the European Research Council (ERC project SIGMA-Vision).	Bach FR, 2008, J MACH LEARN RES, V9, P1179; Bach FR, 2008, J MACH LEARN RES, V9, P1019; Candes Emmanuel J., 2006, CALIF I TECHNOL, V59, P1; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Duval V., 2015, 01135200 HAL; Fuchs JJ, 2004, IEEE T INFORM THEORY, V50, P1341, DOI 10.1109/TIT.2004.828141; Grant M., 2014, CVX MATLAB SOFTWARE; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Jacques L., 2013, TRLJ201301; Jacques L, 2011, IEEE T INFORM THEORY, V57, P559, DOI 10.1109/TIT.2010.2093310; Nikolova M., 2004, J MATH IMAGING VISIO, V20; Rockafellar R. T., 1974, CONJUGATE DUALITY OP, V16; Studer C, 2012, IEEE T INFORM THEORY, V58, P3115, DOI 10.1109/TIT.2011.2179701; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vaiter S., 2014, 00987293 HAL; Vaiter S, 2013, IEEE T INFORM THEORY, V59, P2001, DOI 10.1109/TIT.2012.2233859; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Zhao P, 2006, J MACH LEARN RES, V7, P2541	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704083
C	Djolonga, J; Jegelka, S; Tschiatschek, S; Krause, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Djolonga, Josip; Jegelka, Stefanie; Tschiatschek, Sebastian; Krause, Andreas			Cooperative Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHM	We study a rich family of distributions that capture variable interactions significantly more expressive than those representable with low-treewidth or pairwise graphical models, or log-supermodular models. We call these cooperative graphical models. Yet, this family retains structure, which we carefully exploit for efficient inference techniques. Our algorithms combine the polyhedral structure of submodular functions in new ways with variational inference methods to obtain both lower and upper bounds on the partition function. While our fully convex upper bound is minimized as an SDP or via tree-reweighted belief propagation, our lower bound is tightened via belief propagation or mean-field algorithms. The resulting algorithms are easy to implement and, as our experiments show, effectively obtain good bounds and marginals for synthetic and real-world examples.	[Djolonga, Josip; Tschiatschek, Sebastian; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Jegelka, Stefanie] MIT, CSAIL, Cambridge, MA 02139 USA	ETH Zurich; Massachusetts Institute of Technology (MIT)	Djolonga, J (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	josipd@inf.ethz.ch; stefje@mit.edu; stschia@inf.ethz.ch; krausea@inf.ethz.ch			SNSF [CRSII2_147633]; ERC StG [307036]; Microsoft Research; Google; NSF CAREER [1553284]	SNSF(Swiss National Science Foundation (SNSF)); ERC StG; Microsoft Research(Microsoft); Google(Google Incorporated); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This research was supported in part by SNSF grant CRSII2_147633, ERC StG 307036, a Microsoft Research Faculty Fellowship, a Google European Doctoral Fellowship, and NSF CAREER 1553284.	Bach F., 2013, FOUND TRENDS MACH LE, V6, P2; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boykov Y., 2004, PATTERN ANAL MACHINE, V26; Diamond S., 2016, JMLR; Djolonga Josip, 2014, NIPS; Edmonds Jack, 1970, COMBINATORIAL STRUCT, P69; Frank M., 1956, NAVAL RES LOGIST Q; Fujishige S, 2011, PAC J OPTIM, V7, P3; Goldberg LA, 2007, COMB PROBAB COMPUT, V16, P43, DOI 10.1017/S096354830600767X; Hazan T., 2012, ICML; Heskes T., 2006, JAIR, V26; Iyer R., 2013, ICML; Iyer R., 2015, ARXIV150607329; Jaggi Martin., 2013, ICML; Jegelka S., 2013, NIPS; Jegelka S., 2011, ICML; Jegelka S., 2011, CVPR; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Kakade S., 2009, TECHNICAL REPORT; Kohli P., 2013, CVPR; London B., 2015, ICML; Meshi O., 2009, UAI; Mooij JM, 2010, J MACH LEARN RES, V11, P2169; O'Donoghue B., 2016, J OPTIMIZATION THEOR; Papandreou G., 2011, ICCV; Ruozzi N., 2012, NIPS; Silberman Nathan, 2014, ECCV; Stobbe P., 2010, NIPS; Tarlow D., 2012, UAI; Vilnis L., 2015, UAI; Vineet V., 2014, IJCV, V110; Wainwright M. J., 2006, JMLR, V7; Wainwright M. J., 2002, UAI; Wainwright M. J., 2006, SIGNAL PROCESSING IE, V54; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weller A., 2014, UAI; Zhang J., 2015, ICCV	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700086
C	Djolonga, J; Tschiatschek, S; Krause, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Djolonga, Josip; Tschiatschek, Sebastian; Krause, Andreas			Variational Inference in Mixed Probabilistic Submodular Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of variational inference in probabilistic models with both log-submodular and log-supermodular higher-order potentials. These models can represent arbitrary distributions over binary variables, and thus generalize the commonly used pairwise Markov random fields and models with log-supermodular potentials only, for which efficient approximate inference algorithms are known. While inference in the considered models is #P-hard in general, we present efficient approximate algorithms exploiting recent advances in the field of discrete optimization. We demonstrate the effectiveness of our approach in a large set of experiments, where our model allows reasoning about preferences over sets of items with complements and substitutes.	[Djolonga, Josip; Tschiatschek, Sebastian; Krause, Andreas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	Djolonga, J (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	josipd@inf.ethz.ch; tschiats@inf.ethz.ch; krausea@inf.ethz.ch			SNSF [CRSII2-147633]; ERC StG [307036]; Microsoft Research; Google	SNSF(Swiss National Science Foundation (SNSF)); ERC StG; Microsoft Research(Microsoft); Google(Google Incorporated)	The authors acknowledge fruitful discussions with Diego Ballesteros. This research was supported in part by SNSF grant CRSII2-147633, ERC StG 307036, a Microsoft Research Faculty Fellowship and a Google European Doctoral Fellowship.	Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; DJOLONGA J., 2015, INT C MACH LEARN ICM; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Edmonds J, 2003, LECT NOTES COMPUT SC, V2570, P11; Edmonds J., 1971, MATH PROGRAM, V1, P127, DOI [10.1007/BF01584082, DOI 10.1007/BF01584082, 10.1007/bf01584082]; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gillenwater J., 2014, ADV NEURAL INFORM PR; Gotovos A., 2015, NEURAL INFORM PROCES; Gutmann Michael U, 2012, J MACHINE LEARNING R, V13; Iyer R., 2015, ARXIV150607329; Iyer R., 2012, UAI; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Jian Zhang, 2015, INT C COMP VIS ICCV; Kawahara Yoshinobu, 2015, P 18 INT C ART INT S; Murota K., 2003, DISCRETE CONVEX ANAL; Narasimhan M., 2012, ARXIV12071404; Narasimhan Mukund, 2005, UNCERTAINTY ARTIFICI; Rebeschini Patrick, 2015, P MACHINE LEARNING R, V40, P1480; Rother C., 2007, COMP VIS PATT REC 20; Shioura Akiyoshi, 2014, MATH OPERATIONS RES, V40; Tschiatschek S., 2016, P INT C ART INT STAT; Woodford Oliver, 2009, PATTERN ANAL MACHINE, V31	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701082
C	Dohmatob, E; Mensch, A; Varoquaux, G; Thirion, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dohmatob, Elvis; Mensch, Arthur; Varoquaux, Gael; Thirion, Bertrand			Learning brain regions via large-scale online structured sparse dictionary-learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a multivariate online dictionary-learning method for obtaining decompositions of brain images with structured and sparse components (aka atoms). Sparsity is to be understood in the usual sense: the dictionary atoms are constrained to contain mostly zeros. This is imposed via an l(1) -norm constraint. By "structured", we mean that the atoms are piece-wise smooth and compact, thus making up blobs, as opposed to scattered patterns of activation. We propose to use a Sobolev (Laplacian) penalty to impose this type of structure. Combining the two penalties, we obtain decompositions that properly delineate brain structures from functional images. This non-trivially extends the online dictionary-learning work of Mairal et al. (2010), at the price of only a factor of 2 or 3 on the overall running time. Just like the Mairal et al. (2010) reference method, the online nature of our proposed algorithm allows it to scale to arbitrarily sized datasets. Preliminary xperiments on brain data show that our proposed method extracts structured and denoised dictionaries that are more intepretable and better capture inter-subject variability in small medium, and large-scale regimes alike, compared to state-of-the-art models.	[Dohmatob, Elvis; Mensch, Arthur; Varoquaux, Gael; Thirion, Bertrand] Univ Paris Saclay, Parietal Team, INRIA CEA, Neurospin, St Aubin, France	UDICE-French Research Universities; Universite Paris Saclay	Dohmatob, E (corresponding author), Univ Paris Saclay, Parietal Team, INRIA CEA, Neurospin, St Aubin, France.	elvis.dohmatob@inria.fr; arthur.mensch@inria.fr; gael.varoquaux@inria.fr; bertrand.thirion@inria.fr			EU [604102]; iConnectome Digiteo	EU(European Commission); iConnectome Digiteo	This work has been funded by EU FP7/2007-2013 under grant agreement no. 604102, Human Brain Project (HBP) and the iConnectome Digiteo. We would also like to thank the Human Connectome Projection for making their wonderful data publicly available.	Abraham A., 2013, MICCAI; Abraham A, 2014, FRONT NEUROINFORM, V8, DOI 10.3389/fninf.2014.00014; Baldassarre L., 2012, PRNI; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Beckmann C. F., 2004, T MED IM, V23; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Condat L., 2014, MATH PROGRAM; Dohmatob E., 2014, PRNI; Duchi J., 2008, ICML; FRISTON KJ, 1995, HUM BRAIN MAPP; Grosenick L, 2013, NEUROIMAGE, V72, P304, DOI 10.1016/j.neuroimage.2012.12.062; Hebiri M, 2011, ELECTRON J STAT, V5, P1184, DOI 10.1214/11-EJS638; Hibar D. P., 2013, MICCAI; Jenatton R., 2010, AISTATS; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mensch A., 2016, ICML; Saxe R, 2006, NEUROIMAGE, V30, P1088, DOI 10.1016/j.neuroimage.2005.12.062; Smith SM, 2004, NEUROIMAGE, V23, pS208, DOI 10.1016/j.neuroimage.2004.07.051; van Essen D., 2012, NEUROIMAGE, V62; Varol E., 2014, MED IMAGE COMPUT COM, V17; Varoquaux G, 2010, NEUROIMAGE, V51, P288, DOI 10.1016/j.neuroimage.2010.02.010; Varoquaux G., 2015, ARXIV151206999; Varoquaux G., 2013, IPMI; Varoquaux G., 2011, INF P MED IM; Ying Y., 2006, IEEE T INF THEORY, V52	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702099
C	Dolhansky, B; Bilmes, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dolhansky, Brian; Bilmes, Jeff			Deep Submodular Functions: Definitions & Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose and study a new class of submodular functions called deep submodular functions (DSFs). We define DSFs and situate them within the broader context of classes of submodular functions in relationship both to various matroid ranks and sums of concave composed with modular functions (SCMs). Notably, we find that DSFs constitute a strictly broader class than SCMs, thus motivating their use, but that they do not comprise all submodular functions. Interestingly, some DSFs can be seen as special cases of certain deep neural networks (DNNs), hence the name. Finally, we provide a method to learn DSFs in a max-margin framework, and offer preliminary results applying this both to synthetic and real-world data instances.	[Dolhansky, Brian] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98105 USA; [Bilmes, Jeff] Univ Washington, Dept Elect Engn, Seattle, WA 98105 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Dolhansky, B (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98105 USA.	bdol@cs.washington.edu; bilmes@uw.edu			National Science Foundation [IIS-1162606]; National Institutes of Health [R01GM103544]; Google; Microsoft; Facebook; Intel research award; TerraSwarm, one of six centers of STARnet; Semiconductor Research Corporation program - MARCO; DARPA	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Google(Google Incorporated); Microsoft(Microsoft); Facebook(Facebook Inc); Intel research award; TerraSwarm, one of six centers of STARnet; Semiconductor Research Corporation program - MARCO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	Thanks to Reza Eghbali and Kai Wei for useful discussions. This material is based upon work supported by the National Science Foundation under Grant No. IIS-1162606, the National Institutes of Health under award R01GM103544, and by a Google, a Microsoft, a Facebook, and an Intel research award. This work was supported in part by TerraSwarm, one of six centers of STARnet, a Semiconductor Research Corporation program sponsored by MARCO and DARPA.	Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Balcan Maria-Florina, 2010, TECHNICAL REPORT; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bilmes, 2012, P 28 C UNC ART INT, P407; Bilmes J., 2017, ARXIV170108939; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Buchbinder N., 2014, P 25 ANN ACM SIAM S, P1433; Calinescu G, 2011, SIAM J COMPUT, V40, P1740, DOI 10.1137/080733991; Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26; CUNNINGHAM WH, 1984, J COMB THEORY B, V36, P161, DOI 10.1016/0095-8956(84)90023-6; Feldman V., 2013, CORR; Goemans MX, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P535; Iyer R., 2013, INT C MACH LEARN ICM; Iyer Rishabh K., 2013, ADV NEURAL INFORM PR, P2742; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee J, 2009, ACM S THEORY COMPUT, P323; Lin H., 2011, CLASS SUBMODULAR FUN; Lin H., 2012, UNCERTAINTY ARTIFICI; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Nishihara R., 2014, ADV NEURAL INFO PROC, P640; Pei WZ, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P293; Sipos Ruben, 2012, P 13 C EUR CHAPT ASS, P224; Stobbe P., 2010, ADV NEURAL INFORM PR, P2208; Taskar Ben, 2005, P 22 INT C MACH LEAR, P896; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; Vondrak J., 2011, COMMUNICATION; Wei K., 2014, P IEEE INT C AC SPEE	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702108
C	Dosovitskiy, A; Brox, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dosovitskiy, Alexey; Brox, Thomas			Generating Images with Perceptual Similarity Metrics based on Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a class of loss functions, which we call deep perceptual similarity metrics (DeePSiM), allowing to generate sharp high resolution images from compressed abstract representations. Instead of computing distances in the image space, we compute distances between image features extracted by deep neural networks. This metric reflects perceptual similarity of images much better and, thus, leads to better results. We demonstrate two examples of use cases of the proposed loss: (1) networks that invert the AlexNet convolutional network; (2) a modified version of a variational autoencoder that generates realistic high-resolution random images.	[Dosovitskiy, Alexey; Brox, Thomas] Univ Freiburg, Freiburg, Germany	University of Freiburg	Dosovitskiy, A (corresponding author), Univ Freiburg, Freiburg, Germany.	dosovits@cs.uni-freiburg.de; brox@cs.uni-freiburg.de			ERC Starting Grant VideoLearn [279401]	ERC Starting Grant VideoLearn	We acknowledge funding by the ERC Starting Grant VideoLearn (279401).	[Anonymous], 2014, ICLR; DALY S, 1993, DIGITAL IMAGES HUMAN, P179; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Dosovitskiy A., 2015, CVPR; Dosovitskiy A., 2016, CVPR; GATYS L. A., 2016, CVPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G. E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.1234/12345678; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lamb A., 2016, ARXIV160203220; Larsen ABL, 2016, PR MACH LEARN RES, V48; Lee H., 2009, P ANN INT C MACH LEA, P609; Mahendran A., 2015, CVPR; Mathieu Michael, 2016, P INT C LEARN REPR I; Mirza M., 2014, ARXIV PREPRINT ARXIV; Radford A., 2016, ICLR; Ridgeway K., 2015, ARXIV151106409; van den Branden Lambrecht C. J., 1996, ELECT IMAGING SCI TE; Wang X., 2015, ICCV; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Winkler S., 1998, P IEEE INT C IM PROC, P175	26	0	0	0	15	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701015
C	Dubey, A; Reddi, SJ; Poczos, B; Smola, AJ; Xing, EP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dubey, Avinava; Reddi, Sashank J.; Poczos, Barnabas; Smola, Alexander J.; Xing, Eric P.			Variance Reduction in Stochastic Gradient Langevin Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Stochastic gradient-based Monte Carlo methods such as stochastic gradient Langevin dynamics are useful tools for posterior inference on large scale datasets in many machine learning applications. These methods scale to large datasets by using noisy gradients calculated using a mini-batch or subset of the dataset. However, the high variance inherent in these noisy gradients degrades performance and leads to slower mixing. In this paper, we present techniques for reducing variance in stochastic gradient Langevin dynamics, yielding novel stochastic Monte Carlo methods that improve performance by reducing the variance in the stochastic gradient. We show that our proposed method has better theoretical guarantees on convergence rate than stochastic Langevin dynamics. This is complemented by impressive empirical results obtained on a variety of real world datasets, and on four different machine learning tasks (regression, classification, independent component analysis and mixture modeling). These theoretical and empirical contributions combine to make a compelling case for using variance reduction in stochastic Monte Carlo methods.	[Dubey, Avinava; Reddi, Sashank J.; Poczos, Barnabas; Smola, Alexander J.; Xing, Eric P.] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Dubey, A (corresponding author), Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA.	akdubey@cs.cmu.edu; sjakkamr@cs.cmu.edu; bapoczos@cs.cmu.edu; alex@cs.cmu.edu; epxing@cs.cmu.edu						Ahn S., 2012, ICML; Ahn S., 2014, ICML; Chen C., 2015, NIPS; Chen T., 2014, ICML; DEFAZIO A, 2014, NIPS; Ding N., 2014, NIPS; Girolami M., 2011, J ROYAL STAT SOC B; JOHNSON R., 2013, NIPS; Ma Y. A., 2015, NIPS; Neal R. M., 2010, HDB MARKOV CHAIN MON; Nesterov Y., 2003, INTRO LECT CONVEX OP; Patterson S., 2013, NIPS; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schmidt Mark, 2013, ARXIV13092388; Welling M., 2011, ICML	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702040
C	Dutta, S; Cadambe, V; Grover, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Dutta, Sanghamitra; Cadambe, Viveck; Grover, Pulkit			"Short-Dot": Computing Large Linear Transforms Distributedly Using Coded Short Dot Products	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Faced with saturation of Moore's law and increasing size and dimension of data, system designers have increasingly resorted to parallel and distributed computing to reduce computation time of machine-learning algorithms. However, distributed computing is often bottle necked by a small fraction of slow processors called "stragglers" that reduce the speed of computation because the fusion node has to wait for all processors to complete their processing. To combat the effect of stragglers, recent literature proposes introducing redundancy in computations across processors, e.g., using repetition-based strategies or erasure codes. The fusion node can exploit this redundancy by completing the computation using outputs from only a subset of the processors, ignoring the stragglers. In this paper, we propose a novel technique - that we call "Short-Dot" - to introduce redundant computations in a coding theory inspired fashion, for computing linear transforms of long vectors. Instead of computing long dot products as required in the original linear transform, we construct a larger number of redundant and short dot products that can be computed more efficiently at individual processors. Further, only a subset of these short dot products are required at the fusion node to finish the computation successfully. We demonstrate through probabilistic analysis as well as experiments on computing clusters that Short-Dot offers significant speed-up compared to existing techniques. We also derive trade-offs between the length of the dot-products and the resilience to stragglers (number of processors required to finish), for any such strategy and compare it to that achieved by our strategy.	[Dutta, Sanghamitra; Grover, Pulkit] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Cadambe, Viveck] Penn State Univ, University Pk, PA 16802 USA	Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Dutta, S (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	sanghamd@andrew.cmu.edu; viveck@engr.psu.edu; pgrover@andrew.cmu.edu			MARCO; DARPA; NSF [1350314, 1464336, 1553248]; Prabhu and Poonam Goel Graduate Fellowship	MARCO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Prabhu and Poonam Goel Graduate Fellowship	Systems on Nanoscale Information fabriCs (SONIC), one of the six SRC STARnet Centers, sponsored by MARCO and DARPA. We also acknowledge NSF Awards 1350314, 1464336 and 1553248. S Dutta also received Prabhu and Poonam Goel Graduate Fellowship.	Ballard G, 2014, COMMUN ACM, V57, P107, DOI 10.1145/2556647.2556660; Da Wang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P7, DOI 10.1145/2847220.2847223; Dally W., 2015, NIPS TUTORIAL, V2; Dean J, 2013, COMMUN ACM, V56, P74, DOI 10.1145/2408776.2408794; FOX GC, 1987, PARALLEL COMPUT, V4, P17, DOI 10.1016/0167-8191(87)90060-3; Herault T, 2015, COMPUT COMMUN NETW S, P1, DOI 10.1007/978-3-319-20943-2; HUANG KH, 1984, IEEE T COMPUT, V33, P518, DOI 10.1109/TC.1984.1676475; Joshi G, 2014, IEEE J SEL AREA COMM, V32, P989, DOI 10.1109/JSAC.2014.140518; Kumar V., 1994, INTRO PARALLEL COMPU, P110; Lee Kangwook, 2015, NIPS WORKSH LEARN SY; Li Songze, 2016, ARXIV160901690V1CSIT; Longbo Huang, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2766, DOI 10.1109/ISIT.2012.6284026; Nahlus I, 2014, I SYMPOS LOW POWER E, P315, DOI 10.1145/2627369.2627664; Ryan W., 2009, CHANNEL CODES CLASSI; Shvachko K, 2010, IEEE S MASS STOR SYS; STRASSEN V, 1969, NUMER MATH, V13, P354, DOI 10.1007/BF02165411; Wang NC, 2016, S VLSI TECH	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702076
C	Eghbali, R; Fazel, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Eghbali, Reza; Fazel, Maryam			Designing smoothing functions for improved worst-case competitive ratio in online optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PRIMAL-DUAL ALGORITHMS	Online optimization covers problems such as online resource allocation, online bipartite matching, adwords (a central problem in e-commerce and advertising), and adwords with separable concave returns. We analyze the worst case competitive ratio of two primal-dual algorithms for a class of online convex (conic) optimization problems that contains the previous examples as special cases defined on the positive orthant. We derive a sufficient condition on the objective function that guarantees a constant worst case competitive ratio (greater than or equal to 1/2) for monotone objective functions. We provide new examples of online problems on the positive orthant that satisfy the sufficient condition. We show how smoothing can improve the competitive ratio of these algorithms, and in particular for separable functions, we show that the optimal smoothing can be derived by solving a convex optimization problem. This result allows us to directly optimize the competitive ratio bound over a class of smoothing functions, and hence design effective smoothing customized for a given cost function.	[Eghbali, Reza; Fazel, Maryam] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Eghbali, R (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.	eghbali@uw.edu; mfazel@uw.edu						Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Agrawal S, 2014, P 26 ANN ACM SIAM S, P1405; Azar Yossi, 2014, ARXIV14123507; Buchbinder Niv, 2014, ARXIV14128347; Chan TH, 2015, ARXIV150201802; Devanur N.R., 2011, P 12 ACM C EL COMM, P29; Devanur NR, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P137; Eghbali R., 2016, 55 IEEE C DEC CONTR; Eghbali Reza, 2014, ARXIV14107171; Gupta Anupam, 2014, ARXIV14075298; KARP RM, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P352, DOI 10.1145/100216.100262; Kleinberg R, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P630; Mehta A, 2007, J ACM, V54, DOI 10.1145/1284320.1284321; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Shalev-Shwartz S, 2007, ONLINE LEARNING THEO	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700037
C	Eldridge, J; Belkin, M; Wang, YS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Eldridge, Justin; Belkin, Mikhail; Wang, Yusu			Graphons, mergeons, and so on!	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this work we develop a theory of hierarchical clustering for graphs. Our modeling assumption is that graphs are sampled from a graphon, which is a powerful and general model for generating graphs and analyzing large networks. Graphons are a far richer class of graph models than stochastic blockmodels, the primary setting for recent progress in the statistical theory of graph clustering. We define what it means for an algorithm to produce the "correct" clustering, give sufficient conditions in which a method is statistically consistent, and provide an explicit algorithm satisfying these properties.	[Eldridge, Justin; Belkin, Mikhail; Wang, Yusu] Ohio State Univ, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Eldridge, J (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.	eldridge@cse.ohio-state.edu; mbelkin@cse.ohio-state.edu; yusu@cse.ohio-state.edu			NSF [IIS-1550757]	NSF(National Science Foundation (NSF))	This work was supported by NSF grant IIS-1550757.	Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Airoldi EM., 2013, ADV NEURAL INFORM PR, V26, P692; [Anonymous], 2010, ADV NEURAL INFORM PR; Ash RB., 2000, PROBABILITY MEASURE; Balakrishnan S., 2011, ADV NEURAL INFORM PR, P954; Borgs C., 2016, ARXIV160107134; Caron F., 2014, ARXIV14011137; Chan SH, 2014, PR MACH LEARN RES, V32; Eldridge J., 2015, P 28 C LEARN THEOR, P588; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; HARTIGAN JA, 1981, J AM STAT ASSOC, V76, P388, DOI 10.2307/2287840; Janson Svante, 2008, ARXIV08023795; JWolfe Patrick, 2013, ARXIV13095936; KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225; Lovasz L, 2012, LARGE NETWORKS GRAPH, V60; Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; STEINWART I., 2011, P 24 ANN C LEARN THE, V19, P703; Zhang Y., 2015, ARXIV150908588	21	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700053
C	Ellis, K; Solar-Lezama, A; Tenenbaum, JB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ellis, Kevin; Solar-Lezama, Armando; Tenenbaum, Joshua B.			Sampling for Bayesian Program Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Towards learning programs from data, we introduce the problem of sampling programs from posterior distributions conditioned on that data. Within this setting, we propose an algorithm that uses a symbolic solver to efficiently sample programs. The proposal combines constraint-based program synthesis with sampling via random parity constraints. We give theoretical guarantees on how well the samples approximate the true posterior, and have empirical results showing the algorithm is efficient in practice, evaluating our approach on 22 program learning problems in the domains of text editing and computer-aided programming.	[Ellis, Kevin; Tenenbaum, Joshua B.] MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA; [Solar-Lezama, Armando] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Ellis, K (corresponding author), MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA.	ellisk@mit.edu; asolar@csail.mit.edu; jbt@mit.edu			AFOSR [FA9550-16-1-0012];  [NSF-1161775]	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); 	We are grateful for feedback from Adam Smith, Kuldeep Meel, and our anonymous reviewers. Work supported by NSF-1161775 and AFOSR award FA9550-16-1-0012.	Achlioptas Dimitris, 2015, STOCHASTIC INTEGRATI; [Anonymous], 2015, ADV NEURAL INFORM PR; Chakraborty Supratik, 2013, Computer Aided Verification. 25th International Conference, CAV 2013. Proceedings. LNCS 8044, P608, DOI 10.1007/978-3-642-39799-8_40; Chakraborty Supratik, 2014, AAAI C ART INT; Ermon S, 2014, PR MACH LEARN RES, V32; Ermon Stefano, 2013, ADV NEURAL INFORM PR, P2085; Gomes C. P., 2006, NIPS, P481; Gomes Carla P, 2006, AAAI C ART INT; Graves A, 2014, NEURAL TURING MACHIN; Gulwani S, 2011, ACM SIGPLAN NOTICES, V46, P317, DOI 10.1145/1925844.1926423; JHA S, 2010, P 2010 ACMIEEE 32 IN, P215, DOI DOI 10.1145/1806799.1806833; Katz Yarden, 2008, COGSCI, P71; Koza J. R., 1993, GENETIC PROGRAMMING; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lezama A. S., 2008, THESIS; Liang P., 2010, P 27 INT C MACHINE L, P639; Liang Percy, 2011, P 49 ANN M ASS COMPU, P590; Lin DH, 2014, FRONT ARTIF INTEL AP, V263, P525, DOI 10.3233/978-1-61499-419-0-525; Menon A., 2013, P 30 INT C INT C MAC, P187; RAYCHEV V, 2016, POPL, V51, P761, DOI DOI 10.1145/2837614.2837671; Reed Scott, 2015, ABS151106279 CORR; Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150; Singh R, 2013, ACM SIGPLAN NOTICES, V48, P15, DOI 10.1145/2499370.2462195; Valiant L.G., 1985, P 17 ANN ACM S THEOR, P458	24	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700046
C	Erdogdu, MA; Bayati, M; Dicker, LH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Erdogdu, Murat A.; Bayati, Mohsen; Dicker, Lee H.			Scaled Least Squares Estimator for GLMs in Large-Scale Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of efficiently estimating the coefficients of generalized linear models (GLMs) in the large-scale setting where the number of observations n is much larger than the number of predictors p, i.e. n >> p >> 1. We show that in GLMs with random (not necessarily Gaussian) design, the GLM coefficients are approximately proportional to the corresponding ordinary least squares (OLS) coefficients. Using this relation, we design an algorithm that achieves the same accuracy as the maximum likelihood estimator (MLE) through iterations that attain up to a cubic convergence rate, and that are cheaper than any batch optimization algorithm by at least a factor of 0(p). We provide theoretical guarantees for our algorithm, and analyze the convergence behavior in terms of data dimensions. Finally, we demonstrate the performance of our algorithm through extensive numerical studies on large-scale real and synthetic datasets, and show that it achieves the highest performance compared to several other widely used optimization algorithms.	[Erdogdu, Murat A.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Bayati, Mohsen] Stanford Univ, Grad Sch Business, Stanford, CA 94305 USA; [Dicker, Lee H.] Rutgers State Univ, Dept Stat & Biostat, New Brunswick, NJ USA; [Dicker, Lee H.] Amazon, Seattle, WA USA	Stanford University; Stanford University; Rutgers State University New Brunswick; Amazon.com	Erdogdu, MA (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	erdogdu@stanford.edu; bayati@stanford.edu; ldicker@stat.rutgers.edu		Bayati, Mohsen/0000-0002-7280-912X				Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Bayati M., 2013, ADV NEURAL INFORM PR, V26, P944; Bishop, 1995, NEURAL NETWORKS PATT; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Boyd S, 2004, CONVEX OPTIMIZATION; Dhillon P S, 2013, ADV NEURAL INFORM PR, P360; Erdogdu M. A., 2016, J MACHINE LEARNING R; Erdogdu Murat A, 2015, ADV NEURAL INFORM PR, P1216; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Goldstein L, 2007, ANN PROBAB, V35, P1888, DOI 10.1214/009117906000001123; Goldstein L, 1997, ANN APPL PROBAB, V7, P935, DOI 10.1214/aoap/1043862419; HESTENES MR, 1952, J RES NAT BUR STAND, V49, P409, DOI 10.6028/jres.049.044; Koller D., 2009, PROBABILISTIC GRAPHI; LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254; Martens J., 2010, P 27 INT C MACH LEAR, P735; Mccullagh P, 2004, INTRO LECT CONVEX OP; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; PAIGE CC, 1975, SIAM J NUMER ANAL, V12, P617, DOI 10.1137/0712047; Plan Y., 2015, ARXIV150204071; Rokhlin V, 2008, P NATL ACAD SCI USA, V105, P13212, DOI 10.1073/pnas.0804869105; Thrampoulidis C., 2015, ADV NEURAL INFORM PR, P3402; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704047
C	Esfandiari, H; Korula, N; Mirrokni, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Esfandiari, Hossein; Korula, Nitish; Mirrokni, Vahab			Bi-Objective Online Matching and Submodular Allocations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATIONS	Online allocation problems have been widely studied due to their numerous practical applications (particularly to Internet advertising), as well as considerable theoretical interest. The main challenge in such problems is making assignment decisions in the face of uncertainty about future input; effective algorithms need to predict which constraints are most likely to bind, and learn the balance between short-term gain and the value of long-term resource availability. In many important applications, the algorithm designer is faced with multiple objectives to optimize. In particular, in online advertising it is fairly common to optimize multiple metrics, such as clicks, conversions, and impressions, as well as other metrics which may be largely uncorrelated such as 'share of voice', and 'buyer surplus'. While there has been considerable work on multi-objective offline optimization (when the entire input is known in advance), very little is known about the online case, particularly in the case of adversarial input. In this paper, we give the first results for bi-objective online submodular optimization, providing almost matching upper and lower bounds for allocating items to agents with two submodular value functions. We also study practically relevant special cases of this problem related to Internet advertising, and obtain improved results. All our algorithms are nearly best possible, as well as being efficient and easy to implement in practice.	[Esfandiari, Hossein] Univ Maryland, College Pk, MD 20740 USA; [Korula, Nitish; Mirrokni, Vahab] Google Res, New York, NY 10011 USA	University System of Maryland; University of Maryland College Park; Google Incorporated	Esfandiari, H (corresponding author), Univ Maryland, College Pk, MD 20740 USA.	hossein@cs.umd.edu; nitish@google.com; mirrokni@google.com						Aggarwal G, 2014, LECT NOTES COMPUT SC, V8877, P218, DOI 10.1007/978-3-319-13129-0_16; Agrawal Shipra, 2009, COMPUTING RES REPOSI; BUCHBINDER N, 2007, ESA, V4698, P253; Ciocan DF, 2012, MATH OPER RES, V37, P501, DOI 10.1287/moor.1120.0548; Devanur N.R., 2011, P 12 ACM C EL COMM, P29; Devanur Nikhil R., 2013, EC 2013 ACM, P305; Esfandiari Hossein, 2015, EC; FELDMAN J, 2010, ESA, V6346, P182; Feldman J., 2009, WINE; FISHER ML, 1978, MATH PROGRAM STUD, V8, P73, DOI 10.1007/BFb0121195; Korula N, 2015, ACM S THEORY COMPUT, P889, DOI 10.1145/2746539.2746626; Lehmann B, 2006, GAME ECON BEHAV, V55, P270, DOI 10.1016/j.geb.2005.02.006; Mahdian M, 2011, ACM S THEORY COMPUT, P597; Mehta A, 2007, J ACM, V54, DOI 10.1145/1284320.1284321; Mirrokni V, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P70; Mirrokni Vahab S, 2012, P TWENTYTHIRD ANN AC, P1690; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Papadimitriou CH, 2000, ANN IEEE SYMP FOUND, P86; Tan B, 2011, IEEE DECIS CONTR P, P4504, DOI 10.1109/CDC.2011.6161009; Vee E., 2010, THE EC, P109; Vondrak J, 2008, ACM S THEORY COMPUT, P67; Wei K., 2015, ADV NEURAL INFORM PR, P2233; Yannakakis Mihalis, 2001, WADS, P1	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700049
C	Eslami, SMA; Heess, N; Weber, T; Tassa, Y; Szepesvari, D; Kavukcuoglu, K; Hinton, GE		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Eslami, S. M. Ali; Heess, Nicolas; Weber, Theophane; Tassa, Yuval; Szepesvari, David; Kavukcuoglu, Koray; Hinton, Geoffrey E.			Attend, Infer, Repeat: Fast Scene Understanding with Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a framework for efficient inference in structured image models that explicitly reason about objects. We achieve this by performing probabilistic inference using a recurrent neural network that attends to scene elements and processes them one at a time. Crucially, the model itself learns to choose the appropriate number of inference steps. We use this scheme to learn to perform inference in partially specified 2D models (variable-sized variational auto-encoders) and fully specified 3D models (probabilistic renderers). We show that such models learn to identify multiple objects - counting, locating and classifying the elements of a scene without any supervision, e.g., decomposing 3D images with various numbers of objects in a single forward pass of a neural network at unprecedented speed. We further show that the networks produce accurate inferences when compared to supervised counterparts, and that their structure leads to improved generalization.	[Eslami, S. M. Ali; Heess, Nicolas; Weber, Theophane; Tassa, Yuval; Szepesvari, David; Kavukcuoglu, Koray; Hinton, Geoffrey E.] Google DeepMind, London, England	Google Incorporated	Eslami, SMA (corresponding author), Google DeepMind, London, England.	aeslami@google.com; heess@google.com; theophane@google.com; tassa@google.com; dsz@google.com; korayk@google.com; geoffhinton@google.com						Ba J., 2015, ICLR; Graves Alex, 2016, ABS160308983; Gregor K., 2015, ICML; GRENANDER U, 1976, PATTERN SYNTHESIS LE; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Jaderberg Max, 2015, SPATIAL TRANSFORMER; Jampani Varun, 2015, CVIU; Kendall Alex, 2015, ICCV; Kingma DP, 2 INT C LEARN REPR I, P1; Krizhevsky Alex, 2012, NIPS 25; Kulkarni T. D., 2015, CVPR; Kulkarni Tejas D, 2015, NIPS 28; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lempitsky Victor, 2010, NIPS 23; Loper Matthew M., 2014, ECCV, V8695; Mansinghka Vikash, 2013, NIPS 26; MILCH B, 2005, IJCAI, P1352; Mnih A., 2014, ICML; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mnih Volodymyr, 2014, NIPS 27; Rezende D.J., 2014, ICML; Salakhutdinov R., 2009, AISTATS; Schulman John, 2015, NIPS 28; Tang Y., 2013, ICML; Tang Yichuan, 2014, NIPS 27; Todorov Emanuel, 2012, ICIRS; Zhang J., 2015, CVPR; Zhu S. C., 2006, FDN TRENDS COMPUTER, V2	28	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701087
C	Fathony, R; Liu, AQ; Asif, K; Ziebart, BD		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Fathony, Rizal; Liu, Anqi; Asif, Kaiser; Ziebart, Brian D.			Adversarial Multiclass Classification: A Risk Minimization Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SUPPORT VECTOR MACHINES; CONSISTENCY	Recently proposed adversarial classification methods have shown promising results for cost sensitive and multivariate losses. In contrast with empirical risk minimization (ERM) methods, which use convex surrogate losses to approximate the desired non-convex target loss function, adversarial methods minimize non-convex losses by treating the properties of the training data as being uncertain and worst case within a minimax game. Despite this difference in formulation, we recast adversarial classification under zero-one loss as an ERM method with a novel prescribed loss function. We demonstrate a number of theoretical and practical advantages over the very closely related hinge loss ERM methods. This establishes adversarial classification under the zero-one loss as a method that fills the long standing gap in multiclass hinge loss classification, simultaneously guaranteeing Fisher consistency and universal consistency, while also providing dual parameter sparsity and high accuracy predictions in practice.	[Fathony, Rizal; Liu, Anqi; Asif, Kaiser; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Fathony, R (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	rfatho2@uic.edu; aliu33@uic.edu; kasif2@uic.edu; bziebart@uic.edu			Future of Life Institute FLI-RFP-AI1 program [2016-158710]; NSF [1526379]	Future of Life Institute FLI-RFP-AI1 program; NSF(National Science Foundation (NSF))	This research was supported as part of the Future of Life Institute (futureoflife.org) FLI-RFP-AI1 program, grant#2016-158710 and by NSF grant RI-#1526379.	[Anonymous], 2008, INFORM SCI STAT; [Anonymous], 2014, ADV NEURAL INFORM PR; Asif Kaiser, 2015, P C UNC ART INT; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bartlett PL, 2004, ADV NEUR IN, V16, P1173; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Deng N.Y., 2013, SUPPORT VECTOR MACHI; Dogan U, 2016, J MACH LEARN RES, V17; Farnia F., 2016, ADV NEURAL INFORM PR, P4233; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; HongWang Wei Xing, 2015, ADV NEURAL INFORM PR, P2710; Igel C, 2008, J MACH LEARN RES, V9, P993; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726; Lee YK, 2004, J AM STAT ASSOC, V99, P67, DOI 10.1198/016214504000000098; Lin Y, 2002, DATA MIN KNOWL DISC, V6, P259, DOI 10.1023/A:1015469627679; Liu, 2007, INT C ART INT STAT, V2, P291; MCCuLLAGH P., 1989, GEN LINEAR MODELS, V37; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Steinwart I, 2005, IEEE T INFORM THEORY, V51, P128, DOI 10.1109/TIT.2004.839514; Steinwart I, 2002, J COMPLEXITY, V18, P768, DOI 10.1006/jcom.2002.0642; Tewari A, 2007, J MACH LEARN RES, V8, P1007; TOPSOE F, 1979, KYBERNETIKA, V15, P8; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; VAPNIK V, 1992, ADV NEUR IN, V4, P831; Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700052
C	Feldman, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Feldman, Vitaly			Generalization of ERM in Stochastic Convex Optimization: The Dimension Strikes Back	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				STABILITY	In stochastic convex optimization the goal is to minimize a convex function F(x) (=) over dot Ef similar to D [f (x)] over a convex set K subset of R-d where D is some unknown distribution and each f (.) in the support of D is convex over K. The optimization is commonly based on i.i.d. samples f(1), f(2),, f(n) from D. A standard approach to such problems is empirical risk minimization (ERM) that optimizes F-s(x) (=) over dot 1/n Sigma(i <= n) f(i)(x). Here we consider the question of how many samples are necessary for ERM to succeed and the closely related question of uniform convergence of F-S to F over K. We demonstrate that in the standard l(p)/l(p) setting of Lipschitz-bounded functions over a K of bounded radius, ERM requires sample size that scales linearly with the dimension d. This nearly matches standard upper bounds and improves on Omega (log d) dependence proved for l(2)/l(2) setting in [18]. In stark contrast, these problems can be solved using dimension-independent number of samples for l(2)/l(2) setting and log d dependence for l(1)/l(infinity) setting using other approaches. We further show that our lower bound applies even if the functions in the support of D are smooth and efficiently computable and even if an l(1) regularization term is added. Finally, we demonstrate that for a more general class of bounded-range (but not Lipschitz-bounded) stochastic convex programs an infinite gap appears already in dimension 2.	[Feldman, Vitaly] IBM Res Almaden, San Jose, CA 95120 USA	International Business Machines (IBM)	Feldman, V (corresponding author), IBM Res Almaden, San Jose, CA 95120 USA.							[Anonymous], 2012, ICML; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Belloni A., 2015, C LEARN THEOR, P240; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bubeck S., 2015, FDN TRENDS MACHINE L; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Dwork C., 2015, ABS1506 CORR; Dwork Cynthia, 2014, ABS14112664 CORR; Feldman V., 2016, ABS160804414 CORR; Feldman V., 2015, ABS151209170 CORR; Hardt M, 2016, PR MACH LEARN RES, V48; JUSTESEN J, 1972, IEEE T INFORM THEORY, V18, P652, DOI 10.1109/TIT.1972.1054893; Kakade Sham M., 2008, NIPS; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Rakhlin A., 2015, ABS150107340 CORR; Shalev-Shwartz S., 2009, P 22 C LEARN THEOR C, P1; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Shamir O., 2013, P INT C MACH LEARN A, P71; Shapiro A., 2005, CONTINUOUS OPTIMIZAT, P144; Spielman DA, 1996, IEEE T INFORM THEORY, V42, P1723, DOI 10.1109/18.556668; Vapnik V.N, 1998, STAT LEARNING THEORY	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703103
C	Flamary, R; Fevotte, C; Courty, N; Emiya, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Flamary, Remi; Fevotte, Cedric; Courty, Nicolas; Emiya, Valentin			Optimal spectral transportation with application to music transcription	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many spectral unmixing methods rely on the non-negative decomposition of spectral data onto a dictionary of spectral templates. In particular, state-of-the-art music transcription systems decompose the spectrogram of the input signal onto a dictionary of representative note spectra. The typical measures of fit used to quantify the adequacy of the decomposition compare the data and template entries frequency-wise. As such, small displacements of energy from a frequency bin to another as well as variations of timbre can disproportionally harm the fit. We address these issues by means of optimal transportation and propose a new measure of fit that treats the frequency distributions of energy holistically as opposed to frequency-wise. Building on the harmonic nature of sound, the new measure is invariant to shifts of energy to harmonically-related frequencies, as well as to small and local displacements of energy. Equipped with this new measure of fit, the dictionary of note templates can be considerably simplified to a set of Dirac vectors located at the target fundamental frequencies (musical pitch values). This in turns gives ground to a very fast and simple decomposition algorithm that achieves state-of-the-art performance on real musical data.	[Flamary, Remi] Univ Cote dAzur, CNRS, OCA, Nice, France; [Fevotte, Cedric] IRIT, CNRS, Toulouse, France; [Courty, Nicolas] Univ Bretagne Sud, CNRS, IRISA, Lorient, France; [Emiya, Valentin] Aix Marseille Univ, CNRS, LIF, Marseille, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Cote d'Azur; Observatoire de la Cote d'Azur; Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Aix-Marseille Universite	Flamary, R (corresponding author), Univ Cote dAzur, CNRS, OCA, Nice, France.	remi.flamary@unice.fr; cedric.fevotte@irit.fr; courty@univ-ubs.fr; valentin.emiya@lif.univ-mrs.fr	Flamary, Rémi/AAC-1958-2022	Flamary, Rémi/0000-0002-4212-6627	European Research Council (ERC) under the European Union's Horizon 2020 research & innovation programme (project FACTORY); French ANR JCJC program MAD [ANR-14-CE27-0002]	European Research Council (ERC) under the European Union's Horizon 2020 research & innovation programme (project FACTORY)(European Research Council (ERC)); French ANR JCJC program MAD(French National Research Agency (ANR))	This work is supported in part by the European Research Council (ERC) under the European Union's Horizon 2020 research & innovation programme (project FACTORY) and by the French ANR JCJC program MAD (ANR-14-CE27-0002). Many thanks to Antony Schutz for generating & providing some of the musical data.	Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; Boulanger-Lewandowski N., 2012, P INT SOC MUS INF RE; Courty N., 2014, P EUR C MACH LEARN P; Cuturi M., 2013, ADV NEURAL INFORM PR; Daniel A., 2008, P INT SOC MUS INF RE; Emiya V, 2010, IEEE T AUDIO SPEECH, V18, P1643, DOI 10.1109/TASL.2009.2038819; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; Huang J, 2009, BIOMETRIKA, V96, P339, DOI 10.1093/biomet/asp020; Oudre L, 2011, IEEE T AUDIO SPEECH, V19, P2222, DOI 10.1109/TASL.2011.2139205; Rigaud F, 2013, J ACOUST SOC AM, V133, P3107, DOI 10.1121/1.4799806; Rolet A., 2016, P INT C ART INT STAT; RUBNER Y, 1998, P INT C COMP VIS ICC; Sandler R, 2011, IEEE T PATTERN ANAL, V33, P1590, DOI 10.1109/TPAMI.2011.18; Smaragdis P., 2006, P NIPS WORKSH ADV MO; SMARAGDIS P, 2003, P IEEE WORKSH APPL S; Typke R., 2004, P ACM INT C MULT; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vincent E, 2010, IEEE T AUDIO SPEECH, V18, P528, DOI 10.1109/TASL.2009.2034186; Zen G., 2014, P INT C PATT REC ICP	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704004
C	Fraser, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Fraser, Maia			Multi-step learning and underlying structure in statistical models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MANIFOLD REGULARIZATION	In multi-step learning, where a final learning task is accomplished via a sequence of intermediate learning tasks, the intuition is that successive steps or levels transform the initial data into representations more and more "suited" to the final learning task. A related principle arises in transfer-learning where Baxter (2000) proposed a theoretical framework to study how learning multiple tasks transforms the inductive bias of a learner. The most widespread multi-step learning approach is semi-supervised learning with two steps: unsupervised, then supervised. Several authors (Castelli-Cover, 1996; Balcan-Blum, 2005; Niyogi, 2008; Ben-David et al, 2008; Urner et al, 2011) have analyzed SSL, with Balcan-Blum (2005) proposing a version of the PAC learning framework augmented by a "compatibility function" to link concept class and unlabeled data distribution. We propose to analyze SSL and other multi-step learning approaches, much in the spirit of Baxter's framework, by defining a learning problem generatively as a joint statistical model on X.Y. This determines in a natural way the class of conditional distributions that are possible with each marginal, and amounts to an abstract form of compatibility function. It also allows to analyze both discrete and non-discrete settings. As tool for our analysis, we define a notion of gamma-uniform shattering for statistical models. We use this to give conditions on the marginal and conditional models which imply an advantage for multi-step learning approaches. In particular, we recover a more general version of a result of Poggio et al (2012): under mild hypotheses a multi-step approach which learns features invariant under successive factors of a finite group of invariances has sample complexity requirements that are additive rather than multiplicative in the size of the subgroups.	[Fraser, Maia] Univ Ottawa, Dept Math & Stat, Brain & Mind Res Inst, Ottawa, ON K1N 6N5, Canada	University of Ottawa	Fraser, M (corresponding author), Univ Ottawa, Dept Math & Stat, Brain & Mind Res Inst, Ottawa, ON K1N 6N5, Canada.	mfrase8@uottawa.ca						Ahissar M, 2004, TRENDS COGN SCI, V8, P457, DOI 10.1016/j.tics.2004.08.011; Alain G., 2012, TECHNICAL REPORT; [Anonymous], 1994, INTRO COMPUTATIONAL; Balcan MF, 2005, LECT NOTES COMPUT SC, V3559, P111, DOI 10.1007/11503415_8; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Ben-David S., 2008, C LEARN THEOR COLT; Bourne JA, 2006, CEREB CORTEX, V16, P405, DOI 10.1093/cercor/bhi119; Castelli V, 1996, IEEE T INFORM THEORY, V42, P2102, DOI 10.1109/18.556600; Devroye Luc P., 1996, PROBABILISTIC THEORY, V31; Haussler D., 1989, GEN PAC MODEL SAMPLE, P40; KEARNS MJ, 1994, J COMPUT SYST SCI, V48, P464, DOI 10.1016/S0022-0000(05)80062-5; Koltchinskii V, 2001, IEEE T INFORM THEORY, V47, P1902, DOI 10.1109/18.930926; Mallat S., 2011, ABS11012286 CORR; Mohri M., 2018, FDN MACHINE LEARNING; Niyogi P, 2013, J MACH LEARN RES, V14, P1229; Poggio T., 2012, TECHNICAL REPORT; Urner R., 2011, ICML; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701054
C	Friedrich, J; Paninski, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Friedrich, Johannes; Paninski, Liam			Fast Active Set Methods for Online Spike Inference from Calcium Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DECONVOLUTION; POPULATIONS; CA2+	Fluorescent calcium indicators are a popular means for observing the spiking activity of large neuronal populations. Unfortunately, extracting the spike train of each neuron from raw fluorescence calcium imaging data is a nontrivial problem. We present a fast online active set method to solve this sparse nonnegative deconvolution problem. Importantly, the algorithm progresses through each time series sequentially from beginning to end, thus enabling real-time online spike inference during the imaging session. Our algorithm is a generalization of the pool adjacent violators algorithm (PAVA) for isotonic regression and inherits its linear-time computational complexity. We gain remarkable increases in processing speed: more than one order of magnitude compared to currently employed state of the art convex solvers relying on interior point methods. Our method can exploit warm starts; therefore optimizing model hyperparameters only requires a handful of passes through the data. The algorithm enables real-time simultaneous deconvolution of O(10(5)) traces of whole-brain zebrafish imaging data on a laptop.	[Friedrich, Johannes; Paninski, Liam] Columbia Univ, Grossman Ctr, New York, NY 10027 USA; [Friedrich, Johannes; Paninski, Liam] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Friedrich, Johannes] Janelia Res Campus, Ashburn, VA 20147 USA	Columbia University; Columbia University	Friedrich, J (corresponding author), Columbia Univ, Grossman Ctr, New York, NY 10027 USA.; Friedrich, J (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.; Friedrich, J (corresponding author), Janelia Res Campus, Ashburn, VA 20147 USA.	j.friedrich@columbia.edu; liam@stat.columbia.edu	Friedrich, Johannes/GXH-9356-2022	Friedrich, Johannes/0000-0002-1321-5866	Swiss National Science Foundation [P300P2_158428]; Simons Foundation Global Brain Research Awards [325171, 365002]; NIH BRAIN Initiative [R01 EB22913, R21 EY027592]; DARPA [N66001-15-C-4032]; Google Faculty Research award; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]; ARO MURI [W911NF-12-1-0594]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission); Simons Foundation Global Brain Research Awards; NIH BRAIN Initiative; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Google Faculty Research award(Google Incorporated); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC); ARO MURI(MURI)	Funding for this research was provided by Swiss National Science Foundation Research Award P300P2_158428, Simons Foundation Global Brain Research Awards 325171 and 365002, ARO MURI W911NF-12-1-0594, NIH BRAIN Initiative R01 EB22913 and R21 EY027592, DARPA N66001-15-C-4032 (SIMPLEX), and a Google Faculty Research award; in addition, this work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/NMETH.2434, 10.1038/nmeth.2434]; Andersen E., 2000, HIGH PERFORMANCE OPT, P197, DOI DOI 10.1007/978-1-4757-3216-0_8; AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Barlow R. E., 1972, STAT INFERENCE ORDER; Brent R.P., 1973, ALGORITHMS MINIMIZAT; Chen TW, 2013, NATURE, V499, P295, DOI 10.1038/nature12354; Clancy KB, 2014, NAT NEUROSCI, V17, P807, DOI 10.1038/nn.3712; Diamond S, 2016, J MACH LEARN RES, V17; Domahidi Alexander, 2013, 2013 European Control Conference (ECC), P3071; Friedrich J, 2016, ARXIV160900639; Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453; Grienberger C, 2012, NEURON, V73, P862, DOI 10.1016/j.neuron.2012.02.011; Grosenick L, 2015, NEURON, V86, P106, DOI 10.1016/j.neuron.2015.03.034; Gurobi Optimization Inc, 2015, GUROBI OPTIMIZER REF; Holekamp TF, 2008, NEURON, V57, P661, DOI 10.1016/j.neuron.2008.01.011; Lewi J, 2009, NEURAL COMPUT, V21, P619, DOI 10.1162/neco.2008.08-07-594; O'Donoghue B., 2016, J OPTIM THEORY APPL, V169, P1; Packer AM, 2015, NAT METHODS, V12, P140, DOI 10.1038/nmeth.3217; Park Mijung, 2012, ADV NEURAL INF PROCE, V25, P2; Pnevmatikakis E A, 2013, AS C SIGN SYST COMP; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; Pologruto TA, 2004, J NEUROSCI, V24, P9572, DOI 10.1523/JNEUROSCI.2854-04.2004; Rickgauer JP, 2014, NAT NEUROSCI, V17, P1816, DOI 10.1038/nn.3866; Sasaki T, 2008, J NEUROPHYSIOL, V100, P1668, DOI 10.1152/jn.00084.2008; Shababo B., 2013, ADV NEURAL INFORM PR, V26, P1304; Theis L, 2016, NEURON, V90, P471, DOI 10.1016/j.neuron.2016.04.014; Vladimirov N, 2014, NAT METHODS; Vogelstein JT, 2010, J NEUROPHYSIOL, V104, P3691, DOI 10.1152/jn.01073.2009; Vogelstein JT, 2009, BIOPHYS J, V97, P636, DOI 10.1016/j.bpj.2008.08.005; Yaksi E, 2006, NAT METHODS, V3, P377, DOI 10.1038/NMETH874	30	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704030
C	Fujii, K; Kashima, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Fujii, Kaito; Kashima, Hisashi			Budgeted stream-based active learning via adaptive submodular maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Active learning enables us to reduce the annotation cost by adaptively selecting unlabeled instances to be labeled. For pool-based active learning, several effective methods with theoretical guarantees have been developed through maximizing some utility function satisfying adaptive submodularity. In contrast, there have been few methods for stream-based active learning based on adaptive submodularity. In this paper, we propose a new class of utility functions, policy-adaptive submodular functions, which includes many existing adaptive submodular functions appearing in real world problems. We provide a general framework based on policy-adaptive submodularity that makes it possible to convert existing pool-based methods to stream-based methods and give theoretical guarantees on their performance. In addition we empirically demonstrate their effectiveness by comparing with existing heuristics on common benchmark datasets.	[Fujii, Kaito] Kyoto Univ, JST, ERATO, Kawarabayashi Large Graph Project, Kyoto, Japan; [Kashima, Hisashi] Kyoto Univ, Kyoto, Japan	Japan Science & Technology Agency (JST); Kyoto University; Kyoto University	Fujii, K (corresponding author), Kyoto Univ, JST, ERATO, Kawarabayashi Large Graph Project, Kyoto, Japan.	fujii@ml.ist.i.kyoto-u.ac.jp; kashima@i.kyoto-u.ac.jp						Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Bateni M, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2500121; Beygelzimer A., 2009, P 26 ANN INT C MACH, P49; Chakrabarti A, 2015, MATH PROGRAM, V154, P225, DOI 10.1007/s10107-015-0900-7; Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26; Chen Y., 2013, INT C MACHINE LEARNI, P160; Dagan I., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P150; Das J, 2015, INT J ROBOT RES, V34, P1435, DOI 10.1177/0278364915587723; Dasgupta S., 2004, ADV NEURAL INFORM PR, P337; DYNKIN E. B., 1963, SOV MATH DOKL, V4, P627; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman Moran, 2011, Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques. Proceedings 14th International Workshop, APPROX 2011 and 15th International Workshop, RANDOM 2011, P218, DOI 10.1007/978-3-642-22935-0_19; Freund Y, 1997, MACH LEARN, V28, P133, DOI 10.1023/A:1007330508534; Gabillon V., 2013, ADV NEURAL INFORM PR, P2697; Golovin D, 2010, ARON CULOTTA ADV NEU, P766; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Gonen A, 2013, J MACH LEARN RES, V14, P2583; Gotovos A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1996; Loy C. C., 2012, P IEEE C COMP VIS PA; Cuong NV, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P122; Nguyen Viet Cuong, 2013, ADV NEURAL INFORM PR, V26, P1457; Sabato Sivan, 2016, P 29 ANN C LEARN THE, P1419; Sculley D., 2007, P 4 C EM ANT CEAS; Smailovic J, 2014, INFORM SCIENCES, V285, P181, DOI 10.1016/j.ins.2014.04.034	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700003
C	Ganapathiraman, V; Zhang, XH; Yu, YL; Wen, JF		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ganapathiraman, Vignesh; Zhang, Xinhua; Yu, Yaoliang; Wen, Junfeng			Convex Two-Layer Modeling with Latent Structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Unsupervised learning of structured predictors has been a long standing pursuit in machine learning. Recently a conditional random field auto-encoder has been proposed in a two-layer setting, allowing latent structured representation to be automatically inferred. Aside from being nonconvex, it also requires the demanding inference of normalization. In this paper, we develop a convex relaxation of two-layer conditional model which captures latent structure and estimates model parameters, jointly and optimally. We further expand its applicability by resorting to a weaker form of inference-maximum a-posteriori. The flexibility of the model is demonstrated on two structures based on total unimodularity-graph matching and linear chain. Experimental results confirm the promise of the method.	[Ganapathiraman, Vignesh; Zhang, Xinhua] Univ Illinois, Chicago, IL 60607 USA; [Yu, Yaoliang] Univ Waterloo, Waterloo, ON, Canada; [Wen, Junfeng] Univ Alberta, Edmonton, AB, Canada	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital; University of Waterloo; University of Alberta	Ganapathiraman, V (corresponding author), Univ Illinois, Chicago, IL 60607 USA.	vganap2@uic.edu; zhangx@uic.edu; yaoliang.yu@uwaterloo.ca; junfengwen@gmail.com						Ammar W., 2014, NIPS; Arora S., 2014, ICML; Aslan O, 2014, NIPS; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bengio Yoshua, 2005, NIPS; Chang M.-W., 2010, NAACL; Chang Ming-Wei, 2010, ICML; Chen L.C., 2015, ICML; Chen N, 2012, IEEE T PATTERN ANAL, V34, P2365, DOI 10.1109/TPAMI.2012.64; Collobert R., 2011, ICML; Daume III H., 2009, ICML; Druck G., 2007, KDD; Gane A., 2014, AISTATS; Goldwasser D., 2008, EMNLP; Gotovos A., 2015, NIPS; Haffari G. R., 2007, UAI; Hazan T., 2012, ICML; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Jaggi Martin., 2013, ICML; Larochelle H., 2008, ICML; Livni R., 2014, ARXIV13047045V2; Meshi O., 2015, NIPS; Mnih V., 2011, AISTATS; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; Ratajczak M., 2014, WORKSH LEARN TRACT P; Smith Noah A., 2005, ACL; Sohn K., 2015, NIPS; Taskar B., 2005, ICML; Xu L., 2009, ICML; Xu L., 2006, ICML	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702061
C	Garber, D; Meshi, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Garber, Dan; Meshi, Ofer			Linear-Memory and Decomposition-Invariant Linearly Convergent Conditional Gradient Algorithm for Structured Polytopes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recently, several works have shown that natural modifications of the classical conditional gradient method (aka Frank-Wolfe algorithm) for constrained convex optimization, provably converge with a linear rate when: i) the feasible set is a polytope, and ii) the objective is smooth and strongly-convex. However, all of these results suffer from two significant shortcomings: 1. large memory requirement due to the need to store an explicit convex decomposition of the current iterate, and as a consequence, large running-time overhead per iteration 2. the worst case convergence rate depends unfavorably on the dimension In this work we present a new conditional gradient variant and a corresponding analysis that improves on both of the above shortcomings. In particular: 1. both memory and computation overheads are only linear in the dimension 2. in case the optimal solution is sparse, the new convergence rate replaces a factor which is at least linear in the dimension in previous work, with a linear dependence on the number of non-zeros in the optimal solution At the heart of our method and corresponding analysis, is a novel way to compute decomposition-invariant away-steps. While our theoretical guarantees do not apply to any polytope, they apply to several important structured polytopes that capture central concepts such as paths in graphs, perfect matchings in bipartite graphs, marginal distributions that arise in structured prediction tasks, and more. Our theoretical findings are complemented by empirical evidence which shows that our method delivers state-of-the-art performance.	[Garber, Dan] Toyota Technol Inst, Chicago, IL 60637 USA; [Meshi, Ofer] Google, Mountain View, CA USA	Toyota Technological Institute - Chicago; Google Incorporated	Garber, D (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	dgarber@ttic.edu; meshi@google.com						Ahipasaoglu SD, 2008, OPTIM METHOD SOFTW, V23, P5, DOI 10.1080/10556780701589669; [Anonymous], 2013, P 30 INT C MACH LEAR; Beck A, 2004, MATH METHOD OPER RES, V59, P235, DOI 10.1007/s001860300327; Beck A., 2015, ARXIV150405002; Dudik M., 2012, AISTATS, P327; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garber D, 2015, PR MACH LEARN RES, V37, P541; Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366; Garber Dan, 2016, ARXIV160506203; GueLat Jacques, 1986, MATH PROGRAMMING, V35; Harchaoui Z, 2012, IEEE C COMP VIS PATT; Hazan E., 2012, P 29 INT C MACH LEAR; Hazan Elad, 2016, CORR; Jaggi M., 2010, ICML; Jaggi M., 2013, ICML; Joulin A, 2014, LECT NOTES COMPUT SC, V8694, P253, DOI 10.1007/978-3-319-10599-4_17; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Lan Guanghui, 2013, CORR; Laue Soren, 2012, P 29 INT C MACH LEAR P 29 INT C MACH LEAR; Osokin Anton, 2016, INT C MACH LEARN ICM; SCHRIJVER A., 2003, COMBINATORIAL OPTIMI, V24; Shalev-Shwartz S., 2011, P 28 INT C MACH LEAR; Taskar B., 2003, ADV NEURAL INFORM PR; Wainwright M, 2008, GRAPHICAL MODELS EXP; Ying YM, 2012, J MACH LEARN RES, V13, P1	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700079
C	Garber, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Garber, Dan			Faster Projection-free Convex Optimization over the Spectrahedron	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				EIGENVALUE	Minimizing a convex function over the spectrahedron, i.e., the set of all d x d positive semidefinite matrices with unit trace, is an important optimization task with many applications in optimization, machine learning, and signal processing. It is also notoriously difficult to solve in large-scale since standard techniques require to compute expensive matrix decompositions. An alternative is the conditional gradient method (aka Frank-Wolfe algorithm) that regained much interest in recent years, mostly due to its application to this specific setting. The key benefit of the CG method is that it avoids expensive matrix decompositions all together, and simply requires a single eigenvector computation per iteration, which is much more efficient. On the downside, the CG method, in general, converges with an inferior rate. The error for minimizing a beta-smooth function after t iterations scales like beta/t. This rate does not improve even if the function is also strongly convex. In this work we present a modification of the CG method tailored for the spectrahedron. The per-iteration complexity of the method is essentially identical to that of the standard CG method: only a single eigenvector computation is required. For minimizing an a-strongly convex and beta-smooth function, the expected error of the method after t iterations is: O(min{beta/t, (beta root rank(X*)/alpha(1/4)t)(4/3), (beta/root alpha lambda(min) (X*)t)(2)}), where rank(X*), lambda(min) (X*) are the rank of the optimal solution and smallest non-zero eigenvalue, respectively. Beyond the significant improvement in convergence rate, it also follows that when the optimum is low-rank, our method provides better accuracy-rank tradeoff than the standard CG method. To the best of our knowledge, this is the first result that attains provably faster convergence rates for a CG variant for optimization over the spectrahedron. We also present encouraging preliminary empirical results.	[Garber, Dan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Garber, D (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	dgarber@ttic.edu						Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Dudik M., 2012, AISTATS, P327; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Freund Robert M, 2015, ARXIV151102204; Garber D., 2015, ARXIV150905647; Garber D, 2016, PR MACH LEARN RES, V48; Garber D, 2015, PR MACH LEARN RES, V37, P541; Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Hazan E., 2012, P 29 INT C MACH LEAR; Hazan Elad, 2008, 8 LAT AM THEOR INF S; Jaggi M., 2010, ICML; Jaggi M., 2013, ICML; KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Laue Soren, 2012, P 29 INT C MACH LEAR P 29 INT C MACH LEAR; Shalev-Shwartz S., 2011, P 28 INT C MACH LEAR; Shamir O., 2015, P 32 INT C MACH LEAR; Xing E., 2002, ADV NEURAL INFORM PR, V15, P505, DOI DOI 10.5555/2968618.2968683; Ying YM, 2012, J MACH LEARN RES, V13, P1; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Zhang Xinhua, 2012, ADV NEURAL INFORM PR, P2906	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703033
C	Genevay, A; Cuturi, M; Peyre, G; Bach, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Genevay, Aude; Cuturi, Marco; Peyre, Gabriel; Bach, Francis			Stochastic Optimization for Large-scale Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DISTANCE	Optimal transport (OT) defines a powerful framework to compare probability distributions in a geometrically faithful way. However, the practical impact of OT is still limited because of its computational burden. We propose a new class of stochastic optimization algorithms to cope with large-scale OT problems. These methods can handle arbitrary distributions (either discrete or continuous) as long as one is able to draw samples from them, which is the typical setup in high-dimensional learning problems. This alleviates the need to discretize these densities, while giving access to provably convergent methods that output the correct distance without discretization error. These algorithms rely on two main ideas: (a) the dual OT problem can be re-cast as the maximization of an expectation; (b) the entropic regularization of the primal OT problem yields a smooth dual optimization which can be addressed with algorithms that have a provably faster convergence. We instantiate these ideas in three different setups: (i) when comparing a discrete distribution to another, we show that incremental stochastic optimization schemes can beat Sinkhorn's algorithm, the current state-of-the-art finite dimensional OT solver; (ii) when comparing a discrete distribution to a continuous density, a semi-discrete reformulation of the dual program is amenable to averaged stochastic gradient descent, leading to better performance than approximately solving the problem by discretization; (iii) when dealing with two continuous densities, we propose a stochastic gradient descent over a reproducing kernel Hilbert space (RKHS). This is currently the only known method to solve this problem, apart from computing OT on finite samples. We backup these claims on a set of discrete, semi-discrete and continuous benchmark problems.	[Genevay, Aude] Univ Paris 09, CEREMADE, INRIA, Mokaplan Project Team, Paris, France; [Cuturi, Marco] Univ Paris Saclay, CREST, ENSAE, Paris, France; [Peyre, Gabriel] Ecole Normale Super, CNRS, INRIA, Mokaplan Project Team, Paris, France; [Peyre, Gabriel] Ecole Normale Super, DMA, INRIA, Mokaplan Project Team, Paris, France; [Bach, Francis] INRIA, Sierra Project Team, DI, ENS, Villers Les Nancy, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Inria	Genevay, A (corresponding author), Univ Paris 09, CEREMADE, INRIA, Mokaplan Project Team, Paris, France.	genevay@ceremade.dauphine.fr; marco.cuturi@ensae.fr; gabriel.peyre@ens.fr; francis.bach@inria.fr			European Research Council (ERC SIGMA-Vision); Region Ile-de-France; JSPS [26700002]	European Research Council (ERC SIGMA-Vision)(European Research Council (ERC)); Region Ile-de-France(Region Ile-de-France); JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	GP was supported by the European Research Council (ERC SIGMA-Vision); AG by Region Ile-de-France; MC by JSPS grant 26700002.	Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001; Burkard R., 2009, ASSIGNMENT PROBLEMS; Carlier  G., 2015, ARXIV151202783; COMINETTI R, 1994, MATH PROGRAM, V67, P169, DOI 10.1007/BF01582220; CUTURI M., 2013, P INT C ADV NEURAL I, V26; DIEULEVEUT A., 2014, ARXIV14080361; FRANKLIN J, 1989, LINEAR ALGEBRA APPL, V114, P717, DOI 10.1016/0024-3795(89)90490-4; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Kantorovich L., 1942, DOKL AKAD NAUK+, V37, P227; Kusner M. J., 2015, ICML; Merigot Q, 2011, COMPUT GRAPH FORUM, V30, P1583, DOI 10.1111/j.1467-8659.2011.02032.x; Montavon  G., 2016, ADV NEURAL INFORM PR; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Schmidt M., 2016, MATH PROGRAMMING; SINKHORN R, 1964, ANN MATH STAT, V35, P876, DOI 10.1214/aoms/1177703591; Solomon J., 2015, ACM T GRAPHIC, V34, P66; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Steinwart I., 2008, SUPPORT VECTOR MACHI; Villani C., 2003, TOPICS OPTIMAL TRANS; Wu G., 2006, P 12 ACM SIGKDD INT, P760	25	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704090
C	Georgogiannis, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Georgogiannis, Alexandros			Robust k-means: a Theoretical Revisit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Over the last years, many variations of the quadratic k-means clustering procedure have been proposed, all aiming to robustify the performance of the algorithm in the presence of outliers. In general terms, two main approaches have been developed: one based on penalized regularization methods, and one based on trimming functions. In this work, we present a theoretical analysis of the robustness and consistency properties of a variant of the classical quadratic k-means algorithm, the robust k-means, which borrows ideas from outlier detection in regression. We show that two outliers in a dataset are enough to breakdown this clustering procedure. However, if we focus on "well-structured" datasets, then robust k-means can recover the underlying cluster structure in spite of the outliers. Finally, we show that, with slight modifications, the most general non-asymptotic results for consistency of quadratic k-means remain valid for this robust variant.	[Georgogiannis, Alexandros] Tech Univ Crete, Sch Elect & Comp Engn, Khania, Greece	Technical University of Crete	Georgogiannis, A (corresponding author), Tech Univ Crete, Sch Elect & Comp Engn, Khania, Greece.	alexandrosgeorgogiannis@gmail.com						[Anonymous], 2011, ROBUST STAT APPROACH; Antoniadis Anestis, 2011, J AM STAT ASS; Ben-David S, 2014, PR MACH LEARN RES, V32, P280; Chawla S, K MEANS UNIFIED APPR; DEVROYE L, 1997, STOCHASTIC MODELLING; Forero PA, 2012, IEEE T SIGNAL PROCES, V60, P4163, DOI 10.1109/TSP.2012.2196696; Gallegos MT, 2005, ANN STAT, V33, P347, DOI 10.1214/009053604000000940; Garcia-Escudero LA, 1999, J AM STAT ASSOC, V94, P956, DOI 10.2307/2670010; Garey M.R., 1979, COMPUTERS INTRACTABI; Hennig C, 2012, TRIMCLUSTER CLUSTER; Linder T, 2002, CISM COURSES LECT, P163; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Mazumder R., 2012, J AM STAT ASS; Melnykov V, 2012, J STAT SOFTW, V51, P1; POLLARD D, 1981, ANN STAT, V9, P135, DOI 10.1214/aos/1176345339; Pollard David, 1984, CONVERGENCE STOCHAST; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Ritter G., 2014, CHAPMAN HALL CRC MON; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; She YY, 2009, ELECTRON J STAT, V3, P384, DOI 10.1214/08-EJS348; Teboulle M, 2007, J MACH LEARN RES, V8, P65; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Van De Geer Sara, 2003, COWL WORKSH YAL U; Witten DM, 2013, STAT INTERFACE, V6, P211; Wright SJ, 2015, MATH PROGRAM, V151, P3, DOI 10.1007/s10107-015-0892-3; Yu Y., 2015, AISTATS	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700090
C	Gerchinovitz, S; Lattimore, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gerchinovitz, Sebastien; Lattimore, Tor			Refined Lower Bounds for Adversarial Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We provide new lower bounds on the regret that must be suffered by adversarial bandit algorithms. The new results show that recent upper bounds that either (a) hold with high-probability or (b) depend on the total loss of the best arm or (c) depend on the quadratic variation of the losses, are close to tight. Besides this we prove two impossibility results. First, the existence of a single arm that is optimal in every round cannot improve the regret in the worst case. Second, the regret cannot scale with the effective range of the losses. In contrast, both results are possible in the full-information setting.	[Gerchinovitz, Sebastien] Univ Toulouse 3 Paul Sabatier, Inst Math Toulouse, F-31062 Toulouse, France; [Lattimore, Tor] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada	Universite de Toulouse; Universite Toulouse III - Paul Sabatier; University of Alberta	Gerchinovitz, S (corresponding author), Univ Toulouse 3 Paul Sabatier, Inst Math Toulouse, F-31062 Toulouse, France.	sebastien.gerchinovitz@math.univ-toulouse.fr; tor.lattimore@gmail.com			CIMI (Centre International de Mathematiques et d'Informatique) Excellence program; French Agence Nationale de la Recherche (ANR) [ANR-13-BS01-0005, ANR-13-CORD-0020]	CIMI (Centre International de Mathematiques et d'Informatique) Excellence program; French Agence Nationale de la Recherche (ANR)(French National Research Agency (ANR))	The authors would like to thank Aurelien Garivier and Emilie Kaufmann for insightful discussions. This work was partially supported by the CIMI (Centre International de Mathematiques et d'Informatique) Excellence program. The authors acknowledge the support of the French Agence Nationale de la Recherche (ANR), under grants ANR-13-BS01-0005 (project SPADRO) and ANR-13-CORD-0020 (project ALICIA).	Allenberg C, 2006, LECT NOTES ARTIF INT, V4264, P229; [Anonymous], 2005, THESIS; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bubeck S., 2013, P 26 ANN C LEARN THE, P122; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Gaillard P., 2014, P 27 C LEARN THEOR C; Hazan E., 2011, P 24 C LEARN THEOR, P817; Hazan E, 2011, J MACH LEARN RES, V12, P1287; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Neu G., 2015, C LEARN THEOR, V40, P1360; Neu G, 2015, ADV NEUR IN, V28; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Tsybakov A.B, 2008, INTRO NONPARAMETRIC	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702024
C	Golkov, V; Skwark, MJ; Golkov, A; Dosovitskiy, A; Brox, T; Meiler, J; Cremers, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Golkov, Vladimir; Skwark, Marcin J.; Golkov, Antonij; Dosovitskiy, Alexey; Brox, Thomas; Meiler, Jens; Cremers, Daniel			Protein contact prediction from amino acid co-evolution using convolutional networks for graph-valued images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Proteins are responsible for most of the functions in life, and thus are the central focus of many areas of biomedicine. Protein structure is strongly related to protein function, but is difficult to elucidate experimentally, therefore computational structure prediction is a crucial task on the way to solve many biological questions. A contact map is a compact representation of the three-dimensional structure of a protein via the pairwise contacts between the amino acids constituting the protein. We use a convolutional network to calculate protein contact maps from detailed evolutionary coupling statistics between positions in the protein sequence. The input to the network has an image-like structure amenable to convolutions, but every "pixel" instead of color channels contains a bipartite undirected edge-weighted graph. We propose several methods for treating such "graph-valued images" in a convolutional network. The proposed method outperforms state-of-the-art methods by a considerable margin.	[Golkov, Vladimir; Cremers, Daniel] Tech Univ Munich, Munich, Germany; [Skwark, Marcin J.; Meiler, Jens] Vanderbilt Univ, 221 Kirkland Hall, Nashville, TN 37235 USA; [Golkov, Antonij] Univ Augsburg, Augsburg, Germany; [Dosovitskiy, Alexey; Brox, Thomas] Univ Freiburg, Freiburg, Germany	Technical University of Munich; Vanderbilt University; University of Augsburg; University of Freiburg	Golkov, V (corresponding author), Tech Univ Munich, Munich, Germany.	golkov@cs.tum.edu; marcin@skwark.pl; antonij.golkov@student.uni-augsburg.de; dosovits@cs.uni-freiburg.de; brox@cs.uni-freiburg.de; jens.meiler@vanderbilt.edu; cremers@tum.de	Skwark, Marcin Jerzy/Y-4019-2018	Skwark, Marcin Jerzy/0000-0002-2022-6766	Deutsche Telekom Foundation; ERC Starting Grant "VideoLearn"; ERC Consolidator Grant "3DReloaded"	Deutsche Telekom Foundation; ERC Starting Grant "VideoLearn"; ERC Consolidator Grant "3DReloaded"	Deutsche Telekom Foundation, ERC Consolidator Grant "3DReloaded", ERC Starting Grant "VideoLearn".	[Anonymous], 2015, DEEP CONVOLUTIONAL N; Choi IG, 2006, P NATL ACAD SCI USA, V103, P14056, DOI 10.1073/pnas.0606239103; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dosovitskiy A., 2016, IEEE C COMP VIS PATT; Dunn SD, 2008, BIOINFORMATICS, V24, P333, DOI 10.1093/bioinformatics/btm604; Duvenaud D. K, 2015, ADV NEURAL INFORM PR, P2215; Ekeberg M, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.012707; Estrach J. B., 2014, P 2 INT C LEARN REPR; Feinauer C, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003847; Golkov V, 2016, IEEE T MED IMAGING, V35, P1344, DOI 10.1109/TMI.2016.2551324; Gromiha MM, 1999, BIOPHYS CHEM, V77, P49, DOI 10.1016/S0301-4622(99)00010-1; Jones DT, 2015, BIOINFORMATICS, V31, P999, DOI 10.1093/bioinformatics/btu791; Kingma D.P., 2015, INT C LEARN REPR ICL; Kosciolek T, 2016, PROTEINS, V84, P145, DOI 10.1002/prot.24863; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Leinonen R, 2004, BIOINFORMATICS, V20, P3236, DOI 10.1093/bioinformatics/bth191; Morcos F, 2011, P NATL ACAD SCI USA, V108, pE1293, DOI 10.1073/pnas.1111471108; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704013
C	Gu, Q; Banerjee, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gu, Qilong; Banerjee, Arindam			High Dimensional Structured Superposition Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MATRIX DECOMPOSITION	High dimensional superposition models characterize observations using parameters which can be written as a sum of multiple component parameters, each with its own structure, e.g., sum of low rank and sparse matrices, sum of sparse and rotated sparse vectors, etc. In this paper, we consider general superposition models which allow sum of any number of component parameters, and each component structure can be characterized by any norm. We present a simple estimator for such models, give a geometric condition under which the components can be accurately estimated, characterize sample complexity of the estimator, and give high probability non-asymptotic bounds on the componentwise estimation error. We use tools from empirical processes and generic chaining for the statistical analysis, and our results, which substantially generalize prior work on superposition models, are in terms of Gaussian widths of suitable sets.	[Gu, Qilong; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Gu, Q (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	guxxx396@cs.umn.edu; banerjee@cs.umn.edu			NSF [IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	The research was also supported by NSF grants IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, NASA grant NNX12AQ39A, and gifts from Adobe, IBM, and Yahoo.	Argyriou A., 2012, ADV NEURAL INFORM PR; Banerjee A., 2014, ADV NEURAL INFORM PR; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chandrasekaran V, 2012, ANN STAT, V40, P1935, DOI 10.1214/11-AOS949; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Foygel R, 2014, IEEE T INFORM THEORY, V60, P1223, DOI 10.1109/TIT.2013.2293654; Hsu D, 2011, IEEE T INFORM THEORY, V57, P7221, DOI 10.1109/TIT.2011.2158250; MCCOY M. B, 2013, ACHIEVABLE PERFORMAN; Mendelson S, 2015, J ACM, V62, DOI 10.1145/2699439; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Oymak S., 2015, ARXIV E PRINTS; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rockafellar R. T., 1970, CONVEX ANAL; Talagrand M., 2014, SERIES MODERN SURVEY; Tropp J. A., 2014, CONVEX RECOVERY STRU; Vershynin R, 2015, APPL NUMER HARMON AN, P3, DOI 10.1007/978-3-319-19749-4_1; Wright J., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1276, DOI 10.1109/ISIT.2012.6283062; Yang E., 2012, ADV NEURAL INFORM PR, P1	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700043
C	Gunasekar, S; Koyejo, O; Ghosh, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gunasekar, Suriya; Koyejo, Oluwasanmi; Ghosh, Joydeep			Preference Completion from Partial Rankings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a novel and efficient algorithm for the collaborative preference completion problem, which involves jointly estimating individualized rankings for a set of entities over a shared set of items, based on a limited number of observed affinity values. Our approach exploits the observation that while preferences are often recorded as numerical scores, the predictive quantity of interest is the underlying rankings. Thus, attempts to closely match the recorded scores may lead to over-fitting and impair generalization performance. Instead, we propose an estimator that directly fits the underlying preference order, combined with nuclear norm constraints to encourage low-rank parameters. Besides (approximate) correctness of the ranking order, the proposed estimator makes no generative assumption on the numerical scores of the observations. One consequence is that the proposed estimator can fit any consistent partial ranking over a subset of the items represented as a directed acyclic graph (DAG), generalizing standard techniques that can only fit preference scores. Despite this generality, for supervision representing total or blockwise total orders, the computational complexity of our algorithm is within a log factor of the standard algorithms for nuclear norm regularization based estimates for matrix completion. We further show promising empirical results for a novel and challenging application of collaboratively ranking of the associations between brain-regions and cognitive neuroscience terms.	[Gunasekar, Suriya; Ghosh, Joydeep] Univ Texas Austin, Austin, TX 78712 USA; [Koyejo, Oluwasanmi] Univ Illinois, Urbana, IL 61801 USA	University of Texas System; University of Texas Austin; University of Illinois System; University of Illinois Urbana-Champaign	Gunasekar, S (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	suriya@utexas.edu; sanmi@illinois.edu; ghosh@ece.utexas.edu			NSF [IIS-1421729, SCH 1418511]	NSF(National Science Foundation (NSF))	SG and JG acknowledge funding from NSF grants IIS-1421729 and SCH 1418511.	Acharyya S., 2013, UAI; Acharyya Sreangsu, 2012, UAI; Bartlett P. L., 2003, JMLR; Best M. J., 1990, MATH PROGRAM; CAI J.-F., 2010, SIAM J OPTIM; CANDES E, 2006, IEEE T INF THEORY; Candes E. J., 2009, FOCM; Candes E. J., 2010, P IEEE; Cao Zhe, 2007, P 24 INT C MACH LEAR; Chi E., 2013, GENOME RES; Cremonesi P., 2010, RECSYS; Duchi J., 2010, ICML; Ganti R., 2015, NIPS; Goldberg D., 1992, COMMUN ACM; Grotzinger S. J., 1984, APPL MATH OPTIM; Herbrich  R., 1999, NIPS; Joachims T., 2002, SIGKDD; Kakade S. M., 2011, NIPS; KALAI A. T., 2009, COLT; Keshavan R. H., 2010, IEEE T IT; Koren Y., 2009, IEEE COMPUTER; Koyejo O., 2013, RECSYS; Li P., 2007, NIPS; Liu T. Y., 2009, FDN TRENDS IR; Lu Y., 2015, ANN ALL C COMM CONTR; Ma S., 2011, MATH PROGRAM; Mnih A., 2007, NIPS; Oh S., 2015, NIPS; Parikh N., 2014, FDN TRENDS OPTIMIZAT; Park Dohyung, 2015, ICML; Recht B., 2011, JMLR; Steck H, 2010, KDD; Stout Q. F., 2013, ALGORITHMICA; Weimer Markus, 2008, NIPS; Yarkoni T., 2011, NAT METHODS; Zhou Y., 2008, LNCS, V5034	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702019
C	Gupta, R; Kumar, R; Vassilvitskii, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gupta, Rishi; Kumar, Ravi; Vassilvitskii, Sergei			On Mixtures of Markov Chains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MAXIMUM-LIKELIHOOD	We study the problem of reconstructing a mixture of Markov chains from the trajectories generated by random walks through the state space. Under mild non-degeneracy conditions, we show that we can uniquely reconstruct the underlying chains by only considering trajectories of length three, which represent triples of states. Our algorithm is spectral in nature, and is easy to implement.	[Gupta, Rishi] Stanford Univ, Stanford, CA 94305 USA; [Gupta, Rishi; Kumar, Ravi] Google Res, Mountain View, CA USA; [Vassilvitskii, Sergei] Google Res, New York, NY 10011 USA	Stanford University; Google Incorporated; Google Incorporated	Gupta, R (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	rishig@cs.stanford.edu; ravi.k53@gmail.com; sergeiv@google.com						Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Anandkumar Animashree, 2012, C LEARN THEOR COLT; Caron R., 2005, WSMR REPORT; Chaudhuri K., 2008, COLT, V4, P9; Chierichetti Flavio, 2012, P 21 INT C WORLD WID, P609, DOI [10.1145/2187836, DOI 10.1145/2187836.2187919]; Cohen Shay B., 2013, P NAACL HLT, P148; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Subakan C., 2014, ADV NEURAL INFORM PR, V27, P2249; Subakan Y. C., 2011, THESIS; Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jcss.2003.11.008	15	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700042
C	Gutin, E; Farias, VF		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gutin, Eli; Farias, Vivek F.			Optimistic Gittins Indices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALLOCATION	Starting with the Thomspon sampling algorithm, recent years have seen a resurgence of interest in Bayesian algorithms for the Multi-armed Bandit (MAB) problem. These algorithms seek to exploit prior information on arm biases and while several have been shown to be regret optimal, their design has not emerged from a principled approach. In contrast, if one cared about Bayesian regret discounted over an infinite horizon at a fixed, pre-specified rate, the celebrated Gittins index theorem offers an optimal algorithm. Unfortunately, the Gittins analysis does not appear to carry over to minimizing Bayesian regret over all sufficiently large horizons and computing a Gittins index is onerous relative to essentially any incumbent index scheme for the Bayesian MAB problem. The present paper proposes a sequence of 'optimistic' approximations to the Gittins index. We show that the use of these approximations in concert with the use of an increasing discount factor appears to offer a compelling alternative to state-of-the-art index schemes proposed for the Bayesian MAB problem in recent years by offering substantially improved performance with little to no additional computational overhead. In addition, we prove that the simplest of these approximations yields frequentist regret that matches the Lai-Robbins lower bound, including achieving matching constants.	[Gutin, Eli] MIT, Operat Res Ctr, Cambridge, MA 02142 USA; [Farias, Vivek F.] MIT, Sloan Sch Management, Cambridge, MA 02142 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Gutin, E (corresponding author), MIT, Operat Res Ctr, Cambridge, MA 02142 USA.	gutin@mit.edu; vivekf@mit.edu						AGRAWAL R, 1995, ADV APPL PROBAB, V27, P1054, DOI 10.2307/1427934; Agrawal S., 2013, ARTIF INTELL, P99; AGRAWAL S., P 25 C LEARN THEOR; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berry D. A., 1985, MONOGRAPHS STAT APPL; Bertsimas D, 1996, MATH OPER RES, V21, P257, DOI 10.1287/moor.21.2.257; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Garivier A., 2011, COLT; GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148; JOGDEO K, 1968, ANN MATH STAT, V39, P1191, DOI 10.1214/aoms/1177698243; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Korda N, 2013, ADV NEURAL INFORM PR, P1448; LAI TL, 1987, ANN STAT, V15, P1091, DOI 10.1214/aos/1176350495; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T, 2016, P MACH LEARN RES C L, V49, P1; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Nino-Mora J, 2011, INFORMS J COMPUT, V23, P254, DOI 10.1287/ijoc.1100.0398; Powell W.B, 2012, OPTIMAL LEARNING, V841; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tsitsiklis JN, 1994, ANN APPL PROBAB, V4, P194, DOI 10.1214/aoap/1177005207; Van Roy, 2014, ADV NEURAL INFORM PR, P1583; Weber R., 1992, ANN APPL PROBAB, V2, P1024, DOI 10.1214/aoap/1177005588; WHITTLE P, 1980, J ROY STAT SOC B MET, V42, P143	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700001
C	Hajinezhad, D; Hong, MY; Zhao, T; Wang, ZR		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hajinezhad, Davood; Hong, Mingyi; Zhao, Tuo; Wang, Zhaoran			NESTT: A Nonconvex Primal-Dual Splitting Method for Distributed and Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONVERGENCE; REGRESSION	We study a stochastic and distributed algorithm for nonconvex problems whose objective consists of a sum of N nonconvex L-i/N-smooth functions, plus a non-smooth regularizer. The proposed NonconvEx primal-dual SpliTTing (NESTT) algorithm splits the problem into N subproblems, and utilizes an augmented Lagrangian based primal-dual scheme to solve it in a distributed and stochastic manner. With a special non-uniform sampling, a version of NESTT achieves epsilon-stationary solution using O ((Sigma(N)(i=1) root L-i/N)(2)/epsilon) gradient evaluations, which can be up to O (N) times better than the (proximal) gradient descent methods. It also achieves Q-linear convergence rate for nonconvex l(1) penalized quadratic problems with polyhedral constraints. Further, we reveal a fundamental connection between primal-dual based methods and a few primal only methods such as IAG/SAG/SAGA.	[Hajinezhad, Davood; Hong, Mingyi] Iowa State Univ, Dept Ind & Mfg Syst Engn, Ames, IA 50011 USA; [Hajinezhad, Davood; Hong, Mingyi] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA 50011 USA; [Zhao, Tuo] Georgia Inst Technol, Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Wang, Zhaoran] Princeton Univ, Dept Operat Res, Princeton, NJ 08544 USA	Iowa State University; Iowa State University; University System of Georgia; Georgia Institute of Technology; Princeton University	Hong, MY (corresponding author), Iowa State Univ, Dept Ind & Mfg Syst Engn, Ames, IA 50011 USA.; Hong, MY (corresponding author), Iowa State Univ, Dept Elect & Comp Engn, Ames, IA 50011 USA.	dhaji@iastate.edu; mingyi@iastate.edu; tourzhao@gatech.edu; zhaoran@princeton.edu	Hong, Mingyi/H-6274-2013; Wang, Zhaoran/P-7113-2018					Antoniadis A, 2011, ANN I STAT MATH, V63, P585, DOI 10.1007/s10463-009-0242-4; Bertsekas D., 2000, 2848 LIDS; Bjornson E., 2013, FDN TRENDS COMMUNICA, V9; Blatt D, 2007, SIAM J OPTIMIZ, V18, P29, DOI 10.1137/040615961; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Cevher V, 2014, IEEE SIGNAL PROC MAG, V31, P32, DOI 10.1109/MSP.2014.2329397; Chang TH, 2015, IEEE T SIGNAL PROCES, V63, P482, DOI 10.1109/TSP.2014.2367458; Defazio A., 2014, P NIPS; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hajinezhad D., 2015, P GLOBALSIPT; Hajinezhad D, 2016, INT CONF ACOUST SPEE, P4742, DOI 10.1109/ICASSP.2016.7472577; Hazan E., 2016, PREPRINT; Hong MY, 2016, SIAM J OPTIMIZ, V26, P337, DOI 10.1137/140990309; Johnson R., 2013, P NEUR INF PROC NIPS; Lan G., 2015, OPTIMAL RANDOMIZED I; Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018; Lorenzo P. D., 2016, NEXT IN NETWORK NONC; LUO ZQ, 1992, SIAM J CONTROL OPTIM, V30, P408, DOI 10.1137/0330025; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Razaviyayn M., 2014, P NIPS; Reddi S.J., 2016, PREPRINT; SCHMIDT M, 2013, TECHNICAL REPORT; Sra S., 2012, ADV NEURAL INFORM PR; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Zlobec S, 2005, J GLOBAL OPTIM, V32, P401, DOI 10.1007/s10898-004-3134-4	27	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704031
C	Kasaei, SH; Tome, AM; Lopes, LS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hamidreza Kasaei, S.; Tome, Ana Maria; Lopes, Luis Seabra			Hierarchical Object Representation for Open-Ended Object Category Learning and Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODEL	Most robots lack the ability to learn new objects from past experiences. To migrate a robot to a new environment one must often completely re-generate the knowledge-base that it is running with. Since in open-ended domains the set of categories to be learned is not predefined, it is not feasible to assume that one can pre-program all object categories required by robots. Therefore, autonomous robots must have the ability to continuously execute learning and recognition in a concurrent and interleaved fashion. This paper proposes an open-ended 3D object recognition system which concurrently learns both the object categories and the statistical features for encoding objects. In particular, we propose an extension of Latent Dirichlet Allocation to learn structural semantic features (i.e. topics) from low-level feature co-occurrences for each category independently. Moreover, topics in each category are discovered in an unsupervised fashion and are updated incrementally using new object views. The approach contains similarities with the organization of the visual cortex and builds a hierarchy of increasingly sophisticated representations. Results show the fulfilling performance of this approach on different types of objects. Moreover, this system demonstrates the capability of learning from few training examples and competes with state-of-the-art systems.	[Hamidreza Kasaei, S.; Tome, Ana Maria; Lopes, Luis Seabra] Univ Aveiro, IEETA, P-3810193 Aveiro, Portugal	Universidade de Aveiro	Kasaei, SH (corresponding author), Univ Aveiro, IEETA, P-3810193 Aveiro, Portugal.	seyed.hamidreza@ua.pt; ana@ua.pt; lsl@ua.pt	Lopes, Luis Seabra/A-6012-2012	Lopes, Luis Seabra/0000-0002-5719-5019	FCT [PEst-OE/EEI/UI0127/2016]; FCT scholarship [SFRH/BD/94183/2013]	FCT(Portuguese Foundation for Science and TechnologyEuropean Commission); FCT scholarship(Portuguese Foundation for Science and Technology)	This work was funded by National Funds through FCT project PEst-OE/EEI/UI0127/2016 and FCT scholarship SFRH/BD/94183/2013.	Banerjee A., 2007, SDM, V7, P437; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Canini K., 2009, ARTIF INTELL, P65; Chauhan A, 2011, COGN PROCESS, V12, P341, DOI 10.1007/s10339-011-0407-y; Collet A, 2015, INT J ROBOT RES, V34, P3, DOI 10.1177/0278364914546030; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Hyun Lim  Gi, 2014, ROB HUM INT COMM 23; Jeong S, 2012, NEURAL NETWORKS, V25, P130, DOI 10.1016/j.neunet.2011.06.020; Johnson AE, 1999, IEEE T PATTERN ANAL, V21, P433, DOI 10.1109/34.765655; Kasaei SH, 2015, J INTELL ROBOT SYST, V80, P537, DOI 10.1007/s10846-015-0189-z; Kim JG, 2009, VISION RES, V49, P2297, DOI 10.1016/j.visres.2009.06.020; Lai K, 2011, IEEE INT CONF ROBOT, P1817; Mcauliffe Jon D., 2008, P ADV NEURAL INFORM, P121; Oliveira M, 2016, ROBOT AUTON SYST, V75, P614, DOI 10.1016/j.robot.2015.09.019; Ramage D., 2009, P 2009 C EMP METH NA, V1, P248, DOI DOI 10.3115/1699510.1699543; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Schwarz M, 2015, IEEE INT CONF ROBOT, P1329, DOI 10.1109/ICRA.2015.7139363; Sivic J, 2005, IEEE I CONF COMP VIS, P370; WANG CL, 2009, COMP VIS PATT REC 20, V57, P1903, DOI DOI 10.1109/TCOMM.2009.07.070156; Wang Y, 2007, LECT NOTES COMPUT SC, V4814, P240	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704063
C	Han, J; Liu, Q		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Han, Jun; Liu, Qiang			Bootstrap Model Aggregation for Distributed Statistical Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In distributed, or privacy-preserving learning, we are often given a set of probabilistic models estimated from different local repositories, and asked to combine them into a single model that gives efficient statistical estimation. A simple method is to linearly average the parameters of the local models, which, however, tends to be degenerate or not applicable on non-convex models, or models with different parameter dimensions. One more practical strategy is to generate bootstrap samples from the local models, and then learn a joint model based on the combined bootstrap set. Unfortunately, the bootstrap procedure introduces additional noise and can significantly deteriorate the performance. In this work, we propose two variance reduction methods to correct the bootstrap noise, including a weighted M-estimator that is both statistically efficient and practically powerful. Both theoretical and empirical analysis is provided to demonstrate our methods.	[Han, Jun; Liu, Qiang] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA	Dartmouth College	Han, J (corresponding author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.	jun.han.gr@dartmouth.edu; qiang.liu@dartmouth.edu			NSF CRII [1565796]	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work is supported in part by NSF CRII 1565796.	Boyd S., 2011, FDN TRENDS MACHINE L, V3; Dekel O., 2012, JMLR; Henmi M, 2007, BIOMETRIKA, V94, P985, DOI 10.1093/biomet/asm076; HIRANO K, 2003, ECONOMETRICA, V71; Huang Cheng, 2015, ARXIV151101443; Kawakita M., 2013, MACHINE LEARNING, V91; Korattikara Anoop, 2015, ARXIV150604416; Li Lihong, 2015, AISTATS; Liu Q., 2014, NIPS; Merugu S, 2003, THIRD IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P211; Nelson B. L., 1987, COMPUTERS OPERATIONS, V14; Rosenblatt J., 2014, ARXIV14072724; Shamir Ohad, 2014, ICML; Sokolovska N., 2008, ICML; Wilson J. R., 1984, AM J MATH MANAGEMENT, V4; Zhang Y, 2013, NIPS; Zhang Y., 2012, NIPS	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700014
C	Han, SZ; Meng, ZB; Khan, AS; Tong, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Han, Shizhong; Meng, Zibo; Khan, Ahmed Shehab; Tong, Yan			Incremental Boosting Convolutional Neural Network for Facial Action Unit Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recognizing facial action units (AUs) from spontaneous facial expressions is still a challenging problem. Most recently, CNNs have shown promise on facial AU recognition. However, the learned CNNs are often overfitted and do not generalize well to unseen subjects due to limited AU-coded training images. We proposed a novel Incremental Boosting CNN (IB-CNN) to integrate boosting into the CNN via an incremental boosting layer that selects discriminative neurons from the lower layer and is incrementally updated on successive mini-batches. In addition, a novel loss function that accounts for errors from both the incremental boosted classifier and individual weak classifiers was proposed to fine-tune the IB-CNN. Experimental results on four benchmark AU databases have demonstrated that the IB-CNN yields significant improvement over the traditional CNN and the boosting CNN without incremental learning, as well as outperforming the state-of-the-art CNN-based methods in AU recognition. The improvement is more impressive for the AUs that have the lowest frequencies in the databases.	[Han, Shizhong; Meng, Zibo; Khan, Ahmed Shehab; Tong, Yan] Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA	University of South Carolina System; University of South Carolina Columbia	Han, SZ (corresponding author), Univ South Carolina, Dept Comp Sci & Engn, Columbia, SC 29208 USA.	han38@email.sc.edu; mengz@email.sc.edu; akhan@email.sc.edu; tongy@cse.sc.edu	Han, Shizhong/G-4483-2019	Han, Shizhong/0000-0002-3381-6992	National Science Foundation under CAREER Award [IIS-1149787]	National Science Foundation under CAREER Award	This work is supported by National Science Foundation under CAREER Award IIS-1149787.	[Anonymous], 2010, 2010 IEEE COMPUTER S; Asthana A, 2013, PROC CVPR IEEE, P3444, DOI 10.1109/CVPR.2013.442; BALTRUSAITIS T., 2015, 2015 11 IEEE INT C W, V06, P1, DOI [10.1109/FG.2015.7284869, DOI 10.1109/FG.2015.7284869]; BARTLETT MS, 2005, CVPR, P568; Ekman P., 2002, FACIAL ACTION CODING; Fasel B, 2002, FOURTH IEEE INTERNATIONAL CONFERENCE ON MULTIMODAL INTERFACES, PROCEEDINGS, P529, DOI 10.1109/ICMI.2002.1167051; Ghosh Sayan, 2015, ACII; Gudi A., 2015, FG; Han S., 2014, ICIP; Hinton GE, 2012, IMPROVING NEURAL NET; Jaiswal S., 2016, WACV; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jiang BH, 2014, INT C PATT RECOG, P1776, DOI 10.1109/ICPR.2014.312; Jung H, 2015, IEEE I CONF COMP VIS, P2983, DOI 10.1109/ICCV.2015.341; Kanade T., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P46, DOI 10.1109/AFGR.2000.840611; Liu M., 2014, ACCV; Liu P., 2014, CVPR; Lucey S., 2007, FACE RECOGNITION BOO; Mavadati SM, 2013, IEEE T AFFECT COMPUT, V4, P151, DOI 10.1109/T-AFFC.2013.4; Medera D, 2009, IJCCI 2009: PROCEEDINGS OF THE INTERNATIONAL JOINT CONFERENCE ON COMPUTATIONAL INTELLIGENCE, P547; Nagi J, 2012, 2012 11TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2012), VOL 1, P27, DOI 10.1109/ICMLA.2012.14; Rifai S, 2012, LECT NOTES COMPUT SC, V7577, P808, DOI 10.1007/978-3-642-33783-3_58; Sariyanidi E, 2015, IEEE T PATTERN ANAL, V37, P1113, DOI 10.1109/TPAMI.2014.2366127; Sayette MA, 2001, J NONVERBAL BEHAV, V25, P167, DOI 10.1023/A:1010671109788; Song Y., 2015, FG; Tang Y., 2013, ICML; Tong Y, 2007, IEEE T PATTERN ANAL, V29, P1683, DOI 10.1109/TPAMI.2007.1094; Valstar M., 2015, FG; Valstar MF, 2012, IEEE T SYST MAN CY B, V42, P966, DOI 10.1109/TSMCB.2012.2200675; Yang P, 2009, PATTERN RECOGN LETT, V30, P132, DOI 10.1016/j.patrec.2008.03.014; Yuce A., 2015, FG; Zeng ZH, 2009, IEEE T PATTERN ANAL, V31, P39, DOI 10.1109/TPAMI.2008.52; Zhao G., 2007, TPAMI, V29, P915, DOI DOI 10.1109/TPAMI.2007.1110	33	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702005
C	Hartford, J; Wright, JR; Leyton-Brown, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hartford, Jason; Wright, James R.; Leyton-Brown, Kevin			Deep Learning for Predicting Human Strategic Behavior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Predicting the behavior of human participants in strategic settings is an important problem in many domains. Most existing work either assumes that participants are perfectly rational, or attempts to directly model each participant's cognitive processes based on insights from cognitive psychology and experimental economics. In this work, we present an alternative, a deep learning approach that automatically performs cognitive modeling without relying on such expert knowledge. We introduce a novel architecture that allows a single network to generalize across different input and output dimensions by using matrix units rather than scalar units, and show that its performance significantly outperforms that of the previous state of the art, which relies on expert-constructed features.	[Hartford, Jason; Wright, James R.; Leyton-Brown, Kevin] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada	University of British Columbia	Hartford, J (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.	jasonhar@cs.ubc.ca; jrwright@cs.ubc.ca; kevinlb@cs.ubc.ca						Bengio Y., 2013, IEEE T PATTERN ANAL, V35; Camerer C.F., 2003, BEHAV GAME THEORY EX, DOI 10.1257/jep.9.2.209; Camerer C. F., 2004, Q J EC, V119; Clark C., 2015, P 32 INT C MACH LEAR; Costa-Gomes M, 2001, ECONOMETRICA, V69, P1193, DOI 10.1111/1468-0262.00239; Edelman B., 2007, AM ECON REV, V97, P1, DOI DOI 10.1257/AER.97.1.242; GOLDSTEIN AA, 1964, B AM MATH SOC, V70, P709, DOI 10.1090/S0002-9904-1964-11178-2; Kingma DP, 2015, INT C LEARN REPR ICL; LeCun Y. A., 2015, NATURE; Lin M., 2014, P INT C LEARN REPR I; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023; Milgrom P., 2014, P 15 ACM C EC COMP; Parkes DC, 2015, SCIENCE, V349, P267, DOI 10.1126/science.aaa8403; Schmidhuber Jurgen, 2015, NEURAL NETWORKS; Shoham Y., 2008, MULTIAGENT SYSTEMS A, DOI DOI 10.1017/CBO9780511811654; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stahl D. O., 1994, JEBO, V25; Stahl D. O., 2014, P 15 ACM C EC COMP, P857; Tambe M., 2011, SECURITY GAME THEORY; Varian H.R., 2007, INT J IND ORG, V25; Wright J.R., 2012, AAMAS 12 PRODEECINGS, V2, P921; Wright J. R., 2010, AAAI; Yang R., 2013, ARTIFICIAL INTELLIGE	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704034
C	Hayashi, K; Yoshida, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hayashi, Kohei; Yoshida, Yuichi			Minimizing Quadratic Functions in Constant Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION ALGORITHMS	A sampling-based optimization method for quadratic functions is proposed. Our method approximately solves the following n-dimensional quadratic minimization problem in constant time, which is independent of n : z* = min(v is an element of Rn) < v, Av > + n < v, diag(d)v > + n < b, v >, where A is an element of R-nxn is a matrix and d, b is an element of R-n are vectors. Our theoretical analysis specifies the number of samples k(delta, epsilon) such that the approximated solution z satisfies vertical bar z - z*vertical bar = O(epsilon n(2)) with probability 1-delta. The empirical performance (accuracy and runtime) is positively confirmed by numerical experiments.	[Hayashi, Kohei] Natl Inst Adv Ind Sci & Technol, Tokyo, Japan; [Yoshida, Yuichi] Natl Inst Informat & Preferred Infrastruct Inc, Tokyo, Japan	National Institute of Advanced Industrial Science & Technology (AIST)	Hayashi, K (corresponding author), Natl Inst Adv Ind Sci & Technol, Tokyo, Japan.	hayashi.kohei@gmail.com; yyoshida@nii.ac.jp			MEXT KAKENHI [15K16055]; MEXT [24106001]; JST, CREST; Foundations of Innovative Algorithms for Big Data; JST, ERATO	MEXT KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); MEXT(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)); JST, CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST)); Foundations of Innovative Algorithms for Big Data; JST, ERATO(Japan Science & Technology Agency (JST))	We would like to thank Makoto Yamada for suggesting a motivating problem of our method. K. H. is supported by MEXT KAKENHI 15K16055. Y.Y. is supported by MEXT Grant-in-Aid for Scientific Research on Innovative Areas (No. 24106001), JST, CREST, Foundations of Innovative Algorithms for Big Data, and JST, ERATO, Kawarabayashi Large Graph Project.	Alon N., 2002, P 34 S THEOR COMP, P232; Alon N, 2009, SIAM J COMPUT, V39, P143, DOI 10.1137/060667177; Borgs C., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P261, DOI 10.1145/1132516.1132556; Bottou L, 2004, LECT NOTES ARTIF INT, V3176, P146; Brattka V, 1998, J COMPLEXITY, V14, P490, DOI 10.1006/jcom.1998.0488; Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658; Frieze A, 1996, AN S FDN CO, P12, DOI 10.1109/SFCS.1996.548459; Goldreich O, 1998, J ACM, V45, P653, DOI 10.1145/285055.285060; Lovasz L., 2012, LARGE NETWORKS GRAPH; Lovasz L, 2013, COMB PROBAB COMPUT, V22, P749, DOI 10.1017/S0963548313000205; Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002; MATHIEU C, 2008, SODA, P176; Nguyen HN, 2008, ANN IEEE SYMP FOUND, P327, DOI 10.1109/FOCS.2008.81; Onak Krzysztof, 2012, P 23 ANN ACM SIAM S, P1123; Rubinfeld R, 1996, SIAM J COMPUT, V25, P252, DOI 10.1137/S0097539793255151; Sugiyama M, 2012, DENSITY RATIO ESTIMATION IN MACHINE LEARNING, P1, DOI 10.1017/CBO9781139035613; Suzuki T, 2011, NEURAL COMPUT, V23, P284, DOI 10.1162/NECO_a_00062; Williams C. K. I., 2001, NIPS; Yamada M., 2011, NIPS; Yoshida Y., 2016, P 27 ANN ACM SIAM S, P1391; Yoshida Y, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P154, DOI 10.1145/2591796.2591802; Yoshida Y, 2012, SIAM J COMPUT, V41, P1074, DOI 10.1137/110828691; Yoshida Y, 2011, ACM S THEORY COMPUT, P665	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700009
C	Hazan, E; Ma, TY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hazan, Elad; Ma, Tengyu			A Non-generative Framework and Convex Relaxations for Unsupervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				COMPRESSION	We give a novel formal theoretical framework for unsupervised learning with two distinctive characteristics. First, it does not assume any generative model and based on a worst-case performance metric. Second, it is comparative, namely performance is measured with respect to a given hypothesis class. This allows to avoid known computational hardness results and improper algorithms based on convex relaxations. We show how several families of unsupervised learning models, which were previously only analyzed under probabilistic assumptions and are otherwise provably intractable, can be efficiently learned in our framework by convex optimization.	[Hazan, Elad; Ma, Tengyu] Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA	Princeton University	Hazan, E (corresponding author), Princeton Univ, 35 Olden St, Princeton, NJ 08540 USA.	ehazan@cs.princeton.edu; tengyu@cs.princeton.edu						Acharya J, 2013, IEEE INT SYMP INFO, P2875, DOI 10.1109/ISIT.2013.6620751; Aharon M., 2005, P SPARS, P9, DOI DOI 10.1109/TSP.2006.881199; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 2015, PROC C LEARN THEORY; Arora S., 2013, ARXIV13086273; Balcan MF, 2008, ACM S THEORY COMPUT, P671; Barak B, 2015, ACM S THEORY COMPUT, P143, DOI 10.1145/2746539.2746605; Ben-David S., 2009, ADV NEURAL INFORM PR, V21, P121; Berger T., 1971, RATE DISTORTION THEO; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cover T.M, 2006, WILEY SERIES TELECOM; Donoho DL, 2001, IEEE T INFORM THEORY, V47, P2845, DOI 10.1109/18.959265; Hazan E., 2012, P 29 INT C MACH LEAR; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Jevtic N, 2005, THEOR COMPUT SCI, V332, P293, DOI 10.1016/j.tcs.2004.10.038; Kleinberg J., 2003, ADV NEURAL INFORM PR, P463; Livni R., 2013, P 30 INT C MACH LEAR; Mohri M., 2018, FDN MACHINE LEARNING; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Orlitsky A, 2004, IEEE T INFORM THEORY, V50, P1469, DOI 10.1109/TIT.2004.830761; Paskov S., 2013, NIPS, V26, P2931; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Salakhutdinov R., 2009, THESIS; Shamir  Ohad, 2008, LEARNING GEN INFORM, P92; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tishby  Naftali, 2000, PHYS0004057 CORR; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V.N, 1998, STAT LEARNING THEORY	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704057
C	He, XR; Xu, K; Kempe, D; Liu, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		He, Xinran; Xu, Ke; Kempe, David; Liu, Yan			Learning Influence Functions from Incomplete Observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of learning influence functions under incomplete observations of node activations. Incomplete observations are a major concern as most (online and real-world) social networks are not fully observable. We establish both proper and improper PAC learnability of influence functions under randomly missing observations. Proper PAC learnability under the Discrete-Time Linear Threshold (DLT) and Discrete-Time Independent Cascade (DIC) models is established by reducing incomplete observations to complete observations in a modified graph. Our improper PAC learnability result applies for the DLT and DIC models as well as the Continuous-Time Independent Cascade (CIC) model. It is based on a parametrization in terms of reachability features, and also gives rise to an efficient and practical heuristic. Experiments on synthetic and real-world datasets demonstrate the ability of our method to compensate even for a fairly large fraction of missing observations.	[He, Xinran; Xu, Ke; Kempe, David; Liu, Yan] Univ Southern Calif, Los Angeles, CA 90089 USA	University of Southern California	He, XR (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	xinranhe@usc.edu; xuk@usc.edu; dkempe@usc.edu; yanliu.cs@usc.edu			NSF [IIS-1254206]; U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program [W911NF-12-1-0034]	NSF(National Science Foundation (NSF)); U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We would like to thank anonymous reviewers for useful feedback. The research was sponsored in part by NSF research grant IIS-1254206 and by the U.S. Defense Advanced Research Projects Agency (DARPA) under the Social Media in Strategic Communication (SMISC) program, Agreement Number W911NF-12-1-0034. The views and conclusions are those of the authors and should not be interpreted as representing the official policies of the funding agency or the U.S. Government.	Amin K., 2014, P 31 INT C MACH LEAR, P1845; CHIERICHETTI F, 2011, NIPS, P792; Du N, 2014, PR MACH LEARN RES, V32, P2016; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; Du Nan, 2012, NIPS, P2780; Gomez-Rodriguez M, 2012, ACM T KNOWL DISCOV D, V5, DOI 10.1145/2086737.2086741; Gomez-Rodriguez Manuel, 2011, ICML, P561; Goyal A., 2010, P WSDM 10, P241, DOI [10.1145/1718487.1718518, DOI 10.1145/1718487.1718518]; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Leskovec J, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P497; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Lokhov A., 2016, P 29 C NEUR INF PROC, P3459; Myers Seth, 2010, P INT C ADV NEURAL I, P1741; Myers Seth A., 2012, P 18 ACM SIGKDD INT, P33, DOI [10.1145/2339530.2339540, DOI 10.1145/2339530.2339540]; Narasimhan H., 2015, P 27 NIPS, P3168; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783; Quang Duong, 2011, Proceedings of the 2011 IEEE Third International Conference on Privacy, Security, Risk and Trust and IEEE Third International Conference on Social Computing (PASSAT/SocialCom 2011), P362, DOI 10.1109/PASSAT/SocialCom.2011.50; Rosenfeld N, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P563, DOI 10.1145/2835776.2835802; Sadikov E., 2011, P 4 ACM INT C WEB SE, P55, DOI DOI 10.1145/1935826.1935844; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Wu X., 2013, INT JOINT C ART INT, P2923; Yang Shuang-Hong, 2013, P 30 INT C MACH LEAR, V28; Zhou K., 2013, ARTIF INTELL, V31, P641	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701038
C	Hegde, C; Indyk, P; Schmidt, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hegde, Chinmay; Indyk, Piotr; Schmidt, Ludwig			Fast recovery from a union of subspaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS	We address the problem of recovering a high-dimensional but structured vector from linear observations in a general setting where the vector can come from an arbitrary union of subspaces. This setup includes well-studied problems such as compressive sensing and low-rank matrix recovery. We show how to design more efficient algorithms for the union-of-subspace recovery problem by using approximate projections. Instantiating our general framework for the low-rank matrix recovery problem gives the fastest provable running time for an algorithm with optimal sample complexity. Moreover, we give fast approximate projections for 2D histograms, another well-studied low-dimensional model of data. We complement our theoretical results with experiments demonstrating that our framework also leads to improved time and sample complexity empirically.	[Hegde, Chinmay] Iowa State Univ, Ames, IA 50011 USA; [Indyk, Piotr; Schmidt, Ludwig] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Iowa State University; Massachusetts Institute of Technology (MIT)	Hegde, C (corresponding author), Iowa State Univ, Ames, IA 50011 USA.							[Anonymous], 2015, ARXIV150903025; Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894; Becker Stephen, 2013, SAMPTA C SAMPL THEOR; Bhojanapalli Srinadh, 2015, 150903917 ARXIV; Blumensath T, 2011, IEEE T INFORM THEORY, V57, P4660, DOI 10.1109/TIT.2011.2146550; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Cormode G, 2011, FOUND TRENDS DATABAS, V4, P1, DOI 10.1561/1900000004; Davenport Mark, 2016, 160106422 ARXIV; Gilbert A., 2002, STOC; Gilbert A. C., 2001, Proceedings of the 27th International Conference on Very Large Data Bases, P79; Giryes R, 2015, APPL COMPUT HARMON A, V39, P1, DOI 10.1016/j.acha.2014.07.004; Hegde C, 2015, IEEE T INFORM THEORY, V61, P5129, DOI 10.1109/TIT.2015.2457939; Ioannidis Yannis E., 2003, VLDB, P19; Jain P., 2010, NIPS; Larsen R. M., PROPACK; Musco Cameron, 2015, NIPS; Muthukrishnan S, 1999, LECT NOTES COMPUT SC, V1540, P236; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; SAAD Y, 1980, SIAM J NUMER ANAL, V17, P687, DOI 10.1137/0717059; Stephen Tu, 2016, ICML; Thaper N., 2002, SIGMOD; Zhao Tuo, NONCONVEX LOW RANK M; Zheng Q., 2015, NIPS	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704009
C	Heller, R; Heller, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Heller, Ruth; Heller, Yair			Multivariate tests of association based on univariate tests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					For testing two vector random variables for independence, we propose testing whether the distance of one vector from an arbitrary center point is independent from the distance of the other vector from another arbitrary center point by a univariate test. We prove that under minimal assumptions, it is enough to have a consistent univariate independence test on the distances, to guarantee that the power to detect dependence between the random vectors increases to one with sample size. If the univariate test is distribution-free, the multivariate test will also be distribution-free. If we consider multiple center points and aggregate the center-specific univariate tests, the power may be further improved, and the resulting multivariate test may have a distribution-free critical value for specific aggregation methods (if the univariate test is distribution free). We show that certain multivariate tests recently proposed in the literature can be viewed as instances of this general approach. Moreover, we show in experiments that novel tests constructed using our approach can have better power and computational time than competing approaches.	[Heller, Ruth] Tel Aviv Univ, Dept Stat & Operat Res, IL-6997801 Tel Aviv, Israel	Tel Aviv University	Heller, R (corresponding author), Tel Aviv Univ, Dept Stat & Operat Res, IL-6997801 Tel Aviv, Israel.	ruheller@gmail.com; heller.yair@gmail.com						Benjamini Y, 2001, ANN STAT, V29, P1165; Chwialkowski K, 2015, ADV NEUR IN, V28; Cuesta-Albertos JA, 2006, B BRAZ MATH SOC, V37, P477, DOI 10.1007/s00574-006-0023-0; GRETTON A, 2007, ADV NEURAL INFORM PR, V19; Gretton A., 2008, ADV NEURAL INFORM PR, P585; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hall P, 2002, BIOMETRIKA, V89, P359, DOI 10.1093/biomet/89.2.359; Heller R, 2016, J MACH LEARN RES, V17; Heller R, 2013, BIOMETRIKA, V100, P503, DOI 10.1093/biomet/ass070; HENZE N, 1988, ANN STAT, V16, P772, DOI 10.1214/aos/1176350835; HOEFFDING W, 1948, ANN MATH STAT, V19, P546, DOI 10.1214/aoms/1177730150; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; Hommel G., 1983, BIOMETR J, V25, P423, DOI [10.1002/bimj.19830250502, DOI 10.1002/BIMJ.19830250502]; Hotelling H, 1931, ANN MATH STAT, V2, P360, DOI 10.1214/aoms/1177732979; Kolmogoroff A, 1941, ANN MATH STAT, V12, P461, DOI 10.1214/aoms/1177731684; Maa JF, 1996, ANN STAT, V24, P1069; PETTITT AN, 1976, BIOMETRIKA, V63, P161, DOI 10.1093/biomet/63.1.161; Rawat R, 2000, J FOURIER ANAL APPL, V6, P343, DOI 10.1007/BF02511160; Rosenbaum PR, 2005, J ROY STAT SOC B, V67, P515, DOI 10.1111/j.1467-9868.2005.00513.x; SCHILLING MF, 1986, J AM STAT ASSOC, V81, P799, DOI 10.2307/2289012; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Strathmann H., 2012, P ADV NEUR INF PROC, P1205, DOI DOI 10.5555/2999134.2999269; SZEKELY G., 2004, INTERSTAT; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Thas O, 2004, COMMUN STAT-SIMUL C, V33, P711, DOI 10.1081/SAC-200033335; Wei S, 2016, J COMPUT GRAPH STAT, V25, P549, DOI 10.1080/10618600.2015.1027773	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701077
C	Herbster, M; Pasteris, S; Pontil, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Herbster, Mark; Pasteris, Stephen; Pontil, Massimiliano			Mistake Bounds for Binary Matrix Completion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of completing a binary matrix in an online learning setting. On each trial we predict a matrix entry and then receive the true entry. We propose a Matrix Exponentiated Gradient algorithm [1] to solve this problem. We provide a mistake bound for the algorithm, which scales with the margin complexity [2, 3] of the underlying matrix. The bound suggests an interpretation where each row of the matrix is a prediction task over a finite set of objects, the columns. Using this we show that the algorithm makes a number of mistakes which is comparable up to a logarithmic factor to the number of mistakes made by the Kernel Perceptron with an optimal kernel in hindsight. We discuss applications of the algorithm to predicting as well as the best biclustering and to the problem of predicting the labeling of a graph without knowing the graph in advance.	[Herbster, Mark; Pasteris, Stephen; Pontil, Massimiliano] UCL, Dept Comp Sci, London WC1E 6BT, England; [Pontil, Massimiliano] Ist Italiano Tecnol, I-16163 Genoa, Italy	University of London; University College London; Istituto Italiano di Tecnologia - IIT	Herbster, M (corresponding author), UCL, Dept Comp Sci, London WC1E 6BT, England.	m.herbster@cs.ucl.ac.uk; s.pasteris@cs.ucl.ac.uk; m.pontil@cs.ucl.ac.uk			EPSRC [EP/P009069/1, EP/M006093/1]; U.S. Army Research Laboratory; U.K. Defence Science and Technology Laboratory [W911NF-16-3-0001]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); U.S. Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); U.K. Defence Science and Technology Laboratory	We wish to thank the anonymous reviewers for their useful comments. This work was supported in part by EPSRC Grants EP/P009069/1, EP/M006093/1, and by the U.S. Army Research Laboratory and the U.K. Defence Science and Technology Laboratory and was accomplished under Agreement Number W911NF-16-3-0001. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, ether expressed or implied, of the U.S. Army Research Laboratory, the U.S. Government, the U.K. Defence Science and Technology Laboratory or the U.K. Government. The U.S. and U.K. Governments are authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation herein.	Alquier P., 2016, PREPRINT; Arora S, 2007, ACM S THEORY COMPUT, P227, DOI 10.1145/1250790.1250823; Balcan, 2015, C LEARN THEOR, P191; Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e; Ben-David S., 2003, Journal of Machine Learning Research, V3, P441, DOI 10.1162/153244303321897681; Bhatia Rajendra, 1997, MATRIX ANAL, DOI 10.1007/978-1-4612-0653-8; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Cesa-Bianchi N., 2011, P ADV NEURAL INFORM, P343; Cesa-Bianchi N, 2013, J MACH LEARN RES, V14, P1251; Cesa-Bianchi N, 2011, THEOR COMPUT SCI, V412, P1791, DOI 10.1016/j.tcs.2010.12.056; Gentile C., 2013, P 26 ANN C LEARN THE; Goldman S. A., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P453, DOI 10.1145/168304.168396; Goldman S. A., 1993, SIAM J COMPUT, V22; Hazan E., 2012, P 23 ANN C LEARN THE, V23; Herbster M, 2005, ACM INT C PROCEEDING, V119, P305; Herbster M, 2006, ADV NEURAL INFORM PR, V19, P577; Herbster M, 2015, J MACH LEARN RES, V16, P2003; Lugosi G., 2009, P 22 ANN C LEARN THE; Nie JZ, 2013, LECT NOTES ARTIF INT, V8139, P98; Novikoff, 1962, P S MATH THEOR AUT, P615; Pontil M., 2013, PROC ANN C LEARN THE, P55; Warmuth M. K., 2007, P 24 INT C MACH LEAR, P999; Warmuth MK, 2012, MACH LEARN, V87, P1, DOI 10.1007/s10994-011-5269-0; Wulff S., 2013, P 30 INT C MACH LEAR, P145; Zhu Xiaojin., 2003, P ICLR, P912	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704091
C	Herreros-Alonso, I; Arsiwalla, XD; Verschure, PFMJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Herreros-Alonso, Ivan; Arsiwalla, Xerxes D.; Verschure, Paul F. M. J.			A Forward Model at Purkinje Cell Synapses Facilitates Cerebellar Anticipatory Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				LEARNING CONTROL; MOVEMENT; TIME	How does our motor system solve the problem of anticipatory control in spite of a wide spectrum of response dynamics from different musculo-skeletal systems, transport delays as well as response latencies throughout the central nervous system? To a great extent, our highly-skilled motor responses are a result of a reactive feedback system, originating in the brain-stem and spinal cord, combined with a feed-forward anticipatory system, that is adaptively fine-tuned by sensory experience and originates in the cerebellum. Based on that interaction we design the counterfactual predictive control (CFPC) architecture, an anticipatory adaptive motor control scheme in which a feed-forward module, based on the cerebellum, steers an error feedback controller with counterfactual error signals. Those are signals that trigger reactions as actual errors would, but that do not code for any current or forthcoming errors. In order to determine the optimal learning strategy, we derive a novel learning rule for the feed-forward module that involves an eligibility trace and operates at the synaptic level. In particular, our eligibility trace provides a mechanism beyond co-incidence detection in that it convolves a history of prior synaptic inputs with error signals. In the context of cerebellar physiology, this solution implies that Purkinje cell synapses should generate eligibility traces using a forward model of the system being controlled. From an engineering perspective, CFPC provides a general-purpose anticipatory control architecture equipped with a learning rule that exploits the full dynamics of the closed-loop system.	[Herreros-Alonso, Ivan; Arsiwalla, Xerxes D.] Univ Pompeu Fabra, SPECS Lab, Barcelona, Spain; [Verschure, Paul F. M. J.] UPF, SPECS, Catalan Inst Res & Adv Studies ICREA, Barcelona, Spain	Pompeu Fabra University; ICREA; Pompeu Fabra University	Herreros-Alonso, I (corresponding author), Univ Pompeu Fabra, SPECS Lab, Barcelona, Spain.	ivan.herreros@upf.edu			European Commission's Horizon 2020 socSMC project [socSMC-641321H2020-FETPROACT-2014]; European Research Council's CDAC project [ERC-2013-ADG 341196]	European Commission's Horizon 2020 socSMC project; European Research Council's CDAC project	The research leading to these results has received funding from the European Commission's Horizon 2020 socSMC project (socSMC-641321H2020-FETPROACT-2014) and by the European Research Council's CDAC project (ERC-2013-ADG 341196).	ALBUS J S, 1971, Mathematical Biosciences, V10, P25, DOI 10.1016/0025-5564(71)90051-4; Amann N, 1996, IEE P-CONTR THEOR AP, V143, P217, DOI 10.1049/ip-cta:19960244; Apps R, 2005, NAT REV NEUROSCI, V6, P297, DOI 10.1038/nrn1646; Astrom KJ., 2012, FEEDBACK SYSTEMS INT; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Bastian AJ, 2006, CURR OPIN NEUROBIOL, V16, P645, DOI 10.1016/j.conb.2006.08.016; Bengtsson F, 2006, CEREBELLUM, V5, P7, DOI 10.1080/14734220500462757; Benna M. K., 2016, NATURE NEUROSCIENCE; De Zeeuw CI, 2005, CURR OPIN NEUROBIOL, V15, P667, DOI 10.1016/j.conb.2005.10.008; Dean P, 2010, NAT REV NEUROSCI, V11, P30, DOI 10.1038/nrn2756; Eccles JC, 1967, CEREBELLUM NEURONAL; FUJITA M, 1982, BIOL CYBERN, V45, P195, DOI 10.1007/BF00336192; Gormezano I., 1983, 20 YEARS CLASSICAL C; Herreros I, 2013, NEURAL NETWORKS, V47, P64, DOI 10.1016/j.neunet.2013.01.026; Hesslow Germund, 2002, P86; Hofstotter C, 2002, EUR J NEUROSCI, V16, P1361, DOI 10.1046/j.1460-9568.2002.02182.x; Jordan M. I., 1996, HDB PERCEPTION ACTIO, V2, P71, DOI DOI 10.1016/S1874-5822(06)80005-8; KAWATO M, 1987, BIOL CYBERN, V57, P169, DOI 10.1007/BF00364149; Kettner RE, 1997, J NEUROPHYSIOL, V77, P2115, DOI 10.1152/jn.1997.77.4.2115; Lahiri S, 2013, ADV NEURAL INF PROCE, P1034; Lall S., 2008, ONLINE LECT NOTES; LISBERGER SG, 1987, ANNU REV NEUROSCI, V10, P97, DOI 10.1146/annurev.ne.10.030187.000525; MARR D, 1969, J PHYSIOL-LONDON, V202, P437, DOI 10.1113/jphysiol.1969.sp008820; MASSION J, 1992, PROG NEUROBIOL, V38, P35, DOI 10.1016/0301-0082(92)90034-C; McKinstry JL, 2006, P NATL ACAD SCI USA, V103, P3387, DOI 10.1073/pnas.0511281103; Porrill J, 2007, NEURAL COMPUT, V19, P170, DOI 10.1162/neco.2007.19.1.170; Shibata T, 2001, IROS 2001: PROCEEDINGS OF THE 2001 IEEE/RJS INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P278, DOI 10.1109/IROS.2001.973371; Suvrathan A, 2016, NEURON, V92, P959, DOI 10.1016/j.neuron.2016.10.022; Wang SSH, 2000, NAT NEUROSCI, V3, P1266, DOI 10.1038/81792	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701008
C	Ho, CJ; Frongillo, R; Chen, YL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ho, Chien-Ju; Frongillo, Rafael; Chen, Yiling			Eliciting Categorical Data for Optimal Aggregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PREDICTION	Models for collecting and aggregating categorical data on crowdsourcing platforms typically fall into two broad categories: those assuming agents honest and consistent but with heterogeneous error rates, and those assuming agents strategic and seek to maximize their expected reward. The former often leads to tractable aggregation of elicited data, while the latter usually focuses on optimal elicitation and does not consider aggregation. In this paper, we develop a Bayesian model, wherein agents have differing quality of information, but also respond to incentives. Our model generalizes both categories and enables the joint exploration of optimal elicitation and aggregation. This model enables our exploration, both analytically and experimentally, of optimal aggregation of categorical data and optimal multiple-choice interface design.	[Ho, Chien-Ju] Cornell Univ, Ithaca, NY 14853 USA; [Frongillo, Rafael] CU Boulder, Boulder, CO USA; [Chen, Yiling] Harvard Univ, Cambridge, MA 02138 USA	Cornell University; University of Colorado System; University of Colorado Boulder; Harvard University	Ho, CJ (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	ch624@cornell.edu; raf@colorado.edu; yiling@seas.harvard.edu			NSF [CCF-1512964, CCF-1301976]; ONR [N00014-15-1-2335]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	thank the anonymous reviewers for their helpful comments. This research was partially supported by NSF grant CCF-1512964, NSF grant CCF-1301976, and ONR grant N00014-15-1-2335.	Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Cholleti Sharath R., 2008, P 20 IEEE INT C TOOL; Dawid A. P., 1979, APPL STAT, V28, P20, DOI DOI 10.2307/2346806; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FRONGILLO R., 2015, JMLR WORKSHOP C P, V40, P1; Frongillo R. M., 2015, 29 AAAI C ART INT; Frongillo R, 2014, LECT NOTES COMPUT SC, V8877, P354, DOI 10.1007/978-3-319-13129-0_29; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Ho C., 2013, 30 INT C MACH LEARN; Ipeirotis P., 2014, DATA MINING KNOWLEDG; Jin R., 2003, ADV NEURAL INFORM PR, V15, P897; Karger David R., 2011, P 49 ANN C COMM CONT; Karger David R., 2011, 25 ANN C NEUR INF PR; LAMBERT N., 2009, EC 09, P109; Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129; Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/mnsc.1050.0379; Prelec D, 2004, SCIENCE, V306, P462, DOI 10.1126/science.1102081; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; Shah N. B., 2015, NEURAL INFORM PROCES; Sheng V., 2008, ACM SIGKDD C KNOWL D; Whitehill J., 2009, ADV NEURAL INFORM PR, P2035; Witkowski J., 2012, P 26 AAAI C ART INT; Zou J., 2012, P WORKSH MACH LEARN	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701094
C	Ho, MK; Littman, ML; MacGlashan, J; Cushman, F; Austerweil, JL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ho, Mark K.; Littman, Michael L.; MacGlashan, James; Cushman, Fiery; Austerweil, Joseph L.			Showing versus Doing: Teaching by Demonstration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					People often learn from others' demonstrations, and inverse reinforcement learning (IRL) techniques have realized this capacity in machines. In contrast, teaching by demonstration has been less well studied computationally. Here, we develop a Bayesian model for teaching by demonstration. Stark differences arise when demonstrators are intentionally teaching (i.e. showing) a task versus simply performing (i.e. doing) a task. In two experiments, we show that human participants modify their teaching behavior consistent with the predictions of our model. Further, we show that even standard IRL algorithms benefit when learning from showing versus doing.	[Ho, Mark K.] Brown Univ, Dept Cognit Linguist & Psychol Sci, Providence, RI 02912 USA; [Littman, Michael L.; MacGlashan, James] Brown Univ, Dept Comp Sci, Providence, RI 02912 USA; [Cushman, Fiery] Harvard Univ, Dept Psychol, Cambridge, MA 02138 USA; [Austerweil, Joseph L.] Univ Wisconsin, Dept Psychol, 1202 W Johnson St, Madison, WI 53706 USA	Brown University; Brown University; Harvard University; University of Wisconsin System; University of Wisconsin Madison	Ho, MK (corresponding author), Brown Univ, Dept Cognit Linguist & Psychol Sci, Providence, RI 02912 USA.	mark_ho@brown.edu; mlittman@cs.brown.edu; james_macglashan@brown.edu; cushman@fas.harvard.edu; austerweil@wisc.edu	Ho, Mark/AGI-5014-2022; Ho, Mark/V-8928-2019	Ho, Mark/0000-0002-1454-4768; Ho, Mark/0000-0002-1454-4768	NSF GRFP [DGE-1058262]; DARPA SIMPLEX program [14-46-FP-097, N00014-14-1-0800]; Office of Naval Research	NSF GRFP(National Science Foundation (NSF)NSF - Office of the Director (OD)); DARPA SIMPLEX program; Office of Naval Research(Office of Naval Research)	MKH was supported by the NSF GRFP under Grant No. DGE-1058262. JLA and MLL were supported by DARPA SIMPLEX program Grant No. 14-46-FP-097. FC was supported by grant N00014-14-1-0800 from the Office of Naval Research.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Austerweil JL, 2013, PSYCHOL REV, V120, P817, DOI 10.1037/a0034194; BabeVroman M., 2011, P 28 INT C MACHINE L, P897; Baker CL, 2009, COGNITION, V113, P329, DOI 10.1016/j.cognition.2009.07.005; Buchsbaum D, 2011, COGNITION, V120, P331, DOI 10.1016/j.cognition.2010.12.001; Butler LP, 2014, COGNITION, V130, P116, DOI 10.1016/j.cognition.2013.10.002; Cakmak M., 2012, AAAI C ART INT AAAI; Dragan AD, 2013, ACMIEEE INT CONF HUM, P301, DOI 10.1109/HRI.2013.6483603; Frank MC, 2012, SCIENCE, V336, P998, DOI 10.1126/science.1218633; Hadfield-Menell D., 2016, ADV NEURAL INFORM PR, V28; MacGlashan J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3692; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Shafto P, 2014, COGNITIVE PSYCHOL, V71, P55, DOI 10.1016/j.cogpsych.2013.12.004; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703049
C	Hoiles, W; van der Schaar, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hoiles, William; van der Schaar, Mihaela			A Non-parametric Learning Method for Confidently Estimating Patient's Clinical State and Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Estimating patient's clinical state from multiple concurrent physiological streams plays an important role in determining if a therapeutic intervention is necessary and for triaging patients in the hospital. In this paper we construct a non-parametric learning algorithm to estimate the clinical state of a patient. The algorithm addresses several known challenges with clinical state estimation such as eliminating the bias introduced by therapeutic intervention censoring, increasing the timeliness of state estimation while ensuring a sufficient accuracy, and the ability to detect anomalous clinical states. These benefits are obtained by combining the tools of non-parametric Bayesian inference, permutation testing, and generalizations of the empirical Bernstein inequality. The algorithm is validated using real-world data from a cancer ward in a large academic hospital.	[Hoiles, William; van der Schaar, Mihaela] Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA	University of California System; University of California Los Angeles	Hoiles, W (corresponding author), Univ Calif Los Angeles, Dept Elect Engn, Los Angeles, CA 90024 USA.	whoiles@ucla.edu; mihaela@ee.ucla.edu			Airforce DDDAS program [NSF ECCS 1462245]	Airforce DDDAS program	This research was supported by: NSF ECCS 1462245, and the Airforce DDDAS program.	[Anonymous], P NEUR INF PROC SYST; Bartlett MS, 1937, PROC R SOC LON SER-A, V160, P0268, DOI 10.1098/rspa.1937.0109; Basso D., 2009, PERMUTATION TESTS; Beal MJ, 2001, ADV NEURAL INFORM PR, P577; Bilodeau M., 2008, THEORY MULTIVARIATE; Brockwell P. J., 2013, TIME SERIES THEORY M; Bryant M, 2012, P ADV NEUR INF PROC, V25, P2699; Campbell T., 2015, ADV NEURAL INFORM PR, P280; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Fox E. B., 2008, 25 INT C MACHINE LEA, P312; Gelman A., 2014, BAYESIAN DATA ANAL, V2; Johnson AEW, 2016, P IEEE, V104, P444, DOI 10.1109/JPROC.2015.2501978; Maurer A., 2009, COLT; Montanez GD, 2015, AAAI CONF ARTIF INTE, P1819; Muirhead R.J., 1982, ASPECTS MULTIVARIATE; Paxton C., 2012, ANN S P AMIA S AMIA, V2013, P1109; Rothman MJ, 2013, J BIOMED INFORM, V46, P837, DOI 10.1016/j.jbi.2013.06.011; Schott JR, 2007, COMPUT STAT DATA AN, V51, P6535, DOI 10.1016/j.csda.2007.03.004; Subbe CP, 2001, QJM-MON J ASSOC PHYS, V94, P521, DOI 10.1093/qjmed/94.10.521; Teh Y. W., 2012, J AM STAT ASS; Tenreiro C, 2011, COMPUT STAT DATA AN, V55, P1980, DOI 10.1016/j.csda.2010.12.004; Timm N., 2002, APPL MULTIVARIATE AN, V1; Van Gael J., 2008, PROC 25 INT C MACHIN, V25, P1088; Willsky A. S., 2009, ADV NEURAL INFORM PR, P549	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703090
C	Hosseini, MJ; Lee, SI		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hosseini, Mohammad Javad; Lee, Su-In			Learning Sparse Gaussian Graphical Models with Overlapping Blocks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REGULATORS	We present a novel framework, called GRAB (GRaphical models with overlApping Blocks), to capture densely connected components in a network estimate. GRAB takes as input a data matrix of p variables and n samples and jointly learns both a network of the p variables and densely connected groups of variables (called 'blocks'). GRAB has four major novelties as compared to existing network estimation methods: 1) It does not require blocks to be given a priori. 2) Blocks can overlap. 3) It can jointly learn a network structure and overlapping blocks. 4) It solves a joint optimization problem with the block coordinate descent method that is convex in each step. We show that GRAB reveals the underlying network structure substantially better than four state-of-the-art competitors on synthetic data. When applied to cancer gene expression data, GRAB outperforms its competitors in revealing known functional gene sets and potentially novel cancer driver genes.	[Hosseini, Mohammad Javad; Lee, Su-In] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA; [Lee, Su-In] Univ Washington, Dept Genome Sci, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Hosseini, MJ (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	hosseini@cs.washington.edu; suinlee@cs.washington.edu			National Science Foundation [DBI-1355899]; American Cancer Society [127332-RSG-15-097-01-TBG]	National Science Foundation(National Science Foundation (NSF)); American Cancer Society(American Cancer Society)	We give warm thanks to Reza Eghbali and Amin Jalali for many useful discussions. This work was supported by the National Science Foundation grant DBI-1355899 and the American Cancer Society Research Scholar Award 127332-RSG-15-097-01-TBG.	Ambroise C, 2009, ELECTRON J STAT, V3, P205, DOI 10.1214/08-EJS314; Celik S., 2014, ICML; Duchi J., 2008, UAI; Fang Y, 2015, EUR REV MED PHARMACO, V19, P4811; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Grechkin M., 2015, PATHWAY GRAPHICAL LA; Haferlach T, 2010, J CLIN ONCOL, V28, P2529, DOI 10.1200/JCO.2009.23.4732; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Hsieh C.-J., 2011, ADV NEURAL INFORM PR, P2330; Lasorella A, 2014, NAT REV CANCER, V14, P77, DOI 10.1038/nrc3638; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Lee S. - I., 2003, ADV NEURAL INFORM PR, V16; Liberzon A, 2011, BIOINFORMATICS, V27, P1739, DOI 10.1093/bioinformatics/btr260; Liu Q., 2011, AISTATS, P40; Marlin B. M., 2009, P INT C MACH LEAR, P705; Miyoshi N, 2010, J SURG ONCOL, V101, P156, DOI 10.1002/jso.21459; Mohan Karthik, 2012, Adv Neural Inf Process Syst, V2012, P629; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Rubie C., 2010, RES CCL20 CCR6 EXPRE; Segal E, 2003, NAT GENET, V34, P166, DOI 10.1038/ng1165; Shen Y, 2011, BLOOD, V118, P5593, DOI 10.1182/blood-2011-03-343988; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Tamura K, 2009, CANCER RES, V69, P8133, DOI 10.1158/0008-5472.CAN-09-0775; Tan KM, 2015, COMPUT STAT DATA AN, V85, P23, DOI 10.1016/j.csda.2014.11.015; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Witten DM, 2011, J COMPUT GRAPH STAT, V20, P892, DOI 10.1198/jcgs.2011.11051a; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700061
C	Houthooft, R; Chen, X; Duan, Y; Schulman, J; De Turck, F; Abbeel, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; De Turck, Filip; Abbeel, Pieter			VIME: Variational Information Maximizing Exploration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REINFORCEMENT; CURIOSITY	Scalable and effective exploration remains a key challenge in reinforcement learning (RL). While there are methods with optimality guarantees in the setting of discrete state and action spaces, these methods cannot be applied in high-dimensional deep RL scenarios. As such, most contemporary RL relies on simple heuristics such as epsilon-greedy exploration or adding Gaussian noise to the controls. This paper introduces Variational Information Maximizing Exploration (VIME), an exploration strategy based on maximization of information gain about the agent's belief of environment dynamics. We propose a practical implementation, using variational inference in Bayesian neural networks which efficiently handles continuous state and action spaces. VIME modifies the MDP reward function, and can be applied with several different underlying RL algorithms. We demonstrate that VIME achieves significantly better performance compared to heuristic exploration methods across a variety of continuous control tasks and algorithms, including tasks with very sparse rewards.	[Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; Abbeel, Pieter] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Houthooft, Rein; De Turck, Filip] Univ Ghent, IMEC, Dept Informat Technol, Ghent, Belgium; [Houthooft, Rein; Chen, Xi; Duan, Yan; Schulman, John; Abbeel, Pieter] OpenAI, San Francisco, CA 94110 USA	University of California System; University of California Berkeley; Ghent University; IMEC	Houthooft, R (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.; Houthooft, R (corresponding author), Univ Ghent, IMEC, Dept Informat Technol, Ghent, Belgium.				DARPA; Berkeley Vision and Learning Center (BVLC); Berkeley Artificial Intelligence Research (BAIR) laboratory; ONR through a PECASE award; Ph.D. Fellowship of the Research Foundation - Flanders (FWO); Berkeley AI Research lab Fellowship; Huawei Fellowship; Berkeley Deep Drive (BDD)	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Berkeley Vision and Learning Center (BVLC); Berkeley Artificial Intelligence Research (BAIR) laboratory; ONR through a PECASE award; Ph.D. Fellowship of the Research Foundation - Flanders (FWO)(FWO); Berkeley AI Research lab Fellowship; Huawei Fellowship(Huawei Technologies); Berkeley Deep Drive (BDD)	This work was supported in part by DARPA, the Berkeley Vision and Learning Center (BVLC), the Berkeley Artificial Intelligence Research (BAIR) laboratory, Berkeley Deep Drive (BDD), and ONR through a PECASE award. Rein Houthooft is supported by a Ph.D. Fellowship of the Research Foundation - Flanders (FWO). Xi Chen was also supported by a Berkeley AI Research lab Fellowship. Yan Duan was also supported by a Berkeley AI Research lab Fellowship and a Huawei Fellowship.	Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P., 2009, ADV NEURAL INFORM PR, V21, P89; Blundell C., 2015, ICML; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Duan Y., 2016, ICML; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Guez A., 2014, NIPS, V27, P451; Hester T., 2015, ARTIFICIAL INTELLIGE; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; ITTI L, 2009, VISION RES, V49, P1295, DOI DOI 10.1016/J.VISRES.2008.09.007; Kakade Sham, 2003, P 20 INT C MACHINE L, P306; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Kober J., 2009, PROC NEURAL INF PROC, P849; Kolter J. Z., 2009, P 26 ANN INT C MACHI, P513, DOI DOI 10.1145/1553374.1553441; Little DY, 2014, CLOSING LOOP NEURAL, P295; Lopes M., 2012, ADV NEURAL INFORM PR, V1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mohamed S., 2015, ADV NEURAL INFORM PR, V28, P2116; Montufar G., 2016, ARXIV160509735; Osband I., 2016, ICML; Osband I., 2014, GEN EXPLORATION VIA; Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007; Pazis J., 2013, AAAI; Peters J., 2007, P 24 INT C MACHINE L, P745; Salguero CP, 2014, ENCOUNTERS ASIA, P67; SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605; Schmidhuber J, 2007, LECT NOTES ARTIF INT, V4755, P26; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Schossau J, 2016, ENTROPY-SWITZ, V18, DOI 10.3390/e18010006; Schulman J., 2015, ICML; Stadie B. C., 2015, ARXIV150700814; Still S, 2012, THEOR BIOSCI, V131, P139, DOI 10.1007/s12064-011-0142-z; Storck J., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P159; Subramanian K., 2016, AAMAS; Sutton R., INTRO REINFORCEMENT; Thrun S. B., 1992, TECH REP; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yi Sun, 2011, Artificial General Intelligence. Proceedings 4th International Conference, AGI 2011, P41, DOI 10.1007/978-3-642-22887-2_5; Zahedi K, 2013, FRONT PSYCHOL, V4, DOI 10.3389/fpsyg.2013.00801	42	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705005
C	Hsu, WS; Poupart, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hsu, Wei-Shou; Poupart, Pascal			Online Bayesian Moment Matching for Topic Modeling with Unknown Number of Topics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Latent Dirichlet Allocation (LDA) is a very popular model for topic modeling as well as many other problems with latent groups. It is both simple and effective. When the number of topics (or latent groups) is unknown, the Hierarchical Dirichlet Process (HDP) provides an elegant non-parametric extension; however, it is a complex model and it is difficult to incorporate prior knowledge since the distribution over topics is implicit. We propose two new models that extend LDA in a simple and intuitive fashion by directly expressing a distribution over the number of topics. We also propose a new online Bayesian moment matching technique to learn the parameters and the number of topics of those models based on streaming data. The approach achieves higher log-likelihood than batch and online HDP with fixed hyperparameters on several corpora. The code is publicly available at https://github.com/whsu/bmm.	[Hsu, Wei-Shou; Poupart, Pascal] Univ Waterloo, David R Cheriton Sch Comp Sci, Wateroo, ON N2L 3G1, Canada	University of Waterloo	Hsu, WS (corresponding author), Univ Waterloo, David R Cheriton Sch Comp Sci, Wateroo, ON N2L 3G1, Canada.	wwhsu@uwaterloo.ca; ppoupart@uwaterloo.ca						Anandkumar Anima, 2012, ADV NEURAL INFORM PR, V25, P926; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bryant M, 2012, P ADV NEUR INF PROC, V25, P2699; ehrek Radim., 2010, P LREC 2010 WORKSH N, P45; Ge Rong, 2015, C LEARN THEOR; Goldwater S., 2005, P 18 INT C NEUR INF, P459; Griffiths T, 2002, GIBBS SAMPLING GENER; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Huang FR, 2015, J MACH LEARN RES, V16, P2797; Huang Furong, 2013, ARXIV13090787; Minka T., 2002, P 18 C UNCERTAINTY A, P352; Minka T.P., 2001, P 17 C UNC ART INT, P362; OMAR F, 2016, THESIS; Porteous I., 2008, P 14 ACM SIGKDD INT, P569; Sato I., 2012, PROC 18 ACM SIGKDD I, P105; Teh Y., 2006, ADV NEURAL INFORM PR, V19, P1353; Teh Y.W., 2007, ADV NEURAL INFORM PR, V20, P1481; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Teh YW, 2006, COLING/ACL 2006, VOLS 1 AND 2, PROCEEDINGS OF THE CONFERENCE, P985; Wang C., 2011, P 14 INT C ART INT S, V15, P752, DOI DOI 10.1007/978-3-642-25832-9; Wang C, 2012, ADV NEURAL INFORM PR, V25, P413; Wang C., 2011, AISTATS, V15	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700041
C	Huang, C; Loy, CC; Tang, XO		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, Chen; Loy, Chen Change; Tang, Xiaoou			Local Similarity-Aware Deep Feature Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CLASSIFICATION	Existing deep embedding methods in vision tasks are capable of learning a compact Euclidean space from images, where Euclidean distances correspond to a similarity metric. To make learning more effective and efficient, hard sample mining is usually employed, with samples identified through computing the Euclidean feature distance. However, the global Euclidean distance cannot faithfully characterize the true feature similarity in a complex visual feature space, where the intraclass distance in a high-density region may be larger than the interclass distance in low-density regions. In this paper, we introduce a Position-Dependent Deep Metric (PDDM) unit, which is capable of learning a similarity metric adaptive to local feature structure. The metric can be used to select genuinely hard samples in a local neighborhood to guide the deep embedding learning in an online and robust manner. The new layer is appealing in that it is pluggable to any convolutional networks and is trained end-to-end. Our local similarity-aware feature embedding not only demonstrates faster convergence and boosted performance on two complex image retrieval datasets, its large margin nature also leads to superior generalization results under the large and open set scenarios of transfer learning and zero-shot learning on ImageNet 2010 and ImageNet-10K datasets.	[Huang, Chen; Loy, Chen Change; Tang, Xiaoou] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China	Chinese University of Hong Kong	Huang, C (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	chuang@ie.cuhk.edu.hk; ccloy@ie.cuhk.edu.hk; xtang@ie.cuhk.edu.hk			SenseTime Group Limited; Hong Kong Innovation and Technology Support Programme	SenseTime Group Limited; Hong Kong Innovation and Technology Support Programme	This work is supported by SenseTime Group Limited and the Hong Kong Innovation and Technology Support Programme.	[Anonymous], 2012, ICML; [Anonymous], 2016, CVPR; Bell S., 2015, TOG, V34; Bendale A., 2015, CVPR; Cui Y., 2016, CVPR; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J., 2010, ECCV; Frome A., NIPS2013; Frome A., 2007, ICCV; Fu Z., 2015, CVPR; Goldberger J., 2005, NIPS; Hoi S. C. H., 2006, CVPR; Krause Jonathan, 2013, ICCVW, P5; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Martinez AM, 2001, IEEE T PATTERN ANAL, V23, P228, DOI 10.1109/34.908974; Mensink T, 2013, IEEE T PATTERN ANAL, V35, P2624, DOI 10.1109/TPAMI.2013.83; Norouzi Mohammad, 2014, ICLR; Oh Song H., 2016, CVPR; Perronnin F., 2012, CVPR; Rohrbach M., 2013, NIPS; Rohrbach M., 2011, CVPR; Sanchez J., 2011, CVPR; Schroff F., 2015, CVPR; Simo-Serra E., 2015, ARXIV14126537V2; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang J, 2014, CVPR; Wang X., 2015, ICCV; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xing E. P., 2003, NIPS; Xiong C., 2012, SIGKDD	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703004
C	Huang, CD; Sun, XW; Xiong, JC; Yao, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, Chendi; Sun, Xinwei; Xiong, Jiechao; Yao, Yuan			Split LBI: An Iterative Regularization Path with Structural Sparsity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS; REGRESSION; SELECTION	An iterative regularization path with structural sparsity is proposed in this paper based on variable splitting and the Linearized Bregman Iteration, hence called Split LBI. Despite its simplicity, Split LBI outperforms the popular generalized Lasso in both theory and experiments. A theory of path consistency is presented that equipped with a proper early stopping, Split LBI may achieve model selection consistency under a family of Irrepresentable Conditions which can be weaker than the necessary and sufficient condition for generalized Lasso. Furthermore, some l(2) error bounds are also given at the minimax optimal rates. The utility and benefit of the algorithm are illustrated by applications on both traditional image denoising and a novel example on partial order ranking.	[Huang, Chendi; Sun, Xinwei; Xiong, Jiechao; Yao, Yuan] Peking Univ, Beijing, Peoples R China; [Yao, Yuan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	Peking University; Hong Kong University of Science & Technology	Huang, CD (corresponding author), Peking Univ, Beijing, Peoples R China.	cdhuang@pku.edu.cn; sxwxiaoxiaohehe@pku.edu.cn; xiongjiechao@pku.edu.cn; yuany@ust.hk			National Basic Research Program of China [2012CB825501, 2015CB856000]; NSFC [61071157, 11421110001]	National Basic Research Program of China(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC))	The authors were supported in part by National Basic Research Program of China under grants 2012CB825501 and 2015CB856000, as well as NSFC grants 61071157 and 11421110001.	Arnold TB, 2016, J COMPUT GRAPH STAT, V25, P1, DOI 10.1080/10618600.2015.1008638; Bo W., 2012, IFAC P, V45, P83, DOI DOI 10.3182/20120711-3-BE-2027.00310; Buhlmann P, 2003, J AM STAT ASSOC, V98, P324, DOI 10.1198/016214503000125; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891; Hoefling H, 2010, J COMPUT GRAPH STAT, V19, P984, DOI 10.1198/jcgs.2010.09208; Lee J. D., 2013, ADV NEURAL INFO PROC, P342; Liu J., 2013, P 2013 30 INT C INT, V28, P91; Moeller M., 2012, THESIS; Osher S, 2016, APPL COMPUT HARMON A, V41, P436, DOI 10.1016/j.acha.2016.01.002; Ramdas A, 2016, J COMPUT GRAPH STAT, V25, P839, DOI 10.1080/10618600.2015.1054033; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Sharpnack J., 2012, AISTATS JMLR WCP; Tibshirani R, 2005, J R STAT SOC B, V67, P91, DOI 10.1111/j.1467-9868.2005.00490.x; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani RJ, 2011, ANN STAT, V39, P1335, DOI 10.1214/11-AOS878; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Vaiter S, 2013, IEEE T INFORM THEORY, V59, P2001, DOI 10.1109/TIT.2012.2233859; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Ye GB, 2011, COMPUT STAT DATA AN, V55, P1552, DOI 10.1016/j.csda.2010.10.021; Yin WT, 2008, SIAM J IMAGING SCI, V1, P143, DOI 10.1137/070703983; Zhang F., 2006, SCHUR COMPLEMENT ITS, V4; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zhu YZ, 2017, J COMPUT GRAPH STAT, V26, P195, DOI 10.1080/10618600.2015.1114491	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702035
C	Huang, G; Guo, C; Kusner, MJ; Sun, Y; Weinberger, KQ; Sha, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, Gao; Guo, Chuan; Kusner, Matt J.; Sun, Yu; Weinberger, Kilian Q.; Sha, Fei			Supervised Word Mover's Distance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recently, a new document metric called the word mover's distance (WMD) has been proposed with unprecedented results on k NN-based document classification. The WMD elevates high-quality word embeddings to a document metric by formulating the distance between two documents as an optimal transport problem between the embedded words. However, the document distances are entirely unsupervised and lack a mechanism to incorporate supervision when available. In this paper we propose an efficient technique to learn a supervised metric, which we call the Supervised-WMD (S-WMD) metric. The supervised training minimizes the stochastic leave-one-out nearest neighbor classification error on a per-document level by updating an affine transformation of the underlying word embedding space and a word-imporance weight vector. As the gradient of the original WMD distance would result in an inefficient nested optimization problem, we provide an arbitrarily close approximation that results in a practical and efficient update rule. We evaluate S-WMD on eight real-world text classification tasks on which it consistently outperforms almost all of our 26 competitive baselines.	[Huang, Gao; Guo, Chuan; Sun, Yu; Weinberger, Kilian Q.] Cornell Univ, Ithaca, NY 14853 USA; [Kusner, Matt J.] Univ Warwick, Alan Turing Inst, Coventry, W Midlands, England; [Sha, Fei] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	Cornell University; University of Warwick; University of California System; University of California Los Angeles	Huang, G (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	gh349@cornell.edu; cg563@cornell.edu; mkusner@turing.ac.uk; ys646@cornell.edu; kqw4@cornell.edu; feisha@cs.ucla.edu	Huang, Gao/AAB-1776-2019		National Science Foundation [III-1618134, III-1526012, IIS-1149882]; Bill and Melinda Gates Foundation	National Science Foundation(National Science Foundation (NSF)); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation)	The authors are supported in part by the, III-1618134, III-1526012, IIS-1149882 grants from the National Science Foundation and the Bill and Melinda Gates Foundation. We also thank Dor Kedem for many insightful discussions.	[Anonymous], 2012, ICML; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Blei D., 2014, P 27 INT C NEURAL IN, P3176; Blei D. M., 2003, JMLR; CARDOSOCACHOPO A, 2007, THESIS; Chen K., 2013, WORKSH ICLR; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2016, SIAM J IMAGING SCI, V9, P320, DOI 10.1137/15M1032600; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Cuturi Marco, 2014, JMLR; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Goldberg Y., 2014, NIPS; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Greene D., 2006, P 23 INT C MACHINE L, P377, DOI DOI 10.1145/1143844.1143892; Hinton GE., 2002, NIPS, V15, P833; Kusner M. J., 2015, ICML; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mohan A., 2011, PROC INT C YAHOO LEA, P77; Ontrup J., 2001, NIPS; Pele O, 2009, IEEE I CONF COMP VIS, P460, DOI 10.1109/ICCV.2009.5459199; Perina A., 2013, PROC NEURAL INF PROC, P10; Robertson S. E., 1995, Text REtrieval Conference (TREC-3) (NIST SP 500-225), P109; Rubner Y, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P59, DOI 10.1109/ICCV.1998.710701; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Sanders N. J, 2011, SANDERS TWITTER SENT; Tang DY, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1555; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang F., 2012, ECCV; Wang X., 2009, TECHNICAL REPORT; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Yang L., 2006, DISTANCE METRIC LEAR, V2	37	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700103
C	Huang, H; Paulus, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, He; Paulus, Martin			Learning under uncertainty: a comparison between R-W and Bayesian approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				WIN-STAY	Accurately differentiating between what are truly unpredictably random and systematic changes that occur at random can have profound effect on affect and cognition. To examine the underlying computational principles that guide different learning behavior in an uncertain environment, we compared an R-W model and a Bayesian approach in a visual search task with different volatility levels. Both R-W model and the Bayesian approach reflected an individual's estimation of the environmental volatility, and there is a strong correlation between the learning rate in R-W model and the belief of stationarity in the Bayesian approach in different volatility conditions. In a low volatility condition, R-W model indicates that learning rate positively correlates with lose-shift rate, but not choice optimality (inverted U shape). The Bayesian approach indicates that the belief of environmental stationarity positively correlates with choice optimality, but not lose-shift rate (inverted U shape). In addition, we showed that comparing to Expert learners, individuals with high lose-shift rate (sub-optimal learners) had significantly higher learning rate estimated from R-W model and lower belief of stationarity from the Bayesian model.	[Huang, He; Paulus, Martin] Laureate Inst Brain Res, Tulsa, OK 74133 USA	Laureate Institute for Brain Research, Inc.	Huang, H (corresponding author), Laureate Inst Brain Res, Tulsa, OK 74133 USA.	crane081@gmail.com; mpaulus@laureateinstitute.org						Behrens TEJ, 2007, NAT NEUROSCI, V10, P1214, DOI 10.1038/nn1954; BLAKELY E, 1988, PSYCHOL REC, V38, P111, DOI 10.1007/BF03395009; Bonawitz E, 2014, COGNITIVE PSYCHOL, V74, P35, DOI 10.1016/j.cogpsych.2014.06.003; Browning M, 2015, NAT NEUROSCI, V18, P590, DOI 10.1038/nn.3961; Gershman SJ, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004567; Kahneman D, 2011, THINKING FAST SLOW; Lee MD, 2011, COGN SYST RES, V12, P164, DOI 10.1016/j.cogsys.2010.07.007; Mathys CD, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00825; Rescorla RA., 1972, CLASSICAL CONDITION, pp. 64, DOI DOI 10.1101/GR.110528.110; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Worthy DA, 2014, J MATH PSYCHOL, V59, P41, DOI 10.1016/j.jmp.2013.10.001; Yu A J., 2014, DECISION, V1, P275, DOI [10.1037/dec0000013, DOI 10.1037/DEC0000013]; Yu AJ, 2009, ADV NEURAL INF PROCE, V21, P1873, DOI DOI 10.1371/JOURNAL.PONE.0099909	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703045
C	Huang, KJ; Fu, X; Sidiropoulos, ND		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, Kejun; Fu, Xiao; Sidiropoulos, Nicholas D.			Anchor-Free Correlated Topic Modeling: Identifiability and Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In topic modeling, many algorithms that guarantee identifiability of the topics have been developed under the premise that there exist anchor words - i.e., words that only appear (with positive probability) in one topic. Follow-up work has resorted to three or higher-order statistics of the data corpus to relax the anchor word assumption. Reliable estimates of higher-order statistics are hard to obtain, however, and the identification of topics under those models hinges on uncorrelatedness of the topics, which can be unrealistic. This paper revisits topic modeling based on second-order moments, and proposes an anchor-free topic mining framework. The proposed approach guarantees the identification of the topics under a much milder condition compared to the anchor-word assumption, thereby exhibiting much better robustness in practice. The associated algorithm only involves one eigen-decomposition and a few small linear programs. This makes it easy to implement and scale up to very large problem instances. Experiments using the TDT2 and Reuters-21578 corpus demonstrate that the proposed anchor-free approach exhibits very favorable performance (measured using coherence, similarity count, and clustering accuracy metrics) compared to the prior art.	[Huang, Kejun; Fu, Xiao; Sidiropoulos, Nicholas D.] Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Huang, KJ (corresponding author), Univ Minnesota, Dept Elect & Comp Engn, Minneapolis, MN 55455 USA.	huang663@umn.edu; xfu@umn.edu; nikos@ece.umn.edu			National Science Foundation (NSF) [NSF-ECCS 1608961, NSF IIS-1247632]; Digital Technology Initiative (DTI) Seed Grant, University of Minnesota	National Science Foundation (NSF)(National Science Foundation (NSF)); Digital Technology Initiative (DTI) Seed Grant, University of Minnesota	This work is supported in part by the National Science Foundation (NSF) under the project numbers NSF-ECCS 1608961 and NSF IIS-1247632 and in part by the Digital Technology Initiative (DTI) Seed Grant, University of Minnesota.	Anandkumar A., 2013, ADV NEURAL INFORM PR, P1986; Anandkumar Anima, 2012, NIPS; Arora S., 2013, P ICML 13; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Cai D, 2011, IEEE T KNOWL DATA EN, V23, P902, DOI 10.1109/TKDE.2010.165; Donoho D., 2003, P NIPS 2013, V16; Fu X, 2015, IEEE T SIGNAL PROCES, V63, P2306, DOI 10.1109/TSP.2015.2404577; Gillis N, 2014, SIAM J IMAGING SCI, V7, P1420, DOI 10.1137/130946782; Gillis N, 2014, IEEE T PATTERN ANAL, V36, P698, DOI 10.1109/TPAMI.2013.226; Gillis N, 2013, SIAM J MATRIX ANAL A, V34, P1189, DOI 10.1137/120900629; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Huang K., 2015, P SIAM C DAT MIN SDM; Huang KJ, 2014, IEEE T SIGNAL PROCES, V62, P211, DOI 10.1109/TSP.2013.2285514; Kumar A., 2012, P ICML 12; Liu Y.-K., 2012, NIPS; Ma W.-K., 2010, CONVEX OPTIMIZATION, P229; Recht B., 2012, ADV NEURAL INFORM PR, V25, P1214; Xu W., 2003, P 26 ANN INT ACM SIG, P267, DOI DOI 10.1145/860435.860485	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702055
C	Huang, TK; Li, LH; Vartanian, A; Amershi, S; Zhu, XJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, Tzu-Kuo; Li, Lihong; Vartanian, Ara; Amershi, Saleema; Zhu, Xiaojin			Active Learning with Oracle Epiphany	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a theoretical analysis of active learning with more realistic interactions with human oracles. Previous empirical studies have shown oracles abstaining on difficult queries until accumulating enough information to make label decisions. We formalize this phenomenon with an "oracle epiphany model" and analyze active learning query complexity under such oracles for both the realizable and the agnostic cases. Our analysis shows that active learning is possible with oracle epiphany, but incurs an additional cost depending on when the epiphany happens. Our results suggest new, principled active learning approaches with realistic oracles.	[Huang, Tzu-Kuo] Uber Adv Technol Grp, Pittsburgh, PA 15201 USA; [Li, Lihong; Amershi, Saleema] Microsoft Res, Redmond, WA 98052 USA; [Vartanian, Ara; Zhu, Xiaojin] Univ Wisconsin, Madison, WI USA	Microsoft; University of Wisconsin System; University of Wisconsin Madison	Huang, TK (corresponding author), Uber Adv Technol Grp, Pittsburgh, PA 15201 USA.				NSF [IIS-0953219, IIS-1623605, DGE-1545481, CCF-1423237]; University of Wisconsin-Madison Graduate School; Wisconsin Alumni Research Foundation	NSF(National Science Foundation (NSF)); University of Wisconsin-Madison Graduate School; Wisconsin Alumni Research Foundation	This work is supported in part by NSF grants IIS-0953219, IIS-1623605, DGE-1545481, CCF-1423237, and by the University of Wisconsin-Madison Graduate School with funding from the Wisconsin Alumni Research Foundation.	Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Beygelzimer A., 2010, ADV NEURAL INFORM PR, V23, P199; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Dasgupta S., 2007, C ADV NEUR INF PROC, P353; Donmez P, 2008, P 17 ACM C INF KNOWL, P619, DOI DOI 10.1145/1458082.1458165; El-Yaniv R, 2010, J MACH LEARN RES, V11, P1605; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Hsu D., 2010, THESIS; Huang Tzu-Kuo, 2015, NIPS, P2737; Kakade S. M., 2009, ADV NEURAL INFORM PR, V21; Karampatziakis N, 2011, P 27 C UNC ART INT, P392; Kulesza Todd, 2014, P SIGCHI C HUM FACT, P3075; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Newell E, 2016, 34TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, CHI 2016, P3155, DOI 10.1145/2858036.2858490; Sarkar Advait, 2016, CHI; Shah NB, 2015, ADV NEURAL INFORM PR, V28, P1; Welling M., 2014, ADV NEURAL INFORM PR, V27	17	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701012
C	Ito, S; Fujimaki, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ito, Shinji; Fujimaki, Ryohei			Large-Scale Price Optimization via Network Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ENERGY MINIMIZATION	This paper deals with price optimization, which is to find the best pricing strategy that maximizes revenue or profit, on the basis of demand forecasting models. Though recent advances in regression technologies have made it possible to reveal price-demand relationship of a large number of products, most existing price optimization methods, such as mixed integer programming formulation, cannot handle tens or hundreds of products because of their high computational costs. To cope with this problem, this paper proposes a novel approach based on network flow algorithms. We reveal a connection between supermodularity of the revenue and cross elasticity of demand. On the basis of this connection, we propose an efficient algorithm that employs network flow algorithms. The proposed algorithm can handle hundreds or thousands of products, and returns an exact optimal solution under an assumption regarding cross elasticity of demand. Even if the assumption does not hold, the proposed algorithm can efficiently find approximate solutions as good as other state-of-the-art methods, as empirical results show.	[Ito, Shinji; Fujimaki, Ryohei] NEC Corp Ltd, Tokyo, Japan	NEC Corporation	Ito, S (corresponding author), NEC Corp Ltd, Tokyo, Japan.	s-ito@me.jp.nec.com; rfujimaki@nec-labs.com						Bitran G., 2003, Manufacturing & Service Operations Management, V5, P203, DOI 10.1287/msom.5.3.203.16031; Boros E, 2002, DISCRETE APPL MATH, V123, P155, DOI 10.1016/S0166-218X(01)00336-5; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Caro F, 2012, OPER RES, V60, P1404, DOI 10.1287/opre.1120.1102; Cormen T.H., 2009, INTRO ALGORITHMS; Ferreira K. J., 2015, MANUFACTURING SERVIC, P69; Gorelick L, 2014, PROC CVPR IEEE, P1154, DOI 10.1109/CVPR.2014.151; Ito S., 2016, ARXIV E PRINTS; Klein R, 2008, REVENUE MANAGEMENT G; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Kolmogorov V, 2007, IEEE T PATTERN ANAL, V29, P1274, DOI 10.1109/TPAMI.2007.1031; Koushik D, 2012, INTERFACES, V42, P45, DOI 10.1287/inte.1110.0620; Lee S. M., 2011, THESIS; Marshall A., 1890, PRINCIPLES EC; McGill JI, 1999, TRANSPORT SCI, V33, P233, DOI 10.1287/trsc.33.2.233; Natter M., 2009, MARK INTELL PLAN, V1, P17; Phillips R., 2005, PRICING REVENUE OPTI; Rother C., 2007, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2007.383203; Rusmevichientong P, 2006, OPER RES, V54, P82, DOI 10.1287/opre.1050.0252; STONE CJ, 1985, ANN STAT, V13, P689, DOI 10.1214/aos/1176349548; Tang M, 2014, LECT NOTES COMPUT SC, V8693, P691, DOI 10.1007/978-3-319-10602-1_45; Wang JL, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1245, DOI 10.1145/2783258.2783407	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702048
C	Jabbari, S; Rogers, R; Roth, A; Wu, ZS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jabbari, Shahin; Rogers, Ryan; Roth, Aaron; Wu, Zhiwei Steven			Learning from Rational Behavior: Predicting Solutions to Unknown Linear Programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We define and study the problem of predicting the solution to a linear program (LP) given only partial information about its objective and constraints. This generalizes the problem of learning to predict the purchasing behavior of a rational agent who has an unknown objective function, that has been studied under the name "Learning from Revealed Preferences". We give mistake bound learning algorithms in two settings: in the first, the objective of the LP is known to the learner but there is an arbitrary, fixed set of constraints which are unknown. Each example is defined by an additional known constraint and the goal of the learner is to predict the optimal solution of the LP given the union of the known and unknown constraints. This models the problem of predicting the behavior of a rational agent whose goals are known, but whose resources are unknown. In the second setting, the objective of the LP is unknown, and changing in a controlled way. The constraints of the LP may also change every day, but are known. An example is given by a set of constraints and partial information about the objective, and the task of the learner is again to predict the optimal solution of the partially known LP.	[Jabbari, Shahin; Rogers, Ryan; Roth, Aaron; Wu, Zhiwei Steven] Univ Penn, Philadelphia, PA 19104 USA	University of Pennsylvania	Jabbari, S (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	jabbari@cis.upenn.edu; ryrogers@sas.upenn.edu; aaroth@cis.upenn.edu; wuzhiwei@cis.upenn.edu						Amin K, 2015, AAAI CONF ARTIF INTE, P770; Balcan MF, 2014, LECT NOTES COMPUT SC, V8877, P338, DOI 10.1007/978-3-319-13129-0_28; Beigman E., 2006, P 7 ACM C ELECT COMM, P36; Belloni A., 2015, C LEARN THEOR, P240; Bhaskar U, 2014, ANN IEEE SYMP FOUND, P31, DOI 10.1109/FOCS.2014.12; Blum A., 2015, P 16 ACM C EC COMPUT, P601; Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1; Collins M., 2000, P 17 INT C MACH LEAR, P175; Daniely A., 2014, COLT, P287; Grotschel M., 1993, ALGORITHMS COMBINATO, V2; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Roth A, 2016, ACM S THEORY COMPUT, P949, DOI 10.1145/2897518.2897579; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Zadimoghaddam M., 2012, LNCS, V7695, P114, DOI [10.1007/978-3-642-35311-69, DOI 10.1007/978-3-642-35311-69]; 2010, PREFERENCE LEARNING, P00001	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701007
C	Jalali, A; Han, QY; Dumitriu, I; Fazel, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jalali, Amin; Han, Qiyang; Dumitriu, Ioana; Fazel, Maryam			Exploiting Tradeoffs for Exact Recovery in Heterogeneous Stochastic Block Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The Stochastic Block Model (SBM) is a widely used random graph model for networks with communities. Despite the recent burst of interest in community detection under the SBM from statistical and computational points of view, there are still gaps in understanding the fundamental limits of recovery. In this paper, we consider the SBM in its full generality, where there is no restriction on the number and sizes of communities or how they grow with the number of nodes, as well as on the connectivity probabilities inside or across communities. For such stochastic block models, we provide guarantees for exact recovery via a semidefinite program as well as upper and lower bounds on SBM parameters for exact recoverability. Our results exploit the tradeoffs among the various parameters of heterogenous SBM and provide recovery guarantees for many new interesting SBM configurations.	[Jalali, Amin; Fazel, Maryam] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Han, Qiyang] Univ Washington, Dept Stat, Seattle, WA 98195 USA; [Dumitriu, Ioana] Univ Washington, Dept Math, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Jalali, A (corresponding author), Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA.	amjalali@uw.edu; royhan@uw.edu; dumitriu@uw.edu; mfazel@uw.edu						Abbe E., 2015, ARXIV151209080; Abbe E., 2015, ARXIV150300609; Ailon Nir, 2013, P 30 INT C MACH LEAR, P995; [Anonymous], 2014, ARXIV14043918; Bandeira A. S., 2014, ARXIV14086185; Bandeira A. S., 2015, EFFICIENT ALGORITHM; Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22; Cai TT, 2015, ANN STAT, V43, P1027, DOI 10.1214/14-AOS1290; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Chen JC, 2006, BIOINFORMATICS, V22, P2283, DOI 10.1093/bioinformatics/btl370; Chen Y., 2012, ADV NEURAL INFORM PR, P2204; Chen YY, 2016, BMC MOL BIOL, V17, DOI 10.1186/s12867-016-0055-y; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Guedon O., 2015, PROBABILITY THEORY R, P1; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jiang DX, 2004, IEEE T KNOWL DATA EN, V16, P1370, DOI 10.1109/TKDE.2004.68; Leskovec J., 2010, P 19 INT C WORLD WID, P631; Liu YT, 2014, 2014 INTERNATIONAL CONFERENCE ON MECHATRONICS AND CONTROL (ICMC), P21, DOI 10.1109/ICMC.2014.7231508; Massoulie L., 2014, STOCHASTIC SYSTEMS, V4, P1, DOI DOI 10.1214/11-SSY036; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Meila M., 2001, RANDOM WALKS VIEW SP; Montanari A., 2015, ARXIV150405910; Mossel E, 2015, ACM S THEORY COMPUT, P69, DOI 10.1145/2746539.2746603; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Yun S.-Y., 2014, COLT, P138	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704098
C	Jamieson, K; Haas, D; Recht, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jamieson, Kevin; Haas, Daniel; Recht, Ben			The Power of Adaptivity in Identifying Statistical Alternatives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper studies the trade-off between two different kinds of pure exploration: breadth versus depth. We focus on the most biased coin problem, asking how many total coin flips are required to identify a "heavy" coin from an infinite bag containing both "heavy" coins with mean theta(1) is an element of (0, 1), and "light" coins with mean theta(0) is an element of (0, theta(1)), where heavy coins are drawn from the bag with proportion alpha is an element of(0, 1/2). When alpha, theta(0), theta(1) are unknown, the key difficulty of this problem lies in distinguishing whether the two kinds of coins have very similar means, or whether heavy coins are just extremely rare. While existing solutions to this problem require some prior knowledge of the parameters. theta(0), theta(1), alpha, we propose an adaptive algorithm that requires no such knowledge yet still obtains near-optimal sample complexity guarantees. In contrast, we provide a lower bound showing that non-adaptive strategies require at least quadratically more samples. In characterizing this gap between adaptive and nonadaptive strategies, we make connections to anomaly detection and prove lower bounds on the sample complexity of differentiating between a single parametric distribution and a mixture of two such distributions.	[Jamieson, Kevin; Haas, Daniel; Recht, Ben] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Jamieson, K (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	kjamieson@eecs.berkeley.edu; dhaas@eecs.berkeley.edu; brecht@eecs.berkeley.edu			ONR [N00014-15-1-2620, N00014-13-1-0129]; NSF CISE Expeditions Award [CCF-1139158]; DOE [SN10040 DE-SC0012463]; DARPA XData Award [FA8750-12-2-0331]	ONR(Office of Naval Research); NSF CISE Expeditions Award; DOE(United States Department of Energy (DOE)); DARPA XData Award	Kevin Jamieson is generously supported by ONR awards N00014-15-1-2620, and N00014-13-1-0129. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, DOE Award SN10040 DE-SC0012463, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, IBM, SAP, The Thomas and Stacey Siebel Foundation, Apple Inc., Arimo, Blue Goji, Bosch, Cisco, Cray, Cloudera, Ericsson, Facebook, Fujitsu, Guavus, HP, Huawei, Intel, Microsoft, Pivotal, Samsung, Schlumberger, Splunk, State Farm and VMware.	Acharya Jayadev, 2015, ADV NEURAL INFORM PR, P3591; Agarwal D, 2007, KNOWL INF SYST, V11, P29, DOI 10.1007/s10115-006-0036-4; Arlotto A, 2014, MANAGE SCI, V60, P110, DOI 10.1287/mnsc.2013.1754; Bernstein M. S., 2011, UIST; Berry DA, 1997, ANN STAT, V25, P2103, DOI 10.1214/aos/1069362389; Bonald T., 2013, ADV NEURAL INFORM PR, P2184; Carpentier A, 2015, ARXIV150504627; Chandrasekaran K., 2012, CORR; CHANDRASEKARAN K, 2014, C LEARN THEOR, P394; Eskin E., 2000, P INT C MACH LEARN, P255; Freund Y., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P53, DOI 10.1145/307400.307412; Haas D, 2015, PROC VLDB ENDOW, V9, P372; Hardt Moritz, 2014, SHARP BOUNDS LEARNIN, P1404; Malloy M. L., 2012, IEEE ANN C INF SCI S, P1; Nowak R, 2014, INF SCI SYST CISS 20, P1, DOI DOI 10.1109/CISS.2014.6814096; Pollard D., 2000, ASYMPTOPIA UNPUB; Siegmund D., 2013, SEQUENTIAL ANAL TEST; SPIRA R, 1971, MATH COMPUT, V25, P317, DOI 10.2307/2004927; Thatte G, 2011, IEEE ACM T NETWORK, V19, P512, DOI 10.1109/TNET.2010.2070845; Wang Y., 2009, ADV NEURAL INFORM PR, P1729	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700036
C	Jin, C; Zhang, YC; Balakrishnan, S; Wainwright, MJ; Jordan, MI		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jin, Chi; Zhang, Yuchen; Balakrishnan, Sivaraman; Wainwright, Martin J.; Jordan, Michael, I			Local Maxima in the Likelihood of Gaussian Mixture Models: Structural Results and Algorithmic Consequences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				LEARNING MIXTURES; CONVERGENCE; RATES; IDENTIFIABILITY; EM	We provide two fundamental results on the population (infinite-sample) likelihood function of Gaussian mixture models with M >= 3 components. Our first main result shows that the population likelihood function has bad local maxima even in the special case of equally-weighted mixtures of well-separated and spherical Gaussians. We prove that the log-likelihood value of these bad local maxima can be arbitrarily worse than that of any global optimum, thereby resolving an open question of Srebro [2007]. Our second main result shows that the EM algorithm (or a first-order variant of it) with random initialization will converge to bad critical points with probability at least 1 - e(-Omega(M)).We further establish that a first-order variant of EM will not converge to strict saddle points almost surely, indicating that the poor performance of the first-order method can be attributed to the existence of bad local maxima rather than bad saddle points. Overall, our results highlight the necessity of careful initialization when using the EM algorithm in practice, even when applied in highly favorable settings.	[Jin, Chi; Zhang, Yuchen; Wainwright, Martin J.; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Balakrishnan, Sivaraman] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of California System; University of California Berkeley; Carnegie Mellon University	Jin, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	chijin@cs.berkeley.edu; yuczhang@berkeley.edu; siva@stat.cmu.edu; wainwrig@berkeley.edu; jordan@cs.berkeley.edu	Zhang, Yuchen/GYI-8858-2022; Jordan, Michael I/C-5253-2013		Office of Naval Research MURI [DOD-002888]; Air Force Office of Scientific Research Grant [AFOSR-FA9550-14-1-001]; Mathematical Data Science program of the Office of Naval Research [N00014-15-1-2670]; National Science Foundation [CIF-31712-23800]	Office of Naval Research MURI(MURIOffice of Naval Research); Air Force Office of Scientific Research Grant(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); Mathematical Data Science program of the Office of Naval Research(Office of Naval Research); National Science Foundation(National Science Foundation (NSF))	This work was partially supported by Office of Naval Research MURI grant DOD-002888, Air Force Office of Scientific Research Grant AFOSR-FA9550-14-1-001, the Mathematical Data Science program of the Office of Naval Research under grant number N00014-15-1-2670, and National Science Foundation Grant CIF-31712-23800.	Allman ES, 2009, ANN STAT, V37, P3099, DOI 10.1214/09-AOS689; Amendola C., 2015, INT C MATH ASP COMPU, P579; Arora S, 2005, ANN APPL PROBAB, V15, P69, DOI 10.1214/105051604000000512; Balakrishnan Sivaraman, 2015, ANN STAT; Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16; Chaudhuri K., 2008, COLT, V4, P9; CHEN JH, 1995, ANN STAT, V23, P221, DOI 10.1214/aos/1176324464; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DONOHO DL, 1988, ANN STAT, V16, P552, DOI 10.1214/aos/1176350820; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Genovese CR, 2000, ANN STAT, V28, P1105; Ghosal S, 2001, ANN STAT, V29, P1233; Ho Nhat, 2015, ARXIV150102497; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Lee J.D., 2016, ARXIV160204915, P1246; Loh P.-L., 2013, ADV NEURAL INFORM PR, P476; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; Panageas I., 2016, ARXIV160500405; Pascanu R., 2014, ARXIV14054604; Srebro N, 2007, LECT NOTES COMPUT SC, V4539, P628, DOI 10.1007/978-3-540-72927-3_47; TEICHER H, 1963, ANN MATH STAT, V34, P1265, DOI 10.1214/aoms/1177703862; Titterington DM, 1985, STAT ANAL FINITE MIX; Vempala S, 2002, ANN IEEE SYMP FOUND, P113, DOI 10.1109/SFCS.2002.1181888; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702071
C	Jin, C; Kakade, SM; Netrapalli, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jin, Chi; Kakade, Sham M.; Netrapalli, Praneeth			Provable Efficient Online Matrix Completion via Non-convex Stochastic Gradient Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Matrix completion, where we wish to recover a low rank matrix by observing a few entries from it, is a widely studied problem in both theory and practice with wide applications. Most of the provable algorithms so far on this problem have been restricted to the offline setting where they provide an estimate of the unknown matrix using all observations simultaneously. However, in many applications, the online version, where we observe one entry at a time and dynamically update our estimate, is more appealing. While existing algorithms are efficient for the offline setting, they could be highly inefficient for the online setting. In this paper, we propose the first provable, efficient online algorithm for matrix completion. Our algorithm starts from an initial estimate of the matrix and then performs non-convex stochastic gradient descent (SGD). After every observation, it performs a fast update involving only one row of two tall matrices, giving near linear total runtime. Our algorithm can be naturally used in the offline setting as well, where it gives competitive sample complexity and runtime to state of the art algorithms. Our proofs introduce a general framework to show that SGD updates tend to stay away from saddle surfaces and could be of broader interests to other non-convex problems.	[Jin, Chi] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Kakade, Sham M.] Univ Washington, Seattle, WA 98195 USA; [Netrapalli, Praneeth] Microsoft Res India, Bengaluru, Karnataka, India	University of California System; University of California Berkeley; University of Washington; University of Washington Seattle	Jin, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	chijin@cs.berkeley.edu; sham@cs.washington.edu; praneeth@microsoft.com						Arora S., 2015, J MACH LEARN RES, V40; Brand M, 2003, SIAM PROC S, P37; Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Davidson J., 2010, P 4 ACM C RECOMMENDE, P293, DOI DOI 10.1145/1864708.1864770; De Sa Christopher, 2014, ARXIV14111134; GE R, 2015, ARXIV150302101; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Hardt Moritz, 2014, JMLR WORKSHOP C P, P703; Jain P., 2014, ARXIV14111087; Jain P., 2015, ARXIV150705854, DOI arXiv preprint arXiv:1507.05854; Jain Prateek, 2016, ARXIV160206929; Ji H, 2010, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2010.5539849; Keshavan R. H., 2012, THESIS; Koren Y., 2009, NETFLIX PRIZE DOCUME, V81, P1; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Luo X, 2012, KNOWL-BASED SYST, V27, P271, DOI 10.1016/j.knosys.2011.09.006; Mairal J, 2010, J MACH LEARN RES, V11, P19; Panageas I., 2016, ARXIV160500405; Sun RY, 2015, ANN IEEE SYMP FOUND, P270, DOI 10.1109/FOCS.2015.25; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Yun Se-Young, 2015, ARXIV150403156	26	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701061
C	Jitkrittum, W; Szabo, Z; Chwialkowski, K; Gretton, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jitkrittum, Wittawat; Szabo, Zoltan; Chwialkowski, Kacper; Gretton, Arthur			Interpretable Distribution Features with Maximum Testing Power	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Two semimetrics on probability distributions are proposed, given as the sum of differences of expectations of analytic functions evaluated at spatial or frequency locations (i.e, features). The features are chosen so as to maximize the distinguishability of the distributions, by optimizing a lower bound on test power for a statistical test using these features. The result is a parsimonious and interpretable indication of how and where two distributions differ locally. We show that the empirical estimate of the test power criterion converges with increasing sample size, ensuring the quality of the returned features. In real-world benchmarks on high-dimensional text and image data, linear-time tests using the proposed semimetrics achieve comparable performance to the state-of-the-art quadratic-time maximum mean discrepancy test, while returning human-interpretable features that explain the test results.	[Jitkrittum, Wittawat; Szabo, Zoltan; Chwialkowski, Kacper; Gretton, Arthur] UCL, Gatsby Unit, London, England	University of London; University College London	Jitkrittum, W (corresponding author), UCL, Gatsby Unit, London, England.	wittawatj@gmail.com; zoltan.szabo.m@gmail.com; kacper.chwialkowski@gmail.com; arthur.gretton@gmail.com	Jitkrittum, Wittawat/U-6881-2019	Gretton, Arthur/0000-0003-3169-7624	Gatsby Charitable Foundation	Gatsby Charitable Foundation	We thank the Gatsby Charitable Foundation for the financial support.	Bilodeau M., 2008, THEORY MULTIVARIATE; Bird S., 2009, NATURAL LANGUAGE PRO; Bousquet O, 2003, ANN I STAT MATH, V55, P371, DOI 10.1023/A:1026303510251; Carota C, 1996, J AM STAT ASSOC, V91, P753, DOI 10.2307/2291670; Chwialkowski K.P., 2015, P 28 INT C NEUR INF, P1981; FROMONT M, 2012, 25 ANN C LEARN THEOR, V0023, P00001; Gretton A, 2012, J MACH LEARN RES, V13, P723; Harchaoui Z., 2008, ADV NEURAL INFORM PR, P609; Kosorok MR, 2008, SPRINGER SER STAT, P3; Lloyd JR, 2014, AAAI CONF ARTIF INTE, P1242; Lundqvist D., 1998, TECHNICAL REPORT; Mueller J. W., 2015, NIPS, P1693; Ramdas A, 2015, AAAI CONF ARTIF INTE, P3571; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Srebro N, 2006, LECT NOTES ARTIF INT, V4005, P169, DOI 10.1007/11776420_15; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I., 2008, SUPPORT VECTOR MACHI; Strathmann H., 2012, P ADV NEUR INF PROC, P1205, DOI DOI 10.5555/2999134.2999269; SZEKELY G., 2004, INTERSTAT; van der Vaart A., 2000, SPRINGER SERIES STAT; ZAREMBA W, 2013, ADV NEURAL INFORM PR, V26, P755	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701005
C	Johnson, TB; Guestrin, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Johnson, Tyler B.; Guestrin, Carlos			Unified Methods for Exploiting Piecewise Linear Structure in Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				RULES	We develop methods for rapidly identifying important components of a convex optimization problem for the purpose of achieving fast convergence times. By considering a novel problem formulation-the minimization of a sum of piecewise functions-we describe a principled and general mechanism for exploiting piecewise linear structure in convex optimization. This result leads to a theoretically justified working set algorithm and a novel screening test, which generalize and improve upon many prior results on exploiting structure in convex optimization. In empirical comparisons, we study the scalability of our methods. We find that screening scales surprisingly poorly with the size of the problem, while our working set algorithm convincingly outperforms alternative approaches.	[Johnson, Tyler B.; Guestrin, Carlos] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Johnson, TB (corresponding author), Univ Washington, Seattle, WA 98195 USA.	tbjohns@washington.edu; guestrin@cs.washington.edu			PECASE [N00014-13-1-0023]; NSF [IIS-1258741]; TerraSwarm Research Center [00008169]	PECASE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); TerraSwarm Research Center	We thank Hyunsu Cho, Christopher Aicher, and Tianqi Chen for their helpful feedback as well as assistance preparing datasets used in our experiments. This work is supported in part by PECASE N00014-13-1-0023, NSF IIS-1258741, and the TerraSwarm Research Center 00008169.	Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; CAI M, 2014, INT CONF ACOUST SPEE; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Fercoq O, 2015, PR MACH LEARN RES, V37, P333; Johnson TB, 2015, PR MACH LEARN RES, V37, P1171; Nan F, 2014, INT CONF ACOUST SPEE; Ndiaye E., 2016, ARXIV160206225; Ndiaye E, 2015, ADV NEUR IN, V28; Takeuchi I., 2013, INT C MACH LEARN; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Wang J, 2015, J MACH LEARN RES, V16, P1063; Xiang Z. J., 2012, IEEE INT C AC SPEECH	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700008
C	Kale, S; Lee, C; Pal, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kale, Satyen; Lee, Chansoo; Pal, David			Hardness of Online Sleeping Combinatorial Optimization Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We show that several online combinatorial optimization problems that admit efficient no-regret algorithms become computationally hard in the sleeping setting where a subset of actions becomes unavailable in each round. Specifically, we show that the sleeping versions of these problems are at least as hard as PAC learning DNF expressions, a long standing open problem. We show hardness for the sleeping versions of ONLINE SHORTEST PATHS, ONLINE MINIMUM SPANNING TREE, ONLINE k-SUBSETS, ONLINE k-TRUNCATED PERMUTATIONS, ONLINE MINIMUM CUT, and ONLINE BIPARTITE MATCHING. The hardness result for the sleeping version of the Online Shortest Paths problem resolves an open problem presented at COLT 2015 [Koolen et al., 2015].	[Kale, Satyen; Lee, Chansoo; Pal, David] Yahoo Res, New York, NY 10024 USA; [Lee, Chansoo] Univ Michigan, Ann Arbor, MI 48109 USA; [Kale, Satyen] Google Res, New York, NY 10011 USA	University of Michigan System; University of Michigan; Google Incorporated	Kale, S (corresponding author), Yahoo Res, New York, NY 10024 USA.; Kale, S (corresponding author), Google Res, New York, NY 10011 USA.	satyen@satyenkale.com; chansool@umich.edu; dpal@yahoo-inc.com						Audibert JY, 2014, MATH OPER RES, V39, P31, DOI 10.1287/moor.2013.0598; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Freund Y., 1997, P 20 9 ANN ACM S THE, P334, DOI [10.1145/258533.258616, DOI 10.1145/258533.258616]; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kalai AT, 2012, J COMPUT SYST SCI, V78, P1481, DOI 10.1016/j.jcss.2011.12.026; Kanade V., 2009, ARTIFICIAL INTELLIGE, P272; Kanade V., 2014, ACM T COMPUTATION TH, V6; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; Klivans A.R., 2001, P 33 ANN S THEOR COM, P258, DOI DOI 10.1145/380752.380809; Koolen W. M., 2010, P 23 ANN C LEARN THE, P93; Koolen Wouter M., 2015, P 28 C LEARN THEOR C; Neu G., 2014, ADV NEURAL INFORM PR, P2780; Takimoto E, 2004, J MACH LEARN RES, V4, P773, DOI 10.1162/1532443041424328	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703086
C	Kandasamy, K; Dasarathy, G; Oliva, J; Schneider, J; Poczos, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kandasamy, Kirthevasan; Dasarathy, Gautam; Oliva, Junier; Schneider, Jeff; Poczos, Barnabas			Gaussian Process Bandit Optimisation with Multi-fidelity Evaluations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In many scientific and engineering applications, we are tasked with the optimisation of an expensive to evaluate black box function f. Traditional methods for this problem assume just the availability of this single function. However, in many cases, cheap approximations to f may be obtainable. For example, the expensive real world behaviour of a robot can be approximated by a cheap computer simulation. We can use these approximations to eliminate low function value regions cheaply and use the expensive evaluations of f in a small but promising region and speedily identify the optimum. We formalise this task as a multi-fidelity bandit problem where the target function and its approximations are sampled from a Gaussian process. We develop MF-GP-UCB, a novel method based on upper confidence bound techniques. In our theoretical analysis we demonstrate that it exhibits precisely the above behaviour, and achieves better regret than strategies which ignore multi-fidelity information. MF-GP-UCB outperforms such naive strategies and other multi-fidelity methods on several synthetic and real experiments.	[Kandasamy, Kirthevasan; Oliva, Junier; Schneider, Jeff; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Dasarathy, Gautam] Rice Univ, Houston, TX 77251 USA	Carnegie Mellon University; Rice University	Kandasamy, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kandasamy@cs.cmu.edu; gautamd@rice.edu; joliva@cs.cmu.edu; schneide@cs.cmu.edu; bapoczos@cs.cmu.edu						Adams R. P, 2013, NIPS; Agarwal Alekh, 2011, COLT; Auer Peter, 2003, J MACH LEARN RES; Brochu E., 2010, CORR; Bubeck S., 2012, FDN TRENDS MACHINE L; Cutler Mark, 2014, ICRA; Dani V., 2008, COLT; Davis Tamara M, 2007, ASTROPHYSICAL J; Djolonga Josip, 2013, NIPS; Forrester Alexander I. J., 2007, ANN STAT; Ghosal S., 2006, ANN STAT; Huang D., 2006, STRUCTURAL MULTIDISC; Jones D., 1998, J GLOBAL OPTIMIZATIO; Jones D. R., 1993, J OPTIM THEORY APPL; Kandasamy K., 2015, INT C MACH LEARN; Kandasamy K., 2016, NIPS; Kandasamy K., 2016, ICML; Kawaguchi K., 2015, NIPS; Kirkpatrick S., 1983, SCIENCE; Klein A., 2015, BAYESOPT; Martinez-Cantin R., 2007, P ROB SCI SYST P ROB SCI SYST; Mockus J., 1994, J GLOBAL OPTIMIZATIO; Munos R., 2011, NIPS; Parkinson David, 2006, PHYS REV; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robbins H., 1952, B AM MATH SOC; Sabharwal A., 2015, ARXIV PREPRINT ARXIV; Snoek J., 2012, NIPS; Srinivas N., 2010, ICML; Thompson W., 1933, BIOMETRIKA; Viola P. A., 2001, CVPR 1; Xiong Shifeng, 2013, TECHNOMETRICS; Zhang C., 2015, NIPS	33	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700082
C	Kandasamy, K; Al-Shedivat, M; Xing, EP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kandasamy, Kirthevasan; Al-Shedivat, Maruan; Xing, Eric P.			Learning HMMs with Nonparametric Emissions via Spectral Decompositions of Continuous Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recently, there has been a surge of interest in using spectral methods for estimating latent variable models. However, it is usually assumed that the distribution of the observations conditioned on the latent variables is either discrete or belongs to a parametric family. In this paper, we study the estimation of an m-state hidden Markov model (HMM) with only smoothness assumptions, such as Holderian conditions, on the emission densities. By leveraging some recent advances in continuous linear algebra and numerical analysis, we develop a computationally efficient spectral algorithm for learning nonparametric HMMs. Our technique is based on computing an SVD on nonparametric estimates of density functions by viewing them as continuous matrices. We derive sample complexity bounds via concentration results for nonparametric density estimation and novel perturbation theory results for continuous matrices. We implement our method using Chebyshev polynomial approximations. Our method is competitive with other baselines on synthetic and real problems and is also very computationally efficient.	[Kandasamy, Kirthevasan; Al-Shedivat, Maruan; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Kandasamy, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kandasamy@cs.cmu.edu; alshedivat@cs.cmu.edu; epxing@cs.cmu.edu			NIH [R01GM114311]; AFRL/DARPA [FA87501220324]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); AFRL/DARPA	The authors would like to thank Alex Townsend, Arthur Gretton, Ahmed Hefny, Yaoliang Yu, and Renato Negrinho for the helpful discussions. This work was supported by NIH R01GM114311, AFRL/DARPA FA87501220324.	Anandkumar A., 2012, 25 ANN C LEARN THEOR, V23; Anandkumar A., 2014, JMLR; [Anonymous], 1989, P IEEE; [Anonymous], 2013, APPROXIMATION THEORY; Benaglia T, 2009, J COMPUT GRAPH STAT, V18, P505, DOI 10.1198/jcgs.2009.07175; Birge Lucien, 1995, ANN OF STAT; De Castro Yohann, 2015, ARXIV150104787; Dempster A., 1977, J ROYAL STAT SOC B; Driscoll Tobin A, 2014, PAFNUTY PUBL; Fox L., 1968, CHEBYSHEV POLYNOMIAL; Gine Evarist, 2002, ANN IHP PROBABILITES; Hashemi B., 2016, UNPUB; Hsu D. J., 2009, COLT; Hubner U, 1989, PHYS REV A; Jaeger H., 2000, NEURAL COMPUTATION; Kandasamy Kirthevasan, 2015, NIPS; Lee Woo Young, 1998, INTEGRAL EQUATIONS O; Liu H, 2011, J MACH LEARN RES, V12, P907; Paxson V., 1995, IEEE ACM T NETWORKIN; Robins J, 2009, METRIKA, V69, P227, DOI 10.1007/s00184-008-0214-3; Siddiqi S., 2010, AISTATS; Singh S, 2001, ADV NEURAL INFORM PR, P1555; Singh S., 2004, UAI; Song L., 2010, ICML; Song Le, 2014, NONPARAMETRIC ESTIMA, P640; Stewart G., 1990, MATRIX PERTURBATION; Townsend A., 2014, THESIS U OXFORD; Townsend Alex, 2013, SIAM J SCI COMPUTING; Townsend Alex, 2015, P R SOC A; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Welch LR, 2003, IEEE INFORM THEORY S	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700050
C	Kandasamy, K; Dasarathy, G; Schneider, J; Poczos, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kandasamy, Kirthevasan; Dasarathy, Gautam; Schneider, Jeff; Poczos, Barnabas			The Multi-fidelity Multi-armed Bandit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study a variant of the classical stochastic K-armed bandit where observing the outcome of each arm is expensive, but cheap approximations to this outcome are available. For example, in online advertising the performance of an ad can be approximated by displaying it for shorter time periods or to narrower audiences. We formalise this task as a multi fidelity bandit, where, at each time step, the forecaster may choose to play an arm at any one of M fidelities. The highest fidelity (desired outcome) expends cost lambda((M)). The mth fidelity (an approximation) expends lambda((M)) < lambda((M)) and returns a biased estimate of the highest fidelity. We develop MF-UCB, a novel upper confidence bound procedure for this setting and prove that it naturally adapts to the sequence of available approximations and costs thus attaining better regret than naive strategies which ignore the approximations. For instance, in the above online advertising example, MF-UCB would use the lower fidelities to quickly eliminate suboptimal ads and reserve the larger expensive experiments on a small set of promising candidates. We complement this result with a lower bound and show that MF-UCB is nearly optimal under certain conditions.	[Kandasamy, Kirthevasan; Schneider, Jeff; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Dasarathy, Gautam] Rice Univ, Houston, TX 77251 USA	Carnegie Mellon University; Rice University	Kandasamy, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kandasamy@cs.cmu.edu; gautamd@rice.edu; schneide@cs.cmu.edu; bapoczos@cs.cmu.edu						Agrawal Rajeev, 1995, ADV APPL PROBABILITY; Audibert Jean-Yves, 2009, THEOR COMPUT SCI; Auer Peter, 2003, J MACH LEARN RES; Baram Y, 2004, J MACH LEARN RES, V5, P255; Bubeck S., 2012, FDN TRENDS MACHINE L; Cutler Mark, 2014, IEEE INT C ROB AUT I; Huang D., 2006, STRUCTURAL MULTIDISC; Kandasamy K., 2016, ADV NEURAL INFORM PR; Lai T. L., 1985, ADV APPL MATH; Rajnarayan D., 2008, AIAA ISSMO MULT AN O; Robbins H., 1952, B AM MATH SOC; Thompson W., 1933, BIOMETRIKA; Tran-Thanh Long, 2014, UAI; Wasserman L, 2010, ALL STAT CONCISE COU; Xia Yingce, 2015, IJCAI; Zhang C. C., 2015, ADV NEURAL INFORM PR	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705006
C	Karnin, Z		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Karnin, Zohar			Verification Based Solution for Structured MAB Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of finding the best arm in a stochastic Multi-armed Bandit (MAB) game and propose a general framework based on verification that applies to multiple well-motivated generalizations of the classic MAB problem. In these generalizations, additional structure is known in advance, causing the task of verifying the optimality of a candidate to be easier than discovering the best arm. Our results are focused on the scenario where the failure probability ! must be very low; we essentially show that in this high confidence regime, identifying the best arm is as easy as the task of verification. We demonstrate the effectiveness of our framework by applying it, and matching or improving the state-of-the art results in the problems of: Linear bandits, Dueling bandits with the Condorcet assumption, Copeland dueling bandits, Unimodal bandits and Graphical bandits.	[Karnin, Zohar] Yahoo Res, New York, NY 10036 USA		Karnin, Z (corresponding author), Yahoo Res, New York, NY 10036 USA.	zkarnin@ymail.com						Ailon N, 2014, PR MACH LEARN RES, V32, P856; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Balsubramani Akshay, 2016, P 29 C LEARN THEOR C; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Busa-Fekete Robert, 2014, P 28 AAAI C ART INT; Combes R, 2014, PR MACH LEARN RES, V32; Di Castro Dotan, 2011, CORR; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Grunwald Peter, 2015, P 28 C LEARN THEOR C, V40; Hofmann K, 2013, INFORM RETRIEVAL, V16, P63, DOI 10.1007/s10791-012-9197-9; Jia Y. Y., 2011, P 28 INT C MACH LEAR, P41; Joachims T., 2002, KDD; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Komiyama Junpei, 2016, COPELAND DUELING BAN; Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208; Soare M., 2014, ARXIV14096110; Urvoy T., 2013, INT C MACH LEARN, V28, P91; Yu K, 2006, P 23 INT C MACH LEAR, ppp1081; Yue Y., 2011, ICML; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zoghi M, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P17, DOI 10.1145/2684822.2685290; Zoghi M, 2014, PR MACH LEARN RES, V32, P10; Zoghi Masrour, 2015, ADV NEURAL INFORM PR, P307; 2010, PREFERENCE LEARNING, P00001	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704049
C	Kathuria, T; Deshpande, A; Kohli, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kathuria, Tarun; Deshpande, Amit; Kohli, Pushmeet			Batched Gaussian Process Bandit Optimization via Determinantal Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Gaussian Process bandit optimization has emerged as a powerful tool for optimizing noisy black box functions. One example in machine learning is hyper-parameter optimization where each evaluation of the target function may require training a model which may involve days or even weeks of computation. Most methods for this so-called "Bayesian optimization" only allow sequential exploration of the parameter space. However, it is often desirable to propose batches or sets of parameter values to explore simultaneously, especially when there are large parallel processing facilities at our disposal. Batch methods require modeling the interaction between the different evaluations in the batch, which can be expensive in complex scenarios. In this paper, we propose a new approach for parallelizing Bayesian optimization by modeling the diversity of a batch via Determinantal point processes (DPPs) whose kernels are learned automatically. This allows us to generalize a previous result as well as prove better regret bounds based on DPP sampling. Our experiments on a variety of synthetic and real-world robotics and hyper-parameter optimization tasks indicate that our DPP-based methods, especially those based on DPP sampling, outperform state-of-the-art methods.	[Kathuria, Tarun; Deshpande, Amit; Kohli, Pushmeet] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Kathuria, T (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	t-takat@microsoft.com; amitdesh@microsoft.com; pkohli@microsoft.com						Anari N., 2016, COLT; ANDERSON B, 2000, ICML; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Azimi J., 2010, BATCH BAYESIAN OPTIM; Brualdi R., 1983, LINEAR ALGEBRA ITS A; Civril A, 2013, ALGORITHMICA, V65, P159, DOI 10.1007/s00453-011-9582-6; Civril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018; Contal E., 2013, ECML; Dai Z., 2016, AISTATS; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Deshpande A., 2010, FOCS; Hennig Philipp, 2012, JMLR, V13; Hernandex-Lobato J. M., 2014, NIPS; Katakis I., 2008, ECMLPKDD, P1; Krause A., 2011, NIPS; Kulesza A., 2011, ICML; Li C., 2016, ICML; Lizotte DJ, 2008, PRACTICAL BAYESIAN O; Lyons R, 2003, PUBL MATH-PARIS, P167; Nikolov A, 2015, ACM S THEORY COMPUT, P861, DOI 10.1145/2746539.2746628; Rasmussen CE, 2008, GAUSSIAN PROCESSES M; Robbins H., 1952, B AM MATH SOC; Seeger MW, 2008, IEEE T INFORM THEORY, V54, P2376, DOI 10.1109/TIT.2007.915707; Shah A., 2015, NIPS; Shirai T, 2003, J FUNCT ANAL, V205, P414, DOI 10.1016/S0022-1236(03)00171-X; Snoek J., 2012, NIPS; Srinivas N, 2012, IEEE T INFORM THEORY, V58, P3250, DOI 10.1109/TIT.2011.2182033; Thornton C., 2003, KDD; Tsoumakas G., 2008, ECML PKDD 2008 WORKS; Varma M., 2014, KDD; Wang GG, 2007, J MECH DESIGN, V129, P370, DOI 10.1115/1.2429697; Wang Z., 2016, AISTATS; Westervelt E., 2007, CONTROL AUTOMATION S; Ziemba WT, 2006, STOCHASTIC OPTIMIZAT	35	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703088
C	Khalvati, K; Park, SA; Dreher, JC; Rao, RPN		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Khalvati, Koosha; Park, Seongmin A.; Dreher, Jean-Claude; Rao, Rajesh P. N.			A Probabilistic Model of Social Decision Making based on Reward Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				COOPERATION	A fundamental problem in cognitive neuroscience is how humans make decisions, act, and behave in relation to other humans. Here we adopt the hypothesis that when we are in an interactive social setting, our brains perform Bayesian inference of the intentions and cooperativeness of others using probabilistic representations. We employ the framework of partially observable Markov decision processes (POMDPs) to model human decision making in a social context, focusing specifically on the volunteer's dilemma in a version of the classic Public Goods Game. We show that the POMDP model explains both the behavior of subjects as well as neural activity recorded using fMRI during the game. The decisions of subjects can be modeled across all trials using two interpretable parameters. Furthermore, the expected reward predicted by the model for each subject was correlated with the activation of brain areas related to reward expectation in social interactions. Our results suggest a probabilistic basis for human social decision making within the framework of expected reward maximization.	[Khalvati, Koosha; Rao, Rajesh P. N.] Univ Washington, Dept Comp Sci, Seattle, WA 98105 USA; [Park, Seongmin A.; Dreher, Jean-Claude] CNRS, Inst Sci Cognit Marc Jeannerod, UMR 5229, Lyon, France	University of Washington; University of Washington Seattle; Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Biology (INSB)	Khalvati, K (corresponding author), Univ Washington, Dept Comp Sci, Seattle, WA 98105 USA.	koosha@cs.washington.edu; park@isc.cnrs.fr; dreher@isc.cnrs.fr; rao@cs.washington.edu			LABEX [ANR-11-LABEX-0042, ANR-11-IDEX-0007]; NSF-ANR 'Social_POMDP'; NSF [EEC-1028725, 1318733]; ONR [N000141310817]; CRCNS/NIMH [1R01MH112166-01]; ANR BrainCHOICE [14-CE13-0006]	LABEX; NSF-ANR 'Social_POMDP'; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); CRCNS/NIMH; ANR BrainCHOICE(French National Research Agency (ANR))	This work was supported by LABEX ANR-11-LABEX-0042, ANR-11-IDEX-0007, NSF-ANR 'Social_POMDP' and ANR BrainCHOICE no14-CE13-0006 to JC. D, NSF grants EEC-1028725 and 1318733, ONR grant N000141310817, and CRCNS/NIMH grant 1R01MH112166-01.	Archetti M, 2011, EVOLUTION, V65, P885, DOI 10.1111/j.1558-5646.2010.01176.x; Bault N, 2015, SOC COGN AFFECT NEUR, V10, P877, DOI 10.1093/scan/nsu138; Burton-Chellew MN, 2013, P NATL ACAD SCI USA, V110, P216, DOI 10.1073/pnas.1210960110; Chung D, 2015, SOC COGN AFFECT NEUR, V10, P1210, DOI 10.1093/scan/nsv006; Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429; DIEKMANN A, 1993, INT J GAME THEORY, V22, P75, DOI 10.1007/BF01245571; Fermin A. S. R., 2016, SCI REPORTS, V6; Fischbacher U, 2001, ECON LETT, V71, P397, DOI 10.1016/S0165-1765(01)00394-9; Gmytrasiewicz PJ, 2005, J ARTIF INTELL RES, V24, P49, DOI 10.1613/jair.1579; Hula A, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004254; Khalvati K., 2015, ADV NEURAL INFORM PR, P2413; Khalvati K., 2013, P 27 AAAI C ART INT, P187; Lin A, 2012, SOC COGN AFFECT NEUR, V7, P274, DOI 10.1093/scan/nsr006; Miller EK, 2001, ANNU REV NEUROSCI, V24, P167, DOI 10.1146/annurev.neuro.24.1.167; OLSON M., 1971, LOGIC COLLECTIVE ACT; Park SA, 2013, SOC NEUROSCI-UK, V8, P568, DOI 10.1080/17470919.2013.835280; Rao RPN, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00146; Ruff CC, 2014, NAT REV NEUROSCI, V15, P549, DOI 10.1038/nrn3776; SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071; SMITH T, 2004, P INT C UNC ART INT; Steinbeis N, 2016, CURR OPIN BEHAV SCI, V10, P28, DOI 10.1016/j.cobeha.2016.04.009; Thrun S., 2005, PROBABILISTIC ROBOTI; Tom SM, 2007, SCIENCE, V315, P515, DOI 10.1126/science.1134239; Wang J, 2012, P NATL ACAD SCI USA, V109, P14363, DOI 10.1073/pnas.1120867109; Wunder M., 2013, P 14 ACM C ELECT COM, P891	25	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704061
C	Khetan, A; Oh, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Khetan, Ashish; Oh, Sewoong			Computational and Statistical Tradeoffs in Learning to Rank	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					For massive and heterogeneous modern data sets, it is of fundamental interest to provide guarantees on the accuracy of estimation when computational resources are limited. In the application of learning to rank, we provide a hierarchy of rank-breaking mechanisms ordered by the complexity in thus generated sketch of the data. This allows the number of data points collected to be gracefully traded off against computational resources available, while guaranteeing the desired level of accuracy. Theoretical guarantees on the proposed generalized rank-breaking implicitly provide such trade-offs, which can be explicitly characterized under certain canonical scenarios on the structure of the data.	[Khetan, Ashish; Oh, Sewoong] Univ Illinois, Dept ISE, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Khetan, A (corresponding author), Univ Illinois, Dept ISE, Champaign, IL 61820 USA.	khetan2@illinois.edu; swoh@illinois.edu			NSF SaTC award [CNS-1527754]; NSF CISE award [CCF-1553452]	NSF SaTC award(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CISE award	This work is supported by NSF SaTC award CNS-1527754, and NSF CISE award CCF-1553452.	Agarwal A, 2012, ARXIV12080129; Ali A, 2012, MATH SOC SCI, V64, P28, DOI 10.1016/j.mathsocsci.2011.08.008; Azari H, 2012, NIPS 12, P126; Betzler N, 2014, AUTON AGENT MULTI-AG, V28, P721, DOI 10.1007/s10458-013-9236-y; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110; Chen Y., 2015, ARXIV150407218; Deshpande Yash, 2015, ARXIV150206590; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Hajek S., 2014, ADV NEURAL INFORM PR, P1475; Hayes Thomas P., 2005, COMBINATORICS PROBAB; Kamishima T., 2003, P 9 ACM SIGKDD INT C, P583; Khetan A., 2016, INT C MACH LEARN; Lucic M., 2015, AISTATS; Maystre L., 2015, ADV NEURAL INFORM PR; Meka R, 2015, ACM S THEORY COMPUT, P87, DOI 10.1145/2746539.2746600; Negahban Sahand, 2014, ARXIV12091688; Prekopa A., 1980, STOCHASTIC PROGRAMMI; Shah N. B., 2015, ARXIV151005610; Shah N. B., 2015, ARXIV150501462; Shalev-Shwartz S., 2008, P 25 INT C MACH LEAR, V307, P928, DOI DOI 10.1145/1390156.1390273; Soufiani HA, 2014, PR MACH LEARN RES, V32; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703078
C	Khetan, A; Oh, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Khetan, Ashish; Oh, Sewoong			Achieving Budget-optimality with Adaptive Schemes in Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Adaptive schemes, where tasks are assigned based on the data collected thus far, are widely used in practical crowdsourcing systems to efficiently allocate the budget. However, existing theoretical analyses of crowdsourcing systems suggest that the gain of adaptive task assignments is minimal. To bridge this gap, we investigate this question under a strictly more general probabilistic model, which has been recently introduced to model practical crowdsourcing datasets. Under this generalized Dawid-Skene model, we characterize the fundamental trade-off between budget and accuracy. We introduce a novel adaptive scheme that matches this fundamental limit. A given budget is allocated over multiple rounds. In each round, a subset of tasks with high enough confidence are classified, and increasing budget is allocated on remaining ones that are potentially more difficult. On each round, decisions are made based on the leading eigenvector of (weighted) non-backtracking operator corresponding to the bipartite assignment graph. We further quantify the gain of adaptivity, by comparing the tradeoff with the one for non-adaptive schemes, and confirm that the gain is significant and can be made arbitrarily large depending on the distribution of the difficulty level of the tasks at hand.	[Khetan, Ashish; Oh, Sewoong] Univ Illinois, Dept ISE, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Khetan, A (corresponding author), Univ Illinois, Dept ISE, Champaign, IL 61820 USA.	khetan2@illinois.edu; swoh@illinois.edu			NSF SaTC award [CNS-1527754]; NSF CISE award [CCF-1553452]	NSF SaTC award(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CISE award	This work is supported by NSF SaTC award CNS-1527754, and NSF CISE award CCF-1553452.	Alon N., 2004, PROBABILISTIC METHOD; Dalvi N, 2013, P 22 INT C WORLD WID, P285, DOI DOI 10.1145/2488388.2488414; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Ghosh A., 2011, P 12 ACM C EL COMM, P167, DOI DOI 10.1145/1993574.1993599; Ho Chien Ju, 2013, P 30 INT C MACHINE L, P534; Jin R., 2003, PROC NEURAL INF PROC, P921; Karger David R., 2013, Performance Evaluation Review, V41, P81; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Li H., 2014, ARXIV14114086; Liu Qiang, 2012, ADV NEURAL INFORM PR, V25, P692; Maron O., 1993, HOEFFDING RACES ACCE, P263; Mezard M., 2009, INFORM PHYS COMPUTAT, pp 584, DOI [10.1093/acprof:oso/9780198570837.001.0001, DOI 10.1093/ACPROF:OSO/9780198570837.001, DOI 10.1093/ACPROF:OSO/9780198570837.001.0001]; Ok J., 2016, INT C MACH LEARN; PeterWelinder Steve Branson, 2010, P NIPS, V23, P1; Shah Nihar B, 2016, ARXIV160609632; Sheng Victor S, 2008, P 14 ACM SIGKDD INT, P614, DOI DOI 10.1145/1401890.1401965; Smyth P., 1995, Advances in Neural Information Processing Systems 7, P1085; Whitehill J., 2009, ADV NEURAL INFORM PR, P2035; Williams D., 1991, PROBABILITY MARTINGA; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260; Zhou D., 2015, ARXIV150307240; Zhou Dengyong, 2012, ADV NEURAL INFORM PR, P2204	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700088
C	Khim, J; Jog, V; Loh, PL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Khim, Justin; Jog, Varun; Loh, Po-Ling			Computing and maximizing influence in linear threshold and triggering models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We establish upper and lower bounds for the influence of a set of nodes in certain types of contagion models. We derive two sets of bounds, the first designed for linear threshold models, and the second more broadly applicable to a general class of triggering models, which subsumes the popular independent cascade models, as well. We quantify the gap between our upper and lower bounds in the case of the linear threshold model and illustrate the gains of our upper bounds for independent cascade models in relation to existing results. Importantly, our lower bounds are monotonic and submodular, implying that a greedy algorithm for influence maximization is guaranteed to produce a maximizer within a (1 - 1/epsilon)-factor of the truth. Although the problem of exact influence computation is NP-hard in general, our bounds may be evaluated efficiently. This leads to an attractive, highly scalable algorithm for influence maximization with rigorous theoretical guarantees.	[Khim, Justin] Univ Penn, Dept Stat, Wharton Sch, Philadelphia, PA 19104 USA; [Jog, Varun; Loh, Po-Ling] Univ Wisconsin, Elect & Comp Engn Dept, Madison, WI 53706 USA	University of Pennsylvania; University of Wisconsin System; University of Wisconsin Madison	Khim, J (corresponding author), Univ Penn, Dept Stat, Wharton Sch, Philadelphia, PA 19104 USA.	jkhim@wharton.upenn.edu; vjog@wisc.edu; loh@ece.wisc.edu						Adamic LA, 2003, SOC NETWORKS, V25, P211, DOI 10.1016/S0378-8733(03)00009-1; [Anonymous], 1988, LECT NOTES PARTICLE; Borgs C., 2014, SODA, P946; Chen W., 2010, KDD, P1029, DOI [10.1145/1835804.1835934, DOI 10.1145/1835804.1835934]; Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047; Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57, DOI 10.1145/502512.502525; Eun Jee Lee, 2016, 2016 Annual Conference on Information Science and Systems (CISS), P649, DOI 10.1109/CISS.2016.7460579; Hecker M, 2009, BIOSYSTEMS, V96, P86, DOI 10.1016/j.biosystems.2008.12.004; Jiang CR, 2010, INT ASIA CONF INFORM, P88, DOI 10.1109/CAR.2010.5456772; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kermack WO, 1927, P R SOC LOND A-CONTA, V115, P700, DOI 10.1098/rspa.1927.0118; Lemonnier R., 2016, SPECTRAL BOUNDS RAND; Lemonnier R, 2014, ADV NEUR IN, V27; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Movarraei N., 2015, INT J APPL MATH RES, V4, P30; Movarraei N., 2014, INT J APPL MATH RES, V3, P178; Movarraei N., 2015, INT J APPL MATH RES, V4, P267; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; Newman MEJ, 2003, SIAM REV, V45, P167, DOI 10.1137/S003614450342480; Scaman K., 2015, ADV NEURAL INFORM PR, V28, P2017; Sporns O, 2011, ANN NY ACAD SCI, V1224, P109, DOI 10.1111/j.1749-6632.2010.05888.x; Zhou C, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P421, DOI 10.1145/2567948.2577336	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702094
C	Kingma, DP; Salimans, T; Jozefowicz, R; Chen, X; Sutskever, I; Welling, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kingma, Diederik P.; Salimans, Tim; Jozefowicz, Rafal; Chen, Xi; Sutskever, Ilya; Welling, Max			Improved Variational Inference with Inverse Autoregressive Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The framework of normalizing flows provides a general strategy for flexible variational inference of posteriors over latent variables. We propose a new type of normalizing flow, inverse autoregressive flow (IAF), that, in contrast to earlier published flows, scales well to high-dimensional latent spaces. The proposed flow consists of a chain of invertible transformations, where each transformation is based on an autoregressive neural network. In experiments, we show that IAF significantly improves upon diagonal Gaussian approximate posteriors. In addition, we demonstrate that a novel type of variational autoencoder, coupled with IAF, is competitive with neural autoregressive models in terms of attained log-likelihood on natural images, while allowing significantly faster synthesis.	[Welling, Max] Univ Amsterdam, Amsterdam, Netherlands; [Welling, Max] Univ Calif Irvine, Irvine, CA 92717 USA; [Welling, Max] Canadian Inst Adv Res CIFAR, Toronto, ON, Canada	University of Amsterdam; University of California System; University of California Irvine; Canadian Institute for Advanced Research (CIFAR)	Welling, M (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.; Welling, M (corresponding author), Univ Calif Irvine, Irvine, CA 92717 USA.; Welling, M (corresponding author), Canadian Inst Adv Res CIFAR, Toronto, ON, Canada.	dpkingma@openai.com; tim@openai.com; rafal@openai.com; peter@openai.com; ilya@openai.com; M.Welling@uva.nl						[Anonymous], 2016, ARXIV160903499; [Anonymous], 2014, ARXIV14106460; [Anonymous], 2016, ARXIV160605328; Bowman S. R., 2016, ARXIV, P10; Burda Yuri, 2015, ICLR; Deco G., 1995, Advances in Neural Information Processing Systems 7, P247; Dinh L., 2016, ARXIV160508803CSSTAT; Dinh L., 2014, ARXIV; Germain M, 2015, RXIV150203509; Gregor K, 2016, ARXIV160408772; Gregor K., 2013, ARXIV PREPRINT ARXIV, V2; He K., 2016, ARXIV160305027; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kingma DP, 2013, P 2 INT C LEARN REPR; Paisley J., 2012, P 29 INT C MACH LEAR, P1363; Ranganath R., 2015, ARXIV151102386; Rezende D., 2015, ICML, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T., 2016, ARXIV160208734; Salimans T., 2016, ADV NEURAL INFORM PR; Sohl-Dickstein J., 2015, ARXIV150303585; Sonderby Casper Kaae, 2016, ICML; Tran D., 2015, ARXIV151106499; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; van den Oord A., 2016, ARXIV160106759; van den Oord A., 2014, NIPS, P3518; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zeiler MD, 2010, PROC CVPR IEEE, P2528, DOI 10.1109/CVPR.2010.5539957	30	0	0	2	10	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704105
C	Kirillov, A; Shekhovtsov, A; Rother, C; Savchynskyy, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kirillov, Alexander; Shekhovtsov, Alexander; Rother, Carsten; Savchynskyy, Bogdan			Joint M-Best-Diverse Labelings as a Parametric Submodular Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHM	We consider the problem of jointly inferring the M-best diverse labelings for a binary (high-order) submodular energy of a graphical model. Recently, it was shown that this problem can be solved to a global optimum, for many practically interesting diversity measures. It was noted that the labelings are, so-called, nested. This nestedness property also holds for labelings of a class of parametric submodular minimization problems, where different values of the global parameter 7 give rise to different solutions. The popular example of the parametric submodular minimization is the monotonic parametric max-flow problem, which is also widely used for computing multiple labelings. As the main contribution of this work we establish a close relationship between diversity with submodular energies and the parametric submodular minimization. In particular, the joint M-best diverse labelings can be obtained by running a non-parametric submodular minimization (in the special case- max-flow) solver for M different values of 7 in parallel, for certain diversity measures. Importantly, the values for 7 can be computed in a closed form in advance, prior to any optimization. These theoretical results suggest two simple yet efficient algorithms for the joint M-best diverse problem, which outperform competitors in terms of runtime and quality of results. In particular, as we show in the paper, the new methods compute the exact M-best diverse labelings faster than a popular method of Batra et al., which in some sense only obtains approximate solutions.	[Kirillov, Alexander; Rother, Carsten; Savchynskyy, Bogdan] Tech Univ Dresden, Dresden, Germany; [Shekhovtsov, Alexander] Graz Univ Technol, Graz, Austria	Technische Universitat Dresden; Graz University of Technology	Kirillov, A (corresponding author), Tech Univ Dresden, Dresden, Germany.	alexander.kirillov@tu-dresden.de			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [647769]; ERC [640156]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); ERC(European Research Council (ERC)European Commission)	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement No 647769). A. Shekhovtsov was supported by ERC starting grant agreement 640156.	Ahmed F., 2015, ICCV; [Anonymous], NIPS WORKSH PERT OPT; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Batra D., 2012, UAI; Batra D., 2012, ECCV; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Carreira J, 2010, PROC CVPR IEEE, P3241, DOI 10.1109/CVPR.2010.5540063; Chen C., 2013, AISTATS; Dey D., 2015, ICCV; Everingham M., 2012, PASCAL VISUAL OBJECT; Fleischer L, 2003, DISCRETE APPL MATH, V131, P311, DOI 10.1016/S0166-218X(02)00458-4; Fromer M., 2009, NIPS, V22; GALLO G, 1989, SIAM J COMPUT, V18, P30, DOI 10.1137/0218003; Globerson A, 2011, PROBABILISTIC INFERE; Guzman-Rivera A., 2014, AISTATS; Guzman-Rivera A., 2013, AISTATS; Guzman-Rivera A., 2012, NIPS, V25; Hochbaum DS, 2008, OPER RES, V56, P992, DOI 10.1287/opre.1080.0524; Hower D, 2009, IEEE I CONF COMP VIS, P849, DOI 10.1109/ICCV.2009.5459305; Jolly M., 2001, ICCV; Jug F., 2014, BAMBI; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kirillov A., 2015, NIPS; Kirillov A., 2015, ICCV; KOHLI P, 2007, PAMI; Kolmogorov V., 2007, IEEE 11 INT C COMP V, P1; Kolmogorov V., 2004, TPAMI; Kulesza A., 2010, NIPS, V23; Lawler E. L., 1972, MANAGEMENT SCI, V18; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483; Prasad A., 2014, NIPS, V27; Premachandran V., 2014, CVPR; SCHLESINGER M, 2002, 10 LECT STAT STRUCTU; Sener O., 2015, ROBOTICS SCI SYSTEMS; Szeliski R., 2008, TPAMI, V30, P1068, DOI DOI 10.1109/TPAMI.2007.70844; Tarlow D., 2010, AISTATS; TOPKIS DM, 1978, OPER RES, V26, P305, DOI 10.1287/opre.26.2.305; Wang S., 2015, ICCV; Yadollahpour P., 2013, CVPR; Yanover C., 2004, NIPS, V17	41	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703032
C	Koolen, WM; Grunwald, P; van Erven, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Koolen, Wouter M.; Grunwald, Peter; van Erven, Tim			Combining Adversarial Guarantees and Stochastic Fast Rates in Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BOUNDS	We consider online learning algorithms that guarantee worst-case regret rates in adversarial environments (so they can be deployed safely and will perform robustly), yet adapt optimally to favorable stochastic environments (so they will perform well in a variety of settings of practical importance). We quantify the friendliness of stochastic environments by means of the well-known Bernstein (a.k.a. generalized Tsybakov margin) condition. For two recent algorithms (Squint for the Hedge setting and MetaGrad for online convex optimization) we show that the particular form of their data-dependent individual-sequence regret guarantees implies that they adapt automatically to the Bernstein parameters of the stochastic environment. We prove that these algorithms attain fast rates in their respective settings both in expectation and with high probability.	[Koolen, Wouter M.] Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands; [Grunwald, Peter] CWI, Nampa, ID USA; [Grunwald, Peter; van Erven, Tim] Leiden Univ, Niels Bohrweg 1, NL-2333 CA Leiden, Netherlands	Leiden University; Leiden University - Excl LUMC	Koolen, WM (corresponding author), Ctr Wiskunde & Informat, Sci Pk 123, NL-1098 XG Amsterdam, Netherlands.	wmkoolen@cwi.nl; pdg@cwi.nl; tim@timvanerven.nl			Netherlands Organization for Scientific Research (NWO) [639.021.439]	Netherlands Organization for Scientific Research (NWO)(Netherlands Organization for Scientific Research (NWO))	Koolen acknowledges support by the Netherlands Organization for Scientific Research (NWO, Veni grant 639.021.439).	Audibert J.-Y., 2004, THESIS; Audibert JY, 2009, ANN STAT, V37, P1591, DOI 10.1214/08-AOS623; Bartlett PL, 2006, PROBAB THEORY REL, V135, P311, DOI 10.1007/s00440-005-0462-3; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chiang C., 2012, P 25 C LEARN THEOR C; Crammer K., 2009, NIPS, V22; de Rooij S, 2014, J MACH LEARN RES, V15, P1281; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Even-Dar E., 2008, MACHINE LEARNING, V72; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gaillard P., 2014, P 27 COLT; Gaillard P., 2015, P 28 C LEARN THEOR C; Grunwald P., 2012, ALT 12; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; Koltchinskii V, 2006, ANN STAT, V34, P2593, DOI 10.1214/009053606000001019; Koolen W., 2015, RELATIVE ENTROPY BOU; Koolen Wouter M, 2015, COLT, P1155; Koolen Wouter M, 2014, ADV NEURAL INFORM PR, V27; Luo H., 2015, P 28 COLT; Massart P, 2006, ANN STAT, V34, P2326, DOI 10.1214/009053606000000786; Mehta N., 2014, NIPS, V27; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Rakhlin A., 2014, P 27 COLT; Sani A., 2014, NIPS, V27; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Steinhardt Jacob., 2014, ICML, P1593; Tsybakov AB, 2004, ANN STAT, V32, P135; van Erven T., 2016, ADV NEURAL INFORM PR, V29; van Erven T, 2015, J MACH LEARN RES, V16, P1793; Wintenberger O., 2015, ARXIV14041356	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703110
C	Krause, O; Arbones, DR; Igel, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krause, Oswin; Arbones, Didac R.; Igel, Christian			CMA-ES with Optimal Covariance Update and Storage Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				EVOLUTION	The covariance matrix adaptation evolution strategy (CMA-ES) is arguably one of the most powerful real-valued derivative-free optimization algorithms, finding many applications in machine learning. The CMA-ES is a Monte Carlo method, sampling from a sequence of multi-variate Gaussian distributions. Given the function values at the sampled points, updating and storing the covariance matrix dominates the time and space complexity in each iteration of the algorithm. We propose a numerically stable quadratic-time covariance matrix update scheme with minimal memory requirements based on maintaining triangular Cholesky factors. This requires a modification of the cumulative step-size adaption (CSA) mechanism in the CMA-ES, in which we replace the inverse of the square root of the covariance matrix by the inverse of the triangular Cholesky factor. Because the triangular Cholesky factor changes smoothly with the matrix square root, this modification does not change the behavior of the CMA-ES in terms of required objective function evaluations as verified empirically. Thus, the described algorithm can and should replace the standard CMA-ES if updating and storing the covariance matrix matters.	[Krause, Oswin; Arbones, Didac R.; Igel, Christian] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark	University of Copenhagen	Krause, O (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	oswin.krause@di.ku.dk; didac@di.ku.dk; igel@di.ku.dk	Krause, Oswin/L-8814-2016		Innovation Fund Denmark through the project "Personalized breast cancer screening"; Innovation Fund Denmark through the project "Cyber Fraud Detection Using Advanced Machine Learning Techniques"	Innovation Fund Denmark through the project "Personalized breast cancer screening"; Innovation Fund Denmark through the project "Cyber Fraud Detection Using Advanced Machine Learning Techniques"	We acknowledge support from the Innovation Fund Denmark through the projects "Personalized breast cancer screening" (OK, CI) and "Cyber Fraud Detection Using Advanced Machine Learning Techniques" (DRA, CI).	Akimoto Y, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P373, DOI 10.1145/2576768.2598258; Akimoto Y, 2012, ALGORITHMICA, V64, P698, DOI 10.1007/s00453-011-9564-8; Auger A., 2015, THESIS; Beyer H., 2007, SCHOLARPEDIA, V2, DOI DOI 10.4249/scholarpedia.1965; Beyer HG, 2014, EVOL COMPUT, V22, P679, DOI 10.1162/EVCO_a_00132; Bringmann K, 2013, ARTIF INTELL, V204, P22, DOI 10.1016/j.artint.2013.08.001; Eiben AE, 2015, NATURE, V521, P476, DOI 10.1038/nature14544; Gomez F, 2008, J MACH LEARN RES, V9, P937; Hansen M, 1996, IEEE C EVOL COMPUTAT, P312, DOI 10.1109/ICEC.1996.542381; Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398; Hansen N., 2015, TECHNICAL REPORT; Heidrich-Meisner V., 2009, P 26 ANN INT C MACH, P401, DOI [10.1145/1553374.1553426, DOI 10.1145/1553374.1553426]; Heidrich-Meisner V, 2009, J ALGORITHMS, V64, P152, DOI 10.1016/j.jalgor.2009.04.002; Igel C., 2010, ENCY MACHINE LEARNIN; Igel C, 2008, J MACH LEARN RES, V9, P993; Krause O., 2015, PROC ACM C FDN GEN A, P129; Loshchilov I., 2015, EVOLUTIONARY COMPUTA; Loshchilov I, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P397, DOI 10.1145/2576768.2598294; Omidvar MN, 2010, LECT NOTES ARTIF INT, V6464, P303, DOI 10.1007/978-3-642-17432-2_31; Poland J, 2001, PROC GENET EVOL COMP, P1050; Ros R, 2008, LECT NOTES COMPUT SC, V5199, P296, DOI 10.1007/978-3-540-87700-4_30; Stich S. U., PARALLEL PROBLEM SOL, P448; Sun Y, 2013, PROC 15 ANN C COMPAN, P61; Suttorp T, 2009, MACH LEARN, V75, P167, DOI 10.1007/s10994-009-5102-1	24	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703093
C	Krichene, W; Bayen, AM; Bartlett, PL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krichene, Walid; Bayen, Alexandre M.; Bartlett, Peter L.			Adaptive Averaging in Accelerated Descent Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study accelerated descent dynamics for constrained convex optimization. This dynamics can be described naturally as a coupling of a dual variable accumulating gradients at a given rate eta(t), and a primal variable obtained as the weighted average of the mirrored dual trajectory, with weights w(t). Using a Lyapunov argument, we give sufficient conditions on eta and w to achieve a desired convergence rate. As an example, we show that the replicator dynamics (an example of mirror descent on the simplex) can be accelerated using a simple averaging scheme. We then propose an adaptive averaging heuristic which adaptively computes the weights to speed up the decrease of the Lyapunov function. We provide guarantees on adaptive averaging in continuous-time, prove that it preserves the quadratic convergence rate of accelerated first-order methods in discrete-time, and give numerical experiments to compare it with existing heuristics, such as adaptive restarting. The experiments indicate that adaptive averaging performs at least as well as adaptive restarting, with significant improvements in some cases.	[Krichene, Walid; Bayen, Alexandre M.; Bartlett, Peter L.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Bartlett, Peter L.] QUT, Brisbane, Qld, Australia; [Krichene, Walid] Google, New York, NY USA	University of California System; University of California Berkeley; Queensland University of Technology (QUT); Google Incorporated	Krichene, W (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Krichene, W (corresponding author), Google, New York, NY USA.	walid@eecs.berkeley.edu; bayen@berkeley.edu; bartlett@cs.berkeley.edu						Attouch H., 2015, CORR; Attouch H., 2016, CORR; Attouch H, 2016, SIAM J OPTIMIZ, V26, P1824, DOI 10.1137/15M1046095; Aubin J.P., 1991, VIABILITY THEORY; Bloch A., 1994, HAMILTONIAN GRADIENT; BROWN AA, 1989, J OPTIMIZ THEORY APP, V62, P211, DOI 10.1007/BF00941054; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Flammarion N., 2015, C LEARN THEOR, P658; HELMKE U, 1994, COMMUNICATIONS CONTR; Kingma D. P., 2014, ADAM METHOD STOCHAST; Krichene W., 2015, NIPS; Lyapunov A.M., 1992, CONTROL THEORY APPL; NEMIROVSKY A. S., 1983, WILEY INTERSCIENCE S; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x; O'Donoghue B, 2015, FOUND COMPUT MATH, V15, P715, DOI 10.1007/s10208-013-9150-3; Rockafellar R. T., 1970, CONVEX ANAL; Sigmund K., 1986, COMPLEXITY LANGUAGE, P88; Su Weijie, 2014, NIPS; Teschl G., 2012, ORDINARY DIFFERENTIA, V140; Weibull J W, 1997, EVOLUTIONARY GAME TH; Wibisono A., 2016, CORR	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704077
C	Krishnamurthy, A; Agarwal, A; Langford, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krishnamurthy, Akshay; Agarwal, Alekh; Langford, John			PAC Reinforcement Learning with Rich Observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose and study a new model for reinforcement learning with rich observations, generalizing contextual bandits to sequential decision making. These models require an agent to take actions based on observations ( features) with the goal of achieving long-term performance competitive with a large set of policies. To avoid barriers to sample-efficient learning associated with large observation spaces and general POMDPs, we focus on problems that can be summarized by a small number of hidden states and have long-term rewards that are predictable by a reactive function class. In this setting, we design and analyze a new reinforcement learning algorithm, Least Squares Value Elimination by Exploration. We prove that the algorithm learns near optimal behavior after a number of episodes that is polynomial in all relevant parameters, logarithmic in the number of policies, and independent of the size of the observation space. Our result provides theoretical justification for reinforcement learning with function approximation.	[Krishnamurthy, Akshay] Univ Massachusetts, Amherst, MA 01003 USA; [Agarwal, Alekh; Langford, John] Microsoft Res, New York, NY 10011 USA	University of Massachusetts System; University of Massachusetts Amherst; Microsoft	Krishnamurthy, A (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.	akshay@cs.umass.edu; alekha@microsoft.com; jcl@microsoft.com						Agarwal A., 2012, AISTATS; [Anonymous], 2006, ISAIM; [Anonymous], 2002, ICML; Antos A., 2008, MLJ; Auer P., 2002, SICOMP; Azizzadenesheli  K., 2016, COLT; Baird L., 1995, ICML; Brafman R. I., 2003, JMLR; Dann C., 2015, NIPS; Dudik M., 2011, UAI; Jiang N., 2015, ICML; Jong N., 2007, ABSTRACTION REFORMUL; Kakade S. M., 2003, ICML; Kearns M., 1999, NIPS; Kearns M. J., 2002, MLJ; Langford J., 2008, NIPS; Li L., 2010, ANN MATH AI; Mansour Y., 1999, COLT; Meuleau N., 1999, UAI; Mnih V, 2015, NATURE, V518, P529; Nguyen P., 2013, AISTATS; Pazis Jason, 2016, AAAI; Perkins T. J., 2002, NIPS; Reveliotis S., 2007, DEDS; Strehl A. L., 2006, ICML; Sutton R. S., 1999, NIPS; Tsitsiklis J. N., 1997, IEEE TAC	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704099
C	Krishnamurthy, A; Agarwal, A; Dudik, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krishnamurthy, Akshay; Agarwal, Alekh; Dudik, Miroslav			Contextual semibandits via supervised learning oracles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study an online decision making problem where on each round a learner chooses a list of items based on some side information, receives a scalar feedback value for each individual item, and a reward that is linearly related to this feedback. These problems, known as contextual semibandits, arise in crowdsourcing, recommendation, and many other domains. This paper reduces contextual semibandits to supervised learning, allowing us to leverage powerful supervised learning methods in this partial-feedback setting. Our first reduction applies when the mapping from feedback to reward is known and leads to a computationally efficient algorithm with near-optimal regret. We show that this algorithm outperforms state-of-the-art approaches on real-world learning-to-rank datasets, demonstrating the advantage of oracle-based algorithms. Our second reduction applies to the previously unstudied setting when the linear mapping from feedback to reward is unknown. Our regret guarantees are superior to prior techniques that ignore the feedback.	[Krishnamurthy, Akshay] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Agarwal, Alekh; Dudik, Miroslav] Microsoft Res, New York, NY USA	University of Massachusetts System; University of Massachusetts Amherst; Microsoft	Krishnamurthy, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	akshay@cs.umass.edu; alekha@microsoft.com; mdudik@microsoft.com						Agarwal A., 2014, ICML; [Anonymous], 1989, HLTH SERVICE RES MET; Audibert J.-Y., 2014, MATH OF OR; Auer P., 2002, SIAM J COMPUTING; Cesa-Bianchi N., 2012, JCSS; Chapelle Olivier, 2011, P LEARN RANK CHALL; Chen W., 2013, INT C MACH LEARN; Chu W., 2011, AISTATS; Daum e, 2009, MLJ; Dudik M., 2011, UAI; Gyorgy A., 2007, JMLR; Hsu D., 2010, THESIS; Kale S., 2010, NIPS; Kveton B., 2015, AISTATS; LAFFERTY J, 2001, ICML; Langford J., 2008, NIPS; Li L., 2010, WWW; MSLR, MSLR MICR LEARN RANK; Neu G., 2015, NIPS; Qin Lijing, 2014, ICDM; Rakhlin A., 2016, ICML; Swaminathan A., 2016, ARXIV160504812V2; Syrgkanis V., 2016, ICML	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704038
C	Krummenacher, G; McWilliams, B; Kilcher, Y; Buhmann, JM; Meinshausen, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krummenacher, Gabriel; McWilliams, Brian; Kilcher, Yannic; Buhmann, Joachim M.; Meinshausen, Nicolai			Scalable Adaptive Stochastic Optimization Using Random Projections	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Adaptive stochastic gradient methods such as ADAGRAD have gained popularity in particular for training deep neural networks. The most commonly used and studied variant maintains a diagonal matrix approximation to second order information by accumulating past gradients which are used to tune the step size adaptively. In certain situations the full-matrix variant of ADAGRAD is expected to attain better performance, however in high dimensions it is computationally impractical. We present ADA-LR and RADAGRAD two computationally efficient approximations to full-matrix ADAGRAD based on randomized dimensionality reduction. They are able to capture dependencies between features and achieve similar performance to full-matrix ADAGRAD but at a much smaller computational cost. We show that the regret of ADA-LR is close to the regret of full-matrix ADAGRAD which can have an up-to exponentially smaller dependence on the dimension than the diagonal variant. Empirically, we show that ADA-LR and RADAGRAD perform similarly to full-matrix ADAGRAD. On the task of training convolutional neural networks as well as recurrent neural networks, RADAGRAD achieves faster convergence than diagonal ADAGRAD.	[Krummenacher, Gabriel; Buhmann, Joachim M.; Meinshausen, Nicolai] Swiss Fed Inst Technol, Dept Comp Sci, Inst Machine Learning, Zurich, Switzerland; [Meinshausen, Nicolai] Swiss Fed Inst Technol, Dept Math, Seminar Stat, Zurich, Switzerland; [McWilliams, Brian] Disney Res, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Federal Institutes of Technology Domain; ETH Zurich	Krummenacher, G (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Inst Machine Learning, Zurich, Switzerland.	gabriel.krummenacher@inf.ethz.ch; brian@disneyresearch.com; yannic.kilcher@inf.ethz.ch; jbuhmann@inf.ethz.ch; meinshausen@stat.math.ethz.ch	Buhmann, Joachim/AAU-4760-2020					Allen-Zhu Z., 2016, P 33 INT C MACH LEAR; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Balduzzi D., 2016, ARXIV160401952; Balduzzi D., 2016, P 33 INT C MACH LEAR; Byrd RH, 2014, ARXIV PREPRINT ARXIV; Dauphin Y.N., 2015, P 29 ADV NEUR INF PR; Defazio Aaron, 2014, ADV NEURAL INFORM PR; Desjardins G., 2015, ADV NEURAL INFORM PR, P2071; Duchi J.C., 2013, ADV NEURAL INFORM PR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Gonen A., 2015, ARXIV150602649; Gong YC, 2013, PROC CVPR IEEE, P484, DOI 10.1109/CVPR.2013.69; Grosse RB, 2015, PR MACH LEARN RES, V37, P2304; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Heinze C., 2016, P AISTATS; Heinze C., 2014, ARXIV14063469; Hofmann T, 2015, ADV NEURAL INFORM PR; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Keskar N. S., 2015, ADAQN ADAPTIVE QUASI; Kingma D.P, P 3 INT C LEARNING R; Lucchi A., 2015, ARXIV150308316; Luo H., 2016, ARXIV160202202; Mahoney M. W., 2011, ARXIV11045557V3; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Martens J., 2015, P 32 INT C MACH LEAR; McWilliams B, 2014, ADV NEUR IN, V27; Neyshabur B., 2015, ADV NEURAL INFORM PR, P2413; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Zhang L., 2012, ARXIV12113046	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700018
C	Lahouti, F; Hassibi, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lahouti, Farshad; Hassibi, Babak			Fundamental Limits of Budget-Fidelity Trade-off in Label Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Digital crowdsourcing (CS) is a modern approach to perform certain large projects using small contributions of a large crowd. In CS, a taskmaster typically breaks down the project into small batches of tasks and assigns them to so-called workers with imperfect skill levels. The crowdsourcer then collects and analyzes the results for inference and serving the purpose of the project. In this work, the CS problem, as a human-in-the-loop computation problem, is modeled and analyzed in an information theoretic rate-distortion framework. The purpose is to identify the ultimate fidelity that one can achieve by any form of query from the crowd and any decoding (inference) algorithm with a given budget. The results are established by a joint source channel (de)coding scheme, which represent the query scheme and inference, over parallel noisy channels, which model workers with imperfect skill levels. We also present and analyze a query scheme dubbed k-ary incidence coding and study optimized query pricing in this setting.	[Lahouti, Farshad; Hassibi, Babak] CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA	California Institute of Technology	Lahouti, F (corresponding author), CALTECH, Elect Engn Dept, Pasadena, CA 91125 USA.	lahouti@caltech.edu; hassibi@caltech.edu						Abraham I., 2013, 26 C LEARN THEOR COL; Caltech, 2016, COMM SEISM NETW PROJ; Cover TM, 2006, ELEMENTS INFORM THEO; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Dekel O., 2009, COLT; Gamal A.E., 2011, NETWORK INFORM THEOR; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; National Audubon Society, 2019, CHRISTM BIRD COUNT H; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Smyth P., 1995, Advances in Neural Information Processing Systems 7, P1085; Vinayak R. Korlakai, 2014, ADV NEURAL INFORM PR, P2996; Whitehill J., 2009, ADV NEURAL INFORM PR, P2035; Zhang Y, 2014, PROC 27 INT C NEURAL, P1260	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702020
C	Lakkaraju, H; Leskovec, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lakkaraju, Himabindu; Leskovec, Jure			Confusions over Time: An Interpretable Bayesian Model to Characterize Trends in Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PREDICTION	We propose Confusions over Time (CoT), a novel generative framework which facilitates a multi-granular analysis of the decision making process. The CoT not only models the confusions or error properties of individual decision makers and their evolution over time, but also allows us to obtain diagnostic insights into the collective decision making process in an interpretable manner. To this end, the CoT models the confusions of the decision makers and their evolution over time via time-dependent confusion matrices. Interpretable insights are obtained by grouping similar decision makers (and items being judged) into clusters and representing each such cluster with an appropriate prototype and identifying the most important features characterizing the cluster via a subspace feature indicator vector. Experimentation with real world data on bail decisions, asthma treatments, and insurance policy approval decisions demonstrates that CoT can accurately model and explain the confusions of decision makers and their evolution over time.	[Lakkaraju, Himabindu; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Lakkaraju, H (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	himalv@cs.stanford.edu; jure@cs.stanford.edu						Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Dekel O., 2009, COLT; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Joglekar M, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P686; Kamar Ece, 2012, 11 INT C AUTONOMOUS, P467; Kemp C., 2006, AAAI, V3, P5; Kim B., 2014, ADV NEURAL INFORM PR, P1952; Lakkaraju Himabindu, 2015, P 2015 SIAM INT C DA, P181; Letham B, 2015, ANN APPL STAT, V9, P1350, DOI 10.1214/15-AOAS848; Liu C., 2012, INT C MACH LEARN, P17; Lou Y., 2012, P 18 ACM SIGKDD INT, P150; Neath AA, 2012, WIRES COMPUT STAT, V4, P199, DOI 10.1002/wics.199; Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735; Raykar V. C., 2009, P 26 ANN INT C MACH, P889, DOI DOI 10.1145/1553374.1553488; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; Whitehill J., 2009, ADV NEURAL INFORM PR, P2035; Xu Kevin S., 2013, Social Computing, Behavioral-Cultural Modeling and Prediction. 6th International Conference, SBP 2013. Proceedings, P201, DOI 10.1007/978-3-642-37210-0_22; Zhou Dengyong, 2012, ADV NEURAL INFORM PR, P2204	21	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701091
C	Lasserre, JB; Pauwels, E		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lasserre, Jean-Bernard; Pauwels, Edouard			Sorting out typicality with the inverse moment matrix SOS polynomial	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SHAPE; ZEROS	We study a surprising phenomenon related to the representation of a cloud of data points using polynomials. We start with the previously unnoticed empirical observation that, given a collection (a cloud) of data points, the sublevel sets of a certain distinguished polynomial capture the shape of the cloud very accurately. This distinguished polynomial is a sum-of-squares (SOS) derived in a simple manner from the inverse of the empirical moment matrix. In fact, this SOS polynomial is directly related to orthogonal polynomials and the Christoffel function. This allows to generalize and interpret extremality properties of orthogonal polynomials and to provide a mathematical rationale for the observed phenomenon. Among diverse potential applications, we illustrate the relevance of our results on a network intrusion detection task for which we obtain performances similar to existing dedicated methods reported in the literature.	[Lasserre, Jean-Bernard] Univ Toulouse, CNRS, LAAS, F-31400 Toulouse, France; [Lasserre, Jean-Bernard] Univ Toulouse, IMT, F-31400 Toulouse, France; [Pauwels, Edouard] Univ Toulouse 3 Paul Sabatier, IRIT, F-31400 Toulouse, France; [Pauwels, Edouard] Univ Toulouse 3 Paul Sabatier, IMT, F-31400 Toulouse, France	Centre National de la Recherche Scientifique (CNRS); Universite de Toulouse; Universite de Toulouse; Universite de Toulouse; Universite Toulouse III - Paul Sabatier; Universite de Toulouse; Universite Toulouse III - Paul Sabatier	Lasserre, JB (corresponding author), Univ Toulouse, CNRS, LAAS, F-31400 Toulouse, France.; Lasserre, JB (corresponding author), Univ Toulouse, IMT, F-31400 Toulouse, France.	lasserre@laas.fr; edouard.pauwels@irit.fr	Lasserre, Jean B/ABD-4285-2021	Lasserre, Jean B/0000-0003-0860-9913	project ERC-ADG TAMING [666981]; Air Force Office of Scientific Research, Air Force Material Command [FA9550-15-1-0500]; European Research Council	project ERC-ADG TAMING; Air Force Office of Scientific Research, Air Force Material Command(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); European Research Council(European Research Council (ERC)European Commission)	This work was partly supported by project ERC-ADG TAMING 666981, ERC-Advanced Grant of the European Research Council and grant number FA9550-15-1-0500 from the Air Force Office of Scientific Research, Air Force Material Command.	Berman RJ, 2009, INDIANA U MATH J, V58, P1921, DOI 10.1512/iumj.2009.58.3644; Bos L., 1998, RENDICONTI CIRCOLO M, V52, P277; Davis J., 2006, P 23 INT C MACH LEAR, V148, P233, DOI [DOI 10.1145/1143844.1143874, 10.1145/1143844.1143874]; Dunkl C. F., 2001, ORTOGONAL POLYNOMIAL; Golub GH, 1999, SIAM J SCI COMPUT, V21, P1222, DOI 10.1137/S1064827597328315; Gustafsson B, 2009, ADV MATH, V222, P1405, DOI 10.1016/j.aim.2009.06.010; HADI AS, 1994, J ROY STAT SOC B MET, V56, P393; Helton JW, 2008, ANN PROBAB, V36, P1453, DOI 10.1214/07-AOP365; Knorr E. M., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P126, DOI 10.1145/502512.502532; Lasserre JB, 2015, INT GAME THEORY REV, V17, DOI 10.1142/S0219198915400010; Lasserre JB, 2015, DISCRETE COMPUT GEOM, V54, P993, DOI 10.1007/s00454-015-9739-1; Lichman M, 2013, UCI MACHINE LEARNING; Oliver J. J., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P364; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Press W.H., 2007, NUMERICAL RECIPES, V3rd; Szego  G., 1974, C PUBLICATIONS, P23; Totik V, 2000, J ANAL MATH, V81, P283, DOI 10.1007/BF02788993; Williams G, 2002, 2002 IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P709, DOI 10.1109/ICDM.2002.1184035; Yamanishi K, 2004, DATA MIN KNOWL DISC, V8, P275, DOI 10.1023/B:DAMI.0000023676.72185.7c	19	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704020
C	Lattimore, F; Lattimore, T; Reid, MD		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lattimore, Finnian; Lattimore, Tor; Reid, Mark D.			Causal Bandits: Learning Good Interventions via Causal Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of using causal models to improve the rate at which good interventions can be learned online in a stochastic environment. Our formalism combines multi- arm bandits and causal inference to model a novel type of bandit feedback that is not exploited by existing approaches. We propose a new algorithm that exploits the causal feedback and prove a bound on its simple regret that is strictly better (in all quantities) than algorithms that do not use the additional causal information.	[Lattimore, Finnian; Reid, Mark D.] Australian Natl Univ, Canberra, ACT, Australia; [Lattimore, Finnian; Reid, Mark D.] NICTA, Data61, Sydney, NSW, Australia; [Lattimore, Tor] Indiana Univ, Bloomington, IN USA	Australian National University; Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); Indiana University System; Indiana University Bloomington	Lattimore, F (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Lattimore, F (corresponding author), NICTA, Data61, Sydney, NSW, Australia.	finn.lattimore@gmail.com; tor.lattimore@gmail.com; mark.reid@anu.edu.au						Alon N., 2015, PMLR, V40, P23; Audibert J.-Y., 2010, COLT 23 C LEARN THEO, P13; Avner O., 2012, ICML, P409; Bareinboim E., 2015, ADV NEURAL INFORM PR, V28, P1342; Bartok G, 2014, MATH OPER RES, V39, P967, DOI 10.1287/moor.2014.0663; Bottou L, 2013, J MACH LEARN RES, V14, P3207; CHERNOFF H, 1959, ANN MATH STAT, V30, P755, DOI 10.1214/aoms/1177706205; Eberhardt F., 2005, UAI; Eberhardt F., 2010, CAUSALITY OBJECTIVES, P87; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Gabillon V., 2012, ADV NEURAL INFORM PR, P3212; Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007; Hu H, 2014, P ADV NEUR INF PROC, V27, P2339; Jamieson K., 2014, C LEARN THEOR, P423; Kocak Tomas, 2014, ADV NEURAL INFORM PR, P613; Koller D., 2009, PROBABILISTIC GRAPHI; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li L., 2014, ARXIV14093653; Ortega PA, 2014, COMPLEX ADAPT SYST M, V2, DOI 10.1186/2194-3206-2-2; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Wu Y., 2015, ADV NEURAL INFORM PR, P1360; Yu J.Y., 2009, P 26 ANN INT C MACHI, P1177	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701052
C	Lee, CE; Li, YH; Shah, D; Song, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lee, Christina E.; Li, Yihua; Shah, Devavrat; Song, Dogyoon			Blind Regression: Nonparametric Regression for Latent Variable Models via Collaborative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the framework of blind regression motivated by matrix completion for recommendation systems: given m users, n movies, and a subset of user-movie ratings, the goal is to predict the unobserved user-movie ratings given the data, i.e., to complete the partially observed matrix. Following the framework of non-parametric statistics, we posit that user u and movie i have features x(1) (u) and x(2) (i) respectively, and their corresponding rating y (u; i) is a noisy measurement of f (x(1) (u); x(2) (i)) for some unknown function f. In contrast with classical regression, the features x = (x(1) (u); x(2) (i)) are not observed, making it challenging to apply standard regression methods to predict the unobserved ratings. Inspired by the classical Taylor's expansion for differentiable functions, we provide a prediction algorithm that is consistent for all Lipschitz functions. In fact, the analysis through our framework naturally leads to a variant of collaborative filtering, shedding insight into the widespread success of collaborative filtering in practice. Assuming each entry is sampled independently with probability at least max(m(-1+delta), n(-1/2+delta)) with delta > 0, we prove that the expected fraction of our estimates with error greater than epsilon is less than gamma(2)/epsilon(2) plus a polynomially decaying term, where gamma(2) is the variance of the additive entry-wise noise term. Experiments with the MovieLens and Netflix datasets suggest that our algorithm provides principled improvements over basic collaborative filtering and is competitive with matrix factorization methods.	[Lee, Christina E.; Li, Yihua; Shah, Devavrat; Song, Dogyoon] MIT, Dept Elect Engn & Comp Sci, Lab Informat & Decis Syst, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Lee, CE (corresponding author), MIT, Dept Elect Engn & Comp Sci, Lab Informat & Decis Syst, Cambridge, MA 02139 USA.	celee@mit.edu; liyihua@mit.edu; devavrat@mit.edu; dgsong@mit.edu			ARO under MURI [133668-5079809]; NSF [CMMI-1462158, CMMI-1634259]; Samsung Scholarship; Siebel Scholarship; NSF; Claude E. Shannon Research Assistantship	ARO under MURI(MURI); NSF(National Science Foundation (NSF)); Samsung Scholarship(Samsung); Siebel Scholarship; NSF(National Science Foundation (NSF)); Claude E. Shannon Research Assistantship	This work is supported in parts by ARO under MURI award 133668-5079809, by NSF under grants CMMI-1462158 and CMMI-1634259, and additionally by a Samsung Scholarship, Siebel Scholarship, NSF Graduate Fellowship, and Claude E. Shannon Research Assistantship.	Aditya ST, 2011, IEEE T INFORM THEORY, V57, P2327, DOI 10.1109/TIT.2011.2111190; Bresler G., 2014, ADV NEURAL INFORM PR, P3347; Bresler Guy, 2015, ARXIV150705371; Cai D, 2008, IEEE DATA MINING, P63, DOI 10.1109/ICDM.2008.57; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Fazel M, 2003, P AMER CONTR CONF, P2156, DOI 10.1109/acc.2003.1243393; Ganti R. S., 2015, ADV NEURAL INFORM PR, P1864; Goldberg D., 1992, COMMUN ACM; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan R., 2009, IEEE T INF THEORY, V56; KLEINBERG J, 2004, P 36 ANN ACM S THEOR, P569; Kleinberg J., 2003, P 4 ACM C EL COMM, P1, DOI [10.1145/779928.779929, DOI 10.1145/779928.779929]; Koren Y, 2011, RECOMMENDER SYSTEMS HANDBOOK, P145, DOI 10.1007/978-0-387-85820-3_5; Lin Z., 2009, CAMSAP, V61; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Liu Z, 2009, SIAM J MATRIX ANAL A, V31, P1235, DOI 10.1137/090755436; MACK YP, 1982, Z WAHRSCHEINLICHKEIT, V61, P405, DOI 10.1007/BF00539840; Maurer A., 2009, ARXIV E PRINTS; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Negahban S, 2012, J MACH LEARN RES, V13, P1665; Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860; Shen BH, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P757; Srebro N., 2004, ADV NEURAL INFORM PR, P1321; Wand MP, 1994, KERNEL SMOOTHING	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700072
C	Lee, J; James, LF; Choi, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lee, Juho; James, Lancelot F.; Choi, Seungjin			Finite-Dimensional BFRY Priors and Variational Bayesian Inference for Power Law Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Bayesian nonparametric methods based on the Dirichlet Process (DP), gamma process and beta process, have proven effective in capturing aspects of various datasets arising in machine learning. However, it is now recognized that such processes have their limitations in terms of the ability to capture power law behavior. As such there is now considerable interest in models based on the Stable Processs (SP), Generalized Gamma process (GGP) and Stable-Beta Process (SBP). These models present new challenges in terms of practical statistical implementation. In analogy to tractable processes such as the finite-dimensional Dirichlet process, we describe a class of random processes, we call iid finite-dimensional BFRY processes, that enables one to begin to develop efficient posterior inference algorithms such as variational Bayes that readily scale to massive datasets. For illustrative purposes, we describe a simple variational Bayes algorithm for normalized SP mixture models, and demonstrate its usefulness with experiments on synthetic and real-world datasets.	[Lee, Juho; Choi, Seungjin] POSTECH, Pohang, South Korea; [James, Lancelot F.] HKUST, Hong Kong, Peoples R China	Pohang University of Science & Technology (POSTECH); Hong Kong University of Science & Technology	Lee, J (corresponding author), POSTECH, Pohang, South Korea.	stonecold@postech.ac.kr; lancelot@ust.hk; seungjin@postech.ac.kr	Lee, Juho/AAA-2901-2022		IITP [B0101-16-0307]; National Research Foundation (NRF) of Korea [NRF-2013R1A2A2A01067464]; HKSAR [RGC-HKUST 601712]	IITP(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); National Research Foundation (NRF) of Korea(National Research Foundation of Korea); HKSAR	This work was supported by IITP (No. B0101-16-0307, Basic Software Research in Human-Level Lifelong Machine Learning (Machine Learning Center)) and by National Research Foundation (NRF) of Korea (NRF-2013R1A2A2A01067464), and supported in part by the grant RGC-HKUST 601712 of the HKSAR.	BERTOIN J, 2006, PROBAB MATH STAT-POL, V26, P315; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Blei DM, 2016, ARXIV160100670; Caron F., 2012, NIPS; Caron F., 2014, ARXIV14011137; Chen C., 2012, ICML; Cinlar E., 2010, PROBABILITY STOCHAST; Devroye L, 2014, STAT METHOD APPL-GER, V23, P307, DOI 10.1007/s10260-014-0260-0; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Favaro S, 2013, STAT SCI, V28, P335, DOI 10.1214/13-STS422; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Griffiths T. L., 2006, NIPS; Ishwaran H, 2004, J AM STAT ASSOC, V99, P175, DOI 10.1198/016214504000000179; Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758; Ishwaran H, 2002, STAT SINICA, V12, P941; Ishwaran H, 2001, J AM STAT ASSOC, V96, P1316, DOI 10.1198/016214501753382255; James L. F., 2015, ARXIV151007309; James LF, 2005, ANN STAT, V33, P1771, DOI 10.1214/009053605000000336; KINGMAN JFC, 1975, J ROY STAT SOC B MET, V37, P1; KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59; Kurihara K., 2007, IJCAI; Pitman J, 1997, ANN PROBAB, V25, P855; Pitman J, 2015, BERNOULLI, V21, P2484, DOI 10.3150/14-BEJ652; Teh Y. W., 2009, NIPS; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Thibaux R., 2007, AISTATS; Veitch V., 2015, ARXIV151203099; Welling M., 2011, ICML; Winkel M, 2005, J APPL PROBAB, V42, P138, DOI 10.1239/jap/1110381376	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702095
C	Lee, M; Jin, SH; Mimno, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lee, Moontae; Jin, Seok Hyun; Mimno, David			Beyond Exchangeability: The Chinese Voting Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many online communities present user-contributed responses such as reviews of products and answers to questions. User-provided helpfulness votes can highlight the most useful responses, but voting is a social process that can gain momentum based on the popularity of responses and the polarity of existing votes. We propose the Chinese Voting Process (CVP) which models the evolution of helpfulness votes as a self-reinforcing process dependent on position and presentation biases. We evaluate this model on Amazon product reviews and more than 80 StackExchange forums, measuring the intrinsic quality of individual responses and behavioral coefficients of different communities.	[Lee, Moontae; Jin, Seok Hyun; Mimno, David] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Lee, M (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	moontae@cs.cornell.edu; sj372@cornell.edu; mimno@cornell.edu						Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; Blei D.M., 2003, ADV NEURAL INFORM PR; Blei DM, 2011, J MACH LEARN RES, V12, P2461; Danescu-Niculescu-Mizil C., 2009, WWW, P141, DOI [10.1145/1526709.1526729, DOI 10.1145/1526709.1526729]; Ghose A., 2007, P 9 INT C EL COMM NE, P303, DOI DOI 10.1145/1282100.1282158; Joachims T, 2007, ACM T INFORM SYST, V25, DOI 10.1145/1229179.1229181; Kim S. -M., 2006, P 2006 C EMP METH NA; Liu J, 2007, P 2007 JOINT C EMP M, P334; Mamykina Lena, 2011, P SIGCHI C HUM FACT; Martin L, 2014, AAAI CONF ARTIF INTE, P1551; Otterbacher J, 2009, CHI2009: PROCEEDINGS OF THE 27TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P955; Salganik MJ, 2008, SOC PSYCHOL QUART, V71, P338, DOI 10.1177/019027250807100404; Salganik MJ, 2006, SCIENCE, V311, P854, DOI 10.1126/science.1121066; Shandwick W., 2012, KRC RES; Siersdorfer S, 2014, ACM T WEB, V8, DOI 10.1145/2628441; Sipos R, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P337, DOI 10.1145/2566486.2567998; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Tausczik Y. R., 2014, COMPUTER SUPPORTED C; Yue Y., 2010, P 19 INT C WORLD WID	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702109
C	Li, CT; Jegelka, S; Sra, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit			Fast Mixing Markov Chains for Strongly Rayleigh Measures, DPPs, and Constrained Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study probability measures induced by set functions with constraints. Such measures arise in a variety of real-world settings, where prior knowledge, resource limitations, or other pragmatic considerations impose constraints. We consider the task of rapidly sampling from such constrained measures, and develop fast Markov chain samplers for them. Our first main result is for MCMC sampling from Strongly Rayleigh (SR) measures, for which we present sharp polynomial bounds on the mixing time. As a corollary, this result yields a fast mixing sampler for Determinantal Point Processes (DPPs), yielding (to our knowledge) the first provably fast MCMC sampler for DPPs since their inception over four decades ago. Beyond SR measures, we develop MCMC samplers for probabilistic models with hard constraints and identify sufficient conditions under which their chains mix rapidly. We illustrate our claims by empirically verifying the dependence of mixing times on the key factors governing our theoretical bounds.	[Li, Chengtao; Jegelka, Stefanie; Sra, Suvrit] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Li, CT (corresponding author), MIT, Cambridge, MA 02139 USA.	ctli@mit.edu; stefje@csail.mit.edu; suvrit@mit.edu			NSF CAREER [1553284]; Google Research Award	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Research Award(Google Incorporated)	This research was partially supported by NSF CAREER 1553284 and a Google Research Award.	ALDOUS DJ, 1982, J LOND MATH SOC, V25, P564, DOI 10.1112/jlms/s2-25.3.564; Anari N., 2015, FOCS; Anari N., 2016, COLT; Borcea J, 2009, J AM MATH SOC, V22, P521; Bouchard- Cote A., 2010, NIPS; BRODER A, 1989, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.1989.63516; Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Bubley R, 1997, ANN IEEE SYMP FOUND, P223, DOI 10.1109/SFCS.1997.646111; Cesa- Bianchi N., 2009, COLT; Diaconis Persi, 1991, ANN APPL PROBAB, V1, P36, DOI DOI 10.1214/aoap/1177005980; Djolonga Josip, 2014, ADV NEURAL INFORM PR, P244; Dyer M, 1998, RANDOM STRUCT ALGOR, V13, P285, DOI 10.1002/(SICI)1098-2418(199810/12)13:3/4<285::AID-RSA6>3.0.CO;2-R; Dyer M., 1999, FOCS; Ermon Stefano, 2013, ADV NEURAL INFORM PR, P2085; Feder T., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P26, DOI 10.1145/129712.129716; Frieze A, 2014, SIAM J COMPUT, V43, P497, DOI 10.1137/120890971; Gartrell M., 2016, ARXIV160205436; Gelman A, 1992, STAT SCI, V7, P136, DOI 10.1214/ss/1177011136; Gotovos A., 2015, NIPS; Greig D., 1989, J ROYAL STAT SOC; Iyer R., 2015, AISTATS; Jerrum M., 1993, SIAM J COMPUTING; Jerrum M., 2004, JACM; Kang B., 2013, ADV NEURAL INFORM PR, P2319; Kathuria T., 2016, SAMPLING CONSTRAINED; Kulesza A., 2012, ARXIV12076083; Kulesza Alex, 2011, ICML; Li C., 2016, ICML; Maddison C.J., 2014, NEURIPS; Morris B, 2004, SIAM J COMPUT, V34, P195, DOI 10.1137/S0097539702411915; Rebeschini P., 2015, COLT; Sinclair A., 1992, COMB PROBAB COMPUT, V1, P351, DOI [DOI 10.1017/S0963548300000390, 10.1017/S0963548300000390]; Smith D., 2008, EMNLP; Spielman D., 2008, STOC; Sra, 2016, P INT C LEARN REPR I; Zhang J, 2015, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2015.216; [No title captured]	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701039
C	Li, CJ; Wang, ZR; Liu, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Chris Junchi; Wang, Zhaoran; Liu, Han			Online ICA: Understanding Global Dynamics of Nonconvex Optimization via Diffusion Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Solving statistical learning problems often involves nonconvex optimization. Despite the empirical success of nonconvex statistical optimization methods, their global dynamics, especially convergence to the desirable local minima, remain less well understood in theory. In this paper, we propose a new analytic paradigm based on diffusion processes to characterize the global dynamics of nonconvex statistical optimization. As a concrete example, we study stochastic gradient descent (SGD) for the tensor decomposition formulation of independent component analysis. In particular, we cast different phases of SGD into diffusion processes, i.e., solutions to stochastic differential equations. Initialized from an unstable equilibrium, the global dynamics of SGD transit over three consecutive phases: (i) an unstable Ornstein-Uhlenbeck process slowly departing from the initialization, (ii) the solution to an ordinary differential equation, which quickly evolves towards the desirable local minimum, and (iii) a stable Ornstein-Uhlenbeck process oscillating around the desirable local minimum. Our proof techniques are based upon Stroock and Varadhan's weak convergence of Markov chains to diffusion processes, which are of independent interest.	[Li, Chris Junchi; Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Princeton University	Li, CJ (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	junchil@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018; LI, chris/HDO-6232-2022					Agarwal A., 2013, ARXIV13107991; Aldous D., 1989, APPL MATH SCI, V77; Anandkumar A., 2016, ARXIV160205908; Anandkumar A., 2014, ARXIV14111488; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], 2013, ADV NEURAL INFORM PR; [Anonymous], 2015, ARXIV150903025; Arora S., 2015, J MACH LEARN RES, V40; Balakrishnan Sivaraman, 2014, ARXIV14082156; Bhojanapalli S., 2015, ARXIV150903917; Bronshtein I. N., 1998, HDB MATH; Cai T. T., 2015, ARXIV150603382; Candes E., 2014, ARXIV14071065; Chen Y., 2015, ADV NEURAL INFORM PR; Darken C., 1991, ADV NEURAL INFORM PR; De Sa Christopher, 2014, ARXIV14111134; Durrett R., 2010, CAMBRIDGE SERIES STA, V4th; Ethier S. N., 1985, MARKOV PROCESSES CHA, V282; GE R, 2015, ARXIV150302101; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gu Q., 2014, ADV NEURAL INFORM PR; Gu Q., 2016, INT C ART INT STAT; Hardt M., 2014, FDN COMPUTER SCI; Hirsch MW, 2013, DIFFERENTIAL EQUATIONS, DYNAMICAL SYSTEMS, AND AN INTRODUCTION TO CHAOS, 3RD EDITION, P1; Jain P., 2013, S THEOR COMP; Jain P., 2014, ARXIV14111087; Jain P., 2015, ARXIV150705854, DOI arXiv preprint arXiv:1507.05854; Lee J. D., 2016, ARXIV160204915; Levin D. A., 2009, MARKOV CHAINS MIXING; Li Chris J, 2016, ARXIV PREPRINT ARXIV; LI Q., 2015, ARXIV151106251; Liu, 2015, ADV NEURAL INFORM PR; Loh PL, 2015, J MACH LEARN RES, V16, P559; Mandt Stephan, 2016, ARXIV160202666; Mobahi H., 2016, ARXIV160104114; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Netrapalli P, 2014, ADV NEURAL INFORM PR; Oksendal B., 2013, STOCHASTIC DIFFERENT; Panageas I., 2016, ARXIV160500405; Qu Q, 2014, ADV NEURAL INFORM PR; Stroock D. W., 1979, MULTIDIMENSIONAL DIF, V233; Su Weijie, 2014, ADV NEURAL INFORM PR; Sun J., 2015, ARXIV151006096; Sun J., 2015, ARXIV151104777; Sun Ju, 2015, ARXIV151103607; Sun R., 2015, FDN COMPUTER SCI; Sun W., 2015, ARXIV150201425; Sun W, 2015, ADV NEUR IN, V28; Tan Kean Ming, 2016, ARXIV160408697; Tu S., 2015, ARXIV150703566; Wang Z., 2014, ARXIV14085352; Wang Z., 2015, ADV NEURAL INFORM PR; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; White C. D., 2015, ARXIV150607868; WRIGHT J., 2016, ARXIV160206664; Yang Z., 2015, ARXIV151104514; Zhang Y., 2014, ADV NEURAL INFORM PR; Zheng Q., 2015, ARXIV150606081	58	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702053
C	Li, P; Mitzenmacher, M; Slawski, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Ping; Mitzenmacher, Michael; Slawski, Martin			Quantized Random Projections and Non-Linear Estimation of Cosine Similarity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				JOHNSON	Random projections constitute a simple, yet effective technique for dimensionality reduction with applications in learning and search problems. In the present paper, we consider the problem of estimating cosine similarities when the projected data undergo scalar quantization to b bits. We here argue that the maximum likelihood estimator (MLE) is a principled approach to deal with the non-linearity resulting from quantization, and subsequently study its computational and statistical properties. A specific focus is on the on the trade-off between bit depth and the number of projections given a fixed budget of bits for storage or transmission. Along the way, we also touch upon the existence of a qualitative counterpart to the Johnson-Lindenstrauss lemma in the presence of quantization.	[Li, Ping; Slawski, Martin] Rutgers State Univ, New Brunswick, NJ 07102 USA; [Mitzenmacher, Michael] Harvard Univ, Cambridge, MA 02138 USA	Rutgers State University New Brunswick; Harvard University	Li, P (corresponding author), Rutgers State Univ, New Brunswick, NJ 07102 USA.	pingli@stat.rutgers.edu; michaelm@eecs.harvard.edu; martin.slawski@rutgers.edu			NSF [CCF-1535795, CCF-1320231];  [NSF-Bigdata-1419210];  [NSF-III-1360971]	NSF(National Science Foundation (NSF)); ; 	The work of Ping Li and Martin Slawski is supported by NSF-Bigdata-1419210 and NSF-III-1360971. The work of Michael Mitzenmacher is supported by NSF CCF-1535795 and NSF CCF-1320231.	Bingham E., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P245, DOI 10.1145/502512.502546; Boutsidis C., 2010, ADV NEURAL INFORM PR, P298; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Charikar M. S., 2002, P 34 ANN ACM S THEOR, P380, DOI DOI 10.1145/509907.509965; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; Fradkin D., 2003, P 9 ACM SIGKDD INT C, P517; Genz A., BVN FUNCTION COMPUTI; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Jacques L., 2013, ARXIV13051786; Jacques L, 2015, IEEE T INFORM THEORY, V61, P5012, DOI 10.1109/TIT.2015.2453355; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; KIEFFER JC, 1983, IEEE T INFORM THEORY, V29, P42, DOI 10.1109/TIT.1983.1056622; Laska JN, 2012, IEEE T SIGNAL PROCES, V60, P3496, DOI 10.1109/TSP.2012.2194710; Li M, 2012, IEEE INT WORKSH MULT, P1, DOI 10.1109/MMSP.2012.6343406; Li P., 2014, P INT C MACH LEARN I; Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46; Lopes M., 2011, ADV NEURAL INFORM PR, P1206; Maillard O., 2009, ADV NEURAL INFORM PR, P1213; MAX J, 1960, IRE T INFORM THEOR, V6, P7, DOI 10.1109/TIT.1960.1057548; Shenton L., 1963, J ROYAL STAT SOC B, P305; Srivastava R, 2016, J COMPUT GRAPH STAT, V25, P954, DOI 10.1080/10618600.2015.1062771; Vempala S.S., 2005, RANDOM PROJECTION ME, DOI [10.1090/dimacs/065, DOI 10.1090/DIMACS/065]; Wang F., 2010, P 2010 SIAM INT C DA, P281	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704017
C	Li, RY; Jia, JY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Ruiyu; Jia, Jiaya			Visual Question Answering with Question Representation Update (QRU)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Our method aims at reasoning over natural language questions and visual images. Given a natural language question about an image, our model updates the question representation iteratively by selecting image regions relevant to the query and learns to give the correct answer. Our model contains several reasoning layers, exploiting complex visual relations in the visual question answering (VQA) task. The proposed network is end-to-end trainable through back-propagation, where its weights are initialized using pre-trained convolutional neural network (CNN) and gated recurrent unit (GRU). Our method is evaluated on challenging datasets of COCO-QA [19] and VQA [2] and yields state-of-the-art performance.	[Li, Ruiyu; Jia, Jiaya] Chinese Univ Hong Kong, Hong Kong, Peoples R China	Chinese University of Hong Kong	Li, RY (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	ryli@cse.cuhk.edu.hk; leojia@cse.cuhk.edu.hk	Jia, Jiaya/I-3251-2012		Research Grants Council of the Hong Kong SAR [2150760]; National Science Foundation China [61133009]	Research Grants Council of the Hong Kong SAR(Hong Kong Research Grants Council); National Science Foundation China(National Natural Science Foundation of China (NSFC))	This work is supported by a grant from the Research Grants Council of the Hong Kong SAR (project No. 2150760) and by the National Science Foundation China, under Grant 61133009. We thank NVIDIA for providing Ruiyu Li a Tesla K40 GPU accelerator for this work.	Andreas J, 2016, ARXIV160101705; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Chen Kan, 2015, ARXIV151105960; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Hu B., 2014, P 27 INT C NEUR INF, P2042; Hu R, 2015, ARXIV151104164; Ilija I., 2016, ARXIV160401485; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Ma L., 2015, ARXIV150600333; Malinowski M., 2014, ADV NEURAL INFORM PR, V27, P1682; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Noh H., 2015, ARXIV151105756; Ren M., 2015, ADV NEURAL INFORM PR, V28, P2953; Shih K. J., 2015, ARXIV151107394; Sukhbaatar S., 2015, ABS150308895 CORR; Sutskever I., 2014, NEURIPS, V2; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wong K.-F., 2015, ARXIV PREPRINT ARXIV; WU ZB, 1994, 32ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, P133; Xiong C., 2016, ARXIV160301417; Xu H., 2015, ARXIV151105234; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z., 2015, ARXIV151102274; Zhou Bolei, 2015, ARXIV151202167; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702008
C	Li, YZ; Turner, RE		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Yingzhen; Turner, Richard E.			Renyi Divergence Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper introduces the variational Renyi bound (VR) that extends traditional variational inference to Renyi's alpha-divergences. This new family of variational methods unifies a number of existing approaches, and enables a smooth interpolation from the evidence lower-bound to the log (marginal) likelihood that is controlled by the value of alpha that parametrises the divergence. The reparameterization trick, Monte Carlo approximation and stochastic optimisation methods are deployed to obtain a tractable and unified framework for optimisation. We further consider negative alpha values and propose a novel variational inference method as a new special case in the proposed framework. Experiments on Bayesian neural networks and variational auto-encoders demonstrate the wide applicability of the VR bound.	[Li, Yingzhen; Turner, Richard E.] Univ Cambridge, Cambridge CB2 1PZ, England	University of Cambridge	Li, YZ (corresponding author), Univ Cambridge, Cambridge CB2 1PZ, England.	yl494@cam.ac.uk; ret26@cam.ac.uk			Schlumberger Foundation FFTF fellowship; EPSRC [EP/M026957/1, EP/L000776/1]	Schlumberger Foundation FFTF fellowship(Schlumberger); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank the Cambridge MLG members and the reviewers for comments. YL thanks the Schlumberger Foundation FFTF fellowship. RET thanks EPSRC grants #EP/M026957/1 and EP/L000776/1.	Amari S.-i., 1985, DIFFERENTIAL GEOMETR, V28; Beal M.J, 2003, THESIS; Broderick T., 2013, ADV NEURAL INFORM PR; Bui T. D., 2016, P 33 INT C MACH LEAR; Burda Yuri, 2016, INT C LEARN REPR; Dehaene, 2015, ARXIV150308060; Depeweg S., 2016, ARXIV160507127; Gelman A., 2015, ADV NEURAL INFORM PR; Gelman Andrew, 2014, ARXIV PREPRINT ARXIV, V157; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Hernandez-Lobato J.M., 2016, INT C MACH LEARN ICM; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma DP, 2015, INT C LEARN REPR ICL; Li Y., 2015, ADV NEURAL INFORM PR; Minka T., 2004, MSRT2004149; Minka Tom, 2001, UNCERTAINTY ARTIFICI; Minka Tom, 2005, TECH REP; Opper M, 2005, J MACH LEARN RES, V6, P2177; Paisley J., 2012, INT C MACH LEARN ICM; Ranganath R., 2014, ARTIFICIAL INTELLIGE; Renyi A., 1961, BERK S MATH STAT PRO, V1; Rezende D. J., 2014, P 30 INT C MACH LEAR; TSALLIS C, 1988, J STAT PHYS, V52, P479, DOI 10.1007/BF01016429; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; van Erven T, 2014, IEEE T INFORM THEORY, V60, P3797, DOI 10.1109/TIT.2014.2320500; Xu M., 2014, ADV NEURAL INFORM PR	29	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701065
C	Li, YB; Dong, WS; Xie, XM; Shi, GM; Li, X; Xu, DL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Yongbo; Dong, Weisheng; Xie, Xuemei; Shi, Guangming; Li, Xin; Xu, Donglai			Learning Parametric Sparse Models for Image Super-Resolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Learning accurate prior knowledge of natural images is of great importance for single image super-resolution (SR). Existing SR methods either learn the prior from the low/high-resolution patch pairs or estimate the prior models from the input low-resolution (LR) image. Specifically, high-frequency details are learned in the former methods. Though effective, they are heuristic and have limitations in dealing with blurred LR images; while the latter suffers from the limitations of frequency aliasing. In this paper, we propose to combine those two lines of ideas for image super-resolution. More specifically, the parametric sparse prior of the desirable high-resolution (HR) image patches are learned from both the input low-resolution (LR) image and a training image dataset. With the learned sparse priors, the sparse codes and thus the HR image patches can be accurately recovered by solving a sparse coding problem. Experimental results show that the proposed SR method outperforms existing state-of-the-art methods in terms of both subjective and objective image qualities.	[Li, Yongbo; Dong, Weisheng; Xie, Xuemei] Xidian Univ, Sch Elect Engn, State Key Lab ISN, Xian, Shaanxi, Peoples R China; [Shi, Guangming] Xidian Univ, Chinese Minist Educ, Key Lab IPIU, Xian, Shaanxi, Peoples R China; [Li, Xin] West Virginia Univ, Lane Dept CSEE, Morgantown, WV 26506 USA; [Xu, Donglai] Teesside Univ, Sch Sci & Engn, Middlesbrough, Cleveland, England	Xidian University; Xidian University; West Virginia University; University of Teesside	Dong, WS (corresponding author), Xidian Univ, Sch Elect Engn, State Key Lab ISN, Xian, Shaanxi, Peoples R China.	yongboli@stu.xidian.edu.cn; wsdong@mail.xidian.edu.cn; xmxie@mail.xidian.edu.cn; gmshi@xidian.edu.cn; Xin.Li@mail.wvu.edu	Li, Xin/A-7884-2011; xie, tumor/GYE-3455-2022	Li, Xin/0000-0003-2067-2763; 	Natural Science Foundation (NSF) of China [61622210, 61471281, 61632019, 61472301, 61390512]; Specialized Research Fund for the Doctoral Program of Higher Education [20130203130001]	Natural Science Foundation (NSF) of China(National Natural Science Foundation of China (NSFC)); Specialized Research Fund for the Doctoral Program of Higher Education(Specialized Research Fund for the Doctoral Program of Higher Education (SRFDP))	This work was supported in part by the Natural Science Foundation (NSF) of China under Grants(No. No. 61622210, 61471281, 61632019, 61472301, and 61390512), in part by the Specialized Research Fund for the Doctoral Program of Higher Education (No. 20130203130001).	Bevilacqua Marco, 2012, LOW COMPLEXITY SINGL; Dai D, 2015, COMPUT GRAPH FORUM, V34, P95, DOI 10.1111/cgf.12544; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong WS, 2015, INT J COMPUT VISION, V114, P217, DOI 10.1007/s11263-015-0808-y; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Dong WS, 2011, IEEE I CONF COMP VIS, P1259, DOI 10.1109/ICCV.2011.6126377; Dong Weisheng, 2011, IEEE Trans Image Process, V20, P1838, DOI 10.1109/TIP.2011.2108306; Egiazarian K, 2015, EUR SIGNAL PR CONF, P2849, DOI 10.1109/EUSIPCO.2015.7362905; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li YB, 2015, IEEE I CONF COMP VIS, P450, DOI 10.1109/ICCV.2015.59; Marquina A, 2008, J SCI COMPUT, V37, P367, DOI 10.1007/s10915-008-9214-8; Martin D, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P416, DOI 10.1109/ICCV.2001.937655; Timofte R, 2015, LECT NOTES COMPUT SC, V9006, P111, DOI 10.1007/978-3-319-16817-3_8; Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241; Yang JC, 2010, IEEE T IMAGE PROCESS, V19, P2861, DOI 10.1109/TIP.2010.2050625; Yu GS, 2012, IEEE T IMAGE PROCESS, V21, P2481, DOI 10.1109/TIP.2011.2176743; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47	19	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703014
C	Li, YZ; Liang, YY; Risteski, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Yuanzhi; Liang, Yingyu; Risteski, Andrej			Recovery Guarantee of Non-negative Matrix Factorization via Alternating Updates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Non-negative matrix factorization is a popular tool for decomposing data into feature and weight matrices under non-negativity constraints. It enjoys practical success but is poorly understood theoretically. This paper proposes an algorithm that alternates between decoding the weights and updating the features, and shows that assuming a generative model of the data, it provably recovers the ground-truth under fairly mild conditions. In particular, its only essential requirement on features is linear independence. Furthermore, the algorithm uses ReLU to exploit the non-negativity for decoding the weights, and thus can tolerate adversarial noise that can potentially be as large as the signal, and can tolerate unbiased noise much larger than the signal. The analysis relies on a carefully designed coupling between two potential functions, which we believe is of independent interest.	[Li, Yuanzhi; Liang, Yingyu; Risteski, Andrej] Princeton Univ, Dept Comp Sci, 35 Olden St, Princeton, NJ 08540 USA	Princeton University	Li, YZ (corresponding author), Princeton Univ, Dept Comp Sci, 35 Olden St, Princeton, NJ 08540 USA.	yuanzhil@cs.princeton.edu; yingyul@cs.princeton.edu; risteski@cs.princeton.edu	Li, Yuan/GXV-1310-2022		NSF [CCF-1527371, DMS-1317308]; Simons Investigator Award; Simons Collaboration Grant;  [ONR-N00014-16-1-2329]	NSF(National Science Foundation (NSF)); Simons Investigator Award; Simons Collaboration Grant; 	This work was supported in part by NSF grants CCF-1527371, DMS-1317308, Simons Investigator Award, Simons Collaboration Grant, and ONR-N00014-16-1-2329.	Arora S., 2012, FOCS; Arora S., 2012, ADV NEURAL INFORM PR, P2375; Arora S., 2013, ICML; ARORA S, 2015, COLT; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Awasthi Pranjal, 2015, NIPS, P2089; Bhattacharyya Chiranjib, 2016, P 33 INT C MACH LEAR; Blei D. M., 2012, COMMUNICATIONS ACM; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Lee DD, 1997, ADV NEUR IN, V9, P515; Lee DD, 2001, ADV NEUR IN, V13, P556; Liu Y.-K., 2012, NIPS; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Pehlevan C., 2015, ADV NEURAL INFORM PR, P2260; Pehlevan C, 2014, CONF REC ASILOMAR C, P769, DOI 10.1109/ACSSC.2014.7094553; Zhu J, 2012, ARXIV12023778	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703053
C	Li, YZ; Risteski, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Yuanzhi; Risteski, Andrej			Algorithms and matching lower bounds for approximately-convex optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In recent years, a rapidly increasing number of applications in practice requires optimizing non-convex objectives, like training neural networks, learning graphical models, maximum likelihood estimation. Though simple heuristics such as gradient descent with very few modifications tend to work well, theoretical understanding is very weak. We consider possibly the most natural class of non-convex functions where one could hope to obtain provable guarantees: functions that are "approximately convex", i.e. functions (f) over tilde : R-d -> R for which there exists a convex function f such that for all x, vertical bar(f) over tilde (x) - f(x)vertical bar <= Delta for a fixed value Delta. We then want to minimize (f) over tilde, i.e. output a point (x) over tilde such that (f) over tilde ((x) over tilde) <= min(x) (f) over tilde (x) + epsilon. It is quite natural to conjecture that for fixed epsilon, the problem gets harder for larger Delta, however, the exact dependency of epsilon and Delta is not known. In this paper, we significantly improve the known lower bound on Delta as a function of epsilon and an algorithm matching this lower bound for a natural class of convex bodies. More precisely, we identify a function T : R+ -> R+ such that when Delta = O (T(epsilon)), we can give an algorithm that outputs a point (x) over tilde such that (f) over tilde ((x) over tilde) <= min(x) (f) over tilde (x) + epsilon within time poly (d, 1/epsilon). On the other hand, when Delta = Omega (T(epsilon)), we also prove an information theoretic lower bound that any algorithm that outputs such a (x) over tilde must use super polynomial number of evaluations of (f) over tilde.	[Li, Yuanzhi; Risteski, Andrej] Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA	Princeton University	Li, YZ (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA.	yuanzhil@cs.princeton.edu; risteski@cs.princeton.edu	Li, Yuan/GXV-1310-2022					Agarwal A., 2010, P COLT, P28; Belloni A., 2015, C LEARN THEOR, P240; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Dyer M, 2014, MATH PROGRAM, V147, P207, DOI 10.1007/s10107-013-0718-0; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Nemirovskii Arkadii, 1983, WILEY INTERSCIENCE S; Nesterov Yurii, FDN COMPUTATIONAL MA, P1; Shamir O., 2013, P ANN C LEARN THEOR; Singer Yaron, 2015, NIPS, P3186	10	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704100
C	Li, YZ; Risteski, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Yuanzhi; Risteski, Andrej			Approximate maximum entropy principles via Goemans-Williamson with applications to provable variational methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS; CUT	The well known maximum- entropy principle due to Jaynes, which states that given mean parameters, the maximum entropy distribution matching them is in an exponential family has been very popular in machine learning due to its "Occam's razor" interpretation. Unfortunately, calculating the potentials in the maximum-entropy distribution is intractable [BGS14]. We provide computationally efficient versions of this principle when the mean parameters are pairwise moments: we design distributions that approximately match given pairwise moments, while having entropy which is comparable to the maximum entropy distribution matching those moments. We additionally provide surprising applications of the approximate maximum entropy principle to designing provable variational methods for partition function calculations for Ising models without any assumptions on the potentials of the model. More precisely, we show that we can get approximation guarantees for the log-partition function comparable to those in the low-temperature limit, which is the setting of optimization of quadratic forms over the hypercube. ([AN06])	[Li, Yuanzhi; Risteski, Andrej] Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA	Princeton University	Li, YZ (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08450 USA.	yuanzhil@cs.princeton.edu; risteski@cs.princeton.edu	Li, Yuan/GXV-1310-2022					Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9; Alon N, 2006, SIAM J COMPUT, V35, P787, DOI 10.1137/S0097539704441629; Barak B, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P31, DOI 10.1145/2591796.2591886; Bethge Matthias, 2007, NEAR MAXIMUM ENTROPY; Bresler Guy, 2014, ADV NEURAL INFORM PR, P1062; Charikar M, 2004, ANN IEEE SYMP FOUND, P54, DOI 10.1109/FOCS.2004.39; Ellis R. S., 2012, ENTROPY LARGE DEVIAT, V271; ELLIS RS, 1978, J STAT PHYS, V19, P149, DOI 10.1007/BF01012508; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; GRIFFITH.RB, 1967, J MATH PHYS, V8, P478, DOI 10.1063/1.1705219; JAYNES ET, 1957, PHYS REV, V106, P620, DOI 10.1103/PhysRev.106.620; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Risteski Andrej, 2016, P C LEARN THEOR COLT; Singh M, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P50, DOI 10.1145/2591796.2591803; Sly A, 2012, ANN IEEE SYMP FOUND, P361, DOI 10.1109/FOCS.2012.56; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright Martin J, 2003, TREE REWEIGHTED BELI; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701026
C	Lian, XR; Zhang, H; Hsieh, CJ; Huang, YJ; Liu, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lian, Xiangru; Zhang, Huan; Hsieh, Cho-Jui; Huang, Yijun; Liu, Ji			A Comprehensive Linear Speedup Analysis for Asynchronous Stochastic Parallel Optimization from Zeroth-Order to First-Order	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Asynchronous parallel optimization received substantial successes and extensive attention recently. One of core theoretical questions is how much speedup (or benefit) the asynchronous parallelization can bring to us. This paper provides a comprehensive and generic analysis to study the speedup property for a broad range of asynchronous parallel stochastic algorithms from the zeroth order to the first order methods. Our result recovers or improves existing analysis on special cases, provides more insights for understanding the asynchronous parallel behaviors, and suggests a novel asynchronous parallel zeroth order method for the first time. Our experiments provide novel applications of the proposed asynchronous parallel zeroth order method on hyper parameter tuning and model blending problems.	[Lian, Xiangru; Huang, Yijun; Liu, Ji] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; [Zhang, Huan] Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA; [Hsieh, Cho-Jui] Univ Calif Davis, Dept Comp Sci, Davis, CA 95616 USA	University of Rochester; University of California System; University of California Davis; University of California System; University of California Davis	Lian, XR (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.	xiangru@yandex.com; victzhang@gmail.com; chohsieh@ucdavis.edu; huangyj0@gmail.com; ji.liu.uwisc@gmail.com			NSF [CNS-1548078]	NSF(National Science Foundation (NSF))	This project is in part supported by the NSF grant CNS-1548078. We especially thank Chen-Tse Tsai for providing the code and data for the Yahoo Music Competition.	Agarwal A, 2011, NIPS; [Anonymous], 2014, DEEP LEARNING ELASTI; Avron H, 2015, J ACM, V62, DOI 10.1145/2814566; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Chaturapruek S., 2015, ADV NEURAL INFORM PR, V28, P1531; Chen P.-L., 2012, KDDCUP; De Sa C., 2015, NIPS, P2674; Dean J., 2012, ADV NEURAL INFORM PR, V25; Dror Gideon, 2012, PROC KDD CUP 2011; Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659; Fellus J, 2015, NEUROCOMPUTING, V169, P262, DOI 10.1016/j.neucom.2014.11.076; Feyzmahdavian H. R., 2015, ASYNCHRONOUS MINI BA; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gimpel K., 2010, P 14 C COMP NAT LANG, P213; HONG M., 2014, ARXIV14126058; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2370; Jamieson Kevin G., 2012, NIPS; Li Mu, 2014, P 11 USENIX C OP SYS, P583; Lian X., 2015, P ADV NEUR INF PROC, P2719; Liu J., 2014, ARXIV14033862; Liu J., 2014, ICML; Mania H., 2015, ARXIV150706970; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; NESTEROV Y, 2011, TECHNICAL REPORT; Niu F., 2011, NIPS; Paine T., 2013, NIPS; Peng Z., 2015, AROCK ALGORITHMIC FR; Petroni F., 2014, ACM C REC SYST; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Smola, 2015, ADV NEURAL INFORM PR, P2629; Wei Jinliang, 2013, ARXIV13127869; Yun H., 2013, ARXIV13120193; Zhang R., 2014, ICML; Zhao SY, 2016, AAAI CONF ARTIF INTE, P2379	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704075
C	Liao, RJ; Schwing, A; Zemel, RS; Urtasun, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liao, Renjie; Schwing, Alexander; Zemel, Richard S.; Urtasun, Raquel			Learning Deep Parsimonious Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and flexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine grained categorization, and zero-shot learning.	[Liao, Renjie; Zemel, Richard S.; Urtasun, Raquel] Univ Toronto, Toronto, ON, Canada; [Schwing, Alexander] Univ Illinois, Urbana, IL USA; [Zemel, Richard S.] Canadian Inst Adv Res, Toronto, ON, Canada	University of Toronto; University of Illinois System; University of Illinois Urbana-Champaign; Canadian Institute for Advanced Research (CIFAR)	Liao, RJ (corresponding author), Univ Toronto, Toronto, ON, Canada.	rjliao@cs.toronto.edu; aschwing@illinois.edu; zemel@cs.toronto.edu; urtasun@cs.toronto.edu			Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]; NVIDIA;  [ONR-N00014-14-1-0232]	Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC); NVIDIA; 	This work was partially supported by ONR-N00014-14-1-0232, NVIDIA and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	Akata Z., 2013, P CVPR; Akata Z., 2015, P CVPR; [Anonymous], 2010, CALIFORNIA I TECHNOL; [Anonymous], 2015, TENSORFLOW LARGE SCA; Arthur D., 2007, P SODA; Chen Gang, 2015, ARXIV150103084; Chen W., 2015, ARXIV150404788; Cogswell M., 2016, P ICLR; De Lathauwer Lieven, 2000, SIAM J MATRIX ANAL A; Donahue J., 2013, 31 INT C MACH LEARN; Goodfellow I., 2013, P ICLR; Han S., 2016, P ICLR; Hanson S. J., 1989, P NIPS; Hariharan B., 2014, P ECCV; Hassibi B., 1993, P NIPS; Hershey J. R., 2015, ARXIV150804306; Hinton G., 2012, IEEE SIGNAL PROCESSI; Hinton G. E., 2006, SCIENCE; Ioffe S, 2015, ICML 2015; Jia Y., 2014, ARXIV14085093; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A., 2012, NIPS, P1106, DOI DOI 10.1145/3065386; LeCun Y., 1998, P IEEE; LeCun Y., 1989, P NIPS; LeCun Y. A., 2015, NATURE; Schutze H., 2008, INTRO INFORM RETRIEV, V39; Srivastava N., 2014, JMLR; Sutskever I, 2014, P 27 INT C NEURAL IN, V2; Tian F., 2014, AAAI; Tibshirani R., 1996, J ROYAL STAT SOC; Tikhonov AN, 1943, STABILITY INVERSE PR; Trigeorgis G., 2014, P ICML; Wan L., 2013, P ICML; Wang Z, 2015, ARXIV150900151; Yang J., 2016, ARXIV160403628; Zhang N., 2014, P ECCV	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702010
C	Lin, JH; Rosasco, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lin, Junhong; Rosasco, Lorenzo			Optimal Learning for Multi-pass Stochastic Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION	We analyze the learning properties of the stochastic gradient method when multiple passes over the data and mini-batches are allowed. In particular, we consider the square loss and show that for a universal step-size choice, the number of passes acts as a regularization parameter, and optimal finite sample bounds can be achieved by early-stopping. Moreover, we show that larger step-sizes are allowed when considering mini-batches. Our analysis is based on a unifying approach, encompassing both batch and stochastic gradient methods as special cases.	[Lin, Junhong; Rosasco, Lorenzo] MIT, LCSL, IIT, Cambridge, MA 02139 USA; [Rosasco, Lorenzo] Univ Genoa, DIBRIS, Genoa, Italy	Massachusetts Institute of Technology (MIT); University of Genoa	Lin, JH (corresponding author), MIT, LCSL, IIT, Cambridge, MA 02139 USA.	junhong.lin@iit.it; lrosasco@mit.edu	Lin, Junhong/M-9045-2016	Lin, Junhong/0000-0002-4507-9424	Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216]; Italian Ministry of Education, University and Research [RBFR12M3AC]	Center for Brains, Minds and Machines (CBMM) - NSF STC; Italian Ministry of Education, University and Research(Ministry of Education, Universities and Research (MIUR))	This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. L. R. acknowledges the financial support of the Italian Ministry of Education, University and Research FIRB project RBFR12M3AC.	Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Boyd S., 2007, EE364B STANDF U; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cotter A., 2011, P NEURIPS GRAN SPAIN; Dekel O, 2012, J MACH LEARN RES, V13, P165; Dieuleveut A, 2016, ANN STAT, V44, P1363, DOI 10.1214/15-AOS1391; Hardt Moritz, 2016, INT C MACH LEARN; Lin J, 2016, INT CONF EUR ENERG; Lin Junhong, 2016, J MACH LEARN RES, V17, P2718; Minsker S., 2011, SOME EXTENSIONS BERN; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Ng A, 2016, MACHINE LEARNING; Orabona F, 2014, ADV NEUR IN, V27; PINELIS IF, 1986, THEOR PROBAB APPL+, V30, P143, DOI 10.1137/1130013; Poljak B.T, 1987, INTRO OPTIMIZATION; Rosasco L., 2015, ADV NEURAL INFORM PR, V28, P1630; Rudi A, 2015, ADV NEUR IN, V28; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shamir O., 2013, P INT C MACH LEARN A, P71; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; Steinwart I., 2008, SUPPORT VECTOR MACHI; Tarres P, 2014, IEEE T INFORM THEORY, V60, P5716, DOI 10.1109/TIT.2014.2332531; Tropp J. A., 2012, TECHNICAL REPORT; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Zhang T, 2005, NEURAL COMPUT, V17, P2077, DOI 10.1162/0899766054323008	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701070
C	Lin, M; Ye, JP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lin, Ming; Ye, Jieping			A Non-convex One-Pass Framework for Generalized Factorization Machine and Rank-One Matrix Sensing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We develop an efficient alternating framework for learning a generalized version of Factorization Machine (gFM) on steaming data with provable guarantees. When the instances are sampled from d dimensional random Gaussian vectors and the target second order coefficient matrix in gFM is of rank k, our algorithm converges linearly, achieves O(epsilon) recovery error after retrieving O(k(3)d log(1/epsilon)) training instances, consumes O(kd) memory in one-pass of dataset and only requires matrix-vector product operations in each iteration. The key ingredient of our framework is a construction of an estimation sequence endowed with a so-called Conditionally Independent RIP condition (CI-RIP). As special cases of gFM, our framework can be applied to symmetric or asymmetric rank-one matrix sensing problems, such as inductive matrix completion and phase retrieval.	[Lin, Ming; Ye, Jieping] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Lin, M (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	linmin@umich.edu; jpye@umich.edu			NIH [RF1AG051710]; NSF [III-1421057, III-1421100]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This work was supported in part by research grants from NIH (RF1AG051710) and NSF (III-1421057 and III-1421100).	Blondel M, 2016, PR MACH LEARN RES, V48; Cai TT, 2015, ANN STAT, V43, P102, DOI 10.1214/14-AOS1267; Candes E. J., 2011, ARXIV11090573; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Davenport M. A., 2016, ARXIV160106422; Hardt M., 2013, ARXIV13120925; Hardt M., 2013, ARXIV13112495; Hardt M., 2014, ARXIV14074070; HONG L., 2013, P 6 ACM INT C WEB SE, P557, DOI DOI 10.1145/2433396.2433467; Jain P., 2012, ARXIV12120467; Jain P., 2013, ABS13060626 CORR; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Kueng R, 2014, ARXIV14106913; Lee K., 2013, ARXIV13120525; Netrapalli Praneeth, 2013, ARXIV13060160; Rendle Steffen, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P995, DOI 10.1109/ICDM.2010.127; Rendle S, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P635; Stewart G., 1990, MATRIX PERTURBATION; Tropp J. A., 2015, ARXIV150101571; Tropp Joel A, 2014, ARXIV14051102; Zhao  T., 2015, NONCONVEX LOW RANK M; Zhong K, 2015, LECT NOTES ARTIF INT, V9355, P3, DOI 10.1007/978-3-319-24486-0_1; Zhu Peizhen, 2013, J NUMERICAL MATH, V21	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703046
C	Lin, P; Zhang, B; Guo, T; Wang, Y; Chen, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lin, Peng; Zhang, Bang; Guo, Ting; Wang, Yang; Chen, Fang			Infinite Hidden Semi-Markov Modulated Interaction Point Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The correlation between events is ubiquitous and important for temporal events modelling. In many cases, the correlation exists between not only events' emitted observations, but also their arrival times. State space models (e.g., hidden Markov model) and stochastic interaction point process models (e.g., Hawkes process) have been studied extensively yet separately for the two types of correlations in the past. In this paper, we propose a Bayesian nonparametric approach that considers both types of correlations via unifying and generalizing the hidden semi-Markov model and interaction point process model. The proposed approach can simultaneously model both the observations and arrival times of temporal events, and automatically determine the number of latent states from data. A Metropolis-within-particle-Gibbs sampler with ancestor resampling is developed for efficient posterior inference. The approach is tested on both synthetic and real-world data with promising outcomes.	[Lin, Peng; Zhang, Bang; Guo, Ting; Wang, Yang; Chen, Fang] Data61 CSIRO, Australian Technol Pk,13 Garden St, Eveleigh, NSW 2015, Australia; [Lin, Peng] Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia	Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of New South Wales Sydney	Lin, P (corresponding author), Data61 CSIRO, Australian Technol Pk,13 Garden St, Eveleigh, NSW 2015, Australia.; Lin, P (corresponding author), Univ New South Wales, Sch Comp Sci & Engn, Sydney, NSW, Australia.	peng.lin@data61.csiro.au; bang.zhang@data61.csiro.au; ting.guo@data61.csiro.au; yang.wang@data61.csiro.au; fang.chen@data61.csiro.au						Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; [Anonymous], 2011, P SUSTKDD WORKSH DAT; Beal MJ, 2001, ADV NEURAL INFORM PR, P577; Daley D. J., 2007, INTRO THEORY POINT P, V2; DIGGLE PJ, 1994, INT STAT REV, V62, P99, DOI 10.2307/1403548; Fox E. B., 2008, 25 INT C MACHINE LEA, P312; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; HAWKES AG, 1974, J APPL PROBAB, V11, P493, DOI 10.2307/3212693; Johnson, 2012, ARXIV12033485; KINGMAN JFC, 1964, P CAMB PHILOS SOC, V60, P923; Li LD, 2015, AAAI CONF ARTIF INTE, P672; Lin P., 2016, 30 AAAI C ART INT AA; Lindsten F, 2014, J MACH LEARN RES, V15, P2145; Murphy Kevin, 2002, HIDDEN SEMIMAR UNPUB; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5; Tripuraneni Nilesh, 2015, ADV NEURAL INFORM PR, V28, P2386; Van Gael J., 2008, PROC 25 INT C MACHIN, V25, P1088; Whye Teh Yee, 2006, J AM STAT ASSOC, V101, P476, DOI DOI 10.1198/016214506000000302; Yu SZ, 2010, ARTIF INTELL, V174, P215, DOI 10.1016/j.artint.2009.11.011; Zhou K, 2013, ICML	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701100
C	Liu, C; Zhu, J; Song, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liu, Chang; Zhu, Jun; Song, Yang			Stochastic Gradient Geodesic MCMC Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose two stochastic gradient MCMC methods for sampling from Bayesian posterior distributions defined on Riemann manifolds with a known geodesic flow, e.g. hyperspheres. Our methods are the first scalable sampling methods on these manifolds, with the aid of stochastic gradients. Novel dynamics are conceived and 2nd-order integrators are developed. By adopting embedding techniques and the geodesic integrator, the methods do not require a global coordinate system of the manifold and do not involve inner iterations. Synthetic experiments show the validity of the method, and its application to the challenging inference for spherical topic models indicate practical usability and efficiency.	[Liu, Chang; Zhu, Jun] Tsinghua Univ, State Key Lab Intell Tech & Syst, Ctr Bioinspired Comp Res, Dept Comp Sci & Tech,TNList Lab, Beijing, Peoples R China; [Song, Yang] Tsinghua Univ, Dept Phys, Beijing, Peoples R China; [Song, Yang] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Tsinghua University; Tsinghua University; Stanford University	Song, Y (corresponding author), Tsinghua Univ, Dept Phys, Beijing, Peoples R China.; Song, Y (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	chang-li14@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn; songyang@stanford.edu	Song, Yang/X-5282-2019		National Basic Research Program (973 Program) of China [2013CB329403]; National NSF of China [61620106010, 61322308, 61332007]; Tsinghua Initiative Scientific Research Program [20141080934]; Youth Top-notch Talent Support Program	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National NSF of China(National Natural Science Foundation of China (NSFC)); Tsinghua Initiative Scientific Research Program; Youth Top-notch Talent Support Program	The work was supported by the National Basic Research Program (973 Program) of China (No. 2013CB329403), National NSF of China Projects (Nos. 61620106010, 61322308, 61332007), the Youth Top-notch Talent Support Program, and Tsinghua Initiative Scientific Research Program (No. 20141080934).	Abraham R., 1978, FDN MECH; Ahn S., 2012, ARXIV12066380; Anh N. K., 2013, 4 INT S INF COMM TEC, P131; Banerjee A, 2005, J MACH LEARN RES, V6, P1345; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Brubaker M., 2012, P 15 INT C ART INT S, P161; Byrne S, 2013, SCAND J STAT, V40, P825, DOI 10.1111/sjos.12036; CHEN C., 2015, ADV NEURAL INFORM PR, P2269; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; Du C, 2015, ARXIV150604557; Ghosh K, 2003, J APPL STAT, V30, P145, DOI 10.1080/0266476022000023712; Girolami M, 2011, J R STAT SOC B, V73, P123, DOI 10.1111/j.1467-9868.2010.00765.x; Gopal Siddharth, 2014, P 31 INT C MACH LEAR; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; James I. M., 1976, TOPOLOGY STIEFEL MAN, V24; Lan SW, 2014, PR MACH LEARN RES, V32; Li Chunyuan, 2015, ARXIV151207662; Ma Y.A., 2015, ARXIV150604696, P2917; Mardia K. V., 2000, DIRECTIONAL STAT, P159; NASH J, 1956, ANN MATH, V63, P20, DOI 10.2307/1969989; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2, P2, DOI DOI 10.1201/B10905-6; Patterson S., 2013, P 26 INT C NEUR INF, P3102; Reisinger J., 2010, P 27 INT C MACH LEAR, P903; Song Y, 2016, BMC HEALTH SERV RES, V16, DOI 10.1186/s12913-016-1283-z; Stiefel E., 1935, COMMENT MATH HELV, V8, P305, DOI [10.1007/BF01199559, DOI 10.1007/BF01199559]; Straub J, 2015, JMLR WORKSH CONF PRO, V38, P930; Taghia J, 2014, IEEE T PATTERN ANAL, V36, P1701, DOI 10.1109/TPAMI.2014.2306426; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702029
C	Liu, CY; Belkin, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liu, Chaoyue; Belkin, Mikhail			Clustering with Bregman Divergences: an Asymptotic Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				QUANTIZATION; APPROXIMATION	Clustering, in particular k-means clustering, is a central topic in data analysis. Clustering with Bregman divergences is a recently proposed generalization of k-means clustering which has already been widely used in applications. In this paper we analyze theoretical properties of Bregman clustering when the number of the clusters k is large. We establish quantization rates and describe the limiting distribution of the centers as k -> infinity, extending well-known results for k-means clustering.	[Liu, Chaoyue; Belkin, Mikhail] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Liu, CY (corresponding author), Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA.	liu.2656@osu.edu; mbelkin@cse.ohio-state.edu			National Science Foundation	National Science Foundation(National Science Foundation (NSF))	We thank the National Science Foundation for financial support and to Brian Kulis for discussions.	Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0; Alsabti K., 1998, IPPS SPDP WORKSH HIG; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; BUCKLEW JA, 1982, IEEE T INFORM THEORY, V28, P239, DOI 10.1109/TIT.1982.1056486; Coates Adam, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P561, DOI 10.1007/978-3-642-35289-8_30; Fischer A, 2010, J MULTIVARIATE ANAL, V101, P2207, DOI 10.1016/j.jmva.2010.05.008; Gersho A., 2012, VECTOR QUANTIZATION, V159; Graf S., 2000, FDN QUANTIZATION PRO; Jiang K., 2012, P INT C NEUR INF PRO, P3158; Kaufman L., 2009, FINDING GROUPS DATA, V344; Krishna K, 1999, IEEE T SYST MAN CY B, V29, P433, DOI 10.1109/3477.764879; LINDER T, 1991, PROBL CONTROL INFORM, V20, P475; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274, DOI 10.1007/978-3-642-00202-1_24; MCCLURE DE, 1975, Q APPL MATH, V33, P1; Nock R, 2008, LECT NOTES ARTIF INT, V5212, P154, DOI 10.1007/978-3-540-87481-2_11; PANTER PF, 1951, P IRE, V39, P44, DOI 10.1109/JRPROC.1951.230419; Que QC, 2016, JMLR WORKSH CONF PRO, V51, P1375; Telgarsky M., 2012, ARXIV12066446; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; Zhang JW, 2011, NEUROCOMPUTING, V74, P1720, DOI 10.1016/j.neucom.2011.02.004	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704074
C	Liu, Q; Wang, DL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liu, Qiang; Wang, Dilin			Stein Variational Gradient Descent: A General Purpose Bayesian Inference Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a general purpose variational inference algorithm that forms a natural counterpart of gradient descent for optimization. Our method iteratively transports a set of particles to match the target distribution, by applying a form of functional gradient descent that minimizes the KL divergence. Empirical studies are performed on various real world models and datasets, on which our method is competitive with existing state-of-the-art methods. The derivation of our method is based on a new theoretical result that connects the derivative of KL divergence under smooth transforms with Stein's identity and a recently proposed kernelized Stein discrepancy, which is of independent interest.	[Liu, Qiang; Wang, Dilin] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA	Dartmouth College	Liu, Q (corresponding author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.	qiang.liu@dartmouth.edu; dilin.wang.gr@dartmouth.edu			NSF CRII [1565796]	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work is supported in part by NSF CRII 1565796.	Challis E, 2012, NIPS; Chwialkowski K., 2016, ARXIV160202964; Dai Bo, 2016, AISTATS; Gal Y., 2015, ARXIV150602142; Gershman S., 2012, ICML; Gorham J, 2015, ADV NEUR IN, V28; Han S., 2016, ARTIFICIAL INTELLIGE; Hernandez-Lobato J. M., 2015, ICML; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman Matthew D, 2013, JMLR; JAAKKOLA TS, 1999, LEARNING GRAPHICAL M, P163; Kac M., 1959, PROBABILITY RELATED, V1; Kucukelbir A., 2015, NIPS; Kulkarni T. D., 2014, ARXIV14025715; Lawrence C. M. B. N., 1998, NIPS; LAWRENCE ND, 2001, THESIS; Li Y., 2015, NIPS; Li Y., 2016, ARXIV160202311; Liu Q., 2016, ARXIV160203253; Maclaurin D., 2014, UAI; Marzouk Y., 2016, ARXIV160205023; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Ranganath R., 2014, AISTATS; REZENDE DJ, 2015, ICML; Robert C., 2013, MONTE CARLO STAT MET; STEIN C., 2004, STEINS METHOD, V46, P1; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Tran D., 2015, NIPS; Tran Dustin, 2016, INT C LEARN REPR, V5; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Welling M., 2011, ICML	34	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702085
C	Liu, Y; Chen, YL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liu, Yang; Chen, Yiling			A Bandit Framework for Strategic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PREDICTION	We consider a learner's problem of acquiring data dynamically for training a regression model, where the training data are collected from strategic data sources. A fundamental challenge is to incentivize data holders to exert effort to improve the quality of their reported data, despite that the quality is not directly verifiable by the learner. In this work, we study a dynamic data acquisition process where data holders can contribute multiple times. Using a bandit framework, we leverage the long-term incentive of future job opportunities to incentivize high-quality contributions. We propose a Strategic Regression-Upper Confidence Bound (SRUCB) framework, a UCB-style index combined with a simple payment rule, where the index of a worker approximates the quality of his past contributions and is used by the learner to determine whether the worker receives future work. For linear regression and a certain family of non-linear regression problems, we show that SR-UCB enables an O (root logT/T)-Bayesian Nash Equilibrium (BNE) where each worker exerts a target effort level that the learner has chosen, with T being the number of data acquisition stages. The SR-UCB framework also has some other desirable properties: (1) The indexes can be updated in an online fashion (hence computation is light). (2) A slight variant, namely Private SR-UCB (PSR-UCB), is able to preserve (O (log(-1) T); O (log(-1) T))-differential privacy for workers' data, with only a small compromise on incentives (each worker exerting a target effort level is an O (log(6)T/root T)-BNE).	[Liu, Yang; Chen, Yiling] Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA	Harvard University	Liu, Y (corresponding author), Harvard Univ, Sch Engn & Appl Sci, Cambridge, MA 02138 USA.	yangl@seas.harvard.edu; yiling@seas.harvard.edu			NSF [CCF-1301976]	NSF(National Science Foundation (NSF))	We acknowledge the support of NSF grant CCF-1301976.	[Anonymous], 2008, P 14 ACM SIGKDD INT; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Cai Yang, 2014, ARXIV14082539; Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626; Cummings Rachel, 2015, P 28 C LEARNING THEO, P448; Dwork C., ALGORITHMIC FDN DIFF; Dwork C., 2006, P 33 INT C AUT LANG; Ghosh A., 2013, P 4 C INN THEOR COMP, P233; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Ho C.-J., 2014, P 15 ACM C EC COMP, P359, DOI DOI 10.1145/2600057.2602880; Ioannidis Stratis, 2013, Web and Internet Economics. 9th International Conference, WINE 2013. Proceedings: LNCS 8289, P277, DOI 10.1007/978-3-642-45046-4_23; Jurca R, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P200; Kairouz Peter, 2013, ARXIV13110776; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lebanon Guy, M ESTIMATORS Z ESTIM; Mansour Yishay, 2015, P 16 ACM C EC COMP, P565, DOI DOI 10.1145/2764468.2764508; Miller N, 2005, MANAGE SCI, V51, P1359, DOI 10.1287/mnsc.1050.0379; Rakhlin A, 2011, ARXIV11095647; Rao BLS Prakasa, 1984, J MULTIVARIATE ANAL; Toulis P., 2015, P 16 ACM C EC COMP, P285	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701047
C	Lokhov, AY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lokhov, Andrey Y.			Reconstructing Parameters of Spreading Models from Partial Observations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				NETWORKS	Spreading processes are often modelled as a stochastic dynamics occurring on top of a given network with edge weights corresponding to the transmission probabilities. Knowledge of veracious transmission probabilities is essential for prediction, optimization, and control of diffusion dynamics. Unfortunately, in most cases the transmission rates are unknown and need to be reconstructed from the spreading data. Moreover, in realistic settings it is impossible to monitor the state of each node at every time, and thus the data is highly incomplete. We introduce an efficient dynamic message-passing algorithm, which is able to reconstruct parameters of the spreading model given only partial information on the activation times of nodes in the network. The method is generalizable to a large class of dynamic models, as well to the case of temporal graphs.	[Lokhov, Andrey Y.] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA; [Lokhov, Andrey Y.] Los Alamos Natl Lab, Theoret Div T4, Los Alamos, NM 87545 USA	United States Department of Energy (DOE); Los Alamos National Laboratory; United States Department of Energy (DOE); Los Alamos National Laboratory	Lokhov, AY (corresponding author), Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA.; Lokhov, AY (corresponding author), Los Alamos Natl Lab, Theoret Div T4, Los Alamos, NM 87545 USA.	lokhov@lanl.gov	Lokhov, Andrey/AEC-8069-2022	Lokhov, Andrey/0000-0003-3269-7263	LDRD Program at Los Alamos National Laboratory by the National Nuclear Security Administration of the U.S. Department of Energy [DE-AC52-06NA25396]	LDRD Program at Los Alamos National Laboratory by the National Nuclear Security Administration of the U.S. Department of Energy	The author is grateful to M. Chertkov and T. Misiakiewicz for discussions and comments, and acknowledges support from the LDRD Program at Los Alamos National Laboratory by the National Nuclear Security Administration of the U.S. Department of Energy under Contract No. DE-AC52-06NA25396.	Abrahao B, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P491; Altarelli F, 2014, PHYS REV LETT, V112, DOI 10.1103/PhysRevLett.112.118701; Amin K., 2014, P 31 INT C MACH LEAR, P1845; [Anonymous], 1969, THESIS CORNELL U ITH; Boccaletti S, 2006, PHYS REP, V424, P175, DOI 10.1016/j.physrep.2005.10.009; Daneshmand H, 2014, PR MACH LEARN RES, V32, P793; Dobson I, 2007, CHAOS, V17, DOI 10.1063/1.2737822; Du Nan, 2012, NIPS, P2780; Farajtabar M, 2015, JMLR WORKSH CONF PRO, V38, P232; Gomez-Rodriguez Manuel, 2011, ICML, P561; Gripon V, 2013, IEEE INT SYMP INFO, P2488, DOI 10.1109/ISIT.2013.6620674; Karrer B, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.016101; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Lokhov A. Y., 2016, ARXIV160808278; Lokhov A. Y., 2016, ARXIV150906893; Lokhov AY, 2015, PHYS REV E, V91, DOI 10.1103/PhysRevE.91.012811; Myers Seth, 2010, P INT C ADV NEURAL I, P1741; Netrapalli Praneeth, 2012, Performance Evaluation Review, V40, P211, DOI 10.1145/2318857.2254783; Nowzari C, 2016, IEEE CONTR SYST MAG, V36, P26, DOI 10.1109/MCS.2015.2495000; O'Dea R, 2013, J R SOC INTERFACE, V10, DOI 10.1098/rsif.2013.0016; Pastor-Satorras R, 2015, REV MOD PHYS, V87, P925, DOI 10.1103/RevModPhys.87.925; Pouget-Abadie J, 2015, PR MACH LEARN RES, V37, P977; Sefer E., 2015, DAT ENG ICDE 2015 IE	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700093
C	Lou, XH; Kansky, K; Lehrach, W; Laan, CC; Marthi, B; Phoenix, DS; George, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lou, Xinghua; Kansky, Ken; Lehrach, Wolfgang; Laan, C. C.; Marthi, Bhaskara; Phoenix, D. Scott; George, Dileep			Generative Shape Models: Joint Text Recognition and Segmentation with Very Little Training Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We demonstrate that a generative model for object shapes can achieve state of the art results on challenging scene text recognition tasks, and with orders of magnitude fewer training images than required for competing discriminative methods. In addition to transcribing text from challenging images, our method performs fine-grained instance segmentation of characters. We show that our model is more robust to both affine transformations and non-affine deformations compared to previous approaches.	[Lou, Xinghua; Kansky, Ken; Lehrach, Wolfgang; Laan, C. C.; Marthi, Bhaskara; Phoenix, D. Scott; George, Dileep] Vicarious FPC Inc, San Francisco, CA 94587 USA		Lou, XH (corresponding author), Vicarious FPC Inc, San Francisco, CA 94587 USA.	xinghua@vicarious.com; ken@vicarious.com; wolfgang@vicarious.com; cc@vicarious.com; bhaskara@vicarious.com; scott@vicarious.com; dileep@vicarious.com						Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102; Coates A, 2011, PROC INT CONF DOC, P440, DOI 10.1109/ICDAR.2011.95; COUGHLAN JM, 2002, EUR C COMP VIS, V2352, P453; Felzenszwalb P, 2008, 2008 IEEE C COMP VIS, P1, DOI DOI 10.1109/CVPR.2008.4587597; Jaderberg M, 2014, ARXIV; Jaderberg M., 3 INT C LEARN REPR S, P7; Karatzas D, 2013, PROC INT CONF DOC, P1484, DOI 10.1109/ICDAR.2013.221; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee CY, 2014, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR.2014.516; Mishra A, 2012, PROC CVPR IEEE, P2687, DOI 10.1109/CVPR.2012.6247990; Neumann L, 2013, IEEE I CONF COMP VIS, P97, DOI 10.1109/ICCV.2013.19; Novikova T, 2012, LECT NOTES COMPUT SC, V7577, P752, DOI 10.1007/978-3-642-33783-3_54; Shi CZ, 2013, PROC CVPR IEEE, P2961, DOI 10.1109/CVPR.2013.381; Tsochantaridis I., 2004, P 21 INT C MACH LEAR, P104, DOI DOI 10.1145/1015330.1015341; Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43; Weinman JJ, 2009, IEEE T PATTERN ANAL, V31, P1733, DOI 10.1109/TPAMI.2009.38; Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515; Ye QX, 2015, IEEE T PATTERN ANAL, V37, P1480, DOI 10.1109/TPAMI.2014.2366765	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700035
C	Luo, HP; Agarwal, A; Cesa-Bianchi, N; Langford, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Luo, Haipeng; Agarwal, Alekh; Cesa-Bianchi, Nicolo; Langford, John			Efficient Second Order Online Learning by Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose Sketched Online Newton (SON), an online second order learning algorithm that enjoys substantially improved regret guarantees for ill-conditioned data. SON is an enhanced version of the Online Newton Step, which, via sketching techniques enjoys a running time linear in the dimension and sketch size. We further develop sparse forms of the sketching methods (such as Oja's rule), making the computation linear in the sparsity of features. Together, the algorithm eliminates all computational obstacles in previous second order online learning approaches.	[Luo, Haipeng] Princeton Univ, Princeton, NJ 08544 USA; [Agarwal, Alekh; Langford, John] Microsoft Res, New York, NY USA; [Cesa-Bianchi, Nicolo] Univ Milan, Milan, Italy	Princeton University; Microsoft; University of Milan	Luo, HP (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	haipengl@cs.princeton.edu; alekha@microsoft.com; nicolo.cesa-bianchi@unimi.it; jcl@microsoft.com	Cesa-Bianchi, Nicolò/C-3721-2013	Cesa-Bianchi, Nicolò/0000-0001-8477-4748				Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Balsubramani A., 2013, NIPS; Byrd RH, 2016, SIAM J OPTIMIZ, V26, P1008, DOI 10.1137/140954362; Cesa-Bianchi N, 2005, SIAM J COMPUT, V34, P640, DOI 10.1137/S0097539703432542; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Erdogdu M., 2015, NIPS; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gao W., 2013, ICML; Garber D., 2015, ICML; Garber D, 2016, SIAM J OPTIMIZ, V26, P1493, DOI 10.1137/140985366; Ghashami M., 2016, KDD; Ghashami M, 2016, SIAM J COMPUT, V45, P1762, DOI 10.1137/15M1009718; Gonen A., 2015, ARXIV150602649; Gonen A., 2016, ICML; Hardt  M., 2014, NIPS; Jaggi Martin., 2013, ICML; Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902; Koren T, 2012, ICML; Li C. - L., 2015, ARXIV150601490; Liberty E., 2013, KDD; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; McMahan H Brendan, 2010, COLT; Mokhtari A, 2015, J MACH LEARN RES, V16, P3151; Moritz P, 2016, AISTATS; Motwani R., 1998, STOC; OJA E, 1985, J MATH ANAL APPL, V106, P69, DOI 10.1016/0022-247X(85)90131-3; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Orabona F., 2015, ALT; Orabona F, 2015, MACH LEARN, V99, P411, DOI 10.1007/s10994-014-5474-8; Pilanci M., 2015, ARXIV150502250; Ross S., 2013, UAI; Schraudolph Nicol N, 2007, AISTATS; Sohl-Dickstein J., 2014, ICML; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701064
C	Magdon-Ismail, M; Boutsidis, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Magdon-Ismail, Malik; Boutsidis, Christos			Optimal Sparse Linear Encoders and Sparse PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PRINCIPAL COMPONENT ANALYSIS; POWER METHOD; FIT	Principal components analysis (PCA) is the optimal linear encoder of data. Sparse linear encoders (e.g., sparse PCA) produce more interpretable features that can promote better generalization. (i) Given a level of sparsity, what is the best approximation to PCA? (ii) Are there efficient algorithms which can achieve this optimal combinatorial tradeoff? We answer both questions by providing the first polynomial-time algorithms to construct optimal sparse linear auto-encoders; additionally, we demonstrate the performance of our algorithms on real data.	[Magdon-Ismail, Malik] Rensselaer Polytech Inst, Troy, NY 12211 USA	Rensselaer Polytechnic Institute	Magdon-Ismail, M (corresponding author), Rensselaer Polytech Inst, Troy, NY 12211 USA.	magdon@cs.rpi.edu; christos.boutsidis@gmail.com			NSF:IIS [1124827]; Army Research Laboratory [W911NF-09-2-0053]	NSF:IIS(National Science Foundation (NSF)); Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL))	Magdon-Ismail was partially supported by NSF:IIS 1124827 and by the Army Research Laboratory under Cooperative Agreement W911NF-09-2-0053 (the ARL-NSCTA). The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Laboratory or the U.S. Government. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation here on.	Asteris M., 2011, P ISIT; Asteris M., 2014, P ICML; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; BOURLARD H, 1988, BIOL CYBERN, V59, P291, DOI 10.1007/BF00332918; Boutsidis C., 2014, SIAM J COMPUTING, V43; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Cottrell G., 1988, P SPIE, V1001; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Journee M, 2010, J MACH LEARN RES, V11, P517; KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Mackey L., 2009, P ADV NEUR INF PROC, V21, P1017; Magdon-Ismail M., 2015, ARXIV150205675; Makhzani A., 2014, ICLR; Moghaddam B., 2006, ICML; OJA E, 1991, ARTIFICIAL NEURAL NETWORKS, VOLS 1 AND 2, P737; OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678; Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701109
C	Magliacane, S; Claassen, T; Mooij, JM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Magliacane, Sara; Claassen, Tom; Mooij, Joris M.			Ancestral Causal Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIRECTED ACYCLIC GRAPHS; DISCOVERY; LATENT	Constraint-based causal discovery from limited data is a notoriously difficult challenge due to the many borderline independence test decisions. Several approaches to improve the reliability of the predictions by exploiting redundancy in the independence information have been proposed recently. Though promising, existing approaches can still be greatly improved in terms of accuracy and scalability. We present a novel method that reduces the combinatorial explosion of the search space by using a more coarse-grained representation of causal information, drastically reducing computation time. Additionally, we propose a method to score causal predictions based on their confidence. Crucially, our implementation also allows one to easily combine observational and interventional data and to incorporate various types of available background knowledge. We prove soundness and asymptotic consistency of our method and demonstrate that it can outperform the state-ofthe-art on synthetic data, achieving a speedup of several orders of magnitude. We illustrate its practical feasibility by applying it to a challenging protein data set.	[Magliacane, Sara] Vrije Univ Amsterdam, Amsterdam, Netherlands; [Magliacane, Sara; Mooij, Joris M.] Univ Amsterdam, Amsterdam, Netherlands; [Claassen, Tom] Radboud Univ Nijmegen, Nijmegen, Netherlands	Vrije Universiteit Amsterdam; University of Amsterdam; Radboud University Nijmegen	Magliacane, S (corresponding author), Vrije Univ Amsterdam, Amsterdam, Netherlands.; Magliacane, S (corresponding author), Univ Amsterdam, Amsterdam, Netherlands.	sara.magliacane@gmail.com; tomc@cs.ru.nl; j.m.mooij@uva.nl	Magliacane, Sara/ABD-8241-2020		NWO, the Netherlands Organization for Scientific Research (VIDI grant) [639.072.410]; Dutch programme COMMIT/under the Data2Semantics project; NWO [612.001.202]; EU-FP7 grant [603016]	NWO, the Netherlands Organization for Scientific Research (VIDI grant); Dutch programme COMMIT/under the Data2Semantics project; NWO(Netherlands Organization for Scientific Research (NWO)); EU-FP7 grant	SM and JMM were supported by NWO, the Netherlands Organization for Scientific Research (VIDI grant 639.072.410). SM was also supported by the Dutch programme COMMIT/under the Data2Semantics project. TC was supported by NWO grant 612.001.202 (MoCoCaDi), and EU-FP7 grant agreement n. 603016 (MATRICS). We also thank Sofia Triantafillou for her feedback, especially for pointing out the correct way to read ancestral relations from a PAG.	[Anonymous], P 29 INT C MACH LEAR; Claassen T, 2012, P UAI C, P207; Claassen T., 2011, P 27 C UNC ART INT A, P135; Colombo D, 2012, ANN STAT, V40, P294, DOI 10.1214/11-AOS940; Eaton D, 2007, P MACHINE LEARNING R, V2, P107; Gebser M., 2014, TECHNICAL REPORT; Gelfond M, 2008, FOUND ARTIF INTELL, P285, DOI 10.1016/S1574-6526(07)03007-6; Hyttinen A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P340; Kalisch M, 2007, J MACH LEARN RES, V8, P613; Kalisch M, 2012, J STAT SOFTW, V47, P1; KLEITMAN DJ, 1975, T AM MATH SOC, V205, P205, DOI 10.2307/1997200; Lifschitz V., 2008, AAAI, V8, P1594; Margaritis D, 2009, COMPUT INTELL-US, V25, P367, DOI 10.1111/j.1467-8640.2009.00347.x; Markowetz F., 2005, P 10 INT WORKSHOP AR, VR5, P214; Mooij J. M., 2013, P 29 ANN C UNC ART I, P431; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Peters J., 2015, J ROYAL STAT SOC B, V78, P947; Ramsey J, 2006, P 22 C UNC ART INT, P401; Rothenhausler D, 2015, ADV NEUR IN, V28; Roumpelaki A., 2016, CAUS FDN APPL WORKSH; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Spirtes P, 2001, AISTATS, P121; Spirtes P., 2000, CAUSATION PREDICTION; Tian J., 2001, P 17 C UNC ART INT, P512; Triantafillou S, 2015, J MACH LEARN RES, V16, P2147; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702013
C	Maitin-Shepard, J; Jain, V; Januszewski, M; Li, P; Abbeel, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Maitin-Shepard, Jeremy; Jain, Viren; Januszewski, Michal; Li, Peter; Abbeel, Pieter			Combinatorial Energy Learning for Image Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SCANNING-ELECTRON-MICROSCOPY; RECONSTRUCTION	We introduce a new machine learning approach for image segmentation that uses a neural network to model the conditional energy of a segmentation given an image. Our approach, combinatorial energy learning for image segmentation (CELIS) places a particular emphasis on modeling the inherent combinatorial nature of dense image segmentation problems. We propose efficient algorithms for learning deep neural networks to model the energy function, and for local optimization of this energy in the space of supervoxel agglomerations. We extensively evaluate our method on a publicly available 3-D microscopy dataset with 25 billion voxels of ground truth data. On an 11 billion voxel test set, we find that our method improves volumetric reconstruction accuracy by more than 20% as compared to two state-of-the-art baseline methods: graph-based segmentation of the output of a 3-D convolutional neural network trained to predict boundaries, as well as a random forest classifier trained to agglomerate supervoxels that were generated by a 3-D convolutional neural network.	[Maitin-Shepard, Jeremy; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Maitin-Shepard, Jeremy; Jain, Viren; Januszewski, Michal; Li, Peter] Google, Mountain View, CA 94043 USA	University of California System; University of California Berkeley; Google Incorporated	Maitin-Shepard, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Maitin-Shepard, J (corresponding author), Google, Mountain View, CA 94043 USA.	jbms@google.com; viren@google.com; mjanusz@google.com; phli@google.com; pabbeel@cs.berkeley.edu			National Science Foundation [1118055]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1118055.	Andres B, 2008, LECT NOTES COMPUT SC, V5096, P142, DOI 10.1007/978-3-540-69321-5_15; Andres B, 2012, LECT NOTES COMPUT SC, V7574, P778, DOI 10.1007/978-3-642-33712-3_56; Bogovic J. A., 2013, ARXIV13126159; Briggman KL, 2006, CURR OPIN NEUROBIOL, V16, P562, DOI 10.1016/j.conb.2006.08.010; Calonder M, 2010, LECT NOTES COMPUT SC, V6314, P778, DOI 10.1007/978-3-642-15561-1_56; Chen L., 2014, CORR; Chen L.-C., 2015, P ICML; Ciresan Dan, 2012, ADV NEURAL INFORM PR, P2843, DOI DOI 10.5555/2999325.2999452; Dean J., 2012, NIPS 12, V1, P1223; Denk W, 2004, PLOS BIOL, V2, P1900, DOI 10.1371/journal.pbio.0020329; Denk W, 2012, NAT REV NEUROSCI, V13, P351, DOI 10.1038/nrn3169; Duffield N, 2007, J ACM, V54, DOI 10.1145/1314690.1314696; Funke J, 2012, PROC CVPR IEEE, P1004, DOI 10.1109/CVPR.2012.6247777; Hayworth K., 2006, MICROSCOPY MICROA S2, V12, P86, DOI [DOI 10.1017/S1431927606066268, 10.1017/s1431927606066268]; Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346; Helmstaedter M, 2011, NAT NEUROSCI, V14, P1081, DOI 10.1038/nn.2868; Helmstaedter M, 2008, CURR OPIN NEUROBIOL, V18, P633, DOI 10.1016/j.conb.2009.03.005; Jain V., 2011, ADV NEURAL INFORM PR, V2; Knott G, 2008, J NEUROSCI, V28, P2959, DOI 10.1523/JNEUROSCI.3189-07.2008; Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013; Nunez-Iglesias J, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0071715; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Schwing A. G., 2015, ARXIV150302351 2015A; Sommer C, 2011, I S BIOMED IMAGING, P230, DOI 10.1109/ISBI.2011.5872394; Takemura SY, 2015, P NATL ACAD SCI USA, V112, P13711, DOI 10.1073/pnas.1509820112; Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450; Turaga S., 2009, ADV NEURAL INFORM PR, P1865; Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881; Vazquez-Reina A, 2011, IEEE I CONF COMP VIS, P177, DOI 10.1109/ICCV.2011.6126240; White J. G., 1986, PHILOS T R SOC B, V1165, P1; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zlateski A., 2015, CORR; Zlateski Aleksandar, 2011, THESIS	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705009
C	Mao, JH; Xu, JJ; Jing, YS; Yuille, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mao, Junhua; Xu, Jiajing; Jing, Yushi; Yuille, Alan			Training and Evaluating Multimodal Word Embeddings with Large-scale Web Annotated Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we focus on training and evaluating effective word embeddings with both text and visual information. More specifically, we introduce a large-scale dataset with 300 million sentences describing over 40 million images crawled and downloaded from publicly available Pins (i.e. an image with sentence descriptions uploaded by users) on Pinterest [2]. This dataset is more than 200 times larger than MS COCO [22], the standard large-scale image dataset with sentence descriptions. In addition, we construct an evaluation dataset to directly assess the effectiveness of word embeddings in terms of finding semantically similar or related words and phrases. The word/phrase pairs in this evaluation dataset are collected from the click data with millions of users in an image search system, thus contain rich semantic relationships. Based on these datasets, we propose and compare several Recurrent Neural Networks (RNNs) based multimodal (text and image) models. Experiments show that our model benefits from incorporating the visual information into the word embeddings, and a weight sharing strategy is crucial for learning such multimodal embeddings. The project page is: http://www.stat.ucla.edu/(similar to)junhua.mao/multimodal_embedding.html(1).	[Mao, Junhua; Yuille, Alan] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Xu, Jiajing; Jing, Yushi] Pinterest Inc, San Francisco, CA USA; [Yuille, Alan] Johns Hopkins Univ, Baltimore, MD 21218 USA	University of California System; University of California Los Angeles; Johns Hopkins University	Mao, JH (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90024 USA.	mjhustc@ucla.edu; jiajing@pinterest.com; jing@pinterest.com; alan.l.yuille@gmail.com			Center for Brains, Minds and Machines NSF STC [CCF-1231216]; Army Research Office [ARO 62250-CS]	Center for Brains, Minds and Machines NSF STC; Army Research Office	We are grateful to James Rubinstein for setting up the crowdsourcing experiments for dataset cleanup. We thank Veronica Mapes, Pawel Garbacki, and Leon Wong for discussions and support. We appreciate the comments and suggestions from anonymous reviewers of NIPS 2016. This work is partly supported by the Center for Brains, Minds and Machines NSF STC award CCF-1231216 and the Army Research Office ARO 62250-CS.	Agirre Eneko, 2009, P HUMAN LANGUAGE TEC, P19; [Anonymous], 2015, PROCEEDINGS; Bengio Y, 2006, STUD FUZZ SOFT COMP, V194, P137; Bruni E., 2014, JAIR, V49; Chen X., 2014, ARXIV14115654; Cho K., 2014, P 2014 C EMP METH NA, P1724; Cho S. J. K., 2015, ACL; Donahue J., 2015, CVPR; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Finkelstein Lev, 2001, P 10 INT C WORLD WID, P406, DOI DOI 10.1145/371920.372094; Grubinger M., 2006, INT WORKSHOP ONTOIMA, V2; Hill F., 2015, COMPUTATIONAL LINGUI; Hill Felix, 2014, EMNLP, DOI [10.3115/v1/D14-1032, DOI 10.3115/V1/D14-1032]; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kiela D., 2014, EMNLP, P36, DOI DOI 10.3115/V1/D14-1005; Kottur S., 2015, ARXIV151107067; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Mao J, 2015, 3 INT C LEARN REPR I; Mao JH, 2015, IEEE I CONF COMP VIS, P2533, DOI 10.1109/ICCV.2015.291; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Nair V., 2010, ICML, P807; Nelson DL, 2004, BEHAV RES METH INS C, V36, P402, DOI 10.3758/BF03195588; Ordonez Vicente, 2011, ADV NEURAL INFORM PR, P1143; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Schnabel Tobias, 2015, P 2015 C EMP METH NA, P298, DOI DOI 10.18653/V1/D15-1036; Silberer C, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P721; Simonyan Karen, 2015, INT C LEARN REPR; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Young P., 2014, ACL, P479	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705004
C	Mariet, Z; Sra, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mariet, Zelda; Sra, Suvrit			Kronecker Determinantal Point Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Determinantal Point Processes (DPPs) are probabilistic models over all subsets a ground set of N items. They have recently gained prominence in several applications that rely on "diverse" subsets. However, their applicability to large problems is still limited due to O(N-3) complexity of core tasks such as sampling and learning. We enable efficient sampling and learning for DPPs by introducing KRONDPP, a DPP model whose kernel matrix decomposes as a tensor product of multiple smaller kernel matrices. This decomposition immediately enables fast exact sampling. But contrary to what one may expect, leveraging the Kronecker product structure for speeding up DPP learning turns out to be more difficult. We overcome this challenge, and derive batch and stochastic optimization algorithms for efficiently learning the parameters of a KRONDPP.	[Mariet, Zelda; Sra, Suvrit] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Mariet, Z (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	zelda@csail.mit.edu; suvrit@mit.edu			NSF [IIS-1409802]	NSF(National Science Foundation (NSF))	SS acknowledges partial support from NSF grant IIS-1409802.	Affandi R. H., 2014, ICML; Affandi R. H., 2013, ARTIFICIAL INTELLIGE; Batmanghelich N. K., 2014, ARXIV14116307; Bhatia R, 2007, PRINC SER APPL MATH, P1; Borodin A., 2009, ARXIV09111153; Chao WL, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P191; Decreusefond Laurent, 2015, DETERMINANTAL POINT; Gartrell M., 2016, ARXIV160205436; Gillenwater J., 2014, NIPS; GOLDSCHMIDT O, 1994, NAV RES LOG, V41, P833, DOI 10.1002/1520-6750(199410)41:6<833::AID-NAV3220410611>3.0.CO;2-Q; Hough J. B., 2006, PROBABILITY SURVEYS, V3, P9; Kang B., 2013, ADV NEURAL INFORM PR, P2319; Krause A, 2008, J MACH LEARN RES, V9, P235; Kulesza A., 2011, ICML; Kulesza A., 2013, THESIS; Kulesza A., 2012, DETERMINANTAL POINT, V5; Lavancier F, 2015, J R STAT SOC B, V77, P853, DOI 10.1111/rssb.12096; Li C., 2016, ARXIV160306052; Li C., 2015, ARXIV150901618; Lin H., 2012, UNCERTAINTY ARTIFICI; Loan C. F. Van, 1993, LINEAR ALGEBRA LARGE, P293; Lyons R, 2003, PUBL MATH-PARIS, P167; Macchi O., 1975, ADV APPL PROB, V7; Mariet Z., 2016, INT C LEARN REPR ICL; Mariet Zelda, 2015, ICML; Martens James, 2015, ICML; Wu G, 2005, SIAM PROC S, P611; Yuille AL, 2002, ADV NEUR IN, V14, P1033; Zhang X., 2015, ICCV; Zhou T, 2010, P NATL ACAD SCI USA, V107, P4511, DOI 10.1073/pnas.1000488107	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702043
C	McQueen, J; Meila, M; Perrault-Joncas, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		McQueen, James; Meila, Marina; Perrault-Joncas, Dominique			Nearly Isometric Embedding by Relaxation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIMENSIONALITY REDUCTION; EIGENMAPS; MANIFOLDS	Many manifold learning algorithms aim to create embeddings with low or no distortion (isometric). If the data has intrinsic dimension d, it is often impossible to obtain an isometric embedding in d dimensions, but possible in s > d dimensions. Yet, most geometry preserving algorithms cannot do the latter. This paper proposes an embedding algorithm to overcome this. The algorithm accepts as input, besides the dimension d, an embedding dimension s >= d. For any data embedding Y, we compute a Loss(Y), based on the push-forward Riemannian metric associated with Y, which measures deviation of Y from from isometry. Riemannian Relaxation iteratively updates Y in order to decrease Loss(Y). The experiments confirm the superiority of our algorithm in obtaining low distortion embeddings.	[McQueen, James; Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA; [Perrault-Joncas, Dominique] Google, Seattle, WA 98103 USA	University of Washington; University of Washington Seattle; Google Incorporated	McQueen, J (corresponding author), Univ Washington, Dept Stat, Seattle, WA 98195 USA.	jmcq@u.washington.edu; mmp@stat.washington.edu; dcpjoncas@gmail.com						Abazajian KN, 2009, ASTROPHYS J SUPPL S, V182, P543, DOI 10.1088/0067-0049/182/2/543; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bernstein M., 2000, SCIENCE, V290; Coifman R. R., 2006, APPL COMPUTATIONAL H, V21, p[6, 1]; Delchambre L, 2015, MON NOT R ASTRON SOC, V446, P3545, DOI 10.1093/mnras/stu2219; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Genovese CR, 2012, J MACH LEARN RES, V13, P1263; Hein M., 2005, P 22 INT C MACHINE L, P289; Hein M, 2007, J MACH LEARN RES, V8, P1325; Lee J.M, 2012, INTRO SMOOTH MANIFOL; Lee John M, 2018, INTRO RIEMANNIAN MAN; Nadler B, 2006, APPL COMPUT HARMON A, V21, P113, DOI 10.1016/j.acha.2005.07.004; NASH J, 1956, ANN MATH, V63, P20, DOI 10.2307/1969989; Ozertem U, 2011, J MACH LEARN RES, V12, P1249; Perrault-Joncas Dominique, 2013, ARXIV13057255V1; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Ting D., 2010, P ICML; Vanderplas J, 2009, ASTRON J, V138, P1365, DOI 10.1088/0004-6256/138/5/1365; Verma N, 2013, J MACH LEARN RES, V14, P2415; Weinberger KQ, 2006, INT J COMPUT VISION, V70, P77, DOI 10.1007/s11263-005-4939-z; Zhang ZY, 2004, SIAM J SCI COMPUT, V26, P313, DOI 10.1137/S1064827502419154	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704059
C	Minami, K; Arai, H; Sato, I; Nakagawa, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Minami, Kentaro; Arai, Hiromi; Sato, Issei; Nakagawa, Hiroshi			Differential Privacy without Sensitivity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The exponential mechanism is a general method to construct a randomized estimator that satisfies (epsilon, 0)-differential privacy. Recently, Wang et al. showed that the Gibbs posterior, which is a data-dependent probability distribution that contains the Bayesian posterior, is essentially equivalent to the exponential mechanism under certain boundedness conditions on the loss function. While the exponential mechanism provides a way to build an (epsilon, 0)-differential private algorithm, it requires boundedness of the loss function, which is quite stringent for some learning problems. In this paper, we focus on (epsilon, delta)-differential privacy of Gibbs posteriors with convex and Lipschitz loss functions. Our result extends the classical exponential mechanism, allowing the loss functions to have an unbounded sensitivity.	[Minami, Kentaro; Arai, Hiromi; Sato, Issei; Nakagawa, Hiroshi] Univ Tokyo, Tokyo, Japan	University of Tokyo	Minami, K (corresponding author), Univ Tokyo, Tokyo, Japan.	kentaro_minami@mist.i.u-tokyo.ac.jp; arai@dl.itc.u-tokyo.ac.jp; sato@k.u-tokyo.ac.jp; nakagawa@dl.itc.u-tokyo.ac.jp	Arai, Hiromi/AAY-1406-2021		JSPS KAKENHI [JP15H02700]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was supported by JSPS KAKENHI Grant Number JP15H02700.	Alquier P., 2015, PROPERTIES VARIATION; Bakry D., 2014, GRUNDLEHREN MATH WIS, V34; Bassily R., 2014, FOCS; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bubeck S., 2015, NIPS; Catoni O., 2007, PAC BAYESIAN SUPERVI; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chaudhuri Kamalika, 2014, NIPS; Dalalyan A., 2014, THEORETICAL GUARANTE; Dimitrakakis C., 2014, ALGORITHMIC LEARNING; Dwork C, 2006, LECT NOTES COMPUT SC, V4052, P1; Hall R, 2013, J MACH LEARN RES, V14, P703; Kifer D., 2012, COLT; Ledoux M, 1999, LECT NOTES MATH, V1709, P120; McSherry Frank, 2007, FOCS; Milman E, 2012, PROBAB THEORY REL, V152, P475, DOI 10.1007/s00440-010-0328-1; Mir D. J., 2013, THESIS; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang Y., 2015, ICML; Zhang T, 2006, ANN STAT, V34, P2180, DOI 10.1214/009053606000000704; [No title captured]	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700015
C	Mirzasoleiman, B; Zadimoghaddam, M; Karbasi, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mirzasoleiman, Baharan; Zadimoghaddam, Morteza; Karbasi, Amin			Fast Distributed Submodular Cover: Public-Private Data Summarization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we introduce the public-private framework of data summarization motivated by privacy concerns in personalized recommender systems and online social services. Such systems have usually access to massive data generated by a large pool of users. A major fraction of the data is public and is visible to (and can be used for) all users. However, each user can also contribute some private data that should not be shared with other users to ensure her privacy. The goal is to provide a succinct summary of massive dataset, ideally as small as possible, from which customized summaries can be built for each user, i.e. it can contain elements from the public data (for diversity) and users' private data (for personalization). To formalize the above challenge, we assume that the scoring function according to which a user evaluates the utility of her summary satisfies submodularity, a widely used notion in data summarization applications. Thus, we model the data summarization targeted to each user as an instance of a submodular cover problem. However, when the data is massive it is infeasible to use the centralized greedy algorithm to find a customized summary even for a single user. Moreover, for a large pool of users, it is too time consuming to find such summaries separately. Instead, we develop a fast distributed algorithm for submodular cover, FASTCOVER, that provides a succinct summary in one shot and for all users. We show that the solution provided by FASTCOVER is competitive with that of the centralized algorithm with the number of rounds that is exponentially smaller than state of the art results. Moreover, we have implemented FASTCOVER with Spark to demonstrate its practical performance on a number of concrete applications, including personalized location recommendation, personalized movie recommendation, and dominating set on tens of millions of data points and varying number of users.	[Mirzasoleiman, Baharan] Swiss Fed Inst Technol, Zurich, Switzerland; [Zadimoghaddam, Morteza] Google Res, New York, NY USA; [Karbasi, Amin] Yale Univ, New Haven, CT 06520 USA	ETH Zurich; Google Incorporated; Yale University	Mirzasoleiman, B (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.				Google Faculty Research Award; DARPA Young Faculty Award [D16AP00046]	Google Faculty Research Award(Google Incorporated); DARPA Young Faculty Award	This research was supported by Google Faculty Research Award and DARPA Young Faculty Award (D16AP00046).	Berger Bonnie, 1994, J COMPUTER SYSTEM SC; Blelloch G. E., 2011, SPAA; Candes Emmanuel J, 2009, FDN COMPUTATIONAL MA; Chierichetti Flavio, 2015, KDD; Dean J., 2004, OSDI; Demaine Erik D, 2014, DISTRIBUTED COMPUTIN; Dueck D., 2007, ICCV; El-Arini K, 2011, KDD; Feige U., 1998, J ACM; Gomes R., 2010, ICML; Hui Lin, 2011, N AM CHAPTER ASS COM; Iyer R. K, 2013, NIPS; Krause A., 2008, JMLR; Kumar Ravi, 2015, TOPC; Lindgren Erik M, 2015, NIPS; Mirzasoleiman B., 2013, NIPS; Mirzasoleiman B., 2016, ICML; Mirzasoleiman B., 2015, NIPS; Simon I., 2007, ICCV; Sipos R., 2012, CIKM; Stergiou Stergios, 2015, SIGKDD; Tschiatschek S., 2014, NIPS; Wolsey Laurence A., 1982, COMBINATORICA; Yang J., 2015, KNOWLEDGE INFORM SYS	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704064
C	Mohri, M; Yang, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mohri, Mehryar; Yang, Scott			Optimistic Bandit Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the general and powerful scheme of predicting information re-use in optimization algorithms. This allows us to devise a computationally efficient algorithm for bandit convex optimization with new state-of-the-art guarantees for both Lipschitz loss functions and loss functions with Lipschitz gradients. This is the first algorithm admitting both a polynomial time complexity and a regret that is polynomial in the dimension of the action space that improves upon the original regret bound for Lipschitz loss functions, achieving a regret of (O) over tilde (T(11/16)d(3/8)). Our algorithm further improves upon the best existing polynomial-in-dimension bound (both computationally and in terms of regret) for loss functions with Lipschitz gradients, achieving a regret of (O) over tilde (T(8/13)d(5/3)).	[Mohri, Mehryar; Yang, Scott] Courant Inst, 251 Mercer St, New York, NY 10012 USA; [Mohri, Mehryar] Google, 251 Mercer St, New York, NY 10012 USA	Google Incorporated	Mohri, M (corresponding author), Courant Inst, 251 Mercer St, New York, NY 10012 USA.; Mohri, M (corresponding author), Google, 251 Mercer St, New York, NY 10012 USA.	mohri@cims.nyu.edu; yangs@cims.nyu.edu			NSF [CCF-1535987, IIS-1618662, GRFP DGE-1342536]	NSF(National Science Foundation (NSF))	This work was partly funded by NSF CCF-1535987 and IIS-1618662 and NSF GRFP DGE-1342536.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Abernethy J., 2009, COLT; Agarwal A., 2010, P COLT, P28; Bubeck S., 2015, ABS150706580 CORR; Bubeck S., 2015, ABS150206398 CORR; Bubeck S., 2016, ABS160703084 CORR; Dani V., STOCHASTIC LINEAR OP; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Dekel O., 2015, NIPS, P2908; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Hazan E., 2016, ABS160304350 CORR; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kleinberg R., 2004, ADV NEURAL INFORM PR, V17, P697; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov  Y., 1994, STUDIES APPL MATH; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Saha A, 2011, P 14 INT C ART INT S, P636; Schmidt M. W., 2013, CORR	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703065
C	Mokhtari, A; Daneshmand, H; Lucchi, A; Hofmann, T; Ribeiro, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mokhtari, Aryan; Daneshmand, Hadi; Lucchi, Aurelien; Hofmann, Thomas; Ribeiro, Alejandro			Adaptive Newton Method for Empirical Risk Minimization to Statistical Accuracy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider empirical risk minimization for large-scale datasets. We introduce Ada Newton as an adaptive algorithm that uses Newton's method with adaptive sample sizes. The main idea of Ada Newton is to increase the size of the training set by a factor larger than one in a way that the minimization variable for the current training set is in the local neighborhood of the optimal argument of the next training set. This allows to exploit the quadratic convergence property of Newton's method and reach the statistical accuracy of each training set with only one iteration of Newton's method. We show theoretically that we can iteratively increase the sample size while applying single Newton iterations without line search and staying within the statistical accuracy of the regularized empirical risk. In particular, we can double the size of the training set in each iteration when the number of samples is sufficiently large. Numerical experiments on various datasets confirm the possibility of increasing the sample size by factor 2 at each iteration which implies that Ada Newton achieves the statistical accuracy of the full training set with about two passes over the dataset.(1)	[Mokhtari, Aryan; Ribeiro, Alejandro] Univ Penn, Philadelphia, PA 19104 USA; [Daneshmand, Hadi; Lucchi, Aurelien; Hofmann, Thomas] Swiss Fed Inst Technol, Zurich, Switzerland	University of Pennsylvania; Swiss Federal Institutes of Technology Domain; ETH Zurich	Mokhtari, A (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	aryanm@seas.upenn.edu; hadi.daneshmand@inf.ethz.ch; aurelien.lucchi@inf.ethz.ch; thomas.hofmann@inf.ethz.ch; aribeiro@seas.upenn.edu						Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bottou L., 2007, P NEURIPS, P161; Boyd S, 2004, CONVEX OPTIMIZATION; Daneshmand H., 2016, P 33 INT C MACH LEAR, P1463; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Erdogdu M. A., 2015, ADV NEURAL INFORM PR, V2, P3052; Frostig R., 2015, P C LEARNING THEORY, P728; Gurbuzbalaban M, 2015, MATH PROGRAM, V151, P283, DOI 10.1007/s10107-015-0897-y; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lucchi A., 2015, VARIANCE REDUCED STO; Mokhtari A, 2015, J MACH LEARN RES, V16, P3151; Mokhtari A, 2014, IEEE T SIGNAL PROCES, V62, P6089, DOI 10.1109/TSP.2014.2357775; Moritz P, 2016, JMLR WORKSH CONF PRO, V51, P249; Nesterov Y., 1998, INTRO LECT CONVEX PR, VI; Nesterov Y, 2007, GRADIENT METHODS MIN; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Qu Z, 2016, PR MACH LEARN RES, V48; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Schraudolph N. N., 2007, PROC 11 INT C ARTIF, P436; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Vapnik V., 2013, NATURE STAT LEARNING; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702009
C	Monk, T; Savin, C; Lucke, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Monk, Travis; Savin, Cristina; Luecke, Joerg			Neurons Equipped with Intrinsic Plasticity Learn Stimulus Intensity Statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODEL; EXCITABILITY	Experience constantly shapes neural circuits through a variety of plasticity mechanisms. While the functional roles of some plasticity mechanisms are well-understood, it remains unclear how changes in neural excitability contribute to learning. Here, we develop a normative interpretation of intrinsic plasticity (IP) as a key component of unsupervised learning. We introduce a novel generative mixture model that accounts for the class-specific statistics of stimulus intensities, and we derive a neural circuit that learns the input classes and their intensities. We will analytically show that inference and learning for our generative model can be achieved by a neural circuit with intensity-sensitive neurons equipped with a specific form of IP. Numerical experiments verify our analytical derivations and show robust behavior for artificial and natural stimuli. Our results link IP to non-trivial input statistics, in particular the statistics of stimulus intensities for classes to which a neuron is sensitive. More generally, our work paves the way toward new classification algorithms that are robust to intensity variations.	[Monk, Travis; Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, D-26129 Oldenburg, Germany; [Savin, Cristina] IST Austria, A-3400 Klosterneuburg, Austria	Carl von Ossietzky Universitat Oldenburg; Institute of Science & Technology - Austria	Monk, T (corresponding author), Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, D-26129 Oldenburg, Germany.	travis.monk@uol.de; csavin@ist.ac.at; joerg.luecke@uol.de	Savin, Cristina/ABI-4570-2020	Savin, Cristina/0000-0002-3414-8244	DFG within the Cluster of Excellence EXC 1077/1 (Hearing4all); People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA [291734];  [LU 1196/5-1]	DFG within the Cluster of Excellence EXC 1077/1 (Hearing4all)(German Research Foundation (DFG)); People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA; 	We acknowledge funding by the DFG within the Cluster of Excellence EXC 1077/1 (Hearing4all) and by grant LU 1196/5-1 (JL and TM) and the People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA grant agreement no. 291734 (CS).	Abbott LF, 2000, NAT NEUROSCI, V3, P1178, DOI 10.1038/81453; Cudmore RH, 2004, J NEUROPHYSIOL, V92, P341, DOI 10.1152/jn.01059.2003; Daoudal G, 2003, LEARN MEMORY, V10, P456, DOI 10.1101/lm.64103; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91; Douglas RJ, 2004, ANNU REV NEUROSCI, V27, P419, DOI 10.1146/annurev.neuro.27.070203.144152; Habenschuss Stefan, 2012, ADV NEURAL INFORM PR, V25, P773; Keck C, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002432; Lucke J, 2004, NEURAL COMPUT, V16, P501, DOI 10.1162/089976604772744893; Lucke J, 2008, J MACH LEARN RES, V9, P1227; Lucke J, 2009, NEURAL COMPUT, V21, P2805, DOI 10.1162/neco.2009.07-07-584; Neal R. M., 1998, LEARNING GRAPHICAL M; Neftci E, 2013, P NATL ACAD SCI USA, V110, pE3468, DOI 10.1073/pnas.1212083110; Nessler B., 2009, ADV NEURAL INFORM PR, V22, P1357; Nessler B, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003037; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Rezende D., 2011, ADV NEURAL INFORM PR, V24, P136; Rozell CJ, 2008, NEURAL COMPUT, V20, P2526, DOI 10.1162/neco.2008.03-07-486; Savin C, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003489; Savin C, 2010, PLOS COMPUT BIOL, V6, DOI 10.1371/journal.pcbi.1000757; Schmuker M, 2014, P NATL ACAD SCI USA, V111, P2081, DOI 10.1073/pnas.1303053111; Schwartz O, 2000, ADV NEURAL INF PROCE, P166; Stemmler M, 1999, NAT NEUROSCI, V2, P521, DOI 10.1038/9173; Wainwright MJ, 2001, APPL COMPUT HARMON A, V11, P89, DOI 10.1006/acha.2000.0350; Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704106
C	Munos, R; Stepleton, T; Harutyunyan, A; Bellemare, MG		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Munos, Remi; Stepleton, Thomas; Harutyunyan, Anna; Bellemare, Marc G.			Safe and efficient off-policy reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONVERGENCE	In this work, we take a fresh look at some old and new algorithms for off-policy, return-based reinforcement learning. Expressing these in a common form, we derive a novel algorithm, Retrace(lambda), with three desired properties: (1) it has low variance; (2) it safely uses samples collected from any behaviour policy, whatever its degree of "off-policyness"; and (3) it is efficient as it makes the best use of samples collected from near on-policy behaviour policies. We analyze the contractive nature of the related operator under both off-policy policy evaluation and control settings and derive online sample-based algorithms. We believe this is the first return-based off-policy control algorithm converging a.s. to Q* without the GLIE assumption (Greedy in the Limit with Infinite Exploration). As a corollary, we prove the convergence of Watkins' Q(lambda), which was an open problem since 1989. We illustrate the benefits of Retrace(lambda) on a standard suite of Atari 2600 games.	[Munos, Remi; Stepleton, Thomas; Bellemare, Marc G.] Google DeepMind, London, England; [Harutyunyan, Anna] Vrije Univ Brussel, Brussels, Belgium	Google Incorporated; Vrije Universiteit Brussel	Munos, R (corresponding author), Google DeepMind, London, England.	munos@google.com; stepleton@google.com; anna.harutyunyan@vub.ac.be; bellemare@google.com						[Anonymous], 2016, P INT C MACH LEARN; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Geist M, 2014, J MACH LEARN RES, V15, P289; Hallak A., 2015, ARXIV150905172; Harutyunyan A., 2016, Q OFF POLICY CORRECT; Kearns M.J., 2000, P 13THANNUAL C COMPU, P142; Li L., 2015, P 18 INT C ART INT S; Lin L. J., 1993, ICML 1993, P182; Mahmood A R, 2015, C UNC ART INT; Mahmood A. R., 2015, ARXIV150701569; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Precup D., 2001, P 18 INT C MACH LEAR, P417; Precup D., 2000, P 17 INT C MACH LEAR; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Schaul T., 2016, INT C LEARN REPR ICL; Singh S, 2000, MACH LEARN, V38, P287, DOI 10.1023/A:1007678930559; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R.S., 1996, ADV NEURAL INFORM PR, V8; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tsitsiklis JN, 2003, J MACH LEARN RES, V3, P59, DOI 10.1162/153244303768966102; Watkins CJCH., 1989, THESIS	22	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704062
C	Murugesan, K; Liu, HX; Carbonell, J; Yang, YM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Murugesan, Keerthiram; Liu, Hanxiao; Carbonell, Jaime; Yang, Yiming			Adaptive Smoothed Online Multi-Task Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper addresses the challenge of jointly learning both the per-task model parameters and the inter-task relationships in a multi-task online learning setting. The proposed algorithm features probabilistic interpretation, efficient updating rules and flexible modulation on whether learners focus on their specific task or on jointly address all tasks. The paper also proves a sub-linear regret bound as compared to the best linear predictor in hindsight. Experiments over three multi-task learning benchmark datasets show advantageous performance of the proposed approach over several state-of-the-art online multi-task learning baselines.	[Murugesan, Keerthiram; Liu, Hanxiao; Carbonell, Jaime; Yang, Yiming] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Murugesan, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kmuruges@cs.cmu.edu; hanxiaol@cs.cmu.edu; jgc@cs.cmu.edu; yiming@cs.cmu.edu	Liu, Han/GVT-8296-2022		NSF [IIS-1216282, IIS-1546329]	NSF(National Science Foundation (NSF))	This work is supported in part by NSF under grants IIS-1216282 and IIS-1546329.	Abernethy J, 2007, LECT NOTES COMPUT SC, V4539, P484, DOI 10.1007/978-3-540-72927-3_35; Agarwal A., 2008, UCBEECS2008138; [Anonymous], THESIS; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Blitzer J., P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441; Cavallanti G, 2010, J MACH LEARN RES, V11, P2901; Crammer K, 2006, J MACH LEARN RES, V7, P551; Crammer Koby, 2012, PROC 25 INT C NEURAL, P1475; Dekel O, 2007, J MACH LEARN RES, V8, P2233; Jiang L, 2014, ADV NEUR IN, V27; Kshirsagar M, 2013, BIOINFORMATICS, V29, P217, DOI 10.1093/bioinformatics/btt245; Kshirsagar Meghana, 2013, NIPS WORKSH MACH LEA; Kumar M., 2010, NIPS, P1189, DOI DOI 10.5555/2997189.2997322; Lugosi G., 2009, ARXIV09023526; Nemirovsky A.-S., 1982, PROBLEM COMPLEXITY M; Saha A., 2011, PROC INT C ARTIF INT, P643; Weinberger K., 2009, ANN INT C MACH LEARN, P1113, DOI DOI 10.1145/1553374.1553516; Xue Y, 2007, J MACH LEARN RES, V8, P35; Zhang Y, 2014, ACM T KNOWL DISCOV D, V8, DOI 10.1145/2538028	19	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703070
C	Natarajan, N; Jain, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Natarajan, Nagarajan; Jain, Prateek			Regret Bounds for Non-decomposable Metrics with Missing Labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of recommending relevant labels (items) for a given data point (user). In particular, we are interested in the practically important setting where the evaluation is with respect to non-decomposable (over labels) performance metrics like the F-1 measure, and training data has missing labels. To this end, we propose a generic framework that given a performance metric 41, can devise a regularized objective function and a threshold such that all the values in the predicted score vector above and only above the threshold are selected to be positive. We show that the regret or generalization error in the given metric W is bounded ultimately by estimation error of certain underlying parameters. In particular, we derive regret bounds under three popular settings: a) collaborative filtering, b) multilahel classification, and c) PU (positive-unlabeled) learning. For each of the above problems, we can obtain precise non-asymptotic regret bound which is small even when a large fraction of labels is missing. Our empirical results on synthetic and benchmark datasets demonstrate that by explicitly modeling for missing labels and optimizing the desired performance metric, our algorithm indeed achieves significantly better performance (like F-1 score) when compared to methods that do not model missing label information carefully.	[Natarajan, Nagarajan; Jain, Prateek] Microsoft Res, Bengaluru, India		Natarajan, N (corresponding author), Microsoft Res, Bengaluru, India.	t-nanata@microsoft.com; prajain@microsoft.com						Agarwal S, 2014, J MACH LEARN RES, V15, P1653; Bhatia K., 2015, ADV NEURAL INF PROCE, P721; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Cai T, 2013, J MACH LEARN RES, V14, P3619; Davenport MA, 2014, INF INFERENCE, V3, P189, DOI 10.1093/imaiai/iau006; Dembczynski Krzysztof, 2012, P 29 INT C MACH LEAR, V2012; Gao W, 2013, ARTIF INTELL, V199, P22, DOI 10.1016/j.artint.2013.03.001; Hsieh CJ, 2015, PR MACH LEARN RES, V37, P2445; Jain P., 2013, ABS13060626 CORR; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Kotlowski Wojciech, 2015, ARXIV150407272; Koyejo O., 2014, ADV NEURAL INFORM PR, V3, P2744; Koyejo Oluwasanmi O, 2015, ADV NEURAL INFORM PR, P3303; Lafond J., 2015, ARXIV150206919; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Yu H.-F., 2014, INT C MACH LEARN, P593; Yun H., 2014, ADV NEURAL INFORM PR, P2582; Zhong K., 2015, INT C ALG LEARN THEO	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701035
C	Neykov, M; Wang, ZR; Liu, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Neykov, Matey; Wang, Zhaoran; Liu, Han			Agnostic Estimation for Misspecified Phase Retrieval Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REGRESSION	The goal of noisy high-dimensional phase retrieval is to estimate an s -sparse parameter beta* is an element of R-d from n realizations of the model Y = (X(sic)beta*)(2) + epsilon. Based on this model, we propose a significant semi-parametric generalization called misspecified phase retrieval (MPR), in which Y = f(X(sic)beta*, epsilon) with unknown f and Cov(Y; (X>beta*)(2)) > 0. For example, MPR encompasses Y = h((X(sic)vertical bar beta*vertical bar) + epsilon with increasing h as a special case. Despite the generality of the MPR model, it eludes the reach of most existing semi-parametric estimators. In this paper, we propose an estimation procedure, which consists of solving a cascade of two convex programs and provably recovers the direction of beta*. Our theory is backed up by thorough numerical results.	[Neykov, Matey; Wang, Zhaoran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA	Princeton University	Neykov, M (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08544 USA.	mneykov@princeton.edu; zhaoran@princeton.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018					Adamczak R, 2015, PROBAB THEORY REL, V162, P531, DOI 10.1007/s00440-014-0579-3; Amini A. A., 2008, IEEE INT S INF THEOR; Berthet Q., 2013, C LEARN THEOR; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Boufounos P. T., 2008, ANN C INF SCI SYST; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cai, 2016, J MACHINE LEARNING R, V17, P1; Cai T. T., 2015, ARXIV150603382; Candes EJ, 2015, APPL COMPUT HARMON A, V39, P277, DOI 10.1016/j.acha.2014.09.004; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen Y., 2013, ARXIV13127006; Cook R Dennis, 2005, J AM STAT ASS, V100; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Ganti R., 2015, ARXIV150608910; Gao Chao, 2014, ARXIV14098565; Genzel Martin, 2016, ARXIV160203436; Han F., 2015, ARXIV150907158; Horowitz JL, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-92870-8_1; Laurent B, 2000, ANN STAT, V28, P1302; Lecue G., 2013, ARXIV13115024; LI KC, 1992, J AM STAT ASSOC, V87, P1025, DOI 10.1080/01621459.1992.10476258; LI KC, 1991, J AM STAT ASSOC, V86, P316, DOI 10.2307/2290563; LI KC, 1989, ANN STAT, V17, P1009, DOI 10.1214/aos/1176347254; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Peng H, 2011, J STAT PLAN INFER, V141, P1362, DOI 10.1016/j.jspi.2010.10.003; Plan Yaniv, 2015, IEEE T INFORM THEORY; Radchenko P, 2015, J MULTIVARIATE ANAL, V139, P266, DOI 10.1016/j.jmva.2015.02.007; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Thrampoulidis C., 2015, ADV NEURAL INF PROCE, V28; Wang Zhaoran, 2015, ARXIV151208861; Xia YC, 1999, J AM STAT ASSOC, V94, P1275, DOI 10.2307/2669941; Yang Z., 2015, ARXIV151104514; Yi X., 2015, ADV NEURAL INFORM PR; Yuan XT, 2013, J MACH LEARN RES, V14, P899	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700058
C	Neyshabur, B; Wu, YH; Salakhutdinov, R; Srebro, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Neyshabur, Behnam; Wu, Yuhuai; Salakhutdinov, Ruslan; Srebro, Nathan			Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We investigate the parameter-space geometry of recurrent neural networks (RNNs), and develop an adaptation of path-SGD optimization method, attuned to this geometry, that can learn plain RNNs with ReLU activations. On several datasets that require capturing long-term dependency structure, we show that path-SGD can significantly improve trainability of ReLU RNNs compared to RNNs trained with SGD, even with various recently suggested initialization schemes.	[Neyshabur, Behnam; Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA; [Wu, Yuhuai] Univ Toronto, Toronto, ON, Canada; [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Toyota Technological Institute - Chicago; University of Toronto; Carnegie Mellon University	Neyshabur, B (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	bneyshabur@ttic.edu; ywu@cs.toronto.edu; rsalakhu@cs.cmu.edu; nati@ttic.edu			NSF RI/AF [1302662]; Intel ICRI-CI; ONR [N000141310721]; ADeLAIDE grant [FA8750-16C-0130-001]	NSF RI/AF; Intel ICRI-CI; ONR(Office of Naval Research); ADeLAIDE grant	This research was supported in part by NSF RI/AF grant 1302662, an Intel ICRI-CI award, ONR Grant N000141310721, and ADeLAIDE grant FA8750-16C-0130-001. We thank Saizheng Zhang for sharing a base code for RNNs.	[Anonymous], 2012, SUBWORD LANGUAGE MOD; [Anonymous], 2014, INT C LEARN REPR; Arjovsky M., 2015, ARXIV151106464; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Cho K., 2014, P 2014 C EMP METH NA, P1724; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Han S., 2016, P INT C LEARN REPR; Hochreiter S., 1997, NEURAL COMPUTATION, V9; Hochreiter S., 1998, INT J UNCERTAINTY FU, V06; Kingma D.P., 2015, ICLR, P1; Kiros Ryan, 2015, T ASS COMPUTATIONAL; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le Q.V., 2015, CORR, Vabs/1504.00941; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Memisevic, 2016, P INT C LEARN REPR; Nair V., 2010, ICML, P807; Neyshabur B., 2015, C LEARN THEOR; Neyshabur Behnam, 2015, P NIPS, P2422; Neyshabur  Behnam, 2016, INT C LEARN REPR; Ollivier  Yann, 2015, INFORM INFERENCE; Pachitariu M., 2013, ARXIV13015650; Talathi Sachin S., 2014, INT C LEARN REPR WOR; Zhang Saizheng, 2016, ARXIV160208210	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701071
C	Cuong, V; Xu, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nguyen Viet Cuong; Xu, Huan			Adaptive Maximization of Pointwise Submodular Functions With Budget Constraint	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the worst-case adaptive optimization problem with budget constraint that is useful for modeling various practical applications in artificial intelligence and machine learning. We investigate the near-optimality of greedy algorithms for this problem with both modular and non-modular cost functions. In both cases, we prove that two simple greedy algorithms are not near-optimal but the best between them is near-optimal if the utility function satisfies pointwise submodularity and pointwise cost-sensitive submodularity respectively. This implies a combined algorithm that is near-optimal with respect to the optimal algorithm that uses half of the budget. We discuss applications of our theoretical results and also report experiments comparing the greedy algorithms on the active learning problem.	[Nguyen Viet Cuong] Univ Cambridge, Dept Engn, Cambridge, England; [Xu, Huan] Georgia Inst Technol, Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA	University of Cambridge; University System of Georgia; Georgia Institute of Technology	Cuong, V (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	vcn22@cam.ac.uk; huan.xu@isye.gatech.edu			Agency for Science, Technology and Research (A*STAR) of Singapore through SERC PSF [R266000101305]	Agency for Science, Technology and Research (A*STAR) of Singapore through SERC PSF(Agency for Science Technology & Research (A*STAR))	This work was done when both authors were at the National University of Singapore. The authors were partially supported by the Agency for Science, Technology and Research (A*STAR) of Singapore through SERC PSF Grant R266000101305.	Asadpour Arash, 2008, INTERNET NETWORK EC; Cuong N. V., 2013, NIPS; Cuong N. V., 2014, UAI; Cuong Nguyen Viet, 2016, AAAI; Dean Brian C., 2004, FOCS; Golovin D., 2011, JAIR; Gotovos A., 2015, IJCAI; Guillory A., 2011, ICML; Guillory A., 2010, ARXIV10023345; Guillory Andrew, 2012, THESIS; Guillory Andrew, 2011, NIPS; Javdani Shervin, 2014, AISTATS; Joachims T, 1996, DTIC DOCUMENT; Khuller S, 1999, INFORM PROCESS LETT, V70, P39, DOI 10.1016/S0020-0190(99)00031-9; KRAUSE A, 2007, ICML; Krause A, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1989734.1989736; Krause Andreas, 2007, AAAI; Kusner M. J., 2014, NIPS WORKSH DISCR CO; Leskovec J., 2007, KDD; McCallum A, 1998, ICML; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; Sviridenko M, 2004, OPER RES LETT, V32, P41, DOI 10.1016/S0167-6377(03)00062-2; Wei K., 2015, ICML	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704050
C	Niepert, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Niepert, Mathias			Discriminative Gaifman Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present discriminative Gaifman models, a novel family of relational machine learning models. Gaifman models learn feature representations bottom up from representations of locally connected and bounded-size regions of knowledge bases (KBs). Considering local and bounded-size neighborhoods of knowledge bases renders logical inference and learning tractable, mitigates the problem of over-fitting, and facilitates weight sharing. Gaifman models sample neighborhoods of knowledge bases so as to make the learned relational models more robust to missing objects and relations which is a common situation in open-world KBs. We present the core ideas of Gaifman models and apply them to large-scale relational learning problems. We also discuss the ways in which Gaifman models relate to some existing relational machine learning approaches.	[Niepert, Mathias] NEC Labs Europe, Heidelberg, Germany	NEC Corporation	Niepert, M (corresponding author), NEC Labs Europe, Heidelberg, Germany.	mathias.niepert@neclabs.eu						[Anonymous], 1982, STUDIES LOGIC FDN MA; Berant Jonathan, 2011, P 49 ANN M ASS COMP, P610; Bordes A., 2013, ADV NEURAL INFORM PR; Bordes Antoine, 2012, P 15 INT C ARTIFICIA, P127; Bordes Antoine, 2011, AAAI C ART INT; Carlson A., 2010, 24 AAAI C ART INT; Ceylan Ismail, 2016, P 15 INT C PRINC KNO; Dries A, 2015, LECT NOTES ARTIF INT, V9286, P312, DOI 10.1007/978-3-319-23461-8_37; Evans C., 2008, P 2008 ACM SIGMOD IN, P1247, DOI [DOI 10.1145/1376616.1376746, 10.1145/1376616]; Gardner M., 2012, P 2015 C EMP METH NA, P1488, DOI DOI 10.18653/V1/D15-1173; Hoffart J, 2013, ARTIF INTELL, V194, P28, DOI 10.1016/j.artint.2012.06.001; Ji GL, 2016, AAAI CONF ARTIF INTE, P985; Jonathon R., 2012, P 25 INT C NEURAL IN, P3167; Kersting K, 2012, FRONT ARTIF INTEL AP, V242, P33, DOI 10.3233/978-1-61499-098-7-33; Lao Ni, 2011, P C EMP METH NAT LAN, P529, DOI DOI 10.5555/2145432.2145494; Libkin L., 2004, TEXT THEORET COMP S; Lin Y., 2015, P EMP METH NAT LANG; Lin YC, 2015, ADV SOC SCI EDUC HUM, V39, P2181; Milch Brian Christopher, 2006, THESIS; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Riedel S., 2013, HLT NAACL; Rocktaschel T., 2015, C N AM CHAPT ACL NAA; Schoenmackers S., 2010, P 2010 C EMPIRICAL M, P1088; Socher R., 2013, ADV NEURAL INFORM PR, P926, DOI DOI 10.1109/ICICIP.2013.6568119; Trouillon T, 2016, PR MACH LEARN RES, V48; Van den Broeck G, 2013, LIFTED INFERENCE LEA; Vardi M.Y., 1982, PROC 14 ACM SIGACT S, P137, DOI [/10.1145 /800070.802186, 10.1145/800070.802186, DOI 10.1145/800070.802186]; Yang B., 2015, P INT C LEARN REPR I; Yates A., 2007, ASS COMPUTATIONAL LI	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700062
C	Nock, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nock, Richard			On Regularizing Rademacher Observation Losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					It has recently been shown that supervised learning linear classifiers with two of the most popular losses, the logistic and square loss, is equivalent to optimizing an equivalent loss over sufficient statistics about the class: Rademacher observations (rados). It has also been shown that learning over rados brings solutions to two prominent problems for which the state of the art of learning from examples can be comparatively inferior and in fact less convenient: (i) protecting and learning from private examples, (ii) learning from distributed datasets without entity resolution. Bis repetita placent: the two proofs of equivalence are different and rely on specific properties of the corresponding losses, so whether these can be unified and generalized inevitably comes to mind. This is our first contribution: we show how they can be fit into the same theory for the equivalence between example and rado losses. As a second contribution, we show that the generalization unveils a surprising new connection to regularized learning, and in particular a sufficient condition under which regularizing the loss over examples is equivalent to regularizing the rados (i.e. the data) in the equivalent rado loss, in such a way that an efficient algorithm for one regularized rado loss may be as efficient when changing the regularizer. This is our third contribution: we give a formal boosting algorithm for the regularized exponential rado-loss which boost with any of the ridge, lasso, SLOPE, l(infinity), or elastic net regularizer, using the same master routine for all. Because the regularized exponential rado-loss is the equivalent of the regularized logistic loss over examples we obtain the first efficient proxy to the minimization of the regularized logistic loss over examples using such a wide spectrum of regularizers. Experiments with a readily available code display that regularization significantly improves rado-based learning and compares favourably with example-based learning.	[Nock, Richard] Australian Natl Univ, Data61, Canberra, ACT, Australia; [Nock, Richard] Univ Sydney, Sydney, NSW, Australia	Australian National University; Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of Sydney	Nock, R (corresponding author), Australian Natl Univ, Data61, Canberra, ACT, Australia.; Nock, R (corresponding author), Univ Sydney, Sydney, NSW, Australia.	richard.nock@data61.csiro.au						Bach F, 2011, MACH LEARN, V4, P1, DOI DOI 10.1561/2200000015; Bogdan M, 2015, ANN APPL STAT; Duchi Y. J., 2009, ADV NEURAL INFORM PR, P495; Gentile C, 1998, NIPS, V11, P225; Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543; Lichman M, 2013, UCI MACHINE LEARNING; Menon A., 2015, NIPS 28; Nair V., 2010, ICML, P807; Nock R., 2008, NIPS 21, P1201; Nock R, 2015, PR MACH LEARN RES, V37, P948; Patrini G., 2016, 26 IJCAI; Reid M.D., 2015, C LEARN THEOR, P1501; Schapire RE, 2003, LECT NOTES STAT, V171, P149, DOI 10.1007/978-0-387-21579-2_9; Schapire RE, 1999, MACH LEARN, V37, P297, DOI 10.1023/A:1007614523901; Su W., 2015, CORR; Telgarsky M, 2012, J MACH LEARN RES, V13, P561; Vapnik V.N, 1998, STAT LEARNING THEORY; Xi Y., 2009, J MACHINE LEARNING R, P615; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704096
C	Nock, R; Menon, AK; Ong, CS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nock, Richard; Menon, Aditya Krishna; Ong, Cheng Soon			A scaled Bregman theorem with applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Bregman divergences play a central role in the design and analysis of a range of machine learning algorithms through a handful of popular theorems. We present a new theorem which shows that "Bregman distortions" (employing a potentially non-convex generator) may be exactly re-written as a scaled Bregman divergence computed over transformed data. This property can be viewed from the standpoints of geometry (a scaled isometry with adaptive metrics) or convex optimization (relating generalized perspective transforms). Admissible distortions include geodesic distances on curved manifolds and projections or gauge-normalisation. Our theorem allows one to leverage to the wealth and convenience of Bregman divergences when analysing algorithms relying on the aforementioned Bregman distortions. We illustrate this with three novel applications of our theorem: a reduction from multi-class density ratio to class-probability estimation, a new adaptive projection free yet norm-enforcing dual norm mirror descent algorithm, and a reduction from clustering on flat manifolds to clustering on curved manifolds. Experiments on each of these domains validate the analyses and suggest that the scaled Bregman theorem might be a worthy addition to the popular handful of Bregman divergence properties that have been pervasive in machine learning.	[Nock, Richard; Menon, Aditya Krishna; Ong, Cheng Soon] Data61, Canberra, ACT, Australia; [Nock, Richard; Menon, Aditya Krishna; Ong, Cheng Soon] Australian Natl Univ, Canberra, ACT, Australia; [Nock, Richard] Univ Sydney, Sydney, NSW, Australia	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Australian National University; University of Sydney	Nock, R (corresponding author), Data61, Canberra, ACT, Australia.; Nock, R (corresponding author), Australian Natl Univ, Canberra, ACT, Australia.; Nock, R (corresponding author), Univ Sydney, Sydney, NSW, Australia.	richard.nock@data61.csiro.au; aditya.menon@data61.csiro.au; chengsoon.ong@data61.csiro.au						[Anonymous], 2012, ICML; Arthur D., 2007, 19 SODA; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Boyd S, 2004, CONVEX OPTIMIZATION; Buja A., 2005, LOSS FUNCTIONS UNPUB; Buss SR, 2001, ACM T GRAPHIC, V20, P95, DOI 10.1145/502122.502124; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Collins M., 2002, MLJ; Dacorogna B, 2008, J CONVEX ANAL, V15, P271; Dhillon IS, 2007, SIAM J MATRIX ANAL A, V29, P1120, DOI 10.1137/060649021; Dhillon IS, 2001, MACH LEARN, V42, P143, DOI 10.1023/A:1007612920971; Duchi J., 2008, PROC 25 INT C MACH L, P272; Endo Yasunori, 2015, Modeling Decisions for Artificial Intelligence. 12th International Conference, MDAI 2015. Proceedings: 9321, P103, DOI 10.1007/978-3-319-23240-9_9; GALPERIN GA, 1993, COMMUN MATH PHYS, V154, P63, DOI 10.1007/BF02096832; Hernandez-Lobato M., 2016, 33RD ICML; Jaggi M., 2013, 30 ICML; Kivinen J, 2006, IEEE T SIGNAL PROCES, V54, P1782, DOI 10.1109/TSP.2006.872551; Kuang D, 2015, J GLOBAL OPTIM, V62, P545, DOI 10.1007/s10898-014-0247-2; Marechal P, 2005, J OPTIMIZ THEORY APP, V126, P175, DOI 10.1007/s10957-005-2667-0; Marechal P., 2005, J OPTIMIZATION THEOR, V126, P375; Menon A.-K., 2016, ICML; Nock R., 2008, ECML; Nock R, 2016, IEEE T INFORM THEORY, V62, P527, DOI 10.1109/TIT.2015.2448072; Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225; Reid MD, 2011, J MACH LEARN RES, V12, P731; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Rong G., 2010, 14 ACM SPM; Schwander O., 2013, MATRIX INFORM GEOMET, P403; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Straub J, 2014, PROC CVPR IEEE, P3770, DOI 10.1109/CVPR.2014.488; Sugiyama M, 2012, ANN I STAT MATH, V64, P1009, DOI 10.1007/s10463-011-0343-8; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Williamson R.-C., 2014, COMPOSITE MULT UNPUB; Zhang Han, 2016, CORR	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701033
C	Oh, TH; Matsushita, Y; Kweon, IS; Wipf, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Oh, Tae-Hyun; Matsushita, Yasuyuki; Kweon, In So; Wipf, David			A Pseudo-Bayesian Algorithm for Robust PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Commonly used in many applications, robust PCA represents an algorithmic attempt to reduce the sensitivity of classical PCA to outliers. The basic idea is to learn a decomposition of some data matrix of interest into low rank and sparse components, the latter representing unwanted outliers. Although the resulting problem is typically NP-hard, convex relaxations provide a computationally-expedient alternative with theoretical support. However, in practical regimes performance guarantees break down and a variety of non-convex alternatives, including Bayesian-inspired models, have been proposed to boost estimation quality. Unfortunately though, without additional a priori knowledge none of these methods can significantly expand the critical operational range such that exact principal subspace recovery is possible. Into this mix we propose a novel pseudo-Bayesian algorithm that explicitly compensates for design weaknesses in many existing non-convex approaches leading to state-of-the-art performance with a sound analytical foundation.	[Oh, Tae-Hyun; Kweon, In So] Korea Adv Inst Sci & Technol, Elect Engn, Daejeon, South Korea; [Matsushita, Yasuyuki] Osaka Univ, Multimedia Engn, Osaka, Japan; [Wipf, David] Microsoft Res, Beijing, Peoples R China	Korea Advanced Institute of Science & Technology (KAIST); Osaka University; Microsoft	Oh, TH (corresponding author), Korea Adv Inst Sci & Technol, Elect Engn, Daejeon, South Korea.	thoh.kaist.ac.kr@gmail.com; yasumat@ist.osaka-u.ac.jp; iskweon@kaist.ac.kr; davidwip@microsoft.com	; Oh, Tae-Hyun/D-7854-2016	Wipf, David/0000-0002-2768-4540; Oh, Tae-Hyun/0000-0003-0468-1571; Matsushita, Yasuyui/0000-0002-1935-4752	NRF of Korea grant - Korea government, MSIP [2010-0028680]; JSPS KAKENHI Grant [JP16H01732]	NRF of Korea grant - Korea government, MSIP; JSPS KAKENHI Grant(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	This work was done while the first author was an intern at Microsoft Research, Beijing. The first and third authors were supported by the NRF of Korea grant funded by the Korea government, MSIP (No. 2010-0028680). The second author was partly supported by JSPS KAKENHI Grant Number JP16H01732.	Babacan S. D., 2012, IEEE T SIGNAL PROCES; Bishop CM, 2006, PATTERN RECOGNITION; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V., 2011, SIAM J OPTIM; Chartrand R., 2012, IEEE T SIGNAL PROCES; Chen YL, 2013, IEEE I CONF COMP VIS, P1968, DOI 10.1109/ICCV.2013.247; Ding XH, 2011, IEEE T IMAGE PROCESS, V20, P3419, DOI 10.1109/TIP.2011.2156801; Eckstein J., 2011, FDN TRENDS MACH LEAR, V3, P1, DOI DOI 10.1561/2200000016; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Fan J., 2001, J AM STAT ASS; Hunter DR, 2004, AM STAT, V58, P30, DOI 10.1198/0003130042836; Ji H., 2010, IEEE C COMP VIS PATT; Jordan M. I., 1999, MACH LEARN; Lakshminarayanan B., 2011, AISTATS; Lin Z., 2010, ARXIV10095055, DOI DOI 10.1016/J.JSB.2012.10.010; Lu C., 2015, IEEE T IMAGE PROCESS; Mohan K., 2012, J MACH LEARN RES; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Murphy KP, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P467; Oh T.-H., 2016, IEEE T PATTERN ANAL; Oh T.-H., 2015, ARXIV151202188; Oh TH, 2015, IEEE T PATTERN ANAL, V37, P1219, DOI 10.1109/TPAMI.2014.2361338; Palmer J. A., 2003, TECH REP; Parker J. T., 2013, ARXIV13102632; Peng Y., 2012, IEEE T PATTERN ANAL; Shen HY, 2011, IEEE IC COMP COM NET; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Vidal R., 2011, IEEE SIGNAL PROCESS; Wang NY, 2013, IEEE I CONF COMP VIS, P657, DOI 10.1109/ICCV.2013.87; Wipf D., 2011, IEEE T INFORM THEORY; Wipf D. P., 2012, UAI; Wu L., 2010, AS C COMP VIS; Xin B, 2015, PR MACH LEARN RES, V37, P419	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703071
C	Onken, A; Panzeri, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Onken, Arno; Panzeri, Stefano			Mixed vine copulas as joint models of spike counts and local field potentials	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DISCRETE; CONSTRUCTIONS	Concurrent measurements of neural activity at multiple scales, sometimes performed with multimodal techniques, become increasingly important for studying brain function. However, statistical methods for their concurrent analysis are currently lacking. Here we introduce such techniques in a framework based on vine copulas with mixed margins to construct multivariate stochastic models. These models can describe detailed mixed interactions between discrete variables such as neural spike counts, and continuous variables such as local field potentials. We propose efficient methods for likelihood calculation, inference, sampling and mutual information estimation within this framework. We test our methods on simulated data and demonstrate applicability on mixed data generated by a biologically realistic neural network. Our methods hold the promise to considerably improve statistical analysis of neural data recorded simultaneously at different scales.	[Onken, Arno; Panzeri, Stefano] Ist Italiano Tecnol, I-38068 Rovereto, TN, Italy	Istituto Italiano di Tecnologia - IIT	Onken, A (corresponding author), Ist Italiano Tecnol, I-38068 Rovereto, TN, Italy.	arno.onken@iit.it; stefano.panzeri@iit.it			European Commission [659227]	European Commission(European CommissionEuropean Commission Joint Research Centre)	This work was supported by the European Commission's Horizon 2020 Programme (H2020-MSCA-IF-2014) under grant agreement number 659227 ("STOMMAC").	Aas K, 2009, INSUR MATH ECON, V44, P182, DOI 10.1016/j.insmatheco.2007.02.001; Acar EF, 2012, J MULTIVARIATE ANAL, V110, P74, DOI 10.1016/j.jmva.2012.02.001; AKAIKE H, 1974, IEEE T AUTOMAT CONTR, VAC19, P716, DOI 10.1109/TAC.1974.1100705; Byrd RH, 1999, SIAM J OPTIMIZ, V9, P877, DOI 10.1137/S1052623497325107; Carlson D. E., 2014, ADV NEURAL INFORM PR, P2060; Cover TM, 2006, ELEMENTS INFORM THEO; de Leon AR, 2011, STAT MED, V30, P175, DOI 10.1002/sim.4087; Ince RAA, 2013, J NEUROSCI, V33, P18277, DOI 10.1523/JNEUROSCI.2631-13.2013; Jaworski P., 2013, LECT NOTES STAT P, V213; Jenison RL, 2004, NEURAL COMPUT, V16, P665, DOI 10.1162/089976604322860659; Joe H., 1996, 166 U BRIT COL DEP S; Kelly RC, 2010, J COMPUT NEUROSCI, V29, P567, DOI 10.1007/s10827-009-0208-9; Nelsen RB., 2006, INTRO COPULAS, V2nd; Onken A, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000577; Panagiotelis A, 2012, J AM STAT ASSOC, V107, P1063, DOI 10.1080/01621459.2012.682850; Panzeri S, 2015, TRENDS COGN SCI, V19, P162, DOI 10.1016/j.tics.2015.01.002; Racine J, 2015, EMPIR ECON, V48, P37, DOI 10.1007/s00181-015-0913-3; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Sacerdote L, 2012, BRAIN RES, V1434, P243, DOI 10.1016/j.brainres.2011.08.064; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Sklar A., 1959, PUBLICATIONS I STAT, V8, P229, DOI DOI 10.1007/978-3-642-33590-7; Smith M., 2010, J AM STAT ASS, V105; Smith MS, 2012, J AM STAT ASSOC, V107, P290, DOI 10.1080/01621459.2011.644501; Song PXK, 2009, BIOMETRICS, V65, P60, DOI 10.1111/j.1541-0420.2008.01058.x; Tomsett RJ, 2015, BRAIN STRUCT FUNCT, V220, P2333, DOI 10.1007/s00429-014-0793-x	25	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700033
C	Ortega, PA; Stocker, AA		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ortega, Pedro A.; Stocker, Alan A.			Human Decision-Making under Limited Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REGRET THEORY	Subjective expected utility theory assumes that decision-makers possess unlimited computational resources to reason about their choices; however, virtually all decisions in everyday life are made under resource constraints i.e. decision-makers are bounded in their rationality. Here we experimentally tested the predictions made by a formalization of bounded rationality based on ideas from statistical mechanics and information-theory. We systematically tested human subjects in their ability to solve combinatorial puzzles under different time limitations. We found that our bounded-rational model accounts well for the data. The decomposition of the fitted model parameter into the subjects' expected utility function and resource parameter provide interesting insight into the subjects' information capacity limits Our results confirm that humans gradually fall back on their learned prior choice patterns when confronted with increasing resource limitations.	[Ortega, Pedro A.; Stocker, Alan A.] Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA	University of Pennsylvania	Ortega, PA (corresponding author), Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA.	ope@seas.upenn.edu; astocker@sas.upenn.edu			Office of Naval Research [N000141110744]; University of Pennsylvania	Office of Naval Research(Office of Naval Research); University of Pennsylvania	This work was supported by the Office of Naval Research (Grant N000141110744) and the University of Pennsylvania.	[Anonymous], 2012, ADV NEURAL INFORM PR; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; BELL DE, 1982, OPER RES, V30, P961, DOI 10.1287/opre.30.5.961; Bleichrodt H, 2015, ECON J, V125, P493, DOI 10.1111/ecoj.12200; Fishburn P.C., 1982, FDN EXPECTED UTILITY; Friedman JW, 2005, GAME ECON BEHAV, V51, P296, DOI 10.1016/j.geb.2003.08.004; Gigerenzer G., 2001, BOUNDED RATIONALITY; Lieder F., 2014, ADV NEURAL INFORM PR, V27, P2870; LOOMES G, 1982, ECON J, V92, P805, DOI 10.2307/2232669; Ortega P. A., 2013, P ROYAL SOC A, V469, P2153, DOI 1098/rspa.2012.0683; Rubinstein A., 1998, MODELING BOUNDED RAT; Savage LJ, 1954, FDN STAT; Shenhav A, 2013, NEURON, V79, P217, DOI 10.1016/j.neuron.2013.07.007; Simon H A, 1984, MODELS BOUNDED RATIO; Srivastava N., 2014, ADV NEURAL INFORM PR; Tishby N, 2011, PERCEPTION REASON AC; Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559; Ziebart B. D., 2008, AAAI, V8, P1433	18	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701106
C	Ostrovsky, D; Harchaoui, Z; Juditsky, A; Nemirovski, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ostrovsky, Dmitry; Harchaoui, Zaid; Juditsky, Anatoli; Nemirovski, Arkadi			Structure-Blind Signal Recovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of recovering a signal observed in Gaussian noise. If the set of signals is convex and compact, and can be specified beforehand, one can use classical linear estimators that achieve a risk within a constant factor of the minimax risk. However, when the set is unspecified, designing an estimator that is blind to the hidden structure of the signal remains a challenging problem. We propose a new family of estimators to recover signals observed in Gaussian noise. Instead of specifying the set where the signal lives, we assume the existence of a well-performing linear estimator. Proposed estimators enjoy exact oracle inequalities and can be efficiently computed through convex optimization. We present several numerical illustrations that show the potential of the approach.	[Ostrovsky, Dmitry; Juditsky, Anatoli] Univ Grenoble Alpes, LJK, 700 Ave Cent,38401 Domaine Univ, St Martin Dheres, France; [Harchaoui, Zaid] Univ Washington, Seattle, WA 98195 USA; [Nemirovski, Arkadi] Georgia Inst Technol, Atlanta, GA 30332 USA	Communaute Universite Grenoble Alpes; UDICE-French Research Universities; Universite Grenoble Alpes (UGA); University of Washington; University of Washington Seattle; University System of Georgia; Georgia Institute of Technology	Ostrovsky, D (corresponding author), Univ Grenoble Alpes, LJK, 700 Ave Cent,38401 Domaine Univ, St Martin Dheres, France.	dmitry.ostrovsky@imag.fr; zaid.harchaoui@imag.fr; anatoli.juditsky@imag.fr; arkadi.nemirovski@imag.fr	Nemirovski, Arkadi S/A-8375-2009	Nemirovski, Arkadi S/0000-0002-5001-7420	LabEx PERSYVAL-Lab [ANR-11-LABX-0025]; project Titan (CNRS-Mastodons); project Macaron [ANR-14-CE23-0003-01]; MSR-Inria joint centre; program "Learning in Machines and Brains" (CIFAR); NSF [CMMI-1262063, CCF-1523768]	LabEx PERSYVAL-Lab; project Titan (CNRS-Mastodons); project Macaron; MSR-Inria joint centre; program "Learning in Machines and Brains" (CIFAR); NSF(National Science Foundation (NSF))	We would like to thank Arnak Dalalyan and Gabriel Peyre for fruitful discussions. DO, AJ, ZH were supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025) and the project Titan (CNRS-Mastodons). ZH was also supported by the project Macaron (ANR-14-CE23-0003-01), the MSR-Inria joint centre, and the program "Learning in Machines and Brains" (CIFAR). Research of AN was supported by NSF grants CMMI-1262063, CCF-1523768.	Bhaskar BN, 2013, IEEE T SIGNAL PROCES, V61, P5987, DOI 10.1109/TSP.2013.2273443; DONOHO DL, 1992, ANN STAT, V20, P944, DOI 10.1214/aos/1176348665; DONOHO DL, 1994, ANN STAT, V22, P238, DOI 10.1214/aos/1176325367; Harchaoui Z., 2015, P 28 C LEARN THEOR C, P929; Haykin S., 1991, ADAPTIVE FILTER THEO; IBRAGIMOV IA, 1985, THEOR PROBAB APPL+, V29, P18, DOI 10.1137/1129002; IBRAGIMOV IA, 1988, THEOR PROBAB APPL+, V32, P30, DOI 10.1137/1132002; Juditsky A, 2010, APPL COMPUT HARMON A, V29, P354, DOI 10.1016/j.acha.2010.01.003; Juditsky A, 2009, APPL COMPUT HARMON A, V27, P157, DOI 10.1016/j.acha.2009.02.001; Juditsky AB, 2009, ANN STAT, V37, P2278, DOI 10.1214/08-AOS654; Kailath T, 2000, PR H INF SY, pXIX; Lepski O, 2014, ANN STAT, V42, P1, DOI 10.1214/13-AOS1152; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Ostrovsky D., 2016, ARXIV160705712V2; Tsybakov A.B, 2008, INTRO NONPARAMETRIC	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700027
C	Papa, G; Clemencon, S; Bellet, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Papa, Guillaume; Clemencon, Stephan; Bellet, Aurelien			On Graph Reconstruction via Empirical Risk Minimization: Fast Learning Rates and Scalability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				U-STATISTICS; RANKING	The problem of predicting connections between a set of data points finds many applications, in systems biology and social network analysis among others. This paper focuses on the graph reconstruction problem, where the prediction rule is obtained by minimizing the average error over all n(n - 1)/2 possible pairs of the n nodes of a training graph. Our first contribution is to derive learning rates of order O-P (log n/n) for this problem, significantly improving upon the slow rates of order O-P (1/root n) established in the seminal work of Biau and Bleakley (2006). Strikingly, these fast rates are universal, in contrast to similar results known for other statistical learning problems (e.g., classification, density level set estimation, ranking, clustering) which require strong assumptions on the distribution of the data. Motivated by applications to large graphs, our second contribution deals with the computational complexity of graph reconstruction. Specifically, we investigate to which extent the learning rates can be preserved when replacing the empirical reconstruction risk by a computationally cheaper Monte-Carlo version, obtained by sampling with replacement B << n(2) pairs of nodes. Finally, we illustrate our theoretical results by numerical experiments on synthetic and real graphs.	[Papa, Guillaume; Clemencon, Stephan] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France; [Bellet, Aurelien] INRIA, F-59650 Villeneuve Dascq, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay; Inria	Papa, G (corresponding author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.	guillaume.papa@telecom-paristech.fr; stephan.clemencon@telecom-paristech.fr; aurelien.bellet@inria.fr			chair "Machine Learning for Big Data" of Telecom ParisTech; CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020	chair "Machine Learning for Big Data" of Telecom ParisTech; CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020	This work was partially supported by the chair "Machine Learning for Big Data" of Telecom ParisTech and by a grant from CPER Nord-Pas de Calais/FEDER DATA Advanced data science and technologies 2015-2020.	Agarwal S, 2014, J MACH LEARN RES, V15, P1653; Antos A, 2005, IEEE T INFORM THEORY, V51, P4013, DOI 10.1109/TIT.2005.856976; ARCONES MA, 1994, STOCH PROC APPL, V52, P17, DOI 10.1016/0304-4149(94)90098-1; Bellet A., 2015, METRIC LEARNING; Biau G, 2006, STATIST RISK MODEL, V24, P209, DOI 10.1524/stnd.2006.24.2.209; Clemencon S., 2011, ICML; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Clemencon S, 2010, CONSTR APPROX, V32, P619, DOI 10.1007/s00365-010-9084-9; Cukierski W., 2011, IJCNN; de la Pena V., 2012, DECOUPLING DEPENDENC; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Jansen R, 2003, SCIENCE, V302, P449, DOI 10.1126/science.1087361; JANSON S, 1991, PROBAB THEORY REL, V90, P341, DOI 10.1007/BF01193750; Kanehisa M, 2001, Pharmacogenomics, V2, P373, DOI 10.1517/14622416.2.4.373; Lee A.J., 2019, U STAT THEORY PRACTI; Liben-Nowell D., 2003, CIKM; LICHTENWALTER RN, 2010, KDD; Mammen E, 1999, ANN STAT, V27, P1808; Massart P., 2006, ANN STAT, V34; Mattick JS, 2005, SCIENCE, V307, P856, DOI 10.1126/science.1103737; Rigollet P, 2009, BERNOULLI, V15, P1154, DOI 10.3150/09-BEJ184; Shaw B., 2011, NIPS; Spielman D., 2005, LECT NOTES IPCO SUMM; Tsybakov AB, 2004, ANN STAT, V32, P135; Vert JP, 2007, BMC BIOINFORMATICS, V8, DOI 10.1186/1471-2105-8-S10-S8; Vert JP, 2004, ADV NEURAL INFORM PR, V17, P1433	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705002
C	Papaxanthos, L; Llinares-Lopez, F; Bodenham, D; Borgwardt, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Papaxanthos, Laetitia; Llinares-Lopez, Felipe; Bodenham, Dean; Borgwardt, Karsten			Finding significant combinations of features in the presence of categorical covariates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ASSOCIATION	In high-dimensional settings, where the number of features p is much larger than the number of samples n, methods that systematically examine arbitrary combinations of features have only recently begun to be explored. However, none of the current methods is able to assess the association between feature combinations and a target variable while conditioning on a categorical covariate. As a result, many false discoveries might occur due to unaccounted confounding effects. We propose the Fast Automatic Conditional Search (FACS) algorithm, a significant discriminative itemset mining method which conditions on categorical covariates and only scales as O(k log k), where k is the number of states of the categorical covariate. Based on the Cochran-Mantel-Haenszel Test, FACS demonstrates superior speed and statistical power on simulated and real-world datasets compared to the state of the art, opening the door to numerous applications in biomedicine.	[Papaxanthos, Laetitia; Llinares-Lopez, Felipe; Bodenham, Dean; Borgwardt, Karsten] Swiss Fed Inst Technol, Machine Learning & Computat Biol Lab, D BSSE, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Papaxanthos, L (corresponding author), Swiss Fed Inst Technol, Machine Learning & Computat Biol Lab, D BSSE, Zurich, Switzerland.				SNSF Starting Grant 'Significant Pattern Mining'; Marie Curie ITN MLPM2012 [316861]	SNSF Starting Grant 'Significant Pattern Mining'; Marie Curie ITN MLPM2012	This work was funded in part by the SNSF Starting Grant 'Significant Pattern Mining' (KB) and the Marie Curie ITN MLPM2012, Grant No. 316861 (KB, FLL).	Atwell S, 2010, NATURE, V465, P627, DOI 10.1038/nature08800; Azencott CA, 2013, BIOINFORMATICS, V29, P171, DOI 10.1093/bioinformatics/btt238; Bonferroni C., 1936, TEORIA STAT CLASSI C, V8, P3; Devlin B, 1999, BIOMETRICS, V55, P997, DOI 10.1111/j.0006-341X.1999.00997.x; DUNN OJ, 1959, ANN MATH STAT, V30, P192, DOI 10.1214/aoms/1177706374; Fisher RA, 1922, J R STAT SOC, V85, P87, DOI 10.2307/2340521; Llinares-Lopez F, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P725, DOI 10.1145/2783258.2783363; Llinares-Lopez F, 2015, BIOINFORMATICS, V31, P240, DOI 10.1093/bioinformatics/btv263; MANTEL N, 1959, J NATL CANCER I, V22, P719; Minato Shin-ichi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P422, DOI 10.1007/978-3-662-44851-9_27; Pearson K, 1900, PHILOS MAG, V50, P157, DOI 10.1080/14786440009463897; Price AL, 2006, NAT GENET, V38, P904, DOI 10.1038/ng1847; Sugiyama M., 2015, SIAM DATA MINING SDM; TARONE RE, 1990, BIOMETRICS, V46, P515, DOI 10.2307/2531456; Terada A, 2013, P NATL ACAD SCI USA, V110, P12996, DOI 10.1073/pnas.1302233110; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vilhjalmsson BJ, 2013, NAT REV GENET, V14, P1, DOI 10.1038/nrg3382; Webb Geoffrey I., 2006, P 12 ACM SIGKDD INT, P434, DOI DOI 10.1145/1150402.1150451	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702092
C	Perrot, M; Courty, N; Flamary, R; Habrard, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Perrot, Michael; Courty, Nicolas; Flamary, Remi; Habrard, Amaury			Mapping Estimation for Discrete Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We are interested in the computation of the transport map of an Optimal Transport problem. Most of the computational approaches of Optimal Transport use the Kantorovich relaxation of the problem to learn a probabilistic coupling gamma but do not address the problem of learning the underlying transport map T linked to the original Monge problem. Consequently, it lowers the potential usage of such methods in contexts where out-of-samples computations are mandatory. In this paper we propose a new way to jointly learn the coupling and an approximation of the transport map. We use a jointly convex formulation which can be efficiently optimized. Additionally, jointly learning the coupling and the transport map allows to smooth the result of the Optimal Transport and generalize it to out-of-samples examples. Empirically, we show the interest and the relevance of our method in two tasks: domain adaptation and image editing.	[Perrot, Michael; Habrard, Amaury] Univ Lyon, UJM St Etienne, CNRS, Lab Hubert Curien UMR 5516, F-42023 St Etienne, France; [Courty, Nicolas] Univ Bretagne Sud, IRISA, UMR 6074, CNRS, Lorient, France; [Flamary, Remi] Univ Cote dAzur, UMR 7293, CNRS, Lagrange,OCA, Nice, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Jean Monnet; Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Earth Sciences & Astronomy (INSU); UDICE-French Research Universities; Universite Cote d'Azur; Observatoire de la Cote d'Azur	Perrot, M (corresponding author), Univ Lyon, UJM St Etienne, CNRS, Lab Hubert Curien UMR 5516, F-42023 St Etienne, France.	michael.perrot@univ-st-etienne.fr; courty@univ-ubs.fr; remi.flamary@unice.fr; amaury.habrard@univ-st-etienne.fr	Flamary, Rémi/AAC-1958-2022	Flamary, Rémi/0000-0002-4212-6627	french ANR project LIVES [ANR-15-CE23-0026-03]	french ANR project LIVES(French National Research Agency (ANR))	This work was supported in part by the french ANR project LIVES ANR-15-CE23-0026-03.	Benamou J.-D., 2014, J COMPUTATIONAL PHYS, V260; Benamou J.-D., 2015, SISC; Bruzzone L., 2010, IEEE PAML, V32; Canas G. D., 2012, NIPS; Courty N., 2014, ECML PKDD; Cuturi M., 2013, NEURIPS; Cuturi M., 2016, SIIMS; Cuturi M., 2014, ICML; Deng F., 2012, ACCV CHAPTER COLOR A; Fernando B., 2013, ICCV; Ferradans S., 2014, SIIMS; Frank M., 1956, NRL, V3; Frogner C., 2015, NIPS; Gong B., 2012, CVPR; Jaggi Martin., 2013, ICML; Kantorovich L., 1942, C R DOKLADY ACAD SCI, V37; McCann R., 1997, ADV MATH, V128; Mueller J., 2015, NIPS; Perez P., 2003, ACM T GRAPHICS, V22; Perrot M., 2015, NIPS; Reich S., 2013, SISC; Seguy V., 2015, NIPS; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Solomon Justin, 2014, ICML; Tseng P., 2001, J OPTIMIZATION THEOR, V109; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Zhang Ning, 2014, ICML; Zhong E., 2010, ECML PKDD	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702059
C	Ragain, S; Ugander, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ragain, Stephen; Ugander, Johan			Pairwise Choice Markov Chains	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODELS; ELIMINATION; CONTEXT; AXIOM	As datasets capturing human choices grow in richness and scale-particularly in online domains-there is an increasing need for choice models that escape traditional choice-theoretic axioms such as regularity, stochastic transitivity, and Luce's choice axiom. In this work we introduce the Pairwise Choice Markov Chain (PCMC) model of discrete choice, an inferentially tractable model that does not assume any of the above axioms while still satisfying the foundational axiom of uniform expansion, a considerably weaker assumption than Luce's choice axiom. We show that the PCMC model significantly outperforms both the Multinomial Logit (MNL) model and a mixed MNL (MMNL) model in prediction tasks on both synthetic and empirical datasets known to exhibit violations of Luce's axiom. Our analysis also synthesizes several recent observations connecting the Multinomial Logit model and Markov chains; the PCMC model retains the Multinomial Logit model as a special case.	[Ragain, Stephen; Ugander, Johan] Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA	Stanford University	Ragain, S (corresponding author), Stanford Univ, Management Sci & Engn, Stanford, CA 94305 USA.	sragain@stanford.edu; jugander@stanford.edu			David Morgenthaler II Faculty Fellowship; Dantzig-Lieberman Operations Research Fellowship	David Morgenthaler II Faculty Fellowship; Dantzig-Lieberman Operations Research Fellowship	This work was supported in part by a David Morgenthaler II Faculty Fellowship and a Dantzig-Lieberman Operations Research Fellowship.	ADAMS E, 1958, PSYCHOMETRIKA, V23, P355, DOI 10.1007/BF02289784; Benson A. R., 2016, WWW; Blanchet J., 2013, EC; Block H. D., 1960, CONTRIBUTIONS PROBAB, P97; BORSCHSUPAN A, 1990, J ECONOMETRICS, V43, P373, DOI 10.1016/0304-4076(90)90126-E; BOYD JH, 1980, TRANSPORT RES A-POL, V14, P367, DOI 10.1016/0191-2607(80)90055-2; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Chen Shuo, 2016, WSDM; Debreu G., 1960, AM EC REV; FALMAGNE JC, 1978, J MATH PSYCHOL, V18, P52, DOI 10.1016/0022-2496(78)90048-2; HARARY F, 1966, AM MATH MON, V73, P231, DOI 10.2307/2315334; HUBER J, 1982, J CONSUM RES, V9, P90, DOI 10.1086/208899; Ieong Samuel, 2012, ICML; Kleinberg J., 2015, P 16 ACM C EC COMP, P511; Kohli R, 2015, OPER RES, V63, P512, DOI 10.1287/opre.2015.1373; Koppelman F.S., 2006, SELF INSTRUCTING COU; Kumar R, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P359, DOI 10.1145/2684822.2685310; Luce R, 1959, INDIVIDUAL CHOICE BE; LUCE RD, 1977, J MATH PSYCHOL, V15, P215, DOI 10.1016/0022-2496(77)90032-3; MANSKI CF, 1977, THEOR DECIS, V8, P229, DOI 10.1007/BF00133443; Maystre L., 2015, NIPS; McFadden D, 2000, J APPL ECONOMET, V15, P447, DOI 10.1002/1099-1255(200009/10)15:5<447::AID-JAE570>3.0.CO;2-1; MCFADDEN D, 1980, J BUS, V53, pS13, DOI 10.1086/296093; Negahban Sahand, 2015, ARXIV12091688V4; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; SIMONSON I, 1992, J MARKETING RES, V29, P281, DOI 10.2307/3172740; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Trueblood JS, 2013, PSYCHOL SCI, V24, P901, DOI 10.1177/0956797612464241; TVERSKY A, 1972, PSYCHOL REV, V79, P281, DOI 10.1037/h0032955; YELLOTT JI, 1977, J MATH PSYCHOL, V15, P109, DOI 10.1016/0022-2496(77)90026-8	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702034
C	Rainforth, T; Le, TA; van de Meent, JW; Osborne, MA; Wood, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rainforth, Tom; Tuan Anh Le; van de Meent, Jan-Willem; Osborne, Michael A.; Wood, Frank			Bayesian Optimization for Probabilistic Programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present the first general purpose framework for marginal maximum a posteriori estimation of probabilistic program variables. By using a series of code transformations, the evidence of any probabilistic program, and therefore of any graphical model, can be optimized with respect to an arbitrary subset of its sampled variables. To carry out this optimization, we develop the first Bayesian optimization package to directly exploit the source code of its target, leading to innovations in problem-independent hyperpriors, unbounded optimization, and implicit constraint satisfaction; delivering significant performance improvements over prominent existing packages. We present applications of our method to a number of tasks including engineering design and parameter optimization.	[Rainforth, Tom; Tuan Anh Le; Osborne, Michael A.; Wood, Frank] Univ Oxford, Dept Engn Sci, Oxford, England; [van de Meent, Jan-Willem] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA	University of Oxford; Northeastern University	Rainforth, T (corresponding author), Univ Oxford, Dept Engn Sci, Oxford, England.	twgr@robots.ox.ac.uk; tuananh@robots.ox.ac.uk; j.vandemeent@northeastern.edu; mosb@robots.ox.ac.uk; fwood@robots.ox.ac.uk			BP industrial grant; Google studentship [DF6700]; DARPA PPAML through the U.S. AFRL [FA8750-14-2-0006, 61160290-111668]	BP industrial grant; Google studentship(Google Incorporated); DARPA PPAML through the U.S. AFRL	Tom Rainforth is supported by a BP industrial grant. Tuan Anh Le is supported by a Google studentship, project code DF6700. Frank Wood is supported under DARPA PPAML through the U.S. AFRL under Cooperative Agreement FA8750-14-2-0006, Sub Award number 61160290-111668.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; [Anonymous], 2015, ICML; Berard J, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-3428; Bergstra J. S., 2011, P 24 INT C NEUR INF, P2546, DOI DOI 10.1145/3065386; Carpenter B., 2015, J STAT SOFTWARE; Csillery K, 2010, TRENDS ECOL EVOL, V25, P410, DOI 10.1016/j.tree.2010.04.001; Duane S., 1987, PHYS LETT B; Eggensperger K., 2013, P NIPS WORKSH BAYES, P1; Gardner JR, 2014, PR MACH LEARN RES, V32, P937; Goodman N. D., 2014, DESIGN IMPLEMENTATIO; Goodman N. D., 2008, P 24 C UNCERTAINTY A, P220; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; HERNANDEZLOBATO D, 2016, JMLR, V17; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Mansinghka V., 2014, ARXIV14040099; Minka T, 2010, INFERNET 24 MICROSOF; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Osborne M, 2009, LEARNING INTELLIGENT, P1; Paige B., 2014, ADV NEURAL INFORM PR, P3410; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Shahriari B., 2016, AISTATS; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; van de Meent JW, 2016, JMLR WORKSH CONF PRO, V51, P1195; Wingate D., 2011, J MACH LEARN RES, V15, P770; Wood F., 2014, AISTATS, P2; Xie C., 2012, PHYS TEACHER, V50; Zinkov Robert, 2016, ARXIV160301882	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703057
C	Raju, RV; Pitkow, X		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Raju, Rajkumar V.; Pitkow, Xaq			Inference by Reparameterization in Neural Population Codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BAYESIAN-INFERENCE; INFORMATION	Behavioral experiments on humans and animals suggest that the brain performs probabilistic inference to interpret its environment. Here we present a new general-purpose, biologically-plausible neural implementation of approximate inference. The neural network represents uncertainty using Probabilistic Population Codes (PPCs), which are distributed neural representations that naturally encode probability distributions, and support marginalization and evidence integration in a biologically-plausible manner. By connecting multiple PPCs together as a probabilistic graphical model, we represent multivariate probability distributions. Approximate inference in graphical models can be accomplished by message-passing algorithms that disseminate local information throughout the graph. An attractive and often accurate example of such an algorithm is Loopy Belief Propagation (LBP), which uses local marginalization and evidence integration operations to perform approximate inference efficiently even for complex models. Unfortunately, a subtle feature of LBP renders it neurally implausible. However, LBP can be elegantly reformulated as a sequence of Tree-based Reparameterizations (TRP) of the graphical model. We re-express the TRP updates as a nonlinear dynamical system with both fast and slow timescales, and show that this produces a neurally plausible solution. By combining all of these ideas, we show that a network of PPCs can represent multivariate probability distributions and implement the TRP updates to perform probabilistic inference. Simulations with Gaussian graphical models demonstrate that the neural network inference quality is comparable to the direct evaluation of LBP and robust to noise, and thus provides a promising mechanism for general probabilistic inference in the population codes of the brain.	[Raju, Rajkumar V.] Rice Univ, Dept ECE, Houston, TX 77005 USA; [Pitkow, Xaq] Rice Univ, Baylor Coll Med, Dept Neurosci, Dept ECE, Houston, TX 77005 USA	Rice University; Baylor College of Medicine; Rice University	Raju, RV (corresponding author), Rice Univ, Dept ECE, Houston, TX 77005 USA.	rv12@rice.edu; xaq@rice.edu			McNair Foundation; NSF CAREER Award [IOS-1552868]; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]	McNair Foundation; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)	XP and RR were supported in part by a grant from the McNair Foundation, NSF CAREER Award IOS-1552868, and by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003.<SUP>1</SUP>	Archer E, 2015, STATML151107367 ARXI; Beck J., 2012, ADV NEURAL INFORM PR, V25, P3059; Beck JM, 2011, J NEUROSCI, V31, P15310, DOI 10.1523/JNEUROSCI.1706-11.2011; Beck JM, 2008, NEURON, V60, P1142, DOI 10.1016/j.neuron.2008.09.021; Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136; Doya K., 2007, BAYESIAN BRAIN PROBA; George D, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000532; Grabska-Barwinska A., 2013, ADV NEURAL INFORM PR, V26, P1968; Graf ABA, 2011, NAT NEUROSCI, V14, P239, DOI 10.1038/nn.2733; Hayden BY, 2010, J NEUROSCI, V30, P3339, DOI 10.1523/JNEUROSCI.4874-09.2010; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; Jazayeri M, 2006, NAT NEUROSCI, V9, P690, DOI 10.1038/nn1691; Knill DC, 1996, PERCEPTION BAYESIAN; Lee TS, 2003, J OPT SOC AM A, V20, P1434, DOI 10.1364/JOSAA.20.001434; Litvak S, 2009, NEURAL COMPUT, V21, P3010, DOI 10.1162/neco.2009.05-08-783; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; Ott T., 2006, ADV NEURAL INFORM PR, P1057; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pitkow X, 2015, COSYNE; Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495; Rao R. P. N., 2004, P ADV NEUR INF PROC, P1113; Raposo D, 2014, NAT NEUROSCI, V17, P1784, DOI 10.1038/nn.3865; Rigotti M, 2013, NATURE, V497, P585, DOI 10.1038/nature12160; Rubin DB, 2015, NEURON, V85, P402, DOI 10.1016/j.neuron.2014.12.026; Savin C, 2014, ADV NEURAL INF PROCE, P2024; Steimer A, 2009, NEURAL COMPUT, V21, P2502, DOI 10.1162/neco.2009.08-08-837; Wainwright MJ, 2003, IEEE T INFORM THEORY, V49, P1120, DOI 10.1109/TIT.2003.810642; Yedidia J., 2003, EXPLORING ARTIFICIAL, P236	28	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704001
C	Raziperchikolaei, R; Carreira-Perpinan, MA		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Raziperchikolaei, Ramin; Carreira-Perpinan, Miguel A.			Optimizing Affinity-Based Binary Hashing Using Auxiliary Coordinates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIMENSIONALITY REDUCTION; ALGORITHMS	In supervised binary hashing, one wants to learn a function that maps a high-dimensional feature vector to a vector of binary codes, for application to fast image retrieval. This typically results in a difficult optimization problem, nonconvex and nonsmooth, because of the discrete variables involved. Much work has simply relaxed the problem during training, solving a continuous optimization, and truncating the codes a posteriori. This gives reasonable results but is quite suboptimal. Recent work has tried to optimize the objective directly over the binary codes and achieved better results, but the hash function was still learned a posteriori, which remains suboptimal. We propose a general framework for learning hash functions using affinity-based loss functions that uses auxiliary coordinates. This closes the loop and optimizes jointly over the hash functions and the binary codes so that they gradually match each other. The resulting algorithm can be seen as an iterated version of the procedure of optimizing first over the codes and then learning the hash function. Compared to this, our optimization is guaranteed to obtain better hash functions while being not much slower, as demonstrated experimentally in various supervised datasets. In addition, our framework facilitates the design of optimization algorithms for arbitrary types of loss and hash functions.	[Raziperchikolaei, Ramin; Carreira-Perpinan, Miguel A.] Univ Calif Merced, EECS, Merced, CA 95343 USA	University of California System; University of California Merced	Raziperchikolaei, R (corresponding author), Univ Calif Merced, EECS, Merced, CA 95343 USA.	rraziperchikolaei@ucmerced.edu; mcarreira-perpinan@ucmerced.edu			NSF [IIS-1423515]	NSF(National Science Foundation (NSF))	Work supported by NSF award IIS-1423515.	Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Carreira-Perpinan M., 2014, AISTATS; Carreira-Perpinan M., 2015, NIPS; Carreira-Perpinan M. A., 2012, ARXIV12125921CSLG; Carreira-Perpinan M. A., 2015, CVPR; Carreira-Perpinan M. A, 2010, ICML; Ge T., 2014, ECCV; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Grauman K., 2013, MACHINE LEARNING COM, P49, DOI [DOI 10.1007/978-3-642-28661-2_3, 10.1007/978-3-642-28661-2_3]; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Kulis B., 2009, P INT C NEUR INF PRO; Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219; Lin G., 2014, CVPR; Lin G., 2013, ICCV; Liu W., 2011, ICML; Liu W., 2012, CVPR; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Norouzi M., 2011, ICML; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; van der Maaten L. J. P., 2013, INT C LEARN REPR ICL; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vladymyrov M., 2014, AISTATS; Vladymyrov M., 2012, ICML; Weiss Y., 2009, NIPS; Wu XH, 2012, IEEE SOUTHEASTCON; Yang Z., 2013, ICML; Yu S.X., 2003, ICCV; Zhang D., 2010, SIGIR; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702028
C	Reddi, SJ; Sra, S; Poczos, B; Smola, AJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Reddi, Sashank J.; Sra, Suvrit; Poczos, Barnabas; Smola, Alexander J.			Proximal Stochastic Methods for Nonsmooth Nonconvex Finite-Sum Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We analyze stochastic algorithms for optimizing nonconvex, nonsmooth finite-sum problems, where the nonsmooth part is convex. Surprisingly, unlike the smooth case, our knowledge of this fundamental problem is very limited. For example, it is not known whether the proximal stochastic gradient method with constant minibatch converges to a stationary point. To tackle this issue, we develop fast stochastic algorithms that provably converge to a stationary point for constant minibatches. Furthermore, using a variant of these algorithms, we obtain provably faster convergence than batch proximal gradient descent. Our results are based on the recent variance reduction techniques for convex optimization but with a novel analysis for handling nonconvex and nonsmooth functions. We also prove global linear convergence rate for an interesting subclass of nonsmooth nonconvex functions, which subsumes several recent works.	[Reddi, Sashank J.; Poczos, Barnabas; Smola, Alexander J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Sra, Suvrit] MIT, Cambridge, MA 02139 USA	Carnegie Mellon University; Massachusetts Institute of Technology (MIT)	Reddi, SJ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	sjakkamr@cs.cmu.edu; suvrit@mit.edu; bapoczos@cs.cmu.edu; alex@smola.org			NSF [IIS-1409802]	NSF(National Science Foundation (NSF))	SS acknowledges support of NSF grant: IIS-1409802.	Agarwal, 2014, ARXIV14100723; [Anonymous], ARXIV150702000; Bach F, 2012, OPTIMIZATION FOR MACHINE LEARNING, P19; Bottou Leon, 1991, P NEUR, V91, P8; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; FUKUSHIMA M, 1981, INT J SYST SCI, V12, P989, DOI 10.1080/00207728108963798; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Ghadimi Saeed, 2014, MATH PROGRAM, V155, P267; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Li Xingguo, 2016, ICML; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Nesterov Y., 2003, INTRO LECT CONVEX OP; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi Sashank J, 2016, 54 ANN ALL C COMM CO; Reddi Sashank J., 2016, CORR; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Schmidt Mark, 2013, ARXIV13092388; Shalev-Shwartz Shai, 2015, CORR; Shamir Ohad, 2014, ARXIV14092848; Smola, 2015, ADV NEURAL INFORM PR, P2629; Sra S., 2012, ADV NEURAL INFORM PR, P530; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhu Zeyuan Allen, 2015, CORR	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700080
C	Ren, Y; Li, JL; Luo, YC; Zhu, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ren, Yong; Li, Jialian; Luo, Yucen; Zhu, Jun			Conditional Generative Moment-Matching Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Maximum mean discrepancy (MMD) has been successfully applied to learn deep generative models for characterizing a joint distribution of variables via kernel mean embedding. In this paper, we present conditional generative moment-matching networks (CGMMN), which learn a conditional distribution given some input variables based on a conditional maximum mean discrepancy (CMMD) criterion. The learning is performed by stochastic gradient descent with the gradient calculated by back-propagation. We evaluate CGMMN on a wide range of tasks, including predictive modeling, contextual generation, and Bayesian dark knowledge, which distills knowledge from a Bayesian model by learning a relatively small CGMMN student network. Our results demonstrate competitive performance in all the tasks.	[Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, TNList Lab, Beijing, Peoples R China; Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Syst, Beijing, Peoples R China	Tsinghua University; Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, TNList Lab, Beijing, Peoples R China.	renyong15@mails.tsinghua.edu.cn; jl12@mails.tsinghua.edu.cn; luoyc15@mails.tsinghua.edu.cn; dcszj@tsinghua.edu.cn			National Basic Research Program (973 Program) of China [2013CB329403]; National NSF of China Projects [61620106010, 61322308, 61332007]; Youth Top-notch Talent Support Program; Tencent; Intel	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National NSF of China Projects; Youth Top-notch Talent Support Program; Tencent; Intel(Intel Corporation)	The work was supported by the National Basic Research Program (973 Program) of China (No. 2013CB329403), National NSF of China Projects (Nos. 61620106010, 61322308, 61332007), the Youth Top-notch Talent Support Program, and the Collaborative Projects with Tencent and Intel.	[Anonymous], 2001, NIPS; [Anonymous], 2014, ICLR; Denton E. L., 2015, NIPS; Domingos P, 2011, UAI; Dosovitskiy A., 2015, ARXIV14115928; Dziugaite G. K., 2015, UAI; Gens R., 2012, NIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A., 2008, JMLR; Grunewalder S., 2012, ICML; He XF, 2005, IEEE T PATTERN ANAL, V27, P328, DOI 10.1109/TPAMI.2005.55; Hernandez-Lobato J. M., 2015, ICML; Hofmann T, 2008, ANN STAT, V36, P1171, DOI 10.1214/009053607000000677; Kallenbery O., 2002, FDN MODERN PROBABILI; Kingma D.P., 2015, INT C LEARN REPR, P1; Korattikara A., 2015, NIPS; LAFFERTY J, 2001, ICML; Lee C., 2015, AISTATS; Li C., 2015, NIPS; Li Y., 2015, ICML 2015; Lin M., 2014, ICLR; Mirza M., 2014, ARXIV PREPRINT ARXIV; Nair V., 2010, ICML; Sermanet Pierre, 2012, ICPR; Shalev-Shwartz S., 2011, MATH PROGRAMMING B; Smola A., 2007, INT C ALG LEARN THEO; Sohn K., 2015, NIPS; Song L., 2009, ICML; Srivastava N., 2012, NIPS; Vinyals O., 2015, ARXIV14114555V2; Warde-Farley D., 2013, ICML; Yan X., 2015, ARXIV151200570; Zeiler M., 2013, ICLR	33	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702002
C	Rogers, R; Roth, A; Ullman, J; Vadhan, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rogers, Ryan; Roth, Aaron; Ullman, Jonathan; Vadhan, Salil			Privacy Odometers and Filters: Pay-as-you-Go Composition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				NOISE	In this paper we initiate the study of adaptive composition in differential privacy when the length of the composition, and the privacy parameters themselves can be chosen adaptively, as a function of the outcome of previously run analyses. This case is much more delicate than the setting covered by existing composition theorems, in which the algorithms themselves can be chosen adaptively, but the privacy parameters must be fixed up front. Indeed, it isn't even clear how to define differential privacy in the adaptive parameter setting. We proceed by defining two objects which cover the two main use cases of composition theorems. A privacy filter is a stopping time rule that allows an analyst to halt a computation before his pre-specified privacy budget is exceeded. A privacy odometer allows the analyst to track realized privacy loss as he goes, without needing to pre-specify a privacy budget. We show that unlike the case in which privacy parameters are fixed, in the adaptive parameter setting, these two use cases are distinct. We show that there exist privacy filters with bounds comparable (up to constants) with existing privacy composition theorems. We also give a privacy odometer that nearly matches non-adaptive private composition theorems, but is sometimes worse by a small asymptotic factor. Moreover, we show that this is inherent, and that any valid privacy odometer in the adaptive parameter setting must lose this factor, which shows a formal separation between the filter and odometer use-cases.	[Rogers, Ryan] Univ Penn, Dept Appl Math & Computat Sci, Philadelphia, PA 19104 USA; [Roth, Aaron] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA; [Ullman, Jonathan] Northeastern Univ, Coll Comp & Informat Sci, Boston, MA 02115 USA; [Vadhan, Salil] Harvard Univ, Ctr Res Computat & Soc, Cambridge, MA 02138 USA; [Vadhan, Salil] Harvard Univ, John A Paulson Sch Engn & Appl Sci, Cambridge, MA 02138 USA; [Vadhan, Salil] Natl Chiao Tung Univ, Dept Appl Math, Hsinchu, Taiwan; [Vadhan, Salil] Natl Chiao Tung Univ, Shing Tung Yau Ctr, Hsinchu, Taiwan	University of Pennsylvania; University of Pennsylvania; Northeastern University; Harvard University; Harvard University; National Yang Ming Chiao Tung University; National Yang Ming Chiao Tung University	Rogers, R (corresponding author), Univ Penn, Dept Appl Math & Computat Sci, Philadelphia, PA 19104 USA.	ryrogers@sas.upenn.edu; aaroth@cis.upenn.edu; jullman@ccs.neu.edu; salil@seas.harvard.edu			NSF CAREER award; NSF [CNS-1513694, CNS-1237235]; Sloan Foundation; Simons Investigator Award	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); Sloan Foundation(Alfred P. Sloan Foundation); Simons Investigator Award	Supported in part by an NSF CAREER award, NSF grant CNS-1513694, and a grant from the Sloan Foundation.; Work done while visiting the Department of Applied Mathematics and the Shing-Tung Yau Center at National Chiao-Tung University in Taiwan. Also supported by NSF grant CNS-1237235, a grant from the Sloan Foundation, and a Simons Investigator Award.	Bassily R., 2016, P 48 ANN ACM S THEOR; De La Pena VH, 2004, ANN PROBAB, V32, P1902, DOI 10.1214/009117904000000397; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, ACM S THEORY COMPUT, P117, DOI 10.1145/2746539.2746580; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Ebadi Hamid, 2015, ABS150502642 CORR; Kairouz P, 2015, PR MACH LEARN RES, V37, P1376; Kasivisiwanathan SP., 2014, J PRIVACY CONFIDENTI, V6, P1; Ledoux M., 1991, SERIES MODERN SURVEY; Murtagh J, 2016, LECT NOTES COMPUT SC, V9562, P157, DOI 10.1007/978-3-662-49096-9_7; van de Geer Sara A, 2002, HOEFFDINGS INEQUALIT	12	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701027
C	Rosenfeld, N; Globerson, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rosenfeld, Nir; Globerson, Amir			Optimal Tagging with Markov Chain Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many information systems use tags and keywords to describe and annotate content. These allow for efficient organization and categorization of items, as well as facilitate relevant search queries. As such, the selected set of tags for an item can have a considerable effect on the volume of traffic that eventually reaches an item. In tagging systems where tags are exclusively chosen by an item's owner, who in turn is interested in maximizing traffic, a principled approach for assigning tags can prove valuable. In this paper we introduce the problem of optimal tagging, where the task is to choose a subset of tags for a new item such that the probability of browsing users reaching that item is maximized. We formulate the problem by modeling traffic using a Markov chain, and asking how transitions in this chain should be modified to maximize traffic into a certain state of interest. The resulting optimization problem involves maximizing a certain function over subsets, under a cardinality constraint. We show that the optimization problem is NP-hard, but has a (1 - 1/e)-approximation via a simple greedy algorithm due to monotonicity and submodularity. Furthermore, the structure of the problem allows for an efficient computation of the greedy step. To demonstrate the effectiveness of our method, we perform experiments on three tagging datasets, and show that the greedy algorithm outperforms other baselines.	[Rosenfeld, Nir] Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel; [Globerson, Amir] Tel Aviv Univ, Blavatnik Sch Comp Sci, Tel Aviv, Israel	Hebrew University of Jerusalem; Tel Aviv University	Rosenfeld, N (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, Jerusalem, Israel.	nir.rosenfeld@mail.huji.ac.il; gamir@post.tau.ac.il			ISF Centers of Excellence [2180/15]; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	ISF Centers of Excellence; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	This work was supported by the ISF Centers of Excellence grant 2180/15, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).	Avrachenkov K, 2006, STOCH MODELS, V22, P319, DOI 10.1080/15326340600649052; Brin S, 2012, COMPUT NETW, V56, P3825, DOI 10.1016/j.comnet.2012.10.007; Cantador I., 2011, P 5 ACM C RECOMMENDE; Csaji BC, 2010, LECT NOTES ARTIF INT, V6331, P89, DOI 10.1007/978-3-642-16108-7_11; Fang Xiaomin, 2015, 29 AAAI C ART INT; Gionis A., 2013, P 2013 SIAM INT C DA, P387; Goyal A., 2011, PROC 20 INT C COMPAN, P47, DOI DOI 10.1145/1963192.1963217; Hotho A, 2006, LWA, V1, P111; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kim H.-N., 2011, P 5 ACM C REC SYST R, P45; Krause A, 2008, J MACH LEARN RES, V9, P235; Mavroforakis Charalampos, 2015, ARXIV150902533; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Olsen M, 2014, THEOR COMPUT SCI, V518, P96, DOI 10.1016/j.tcs.2013.08.003; Olsen M, 2010, LECT NOTES COMPUT SC, V6509, P87, DOI 10.1007/978-3-642-17461-2_7; Olsen M, 2010, LECT NOTES COMPUT SC, V6078, P37, DOI 10.1007/978-3-642-13073-1_5; REID JK, 1982, MATH PROGRAM, V24, P55, DOI 10.1007/BF01585094; Sigurbjornsson B., 2008, P 17 INT C WORLD WID, P327, DOI [10.1145/1367497.1367542, DOI 10.1145/1367497.1367542]	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700006
C	Roy, A; Pokutta, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Roy, Aurko; Pokutta, Sebastian			Hierarchical Clustering via Spreading Metrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the cost function for hierarchical clusterings introduced by [16] where hierarchies are treated as first-class objects rather than deriving their cost from projections into flat clusters. It was also shown in [16] that a top-down algorithm returns a hierarchical clustering of cost at most O (alpha(n) log n) times the cost of the optimal hierarchical clustering, where alpha(n) is the approximation ratio of the Sparsest Cut subroutine used. Thus using the best known approximation algorithm for Sparsest Cut due to Arora-Rao-Vazirani, the top-down algorithm returns a hierarchical clustering of cost at most O (log(3/2) n) times the cost of the optimal solution. We improve this by giving an O(log n)-approximation algorithm for this problem. Our main technical ingredients are a combinatorial characterization of ultrametrics induced by this cost function, deriving an Integer Linear Programming (ILP) formulation for this family of ultrametrics, and showing how to iteratively round an LP relaxation of this formulation by using the idea of sphere growing which has been extensively used in the context of graph partitioning. We also prove that our algorithm returns an O(log n)-approximate hierarchical clustering for a generalization of this cost function also studied in [16]. We also give constant factor inapproximability results for this problem.	[Roy, Aurko] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Pokutta, Sebastian] Georgia Inst Technol, ISyE, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Roy, A (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.	aurko@gatech.edu; sebastian.pokutta@isye.gatech.edu			NSF CAREER award [CMMI-1452463]; NSF [CMMI-1333789]	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF))	Research reported in this paper was partially supported by NSF CAREER award CMMI-1452463 and NSF grant CMMI-1333789. The authors thank Kunal Talwar and Mohit Singh for helpful discussions and anonymous reviewers for helping improve the presentation of this paper.	Ackerman M., 2010, COLT, P270; Ailon N, 2005, ANN IEEE SYMP FOUND, P73, DOI 10.1109/SFCS.2005.36; Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794; Balcan MF, 2008, ACM S THEORY COMPUT, P671; Bartal Y, 2004, LECT NOTES COMPUT SC, V3221, P89; Charikar M, 2003, ANN IEEE SYMP FOUND, P524, DOI 10.1109/SFCS.2003.1238225; Charikar Moses, 2016, ARXIV160909548; Dasgupta S, 2005, J COMPUT SYST SCI, V70, P555, DOI 10.1016/j.jcss.2004.10.006; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Di Summa M, 2015, DISCRETE APPL MATH, V180, P70, DOI 10.1016/j.dam.2014.07.023; Even G, 1999, SIAM J COMPUT, V28, P2187, DOI 10.1137/S0097539796308217; Even G, 2000, J ACM, V47, P585, DOI 10.1145/347476.347478; Felsenstein J, 2004, INFERRING PHYLOGENIE, V2; Friedman J., 2015, PACKAGE GLASSO; Garg N, 1996, SIAM J COMPUT, V25, P235, DOI 10.1137/S0097539793243016; Krauthgamer R, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P942; Leighton T., 1988, 29th Annual Symposium on Foundations of Computer Science (IEEE Cat. No.88CH2652-6), P422, DOI 10.1109/SFCS.1988.21958; Lichman M., 2013, UCI MACHINE LEARNING; Meila M, 2001, MACH LEARN, V42, P9, DOI 10.1023/A:1007648401407	19	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702072
C	Rudolph, M; Ruiz, FJR; Mandt, S; Blei, DM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rudolph, Maja; Ruiz, Francisco J. R.; Mandt, Stephan; Blei, David M.			Exponential Family Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Word embeddings are a powerful approach for capturing semantic similarity among terms in a vocabulary. In this paper, we develop exponential family embeddings, a class of methods that extends the idea of word embeddings to other types of high-dimensional data. As examples, we studied neural data with real-valued observations, count data from a market basket analysis, and ratings data from a movie recommendation system. The main idea is to model each observation conditioned on a set of other observations. This set is called the context, and the way the context is defined is a modeling choice that depends on the problem. In language the context is the surrounding words; in neuroscience the context is close-by neurons; in market basket data the context is other items in the shopping cart. Each type of embedding model defines the context, the exponential family of conditional distributions, and how the latent embedding vectors are shared across data. We infer the embeddings with a scalable algorithm based on stochastic gradient descent. On all three applications-neural activity of zebrafish, users' shopping behavior, and movie ratings-we found exponential family embedding models to be more effective than other types of dimension reduction. They better reconstruct held-out data and find interesting qualitative structure.	[Rudolph, Maja; Ruiz, Francisco J. R.; Mandt, Stephan; Blei, David M.] Columbia Univ, New York, NY 10027 USA; [Ruiz, Francisco J. R.] Univ Cambridge, Cambridge, England	Columbia University; University of Cambridge	Rudolph, M (corresponding author), Columbia Univ, New York, NY 10027 USA.				EU H2020 programme [706760]; NFS [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; Adobe; John Templeton Foundation; Sloan Foundation	EU H2020 programme; NFS; ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Adobe; John Templeton Foundation; Sloan Foundation(Alfred P. Sloan Foundation)	This work is supported by the EU H2020 programme (Marie Sklodowska-Curie grant agreement 706760), NFS IIS-1247664, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, the John Templeton Foundation, and the Sloan Foundation.	Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/NMETH.2434, 10.1038/nmeth.2434]; Arnold BC, 2001, STAT SCI, V16, P249; Bengio Y, 2006, STUD FUZZ SOFT COMP, V194, P137; Bronnenberg BJ, 2008, MARKET SCI, V27, P745, DOI 10.1287/mksc.1080.0450; Brown L.D., 1986, IMS; Collins M, 2001, ADV NEURAL INFORM PR, V14, P617; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Friedrich J., 2015, P NIPS WORKSHOP STAT; Gopalan P, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P326; Gutmann Michael, 2010, J MACHINE LEARNING R; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Harris ZS, 1954, WORD, V10, P146, DOI 10.1080/00437956.1954.11659520; Hu Y, 2008, DATA MINING; Levy O, 2014, ADV NEUR IN, V27; MCCuLLAGH P., 1989, GEN LINEAR MODELS, V37; Mikolov T., 2013, P NAACL 2013, P746; Mikolov T., 2013, ICLR WORKSH P; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mnih A., 2013, ADV NEURAL INFORM PR, V26, P2265; Mnih A., 2012, P 29 INT C MACH LEAR, P1751; Neal R. M., 1990, LEARNING STOCHASTIC, V64, P1577; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Ranganath R., 2015, ARTIFICIAL INTELLIGE; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Taddy M, 2015, ANN APPL STAT, V9, P1394, DOI 10.1214/15-AOAS831; VILNIS L., 2015, INT C LEARN REPR	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704095
C	Ruiz, FJR; Titsias, MK; Blei, DM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ruiz, Francisco J. R.; Titsias, Michalis K.; Blei, David M.			The Generalized Reparameterization Gradient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The reparameterization gradient has become a widely used method to obtain Monte Carlo gradients to optimize the variational objective. However, this technique does not easily apply to commonly used distributions such as beta or gamma without further approximations, and most practical applications of the reparameterization gradient fit Gaussian distributions. In this paper, we introduce the generalized reparameterization gradient, a method that extends the reparameterization gradient to a wider class of variational distributions. Generalized reparameterizations use invertible transformations of the latent variables which lead to transformed distributions that weakly depend on the variational parameters. This results in new Monte Carlo gradients that combine reparameterization gradients and score function gradients. We demonstrate our approach on variational inference for two complex probabilistic models. The generalized reparameterization is effective: even a single sample from the variational distribution is enough to obtain a low-variance gradient.	[Ruiz, Francisco J. R.] Univ Cambridge, Cambridge, England; [Ruiz, Francisco J. R.; Blei, David M.] Columbia Univ, New York, NY 10027 USA; [Titsias, Michalis K.] Athens Univ Econ & Business, Athens, Greece	University of Cambridge; Columbia University; Athens University of Economics & Business	Ruiz, FJR (corresponding author), Univ Cambridge, Cambridge, England.; Ruiz, FJR (corresponding author), Columbia Univ, New York, NY 10027 USA.				EU [706760]; NFS [IIS-1247664]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; Adobe; John Templeton Foundation; Sloan Foundation	EU(European Commission); NFS; ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Adobe; John Templeton Foundation; Sloan Foundation(Alfred P. Sloan Foundation)	This project has received funding from the EU H2020 programme (Marie Sklodowska-Curie grant agreement 706760), NFS IIS-1247664, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, DARPA N66001-15-C-4032, Adobe, the John Templeton Foundation, and the Sloan Foundation. The authors would also like to thank Kriste Krstovski, Alp Kuckukelbir, and Christian A. Naesseth for helpful comments and discussions.	[Anonymous], 2014, INT C MACH LEARN; [Anonymous], 2014, INT C MACH LEARN; Baydin Atilim Gunes, 2015, ABS150205767 CORR; BONNET G, 1964, ANNALES TELECOMMUN, V19, P203; Carbonetto P., 2009, ADV NEURAL INFORM PR, V22; Casella G, 1996, BIOMETRIKA, V83, P81, DOI 10.1093/biomet/83.1.81; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fan Kai, 2015, ADV NEURAL INFORM PR; Gelman A., 2015, ADV NEURAL INFORM PR; Ghahramani Z., 2001, ADV NEURAL INFORM PR; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Gu S, 2016, INT C LEARN REPR; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D., 2014, ADAM METHOD STOCHAST; Knowles D. A., 2015, ARXIV150901631V1; Kucukelbir Alp, 2016, ARXIV160300788; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Paisley J. W., 2012, INT C MACH LEARN; PRICE R, 1958, IRE T INFORM THEOR, V4, P69, DOI 10.1109/TIT.1958.1057444; Ranganath R., 2014, ARTIFICIAL INTELLIGE; Ranganath R., 2015, ARTIFICIAL INTELLIGE; Ross S., 2002, SIMULATION; Ruiz F. J. R., 2016, UNCERTAINTY ARTIFICI; Schulman J., 2015, ADV NEURAL INFORM PR; Tieleman T, 2012, COURSERA NEURAL NETW, V4; TITSIAS M. K., 2014, INT C MACH LEARN; Titsias M. K., 2015, ADV NEURAL INFORM PR; van de Meent J.-W., 2016, ARTIFICIAL INTELLIGE; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wingate D., 2013, ARXIV13011299	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702075
C	Saad, F; Mansinghka, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Saad, Feras; Mansinghka, Vikash			A Probabilistic Programming Approach To Probabilistic Data Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Probabilistic techniques are central to data analysis, but different approaches can be challenging to apply, combine, and compare. This paper introduces composable generative population models (CGPMs), a computational abstraction that extends directed graphical models and can be used to describe and compose a broad class of probabilistic data analysis techniques. Examples include discriminative machine learning, hierarchical Bayesian models, multivariate kernel methods, clustering algorithms, and arbitrary probabilistic programs. We demonstrate the integration of CGPMs into BayesDB, a probabilistic programming platform that can express data analysis tasks using a modeling definition language and structured query language. The practical value is illustrated in two ways. First, the paper describes an analysis on a database of Earth satellites, which identifies records that probably violate Kepler's Third Law by composing causal probabilistic programs with non-parametric Bayes in 50 lines of probabilistic code. Second, it reports the lines of code and accuracy of CGPMs compared with baseline solutions from standard machine learning libraries.	[Saad, Feras; Mansinghka, Vikash] MIT, Probabilist Comp Project, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Saad, F (corresponding author), MIT, Probabilist Comp Project, Cambridge, MA 02139 USA.	fsaad@mit.edu; vkm@mit.edu						[Anonymous], 2015, UCS SATELLITE DATABA; Carpenter B., 2016, J STAT SOFTW; Casella G, 2002, STAT INFERENCE, V2nd; Devroye L., 1986, 1986 Winter Simulation Conference Proceedings, P260, DOI 10.1145/318242.318443; Fink D., 1997, COMPENDIUM CONJUGATE; Friedman N, 1999, IJCAI-99: PROCEEDINGS OF THE SIXTEENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, VOLS 1 & 2, P1300; Giltinan DM, 1995, NONLINEAR MODELS REP, V62; Koller D., 2009, PROBABILISTIC GRAPHI; Mansinghka V., 2015, ARXIV151201272; Mansinghka V., 2015, ARXIV151205006; Mansinghka V. K., 2014, CORR; Milch B., 2007, STAT RELATIONAL LEAR; Pfeffer A., 2009, OBJECT ORIENTED PROB, V137; Saad Feras, 2016, ARXIV160805347; Spiegelhalter DJ, 1996, BUGS BAYESIAN INFERE	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700024
C	Sangnier, M; Fercoq, O; d'Alche-Buc, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sangnier, Maxime; Fercoq, Olivier; d'Alche-Buc, Florence			Joint quantile regression in vector-valued RKHSs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CURVES	Addressing the will to give a more complete picture than an average relationship provided by standard regression, a novel framework for estimating and predicting simultaneously several conditional quantiles is introduced. The proposed methodology leverages kernel-based multi-task learning to curb the embarrassing phenomenon of quantile crossing, with a one-step estimation procedure and no post-processing. Moreover, this framework comes along with theoretical guarantees and an efficient coordinate descent learning algorithm. Numerical experiments on benchmark and real datasets highlight the enhancements of our approach regarding the prediction error, the crossing occurrences and the training time.	[Sangnier, Maxime; Fercoq, Olivier; d'Alche-Buc, Florence] Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Sangnier, M (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France.	maxime.sangnier@telecom-paristech.fr; olivier.fercoq@telecom-paristech.fr; florence.dalche@telecom-paristech.fr			industrial chair "Machine Learning for Big Data"	industrial chair "Machine Learning for Big Data"	This work was supported by the industrial chair "Machine Learning for Big Data".	Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Anderson M. S., 2012, CVXOPT PYTHON PACKAG; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bondell HD, 2010, BIOMETRIKA, V97, P825, DOI 10.1093/biomet/asq048; Brouard C., 2011, P 28 INT C MACH LEAR, VProceedings of the 28th International Conference on Machine Learning (ICML 2011); Brouard C, 2016, J MACH LEARN RES, V17; Chernozhukov V, 2010, ECONOMETRICA, V78, P1093, DOI 10.3982/ECTA7880; Dette H, 2008, J ROY STAT SOC B, V70, P609, DOI 10.1111/j.1467-9868.2008.00651.x; Dinuzzo F., 2011, P 28 INT C MACH LEAR; Fercoq O., 2015, ARXIV150804625MATH; Minh HQ, 2016, J MACH LEARN RES, V17; Hallin M, 2016, STAT PROBABIL LETT, V109, P232, DOI 10.1016/j.spl.2015.11.021; He XM, 1997, AM STAT, V51, P186, DOI 10.2307/2685417; Kadri H., 2015, J MACH LEARN RES, V16, P1; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Koenker R., 2005, QUANTILE REGRESSION, DOI DOI 10.1017/CBO9780511754098; Li YJ, 2007, J AM STAT ASSOC, V102, P255, DOI 10.1198/016214506000000979; Liu YF, 2011, J NONPARAMETR STAT, V23, P415, DOI 10.1080/10485252.2010.537336; Maurer A., 2016, P 27 INT C ALG LEARN; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Schnabel SK, 2013, ASTA-ADV STAT ANAL, V97, P77, DOI 10.1007/s10182-012-0198-1; Sindhwani V., 2013, P 29 C UNC ART INT; Takeuchi I, 2004, IEEE IJCNN, P401, DOI 10.1109/IJCNN.2004.1379939; Takeuchi I., 2013, ADV NEURAL INFORM PR, V26, P1358; Takeuchi I, 2006, J MACH LEARN RES, V7, P1231; Wu YC, 2009, STAT INTERFACE, V2, P299	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701096
C	Savin, C; Tkacik, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Savin, Cristina; Tkacik, Gasper			Estimating Nonlinear Neural Response Functions using GP Priors and Kronecker Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GRID CELLS; HIPPOCAMPUS; POSITION; MAPS	Jointly characterizing neural responses in terms of several external variables promises novel insights into circuit function, but remains computationally prohibitive in practice. Here we use gaussian process (GP) priors and exploit recent advances in fast GP inference and learning based on Kronecker methods, to efficiently estimate multidimensional nonlinear tuning functions. Our estimator requires considerably less data than traditional methods and further provides principled uncertainty estimates. We apply these tools to hippocampal recordings during open field exploration and use them to characterize the joint dependence of CA1 responses on the position of the animal and several other variables, including the animal's speed, direction of motion, and network oscillations. Our results provide an unprecedentedly detailed quantification of the tuning of hippocampal neurons. The model's generality suggests that our approach can be used to estimate neural response properties in other brain regions.	[Savin, Cristina; Tkacik, Gasper] IST Austria, AT-3400 Klosterneuburg, Austria	Institute of Science & Technology - Austria	Savin, C (corresponding author), IST Austria, AT-3400 Klosterneuburg, Austria.	csavin@ist.ac.at; tkacik@ist.ac.at	Savin, Cristina/ABI-4570-2020	Savin, Cristina/0000-0002-3414-8244	People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA [291734]	People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA	We thank Jozsef Csicsvari for kindly sharing the CA1 data. This work was supported by the People Programme (Marie Curie Actions) of the European Union's Seventh Framework Programme (FP7/2007-2013) under REA grant agreement no. 291734.	Brun VH, 2008, HIPPOCAMPUS, V18, P1200, DOI 10.1002/hipo.20504; Dupret D, 2010, NAT NEUROSCI, V13, P995, DOI 10.1038/nn.2599; Fiser J., 2013, NOT ASSESS IMPORTANC; Frank LM, 2002, J NEUROSCI, V22, P3817; Grossmann Allie H, 2019, Small GTPases, V10, P1, DOI 10.1080/21541248.2016.1259710; Hensman J., 2015, ADV NEURAL INFORM PR; Huxter JR, 2008, NAT NEUROSCI, V11, P587, DOI 10.1038/nn.2106; Macke JH, 2011, NEUROIMAGE, V56, P570, DOI 10.1016/j.neuroimage.2010.04.272; MCNAUGHTON BL, 1983, EXP BRAIN RES, V52, P41; Moser EI, 2008, ANNU REV NEUROSCI, V31, P69, DOI 10.1146/annurev.neuro.31.061307.090723; Moser EI, 2014, NAT REV NEUROSCI, V15, P466, DOI 10.1038/nrn3766; Okun M., 2015, NATURE; Park M, 2014, NEURAL COMPUT, V26, P1519, DOI 10.1162/NECO_a_00615; Pillow J. W., 2006, BAYESIAN BRAIN PROBA, P1; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Rad KR, 2010, NETWORK-COMP NEURAL, V21, P142, DOI 10.3109/0954898X.2010.532288; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Saatci Y., 2012, THESIS U CAMBRIDGE C; Tkacik G, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003408; Tkacik G, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03011; Wilson A., 2013, P 30 INT C MACH LEAR, V28, P1075	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701010
C	Schulam, P; Arora, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Schulam, Peter; Arora, Raman			Disease Trajectory Maps	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODELS	Medical researchers are coming to appreciate that many diseases are in fact complex, heterogeneous syndromes composed of subpopulations that express different variants of a related complication. Longitudinal data extracted from individual electronic health records (EHR) offer an exciting new way to study subtle differences in the way these diseases progress over time. In this paper, we focus on answering two questions that can be asked using these databases of longitudinal EHR data. First, we want to understand whether there are individuals with similar disease trajectories and whether there are a small number of degrees of freedom that account for differences in trajectories across the population. Second, we want to understand how important clinical outcomes are associated with disease trajectories. To answer these questions, we propose the Disease Trajectory Map (DTM), a novel probabilistic model that learns low-dimensional representations of sparse and irregularly sampled longitudinal data. We propose a stochastic variational inference algorithm for learning the DTM that allows the model to scale to large modern medical datasets. To demonstrate the DTM, we analyze data collected on patients with the complex autoimmune disease, scleroderma. We find that DTM learns meaningful representations of disease trajectories and that the representations are significantly associated with important clinical outcomes.	[Schulam, Peter; Arora, Raman] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Johns Hopkins University	Schulam, P (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.	pschulam@cs.jhu.edu; arora@cs.jhu.edu			NSF Graduate Research Fellowship; NSF BIGDATA [IIS-1546482]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF BIGDATA	PS is supported by an NSF Graduate Research Fellowship. RA is supported in part by NSF BIGDATA grant IIS-1546482.	Allanore Y, 2015, NAT REV DIS PRIMERS, V1, DOI 10.1038/nrdp.2015.2; Bigelow Jamie L, 2012, J AM STAT ASS; Brillinger D. R., 2001, TIME SERIES DATA ANA, V36; Carvalho, 2012, J AM STAT ASS; Castaldi P. J., 2014, THORAX; CASTRO PE, 1986, TECHNOMETRICS, V28, P329, DOI 10.2307/1268982; CRAIG J, 2008, NAT ED, V1, P184; Damianou Andreas C., 2015, JMLR, V2; Gelman A., 2014, BAYESIAN DATA ANAL, V2; Hensman J., 2013, P C UNC ART INT UAI, P282; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; James GM, 2000, BIOMETRIKA, V87, P587, DOI 10.1093/biomet/87.3.587; KAISER HF, 1958, PSYCHOMETRIKA, V23, P187, DOI 10.1007/BF02289233; Keogh E, 2001, SIGMOD REC, V30, P151, DOI 10.1145/376284.375680; Kleinman KP, 1998, BIOMETRICS, V54, P921, DOI 10.2307/2533846; Lawrence ND, 2004, ADV NEUR IN, V16, P329; Levin K, 2013, 2013 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P410, DOI 10.1109/ASRU.2013.6707765; Lin J, 2007, DATA MIN KNOWL DISC, V15, P107, DOI 10.1007/s10618-007-0064-z; Lotvall J, 2011, J ALLERGY CLIN IMMUN, V127, P355, DOI 10.1016/j.jaci.2010.11.037; MacLehose RF, 2009, STAT SINICA, V19, P611; Marlin B.M., 2012, P 2 ACM SIGHIT INT H, P389; Ramsay J., 2002, APPL FUNCTIONAL DATA, DOI 10.1111/j.1467-985x.2004.t01-5-.x; Ramsay J. O., 2006, FUNCTIONAL DATA ANAL; Rice JA, 2001, BIOMETRICS, V57, P253, DOI 10.1111/j.0006-341X.2001.00253.x; Saria S., 2015, INT SYS IEEE; Saria S., 2011, IJCAI, V22; Schulam P., 2015, NIPS; Schulam P, 2015, AAAI CONF ARTIF INTE, P2956; Snelson E., 2005, NIPS; Duong T, 2012, P NATL ACAD SCI USA, V109, P8382, DOI 10.1073/pnas.1117796109; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; Titsias Michalis K., 2009, AISTATS; Titsias Michalis K., 2010, AISTATS; Varadarajan B., 2008, P ACL 08 HLT SHORT P, P165, DOI DOI 10.3115/1557690.1557736; Varga J, 2012, SCLERODERMA: FROM PATHOGENESIS TO COMPREHENSIVE MANAGEMENT, P1, DOI 10.1007/978-1-4419-5774-0; Verbeke G, 2009, SPRINGER SER STAT, P1; WATANABE S, 1965, P 4 PRAG C INF THEOR; Wiggins LD, 2012, J AUTISM DEV DISORD, V42, P191, DOI 10.1007/s10803-011-1230-0	38	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701034
C	Senanayake, R; Ott, L; O'Callaghan, S; Ramos, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Senanayake, Ransalu; Ott, Lionel; O'Callaghan, Simon; Ramos, Fabio			Spatio-Temporal Hilbert Maps for Continuous Occupancy Representation in Dynamic Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of building continuous occupancy representations in dynamic environments for robotics applications. The problem has hardly been discussed previously due to the complexity of patterns in urban environments, which have both spatial and temporal dependencies. We address the problem as learning a kernel classifier on an efficient feature space. The key novelty of our approach is the incorporation of variations in the time domain into the spatial domain. We propose a method to propagate motion uncertainty into the kernel using a hierarchical model. The main benefit of this approach is that it can directly predict the occupancy state of the map in the future from past observations, being a valuable tool for robot trajectory planning under uncertainty. Our approach preserves the main computational benefits of static Hilbert maps - using stochastic gradient descent for fast optimization of model parameters and incremental updates as new data are captured. Experiments conducted in road intersections of an urban environment demonstrated that spatio-temporal Hilbert maps can accurately model changes in the map while outperforming other techniques on various aspects.	[Senanayake, Ransalu; Ott, Lionel; Ramos, Fabio] Univ Sydney, Sydney, NSW, Australia; [O'Callaghan, Simon] CSIRO, Data61, Canberra, ACT, Australia	University of Sydney; Commonwealth Scientific & Industrial Research Organisation (CSIRO)	Senanayake, R (corresponding author), Univ Sydney, Sydney, NSW, Australia.	rsen4557@uni.sydney.edu.au; lionel.ott@sydney.edu.au; simon.ocallaghan@data61.csiro.au; fabio.ramos@sydney.edu.au						Bottou L., 2008, NEURAL INFORM PROCES; Doherty K., 2016, IEEE INT C ROB AUT I; ELFES A, 1987, IEEE T ROBOTIC AUTOM, V3, P249, DOI 10.1109/JRA.1987.1087096; Fleet D, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P239; Girard A., 2002, NEURAL INFORM PROCES; Hastie TJ, 2001, ELEMENTS STAT LEARNI; Krajnik T., 2014, IEEE INT C ROB AUT I; Kuhn Harold W, 1955, NAVAL RES LOGISTICS; Lucas B.D., 1981, IJCAI 81 P 7 INT JOI, P674, DOI DOI 10.1109/HPDC.2004.1323531; O'Callaghan ST, 2012, INT J ROBOT RES, V31, P42, DOI 10.1177/0278364911421039; Rahimi A., 2009, NEURAL INFORM PROCES; Rahimi A., 2008, NEURAL INFORM PROCES; Ramos F., 2015, P ROBOTICS SCI SYSTE; Ramos F., 2014, P INT S EXP ROB ISER; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Wackernagel H., 2003, MULTIVAR GEOSTATISTI, DOI DOI 10.1007/978-3-662-05294-5	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704065
C	Shaloudegi, K; Gyorgy, A; Szepesvari, C; Xu, W		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shaloudegi, Kiarash; Gyorgy, Andras; Szepesvari, Csaba; Xu, Wilsun			SDP Relaxation with Randomized Rounding for Energy Disaggregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We develop a scalable, computationally efficient method for the task of energy disaggregation for home appliance monitoring. In this problem the goal is to estimate the energy consumption of each appliance over time based on the total energy-consumption signal of a household. The current state of the art is to model the problem as inference in factorial HMMs, and use quadratic programming to find an approximate solution to the resulting quadratic integer program. Here we take a more principled approach, better suited to integer programming problems, and find an approximate optimum by combining convex semidefinite relaxations randomized rounding, as well as a scalable ADMM method that exploits the special structure of the resulting semidefinite program. Simulation results both in synthetic and real-world datasets demonstrate the superiority of our method.	[Shaloudegi, Kiarash; Gyorgy, Andras] Imperial Coll London, London, England; [Szepesvari, Csaba; Xu, Wilsun] Univ Alberta, Edmonton, AB, Canada	Imperial College London; University of Alberta	Shaloudegi, K (corresponding author), Imperial Coll London, London, England.	k.shaloudegi16@imperial.ac.uk; a.gyorgy@imperial.ac.uk; szepesva@ualberta.ca; wxu@ualberta.ca			Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning; NSERC	Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and by NSERC. K. is indebted to Pooria Joulani and Mohammad Ajallooeian, whom provided much useful technical advise, while all authors are grateful for Zico Kolter for sharing his code.	Anandkumar A., 2012, J MACHINE LEARNING R, V23; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chang HH, 2012, IEEE T IND APPL, V48, P764, DOI 10.1109/TIA.2011.2180497; Dong M, 2013, IEEE T SMART GRID, V4, P1421, DOI 10.1109/TSG.2013.2245926; Dong M, 2012, IEEE T SMART GRID, V3, P787, DOI 10.1109/TSG.2012.2185522; Egarter D, 2015, IEEE T INSTRUM MEAS, V64, P467, DOI 10.1109/TIM.2014.2344373; Figueiredo M, 2012, NEUROCOMPUTING, V96, P66, DOI 10.1016/j.neucom.2011.10.037; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Guo ZY, 2015, IEEE T POWER SYST, V30, P254, DOI 10.1109/TPWRS.2014.2327041; Kelly J, 2015, BUILDSYS'15 PROCEEDINGS OF THE 2ND ACM INTERNATIONAL CONFERENCE ON EMBEDDED SYSTEMS FOR ENERGY-EFFICIENT BUILT, P55, DOI 10.1145/2821650.2821672; Kim H., 2011, P SIAM C DAT MIN MES, DOI [10.1137/1.9781611972818.64, 10.1137/1, DOI 10.1137/1]; Koller D., 2009, PROBABILISTIC GRAPHI; Kolter J.Z., 2012, P ARTIFICIAL INTELLI, P1472; Kontorovich Aryeh, 2013, INT C MACH LEARN P 30 INT C MACH LEAR, P702; Liang J, 2010, IEEE T POWER DELIVER, V25, P551, DOI 10.1109/TPWRD.2009.2033799; Lofberg J., 2004, CACSD; Lovasz L, 1991, SIAM J OPTIMIZ, V1, P166, DOI 10.1137/0801013; Malick J, 2009, SIAM J OPTIMIZ, V20, P336, DOI 10.1137/070704575; Mattfeld Carl, 2014, ARXIV14047472; Nesterov Y., 2018, APPL OPTIMIZATION; Park J., 2015, ARXIV150407672; Prudenzi A, 2002, 2002 IEEE POWER ENGINEERING SOCIETY WINTER MEETING, VOLS 1 AND 2, CONFERENCE PROCEEDINGS, P941, DOI 10.1109/PESW.2002.985144; Rennie SJ, 2009, INT CONF ACOUST SPEE, P3845, DOI 10.1109/ICASSP.2009.4960466; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weiss M, 2012, INT CONF PERVAS COMP, P190, DOI 10.1109/PerCom.2012.6199866; Wen ZW, 2010, MATH PROGRAM COMPUT, V2, P203, DOI 10.1007/s12532-010-0017-1; Zhong MJ, 2014, ADV NEUR IN, V27; Zia Tehseen, 2011, IECON 2011 - 37th Annual Conference of IEEE Industrial Electronics, P3218, DOI 10.1109/IECON.2011.6119826; Zico Kolter J., 2010, ADV NEURAL INFORM PR, V1, P1153; Zico Kolter J, 2011, WORKSH DAT MIN APPL, V25, P59	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704079
C	Sheikh, AS; Lucke, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sheikh, Abdul-Saboor; Luecke, Joerg			Select-and-Sample for Spike-and-Slab Sparse Coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Probabilistic inference serves as a popular model for neural processing. It is still unclear, however, how approximate probabilistic inference can be accurate and scalable to very high-dimensional continuous latent spaces. Especially as typical posteriors for sensory data can be expected to exhibit complex latent dependencies including multiple modes. Here, we study an approach that can efficiently be scaled while maintaining a richly structured posterior approximation under these conditions. As example model we use spike-and-slab sparse coding for V1 processing, and combine latent subspace selection with Gibbs sampling (select-and-sample). Unlike factored variational approaches, the method can maintain large numbers of posterior modes and complex latent dependencies. Unlike pure sampling, the method is scalable to very high-dimensional latent spaces. Among all sparse coding approaches with non-trivial posterior approximations (MAP or ICA-like models), we report the largest-scale results. In applications we firstly verify the approach by showing competitiveness in standard denoising benchmarks. Secondly, we use its scalability to, for the first time, study highly-overcomplete settings for V1 encoding using sophisticated posterior representations. More generally, our study shows that very accurate probabilistic inference for multi-modal posteriors with complex dependencies is tractable, functionally desirable and consistent with models for neural inference.	[Sheikh, Abdul-Saboor] Tech Univ Berlin, Berlin, Germany; [Sheikh, Abdul-Saboor; Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Cluster Excellence Hearing4all, Oldenburg, Germany; [Sheikh, Abdul-Saboor] SAP Innovat Ctr Network, Berlin, Germany; [Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Res Ctr Neurosensory Sci, Oldenburg, Germany; [Luecke, Joerg] Carl von Ossietzky Univ Oldenburg, Dept Med Phys & Acoust, Oldenburg, Germany	Technical University of Berlin; Carl von Ossietzky Universitat Oldenburg; Carl von Ossietzky Universitat Oldenburg; Carl von Ossietzky Universitat Oldenburg	Sheikh, AS (corresponding author), Tech Univ Berlin, Berlin, Germany.; Sheikh, AS (corresponding author), SAP Innovat Ctr Network, Berlin, Germany.	sheikh.abdulsaboor@gmail.com; joerg.luecke@uol.de			DFG: Cluster of Excellence EXC 1077/1 (Hearing4all);  [LU 1196/5-1]	DFG: Cluster of Excellence EXC 1077/1 (Hearing4all)(German Research Foundation (DFG)); 	We thank E. Guiraud for help with Alg. 1 (illustration) and acknowledge funding by the DFG: Cluster of Excellence EXC 1077/1 (Hearing4all) and grant LU 1196/5-1.	Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870; Coates Adam, 2011, P 28 INT C MACH LEAR, P921; Dai Zhenwen, 2013, ADV NEURAL INFORM PR, P243; Exarchakis Georgios, 2012, Latent Variable Analysis and Signal Separation. Proceedings 10th International Conference, LVA/ICA 2012, P204, DOI 10.1007/978-3-642-28551-6_26; Garrigues P., 2007, NIPS; Goodfellow IJ, 2013, IEEE T PATTERN ANAL, V35, P1902, DOI 10.1109/TPAMI.2012.273; Hinton GE, 1998, NATO ADV SCI I D-BEH, V89, P479; Ilin A, 2005, NEURAL PROCESS LETT, V22, P183, DOI 10.1007/s11063-005-5265-0; Jost P., 2006, IEEE INT C AC SPEECH, V5; Kruger N., 1997, EUR S ANNS; Li HB, 2009, PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON IMAGE AND GRAPHICS (ICIG 2009), P754, DOI 10.1109/ICIG.2009.101; Lucke Jorg, 2012, Latent Variable Analysis and Signal Separation. Proceedings 10th International Conference, LVA/ICA 2012, P213, DOI 10.1007/978-3-642-28551-6_27; Lucke J, 2010, J MACH LEARN RES, V11, P2855; Mairal J, 2012, FOUND TRENDS COMPUT, V8, DOI 10.1561/0600000058; Mensch A., 2016, ICML; Mohamed S., 2012, ICML; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Olshausen BA, 2013, PROC SPIE, V8651, DOI 10.1117/12.2013504; Patel A. B., 2016, ADV NEURAL INFORM PR; Puertas G., 2010, ADV NEURAL INF PROCE, V23, P1939; REZENDE DJ, 2015, ICML; Ringach DL, 2002, J NEUROPHYSIOL, V88, P455, DOI 10.1152/jn.2002.88.1.455; Salimans T., 2015, ICML; Schnass K, 2015, J MACH LEARN RES, V16, P1211; Sheikh AS, 2014, J MACH LEARN RES, V15, P2653; Shelton J. A., 2011, ADV NEURAL INFORM PR, V24, P2618; Shelton JA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0124088; Titsias M. K., 2011, PROC 24 INT C NEURAL; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; Zhou M, 2009, ADV NEURAL INFORM PR, P2295; Zylberberg J, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002250	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702023
C	Shen, YY; Huang, QX; Srebro, N; Sanghavi, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shen, Yanyao; Huang, Qixing; Srebro, Nathan; Sanghavi, Sujay			Normalized Spectral Map Synchronization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Estimating maps among large collections of objects (e.g., dense correspondences across images and 3D shapes) is a fundamental problem across a wide range of domains. In this paper, we provide theoretical justifications of spectral techniques for the map synchronization problem, i.e., it takes as input a collection of objects and noisy maps estimated between pairs of objects along a connected object graph, and outputs clean maps between all pairs of objects. We show that a simple normalized spectral method (or NormSpecSync) that projects the blocks of the top eigenvectors of a data matrix to the map space, exhibits surprisingly good behavior -NormSpecSync is much more efficient than state-of-the-art convex optimization techniques, yet still admitting similar exact recovery conditions. We demonstrate the usefulness of NormSpecSync on both synthetic and real datasets.	[Shen, Yanyao; Huang, Qixing; Sanghavi, Sujay] UT Austin, Austin, TX 78712 USA; [Huang, Qixing; Srebro, Nathan] TTI Chicago, Chicago, IL 60637 USA	University of Texas System; University of Texas Austin	Shen, YY (corresponding author), UT Austin, Austin, TX 78712 USA.	shenyanyao@utexas.edu; huangqx@cs.utexas.edu; nati@ttic.edu; sanghavi@mail.utexas.edu			 [DMS-1700234];  [CCF-1302435];  [CCF-1320175];  [CCF-1564000];  [CNS-0954059];  [IIS-1302662];  [IIS-1546500]	; ; ; ; ; ; 	We would like to thank the anonymous reviewers for detailed comments on how to improve the paper. The authors would like to thank the support of DMS-1700234, CCF-1302435, CCF-1320175, CCF-1564000, CNS-0954059, IIS-1302662, and IIS-1546500.	Nguyen A, 2011, COMPUT GRAPH FORUM, V30, P1481, DOI 10.1111/j.1467-8659.2011.02022.x; Burkard R., 2009, ASSIGNMENT PROBLEMS; Caetano TS, 2007, IEEE I CONF COMP VIS, P86; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chen Y., 2015, CORR; Crandall D., 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3001, DOI 10.1109/CVPR.2011.5995626; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Huang QX, 2006, ACM T GRAPHIC, V25, P569, DOI 10.1145/1141911.1141925; Huang QX, 2013, COMPUT GRAPH FORUM, V32, P177, DOI 10.1111/cgf.12184; Huber D. F., 2002, TECH REP; Kim V.G, 2012, TOG; Kim VG, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964974; Liu C, 2008, LECT NOTES COMPUT SC, V5304, P28, DOI 10.1007/978-3-540-88690-7_3; Liu HL, 2014, INT J OPT, V2014, DOI 10.1155/2014/693807; Marande W, 2007, SCIENCE, V318, P415, DOI 10.1126/science.1148033; Osada R, 2002, ACM T GRAPHIC, V21, P807, DOI 10.1145/571647.571648; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Wang L., 2012, CORR; Zach C, 2010, PROC CVPR IEEE, P1426, DOI 10.1109/CVPR.2010.5539801	21	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700092
C	Shen, YJ; Choi, A; Darwiche, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shen, Yujia; Choi, Arthur; Darwiche, Adnan			Tractable Operations for Arithmetic Circuits of Probabilistic Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BELIEF PROPAGATION; INFERENCE	We consider tractable representations of probability distributions and the polytime operations they support. In particular, we consider a recently proposed arithmetic circuit representation, the Probabilistic Sentential Decision Diagram (PSDD). We show that PSDDs support a polytime multiplication operator, while they do not support a polytime operator for summing-out variables. A polytime multiplication operator makes PSDDs suitable for a broader class of applications compared to classes of arithmetic circuits that do not support multiplication. As one example, we show that PSDD multiplication leads to a very simple but effective compilation algorithm for probabilistic graphical models: represent each model factor as a PSDD, and then multiply them.	[Shen, Yujia; Choi, Arthur; Darwiche, Adnan] Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Shen, YJ (corresponding author), Univ Calif Los Angeles, Comp Sci Dept, Los Angeles, CA 90095 USA.	yujias@cs.ucla.edu; ychoi@cs.ucla.edu; darwiche@cs.ucla.edu			NSF [IIS-1514253]; ONR [N00014-15-1-2339]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	This work was partially supported by NSF grant #IIS-1514253 and ONR grant #N00014-15-1-2339.	Acar U. A., 2008, UAI, P1; BAHAR RI, 1993, 1993 IEEE/ACM INTERNATIONAL CONFERENCE ON COMPUTER-AIDED DESIGN - DIGEST OF TECHNICAL PAPERS, P188, DOI 10.1109/ICCAD.1993.580054; Bekker J., 2015, NIPS; Boutilier C, 1996, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P115; Bova S, 2016, AAAI CONF ARTIF INTE, P929; Chan H., 2006, UAI; Chavira M, 2008, ARTIF INTELL, V172, P772, DOI 10.1016/j.artint.2007.11.002; Choi Arthur, 2013, Symbolic and Quantitative Approaches to Reasoning with Uncertainty. 12th European Conference, ECSQARU 2013. Proceedings. LNCS 7958, P121, DOI 10.1007/978-3-642-39091-3_11; Choi Arthur, 2011, New Frontiers in Artificial Intelligence. JSAI-isAI 2010 Workshops LENLS, JURISIN, AMBN, ISS. Revised Selected Papers, P167, DOI 10.1007/978-3-642-25655-4_16; Choi A., 2016, AAAI; Choi A., 2015, IJCAI; Darwiche A, 2009, MODELING AND REASONING WITH BAYESIAN NETWORKS, P1, DOI 10.1017/CBO9780511811357; Darwiche A, 2001, J ACM, V48, P608, DOI 10.1145/502090.502091; Darwiche A, 2002, J ARTIF INTELL RES, V17, P229, DOI 10.1613/jair.989; Darwiche A, 2003, J ACM, V50, P280, DOI 10.1145/765568.765570; Darwiche A., 2011, P 22 INT JOINT C ART, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143; Darwiche A., 2001, J APPL NONCLASSICAL, V11, P11; Darwiche Adnan, 2008, RESULTS PROBABILISTI; Delcher A. L., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P116; Domingos P, 2008, P 24 C UNC ART INT, P383; Felzenszwalb PF, 2006, INT J COMPUT VISION, V70, P41, DOI 10.1007/s11263-006-7899-4; Friedman N, 1998, NATO ADV SCI I D-BEH, V89, P421; Gogate Vibhav., 2013, UAI; Herrmann RG, 2013, 2013 BRAZILIAN CONFERENCE ON INTELLIGENT SYSTEMS (BRACIS), P175, DOI 10.1109/BRACIS.2013.37; Jaeger M, 2004, INT J UNCERTAIN FUZZ, V12, P19, DOI 10.1142/S0218488504002564; Kisa D., 2014, KR; Koller D., 2009, PROBABILISTIC GRAPHI; Larkin D., 2003, AISTATS; Lowd D., 2013, P 16 INT C ART INT S, P406; Mateescu R, 2008, J ARTIF INTELL RES, V33, P465, DOI 10.1613/jair.2605; Oztok U, 2014, LECT NOTES COMPUT SC, V8656, P42, DOI 10.1007/978-3-319-10428-7_7; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Poon Hoifung, 2011, P 20 C UNC ART INT U, P337; Rooshenas A, 2014, PR MACH LEARN RES, V32; Roth D, 1996, ARTIF INTELL, V82, P273, DOI 10.1016/0004-3702(94)00092-1; Sanner S, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1384; SHIMONY SE, 1994, ARTIF INTELL, V68, P399, DOI 10.1016/0004-3702(94)90072-8; Sieling D., 1993, Parallel Processing Letters, V3, P3, DOI 10.1142/S0129626493000022; Xue Y, 2012, AAAI, P842; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	40	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702110
C	Shishkin, A; Bezzubtseva, A; Drutsa, A; Shishkov, I; Gladkikh, E; Gusev, G; Serdyukov, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shishkin, Alexander; Bezzubtseva, Anastasia; Drutsa, Alexey; Shishkov, Ilia; Gladkikh, Ekaterina; Gusev, Gleb; Serdyukov, Pavel			Efficient High-Order Interaction-Aware Feature Selection Based on Conditional Mutual Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				RELEVANCE	This study introduces a novel feature selection approach CMICOT, which is a further evolution of filter methods with sequential forward selection (SFS) whose scoring functions are based on conditional mutual information (MI). We state and study a novel saddle point (max-min) optimization problem to build a scoring function that is able to identify joint interactions between several features. This method fills the gap of MI-based SFS techniques with high-order dependencies. In this high-dimensional case, the estimation of MI has prohibitively high sample complexity. We mitigate this cost using a greedy approximation and binary representatives what makes our technique able to be effectively used. The superiority of our approach is demonstrated by comparison with recently proposed interaction-aware filters and several interaction-agnostic state-of-the-art ones on ten publicly available benchmark datasets.	[Shishkin, Alexander; Bezzubtseva, Anastasia; Drutsa, Alexey; Shishkov, Ilia; Gladkikh, Ekaterina; Gusev, Gleb; Serdyukov, Pavel] Yandex, 16 Leo Tolstoy St, Moscow 119021, Russia		Shishkin, A (corresponding author), Yandex, 16 Leo Tolstoy St, Moscow 119021, Russia.	sisoid@yandex-team.ru; nstbezz@yandex-team.ru; adrutsa@yandex-team.ru; ishfb@yandex-team.ru; kglad@yandex-team.ru; gleb57@yandex-team.ru; pavser@yandex-team.ru	Gusev, Gleb G./C-8263-2014					BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Brown G, 2012, J MACH LEARN RES, V13, P27; Chen  Z., 2015, ARXIV150200231; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dougherty J., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P194; Elisseeff A., 2003, J MACH LEARN RES, V3, P1157, DOI DOI 10.1162/153244303322753616; Friedman J., 2001, ANN STAT; Hand D. J., 2001, MACHINE LEARNING; Hutter M, 2002, ADV NEUR IN, V14, P399; Jakulin A., 2003, ANAL ATTRIBUTE DEPEN; Lewis D. D., 1992, P WORKSH SPEECH NAT, P212; Liu Jie, 2012, JMLR Workshop Conf Proc, V22, P712; Meyer PE, 2008, IEEE J-STSP, V2, P261, DOI 10.1109/JSTSP.2008.923858; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Vergara JR, 2014, NEURAL COMPUT APPL, V24, P175, DOI 10.1007/s00521-013-1368-0; Vinh N. X., 2015, PATTERN RECOGNITION; Wang G., P 13 ACM INT C INF K, P342, DOI DOI 10.1145/1031171.1031241; WHITNEY AW, 1971, IEEE T COMPUT, VC 20, P1100, DOI 10.1109/T-C.1971.223410; Yang H., 1999, P INT ICSC S ADV INT, P22; Yu L, 2004, J MACH LEARN RES, V5, P1205; Zaffalon M., 2002, P 18 INT C UNC ART I, P577; Zeng ZL, 2015, PATTERN RECOGN, V48, P2656, DOI 10.1016/j.patcog.2015.02.025; Zhao Z, 2009, INTELL DATA ANAL, V13, P207, DOI 10.3233/IDA-2009-0364	25	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704108
C	Shpakova, T; Bach, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shpakova, Tatiana; Bach, Francis			Parameter Learning for Log-supermodular Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider log-supermodular models on binary variables, which are probabilistic models with negative log-densities which are submodular. These models provide probabilistic interpretations of common combinatorial optimization tasks such as image segmentation. In this paper, we focus primarily on parameter estimation in the models from known upper-bounds on the intractable log-partition function. We show that the bound based on separable optimization on the base polytope of the submodular function is always inferior to a bound based on "perturb-and-MAP" ideas. Then, to learn parameters, given that our approximation of the log-partition function is an expectation (over our own randomization), we use a stochastic subgradient technique to maximize a lower-bound on the log-likelihood. This can also be extended to conditional maximum likelihood. We illustrate our new results in a set of experiments in binary image denoising, where we highlight the flexibility of a probabilistic model to learn with missing data.	[Shpakova, Tatiana; Bach, Francis] Ecole Normale Super Paris, INRIA, Paris, France	Ecole Normale Superieure (ENS); Inria	Shpakova, T (corresponding author), Ecole Normale Super Paris, INRIA, Paris, France.	tatiana.shpakova@inria.fr; francis.bach@inria.fr			European Union's H2020 Framework Programme under grant MacSeNet [H2020-MSCA-ITN-2014]	European Union's H2020 Framework Programme under grant MacSeNet	We acknowledge support the European Union's H2020 Framework Programme (H2020-MSCA-ITN-2014) under grant agreement no642685 MacSeNet, and thank Sesh Kumar, Anastasia Podosinnikova and Anton Osokin for interesting discussions related to this work.	[Anonymous], MAX MARGIN MARKOV NE; Bach F., 2015, 151100394 ARXIV; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Borenstein E., 2004, P ECCV; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Djolonga J., 2015, P ICML; Djolonga J., 2014, ADV NIPS; Fujishige S., 2005, ANN DISCRETE MATH; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Hazan T., 2012, P ICML; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Kotz S., 2005, INT J MATH MATH SCI, DOI [10.1155/IJMMS.2005.3169, DOI 10.1155/IJMMS.2005.3169]; Krause A, 2014, TRACTABILITY, P71; Lafferty J., 2001, DEP PAPERS CIS; Lin H., 2011, P NAACL HLT; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Papandreou G., 2011, P ICCV; Szummer M., 2008, P ECCV; Tarlow D., 2012, P AISTATS; Tschiatschek S., 2016, P AISTATS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zhang J, 2015, P ICCV	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703038
C	Shpitser, I		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shpitser, Ilya			Consistent Estimation of Functions of Data Missing Non-Monotonically and Not at Random	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CAUSAL INFERENCE	Missing records are a perennial problem in analysis of complex data of all types, when the target of inference is some function of the full data law. In simple cases, where data is missing at random or completely at random [15], well-known adjustments exist that result in consistent estimators of target quantities. Assumptions underlying these estimators are generally not realistic in practical missing data problems. Unfortunately, consistent estimators in more complex cases where data is missing not at random, and where no ordering on variables induces monotonicity of missingness status are not known in general, with some notable exceptions [13, 18, 16]. In this paper, we propose a general class of consistent estimators for cases where data is missing not at random, and missingness status is non-monotonic. Our estimators, which are generalized inverse probability weighting estimators, make no assumptions on the underlying full data law, but instead place independence restrictions, and certain other fairly mild assumptions, on the distribution of missingness status conditional on the data. The assumptions we place on the distribution of missingness status conditional on the data can be viewed as a version of a conditional Markov random field (MRF) corresponding to a chain graph. Assumptions embedded in our model permit identification from the observed data law, and admit a natural fitting procedure based on the pseudo likelihood approach of [2]. We illustrate our approach with a simple simulation study, and an analysis of risk of premature birth in women in Botswana exposed to highly active anti-retroviral therapy.	[Shpitser, Ilya] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Johns Hopkins University	Shpitser, I (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.	ilyas@cs.jhu.edu						Bang H, 2005, BIOMETRICS, V61, P962, DOI 10.1111/j.1541-0420.2005.00377.x; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; DAWID AP, 1979, J ROY STAT SOC B MET, V41, P1; Eric J., DISCRETE CHOICE MODE; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Mohan K., 2014, ADV NEURAL INFORM PR, P1520; Mohan K, 2013, ADV NEUTRAL INFORM P, V26, P1277; Mozeika A, 2014, PHYS REV E, V90, DOI 10.1103/PhysRevE.90.010101; Newey W.K., 1994, HDB ECONOMETRICS, V4, P2111, DOI DOI 10.1016/S1573-4412(05)80005-4; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; ROBINS J, 1986, MATH MODELLING, V7, P1393, DOI 10.1016/0270-0255(86)90088-6; Robins JM, 1997, STAT MED, V16, P21; RUBIN DB, 1976, BIOMETRIKA, V63, P581, DOI 10.2307/2335739; Sadinle Mauricio, 2016, WORKING PAPER; Shpitser I, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P802; Van der Laan M., 2003, SPR S STAT	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702044
C	Silva, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Silva, Ricardo			Observational-Interventional Priors for Dose-Response Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Controlled interventions provide the most direct source of information for learning causal effects. In particular, a dose-response curve can be learned by varying the treatment level and observing the corresponding outcomes. However, interventions can be expensive and time-consuming. Observational data, where the treatment is not controlled by a known mechanism, is sometimes available. Under some strong assumptions, observational data allows for the estimation of dose-response curves. Estimating such curves nonparametrically is hard: sample sizes for controlled interventions may be small, while in the observational case a large number of measured confounders may need to be marginalized. In this paper, we introduce a hierarchical Gaussian process prior that constructs a distribution over the dose-response curve by learning from observational data, and reshapes the distribution with a nonparametric affine transform learned from controlled interventions. This function composition from different sources is shown to speed-up learning, which we demonstrate with a thorough sensitivity analysis and an application to modeling the effect of therapy on cognitive skills of premature infants.	[Silva, Ricardo] UCL, Dept Stat Sci, London, England; [Silva, Ricardo] UCL, Ctr Computat Stat & Machine Learning, London, England	University of London; University College London; University of London; University College London	Silva, R (corresponding author), UCL, Dept Stat Sci, London, England.; Silva, R (corresponding author), UCL, Ctr Computat Stat & Machine Learning, London, England.	ricardo@stats.ucl.ac.uk						Bareinboim E., 2016, P NATL ACAD SCI; Bayarri MJ, 2007, TECHNOMETRICS, V49, P138, DOI 10.1198/004017007000000092; BROOKSGUNN J, 1992, J PEDIATR-US, V120, P350, DOI 10.1016/S0022-3476(05)80896-0; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Ernest J, 2015, ELECTRON J STAT, V9, P3155, DOI 10.1214/15-EJS1075; Gramacy RB, 2008, J AM STAT ASSOC, V103, P1119, DOI 10.1198/016214508000000689; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Liu F, 2009, BAYESIAN ANAL, V4, P119, DOI 10.1214/09-BA404; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; MacKay DJC, 1994, ASHRAE T, V100, P1053; McCandless LC, 2007, STAT MED, V26, P2331, DOI 10.1002/sim.2711; Morgan SL, 2015, ANAL METHOD SOC RES, P1; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Robins J.M., 2007, STAT SCI, V22, P544, DOI [https://doi.org/10.1214/07-STS227D, DOI 10.1214/07-STS227D]; Spirtes P., 2000, CAUSATION PREDICTION	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700071
C	Simon-Gabriel, CJ; Scibior, A; Tolstikhin, I; Scholkopf, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Simon-Gabriel, Carl-Johann; Scibior, Adam; Tolstikhin, Ilya; Schoelkopf, Bernhard			Consistent Kernel Mean Estimation for Functions of Random Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We provide a theoretical foundation for non-parametric estimation of functions of random variables using kernel mean embeddings. We show that for any continuous function f, consistent estimators of the mean embedding of a random variable X lead to consistent estimators of the mean embedding of f(X). For Matern kernels and sufficiently smooth functions we also provide rates of convergence. Our results extend to functions of multiple random variables. If the variables are dependent, we require an estimator of the mean embedding of their joint distribution as a starting point; if they are independent, it is sufficient to have separate estimators of the mean embeddings of their marginal distributions. In either case, our results cover both mean embeddings based on i.i.d. samples as well as "reduced set" expansions in terms of dependent expansion points. The latter serves as a justification for using such expansions to limit memory resources when applying the approach as a basis for probabilistic programming.	[Simon-Gabriel, Carl-Johann; Scibior, Adam; Tolstikhin, Ilya; Schoelkopf, Bernhard] Max Planck Inst Intelligent Syst, Dept Empir Inference, Spemanstr 38, D-72076 Tubingen, Germany; [Scibior, Adam] Univ Cambridge, Engn Dept, Cambridge, England	Max Planck Society; University of Cambridge	Simon-Gabriel, CJ (corresponding author), Max Planck Inst Intelligent Syst, Dept Empir Inference, Spemanstr 38, D-72076 Tubingen, Germany.	cjsimon@tuebingen.mpg.de; adam.scibior@tuebingen.mpg.de; ilya@tuebingen.mpg.de; bs@tuebingen.mpg.de			Google European Fellowship in Causal Inference	Google European Fellowship in Causal Inference(Google Incorporated)	We thank Krikamol Muandet for providing the code used to generate Figure 1, Paul Rubenstein, Motonobu Kanagawa and Bharath Sriperumbudur for very useful discussions, and our anonymous reviewers for their valuable feedback. Carl-Johann Simon-Gabriel is supported by a Google European Fellowship in Causal Inference.	Adams R., 2003, SOBOLEV SPACES, V2nd; [Anonymous], 2004, INTRO HARMONIC ANAL, DOI DOI 10.1017/CBO9781139165372; Bennett C., 1988, INTERPOLATION OPERAT; Berlinet A., 2004, RKHS PROBABILITY STA; Chen Yutian, 2010, UAI; Fukumizu K, 2013, J MACH LEARN RES, V14, P3753; Jeffrey A., 2007, TABLE INTEGRALS SERI; Kalos M.H., 2008, MONTE CARLO METHODS, VSecond; Kanagawa M., 2016, ARXIV160507254STAT; Korzen M, 2014, J STAT SOFTW, V57, P1; Lacoste-Julien S, 2015, JMLR WORKSH CONF PRO, V38, P544; Mathai A., 1973, INDIAN J STAT A, P39; McKinley KS, 2016, ACM SIGPLAN NOTICES, V51, P1, DOI 10.1145/2837614.2843895; Milios D., 2009, THESIS; Poisson S.D., 1837, RECHERCHES PROBABILI; Scholkopf B, 2015, STAT COMPUT, V25, P755, DOI 10.1007/s11222-015-9558-5; Scholkopf B., 2001, LEARNING KERNELS SUP; Scovel C., 2014, J COMPLEXITY, V26; Simon-Gabriel C.-J., 2016, TECHNICAL REPORT; Smola A., 2007, ALT; Song L, 2009, INT WORKSH INT SYST, P1, DOI DOI 10.1145/1553374; Springer M.D., 1979, ALGEBRA RANDOM VARIA; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I, 2012, CONSTR APPROX, V35, P363, DOI 10.1007/s00365-012-9153-3; Tolstikhin I., 2016, ARXIV160204361MATHST; Wendland H, 2004, SCATTERED DATA APPRO, V17; Williamson R. C., 1989, THESIS	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704069
C	Singh, S; Hoiem, D; Forsyth, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Singh, Saurabh; Hoiem, Derek; Forsyth, David			Swapout: Learning an ensemble of deep architectures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We describe Swapout, a new stochastic training method, that outperforms ResNets of identical network structure yielding impressive results on CIFAR-10 and CIFAR-100. Swapout samples from a rich set of architectures including dropout [20], stochastic depth [7] and residual architectures [5, 6] as special cases. When viewed as a regularization method swapout not only inhibits co-adaptation of units in a layer, similar to dropout, but also across network layers. We conjecture that swapout achieves strong regularization by implicitly tying the parameters across layers. When viewed as an ensemble training method, it samples a much richer set of architectures than existing methods such as dropout or stochastic depth. We propose a parameterization that reveals connections to exiting architectures and suggests a much richer set of architectures to be explored. We show that our formulation suggests an efficient training method and validate our conclusions on CIFAR-10 and CIFAR-100 matching state of the art accuracy. Remarkably, our 32 layer wider model performs similar to a 1001 layer ResNet model.	[Singh, Saurabh; Hoiem, Derek; Forsyth, David] Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Singh, S (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	ss1@illinois.edu; dhoiem@illinois.edu; daf@illinois.edu			ONR MURI [N00014-10-1-0934, N00014-16-1-2007]	ONR MURI(MURIOffice of Naval Research)	This work is supported in part by ONR MURI Awards N00014-10-1-0934 and N00014-16-1-2007. We would like to thank NVIDIA for donating some of the GPUs used in this work.	[Anonymous], 2013, ARXIV201313013557; Bengio Y., 2011, P 22 INT C ALG LEARN; Gal Y., 2015, BAYESIAN CONVOLUTION; Glorot X., 2011, AISTATS; Greff K., 2015, NIPS; Hardt M., 2015, ABS150901240 CORR; He K., 2015, ABS151203385 CORR; He K, 2016, ABS160305027 CORR; Huang G., 2016, ABS160309382 CORR; Ioffe S., 2015, ICML, P448; Krizhevsky A., 2012, ADV NEURAL INF PROCE; LeCun Y., 1998, P IEEE; Lee C., 2015, AISTATS; Lin M., 2013, ABS13124400 CORR; Maharaj T., 2016, ARXIV160601305; Nair V., 2010, ICML; Pinheiro P. H., 2013, ARXIV13062795; Rahimi A., 2007, NIPS; Romero A., 2015, ICLR; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Srivastava N., 2014, J MACHINE LEARNING R; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wan L., 2013, P INT C MACHINE LEAR, P1058	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701062
C	Singh, S; Poczos, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Singh, Shashank; Poczos, Barnabas			Finite-Sample Analysis of Fixed-k Nearest Neighbor Density Functional Estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ENTROPY	We provide finite-sample analysis of a general framework for using k-nearest neighbor statistics to estimate functionals of a nonparametric continuous probability density, including entropies and divergences. Rather than plugging a consistent density estimate (which requires k -> infinity as the sample size n -> infinity) into the functional of interest, the estimators we consider fix k and perform a bias correction. This is more efficient computationally, and, as we show in certain cases, statistically, leading to faster convergence rates. Our framework unifies several previous estimators, for most of which ours are the first finite sample guarantees.	[Singh, Shashank] Carnegie Mellon Univ, Stat Dept, Pittsburgh, PA 15213 USA; [Singh, Shashank; Poczos, Barnabas] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Singh, S (corresponding author), Carnegie Mellon Univ, Stat Dept, Pittsburgh, PA 15213 USA.; Singh, S (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	sss1@andrew.cmu.edu; bapoczos@cs.cmu.edu	Singh, Shashank/V-8230-2019	Singh, Shashank/0000-0002-7305-673X	National Science Foundation Graduate Research Fellowship [DGE-1252522]	National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	This material is based upon work supported by a National Science Foundation Graduate Research Fellowship to the first author under Grant No. DGE-1252522.	Adami C, 2004, PHYS LIFE REV, V1, P3, DOI 10.1016/j.plrev.2004.01.002; Aghagolzadeh M., 2007, P IEEE INT C IM PROC; ALEMANY PA, 1994, PHYS REV E, V49, pR956, DOI 10.1103/PhysRevE.49.R956; BERRETT TB, 2016, ARXIV160600304; Biau Gerard, 2015, LECT NEAREST NEIGHBO, P75; Chai B., 2009, NIPS; Chaudhuri K, 2014, IEEE T INFORM THEORY, V60, P7900, DOI 10.1109/TIT.2014.2361055; Chaudhuri Kamalika, 2014, ADV NEURAL INFORM PR, P3437; EFRON B, 1981, ANN STAT, V9, P586, DOI 10.1214/aos/1176345462; Evans D, 2008, P ROY SOC A-MATH PHY, V464, P3175, DOI 10.1098/rspa.2008.0235; Gao W., 2016, ARXIV160403006; Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815; Hero A. O., CSPL328; Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355; Hlavackova-Schindler K, 2007, PHYS REP, V441, P1, DOI 10.1016/j.physrep.2006.12.004; Kontorovich A, 2015, JMLR WORKSH CONF PRO, V38, P480; Kozachenko L.F., 1987, PROBL PEREDACHI INF, V23, P9; KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225; Krishnamurthy A., 2014, INT C MACH LEARN ICM; Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539; Lewi J., 2007, ADV NEURAL INFORM PR, V19; LIU H, 2012, NEURAL INFORM PROCES; LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079; Mack YP, 1979, J MULTIVAR ANAL; Miller E G L, 2003, J MACHINE LEARNING R, V4, P1271; Moon Kevin, 2014, ADV NEURAL INFORM PR, P2420; Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854; Nguyen X., 2010, IEEE T INFORM THEORY; Oliva J., 2013, INT C MACH LEARN ICM; Pal D., 2010, P NEURAL INFORM PROC; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Perez- Cruz F., 2008, ADV NEURAL INFORM PR, V21; Poczos B., 2012, 25 IEEE C COMP VIS P; Poczos B., 2011, P 14 INT C ARTIFICIA, P609; Poczos B., 2005, ICML; Poczos B, 2009, J MACH LEARN RES, V10, P515; Shan Caifeng, 2005, BRIT MACH VIS C BMVC; Singh S., 2014, INT C MACH LEARN ICM; Singh S., 2014, NEURAL INFORM PROCES; Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549; Sricharan K, 2011, IEEE INT SYMP INFO, P1205, DOI 10.1109/ISIT.2011.6033726; Szabo Z, 2007, J MACH LEARN RES, V8, P1063; Szabo Z, 2014, J MACH LEARN RES, V15, P283; Tsybakov AB, 1996, SCAND J STAT, V23, P75; Van Hulle MM, 2008, NEURAL COMPUT, V20, P964, DOI 10.1162/neco.2008.10-06-383; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060; Wolsztynski E, 2005, SIGNAL PROCESS, V85, P937, DOI 10.1016/j.sigpro.2004.11.028	50	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700087
C	Singh, S; Du, SS; Poczos, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Singh, Shashank; Du, Simon S.; Poczos, Barnabas			Efficient Nonparametric Smoothness Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INTEGRAL FUNCTIONALS	Sobolev quantities (norms, inner products, and distances) of probability density functions are important in the theory of nonparametric statistics, but have rarely been used in practice, due to a lack of practical estimators. They also include, as special cases, L-2 quantities which are used in many applications. We propose and analyze a family of estimators for Sobolev quantities of unknown probability density functions. We bound the finite-sample bias and variance of our estimators, finding that they are generally minimax rate-optimal. Our estimators are significantly more computationally tractable than previous estimators, and exhibit a statistical/computational trade-off allowing them to adapt to computational constraints. We also draw theoretical connections to recent work on fast two-sample testing and empirically validate our estimators on synthetic data.	[Singh, Shashank; Du, Simon S.; Poczos, Barnabas] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Singh, S (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	sss1@andrew.cmu.edu; ssdu@cs.cmu.edu; bapoczos@cs.cmu.edu	Singh, Shashank/V-8230-2019	Singh, Shashank/0000-0002-7305-673X	National Science Foundation Graduate Research Fellowship [DGE-1252522]	National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	This material is based upon work supported by a National Science Foundation Graduate Research Fellowship to the first author under Grant No. DGE-1252522.	ANDERSON NH, 1994, J MULTIVARIATE ANAL, V50, P41, DOI 10.1006/jmva.1994.1033; [Anonymous], 2002, TRIGONOMETRIC SERIES; BICKEL PJ, 1988, SANKHYA SER A, V50, P381; Chwialkowski K.P., 2015, P 28 INT C NEUR INF, P1981; Epps TW., 1986, J STAT COMPUT SIM, V26, P177, DOI [10.1080/00949658608810963, DOI 10.1080/00949658608810963]; Ginc E, 2008, BERNOULLI, V14, P47, DOI 10.3150/07-BEJ110; Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815; Gretton A., 2006, NIPS, P513; Gretton A, 2012, J MACH LEARN RES, V13, P723; HALL P, 1987, STAT PROBABIL LETT, V6, P109, DOI 10.1016/0167-7152(87)90083-6; Hasminskii RafaelZ., 1978, P 2 PRAGUE S ASYMPTO, P41; HEATHCOTE CR, 1972, AUST J STAT, V14, P172, DOI 10.1111/j.1467-842X.1972.tb00355.x; Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355; KREISS HO, 1979, SIAM J NUMER ANAL, V16, P421, DOI 10.1137/0716035; Krishnamurthy A., 2015, AISTATS; Krishnamurthy A, 2014, PR MACH LEARN RES, V32, P919; Laurent B, 1996, ANN STAT, V24, P659; Laurent B., 1992, EFFICIENT ESTIMATION; Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539; Leoni G., 2009, 1 COURSE SOBOLEV SPA, V105; Moon K. R., 2016, ARXIV160106884; Moon Kevin, 2014, ADV NEURAL INFORM PR, P2420; Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854; Pardo L, 2005, STAT INFERENCE BASED; Poczos B., 2011, P 14 INT C ARTIFICIA, P609; Poczos B, 2012, ARXIV12023758; Poczos B, 2012, PROC CVPR IEEE, P2989, DOI 10.1109/CVPR.2012.6248028; Principe JC, 2010, INFORM SCI STAT, P1, DOI 10.1007/978-1-4419-1570-2; Quadrianto N., 2009, ADV NEURAL INFORM PR, P1500; Ram P., 2009, ADV NEURAL INFORM PR, P1527; Schweder T., 1975, Scandinavian Journal of Statistics Theory and Applications, V2, P113; Singh S, 2014, PR MACH LEARN RES, V32; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Wolsztynski E, 2005, SIGNAL PROCESS, V85, P937, DOI 10.1016/j.sigpro.2004.11.028; ZAREMBA W, 2013, ADV NEURAL INFORM PR, V26, P755; Zhao J, 2015, NEURAL COMPUT, V27, P1345, DOI 10.1162/NECO_a_00732	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703005
C	Sinha, A; Duchi, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sinha, Aman; Duchi, John			Learning Kernels with Random Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GENERALIZATION ERROR	Randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks. However, such methods require a user-defined kernel as input. We extend the randomized-feature approach to the task of learning a kernel (via its associated random features). Specifically, we present an efficient optimization problem that learns a kernel in a supervised manner. We prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets. Our approach is efficient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques.	[Sinha, Aman; Duchi, John] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Duchi, John] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University; Stanford University	Sinha, A (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	amans@stanford.edu; jduchi@stanford.edu			Fannie & John Hertz Foundation Fellowship; Stanford Graduate Fellowship	Fannie & John Hertz Foundation Fellowship; Stanford Graduate Fellowship(Stanford University)	This research was supported by a Fannie & John Hertz Foundation Fellowship and a Stanford Graduate Fellowship.	Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Ben-Hur A, 2005, BIOINFORMATICS, V21, pI38, DOI 10.1093/bioinformatics/bti1016; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Cho M., 2013, P NATL ACAD SCI USA, V110; Cho Y., 2009, NIPS, P342; Cortes C., 2010, ICML 2010 P 27 INT C; Cortes C, 2012, J MACH LEARN RES, V13, P795; Cristianini N, 2006, STUD FUZZ SOFT COMP, V194, P205; Duchi J., 2008, P 25 INT C MACH LEAR; Duvenaud D., 2013, INT C MACH LEARN PML; Girolami M., 2005, P 22 INT C MACH LEAR, P241, DOI DOI 10.1145/1102351.1102382; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Hinton G.E., 2008, ADV NEURAL INFORM PR, P1249; KANDOLA J, 2002, OPTIMIZING KERNEL AL; Kloft M, 2011, J MACH LEARN RES, V12, P953; Koltchinskii V, 2002, ANN STAT, V30, P1; Koltchinskii V, 2005, ANN STAT, V33, P1455, DOI 10.1214/009053605000000228; Le Quoc V., 2013, INT C MACH LEARN, P244; Luenberger D. G., 1969, OPTIMIZATION VECTOR; Qiu SB, 2009, IEEE ACM T COMPUT BI, V6, P190, DOI 10.1109/TCBB.2008.139; Rahimi A, 2007, ADV NEURAL INFORM PR, V20; Rahimi A., 2008, ADV NEURAL INFORM PR, V21; Samson PM, 2000, ANN PROBAB, V28, P416; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Ying Y., 2009, BMC BIOINFORMATICS, V10, P1; Zien A., 2007, P 24 INT C MACH LEAR, V1, P1191, DOI DOI 10.1145/1273496.1273646	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701037
C	Sinha, A; Gleich, DF; Ramani, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sinha, Ayan; Gleich, David F.; Ramani, Karthik			Deconvolving Feedback Loops in Recommender Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODEL	Collaborative filtering is a popular technique to infer users' preferences on new content based on the collective information of all users preferences. Recommender systems then use this information to make personalized suggestions to users. When users accept these recommendations it creates a feedback loop in the recommender system, and these loops iteratively influence the collaborative filtering algorithm's predictions over time. We investigate whether it is possible to identify items affected by these feedback loops. We state sufficient assumptions to deconvolve the feedback loops while keeping the inverse solution tractable. We furthermore develop a metric to unravel the recommender system's influence on the entire user-item rating matrix. We use this metric on synthetic and real-world datasets to (1) identify the extent to which the recommender system affects the final rating matrix, (2) rank frequently recommended items, and (3) distinguish whether a user's rated item was recommended or an intrinsic preference. Our results indicate that it is possible to recover the ratings matrix of intrinsic user preferences using a single snapshot of the ratings matrix without any temporal information.	[Sinha, Ayan; Gleich, David F.; Ramani, Karthik] Purdue Univ, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Sinha, A (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	sinhayan@mit.edu; dgleich@purdue.edu; ramani@purdue.edu			NSF [CAREER CCF-1149756, IIS-1422918, IIS-1546488]; Center for Science of Information STC [CCF-093937]; DARPA SIMPLEX	NSF(National Science Foundation (NSF)); Center for Science of Information STC; DARPA SIMPLEX	David Gleich would like to acknowledge the support of the NSF via awards CAREER CCF-1149756, IIS-1422918, IIS-1546488, and the Center for Science of Information STC, CCF-093937, as well as the support of DARPA SIMPLEX.	Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99; Amatriain Xavier, 2009, P 3 ACM C RECOMMENDE, P173, DOI [10.1145/1639714.1639744, DOI 10.1145/1639714.1639744]; Barzel B, 2013, NAT BIOTECHNOL, V31, P720, DOI 10.1038/nbt.2601; Bennett J., 2007, P KDD CUP WORKSH, P35, DOI DOI 10.1145/1345448.1345459; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Chen L, 2015, USER MODEL USER-ADAP, V25, P99, DOI 10.1007/s11257-015-9155-5; Cosley D., 2003, P SIGCHI C HUMAN FAC, P585, DOI [10.1145/642611.642713, DOI 10.1145/642611.642713]; Deshpande M, 2004, ACM T INFORM SYST, V22, P143, DOI 10.1145/963770.963776; Edelman B, 2007, AM ECON REV, V97, P242, DOI 10.1257/aer.97.1.242; Feizi S, 2013, NAT BIOTECHNOL, V31, P726, DOI 10.1038/nbt.2635; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Gleich David F, 2011, P ACM SIGKDD C KNOWL, P60; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Jawaheer G., 2010, P 1 INT WORKSH INF H, P47, DOI DOI 10.1145/1869446.1869453; Lathia N, 2010, SIGIR 2010: PROCEEDINGS OF THE 33RD ANNUAL INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH DEVELOPMENT IN INFORMATION RETRIEVAL, P210; Li Lihong, 2010, P 19 INT C WORLD WID, P661, DOI DOI 10.1145/1772690.1772758; Li W, 2010, P 16 ACM SIGKDD INT; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; March JG, 1991, ORGAN SCI, V2, P71, DOI 10.1287/orsc.2.1.71; McAuley Julian John, 2013, P 22 INT C WORLD WID, P897, DOI DOI 10.1145/2488388.2488466; Poston RS, 2005, MIS QUART, V29, P221; Ricci F, 2011, RECOMMENDER SYSTEMS HANDBOOK, P1, DOI 10.1007/978-0-387-85820-3_1; Salganik MJ, 2006, SCIENCE, V311, P854, DOI 10.1126/science.1121066; Sarwar Badrul, 2001, P 10 INT C WORLD WID, P285, DOI 10.1145/371920.372071	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702030
C	Sokolov, A; Kreutzer, J; Lo, C; Riezler, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sokolov, Artem; Kreutzer, Julia; Lo, Christopher; Riezler, Stefan			Stochastic Structured Prediction under Bandit Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Stochastic structured prediction under bandit feedback follows a learning protocol where on each of a sequence of iterations, the learner receives an input, predicts an output structure, and receives partial feedback in form of a task loss evaluation of the predicted structure. We present applications of this learning scenario to convex and non-convex objectives for structured prediction and analyze them as stochastic first-order methods. We present an experimental evaluation on problems of natural language processing over exponential output spaces, and compare convergence speed across different objectives under the practical criterion of optimal task performance on development data and the optimization-theoretic criterion of minimal squared gradient norm. Best results under both criteria are obtained for a non-convex objective for pairwise preference learning under bandit feedback.	[Sokolov, Artem; Kreutzer, Julia; Lo, Christopher; Riezler, Stefan] Heidelberg Univ, Computat Linguist, Heidelberg, Germany; [Riezler, Stefan] Heidelberg Univ, IWR, Heidelberg, Germany; [Lo, Christopher] Tufts Univ, Dept Math, Boston, MA 02111 USA; [Sokolov, Artem] Amazon Dev Ctr, Berlin, Germany	Ruprecht Karls University Heidelberg; Ruprecht Karls University Heidelberg; Tufts University; Amazon.com	Sokolov, A (corresponding author), Heidelberg Univ, Computat Linguist, Heidelberg, Germany.; Sokolov, A (corresponding author), Amazon Dev Ctr, Berlin, Germany.	sokolov@cl.uni-heidelberg.de; kreutzer@cl.uni-heidelberg.de; chris.aa.lo@gmail.com; riezler@cl.uni-heidelberg.de			German research foundation (DFG); Amazon Development Center Germany	German research foundation (DFG)(German Research Foundation (DFG)); Amazon Development Center Germany	This research was supported in part by the German research foundation (DFG), and in part by a research cooperation grant with the Amazon Development Center Germany.	Agarwal A., 2010, COLT; Busa- Fekete H, 2014, ALT; Chang Kai-Wei, 2015, ICML; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Dyer C., 2010, ACL DEMO; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; Ghadimi S., 2012, SIAM J OPTIMIZ, V4, P2342; Herbrich R, 2000, ADV NEUR IN, P115; Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456; Joachims T., 2002, KDD; Langford J., 2007, ADV NEURAL INFORM PR, V20; Lazaric A, 2012, J COMPUT SYST SCI, V78, P1516, DOI 10.1016/j.jcss.2011.12.027; Li L., 2010, WWW; Polyak B. T., 1987, INTRO OPTIMIZATION; Ranzato M., 2016, ICLR; Sha Fei, 2003, NAACL; Simianer Patrick, 2012, ACL; Smith Noah A., 2011, LINGUISTIC STRUCTURE; Sokolov A., 2016, ACL; Sokolov A., 2015, MT SUMMIT 15; Solodov MV, 1998, COMPUT OPTIM APPL, V11, P23, DOI 10.1023/A:1018366000512; Sutton R. S., 2000, NIPS; Thurstone LL, 1927, PSYCHOL REV, V34, P273, DOI 10.1037/h0070288; Yue Y., 2009, ICML; Yuille Alan, 2012, Frontiers of Electrical and Electronic Engineering in China, V7, P94, DOI 10.1007/s11460-012-0170-6	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700098
C	Song, Y; Zhu, J; Ren, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Song, Yang; Zhu, Jun; Ren, Yong			Kernel Bayesian Inference with Posterior Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a vector-valued regression problem whose solution is equivalent to the reproducing kernel Hilbert space (RKHS) embedding of the Bayesian posterior distribution. This equivalence provides a new understanding of kernel Bayesian inference. Moreover, the optimization problem induces a new regularization for the posterior embedding estimator, which is faster and has comparable performance to the squared regularization in kernel Bayes' rule. This regularization coincides with a former thresholding approach used in kernel POMDPs whose consistency remains to be established. Our theoretical work solves this open problem and provides consistency analysis in regression settings. Based on our optimizational formulation, we propose a flexible Bayesian posterior regularization framework which for the first time enables us to put regularization at the distribution level. We apply this method to nonparametric state-space filtering tasks with extremely nonlinear dynamics and show performance gains over all other baselines.	[Song, Yang] Tsinghua Univ, Dept Phys, Beijing, Peoples R China; [Zhu, Jun; Ren, Yong] Tsinghua Univ, TNList Lab, Dept Comp Sci & Tech, Beijing, Peoples R China; [Zhu, Jun; Ren, Yong] Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Syst, Beijing, Peoples R China	Tsinghua University; Tsinghua University; Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, TNList Lab, Dept Comp Sci & Tech, Beijing, Peoples R China.; Zhu, J (corresponding author), Tsinghua Univ, Ctr Bioinspired Comp Res, State Key Lab Intell Tech & Syst, Beijing, Peoples R China.	yangsong@cs.stanford.edu; dcszj@.tsinghua.edu.cn; renyong15@mails.tsinghua.edu.cn	Song, Yang/X-5282-2019		National Basic Research Program (973 Program) of China [2013CB329403]; National NSF of China [61620106010, 61322308, 61332007]; Youth Top-notch Talent Support Program; Tsinghua Initiative Scientific Research Program [20141080934]	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National NSF of China(National Natural Science Foundation of China (NSFC)); Youth Top-notch Talent Support Program; Tsinghua Initiative Scientific Research Program	We thank all the anonymous reviewers for valuable suggestions. The work was supported by the National Basic Research Program (973 Program) of China (No. 2013CB329403), National NSF of China Projects (Nos. 61620106010, 61322308, 61332007), the Youth Top-notch Talent Support Program, and Tsinghua Initiative Scientific Research Program (No. 20141080934).	ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Berlinet A., 2011, REPRODUCING KERNEL H; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; De Vito Ernesto, 2005, TECHNICAL REPORT; Engl HW, 1996, REGULARIZATION INVER, V375; Fukumizu K., 2011, ADV NEURAL INFORM PR, P1737; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Grunewalder S., 2012, P 29 INT C MACH LEAR, P1823; Grunewalder Steffen, 2012, ARXIV12064655; Julier SJ, 1997, P SOC PHOTO-OPT INS, V3068, P182, DOI 10.1117/12.280797; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Micchelli CA, 2005, NEURAL COMPUT, V17, P177, DOI 10.1162/0899766052530802; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Nishiyama Y., 2012, ARXIV12104887; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Smola Alex J, 1998, LEARNING KERNELS, V4; Song L., 2010, HILBERT SPACE EMBEDD; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Song Le, 2010, AISTATS JMLR W CP, P765; Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961; Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463; WILLIAMS PM, 1980, BRIT J PHILOS SCI, V31, P131, DOI 10.1093/bjps/31.2.131; Zhu J, 2014, J MACH LEARN RES, V15, P1799; Zhu J, 2012, J MACH LEARN RES, V13, P2237	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701092
C	Song, Z; Parr, R; Liao, XJ; Carin, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Song, Zhao; Parr, Ronald; Liao, Xuejun; Carin, Lawrence			Linear Feature Encoding for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Feature construction is of vital importance in reinforcement learning, as the quality of a value function or policy is largely determined by the corresponding features. The recent successes of deep reinforcement learning (RL) only increase the importance of understanding feature construction. Typical deep RL approaches use a linear output layer, which means that deep RL can be interpreted as a feature construction/encoding network followed by linear value function approximation. This paper develops and evaluates a theory of linear feature encoding. We extend theoretical results on feature quality for linear value function approximation from the uncontrolled case to the controlled case. We then develop a supervised linear feature encoding method that is motivated by insights from linear value function approximation theory, as well as empirical successes from deep RL. The resulting encoder is a surprisingly effective method for linear value function approximation using raw images as inputs.	[Song, Zhao; Liao, Xuejun; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA; [Parr, Ronald] Duke Univ, Dept Comp Sci, Durham, NC 27708 USA	Duke University; Duke University	Song, Z (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.		Song, Zhao/AAW-4042-2020		ARO; DARPA; DOE; NGA; ONR; NSF	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	We thank the anonymous reviewers for their helpful comments and suggestions. This research was supported in part by ARO, DARPA, DOE, NGA, ONR and NSF.	[Anonymous], 1996, JRSSB; Boyd S, 2004, CONVEX OPTIMIZATION; Bradtke S., 1996, MACHINE LEARNING; Farahmand A. M., 2012, NIPS; Geramifard A., 2013, UAI; Ghavamzadeh M., 2010, NIPS; Hansen P. C., 1987, BIT NUMERICAL MATH; Johns J., 2010, NIPS; Kolter J. Z., 2009, ICML; Lagoudakis M.G, 2003, JMLR; Liang Y., 2016, AAMAS; Mahadevan S., 2007, JMLR; Mallat S. G., 1993, IEEE TSP; Mnih V, 2015, NATURE, V518, P529; Oh J., 2015, NIPS; Painter-Wakefield C., 2012, ICML; Parr R., 2007, ICML; Parr R., 2008, ICML; Petrik M., 2010, ICML; Schoknecht R., 2002, NIPS; Sutton R. S., 2008, UAI; Sutton R. S., 1988, MACHINE LEARNING; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tesauro G., 1994, NEURAL COMPUTATION; Williams R. J., 1993, TECH REP; Yu H., 2009, IEEE TAC	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702052
C	Song, Z; Woodruff, DP; Zhang, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Song, Zhao; Woodruff, David P.; Zhang, Huan			Sublinear Time Orthogonal Tensor Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A recent work (Wang et. al., NIPS 2015) gives the fastest known algorithms for orthogonal tensor decomposition with provable guarantees. Their algorithm is based on computing sketches of the input tensor, which requires reading the entire input. We show in a number of cases one can achieve the same theoretical guarantees in sublinear time, i.e., even without reading most of the input tensor. Instead of using sketches to estimate inner products in tensor decomposition algorithms, we use importance sampling. To achieve sublinear time, we need to know the norms of tensor slices, and we show how to do this in a number of important cases. For symmetric tensors T = Sigma(k)(i=1) lambda(i)u(i)(circle times p) with lambda(i) > 0 for all i, we estimate such norms in sublinear time whenever p is even. For the important case of p = 3 and small values of k, we can also estimate such norms. For asymmetric tensors sublinear time is not possible in general, but we show if the tensor slice norms are just slightly below parallel to T parallel to(F) then sublinear time is again possible. One of the main strengths of our work is empirical - in a number of cases our algorithm is orders of magnitude faster than existing methods with the same accuracy.	[Song, Zhao] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA; [Woodruff, David P.] IBM Almaden Res Ctr, San Jose, CA USA; [Zhang, Huan] Univ Calif Davis, Dept Elect & Comp Engn, Davis, CA 95616 USA	University of Texas System; University of Texas Austin; International Business Machines (IBM); University of California System; University of California Davis	Song, Z (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	zhaos@utexas.edu; dpwoodru@us.ibm.com; ecezhang@ucdavis.edu			XDATA DARPA Air Force Research Laboratory [FA8750-12-C-0323]	XDATA DARPA Air Force Research Laboratory	Supported by XDATA DARPA Air Force Research Laboratory contract FA8750-12-C-0323.	Anandkumar Anima, 2012, NIPS; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; BENTLEY JL, 1980, ACM T MATH SOFTWARE, V6, P359, DOI 10.1145/355900.355907; Bhojanapalli S., 2015, CORR; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bringmann K, 2012, LECT NOTES COMPUT SC, V7391, P133, DOI 10.1007/978-3-642-31594-7_12; Chi Wang, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P290, DOI 10.1007/978-3-662-44845-8_19; Choi JH, 2014, ADV NEUR IN, V27; Clarkson KL, 2012, J ACM, V59, DOI 10.1145/2371656.2371658; Harshman R. A., 1970, FDN PARAFAC PROCEDUR, V16, P84; Huang Furong, 2013, CORR; Kang U, 2012, P 18 ACM SIGKDD INT, P316, DOI 10.1145/2339530.2339583; Knuth D. E., 1969, ART COMPUTER PROGRAM, V2, P229; Ministry of Foreign Affairs of the People's Republic of China, 2016, COMMUNICATION; Moitra A., 2014, TENSOR DECOMPOSITION; MONEMIZADEH M, 2010, SODA, V135, P1143; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Phan AH, 2013, IEEE T SIGNAL PROCES, V61, P4834, DOI 10.1109/TSP.2013.2269903; Tsourakakis C. E., 2010, P 2010 SIAM INT C DA, P689, DOI DOI 10.1137/1.9781611972801.60; Tung H.-Y. F., 2015, SPECTRAL METHODS HIE; Walker A. J., 1977, ACM Transactions on Mathematical Software, V3, P253, DOI 10.1145/355744.355749; Wang Y., 2015, ADV NEURAL INFORM PR, P991; Wang Y. X., 2016, ARXIV161200991	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704021
C	Soto, V; Suarez, A; Martinez-Munoz, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Soto, Victor; Suarez, Alberto; Martinez-Munoz, Gonzalo			An urn model for majority voting in classification ensembles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this work we analyze the class prediction of parallel randomized ensembles by majority voting as an urn model. For a given test instance, the ensemble can be viewed as an urn of marbles of different colors. A marble represents an individual classifier. Its color represents the class label prediction of the corresponding classifier. The sequential querying of classifiers in the ensemble can be seen as draws without replacement from the urn. An analysis of this classical urn model based on the hypergeometric distribution makes it possible to estimate the confidence on the outcome of majority voting when only a fraction of the individual predictions is known. These estimates can be used to speed up the prediction by the ensemble. Specifically, the aggregation of votes can be halted when the confidence in the final prediction is sufficiently high. If one assumes a uniform prior for the distribution of possible votes the analysis is shown to be equivalent to a previous one based on Dirichlet distributions. The advantage of the current approach is that prior knowledge on the possible vote outcomes can be readily incorporated in a Bayesian framework. We show how incorporating this type of problem-specific knowledge into the statistical analysis of majority voting leads to faster classification by the ensemble and allows us to estimate the expected average speed-up beforehand.	[Soto, Victor] Columbia Univ, Comp Sci Dept, New York, NY 10027 USA; [Suarez, Alberto; Martinez-Munoz, Gonzalo] Univ Autonoma Madrid, Comp Sci Dept, Madrid, Spain	Columbia University; Autonomous University of Madrid	Soto, V (corresponding author), Columbia Univ, Comp Sci Dept, New York, NY 10027 USA.	vsoto@cs.columbia.edu; gonzalo.martinez@uam.es; alberto.suarez@uam.es	Suárez, Alberto/D-6293-2011; Martínez-Muñoz, Gonzalo/K-7269-2012	Suárez, Alberto/0000-0003-4534-0909; Martínez-Muñoz, Gonzalo/0000-0002-6125-6056	Comunidad de Madrid [CASI-CAM-CM S2013/ICE-2845]; Spanish Ministerio de Economia y Competitividad [TIN2013-42351-P, TIN2015-70308-REDT]	Comunidad de Madrid(Comunidad de Madrid); Spanish Ministerio de Economia y Competitividad(Spanish Government)	The authors acknowledge financial support from the Comunidad de Madrid (project CASI-CAM-CM S2013/ICE-2845), and from the Spanish Ministerio de Economia y Competitividad (projects TIN2013-42351-P and TIN2015-70308-REDT).	Asuncion A, 2007, UCI MACHINE LEARNING; Basilico J. D., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P41, DOI 10.1109/ICDM.2011.39; Benbouzid D., 2012, P 29 INT C MACH LEAR, P747; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1996, 460 U CAL STAT DEP; Caruana R, 2006, P 23 INT C MACH LEAR, P161, DOI [DOI 10.1145/1143844.1143865, 10.1145/1143844.1143865]; Caruana Rich, 2004, ICML, DOI DOI 10.1145/1015330.1015432; Dietterich TG, 2000, MACH LEARN, V40, P139, DOI 10.1023/A:1007607513941; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; Fan W, 2002, EIGHTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-02)/FOURTEENTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE (IAAI-02), PROCEEDINGS, P146; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Gao T., 2011, ADV NEURAL INFORM PR, V24, P1062; HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871; Hernandez-Lobato D, 2009, IEEE T PATTERN ANAL, V31, P364, DOI 10.1109/TPAMI.2008.204; HO TK, 1994, IEEE T PATTERN ANAL, V16, P66, DOI 10.1109/34.273716; Margineantu D.D., 1997, ICML, P211, DOI [10.5555/645526.757762, DOI 10.5555/645526.757762]; Markatopoulou F, 2015, NEUROCOMPUTING, V150, P501, DOI 10.1016/j.neucom.2014.07.063; Reyzin L., 2011, P INT C MACH LEARN, P529; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Wang H., 2003, P 9 ACM SIGKDD INT C, P226, DOI DOI 10.1145/956750.956778; Zhang Y, 2006, J MACH LEARN RES, V7, P1315	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700084
C	Steinhardt, J; Liang, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Steinhardt, Jacob; Liang, Percy			Unsupervised Risk Estimation Using Only Conditional Independence Structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CLASSIFICATION	We show how to estimate a model's test error from unlabeled data, on distributions very different from the training distribution, while assuming only that certain conditional independencies are preserved between train and test. We do not need to assume that the optimal predictor is the same between train and test, or that the true distribution lies in any parametric family. We can also efficiently compute gradients of the estimated error and hence perform unsupervised discriminative learning. Our technical tool is the method of moments, which allows us to exploit conditional independencies in the absence of a fully-specified model. Our framework encompasses a large family of losses including the log and exponential loss, and extends to structured output settings such as conditional random fields.	[Steinhardt, Jacob; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Steinhardt, J (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	jsteinhardt@cs.stanford.edu; pliang@cs.stanford.edu			Fannie & John Hertz Foundation; NSF; Future of Life Institute	Fannie & John Hertz Foundation; NSF(National Science Foundation (NSF)); Future of Life Institute	This research was supported by a Fannie & John Hertz Foundation Fellowship, a NSF Graduate Research Fellowship, and a Future of Life Institute grant.	Anandkumar A., 2012, COLT; Anandkumar A., 2013, TENSOR DECOMPOSITION; ANDERSON TW, 1949, ANN MATH STAT, V20, P46, DOI 10.1214/aoms/1177730090; ANDERSON TW, 1950, ANN MATH STAT, V21, P570, DOI 10.1214/aoms/1177729752; Ando R. K., 2007, COLT; Balasubramanian K, 2011, J MACH LEARN RES, V12, P3119; Balcan MF, 2010, J ACM, V57, DOI 10.1145/1706591.1706599; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; Blitzer J., 2011, AISTATS; Blum A, 1998, COLT; Bottou L., 2015, COMMUNICATION; Chaganty A., 2013, ICML; Chaganty A., 2014, ICML; Comon P, 2009, J CHEMOMETR, V23, P393, DOI 10.1002/cem.1236; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; De Lathauwer L, 2006, SIAM J MATRIX ANAL A, V28, P642, DOI 10.1137/040608830; Donmez P, 2010, J MACH LEARN RES, V11, P1323; Duchi J., 2010, COLT; EDMONDS J, 1972, J ACM, V19, P248, DOI 10.1145/321694.321699; Green JBD, 2014, J ANAL METHODS CHEM, V2014, DOI 10.1155/2014/810589; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hansen L. P., 2014, J POLITICAL EC, V122; HANSEN LP, 1982, ECONOMETRICA; Hsu Daniel, 2012, NIPS; Jaffe A, 2015, JMLR WORKSH CONF PRO, V38, P407; Jaffe A, 2016, JMLR WORKSH CONF PRO, V51, P351; Joachims T, 1999, ICML; Johansson F. D., 2016, ICML; Kakade S.M., 2007, COLT; Khani F., 2016, ACL; Kuleshov  V., 2015, AISTATS; Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4; Liang P., 2008, HLT ACL; Mansour Y., 2009, COLT; Merialdo B., 1994, Computational Linguistics, V20, P155; Nigam K., 1998, ASS ADVANCEMENT ARTI; Platanios E. A., 2015, THESIS; Powell J.L., 1994, HDB ECONOMETRICS, VIV; Quinonero-Candela J, 2009, NEURAL INF PROCESS S, pXI; Sargan J. D., 1958, ECONOMETRICA; SARGAN JD, 1959, J ROY STAT SOC B, V21, P91; Sculley D., 2015, P C ADV NEURAL INFOR, V2, P2503, DOI DOI 10.5555/2969442.2969519; Shafer G, 2008, J MACH LEARN RES, V9, P371; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Steinhardt J., 2016, COLT; Tomizawa N., 1971, NETWORKS; Zien A., 2006, SEMISUPERVISED LEARN, P56	48	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701058
C	Subramaniam, A; Chatterjee, M; Mittal, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Subramaniam, Arulkumar; Chatterjee, Moitreya; Mittal, Anurag			Deep Neural Networks with Inexact Matching for Person Re-Identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Person Re-Identification is the task of matching images of a person across multiple camera views. Almost all prior approaches address this challenge by attempting to learn the possible transformations that relate the different views of a person from a training corpora. Then, they utilize these transformation patterns for matching a query image to those in a gallery image bank at test time. This necessitates learning good feature representations of the images and having a robust feature matching technique. Deep learning approaches, such as Convolutional Neural Networks (CNN), simultaneously do both and have shown great promise recently. In this work, we propose two CNN-based architectures for Person Re-Identification. In the first, given a pair of images, we extract feature maps from these images via multiple stages of convolution and pooling. A novel inexact matching technique then matches pixels in the first representation with those of the second. Furthermore, we search across a wider region in the second representation for matching. Our novel matching technique allows us to tackle the challenges posed by large viewpoint variations, illumination changes or partial occlusions. Our approach shows a promising performance and requires only about half the parameters as a current state-of-the-art technique. Nonetheless, it also suffers from false matches at times. In order to mitigate this issue, we propose a fused architecture that combines our inexact matching pipeline with a state-of-the-art exact matching technique. We observe substantial gains with the fused model over the current state-of-the-art on multiple challenging datasets of varying sizes, with gains of up to about 21%.	[Subramaniam, Arulkumar; Chatterjee, Moitreya; Mittal, Anurag] Indian Inst Technol Madras, Chennai 600036, Tamil Nadu, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Madras	Subramaniam, A (corresponding author), Indian Inst Technol Madras, Chennai 600036, Tamil Nadu, India.	aruls@cse.iitm.ac.in; metro.smiles@gmail.com; amittal@cse.iitm.ac.in						Ahmed E, 2015, PROC CVPR IEEE, P3908, DOI 10.1109/CVPR.2015.7299016; Chen DP, 2015, PROC CVPR IEEE, P1565, DOI 10.1109/CVPR.2015.7298764; Chen JX, 2014, INT C PATT RECOG, P1657, DOI 10.1109/ICPR.2014.292; Chen YC, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3402; Farenzena M, 2010, PROC CVPR IEEE, P2360, DOI 10.1109/CVPR.2010.5539926; Guillaumin M, 2009, IEEE I CONF COMP VIS, P498, DOI 10.1109/ICCV.2009.5459197; Hirzer M, 2012, 2012 IEEE NINTH INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL-BASED SURVEILLANCE (AVSS), P203, DOI 10.1109/AVSS.2012.55; Kostinger M, 2012, PROC CVPR IEEE, P2288, DOI 10.1109/CVPR.2012.6247939; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li S, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2155; Li W., 2013, LNCS, V7724, P31, DOI [10.1007/978-3-642-37331-2, DOI 10.1007/978-3-642-37331-2]; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Liao SC, 2015, IEEE I CONF COMP VIS, P3685, DOI 10.1109/ICCV.2015.420; Liao SC, 2015, PROC CVPR IEEE, P2197, DOI 10.1109/CVPR.2015.7298832; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Loy CC, 2013, IEEE IMAGE PROC, P3567, DOI 10.1109/ICIP.2013.6738736; Loy CC, 2009, PROC CVPR IEEE, P1988, DOI 10.1109/CVPRW.2009.5206827; Ma LY, 2014, IEEE T IMAGE PROCESS, V23, P3656, DOI 10.1109/TIP.2014.2331755; Martinel N, 2015, IEEE T IMAGE PROCESS, V24, P5645, DOI 10.1109/TIP.2015.2487048; Paisitkriangkrai S, 2015, PROC CVPR IEEE, P1846, DOI 10.1109/CVPR.2015.7298794; Prosser B. J., 2010, PROC BRIT MACH VIS C, P6, DOI DOI 10.5244/C.24.21; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zhao R, 2014, PROC CVPR IEEE, P144, DOI 10.1109/CVPR.2014.26; Zhao R, 2013, IEEE I CONF COMP VIS, P2528, DOI 10.1109/ICCV.2013.314; Zhao R, 2013, PROC CVPR IEEE, P3586, DOI 10.1109/CVPR.2013.460; Zheng WS, 2013, IEEE T PATTERN ANAL, V35, P653, DOI 10.1109/TPAMI.2012.138	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703003
C	Sumbul, U; Roossien, D; Chen, F; Barry, N; Boyden, ES; Cai, DW; Cunningham, JP; Paninski, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sumbul, Uygar; Roossien, Douglas, Jr.; Chen, Fei; Barry, Nicholas; Boyden, Edward S.; Cai, Dawen; Cunningham, John P.; Paninski, Liam			Automated scalable segmentation of neurons from multispectral images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				RECONSTRUCTION; VISUALIZATION	Reconstruction of neuroanatomy is a fundamental problem in neuroscience. Stochastic expression of colors in individual cells is a promising tool, although its use in the nervous system has been limited due to various sources of variability in expression. Moreover, the intermingled anatomy of neuronal trees is challenging for existing segmentation algorithms. Here, we propose a method to automate the segmentation of neurons in such (potentially pseudo-colored) images. The method uses spatio-color relations between the voxels, generates supervoxels to reduce the problem size by four orders of magnitude before the final segmentation, and is parallelizable over the supervoxels. To quantify performance and gain insight, we generate simulated images, where the noise level and characteristics, the density of expression, and the number of fluorophore types are variable. We also present segmentations of real Brainbow images of the mouse hippocampus, which reveal many of the dendritic segments.	[Sumbul, Uygar; Cunningham, John P.; Paninski, Liam] Columbia Univ, Grossman Ctr Stat Mind, New York, NY 10027 USA; [Sumbul, Uygar; Cunningham, John P.; Paninski, Liam] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Roossien, Douglas, Jr.; Cai, Dawen] Univ Michigan, Sch Med, Ann Arbor, MI 48109 USA; [Chen, Fei; Barry, Nicholas; Boyden, Edward S.] MIT, Media Lab, Cambridge, MA 02139 USA; [Chen, Fei; Barry, Nicholas; Boyden, Edward S.] MIT, McGovern Inst, Cambridge, MA 02139 USA	Columbia University; Columbia University; University of Michigan System; University of Michigan; Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Sumbul, U (corresponding author), Columbia Univ, Grossman Ctr Stat Mind, New York, NY 10027 USA.; Sumbul, U (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.		Cai, Dawen/Q-3354-2017; Chen, Fei/HGE-5690-2022	Cai, Dawen/0000-0003-4471-2061; Chen, Fei/0000-0003-2308-3649	ARO MURI [W911NF-12-1-0594]; DARPA [N66001-15-C-4032]; Google Faculty Research award; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) [D16PC00008]	ARO MURI(MURI); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Google Faculty Research award(Google Incorporated); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC)	Funding for this research was provided by ARO MURI W911NF-12-1-0594, DARPA N66001-15-C-4032 (SIMPLEX), and a Google Faculty Research award; in addition, this work was supported by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00008. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; BERTRAND G, 1994, PATTERN RECOGN LETT, V15, P169, DOI 10.1016/0167-8655(94)90046-9; Betzig E, 2006, SCIENCE, V313, P1642, DOI 10.1126/science.1127344; Cai D, 2013, NAT METHODS, V10, P540, DOI [10.1038/NMETH.2450, 10.1038/nmeth.2450]; Chen KH, 2015, SCIENCE, V348, DOI 10.1126/science.aaa6090; Cho I, 2018, J MICROSC-OXFORD, V271, P123, DOI 10.1111/jmi.12712; Chung K, 2013, NAT METHODS, V10, P508, DOI [10.1038/NMETH.2481, 10.1038/nmeth.2481]; Cousty J, 2009, IEEE T PATTERN ANAL, V31, P1362, DOI 10.1109/TPAMI.2008.173; HUBERT L, 1985, J CLASSIF, V2, P193, DOI 10.1007/BF01908075; Jain V, 2010, PROC CVPR IEEE, P2488, DOI 10.1109/CVPR.2010.5539950; Kim JS, 2014, NATURE, V509, P331, DOI 10.1038/nature13240; Kurihara K, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2796; Lee DD, 2001, ADV NEUR IN, V13, P556; Lee JH, 2014, SCIENCE, V343, P1360, DOI 10.1126/science.1250212; Livet J, 2007, NATURE, V450, P56, DOI 10.1038/nature06293; Longair MH, 2011, BIOINFORMATICS, V27, P2453, DOI 10.1093/bioinformatics/btr390; Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725; Marblestone A.H., 2014, ARXIV14045103; MEYER F, 1994, SIGNAL PROCESS, V38, P113, DOI 10.1016/0165-1684(94)90060-4; Meyer F, 1994, COMP IMAG VIS, V2, P77; Miller J. W., 2013, ADV NEURAL INFORM PR, P199; Ng AY, 2002, ADV NEUR IN, V14, P849; Peng HC, 2010, NAT BIOTECHNOL, V28, P348, DOI 10.1038/nbt.1612; Ren XF, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P10; Rust MJ, 2006, NAT METHODS, V3, P793, DOI 10.1038/nmeth929; Sumbul U., 2014, NATURE COMMUNICATION, P5; Sumbul  U, 2014, FRONTIERS NEUROSCIEN; Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; WU Z, 1993, IEEE T PATTERN ANAL, V15, P1101, DOI 10.1109/34.244673; Zador AM, 2012, PLOS BIOL, V10, DOI 10.1371/journal.pbio.1001411	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704073
C	Suzuki, T; Kanagawa, H; Kobayash, H; Shimizu, N; Tagami, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Suzuki, Taiji; Kanagawa, Heishiro; Kobayash, Hayato; Shimizu, Nobuyuki; Tagami, Yukihiro			Minimax Optimal Alternating Minimization for Kernel Nonparametric Tensor Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DECOMPOSITIONS	We investigate the statistical performance and computational efficiency of the alternating minimization procedure for nonparametric tensor learning. Tensor modeling has been widely used for capturing the higher order relations between multimodal data sources. In addition to a linear model, a nonlinear tensor model has been received much attention recently because of its high flexibility. We consider an alternating minimization procedure for a general nonlinear model where the true function consists of components in a reproducing kernel Hilbert space (RKHS). In this paper, we show that the alternating minimization method achieves linear convergence as an optimization algorithm and that the generalization error of the resultant estimator yields the minimax optimality. We apply our algorithm to some multitask learning problems and show that the method actually shows favorable performances.	[Suzuki, Taiji; Kanagawa, Heishiro] Tokyo Inst Technol, Dept Math & Comp Sci, Tokyo, Japan; [Suzuki, Taiji] Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan; [Suzuki, Taiji] RIKEN, Ctr Adv Integrated Intelligence Res, Wako, Saitama, Japan; [Kobayash, Hayato; Shimizu, Nobuyuki; Tagami, Yukihiro] Yahoo Japan Corp, Tokyo, Japan	Tokyo Institute of Technology; Japan Science & Technology Agency (JST); RIKEN	Suzuki, T (corresponding author), Tokyo Inst Technol, Dept Math & Comp Sci, Tokyo, Japan.; Suzuki, T (corresponding author), Japan Sci & Technol Agcy, PRESTO, Kawaguchi, Saitama, Japan.; Suzuki, T (corresponding author), RIKEN, Ctr Adv Integrated Intelligence Res, Wako, Saitama, Japan.	s-taiji@is.titech.ac.jp; kanagawa.h.ab@m.titech.ac.jp			MEXT kakenhi [25730013, 25120012, 26280009, 15H01678, 15H05707]; JST-PRESTO; JST-CREST	MEXT kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); JST-PRESTO(Japan Science & Technology Agency (JST)); JST-CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	This work was partially supported by MEXT kakenhi (25730013, 25120012, 26280009, 15H01678 and 15H05707), JST-PRESTO and JST-CREST.	Aswani A., 2014, ARXIV14120620; Bahadori M. T., ADV NEURAL INFORM PR, V27; Barak B., 2015, ARXIV150106521; Bartlett PL, 2005, ANN STAT, V33, P1497, DOI 10.1214/009053605000000282; Bhojanapalli S., 2015, ARXIV150205023; Blanca V.-G., 2011, P 3 WORKSH CONT AW R; Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46; Gandy S, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/2/025010; Hitchcock F.L., 1927, J MATH PHYS CAMB, V6, P164, DOI 10.1002/sapm192761164; Hitchcock FL., 1927, J MATH PHYS, V7, P39, DOI [DOI 10.1002/SAPML9287139, DOI 10.1002/SAPM19287139]; Imaizumi M., 2016, INT C MACH LEARN ICM; Kanagawa H., 2016, INT C MACH LEARN, P1632; Karatzoglou A, 2010, P 4 ACM C REC SYST, P79, DOI DOI 10.1145/1864708.1864727; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Ledoux M., 2005, MATH SURVEYS MONOGRA, V89; Liu J, 2009, IEEE I CONF COMP VIS, P2114; Morup M, 2011, WIRES DATA MIN KNOWL, V1, P24, DOI 10.1002/widm.1; Romera-Paredes B., 2013, P 30 INT C MACHINE L, P1444; Shah  P., 2015, ARXIV150504085; Shen WN, 2016, BERNOULLI, V22, P396, DOI 10.3150/14-BEJ663; SIGNORETTO M, 2010, 10186 ESATSISTA KU L; Signoretto M., 2013, ABS13104977 CORR; Steinwart I., 2009, P 22 ANN C LEARN THE, V22, P79; Steinwart I., 2008, SUPPORT VECTOR MACHI; Sun Wei, 2015, Adv Neural Inf Process Syst, V28, P1081; Suzuki T, 2015, PR MACH LEARN RES, V37, P1273; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Van de Geer S., 2000, APPL EMPIRICAL PROCE; Van Der VaartJon A., 1996, WEAK CONVERGENCE EMP; Wimalawarne K., 2014, PROC 27 INT C NEURAL, P2825; Xu Z., 2011, ABS11086296 CORR; Zhang Z., 2015, ARXIV150204689	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703055
C	Tandon, P; Malviya, YH; Rajendran, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tandon, Pulkit; Malviya, Yash H.; Rajendran, Bipin			Efficient and Robust Spiking Neural Circuit for Navigation Inspired by Echolocating Bats	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				NETWORKS	We demonstrate a spiking neural circuit for azimuth angle detection inspired by the echolocation circuits of the Horseshoe bat Rhinolophus ferrumequinum and utilize it to devise a model for navigation and target tracking, capturing several key aspects of information transmission in biology. Our network, using only a simple local-information based sensor implementing the cardioid angular gain function, operates at biological spike rate of approximately 10 Hz. The network tracks large angular targets (60 degrees) within 1 sec with a 10% RMS error. We study the navigational ability of our model for foraging and target localization tasks in a forest of obstacles and show that it requires less than 200 X spike-triggered decisions, while suffering less than 1% loss in performance compared to a proportional-integral-derivative controller, in the presence of 50% additive noise. Superior performance can be obtained at a higher average spike rate of 100 Hz and 1000 Hz, but even the accelerated networks require 20 X and 10 X lesser decisions respectively, demonstrating the superior computational efficiency of bio-inspired information processing systems.	[Tandon, Pulkit; Malviya, Yash H.] Indian Inst Technol, Bombay, Maharashtra, India; [Rajendran, Bipin] New Jersey Inst Technol, Newark, NJ 07102 USA	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Bombay; New Jersey Institute of Technology	Tandon, P (corresponding author), Indian Inst Technol, Bombay, Maharashtra, India.	pulkit1495@gmail.com; yashmalviya94@gmail.com; bipin@njit.edu			CISCO Systems Inc.	CISCO Systems Inc.	This research was supported in part by the CAMPUSENSE project grant from CISCO Systems Inc.	Brette R, 2005, J NEUROPHYSIOL, V94, P3637, DOI 10.1152/jn.00686.2005; Burger RM, 2001, J NEUROSCI, V21, P4830, DOI 10.1523/JNEUROSCI.21-13-04830.2001; Hayward B., 1964, FLIGHT SPEEDS W BATS, V45, P236; Kandel E. R., 2000, NOBEL LECT PHISIOLOG; LeCun Y., 2015, NATURE, V521, P436, DOI DOI 10.1038/NATURE14539; Maass W, 1997, NEURAL NETWORKS, V10, P1659, DOI 10.1016/S0893-6080(97)00011-7; Moss C. F., 2010, PROBING NATURAL SCEN; Moss CF, 2003, CURR OPIN NEUROBIOL, V13, P751, DOI 10.1016/j.conb.2003.10.016; Schuller G, 1988, TARGET DISCRIMINATIO, P413; Shi R. Z., 2007, NEUROMORPHIC VLSI MO, P74; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simmons J. A, 1996, EPTESICUS FUSCUS, V100, P1764; Sjostrom P. J, 2012, SPIKE TIMING DEPENDE, V4, P2; Suga N., 1990, BIOSONAR NEURAL COMP, V262, P60	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704082
C	Tang, PF; Phillips, JM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tang, Pingfan; Phillips, Jeff M.			The Robustness of Estimator Composition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We formalize notions of robustness for composite estimators via the notion of a breakdown point. A composite estimator successively applies two (or more) estimators: on data decomposed into disjoint parts, it applies the first estimator on each part, then the second estimator on the outputs of the first estimator. And so on, if the composition is of more than two estimators. Informally, the breakdown point is the minimum fraction of data points which if significantly modified will also significantly modify the output of the estimator, so it is typically desirable to have a large breakdown point. Our main result shows that, under mild conditions on the individual estimators, the breakdown point of the composite estimator is the product of the breakdown points of the individual estimators. We also demonstrate several scenarios, ranging from regression to statistical testing, where this analysis is easy to apply, useful in understanding worst case robustness, and sheds powerful insights onto the associated data analysis.	[Tang, Pingfan; Phillips, Jeff M.] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Tang, PF (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.	tang1984@cs.utah.edu; jeffp@cs.utah.edu						Aloupis G., 2006, DATA DEPTH ROBUST MU; CORMODE G, 2008, PODS; Das Sarma A, 2009, VLDB J, V18, P989, DOI 10.1007/s00778-009-0147-0; Davies PL, 2007, REVSTAT-STAT J, V5, P1; HAMPEL FR, 1971, ANN MATH STAT, V42, P1887, DOI 10.1214/aoms/1177693054; Hampel FR., 2011, WILEY SERIES PROBABI; HE XM, 1990, J AM STAT ASSOC, V85, P446, DOI 10.2307/2289782; Huber P., 1981, ROBUST STAT; Huber P. J., 2009, ROBUST STAT, P8; Jorgensen A. G., 2011, WADS; SIEGEL AF, 1982, BIOMETRIKA, V69, P242, DOI 10.1093/biomet/69.1.242; Tang P., 2016, ARXIV160901226; Weiszfeld E, 2009, ANN OPER RES, V167, P7, DOI 10.1007/s10479-008-0352-z; Welsh A. H., 1996, ASPECTS STAT INFEREN, P245	14	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700020
C	Tenzer, Y; Schwing, A; Gimpel, K; Hazan, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tenzer, Yaniv; Schwing, Alexander; Gimpel, Kevin; Hazan, Tamir			Constraints Based Convex Belief Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Inference in Markov random fields subject to consistency structure is a fundamental problem that arises in many real-life applications. In order to enforce consistency, classical approaches utilize consistency potentials or encode constraints over feasible instances. Unfortunately this comes at the price of a tremendous computational burden. In this paper we suggest to tackle consistency by incorporating constraints on beliefs. This permits derivation of a closed-form message-passing algorithm which we refer to as the Constraints Based Convex Belief Propagation (CBCBP). Experiments show that CBCBP outperforms the conventional consistency potential based approach, while being at least an order of magnitude faster.	[Tenzer, Yaniv] Hebrew Univ Jerusalem, Dept Stat, Jerusalem, Israel; [Schwing, Alexander] Univ Illinois, Dept Elect & Comp Engn, Champaign, IL USA; [Gimpel, Kevin] Toyota Technol Inst, Chicago, IL USA; [Hazan, Tamir] Technion Israel Inst Technol, Fac Ind Engn & Management, Haifa, Israel	Hebrew University of Jerusalem; University of Illinois System; University of Illinois Urbana-Champaign; Toyota Technological Institute - Chicago; Technion Israel Institute of Technology	Tenzer, Y (corresponding author), Hebrew Univ Jerusalem, Dept Stat, Jerusalem, Israel.							Bach S., 2012, ADV NEURAL INF PROCE, V25, P2654; Callison- Burch C., 2011, P WMT; Chen L. -C., 2015, P ICML EQ CONTR; Cipoll Roberto, 2008, PROC CVPR IEEE, P1; Everingham M., 2012, PASCAL VISUAL OBJECT; Hazan T., 2012, P UAI; Heskes T, 2004, NEURAL COMPUT, V16, P2379, DOI 10.1162/0899766041941943; Koehn P., 2003, P 2003 C N AM CHAPT, P127, DOI DOI 10.3115/1073445.1073462; KOEHN P, 2007, P ACL; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Koller D., 2009, PROBABILISTIC GRAPHI; Ladicky L, 2010, LECT NOTES COMPUT SC, V6315, P239, DOI 10.1007/978-3-642-15555-0_18; Martins A. F., 2011, AUGMENTED LAGRANGIAN; Martins A. F. R. S., 2012, THESIS; Meltzer T., 2009, UAI; Meshi O., 2010, INT C MACH LEARN; Nowozin S, 2009, PROC CVPR IEEE, P818, DOI 10.1109/CVPRW.2009.5206567; Papineni K., 2002, P 40 ANN M ASS COMP, P311, DOI [10.3115/1073083.1073135, DOI 10.3115/1073083.1073135]; Schwing A., 2011, P CVPR; Schwing A. G., 2012, P NIPS; Schwing A.G., 2013, P ICCV; Smith David, 2008, P C EMP METH NAT LAN, P145; Tarlow Daniel, 2011, P 28 INT C MACH LEAR, P113; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701076
C	Titsias, MK		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Titsias, Michalis K.			One-vs-Each Approximation to Softmax for Scalable Estimation of Probabilities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The softmax representation of probabilities for categorical variables plays a prominent role in modern machine learning with numerous applications in areas such as large scale classification, neural language modeling and recommendation systems. However, softmax estimation is very expensive for large scale inference because of the high cost associated with computing the normalizing constant. Here, we introduce an efficient approximation to softmax probabilities which takes the form of a rigorous lower bound on the exact probability. This bound is expressed as a product over pairwise probabilities and it leads to scalable estimation based on stochastic optimization. It allows us to perform doubly stochastic estimation by subsampling both training instances and class labels. We show that the new bound has interesting theoretical properties and we demonstrate its use in classification problems.	[Titsias, Michalis K.] Athens Univ Econ & Business, Dept Informat, Athens, Greece	Athens University of Economics & Business	Titsias, MK (corresponding author), Athens Univ Econ & Business, Dept Informat, Athens, Greece.	mtitsias@aueb.gr						[Anonymous], 2012, ARTIF INTELL; Bengio Yoshua, 2003, P C ART INT STAT AIS; Bhatia Kush, 2015, ADV NEURAL INFORM PR, P730, DOI DOI 10.5555/2969239.2969321; Bishop C.M, 2006, PATTERN RECOGN; BOHNING D, 1992, ANN I STAT MATH, V44, P197, DOI 10.1007/bf00048682; Bouchard Guillaume, 2007, TECHNICAL REPORT; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Devlin J, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1370; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gopal Siddharth, 2013, JMLR WORKSHOP C P, P289; Huang TK, 2006, J MACH LEARN RES, V7, P85; Ji Shihao, 2015, BLACKOUT SPEEDING RE; Katakis I., 2008, P ECML PKDD 08 WORKS; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Mnih A., 2012, P 29 INT C MACH LEAR, P1751; Morin F., 2005, PROC INT WORKSHOP AR, P246; Paquet Ulrich, 2012, ABS14092824 CORR; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Vijayanarasimhan S., 2014, ABS14127479 CORR	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703104
C	Tolstikhin, I; Sriperumbudur, BK; Scholkopf, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tolstikhin, Ilya; Sriperumbudur, Bharath K.; Schoelkopf, Bernhard			Minimax Estimation of Maximum Mean Discrepancy with Radial Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				METRICS	Maximum Mean Discrepancy (MMD) is a distance on the space of probability measures which has found numerous applications in machine learning and nonparametric testing. This distance is based on the notion of embedding probabilities in a reproducing kernel Hilbert space. In this paper, we present the first known lower bounds for the estimation of MMD based on finite samples. Our lower bounds hold for any radial universal kernel on R-d and match the existing upper bounds up to constants that depend only on the properties of the kernel. Using these lower bounds, we establish the minimax rate optimality of the empirical estimator and its U-statistic variant, which are usually employed in applications.	[Tolstikhin, Ilya; Schoelkopf, Bernhard] MPI Intelligent Syst, Dept Empir Inference, D-72076 Tubingen, Germany; [Sriperumbudur, Bharath K.] Penn State Univ, Dept Stat, University Pk, PA 16802 USA	Max Planck Society; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Tolstikhin, I (corresponding author), MPI Intelligent Syst, Dept Empir Inference, D-72076 Tubingen, Germany.	ilya@tuebingen.mpg.de; bks18@psu.edu; bs@tuebingen.mpg.de						Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; FUKUMIZU K, 2008, ADV NEURAL INFORM PR, V20, P489; Fukumizu K, 2013, J MACH LEARN RES, V14, P3753; Gretton A., 2008, ADV NEURAL INFORM PR, P585; Gretton A, 2012, J MACH LEARN RES, V13, P723; Lehmann E. L., 2008, THEORY POINT ESTIMAT; Lopez-Paz  D., 2015, P 32 INT C MACH LEAR; Muandet  K., 2016, J MACHINE LEARNING R; Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Song L, 2012, J MACH LEARN RES, V13, P1393; Song Le, 2008, P 25 INT C MACHINE L, P992; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I., 2008, SUPPORT VECTOR MACHI; Szabo Z, 2015, JMLR WORKSH CONF PRO, V38, P948; Tolstikhin I., 2016, ARXIV160204361MATHST; Tsybakov A.B, 2008, INTRO NONPARAMETRIC	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704008
C	Torrecilla, JL; Suarez, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Torrecilla, Jose L.; Suarez, Alberto			Feature selection in functional data classification with recursive maxima hunting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				VARIABLE SELECTION; LOGISTIC-REGRESSION	Dimensionality reduction is one of the key issues in the design of effective machine learning methods for automatic induction. In this work, we introduce recursive maxima hunting (RMH) for variable selection in classification problems with functional data. In this context, variable selection techniques are especially attractive because they reduce the dimensionality, facilitate the interpretation and can improve the accuracy of the predictive models. The method, which is a recursive extension of maxima hunting (MH), performs variable selection by identifying the maxima of a relevance function, which measures the strength of the correlation of the predictor functional variable with the class label. At each stage, the information associated with the selected variable is removed by subtracting the conditional expectation of the process. The results of an extensive empirical evaluation are used to illustrate that, in the problems investigated, RMH has comparable or higher predictive accuracy than standard dimensionality reduction techniques, such as PCA and PLS, and state-of-the-art feature selection methods for functional data, such as maxima hunting.	[Torrecilla, Jose L.; Suarez, Alberto] Univ Autonoma Madrid, Comp Sci Dept, E-28049 Madrid, Spain	Autonomous University of Madrid	Torrecilla, JL (corresponding author), Univ Autonoma Madrid, Comp Sci Dept, E-28049 Madrid, Spain.	joseluis.torrecilla@uam.es; alberto.suarez@uam.es	Torrecilla, José Luis/L-2688-2013; Suárez, Alberto/D-6293-2011	Torrecilla, José Luis/0000-0003-3719-5190; Suárez, Alberto/0000-0003-4534-0909	Spanish Ministry of Economy and Competitiveness [TIN2013-42351-P]; Regional Government of Madrid, CASI-CAM-CM project [S2013/ICE-2845]	Spanish Ministry of Economy and Competitiveness(Spanish Government); Regional Government of Madrid, CASI-CAM-CM project	The authors thank Dr. Jose R. Berrendero for his insightful suggestions. We also acknowledge financial support from the Spanish Ministry of Economy and Competitiveness, project TIN2013-42351-P and from the Regional Government of Madrid, CASI-CAM-CM project (S2013/ICE-2845).	Aneiros G, 2014, STAT PROBABIL LETT, V94, P12, DOI 10.1016/j.spl.2014.06.025; Baillo A., 2011, CLASSIFICATION METHO, P259; Berrendero JR, 2016, J STAT COMPUT SIM, V86, P891, DOI 10.1080/00949655.2015.1042378; Berrendero J. R., 2015, ARXIV150704398, P1; Berrendero JR, 2016, STAT SINICA, V26, P619, DOI 10.5705/ss.202014.0014; Delaigle A, 2012, BIOMETRIKA, V99, P299, DOI 10.1093/biomet/ass003; Delaigle A, 2012, ANN STAT, V40, P322, DOI 10.1214/11-AOS958; Delaigle A, 2012, J R STAT SOC B, V74, P267, DOI 10.1111/j.1467-9868.2011.01003.x; Fernandez-Lozano C, 2015, SOFT COMPUT, V19, P2469, DOI 10.1007/s00500-014-1573-5; Ferraty F, 2010, BIOMETRIKA, V97, P807, DOI 10.1093/biomet/asq058; Ferraty F., 2006, SPR S STAT; Fraiman R, 2016, J MULTIVARIATE ANAL, V146, P191, DOI 10.1016/j.jmva.2015.09.006; Galeano P, 2015, TECHNOMETRICS, V57, P281, DOI 10.1080/00401706.2014.902774; GENTLEMAN R, 2005, J BIOINF COMPUT BIOL, P189; Gomez-Verdejo V, 2009, NEUROCOMPUTING, V72, P3580, DOI 10.1016/j.neucom.2008.12.035; Grosenick L, 2008, IEEE T NEUR SYS REH, V16, P539, DOI 10.1109/TNSRE.2008.926701; Guyon I., 2006, FEATURE EXTRACTION F; Kneip A, 2011, ANN STAT, V39, P2410, DOI 10.1214/11-AOS905; Lange T, 2014, STAT PAP, V55, P49, DOI 10.1007/s00362-012-0488-4; Li B, 2008, COMPUT STAT DATA AN, V52, P4790, DOI 10.1016/j.csda.2008.03.024; Lindquist MA, 2009, J AM STAT ASSOC, V104, P1575, DOI 10.1198/jasa.2009.tm08496; McKeague IW, 2010, ANN STAT, V38, P2559, DOI 10.1214/10-AOS791; Morters P., 2010, BROWNIAN MOTION; Preda C, 2007, COMPUTATION STAT, V22, P223, DOI 10.1007/s00180-007-0041-4; Ramsay J, 2005, FUNCTIONAL DATA ANAL, Vsecond, DOI 10.1007/b98888; Ryali S, 2010, NEUROIMAGE, V51, P752, DOI 10.1016/j.neuroimage.2010.02.040; Szekely GJ, 2007, ANN STAT, V35, P2769, DOI 10.1214/009053607000000505; Szekely GJ, 2012, STAT PROBABIL LETT, V82, P2278, DOI 10.1016/j.spl.2012.08.007; Tian TS, 2013, COMPUT STAT DATA AN, V57, P282, DOI 10.1016/j.csda.2012.06.017; Yu L, 2004, J MACH LEARN RES, V5, P1205; Zhou JH, 2013, STAT SINICA, V23, P25, DOI 10.5705/ss.2010.237; Zou XB, 2010, ANAL CHIM ACTA, V667, P14, DOI 10.1016/j.aca.2010.03.048	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703028
C	Toulis, P; Parkes, DC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Toulis, Panagiotis (Panos); Parkes, David C.			Long-term causal effects via behavioral game theory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIFFERENCE-IN-DIFFERENCES	Planned experiments are the gold standard in reliably comparing the causal effect of switching from a baseline policy to a new policy. One critical shortcoming of classical experimental methods, however, is that they typically do not take into account the dynamic nature of response to policy changes. For instance, in an experiment where we seek to understand the effects of a new ad pricing policy on auction revenue, agents may adapt their bidding in response to the experimental pricing changes. Thus, causal effects of the new pricing policy after such adaptation period, the long-term causal effects, are not captured by the classical methodology even though they clearly are more indicative of the value of the new policy. Here, we formalize a framework to define and estimate long-term causal effects of policy changes in multiagent economies. Central to our approach is behavioral game theory, which we leverage to formulate the ignorability assumptions that are necessary for causal inference. Under such assumptions we estimate long-term causal effects through a latent space approach, where a behavioral model of how agents act conditional on their latent behaviors is combined with a temporal model of how behaviors evolve over time.	[Toulis, Panagiotis (Panos)] Univ Chicago, Booth Sch, Econometr & Stat, Chicago, IL 60637 USA; [Parkes, David C.] Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA	University of Chicago; Harvard University	Toulis, P (corresponding author), Univ Chicago, Booth Sch, Econometr & Stat, Chicago, IL 60637 USA.	panos.toulis@chicagobooth.edu; parkes@eecs.harvard.edu			Google US/Canada Fellowship in Statistics; NSF [CCF-1301976]; SEAS TomKat fund	Google US/Canada Fellowship in Statistics; NSF(National Science Foundation (NSF)); SEAS TomKat fund	The authors wish to thank Leon Bottou, the organizers and participants of CODE@MIT' 15, GAMES' 16, the Workshop on Algorithmic Game Theory and Data Science (EC' 15), and the anonymous NIPS reviewers for their valuable feedback. Panos Toulis has been supported in part by the 2012 Google US/Canada Fellowship in Statistics. David C. Parkes was supported in part by NSF grant CCF-1301976 and the SEAS TomKat fund.	Abadie A, 2005, REV ECON STUD, V72, P1, DOI 10.1111/0034-6527.00321; Aitchison J., 1986, MONOGRAPHS STAT APPL; Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1; Athey S, 2011, Q J ECON, V126, P207, DOI 10.1093/qje/qjq001; Bottou L, 2013, J MACH LEARN RES, V14, P3207; CARD D, 1994, AM ECON REV, V84, P772; Donald SG, 2007, REV ECON STAT, V89, P221, DOI 10.1162/rest.89.2.221; Fisher RA, 1935, DESIGN EXPT; GRUNWALD GK, 1993, J ROY STAT SOC B MET, V55, P103; Hahn PR, 2015, ANN APPL STAT, V9, P1459, DOI 10.1214/15-AOAS830; Heckman James J, 1998, AM ECON REV, V88; Heckman JJ, 2005, ECONOMETRICA, V73, P669, DOI 10.1111/j.1468-0262.2005.00594.x; HOLLAND JH, 1991, AM ECON REV, V81, P365; HOLLAND PW, 1986, J AM STAT ASSOC, V81, P945, DOI 10.2307/2289064; MCKELVEY RD, 1995, GAME ECON BEHAV, V10, P6, DOI 10.1006/game.1995.1023; Michael Ostrovsky, 2011, P 12 ACM C ELECT COM, P59, DOI DOI 10.1145/1993574.1993585; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; RAPOPORT A, 1992, GAME ECON BEHAV, V4, P261, DOI 10.1016/0899-8256(92)90019-O; Rubin Donald B, 2011, J AM STAT ASSOC; STAHL DO, 1994, J ECON BEHAV ORGAN, V25, P309, DOI 10.1016/0167-2681(94)90103-1; Torrance G W, 1989, Int J Technol Assess Health Care, V5, P559; Wright JR, 2010, AAAI CONF ARTIF INTE, P901	22	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700023
C	Tsai, CY; Saxe, A; Cox, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Tsai, Chuan-Yung; Saxe, Andrew; Cox, David			Tensor Switching Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a novel neural network algorithm, the Tensor Switching (TS) network, which generalizes the Rectified Linear Unit (ReLU) nonlinearity to tensor-valued hidden units. The TS network copies its entire input vector to different locations in an expanded representation, with the location determined by its hidden unit activity. In this way, even a simple linear readout from the TS representation can implement a highly expressive deep-network-like function. The TS network hence avoids the vanishing gradient problem by construction, at the cost of larger representation size. We develop several methods to train the TS network, including equivalent kernels for infinitely wide and deep TS networks, a one-pass linear learning algorithm, and two backpropagation-inspired representation learning algorithms. Our experimental results demonstrate that the TS network is indeed more expressive and consistently learns faster than standard ReLU networks.	[Tsai, Chuan-Yung; Saxe, Andrew; Cox, David] Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA	Harvard University	Tsai, CY (corresponding author), Harvard Univ, Ctr Brain Sci, Cambridge, MA 02138 USA.	chuanyungtsai@fas.harvard.edu; asaxe@fas.harvard.edu; davidcox@fas.harvard.edu			NSF [IIS 1409097]; IARPA [D16PC00002]; Swartz Foundation	NSF(National Science Foundation (NSF)); IARPA; Swartz Foundation	We would like to thank James Fitzgerald, Mien "Brabeeba" Wang, Scott Linderman, and Yu Hu for fruitful discussions. We also thank the anonymous reviewers for their valuable comments. This work was supported by NSF (IIS 1409097), IARPA (contract D16PC00002), and the Swartz Foundation.	Amos  B., 2015, ICLR WORKSH; Anden  J., 2014, IEEE T SP; [Anonymous], 2015, ICLR WORKSH; Aslan O, 2014, NIPS; Bai  Q., 2016, ICML; Cho Youngmin, 2010, NEURAL COMPUTATION; Courville  A., 2011, AISTATS; Deng Li, 2011, INTERSPEECH; Duvenaud  D., 2014, AISTATS; Greff K., 2015, NIPS; He K., 2016, ECCV; Hochreiter S., 2001, FIELD GUIDE DYNAMICA; Huang G., P IEEE C COMP VIS PA, P4700; Janzamin M., 2015, BEATING PERILS NONCO; Konda K., 2015, ICLR; LeCun Y. A., 2015, NATURE; Miyato T., 2016, ICLR; Nair V., 2010, ICML; Saxe  A., 2015, COSYNE; Saxe Andrew M., 2014, ICLR; Schmidhuber Jurgen, 2015, NEURAL NETWORKS; Simonyan Karen, 2015, INT C LEARN REPR; Sonoda S., 2015, APPL COMPUTATIONAL H; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Wang  S., 2016, ICML; Warde-Farley D., 2013, ICML	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704041
C	Vinayak, RK; Hassibi, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vinayak, Ramya Korlakai; Hassibi, Babak			Crowdsourced Clustering: Querying Edges vs Triangles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the task of clustering items using answers from non-expert crowd workers. In such cases, the workers are often not able to label the items directly, however, it is reasonable to assume that they can compare items and judge whether they are similar or not. An important question is what queries to make, and we compare two types: random edge queries, where a pair of items is revealed, and random triangles, where a triple is. Since it is far too expensive to query all possible edges and/or triangles, we need to work with partial observations subject to a fixed query budget constraint. When a generative model for the data is available (and we consider a few of these) we determine the cost of a query by its entropy; when such models do not exist we use the average response time per query of the workers as a surrogate for the cost. In addition to theoretical justification, through several simulations and experiments on two real data sets on Amazon Mechanical Turk, we empirically demonstrate that, for a fixed budget, triangle queries uniformly outperform edge queries. Even though, in contrast to edge queries, triangle queries reveal dependent edges, they provide more reliable edges and, for a fixed budget, many more of them. We also provide a sufficient condition on the number of observations, edge densities inside and outside the clusters and the minimum cluster size required for the exact recovery of the true adjacency matrix via triangle queries using a convex optimization-based clustering algorithm.	[Vinayak, Ramya Korlakai; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Vinayak, RK (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	ramya@caltech.edu; hassibi@systems.caltech.edu						Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Gomes Ryan G, 2011, NEURAL INFORM PROCES, P558; Heikinheimo H., 2013, HCOMP; Heim Eric, MACH LEARN KNOWL DIS, P563; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Karger D. R., 2011, NEUR INF PROC SYST C; Karger DR, 2014, OPER RES, V62, P1, DOI 10.1287/opre.2013.1235; Khosla Aditya, 2011, P C COMP VIS PATT RE; Liu Q., 2012, NEUR INF PROC SYST C; Meila M, 2007, J MULTIVARIATE ANAL, V98, P873, DOI 10.1016/j.jmva.2006.11.013; Ng AY, 2002, ADV NEUR IN, V14, P849; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Simpson R, 2014, P 23 INT C WORLD WID; Snow Rion, 2008, P 2008 C EMP METH NA, P254, DOI DOI 10.3115/1613715.1613751; Sorokin A., 2008, P 1 IEEE WORKSH INT, V08, P1, DOI DOI 10.1109/CVPRW.2008.4562953; Tamuz Omer, 2011, CORR; van der Maaten L., 2012, MACH LEARN SIGN PROC, P1; Vempaty Aditya, 2013, CORR; Vinayak Ramya Korlakai, 2014, NEUR INF PROC SYST C; von Ahn L, 2008, SCIENCE, V321, P1465, DOI 10.1126/science.1160379; Wah C, 2014, PROC CVPR IEEE, P859, DOI 10.1109/CVPR.2014.115; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Welinder P., 2010, NEUR INF PROC SYST C; Wilber M., 2014, HUMAN COMPUTATION CR; Yi Jinfeng, 2012, NEUR INF PROC SYST C; Zhang Yuchen, 2014, NEUR INF PROC SYST C; Zhou D., 2012, ADV NEURAL INFORM PR, P2195; [No title captured]	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704024
C	Vinyals, O; Blundell, C; Lillicrap, T; Kavukcuoglu, K; Wierstra, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vinyals, Oriol; Blundell, Charles; Lillicrap, Timothy; Kavukcuoglu, Koray; Wierstra, Daan			Matching Networks for One Shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Learning from a few examples remains a key challenge in machine learning. Despite recent advances in important domains such as vision and language, the standard supervised deep learning paradigm does not offer a satisfactory solution for learning new concepts rapidly from little data. In this work, we employ ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. Our framework learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. We then define one-shot learning problems on vision (using Omniglot, ImageNet) and language tasks. Our algorithm improves one-shot accuracy on ImageNet from 87.6% to 93.2% and from 88.0% to 93.8% on Omniglot compared to competing approaches. We also demonstrate the usefulness of the same model on language modeling by introducing a one-shot task on the Penn Treebank.	[Vinyals, Oriol; Blundell, Charles; Lillicrap, Timothy; Kavukcuoglu, Koray; Wierstra, Daan] Google DeepMind, London, England	Google Incorporated	Vinyals, O (corresponding author), Google DeepMind, London, England.	vinyals@google.com; cblundell@google.com; countzero@google.com; korayk@google.com; wierstra@google.com						Atkeson C. G., 1997, ARTIFICIAL INTELLIGE; Bahdanau Dzmitry, 2014, ICLR; Graves A., 2014, NEURAL TURING MACHIN; Hermann K. M., 2015, NIPS; Hill Felix, 2015, ARXIV151102301; Hinton G., 2012, SIGNAL PROCESSING MA; Hochreiter S, 1997, NEURAL COMPUTATION; Hoffer E., 2015, SIMILARITY BASED PAT; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Koch G., 2015, ICML DEEP LEARN WORK; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Krizhevsky A., 2010, CONVOLUTIONAL DEEP B, P1; Lake B., 2011, COGSCI; Marcus Mitchell P., 1993, COMPUT LINGUIST; Mikolov T., 2010, INTERSPEECH; Roweis S., 2004, NIPS; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Salakhutdinov R., 2007, AISTATS; Santoro A., 2016, ICML; Simonyan K., 2015, ICLR; Sutskever I., 2014, NEURIPS; Sutskever I., 2014, ARXIV; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Szegedy C., 2015, ARXIV151200567; Vinyals O., 2015, NIPS; Vinyals Oriol, 2015, ARXIV151106391; Weinberger K. Q., 2009, JMLR; Weston J., 2014, ICLR; Zhang Ning, 2014, ICML	30	0	0	8	109	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703021
C	Vondrick, C; Pirsiavash, H; Torralba, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vondrick, Carl; Pirsiavash, Hamed; Torralba, Antonio			Generating Videos with Scene Dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We capitalize on large amounts of unlabeled video in order to learn a model of scene dynamics for both video recognition tasks (e.g. action classification) and video generation tasks (e.g. future prediction). We propose a generative adversarial network for video with a spatio-temporal convolutional architecture that untangles the scene's foreground from the background. Experiments suggest this model can generate tiny videos up to a second at full frame rate better than simple baselines, and we show its utility at predicting plausible futures of static images. Moreover, experiments and visualizations show the model internally learns useful features for recognizing actions with minimal supervision, suggesting scene dynamics are a promising signal for representation learning. We believe generative video models can impact many applications in video understanding and simulation.	[Vondrick, Carl; Torralba, Antonio] MIT, Cambridge, MA 02139 USA; [Pirsiavash, Hamed] UMBC, Baltimore, MD USA	Massachusetts Institute of Technology (MIT); University System of Maryland; University of Maryland Baltimore County	Vondrick, C (corresponding author), MIT, Cambridge, MA 02139 USA.	vondrick@mit.edu; hpirsiav@umbc.edu; torralba@mit.edu			NSF [1524817]; START program at UMBC; Google PhD fellowship	NSF(National Science Foundation (NSF)); START program at UMBC; Google PhD fellowship(Google Incorporated)	We thank Yusuf Aytar for dataset discussions. We thank MIT TIG, especially Garrett Wollman, for troubleshooting issues on storing the 26 TB of video. We are grateful for the Torch7 community for answering many questions. NVidia donated GPUs used for this research. This work was supported by NSF grant #1524817 to AT, START program at UMBC to HP, and the Google PhD fellowship to CV.	[Anonymous], 2015, GOOD PRACTICES VERY; [Anonymous], 2012, UCF101 DATASET 101 H; Aytar Y., 2016, NIPS; Basha T. D., 2012, ECCV; Chen C., 2013, CVPR; Chen T., 2015, EMPIRICAL EVALUATION; Denton E. L., 2015, NIPS; Doersch Carl, 2015, P INT C COMP VIS ICC; Finn Chelsea, 2016, IAN GOODFELLOW SERGE; Fiser Jozsef, 2002, JEP; Fragkiadaki K., 2015, ICCV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hadsell R., 2006, P CVPR; Hebert M., 2016, ECCV; Ioffe S., 2015, INT C MACH LEARN, P448, DOI [10.5555/3045118.3045167, DOI 10.5555/3045118.3045167]; Isola P., 2015, CVPR; Ji S., 2013, PAMI; Kalchbrenner N., 2016, VIDEO PIXEL NETWORKS; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Kitani K. M, 2012, ECCV; Le Quoc V, 2013, CASSP; Li Yin, 2015, UNSUPERVISED LEARNIN; Lotter W., 2016, ARXIV; Lowe D. G., 1999, ICCV; Mathieu M., 2015, ARXIV; Mirza M., 2014, COMPUT SCI, P2672; Mobahi H., 2009, ICML; Nguyen P. X., 2016, OPEN WORLD MICROVIDE; Owens A., 2016, AMBIENT SOUND PROVID; Pathak D., 2016, CONTEXT ENCODERS FEA; Petrovic Nikola, 2006, CVPR; Pickup L. C., 2014, IEEE C COMP VIS PATT; Radford A., 2015, CVPR; Ramanathan V., 2015, CVPR; Ranzato MarcAurelio, 2014, VIDEO LANGUAGE MODEL; Simonyan Karen, 2014, NIPS; Srivastava N., 2014, JMLR; Srivastava N, 2015, UNSUPERVISED LEARNIN; Theis L., 2015, NOTE EVALUATION GENE; Thomee Bart, 2016, ACM; Tran D, 2014, LEARNING SPATIOTEMPO; Vondrick Carl, 2013, IJCV, P7; Vondrick Carl, 2015, CVPR; Walker J., 2014, CVPR; Wang H., 2013, ICCV; Wang X., 2016, GENERATIVE IMAGE MOD; Wang X., 2015, ICCV; Xue T., 2016, VISUAL DYNAMICS PROB; Yuen J., 2010, ECCV; Zeiler Matthew D., 2010, CVPR; Zhou B., 2014, OBJECT DETECTORS EME; Zhou B., 2014, NIPS; Zhou Y., 2015, ICCV; Zhou Yipin, 2016, ECCV	55	0	0	4	20	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701051
C	Dinh, V; Ho, LST; Nguyen, D; Nguyen, BT		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vu Dinh; Ho, Lam Si Tung; Duy Nguyen; Nguyen, Binh T.			Fast learning rates with heavy-tailed losses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INEQUALITIES; CONVERGENCE	We study fast learning rates when the losses are not necessarily bounded and may have a distribution with heavy tails. To enable such analyses, we introduce two new conditions: (i) the envelope function sup(f) (is an element of) (F) vertical bar l o f vertical bar, where l is the loss function and F is the hypothesis class, exists and is L-r -integrable, and (ii) l satisfies the multi-scale Bernstein's condition on F. Under these assumptions, we prove that learning rate faster than O (n(1/2)) can be obtained and, depending on r and the multi-scale Bernstein's powers, can be arbitrarily close to O (n(-1)). We then verify these assumptions and derive fast learning rates for the problem of vector quantization by k -means clustering with heavy-tailed distributions. The analyses enable us to obtain novel learning rates that extend and complement existing results in the literature from both theoretical and practical viewpoints.	[Vu Dinh] Fred Hutchinson Canc Res Ctr, Program Computat Biol, 1124 Columbia St, Seattle, WA 98104 USA; [Ho, Lam Si Tung] Univ Calif Los Angeles, Dept Biostat, Los Angeles, CA 90024 USA; [Duy Nguyen] Univ Wisconsin, Dept Stat, Madison, WI 53706 USA; [Nguyen, Binh T.] Univ Sci, Dept Comp Sci, Ho Chi Minh City, Vietnam	Fred Hutchinson Cancer Center; University of California System; University of California Los Angeles; University of Wisconsin System; University of Wisconsin Madison	Dinh, V (corresponding author), Fred Hutchinson Canc Res Ctr, Program Computat Biol, 1124 Columbia St, Seattle, WA 98104 USA.				National Science Foundation [DMS-1223057, CISE-1564137]; National Institutes of Health [U54GM111274]; NSF [IIS 1251151]	National Science Foundation(National Science Foundation (NSF)); National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	Vu Dinh was supported by DMS-1223057 and CISE-1564137 from the National Science Foundation and U54GM111274 from the National Institutes of Health. Lam Si Tung Ho was supported by NSF grant IIS 1251151.	Antos A, 2005, IEEE T INFORM THEORY, V51, P4013, DOI 10.1109/TIT.2005.856976; Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P1802, DOI 10.1109/18.705560; Ben-David S, 2007, MACH LEARN, V66, P243, DOI 10.1007/s10994-006-0587-3; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bousquet O, 2004, LECT NOTES ARTIF INT, V3176, P169; Brownlees C, 2015, ANN STAT, V43, P2507, DOI 10.1214/15-AOS1350; Cortes C., 2013, ARXIV13105796; Grunwald Peter, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P169, DOI 10.1007/978-3-642-34106-9_16; Hang H, 2014, J MULTIVARIATE ANAL, V127, P184, DOI 10.1016/j.jmva.2014.02.012; Hsu D, 2016, J MACH LEARN RES, V17; Lecu G., 2013, ARXIV13054825; Lederer J, 2014, BERNOULLI, V20, P2020, DOI 10.3150/13-BEJ549; Levrard C, 2013, ELECTRON J STAT, V7, P1716, DOI 10.1214/13-EJS822; LINDER T, 1994, IEEE T INFORM THEORY, V40, P1728, DOI 10.1109/18.340451; Mehta N.A., 2014, P 27 INT C NEUR INF, P1197; Mendelson S, 2008, J COMPLEXITY, V24, P380, DOI 10.1016/j.jco.2007.09.001; POLLARD D, 1982, ANN PROBAB, V10, P919, DOI 10.1214/aop/1176993713; Steinwart I., 2009, ADV NEURAL INF PROCE, V22, P1768; Telgarsky M. J., 2013, P 26 INT C NEUR INF, P2940; van Erven T, 2015, J MACH LEARN RES, V16, P1793; Dinh V, 2015, LECT NOTES COMPUT SC, V9076, P375, DOI 10.1007/978-3-319-17142-5_32; Zhang T, 2006, IEEE T INFORM THEORY, V52, P1307, DOI 10.1109/TIT.2005.864439; Zhang T, 2006, ANN STAT, V34, P2180, DOI 10.1214/009053606000000704	24	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700068
C	Vuffray, M; Misra, S; Lokhov, AY; Chertkov, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Vuffray, Marc; Misra, Sidhant; Lokhov, Andrey Y.; Chertkov, Michael			Interaction Screening: Efficient and Sample-Optimal Learning of Ising Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				FRAMEWORK; FIELDS	We consider the problem of learning the underlying graph of an unknown Ising model on p spins from a collection of i.i.d. samples generated from the model. We suggest a new estimator that is computationally efficient and requires a number of samples that is near-optimal with respect to previously established information theoretic lower-bound. Our statistical estimator has a physical interpretation in terms of "interaction screening". The estimator is consistent and is efficiently implemented using convex optimization. We prove that with appropriate regularization, the estimator recovers the underlying graph using a number of samples that is logarithmic in the system size p and exponential in the maximum coupling-intensity and maximum node-degree.	[Vuffray, Marc; Lokhov, Andrey Y.; Chertkov, Michael] Los Alamos Natl Lab, Theoret Div T 4, Los Alamos, NM 87545 USA; [Misra, Sidhant] Los Alamos Natl Lab, Theoret Div T 5, Los Alamos, NM 87545 USA; [Lokhov, Andrey Y.; Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM 87545 USA; [Chertkov, Michael] Skolkovo Inst Sci & Technol, Moscow 143026, Russia	United States Department of Energy (DOE); Los Alamos National Laboratory; United States Department of Energy (DOE); Los Alamos National Laboratory; United States Department of Energy (DOE); Los Alamos National Laboratory; Skolkovo Institute of Science & Technology	Vuffray, M (corresponding author), Los Alamos Natl Lab, Theoret Div T 4, Los Alamos, NM 87545 USA.	vuffray@lanl.gov; sidhant@lanl.gov; lokhov@lanl.gov; chertkov@lanl.gov	Vuffray, Marc/AAS-3263-2021; Chertkov, Michael/O-8828-2015; Lokhov, Andrey/AEC-8069-2022	Vuffray, Marc/0000-0001-7999-9897; Chertkov, Michael/0000-0002-6758-515X; Lokhov, Andrey/0000-0003-3269-7263	U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative	U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative(United States Department of Energy (DOE))	We are thankful to Guy Bresler and Andrea Montanari for valuable discussions, comments and insights. The work was supported by funding from the U.S. Department of Energy's Office of Electricity as part of the DOE Grid Modernization Initiative.	Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; [Anonymous], 2009, ADV NEURAL INFORM PR; Bresler G, 2015, ACM S THEORY COMPUT, P771, DOI 10.1145/2746539.2746631; Bresler G, 2013, SIAM J COMPUT, V42, P563, DOI 10.1137/100796029; Bresler Guy, 2014, ADV NEURAL INFORM PR, P1062; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Deka D., 2015, IEEE CONTROL N UNPUB; Eagle N, 2009, P NATL ACAD SCI USA, V106, P15274, DOI 10.1073/pnas.0900282106; He M, 2011, IEEE T SMART GRID, V2, P342, DOI 10.1109/TSG.2011.2129544; Johnson J. K., 2015, J MACHINE LEARNING; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/NMETH.2016, 10.1038/nmeth.2016]; Montanari A, 2015, ELECTRON J STAT, V9, P2370, DOI 10.1214/15-EJS1059; Morcos F, 2011, P NATL ACAD SCI USA, V108, pE1293, DOI 10.1073/pnas.1111471108; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Ricci-Tersenghi F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08015; Roth S, 2005, PROC CVPR IEEE, P860; Roudi Y, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.051915; Schneidman E, 2006, NATURE, V440, P1007, DOI 10.1038/nature04701; Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703011
C	Wan, YL; Meila, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wan, Yali; Meila, Marina			Graph Clustering: Block-models and model free results	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Clustering graphs under the Stochastic Block Model (SBM) and extensions are well studied. Guarantees of correctness exist under the assumption that the data is sampled from a model. In this paper, we propose a framework, in which we obtain "correctness" guarantees without assuming the data comes from a model. The guarantees we obtain depend instead on the statistics of the data that can be checked. We also show that this framework ties in with the existing model-based framework, and that we can exploit results in model-based recovery, as well as strengthen the results existing in that area of research.	[Wan, Yali; Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Wan, YL (corresponding author), Univ Washington, Dept Stat, Seattle, WA 98195 USA.	yaliwan@washington.edu; mmp@stat.washington.edu						Abbe E., 2015, ARXIV150300609; Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Airoldi Edoardo M., 2011, ARXIV11056245; Awasthi Pranjal, 2016, ENCY ALGORITHMS, P331; Bach FR, 2006, J MACH LEARN RES, V7, P1963; Balcan MF, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P767; Ben-David Shai, 2015, CORR; Bhatia R., 2013, MATRIX ANAL, V169; Bilu Yonatan, 2009, CORR; Chung FR, 1997, SPECTRAL GRAPH THEOR, V92; Karrer B, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.046119; Meila M., 2005, P ART INT STAT WORKS; Meila M, 2012, MACH LEARN, V86, P369, DOI 10.1007/s10994-011-5267-2; Ng A.Y., 2002, ADV NEURAL INFORM PR; Peng R., 2015, ARXIV 14112021, P1423; Qin T., 2013, ADV NEURAL INFORM PR, P3120; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Stewart G.W., 1990, MATRIX PERTURBATION, V175; Wan Yali, 2015, ADV NEURAL INFORM PR	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700104
C	Wang, B; Zhu, JJ; Ursu, O; Pourshafeie, A; Batzoglou, S; Kundaje, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Bo; Zhu, Junjie; Ursu, Oana; Pourshafeie, Armin; Batzoglou, Serafim; Kundaje, Anshul			Unsupervised Learning from Noisy Networks with Applications to Hi-C Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CHROMATIN; PRINCIPLES; DOMAINS; GENOME	Complex networks play an important role in a plethora of disciplines in natural sciences. Cleaning up noisy observed networks poses an important challenge in network analysis. Existing methods utilize labeled data to alleviate the noise the noise levels. However, labeled data is usually expensive to collect while unlabeled data can be gathered cheaply. In this paper, we propose an optimization framework to mine useful structures from noisy networks in an unsupervised manner. The key feature of our optimization framework is its ability to utilize local structures as well as global patterns in the network. We extend our method to incorporate multi-resolution networks in order to add further resistance in the presence of high-levels of noise. The framework is generalized to utilize partial labels in order to further enhance the performance. We empirically test the effectiveness of our method in denoising a network by demonstrating an improvement in community detection results on multi-resolution Hi-C data both with and without Capture-C-generated partial labels.	[Wang, Bo; Batzoglou, Serafim; Kundaje, Anshul] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Zhu, Junjie] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Ursu, Oana; Kundaje, Anshul] Stanford Univ, Dept Genet, Stanford, CA 94305 USA; [Pourshafeie, Armin] Stanford Univ, Dept Phys, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University; Stanford University	Wang, B (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	bowang87@stanford.edu	Kundaje, Anshul/Y-2739-2019; Wang, Bo/HDO-6738-2022; Kundaje, Anshul/Y-2598-2019	Kundaje, Anshul/0000-0003-3084-2287; Wang, Bo/0000-0002-9620-3413; Kundaje, Anshul/0000-0003-3084-2287	Stanford Graduate Fellowship; Stanford Genome Training Program [NIH 5T32HG000044-17]; Alfred Sloan Foundation Fellowship; HHMI International Students Research Fellowship; NIH Sidow grant [1R01CA183904-01A1]	Stanford Graduate Fellowship(Stanford University); Stanford Genome Training Program; Alfred Sloan Foundation Fellowship; HHMI International Students Research Fellowship; NIH Sidow grant(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We would also like to thank Nasa Sinnott-Armstrong for initial advice on this project. JZ acknowledges support from Stanford Graduate Fellowship. AP was partially supported by Stanford Genome Training Program: NIH 5T32HG000044-17. AK was supported by the Alfred Sloan Foundation Fellowship. OU is supported by the HHMI International Students Research Fellowship. BW and SB were supported by NIH Sidow grant (1R01CA183904-01A1).	Alexander S., 2003, CLUSTER ENSEMBLES KN; Blondel VD, 2008, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2008/10/P10008; Cabreros I., 2015, 150905121 ARXIV; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Chiquet J., 2011, INFERRING MULTIPLE G; de Laat W, 2013, NATURE, V502, P499, DOI 10.1038/nature12753; Dekker J, 2002, SCIENCE, V295, P1306, DOI 10.1126/science.1067799; Dekker J, 2008, SCIENCE, V319, P1793, DOI 10.1126/science.1152850; Dixon JR, 2012, NATURE, V485, P376, DOI 10.1038/nature11082; Donnelly P., 2000, GENETICS; Dostie J, 2006, GENOME RES, V16, P1299, DOI 10.1101/gr.5571506; Ernst J, 2012, NAT METHODS, V9, P215, DOI 10.1038/nmeth.1906; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Grubert F, 2015, CELL, V162, P1051, DOI 10.1016/j.cell.2015.07.048; Huang J, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-83; Lettice LA, 2003, HUM MOL GENET, V12, P1725, DOI 10.1093/hmg/ddg180; Lieberman-Aiden E, 2009, SCIENCE, V326, P289, DOI 10.1126/science.1181369; Linden G, 2003, IEEE INTERNET COMPUT, V7, P76, DOI 10.1109/MIC.2003.1167344; Liu G., 2013, ROBUST RECOVERY SUBS; Marcotte EM, 1999, SCIENCE, V285, P751, DOI 10.1126/science.285.5428.751; Mifsud B, 2015, NAT GENET, V47, P598, DOI 10.1038/ng.3286; Qin YJ, 2004, HUM MOL GENET, V13, P1213, DOI 10.1093/hmg/ddh141; Rao SSP, 2014, CELL, V159, P1665, DOI 10.1016/j.cell.2014.11.021; Shaw Peter J, 2010, F1000 Biol Rep, V2, DOI 10.3410/B2-18; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Simonis M, 2006, NAT GENET, V38, P1348, DOI 10.1038/ng1896; The ENCODE Project Consortium, 2012, NATURE; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wang B., 2012, COMPUTER VISION PATT; Wang B, 2014, NAT METHODS, V11, P333, DOI [10.1038/NMETH.2810, 10.1038/nmeth.2810]; Wang Bo, 2013, SPARSE SUBSPACE DENO; Yang J, 2013, IEEE DATA MINING, P1151, DOI 10.1109/ICDM.2013.167; Zelnik-Manor L., 2005, SELF TUNING SPECTRAL, P1601; Zhao Z, 2006, NAT GENET, V38, P1341, DOI 10.1038/ng1891	34	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702038
C	Wang, JZ; Wang, WM; Chen, XT; Wang, RG; Gao, W		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Jinzhuo; Wang, Wenmin; Chen, Xiongtao; Wang, Ronggang; Gao, Wen			Deep Alternative Neural Network: Exploring Contexts as Early as Possible for Action Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				RECEPTIVE FIELDS	Contexts are crucial for action recognition in video. Current methods often mine contexts after extracting hierarchical local features and focus on their high-order encoding s. This paper instead explores contexts as early as possible and leverages their evolutions for action recognition. In particular, we introduce a novel architecture called deep alternative neural network (DANN) stacking alternative layers. Each alternative layer consists of a volumetric convolutional layer followed by a recurrent layer. The former acts as local feature learner while the latter is used to collect contexts. Compared with feed-forward neural networks, DANN learns contexts of local features from the very beginning. This setting helps to preserve hierarchical context evolutions which we show are essential to recognize similar actions. Besides, we present an adaptive method to determine the temporal size for network input based on optical flow energy, and develop a volumetric pyramid pooling layer to deal with input clips of arbitrary sizes. We demonstrate the advantages of DANN on two benchmarks HMDB51 and UCF101 and report competitive or superior results to the state-of-the-art.	[Wang, Jinzhuo; Wang, Wenmin; Chen, Xiongtao; Wang, Ronggang] Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China; [Gao, Wen] Peking Univ, Sch Elect Engn & Comp Sci, Beijing, Peoples R China	Peking University; Peking University	Wang, JZ (corresponding author), Peking Univ, Sch Elect & Comp Engn, Beijing, Peoples R China.	jzwang@pku.edu.cn; wangwm@ece.pku.edu.cn; cxt@pku.edu.cn; rgwang@ece.pku.edu.cn; wgao@pku.edu.cn	Wang, Wenmin/W-3511-2019	Wang, Wenmin/0000-0003-2664-4413	Shenzhen Peacock Plan [20130408-183003656]	Shenzhen Peacock Plan	The work was supported by Shenzhen Peacock Plan (20130408-183003656).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Brox T, 2004, LECT NOTES COMPUT SC, V2034, P25, DOI 10.1007/978-3-540-24673-2_3; Collobert R., 2011, NIPS; Dayan P., 2001, THEORETICAL NEUROSCI, P806; Deco G, 2004, EUR J NEUROSCI, V20, P1089, DOI 10.1111/j.1460-9568.2004.03528.x; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gkioxari G, 2015, PROC CVPR IEEE, P759, DOI 10.1109/CVPR.2015.7298676; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; HUBEL DH, 1959, J PHYSIOL-LONDON, V148, P574, DOI 10.1113/jphysiol.1959.sp006308; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kantorov V, 2014, PROC CVPR IEEE, P2593, DOI 10.1109/CVPR.2014.332; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kuehne H, 2011, IEEE I CONF COMP VIS, P2556, DOI 10.1109/ICCV.2011.6126543; Lan ZZ, 2015, PROC CVPR IEEE, P204, DOI 10.1109/CVPR.2015.7298616; Liang M, 2015, ADV NEURAL INFORM PR, P937; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Peng X, 2014, CORR; Peng XJ, 2014, LECT NOTES COMPUT SC, V8693, P581, DOI 10.1007/978-3-319-10602-1_38; Pinheiro PO, 2014, PR MACH LEARN RES, V32; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Soomro K., 2012, ARXIV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Taylor GW, 2010, LECT NOTES COMPUT SC, V6316, P140, DOI 10.1007/978-3-642-15567-3_11; Varol Gul, 2016, ARXIV160404494; Wang H, 2013, IEEE I CONF COMP VIS, P3551, DOI 10.1109/ICCV.2013.441; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang Limin, 2015, ARXIV150702159; WATERS RL, 1972, J ANAT, V111, P191; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zeiler MD, 2013, ARXIV13013557, DOI DOI 10.1007/978-3-319-26532-2_6	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702082
C	Wang, MD; Liu, J; Fang, EX		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Mengdi; Liu, Ji; Fang, Ethan X.			Accelerating Stochastic Composition Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Consider the stochastic composition optimization problem where the objective is a composition of two expected-value functions. We propose a new stochastic first-order method, namely the accelerated stochastic compositional proximal gradient (ASC-PG) method, which updates based on queries to the sampling oracle using two different timescales. The ASC-PG is the first proximal gradient method for the stochastic composition problem that can deal with nonsmooth regularization penalty. We show that the ASC-PG exhibits faster convergence than the best known algorithms, and that it achieves the optimal sample-error complexity in several important special cases. We further demonstrate the application of ASC-PG to reinforcement learning and conduct numerical experiments.	[Wang, Mengdi] Princeton Univ, Princeton, NJ 08544 USA; Univ Rochester, Rochester, NY 14627 USA; Penn State Univ, University Pk, PA 16802 USA	Princeton University; University of Rochester; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Wang, MD (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	mengdiw@princeton.edu; ji.liu.uwisc@gmail.com; xxf13@psu.edu			NSF [CNS-1548078, DMS-10009141]	NSF(National Science Foundation (NSF))	This project is in part supported by NSF grants CNS-1548078 and DMS-10009141.	[Anonymous], 2015, ARXIV150602081; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas DP, 2011, MATH PROGRAM, V129, P163, DOI 10.1007/s10107-011-0472-0; Dai B., 2016, ARXIV160704579; Dentcheva D., 2015, ARXIV150402658; ERMOLIEV Y., 1976, MONOGRAPHS OPTIMIZAT; Ghadimi S., 2015, MATH PROGRAM, V156, P1; Liu B., 2015, 31 C UNC ART INT AMS; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Nedic A, 2001, SIAM J OPTIMIZ, V12, P109, DOI 10.1137/S1052623499362111; Nedic A, 2011, MATH PROGRAM, V129, P225, DOI 10.1007/s10107-011-0468-9; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Rakhlin A., 2012, P 29 INT C MACH LEAR, P449; Shamir O., 2013, P INT C MACH LEARN A, P71; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Wang M., 2016, P WINT SIM C; Wang M., 2015, ARXIV151103760; Wang M., 2016, MATH PROGRAMMING A; Wang MD, 2016, SIAM J OPTIMIZ, V26, P681, DOI 10.1137/130931278; White Adam M, 2016, INT C AUT AG MULT SY	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703074
C	Wang, SL; Fidler, S; Urtasun, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Shenlong; Fidler, Sanja; Urtasun, Raquel			Proximal Deep Structured Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many problems in real-world applications involve predicting continuous-valued random variables that are statistically related. In this paper, we propose a powerful deep structured model that is able to learn complex non-linear functions which encode the dependencies between continuous output variables. We show that inference in our model using proximal methods can be efficiently solved as a feed-foward pass of a special type of deep recurrent neural network. We demonstrate the effectiveness of our approach in the tasks of image denoising, depth refinement and optical flow estimation.	[Wang, Shenlong; Fidler, Sanja; Urtasun, Raquel] Univ Toronto, Toronto, ON, Canada	University of Toronto	Wang, SL (corresponding author), Univ Toronto, Toronto, ON, Canada.	slwang@cs.toronto.edu; fidler@cs.toronto.edu; urtasun@cs.toronto.edu						Belanger D, 2016, ICML; Black M. J, 2005, CVPR; Chambolle A., 2011, JMIV; Chen L.C., 2015, ICML; Chen T, 2015, COMPUTER SCI; Chen Y.-T., 2015, CVPR; Dabov Kostadin, 2007, TIP, P7; Deng J., 2014, ECCV; Domke J., 2012, AISTATS; Eigen D., 2015, ICCV; Fanello S., 2014, CVPR; Fischer P., 2015, CVPR; Gabay D., 1976, COMPUTERS MATH APPL; Geman D., 1995, TIP; He K, 2016, 2016 IEEE C COMP VIS, DOI [10.1109/cvpr.2016.90, 10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2015, ICCV; Hinton G., 2012, DEEP NEURAL NETWORKS; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Ihler A., 2009, AISTATS; Kingma D. P., 2015, 3 INT C LEARN REPR I, P1; Krishnan D., 2009, NIPS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; LeCun Y., 2010, ICML; Mairal J., 2009, ICCV; Martin D. R., 2001, ICCV; Mayer N., 2015, LARGE DATASET TRAIN; Newcombe R. a, 2011, ISMAR; Parikh N., 2014, FDN TRENDS OPTIMIZAT; Ross S., 2011, CVPR; Schmidt U., 2014, CVPR; Schmidt U., 2013, PAMI; Schwing A. G., 2015, FULLY CONNECTED DEEP; Sudderth E., 2010, COMMUNICATIONS ACM; Sutskever I., 2014, NEURIPS; Wang Shenlong, 2014, NIPS; Weiss Y., 2001, NEURAL COMPUTATION; Zach C., 2012, ECCV; Zbontar J., 2015, CVPR; Zheng S., 2015, ICCV; Zoran Daniel, 2011, ICCV	40	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700038
C	Wang, TY; Berthet, Q; Plan, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Tengyao; Berthet, Quentin; Plan, Yaniv			Average-case hardness of RIP certification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				RESTRICTED ISOMETRY PROPERTY; SIGNAL RECOVERY; SPARSE; CLIQUES	The restricted isometry property (RIP) for design matrices gives guarantees for optimal recovery in sparse linear models. It is of high interest in compressed sensing and statistical learning. This property is particularly important for computationally efficient recovery methods. As a consequence, even though it is in general NP-hard to check that RIP holds, there have been substantial efforts to find tractable proxies for it. These would allow the construction of RIP matrices and the polynomial-time verification of RIP given an arbitrary matrix. We consider the framework of average-case certifiers, that never wrongly declare that a matrix is RIP, while being often correct for random instances. While there are such functions which are tractable in a suboptimal parameter regime, we show that this is a computationally hard task in any better regime. Our results are based on a new, weaker assumption on the problem of detecting dense subgraphs.	[Wang, Tengyao; Berthet, Quentin] Ctr Math Sci, Cambridge CB3 0WB, England; [Plan, Yaniv] 1986 Math Rd, Vancouver, BC V6T 1Z2, Canada	University of Cambridge	Wang, TY (corresponding author), Ctr Math Sci, Cambridge CB3 0WB, England.	t.wang@statslab.cam.ac.uk; q.berthet@statslab.cam.ac.uk; yaniv@math.ubc.ca						Alon N, 2007, ACM S THEORY COMPUT, P496, DOI 10.1145/1250790.1250863; Awasthi P., 2015, J MACH LEARN RES COL, V40; Bandeira A. S., 2014, INT MATH RES NOTICES; Bandeira AS, 2013, IEEE T INFORM THEORY, V59, P3448, DOI 10.1109/TIT.2013.2248414; Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Berthet Q., 2013, C LEARN THEOR, P1046; Berthet Q., 2015, DETECTION PLANTED SO; Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127; Bhaskara A, 2010, ACM S THEORY COMPUT, P201; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Blum A, 2003, J ACM, V50, P506, DOI 10.1145/792538.792543; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Bourgain J, 2011, DUKE MATH J, V159, P145, DOI 10.1215/00127094-1384809; Candes E. J., 2006, COMMUNICATIONS PURE, V59, P2006; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Chen Y., 2014, ARXIV14021267; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2011, MATH PROGRAM, V127, P123, DOI 10.1007/s10107-010-0416-0; Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P2197, DOI 10.1073/pnas.0437847100; Feige U, 2003, SIAM J COMPUT, V32, P345, DOI 10.1137/S009753970240118X; Feige U., P 34 ANN ACM S THEOR, P534; Feldman V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P655; Gao Chao, 2014, ARXIV14098565; HAJEK B., 2015, C LEARNING THEORY, P899; Hazan E, 2011, SIAM J COMPUT, V40, P79, DOI 10.1137/090766991; JERRUM M, 1992, RANDOM STRUCT ALGOR, V3, P347, DOI 10.1002/rsa.3240030402; Jordan M.I., 2014, C LEARN THEOR, P921; Juditsky A, 2011, MATH PROGRAM, V127, P57, DOI 10.1007/s10107-010-0417-z; Juels A, 2000, DESIGN CODE CRYPTOGR, V20, P269, DOI 10.1023/A:1008374125234; Koiran P., 2012, ARXIV12110665; Lee K, 2008, INT CONF ACOUST SPEE, P5129; Ma Z., 2013, COMPUTATIONAL BARRIE; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Rauhut H, 2013, MATH INTRO COMPRESSI; Roman V., 2012, COMPRESSED SENSING T; Tillmann Andreas M., 2014, IEEE Transactions on Information Theory, V60, P1248, DOI 10.1109/TIT.2013.2290112; Wang TY, 2016, ANN STAT, V44, P1896, DOI 10.1214/15-AOS1369	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700096
C	Wang, WR; Wang, JL; Garber, D; Srebro, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Weiran; Wang, Jialei; Garber, Dan; Srebro, Nathan			Efficient Globally Convergent Stochastic Optimization for Canonical Correlation Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the stochastic optimization of canonical correlation analysis (CCA), whose objective is nonconvex and does not decouple over training samples. Although several stochastic gradient based optimization algorithms have been recently proposed to solve this problem, no global convergence guarantee was provided by any of them. Inspired by the alternating least squares/power iterations formulation of CCA, and the shift-and-invert preconditioning method for PCA, we propose two globally convergent meta-algorithms for CCA, both of which transform the original problem into sequences of least squares problems that need only be solved approximately. We instantiate the meta-algorithms with state-of-the-art SGD methods and obtain time complexities that significantly improve upon that of previous work. Experimental results demonstrate their superior performance.	[Wang, Weiran; Garber, Dan; Srebro, Nathan] Toyota Technol Inst, Chicago, IL 60637 USA; [Wang, Jialei] Univ Chicago, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago; University of Chicago	Wang, WR (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	weiranwang@ttic.edu; jialei@uchicago.edu; dgarber@ttic.edu; nati@ttic.edu			NSF BIGDATA [1546500]	NSF BIGDATA	Research partially supported by NSF BIGDATA grant 1546500.	[Anonymous], 2015, NIPS; Arora R., 2012, ALLERTON; Balsubramani A., 2013, NIPS; FROSTIG R, 2015, ICML; Garber D., 2015, FAST SIMPLE PCA VIA; Ge R., 2016, EFFICIENT ALGORITHMS; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Golub GH, 1992, LINEAR ALGEBRA SIGNA, P27, DOI 10.1007/978-1-4612-4228-4_3; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Jin C., 2015, ROBUST SHIFT AND INV; JOHNSON R., 2013, NIPS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lu Y., 2014, NIPS; Ma Z., 2015, ICML; Nesterov Y., 2018, APPL OPTIMIZATION; Schmidt Mark, 2013, 00860051 HAL; Shai Shalev-Shwartz, 2013, J MACHINE LEARNING R; Shamir O., 2015, ICML; Snoek C., 2006, MULTIMEDIA; Vinod H.D., 1976, J ECONOMETRICS; Wang W., 2015, ALLERTON; Warmuth M. K., 2008, J MACHINE LEARNING R; Westbury JR, 1994, XRAY MICROBEAM SPEEC; Witten Daniela M, 2009, BIOSTATISTICS; Xie B., 2015, NIPS; Yger F., 2012, ICML	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703095
C	Wang, XY; Dunson, D; Leng, CL		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Xiangyu; Dunson, David; Leng, Chenlei			DECOrrelated feature space partitioning for distributed sparse regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODEL SELECTION; REGULARIZATION	Fitting statistical models is computationally challenging when the sample size or the dimension of the dataset is huge. An attractive approach for down-scaling the problem size is to first partition the dataset into subsets and then fit using distributed algorithms. The dataset can be partitioned either horizontally (in the sample space) or vertically (in the feature space). While the majority of the literature focuses on sample space partitioning, feature space partitioning is more effective when p >> n. Existing methods for partitioning features, however, are either vulnerable to high correlations or inefficient in reducing the model dimension. In this paper, we solve these problems through a new embarrassingly parallel framework named DECO for distributed variable selection and parameter estimation. In DECO, variables are first partitioned and allocated to m distributed workers. The decorrelated subset data within each worker are then fitted via any algorithm designed for high-dimensional problems. We show that by incorporating the decorrelation step, DECO can achieve consistent variable selection and parameter estimation on each subset with (almost) no assumptions. In addition, the convergence rate is nearly minimax optimal for both sparse and weakly sparse models and does NOT depend on the partition number m. Extensive numerical experiments are provided to illustrate the performance of the new framework.	[Wang, Xiangyu; Dunson, David] Duke Univ, Dept Stat Sci, Durham, NC 27708 USA; [Leng, Chenlei] Univ Warwick, Dept Stat, Coventry, W Midlands, England	Duke University; University of Warwick	Wang, XY (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27708 USA.	wwrechard@gmail.com; dunson@stat.duke.edu; C.Leng@warwick.ac.uk						Chen JH, 2008, BIOMETRIKA, V95, P759, DOI 10.1093/biomet/asn034; Cortez P., 2008, USING DATA MINING PR; Dunson, 2014, ADV NEURAL INFORM PR, P2195; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Jia J., 2012, ARXIV12085584; Mairal J., 2012, P 29 INT C INT C MAC, P1835; Meinshausen N, 2010, J R STAT SOC B, V72, P417, DOI 10.1111/j.1467-9868.2010.00740.x; Raskutti Garvesh, 2009, COMM CONTR COMP 2009, P251; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; Scheetz TE, 2006, P NATL ACAD SCI USA, V103, P14429, DOI 10.1073/pnas.0602562103; Scott Steven L, 2013, EFABBAYES 250 C, V16; Song Q., 2014, J ROYAL STAT SOC B; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Trindade Artur, 2014, UCI MACHINE LEARNING; Wang XY, 2015, ADV NEUR IN, V28; Ye F, 2010, J MACH LEARN RES, V11, P3519; Zhang Y., 2012, ADV NEURAL INFORM PR, P1502; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zhou YB, 2014, ADV NEUR IN, V27; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702096
C	Wang, XA; Dasgupta, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Xinan; Dasgupta, Sanjoy			An algorithm for l(1) nearest neighbor search via monotonic embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				OPTIMAL HASHING ALGORITHMS; RECOGNITION; JOHNSON	Fast algorithms for nearest neighbor (NN) search have in large part focused on l(2) distance. Here we develop an approach for l(1) distance that begins with an explicit and exactly distance-preserving embedding of the points into l(2)(2). We show how this can efficiently be combined with random-projection based methods for l(2) NN search, such as locality-sensitive hashing (LSH) or random projection trees. We rigorously establish the correctness of the methodology and show by experimentation using LSH that it is competitive in practice with available alternatives.	[Wang, Xinan; Dasgupta, Sanjoy] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Wang, XA (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	xinan@ucsd.edu; dasgupta@cs.ucsd.edu			National Science Foundation [IIS-1162581]	National Science Foundation(National Science Foundation (NSF))	The authors are grateful to the National Science Foundation for support under grant IIS-1162581.	Achlioptas D., 2001, P ACM SIGMOD SIGACT, P274, DOI DOI 10.1145/375551.375608; Ailon N, 2009, SIAM J COMPUT, V39, P302, DOI 10.1137/060673096; Andoni A., 2005, TECHNICAL REPORT; Andoni A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1203, DOI 10.1145/1109557.1109690; Andoni A, 2008, COMMUN ACM, V51, P117, DOI 10.1145/1327452.1327494; Andoni A, 2006, ANN IEEE SYMP FOUND, P459; Arriaga R. I., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P616, DOI 10.1109/SFFCS.1999.814637; Belongie S, 2002, IEEE T PATTERN ANAL, V24, P509, DOI 10.1109/34.993558; BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Beygelzimer A., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857; Borg I., 1997, MODERN MULTIDIMENSIO; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; Broder AZ, 2000, J COMPUT SYST SCI, V60, P630, DOI 10.1006/jcss.1999.1690; CARDOSOCACHOPO A, 2007, THESIS; Charikar M, 2002, ANN IEEE SYMP FOUND, P551, DOI 10.1109/SFCS.2002.1181979; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dasgupta S, 2015, ALGORITHMICA, V72, P237, DOI 10.1007/s00453-014-9885-5; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Indyk P, 2006, J ACM, V53, P307, DOI 10.1145/1147954.1147955; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; Kleinberg J. M., 1997, STOC, P599; Li P., 2013, ADV NEURAL INFORM PR, P2571; Linial N., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P577, DOI 10.1109/SFCS.1994.365733; Liu T., 2004, NIPS 2004, P825; Omohundro S. M., 1991, NIPS, V40, P175; Rudelson M, 2007, J ACM, V54, DOI 10.1145/1255443.1255449; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239; Uhlmann J. K., 1991, IMPLEMENTING M UNPUB; UHLMANN JK, 1991, INFORM PROCESS LETT, V40, P175, DOI 10.1016/0020-0190(91)90074-R; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701084
C	Wang, YN; Anandkumar, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Yining; Anandkumar, Animashree			Online and Differentially-Private Tensor Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN			Tensor decomposition; tensor power method; online methods; streaming; differential privacy; perturbation analysis		Tensor decomposition is an important tool for big data analysis. In this paper, we resolve many of the key algorithmic questions regarding robustness, memory efficiency, and differential privacy of tensor decomposition. We propose simple variants of the tensor power method which enjoy these strong properties. We present the first guarantees for online tensor power method which has a linear memory requirement. Moreover, we present a noise calibrated tensor power method with efficient privacy guarantees. At the heart of all these guarantees lies a careful perturbation analysis derived in this paper which improves up on the existing results significantly.	[Wang, Yining] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Anandkumar, Animashree] Univ Calif Irvine, Dept EECS, Irvine, CA USA	Carnegie Mellon University; University of California System; University of California Irvine	Wang, YN (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	yiningwa@cs.cmu.edu; a.anandkumar@uci.edu			Microsoft Faculty Fellowship; NSF [CCF-1254106]; ONR [N00014-14-1-0665]; ARO YIP [W911NF-13-1-0084]; AFOSR YIP [FA9550-15-1-0221]	Microsoft Faculty Fellowship(Microsoft); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO YIP; AFOSR YIP(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	A. Anandkumar is supported in part by Microsoft Faculty Fellowship, NSF Career award CCF-1254106, ONR Award N00014-14-1-0665, ARO YIP Award W911NF-13-1-0084 and AFOSR YIP FA9550-15-1-0221.	Anandkumar  A., 2015, P COLT; Anandkumar  A., 2012, NIPS; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Azizzadenesheli  K., 2016, COLT; Bader BW, 2006, ACM T MATH SOFTWARE, V32, P635, DOI 10.1145/1186785.1186794; Balcan M. - F., 2016, COLT; Cirel'son B. S., 1976, Proceedings of the 3rd Japan-USSR Symposium on Probability Theory, P20; Dwork  C., 2014, STOC; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Ge R, 2015, COLT; Hardt  M., 2014, NIPS; Hillar CJ, 2013, J ACM, V60, DOI 10.1145/2512329; Hopkins S. B., 2015, COLT; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Huang  F., 2014, ARXIV14064566; Huang FR, 2015, J MACH LEARN RES, V16, P2797; Janzamin M., 2015, ARXIV150608473; Kamath  G., BOUNDS EXPECTATION M; Kolda TG, 2011, SIAM J MATRIX ANAL A, V32, P1095, DOI 10.1137/100801482; Kuleshov  V., 2015, AISTATS; Laurent B, 2000, ANN STAT, V28, P1302; Montanari  A., 2014, NIPS; Mu C, 2015, SIAM J MATRIX ANAL A, V36, P1638, DOI 10.1137/15M1010890; Stewart G., 1990, MATRIX PERTURBATION; Tomioka R., 2014, ARXIV14071870; Wang  Y., 2014, NIPS; Wang  Y., 2015, NIPS; Zemel Rich, 2013, ICML	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704023
C	Wang, YZ; Miller, DJ; Poskanzer, K; Wang, Y; Tian, L; Yu, GQ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Yizhi; Miller, David J.; Poskanzer, Kira; Wang, Yue; Tian, Lin; Yu, Guoqiang			Graphical Time Warping for Joint Alignment of Multiple Curves	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SERIES DATA	Dynamic time warping (DTW) is a fundamental technique in time series analysis for comparing one curve to another using a flexible time-warping function. However, it was designed to compare a single pair of curves. In many applications, such as in metabolomics and image series analysis, alignment is simultaneously needed for multiple pairs. Because the underlying warping functions are often related, independent application of DTW to each pair is a sub-optimal solution. Yet, it is largely unknown how to efficiently conduct a joint alignment with all warping functions simultaneously considered, since any given warping function is constrained by the others and dynamic programming cannot be applied. In this paper, we show that the joint alignment problem can be transformed into a network flow problem and thus can be exactly and efficiently solved by the max flow algorithm, with a guarantee of global optimality. We name the proposed approach graphical time warping (GTW), emphasizing the graphical nature of the solution and that the dependency structure of the warping functions can be represented by a graph. Modifications of DTW, such as windowing and weighting, are readily derivable within GTW. We also discuss optimal tuning of parameters and hyperparameters in GTW. We illustrate the power of GTW using both synthetic data and a real case study of an astrocyte calcium movie.	[Wang, Yizhi; Wang, Yue; Yu, Guoqiang] Virginia Tech, Blacksburg, VA 24061 USA; [Miller, David J.] Penn State Univ, University Pk, PA 16802 USA; [Poskanzer, Kira] Univ Calif San Francisco, San Francisco, CA 94143 USA; [Tian, Lin] Univ Calif Davis, Davis, CA 95616 USA	Virginia Polytechnic Institute & State University; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park; University of California System; University of California San Francisco; University of California System; University of California Davis	Wang, YZ (corresponding author), Virginia Tech, Blacksburg, VA 24061 USA.	yzwang@vt.edu; djmiller@engr.psu.edu; Kira.Poskanzer@ucsf.edu; yuewang@vt.edu; lintian@ucdavis.edu; yug@vt.edu						Ahuja R. K., 1993, NETWORK FLOWS THEORY; Berndt D. J., 1994, P 3 INT C KNOWL DISC, P359; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chudova D., 2002, P 19 C UNC ART INT M, P134; Felzenszwalb PF, 2011, IEEE T PATTERN ANAL, V33, P721, DOI 10.1109/TPAMI.2010.135; Friedman J., 2001, SPRINGER SERIES STAT, V1; Fu TC, 2011, ENG APPL ARTIF INTEL, V24, P164, DOI 10.1016/j.engappai.2010.09.007; HOGEWEG P, 1984, J MOL EVOL, V20, P175, DOI 10.1007/BF02257378; Ishikawa H., 1998, LECT NOTES COMPUTER, P232, DOI [10.1007/BFb0055670, DOI 10.1007/BFB0055670]; Jeong YS, 2011, PATTERN RECOGN, V44, P2231, DOI 10.1016/j.patcog.2010.09.022; Keogh E, 2005, KNOWL INF SYST, V7, P358, DOI 10.1007/s10115-004-0154-9; Keogh E.J., 2001, P 2001 SIAM INT C DA, P1, DOI [10.1137/1.9781611972719.1, DOI 10.1137/1.9781611972719.1]; Korte B., 2012, COMBINATORIAL OPTIMI, V2; Liao TW, 2005, PATTERN RECOGN, V38, P1857, DOI 10.1016/j.patcog.2005.01.025; Rakthanmanon Thanawin, 2012, KDD, V2012, P262, DOI 10.1145/2339530.2339576; Schmidt FR, 2007, LECT NOTES COMPUT SC, V4679, P39; Shokoohi-Yekta M., 2015, P 2015 SIAM INT C DA, P289, DOI DOI 10.1137/1.9781611974010.33; Sievers F, 2011, MOL SYST BIOL, V7, DOI 10.1038/msb.2011.75; Tsai TH, 2013, BIOINFORMATICS, V29, P2774, DOI 10.1093/bioinformatics/btt461; Uchida S, 2012, INT C PATT RECOG, P2294; Volterra A, 2014, NAT REV NEUROSCI, V15, P327, DOI 10.1038/nrn3725; Wang YX, 2016, I S BIOMED IMAGING, P351, DOI 10.1109/ISBI.2016.7493281	22	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702016
C	Wang, Z; Wei, XX; Stocker, AA; Lee, DD		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Zhuo; Wei, Xue-Xin; Stocker, Alan A.; Lee, Daniel D.			Efficient Neural Codes under Metabolic Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				FISHER INFORMATION; NEURONS; VARIABILITY; POPULATIONS	Neural codes are inevitably shaped by various kinds of biological constraints, e.g. noise and metabolic cost. Here we formulate a coding framework which explicitly deals with noise and the metabolic costs associated with the neural representation of information, and analytically derive the optimal neural code for monotonic response functions and arbitrary stimulus distributions. For a single neuron, the theory predicts a family of optimal response functions depending on the metabolic budget and noise characteristics. Interestingly, the well-known histogram equalization solution can be viewed as a special case when metabolic resources are unlimited. For a pair of neurons, our theory suggests that under more severe metabolic constraints, ON-OFF coding is an increasingly more efficient coding scheme compared to ON-ON or OFF-OFF. The advantage could be as large as one-fold, substantially larger than the previous estimation. Some of these predictions could be generalized to the case of large neural populations. In particular, these analytical results may provide a theoretical basis for the predominant segregation into ON- and OFF-cells in early visual processing areas. Overall, we provide a unified framework for optimal neural codes with monotonic tuning curves in the brain, and makes predictions that can be directly tested with physiology experiments.	[Wang, Zhuo] Univ Penn, Dept Math, Philadelphia, PA 19104 USA; [Wei, Xue-Xin; Stocker, Alan A.] Univ Penn, Dept Psychol, Philadelphia, PA 19104 USA; [Lee, Daniel D.] Univ Penn, Dept Elect & Syst Engn, Philadelphia, PA 19104 USA; [Wang, Zhuo] NYU, Ctr Neural Sci, New York, NY 10003 USA; [Wei, Xue-Xin] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Wei, Xue-Xin] Columbia Univ, Ctr Theoret Neurosci, New York, NY 10027 USA	University of Pennsylvania; University of Pennsylvania; University of Pennsylvania; New York University; Columbia University; Columbia University	Wang, Z (corresponding author), Univ Penn, Dept Math, Philadelphia, PA 19104 USA.; Wang, Z (corresponding author), NYU, Ctr Neural Sci, New York, NY 10003 USA.	wangzhuo@nyu.edu; weixxpku@gmail.com; astocker@sas.upenn.edu; ddlee@seas.upenn.edu						Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308; Aticky JJ, 2011, NETWORK-COMP NEURAL, V22, P4, DOI 10.3109/0954898X.2011.638888; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Attwell D, 2001, J CEREBR BLOOD F MET, V21, P1133, DOI 10.1097/00004647-200110000-00001; Balasubramanian V, 2001, NEURAL COMPUT, V13, P799, DOI 10.1162/089976601300014358; Barlow H, 2001, NETWORK-COMP NEURAL, V12, P241, DOI 10.1088/0954-898X/12/3/301; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Bethge M, 2002, NEURAL COMPUT, V14, P2317, DOI 10.1162/08997660260293247; Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115; Carandini M, 2004, PLOS BIOL, V2, P1483, DOI 10.1371/journal.pbio.0020264; Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; Gjorgjieva J, 2014, J NEUROSCI, V34, P12127, DOI 10.1523/JNEUROSCI.1032-14.2014; Gottschalk A, 2002, NEURAL COMPUT, V14, P527, DOI 10.1162/089976602317250889; Gur M, 2006, CEREB CORTEX, V16, P888, DOI 10.1093/cercor/bhj032; Harper NS, 2004, NATURE, V430, P682, DOI 10.1038/nature02768; Johnson DH, 2004, J COMPUT NEUROSCI, V16, P129, DOI 10.1023/B:JCNS.0000014106.09948.83; Karklin Yan, 2011, Adv Neural Inf Process Syst, V24, P999; Kastner DB, 2015, P NATL ACAD SCI USA, V112, P2533, DOI 10.1073/pnas.1418092112; LAUGHLIN S, 1981, Z NATURFORSCH C, V36, P910; Laughlin SB, 1998, NAT NEUROSCI, V1, P36, DOI 10.1038/236; Levy WB, 1996, NEURAL COMPUT, V8, P531, DOI 10.1162/neco.1996.8.3.531; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Liu Z., 2016, NEUROL SCI, P1; McDonnell MD, 2008, PHYS REV LETT, V101, DOI 10.1103/PhysRevLett.101.058103; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Ratliff CP, 2010, P NATL ACAD SCI USA, V107, P17368, DOI 10.1073/pnas.1005846107; Rieke F, 1995, P ROY SOC B-BIOL SCI, V262, P259, DOI 10.1098/rspb.1995.0204; SCHILLER PH, 1992, TRENDS NEUROSCI, V15, P86, DOI 10.1016/0166-2236(92)90017-3; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; TOLHURST DJ, 1981, EXP BRAIN RES, V41, P414; TOMKO GJ, 1974, BRAIN RES, V79, P405, DOI 10.1016/0006-8993(74)90438-7; von der Twer T, 2001, NETWORK-COMP NEURAL, V12, P395, DOI 10.1088/0954-898X/12/3/309; Wang Z., 2012, ADV NEURAL INF PROCE, V25, P2177; Wassle H, 2004, NAT REV NEUROSCI, V5, P747, DOI 10.1038/nrn1497; Wei Xue-Xin, 2016, NEURAL COMPUTATION; Wei Xue-Xin, 2015, NATURE NEUROSCIENCE	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703106
C	Wei, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wei, Dennis			A Constant-Factor Bi-Criteria Approximation Guarantee for k-means plus	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				FACILITY LOCATION; ALGORITHM	This paper studies the k-means++ algorithm for clustering as well as the class of D-l sampling algorithms to which k-means++ belongs. It is shown that for any constant factor beta > 1, selecting beta k cluster centers by D-l sampling yields a constant-factor approximation to the optimal clustering with k centers, in expectation and without conditions on the dataset. This result extends the previously known O(log k) guarantee for the case beta = 1 to the constant-factor bi-criteria regime. It also improves upon an existing constant-factor bi-criteria result that holds only with constant probability.	[Wei, Dennis] IBM Res, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Wei, D (corresponding author), IBM Res, Yorktown Hts, NY 10598 USA.	dwei@us.ibm.com						Aggarwal A, 2009, LECT NOTES COMPUT SC, V5687, P15, DOI 10.1007/978-3-642-03685-9_2; Ailon N., 2009, PROC 22 ADV NEURAL I, P10; Aloise D, 2009, MACH LEARN, V75, P245, DOI 10.1007/s10994-009-5103-0; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Awasthi Pranjal, 2015, ARXIV150203316, V34, P754; Bandyapadhyay S., 2015, TECHNICAL REPORT; Charikar M, 2002, J COMPUT SYST SCI, V65, P129, DOI 10.1006/jcss.2002.1882; Chen K, 2009, SIAM J COMPUT, V39, P923, DOI 10.1137/070699007; Cohen-Addad V., 2016, TECHNICAL REPORT; Dasgupta S., 2008, CS20080916 DEP COMP; Feldman D., 2007, P 23 ACM S COMP GEOM, P11, DOI DOI 10.1145/1247069.1247072; Friggstad Z., 2016, TECHNICAL REPORT; Inaba M., 1994, Proceedings of the Tenth Annual Symposium on Computational Geometry, P332, DOI 10.1145/177424.178042; Jain AK, 2010, PATTERN RECOGN LETT, V31, P651, DOI 10.1016/j.patrec.2009.09.011; Jain K, 2001, J ACM, V48, P274, DOI 10.1145/375827.375845; Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003; Kumar A, 2010, J ACM, V57, DOI 10.1145/1667053.1667054; Lloyd S. P., 1957, TECHNICAL REPORT; Mahajan M, 2009, LECT NOTES COMPUT SC, V5431, P274, DOI 10.1007/978-3-642-00202-1_24; Makarychev K., 2015, TECHNICAL REPORT; Matousek J, 2000, DISCRETE COMPUT GEOM, V24, P61, DOI 10.1007/s004540010019; Mettu RR, 2004, MACH LEARN, V56, P35, DOI 10.1023/B:MACH.0000033114.18632.e0; Ostrovsky R, 2012, J ACM, V59, DOI 10.1145/2395116.2395117	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702056
C	Wei, ZJ; Adeli, H; Zelinsky, G; Hoai, M; Samaras, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wei, Zijun; Adeli, Hossein; Zelinsky, Gregory; Hoai, Minh; Samaras, Dimitris			Learned Region Sparsity and Diversity Also Predict Visual Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SEARCH	Learned region sparsity has achieved state-of-the-art performance in classification tasks by exploiting and integrating a sparse set of local information into global decisions. The underlying mechanism resembles how people sample information from an image with their eye movements when making similar decisions. In this paper we incorporate the biologically plausible mechanism of Inhibition of Return into the learned region sparsity model, thereby imposing diversity on the selected regions. We investigate how these mechanisms of sparsity and diversity relate to visual attention by testing our model on three different types of visual search tasks. We report state-of-the-art results in predicting the locations of human gaze fixations, even though our model is trained only on image-level labels without object location annotations. Notably, the classification performance of the extended model remains the same as the original. This work suggests a new computational perspective on visual attention mechanisms, and shows how the inclusion of attention-based mechanisms can improve computer vision techniques.	[Wei, Zijun; Zelinsky, Gregory; Hoai, Minh; Samaras, Dimitris] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Adeli, Hossein; Zelinsky, Gregory] SUNY Stony Brook, Dept Psychol, Stony Brook, NY 11794 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Wei, ZJ (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	zijwei@cs.stonybrook.edu; hossein.adelijelodar@stonybrook.edu; gregory.zelinsky@stonybrook.edu; minhhoai@cs.stonybrook.edu; samaras@cs.stonybrook.edu			National Science Foundation [IIS-1161876, IIS-1566248]; Subsample project from the Digiteo Institute, France	National Science Foundation(National Science Foundation (NSF)); Subsample project from the Digiteo Institute, France	This project was partially supported by the National Science Foundation Awards IIS-1161876 and IIS-1566248 and the Subsample project from the Digiteo Institute, France.	Ba J., 2015, ICLR; Borji A., 2013, ICCV; Borji A, 2013, IEEE T PATTERN ANAL, V35, P185, DOI 10.1109/TPAMI.2012.89; Bruce NDB, 2009, J VISION, V9, DOI 10.1167/9.3.5; Bylinskii Zoya, 2016, ARXIV160403605; Dario P., 2012, NATO ADV WORKSH; Ehinger KA, 2009, VIS COGN, V17, P945, DOI 10.1080/13506280902834720; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Fecteau JH, 2006, TRENDS COGN SCI, V10, P382, DOI 10.1016/j.tics.2006.06.011; Gilani S. O., 2015, ICME; Girshick R., 2014, CVPR, DOI 10.1109/CVPR.2014.81; Graves Alex, 2013, ARXIV13080850 CORR; Hoai M., 2014, P BRIT MACH VIS C; Hoai M., 2014, P ACCV; Itti L, 2000, VISION RES, V40, P1489, DOI 10.1016/S0042-6989(99)00163-7; Judd Tilke, 2009, P ICCV; Kanan C, 2009, VIS COGN, V17, P979, DOI 10.1080/13506280902771138; Kannan A., 2007, NIPS; Koch C., 1987, MATTERINTELLIGENCE, P115, DOI [10.1007/978-94-009-3833-5, DOI 10.1007/978-94-009-3833-5, DOI 10.1007/978-94-009-3833-5_5]; Kokkinos I., 2007, RR6317 INRIA; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lazebnik S., 2006, P 2006 IEEE COMP VIS, P2169; Lee T. S., 1999, NIPS; Mnih V., 2014, NIPS; Papadopoulos D. P., 2014, ECCV; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Simonyan Karen, 2015, INT C LEARN REPR; Wei Z., 2016, CVPR; Xu K., 2015, ICML; Zelinsky GJ, 2015, ANN NY ACAD SCI, V1339, P154, DOI 10.1111/nyas.12606; Zelinsky GJ, 2013, PHILOS T R SOC B, V368, DOI 10.1098/rstb.2013.0058; Zhang LY, 2008, J VISION, V8, DOI 10.1167/8.7.32; Zhou B., 2016, CVPR	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703087
C	Weston, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Weston, Jason			Dialog-based Language Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A long-term goal of machine learning research is to build an intelligent dialog agent. Most research in natural language understanding has focused on learning from fixed training sets of labeled data, with supervision either at the word level (tagging, parsing tasks) or sentence level (question answering, machine translation). This kind of supervision is not realistic of how humans learn, where language is both learned by, and used for, communication. In this work, we study dialog-based language learning, where supervision is given naturally and implicitly in the response of the dialog partner during the conversation. We study this setup in two domains: the bAbI dataset of [23] and large-scale question answering from [3]. We evaluate a set of baseline learning strategies on these tasks, and show that a novel model incorporating predictive lookahead is a promising approach for learning from a teacher's response. In particular, a surprising result is that it can learn to answer questions correctly without any reward-based supervision at all.	[Weston, Jason] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Weston, J (corresponding author), Facebook AI Res, New York, NY 10003 USA.	jase@fb.com						[Anonymous], 2015, C EMP METH NAT LANG; Bassiri M. A., 2011, ENGLISH LANGUAGE LIT, V1, P61; Clarke J., 2010, P COMP NAT LANG LEAR; Dodge Jesse, 2015, ARXIV151106931; Higgins R, 2002, STUD HIGH EDUC, V27, P53, DOI 10.1080/03075070120099368; Hixon B., 2015, ACL; Kuhl PK, 2004, NAT REV NEUROSCI, V5, P831, DOI 10.1038/nrn1533; Kuhlmann G., 2004, P AAAI WORKSH SUP CO, P30; Latham AS, 1997, EDUC LEADERSHIP, V54, P86; Lenz I., 2015, ROBOTICS SCI SYSTEMS; MARCUS GF, 1993, COGNITION, V46, P53, DOI 10.1016/0010-0277(93)90022-N; Mikolov Tomas, 2015, ARXIV151108130; Pappu A., 2013, P SIGDIAL, P242; Ranzato MarcAurelio, 2015, ARXIV; Rieser V, 2011, THEOR APPL NAT LANG, P1, DOI 10.1007/978-3-642-24942-6; Schmidhuber J., 1991, INT J NEURAL SYST, V2, P125, DOI [10.1142/S012906579100011X, DOI 10.1142/S012906579100011X]; Sordoni A., 2015, NAACL; Su P.-H., 2015, ARXIV150803391; Sukhbaatar S, 2015, ADV NEUR IN, V28; Wayne G., 2014, NEURAL COMPUT; Werts MG., 1995, J BEHAV EDUC, V5, P55, DOI [10.1007/BF02110214, DOI 10.1007/BF02110214]; Weston J, 2015, ARXIV150205698; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702011
C	Wu, H; Noe, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Hao; Noe, Frank			Spectral Learning of Dynamic Systems from Nonequilibrium Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				OBSERVABLE OPERATOR MODELS	Observable operator models (OOMs) and related models are one of the most important and powerful tools for modeling and analyzing stochastic systems. They exactly describe dynamics of finite-rank systems and can be efficiently and consistently estimated through spectral learning under the assumption of identically distributed data. In this paper, we investigate the properties of spectral learning without this assumption due to the requirements of analyzing large-time scale systems, and show that the equilibrium dynamics of a system can be extracted from nonequilibrium observation data by imposing an equilibrium constraint. In addition, we propose a binless extension of spectral learning for continuous data. In comparison with the other continuous-valued spectral algorithms, the binless algorithm can achieve consistent estimation of equilibrium dynamics with only linear complexity.	[Wu, Hao; Noe, Frank] Free Univ Berlin, Dept Math & Comp Sci, Arnimallee 6, D-14195 Berlin, Germany	Free University of Berlin	Wu, H (corresponding author), Free Univ Berlin, Dept Math & Comp Sci, Arnimallee 6, D-14195 Berlin, Germany.	hao.wu@fu-berlin.de; frank.noe@fu-berlin.de	Noe, Frank/Y-2766-2019		Deutsche Forschungsgemeinschaft [SFB 1114]; European Research Council	Deutsche Forschungsgemeinschaft(German Research Foundation (DFG)); European Research Council(European Research Council (ERC)European Commission)	This work was funded by Deutsche Forschungsgemeinschaft (SFB 1114) and European Research Council (starting grant "pcCells").	Beimel A, 2000, J ACM, V47, P506, DOI 10.1145/337244.337257; Boots B., 2012, THESIS; Boots B., 2010, P 27 INT C MACH LEAR; Bowman G.R., 2013, INTRO MARKOV STATE M; Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsu D., 2005, P 22 C LEARN THEOR C, P964; Huang T. - K., 2013, P 30 INT C MACH LEAR, P630; Jaeger H, 2000, NEURAL COMPUT, V12, P1371, DOI 10.1162/089976600300015411; Jaeger H., 2005, ADV NEURAL INF PROCE, P555; Jaeger H., 2001, 102 GMD GERM NAT RES; Jaeger H., 2012, TECH REP; Jiang N., 2016, P 30 AAAI C ART INT; Littman M. L., 2001, NIPS; Noe F, 2013, J CHEM PHYS, V139, DOI 10.1063/1.4828816; Perez-Hernandez G, 2013, J CHEM PHYS, V139, DOI 10.1063/1.4811489; Prinz JH, 2011, J CHEM PHYS, V134, DOI 10.1063/1.3565032; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Rosencrantz M., 2004, P 21 INT C MACH LEAR; Ruttor A., 2013, P 26 INT C NEUR INF, V26, P2040; Schaudinnus N, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.050602; Shirts M, 2000, SCIENCE, V290, P1903, DOI 10.1126/science.290.5498.1903; Siddiqi S., 2010, J MACHINE LEARNING R, V9, P741; Singh S., 2004, P 20 C UNCERTAINTY A, P512; Svensson A, 2016, JMLR WORKSH CONF PRO, V51, P213; Thon M, 2015, J MACH LEARN RES, V16, P103; Turner R., 2010, W CP, P868; Wiewiora E., 2005, P 22 INT C MACH LEAR, P964; Wu HR, 2015, J APPL PHYS, V117, DOI 10.1063/1.4917218; Zhao MJ, 2009, NEURAL COMPUT, V21, P2687, DOI 10.1162/neco.2009.01-08-687	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701048
C	Wu, HS; Liu, X		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Huasen; Liu, Xin			Double Thompson Sampling for Dueling Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we propose a Double Thompson Sampling (D-TS) algorithm for dueling bandit problems. As its name suggests, D-TS selects both the first and the second candidates according to Thompson Sampling. Specifically, D-TS maintains a posterior distribution for the preference matrix, and chooses the pair of arms for comparison according to two sets of samples independently drawn from the posterior distribution. This simple algorithm applies to general Copeland dueling bandits, including Condorcet dueling bandits as a special case. For general Copeland dueling bandits, we show that D-TS achieves O (K-2 log T) regret. Moreover, using a back substitution argument, we refine the regret to O (K log T + K-2 log log T) in Condorcet dueling bandits and most practical Copeland dueling bandits. In addition, we propose an enhancement of D-TS, referred to as D-TS+, to reduce the regret in practice by carefully breaking ties. Experiments based on both synthetic and real-world data demonstrate that D-TS and D-TS+ significantly improve the overall performance, in terms of regret and robustness.	[Wu, Huasen; Liu, Xin] Univ Calif Davis, Davis, CA 95616 USA	University of California System; University of California Davis	Wu, HS (corresponding author), Univ Calif Davis, Davis, CA 95616 USA.	hswu@ucdavis.edu; xinliu@ucdavis.edu			NSF [CCF-1423542, CNS-1457060, CNS-1547461]	NSF(National Science Foundation (NSF))	This research was supported in part by NSF Grants CCF-1423542, CNS-1457060, and CNS-1547461. The authors would like to thank Prof. R. Srikant (UIUC), Prof. Shipra Agrawal (Columbia University), Masrour Zoghi (University of Amsterdam), and Dr. Junpei Komiyama (University of Tokyo) for their helpful discussions and suggestions.	Agrawal S., 2013, ARTIF INTELL, P99; Agrawal S., 2012, C LEARN THEOR COLT; Ailon N, 2014, PR MACH LEARN RES, V32, P856; Bubeck S., 2010, THESIS; Gopalan A., 2015, P 28 C LEARNING THEO, P861; Gopalan A, 2014, PR MACH LEARN RES, V32; Jamieson K., 2015, C LEARN THEOR COLT; Komiyama J., 2016, INT C MACH LEARN ICM; Komiyama J., 2015, INT C MACH LEARN ICM; Komiyama J., 2015, P C LEARN THEOR; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Microsoft Research, 2010, MICR LEARN RANK DAT; Russo D., 2014, ARXIV14035341; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Urvoy T., 2013, INT C MACH LEARN, V28, P91; Welsh N., 2012, LARGE SCALE ONLINE L; Xia Y., 2015, INT JOINT C ART INT; Yue Y., 2009, ICML, P1201; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Zoghi M, 2014, WSDM'14: PROCEEDINGS OF THE 7TH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P73, DOI 10.1145/2556195.2556256; Zoghi M, 2014, PR MACH LEARN RES, V32, P10; Zoghi Masrour, 2015, ADV NEURAL INFORM PR, P307	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701014
C	Wu, JJ; Zhang, CK; Xue, TF; Freeman, WT; Tenenbaum, JB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Jiajun; Zhang, Chengkai; Xue, Tianfan; Freeman, William T.; Tenenbaum, Joshua B.			Learning a Probabilistic Latent Space of Object Shapes via 3D Generative-Adversarial Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of 3D object generation. We propose a novel framework, namely 3D Generative Adversarial Network (3D-GAN), which generates 3D objects from a probabilistic space by leveraging recent advances in volumetric convolutional networks and generative adversarial nets. The benefits of our model are three-fold: first, the use of an adversarial criterion, instead of traditional heuristic criteria, enables the generator to capture object structure implicitly and to synthesize high-quality 3D objects; second, the generator establishes a mapping from a low-dimensional probabilistic space to the space of 3D objects, so that we can sample objects without a reference image or CAD models, and explore the 3D object manifold; third, the adversarial discriminator provides a powerful 3D shape descriptor which, learned without supervision, has wide applications in 3D object recognition. Experiments demonstrate that our method generates high-quality 3D objects, and our unsupervisedly learned features achieve impressive performance on 3D object recognition, comparable with those of supervised learning methods.	[Wu, Jiajun; Zhang, Chengkai; Xue, Tianfan; Tenenbaum, Joshua B.] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Freeman, William T.] MIT, CSAIL, Google Res, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Google Incorporated; Massachusetts Institute of Technology (MIT)	Wu, JJ (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	jiajunwu@mit.edu; ckzhang@mit.edu; tfxue@mit.edu; billf@mit.edu; jbt@mit.edu	Wu, JiaJun/GQH-7885-2022; Xue, Tianfan/AAG-5546-2019		NSF [1212849, 1447476]; ONR MURI [N00014-16-1-2007]; Center for Brain, Minds and Machines (NSF STC) [CCF-1231216]; Toyota Research Institute; Adobe; Shell; IARPA MICrONS	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Center for Brain, Minds and Machines (NSF STC); Toyota Research Institute; Adobe; Shell(Royal Dutch Shell); IARPA MICrONS	This work is supported by NSF grants #1212849 and #1447476, ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF STC award CCF-1231216), Toyota Research Institute, Adobe, Shell, IARPA MICrONS, and a hardware donation from Nvidia.	[Anonymous], 2015, ICLR WORKSH; [Anonymous], 2014, ICLR; Bansal A., 2016, P CVPR; Blanz V., 1999, SIGGRAPH; Carlson W. E., 1982, SIGGRAPH; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chaudhuri S, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964930; Chen Ding-Yun, 2003, CGF; Choy C.B., 2016, ECCV; Denton E. L., 2015, NIPS; Dosovitskiy A., 2015, CVPR; Girdhar R., 2016, ECCV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heess N, 2016, NIPS; Huang HB, 2015, COMPUT GRAPH FORUM, V34, P25, DOI 10.1111/cgf.12694; Im Daniel Jiwoong, 2016, ARXIV160205110; Kalogerakis E, 2012, ACM T GRAPHIC, V31, DOI [10.1145/2077341.2077342, 10.1145/2185520.2185551]; Kar A., 2015, CVPR; Kazhdan Michael, 2003, SGP; Kingma D.P., 2015, INT C LEARN REPR, P1; Larsen A., 2016, ICML; Li C., 2016, ARXIV160404382; Li YY, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818071; Lim Joseph J., 2013, ICCV; Maas Andrew L, 2013, P ICML; Maturana Daniel, 2015, IROS; Qi C.R., 2016, CVPR; Radford A., 2016, ICLR; Sedaghat N, 2016, ARXIV PREPRINT ARXIV; Sharma A., 2016, ARXIV160403755; Shi BG, 2015, IEEE SIGNAL PROC LET, V22, P2339, DOI 10.1109/LSP.2015.2480802; Su H., 2015, P INT C COMP VIS; Tangelder JH, 2008, MULTIMED TOOLS APPL, V39, P441, DOI 10.1007/s11042-007-0181-0; VanKaick O., 2011, CGF; Wang X., 2016, ECCV; Wu Jiajun, 2016, ECCV; Wu Z., 2015, CVPR, V1, P2; Xiao J., 2010, CVPR; Xue T., 2012, CVPR; Yan X., 2016, NIPS; Yu X., 2015, CVPR; Zhu J. -Y., 2016, ECCV	42	0	0	5	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700060
C	Wu, SS; Bhojanapalli, S; Sanghavi, S; Dimakis, AG		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Shanshan; Bhojanapalli, Srinadh; Sanghavi, Sujay; Dimakis, Alexandros G.			Single Pass PCA of Matrix Products	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION; ALGORITHMS	In this paper we present a new algorithm for computing a low rank approximation of the product A(T) B by taking only a single pass of the two matrices A and B. The straightforward way to do this is to (a) first sketch A and B individually, and then (b) find the top components using PCA on the sketch. Our algorithm in contrast retains additional summary information about A, B (e.g. row and column norms etc.) and uses this additional information to obtain an improved approximation from the sketches. Our main analytical result establishes a comparable spectral norm guarantee to existing two-pass methods; in addition we also provide results from an Apache Spark implementation(1) that shows better computational and statistical performance on real-world and synthetic evaluation datasets.	[Wu, Shanshan; Sanghavi, Sujay; Dimakis, Alexandros G.] Univ Texas Austin, Austin, TX 78712 USA; [Bhojanapalli, Srinadh] Toyota Technol Inst, Chicago, IL USA	University of Texas System; University of Texas Austin; Toyota Technological Institute - Chicago	Wu, SS (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	shanshan@utexas.edu; srinadh@ttic.edu; sanghavi@mail.utexas.edu; dimakis@austin.utexas.edu	Dimakis, Alexandros G/A-5496-2011; Dimakis, Alexandros G/P-6034-2019	Dimakis, Alexandros G/0000-0002-4244-7033; Dimakis, Alexandros G/0000-0002-4244-7033	NSF [CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000]; ARO YIP [W911NF-14-1-0258]	NSF(National Science Foundation (NSF)); ARO YIP	We thank the anonymous reviewers for their valuable comments. This research has been supported by NSF Grants CCF 1344179, 1344364, 1407278, 1422549, 1302435, 1564000, and ARO YIP W911NF-14-1-0258.	Bhojanapalli S., 2015, P 2015 ANN ACM SIAM, P902, DOI DOI 10.1137/1.9781611973730.62; Boufounos P. T., 2013, SPIE OPTICAL ENG APP; Chen X., 2012, PROC 15 INT C ARTIF, P199, DOI DOI 10.1184/R1/6473711; Chen Yudong, 2013, ARXIV13062979; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen M. B., 2015, ARXIV150702268; Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696; Gittens A., 2016, ARXIV160701335; Jegou H, 2011, IEEE T PATTERN ANAL, V33, P117, DOI 10.1109/TPAMI.2010.57; Karnin Z., 2015, P 28 C LEARN THEOR C, P1129; Ma J., 2009, PROC 26 ANN INT C MA, V382, P681, DOI [DOI 10.1145/1553374.1553462, 10.1145/1553374.1553462]; Ma Z., 2015, ARXIV150608170; Magen A, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1422; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787; Woodruff David P., 2014, ARXIV14114357; Wu S., 2016, GITHUB REPOSITORY SI; Zaharia Matei, 2012, P 9 USENIX C NETW SY	18	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700039
C	Wu, T; Benson, AR; Gleich, DF		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Tao; Benson, Austin R.; Gleich, David F.			General Tensor Spectral Co-clustering for Higher-Order Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Spectral clustering and co-clustering are well-known techniques in data analysis, and recent work has extended spectral clustering to square, symmetric tensors and hypermatrices derived from a network. We develop a new tensor spectral co-clustering method that simultaneously clusters the rows, columns, and slices of a nonnegative three-mode tensor and generalizes to tensors with any number of modes. The algorithm is based on a new random walk model which we call the super-spacey random surfer. We show that our method out-performs state-of-the-art co-clustering methods on several synthetic datasets with ground truth clusters and then use the algorithm to analyze several real-world datasets.	[Wu, Tao; Gleich, David F.] Purdue Univ, W Lafayette, IN 47907 USA; [Benson, Austin R.] Stanford Univ, Stanford, CA 94305 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Stanford University	Wu, T (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	wu577@purdue.edu; arbenson@stanford.edu; dgleich@purdue.edu			NSF [IIS-1422918]; DARPA SIMPLEX; Stanford Graduate Fellowship	NSF(National Science Foundation (NSF)); DARPA SIMPLEX; Stanford Graduate Fellowship(Stanford University)	TW and DFG are supported by NSF IIS-1422918 and DARPA SIMPLEX. ARB is supported by a Stanford Graduate Fellowship.	Bader B.W., 2015, MATLAB TENSOR TOOLBO; Bao B.-K., 2013, P 3 ACM C INT C MULT, P135; Benaim M, 1997, ANN PROBAB, V25, P361; Benson A. R., 2016, CSNA160202102 ARXIV; Benson Austin R, 2015, Proc SIAM Int Conf Data Min, V2015, P118; Boley D, 1998, DATA MIN KNOWL DISC, V2, P325, DOI 10.1023/A:1009740529316; Chung F. R., 2007, ICCM; Chung F.R.K., 1992, SPECTRAL GRAPH THEOR; Chung F, 2005, ANN COMB, V9, P1, DOI 10.1007/s00026-005-0237-z; Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269, DOI 10.1145/502512.502550; Gao B., 2005, P ACM SIGKDD INT C K, P41, DOI DOI 10.1145/1081870.1081879; Ghoshdastidar D, 2015, AAAI CONF ARTIF INTE, P2610; Gleich DF, 2015, SIAM J MATRIX ANAL A, V36, P1507, DOI 10.1137/140985160; Hein M., 2013, P ADV NEUR INF PROC, V26; Huang H., 2008, P 14 ACM SIGKDD INT, P327; Jegelka S, 2009, LECT NOTES ARTIF INT, V5809, P368, DOI 10.1007/978-3-642-04414-4_30; Kivela M, 2014, J COMPLEX NETW, V2, P203, DOI 10.1093/comnet/cnu016; Meila M., 2001, INT WORKSH ART INT S; MIHAIL M, 1989, ANN IEEE SYMP FOUND, P526, DOI 10.1109/SFCS.1989.63529; Ni JC, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P835, DOI 10.1145/2783258.2783262; Papalexakis EE, 2011, INT CONF ACOUST SPEE, P2064; Pechenick EA, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0137041; Ragnarsson S, 2013, LINEAR ALGEBRA APPL, V438, P853, DOI 10.1016/j.laa.2011.04.014; Schaeffer SE, 2007, COMPUT SCI REV, V1, P27, DOI 10.1016/j.cosrev.2007.05.001; Stewart WJ, 1994, INTRO NUMERICAL SOLU; Wagner D., 1993, Mathematical Foundations of Computer Science 1993. 18th International Symposium, MFCS '93 Proceedings, P744; Zhou D., 2007, P 24 INT C MACHINE L, P1159, DOI DOI 10.1145/1273496.1273642	27	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703012
C	Xu, J; Hsu, D; Maleki, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xu, Ji; Hsu, Daniel; Maleki, Arian			Global Analysis of Expectation Maximization for Mixtures of Two Gaussians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MAXIMUM-LIKELIHOOD; CONVERGENCE PROPERTIES; LEARNING MIXTURES; EM ALGORITHM	E xpectation Maximization (EM) is among the most popular algorithms for estimating parameters of statistical models. However, EM, which is an iterative algorithm based on the maximum likelihood principle, is generally only guaranteed to find stationary points of the likelihood objective, and these points may be far from any maximizer. This article addresses this disconnect between the statistical principles behind EM and its algorithmic properties. Specifically, it provides a global analysis of EM for specific models in which the observations comprise an i.i.d. sample from a mixture of two Gaussians. This is achieved by (i) studying the sequence of parameters from idealized execution of EM in the infinite sample limit, and fully characterizing the limit points of the sequence in terms of the initial parameters; and then (ii) based on this convergence analysis, establishing statistical consistency (or lack thereof) for the actual sequence of parameters produced by EM.	[Xu, Ji; Hsu, Daniel; Maleki, Arian] Columbia Univ, New York, NY 10027 USA	Columbia University	Xu, J (corresponding author), Columbia Univ, New York, NY 10027 USA.	jixu@cs.columbia.edu; djhsu@cs.columbia.edu; arian@stat.columbia.edu			NSF [CCF-1420328, DMREF-1534910]; Sloan Fellowship	NSF(National Science Foundation (NSF)); Sloan Fellowship(Alfred P. Sloan Foundation)	The second named author thanks Yash Deshpande and Sham Kakade for many helpful initial discussions. JX and AM were partially supported by NSF grant CCF-1420328. DH was partially supported by NSF grant DMREF-1534910 and a Sloan Fellowship.	Achlioptas D, 2005, LECT NOTES COMPUT SC, V3559, P458, DOI 10.1007/11503415_31; Arora S, 2005, ANN APPL PROBAB, V15, P69, DOI 10.1214/105051604000000512; Balakrishnan Sivaraman, 2014, ARXIV14082156; BARKAI N, 1994, PHYS REV E, V50, P1766, DOI 10.1103/PhysRevE.50.1766; Belkin M, 2010, ANN IEEE SYMP FOUND, P103, DOI 10.1109/FOCS.2010.16; Brubaker S. C., 2008, 49 ANN IEEE S FDN CO; Chaudhuri K., 2008, COLT, V4, P9; Chaudhuri K, 2009, ICML; Chaudhuri K., 2009, ABS09120086 CORR; Chr?tien S., 2008, ESAIM-PROBAB STAT, V12, P308, DOI DOI 10.1051/PS:2007041; Conniffe D., 1987, J ROY STAT SOC D-STA, V36, P317; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fisher R.A., 1922, PHILOS T R SOC LON A, V222, P309, DOI [DOI 10.1098/RSTA.1922.0009, 10.1098/rsta.1922.0009]; Hardt M, 2015, ACM S THEORY COMPUT, P753, DOI 10.1145/2746539.2746579; Hsu D, 2013, 4 INNOVATIONS THEORE; Jin C., 2016, ARXIV160900978; Kalai AT, 2010, ACM S THEORY COMPUT, P553; Kannan R, 2008, SIAM J COMPUT, V38, P1141, DOI 10.1137/S0097539704445925; Klusowski J. M., 2016, ARXIV160802280; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Moitra A, 2010, ANN IEEE SYMP FOUND, P93, DOI 10.1109/FOCS.2010.15; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; REDNER RA, 1984, SIAM REV, V26, P195, DOI 10.1137/1026034; Srebro N, 2007, LECT NOTES COMPUT SC, V4539, P628, DOI 10.1007/978-3-540-72927-3_47; Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073; Vaida F, 2005, STAT SINICA, V15, P831; Vempala S, 2004, J COMPUT SYST SCI, V68, P841, DOI 10.1016/j.jcss.2003.11.008; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060; Xu J., 2016, ARXIV160807630; Xu L, 1996, NEURAL COMPUT, V8, P129, DOI 10.1162/neco.1996.8.1.129; ZAMPETAKIS M., 2016, ARXIV160900368	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700012
C	Xu, P; Gu, QQ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xu, Pan; Gu, Quanquan			Semiparametric Differential Graph Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INVERSE COVARIANCE ESTIMATION; VARIABLE SELECTION; NETWORKS	In many cases of network analysis, it is more attractive to study how a network varies under different conditions than an individual static network. We propose a novel graphical model, namely Latent Differential Graph Model, where the networks under two different conditions are represented by two semiparametric elliptical distributions respectively, and the variation of these two networks (i.e., differential graph) is characterized by the difference between their latent precision matrices. We propose an estimator for the differential graph based on quasi likelihood maximization with nonconvex regularization. We show that our estimator attains a faster statistical rate in parameter estimation than the state-of-the-art methods, and enjoys the oracle property under mild conditions. Thorough experiments on both synthetic and real world data support our theory.	[Xu, Pan; Gu, Quanquan] Univ Virginia, Charlottesville, VA 22903 USA	University of Virginia	Xu, P (corresponding author), Univ Virginia, Charlottesville, VA 22903 USA.	px3ds@virginia.edu; qg5w@virginia.edu	Xu, Pan/AAH-3620-2019; X, Pan/GVS-4402-2022	Xu, Pan/0000-0002-2559-8622; 	NSF [III-1618948]	NSF(National Science Foundation (NSF))	We would like to thank the anonymous reviewers for their helpful comments. Research was supported by NSF grant III-1618948.	Bandyopadhyay S, 2010, SCIENCE, V330, P1385, DOI 10.1126/science.1195618; Barber R. F., 2015, ARXIV150207641; Basso K, 2005, NAT GENET, V37, P382, DOI 10.1038/ng1532; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bellail AC, 2009, REV RECENT CLIN TRIA, V4, P34, DOI 10.2174/157488709787047530; Carter SL, 2004, BIOINFORMATICS, V20, P2242, DOI 10.1093/bioinformatics/bth234; Chiquet J, 2011, STAT COMPUT, V21, P537, DOI 10.1007/s11222-010-9191-2; Danaher P, 2014, J R STAT SOC B, V76, P373, DOI 10.1111/rssb.12033; de la Fuente A, 2010, TRENDS GENET, V26, P326, DOI 10.1016/j.tig.2010.05.001; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; FAZAYELI F, 2016, ARXIV160605302; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Guo J., 2011, BIOMETRIKA; HE S, 2015, BMC BIOINFORMATICS, V16, DOI DOI 10.1186/S12864-015-1217-X; Hudson NJ, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000382; Kanehisa M, 2000, NUCLEIC ACIDS RES, V28, P27, DOI 10.1093/nar/28.1.27; Kanehisa M, 2012, NUCLEIC ACIDS RES, V40, pD109, DOI 10.1093/nar/gkr988; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Liu H, 2009, J MACH LEARN RES, V10, P2295; Liu Han, 2012, NIPS; LIU S, 2014, ARXIV14070581; LOH P. -L., 2013, NIPS; Marchini S, 2013, EUR J CANCER, V49, P520, DOI 10.1016/j.ejca.2012.06.026; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Oshlack A, 2010, GENOME BIOL, V11, DOI 10.1186/gb-2010-11-12-220; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Tian DC, 2016, NUCLEIC ACIDS RES, V44, DOI 10.1093/nar/gkw581; Tothill RW, 2008, CLIN CANCER RES, V14, P5198, DOI 10.1158/1078-0432.CCR-08-0196; Vaart A. W., 1998, ASYMPTOTIC STAT; Vucic D, 2007, CLIN CANCER RES, V13, P5995, DOI 10.1158/1078-0432.CCR-07-0729; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Wegkamp M., 2013, ARXIV13056526; YUAN H, 2015, ARXIV151109188; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; ZHANG T, 2014, BIOMETRIKA; Zhao SD, 2014, BIOMETRIKA, V101, P253, DOI 10.1093/biomet/asu009	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704053
C	Xu, P; Yang, JY; Roosta-Khorasani, F; Ree, C; Mahoney, MW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xu, Peng; Yang, Jiyan; Roosta-Khorasani, Farbod; Re, Christopher; Mahoney, Michael W.			Sub-sampled Newton Methods with Non-uniform Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION	We consider the problem of finding the minimizer of a convex function F : R-d -> R of the form F (w) := Sigma(n)(i=1) f(i) (w) + R (w) where a low-rank factorization of del(2) f(i) (w) is readily available. We consider the regime where n >> d. We propose randomized Newton-type algorithms that exploit non-uniform sub-sampling of {del(2) f(i) (w)}(i=1)(n), as well as inexact updates, as means to reduce the computational complexity, and are applicable to a wide range of problems in machine learning. Two non-uniform sampling distributions based on block norm squares and block partial leverage scores are considered. Under certain assumptions, we show that our algorithms inherit a linear-quadratic convergence rate in w and achieve a lower computational complexity compared to similar existing methods. In addition, we show that our algorithms exhibit more robustness and better dependence on problem specific quantities, such as the condition number. We empirically demonstrate that our methods are at least twice as fast as Newton's methods on several real datasets.	[Xu, Peng; Yang, Jiyan; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA; [Roosta-Khorasani, Farbod; Mahoney, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Stanford University; University of California System; University of California Berkeley	Xu, P (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	pengxu@stanford.edu; jiyan@stanford.edu; farbod@icsi.berkeley.edu; chrismre@cs.stanford.edu; mmahoney@stat.berkeley.edu			Army Research Office; Defense Advanced Research Projects Agency; Intel; Toshiba; Moore Foundation; DARPA [FA8750-14-2-0240, N66001-15-C-4043, FA8750-12-2-0335]; Office of Naval Research [N000141410102, N000141210041, N000141310129]	Army Research Office; Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Intel(Intel Corporation); Toshiba; Moore Foundation(Gordon and Betty Moore Foundation); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Office of Naval Research(Office of Naval Research)	We would like to thank the Army Research Office and the Defense Advanced Research Projects Agency as well as Intel, Toshiba and the Moore Foundation for support along with DARPA through MEMEX (FA8750-14-2-0240), SIMPLEX (N66001-15-C-4043), and XDATA (FA8750-12-2-0335) programs, and the Office of Naval Research (N000141410102, N000141210041 and N000141310129). Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of DARPA, ONR, or the U.S. government.	Agarwal N., 2016, ARXIV160203943, V18, P1; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Bubeck S., 2014, ARXIV PREPRINT ARXIV; Byrd RH, 2011, SIAM J OPTIMIZ, V21, P977, DOI 10.1137/10079923X; DEMBO RS, 1982, SIAM J NUMER ANAL, V19, P400, DOI 10.1137/0719025; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Franeois Kawala E. G., 2013, 4IEME C MODELES LANA, P16; Friedman J., 2001, SPRINGER SERIES STAT, V1; Graf F, 2011, LECT NOTES COMPUT SC, V6892, P607, DOI 10.1007/978-3-642-23629-7_74; Holodnak JT, 2015, SIAM J MATRIX ANAL A, V36, P110, DOI 10.1137/130940116; Kulis B, 2013, FOUND TRENDS MACH LE, V5, P287, DOI 10.1561/2200000019; Lichman M., 2013, UCI MACHINE LEARNING; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Mahoney MW, 2011, RANDOMIZED ALGORITHM; Martens J., 2010, P 27 INT C MACH LEAR, P735; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Pilanci M., 2015, ARXIV150502250; Roosta-Khorasani, 2016, ARXIV PREPRINT ARXIV; Roosta-Khorasani Farbod, 2016, ARXIV160104737; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vinyals O., 2011, ARXIV11114259; Xu P., 2016, ARXIV160700559	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700002
C	Xu, Y; Yan, Y; Lin, QH; Yang, TB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xu, Yi; Yan, Yan; Lin, Qihang; Yang, Tianbao			Homotopy Smoothing for Non-Smooth Problems with Lower Complexity than O(1/epsilon)	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GRADIENT; MINIMIZATION	In this paper, we develop a novel homotopy smoothing (HOPS) algorithm for solving a family of non-smooth problems that is composed of a non-smooth term with an explicit max-structure and a smooth term or a simple non-smooth term whose proximal mapping is easy to compute. The best known iteration complexity for solving such non-smooth optimization problems is O(1/epsilon) without any assumption on the strong convexity. In this work, we will show that the proposed HOPS achieved a lower iteration complexity of (O) over tilde (1/epsilon(1-theta)) with theta is an element of (0, 1] capturing the local sharpness of the objective function around the optimal solutions. To the best of our knowledge, this is the lowest iteration complexity achieved so far for the considered non-smooth optimization problems without strong convexity assumption. The HOPS algorithm employs Nesterov's smoothing technique and Nesterov's accelerated gradient method and runs in stages, which gradually decreases the smoothing parameter in a stage-wise manner until it yields a sufficiently good approximation of the original function. We show that HOPS enjoys a linear convergence for many well-known non-smooth problems (e.g., empirical risk minimization with a piece-wise linear loss function and l(1) norm regularizer, finding a point in a polyhedron, cone programming, etc). Experimental results verify the effectiveness of HOPS in comparison with Nesterov's smoothing algorithm and the primal-dual style of first-order methods.	[Xu, Yi; Yang, Tianbao] Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA; [Yan, Yan] Univ Technol Sydney, QCIS, Sydney, NSW 2007, Australia; [Lin, Qihang] Univ Iowa, Dept Management Sci, Iowa City, IA 52242 USA	University of Iowa; University of Technology Sydney; University of Iowa	Yang, TB (corresponding author), Univ Iowa, Dept Comp Sci, Iowa City, IA 52242 USA.	yi-xu@uiowa.edu; yan.yan-3@student.uts.edu.au; qihang-lin@uiowa.edu; tianbao-yang@uiowa.edu			National Science Foundation [IIS-1463988, IIS-1545995]	National Science Foundation(National Science Foundation (NSF))	We thank the anonymous reviewers for their helpful comments. Y. Xu and T. Yang are partially supported by National Science Foundation (IIS-1463988, IIS-1545995).	[Anonymous], CORR; Attouch H, 2010, MATH OPER RES, V35, P438, DOI 10.1287/moor.1100.0449; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Becker S, 2011, SIAM J IMAGING SCI, V4, P1, DOI 10.1137/090756855; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte Jerome, 2015, CORR; Burke JV, 1996, SIAM J OPTIMIZ, V6, P265, DOI 10.1137/0806015; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514; Gilpin A, 2012, MATH PROGRAM, V133, P279, DOI 10.1007/s10107-010-0430-2; Goebel R, 2008, J CONVEX ANAL, V15, P263; Lan GH, 2011, MATH PROGRAM, V126, P1, DOI 10.1007/s10107-008-0261-6; Lojasiewicz S., 1965, ENSEMBLES SEMIANALYT; Necoara I., 2015, CORR; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Pang JS, 1997, MATH PROGRAM, V79, P299, DOI 10.1007/BF02614322; Rockafellar RT., 1970, CONVEX ANAL; Tseng P., 2008, SIAM J OPTIM; Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997; Yang T.-J., 2016, CORR; Yang Tianbao, 2014, MACH LEARN; Zhang H, 2016, ARXIV160600269; Zhang XH, 2012, J MACH LEARN RES, V13, P3623; Zhi-Quan Luo, 1993, Annals of Operations Research, V46-47, P157	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703043
C	Xue, TF; Wu, JJ; Bouman, KL; Freeman, WT		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xue, Tianfan; Wu, Jiajun; Bouman, Katherine L.; Freeman, William T.			Visual Dynamics: Probabilistic Future Frame Synthesis via Cross Convolutional Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of synthesizing a number of likely future frames from a single input image. In contrast to traditional methods, which have tackled this problem in a deterministic or non-parametric way, we propose to model future frames in a probabilistic manner. Our probabilistic model makes it possible for us to sample and synthesize many possible future frames from a single input image. To synthesize realistic movement of objects, we propose a novel network structure, namely a Cross Convolutional Network; this network encodes image and motion information as feature maps and convolutional kernels, respectively. In experiments, our model performs well on synthetic data, such as 2D shapes and animated game sprites, as well as on real-world video frames. We also show that our model can be applied to visual analogy-making, and present an analysis of the learned network representations.	[Xue, Tianfan; Wu, Jiajun; Bouman, Katherine L.; Freeman, William T.] MIT, Cambridge, MA 02139 USA; [Freeman, William T.] Google Res, Cambridge, MA USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Xue, TF (corresponding author), MIT, Cambridge, MA 02139 USA.	tfxue@mit.edu; jiajunwu@mit.edu; klbouman@mit.edu; billf@mit.edu	Wu, JiaJun/GQH-7885-2022; Xue, Tianfan/AAG-5546-2019		NSF Robust Intelligence [1212849]; NSF Big Data [1447476]; ONR MURI [6923196]; Adobe; Shell Research	NSF Robust Intelligence; NSF Big Data; ONR MURI(MURIOffice of Naval Research); Adobe; Shell Research	The authors thank Yining Wang for helpful discussions. This work is supported by NSF Robust Intelligence 1212849, NSF Big Data 1447476, ONR MURI 6923196, Adobe, and Shell Research. The authors would also like to thank Nvidia for GPU donations.	Agarwala A, 2005, ACM T GRAPHIC, V24, P821, DOI 10.1145/1073204.1073268; [Anonymous], 2016, NIPS; [Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2015, NEURIPS; [Anonymous], 2014, ICLR; Denton E. L., 2015, NIPS; Finn Chelsea, 2016, NIPS; Fleet DJ, 2000, INT J COMPUT VISION, V36, P171, DOI 10.1023/A:1008156202475; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gregor K., 2015, ICML; Hinton Geoffrey E, 1993, COLT, P8; Jia X., 2016, NIPS; Joshi N., 2012, UIST; Kingma D. P., 2014, NIPS; Liao Z, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461950; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Liu Ce, 2009, THESIS, P3; Mathieu Michael, 2016, P INT C LEARN REPR I; Oh J., 2015, NIPS; Pintea S.L, 2014, EUR C COMP VIS; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Radford A., 2016, ICLR; Reed S. E., 2015, P ADV NEUR INF PROC, P1; Roth S., 2005, ICCV, P2281; Schodl A, 2000, COMP GRAPH, P489, DOI 10.1145/344779.345012; Srivastava Nitish, 2015, ICML; Walker J., 2015, ICCV; Walker J., 2014, CVPR; Walker Jacob, 2016, P EUR C COMP VIS ECC; Wang J.Y.A., 1993, CVPR; Weiss Y., 1998, MEMO NO1624, P1; Wexler Yonatan, 2004, CVPR; Xie J., 2016, ARXIV160403650; Xie Jianwen, 2016, ARXIV160600972; Xue Tianfan, 2014, ECCV; Yan XH, 2016, EARTHS FUTURE, V4, P472, DOI 10.1002/2016EF000417; Zhou T., 2016, ECCV	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704076
C	Xue, YX; Li, ZY; Ermon, S; Gomes, CP; Selman, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xue, Yexiang; Li, Zhiyuan; Ermon, Stefano; Gomes, Carla P.; Selman, Bart			Solving Marginal MAP Problems with NP Oracles and Parity Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Arising from many applications at the intersection of decision-making and machine learning, Marginal Maximum A Posteriori (Marginal MAP) problems unify the two main classes of inference, namely maximization (optimization) and marginal inference (counting), and are believed to have higher complexity than both of them. We propose XOR_MMAP, a novel approach to solve the Marginal MAP problem, which represents the intractable counting subproblem with queries to NP oracles, subject to additional parity constraints. XOR_MMAP provides a constant factor approximation to the Marginal MAP problem, by encoding it as a single optimization in a polynomial size of the original problem. We evaluate our approach in several machine learning and decision-making applications, and show that our approach outperforms several state-of-the-art Marginal MAP solvers.	[Xue, Yexiang; Gomes, Carla P.; Selman, Bart] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Li, Zhiyuan] Tsinghua Univ, Inst Interdisciplinary Informat Sci, Beijing, Peoples R China; [Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Li, Zhiyuan] Cornell Univ, Ithaca, NY USA	Cornell University; Tsinghua University; Stanford University; Cornell University	Xue, YX (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	yexiang@cs.cornell.edu; lizhiyuan13@mails.tsinghua.edu.cn; ermon@cs.stanford.edu; gomes@cs.cornell.edu; selman@cs.cornell.edu	li, zhiyuan/HGD-9581-2022		National Science Foundation [0832782, 1522054, 1059284, 1649208]; Future of Life Institute [2015-143902]	National Science Foundation(National Science Foundation (NSF)); Future of Life Institute	This research was supported by National Science Foundation (Awards #0832782, 1522054, 1059284, 1649208) and Future of Life Institute (Grant 2015-143902).	Achlioptas D, 2015, P UNC ART INT; Belle Vaishak, 2015, P 31 UAI C; Chakraborty Supratik, 2015, P 24 INT JOINT C AI; Chavira Mark, 2006, INT J APPROX REASONI; Ermon S., 2013, UAI; Ermon S., 2014, P 31 INT C MACH LEAR; Ermon S., 2013, P ICML 2013 JMLR ORG, P334; Ermon Stefano, 2013, ADV NEURAL INFORM PR, P2085; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Jiang Jiarong, 2011, ADV NEURAL INFORM PR, V24; Lamblin P., 2006, ADV NEURAL INFORM PR, V19; Lee Junkyu, 2016, P 30 AAAI C ART INT; Liu Qiang, 2013, J MACHINE LEARNING R, V14; Marinescu Radu, 2015, P 24 INT C ART INT I; Marinescu Radu, 2014, P 30 C UNC ART INT U; Maua D.D., 2012, P 29 INT C MACH LEAR; Park James D., 2003, P 19 C UNC ART INT U; Park James D., 2004, J ARTIF INT RES; Ping W, 2015, ADV NEUR IN, V28; Sheldon D., 2010, UAI; Xue Shan, 2015, J ARTIF INTELL RES J; Xue Yexiang, 2016, P 15 INT C AUT AG MU	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703098
C	Yan, BW; Sarkar, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yan, Bowei; Sarkar, Purnamrita			On Robustness of Kernel Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONSISTENCY	Clustering is an important unsupervised learning problem in machine learning and statistics. Among many existing algorithms, kernel k-means has drawn much research attention due to its ability to find non-linear cluster boundaries and its inherent simplicity. There are two main approaches for kernel k-means: SVD of the kernel matrix and convex relaxations. Despite the attention kernel clustering has received both from theoretical and applied quarters, not much is known about robustness of the methods. In this paper we first introduce a semidefinite programming relaxation for the kernel clustering problem, then prove that under a suitable model specification, both K-SVD and SDP approaches are consistent in the limit, albeit SDP is strongly consistent, i.e. achieves exact recovery, whereas K-SVD is weakly consistent, i.e. the fraction of misclassified nodes vanish. Also the error bounds suggest that SDP is more resilient towards outliers, which we also demonstrate with experiments.	[Yan, Bowei; Sarkar, Purnamrita] Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Yan, BW (corresponding author), Univ Texas Austin, Dept Stat & Data Sci, Austin, TX 78712 USA.							Amini A. A., 2014, ARXIV14065647; Awasthi Pranjal, 2012, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Proceedings 15th International Workshop, APPROX 2012 and 16th International Workshop, RANDOM 2012, P37, DOI 10.1007/978-3-642-32512-0_4; Cai TT, 2015, ANN STAT, V43, P1027, DOI 10.1214/14-AOS1290; Christmann A, 2007, BERNOULLI, V13, P799, DOI 10.3150/07-BEJ5102; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; De Brabanter K, 2009, LECT NOTES COMPUT SC, V5768, P100, DOI 10.1007/978-3-642-04274-4_11; Debruyne M., 2008, J MACHINE LEARNING R, V9; Debruyne M, 2010, COMPUT STAT DATA AN, V54, P3007, DOI 10.1016/j.csda.2009.08.018; Duan L, 2009, ANN OPER RES, V168, P151, DOI 10.1007/s10479-008-0371-9; El Karoui N, 2010, ANN STAT, V38, P3191, DOI 10.1214/10-AOS801; Kim DW, 2005, PATTERN RECOGN, V38, P607, DOI 10.1016/j.patcog.2004.09.006; Kim J, 2012, J MACH LEARN RES, V13, P2529; Kulis B., 2004, P 10 ACM SIGKDD INT, P551, DOI DOI 10.1145/1014052.1014118; MacQueen J., 1967, 5 BERK S MATH STAT P, V1, P281; Mixon Dustin G, 2016, ARXIV160206612; Ng AY, 2002, ADV NEUR IN, V14, P849; Pamula R., 2011, Proceedings of the Second International Conference on Emerging Applications of Information Technology (EAIT 2011), P253, DOI 10.1109/EAIT.2011.25; Peng JM, 2007, SIAM J OPTIMIZ, V18, P186, DOI 10.1137/050641983; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Steinhaus Hugo, 1957, B ACAD POL SCI, V4, P801; von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640; Xu L., 2006, PROC 21 AAAI C ARTIF, P536; Yang MS, 2004, IEEE T PATTERN ANAL, V26, P434, DOI 10.1109/TPAMI.2004.1265860; Yu Y, 2015, BIOMETRIKA, V102, P315, DOI 10.1093/biomet/asv008	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703025
C	Yan, SB; Chaudhuri, K; Javidi, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yan, Songbai; Chaudhuri, Kamalika; Javidi, Tara			Active Learning from Imperfect Labelers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study active learning where the labeler can not only return incorrect labels but also abstain from labeling. We consider different noise and abstention conditions of the labeler. We propose an algorithm which utilizes abstention responses, and analyze its statistical consistency and query complexity under fairly natural assumptions on the noise and abstention rate of the labeler. This algorithm is adaptive in a sense that it can automatically request less queries with a more informed or less noisy labeler. We couple our algorithm with lower bounds to show that under some technical conditions, it achieves nearly optimal query complexity.	[Yan, Songbai; Chaudhuri, Kamalika; Javidi, Tara] Univ Calif San Diego, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Yan, SB (corresponding author), Univ Calif San Diego, La Jolla, CA 92093 USA.	yansongbai@eng.ucsd.edu; kamalika@cs.ucsd.edu; tjavidi@eng.ucsd.edu			NSF [IIS-1162581, CCF-1513883, CNS-1329819]	NSF(National Science Foundation (NSF))	We thank NSF under IIS-1162581, CCF-1513883, and CNS-1329819 for research support.	[Anonymous], 2013, APPL MATH; Balcan M. -F., 2012, P 25 C LEARN THEOR; Balcan M. F, 2013, COLT; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Beygelzimer A., 2010, NIPS; Beygelzimer Alina, 2016, ARXIV160207265; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; Chen Y., 2015, C LEARN THEOR, P338; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Dasgupta S., 2007, NIPS; DASGUPTA S, 2005, NIPS; Fang M, 2012, INT C PATT RECOG, P2238; Hanneke S, 2007, LECT NOTES COMPUT SC, V4539, P66, DOI 10.1007/978-3-540-72927-3_7; Hegedus T., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, P108, DOI 10.1145/225298.225311; Kaariainen M., 2006, ALT; Kading C, 2015, PROC CVPR IEEE, P4343, DOI 10.1109/CVPR.2015.7299063; Minsker S, 2012, J MACH LEARN RES, V13, P67; Naghshvar M, 2015, IEEE T INFORM THEORY, V61, P4080, DOI 10.1109/TIT.2015.2432101; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; Raginsky M., 2011, NIPS, P1026; Ramdas Aaditya, 2016, P C UNC ART INT; Tsybakov AB, 2004, ANN STAT, V32, P135; Urner R., 2012, ARTIFICIAL INTELLIGE, P1252; Welling M., 2014, ADV NEURAL INFORM PR, V27; Yan Songbai, 2015, COMM CONTR COMP ALL; Zhang C, 2015, ADV NEURAL INFORM PR, P703	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701019
C	Yang, JY; Mahoney, MW; Saunders, MA; Sun, YK		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yang, Jiyan; Mahoney, Michael W.; Saunders, Michael A.; Sun, Yuekai			Feature-distributed sparse regression: a screen-and-clean approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Most existing approaches to distributed sparse regression assume the data is partitioned by samples. However, for high-dimensional data (D >> N), it is more natural to partition the data by features. We propose an algorithm to distributed sparse regression when the data is partitioned by features rather than samples. Our approach allows the user to tailor our general method to various distributed computing platforms by trading-off the total amount of data (in bits) sent over the communication network and the number of rounds of communication. We show that an implementation of our approach is capable of solving l(1)-regularized l(2) regression problems with millions of features in minutes.	[Yang, Jiyan; Saunders, Michael A.] Stanford Univ, Stanford, CA 94305 USA; [Mahoney, Michael W.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Sun, Yuekai] Univ Michigan, Ann Arbor, MI 48109 USA	Stanford University; University of California System; University of California Berkeley; University of Michigan System; University of Michigan	Yang, JY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	jiyan@stanford.edu; mmahoney@stat.berkeley.edu; saunders@stanford.edu; yuekai@umich.edu	Saunders, Michael A/D-1083-2012		Army Research Office; Defense Advanced Research Projects Agency	Army Research Office; Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We would like to thank the Army Research Office and the Defense Advanced Research Projects Agency for providing partial support for this work.	Agarwal A, 2012, ANN STAT, V40, P2452, DOI 10.1214/12-AOS1032; [Anonymous], 2019, STAT LEARNING SPARSI; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chen SSB, 2001, SIAM REV, V43, P129, DOI 10.1137/S1064827596304010; Clarkson K. L., 2013, S THEOR COMP STOC; Demmel J, 2012, 2012 SC COMPANION: HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS (SCC), P1942; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Gittens A., 2016, ARXIV160701335; Lee J., 2015, ARXIV150304337; Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Pilanci M., 2014, ARXIV14110347; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Roosta-Khorasani, 2016, ARXIV PREPRINT ARXIV; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2011, ADV DATA SCI ADAPT, V3, P115, DOI 10.1142/S1793536911000787; Wang X., 2015, J ROYAL STAT SOC B; Wang X., 2016, ARXIV160202575; Woodruff David P., 2014, ARXIV14114357; Zhang YC, 2013, J MACH LEARN RES, V14, P3321	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701044
C	Yang, Y; Aminoff, EM; Tarr, MJ; Kass, RE		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yang, Ying; Aminoff, Elissa M.; Tarr, Michael J.; Kass, Robert E.			A state-space model of cross-region dynamic connectivity in MEG/EEG	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				INVERSE PROBLEM; MEG; BRAIN; EEG	Cross-region dynamic connectivity, which describes the spatio-temporal dependence of neural activity among multiple brain regions of interest (ROIs), can provide important information for understanding cognition. For estimating such connectivity, magnetoencephalography (MEG) and electroencephalography (EEG) are well-suited tools because of their millisecond temporal resolution. However, localizing source activity in the brain requires solving an under-determined linear problem. In typical two-step approaches, researchers first solve the linear problem with generic priors assuming independence across ROIs, and secondly quantify cross-region connectivity. In this work, we propose a one-step state-space model to improve estimation of dynamic connectivity. The model treats the mean activity in individual ROIs as the state variable and describes non-stationary dynamic dependence across ROIs using time-varying auto-regression. Compared with a two-step method, which first obtains the commonly used minimum-norm estimates of source activity, and then fits the auto-regressive model, our state-space model yielded smaller estimation errors on simulated data where the model assumptions held. When applied on empirical MEG data from one participant in a scene-processing experiment, our state-space model also demonstrated intriguing preliminary results, indicating leading and lagged linear dependence between the early visual cortex and a higher-level scene-sensitive region, which could reflect feedforward and feedback information flow within the visual cortex during scene processing.	[Yang, Ying; Tarr, Michael J.; Kass, Robert E.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Aminoff, Elissa M.] Fordham Univ, Bronx, NY 10458 USA	Carnegie Mellon University; Fordham University	Yang, Y (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ying.yang.cnbc.cmu@gmail.com; eaminoff@fordham.edu; michaeltarr@cmu.edu; kass@stat.cmu.edu	Aminoff, Elissa/AAF-7718-2019; Yang, Ying/AAN-2630-2021		National Science Foundation [1439237]; National Institute of Mental Health [RO1 MH64537]; Henry L. Hillman Presidential Fellowship at Carnegie Mellon University	National Science Foundation(National Science Foundation (NSF)); National Institute of Mental Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH)); Henry L. Hillman Presidential Fellowship at Carnegie Mellon University	This work was supported in part by the National Science Foundation Grant 1439237, the National Institute of Mental Health Grant RO1 MH64537, as well as the Henry L. Hillman Presidential Fellowship at Carnegie Mellon University.	Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103; Boyd S, 2004, CONVEX OPTIMIZATION; Cichy R. M, 2016, ARXIV160102970; Dale AM, 2000, NEURON, V26, P55, DOI 10.1016/S0896-6273(00)81138-1; David O, 2006, NEUROIMAGE, V30, P1255, DOI 10.1016/j.neuroimage.2005.10.045; DiCarlo JJ, 2007, TRENDS COGN SCI, V11, P333, DOI 10.1016/j.tics.2007.06.010; Epstein R, 1999, NEURON, V23, P115, DOI 10.1016/S0896-6273(00)80758-8; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Fukushima M, 2015, NEUROIMAGE, V105, P408, DOI 10.1016/j.neuroimage.2014.09.066; Galka A, 2004, NEUROIMAGE, V23, P435, DOI 10.1016/j.neuroimage.2004.02.022; Gramfort A, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00267; Gramfort A, 2012, PHYS MED BIOL, V57, P1937, DOI 10.1088/0031-9155/57/7/1937; HAMALAINEN M, 1993, REV MOD PHYS, V65, P413, DOI 10.1103/RevModPhys.65.413; HAMALAINEN MS, 1994, MED BIOL ENG COMPUT, V32, P35, DOI 10.1007/BF02512476; Lamus C, 2012, NEUROIMAGE, V63, P894, DOI 10.1016/j.neuroimage.2011.11.020; Mattout J, 2006, NEUROIMAGE, V30, P753, DOI 10.1016/j.neuroimage.2005.10.037; Mosher JC, 1999, IEEE T BIO-MED ENG, V46, P245, DOI 10.1109/10.748978; PASCUALMARQUI RD, 1994, INT J PSYCHOPHYSIOL, V18, P49, DOI 10.1016/0167-8760(84)90014-X; Sakkalis V, 2011, COMPUT BIOL MED, V41, P1110, DOI 10.1016/j.compbiomed.2011.06.020; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705007
C	Yang, ZL; Yuan, Y; Wu, YX; Salakhutdinov, R; Cohen, WW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yang, Zhilin; Yuan, Ye; Wu, Yuexin; Salakhutdinov, Ruslan; Cohen, William W.			Review Networks for Caption Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a novel extension of the encoder-decoder framework, called a review network. The review network is generic and can enhance any existing encoder-decoder model: in this paper, we consider RNN decoders with both CNN and RNN encoders. The review network performs a number of review steps with attention mechanism on the encoder hidden states, and outputs a thought vector after each review step; the thought vectors are used as the input of the attention mechanism in the decoder. We show that conventional encoder-decoders are a special case of our framework. Empirically, we show that our framework improves over state-of-the-art encoder-decoder systems on the tasks of image captioning and source code captioning.(1)	[Yang, Zhilin; Yuan, Ye; Wu, Yuexin; Salakhutdinov, Ruslan; Cohen, William W.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Yang, ZL (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	zhiliny@cs.cmu.edu; yey1@cs.cmu.edu; yuexinw@cs.cmu.edu; rsalakhu@cs.cmu.edu; wcohen@cs.cmu.edu			NSF [CCF-1414030, IIS-1250956]; Google; Disney Research; ONR [N000141512791]; ADeLAIDE grant [FA8750-16C-0130-001]	NSF(National Science Foundation (NSF)); Google(Google Incorporated); Disney Research; ONR(Office of Naval Research); ADeLAIDE grant	This work was funded by the NSF under grants CCF-1414030 and IIS-1250956, Google, Disney Research, the ONR grant N000141512791, and the ADeLAIDE grant FA8750-16C-0130-001.	Bahdanau Dzmitry, 2015, ICLR; Chen X, 2015, CORR, V1504, P325; Cho K., 2014, ACL; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Graves A., 2016, ADAPTIVE COMPUTATION; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kumar A., 2016, ICML; Luong M.-T., 2015, ACL; Maddison Chris J., 2014, ICML; Movshovitz-Attias D., 2013, ACL; Rush Alexander M., 2015, EMNLP; Simonyan Karen, 2015, INT C LEARN REPR; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Vinyals Oriol, 2016, ICLR; Weston J., 2015, ICLR; Xu K., 2015, ICML; You Q., 2016, CVPR	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701024
C	Ye, HJ; Zhan, DC; Si, XM; Jiang, Y; Zhou, ZH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ye, Han-Jia; Zhan, De-Chuan; Si, Xue-Min; Jiang, Yuan; Zhou, Zhi-Hua			What Makes Objects Similar: A Unified Multi-Metric Learning Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Linkages are essentially determined by similarity measures that may be derived from multiple perspectives. For example, spatial linkages are usually generated based on localities of heterogeneous data, whereas semantic linkages can come from various properties, such as different physical meanings behind social relations. Many existing metric learning models focus on spatial linkages, but leave the rich semantic factors unconsidered. Similarities based on these models are usually overdetermined on linkages. We propose a Uni fi ed Multi-Metric Learning ((UML)-L-2) framework to exploit multiple types of metrics. In UM2L, a type of combination operator is introduced for distance characterization from multiple perspectives, and thus can introduce fl exibilities for representing and utilizing both spatial and semantic linkages. Besides, we propose a uniform solver for UM2L which is guaranteed to converge. Extensive experiments on diverse applications exhibit the superior classi fi cation performance and comprehensibility of (UML)-L-2. Visualization results also validate its ability on physical meanings discovery.	[Ye, Han-Jia; Zhan, De-Chuan; Si, Xue-Min; Jiang, Yuan; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China	Nanjing University	Ye, HJ (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	yehj@lamda.nju.edu.cn; zhandc@lamda.nju.edu.cn; sixm@lamda.nju.edu.cn; jiangy@lamda.nju.edu.cn; zhouzh@lamda.nju.edu.cn	jiang, anyi/GPT-0379-2022		NSFC [61273301, 61333014]; Collaborative Innovation Center of Novel Software Technology and Industrialization; Tencent Fund	NSFC(National Natural Science Foundation of China (NSFC)); Collaborative Innovation Center of Novel Software Technology and Industrialization; Tencent Fund	This research was supported by NSFC (61273301, 61333014), Collaborative Innovation Center of Novel Software Technology and Industrialization, and Tencent Fund.	Amid E, 2015, PR MACH LEARN RES, V37, P1472; [Anonymous], 2013, 30 INT C MACH LEARN; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Chakrabarti D, 2014, PR MACH LEARN RES, V32, P874; Changpinyo S, 2013, ADV NEURAL INFORM PR, P1511; Duchi J, 2009, J MACH LEARN RES, V10, P2899; Fetaya E, 2015, PR MACH LEARN RES, V37, P162; Frank M, 2012, J MACH LEARN RES, V13, P459; Hu JH, 2015, ACM T KNOWL DISCOV D, V9, DOI 10.1145/2700405; Huang KZ, 2009, IEEE DATA MINING, P189, DOI 10.1109/ICDM.2009.22; Jun Wang, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P223, DOI 10.1007/978-3-642-33460-3_20; Leskovec J., 2012, P 25 INT C NEUR INF, P539, DOI DOI 10.1109/ICDM.2012.159; Noh Yung-kyun, 2010, ADV NEURAL INFORM PR, V23, P1822; Qian Q, 2015, PROC CVPR IEEE, P3716, DOI 10.1109/CVPR.2015.7298995; Shi Y, 2014, AAAI CONF ARTIF INTE, P2078; Wang B, 2012, PROC CVPR IEEE, P2997, DOI 10.1109/CVPR.2012.6248029; Wang Jun, 2012, ADV NEURAL INF PROCE, V25, P1601; Wang W., 2010, PROC 27 INT C MACH L, P1135; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Ying YM, 2012, J MACH LEARN RES, V13, P1; Zhan D. C., 2009, P 26 INT C MACH LEAR, P1225, DOI DOI 10.1145/1553374.1553530; Zhang ML, 2007, PATTERN RECOGN, V40, P2038, DOI 10.1016/j.patcog.2006.12.019; Zhou Z.-H, 2012, ENSEMBLE METHODS FDN, DOI DOI 10.1201/B12207; Zhou ZH, 2016, FRONT COMPUT SCI-CHI, V10, P589, DOI 10.1007/s11704-016-6906-3	25	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701049
C	Yi, XY; Wang, ZR; Yang, ZR; Caramanis, C; Liu, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yi, Xinyang; Wang, Zhaoran; Yang, Zhuoran; Caramanis, Constantine; Liu, Han			More Supervision, Less Computation: Statistical-Computational Tradeoffs in Weakly Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the weakly supervised binary classification problem where the labels are randomly flipped with probability 1 W-alpha. Although there exist numerous algorithms for this problem, it remains theoretically unexplored how the statistical accuracies and computational efficiency of these algorithms depend on the degree of supervision, which is quantified by alpha. In this paper, we characterize the effect of a by establishing the information-theoretic and computational boundaries, namely, the minimax-optimal statistical accuracy that can be achieved by all algorithms, and polynomial-time algorithms under an oracle computational model. For small alpha, our result shows a gap between these two boundaries, which represents the computational price of achieving the information-theoretic boundary due to the lack of supervision. Interestingly, we also show that this gap narrows as alpha increases. In other words, having more supervision, i.e., more correct labels, not only improves the optimal statistical accuracy as expected, but also enhances the computational efficiency for achieving such accuracy.	[Yi, Xinyang; Caramanis, Constantine] Univ Texas Austin, Austin, TX 78712 USA; [Wang, Zhaoran; Yang, Zhuoran; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA	University of Texas System; University of Texas Austin; Princeton University	Yi, XY (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	yixy@utexas.edu; zhaoran@princeton.edu; zy6@princeton.edu; constantine@utexas.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018					Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Berthet Q., 2013, C LEARN THEOR; Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127; Cai TT, 2014, J R STAT SOC B, V76, P349, DOI 10.1111/rssb.12034; Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110; Chen Y., 2014, ARXIV14021267; Deshpande Y., 2014, ADV NEURAL INFORM PR; Fan J., 2016, CURSE HETEROGE UNPUB; Fan JQ, 2012, J R STAT SOC B, V74, P745, DOI 10.1111/j.1467-9868.2012.01029.x; Feldman V., 2013, ACM S THEOR COMP; Feldman V., 2015, ACM S THEOR COMP; FELDMAN V., 2015, ARXIV151209170; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Gao Chao, 2014, ARXIV14098565; Garcia-Garcia D., 2011, P 25 C NEUR INF PROC; Hajek Bruce, 2014, ARXIV14066625; JOHNSTONE IM, 1994, ANN STAT, V22, P271, DOI 10.1214/aos/1176325368; Joulin A., 2012, INT C MACH LEARN; Kearns M., 1993, ACM S THEOR COMP; Ma ZM, 2015, ANN STAT, V43, P1089, DOI 10.1214/14-AOS1300; Nettleton DF, 2010, ARTIF INTELL REV, V33, P275, DOI 10.1007/s10462-010-9156-z; Patrini G., 2016, ARXIV160202450; Ramdas A., 2016, ARXIV160202210; Samworth, 2014, ARXIV14085369; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; Wang Zhaoran, 2015, ARXIV151208861; Zhang Y., 2014, C LEARN THEOR	28	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704043
C	Ying, YM; Wen, LY; Lyu, SW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ying, Yiming; Wen, Longyin; Lyu, Siwei			Stochastic Online AUC Maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Area under ROC (AUC) is a metric which is widely used for measuring the classification performance for imbalanced data. It is of theoretical and practical interest to develop online learning algorithms that maximizes AUC for large-scale data. A specific challenge in developing online AUC maximization algorithm is that the learning objective function is usually defined over a pair of training examples of opposite classes, and existing methods achieves on-line processing with higher space and time complexity. In this work, we propose a new stochastic online algorithm for AUC maximization. In particular, we show that AUC optimization can be equivalently formulated as a convex-concave saddle point problem. From this saddle representation, a stochastic online algorithm (SOLAM) is proposed which has time and space complexity of one datum. We establish theoretical convergence of SOLAM with high probability and demonstrate its effectiveness on standard benchmark datasets.	[Ying, Yiming] SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA; [Wen, Longyin; Lyu, Siwei] SUNY Albany, Dept Comp Sci, Albany, NY 12222 USA	State University of New York (SUNY) System; State University of New York (SUNY) Albany; State University of New York (SUNY) System; State University of New York (SUNY) Albany	Ying, YM (corresponding author), SUNY Albany, Dept Math & Stat, Albany, NY 12222 USA.		Ying, Yiming/AGD-7246-2022	Ying, Yiming/0000-0001-7345-6672				[Anonymous], 2012, ICML; Bach F., 2011, NIPS; Bottou L., 2003, NIPS; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Gao W., 2013, ICML; Gao Wei, 2015, INT JOINT C ART INT; HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Joachims T., 2005, ICML; Kar P., 2013, ICML; Kotlowski Wojciech, 2011, ICML; Mohri M., 2003, NIPS; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; Rakotomamonjy A., 2004, 1 INT WORKSH ROC AN; Wang Y., 2012, COLT; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Ying YM, 2016, NEURAL COMPUT, V28, P743, DOI 10.1162/NECO_a_00817; Zhao Peilin, 2011, ICML; Zinkevich M, 2003, ICML	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700029
C	You, Y; Lian, XR; Liu, J; Yu, HF; Dhillon, IS; Demmel, J; Hsieh, CJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		You, Yang; Lian, XiangRu; Liu, Ji; Yu, Hsiang-Fu; Dhillon, Inderjit S.; Demmel, James; Hsieh, Cho-Jui			Asynchronous Parallel Greedy Coordinate Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we propose and study an Asynchronous parallel Greedy Coordinate Descent (Asy-GCD) algorithm for minimizing a smooth function with bounded constraints. At each iteration, workers asynchronously conduct greedy coordinate descent updates on a block of variables. In the first part of the paper, we analyze the theoretical behavior of Asy-GCD and prove a linear convergence rate. In the second part, we develop an efficient kernel SVM solver based on Asy-GCD in the shared memory multi-core setting. Since our algorithm is fully asynchronous-each core does not need to idle and wait for the other cores-the resulting algorithm enjoys good speedup and outperforms existing multi-core kernel SVM solvers including asynchronous stochastic coordinate descent and multi-core LIBSVM.	[Hsieh, Cho-Jui] Univ Calif Davis, Davis, CA 95616 USA; [Lian, XiangRu; Liu, Ji] Univ Rochester, Rochester, NY 14627 USA; [Yu, Hsiang-Fu; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA; [You, Yang; Demmel, James] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Davis; University of Rochester; University of Texas System; University of Texas Austin; University of California System; University of California Berkeley	You, Y (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	youyang@cs.berkeley.edu; xiangru@yandex.com; jliu@cs.rochester.edu; rofuyu@cs.utexas.edu; inderjit@cs.utexas.edu; demmel@eecs.berkeley.edu; chohsieh@cs.ucdavis.edu			NSF [CNS-1548078, CCF-1320746, IIS-1546459, CCF-1564000]; U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program [DE-SC0010200]; U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research [DE-SC0008700, AC02-05CH11231]; DARPA [HR0011-12-2-0016]; Intel; Google; HP; Huawei; LGE; Nokia; NVIDIA; Oracle; S Samsung; Mathworks; Cray; XSEDE	NSF(National Science Foundation (NSF)); U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program(United States Department of Energy (DOE)); U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research(United States Department of Energy (DOE)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Intel(Intel Corporation); Google(Google Incorporated); HP; Huawei(Huawei Technologies); LGE; Nokia(Nokia Corporation); NVIDIA; Oracle; S Samsung(Samsung); Mathworks; Cray; XSEDE	XL and JL are supported by the NSF grant CNS-1548078. HFY and ISD are supported by the NSF grants CCF-1320746, IIS-1546459 and CCF-1564000. YY and JD are supported by the U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research, Applied Mathematics program under Award Number DE-SC0010200; by the U.S. Department of Energy Office of Science, Office of Advanced Scientific Computing Research under Award Numbers DE-SC0008700 and AC02-05CH11231; by DARPA Award Number HR0011-12-2-0016, Intel, Google, HP, Huawei, LGE, Nokia, NVIDIA, Oracle and S Samsung, Mathworks and Cray. CJH also thank the XSEDE and Nvidia support.	Avron H, 2014, INT PARALL DISTRIB P, DOI 10.1109/IPDPS.2014.31; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Boser B. E., 1992, COLT; Canutescu A., 2003, PROTEIN SCI; CHANG CC, 2000, LIBSVM INTRO BENCHMA; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dean J., 2012, ADV NEURAL INFORM PR, V25; Dhillon I. S., 2011, NIPS; Duchi J. C., 2015, ARXIV150800882; Hsieh C. - J., 2011, KDD; Hsieh C. - J., 2015, INT C MACH LEARN ICM; Hsieh Cho-Jui, 2008, ICML; Hsieh Cho-Jui, 2014, ICML; Joachims T., 1998, MAKING LARGE SCALE S; Li Mu, 2014, P 11 USENIX C OP SYS, P583; Liu J., 2014, ICML; Liu J., 2014, ASYNCHRONOUS STOCHAS; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nutini J., 2015, ICML; Platt J C, 1999, ADV KERNEL METHODS S; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Scherrer C., 2012, ICML; Scherrer C., 2012, NIPS; Wang PW, 2014, J MACH LEARN RES, V15, P1523; Xing E. P., 2015, KDD; Yen I., 2013, KDD; Yu H. - F., 2013, KAIS; Yun H., 2014, VLDB; Zhang H., 2015, ARXIV E PRINTS	32	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700034
C	Yuan, XT; Li, P; Zhang, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yuan, Xiao-Tong; Li, Ping; Zhang, Tong			Exact Recovery of Hard Thresholding Pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHM; SPARSITY	The Hard Thresholding Pursuit (HTP) is a class of truncated gradient descent methods for finding sparse solutions of l(0)-constrained loss minimization problems. The HTP-style methods have been shown to have strong approximation guarantee and impressive numerical performance in high dimensional statistical learning applications. However, the current theoretical treatment of these methods has traditionally been restricted to the analysis of parameter estimation consistency. It remains an open problem to analyze the support recovery performance (a.k.a., sparsistency) of this type of methods for recovering the global minimizer of the original NP-hard problem. In this paper, we bridge this gap by showing, for the first time, that exact recovery of the global sparse minimizer is possible for HTP-style methods under restricted strong condition number bounding conditions. We further show that HTP-style methods are able to recover the support of certain relaxed sparse solutions without assuming bounded restricted strong condition number. Numerical results on simulated data confirms our theoretical predictions.	[Yuan, Xiao-Tong] Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China; [Li, Ping; Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ 08854 USA; [Li, Ping] Rutgers State Univ, Dept CS, Piscataway, NJ 08854 USA	Rutgers State University New Brunswick; Rutgers State University New Brunswick	Yuan, XT (corresponding author), Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China.	xtyuan@nuist.edu.cn; pingli@stat.rutgers.edu; tzhang@stat.rutgers.edu	Zhang, Tong/HGC-1090-2022		 [NSF-Bigdata-1419210];  [NSF-III-1360971];  [ONR-N00014-13-1-0764];  [AFOSR-FA9550-13-1-0137];  [NSFC-61402232];  [NSFC-61522308];  [NSFJP-BK20141003];  [NSF-IIS-1407939];  [NSF-IIS-1250985]	; ; ; ; ; ; ; ; 	Xiao-Tong Yuan and Ping Li were partially supported by NSF-Bigdata-1419210, NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137. Xiao-Tong Yuan is also partially supported by NSFC-61402232, NSFC-61522308, and NSFJP-BK20141003. Tong Zhang is supported by NSF-IIS-1407939 and NSF-IIS-1250985.	Agarwal A., 2010, P 24 ANN C NEUR INF; Bahmani S, 2013, J MACH LEARN RES, V14, P807; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Blumensath T, 2013, IEEE T INFORM THEORY, V59, P3466, DOI 10.1109/TIT.2013.2245716; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278; Jain P., 2016, STRUCTURED SPARSE RE; Jalali A., 2011, P 25 ANN C NEUR INF; Jie Shen, 2016, TIGHT BOUND HARD THR; Kar P., 2014, ADV NEURAL INFORM PR, P685; Li X., 2016, P 33 INT C MACH LEAR; Li Yen-Huan, 2015, P 18 INT C ART INT S; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Nesterov Y., 2018, APPL OPTIMIZATION; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574; Yuan X. -T., 2014, P 31 INT C MACH LEAR; Yuan XT, 2013, IEEE T PATTERN ANAL, V35, P3025, DOI 10.1109/TPAMI.2013.85	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703068
C	Yuan, XT; Li, P; Zhang, T; Liu, QS; Liu, GC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yuan, Xiao-Tong; Li, Ping; Zhang, Tong; Liu, Qingshan; Liu, Guangcan			Learning Additive Exponential Family Graphical Models via l(2,1)-norm Regularized M-Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				COVARIANCE ESTIMATION; SELECTION	We investigate a subclass of exponential family graphical models of which the sufficient statistics are de fined by arbitrary additive forms. We propose two l(2;1)-norm regularized maximum likelihood estimators to learn the model parameters from i.i.d. samples. The first one is a joint MLE estimator which estimates all the parameters simultaneously. The second one is a node-wise conditional MLE estimator which estimates the parameters for each node individually. For both estimators, statistical analysis shows that under mild conditions the extra flexibility gained by the additive exponential family models comes at almost no cost of statistical efficiency. A Monte-Carlo approximation method is developed to efficiently optimize the proposed estimators. The advantages of our estimators over Gaussian graphical models and Nonparanormal estimators are demonstrated on synthetic and real data sets.	[Yuan, Xiao-Tong; Liu, Qingshan; Liu, Guangcan] Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China; [Li, Ping; Zhang, Tong] Rutgers State Univ, Dept Stat, Piscataway, NJ USA; [Li, Ping] Rutgers State Univ, Dept Comp Sci, Piscataway, NJ USA	Rutgers State University New Brunswick; Rutgers State University New Brunswick	Yuan, XT (corresponding author), Nanjing Univ Info Sci & Tech, B DAT Lab, Nanjing 210044, Jiangsu, Peoples R China.	xtyuan@nuist.edu.cn; pingli@stat.rutgers.edu; tzhang@stat.rutgers.edu; qsliu@nuist.edu.cn; gcliu@nuist.edu.cn	Zhang, Tong/HGC-1090-2022; liu, qingqing/HHD-0360-2022; Liu, Qing/GWC-9222-2022		 [NSF-Bigdata-1419210];  [NSF-III-1360971];  [ONR-N00014-13-1-0764];  [AFOSR-FA9550-13-1-0137];  [NSFC-61402232];  [NSFC-61522308];  [NSFJP-BK20141003];  [NSF-IIS-1407939];  [NSF-IIS-1250985];  [NSFC-61532009];  [NSFC-61622305];  [NSFC-61502238];  [NSFJP-BK20160040]	; ; ; ; ; ; ; ; ; ; ; ; 	Xiao-Tong Yuan and Ping Li were partially supported by NSF-Bigdata-1419210, NSF-III-1360971, ONR-N00014-13-1-0764, and AFOSR-FA9550-13-1-0137. Xiao-Tong Yuan is also partially supported by NSFC-61402232, NSFC-61522308, and NSFJP-BK20141003. Tong Zhang is supported by NSF-IIS-1407939 and NSF-IIS-1250985. Qingshan Liu is supported by NSFC-61532009. Guangcan Liu is supported by NSFC-61622305, NSFC-61502238 and NSFJP-BK20160040.	Bach F., 2002, P 16 ANN C NEUR INF; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Candes EJ, 2011, APPL COMPUT HARMON A, V31, P59, DOI 10.1016/j.acha.2010.10.002; Dobra A, 2011, ANN APPL STAT, V5, P969, DOI 10.1214/10-AOAS397; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gu C, 2013, STAT SINICA, V23, P1131, DOI 10.5705/ss.2011.319; Lafferty J, 2012, STAT SCI, V27, P519, DOI 10.1214/12-STS391; Lin LN, 2016, ELECTRON J STAT, V10, P806, DOI 10.1214/16-EJS1126; Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037; Liu H, 2009, J MACH LEARN RES, V10, P2295; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Schmidt M., 2011, ADV NEURAL INFORM PR, P1458; SPEED TP, 1986, ANN STAT, V14, P138, DOI 10.1214/aos/1176349846; Sun S., 2015, P 29 ANN C NEUR INF; Tansey W, 2015, PR MACH LEARN RES, V37, P684; Tseng P., 2008, SIAM J OPTIMIZATION; Vershynin R., 2011, ARXIVABS10113027 COR; Voorman A, 2014, BIOMETRIKA, V101, P85, DOI 10.1093/biomet/ast053; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Xue LZ, 2012, ANN STAT, V40, P2541, DOI 10.1214/12-AOS1041; Yang EH, 2015, J MACH LEARN RES, V16, P3813; Yang Z., 2014, ARXIV14128697; Yuan M, 2010, J MACH LEARN RES, V11, P2261; Zhang C. -H., 2012, ARXIVABS12013302 COR	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700070
C	Yurochkin, M; Nguyen, X		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yurochkin, Mikhail; Nguyen, XuanLong			Geometric Dirichlet Means algorithm for topic inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a geometric algorithm for topic learning and inference that is built on the convex geometry of topics arising from the Latent Dirichlet Allocation (LDA) model and its nonparametric extensions. To this end we study the optimization of a geometric loss function, which is a surrogate to the LDA's likelihood. Our method involves a fast optimization based weighted clustering procedure augmented with geometric corrections, which overcomes the computational and statistical inefficiencies encountered by other techniques based on Gibbs sampling and variational inference, while achieving the accuracy comparable to that of a Gibbs sampler. The topic estimates produced by our method are shown to be statistically consistent under some conditions. The algorithm is evaluated with extensive experiments on simulated and real data.	[Yurochkin, Mikhail; Nguyen, XuanLong] Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Yurochkin, M (corresponding author), Univ Michigan, Dept Stat, Ann Arbor, MI 48109 USA.	moonfolk@umich.edu; xuanlong@umich.edu			 [NSF CAREER DMS-1351362];  [NSF CNS-1409303]	; 	This research is supported in part by grants NSF CAREER DMS-1351362 and NSF CNS-1409303.	Anandkumar A., 2012, ADV NEURAL INFORM PR; Arora S., 2012, ARXIV12124777; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; BLEI DM, 2006, ADV NEURAL INFORM PR; Blei DM, 2006, INT C MACH LEARN ICM, V148, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859]; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Ding C., 2006, P 21 NATL C ARTIFICI, V1, P342; Du Q, 1999, SIAM REV, V41, P637, DOI 10.1137/S0036144599352836; Golubitsky O., 2012, ACM COMMUN COMPUT AL, V46, P57; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hartigan J. A., 1979, Applied Statistics, V28, P100, DOI 10.2307/2346830; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; Jiang K., 2012, P INT C NEUR INF PRO, P3158; Kulis B., 2012, P 29 INT C MACH LEAR; Mcauliffe Jon D., 2008, P ADV NEURAL INFORM, P121; Nguyen XL, 2015, BERNOULLI, V21, P618, DOI 10.3150/13-BEJ582; Pritchard JK, 2000, GENETICS, V155, P945; Tang J, 2014, PR MACH LEARN RES, V32; Teh Y. W., 2006, J AM STAT ASS, V101; Xu W., 2003, P 26 ANN INT ACM SIG, P267, DOI DOI 10.1145/860435.860485	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702079
C	Zanella, G; Betancourt, B; Wallach, H; Miller, J; Zaidi, A; Steorts, RC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zanella, Giacomo; Betancourt, Brenda; Wallach, Hanna; Miller, Jeffrey; Zaidi, Abbas; Steorts, Rebecca C.			Flexible Models for Microclustering with Application to Entity Resolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Most generative models for clustering implicitly assume that the number of data points in each cluster grows linearly with the total number of data points. Finite mixture models, Dirichlet process mixture models, and Pitman-Yor process mixture models make this assumption, as do all other infinitely exchangeable clustering models. However, for some applications, this assumption is inappropriate. For example, when performing entity resolution, the size of each cluster should be unrelated to the size of the data set, and each cluster should contain a negligible fraction of the total number of data points. These applications require models that yield clusters whose sizes grow sublinearly with the size of the data set. We address this requirement by defining the microclustering property and introducing a new class of models that can exhibit this property. We compare models within this class to two commonly used clustering models using four entity-resolution data sets.	[Zanella, Giacomo] Bocconi Univ, Dept Decis Sci, Milan, Italy; [Betancourt, Brenda; Zaidi, Abbas; Steorts, Rebecca C.] Duke Univ, Dept Stat Sci, Durham, NC USA; [Wallach, Hanna] Microsoft Res, New York, NY USA; [Miller, Jeffrey] Harvard Univ, Dept Biostat, Cambridge, MA 02138 USA; [Steorts, Rebecca C.] Duke Univ, Dept Comp Sci, Durham, NC USA	Bocconi University; Duke University; Microsoft; Harvard University; Duke University	Zanella, G (corresponding author), Bocconi Univ, Dept Decis Sci, Milan, Italy.	giacomo.zanella@unibocconi.it; bb222@stat.duke.edu; hanna@dirichlet.net; jwmiller@hsph.harvard.edu; amz19@stat.duke.edu; beka@stat.duke.edu			NSF [SBE-0965436, DMS-1045153, IIS-1320219]; NIH [5R01ES017436-05]; John Templeton Foundation; Foerster-Bernstein Postdoctoral Fellowship; UMass Amherst CIIR; EPSRC	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); John Templeton Foundation; Foerster-Bernstein Postdoctoral Fellowship; UMass Amherst CIIR; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank Tamara Broderick, David Dunson, Merlise Clyde, and Abel Rodriguez for conversations that helped form the ideas in this paper. In particular, Tamara Broderick played a key role in developing the idea of microclustering. We also thank the Human Rights Data Analysis Group for providing us with data. This work was supported in part by NSF grants SBE-0965436, DMS-1045153, and IIS-1320219; NIH grant 5R01ES017436-05; the John Templeton Foundation; the Foerster-Bernstein Postdoctoral Fellowship; the UMass Amherst CIIR; and an EPSRC Doctoral Prize Fellowship.	Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; Broderick T., 2014, NIPS 2014 WORKSH ADV; Christen P., 2012, DATA MATCHING CONCEP, DOI 10.1007/978-3-642-31164-2; Christen P., 2012, IEEE T KNOWLEDGE DAT, V24; Ishwaran H, 2003, STAT SINICA, V13, P1211; KINGMAN JFC, 1978, J LOND MATH SOC, V18, P374, DOI 10.1112/jlms/s2-18.2.374; KOLCHIN VF, 1971, THEOR PROBAB APPL+, V16, P74, DOI 10.1137/1116005; Miller J. W, 2015, ARXIV150206241; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Pitman J., 2006, ECOLE ETE PROBABILIT; Price M, 2013, UPDATED STAT ANAL DO; Price M, 2014, UPDATED STAT ANAL DO; Richardson S, 1997, J ROY STAT SOC B MET, V59, P731, DOI 10.1111/1467-9868.00095; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Steorts R. C., J AM STAT SOC; Steorts RC, 2014, JMLR WORKSH CONF PRO, V33, P922; Steorts RC, 2015, BAYESIAN ANAL, V10, P849, DOI 10.1214/15-BA965SI; Steorts RC, 2014, LECT NOTES COMPUT SC, V8744, P253, DOI 10.1007/978-3-319-11257-2_20; Wallach H. M., 2010, P 13 INT C ART INT S; Winkler W.E., 2006, TECHNICAL REPORT	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702081
C	Zantedeschi, V; Emonet, R; Sebban, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zantedeschi, Valentina; Emonet, Remi; Sebban, Marc			beta-risk: a New Surrogate Risk for Learning from Weakly Labeled Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					During the past few years, the machine learning community has paid attention to developing new methods for learning from weakly labeled data. This field covers different settings like semi-supervised learning, learning with label proportions, multi-instance learning, noise-tolerant learning, etc. This paper presents a generic framework to deal with these weakly labeled scenarios. We introduce the beta-risk as a generalized formulation of the standard empirical risk based on surrogate margin-based loss functions. This risk allows us to express the reliability on the labels and to derive different kinds of learning algorithms. We specifically focus on SVMs and propose a soft margin beta-SVM algorithm which behaves better that the state of the art.	[Zantedeschi, Valentina; Emonet, Remi; Sebban, Marc] Univ Lyon, UJM St Etienne, CNRS, Inst Opt,Grad Sch,Lab Hubert Curien,UMR 5516, F-42023 St Etienne, France	Centre National de la Recherche Scientifique (CNRS); CNRS - Institute for Engineering & Systems Sciences (INSIS); Universite Jean Monnet	Zantedeschi, V (corresponding author), Univ Lyon, UJM St Etienne, CNRS, Inst Opt,Grad Sch,Lab Hubert Curien,UMR 5516, F-42023 St Etienne, France.	valentina.zantedeschi@univ-st-etienne.fr; remi.emonet@univ-st-etienne.fr; marc.sebban@univ-st-etienne.fr			ANR project SOLSTICE [ANR-13-BS02-01]; ANR project LIVES [ANR-15-CE230026-03]	ANR project SOLSTICE(French National Research Agency (ANR)); ANR project LIVES(French National Research Agency (ANR))	We thank the reviewers for their valuable remarks. We also thank the ANR projects SOLSTICE (ANR-13-BS02-01) and LIVES (ANR-15-CE230026-03).	Angluin D., 1988, Machine Learning, V2, P343, DOI 10.1007/BF00116829; Ben-David S, 2012, P 29 INT C MACH LEAR; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Boyd S, 2004, CONVEX OPTIMIZATION; Bruzzone L, 2006, IEEE T GEOSCI REMOTE, V44, P3363, DOI 10.1109/TGRS.2006.877950; Collins M, 2002, MACH LEARN, V48, P253, DOI 10.1023/A:1013912006537; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Domahidi A, 2013, 2013 EUROPEAN CONTROL CONFERENCE (ECC), P3077; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148; Hastie T., 2009, ELEMENTS STAT LEARNI; Joulin A., 2012, ARXIV12066413; Kearns M., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P459, DOI 10.1145/237814.237994; Li YF, 2013, J MACH LEARN RES, V14, P2151; Lichman M., 2013, UCI MACHINE LEARNING; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225; Patrini G., 2016, ARXIV160202450; Patrini Giorgio, 2014, ADV NEURAL INFORM PR, P190; Rosasco L, 2004, NEURAL COMPUT, V16, P1063, DOI 10.1162/089976604773135104; Sheng Victor S, 2008, P 14 ACM SIGKDD INT, P614, DOI DOI 10.1145/1401890.1401965; Zhu X.J, 2005, SEMISUPERVISED LEARN	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701013
C	Zhang, HY; Reddi, SJ; Sra, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhang, Hongyi; Reddi, Sashank J.; Sra, Suvrit			Riemannian SVRG: Fast Stochastic Optimization on Riemannian Manifolds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS; CONVEXITY	We study optimization of finite sums of geodesically smooth functions on Riemannian manifolds. Although variance reduction techniques for optimizing finite-sums have witnessed tremendous attention in the recent years, existing work is limited to vector space problems. We introduce Riemannian SVRG (RSVRG), a new variance reduced Riemannian optimization method. We analyze RSVRG for both geodesically convex and nonconvex (smooth) functions. Our analysis reveals that RSVRG inherits advantages of the usual SVRG method, but with factors depending on curvature of the manifold that influence its convergence. To our knowledge, RSVRG is the first provably fast stochastic Riemannian method. Moreover, our paper presents the first non-asymptotic complexity analysis (novel even for the batch setting) for nonconvex Riemannian optimization. Our results have several implications; for instance, they offer a Riemannian perspective on variance reduced PCA, which promises a short, transparent convergence analysis.	[Zhang, Hongyi; Sra, Suvrit] MIT, Cambridge, MA 02139 USA; [Reddi, Sashank J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Massachusetts Institute of Technology (MIT); Carnegie Mellon University	Zhang, HY (corresponding author), MIT, Cambridge, MA 02139 USA.	hongyiz@mit.edu; sjakkamr@cs.cmu.edu; suvrit@mit.edu			NSF [IIS-1409802]; Leventhal Fellowship	NSF(National Science Foundation (NSF)); Leventhal Fellowship	SS acknowledges support of NSF grant: IIS-1409802. HZ acknowledges support from the Leventhal Fellowship.	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Agarwal A, 2015, PR MACH LEARN RES, V37, P78; Allen-Zhu Zeyuan, 2016, ARXIV160305643; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Cherian A., 2015, ARXIV150702772; Congedo M, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0121423; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Edelman A, 1998, SIAM J MATRIX ANAL A, V20, P303, DOI 10.1137/S0895479895290954; Garber D., 2015, ARXIV150905647; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gong P., 2014, ARXIV PREPRINT ARXIV; Hosseini  R., 2015, NIPS; Jeuris B, 2012, ELECTRON T NUMER ANA, V39, P379; Jin Chi, 2015, ARXIV151008896; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kasai H., 2016, ARXIV160507367; Konecny J., 2013, ARXIV PREPRINT ARXIV; Liu XW, 2004, IEEE T PATTERN ANAL, V26, P662, DOI 10.1109/TPAMI.2004.1273986; Moakher M, 2002, SIAM J MATRIX ANAL A, V24, P1, DOI 10.1137/S0895479801383877; OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9; Petersen P, 2006, RIEMANNIAN GEOMETRY, V171; Reddi S. J., 2016, ARXIV160306160; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Rubinstein R. Y., 2011, SIMULATION MONTE CAR, V707; Schmidt Mark, 2013, ARXIV13092388; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Sra S., 2013, ADV NEURAL INFORM PR, P2562; Sun J., 2015, ARXIV151104777; Tan MK, 2014, PR MACH LEARN RES, V32, P1539; Udriste Constantin, 1994, CONVEX FUNCTIONS OPT, V297; Vandereycken B, 2013, SIAM J OPTIMIZ, V23, P1214, DOI 10.1137/110845768; Wiesel A, 2012, IEEE T SIGNAL PROCES, V60, P6182, DOI 10.1109/TSP.2012.2218241; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yuan XR, 2016, PROCEDIA COMPUT SCI, V80, P2147, DOI 10.1016/j.procs.2016.05.534; Zhang H., 2016, C LEARN THEOR; Zhang T, 2013, IEEE T SIGNAL PROCES, V61, P4141, DOI 10.1109/TSP.2013.2267740	39	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704040
C	Zhang, HS; Liang, YB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhang, Huishuai; Liang, Yingbin			Reshaped Wirtinger Flow for Solving Quadratic System of Equations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GRADIENT SAMPLING ALGORITHM; PHASE RETRIEVAL; NONSMOOTH; RECOVERY	We study the problem of recovering a vector x is an element of R-n from its magnitude measurements y(i) = vertical bar < a(i),x >vertical bar, i = 1, ..., m. Our work is along the line of the Wirtinger flow (WF) approach Candes et al. [2015], which solves the problem by minimizing a nonconvex loss function via a gradient algorithm and can be shown to converge to a global optimal point under good initialization. In contrast to the smooth loss function used in WF, we adopt a nonsmooth but lower-order loss function, and design a gradient-like algorithm (referred to as reshaped-WF). We show that for random Gaussian measurements, reshaped-WF enjoys geometric convergence to a global optimal point as long as the number m of measurements is at the order of O(n), where n is the dimension of the unknown x. This improves the sample complexity of WF, and achieves the same sample complexity as truncated-WF Chen and Candes [2015] but without truncation at gradient step. Furthermore, reshaped-WF costs less computationally than WF, and runs faster numerically than both WF and truncated-WE Bypassing higher-order variables in the loss function and truncations in the gradient loop, analysis of reshaped-WF is simplified.	[Zhang, Huishuai; Liang, Yingbin] Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA	Syracuse University	Zhang, HS (corresponding author), Syracuse Univ, Dept EECS, Syracuse, NY 13244 USA.	hzhan23@syr.edu; yliang06@syr.edu			 [AFOSR FA9550-16-1-0077];  [NSF ECCS 16-09916]	; 	This work is supported in part by the grants AFOSR FA9550-16-1-0077 and NSF ECCS 16-09916.	[Anonymous], 2013, ADV NEURAL INFORM PR; Burke JV, 2005, SIAM J OPTIMIZ, V15, P751, DOI 10.1137/030601296; Cai T. T., 2015, ARXIV150603382; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Chen Y., 2015, ADV NEURAL INFORM PR; Donahue James D., 1964, TECHNICAL REPORT; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Fogel F, 2013, ARXIV13047735; GERCHBERG RW, 1972, OPTIK, V35, P237; Glorot X., 2011, INT C ART INT STAT A; GROSS D., 2015, APPL COMPUTATIONAL H; Kiwiel KC, 2007, SIAM J OPTIMIZ, V18, P379, DOI 10.1137/050639673; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Ochs P, 2015, SIAM J IMAGING SCI, V8, P331, DOI 10.1137/140971518; Shechtman Y, 2015, IEEE SIGNAL PROC MAG, V32, P87, DOI 10.1109/MSP.2014.2352673; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wei K, 2015, INVERSE PROBL, V31, DOI 10.1088/0266-5611/31/12/125008; WRIGHT J., 2016, ARXIV160206664; Zhang H., 2016, ARXIV160303805	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702066
C	Zhang, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhang, Pan			Robust Spectral Detection of Global Structures in the Data by Learning a Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Spectral methods are popular in detecting global structures in the given data that can be represented as a matrix. However when the data matrix is sparse or noisy, classic spectral methods usually fail to work, due to localization of eigenvectors (or singular vectors) induced by the sparsity or noise. In this work, we propose a general method to solve the localization problem by learning a regularization matrix from the localized eigenvectors. Using matrix perturbation analysis, we demonstrate that the learned regularizations suppress down the eigenvalues associated with localized eigenvectors and enable us to recover the informative eigenvectors representing the global structure. We show applications of our method in several inference problems: community detection in networks, clustering from pairwise similarities, rank estimation and matrix completion problems. Using extensive experiments, we illustrate that our method solves the localization problem and works down to the theoretical detectability limits in different kinds of synthetic data. This is in contrast with existing spectral algorithms based on data matrix, non-backtracking matrix, Laplacians and those with rank-one regularizations, which perform poorly in the sparse case with noise.	[Zhang, Pan] Chinese Acad Sci, Inst Theoret Phys, Beijing 100190, Peoples R China	Chinese Academy of Sciences; Institute of Theoretical Physics, CAS	Zhang, P (corresponding author), Chinese Acad Sci, Inst Theoret Phys, Beijing 100190, Peoples R China.	panzhang@itp.ac.cn						Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Amini AA, 2013, ANN STAT, V41, P2097, DOI 10.1214/13-AOS1138; Bell R. J., 1970, DISCUSS FARADAY SOC, V50, P55, DOI DOI 10.1039/DF9705000055; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Hashimoto K., 1989, AUTOMORPHIC FORMS GE, V15, P211, DOI DOI 10.1016/B978-0-12-330580-0.50015-X; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Javanmard A, 2016, P NATL ACAD SCI USA, V113, pE2218, DOI 10.1073/pnas.1523097113; Johnson SG, 2014, NLOPT NONLINEAR OPTI; Joseph Antony, 2013, ARXIV13121733; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Keshavan RH, 2009, IEEE INT SYMP INFO, P324, DOI 10.1109/ISIT.2009.5205567; Keshavan RH, 2009, ANN ALLERTON CONF, P1216, DOI 10.1109/ALLERTON.2009.5394534; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Le C. M., 2015, ARXIV150203049; Le Can M., 2015, ARXIV150600669; Lei J, 2015, ANN STAT, V43, P215, DOI 10.1214/14-AOS1274; Luxburg U. V., 2007, STAT COMPUT; Massoulie L, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P694, DOI 10.1145/2591796.2591857; Mossel E., 2012, ARXIV12021499; Ng AY, 2002, ADV NEUR IN, V14, P849; Qin T., 2013, ADV NEURAL INFORM PR, P3120; SAADE A., 2014, ADV NEURAL INFORM PR, V27, P406; Saade A., 2015, ADV NEURAL INFORM PR, P1261; Saade A, 2016, IEEE INT SYMP INFO, P780, DOI 10.1109/ISIT.2016.7541405	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704016
C	Zhang, WH; Wang, H; Wong, KYM; Wu, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhang, Wen-Hao; Wang, He; Wong, K. Y. Michael; Wu, Si			"Congruent" and "Opposite" Neurons: Sisters for Multisensory Integration and Segregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BAYESIAN-INFERENCE; INFORMATION; REPRESENTATION; PERCEPTION; MOTION; CORTEX; AREA	Experiments reveal that in the dorsal medial superior temporal (MSTd) and the ventral intraparietal (VIP) areas, where visual and vestibular cues are integrated to infer heading direction, there are two types of neurons with roughly the same number. One is "congruent" cells, whose preferred heading directions are similar in response to visual and vestibular cues; and the other is "opposite" cells, whose preferred heading directions are nearly "opposite" (with an offset of 180 degrees) in response to visual vs. vestibular cues. Congruent neurons are known to be responsible for cue integration, but the computational role of opposite neurons remains largely unknown. Here, we propose that opposite neurons may serve to encode the disparity information between cues necessary for multisensory segregation. We build a computational model composed of two reciprocally coupled modules, MSTd and VIP, and each module consists of groups of congruent and opposite neurons. In the model, congruent neurons in two modules are reciprocally connected with each other in the congruent manner, whereas opposite neurons are reciprocally connected in the opposite manner. Mimicking the experimental protocol, our model reproduces the characteristics of congruent and opposite neurons, and demonstrates that in each module, the sisters of congruent and opposite neurons can jointly achieve optimal multisensory information integration and segregation. This study sheds light on our understanding of how the brain implements optimal multisensory integration and segregation concurrently in a distributed manner.	[Zhang, Wen-Hao; Wang, He; Wong, K. Y. Michael] Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Peoples R China; [Zhang, Wen-Hao; Wu, Si] Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing, Peoples R China; [Zhang, Wen-Hao; Wu, Si] Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing, Peoples R China; [Zhang, Wen-Hao] Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA	Hong Kong University of Science & Technology; Beijing Normal University; Beijing Normal University; Carnegie Mellon University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh	Zhang, WH (corresponding author), Hong Kong Univ Sci & Technol, Dept Phys, Hong Kong, Peoples R China.; Zhang, WH (corresponding author), Beijing Normal Univ, State Key Lab Cognit Neurosci & Learning, Beijing, Peoples R China.; Zhang, WH (corresponding author), Beijing Normal Univ, IDG McGovern Inst Brain Res, Beijing, Peoples R China.; Zhang, WH (corresponding author), Carnegie Mellon Univ, Ctr Neural Basis Cognit, Pittsburgh, PA 15213 USA.	wenhaoz@ust.hk; hwangaa@connect.ust.hk; phkywong@ust.hk; wusi@bnu.edu.cn			Research Grants Council of Hong Kong [N_HKUST606/12, 605813]; National Basic Research Program of China [2014CB846101]; Natural Science Foundation of China [31261160495]	Research Grants Council of Hong Kong(Hong Kong Research Grants Council); National Basic Research Program of China(National Basic Research Program of China); Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by the Research Grants Council of Hong Kong (N_HKUST606/12 and 605813) and National Basic Research Program of China (2014CB846101) and the Natural Science Foundation of China (31261160495).	Alais D, 2004, CURR BIOL, V14, P257, DOI 10.1016/j.cub.2004.01.029; BAIZER JS, 1991, J NEUROSCI, V11, P168; BOUSSAOUD D, 1990, J COMP NEUROL, V296, P462, DOI 10.1002/cne.902960311; Bresciani JP, 2006, J VISION, V6, P554, DOI 10.1167/6.5.2; Carandini M, 2012, NAT REV NEUROSCI, V13, P51, DOI 10.1038/nrn3136; Chen AH, 2013, J NEUROSCI, V33, P3567, DOI 10.1523/JNEUROSCI.4522-12.2013; Chen AH, 2011, J NEUROSCI, V31, P12036, DOI 10.1523/JNEUROSCI.0395-11.2011; Engel TA, 2011, J NEUROSCI, V31, P6982, DOI 10.1523/JNEUROSCI.6150-10.2011; Ernst MO, 2002, NATURE, V415, P429, DOI 10.1038/415429a; Ernst MO, 2004, TRENDS COGN SCI, V8, P162, DOI 10.1016/j.tics.2004.02.002; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; Girshick AR, 2009, J VISION, V9, DOI 10.1167/9.9.8; Gu Y, 2006, J NEUROSCI, V26, P73, DOI 10.1523/JNEUROSCI.2356-05.2006; Gu Y, 2008, NAT NEUROSCI, V11, P1201, DOI 10.1038/nn.2191; Gu Y, 2012, J NEUROSCI, V32, P2299, DOI 10.1523/JNEUROSCI.5154-11.2012; Jazayeri M, 2006, NAT NEUROSCI, V9, P690, DOI 10.1038/nn1691; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; Morgan ML, 2008, NEURON, V59, P662, DOI 10.1016/j.neuron.2008.06.024; Murray RF, 2010, J VISION, V10, DOI 10.1167/10.11.15; Ohshiro T, 2011, NAT NEUROSCI, V14, P775, DOI 10.1038/nn.2815; Roach NW, 2006, P ROY SOC B-BIOL SCI, V273, P2159, DOI 10.1098/rspb.2006.3578; Sato Y, 2007, NEURAL COMPUT, V19, P3335, DOI 10.1162/neco.2007.19.12.3335; Wallace MT, 2004, EXP BRAIN RES, V158, P252, DOI 10.1007/s00221-004-1899-9; Zhang WH, 2016, J NEUROSCI, V36, P532, DOI 10.1523/JNEUROSCI.0578-15.2016; Zhang Wen- Hao, 2013, ADV NEURAL INFOR PRO, P19	25	0	0	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702064
C	Zhang, YZ; Wang, XY; Chen, CY; Henao, R; Fan, K; Carin, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhang, Yizhe; Wang, Xiangyu; Chen, Changyou; Henao, Ricardo; Fan, Kai; Carin, Lawrence			Towards Unifying Hamiltonian Monte Carlo and Slice Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We unify slice sampling and Hamiltonian Monte Carlo (HMC) sampling, demonstrating their connection via the Hamiltonian-Jacobi equation from Hamiltonian mechanics. This insight enables extension of HMC and slice sampling to a broader family of samplers, called Monomial Gamma Samplers (MGS). We provide a theoretical analysis of the mixing performance of such samplers, proving that in the limit of a single parameter, the MGS draws decorrelated samples from the desired target distribution. We further show that as this parameter tends toward this limit, performance gains are achieved at a cost of increasing numerical difficulty and some practical convergence issues. Our theoretical results are validated with synthetic data and real-world applications.	[Zhang, Yizhe; Wang, Xiangyu; Chen, Changyou; Henao, Ricardo; Fan, Kai; Carin, Lawrence] Duke Univ, Durham, NC 27708 USA	Duke University	Zhang, YZ (corresponding author), Duke Univ, Durham, NC 27708 USA.	yz196@duke.edu; xw56@duke.edu; changyou.chen@duke.edu; ricardo.henao@duke.edu; kf96@duke.edu; lcarin@duke.edu						Arnold V.I., 2013, MATH METHODS CLASSIC, V60; Betancourt M., 2014, OPTIMIZING INTEGRATO; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Cances Eric, 2007, ESAIM MATH MODELLING, V41; Chao Wei- Lun, 2015, ICML; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Ekeland Ivar, 1980, ANN MATH; Girolami Mark, 2011, J ROYAL STAT SOC B, V73; Goldstein H., 1964, CLASSICAL MECH; Homan Matthew D., 2014, J MACHINE LEARNING R, V15; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Isaac Richard, 1963, ANN MATH STAT; Jiang Chengxiang, 2015, J APPL ANAL COMPUTAT, V5; Johnson A. A., 2009, THESIS; Korattikara Anoop, 2013, AUSTERITY MCMC LAND; Landau L.D., 1976, MECHANICS; Lichman M, 2013, UCI MACHINE LEARNING; LIVINGSTONE S., 2016, GEOMETRIC ERGODICITY; Murray I, 2009, ELLIPTICAL SLICE SAM; Nadarajah S, 2005, J APPL STAT, V32, P685, DOI 10.1080/02664760500079464; Neal R. M., 2011, HDB MARKOV CHAIN MON, V2; Neal Radford M., 2003, ANN STAT; Pakman A., 2013, NIPS; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Roberts Gareth O, 1998, CANADIAN J STAT, V26; Roberts Gareth O, 1996, BERNOULLI; Rosenthal Jeffrey S, 1995, J AM STAT ASS, V90; Salimans Tim, 2014, MARKOV CHAIN MONTE C; Striebel Michael, 2011, ACCURACY SYMMETRIC P, V12; Taylor J.R., 2005, CLASSICAL MECH; Tierney L, 1999, STAT MED, V18, P2507, DOI 10.1002/(SICI)1097-0258(19990915/30)18:17/18<2507::AID-SIM272>3.0.CO;2-J; Van Der Putten Peter, 2000, SENTIENT MACHINE RES, V9; Vigario Ricardo, 1998, NIPS; Wang Z., 2013, ICML; Zhang Y., 2012, NIPS	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701004
C	Zhao, SJ; Zhou, EZ; Sabharwal, A; Ermon, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhao, Shengjia; Zhou, Enze; Sabharwal, Ashish; Ermon, Stefano			Adaptive Concentration Inequalities for Sequential Decision Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A key challenge in sequential decision problems is to determine how many samples are needed for an agent to make reliable decisions with good probabilistic guarantees. We introduce Hoeffding-like concentration inequalities that hold for a random, adaptively chosen number of samples. Our inequalities are tight under natural assumptions and can greatly simplify the analysis of common sequential decision problems. In particular, we apply them to sequential hypothesis testing, best arm identification, and sorting. The resulting algorithms rival or exceed the state of the art both theoretically and empirically.	[Zhao, Shengjia; Zhou, Enze] Tsinghua Univ, Beijing, Peoples R China; [Sabharwal, Ashish] Allen Inst AI, Seattle, WA USA; [Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Tsinghua University; Stanford University	Zhao, SJ (corresponding author), Tsinghua Univ, Beijing, Peoples R China.	zhaosj12@stanford.edu; zhouez_thu_12@126.com; AshishS@allenai.org; ermon@cs.stanford.edu			NSF [1649208]; Future of Life Institute [2016-158687]	NSF(National Science Foundation (NSF)); Future of Life Institute	This research was supported by NSF (#1649208) and Future of Life Institute (#2016-158687).	Auer  Peter, 2002, FINITE TIME ANAL MUL; Balsubramani  A., 2014, SHARP FINITE TIME IT; Balsubramani  A., 2015, SEQUENTIAL NONPARAME; Breiman L, 1992, PROBABILITY; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chen L., 2015, ABS151103774 CORR; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Even-Dar Eyal, 2006, J MACHINE LEARNING R; Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115; FARRELL RH, 1964, ANN MATH STAT, V35, P36, DOI 10.1214/aoms/1177703731; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hoeffding W., 1963, J AM STAT ASS; Jamieson  Kevin, 2014, J MACHINE LEARNING R; Jamieson  Kevin, 2013, FINDING LARGEST MEAN; Jamieson  Kevin, 2014, BEST ARM IDENTIFICAT; Kalyanakrishnan S, 2012, P 29 INT C MACH LEAR, P655; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Karp RM, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P881; Mnih V., 2008, P 25 INT C MACH LEAR; Rivasplata O., 2012, SUBGAUSSIAN RANDOM V; Sen Pranab K., 1993, LARGE SAMPLE METHODS	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704018
C	Zhe, S; Zhang, K; Wang, PY; Lee, KC; Xu, ZL; Qi, Y; Gharamani, Z		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhe, Shandian; Zhang, Kai; Wang, Pengyuan; Lee, Kuang-chih; Xu, Zenglin; Qi, Yuan; Gharamani, Zoubin			Distributed Flexible Nonlinear Tensor Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Tensor factorization is a powerful tool to analyse multi-way data. Recently proposed nonlinear factorization methods, although capable of capturing complex relationships, are computationally quite expensive and may suffer a severe learning bias in case of extreme data sparsity. Therefore, we propose a distributed, flexible nonlinear tensor factorization model, which avoids the expensive computations and structural restrictions of the Kronecker-product in the existing TGP formulations, allowing an arbitrary subset of tensorial entries to be selected for training. Meanwhile, we derive a tractable and tight variational evidence lower bound (ELBO) that enables highly decoupled, parallel computations and high-quality inference. Based on the new bound, we develop a distributed, key-value-free inference algorithm in the MAPREDUCE framework, which can fully exploit the memory cache mechanism in fast MAPREDUCE systems such as SPARK. Experiments demonstrate the advantages of our method over several state-of-the-art approaches, in terms of both predictive performance and computational efficiency.	[Zhe, Shandian] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA; [Zhang, Kai] NEC Labs Amer, Princeton, NJ USA; [Wang, Pengyuan] Univ Georgia Athens, Dept Mkt, Athens, GA USA; [Lee, Kuang-chih] Yahoo Res, Sunnyvale, CA USA; [Xu, Zenglin] Univ Elect Sci & Tech China, Sch Comp Sci Engn, Big Data Res Ctr, Chengdu, Sichuan, Peoples R China; [Qi, Yuan] Alibaba, Ant Financial Serv Grp, Hangzhou, Zhejiang, Peoples R China; [Gharamani, Zoubin] Univ Cambridge, Cambridge, England	Purdue University System; Purdue University; Purdue University West Lafayette Campus; NEC Corporation; University System of Georgia; University of Georgia; University of Electronic Science & Technology of China; Alibaba Group; University of Cambridge	Zhe, S (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	szhe@purdue.edu; kzhang@nec-labs.com; pengyuan@uga.edu; kclee@yahoo-inc.com; zlxu@uestc.edu.cn; alanqi0@outlook.com; zoubin@cam.ac.uk	Wang, Pengyuan/ABG-1291-2021	Wang, Pengyuan/0000-0001-8689-3011	NSF China [61572111]	NSF China(National Natural Science Foundation of China (NSFC))	Dr. Zenglin Xu was supported by a grant from NSF China under No. 61572111. We thank IBM T.J. Watson Research Center for providing one dataset. We also thank Jiasen Yang for proofreading this paper.	Choi J. H., 2014, NIPS; Chu W., 2009, AISTATS; Davidson A., 2013, OPTIMIZING SHUFFLE P; De Lathauwer L, 2000, SIAM J MATRIX ANAL A, V21, P1253, DOI 10.1137/S0895479896305696; Gal Y., 2014, NIPS; Harshman R.A., 1970, MULTIMODAL FACTOR AN; Hoff P., 2011, COMPUTATIONAL STAT D; Hu C., 2015, UAI; Kang U., 2012, KDD; Lawrence Neil D., 2004, NIPS; Lloyd James, 2012, NIPS; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rai P., 2015, IJCAI; Rai P., 2014, ICML; Shashua Amnon, 2005, ICML; Sutskever I., 2009, NIPS; Titsias Michalis K., 2009, AISTATS; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Xu Z, 2012, ARXIV12066451; Yang Y, 2016, J AM STAT ASSOC, V111, P656, DOI 10.1080/01621459.2015.1029129; Zaharia M., 2012, NSDI; Zhe S, 2015, AISTATS; Zhe Shandian, 2016, AAAI	23	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704109
C	Zhong, K; Jain, P; Dhillon, IS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhong, Kai; Jain, Prateek; Dhillon, Inderjit S.			Mixed Linear Regression with Multiple Components	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MIXTURES	In this paper, we study the mixed linear regression (MLR) problem, where the goal is to recover multiple underlying linear models from their unlabeled linear measurements. We propose a non-convex objective function which we show is locally strongly convex in the neighborhood of the ground truth. We use a tensor method for initialization so that the initial models are in the local strong convexity region. We then employ general convex optimization algorithms to minimize the objective function. To the best of our knowledge, our approach provides first exact recovery guarantees for the MLR problem with K >= 2 components. Moreover, our method has near-optimal computational complexity (O) over tilde (N d) as well as near-optimal sample complexity (O) over tilde (d) for constant K. Furthermore, we show that our non convex formulation can be extended to solving the subspace clustering problem as well. In particular, when initialized within a small constant distance to the true subspaces, our method converges to the global optima (and recovers true subspaces) in time linear in the number of points. Furthermore, our empirical results indicate that even with random initialization, our approach converges to the global optima in linear time, providing speed-up of up to two orders of magnitude.	[Zhong, Kai; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA; [Jain, Prateek] Microsoft Res India, Bengaluru, Karnataka, India	University of Texas System; University of Texas Austin	Zhong, K (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	zhongkai@ices.utexas.edu; prajain@microsoft.com; inderjit@cs.utexas.edu			NSF [CCF-1320746, IIS-1546459, CCF-1564000]	NSF(National Science Foundation (NSF))	This research was supported by NSF grants CCF-1320746, IIS-1546459 and CCF-1564000.	Adler A, 2015, IEEE T NEUR NET LEAR, V26, P2234, DOI 10.1109/TNNLS.2014.2374631; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Arbenz P., LECT NOTES SOLVING L; Balakrishnan Sivaraman, 2015, ANN STAT; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Carvalho AX, 2005, IEEE T NEURAL NETWOR, V16, P39, DOI 10.1109/TNN.2004.839356; Chai XJ, 2007, IEEE T IMAGE PROCESS, V16, P1716, DOI 10.1109/TIP.2007.899195; Chen Yudong, 2014, COLT; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Deb P, 2000, HEALTH ECON, V9, P475, DOI 10.1002/1099-1050(200009)9:6<475::AID-HEC544>3.0.CO;2-H; Dyer EL, 2013, J MACH LEARN RES, V14, P2487; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Ferrari-Trecate G, 2002, LECT NOTES COMPUT SC, V2415, P444; Gaffney Scott, 1999, KDD; Hamm J., 2008, P INT C MACH LEARN I, P376, DOI DOI 10.1145/1390156.1390204; Heckel R, 2013, INT CONF ACOUST SPEE, P3263, DOI 10.1109/ICASSP.2013.6638261; Ho J, 2003, PROC CVPR IEEE, P11, DOI 10.1109/cvpr.2003.1211332; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Hsu D., 2013, P 4 C INN THEOR COMP, P11, DOI DOI 10.1145/2422436.2422439; Jie Shen, 2016, ICML; Khalili Abbas, 2012, J AM STAT ASS; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Park D., 2014, ADV NEURAL INFORM PR, P2753; Sedghi H., 2014, ARXIV14123046; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Viele K, 2002, STAT COMPUT, V12, P315, DOI 10.1023/A:1020779827503; VIETH E, 1989, J APPL PHYSIOL, V67, P390, DOI 10.1152/jappl.1989.67.1.390; Yi XY, 2014, PR MACH LEARN RES, V32, P613	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701097
C	Zhou, HH; Ravi, SN; Ithapu, VK; Johnson, SC; Wahba, G; Singh, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhou, Hao Henry; Ravi, Sathya N.; Ithapu, Vamsi K.; Johnson, Sterling C.; Wahba, Grace; Singh, Vikas			Hypothesis Testing in Unsupervised Domain Adaptation with Applications in Alzheimer's Disease	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Consider samples from two different data sources {X-s(i)} similar to P-source and {X-t(i)} similar to Ptarget. We only observe their transformed versions h (xi s) and g (X-t(i)), for some known function class h (center dot) and g (center dot). Our goal is to perform a statistical test checking if P-source = Ptarget while removing the distortions induced by the transformations. This problem is closely related to domain adaptation, and in our case, is motivated by the need to combine clinical and imaging based biomarkers from multiple sites and/or batches - a fairly common impediment in conducting analyses with much larger sample sizes. We address this problem using ideas from hypothesis testing on the transformed measurements, wherein the distortions need to be estimated in tandem with the testing. We derive a simple algorithm and study its convergence and consistency properties in detail, and provide lower-bound strategies based on recent work in continuous optimization. On a dataset of individuals at risk for Alzheimer's disease, our framework is competitive with alternative procedures that are twice as expensive and in some cases operationally infeasible to implement.	[Johnson, Sterling C.] William S Middleton Mem VA Hosp, Shorewood Hills, WI USA; [Zhou, Hao Henry; Ravi, Sathya N.; Ithapu, Vamsi K.; Johnson, Sterling C.; Wahba, Grace; Singh, Vikas] Univ Wisconsin, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Zhou, HH (corresponding author), Univ Wisconsin, Madison, WI 53706 USA.				NIH [AG040396, U54AI117924]; NSF [DMS1308847]; NSF CAREER [1252725]; NSF CCF [1320755]; UW CPCP [AI117924]; UW ADRC [AG033514]; UW ICTR [1UL1RR025011]; Stay Sharp fund	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF CCF; UW CPCP; UW ADRC; UW ICTR; Stay Sharp fund	This work is supported by NIH AG040396, NIH U54AI117924, NSF DMS1308847, NSF CAREER 1252725, NSF CCF 1320755 and UW CPCP AI117924. The authors are grateful for partial support from UW ADRC AG033514 and UW ICTR 1UL1RR025011. We thank Marilyn S. Albert (Johns Hopkins) and Anne Fagan (Washington University at St. Louis) for discussions at a preclinical Alzheimer's disease meeting in 2015 (supported by Stay Sharp fund).	[Anonymous], 2011, CVPR; Baktashmotlagh M, 2013, P IEEE ICCV; Ben- David S., 2010, MACHINE LEARNING; Chalise B, 2011, IEEE CAMSAP; Chandrasekaran V, 2014, ARXIV14097640; Cortes Corinna, 2011, ALGORITHMIC LEARNING; Dall TM, 2013, HLTH AFFAIRS; Daume Hal, 2010, P WORKSH DOM AD NAT; Dollar P., 2009, CVPR; Fernando B, 2013, P IEEE ICCV; Ganin Yaroslav, 2014, ARXIV14097495; Glioma Meta-analysis Trialists GMT Group, 2002, LANCET; Gong B., 2012, CVPR; Gong B., 2013, P INT C MACH LEARN J, V711, P712; Gong B, 2015, THESIS; Gopalan R, 2011, P IEEE ICCV; Gretton A., 2012, JMLR; Gretton A., 2009, NIPS; Haase M, 2009, AM J KIDNEY DIS; Huang J., 2006, NIPS; Klunk W, 2015, ALZHEIMERS DEMENTI S; Kumar A, 2010, NIPS; Lacoste-Julien S., 2012, ARXIV12074747; Nguyen X, 2010, INFORM THEORY IEEE T; Pan Sinno Jialin, 2011, NEURAL NETWORKS IEEE; Patel V. M., 2015, SIGNAL PROCESSING MA; Plassman B, 2007, NEUROEPIDEMIOLOGY; Qi L., 2012, LIT SURVEY DOMAIN AD; Saenko K., 2010, ECCV, P2; Sejdinovic D, 2013, ANN STAT; Sriperumbudur B. K., 2009, NIPS; Tuncel L., 2010, FIELDS I MONOGRAPH; Vanderstichele H, 2012, ALZHEIMERS DEMENTI S	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701066
C	Zhu, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhu, Rong			Gradient-based Sampling: An Adaptive Importance Sampling for Least-squares	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MONTE-CARLO ALGORITHMS; APPROXIMATION	In modern data analysis, random sampling is an efficient and widely-used strategy to overcome the computational difficulties brought by large sample size. In previous studies, researchers conducted random sampling which is according to the input data but independent on the response variable, however the response variable may also be informative for sampling. In this paper we propose an adaptive sampling called the gradient-based sampling which is dependent on both the input data and the output for fast solving of least-square (LS) problems. We draw the data points by random sampling from the full data according to their gradient values. This sampling is computationally saving, since the running time of computing the sampling probabilities is reduced to O(nd) where n is the full sample size and d is the dimension of the input. Theoretically, we establish an error bound analysis of the general importance sampling with respect to LS solution from full data. The result establishes an improved performance of the use of our gradient-based sampling. Synthetic and real data sets are used to empirically argue that the gradient-based sampling has an obvious advantage over existing sampling methods from two aspects of statistical efficiency and computational saving.	[Zhu, Rong] Chinese Acad Sci, Acad Math & Syst Sci, Beijing, Peoples R China	Chinese Academy of Sciences; Academy of Mathematics & System Sciences, CAS	Zhu, R (corresponding author), Chinese Acad Sci, Acad Math & Syst Sci, Beijing, Peoples R China.	rongzhu@amss.ac.cn			National Natural Science Foundation of China [11301514, 71532013]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This research was supported by National Natural Science Foundation of China grants 11301514 and 71532013. We thank Xiuyuan Cheng for comments in a preliminary version.	Clarkson D. P., 2013, STOC; Cohen Michael B., 2014, ARXIV14085099; Dhillon P S, 2013, ADV NEURAL INFORM PR, P360; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684; Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696; Drineas P, 2006, SIAM J COMPUT, V36, P184, DOI 10.1137/S0097539704442702; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; Ma P., 2014, P 31 INT C MACH LEAR; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Raskutti  G., 2015, P 32 ICML C; Sarndal C. E., 2003, MODEL ASSISTED SURVE; Shender  D., 2013, P 30 INT C MACH LEAR; Yang  T., 2015, P 32 ICML C	16	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704103
C	Zhu, YC; Chatterjee, S; Duchi, J; Lafferty, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhu, Yuancheng; Chatterjee, Sabyasachi; Duchi, John; Lafferty, John			Local Minimax Complexity of Stochastic Convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We extend the traditional worst-case, minimax analysis of stochastic convex optimization by introducing a localized form of minimax complexity for individual functions. Our main result gives function-specific lower and upper bounds on the number of stochastic subgradient evaluations needed to optimize either the function or its "hardest local alternative" to a given numerical precision. The bounds are expressed in terms of a localized and computational analogue of the modulus of continuity that is central to statistical minimax analysis. We show how the computational modulus of continuity can be explicitly calculated in concrete cases, and relates to the curvature of the function at the optimum. We also prove a superefficiency result that demonstrates it is a meaningful benchmark, acting as a computational analogue of the Fisher information in statistical estimation. The nature and practical implications of the results are demonstrated in simulations.	[Zhu, Yuancheng] Univ Penn, Wharton Stat Dept, Philadelphia, PA 19104 USA; [Chatterjee, Sabyasachi] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Duchi, John] Stanford Univ, Dept Elect Engn, Dept Stat, Stanford, CA 94305 USA; [Lafferty, John] Univ Chicago, Dept Comp Sci, Dept Stat, Chicago, IL 60637 USA	University of Pennsylvania; University of Chicago; Stanford University; University of Chicago	Zhu, YC (corresponding author), Univ Penn, Wharton Stat Dept, Philadelphia, PA 19104 USA.				ONR [11896509]; NSF [DMS-1513594]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	Research supported in part by ONR grant 11896509 and NSF grant DMS-1513594. The authors thank Tony Cai, Praneeth Netrapalli, Rob Nowak, Aaron Sidford, and Steve Wright for insightful discussions and valuable comments on this work.	BACH F., 2011, ADV NEURAL INFORM PR, P451; Brown LD, 1996, ANN STAT, V24, P2524; Cai TT, 2015, STAT SINICA, V25, P423, DOI 10.5705/ss.2013.279; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; Donoho David, 1987, TECHNICAL REPORT, V137; DONOHO DL, 1991, ANN STAT, V19, P633, DOI 10.1214/aos/1176348114; DONOHO DL, 1994, ANN STAT, V22, P238, DOI 10.1214/aos/1176325367; Jean-Baptiste Hiriart-Urruty, 1993, CONVEX ANAL MINIMIZA; Karp RM, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P881; NAZIN AV, 1989, AUTOMAT REM CONTR+, V50, P531; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; RAMDAS A, 2013, P 30 INT C MACH LEAR, P365; Ramdas Aaditya, 2015, ARXIV150504214; Ruppert David, 1988, 781 CORN U OP RES IN, V781; Tsybakov AB, 2009, SPRINGER SER STAT, P1, DOI 10.1007/978-0-387-79052-7_1	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705015
C	Abbe, E; Sandon, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Abbe, Emmanuel; Sandon, Colin			Recovering Communities in the General Stochastic Block Model Without Knowing the Parameters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BLOCKMODELS	The stochastic block model (SBM) has recently gathered significant attention due to new threshold phenomena. However, most developments rely on the knowledge of the model parameters, or at least on the number of communities. This paper introduces efficient algorithms that do not require such knowledge and yet achieve the optimal information-theoretic tradeoffs identified in Abbe-Sandon '15. In the constant degree regime, an algorithm is developed that requires only a lower-bound on the relative sizes of the communities and achieves the optimal accuracy scaling for large degrees. This lower-bound requirement is removed for the regime of diverging degrees. For the logarithmic degree regime, this is further enhanced into a fully agnostic algorithm that simultaneously learns the model parameters, achieves the optimal CH-limit for exact recovery, and runs in quasi-linear time. These provide the first algorithms affording efficiency, universality and information-theoretic optimality for strong and weak consistency in the SBM.	[Abbe, Emmanuel] Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA; [Abbe, Emmanuel] Princeton Univ, PACM, Princeton, NJ 08540 USA; [Sandon, Colin] Princeton Univ, Dept Math, Princeton, NJ 08540 USA	Princeton University; Princeton University; Princeton University	Abbe, E (corresponding author), Princeton Univ, Dept Elect Engn, Princeton, NJ 08540 USA.	eabbe@princeton.edu; sandon@princeton.edu						Abbe E., 2015, FOCS15; Abbe E., 2014, IEEE T INFORM THEORY; Abbe E., 2015, ARXIV150603729; Alon N., 1994, Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, P346, DOI 10.1145/195058.195187; [Anonymous], 2014, ARXIV14043918; [Anonymous], [No title captured]; Bandeira A. S., 2015, ARXIV150403987; Bhattacharyya S., 2014, ARXIV E PRINTS; Bickel P. J., 2009, P NATL ACAD SCI; Borgs C., 2015, PRIVATE GRAPHO UNPUB; BUI TN, 1987, COMBINATORICA, V7, P171, DOI 10.1007/BF02579448; Chin P., 2015, ARXIV150105021; Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053; Condon A., 1999, Randomization, Approximation, and Combinatorial Optimization. Algorithms and Techniques. Third International Workshop on Radomization and Approximation Techniques in Computer Science, and Second International Workshop on Approximation Algorithms for Combinatorial Optimization Problems RANDOM-APPROX'99. Proceedings (Lecture Notes in Computer Science Vol.1671), P221; DYER ME, 1989, J ALGORITHM, V10, P451, DOI 10.1016/0196-6774(89)90001-1; Gopalan P. K., 2013, P NATL ACAD SCI; Guedon O., 2014, ARXIV14114686; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jerrum M, 1998, DISCRETE APPL MATH, V82, P155, DOI 10.1016/S0166-218X(97)00133-9; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Mossel E., 2014, ARXIV13114115MATHPR; Mossel E., 2012, ARXIV12021499MATHPR; Mossel E., 2014, STOC15; Mossel E., 2013, ARXIVARXIV13091380; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Snijders TAB, 1997, J CLASSIF, V14, P75, DOI 10.1007/s003579900004; Xu J., 2014, ARXIV14021267; Xu J., 2014, ARXIV14126156; Yun S., 2014, ARXIV14127335; Zhang A. Y., 2015, ARXIV150503772	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102055
C	Abernethy, J; Lee, C; Tewari, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Abernethy, Jacob; Lee, Chansoo; Tewari, Ambuj			Fighting Bandits with a New Kind of Smoothness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We provide a new analysis framework for the adversarial multi-armed bandit problem. Using the notion of convex smoothing, we define a novel family of algorithms with minimax optimal regret guarantees. First, we show that regularization via the Tsallis entropy, which includes EXP3 as a special case, matches the O(root NT) minimax regret with a smaller constant factor. Second, we show that a wide class of perturbation methods achieve a near-optimal regret as low as O(root NT logN), as long as the perturbation distribution has a bounded hazard function. For example, the Gumbel, Weibull, Frechet, Pareto, and Gamma distributions all satisfy this key property and lead to near-optimal algorithms.	[Abernethy, Jacob; Lee, Chansoo; Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Abernethy, J (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	jabernet@umich.edu; chansool@umich.edu; tewaria@umich.edu			NSF under CAREER [IIS-1453304, IIS-1452099]	NSF under CAREER	J. Abernethy acknowledges the support of NSF under CAREER grant IIS-1453304. A. Tewari acknowledges the support of NSF under CAREER grant IIS-1452099.	Abernethy J. D., 2014, COLT, P807; Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096; Audibert J.-Y., 2011, COLT; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P., 1995, FOCS; Bertsekas DP, 1973, J OPTIMIZ THEORY APP, V12, P218, DOI 10.1007/BF00934819; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Dani V., 2008, NIPS; Dani V, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P937, DOI 10.1145/1109557.1109660; Devroye L., 2013, P 25 ANN C LEARN THE, P460; Elsayed E.A., 2012, RELIABILITY ENG, V2nd; Embrechts P., 1997, APPL MATH; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; GITTINS J, 1996, DRUG INF J, V30, P479; Gittins J. C., 2011, MULTIARMED BANDIT AL, V2nd; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kocak Tomas, 2014, ADV NEURAL INFORM PR, P613; Kujala J, 2005, LECT NOTES ARTIF INT, V3734, P371; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; McMahan HB, 2004, LECT NOTES COMPUT SC, V3120, P109, DOI 10.1007/978-3-540-27819-1_8; Neu G, 2013, LECT NOTES ARTIF INT, V8139, P234; Pacula Maciej, 2012, Applications of Evolutionary Computation. Proceedings of EvoApplications 2012: EvoCOMNET, EvoCOMPLEX, EvoFIN, EvoGAMES, EvoHOT, EvoIASP, EvoNUM, EvoPAR, EvoRISK, EvoSTIM, and EvoSTOC, P73, DOI 10.1007/978-3-642-29178-4_8; PENOT JP, 1994, NONLINEAR ANAL-THEOR, V23, P689, DOI 10.1016/0362-546X(94)90212-7; RAKHLIN A., 2012, ADV NEURAL INFORM PR, V25, P2141; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; TSALLIS C, 1988, J STAT PHYS, V52, P479, DOI 10.1007/BF01016429; Van den Broeck G, 2009, LECT NOTES ARTIF INT, V5828, P367, DOI 10.1007/978-3-642-05224-8_28; Van Erven T., 2014, COLT	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103068
C	Ahn, S; Park, S; Chertkov, M; Shin, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ahn, Sungsoo; Park, Sejun; Chertkov, Michael; Shin, Jinwoo			Minimum Weight Perfect Matching via Blossom Belief Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Max-product Belief Propagation (BP) is a popular message-passing algorithm for computing a Maximum-A-Posteriori (MAP) assignment over a distribution represented by a Graphical Model (GM). It has been shown that BP can solve a number of combinatorial optimization problems including minimum weight matching, shortest path, network flow and vertex cover under the following common assumption: the respective Linear Programming (LP) relaxation is tight, i.e., no integrality gap is present. However, when LP shows an integrality gap, no model has been known which can be solved systematically via sequential applications of BP. In this paper, we develop the first such algorithm, coined Blossom-BP, for solving the minimum weight matching problem over arbitrary graphs. Each step of the sequential algorithm requires applying BP over a modified graph constructed by contractions and expansions of blossoms, i.e., odd sets of vertices. Our scheme guarantees termination in O(n(2)) of BP runs, where n is the number of vertices in the original graph. In essence, the Blossom-BP offers a distributed version of the celebrated Edmonds' Blossom algorithm by jumping at once over many sub-steps with a single BP. Moreover, our result provides an interpretation of the Edmonds' algorithm as a sequence of LPs.	[Ahn, Sungsoo; Park, Sejun; Shin, Jinwoo] Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea; [Chertkov, Michael] Los Alamos Natl Lab, Theoret Div, Los Alamos, NM USA; [Chertkov, Michael] Los Alamos Natl Lab, Ctr Nonlinear Studies, Los Alamos, NM USA	Korea Advanced Institute of Science & Technology (KAIST); United States Department of Energy (DOE); Los Alamos National Laboratory; United States Department of Energy (DOE); Los Alamos National Laboratory	Ahn, S (corresponding author), Korea Adv Inst Sci & Technol, Sch Elect Engn, Daejeon, South Korea.	sungsoo.ahn@kaist.ac.kr; sejun.park@kaist.ac.kr; chertkov@lanl.gov; jinwoos@kaist.ac.kr	Chertkov, Michael/O-8828-2015	Chertkov, Michael/0000-0002-6758-515X	Institute for Information & communications Technology Promotion(IITP) - Korea government(MSIP) [R0132-15-1005]; National Nuclear Security Administration of the U.S. Department of Energy [DE-AC52-06NA25396]	Institute for Information & communications Technology Promotion(IITP) - Korea government(MSIP); National Nuclear Security Administration of the U.S. Department of Energy(National Nuclear Security Administration)	This work was supported by Institute for Information & communications Technology Promotion(IITP) grant funded by the Korea government(MSIP) (No. R0132-15-1005), Content visual browsing technology in the online and offline environments. The work at LANL was carried out under the auspices of the National Nuclear Security Administration of the U.S. Department of Energy under Contract No. DE-AC52-06NA25396.	Bayati M, 2008, IEEE T INFORM THEORY, V54, P1241, DOI 10.1109/TIT.2007.915695; Bayati M, 2011, SIAM J DISCRETE MATH, V25, P989, DOI 10.1137/090753115; Chandra R, 2000, PARALLEL PROGRAMMING; Chandrasekaran K, 2012, ANN IEEE SYMP FOUND, P571, DOI 10.1109/FOCS.2012.35; EDMONDS J, 1965, CANADIAN J MATH, V17, P449, DOI 10.4153/CJM-1965-045-4; Fischetti M, 2007, MATH PROGRAM, V110, P3, DOI 10.1007/s10107-006-0054-8; GAMARNIK D, 2010, SODA, V135, P279; Gonzalez J., 2009, INT C ART INT STAT F, P177; GROTSCHEL M, 1985, MATH PROGRAM, V33, P243, DOI 10.1007/BF01584376; HUANG B, 2007, ARTIFICIAL INTELLIGE; Kolmogorov V, 2009, MATH PROGRAM COMPUT, V1, P43, DOI 10.1007/s12532-009-0002-8; Kyrola A., 2012, PROC 10 USENIX S OPE, P31; Low Y., 2010, 26 C UNC ART INT CAL; Malioutov DM, 2006, J MACH LEARN RES, V7, P2031; Mezard M., 2009, OXFORD GRADUATE TEXT; Moallemi C., 2008, 45 ALL C COMM CONTR; PADBERG MW, 1982, MATH OPER RES, V7, P67, DOI 10.1287/moor.7.1.67; Park S, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P662; Richardson T., 2008, MODERN CODING THEORY; Ruozzi N, 2008, ANN ALLERTON CONF, P918, DOI 10.1109/ALLERTON.2008.4797655; Sanghavi S., 2007, NEURAL INFORM PROCES; Trick M., 1978, THESIS; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weiss Y., 2007, PROC C UNCERTAINTY A, P416; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100104
C	Andreas, J; Rabinovich, M; Jordan, MI; Klein, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Andreas, Jacob; Rabinovich, Maxim; Jordan, Michael I.; Klein, Dan			On the Accuracy of Self-Normalized Log-Linear Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Calculation of the log-normalizer is a major computational obstacle in applications of log-linear models with large output spaces. The problem of fast normalizer computation has therefore attracted significant attention in the theoretical and applied machine learning literature. In this paper, we analyze a recently proposed technique known as "self-normalization", which introduces a regularization term in training to penalize log normalizers for deviating from zero. This makes it possible to use unnormalized model scores as approximate probabilities. Empirical evidence suggests that self-normalization is extremely effective, but a theoretical understanding of why it should work, and how generally it can be applied, is largely lacking. We prove upper bounds on the loss in accuracy due to self-normalization, describe classes of input distributions that self-normalize easily, and construct explicit examples of high-variance input distributions. Our theoretical results make predictions about the difficulty of fitting self-normalized models to several classes of distributions, and we conclude with empirical validation of these predictions.	[Andreas, Jacob; Rabinovich, Maxim; Jordan, Michael I.; Klein, Dan] Univ Calif Berkeley, Comp Sci Div, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Andreas, J (corresponding author), Univ Calif Berkeley, Comp Sci Div, Berkeley, CA 94720 USA.	jda@cs.berkeley.edu; rabinovich@cs.berkeley.edu; jordan@cs.berkeley.edu; klein@cs.berkeley.edu	Jordan, Michael I/C-5253-2013	Jordan, Michael/0000-0001-8935-817X	NSF Graduate Fellowships; Fannie and John Hertz Foundation Fellowship	NSF Graduate Fellowships; Fannie and John Hertz Foundation Fellowship	The authors would like to thank Robert Nishihara for useful discussions. JA and MR are supported by NSF Graduate Fellowships, and MR is additionally supported by the Fannie and John Hertz Foundation Fellowship.	Andreas J., 2014, P ANN M N AM CHAPT A; Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Chen YH, 2010, ADV INTEL SOFT COMPU, V66, P109, DOI 10.1145/1866919.1866935; Devlin J., 2014, P ANN M ASS COMP LIN; Doucet A, 2001, STAT ENG IN, P3; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Morin F, 2005, P INT WORKSH ART INT, P246; OHAGAN A, 1991, J STAT PLAN INFER, V29, P245, DOI 10.1016/0378-3758(91)90002-V; Pereira F. C. N., 2001, P 18 INT C MACH LEAR, P282; Vaswani A., 2013, P 2013 C EMP METH NA, P1387; Yang E., 2012, ADV NEURAL INFORM PR, P1358	13	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101066
C	Audiffren, J; Ralaivola, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Audiffren, Julien; Ralaivola, Liva			Cornering Stationary and Restless Mixing Bandits with Remix-UCB	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INEQUALITIES; CONVERGENCE; BOUNDS; RATES; SUMS	We study the restless bandit problem where arms are associated with stationary phi-mixing processes and where rewards are therefore dependent: the question that arises from this setting is that of carefully recovering some independence by 'ignoring' the values of some rewards. As we shall see, the bandit problem we tackle requires us to address the exploration/exploitation/independence trade-off, which we do by considering the idea of a waiting arm in the new Remix-UCB algorithm, a generalization of Improved-UCB for the problem at hand, that we introduce. We provide a regret analysis for this bandit strategy; two noticeable features of Remix-UCB are that i) it reduces to the regular Improved-UCB when the phi-mixing coefficients are all 0, i.e. when the i.i.d scenario is recovered, and ii) when phi(n) = O(n(-alpha)), it is able to ensure a controlled regret of order (circle dot) over tilde(Delta((alpha-2)/alpha)(*) log(1/alpha) T), where Delta(*) encodes the distance between the best arm and the best suboptimal arm, even in the case when alpha < 1, i.e. the case when the phi-mixing coefficients are not summable.	[Audiffren, Julien] Paris Saclay Univ, ENS Cachan, CMLA, F-94235 Cachan, France; [Ralaivola, Liva] Aix Marseille Univ, QARMA, LIF, CNRS, F-13289 Marseille 9, France	UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Aix-Marseille Universite	Audiffren, J (corresponding author), Paris Saclay Univ, ENS Cachan, CMLA, F-94235 Cachan, France.	audiffren@cmla.ens-cachan.fr; liva.ralaivola@lif.univ-mrs.fr		Audiffren, Julien/0000-0003-4321-2575	ANR [ANR-12-BS02-004-01]; ND project	ANR(French National Research Agency (ANR)); ND project	This work is partially supported by the ANR-funded projet GRETA - Greediness: theory and algorithms (ANR-12-BS02-004-01) and the ND project.	Audibert JY, 2009, ANN C LEARN THEOR; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6; Bernstein S, 1927, MATH ANN, V97, P1, DOI 10.1007/BF01447859; Bubeck S, 2012, FDN TRENDS MACHINE L, V5; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; HOEFFDING W, 1948, ANN MATH STAT, V19, P293, DOI 10.1214/aoms/1177730196; Karandikar RL, 2002, STAT PROBABIL LETT, V58, P297, DOI 10.1016/S0167-7152(02)00124-4; Kontorovich L, 2008, ANN PROBAB, V36, P2126, DOI 10.1214/07-AOP384; KULKARNI S., 2005, ADV NEURAL INFORM PR, P819; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; McDonald D, 2011, ARXIV11030941; Mohri M., 2009, ADV NEURAL INFORM PR, V21, P1097; Mohri M, 2010, J MACH LEARN RES, V11, P789; Ortner Ronald, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P214, DOI 10.1007/978-3-642-34106-9_19; Pandey S., 2007, P 24 INT C MACHINE L, P721, DOI DOI 10.1145/1273496.1273587; Ralaivola L, 2010, J MACH LEARN RES, V11, P1927; Seldin Y, 2014, PR MACH LEARN RES, V32, P1287; Steinwart I., 2009, ADV NEURAL INF PROCE, V22, P1768; Steinwart I, 2009, J MULTIVARIATE ANAL, V100, P175, DOI 10.1016/j.jmva.2008.04.001; Tekin C, 2012, IEEE T INFORM THEORY, V58, P5588, DOI 10.1109/TIT.2012.2198613	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103067
C	Awasthi, P; Risteski, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Awasthi, Pranjal; Risteski, Andrej			On some provably correct cases of variational inference for topic models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				EM	Variational inference is an efficient, popular heuristic used in the context of latent variable models. We provide the first analysis of instances where variational inference algorithms converge to the global optimum, in the setting of topic models. Our initializations are natural, one of them being used in LDA-c, the most popular implementation of variational inference. In addition to providing intuition into why this heuristic might work in practice, the multiplicative, rather than additive nature of the variational inference updates forces us to use non-standard proof arguments, which we believe might be of general theoretical interest.	[Awasthi, Pranjal] Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA; [Risteski, Andrej] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA	Rutgers State University New Brunswick; Princeton University	Awasthi, P (corresponding author), Rutgers State Univ, Dept Comp Sci, New Brunswick, NJ 08901 USA.	pranjal.awasthi@rutgers.edu; risteski@cs.princeton.edu						Agarwal A., 2013, P 27 C LEARN THEOR C; Anandkumar A., 2013, P 30 INT C MACH LEAR; [Anonymous], 2013, ADV NEURAL INFORM PR; ARORA S., 2013, P 30 INT C MACH LEAR; Arora S., 2014, P 27 C LEARN THEOR C; Arora S., 2012, P 53 ANN IEEE S FDN; Arora S, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P145; Arora Sanjeev, 2015, P 28 C LEARN THEOR C; Balakrishnan Sivaraman, 2014, ARXIV14082156; Bansal T., 2014, ADV NEURAL INFORM PR; Blei D.M., 2009, TEXT MINING CLASSIFI, V10, P34; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Dasgupta S., 2000, P UNC ART INT UAI; Dasgupta S, 2007, J MACH LEARN RES, V8, P203; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ding WC, 2014, JMLR WORKSH CONF PRO, V33, P167; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kumar A., 2010, P FDN COMP SCI FOCS; Lee D. D., 2000, ADV NEURAL INFORM PR; Liu Y.-K., 2012, NIPS; Saligrama V., 2013, ARXIV13033664; Sontag D., 2000, ADV NEURAL INFORM PR; Sundberg R., 1974, SCAND J STAT, V1, P49; Telgarsky M., 2013, DIRICHLET DRAW UNPUB	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101006
C	Bahmani, S; Romberg, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bahmani, Sohail; Romberg, Justin			Efficient Compressive Phase Retrieval with Constrained Sensing Vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				RESTRICTED ISOMETRY PROPERTY; SIGNAL RECOVERY	We propose a robust and efficient approach to the problem of compressive phase retrieval in which the goal is to reconstruct a sparse vector from the magnitude of a number of its linear measurements. The proposed framework relies on constrained sensing vectors and a two-stage reconstruction method that consists of two standard convex programs that are solved sequentially. In recent years, various methods are proposed for compressive phase retrieval, but they have suboptimal sample complexity or lack robustness guarantees. The main obstacle has been that there is no straightforward convex relaxations for the type of structure in the target. Given a set of underdetermined measurements, there is a standard framework for recovering a sparse matrix, and a standard framework for recovering a low-rank matrix. However, a general, efficient method for recovering a jointly sparse and low-rank matrix has remained elusive. Deviating from the models with generic measurements, in this paper we show that if the sensing vectors are chosen at random from an incoherent subspace,then the low-rank and sparse structures of the target signal can be effectively decoupled. We show that a recovery algorithm that consists of a low-rank recovery stage followed by a sparse recovery stage will produce an accurate estimate of the target when the number of measurements is O(k log d/k), where k and d denote the sparsity level and the dimension of the input signal. We also evaluate the algorithm through numerical simulation.	[Bahmani, Sohail; Romberg, Justin] Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Bahmani, S (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.	sohail.bahmani@ece.gatech.edu; jrom@ece.gatech.edu			ONR [N00014-11-1-0459]; NSF [CCF-1415498, CCF-1422540]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	This work was supported by ONR grant N00014-11-1-0459, and NSF grants CCF-1415498 and CCF-1422540.	Baraniuk R, 2008, CONSTR APPROX, V28, P253, DOI 10.1007/s00365-007-9003-x; Becker SR, 2011, MATH PROGRAM COMPUT, V3, P165, DOI 10.1007/s12532-011-0029-5; Bertolotti J, 2012, NATURE, V491, P232, DOI 10.1038/nature11578; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Candes EJ, 2008, CR MATH, V346, P589, DOI 10.1016/j.crma.2008.03.014; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Iwen M., 2015, APPL COMPUTATIONAL H; Kueng R., 2015, APPL COMPUTATIONAL H; Li XD, 2013, SIAM J MATH ANAL, V45, P3019, DOI 10.1137/120893707; Liutkus A, 2014, SCI REP-UK, V4, DOI 10.1038/srep05552; Moravec ML, 2007, PROC SPIE, V6701, DOI 10.1117/12.736360; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Ohlsson H., 2012, P NEUR INF PROC SYST; Oymak S, 2015, IEEE T INFORM THEORY, V61, P2886, DOI 10.1109/TIT.2015.2401574; Pedarsani R, 2014, ANN ALLERTON CONF, P842, DOI 10.1109/ALLERTON.2014.7028542; Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687; Shechtman Y, 2011, OPT EXPRESS, V19, P14807, DOI 10.1364/OE.19.014807; Tropp Joel A., 2014, ARXIV14051102CSIT; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103059
C	Banerjee, S; Lofgren, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Banerjee, Siddhartha; Lofgren, Peter			Fast Bidirectional Probability Estimation in Markov Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We develop a new bidirectional algorithm for estimating Markov chain multi-step transition probabilities: given a Markov chain, we want to estimate the probability of hitting a given target state in ` steps after starting from a given source distribution. Given the target state t, we use a (reverse) local power iteration to construct an ` expanded target distribution', which has the same mean as the quantity we want to estimate, but a smaller variance -this can then be sampled efficiently by a Monte Carlo algorithm. Our method extends to any Markov chain on a discrete (finite or countable) state-space, and can be extended to compute functions of multi-step transition probabilities such as PageRank, graph diffusions, hitting/ return times, etc. Our main result is that in ` sparse' Markov Chains -wherein the number of transitions between states is comparable to the number of states the running time of our algorithm for a uniform-random target node is order-wise smaller than Monte Carlo and power iteration based algorithms; in particular, our method can estimate a probability p using only O(1/root p) running time.	[Banerjee, Siddhartha] Cornell, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA; [Lofgren, Peter] Stanford, Comp Sci Dept, Stanford, CA 94305 USA	Cornell University; Stanford University	Banerjee, S (corresponding author), Cornell, Sch Operat Res & Informat Engn, Ithaca, NY 14850 USA.	sbanerjee@cornell.edu; plofgren@cs.stanford.edu			DARPA GRAPHS program [FA9550-12-1-0411]; NSF [1447697]; NPSC fellowship	DARPA GRAPHS program; NSF(National Science Foundation (NSF)); NPSC fellowship	Research supported by the DARPA GRAPHS program via grant FA9550-12-1-0411, and by NSF grant 1447697. Peter Lofgren was supported by an NPSC fellowship. Thanks to Ashish Goel and other members of the Social Algorithms Lab at Stanford for many helpful discussions.	Andersen R., 2007, ALGORITHMS MODELS WE; Andersen Reid, 2006, IEEE FOCS 06; Athreya Krishna B, 2003, SANKHYA, P763; Banerjee Siddhartha, 2015, TECHNICAL REPORT; Boldi Paolo, 2011, ACM WWW 11; Chung F, 2007, P NATL ACAD SCI USA, V104, P19735, DOI 10.1073/pnas.0708838104; Doeblin W., 1940, ANN SCI ECOLE NORM S, V57, P61, DOI [10.24033/asens.883, DOI 10.24033/ASENS.883]; Dubhashi DP, 2009, CONCENTRATION OF MEASURE FOR THE ANALYSIS OF RANDOMIZED ALGORITHMS, P1, DOI 10.1017/CBO9780511581274; Goldreich Oded, 2011, STUDIES COMPLEXITY C; Kale Satyen, 2008, IEEE FOCS 08; Kloster Kyle, 2014, ACM SIGKDD 14; Lee Christina E, 2013, P NIPS, P1376; Lempel R, 2000, COMPUT NETW, V33, P387, DOI 10.1016/S1389-1286(00)00034-7; Lofgren Peter, 2014, ACM SIGKDD 14; Mislove A, 2007, IMC'07: PROCEEDINGS OF THE 2007 ACM SIGCOMM INTERNET MEASUREMENT CONFERENCE, P29; Motwani R, 2007, LECT NOTES COMPUT SC, V4596, P53; Negahban Sahand, 2012, NIPS, P2483; Page L., 1999, PAGERANK CITATION RA; Sarkar P., 2008, ICML, P896; Steinhardt Jacob, 2015, ICML 15; Takac L., 2012, INT SCI C WORKSH PRE	21	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101074
C	Bareinboim, E; Forney, A; Pearl, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bareinboim, Elias; Forney, Andrew; Pearl, Judea			Bandits with Unobserved Confounders: A Causal Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MULTIARMED BANDIT	The Multi-Armed Bandit problem constitutes an archetypal setting for sequential decision-making, permeating multiple domains including engineering, business, and medicine. One of the hallmarks of a bandit setting is the agent's capacity to explore its environment through active intervention, which contrasts with the ability to collect passive data by estimating associational relationships between actions and payouts. The existence of unobserved confounders, namely unmeasured variables affecting both the action and the outcome variables, implies that these two data-collection modes will in general not coincide. In this paper, we show that formalizing this distinction has conceptual and algorithmic implications to the bandit setting. The current generation of bandit algorithms implicitly try to maximize rewards based on estimation of the experimental distribution, which we show is not always the best strategy to pursue. Indeed, to achieve low regret in certain realistic classes of bandit problems (namely, in the face of unobserved confounders), both experimental and observational quantities are required by the rational agent. After this realization, we propose an optimization metric (employing both experimental and observational distributions) that bandit agents should pursue, and illustrate its benefits over traditional algorithms.	[Bareinboim, Elias] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA; [Forney, Andrew; Pearl, Judea] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90024 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; University of California System; University of California Los Angeles	Bareinboim, E (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	eb@purdue.edu; forns@cs.ucla.edu; judea@cs.ucla.edu						Agrawal S., 2011, ABS11111797 CORR; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bareinboim E., 2015, R460 UCLA COGN SYST; Bubeck S., 2012, ABS12024473 CORR; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Busa-Fekete R., 2010, INT C MACH LEARN, V27, P143; Dudik M., 2011, ABS11062369 CORR; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Fisher R.A., 1951, DESIGN EXPT, V6; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Langford J., 2008, ADV NEURAL INFORM PR, P817; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Ortega PA, 2010, J ARTIF INTELL RES, V38, P475, DOI 10.1613/jair.3062; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874; Seldin Yevgeny, 2014, INT C MACH LEARN; Shpitser I., 2007, P 23 C UNC ART INT, P352	19	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100060
C	Bhatia, K; Jain, H; Kar, P; Varma, M; Jain, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bhatia, Kush; Jain, Himanshu; Kar, Purushottam; Varma, Manik; Jain, Prateek			Sparse Local Embeddings for Extreme Multi-label Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies, or scale to large problems as the low rank assumption is violated in most real world applications. In this paper we develop the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring )tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world, as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based )by as much as 35%) as well as tree-based )by as much as 6%) methods. SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.	[Bhatia, Kush; Varma, Manik; Jain, Prateek] Microsoft Res, Bengaluru, India; [Jain, Himanshu] Indian Inst Technol Delhi, Delhi, India; [Kar, Purushottam] Indian Inst Technol Kanpur, Kanpur, Uttar Pradesh, India; [Kar, Purushottam] Microsoft Res India, Bengaluru, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur	Bhatia, K (corresponding author), Microsoft Res, Bengaluru, India.	t-kushb@microsoft.com; himanshu.j689@gmail.com; purushot@cse.iitk.ac.in; manik@microsoft.com; prajain@microsoft.com	Kar, Purushottam/W-8113-2019	Kar, Purushottam/0000-0003-2096-5267	Google India PhD Fellowship at IIT Delhi	Google India PhD Fellowship at IIT Delhi(Google Incorporated)	We are grateful to Abhishek Kadian for helping with the experiments. Himanshu Jain is supported by a Google India PhD Fellowship at IIT Delhi	Agrawal Rahul, 2013, WWW 13, P13; [Anonymous], 2011, IJCAI; [Anonymous], 2014, WIKIPEDIA DATASET 4; Balasubramanian K., 2012, ARXIV PREPRINT ARXIV; Bi W., 2013, ICML; Chen Y.N., 2012, P ADV NEUR INF PROC, P1529; Feng C.-S., 2011, JMLR, V20; Gallinari P, 2013, ADV NEURAL INFORM PR, P1851; Hariharan B., 2012, ML; Hsu D.J., 2009, NIPS, P772; Jain P, 2010, P ADV NEUR INF PROC, P937; Ji S., 2008, KDD; Kar P., 2013, ICML; Katakis I., 2008, P ECML PKDD 2008 DIS; Leskovec J, 2014, SNAP DATASETS STANFO; Lin ZJ, 2014, PR MACH LEARN RES, V32, P325; Mencia E. L., 2008, ECML PKDD; Ng A., 2002, NIPS; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Shaw B., 2007, JMLR WORKSHOP C P, V2, P460; SNOEK CGM, 2006, ACM MULTIMEDIA; Sprechmann P., 2013, NIPS; Tai F., 2010, WORKSH P LEARN MULT WORKSH P LEARN MULT; Tsoumakas G., 2008, ECML PKDD; Weinberger K. Q., 2006, AAAI, V6, P1683; Weston J., 2013, ICML; Wetzker Robert, 2008, P ECAI 2008 MIN SOC, P26; Yi Z., 2011, J MACHINE LEARNING R, V15, P873; Yu H.-F., 2014, ICML; Zubiaga A., 2009, ENHANCING NAVIGATION	30	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103007
C	Bhattacharya, BB; Valiant, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bhattacharya, Bhaswar B.; Valiant, Gregory			Testing Closeness With Unequal Sized Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ENTROPY	We consider the problem of testing whether two unequal-sized samples were drawn from identical distributions, versus distributions that differ significantly. Specifically, given a target error parameter epsilon > 0, m(1) independent draws from an unknown distribution p with discrete support, and m(2) draws from an unknown distribution q of discrete support, we describe a test for distinguishing the case that p = q from the case that parallel to p - q parallel to(1) >= epsilon. If p and q are supported on at most n elements, then our test is successful with high probability provided m(1) >= n(2/3)/epsilon(4/3) and m(2) = Omega (max{n/root m(1)epsilon(2), root n/epsilon(2)}). We show that this tradeoff is information theoretically optimal throughout this range in the dependencies on all parameters, n, m(1), and epsilon, to constant factors for worst-case distributions. As a consequence, we obtain an algorithm for estimating the mixing time of a Markov chain on n states up to a log n factor that uses (O) over tilde (n(3/2)T(mix)) queries to a "next node" oracle. The core of our testing algorithm is a relatively simple statistic that seems to perform well in practice, both on synthetic and on natural language data. We believe that this statistic might prove to be a useful primitive within larger machine learning and natural language processing systems.	[Bhattacharya, Bhaswar B.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Valiant, Gregory] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University; Stanford University	Bhattacharya, BB (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	bhaswar@stanford.edu; valiant@stanford.edu			NSF CAREER Award [CCF-1351108]	NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	Supported in part by NSF CAREER Award CCF-1351108	Acharya J., 2012, COLT; Acharya J., 2015, NIPS; Acharya J., 2011, COLT; Acharya J, 2014, IEEE INT SYMP INFO, P3200, DOI 10.1109/ISIT.2014.6875425; BARYOSSEF Z, 2001, STOC; Batu T., 2001, FOCS; Batu T., 2000, FOCS; Batu T., 2005, SIAM J COMPUTING; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Charikar M., 2000, S PRINC DAT SYST POD; Czumaj A., 2007, FOCS; GOLDREICH O., 2000, ECCC; Guha S., 2006, S DISCR ALG SODA; Hsu D., 2015, NIPS; Kale S, 2008, LECT NOTES COMPUT SC, V5125, P527, DOI 10.1007/978-3-540-70575-8_43; Keinan A, 2012, SCIENCE, V336, P740, DOI 10.1126/science.1217283; Levin D. A., 2009, MARKOV CHAINS MIXING; Nachmias A., 2007, ELECT C COMPUTATIONA, V14; Nelson MR, 2012, SCIENCE, V337, P100, DOI 10.1126/science.1217876; Paninski L, 2004, IEEE T INFORM THEORY, V50, P2200, DOI 10.1109/TIT.2004.833360; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4750, DOI 10.1109/TIT.2008.928987; Raskhodnikova S, 2009, SIAM J COMPUT, V39, P813, DOI 10.1137/070701649; Rubinfeld R., 2012, XRDS CROSSROADS FAL, V19, P24, DOI DOI 10.1145/2331042.2331052; SINCLAIR A, 1989, INFORM COMPUT, V82, P93, DOI 10.1016/0890-5401(89)90067-9; Tennessen JA, 2012, SCIENCE, V337, P64, DOI 10.1126/science.1219240; Valiant G., 2011, STOC; Valiant G., 2013, NIPS; Valiant G, 2014, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2014.14; Valiant P., 2008, STOC; Valiant  P., 2008, THESIS	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102057
C	Bitzer, S; Kiebel, SJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bitzer, Sebastian; Kiebel, Stefan J.			The Brain Uses Reliability of Stimulus Information when Making Perceptual Decisions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				AREA	In simple perceptual decisions the brain has to identify a stimulus based on noisy sensory samples from the stimulus. Basic statistical considerations state that the reliability of the stimulus information, i.e., the amount of noise in the samples, should be taken into account when the decision is made. However, for perceptual decision making experiments it has been questioned whether the brain indeed uses the reliability for making decisions when confronted with unpredictable changes in stimulus reliability. We here show that even the basic drift diffusion model, which has frequently been used to explain experimental findings in perceptual decision making, implicitly relies on estimates of stimulus reliability. We then show that only those variants of the drift diffusion model which allow stimulus-specific reliabilities are consistent with neurophysiological findings. Our analysis suggests that the brain estimates the reliability of the stimulus on a short time scale of at most a few hundred milliseconds.	[Bitzer, Sebastian; Kiebel, Stefan J.] Tech Univ Dresden, Dept Psychol, D-01062 Dresden, Germany	Technische Universitat Dresden	Bitzer, S (corresponding author), Tech Univ Dresden, Dept Psychol, D-01062 Dresden, Germany.	sebastian.bitzer@tu-dresden.de; stefan.kiebel@tu-dresden.de	Kiebel, Stefan J/B-1551-2009	Kiebel, Stefan J/0000-0002-5052-1117				Beck JM, 2008, NEURON, V60, P1142, DOI 10.1016/j.neuron.2008.09.021; Bitzer S, 2014, FRONT HUM NEUROSCI, V8, DOI 10.3389/fnhum.2014.00102; Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700; Churchland AK, 2008, NAT NEUROSCI, V11, P693, DOI 10.1038/nn.2123; Churchland AK, 2011, NEURON, V69, P818, DOI 10.1016/j.neuron.2010.12.037; Dayan P, 2008, COGN AFFECT BEHAV NE, V8, P429, DOI 10.3758/CABN.8.4.429; Deneve S, 2012, FRONT NEUROSCI-SWITZ, V6, DOI 10.3389/fnins.2012.00075; Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038; Hanks Timothy D., 2015, NATURE; Huang Y., 2012, ADV NEURAL INFORM PR, V25, P1277; JOHN ID, 1967, AUST J PSYCHOL, V19, P27, DOI 10.1080/00049536708255558; Luce R. D, 1986, OXFORD PSYCHOL SERIE, V8; NEWSOME WT, 1988, J NEUROSCI, V8, P2201; Pilly PK, 2009, VISION RES, V49, P1599, DOI 10.1016/j.visres.2009.03.019; Rao RPN, 2004, NEURAL COMPUT, V16, P1, DOI 10.1162/08997660460733976; Roitman JD, 2002, J NEUROSCI, V22, P9475; Shadlen MN, 2008, BETTER CONSCIOUS DEC; Solway A, 2012, PSYCHOL REV, V119, P120, DOI 10.1037/a0026435; Wald A., 1947, SEQUENTIAL ANAL; Wang XJ, 2002, NEURON, V36, P955, DOI 10.1016/S0896-6273(02)01092-9; Yu A, 2005, ADV NEURAL INFORM PR, V17, P1577	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101049
C	Bourdoukan, R; Deneve, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bourdoukan, Ralph; Deneve, Sophie			Enforcing balance allows local supervised learning in spiking recurrent networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NEURON	To predict sensory inputs or control motor trajectories, the brain must constantly learn temporal dynamics based on error feedback. However, it remains unclear how such supervised learning is implemented in biological neural networks. Learning in recurrent spiking networks is notoriously difficult because local changes in connectivity may have an unpredictable effect on the global dynamics. The most commonly used learning rules, such as temporal back-propagation, are not local and thus not biologically plausible. Furthermore, reproducing the Poisson-like statistics of neural responses requires the use of networks with balanced excitation and inhibition. Such balance is easily destroyed during learning. Using a top-down approach, we show how networks of integrate-and-fire neurons can learn arbitrary linear dynamical systems by feeding back their error as a feed-forward input. The network uses two types of recurrent connections: fast and slow. The fast connections learn to balance excitation and inhibition using a voltage-based plasticity rule. The slow connections are trained to minimize the error feedback using a current-based Hebbian learning rule. Importantly, the balance maintained by fast connections is crucial to ensure that global error signals are available locally in each neuron, in turn resulting in a local learning rule for the slow connections. This demonstrates that spiking networks can learn complex dynamics using purely local learning rules, using E/I balance as the key rather than an additional constraint. The resulting network implements a given function within the predictive coding scheme, with minimal dimensions and activity.	[Bourdoukan, Ralph; Deneve, Sophie] ENS Paris, Grp Neural Theory, Rue Ulm 29, Paris, France	PSL Research University Paris (ComUE); Ecole Normale Superieure (ENS)	Bourdoukan, R (corresponding author), ENS Paris, Grp Neural Theory, Rue Ulm 29, Paris, France.	ralph.bourdoukan@gmail.com; sophie.deneve@ens.fr			ERC grant FP7-PREDISPIKE; James McDonnell Foundation Award - Human Cognition;  [ANR-10-LABX-0087 IEC];  [ANR-10-IDEX-0001-02 PSL]	ERC grant FP7-PREDISPIKE; James McDonnell Foundation Award - Human Cognition; ; 	This work was supported by ANR-10-LABX-0087 IEC, ANR-10-IDEX-0001-02 PSL, ERC grant FP7-PREDISPIKE and the James McDonnell Foundation Award - Human Cognition.	Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258; Bourdoukan R, 2012, ADV NEURAL INFORM PR, V25, P2285; Brunel N, 2000, J PHYSIOLOGY-PARIS, V94, P445, DOI 10.1016/S0928-4257(00)01084-6; Chen C, 1995, CELL, V83, P1233, DOI 10.1016/0092-8674(95)90148-5; ECCLES JC, 1966, J PHYSIOL-LONDON, V182, P268, DOI 10.1113/jphysiol.1966.sp007824; Gutig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643; Jaeger H., 2001, 148 GMD GERM NAT RES; Kawato M, 1999, CURR OPIN NEUROBIOL, V9, P718, DOI 10.1016/S0959-4388(99)00028-8; Knudsen E.I., 1994, J NEUROSCI, V14; Lackner JR, 1998, J NEUROPHYSIOL, V80, P546, DOI 10.1152/jn.1998.80.2.546; Legenstein R, 2005, NEURAL COMPUT, V17, P2337, DOI 10.1162/0899766054796888; Maass W, 2002, NEURAL COMPUT, V14, P2531, DOI 10.1162/089976602760407955; Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026; Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318; Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901; Rumelhart D. E., 1988, COGNITIVE MODELING, V5; Sussillo D, 2009, NEURON, V63, P544, DOI 10.1016/j.neuron.2009.07.018; Thalmeier D, LEARNING UNIVERSAL C; vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724; Vertechi P., 2014, ADV NEURAL INFORM PR, P3653; Watanabe M, 2011, EUR J NEUROSCI, V34, P1697, DOI 10.1111/j.1460-9568.2011.07894.x; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102096
C	Busa-Fekete, R; Szorenyi, B; Dembczynski, K; Hullermeier, E		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Busa-Fekete, Robert; Szorenyi, Balazs; Dembczynski, Krzysztof; Huellermeier, Eyke			Online F-Measure Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The F-measure is an important and commonly used performance metric for binary prediction tasks. By combining precision and recall into a single score, it avoids disadvantages of simple metrics like the error rate, especially in cases of imbalanced class distributions. The problem of optimizing the F-measure, that is, of developing learning algorithms that perform optimally in the sense of this measure, has recently been tackled by several authors. In this paper, we study the problem of F-measure maximization in the setting of online learning. We propose an efficient online algorithm and provide a formal analysis of its convergence properties. Moreover, first experimental results are presented, showing that our method performs well in practice.	[Busa-Fekete, Robert; Huellermeier, Eyke] Univ Paderborn, Dept Comp Sci, Paderborn, Germany; [Szorenyi, Balazs] Technion, Haifa, Israel; [Szorenyi, Balazs] MTA SZTE Res Grp Artificial Intelligence, Szeged, Hungary; [Dembczynski, Krzysztof] Poznan Univ Tech, Inst Comp Sci, Poznan, Poland	University of Paderborn; Poznan University of Technology	Busa-Fekete, R (corresponding author), Univ Paderborn, Dept Comp Sci, Paderborn, Germany.	busarobi@upb.de; szorenyibalazs@gmail.com; kdembczynski@cs.put.poznan.pl; eyke@upb.de			Polish National Science Centre [2013/09/D/ST6/03917]	Polish National Science Centre	Krzysztof Dembczynski is supported by the Polish National Science Centre under grant no. 2013/09/D/ST6/03917.	Amigo E, 2013, LECT NOTES COMPUT SC, V8138, P333, DOI 10.1007/978-3-642-40802-1_31; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Busa-Fekete R, 2013, MACH LEARN, V93, P261, DOI 10.1007/s10994-013-5360-9; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; DEVROYE L, 1985, NONPARAMETRIC DENSIT; Gao W., 2013, P 30 INT C MACHINE L, P906; Hangya V., 2013, WORK NOT CLEF 2013 E; Kar P., 2014, NIPS; Koyejo O., 2014, ADV NEURAL INFORM PR, V3, P2744; Narasimhan H., 2014, NIPS; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Tsuruokas Y., 2009, P 47 ANN M ACL 4 IJC, V1, P477; VANRIJSBERGEN CJ, 1974, J DOC, V30, P365, DOI 10.1108/eb026584; Varadhan S. R. S., 2000, PROBABILITY THEORY; Waegeman W, 2014, J MACH LEARN RES, V15, P3333; Ye N., 2012, ICML; Zhao MJ, 2013, J MACH LEARN RES, V14, P1033; Zhao P., 2011, P 28 INT C MACHINE L, P233	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103032
C	Carlson, DE; Collins, E; Hsieh, YP; Carin, L; Cevher, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Carlson, David E.; Collins, Edo; Hsieh, Ya-Ping; Carin, Lawrence; Cevher, Volkan			Preconditioned Spectral Descent for Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep learning presents notorious computational challenges. These challenges include, but are not limited to, the non-convexity of learning objectives and estimating the quantities needed for optimization algorithms, such as gradients. While we do not address the non-convexity, we present an optimization solution that exploits the so far unused "geometry" in the objective function in order to best make use of the estimated gradients. Previous work attempted similar goals with preconditioned methods in the Euclidean space, such as L-BFGS, RMSprop, and ADAgrad. In stark contrast, our approach combines a non-Euclidean gradient method with preconditioning. We provide evidence that this combination more accurately captures the geometry of the objective function compared to prior work. We theoretically formalize our arguments and derive novel preconditioned non-Euclidean algorithms. The results are promising in both computational time and quality when applied to Restricted Boltzmann Machines, Feedforward Neural Nets, and Convolutional Neural Nets.	[Carlson, David E.] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Collins, Edo; Hsieh, Ya-Ping; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst LIONS, Lausanne, Switzerland; [Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27706 USA	Columbia University; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Duke University	Carlson, DE (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.			Carlson, David/0000-0003-1005-6385	ARO; DARPA; ONR; European Commission [MIRG-268398]; European Commission under ERC Future Proof; Swiss Science Foundation [SNF 200021-146750, SNF CRSII2-147633]; NCCR Marvel; DOE; NGA	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); European Commission(European CommissionEuropean Commission Joint Research Centre); European Commission under ERC Future Proof; Swiss Science Foundation(Swiss National Science Foundation (SNSF)); NCCR Marvel; DOE(United States Department of Energy (DOE)); NGA	The research reported here was funded in part by ARO, DARPA, DOE, NGA and ONR, and in part by the European Commission under grants MIRG-268398 and ERC Future Proof, by the Swiss Science Foundation under grants SNF 200021-146750, SNF CRSII2-147633, and the NCCR Marvel. We thank the reviewers for their helpful comments.	[Anonymous], 2013, ICML; Carlson D., 2016, IEEE J SPECIAL TOPIC; Carlson D., 2015, AISTATS; Cho K., 2013, NEURAL COMPUTATION; Choromanska A., 2015, AISTATS; Dauphin Y. N., 2014, NIPS; Dauphin YN, ARXIV150204390; Duchi J., 2010, JMLR; Erhan Dumitru, 2010, JMLR; Halko N., 2011, SIAM REV; Hinton G, 2002, NEURAL COMPUTATION; Hinton G., 2010, 2010003 UTML TR DEP; Hinton G.E., 2006, NEURAL COMPUTATION; Kelner J. A., 2013, ALMOST LINEAR TIME A; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Le Q. V., 2011, P ICML WASH DC US, V1261, P1262; Marlin B., 2010, ICML; Martens J., 2010, AISTATS; Martens J., 2015, ARXIV150305671; Nair V., 2010, P 27 INT C MACHINE L, P807, DOI DOI 10.5555/3104322.3104425; Neal R.M., 1998, ANNEALED IMPORTANCE; Rokhlin V., 2010, SIAM J MATRIX ANAL A; Rumelhart D., 1986, P 1986 PARALLEL DIST, P194; Salakhutdinov R., 2008, ICML; Salakhutdinov R., 2009, AISTATS; Schaul T., 2012, ARXIV12061106; Snoek J., 2012, NIPS; Tieleman T., 2009, ICML; Zeiler M.D, 2012, CORR ABS12125701	30	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101055
C	Carreira-Perpinan, MA; Vladymyrov, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Carreira-Perpinan, Miguel A.; Vladymyrov, Max			A Fast, Universal Algorithm to Learn Parametric Nonlinear Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DIMENSIONALITY REDUCTION; NEURAL-NETWORKS	Nonlinear embedding algorithms such as stochastic neighbor embedding do dimensionality reduction by optimizing an objective function involving similarities between pairs of input patterns. The result is a low-dimensional projection of each input pattern. A common way to define an out-of-sample mapping is to optimize the objective directly over a parametric mapping of the inputs, such as a neural net. This can be done using the chain rule and a nonlinear optimizer, but is very slow, because the objective involves a quadratic number of terms each dependent on the entire mapping's parameters. Using the method of auxiliary coordinates, we derive a training algorithm that works by alternating steps that train an auxiliary embedding with steps that train the mapping. This has two advantages: 1) The algorithm is universal in that a specific learning algorithm for any choice of embedding and mapping can be constructed by simply reusing existing algorithms for the embedding and for the mapping. A user can then try possible mappings and embeddings with less effort. 2) The algorithm is fast, and it can reuse N-body methods developed for nonlinear embeddings, yielding linear-time iterations.	[Carreira-Perpinan, Miguel A.] Univ Calif Merced, EECS, Merced, CA 95343 USA; [Vladymyrov, Max] UC Merced, Merced, CA USA; [Vladymyrov, Max] Yahoo Labs, Sunnyvale, CA USA	University of California System; University of California Merced; University of California System; University of California Merced	Carreira-Perpinan, MA (corresponding author), Univ Calif Merced, EECS, Merced, CA 95343 USA.	maxv@yahoo-inc.com			NSF [IIS-1423515]	NSF(National Science Foundation (NSF))	Work funded by NSF award IIS-1423515. We thank Weiran Wang for help with training the deep net in the MNIST experiment.	[Anonymous], 2008, ICML; [Anonymous], 2004, NIPS; BARNES J, 1986, NATURE, V324, P446, DOI 10.1038/324446a0; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bengio Y, 2004, NEURAL COMPUT, V16, P2197, DOI 10.1162/0899766041732396; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Carreira-Perpinan M., 2014, AISTATS; Carreira-Perpinan M. A., 2012, ARXIV12125921CSLG; Carreira-Perpinan M. A, 2010, ICML; CARREIRAPERPINA.MA, 2007, AISTATS; Globerson A., 2006, NIPS; Goldberger J., 2005, NIPS; Greengard L., 1987, J COMP PHYS, V73; Hadsell R., 2006, P CVPR; Hinton Geoffrey E, 2003, NIPS; Lowe D, 1996, NEURAL COMPUT APPL, V4, P83, DOI 10.1007/BF01413744; MAO JC, 1995, IEEE T NEURAL NETWOR, V6, P296, DOI 10.1109/72.363467; Min R, 2010, ICML; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Peltonen J, 2005, IEEE T NEURAL NETWOR, V16, P68, DOI 10.1109/TNN.2004.836194; Raziperchikolaei R., 2015, ARXIV150105352CSLG; Salakhutdinov R., 2007, AISTATS; SAMMON JW, 1969, IEEE T COMPUTERS, V18; Teh Y. W., 2003, NIPS; van der Maaten L. J. P., 2013, INT C LEARN REPR ICL; van der Maaten L. J. P., 2009, AISTATS; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venna J, 2010, J MACH LEARN RES, V11, P451; Vladymyrov M., 2014, AISTATS; Vladymyrov M., 2013, ICML; Vladymyrov M., 2012, ICML; WEBB AR, 1995, PATTERN RECOGN, V28, P753, DOI 10.1016/0031-3203(94)00135-9; Yang Z., 2013, ICML	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103010
C	Chakrabarti, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chakrabarti, Ayan			Color Constancy by Learning to Predict Chromaticity from Luminance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Color constancy is the recovery of true surface color from observed color, and requires estimating the chromaticity of scene illumination to correct for the bias it induces. In this paper, we show that the per-pixel color statistics of natural scenes-without any spatial or semantic context-can by themselves be a powerful cue for color constancy. Specifically, we describe an illuminant estimation method that is built around a "classifier" for identifying the true chromaticity of a pixel given its luminance (absolute brightness across color channels). During inference, each pixel's observed color restricts its true chromaticity to those values that can be explained by one of a candidate set of illuminants, and applying the classifier over these values yields a distribution over the corresponding illuminants. A global estimate for the scene illuminant is computed through a simple aggregation of these distributions across all pixels. We begin by simply defining the luminance-to-chromaticity classifier by computing empirical histograms over discretized chromaticity and luminance values from a training set of natural images. These histograms reflect a preference for hues corresponding to smooth reflectance functions, and for achromatic colors in brighter pixels. Despite its simplicity, the resulting estimation algorithm outperforms current state-of-the-art color constancy methods. Next, we propose a method to learn the luminanceto- chromaticity classifier "end-to-end". Using stochastic gradient descent, we set chromaticity-luminance likelihoods to minimize errors in the final scene illuminant estimates on a training set. This leads to further improvements in accuracy, most significantly in the tail of the error distribution.	[Chakrabarti, Ayan] Toyota Technol Inst, 6045 S Kenwood Ave, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Chakrabarti, A (corresponding author), Toyota Technol Inst, 6045 S Kenwood Ave, Chicago, IL 60637 USA.	ayanc@ttic.edu						Bianco S., 2010, PATTERN RECOGNITION; Bianco S., 2008, J ELECT IMAG; Bianco S., 2015, ARXIV150404548CSCV; Brainard DH, 2014, NEW VISUAL NEUROSCIENCES, P545; Buchsbaum G., 1980, J FRANKLIN I; Chakrabarti A., 2011, P CVPR; Chakrabarti A., 2012, PAMI; Chong H., 2007, P ICCV; Drew M. S., 2014, PAMI; Forsyth D., 1990, IJCV; Gehler P., 2008, CVPR; Gijsenij A., 2011, IEEE T IMAGE P; Gijsenij A., 2010, IJCV; Judd D. B., 1964, JOSA; Land E. H., 1971, SCI AM; Li B., 2014, IEEE T IMAG P; Li B, 2010, ACM T APPL PERCEPT, V8, DOI 10.1145/1857893.1857898; Lu R., 2009, ICCV; Shi L., RE PROCESSED VERSION; Srivastava N., 2014, JMLR; van de Weijer J., 2007, IEEE T IMAGE P; Xiong W., 2006, J IMAG SCI TECHNOL	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102013
C	Chaudhuri, K; Kakade, SM; Netrapalli, P; Sanghavi, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chaudhuri, Kamalika; Kakade, Sham M.; Netrapalli, Praneeth; Sanghavi, Sujay			Convergence Rates of Active Learning for Maximum Likelihood Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					An active learner is given a class of models, a large set of unlabeled examples, and the ability to interactively query labels of a subset of these examples; the goal of the learner is to learn a model in the class that fits the data well. Previous theoretical work has rigorously characterized label complexity of active learning, but most of this work has focused on the PAC or the agnostic PAC model. In this paper, we shift our attention to a more general setting - maximum likelihood estimation. Provided certain conditions hold on the model class, we provide a two-stage active learning algorithm for this problem. The conditions we require are fairly general, and cover the widely popular class of Generalized Linear Models, which in turn, include models for binary and multi-class classification, regression, and conditional random fields. We provide an upper bound on the label requirement of our algorithm, and a lower bound that matches it up to lower order terms. Our analysis shows that unlike binary classification in the realizable case, just a single extra round of interaction is sufficient to achieve near-optimal performance in maximum likelihood estimation. On the empirical side, the recent work in [12] and [13] (on active linear and logistic regression) shows the promise of this approach.	[Chaudhuri, Kamalika] Univ Calif San Diego, Dept CS, La Jolla, CA 92093 USA; [Kakade, Sham M.] Univ Washington, Dept CS & Stat, Seattle, WA 98195 USA; [Netrapalli, Praneeth] Microsoft Res New England, Cambridge, MA USA; [Sanghavi, Sujay] Univ Texas Austin, Dept ECE, Austin, TX 78712 USA	University of California System; University of California San Diego; University of Washington; University of Washington Seattle; Microsoft; University of Texas System; University of Texas Austin	Chaudhuri, K (corresponding author), Univ Calif San Diego, Dept CS, La Jolla, CA 92093 USA.	kamalika@cs.ucsd.edu; sham@cs.washington.edu; praneeth@microsoft.com; sanghavi@mail.utexas.edu			NSF [IIS 1162581]	NSF(National Science Foundation (NSF))	KC thanks NSF under IIS 1162581 for research support.	Agarwal A., 2013, INT C MACHINE LEARNI, P1220; Balcan M. F, 2013, COLT; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Beygelzimer A., 2010, NIPS; Cam L., 2000, ASYMPTOTICS STAT SOM; Cornell J.A., 2002, EXPT MIXTURES DESIGN, V3rd, DOI [10.1002/9781118204221, DOI 10.1002/9781118204221, DOI 10.1198/004017002320256620]; Dasgupta S., 2008, ICML; Dasgupta S., 2007, NIPS; DASGUPTA S, 2005, NIPS; Dasgupta S, 2011, THEOR COMPUT SCI, V412, P1767, DOI 10.1016/j.tcs.2010.12.054; Frostig R., 2014, ARXIV14126606; Gu Q., 2012, P ADV NEUR INF PROC; Gu Q., 2014, 30 C UNC ART INT UAI; Hanneke S, 2007, ICML; Kaariainen M., 2006, ALT; Le Cam L., 1986, ASYMPTOTIC METHODS S; Lehmann E. L., 1998, THEORY POINT ESTIMAT, V31; Nowak RD, 2011, IEEE T INFORM THEORY, V57, P7893, DOI 10.1109/TIT.2011.2169298; Sabato S., 2014, NIPS; Urner R., 2013, COLT; Van der Vaart A. W., 2000, ASYMPTOTIC STAT; Zhang C., 2014, P NEUR INF PROC SYST	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102069
C	Chen, CY; Ding, N; Carin, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Changyou; Ding, Nan; Carin, Lawrence			On the Convergence of Stochastic Gradient MCMC Algorithms with High-Order Integrators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recent advances in Bayesian learning with large-scale data have witnessed emergence of stochastic gradient MCMC algorithms (SG-MCMC), such as stochastic gradient Langevin dynamics (SGLD), stochastic gradient Hamiltonian MCMC (SGHMC), and the stochastic gradient thermostat. While finite-time convergence properties of the SGLD with a 1st-order Euler integrator have recently been studied, corresponding theory for general SG-MCMCs has not been explored. In this paper we consider general SG-MCMCs with high-order integrators, and develop theory to analyze finite-time convergence properties and their asymptotic invariant measures. Our theoretical results show faster convergence rates and more accurate invariant measures for SG-MCMCs with higher-order integrators. For example, with the proposed efficient 2nd-order symmetric splitting integrator, the mean square error (MSE) of the posterior average for the SGHMC achieves an optimal convergence rate of L-4/5 at L iterations, compared to L-2/3 for the SGHMC and SGLD with 1st-order Euler integrators. Furthermore, convergence results of decreasing-step-size SG-MCMCs are also developed, with the same convergence rates as their fixed-step-size counterparts for a specific decreasing sequence. Experiments on both synthetic and real datasets verify our theory, and show advantages of the proposed method in two large-scale real applications.	[Chen, Changyou; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA; [Ding, Nan] Google Inc, Venice, CA USA	Duke University; Google Incorporated	Chen, CY (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	cchangyou@gmail.com; dingnan@google.com; lcarin@duke.edu			ARO; DARPA; DOE; NGA; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research)	Supported in part by ARO, DARPA, DOE, NGA and ONR. We acknowledge Jonathan C. Mattingly and Chunyuan Li for inspiring discussions; David Carlson for the AIS codes.	Abdulle A, 2015, SIAM J NUMER ANAL, V53, P1, DOI 10.1137/140962644; Betancourt M., 2015, ICML; Blei D. M., 2003, JMLR; Chen T., 2014, ICML; Debussche A, 2012, SIAM J NUMER ANAL, V50, P1735, DOI 10.1137/110831544; Ding N., 2014, NIPS; Gan Z., 2015, AISTATS; Gan Z., 2015, ICML; Giesl P, 2007, LECT NOTES MATH, V1904, P1, DOI 10.1007/978-3-540-69909-5; Hasminskii R.Z.., 2012, STOCHASTIC STABILITY, VSecond; Hoffman M. D., 2010, NEURAL; Kopec M., 2014, IMA J NUMER ANAL; Kryloff N, 1937, ANN MATH, V38, P65, DOI 10.2307/1968511; Leimkuhler B., 2015, ARXIV150506889V1 U E; Leimkuhler B, 2013, APPL MATH RES EXPRES, P34, DOI 10.1093/amrx/abs010; Mattingly JC, 2010, SIAM J NUMER ANAL, V48, P552, DOI 10.1137/090770527; Mnih A., 2014, ICML; Patterson S., 2013, NIPS; Risken H., 1989, SPRINGER SERIES SYNE, DOI DOI 10.1007/978-3-642-61544-3; Salakhutdinov R., 2008, ICML; Sato I., 2014, ICML; Teh Y. W., 2014, ARXIV14090578 U OXF; Vollmer S. J., 2015, ARXIV150100438 U OXF; Welling M., 2011, ICML	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100064
C	Chen, S; Banerjee, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Sheng; Banerjee, Arindam			Structured Estimation with Atomic Norms: General Bounds and Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				REGRESSION; SELECTION	For structured estimation problems with atomic norms, recent advances in the literature express sample complexity and estimation error bounds in terms of certain geometric measures, in particular Gaussian width of the unit norm ball, Gaussian width of a spherical cap induced by a tangent cone, and a restricted norm compatibility constant. However, given an atomic norm, bounding these geometric measures can be difficult. In this paper, we present general upper bounds for such geometric measures, which only require simple information of the atomic norm under consideration, and we establish tightness of these bounds by providing the corresponding lower bounds. We show applications of our analysis to certain atomic norms, especially k-support norm, for which existing result is incomplete.	[Chen, Sheng; Banerjee, Arindam] Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Chen, S (corresponding author), Univ Minnesota, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	shengc@cs.umn.edu; banerjee@cs.umn.edu			NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.	Amelunxen D, 2014, INF INFERENCE, V3, P224, DOI 10.1093/imaiai/iau005; Argyriou A., 2012, ADV NEURAL INFORM PR; Banerjee A., 2014, ADV NEURAL INFORM PR; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Bogdan M., 2013, ARXIV13101969, P1; Cai T. T., 2014, ARXIV14044408; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Candes E, 2013, MATH PROGRAM, V141, P577, DOI 10.1007/s10107-012-0540-0; Candes EJ, 2006, COMMUN PUR APPL MATH, V59, P1207, DOI 10.1002/cpa.20124; Chatterjee S., 2014, ADV NEURAL INFORM PR; Chen S., 2015, INT C ART INT STAT A; Figueiredo Mario AT, 2014, ARXIV14094005; GORDON Y, 1985, ISR J MATH, V50, P265, DOI 10.1007/BF02759761; Jacob L., 2009, INT C MACH LEARN ICM; Maurer A., 2014, C LEARN THEOR COLT; McDonald A. M., 2014, ADV NEURAL INFORM PR; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Oymak S., 2013, ARXIV13110830; Plan Y, 2013, IEEE T INFORM THEORY, V59, P482, DOI 10.1109/TIT.2012.2207945; Rao N., 2012, INT C ART INT STAT A; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2015, APPL NUMER HARMON AN, P67, DOI 10.1007/978-3-319-19749-4_2; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zeng X., 2014, ARXIV14094271; Zhang X., 2013, ADV NEURAL INFORM PR	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101093
C	Chen, YX; Candes, EJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Yuxin; Candes, Emmanuel J.			Solving Random Quadratic Systems of Equations Is Nearly as Easy as Solving Linear Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PHASE RETRIEVAL	This paper is concerned with finding a solution x to a quadratic system of equations y(i) = vertical bar < a(i); x >vertical bar(2), i = 1, . . . , m. We demonstrate that it is possible to solve unstructured random quadratic systems in n variables exactly from O(n) equations in linear time, that is, in time proportional to reading the data {a(i)} and {y(i)}. This is accomplished by a novel procedure, which starting from an initial guess given by a spectral initialization procedure, attempts to minimize a nonconvex objective. The proposed algorithm distinguishes from prior approaches by regularizing the initialization and descent procedures in an adaptive fashion, which discard terms bearing too much influence on the initial estimate or search directions. These careful selection rules-which effectively serve as a variance reduction scheme-provide a tighter initial guess, more robust descent directions, and thus enhanced practical performance. Further, this procedure also achieves a nearoptimal statistical accuracy in the presence of noise. Empirically, we demonstrate that the computational cost of our algorithm is about four times that of solving a least-squares problem of the same size.	[Chen, Yuxin; Candes, Emmanuel J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Candes, Emmanuel J.] Stanford Univ, Dept Math, Stanford, CA 94305 USA	Stanford University; Stanford University	Chen, YX (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	yxchen@stanfor.edu; candes@stanford.edu			NSF [CCF-0963835]; Math + X Award from the Simons Foundation; NSF	NSF(National Science Foundation (NSF)); Math + X Award from the Simons Foundation; NSF(National Science Foundation (NSF))	E. C. is partially supported by NSF under grant CCF-0963835 and by the Math + X Award from the Simons Foundation. Y. C. is supported by the same NSF grant.	[Anonymous], 2014, THESIS STANFORD U ST; [Anonymous], 2001, LECT MODERN CONVEX O; Arora S., 2015, C LEARN THEOR COLT; Balakrishnan Sivaraman, 2014, ARXIV14082156; Cai T., ANN STATS; Candes E. J., 2014, APPL COMPUTATIONAL H; Candes E. J., 2013, COMMUNICATIONS PURE, V66, P1017; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z; Chen Y., 2014, C LEARN THEOR COLT; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; Gross D, 2015, J FOURIER ANAL APPL, V21, P229, DOI 10.1007/s00041-014-9361-2; Jaganathan Kishore, 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P1473, DOI 10.1109/ISIT.2012.6283508; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Netrapalli P., 2013, NIPS; Ohlsson H., 2012, ADV NEURAL INFORM PR; Repetti A, 2014, IEEE IMAGE PROC, P1753, DOI 10.1109/ICIP.2014.7025351; Shechtman Y, 2014, IEEE T SIGNAL PROCES, V62, P928, DOI 10.1109/TSP.2013.2297687; Shechtman Y, 2011, OPT EXPRESS, V19, P14807, DOI 10.1364/OE.19.014807; Sun J., 2015, ICML; Sun R., 2015, FOCS; Trefethen L. N., 1997, NUMERICAL LINEAR ALG, V50; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wei K., 2015, ARXIV150201822; White C. D., 2015, ARXIV150607868; Yi  X., 2014, INT C MACH LEARN	29	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101003
C	Cho, M; Dhir, CS; Lee, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Cho, Minhyung; Dhir, Chandra Shekhar; Lee, Jaehyung			Hessian-free Optimization for Learning Deep Multidimensional Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Multidimensional recurrent neural networks (MDRNNs) have shown a remarkable performance in the area of speech and handwriting recognition. The performance of an MDRNN is improved by further increasing its depth, and the difficulty of learning the deeper network is overcome by using Hessian-free (HF) optimization. Given that connectionist temporal classification (CTC) is utilized as an objective of learning an MDRNN for sequence labeling, the non-convexity of CTC poses a problem when applying HF to the network. As a solution, a convex approximation of CTC is formulated and its relationship with the EM algorithm and the Fisher information matrix is discussed. An MDRNN up to a depth of 15 layers is successfully trained using HF, resulting in an improved performance for sequence labeling.	[Cho, Minhyung; Dhir, Chandra Shekhar; Lee, Jaehyung] Gracenote Inc, Appl Res Korea, Seoul, South Korea		Cho, M (corresponding author), Gracenote Inc, Appl Res Korea, Seoul, South Korea.	mhyung.cho@gmail.com; shekhardhir@gmail.com; jaehyung.lee@kaist.ac.kr						Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Bishop Christopher M., 2007, PATTERN RECOGNITION, V4, DOI 10.1117/1.2819119; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; DARPA-ISTO, 1990, SPEECH DISC CD1 1 1; Graves A, 2012, ICML REPRESENTATION; Graves A., 2008, ADV NEURAL INFORM PR, V20, P1; Graves A., 2008, ADV NEURAL INFORM PR, P545, DOI DOI 10.1007/978-1-4471-4072-6; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Graves A., 2006, P 23 INT C MACH LEAR, P369; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Graves Alex, 2008, RNNLIB RECURRENT NEU; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546; Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27; Martens J., 2010, P 27 INT C MACH LEAR, P735; Park H, 2000, NEURAL NETWORKS, V13, P755, DOI 10.1016/S0893-6080(00)00051-4; Pascanu R., 2014, ICLR; PEARLMUTTER BA, 1994, NEURAL COMPUT, V6, P147, DOI 10.1162/neco.1994.6.1.147; Pechwitz M., 2002, PROC CIFED, V2, P127; Romero Adriana, 2014, CORR; Schraudolph NN, 2002, NEURAL COMPUT, V14, P1723, DOI 10.1162/08997660260028683; Sutskever I., 2011, P 28 INT C MACH LEAR, P1033, DOI DOI 10.1145/346152.346166; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100032
C	Choromanska, A; Langford, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Choromanska, Anna; Langford, John			Logarithmic Time Online Multiclass prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of multiclass classification with an extremely large number of classes (k), with the goal of obtaining train and test time complexity logarithmic in the number of classes. We develop top-down tree construction approaches for constructing logarithmic depth trees. On the theoretical front, we formulate a new objective function, which is optimized at each node of the tree and creates dynamic partitions of the data which are both pure (in terms of class labels) and balanced. We demonstrate that under favorable conditions, we can construct logarithmic depth trees that have leaves with low label entropy. However, the objective function at the nodes is challenging to optimize computationally. We address the empirical problem with a new online decision tree construction procedure. Experiments demonstrate that this online algorithm quickly achieves improvement in test error compared to more common logarithmic training time approaches, which makes it a plausible method in computationally constrained large-k applications.	[Choromanska, Anna] Courant Inst Math Sci, New York, NY 10012 USA; [Langford, John] Microsoft Res, New York, NY USA	Microsoft	Choromanska, A (corresponding author), Courant Inst Math Sci, New York, NY 10012 USA.	achoroma@cims.nyu.edu; jcl@microsoft.com						Agarwal A., 2014, ICML; Agarwal R., 2013, WWW; Beijbom O., 2014, ICML; Bengio S., 2010, NIPS; Bennett P.N., 2009, SIGIR; Beygelzimer A., 2009, ALT; Beygelzimer A., 2009, UAI; Bishop C.M, 2006, PATTERN RECOGN; Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142; Carnap R., 1962, LOGICAL FDN PROBABIL, P468; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Deng J., 2011, ADV NEURAL INFORM PR; Hsu D.J., 2009, NIPS, P772; Kearns M, 1999, J COMPUT SYST SCI, V58, P109, DOI 10.1006/jcss.1997.1543; Liu T. -Y., 2005, SIGKDD EXPLORATIONS; Madzarov G, 2009, INFORM-J COMPUT INFO, V33, P225; Montillo A., 2013, DECISION FORESTCOM; Prabhu Y., 2014, ACM SIGKDD; Rifkin R, 2004, J MACH LEARN RES, V5, P101; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Tentori K, 2007, COGNITION, V103, P107, DOI 10.1016/j.cognition.2005.09.006; Weston J., 2013, ICML; Yu H.-F., 2014, ICML; Zhao B., 2013, CVPR	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102085
C	Chow, Y; Tamar, A; Mannor, S; Pavone, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chow, Yinlam; Tamar, Aviv; Mannor, Shie; Pavone, Marco			Risk-Sensitive and Robust Decision-Making: a CVaR Optimization Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				VARIANCE	In this paper we address the problem of decision making within a Markov decision process (MDP) framework where risk and modeling errors are taken into account. Our approach is to minimize a risk-sensitive conditional-value-at-risk (CVaR) objective, as opposed to a standard risk-neutral expectation. We refer to such problem as CVaR MDP. Our first contribution is to show that a CVaR objective, besides capturing risk sensitivity, has an alternative interpretation as expected cost under worst-case modeling errors, for a given error budget. This result, which is of independent interest, motivates CVaR MDPs as a unifying framework for risk-sensitive and robust decision making. Our second contribution is to present an approximate value-iteration algorithm for CVaR MDPs and analyze its convergence rate. To our knowledge, this is the first solution algorithm for CVaR MDPs that enjoys error guarantees. Finally, we present results from numerical experiments that corroborate our theoretical findings and show the practicality of our approach.	[Chow, Yinlam; Pavone, Marco] Stanford Univ, Stanford, CA 94305 USA; [Tamar, Aviv] Univ Calif Berkeley, Berkeley, CA USA; [Mannor, Shie] Technion, Haifa, Israel	Stanford University; University of California System; University of California Berkeley	Chow, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ychow@stanford.edu; avivt@berkeley.edu; shie@ee.technion.ac.il; pavone@stanford.edu		Pavone, Marco/0000-0002-0206-4337	Croucher Foundation; Office of Naval Research, Science of Autonomy Program [N00014-15-1-2673]; European Community [306638]	Croucher Foundation; Office of Naval Research, Science of Autonomy Program(Office of Naval Research); European Community(European Commission)	The authors would like to thank Mohammad Ghavamzadeh for helpful comments on the technical details, and Daniel Vainsencher for practical optimization advice. Y-L. Chow and M. Pavone are partially supported by the Croucher Foundation doctoral scholarship and the Office of Naval Research, Science of Autonomy Program, under Contract N00014-15-1-2673. Funding for Shie Mannor and Aviv Tamar were partially provided by the European Community's Seventh Framework Programme (FP7/2007-2013) under grant agreement 306638 (SUPREL).	Artzner P, 1999, MATH FINANC, V9, P203, DOI 10.1111/1467-9965.00068; Baeuerle N, 2011, MATH METHOD OPER RES, V74, P361, DOI 10.1007/s00186-011-0367-0; Bertsekas D. P., 2012, APPROXIMATE DYNAMIC, V2; Borkar V, 2014, IEEE T AUTOMAT CONTR, V59, P2574, DOI 10.1109/TAC.2014.2309262; Chow Y., 2014, P ADV NEURAL INFORM, P3509; Dowd K., 2007, MEASURING MARKET RIS; Haskell W., 2014, SIAM J CONTROL OPTIM; HOWARD RA, 1972, MANAGE SCI, V18, P356, DOI 10.1287/mnsc.18.7.356; Iyengar G., 2005, MATH OPER RES, V30; Iyengar G, 2013, ANN OPER RES, V205, P203, DOI 10.1007/s10479-012-1245-8; Kalinchenko K, 2010, CARISMA C; Mannor S., 2012, P 29 INT C MACH LEAR, P385; Mannor S, 2007, MANAGE SCI, V53, P308, DOI 10.1287/mnsc.1060.0614; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; Nilim A, 2005, OPER RES, V53, P780, DOI 10.1287/opre.1050.0216; Osogami, 2012, ADV NEURAL INFORM PR, V1, P233; Pflug G., 2015, OPTIMIZATION; Phillips M., 2003, INTERPOLATION APPROX, V14; Prashanth LA, 2014, LECT NOTES ARTIF INT, V8776, P155, DOI 10.1007/978-3-319-11662-4_12; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; Rockafellar RT, 2006, J BANK FINANC, V30, P743, DOI 10.1016/j.jbankfin.2005.04.004; Serraino G., 2013, ENCY OPERATIONS RES, P258, DOI DOI 10.1007/978-1-4419-1153-7_1232; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; Tamar A., 2015, AAAI; Xu H, 2006, ADV NEURAL INFORM PR, P1537	27	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103052
C	Colin, I; Salmon, J; Clemencon, S; Bellet, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Colin, Igor; Salmon, Joseph; Clemencon, Stephan; Bellet, Aurelien			Extending Gossip Algorithms to Distributed Estimation of U-Statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Efficient and robust algorithms for decentralized estimation in networks are essential to many distributed systems. Whereas distributed estimation of sample mean statistics has been the subject of a good deal of attention, computation of U-statistics, relying on more expensive averaging over pairs of observations, is a less investigated area. Yet, such data functionals are essential to describe global properties of a statistical population, with important examples including Area Under the Curve, empirical variance, Gini mean difference and within-cluster point scatter. This paper proposes new synchronous and asynchronous randomized gossip algorithms which simultaneously propagate data across the network and maintain local estimates of the U-statistic of interest. We establish convergence rate bounds of O(1/t) and O(log t/t) for the synchronous and asynchronous cases respectively, where t is the number of iterations, with explicit data and network dependent terms. Beyond favorable comparisons in terms of rate analysis, numerical experiments provide empirical evidence the proposed algorithms surpasses the previously introduced approach.	[Colin, Igor; Salmon, Joseph; Clemencon, Stephan] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France; [Bellet, Aurelien] INRIA Lille Nord Europe, Magnet Team, F-59650 Villeneuve Dascq, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Colin, I (corresponding author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.	igor.colin@telecom-paristech.fr; joseph.salmon@telecom-paristech.fr; stephan.clemencon@telecom-paristech.fr; aurelien.bellet@inria.fr			chair Machine Learning for Big Data of Telecom ParisTech	chair Machine Learning for Big Data of Telecom ParisTech	This work was supported by the chair Machine Learning for Big Data of Telecom ParisTech, and was conducted when A. Bellet was affiliated with Telecom ParisTech.	Bollobas B, 1998, GRAD TEXT M, V184; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; Chung FR, 1997, SPECTRAL GRAPH THEOR, V92; Dimakis ADG, 2008, IEEE T SIGNAL PROCES, V56, P1205, DOI 10.1109/TSP.2007.908946; Dimakis AG, 2010, P IEEE, V98, P1847, DOI 10.1109/JPROC.2010.2052531; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747; Karp R, 2000, ANN IEEE SYMP FOUND, P565, DOI 10.1109/SFCS.2000.892324; Kempe D, 2003, ANN IEEE SYMP FOUND, P482, DOI 10.1109/SFCS.2003.1238221; Kowalczyk W., 2004, ADV NEURAL INFORM PR, P713; Lee A.J., 2019, U STAT THEORY PRACTI; Li WJ, 2010, IEEE T INFORM THEORY, V56, P6208, DOI 10.1109/TIT.2010.2081030; MANN HB, 1947, ANN MATH STAT, V18, P50, DOI 10.1214/aoms/1177730491; Mosk-Aoyama D, 2008, IEEE T INFORM THEORY, V54, P2997, DOI 10.1109/TIT.2008.924648; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Pelckmans Kristiaan, 2009, IFAC P, V42, P48; Shah D, 2008, FOUND TRENDS NETW, V3, P1, DOI 10.1561/1300000014; Tsitsiklis J., 1984, THESIS; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918	20	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101007
C	Comanici, G; Precup, D; Panangaden, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Comanici, Gheorghe; Precup, Doina; Panangaden, Prakash			Basis Refinement Strategies for Linear Value Function Approximation in MDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We provide a theoretical framework for analyzing basis function construction for linear value function approximation in Markov Decision Processes (MDPs). We show that important existing methods, such as Krylov bases and Bellman-error-based methods are a special case of the general framework we develop. We provide a general algorithmic framework for computing basis function refinements which "respect" the dynamics of the environment, and we derive approximation error bounds that apply for any algorithm respecting this general framework. We also show how, using ideas related to bisimulation metrics, one can translate basis refinement into a process of finding "prototypes" that are diverse enough to represent the given MDP.	[Comanici, Gheorghe; Precup, Doina; Panangaden, Prakash] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada	McGill University	Comanici, G (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.	gcoman@cs.mcgill.ca; dprecup@cs.mcgill.ca; prakash@cs.mcgill.ca	Panangaden, Prakash/GOH-1547-2022					[Anonymous], 2005, P 22 INT C MACH LEAR, DOI DOI 10.1145/1102351.1102421; Barreto A., 2011, ADV NEURAL INFORM PR, V24, P720; Bertsekas D. P., 1989, IEEE T AUTOMATIC CON, V34; Blackwell D., 1965, ANN MATH STAT, V36, P226, DOI DOI 10.1214/AOMS/1177700285; Comanici G., 2011, AAAI; Desharnais J, 2004, THEOR COMPUT SCI, V318, P323, DOI 10.1016/j.tcs.2003.09.013; Desharnais J., 1999, CONCUR; Ferns Norm, 2004, UAI, P162; Geramifard A., 2011, P 28 INT C MACH LEAR, P881; Givan R, 2003, ARTIF INTELL, V147, P163, DOI 10.1016/S0004-3702(02)00376-4; Jong N, 2006, ICML WORKSH KERN MAC; Keller PW, 2006, P 23 INT C MACH LEAR, P449; Konidaris G., 2011, AAAI, V380, P5; LARSEN KG, 1991, INFORM COMPUT, V94, P1, DOI 10.1016/0890-5401(91)90030-6; Munos R, 2002, MACH LEARN, V49, P291, DOI 10.1023/A:1017992615625; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Parr R., 2008, P 25 INT C MACH LEAR, P752; Parr R., 2008, ICML, P737; Ravindran B., 2002, Abstraction, Reformulation, and Approximation. 5th International Symposium, SARA 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2371), P196; Ruan SS, 2015, AAAI CONF ARTIF INTE, P3578; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Villani C., 2003, TOPICS OPTIMAL TRANS; Yu H., 2006, TECHNICAL REPORT	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100062
C	Dann, C; Brunskill, E		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dann, Christoph; Brunskill, Emma			Sample Complexity of Episodic Fixed-Horizon Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recently, there has been significant progress in understanding reinforcement learning in discounted infinite-horizon Markov decision processes (MDPs) by deriving tight sample complexity bounds. However, in many real-world applications, an interactive learning agent operates for a fixed or bounded period of time, for example tutoring students for exams or handling customer service requests. Such scenarios can often be better treated as episodic fixed-horizon MDPs, for which only looser bounds on the sample complexity exist. A natural notion of sample complexity in this setting is the number of episodes required to guarantee a certain performance with high probability (PAC guarantee). In this paper, we derive an upper PAC bound (O) over tilde(vertical bar S vertical bar(2) vertical bar A vertical bar H-2/epsilon(2) In 1/delta) and a lower PAC bound (Omega) over tilde(O) over tilde(vertical bar S vertical bar(2) vertical bar A vertical bar H-2/epsilon(2) In 1/delta+c) that match up to log-terms and an additional linear dependency on the number of states vertical bar S vertical bar. The lower bound is the first of its kind for this setting. Our upper bound leverages Bernstein's inequality to improve on previous bounds for episodic finitehorizon MDPs which have a time-horizon dependency of at least H-3.	[Dann, Christoph] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Brunskill, Emma] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Dann, C (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	cdann@cdann.net; ebrun@cs.cmu.edu			NSF CAREER award; ONR Young Investigator Program	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR Young Investigator Program	We thank Tor Lattimore for the helpful suggestions and comments. This work was supported by an NSF CAREER award and the ONR Young Investigator Program.	Auer Peter, 2005, P 1 AUSTR COGN VIS W; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Chung F, 2006, INTERNET MATH, V3, P79, DOI 10.1080/15427951.2006.10129115; Fiechter Claude-Nicolas, 1997, INT C MACH LEARN; Fiechter Claude-Nicolas, 1994, C LEARN THEOR; Gheshlaghi Azar Mohammad, 2012, INT C MACH LEARN; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Jaksch Thomas, 2010, ADV NEURAL INFORM PR; Kakade Sham M., 2003, THESIS; Kearns Michael J, 1999, ADV NEURAL INFORM PR; Kolter J. Z, 2009, INT C MACH LEARN; Lattimore Tor, 2012, INT C ALG LEARN THEO; Mannor S, 2004, J MACH LEARN RES, V5, P623; MAURER A., 2009, C LEARN THEOR; Reveliotis S, 2007, DISCRETE EVENT DYN S, V17, P307, DOI 10.1007/s10626-007-0014-3; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Strehl AL, 2009, J MACH LEARN RES, V10, P2413; Strehl Alexander L, 2006, C UNC ART INT; Strehl Alexander L., 2006, INT C MACH LEARN; Szepesvari C., 2010, INT C MACH LEARN	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101086
C	De Sa, C; Zhang, C; Olukotun, K; Re, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		De Sa, Christopher; Zhang, Ce; Olukotun, Kunle; Re, Christopher			Taming the Wild: A Unified Analysis of HOGWILD!-Style Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Stochastic gradient descent (SGD) is a ubiquitous algorithm for a variety of machine learning problems. Researchers and industry have developed several techniques to optimize SGD's runtime performance, including asynchronous execution and reduced precision. Our main result is a martingale-based analysis that enables us to capture the rich noise models that may arise from such techniques. Specifically, we use our new analysis in three ways: (1) we derive convergence rates for the convex case (HOGWILD!) with relaxed assumptions on the sparsity of the problem; (2) we analyze asynchronous SGD algorithms for non-convex matrix problems including matrix completion; and (3) we design and analyze an asynchronous SGD algorithm, called BUCKWILD!, that uses lower-precision arithmetic. We show experimentally that our algorithms run efficiently for a variety of problems on modern hardware.	[De Sa, Christopher] Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA; Stanford Univ, Dept Comp Sci, Stanford, CA 94309 USA	Stanford University; Stanford University	De Sa, C (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94309 USA.	cdesa@stanford.edu; czhang@cs.wisc.edu; kunle@stanford.edu; chrismre@stanford.edu			DARPA [FA8750-12-2-0335, FA8750-13-2-0039]; NSF [IIS-1247701, CCF-1111943, CCF-1337375, IIS-1353606]; DOE [108845]; ONR [N000141210041, N000141310129]; NIH [U54EB020405]; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; Toshiba	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE)); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Oracle; NVIDIA; Huawei(Huawei Technologies); SAP Labs; Sloan Research Fellowship(Alfred P. Sloan Foundation); Moore Foundation(Gordon and Betty Moore Foundation); American Family Insurance; Google(Google Incorporated); Toshiba	The BUCKWILD! name arose out of conversations with Benjamin Recht. Thanks also to Madeleine Udell for helpful conversations. The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba.	[Anonymous], 2012, ICML; Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; De Sa Christopher, 2015, ICML; Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659; Fercoq O., 2013, ARXIV13125799; Fleming Thomas R, 1991, COUNTING PROCESSES S, V169, P56; Gupta P., 2013, P 22 INT C WORLD WID, P505, DOI [DOI 10.1145/2488388.2488433, 10.1145/2488388.2488433]; Gupta S., 2015, ICML; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Johansson B, 2009, SIAM J OPTIMIZ, V20, P1157, DOI 10.1137/08073038X; Konecny Jakub, 2014, NIPS OPT MACH LEARN; LeCun Y., 1998, NEURAL NETWORKS TRIC; Liu J, 2015, J MACH LEARN RES, V16, P285; Liu J, 2015, SIAM J OPTIMIZ, V25, P351, DOI 10.1137/140961134; Mitliagkas Ioannis, 2015, PROC VLDB ENDOW; Osindero S., 2014, DOGWILD DISTRIBUTED; Parambath Shameem Ahamed Puthiya, 2013, MATRIX FACTORIZATION; Qing Tao, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P537, DOI 10.1007/978-3-642-33460-3_40; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Richtarik P., 2012, MATH PROGRAM, P1; Tappenden Rachael, 2015, ARXIV150303033; Yu HF, 2012, IEEE DATA MINING, P765, DOI 10.1109/ICDM.2012.168; Zhang C., 2014, PVLDB	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100085
C	Dehaene, G; Barthelme, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dehaene, Guillaume; Barthelme, Simon			Bounding errors of Expectation-Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Expectation Propagation is a very popular algorithm for variational inference, but comes with few theoretical guarantees. In this article, we prove that the approximation errors made by EP can be bounded. Our bounds have an asymptotic interpretation in the number n of datapoints, which allows us to study EP's convergence with respect to the true posterior. In particular, we show that EP converges at a rate of O(n(-2)) for the mean, up to an order of magnitude faster than the traditional Gaussian approximation at the mode. We also give similar asymptotic expansions for moments of order 2 to 4, as well as excess Kullback-Leibler cost (defined as the additional KL cost incurred by using EP rather than the ideal Gaussian approximation). All these expansions highlight the superior convergence properties of EP. Our approach for deriving those results is likely applicable to many similar approximate inference methods. In addition, we introduce bounds on the moments of log-concave distributions that may be of independent interest.	[Dehaene, Guillaume] Univ Geneva, Geneva, Switzerland; [Barthelme, Simon] CNRS, Gipsa lab, Paris, France	University of Geneva; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Universite Paris Cite	Dehaene, G (corresponding author), Univ Geneva, Geneva, Switzerland.	guillaume.dehaene@gmail.com; simon.barthelme@gipsa-lab.fr						Bishop Christopher M., 2006, CORR; BRASCAMP HJ, 1976, ADV MATH, V20, P151, DOI 10.1016/0001-8708(76)90184-5; DasGupta A, 2008, SPRINGER TEXTS STAT, P1; Dehaene Guillaume, 2015, TECHNICAL REPORT; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Minka T.P., 2001, P 17 C UNC ART INT, P362; Minka Tom, 2005, TECHNICAL REPORT; Nickisch H, 2008, J MACH LEARN RES, V9, P2035; Raymond Jack, 2014, EXPECTATION PROPAGAT; Seeger M. W., 2005, EXPECTATION PROPAGAT	11	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102061
C	Domke, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Domke, Justin			Maximum Likelihood LearningWith Arbitrary Treewidth via Fast-Mixing Parameter Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				POLYNOMIAL-TIME; MODELS	Inference is typically intractable in high-treewidth undirected graphical models, making maximum likelihood learning a challenge. One way to overcome this is to restrict parameters to a tractable set, most typically the set of tree-structured parameters. This paper explores an alternative notion of a tractable set, namely a set of "fast-mixing parameters" where Markov chainMonte Carlo (MCMC) inference can be guaranteed to quickly converge to the stationary distribution. While it is common in practice to approximate the likelihood gradient using samples obtained from MCMC, such procedures lack theoretical guarantees. This paper proves that for any exponential family with bounded sufficient statistics, (not just graphical models) when parameters are constrained to a fast-mixing set, gradient descent with gradients approximated by sampling will approximate the maximum likelihood solution inside the set with high-probability. When unregularized, to find a solution epsilon-accurate in log-likelihood requires a total amount of effort cubic in 1/epsilon,disregarding logarithmic factors. When ridge-regularized, strong convexity allows a solution epsilon-accurate in parameter distance with effort quadratic in 1/epsilon. Both of these provide of a fully-polynomial time randomized approximation scheme.	[Domke, Justin] Australian Natl Univ, NICTA, Canberra, ACT, Australia	Australian National University	Domke, J (corresponding author), Australian Natl Univ, NICTA, Canberra, ACT, Australia.	justin.domke@nicta.com.au			Australian Government through the Dept. of Communications; Australian Research Council through the ICT Centre of Excellence Program	Australian Government through the Dept. of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	Thanks to Ivona Bezakova, Aaron Defazio, Nishant Mehta, Aditya Menon, Cheng Soon Ong and Christfried Webers. NICTA is funded by the Australian Government through the Dept. of Communications and the Australian Research Council through the ICT Centre of Excellence Program.	Abbeel P, 2006, J MACH LEARN RES, V7, P1743; Asuncion A., 2010, AISTATS; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Carreira-Peripinan M. A, 2005, AISTATS; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Descombes X, 1999, IEEE T IMAGE PROCESS, V8, P954, DOI 10.1109/83.772239; Domke J., 2013, NIPS; Dyer M, 2009, ANN APPL PROBAB, V19, P71, DOI 10.1214/08-AAP532; Geyer C, 1991, S INT; Gu MG, 2001, J ROY STAT SOC B, V63, P339, DOI 10.1111/1467-9868.00289; Hayes T, 2006, FOCS; Heinemann U., 2014, ICML; Hinton G., 2010, TECHNICAL REPORT; Hinton G. E, 2009, NIPS; Huber M, 2011, J STAT THEORY PRACT, V5, P413, DOI 10.1080/15598608.2011.10412038; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; JERRUM M, 1993, SIAM J COMPUT, V22, P1087, DOI 10.1137/0222066; Koller D., 2009, PROBABILISTIC GRAPHI; Lindsay BG, 1988, CONT MATH, V80, P221, DOI DOI 10.1090/CONM/080/999014; Liu X, 2014, NIPS; Marlin B., 2011, UAI; Mizrahi Y, 2014, ICML; Papandreou G., 2011, ICCV; SCHMIDT M, 2011, NIPS; SCHMIDT U, 2010, CVPR; Steinhardt J., 2015, ICML; Tieleman T., 2008, ICML; Varin C, 2011, STAT SINICA, V21, P5; Wainwright MJ, 2006, J MACH LEARN RES, V7, P1829; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420	33	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102056
C	Erdogdu, MA		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Erdogdu, Murat A.			Newton-Stein Method: A Second Order Method for GLMs via Stein's Lemma	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the problem of efficiently computing the maximum likelihood estimator in Generalized Linear Models (GLMs) when the number of observations is much larger than the number of coefficients (n >> p >> 1). In this regime, optimization algorithms can immensely benefit from approximate second order information. We propose an alternative way of constructing the curvature information by formulating it as an estimation problem and applying a Stein-type lemma, which allows further improvements through sub-sampling and eigenvalue thresholding. Our algorithm enjoys fast convergence rates, resembling that of second order methods, with modest per-iteration cost. We provide its convergence analysis for the case where the rows of the design matrix are i.i.d. samples with bounded support. We show that the convergence has two phases, a quadratic phase followed by a linear phase. Finally, we empirically demonstrate that our algorithm achieves the highest performance compared to various algorithms on several datasets.	[Erdogdu, Murat A.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University	Erdogdu, MA (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	erdogdu@stanford.edu						Baik J, 2006, J MULTIVARIATE ANAL, V97, P1382, DOI 10.1016/j.jmva.2005.08.003; Bishop, 1995, NEURAL NETWORKS PATT; Blackard JA, 1999, COMPUT ELECTRON AGR, V24, P131, DOI 10.1016/S0168-1699(99)00046-0; Bottou Leon, 2010, LARGESCALE MACHINE L; Boyd S, 2004, CONVEX OPTIMIZATION; Byrd RH, 2014, ARXIV PREPRINT ARXIV; Byrd Richard H, 2011, SIAM J OPTIMIZATION; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Dicker Lee H, 2015, ARXIV150904388; DONOHO D, 2013, ARXIV13055870; Donoho David L, 2013, ARXIV13110851; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Erdogdu Murat A., 2015, NIPS; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Gibbs Alison L, 2002, ISR, V70; Graf Franz, 2011, 2D IMAGE REGISTRATIO; Koller D., 2009, PROBABILISTIC GRAPHI; Le Roux Nicolas, 2010, FAST NATURAL NEWTON; Le Roux Nicolas, 2008, TOPMOUMOUTE ONLINE N; Lichman M., 2013, UCI MACHINE LEARNING; Lin C.-J., 2008, JMLR; Martens J., 2010, DEEP LEARNING VIA HE, P735; McCullagh P., 1989, GEN LINEAR MODELS, V2; NESTEROV IE, 1983, DOKL AKAD NAUK SSSR+, V269, P543; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Shun-Ichi Amari, 1998, NEURAL COMPUTATION, V10; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Vinyals Oriol, 2012, KRYLOV SUBSPACE DESC	31	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101010
C	Feldman, V; Perkins, W; Vempala, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Feldman, Vitaly; Perkins, Will; Vempala, Santosh			Subsampled Power Iteration: a Unified Algorithm for Block Models and Planted CSP's	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ATTRIBUTE-EFFICIENT; COMPLEXITY	We present an algorithm for recovering planted solutions in two well-known models, the stochastic block model and planted constraint satisfaction problems (CSP), via a common generalization in terms of random bipartite graphs. Our algorithm matches up to a constant factor the best-known bounds for the number of edges (or constraints) needed for perfect recovery and its running time is linear in the number of edges used. The time complexity is significantly better than both spectral and SDP-based approaches. The main contribution of the algorithm is in the case of unequal sizes in the bi-partition that arises in our reduction from the planted CSP. Here our algorithm succeeds at a significantly lower density than the spectral approaches, surpassing a barrier based on the spectral norm of a random matrix. Other significant features of the algorithm and analysis include (i) the critical use of power iteration with subsampling, which might be of independent interest; its analysis requires keeping track of multiple norms of an evolving solution (ii) the algorithm can be implemented statistically, i.e., with very limited access to the input distribution (iii) the algorithm is extremely simple to implement and runs in linear time, and thus is practical even for very large instances.	[Feldman, Vitaly] IBM Res Almaden, San Jose, CA 95120 USA; [Perkins, Will] Univ Birmingham, Birmingham, W Midlands, England; [Vempala, Santosh] Georgia Tech, Atlanta, GA 30332 USA	International Business Machines (IBM); University of Birmingham; University System of Georgia; Georgia Institute of Technology	Feldman, V (corresponding author), IBM Res Almaden, San Jose, CA 95120 USA.	vitaly@post.harvard.edu; w.f.perkins@bham.ac.uk; vempala@cc.gatech.edu		Perkins, Will/0000-0002-7937-7016	NSF [CCF-1217793]	NSF(National Science Foundation (NSF))	S. Vempala supported in part by NSF award CCF-1217793.	Achlioptas D., 2001, P 33 ANN ACM S THEOR, P611; [Anonymous], 2014, ARXIV14043918; [Anonymous], 2014, ARXIV14053267; Berthet Q., 2013, C LEARN THEOR, P1046; BLUM A, 1992, MACH LEARN, V9, P373, DOI 10.1023/A:1022653502461; Bogdanov A, 2009, LECT NOTES COMPUT SC, V5687, P392, DOI 10.1007/978-3-642-03685-9_30; Boppana R. B., 1987, 28th Annual Symposium on Foundations of Computer Science (Cat. No.87CH2471-1), P280, DOI 10.1109/SFCS.1987.22; Coja-Oghlan A, 2003, LECT NOTES COMPUT SC, V2751, P15; Coja-Oghlan A, 2010, SIAM J DISCRETE MATH, V23, P2000, DOI 10.1137/080730160; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Daniely A., 2013, ADV NEURAL INFORM PR, P145; Daniely A., 2014, CORRABS14043378; Decatur SE, 2000, SIAM J COMPUT, V29, P854, DOI 10.1137/S0097539797325648; Feige U, 2005, RANDOM STRUCT ALGOR, V27, P251, DOI 10.1002/rsa.20089; Feige U, 2004, LECT NOTES COMPUT SC, V3142, P519; Feldman V., 2014, CORRABS14072774; Feldman V., 2014, COLT, P1283; Feldman V, 2007, J MACH LEARN RES, V8, P1431; Feldman V, 2015, ACM S THEORY COMPUT, P77, DOI 10.1145/2746539.2746577; Feldman V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P655; Florescu  Laura, 2015, ARXIV150606737; FREDMAN ML, 1984, J ACM, V31, P538, DOI 10.1145/828.1884; Friedman J, 2005, SIAM J COMPUT, V35, P408, DOI 10.1137/S009753970444096X; Goerdt A., 2001, STACS 2001. 18th Annual Symposium on Theoretical Aspects of Computer Science. Proceedings (Lecture Notes in Computer Science Vol.2010), P294; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Korada S. B., 2011, P ACM SIGMETRICS JOI, P209; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Massone L, 2014, SPRINGER SER ADV MAN, P1, DOI 10.1007/978-94-007-7593-0_1; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Mossel E., 2013, PROOF BLOCK MODEL TH; O'Donnell R., 2014, C COMP COMPL; Servedio RA, 2000, J COMPUT SYST SCI, V60, P161, DOI 10.1006/jcss.1999.1666; Shalev-Shwartz S., 2012, P 15 INT C ART INT S, P1019	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101094
C	Frongillo, R; Reid, MD		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Frongillo, Rafael; Reid, Mark D.			Convergence Analysis of Prediction Markets via Randomized Subspace Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				OPTIMIZATION; OLD	Prediction markets are economic mechanisms for aggregating information about future events through sequential interactions with traders. The pricing mechanisms in these markets are known to be related to optimization algorithms in machine learning and through these connections we have some understanding of how equilibrium market prices relate to the beliefs of the traders in a market. However, little is known about rates and guarantees for the convergence of these sequential mechanisms, and two recent papers cite this as an important open question. In this paper we show how some previously studied prediction market trading models can be understood as a natural generalization of randomized coordinate descent which we call randomized subspace descent (RSD). We establish convergence rates for RSD and leverage them to prove rates for the two prediction market models above, answering the open questions. Our results extend beyond standard centralized markets to arbitrary trade networks.	[Frongillo, Rafael] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA; [Reid, Mark D.] Australian Natl Univ, Res Sch Comp Sci, Canberra, ACT, Australia; [Reid, Mark D.] NICTA, Sydney, NSW, Australia	University of Colorado System; University of Colorado Boulder; Australian National University; Australian National University	Frongillo, R (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.	raf@colorado.edu; mark.reid@anu.edu.au			ARC Discovery Early Career Research Award [DE130101605]	ARC Discovery Early Career Research Award(Australian Research Council)	We would like to thank Matus Telgarsky for his generous help, as well as the lively discussions with, and helpful comments of, Sebastien Lahaie, Miro Dudik, Jenn Wortman Vaughan, Yiling Chen, David Parkes, and Nageeb Ali. MDR is supported by an ARC Discovery Early Career Research Award (DE130101605). Part of this work was developed while he was visiting Microsoft Research.	Abernethy J., 2014, P 15 ACM C EC COMP, P395; Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12; Abernethy J.D., 2011, ADV NEURAL INFORM PR, P2600; Boyd S, 2004, CONVEX OPTIMIZATION; Burgert C, 2006, STATIST RISK MODEL, V24, P153, DOI 10.1524/stnd.2006.24.1.153; Chen Y., 2010, P 11 ACM C EL COMM, P189; de Abreu NMM, 2007, LINEAR ALGEBRA APPL, V423, P53, DOI 10.1016/j.laa.2006.08.017; FOLLMER H, 2004, GRUYTER STUDIES MATH, V27; Frongillo Rafael M, 2012, P NEUR INF PROC SYST; Grunwald PD, 2004, ANN STAT, V32, P1367, DOI 10.1214/009053604000000553; HIRIARTURRUTY JB, 1993, CONVEX ANAL MINIMIZA, V2, P306; Hu Jinli, 2014, P 31 INT C MACH LEAR; Millin Jono, 2012, P 29 INT C MACH LEAR, P1815; Mohar B., 1991, GRAPH THEORY COMBINA; Necoara I, 2014, TECHNICAL REPORT; Necoara I, 2013, IEEE T AUTOMAT CONTR, V58, P2001, DOI 10.1109/TAC.2013.2250071; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Premachandra Mindika, 2013, P 5 AS C MACH LEARN, P373; Reddi S., 2014, ARXIV14092617; Reid Mark D, 2015, P C LEARN THEOR COLT; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Rockafellar R. T., 1997, CONVEX ANAL; Storkey A. J., 2011, P 14 INT C ART INT S, P716	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100095
C	Frongillo, R; Kash, IA		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Frongillo, Rafael; Kash, Ian A.			On Elicitation Complexity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Elicitation is the study of statistics or properties which are computable via empirical risk minimization. While several recent papers have approached the general question of which properties are elicitable, we suggest that this is the wrong question-all properties are elicitable by first eliciting the entire distribution or data set, and thus the important question is how elicitable. Specifically, what is the minimum number of regression parameters needed to compute the property? Building on previous work, we introduce a new notion of elicitation complexity and lay the foundations for a calculus of elicitation. We establish several general results and techniques for proving upper and lower bounds on elicitation complexity. These results provide tight bounds for eliciting the Bayes risk of any loss, a large class of properties which includes spectral risk measures and several new properties of interest.	[Frongillo, Rafael] Univ Colorado, Boulder, CO 80309 USA; [Kash, Ian A.] Microsoft Res, New York, NY USA	University of Colorado System; University of Colorado Boulder; Microsoft	Frongillo, R (corresponding author), Univ Colorado, Boulder, CO 80309 USA.	raf@colorado.edu; iankash@microsoft.com						Abernethy J. D., 2012, C LEARN THEOR INT MA, P1; Agarwal A., 2015, COLT; Banerjee A, 2005, IEEE T INFORM THEORY, V51, P2664, DOI 10.1109/TIT.2005.850145; Bellini Fabio, 2013, QUANTITATIV IN PRESS, DOI [10.1080/14697688.2014.946955, DOI 10.1080/14697688.2014.946955]; Emmer Susanne, 2013, ARXIV13121645QFIN; Fissler Tobias, 2015, ARXIV150308123MATHQF; Fissler Tobias, 2015, ARXIV150700244QFIN; Fllmer Hans, 2015, ANN REV FINANCIAL EC, V7; FRONGILLO R., 2015, JMLR WORKSHOP C P, V40, P1; Frongillo R, 2014, LECT NOTES COMPUT SC, V8877, P354, DOI 10.1007/978-3-319-13129-0_29; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Gneiting T, 2011, J AM STAT ASSOC, V106, P746, DOI 10.1198/jasa.2011.r10138; Heinrich C., 2013, BIOMETRIKA; Lambert N.S., 2011, PREPRINT; Lambert N, 2009, 10TH ACM CONFERENCE ON ELECTRONIC COMMERCE - EC 2009, P109; Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129; NEWEY WK, 1987, ECONOMETRICA, V55, P819, DOI 10.2307/1911031; Rockafellar RT, 2014, EUR J OPER RES, V234, P140, DOI 10.1016/j.ejor.2013.10.046; Rockafellar R.T., 2013, SURV OPER RES MANAG, V18, P33, DOI DOI 10.1016/J.SORMS.2013.03.001; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; STEINWART I, 2014, J MACHINE LEARNING R, V35, P482; Wang RD, 2015, STAT PROBABIL LETT, V100, P172, DOI 10.1016/j.spl.2015.02.004; [No title captured]	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101091
C	Gan, Z; Li, CY; Henao, R; Carlson, D; Carin, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gan, Zhe; Li, Chunyuan; Henao, Ricardo; Carlson, David; Carin, Lawrence			Deep Temporal Sigmoid Belief Networks for Sequence Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep dynamic generative models are developed to learn sequential dependencies in time-series data. The multi-layered model is designed by constructing a hierarchy of temporal sigmoid belief networks (TSBNs), defined as a sequential stack of sigmoid belief networks (SBNs). Each SBN has a contextual hidden state, inherited from the previous SBNs in the sequence, and is used to regulate its hidden bias. Scalable learning and inference algorithms are derived by introducing a recognition model that yields fast sampling from the variational posterior. This recognition model is trained jointly with the generative model, by maximizing its variational lower bound on the log-likelihood. Experimental results on bouncing balls, polyphonic music, motion capture, and text streams show that the proposed approach achieves state-of-the-art predictive performance, and has the capacity to synthesize various sequences.	[Gan, Zhe; Li, Chunyuan; Henao, Ricardo; Carlson, David; Carin, Lawrence] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	Duke University	Gan, Z (corresponding author), Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA.	zhe.gan@duke.edu; chunyuan.li@duke.edu; r.henao@duke.edu; david.carlson@duke.edu; lcarin@duke.edu	Li, Chunyuan/AAG-1303-2020		ARO; DARPA; ONR; NGA; DOE	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ONR(Office of Naval Research); NGA; DOE(United States Department of Energy (DOE))	This research was supported in part by ARO, DARPA, DOE, NGA and ONR.	Acharya Ayan, 2015, AISTATS; [Anonymous], P IEEE; Ba J., 2017, P 3 INT C LEARN REPR; Bayer J, 2014, ARXIV14117610; Boulanger-Lewandowski N., 2012, ICML; Chung J., 2015, NIPS; Fan K., 2015, NIPS; Gan Z., 2015, AISTATS; Gan Z., 2015, ICML; Graves Alex, 2013, ARXIV13080850 CORR; Han S., 2014, NIPS; Henderson J., 2010, JMLR; Hermans Michiel, 2013, NIPS; Hinton G, 2002, NEURAL COMPUTATION; Hinton G., 1995, P ICANN; Hinton G., 2007, AISTATS; Hinton G. E., 1995, SCIENCE; Hinton G.E., 2006, NEURAL COMPUTATION; Hinton G. E., 2009, NIPS; Kalman R., 1963, J SOC IND APPL MAT A; Martens J., 2011, ICML; Mittelman R., 2014, ICML; Mnih A., 2014, ICML; Neal Radford M., 1992, ARTIFICIAL INTELLIGE; Pascanu Razvan, 2013, P INT C MACH LEAR; Rabiner L., 1986, ASSP MAGAZINE; Rezende D.J., 2014, PROC INT CONFER ENCE; Sutskever I., 2009, NIPS; Taylor G. W., 2006, NIPS; Taylor G. W., 2009, ICML; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; van Amersfoort J. R., 2014, ARXIV14126581	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100023
C	Gardner, JR; Malkomes, G; Garnett, R; Weinberger, KQ; Barbour, D; Cunningham, JP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gardner, Jacob R.; Malkomes, Gustavo; Garnett, Roman; Weinberger, Kilian Q.; Barbour, Dennis; Cunningham, John P.			Bayesian Active Model Selection with an Application to Automated Audiometry	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce a novel information-theoretic approach for active model selection and demonstrate its effectiveness in a real-world application. Although our method can work with arbitrary models, we focus on actively learning the appropriate structure for Gaussian process (GP) models with arbitrary observation likelihoods. We then apply this framework to rapid screening for noise-induced hearing loss (NIHL), a widespread and preventible disability, if diagnosed early. We construct a GP model for pure-tone audiometric responses of patients with NIHL. Using this and a previously published model for healthy responses, the proposed method is shown to be capable of diagnosing the presence or absence of NIHL with drastically fewer samples than existing approaches. Further, the method is extremely fast and enables the diagnosis to be performed in real time.	[Gardner, Jacob R.; Weinberger, Kilian Q.] Cornell Univ, CS, Ithaca, NY 14850 USA; [Malkomes, Gustavo; Garnett, Roman] WUSTL, CSE, St Louis, MO 63130 USA; [Barbour, Dennis] WUSTL, BME, St Louis, MO 63130 USA; [Cunningham, John P.] Columbia Univ, Stat, New York, NY 10027 USA	Cornell University; Washington University (WUSTL); Washington University (WUSTL); Columbia University	Gardner, JR (corresponding author), Cornell Univ, CS, Ithaca, NY 14850 USA.	jrg365@cornell.edu; luizgustavo@wustl.edu; garnett@wustl.edu; kqw4@cornell.edu; dbarbour@wustl.edu; jpc2181@columbia.edu			National Science Foundation (NSF) [IIA-1355406]; NSF [IIS-1525919, IIS-1550179, EFMA-1137211]; CAPES/BR; NIH [R01-DC009215]; CIMIT	National Science Foundation (NSF)(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); CAPES/BR(Coordenacao de Aperfeicoamento de Pessoal de Nivel Superior (CAPES)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); CIMIT	This material is based upon work supported by the National Science Foundation (NSF) under award number IIA-1355406. Additionally, JRG and KQW are supported by NSF grants IIS-1525919, IIS-1550179, and EFMA-1137211; GM is supported by CAPES/BR; DB acknowledges NIH grant R01-DC009215 as well as the CIMIT; JPC acknowledges the Sloan Foundation.	Ali A., 2014, AAAI; Bailey TC, 2013, J HOSP MED, V8, P236, DOI 10.1002/jhm.2009; CARHART R, 1959, J SPEECH HEAR DISORD, V24, P330, DOI 10.1044/jshd.2404.330; DON M, 1979, ANN OTO RHINOL LARYN, V88, P1; Duvenaud D., 2014, AUTOMATIC MODEL CONS; Duvenaud D., 2013, P INT C MACH LEARN I, P1166; Gardner J. R., 2015, UAI; Garnett R, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P230; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Houlsby N., 2012, NIPS, V3, P2096; Houlsby N, 2014, PR MACH LEARN RES, V32, P766; HUGHSON W, 1944, T AM ACAD OPHTHALMOL, V48, P1; Kononenko I, 2001, ARTIF INTELL MED, V23, P89, DOI 10.1016/S0933-3657(01)00077-X; Kuha J, 2004, SOCIOL METHOD RES, V33, P188, DOI 10.1177/0049124103262065; Kulick J., 2014, ABS14097552 CORR; Kuss M, 2005, J MACH LEARN RES, V6, P1679; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; McBride DI, 2001, OCCUP ENVIRON MED, V58, P46, DOI 10.1136/oem.58.1.46; Minka T.P., 2001, P 17 C UNC ART INT, P362; Nelson DI, 2005, AM J IND MED, V48, P446, DOI 10.1002/ajim.20223; Raftery AE, 1996, BIOMETRIKA, V83, P251, DOI 10.1093/biomet/83.2.251; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Saria S, 2010, SCI TRANSL MED, V2, DOI 10.1126/scitranslmed.3001304; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136; Shargorodsky J, 2010, JAMA-J AM MED ASSOC, V304, P772, DOI 10.1001/jama.2010.1124; Williams C., 1996, NIPS; Wilson A., 2014, P ADV NEUR INF PROC, V4, P3626	28	0	0	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102020
C	Giordano, R; Broderick, T; Jordan, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Giordano, Ryan; Broderick, Tamara; Jordan, Michael			Linear Response Methods for Accurate Covariance Estimates from Mean Field Variational Bayes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INFERENCE; MODELS	Mean field variational Bayes (MFVB) is a popular posterior approximation method due to its fast runtime on large-scale data sets. However, a well known major failing of MFVB is that it underestimates the uncertainty of model variables (sometimes severely) and provides no information about model variable covariance. We generalize linear response methods from statistical physics to deliver accurate uncertainty estimates for model variables-both for individual variables and coherently across variables. We call our method linear response variational Bayes (LRVB). When the MFVB posterior approximation is in the exponential family, LRVB has a simple, analytic form, even for non-conjugate models. Indeed, we make no assumptions about the form of the true posterior. We demonstrate the accuracy and scalability of our method on a range of models for both simulated and real data.	[Giordano, Ryan; Jordan, Michael] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Broderick, Tamara] MIT, Cambridge, MA 02139 USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT)	Giordano, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	rgiordano@berkeley.edu; tbroderick@csail.mit.edu; jordan@cs.berkeley.edu	Jordan, Michael I/C-5253-2013		Berkeley Fellowships	Berkeley Fellowships	The authors thank Alex Blocker for helpful comments. R. Giordano and T. Broderick were funded by Berkeley Fellowships.	Bates D, 2013, J STAT SOFTW, V52, P1, DOI 10.18637/jss.v052.i05; Bishop C. M., 2006, PATTERN RECOGN; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Hadfield JD, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i02; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hojen-Sorensen PADFR, 2002, NEURAL COMPUT, V14, P889, DOI 10.1162/089976602317319009; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lubin M, 2015, INFORMS J COMPUT, V27, P238, DOI 10.1287/ijoc.2014.0623; MacKay D. J. C., 2003, INFORM THEORY INFERE, P269; MENG XL, 1991, J AM STAT ASSOC, V86, P899, DOI 10.2307/2290503; Opper M., 2003, ADV NEURAL INFORM PR; Opper M, 2001, ADV MEAN FIELD METHO; Parisi G., 1988, STAT FIELD THEORY, V4; Plummer M., 2006, R NEWS, V6, P7, DOI DOI 10.1159/000323281; Rue H, 2009, J R STAT SOC B, V71, P319, DOI 10.1111/j.1467-9868.2008.00700.x; Tanaka T, 1998, PHYS REV E, V58, P2302, DOI 10.1103/PhysRevE.58.2302; Tanaka T, 2000, NEURAL COMPUT, V12, P1951, DOI 10.1162/089976600300015213; Turner R., 2011, BAYESIAN TIME SERIES; Wachter A, 2006, MATH PROGRAM, V106, P25, DOI 10.1007/s10107-004-0559-y; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang B., 2004, WORKSH ART INT STAT, P373; Welling M, 2004, NEURAL COMPUT, V16, P197, DOI 10.1162/08997660460734056	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101015
C	Goldstein, T; Li, M; Yuan, XM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Goldstein, Thomas; Li, Min; Yuan, Xiaoming			Adaptive Primal-Dual Splitting Methods for Statistical Learning and Image Processing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHMS; CONVERGENCE; PENALTY	The alternating direction method of multipliers (ADMM) is an important tool for solving complex optimization problems, but it involves minimization sub-steps that are often difficult to solve efficiently. The Primal-Dual Hybrid Gradient (PDHG) method is a powerful alternative that often has simpler sub-steps than ADMM, thus producing lower complexity solvers. Despite the flexibility of this method, PDHG is often impractical because it requires the careful choice of multiple stepsize parameters. There is often no intuitive way to choose these parameters to maximize efficiency, or even achieve convergence. We propose self-adaptive stepsize rules that automatically tune PDHG parameters for optimal convergence. We rigorously analyze our methods, and identify convergence rates. Numerical experiments show that adaptive PDHG has strong advantages over non-adaptive methods in terms of both efficiency and simplicity for the user.	[Goldstein, Thomas] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Li, Min] Southeast Univ, Sch Econ & Management, Nanjing, Jiangsu, Peoples R China; [Yuan, Xiaoming] Hong Kong Baptist Univ, Dept Math, Kowloon Tong, Hong Kong, Peoples R China	University System of Maryland; University of Maryland College Park; Southeast University - China; Hong Kong Baptist University	Goldstein, T (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	tomg@cs.umd.edu; limin@seu.edu.cn; xmyuan@hkbu.edu.hk			National Science Foundation [1535902]; Office of Naval Research [N00014-15-1-2676]; Hong Kong Research Grants Council's General Research Fund [HKBU 12300515]; Program for New Century Excellent University Talents [NCET-12-0111]; Qing Lan Project	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); Hong Kong Research Grants Council's General Research Fund; Program for New Century Excellent University Talents(Program for New Century Excellent Talents in University (NCET)); Qing Lan Project(Jiangsu Polytech Institute)	This work was supported by the National Science Foundation (#1535902), the Office of Naval Research (#N00014-15-1-2676), and the Hong Kong Research Grants Council's General Research Fund (HKBU 12300515). The second author was supported in part by the Program for New Century Excellent University Talents under Grant No. NCET-12-0111, and the Qing Lan Project.	BARANIUK R., 2013, PREPRINT; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; Bonettini S, 2012, J MATH IMAGING VIS, V44, P236, DOI 10.1007/s10851-011-0324-9; Boyd S., 2010, FDN TRENDS MACHINE L; Chambolle A., 2010, J MATH IMAGING VIS, P1; Condat L, 2013, J OPTIMIZ THEORY APP, V158, P460, DOI 10.1007/s10957-012-0245-9; Esser E, 2010, SIAM J IMAGING SCI, V3, P1015, DOI 10.1137/09076934X; GLOWINSKI R, 1975, REV FR AUTOMAT INFOR, V9, P41; Glowinski R., 1989, AUGMENTED LAGRANGIAN; Goldstein T, 2009, SIAM J IMAGING SCI, V2, P323, DOI 10.1137/080725891; He BS, 2012, SIAM J IMAGING SCI, V5, P119, DOI 10.1137/100814494; He BS, 2000, J OPTIMIZ THEORY APP, V106, P337, DOI 10.1023/A:1004603514434; Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391; Ouyang Yuyuan, 2014, ARXIV14016607; Pock T, 2009, IEEE I CONF COMP VIS, P1133, DOI 10.1109/ICCV.2009.5459348; POPOV LD, 1980, MATH NOTES+, V28, P845, DOI 10.1007/BF01141092; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Zhang XQ, 2005, IEEE NUCL SCI CONF R, P2332; Zhu M., 2008, 0834 UCLA CAM	21	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100091
C	Gong, PH; Ye, JP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gong, Pinghua; Ye, Jieping			HONOR: Hybrid Optimization for NOn-convex Regularized problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NONCONCAVE PENALIZED LIKELIHOOD; VARIABLE SELECTION; SPARSE; ALGORITHM	Recent years have witnessed the superiority of non-convex sparse learning formulations over their convex counterparts in both theory and practice. However, due to the non-convexity and non-smoothness of the regularizer, how to efficiently solve the non-convex optimization problem for large-scale data is still quite challenging. In this paper, we propose an efficient Hybrid Optimization algorithm for NOn-convex Regularized problems (HONOR). Specifically, we develop a hybrid scheme which effectively integrates a Quasi-Newton (QN) step and a Gradient Descent (GD) step. Our contributions are as follows: (1) HONOR incorporates the second-order information to greatly speed up the convergence, while it avoids solving a regularized quadratic programming and only involves matrixvector multiplications without explicitly forming the inverse Hessian matrix. (2) We establish a rigorous convergence analysis for HONOR, which shows that convergence is guaranteed even for non-convex problems, while it is typically challenging to analyze the convergence for non-convex problems. (3) We conduct empirical studies on large-scale data sets and results demonstrate that HONOR converges significantly faster than state-of-the-art algorithms.	[Gong, Pinghua; Ye, Jieping] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Gong, PH (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	gongp@umich.edu; jpye@umich.edu			NIH [R01 LM010730, U54 EB020403]; NSF [IIS-0953662, III-1539991, III-1539722]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF(National Science Foundation (NSF))	This work is supported in part by research grants from NIH (R01 LM010730, U54 EB020403) and NSF (IIS-0953662, III-1539991, III-1539722).	An LTH, 2005, ANN OPER RES, V133, P23, DOI 10.1007/s10479-004-5022-1; Andrew G., 2007, P 24 INT C MACH LEAR, V24, P33, DOI DOI 10.1145/1273496.1273501; Bioucas-Dias JM, 2007, IEEE T IMAGE PROCESS, V16, P2992, DOI 10.1109/TIP.2007.909319; Byrd R. H., 2012, TECHNICAL REPORT; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Byrd RH, 2012, MATH PROGRAM, V134, P127, DOI 10.1007/s10107-012-0572-5; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Clarke F.H, 1990, CANADIAN MATH SOC SE, V2; DUTTA J, 2005, TOP, V13, P185; El Ghaoui L., 2011, P CIDU MOUNT VIEW CA, P159; Fan JQ, 2014, ANN STAT, V42, P819, DOI 10.1214/13-AOS1198; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Gasso G, 2009, IEEE T SIGNAL PROCES, V57, P4686, DOI 10.1109/TSP.2009.2026004; Gong P., 2015, ICML; Gong Pinghua, 2013, JMLR Workshop Conf Proc, V28, P37; Jorge N., 1999, NUMERICAL OPTIMIZATI; Mazumder R., 2011, J AM STAT ASS, V106; Oztoprak F., 2012, P 25 INT C NEUR INF, V25, P755; Rakotomamonjy A., 2014, DC PROXIMAL NEWTON N; Shevade SK, 2003, BIOINFORMATICS, V19, P2246, DOI 10.1093/bioinformatics/btg308; Tan X, 2011, IEEE T SIGNAL PROCES, V59, P1088, DOI 10.1109/TSP.2010.2096218; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang T., 2012, MULTISTAGE CONVEX RE; Zhang T, 2010, J MACH LEARN RES, V11, P1081; Zou H, 2008, ANN STAT, V36, P1509, DOI 10.1214/009053607000000802	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101088
C	He, B; Yue, YS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		He, Bryan; Yue, Yisong			Smooth Interactive Submodular Set Cover	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Interactive submodular set cover is an interactive variant of submodular set cover over a hypothesis class of submodular functions, where the goal is to satisfy all sufficiently plausible submodular functions to a target threshold using as few (cost-weighted) actions as possible. It models settings where there is uncertainty regarding which submodular function to optimize. In this paper, we propose a new extension, which we call smooth interactive submodular set cover, that allows the target threshold to vary depending on the plausibility of each hypothesis. We present the first algorithm for this more general setting with theoretical guarantees on optimality. We further show how to extend our approach to deal with realvalued functions, which yields new theoretical results for real-valued submodular set cover for both the interactive and non-interactive settings.	[He, Bryan] Stanford Univ, Stanford, CA 94305 USA; [Yue, Yisong] CALTECH, Pasadena, CA 91125 USA	Stanford University; California Institute of Technology	He, B (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	bryanhe@stanford.edu; yyue@caltech.edu						Batra D., 2012, EUR C COMP VIS ECCV; Chen Y., 2013, INT C MACH LEARN ICM; Dey Debadeepta, 2012, ROB SCI SYST C RSS; El-Arini Khalid, 2009, ACM C KNOWL DISC DAT; El-Arini Khalid, 2011, ACM C KNOWL DISC DAT; Gabillon V., 2013, ADV NEURAL INFORM PR; Golovin Daniel, 2010, C LEARN THEOR COLT; GUILLORY A, 2011, INT C MACH LEARN ICM; Guillory Andrew, 2012, THESIS; Guillory Andrew, 2010, INT C MACH LEARN ICM; Hanneke Steve, 2007, THESIS; Javdani Shervin, 2013, IEEE INT C ROB AUT I; Javdani Shervin, 2014, ARTIFICIAL INTELLIGE; Krause Andreas, 2005, INT C MACH LEARN ICM; Leskovec Jure, 2007, ACM C KNOWL DISC DAT; Lin H., 2012, UNCERTAINTY ARTIFICI; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Prasad A., 2014, NEURAL INFORM PROCES; Radlinski Filip, 2008, INT C MACH LEARN ICM; Raman Karthik, 2012, ACM C KNOWL DISC DAT; Rodriguez Manuel Gomez, 2010, ACM C KNOWL DISC DAT; Ross Stephane, 2013, INT C MACH LEARN ICM; Tschiatschek S., 2014, NEURAL INFORM PROCES; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435; Yue Y., 2011, NEURAL INFORM PROCES; Yue Yisong, 2008, INT C MACH LEARN ICM	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103040
C	Hefny, A; Downey, C; Gordon, GJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hefny, Ahmed; Downey, Carlton; Gordon, Geoffrey J.			Supervised Learning for Dynamical System Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recently there has been substantial interest in spectral methods for learning dynamical systems. These methods are popular since they often offer a good tradeoff between computational and statistical efficiency. Unfortunately, they can be difficult to use and extend in practice: e.g., they can make it difficult to incorporate prior information such as sparsity or structure. To address this problem, we present a new view of dynamical system learning: we show how to learn dynamical systems by solving a sequence of ordinary supervised learning problems, thereby allowing users to incorporate prior knowledge via standard techniques such as L-1 regularization. Many existing spectral methods are special cases of this new framework, using linear regression as the supervised learner. We demonstrate the effectiveness of our framework by showing examples where nonlinear regression or lasso let us learn better state representations than plain linear regression does; the correctness of these instances follows directly from our general analysis.	[Hefny, Ahmed; Downey, Carlton; Gordon, Geoffrey J.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Hefny, A (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ahefny@cs.cmu.edu; cmdowney@cs.cmu.edu; ggordon@cs.cmu.edu			Department of Defense [FA8721-05-C-0003]; Carnegie Mellon University for the operation of the Software Engineering Institute; PNC Center for Financial Services Innovation; NIH [R01 MH 064537]; ONR [N000141512365]	Department of Defense(United States Department of Defense); Carnegie Mellon University for the operation of the Software Engineering Institute; PNC Center for Financial Services Innovation; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research)	This material is based upon work funded and supported by the Department of Defense under Contract No. FA8721-05-C-0003 with Carnegie Mellon University for the operation of the Software Engineering Institute, a federally funded research and development center.; Supported by a grant from the PNC Center for Financial Services Innovation; Supported by NIH grant R01 MH 064537 and ONR contract N000141512365.	Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; [Anonymous], ADDISON WESLEY SERIE; [Anonymous], 2012, P 29 INT C MACH LEAR; Balle B, 2014, PR MACH LEARN RES, V32, P1386; BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196; Boots B., 2012, THESIS; Boots B., 2013, P 29 INT C UNC ART I; Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092; Boots Byron, 2011, P 25 NAT C ART INT A; CORBETT AT, 1994, USER MODEL USER-ADAP, V4, P253, DOI 10.1007/BF01099821; Fukumizu K, 2013, J MACH LEARN RES, V14, P3753; Gilks WR, 1996, MARKOV CHAIN MONTE C; Hsu D. J., 2009, COLT; Koedinger K.R., 2010, HDB ED DATA MINING, V43, P43, DOI [10.1201/b10274-6, DOI 10.1201/B10274-6]; Langford J., 2009, P 26 ANN INT C MACH, P593; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Rosencrantz M., 2004, P 21 INT C MACH LEAR; Siddiqi S. M., 2010, P 13 INT C ART INT S; Song L., 2010, P 27 INT C MACH LEAR, P1; Song LD, 2009, PROCEEDINGS OF THE FIBER SOCIETY 2009 SPRING CONFERENCE, VOLS I AND II, P961; Van Overschee P., 1996, SUBSPACE IDENTIFICAT	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100041
C	Herbster, M; Pasteris, S; Ghosh, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Herbster, Mark; Pasteris, Stephen; Ghosh, Shaona			Online Prediction at the Limit of Zero Temperature	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				COMPLEXITY; CUTS	We design an online algorithm to classify the vertices of a graph. Underpinning the algorithm is the probability distribution of an Ising model isomorphic to the graph. Each classification is based on predicting the label with maximum marginal probability in the limit of zero-temperature with respect to the labels and vertices seen so far. Computing these classifications is unfortunately based on a #P complete problem. This motivates us to develop an algorithm for which we give a sequential guarantee in the online mistake bound framework. Our algorithm is optimal when the graph is a tree matching the prior results in [1]. For a general graph, the algorithm exploits the additional connectivity over a tree to provide a per-cluster bound. The algorithm is efficient, as the cumulative time to sequentially predict all of the vertices of the graph is quadratic in the size of the graph.	[Herbster, Mark; Pasteris, Stephen] UCL, Dept Comp Sci, London WC1E 6BT, England; [Ghosh, Shaona] Univ Southampton, ECS, Southampton SO17 1BJ, Hants, England	University of London; University College London; University of Southampton	Herbster, M (corresponding author), UCL, Dept Comp Sci, London WC1E 6BT, England.	m.herbster@cs.ucl.ac.uk; s.pasteris@cs.ucl.ac.uk; ghosh.shaona@gmail.com		GHOSH, SHAONA/0000-0003-4658-5174				BALL MO, 1983, NETWORKS, V13, P253, DOI 10.1002/net.3230130210; Barzdin J. M., 1972, SOV MATH DOKL, V13, P1224; Belkin M, 2004, MACH LEARN, V56, P209, DOI 10.1023/B:MACH.0000033120.25363.1e; Blum A., 2001, P INT C MACH LEARN I, P19, DOI DOI 10.1184/R1/6606860.V1; Cesa-Bianchi N., 2010, P 27 INT C MACH LEAR, P175; Cesa-Bianchi N., 2009, P 22 ANN C LEARN; Chapelle O., 2003, ADV NEURAL INFORM PR; Gartner Thomas, 2007, P 18 EUR C MACH LEAR; Goldberg LA, 2007, COMB PROBAB COMPUT, V16, P43, DOI 10.1017/S096354830600767X; Herbster M., 2009, ADV NEURAL INFORM PR, P649; Herbster M, 2005, ACM INT C PROCEEDING, V119, P305; HERBSTER M, 2009, P 22 ANN C LEARN THE; Herbster M, 2008, LECT NOTES ARTIF INT, V5254, P54, DOI 10.1007/978-3-540-87987-9_9; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; PICARD JC, 1980, MATH PROGRAM STUD, V13, P8, DOI 10.1007/BFb0120902; PROVAN JS, 1983, SIAM J COMPUT, V12, P777, DOI 10.1137/0212053; SZUMMER M, 2001, NIPS, P945; Vitale Fabio, 2011, NIPS, P1584; Zhou D., 2003, NIPS; Zhu Xiaojin., 2003, P ICLR, P912	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100058
C	Hsu, D; Kontorovich, A; Szepesvari, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hsu, Daniel; Kontorovich, Aryeh; Szepesvari, Csaba			Mixing Time Estimation in Reversible Markov Chains from a Single Sample Path	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DISTRIBUTIONS; CONVERGENCE; EXTENSION; THEOREM; FINITE; RATES	This article provides the first procedure for computing a fully data-dependent interval that traps the mixing time t(mix) of a finite reversible ergodic Markov chain at a prescribed confidence level. The interval is computed from a single finite-length sample path from the Markov chain, and does not require the knowledge of any parameters of the chain. This stands in contrast to previous approaches, which either only provide point estimates, or require a reset mechanism, or additional prior knowledge. The interval is constructed around the relaxation time t(relax), which is strongly related to the mixing time, and the width of the interval converges to zero roughly at a root n rate, where n is the length of the sample path. Upper and lower bounds are given on the number of samples required to achieve constant-factor multiplicative accuracy. The lower bounds indicate that, unless further restrictions are placed on the chain, no procedure can achieve this accuracy level before seeing each state at least Omega(t(relax)) times on the average. Finally, future directions of research are identified.	[Hsu, Daniel] Columbia Univ, New York, NY 10027 USA; [Kontorovich, Aryeh] Ben Gurion Univ Negev, Beer Sheva, Israel; [Szepesvari, Csaba] Univ Alberta, Edmonton, AB, Canada	Columbia University; Ben Gurion University; University of Alberta	Hsu, D (corresponding author), Columbia Univ, New York, NY 10027 USA.	djhsu@cs.columbia.edu; karyeh@cs.bgu.ac.il; szepesva@cs.ualberta.ca	Kontorovich, Aryeh/AAB-4744-2020; Kontorovich, Aryeh/X-9225-2019	Kontorovich, Aryeh/0000-0001-8038-8671; 				Atchade Y., 2015, BERNOULLI; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Batu T, 2000, ANN IEEE SYMP FOUND, P259, DOI 10.1109/SFCS.2000.892113; Batu T, 2013, J ACM, V60, DOI 10.1145/2432622.2432626; Bernstein S, 1927, MATH ANN, V97, P1, DOI 10.1007/BF01447859; Bhatnagar Nayantara, 2011, Approximation, Randomization, and Combinatorial Optimization Algorithms and Techniques. Proceedings 14th International Workshop, APPROX 2011 and 15th International Workshop, RANDOM 2011, P424, DOI 10.1007/978-3-642-22935-0_36; Cho GE, 2001, LINEAR ALGEBRA APPL, V335, P137, DOI 10.1016/S0024-3795(01)00320-2; Flegal JM, 2011, CH CRC HANDB MOD STA, P175; Gamarnik D, 2003, IEEE T INFORM THEORY, V49, P338, DOI 10.1109/TIT.2002.806131; Garren ST, 2000, BERNOULLI, V6, P215, DOI 10.2307/3318575; Gillman D, 1998, SIAM J COMPUT, V27, P1203, DOI 10.1137/S0097539794268765; Gyori B. M., 2014, ARXIV12122016; Hsu D., 2015, CORR; Jones GL, 2001, STAT SCI, V16, P312, DOI 10.1214/ss/1015346317; Karandikar RL, 2002, STAT PROBABIL LETT, V58, P297, DOI 10.1016/S0167-7152(02)00124-4; KIPNIS C, 1986, COMMUN MATH PHYS, V104, P1, DOI 10.1007/BF01210789; KONTOYIANNIS I., 2006, VALUETOOLS, P45; Leon CA, 2004, ANN APPL PROBAB, V14, P958, DOI 10.1214/105051604000000170; Levin D., 2008, MARKOV CHAINS MIXING; Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4; Liu J. S., 2001, SPRINGER SERIES STAT; Maurer A., 2009, COLT; McDonald Daniel J, 2011, JMLR Workshop Conf Proc, V15, P516; MEYER CD, 1975, SIAM REV, V17, P443, DOI 10.1137/1017044; Meyn S.P., 1993, MARKOV CHAINS STOCHA; Mnih V., 2008, P 25 INT C MACH LEAR; Mohri M., 2008, NIPS; Mohri M., 2009, NIPS; Steinwart I., 2009, NIPS; Steinwart I, 2009, J MULTIVARIATE ANAL, V100, P175, DOI 10.1016/j.jmva.2008.04.001; Stewart G., 1990, MATRIX PERTURBATION; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Swaminathan A., 2015, ICML; Tropp J. A., 2015, FDN TRENDS MACHINE L	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103058
C	Huang, QQ; Kakade, SM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Huang, Qingqing; Kakade, Sham M.			Super-Resolution Off the Grid	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DECOMPOSITION	Super-resolution is the problem of recovering a superposition of point sources using bandlimited measurements, which may be corrupted with noise. This signal processing problem arises in numerous imaging problems, ranging from astronomy to biology to spectroscopy, where it is common to take (coarse) Fourier measurements of an object. Of particular interest is in obtaining estimation procedures which are robust to noise, with the following desirable statistical and computational properties: we seek to use coarse Fourier measurements (bounded by some cutoff frequency); we hope to take a (quantifiably) small number of measurements; we desire our algorithm to run quickly. Suppose we have k point sources in d dimensions, where the points are separated by at least Delta from each other (in Euclidean distance). This work provides an algorithm with the following favorable guarantees: The algorithm uses Fourier measurements, whose frequencies are bounded by O(1/Delta) (up to log factors). Previous algorithms require a cutoff frequency which may be as large as Omega(root d/Delta). The number of measurements taken by and the computational complexity of our algorithm are bounded by a polynomial in both the number of points k and the dimension d, with no dependence on the separation Delta. In contrast, previous algorithms depended inverse polynomially on the minimal separation and exponentially on the dimension for both of these quantities. Our estimation procedure itself is simple: we take random bandlimited measurements (as opposed to taking an exponential number of measurements on the hypergrid). Furthermore, our analysis and algorithm are elementary (based on concentration bounds for sampling and the singular value decomposition).	[Huang, Qingqing] MIT, EECS, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Kakade, Sham M.] Univ Washington, Dept Stat Comp Sci & Engn, Seattle, WA 98195 USA	Massachusetts Institute of Technology (MIT); University of Washington; University of Washington Seattle	Huang, QQ (corresponding author), MIT, EECS, LIDS, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	qqh@mit.edu; sham@cs.washington.edu			Washington Research Foundation for innovation in Data-intensive Discovery	Washington Research Foundation for innovation in Data-intensive Discovery	The authors thank Rong Ge and Ankur Moitra for very helpful discussions. Sham Kakade acknowledges funding from the Washington Research Foundation for innovation in Data-intensive Discovery.	Anandkumar A., 2012, 25 ANN C LEARN THEOR, V23; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Candes EJ, 2014, COMMUN PUR APPL MATH, V67, P906, DOI 10.1002/cpa.21455; Chen YX, 2014, IEEE T INFORM THEORY, V60, P6576, DOI 10.1109/TIT.2014.2343623; Dasgupta S, 2003, RANDOM STRUCT ALGOR, V22, P60, DOI 10.1002/rsa.10073; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; Dasgupta S., 2000, P 16 C UNC ART INT, P152; DONOHO DL, 1992, SIAM J MATH ANAL, V23, P1309, DOI 10.1137/0523074; Fernandez-Granda C., 2014, THESIS; Harshman R.A., 1970, WORKING PAPERS PHONE; Komornik V., 2005, SPRINGER MG MATH; LEURGANS SE, 1993, SIAM J MATRIX ANAL A, V14, P1064, DOI 10.1137/0614071; Liao W., 2014, APPL COMPUTATIONAL H; Moitra A., 2014, ARXIV14081681; Mossel Elchanan, 2005, P 37 ANN ACM S THEOR, P366; Nandi S, 2013, COMPUT STAT DATA AN, V58, P147, DOI 10.1016/j.csda.2011.03.002; Pearson K., 1894, Philosophical Transactions, V185a, P71, DOI 10.1098/rsta.1894.0003; Potts D, 2013, LINEAR ALGEBRA APPL, V439, P1024, DOI 10.1016/j.laa.2012.10.036; RUSSELL DL, 1978, SIAM REV, V20, P639, DOI 10.1137/1020095; Sanjeev A., 2001, P 33 ANN ACM SIGACT, P247, DOI [DOI 10.1145/380752.380808, 10.1145/380752.380808]; Schiebinger G., 2015, ARXIV150603144; Tang GG, 2013, IEEE T INFORM THEORY, V59, P7465, DOI 10.1109/TIT.2013.2277451; Vempala S. S., 2014, ARXIV14122954	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100105
C	Huang, TK; Agarwal, A; Hsu, D; Langford, J; Schapire, RE		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Huang, Tzu-Kuo; Agarwal, Alekh; Hsu, Daniel; Langford, John; Schapire, Robert E.			Efficient and Parsimonious Agnostic Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We develop a new active learning algorithm for the streaming setting satisfying three important properties: 1) It provably works for any classifier representation and classification problem including those with severe noise. 2) It is efficiently implementable with an ERM oracle. 3) It is more aggressive than all previous approaches satisfying 1 and 2. To do this, we create an algorithm based on a newly defined optimization problem and analyze it. We also conduct the first experimental analysis of all efficient agnostic active learning algorithms, evaluating their strengths and weaknesses in different settings.	[Huang, Tzu-Kuo; Agarwal, Alekh; Langford, John; Schapire, Robert E.] Microsoft Res, New York, NY 10011 USA; [Hsu, Daniel] Columbia Univ, New York, NY 10027 USA	Microsoft; Columbia University	Huang, TK (corresponding author), Microsoft Res, New York, NY 10011 USA.	tkhuang@microsoft.com; alekha@microsoft.com; djhsu@cs.columbia.edu; jcl@microsoft.com; schapire@microsoft.com		Hsu, Daniel/0000-0002-3495-7113				Balcan MF, 2007, LECT NOTES COMPUT SC, V4539, P35, DOI 10.1007/978-3-540-72927-3_5; Balcan P. Long, 2013, C LEARNING THEORY CO, P288; Balean Maria-Florina, 2006, P 23 INT C MACH LEAR, P65, DOI DOI 10.1145/1143844.1143853]; Beygelzimer A., 2010, NIPS; Beygelzimer Alina, 2009, ICML; Castro RM, 2008, IEEE T INFORM THEORY, V54, P2339, DOI 10.1109/TIT.2008.920189; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Dasgupta S., 2005, ADV NEURAL INFORM PR, V18; Dasgupta S., 2007, NIPS; Hanneke S., 2009, THESIS; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Hsu D., 2010, THESIS; Huang Tzu-Kuo, 2015, ARXIV150608669; Karampatziakis N, 2011, P 27 C UNC ART INT, P392; Koltchinskii V, 2010, J MACH LEARN RES, V11, P2457; Tsybakov AB, 2004, ANN STAT, V32, P135; Welling M., 2014, ADV NEURAL INFORM PR, V27	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102087
C	Inouye, DI; Ravikumar, P; Dhillon, IS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Inouye, David I.; Ravikumar, Pradeep; Dhillon, Inderjit S.			Fixed-Length Poisson MRF: Adding Dependencies to the Multinomial	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a novel distribution that generalizes the Multinomial distribution to enable dependencies between dimensions. Our novel distribution is based on the parametric form of the Poisson MRF model [1] but is fundamentally different because of the domain restriction to a fixed-length vector like in a Multinomial where the number of trials is fixed or known. Thus, we propose the Fixed-Length Poisson MRF (LPMRF) distribution. We develop AIS sampling methods to estimate the likelihood and log partition function (i.e. the log normalizing constant), which was not developed for the Poisson MRF model. In addition, we propose novel mixture and topic models that use LPMRF as a base distribution and discuss the similarities and differences with previous topic models such as the recently proposed Admixture of Poisson MRFs [2]. We show the effectiveness of our LPMRF distribution over Multinomial models by evaluating the test set perplexity on a dataset of abstracts and Wikipedia. Qualitatively, we show that the positive dependencies discovered by LPMRF are interesting and intuitive. Finally, we show that our algorithms are fast and have good scaling (code available online).	[Inouye, David I.; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Inouye, DI (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	dinouye@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu		Inouye, David/0000-0003-4493-3358	NSF [DGE-1110007, IIS-1149803, IIS-1447574, DMS-1264033, CCF-1320746]; ARO [W911NF-12-1-0390]	NSF(National Science Foundation (NSF)); ARO	This work was supported by NSF (DGE-1110007, IIS-1149803, IIS-1447574, DMS-1264033, CCF-1320746) and ARO (W911NF-12-1-0390).	Aletras Nikolaos, 2013, P 10 INT C COMP SEM, P13; Altham P.M.E., 1978, J ROYAL STAT SOC SER, V27, P162, DOI [10.2307/2346943, DOI 10.2307/2346943]; Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chang J., 2009, NIPS; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Inouye D., 2014, P 31 INT C MACH LEAR, P683; Inouye D. I., 2014, P ANN C NEUR INF PRO, P3158; Mimno D, 2011, P 2011 C EMPIRICAL M, P227, DOI 10.5555/2145432.2145459; Mimno D, 2011, P C EMP METH NAT LAN, P262, DOI DOI 10.5555/2145432.2145462; Nallapati R., 2007, ICDM, P343; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Newman D, 2010, P 10 ANN JOINT C DIG, P215, DOI DOI 10.1145/1816123.1816156; Steyvers M., 2007, HDB LATENT SEMANTIC, V427, P424, DOI DOI 10.1109/MSP.2010.938079; Yang E., 2013, ADV NEURAL INFORM PR, P1718; Yang E., 2012, ADV NEURAL INFO PROC, P1358	16	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102026
C	Jain, P; Tewari, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Jain, Prateek; Tewari, Ambuj			Alternating Minimization for Regression Problems with Vector-valued Outputs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				RECOVERY	In regression problems involving vector-valued outputs (or equivalently, multiple responses), it is well known that the maximum likelihood estimator (MLE), which takes noise covariance structure into account, can be significantly more accurate than the ordinary least squares (OLS) estimator. However, existing literature compares OLS and MLE in terms of their asymptotic, not finite sample, guarantees. More crucially, computing the MLE in general requires solving a non-convex optimization problem and is not known to be efficiently solvable. We provide finite sample upper and lower bounds on the estimation error of OLS and MLE, in two popular models: a) Pooled model, b) Seemingly Unrelated Regression (SUR) model. We provide precise instances where the MLE is significantly more accurate than OLS. Furthermore, for both models, we show that the output of a computationally efficient alternating minimization procedure enjoys the same performance guarantee as MLE, up to universal constants. Finally, we show that for high-dimensional settings as well, the alternating minimization procedure leads to significantly more accurate solutions than the corresponding OLS solutions but with error bound that depends only logarithmically on the data dimensionality.	[Jain, Prateek] Microsoft Res, Bangalore, Karnataka, India; [Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA	Microsoft; University of Michigan System; University of Michigan	Jain, P (corresponding author), Microsoft Res, Bangalore, Karnataka, India.	prajain@microsoft.com; tewaria@umich.edu						Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Arora S., 2014, C LEARNING THEORY, P779; Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Fiebig D.G., 2001, COMPANION THEORETICA; Giles, 1987, SEEMINGLY UNRELATED; Greene WH, 2003, ECONOMETRIC ANAL; Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1; Izenman A., 2008, MODERN MULTIVARIATE; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Jordan M.I., 2014, C LEARN THEOR, P921; Kar P., 2014, ADV NEURAL INFORM PR, P685; Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013; Lounici Karim, 2009, COLT; Moon H. R, 2008, NEW PALGRAVE DICT EC; Negahban SN, 2011, IEEE T INFORM THEORY, V57, P3841, DOI 10.1109/TIT.2011.2144150; OBERHOFER W, 1974, ECONOMETRICA, V42, P579, DOI 10.2307/1911792; Obozinski G, 2011, ANN STAT, V39, P1, DOI 10.1214/09-AOS776; Pourahmadi M., 2013, HIGH DIMENSIONAL COV; Rai Piyush, 2012, P 25 INT C NEUR INF, V25, P3185; Reinsel Gregory, 2004, ENCY STAT SCI; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Sohn K., 2012, AISTATS; Vershynin Roman, 2010, CORR; ZELLNER A, 1962, J AM STAT ASSOC, V57, P348, DOI 10.2307/2281644; Zellner A, 2010, J ECONOMETRICS, V159, P33, DOI 10.1016/j.jeconom.2010.04.005	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101079
C	Jain, P; Natarajan, N; Tewari, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Jain, Prateek; Natarajan, Nagarajan; Tewari, Ambuj			Predtron: A Family of Online Algorithms for General Prediction Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Modern prediction problems arising in multilabel learning and learning to rank pose unique challenges to the classical theory of supervised learning. These problems have large prediction and label spaces of a combinatorial nature and involve sophisticated loss functions. We offer a general framework to derive mistake driven online algorithms and associated loss bounds. The key ingredients in our framework are a general loss function, a general vector space representation of predictions, and a notion of margin with respect to a general norm. Our general algorithm, Predtron, yields the perceptron algorithm and its variants when instantiated on classic problems such as binary classification, multiclass classification, ordinal regression, and multilabel classification. For multilabel ranking and subset ranking, we derive novel algorithms, notions of margins, and loss bounds. A simulation study confirms the behavior predicted by our bounds and demonstrates the flexibility of the design choices in our framework.	[Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India; [Natarajan, Nagarajan] Univ Texas Austin, Austin, TX 78712 USA; [Tewari, Ambuj] Univ Michigan, Ann Arbor, MI 48109 USA	University of Texas System; University of Texas Austin; University of Michigan System; University of Michigan	Jain, P (corresponding author), Microsoft Res, Bengaluru, Karnataka, India.	prajain@microsoft.com; naga86@cs.utexas.edu; tewaria@umich.edu			NSF [IIS-1319810]	NSF(National Science Foundation (NSF))	A. Tewari acknowledges the support of NSF under grant IIS-1319810.	Collins M, 2002, PROCEEDINGS OF THE 2002 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING, P1; Cossock D, 2008, IEEE T INFORM THEORY, V54, P5140, DOI 10.1109/TIT.2008.929939; Crammer K, 2002, ADV NEUR IN, V14, P641; Crammer K, 2003, J MACH LEARN RES, V3, P1025, DOI 10.1162/153244303322533188; Crammer K, 2003, J MACH LEARN RES, V3, P951, DOI 10.1162/jmlr.2003.3.4-5.951; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Li Y., 2002, PROC ICML, V2, P379; Mencia EL, 2008, IEEE IJCNN, P2899, DOI 10.1109/IJCNN.2008.4634206; Mohri M., 2018, FDN MACHINE LEARNING; Mroueh Y., 2012, ADV NEURAL INFORM PR, P2789; Novikoff, 1962, P S MATH THEOR AUT, P615; Ramaswamy Harish G, 2012, ADV NEURAL INFORM PR, P2078; Ratliff N. D., 2007, INT C ART INT STAT, P380; Ratsch Gunnar, NIPS 2002 WORKSH CLA; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Tewari Ambuj, 2015, JMLR WORKSHOP C P, V37	17	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103038
C	Jawanpuria, P; Lapin, M; Hein, M; Schiele, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Jawanpuria, Pratik; Lapin, Maksim; Hein, Matthias; Schiele, Bernt			Efficient Output Kernel Learning for Multiple Tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The paradigm of multi-task learning is that one can achieve better generalization by learning tasks jointly and thus exploiting the similarity between the tasks rather than learning them independently of each other. While previously the relationship between tasks had to be user-defined in the form of an output kernel, recent approaches jointly learn the tasks and the output kernel. As the output kernel is a positive semidefinite matrix, the resulting optimization problems are not scalable in the number of tasks as an eigendecomposition is required in each step. Using the theory of positive semidefinite kernels we show in this paper that for a certain class of regularizers on the output kernel, the constraint of being positive semidefinite can be dropped as it is automatically satisfied for the relaxed problem. This leads to an unconstrained dual problem which can be solved efficiently. Experiments on several multi-task and multi-class data sets illustrate the efficacy of our approach in terms of computational efficiency as well as generalization performance.	[Jawanpuria, Pratik; Hein, Matthias] Saarland Univ, Saarbrucken, Germany; [Lapin, Maksim; Schiele, Bernt] Max Planck Inst Informat, Saarbrucken, Germany	Saarland University; Max Planck Society	Jawanpuria, P (corresponding author), Saarland Univ, Saarbrucken, Germany.				Cluster of Excellence (MMCI)	Cluster of Excellence (MMCI)	P.J. and M.H. acknowledge the support by the Cluster of Excellence (MMCI).	Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; [Anonymous], 2010, NIPS; [Anonymous], 2002, LEARNING KERNELS; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; BENISRAEL A, 1986, J AUST MATH SOC B, V28, P1, DOI 10.1017/S0334270000005142; Caponnetto A, 2008, J MACH LEARN RES, V9, P1615; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Ciliberto C., 2015, ICML; Dinuzzo F, 2011, ICML; Evgeniou T, 2005, J MACH LEARN RES, V6, P615; Evgeniou T., 2004, KDD; Hein M., 2004, TR127 M PLANCK I BIO; Hiai F, 2009, LINEAR ALGEBRA APPL, V431, P1125, DOI 10.1016/j.laa.2009.04.001; HORN RA, 1969, T AM MATH SOC, V136, P269, DOI 10.2307/1994714; Jacob L., 2008, NIPS; Jawanpuria P., 2012, ICML; Jawanpuria P., 2011, SDM; Jawanpuria P, 2015, J MACH LEARN RES, V16, P617; Kang Z., 2011, ICML; Koskela M., 2014, P ACM INT C MULT; Lapin M., 2014, CVPR; Lounici Karim, 2009, COLT; Maurer A., 2013, ICML; Micchelli C. A., 2005, NIPS; Xiao J., 2010, CVPR; Zhang Y, 2010, UAI; Zhou B., 2014, NIPS	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102047
C	Johansson, FD; Chattoraj, A; Bhattacharyya, C; Dubhashi, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Johansson, Fredrik D.; Chattoraj, Ankani; Bhattacharyya, Chiranjib; Dubhashi, Devdatt			Weighted Theta Functions and Embeddings with Applications to Max-Cut, Clustering and Summarization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHMS; SUPPORT	We introduce a unifying generalization of the Lovasz theta function, and the associated geometric embedding, for graphs with weights on both nodes and edges. We show how it can be computed exactly by semidefinite programming, and how to approximate it using SVM computations. We show how the theta function can be interpreted as a measure of diversity in graphs and use this idea, and the graph embedding in algorithms for Max-Cut, correlation clustering and document summarization, all of which are well represented as problems on weighted graphs.	[Johansson, Fredrik D.; Dubhashi, Devdatt] Chalmers Univ Technol, Comp Sci & Engn, SE-41296 Gothenburg, Sweden; [Chattoraj, Ankani] Univ Rochester, Brain & Cognit Sci, Rochester, NY 14627 USA; [Bhattacharyya, Chiranjib] Indian Inst Sci, Comp Sci & Automat, Bangalore 560012, Karnataka, India	Chalmers University of Technology; University of Rochester; Indian Institute of Science (IISC) - Bangalore	Johansson, FD (corresponding author), Chalmers Univ Technol, Comp Sci & Engn, SE-41296 Gothenburg, Sweden.	frejohk@chalmers.se; achattor@ur.rochester.edu; chiru@csa.iisc.ernet.in; dubhashi@chalmers.se		Johansson, Fredrik/0000-0002-4323-3715	Swedish Foundation for Strategic Research (SSF)	Swedish Foundation for Strategic Research (SSF)(Swedish Foundation for Strategic Research)	This work is supported in part by the Swedish Foundation for Strategic Research (SSF).	Arora S, 2005, ANN IEEE SYMP FOUND, P339, DOI 10.1109/SFCS.2005.35; Arora Sanjeev, 2012, THEOR COMPUT, V8, P121; Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; Bonchi F, 2013, KNOWL INF SYST, V35, P1, DOI 10.1007/s10115-012-0522-9; Brand M, 2006, LINEAR ALGEBRA APPL, V415, P20, DOI 10.1016/j.laa.2005.07.021; Burer S, 2001, OPTIM METHOD SOFTW, V15, P175, DOI 10.1080/10556780108805818; Elsner Micha, 2009, P WORKSH INT LIN PRO, P19; Goemans MX, 1997, MATH PROGRAM, V79, P143, DOI 10.1007/BF02614315; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Grotschel M, 1988, ALGORITHMS COMBINATO, V2; Helmberg C, 2000, SIAM J OPTIMIZ, V10, P673, DOI 10.1137/S1052623497328987; Hush D, 2006, J MACH LEARN RES, V7, P733; Iyengar G, 2011, SIAM J OPTIMIZ, V21, P231, DOI 10.1137/090762671; Jethava V., 2013, INF THEOR WORKSH ITW, P1; Jethava V, 2013, J MACH LEARN RES, V14, P3495; Johanson F. D., 2015, SUPPLEMENTARY MAT; Knuth D. E., 1994, ELECT J COMB, V1; Lin Hui, 2011, P 49 ANN M ASS COMP, P510; LOVASZ L, 1979, IEEE T INFORM THEORY, V25, P1, DOI 10.1109/TIT.1979.1055985; LOVASZ L, 1999, P ERDOS HIS MATH; Marti R, 2009, INFORMS J COMPUT, V21, P26, DOI 10.1287/ijoc.1080.0275; Pham DT, 2005, P I MECH ENG C-J MEC, V219, P103, DOI 10.1243/095440605X8298; Scholkopf B, 2001, NEURAL COMPUT, V13, P1443, DOI 10.1162/089976601750264965; SCHRIJVER A, 1979, IEEE T INFORM THEORY, V25, P425, DOI 10.1109/TIT.1979.1056072; Wang J, 2013, J MACH LEARN RES, V14, P771	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102051
C	Kappel, D; Habenschuss, S; Legenstein, R; Maass, W		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kappel, David; Habenschuss, Stefan; Legenstein, Robert; Maass, Wolfgang			Synaptic Sampling: A Bayesian Approach to Neural Network Plasticity and Rewiring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				NEURONS; EMERGENCE	We reexamine in this article the conceptual and mathematical framework for understanding the organization of plasticity in spiking neural networks. We propose that inherent stochasticity enables synaptic plasticity to carry out probabilistic inference by sampling from a posterior distribution of synaptic parameters. This view provides a viable alternative to existing models that propose convergence of synaptic weights to maximum likelihood parameters. It explains how priors on weight distributions and connection probabilities can be merged optimally with learned experience. In simulations we show that our model for synaptic plasticity allows spiking neural networks to compensate continuously for unforeseen disturbances. Furthermore it provides a normative mathematical framework to better understand the permanent variability and rewiring observed in brain networks.	[Kappel, David; Habenschuss, Stefan; Legenstein, Robert; Maass, Wolfgang] Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Kappel, D (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.	kappel@igi.tugraz.at; habenschuss@igi.tugraz.at; legi@igi.tugraz.at; maass@igi.tugraz.at			European Union [604102]; CHIST-ERA ERA-Net [I753-N23]	European Union(European Commission); CHIST-ERA ERA-Net	Written under partial support of the European Union project #604102 The Human Brain Project (HBP) and CHIST-ERA ERA-Net (Project FWF #I753-N23, PNEUMA).	Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Bill J, 2014, FRONTIERS NEUROSCIEN, V8; Bishop C.M, 2006, PATTERN RECOGN; Brea J., 2011, ADV NEURAL INF PROCE, V24, P1422; Carandini M, 2012, NAT NEUROSCI, V15, P507, DOI 10.1038/nn.3043; Gardiner C. W., 2004, HDB STOCHASTIC METHO; Gerstner W., 2002, SPIKING NEURON MODEL; Habenschuss S, 2012, ADV NEURAL INFORM PR, V25, P782; Habenschuss S, 2013, NEURAL COMPUT, V25, P1371, DOI 10.1162/NECO_a_00446; Hanchen Xiong, 2014, Artificial Neural Networks and Machine Learning - ICANN 2014. 24th International Conference on Artificial Neural Networks. Proceedings: LNCS 8681, P419, DOI 10.1007/978-3-319-11179-7_53; Hatfield G., 2002, PERCEPTION PHYS WORL, P113, DOI [10.1002/0470013427.ch5, DOI 10.1002/0470013427.CH5]; Holtmaat AJGD, 2005, NEURON, V45, P279, DOI 10.1016/j.neuron.2005.01.003; Jolivet R, 2006, J COMPUT NEUROSCI, V21, P35, DOI 10.1007/s10827-006-7074-5; Kappel D, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004485; Kappel D, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003511; Kennedy AD, 1999, PARALLEL COMPUT, V25, P1311, DOI 10.1016/S0167-8191(99)00053-8; Loewenstein Y, 2011, J NEUROSCI, V31, P9481, DOI 10.1523/JNEUROSCI.6130-10.2011; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Marder E, 2011, P NATL ACAD SCI USA, V108, P15542, DOI 10.1073/pnas.1010674108; Mensi S., 2011, ADV NEURAL INF PROCE, P1377; Montgomery JM, 2001, NEURON, V29, P691, DOI 10.1016/S0896-6273(01)00244-6; Nessler B., 2009, ADV NEURAL INFORM PR, V22, P1357; Pouget A, 2013, NAT NEUROSCI, V16, P1170, DOI 10.1038/nn.3495; Rezende DJ, 2014, FRONT COMPUT NEUROSC, V8, DOI 10.3389/fncom.2014.00038; Sato I, 2014, PR MACH LEARN RES, V32, P982; Schemmel J, 2006, IEEE IJCNN, P1; Sjostrom PJ, 2001, NEURON, V32, P1149, DOI 10.1016/s0896-6273(01)00542-6; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Winkler I, 2012, PHILOS T R SOC B, V367, P1001, DOI 10.1098/rstb.2011.0359	29	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102100
C	Kirillov, A; Schlesinger, D; Vetrov, D; Rother, C; Savchynskyy, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kirillov, Alexander; Schlesinger, Dmitrij; Vetrov, Dmitry; Rother, Carsten; Savchynskyy, Bogdan			M-Best-Diverse Labelings for Submodular Energies and Beyond	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MINIMIZATION	We consider the problem of finding M best diverse solutions of energy minimization problems for graphical models. Contrary to the sequential method of Batra et al., which greedily finds one solution after another, we infer all M solutions jointly. It was shown recently that such jointly inferred labelings not only have smaller total energy but also qualitatively outperform the sequentially obtained ones. The only obstacle for using this new technique is the complexity of the corresponding inference problem, since it is considerably slower algorithm than the method of Batra et al. In this work we show that the joint inference of M best diverse solutions can be formulated as a submodular energy minimization if the original MAP-inference problem is submodular, hence fast inference techniques can be used. In addition to the theoretical results we provide practical algorithms that outperform the current state-of-the-art and can be used in both submodular and non-submodular case.	[Kirillov, Alexander; Schlesinger, Dmitrij; Rother, Carsten; Savchynskyy, Bogdan] Tech Univ Dresden, Dresden, Germany; [Vetrov, Dmitry] Skoltech, Moscow, Russia	Technische Universitat Dresden	Kirillov, A (corresponding author), Tech Univ Dresden, Dresden, Germany.	alexander.kirillov@tu-dresden.de			European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme [647769]; RFBR [15-31-20596]; Microsoft [RPD 1053945]	European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme(European Research Council (ERC)); RFBR(Russian Foundation for Basic Research (RFBR)); Microsoft(Microsoft)	This project has received funding from the European Research Council (ERC) under the European Unions Horizon 2020 research and innovation programme (grant agreement No 647769). D. Vetrov was supported by RFBR proj. (No. 15-31-20596) and by Microsoft (RPD 1053945).	[Anonymous], NIPS WORKSH PERT OPT; Arora C., 2015, TPAMI; Batra D., 2012, ECCV; Batra D., 2012, ARXIV12104841; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chen C., 2013, AISTATS; Elidan G., PROBABILISTIC INFERE; Everingham M., 2012, PASCAL VISUAL OBJECT; Fix A., 2011, ICCV; Franc V, 2008, J MACH LEARN RES, V9, P67; Fromer M., 2009, NIPS, V22; Guzman-Rivera A., 2014, AISTATS; Guzman-Rivera A., 2013, AISTATS; Guzman-Rivera A., 2012, NIPS, V25; Ishikawa H., 2003, TPAMI; Jegelka S., 2011, CVPR; Jolly M., 2001, ICCV; Kappes JH, 2015, INT J COMPUT VISION, V115, P155, DOI 10.1007/s11263-015-0809-x; Kirillov A., 2015, ICCV; Kolmogorov V., 2012, DISCRETE APPL MATH; Kolmogorov V., 2004, TPAMI; Kulesza A., 2010, NIPS, V23; Lawler E. L., 1972, MANAGEMENT SCI, V18; Nilsson D, 1998, STAT COMPUT, V8, P159, DOI 10.1023/A:1008990218483; Prasad A., 2014, NIPS, V27; Premachandran V., 2014, CVPR; Schlesinger D., 2006, TRANSFORMING ARBITRA; Schlesinger M. I., 2002, COMP IMAG VIS, V24; Tarlow D., 2010, AISTATS; Werner T., 2007, TPAMI, V29; Yadollahpour P., 2013, CVPR; Yanover C., 2004, NIPS, V17	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103015
C	Kiros, R; Zhu, YK; Salakhutdinov, R; Zemel, RS; Torralba, A; Urtasun, R; Fidler, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kiros, Ryan; Zhu, Yukun; Salakhutdinov, Ruslan; Zemel, Richard S.; Torralba, Antonio; Urtasun, Raquel; Fidler, Sanja			Skip-Thought Vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We describe an approach for unsupervised learning of a generic, distributed sentence encoder. Using the continuity of text from books, we train an encoder-decoder model that tries to reconstruct the surrounding sentences of an encoded passage. Sentences that share semantic and syntactic properties are thus mapped to similar vector representations. We next introduce a simple vocabulary expansion method to encode words that were not seen as part of training, allowing us to expand our vocabulary to a million words. After training our model, we extract and evaluate our vectors with linear models on 8 tasks: semantic relatedness, paraphrase detection, image-sentence ranking, question-type classification and 4 benchmark sentiment and subjectivity datasets. The end result is an off-the-shelf encoder that can produce highly generic sentence representations that are robust and perform well in practice.	[Kiros, Ryan; Zhu, Yukun; Salakhutdinov, Ruslan; Zemel, Richard S.; Urtasun, Raquel; Fidler, Sanja] Univ Toronto, Toronto, ON, Canada; [Salakhutdinov, Ruslan; Zemel, Richard S.] Canadian Inst Adv Res, Toronto, ON, Canada; [Torralba, Antonio] MIT, Cambridge, MA 02139 USA	University of Toronto; Canadian Institute for Advanced Research (CIFAR); Massachusetts Institute of Technology (MIT)	Kiros, R (corresponding author), Univ Toronto, Toronto, ON, Canada.				NSERC; Samsung; CIFAR; Google; ONR [N00014-14-1-0232]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Samsung(Samsung); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Google(Google Incorporated); ONR(Office of Naval Research)	We thank Geoffrey Hinton for suggesting the name skip-thoughts. We also thank Felix Hill, Kelvin Xu, Kyunghyun Cho and Ilya Sutskever for valuable comments and discussion. This work was supported by NSERC, Samsung, CIFAR, Google and ONR Grant N00014-14-1-0232.	[Anonymous], 2014, SEMEVAL 2014; [Anonymous], 2014, 8 INT WORKSHOP SEMAN; [Anonymous], 2013, EMNLP; Bahdanau Dzmitry, 2015, ICLR; Bengio Y., 2014, NIPS DEEP LEARN WORK; Brockett C., 2004, P 20 INT C COMP LING; Cho Kyunghyun, 2014, SSST8; Cho Kyunghyun, 2014, EMNLP; Das Dipanjan, 2009, ACL; Finch Andrew, 2005, IWP; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Ji Y., 2013, EMNLP, P891; Jimenez Sergio, 2014, SEMEVAL 2014; Kalchbrenner N., 2014, ACL; Kalchbrenner Nal, 2013, P 2013 C EMP METH NA, P1700, DOI DOI 10.1146/ANNUREV.NEURO.26.041002.131047; Karpathy A., 2015, CVPR; Kim Yoon, 2014, EMNLP; Kingma D.P., 2015, INT C LEARN REPR, P1; Klein B., 2015, CVPR; Lai Alice, 2014, SEMEVAL 2014; Le Quoc V, 2014, ICML; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Maaten L. v. d, 2008, JMLR; Madnani Nitin, 2012, NAACL; Mao J, 2015, 3 INT C LEARN REPR I; Mikolov T., 2013, COMPUTER SCI; Mikolov T., 2013, EFFICIENT ESTIMATION; Saxe Andrew M., 2014, ICLR; Simonyan Karen, 2015, INT C LEARN REPR; Socher R., 2014, TACL; Socher Richard, 2011, NIPS; Sutskever I., 2014, NEURIPS; Tai Kai Sheng, 2015, ACL; Wan Stephen, 2006, P AUSTR LANG TECHN W; Wang Sida, 2012, ACL; Zhao Han, 2015, IJCAI; Zhao Jiang, 2014, SEMEVAL 2014; Zhu Y., 2015, ICCV	38	0	0	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102098
C	Kobilarov, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kobilarov, Marin			Sample Complexity Bounds for Iterative Stochastic Policy Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper is concerned with robustness analysis of decision making under uncertainty. We consider a class of iterative stochastic policy optimization problems and analyze the resulting expected performance for each newly updated policy at each iteration. In particular, we employ concentration-of-measure inequalities to compute future expected cost and probability of constraint violation using empirical runs. A novel inequality bound is derived that accounts for the possibly unbounded change-of-measure likelihood ratio resulting from iterative policy adaptation. The bound serves as a high-confidence certificate for providing future performance or safety guarantees. The approach is illustrated with a simple robot control scenario and initial steps towards applications to challenging aerial vehicle navigation problems are presented.	[Kobilarov, Marin] Johns Hopkins Univ, Dept Mech Engn, Baltimore, MD 21218 USA	Johns Hopkins University	Kobilarov, M (corresponding author), Johns Hopkins Univ, Dept Mech Engn, Baltimore, MD 21218 USA.	marin@jhu.edu						Bemporad A, 1999, LECT NOTES CONTR INF, V245, P207; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Cortes Corinna, 2010, ADV NEURAL INFORM PR, V23; Deisenroth M. P., 2013, SURVEY POLICY SEARCH, P388; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Igel C, 2007, EVOL COMPUT, V15, P1, DOI 10.1162/evco.2007.15.1.1; Kobilarov M, 2013, P AMER CONTR CONF, P1044; Kobilarov M, 2012, INT J ROBOT RES, V31, P855, DOI 10.1177/0278364912444543; Koltchinskii V, 2001, APPL MATH COMPUT, V120, P31, DOI 10.1016/S0096-3003(99)00283-0; Langford J, 2005, J MACH LEARN RES, V6, P273; Larraaga, 2002, ESTIMATION DISTRIBUT; Levine S, 2014, NEURAL INFORM PROCES; Mahony R, 2004, INT J ROBUST NONLIN, V14, P1035, DOI 10.1002/rnc.931; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Pelikan M, 2002, COMPUT OPTIM APPL, V21, P5, DOI 10.1023/A:1013500812258; RAY LR, 1993, AUTOMATICA, V29, P229, DOI 10.1016/0005-1098(93)90187-X; Rubinstein R., 2004, CROSS ENTROPY METHOD; Schaal S, 2010, IEEE ROBOT AUTOM MAG, V17, P20, DOI 10.1109/MRA.2010.936957; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Tempo R., 2004, RANDOMIZED ALGORITHM; Theodorou EA, 2010, J MACH LEARN RES, V11, P3137; Thrun, 2005, PRINCIPLES ROBOT MOT; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Vidyasagar M, 2008, J PROCESS CONTR, V18, P421, DOI 10.1016/j.jprocont.2007.10.009; Vidyasagar M, 2001, AUTOMATICA, V37, P1515, DOI 10.1016/S0005-1098(01)00122-4; Wang Q, 2006, PROBABILISTIC AND RANDOMIZED METHODS FOR DESIGN UNDER UNCERTAINTY, P381, DOI 10.1007/1-84628-095-8_15; Zhigljavsky A., 2008, STOCHASTIC GLOBAL OP	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100061
C	Komiyama, J; Honda, J; Nakagawa, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Komiyama, Junpei; Honda, Junya; Nakagawa, Hiroshi			Regret Lower Bound and Optimal Algorithm in Finite Stochastic Partial Monitoring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Partial monitoring is a general model for sequential learning with limited feedback formalized as a game between two players. In this game, the learner chooses an action and at the same time the opponent chooses an outcome, then the learner suffers a loss and receives a feedback signal. The goal of the learner is to minimize the total loss. In this paper, we study partial monitoring with finite actions and stochastic outcomes. We derive a logarithmic distribution-dependent regret lower bound that defines the hardness of the problem. Inspired by the DMED algorithm (Honda and Takemura, 2010) for the multi-armed bandit problem, we propose PM-DMED, an algorithm that minimizes the distribution-dependent regret. PM-DMED significantly outperforms state-of-the-art algorithms in numerical experiments. To show the optimality of PM-DMED with respect to the regret bound, we slightly modify the algorithm by introducing a hinge function (PM-DMED-Hinge). Then, we derive an asymptotically optimal regret upper bound of PM-DMED-Hinge that matches the lower bound.	[Komiyama, Junpei; Honda, Junya; Nakagawa, Hiroshi] Univ Tokyo, Tokyo, Japan	University of Tokyo	Komiyama, J (corresponding author), Univ Tokyo, Tokyo, Japan.	junpei@komiyama.info; honda@stat.t.u-tokyo.ac.jp; nakagawa@dl.itc.u-tokyo.ac.jp	Komiyama, Junpei/ABR-9951-2022		JSPS KAKENHI [15J09850, 26106506]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors gratefully acknowledge the advice of Kentaro Minami and sincerely thank the anonymous reviewers for their useful comments. This work was supported in part by JSPS KAKENHI Grant Number 15J09850 and 26106506.	Agarwal A., 2010, P 13 INT C ART INT S, P9; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bartok G., 2013, COLT, P696; Bartok Gabor, 2011, P INT C COMP LEARN T, P133; Bartok Gabor, 2012, ICML; Cesa-Bianchi N, 2005, IEEE T INFORM THEORY, V51, P2152, DOI 10.1109/TIT.2005.847729; Cesa-Bianchi N, 2006, MATH OPER RES, V31, P562, DOI 10.1287/moor.1060.0206; Dani V, 2008, P C LEARN THEOR COLT, P355; Dembo A, 1998, APPL MATH; Fiacco A. V., 1983, INTRO SENSITIVITY ST; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Honda Junya, 2010, COLT, P67; Ito S, 2000, ANN OPER RES, V98, P189, DOI 10.1023/A:1019208524259; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Piccolboni A, 2001, LECT NOTES ARTIF INT, V2111, P208; Vanchinathan H. P., 2014, NIPS, P1691; Wachter Andreas, INTERIOR POINT OPTIM; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103062
C	Kopp, T; Singla, P; Kautz, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kopp, Tim; Singla, Parag; Kautz, Henry			Lifted Symmetry Detection and Breaking for MAP Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Symmetry breaking is a technique for speeding up propositional satisfiability testing by adding constraints to the theory that restrict the search space while preserving satisfiability. In this work, we extend symmetry breaking to the problem of model finding in weighted and unweighted relational theories, a class of problems that includes MAP inference in Markov Logic and similar statistical-relational languages. We introduce term symmetries, which are induced by an evidence set and extend to symmetries over a relational theory. We provide the important special case of term equivalent symmetries, showing that such symmetries can be found in low-degree polynomial time. We show how to break an exponential number of these symmetries with added constraints whose number is linear in the size of the domain. We demonstrate the effectiveness of these techniques through experiments in two relational domains. We also discuss the connections between relational symmetry breaking and work on lifted inference in statistical-relational reasoning.	[Kopp, Tim; Kautz, Henry] Univ Rochester, Rochester, NY 14627 USA; [Singla, Parag] IIT Delhi, New Delhi, India	University of Rochester; Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Delhi	Kopp, T (corresponding author), Univ Rochester, Rochester, NY 14627 USA.	tkopp@cs.rochester.edu; parags@cse.iitd.ac.in; kautz@cs.rochester.edu	Kautz, Henry/AAF-5190-2020					Aloul FA, 2003, DES AUT CON, P836; Apsel U, 2014, AAAI CONF ARTIF INTE, P2403; Audemard G, 2006, J AUTOM REASONING, V36, P177, DOI 10.1007/s10817-006-9040-3; Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319; Bui Hung B., 2012, P AAAI; Bui Hung Hai, 2013, UAI, P132; CHVATAL V, 1988, J ACM, V35, P759, DOI 10.1145/48014.48016; Crawford J, 1996, MOR KAUF R, P148; Darwiche A., 2013, ADV NEURAL INFORM PR, V26, P2868; den Broeck G.V., 2011, IJCAI 2011 P 22 INT, P2178, DOI [10.5591/978-1-57735-516-8/IJCAI11-363, DOI 10.5591/978-1-57735-516-8/IJCAI11-363]; Domingos P., 2009, SYNTHESIS LECT ARTIF; Domingos P., 2011, UAI, P256; Flener P, 2009, CONSTRAINTS, V14, P506, DOI 10.1007/s10601-008-9059-7; Heras F, 2008, J ARTIF INTELL RES, V31, P1; Katebi H, 2010, LECT NOTES COMPUT SC, V6175, P113, DOI 10.1007/978-3-642-14186-7_11; Le Berre D., 2010, SAT4J LIB RELEASE 2, V7, P59; LUKS EM, 1982, J COMPUT SYST SCI, V25, P42, DOI 10.1016/0022-0000(82)90009-5; Martins Ruben, 2014, LECT NOTES COMPUTER, V8561; Meseguer P, 2001, ARTIF INTELL, V129, P133, DOI 10.1016/S0004-3702(01)00104-7; Mittal H., 2014, P NIPS 14, P649; Mladenov M., 2012, JMLR W CP, V22, P788; Mladenov M, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P603; Niepert M., 2012, UAI, P624; Niepert Mathias, 2013, P AAAI; Noessner Jan, 2013, P AAAI; Poole D., 2003, P INT JOINT C ART IN, P985; Sellmann M, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P298; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; Singla P, 2014, AAAI CONF ARTIF INTE, P2497; Van den Broeck Guy, 2012, P AAAI; Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17; Walsh Toby, 2012, P AAAI	32	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100059
C	Koren, T; Levy, KY		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Koren, Tomer; Levy, Kfir Y.			Fast Rates for Exp-concave Empirical Risk Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ONLINE; STABILITY	We consider Empirical Risk Minimization (ERM) in the context of stochastic optimization with exp-concave and smooth losses-a general optimization framework that captures several important learning problems including linear and logistic regression, learning SVMs with the squared hinge-loss, portfolio selection and more. In this setting, we establish the first evidence that ERM is able to attain fast generalization rates, and show that the expected loss of the ERM solution in d dimensions converges to the optimal expected loss in a rate of d=n. This rate matches existing lower bounds up to constants and improves by a log n factor upon the state-of-the-art, which is only known to be attained by an online-to-batch conversion of computationally expensive online algorithms.	[Koren, Tomer; Levy, Kfir Y.] Technion, IL-32000 Haifa, Israel		Koren, T (corresponding author), Technion, IL-32000 Haifa, Israel.	tomerk@technion.ac.il; kfiryl@tx.technion.ac.il						Abernethy JD, 2012, IEEE T INFORM THEORY, V58, P4164, DOI 10.1109/TIT.2012.2192096; Azoury KS, 2001, MACH LEARN, V43, P211, DOI 10.1023/A:1010896012157; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kivinen J, 1999, LECT NOTES ARTIF INT, V1572, P153; KOREN T, 2013, C LEARN THEOR, P1073; Lecue G., 2014, ARXIV14025763; Mahdavi M., 2015, P 28 C LEARN THEOR; Shalev-Shwartz S, 2016, MATH PROGRAM, V155, P105, DOI 10.1007/s10107-014-0839-0; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Shamir O., 2014, ARXIV14065143; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; Sridharan K., 2009, ADV NEURAL INFORM PR, P1545; Vovk V, 2001, INT STAT REV, V69, P213	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103072
C	Kundu, A; Drineas, P; Magdon-Ismail, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kundu, Abhisek; Drineas, Petros; Magdon-Ismail, Malik			Approximating Sparse PCA from Incomplete Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PRINCIPAL COMPONENT ANALYSIS	We study how well one can recover sparse principal components of a data matrix using a sketch formed from a few of its elements. We show that for a wide class of optimization problems, if the sketch is close (in the spectral norm) to the original data matrix, then one can recover a near optimal solution to the optimization problem by using the sketch. In particular, we use this approach to obtain sparse principal components and show that for m data points in n dimensions, O (e(-2)k max {m, n) elements gives an epsilon-additive approximation to the sparse PCA problem ((k) over tilde is the stable rank of the data matrix). We demonstrate our algorithms extensively on image, text, biological and financial data. The results show that not only are we able to recover the sparse PCAs from the incomplete data, but by using our sparse sketch, the running time drops by a factor of five or more.	[Kundu, Abhisek; Drineas, Petros; Magdon-Ismail, Malik] Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Kundu, A (corresponding author), Rensselaer Polytech Inst, Dept Comp Sci, Troy, NY 12180 USA.	kundua2@rpi.edu; drinep@cs.rpi.edu; magdon@cs.rpi.edu		Drineas, Petros/0000-0003-1994-8670	NSF [IIS-1447283, IIS-1319280]	NSF(National Science Foundation (NSF))	AK and PD are partially supported by NSF IIS-1447283 and IIS-1319280.	Asteris M., 2014, P ICML; CADIMA J, 1995, J APPL STAT, V22, P203, DOI 10.1080/757584614; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Gabrilovich E., 2004, P INT C MACH LEARN; HULL JJ, 1994, IEEE T PATTERN ANAL, V16, P550, DOI 10.1109/34.291440; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Kundu A., 2015, RECOVERING PCA HYBRI; Lei J, 2015, ANN STAT, V43, P299, DOI 10.1214/14-AOS1273; Lounici Karim, 2012, SPARSE PRINCIPAL COM; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; Magdon-Ismail M., 2015, NP HARDNESS INAPPROX; Magdon-Ismail M., 2015, ARXIV REPORT; MOGHADDAM B, 2006, P ICML; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007; Sjstrand K., 2012, J STAT SOFTWARE; Wang Z., 2014, NONCONVEX STAT OPTIM; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102054
C	Kuznetsov, V; Mohri, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kuznetsov, Vitaly; Mohri, Mehryar			Learning Theory and Algorithms for Forecasting Non-Stationary Time Series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				CONVERGENCE; PREDICTION	We present data-dependent learning bounds for the general scenario of non-stationary non-mixing stochastic processes. Our learning guarantees are expressed in terms of a data-dependent measure of sequential complexity and a discrepancy measure that can be estimated from data under some mild assumptions. We use our learning bounds to devise new algorithms for non-stationary time series forecasting for which we report some preliminary experimental results.	[Kuznetsov, Vitaly; Mohri, Mehryar] Courant Inst, New York, NY 10011 USA; [Mohri, Mehryar] Google Res, New York, NY 10011 USA	Google Incorporated	Kuznetsov, V (corresponding author), Courant Inst, New York, NY 10011 USA.	vitaly@cims.nyu.edu; mohri@cims.nyu.edu			NSF [IIS-1117591, CCF-1535987]; NSERC PGS D3	NSF(National Science Foundation (NSF)); NSERC PGS D3	This work was partly funded by NSF IIS-1117591 and CCF-1535987, and the NSERC PGS D3.	Adams TM, 2010, ANN PROBAB, V38, P1345, DOI 10.1214/09-AOP511; Agarwal A, 2013, IEEE T INFORM THEORY, V59, P573, DOI 10.1109/TIT.2012.2212414; Alquier P., 2014, DEPENDENCE MODELLING, V1, P65; Alquier P., 2010, 201039 CTR RECH EC S; Andrews  D., 1983, COWLES FDN DISCUSSIO, V664; Baillie RT, 1996, J ECONOMETRICS, V73, P5, DOI 10.1016/0304-4076(95)01732-1; Barve Rakesh D., 1996, COLT; Berti P, 1997, STAT PROBABIL LETT, V32, P385, DOI 10.1016/S0167-7152(96)00098-3; Bollerslev T, 1986, J ECONOMETRICS; Box G. E. P., 1990, TIME SERIES ANAL FOR; Brockwell P. J., 1986, TIME SERIES THEORY M; de la Pena V. H., 1999, PROBABILITY ITS APPL; Doukhan P, 1994, MIXING PROPERTIES EX, V85; ENGLE RF, 1982, ECONOMETRICA, V50, P987, DOI 10.2307/1912773; Hamilton J.D., 1994, TIME SERIES ANAL, DOI 10.2307/j.ctv14jx6sm; Kuznetsov Vitaly, 2014, ALT; Lozano A. C., 2006, ADV NEURAL INFORM PR, P819; Meir R, 2000, MACH LEARN, V39, P5, DOI 10.1023/A:1007602715810; Modha DS, 1998, IEEE T INFORM THEORY, V44, P117, DOI 10.1109/18.650998; Mohri M., 2012, ALT; Mohri M, 2010, J MACH LEARN RES, V11, P789; Pestov V, 2010, GRC; Rakhlin A., 2010, NIPS; Rakhlin Alexander, 2011, NIPS; Rakhlin Alexander, 2015, PROBABILITY THEORY R; Shalizi Cosma Rohilla, 2013, Adv Neural Inf Process Syst, V26; Steinwart I., 2009, NIPS; Tao PD, 1998, SIAM J OPTIMIZ, V8, P476, DOI 10.1137/S1052623494274313; Vidyasagar M., 1997, THEORY LEARNING GEN	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101095
C	Kveton, B; Wen, Z; Ashkan, A; Szepesvari, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kveton, Branislav; Wen, Zheng; Ashkan, Azin; Szepesvari, Csaba			Combinatorial Cascading Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose combinatorial cascading bandits, a class of partial monitoring problems where at each step a learning agent chooses a tuple of ground items subject to constraints and receives a reward if and only if the weights of all chosen items are one. The weights of the items are binary, stochastic, and drawn independently of each other. The agent observes the index of the first chosen item whose weight is zero. This observation model arises in network routing, for instance, where the learning agent may only observe the first link in the routing path which is down, and blocks the path. We propose a UCB-like algorithm for solving our problems, CombCascade; and prove gap-dependent and gap-free upper bounds on its n-step regret. Our proofs build on recent work in stochastic combinatorial semi-bandits but also address two novel challenges of our setting, a non-linear reward function and partial observability. We evaluate CombCascade on two real-world problems and show that it performs well even when our modeling assumptions are violated. We also demonstrate that our setting requires a new learning algorithm.	[Kveton, Branislav] Adobe Res, San Jose, CA 95110 USA; [Wen, Zheng] Yahoo Labs, Sunnyvale, CA USA; [Ashkan, Azin] Technicolor Res, Los Altos, CA USA; [Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada	Adobe Systems Inc.; Technicolor SA; University of Alberta	Kveton, B (corresponding author), Adobe Res, San Jose, CA 95110 USA.	kveton@adobe.com; zhengwen@yahoo-inc.com; azin.ashkan@technicolor.com; szepesva@cs.ualberta.ca						AGRAWAL R, 1989, IEEE T AUTOMAT CONTR, V34, P258, DOI 10.1109/9.16415; Agrawal S., 2012, P 25 ANN C LEARNING, V23; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bartok Gabor, 2012, P 29 INT C MACH LEAR; Chen W., 2013, ICML 2013, P151; Choi Baek-Young, 2004, P 23 ANN JOINT C IEE; Combes Richard, 2015, P 2015 ACM SIGMET RI; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Garivier A., 2011, P 24 ANN C LEARNING, V24, P359; Kveton B., 2015, P 32 INT C MACH LEAR; Kveton B, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P420; Kveton Branislav, 2015, P 18 INT C ART INT S; Lam Shyong, 2015, MOVIELENS DATASET; Papadimitriou C.H., 1998, COMBINATORIAL OPTIMI, VUnabridged edition; Spring N, 2004, IEEE ACM T NETWORK, V12, P2, DOI 10.1109/TNET.2003.822655; Le T, 2014, IEEE T SIGNAL PROCES, V62, P5919, DOI 10.1109/TSP.2014.2357779; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Wen Zheng, 2015, P 32 INT C MACH LEAR P 32 INT C MACH LEAR	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100090
C	Kyng, R; Rao, A; Sachdeva, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kyng, Rasmus; Rao, Anup; Sachdeva, Sushant			Fast, Provable Algorithms for Isotonic Regression in all l(p)-norms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Given a directed acyclic graph G; and a set of values y on the vertices, the Isotonic Regression of y is a vector x that respects the partial order described by G; and minimizes parallel to x - y parallel to; for a specified norm. This paper gives improved algorithms for computing the Isotonic Regression for all weighted l(p)-norms with rigorous performance guarantees. Our algorithms are quite practical, and variants of them can be implemented to run fast in practice.	[Kyng, Rasmus; Sachdeva, Sushant] Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA; [Rao, Anup] Georgia Tech, Sch Comp Sci, Atlanta, GA USA	Yale University; University System of Georgia; Georgia Institute of Technology	Kyng, R (corresponding author), Yale Univ, Dept Comp Sci, New Haven, CT 06520 USA.	rasmus.kyng@yale.edu; arao89@gatech.edu; sachdeva@cs.yale.edu			AFOSR Award [FA9550-12-1-0175]; NSF [CCF-1111257]; Simons Investigator Award	AFOSR Award(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); NSF(National Science Foundation (NSF)); Simons Investigator Award	We thank Sabyasachi Chatterjee for introducing the problem to us, and Daniel Spielman for his advice and comments. We would also like to thank Quentin Stout and anonymous reviewers for their suggestions. This research was partially supported by AFOSR Award FA9550-12-1-0175, NSF grant CCF-1111257, and a Simons Investigator Award to Daniel Spielman.	Acton ST, 1998, IEEE T IMAGE PROCESS, V7, P979, DOI 10.1109/83.701153; Angelov S., 2006, SODA; AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Barlow D. J., 1972, STAT INFERENCE ORDER; Boyd S, 2004, CONVEX OPTIMIZATION; Chatterjee S., ANN STAT; Cohen M. B., 2014, STOC 14; Daitch SI, 2008, ACM S THEORY COMPUT, P451; DENHERTOG D, 1995, MATH PROGRAM, V69, P75, DOI 10.1007/BF01585553; DYKSTRA RL, 1982, ANN STAT, V10, P708, DOI 10.1214/aos/1176345866; GEBHARDT F, 1970, BIOMETRIKA, V57, P263, DOI 10.1093/biomet/57.2.263; Hochbaum DS, 2003, SIAM J DISCRETE MATH, V16, P192, DOI 10.1137/S0895480100369584; Kakade S. M., 2011, NIPS; KALAI A. T., 2009, COLT; KAUFMAN Y, 1993, DISCRETE APPL MATH, V47, P251, DOI 10.1016/0166-218X(93)90130-G; Koutis I, 2011, ANN IEEE SYMP FOUND, P590, DOI 10.1109/FOCS.2011.85; Kyng R., 2015, COLT, P1190; LEE CIC, 1983, ANN STAT, V11, P467, DOI 10.1214/aos/1176346153; Lee Y. T., 2014, FOCS; Madry A., 2013, FOCS; MAXWELL WL, 1985, OPER RES, V33, P1316, DOI 10.1287/opre.33.6.1316; McShane E.J., 1934, B AM MATH SOC, V40, P837, DOI [10.1090/S0002-9904-1934-05978-0, DOI 10.1090/S0002-9904-1934-05978-0]; Moon T., 2010, P 3 ACM INT C WEB SE, P151, DOI DOI 10.1145/1718487.1718507; Narasimhan H., 2013, NIPS; Nemirovski A., 2004, LECURE NOTES INTERIO; Punera K., 2008, WWW; Renegar J., 2001, MATH VIEW INTERIOR P, DOI 10.1137/1.9780898718812; ROUNDY R, 1986, MATH OPER RES, V11, P699, DOI 10.1287/moor.11.4.699; Spielman D., 2003, C P ANN ACM S THEOR, DOI 10.1145/1007352.1007372; Stout Q. F, FASTEST ISOTONIC REG; Stout Q. F., 2015, CORR; Stout QF, 2015, ALGORITHMICA, V71, P450, DOI 10.1007/s00453-013-9814-z; Stout QF, 2013, ALGORITHMICA, V66, P93, DOI 10.1007/s00453-012-9628-4; Stout QF, 2012, J OPTIMIZ THEORY APP, V152, P121, DOI 10.1007/s10957-011-9865-8; Whitney H, 1934, T AM MATH SOC, V36, P63, DOI 10.2307/1989708; Zheng Z., 2008, COMM CONTR COMP ALL; [No title captured]	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101083
C	Lattimore, T; Crammer, K; Szepesvari, C		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lattimore, Tor; Crammer, Koby; Szepesvari, Csaba			Linear Multi-Resource Allocation with Semi-Bandit Feedback	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study an idealised sequential resource allocation problem. In each time step the learner chooses an allocation of several resource types between a number of tasks. Assigning more resources to a task increases the probability that it is completed. The problem is challenging because the alignment of the tasks to the resource types is unknown and the feedback is noisy. Our main contribution is the new setting and an algorithm with nearly-optimal regret analysis. Along the way we draw connections to the problem of minimising regret for stochastic linear bandits with heteroscedastic noise. We also present some new results for stochastic linear bandits on the hypercube that significantly improve on existing work, especially in the sparse case.	[Lattimore, Tor; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Crammer, Koby] Technion, Dept Elect Engn, Haifa, Israel	University of Alberta; Technion Israel Institute of Technology	Lattimore, T (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	tor.lattimore@gmail.com; koby@ee.technion.ac.il; szepesva@ualberta.ca						Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Abbasi-Yadkori Yasin, 2012, JMLR WORKSHOP C P, P1; Agrawal S., 2012, ARXIV12093352; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bennett K. P., 1993, Computational Optimization and Applications, V2, P207, DOI 10.1007/BF01299449; Bubeck S., 2012, FDN TRENDS MACHINE L; Dani V, 2008, P C LEARN THEOR COLT, P355; Krishnamurthy Akshay, 2015, ARXIV150205890; Kveton B., 2014, ARXIV14100949; Lattimore Tor, 2014, P 30 C UNC ART INT U; Petrik M, 2011, J MACH LEARN RES, V12, P3027; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Sowell Thomas, 1993, IS REALITY OPTIONAL	14	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102079
C	Lattimore, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lattimore, Tor			The Pareto Regret Frontier for Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Given a multi-armed bandit problem it may be desirable to achieve a smaller-than-usual worst-case regret for some special actions. I show that the price for such unbalanced worst-case regret guarantees is rather high. Specifically, if an algorithm enjoys a worst-case regret of B with respect to some action, then there must exist another action for which the worst-case regret is at least Omega(nK/B), where n is the horizon and K the number of actions. I also give upper bounds in both the stochastic and adversarial settings showing that this result cannot be improved. For the stochastic case the pareto regret frontier is characterised exactly up to constant factors.	[Lattimore, Tor] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada	University of Alberta	Lattimore, T (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	tor.lattimore@gmail.com						Agrawal Shipra, 2012, P INT C ART INT STAT; Agrawal Shipra, 2012, P C LEARN THEOR COLT; Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 1995, AN S FDN CO, P322, DOI 10.1109/SFCS.1995.492488; Bubeck S., 2012, FDN TRENDS MACHINE L; Cappe O, 2013, ANN STAT, V41, P1516, DOI 10.1214/13-AOS1119; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Even-Dar E, 2008, MACH LEARN, V72, P21, DOI 10.1007/s10994-008-5060-z; Hutter M, 2005, J MACH LEARN RES, V6, P639; Kapralov M., 2011, ADV NEURAL INFORM PR, V24, P828; Koolen W.M., 2013, ADV NEURAL INFORM PR, P863; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Lattimore T., 2015, TECHNICAL REPORT; Liu Che-Yu, 2015, ARXIV150603378; Sani A., 2014, ADV NEURAL INFORM PR, V27, P810; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285	17	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103070
C	Lee, JD; Sun, YK; Taylor, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lee, Jason D.; Sun, Yuekai; Taylor, Jonathan			Evaluating the statistical significance of biclusters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Biclustering (also known as submatrix localization) is a problem of high practical relevance in exploratory analysis of high-dimensional data. We develop a framework for performing statistical inference on biclusters found by score-based algorithms. Since the bicluster was selected in a data dependent manner by a biclustering or localization algorithm, this is a form of selective inference. Our framework gives exact (non-asymptotic) confidence intervals and p-values for the significance of the selected biclusters.	[Lee, Jason D.; Sun, Yuekai; Taylor, Jonathan] Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA	Stanford University	Lee, JD (corresponding author), Stanford Univ, Inst Computat & Math Engn, Stanford, CA 94305 USA.	jdl17@stanford.edu; yuekai@stanford.edu; jonathan.taylor@stanford.edu						Addario-Berry L, 2010, ANN STAT, V38, P3063, DOI 10.1214/10-AOS817; Ames B., 2012, MATH PROGRAM, P1; Ames BPW, 2014, MATH PROGRAM, V143, P299, DOI 10.1007/s10107-013-0733-1; Arias-Castro E, 2011, ANN STAT, V39, P278, DOI 10.1214/10-AOS839; BALAKRISHNAN S., 2011, NIPS 2011 WORKSHOP C; Bhamidi S., 2012, ARXIV12112284; Chen Y., 2014, ARXIV14021267; Cheng Y, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P93; Lazzeroni L, 2002, STAT SINICA, V12, P61; Lee J.D., 2013, ARXIV13116238; Ma Z., 2013, ARXIV13095914; Shabalin AA, 2009, ANN APPL STAT, V3, P985, DOI 10.1214/09-AOAS239	12	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101075
C	Lee, J; Choi, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lee, Juho; Choi, Seungjin			Tree-Guided MCMC Inference for Normalized Random Measure Mixture Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Normalized random measures (NRMs) provide a broad class of discrete random measures that are often used as priors for Bayesian nonparametric models. Dirichlet process is a well-known example of NRMs. Most of posterior inference methods for NRM mixture models rely on MCMC methods since they are easy to implement and their convergence is well studied. However, MCMC often suffers from slow convergence when the acceptance rate is low. Tree-based inference is an alternative deterministic posterior inference method, where Bayesian hierarchical clustering (BHC) or incremental Bayesian hierarchical clustering (IBHC) have been developed for DP or NRM mixture (NRMM) models, respectively. Although IBHC is a promising method for posterior inference for NRMM models due to its efficiency and applicability to online inference, its convergence is not guaranteed since it uses heuristics that simply selects the best solution after multiple trials are made. In this paper, we present a hybrid inference algorithm for NRMM models, which combines the merits of both MCMC and IBHC. Trees built by IBHC outlines partitions of data, which guides Metropolis-Hastings procedure to employ appropriate proposals. Inheriting the nature of MCMC, our tree-guided MCMC (tgMCMC) is guaranteed to converge, and enjoys the fast convergence thanks to the effective proposals guided by trees. Experiments on both synthetic and realworld datasets demonstrate the benefit of our method.	[Lee, Juho; Choi, Seungjin] Pohang Univ Sci & Technol, Dept Comp Sci & Engn, 77 Cheongam Ro, Pohang 37673, South Korea	Pohang University of Science & Technology (POSTECH)	Lee, J (corresponding author), Pohang Univ Sci & Technol, Dept Comp Sci & Engn, 77 Cheongam Ro, Pohang 37673, South Korea.	stonecold@postech.ac.kr; seungjin@postech.ac.kr	Lee, Juho/AAA-2901-2022		IT R&D Program of MSIP/IITP [B0101-15-0307]; National Research Foundation (NRF) of Korea [NRF-2013R1A2A2A01067464]; IITP-MSRA Creative ICT/SW Research Project	IT R&D Program of MSIP/IITP; National Research Foundation (NRF) of Korea(National Research Foundation of Korea); IITP-MSRA Creative ICT/SW Research Project	This work was supported by the IT R&D Program of MSIP/IITP (B0101-15-0307, Machine Learning Center), National Research Foundation (NRF) of Korea (NRF-2013R1A2A2A01067464), and IITP-MSRA Creative ICT/SW Research Project.	ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; Brix A, 1999, ADV APPL PROBAB, V31, P929, DOI 10.1017/S0001867800009538; Chen C., 2013, P INT C MACH LEARN I; Chen C., 2012, P INT C MACH LEARN I; Favaro S, 2013, STAT SCI, V28, P335, DOI 10.1214/13-STS422; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Griffin JE, 2011, J COMPUT GRAPH STAT, V20, P241, DOI 10.1198/jcgs.2010.08176; Heller K. A., 2005, P INT C MACH LEARN I; Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001; James LF, 2009, SCAND J STAT, V36, P76, DOI 10.1111/j.1467-9469.2008.00609.x; James LF, 2005, ANN STAT, V33, P1771, DOI 10.1214/009053605000000336; Lee J., 2014, P INT C ART INT STAT; Lijoi A, 2005, J AM STAT ASSOC, V100, P1278, DOI 10.1198/016214505000000132; Lijoi A, 2007, J R STAT SOC B, V69, P715, DOI 10.1111/j.1467-9868.2007.00609.x; Regazzini E, 2003, ANN STAT, V31, P560	15	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101060
C	Lee, K; Zlateski, A; Vishwanathan, A; Seung, HS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lee, Kisuk; Zlateski, Aleksandar; Vishwanathan, Ashwin; Seung, H. Sebastian			Recursive Training of 2D-3D Convolutional Networks for Neuronal Boundary Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ELECTRON-MICROSCOPY; RECONSTRUCTION; VOLUME	Efforts to automate the reconstruction of neural circuits from 3D electron microscopic (EM) brain images are critical for the field of connectomics. An important computation for reconstruction is the detection of neuronal boundaries. Images acquired by serial section EM, a leading 3D EM technique, are highly anisotropic, with inferior quality along the third dimension. For such images, the 2D maxpooling convolutional network has set the standard for performance at boundary detection. Here we achieve a substantial gain in accuracy through three innovations. Following the trend towards deeper networks for object recognition, we use a much deeper network than previously employed for boundary detection. Second, we incorporate 3D as well as 2D filters, to enable computations that use 3D context. Finally, we adopt a recursively trained architecture in which a first network generates a preliminary boundary map that is provided as input along with the original image to a second network that generates a final boundary map. Backpropagation training is accelerated by ZNN, a new implementation of 3D convolutional networks that uses multicore CPU parallelism for speed. Our hybrid 2D3D architecture could be more generally applicable to other types of anisotropic 3D images, including video, and our recursive framework for any image labeling problem.	[Lee, Kisuk; Zlateski, Aleksandar] MIT, Cambridge, MA 02139 USA; [Vishwanathan, Ashwin; Seung, H. Sebastian] Princeton Univ, Princeton, NJ 08544 USA	Massachusetts Institute of Technology (MIT); Princeton University	Lee, K (corresponding author), MIT, Cambridge, MA 02139 USA.	kisuklee@mit.edu; zlateski@mit.edu; ashwinv@princeton.edu; sseung@princeton.edu			Samsung Scholarship; Mathers Foundation; Keating Fund for Innovation; Simons Center for the Social Brain; DARPA [HR0011-14-2-0004]; ARO [W911NF-12-1-0594]	Samsung Scholarship(Samsung); Mathers Foundation; Keating Fund for Innovation; Simons Center for the Social Brain; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); ARO	We thank Juan C. Tapia, Gloria Choi and Dan Stettler for initial help with tissue handling and Jeff Lichtman and Richard Schalek with help in setting up tape collection. Kisuk Lee was supported by a Samsung Scholarship. The recursive approach proposed in this paper was partially motivated by Matthew J. Greene's preliminary experiments. We are grateful for funding from the Mathers Foundation, Keating Fund for Innovation, Simons Center for the Social Brain, DARPA (HR0011-14-2-0004), and ARO (W911NF-12-1-0594).	Ballas N, 2015, ARXIV PREPRINT ARXIV; Briggman KL, 2012, CURR OPIN NEUROBIOL, V22, P154, DOI 10.1016/j.conb.2011.10.022; Chen MG, 2014, NEURON, V82, P682, DOI 10.1016/j.neuron.2014.03.023; Ciresan D., 2012, NIPS; Eigen D., 2014, 6 INT C LEARN REPR I; Giusti A., 2013, ICIP; Hayworth KJ, 2014, FRONT NEURAL CIRCUIT, V8, DOI 10.3389/fncir.2014.00068; Helmstaedter M, 2013, NATURE, V500, P168, DOI 10.1038/nature12346; Helmstaedter M, 2013, NAT METHODS, V10, P501, DOI [10.1038/NMETH.2476, 10.1038/nmeth.2476]; Huang G. B., 2014, ICLR; Jain V, 2010, CURR OPIN NEUROBIOL, V20, P653, DOI 10.1016/j.conb.2010.07.004; Jurrus E, 2010, MED IMAGE ANAL, V14, P770, DOI 10.1016/j.media.2010.06.002; Kasthuri N, 2015, CELL, V162, P648, DOI 10.1016/j.cell.2015.06.054; Kim JS, 2014, NATURE, V509, P331, DOI 10.1038/nature13240; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Liu T, 2014, J NEUROSCI METH, V226, P88, DOI 10.1016/j.jneumeth.2014.01.022; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Masci J., 2013, ICIP; Mathieu M., 2014, ICLR; Pinheiro Pedro H. O., 2014, ICML; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Seyedhosseini M, 2013, IEEE T IMAGE PROCESS, V22, P4486, DOI 10.1109/TIP.2013.2274388; Simonyan Karen, 2015, INT C LEARN REPR; Takemura S, 2013, NATURE, V500, P175, DOI 10.1038/nature12450; Tapia JC, 2012, NAT PROTOC, V7, P193, DOI 10.1038/nprot.2011.439; Tasdizen T., 2014, COMPUTATIONAL INTELL, P237; Tran D., 2014, ARXIV; Tu Z., 2008, CVPR; Turaga S.C., 2009, NIPS; Turaga SC, 2010, NEURAL COMPUT, V22, P511, DOI 10.1162/neco.2009.10-08-881; Unnikrishnan R, 2007, IEEE T PATTERN ANAL, V29, P929, DOI 10.1109/TPAMI.2007.1046; Vasilache N, 2015, ICLR; Zlateski A., 2015, ARXIV151006706; Zlateski A., 2015, ARXIV150500249	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100004
C	Lee, M; Bindel, D; Mimno, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lee, Moontae; Bindel, David; Mimno, David			Robust Spectral Inference for Joint Stochastic Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Spectral inference provides fast algorithms and provable optimality for latent topic analysis. But for real data these algorithms require additional ad-hoc heuristics, and even then often produce unusable results. We explain this poor performance by casting the problem of topic inference in the framework of Joint Stochastic Matrix Factorization (JSMF) and showing that previous methods violate the theoretical conditions necessary for a good solution to exist. We then propose a novel rectification method that learns high quality topics and their interactions even on small, noisy data. This method achieves results comparable to probabilistic techniques in several domains while maintaining scalability and provable optimality.	[Lee, Moontae; Bindel, David] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA; [Mimno, David] Cornell Univ, Dept Informat Sci, Ithaca, NY 14850 USA	Cornell University; Cornell University	Lee, M (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.	moontae@cs.cornell.edu; bindel@cs.cornell.edu; mimno@cornell.edu			NSF [HCC: Large-0910664]	NSF(National Science Foundation (NSF))	This research is supported by NSF grant HCC: Large-0910664. We thank Adrian Lewis for valuable discussions on AP convergence.	Anandkumar Anima, 2012, ADV NEURAL INFORM PR, V25, P926; [Anonymous], [No title captured]; [Anonymous], P 19 ACM INT C INF K, DOI DOI 10.1145/1871437.1871729; Arora S., 2012, FOCS; Arora S., 2013, ICML; Blei DM, 2007, ANN APPL STAT, V1, P17, DOI 10.1214/07-AOAS114; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boyle J. P., 1986, ADV ORDER RESTRICTED, P28, DOI DOI 10.1007/978-1-4613-9940-7_3; Broadbent ME., 2010, SIAM UNDERGRAD RES O, V3, P50, DOI [10.1137/09S010435, DOI 10.1137/09S010435]; Chen S., 2012, KDD, P714, DOI [10.1145/2339530.2339643, DOI 10.1145/2339530.2339643]; Daniilidis A, 2008, J CONVEX ANAL, V15, P547; Goldberg Y., 2014, NIPS; Gomez C, 2007, INT J REMOTE SENS, V28, P5315, DOI 10.1080/01431160701227679; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Kumari AC, 2012, 2012 NATL C COMM NCC, P1, DOI [10.3109/07388551.2012.716810, DOI 10.3109/07388551.2012.716810)]; Lewis AS, 2009, FOUND COMPUT MATH, V9, P485, DOI 10.1007/s10208-008-9036-y; Mimno D., 2014, P 2014 C EMPIRICAL M, P1319, DOI DOI 10.3115/V1/D14-1138; Mimno D., 2011, EMNLP; Mislove A., 2010, P WSDM, DOI DOI 10.1145/1718487.1718519; Nascimento JMP, 2005, IEEE T GEOSCI REMOTE, V43, P898, DOI 10.1109/TGRS.2005.844293; Nguyen Thang, 2014, ASS COMPUTATIONAL LI; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Pennington Jeffrey, 2014, EMNLP; Zhou TY, 2014, ADV NEUR IN, V27	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103008
C	Li, S; Xie, Y; Dai, HJ; Song, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Shuang; Xie, Yao; Dai, Hanjun; Song, Le			M-Statistic for Kernel Change-Point Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Detecting the emergence of an abrupt change-point is a classic problem in statistics and machine learning. Kernel-based nonparametric statistics have been proposed for this task which make fewer assumptions on the distributions than traditional parametric approach. However, none of the existing kernel statistics has provided a computationally efficient way to characterize the extremal behavior of the statistic. Such characterization is crucial for setting the detection threshold, to control the significance level in the offline case as well as the average run length in the online case. In this paper we propose two related computationally efficient M-statistics for kernel-based change-point detection when the amount of background data is large. A novel theoretical result of the paper is the characterization of the tail probability of these statistics using a new technique based on change-ofmeasure. Such characterization provides us accurate detection thresholds for both offline and online cases in computationally efficient manner, without the need to resort to the more expensive simulations such as bootstrapping. We show that our methods perform well in both synthetic and real world data.	[Li, Shuang; Xie, Yao] Georgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA; [Dai, Hanjun; Song, Le] Georgia Inst Technol, Coll Comp, Computat Sci & Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Li, S (corresponding author), Georgia Inst Technol, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA 30332 USA.	sli370@gatech.edu; yao.xie@isye.gatech.edu; hanjundai@gatech.edu; lsong@cc.gatech.edu	Dai, Hanjun/AAQ-8943-2021		NSF/NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSF [IIS-1218749]; NSF CAREER [IIS-1350983];  [CMMI-1538746];  [CCF-1442635]	NSF/NIH BIGDATA; ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ; 	This research was supported in part by CMMI-1538746 and CCF-1442635 to Y.X.; NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, NSF CAREER IIS-1350983 to L.S..	Desobry F., 2005, IEEE T SIG P; Enikeeva F., 2014, ARXIV13121900; Gretton A, 2012, J MACH LEARN RES, V13, P723; Harchaoui Z., 2008, ADV NEURAL INFORM PR; Harchaoui Z, 2013, IEEE SIGNAL PROC MAG, V30, P87, DOI 10.1109/MSP.2013.2253631; KIFER D, 2004, P 30 VLDB C; Liu S, 2013, NEURAL NETWORKS, V43, P72, DOI 10.1016/j.neunet.2013.01.012; Ramdas A., 2015, 29 AAAI C ART INT; Ross Z. E., 2014, GEOPHYS J INT; Scholkopf B., 2001, LEARNING KERNELS SUP; Serfling R. J., 1980, U STAT; SIEGMUND D, 1995, ANN STAT, V23, P255, DOI 10.1214/aos/1176324466; Siegmund D, 2008, STAT INTERFACE, V1, P3; Siegmund DO, 1985, SEQUENTIAL ANAL TEST; Xie Y, 2013, ANN STAT, V41, P670, DOI 10.1214/13-AOS1094; Yakir B, 2013, WILEY SER PROBAB ST, P1, DOI 10.1002/9781118720608; Zaremba W., 2013, ADV NEURAL INFO PROC; Zou Shaofeng, 2014, ARXIV14052294	18	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100052
C	Li, YZ; Hernandez-Lobato, JM; Turner, RE		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Yingzhen; Hernandez-Lobato, Jose Miguel; Turner, Richard E.			Stochastic Expectation Propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Expectation propagation (EP) is a deterministic approximation algorithm that is often used to perform approximate Bayesian parameter learning. EP approximates the full intractable posterior distribution through a set of local approximations that are iteratively refined for each datapoint. EP can offer analytic and computational advantages over other approximations, such as Variational Inference (VI), and is the method of choice for a number of models. The local nature of EP appears to make it an ideal candidate for performing Bayesian learning on large models in large-scale dataset settings. However, EP has a crucial limitation in this context: the number of approximating factors needs to increase with the number of data-points, N, which often entails a prohibitively large memory overhead. This paper presents an extension to EP, called stochastic expectation propagation (SEP), that maintains a global posterior approximation (like VI) but updates it in a local way (like EP). Experiments on a number of canonical learning problems using synthetic and real-world datasets indicate that SEP performs almost as well as full EP, but reduces the memory consumption by a factor of N. SEP is therefore ideally suited to performing approximate Bayesian learning in the large model, large dataset setting.	[Li, Yingzhen; Turner, Richard E.] Univ Cambridge, Cambridge CB2 1PZ, England; [Hernandez-Lobato, Jose Miguel] Harvard Univ, Cambridge, MA 02138 USA	University of Cambridge; Harvard University	Li, YZ (corresponding author), Univ Cambridge, Cambridge CB2 1PZ, England.	yl494@cam.ac.uk; jmh@seas.harvard.edu; ret26@cam.ac.uk			Schlumberger Foundation Faculty; Rafael del Pino Foundation; EPSRC [EP/G050821/1, EP/L000776/1]	Schlumberger Foundation Faculty(Schlumberger); Rafael del Pino Foundation; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank the reviewers for valuable comments. YL thanks the Schlumberger Foundation Faculty for the Future fellowship on supporting her PhD study. JMHL acknowledges support from the Rafael del Pino Foundation. RET thanks EPSRC grant #EP/G050821/1 and EP/L000776/1.	Ahn S, 2014, PR MACH LEARN RES, V32, P1044; Amari S., 2000, METHODS INFORM GEOME, V191; [Anonymous], ARXIV150205336; Bardenet R, 2014, PR MACH LEARN RES, V32; Barthelme S, 2014, J AM STAT ASSOC, V109, P315, DOI 10.1080/01621459.2013.864178; Beal M.J, 2003, THESIS; Cunningham, 2011, ARXIV11116832; Dehaene, 2015, ARXIV150308060; Gelman Andrew, 2014, ARXIV PREPRINT ARXIV, V157; Herbrich R., 2006, ADV NEURAL INFORM PR, P569; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kuss M, 2005, J MACH LEARN RES, V6, P1679; Maybeck P. S., 1982, STOCHASTIC MODELS ES; MINKA T, 2004, MSRTR2004149; Minka T., 2005, DIVERGENCE MEASURES; Minka T.P., 2001, P 17 C UNC ART INT, P362; Opper M, 2005, J MACH LEARN RES, V6, P2177; Qi Yuan, 2010, UNCERTAINTY ARTIFICI; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Turner R., 2011, ADV NEURAL INF PROCE, P981; Turner R. E., 2011, BAYESIAN TIME SERIES, P109; Winn J, 2005, J MACH LEARN RES, V6, P661; Xu Minjie, 2014, NIPS	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101020
C	Lim, ZW; Hsu, D; Lee, WS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lim, Zhan Wei; Hsu, David; Lee, Wee Sun			Adaptive Stochastic Optimization: From Sets to Paths	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Adaptive stochastic optimization (ASO) optimizes an objective function adaptively under uncertainty. It plays a crucial role in planning and learning under uncertainty, but is, unfortunately, computationally intractable in general. This paper introduces two conditions on the objective function, the marginal likelihood rate bound and the marginal likelihood bound, which, together with pointwise submodularity, enable efficient approximate solution of ASO. Several interesting classes of functions satisfy these conditions naturally, e.g., the version space reduction function for hypothesis learning. We describe Recursive Adaptive Coverage, a new ASO algorithm that exploits these conditions, and apply the algorithm to two robot planning tasks under uncertainty. In contrast to the earlier submodular optimization approach, our algorithm applies to ASO over both sets and paths.	[Lim, Zhan Wei; Hsu, David; Lee, Wee Sun] Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore	National University of Singapore	Lim, ZW (corresponding author), Natl Univ Singapore, Dept Comp Sci, Singapore, Singapore.	limzhanw@comp.nus.edu.sg; dyhsu@comp.nus.edu.sg; leews@comp.nus.edu.sg			NUS AcRF [R-252-000-587-112]; National Research Foundation Singapore through the SMART Phase 2 Pilot Program [09]; US Air Force Research Laboratory [FA2386-15-1-4010]	NUS AcRF; National Research Foundation Singapore through the SMART Phase 2 Pilot Program; US Air Force Research Laboratory(United States Department of DefenseUS Air Force Research Laboratory)	This work is supported in part by NUS AcRF grant R-252-000-587-112, National Research Foundation Singapore through the SMART Phase 2 Pilot Program (Subaward Agreement No. 09), and US Air Force Research Laboratory under agreement number FA2386-15-1-4010.	Asadpour A, 2008, LECT NOTES COMPUT SC, V5385, P477, DOI 10.1007/978-3-540-92185-1_53; Calinescu G, 2005, J COMB OPTIM, V9, P281, DOI 10.1007/s10878-005-1412-9; Cuong Nguyen Viet, 2013, ADV NEURAL INFORM PR; Cuong Nguyen Viet, 2014, P UNC ART INT; Golovin D, 2010, ARON CULOTTA ADV NEU, P766; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Guillory Andrew, 2010, INT C MACH LEARN ICM; Gupta A, 2010, LECT NOTES COMPUT SC, V6198, P690, DOI 10.1007/978-3-642-14165-2_58; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Lim Zhan Wei, 2014, WORKSP ALG FDN ROB; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Ong SCW, 2010, INT J ROBOT RES, V29, P1053, DOI 10.1177/0278364910369861; Silver D., 2010, ADV NEURAL INFORM PR; Somani A., 2013, ADV NEURAL INFORM PR, P1772; Zheng Alice X., 2005, P UNC ART INT	15	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103043
C	Lin, GS; Shen, CH; Reid, I; van den Hengel, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lin, Guosheng; Shen, Chunhua; Reid, Ian; van den Hengel, Anton			Deeply Learning the Messages in Message Passing Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep structured output learning shows great promise in tasks like semantic image segmentation. We proffer a new, efficient deep structured model learning scheme, in which we show how deep Convolutional Neural Networks (CNNs) can be used to directly estimate the messages in message passing inference for structured prediction with Conditional Random Fields (CRFs). With such CNN message estimators, we obviate the need to learn or evaluate potential functions for message calculation. This confers significant efficiency for learning, since otherwise when performing structured learning for a CRF with CNN potentials it is necessary to undertake expensive inference for every stochastic gradient iteration. The network output dimension of message estimators is the same as the number of classes, rather than exponentially growing in the order of the potentials. Hence it is more scalable for cases that involve a large number of classes. We apply our method to semantic image segmentation and achieve impressive performance, which demonstrates the effectiveness and usefulness of our CNN message learning method.	[Lin, Guosheng] Univ Adelaide, Adelaide, SA, Australia; Australian Ctr Robot Vis, Brisbane, Qld, Australia	University of Adelaide; Australian Centre for Robotic Vision	Lin, GS (corresponding author), Univ Adelaide, Adelaide, SA, Australia.	guosheng.lin@adelaide.edu.au; chunhua.shen@adelaide.edu.au; ian.reid@adelaide.edu.au; anton.vandenhengel@adelaide.edu.au	Lin, Guosheng/N-9110-2019	Lin, Guosheng/0000-0002-0329-7458	Data to Decisions Cooperative Research Centre; Australian Research Council through the ARC Centre for Robotic Vision [CE140100016]; Australian Research Council through a Laureate Fellowship [FL130100102]	Data to Decisions Cooperative Research Centre; Australian Research Council through the ARC Centre for Robotic Vision(Australian Research Council); Australian Research Council through a Laureate Fellowship(Australian Research Council)	This research was supported by the Data to Decisions Cooperative Research Centre and by the Australian Research Council through the ARC Centre for Robotic Vision CE140100016 and through a Laureate Fellowship FL130100102 to I. Reid. Correspondence should be addressed to C. Shen.	Besag Julian, 1977, BIOMETRIKA; Chen L., 2014, LEARNING DEEP STRUCT; Chen L.C., 2014, SEMANTIC IMAGE SEGME, V4, P357; Dai J., 2015, BOXSUP EXPLOITING BO; Ess A., 2010, INT J COMP VIS; Hariharan B., 2011, P INT C COMP VIS; Hariharan B., 2014, P EUR C COMP VIS; Kolmogorov V., 2006, IEEE T PATTERN ANAL; Krahenbuhl P., 2012, P ADV NEUR INF PROC; Lin G., 2015, EFFICIENT PIECEWISE; Liu F., 2015, P IEEE C COMP VIS PA; Liu F., 2015, LEARNING DEPTH SINGL; Long J., 2015, P IEEE C COMP VIS PA, P3431, DOI 10.1109/CVPR.2015.7298965; Mostajabi M., 2014, FEEDFORWARD SEMANTIC; Noh H., 2015, P IEEE C COMP VIS PA; Nowozin S., 2011, FDN TRENDS COMPUT GR; Papandreou G., 2015, WEAKLY AND SEMISUPER; Ross S, 2011, PROC CVPR IEEE; Schwing A. G., 2015, FULLY CONNECTED DEEP; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sutton C. A., 2005, P C UNC ART INT; Tompson J, 2014, ADV NEUR IN, V27; Vedaldi A., 2015, P ACM INT C MULT; Yedidia J. S., 2000, P ADV NEUR INF PROC; Zheng Shuai, 2015, CONDITIONAL RANDOM F	25	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101051
C	Ma, TY; Wigderson, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ma, Tengyu; Wigderson, Avi			Sum-of-Squares Lower Bounds for Sparse PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PRINCIPAL-COMPONENTS; SEMIDEFINITE RELAXATIONS; POSITIVSTELLENSATZ; OPTIMIZATION; POLYNOMIALS; COMPLEXITY; MATRICES; PROOFS	This paper establishes a statistical versus computational trade-off for solving a basic high-dimensional machine learning problem via a basic convex relaxation method. Specifically, we consider the Sparse Principal Component Analysis (Sparse PCA) problem, and the family of Sum-of-Squares (SoS, aka Lasserre/Parillo) convex relaxations. It was well known that in large dimension p, a planted k-sparse unit vector can be in principle detected using only n approximate to k log p (Gaussian or Bernoulli) samples, but all efficient (polynomial time) algorithms known require n approximate to k(2) samples. It was also known that this quadratic gap cannot be improved by the the most basic semi-definite (SDP, aka spectral) relaxation, equivalent to a degree-2 SoS algorithms. Here we prove that also degree-4 SoS algorithms cannot improve this quadratic gap. This average-case lower bound adds to the small collection of hardness results in machine learning for this powerful family of convex relaxation algorithms. Moreover, our design of moments (or "pseudo-expectations") for this lower bound is quite different than previous lower bounds. Establishing lower bounds for higher degree SoS algorithms for remains a challenging problem.	[Ma, Tengyu] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA; [Wigderson, Avi] Inst Adv Study, Sch Math, Princeton, NJ USA	Princeton University; Institute for Advanced Study - USA	Ma, TY (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.				Simons Award for Graduate Students in Theoretical Computer Science; NSF [CCF-1412958]	Simons Award for Graduate Students in Theoretical Computer Science; NSF(National Science Foundation (NSF))	Supported in part by Simons Award for Graduate Students in Theoretical Computer Science; Supported in part by NSF grant CCF-1412958	Alon U, 1999, P NATL ACAD SCI USA, V96, P6745, DOI 10.1073/pnas.96.12.6745; Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Artin E., 1927, HAMB ABH, V5, P100; Barak B., 2015, ABS150106521 CORR; Barak B., 2015, P 47 ANN ACM S THEOR; Barak B, 2014, PROCEEDINGS OF THE INTERNATIONAL CONGRESS OF MATHEMATICIANS (ICM 2014), VOL IV, P509; Barak B, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P31, DOI 10.1145/2591796.2591886; Berthet Q., 2013, C LEARN THEOR, P1046; Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127; Chandrasekaran V, 2013, P NATL ACAD SCI USA, V110, pE1181, DOI 10.1073/pnas.1302293110; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Daniely A., 2013, ADV NEURAL INFORM PR, P145; Decatur S., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P130, DOI 10.1145/267460.267489; Deshpande Y., 2015, ARXIV E PRINTS; DONOHO DL, 1995, IEEE T INFORM THEORY, V41, P613, DOI 10.1109/18.382009; Donoho DL, 1998, ANN STAT, V26, P879; Gao C., 2014, ARXIV E PRINTS; Grigoriev D, 2001, COMPUT COMPLEX, V10, P139, DOI 10.1007/s00037-001-8192-0; Grigoriev D, 2001, THEOR COMPUT SCI, V259, P613, DOI 10.1016/S0304-3975(00)00157-2; Hopkins Samuel B., 2015, ABS150705230 CORR; Jenatton Rodolphe, 2010, P 13 INT C ART INT S, P366; Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Johnstone IM, 2002, UNPUB; Krivine Jean-Louis, 1964, J ANAL MATH; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Lasserre JB, 2015, CAMBRIDGE TEXTS APPL; Laurent M, 2009, IMA VOL MATH APPL, V149, P157; Lovasz L, 1991, SIAM J OPTIMIZ, V1, P166, DOI 10.1137/0801013; Ma Tengyu, 2014, COMMUNICATION; Ma ZM, 2013, ANN STAT, V41, P772, DOI 10.1214/13-AOS1097; Meka Raghu, 2015, ABS150306447 CORR; Nesterov Y., 2000, HIGH PERFORMANCE OPT, V33, P405, DOI [DOI 10.1007/978-1-4757-3216-0_17, 10.1007/978-1-4757-3216-017, DOI 10.1007/978-1-4757-3216-017]; Parrilo PA., 2000, THESIS CALTECH; Paul D., 2012, ARXIV12021242; PUTINAR M, 1993, INDIANA U MATH J, V42, P969, DOI 10.1512/iumj.1993.42.42045; Raghavendra Prasad, 2015, ABS150705136 CORR; SCHMUDGEN K, 1991, MATH ANN, V289, P203, DOI 10.1007/BF01446568; Schoenebeck G, 2008, ANN IEEE SYMP FOUND, P593, DOI 10.1109/FOCS.2008.74; Servedio RA, 2000, J COMPUT SYST SCI, V60, P161, DOI 10.1006/jcss.1999.1666; SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036; SHOR NZ, 1987, CYBERNETICS+, V23, P695, DOI 10.1007/BF01074929; STENGLE G, 1974, MATH ANN, V207, P87, DOI 10.1007/BF01362149; Vu V., 2012, INT C ARTIFICIAL INT, P1278; Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151; Wang Z., 2015, ARXIV E PRINTS; Xi Chen, 2011, STAT APPL GENET MOL, P10	49	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100092
C	Mahsereci, M; Hennig, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mahsereci, Maren; Hennig, Philipp			Probabilistic Line Searches for Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MINIMIZATION	In deterministic optimization, line searches are a standard tool ensuring stability and efficiency. Where only stochastic gradients are available, no direct equivalent has so far been formulated, because uncertain gradients do not allow for a strict sequence of decisions collapsing the search space. We construct a probabilistic line search by combining the structure of existing deterministic methods with notions from Bayesian optimization. Our method retains a Gaussian process surrogate of the univariate optimization objective, and uses a probabilistic belief over the Wolfe conditions to monitor the descent. The algorithm has very low computational cost, and no user-controlled parameters. Experiments show that it effectively removes the need to define a learning rate for stochastic gradient descent.	[Mahsereci, Maren; Hennig, Philipp] Max Planck Inst Intelligent Syst, Spemannstr 38, D-72076 Tubingen, Germany	Max Planck Society	Mahsereci, M (corresponding author), Max Planck Inst Intelligent Syst, Spemannstr 38, D-72076 Tubingen, Germany.	mmahsereci@tue.mpg.de; phennig@tue.mpg.de						Adler R. J., 1981, GEOMETRY RANDOM FIEL; Amari S, 2000, NEURAL COMPUT, V12, P1399, DOI 10.1162/089976600300015420; [Anonymous], 1999, NUMERICAL OPTIMIZATI; ARMIJO L, 1966, PAC J MATH, V16, P1, DOI 10.2140/pjm.1966.16.1; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; BROYDEN CG, 1969, NOT AM MATH SOC, V16, P670; Drezner Z., 1990, J STAT COMPUT SIM, V35, P101, DOI [10.1080/00949659008811236, DOI 10.1080/00949659008811236]; Duchi J, 2011, J MACH LEARN RES, V12, P2121; FLETCHER R, 1964, COMPUT J, V7, P149, DOI 10.1093/comjnl/7.2.149; FLETCHER R, 1970, COMPUT J, V13, P317, DOI 10.1093/comjnl/13.3.317; George AP, 2006, MACH LEARN, V65, P167, DOI 10.1007/S10994-006-8365-9; GOLDFARB D, 1970, MATH COMPUT, V24, P23, DOI 10.2307/2004873; Hennig P., 2013, 30 INT C MACH LEARN; Hensman J., 2012, ADV NEURAL INFORM PR, P2888; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Le Roux N., 2010, P 27 INT C MACH LEAR, P623; Papoulis A., 1991, COMMUNICATIONS SIGNA, V3; Ranganath R., 2013, P 30 INT C MACH LEAR, V28, P298; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Sarkka S., 2013, BAYESIAN FILTERING S; Schaul T., 2013, INT C MACHINE LEARNI, P343; Schraudolph NN, 1999, IEE CONF PUBL, P569, DOI 10.1049/cp:19991170; SHANNO DF, 1970, MATH COMPUT, V24, P647, DOI 10.2307/2004840; Wahba G., 1990, CBMS NSF REGIONAL C; WOLFE P, 1969, SIAM REV, V11, P226, DOI 10.1137/1011036; Zhang T., 2004, 21 INT C MACH LEARN	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101013
