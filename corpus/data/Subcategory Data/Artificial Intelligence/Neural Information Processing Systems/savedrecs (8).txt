PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Advani, M; Ganguli, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Advani, Madhu; Ganguli, Surya			An equivalence between high dimensional Bayes optimal inference and M-estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SELECTION	When recovering an unknown signal from noisy measurements, the computational difficulty of performing optimal Bayesian MMSE (minimum mean squared error) inference often necessitates the use of maximum a posteriori (MAP) inference, a special case of regularized M-estimation, as a surrogate. However, MAP is suboptimal in high dimensions, when the number of unknown signal components is similar to the number of measurements. In this work we demonstrate, when the signal distribution and the likelihood function associated with the noise are both log-concave, that optimal MMSE performance is asymptotically achievable via another M-estimation procedure. This procedure involves minimizing convex loss and regularizer functions that are nonlinearly smoothed versions of the widely applied MAP optimization problem. Our findings provide a new heuristic derivation and interpretation for recent optimal M-estimators found in the setting of linear measurements and additive noise, and further extend these results to nonlinear measurements with non-additive noise. We numerically demonstrate superior performance of our optimal M-estimators relative to MAP. Overall, at the heart of our work is the revelation of a remarkable equivalence between two seemingly very different computational problems: namely that of high dimensional Bayesian integration underlying MMSE inference, and high dimensional convex optimization underlying M-estimation. In essence we show that the former difficult integral may be computed by solving the latter, simpler optimization problem.	[Advani, Madhu; Ganguli, Surya] Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA	Stanford University	Advani, M (corresponding author), Stanford Univ, Dept Appl Phys, Stanford, CA 94305 USA.	msadvani@stanford.edu; sganguli@stanford.edu			Stanford MBC; SGF; Burroughs Wellcome foundation; Simons foundation; Sloan foundation; McKnight foundation; McDonnell foundation; Office of Naval Research	Stanford MBC; SGF; Burroughs Wellcome foundation(Burroughs Wellcome Fund); Simons foundation; Sloan foundation(Alfred P. Sloan Foundation); McKnight foundation; McDonnell foundation; Office of Naval Research(Office of Naval Research)	The authors would like to thank Lenka Zdeborova and Stephen Boyd for useful discussions and also Chris Stock and Ben Poole for comments on the manuscript. M.A. thanks the Stanford MBC and SGF for support. S.G. thanks the Burroughs Wellcome, Simons, Sloan, McKnight, and McDonnell foundations, and the Office of Naval Research for support.	Bayati M, 2015, ANN APPL PROBAB, V25, P753, DOI 10.1214/14-AAP1010; Bayati M, 2011, IEEE T INFORM THEORY, V57, P764, DOI 10.1109/TIT.2010.2094817; Bruckstein AM, 2009, SIAM REV, V51, P34, DOI 10.1137/060657704; Candes EJ, 2008, IEEE SIGNAL PROC MAG, V25, P21, DOI 10.1109/MSP.2007.914731; DONOHO D., 2016, PROBAB THEORY REL, V166, P935, DOI [10.1007/s00440-015-0675-z, DOI 10.1007/S00440-015-0675-Z]; Guo D, 2007, INF THEOR 2007 ISIT; Huber PJ, 2009, WILEY SERIES PROBABI; Javanmard A., 2013, INFORM INFERENCE; Kabashima Y, 2016, IEEE T INFORM THEORY, V62, P4228, DOI 10.1109/TIT.2016.2556702; Krzakala F, 2012, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2012/08/P08009; PARIKH N., 2013, FDN TRENDS OPTIM, V1, P123, DOI DOI 10.1561/2400000003; Rangan S., 2011, Proceedings of the 2011 IEEE International Symposium on Information Theory - ISIT, P2168, DOI 10.1109/ISIT.2011.6033942; Rangan S, 2010, INF SCI SYST CISS 20; Thrampoulidis C, 2015, ANN ALLERTON CONF, P410, DOI 10.1109/ALLERTON.2015.7447033; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang C.-C., 2006, P 44 ANN ALL C COMM, P926; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705013
C	Ali, A; Kolter, JZ; Tibshirani, RJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ali, Alnur; Kolter, J. Zico; Tibshirani, Ryan J.			The Multiple Quantile Graphical Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ADDITIVE-MODELS; SELECTION; NETWORKS; INFERENCE	We introduce the Multiple Quantile Graphical Model (MQGM), which extends the neighborhood selection approach of Meinshausen and Buhlmann for learning sparse graphical models. The latter is defined by the basic subproblem of modeling the conditional mean of one variable as a sparse function of all others. Our approach models a set of conditional quantiles of one variable as a sparse function of all others, and hence offers a much richer, more expressive class of conditional distribution estimates. We establish that, under suitable regularity conditions, the MQGM identifies the exact conditional independencies with probability tending to one as the problem size grows, even outside of the usual homoskedastic Gaussian data model. We develop an efficient algorithm for fitting the MQGM using the alternating direction method of multipliers. We also describe a strategy for sampling from the joint distribution that underlies the MQGM estimate. Lastly, we present detailed experiments that demonstrate the flexibility and effectiveness of the MQGM in modeling hetereoskedastic non-Gaussian data.	[Ali, Alnur] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Kolter, J. Zico] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Tibshirani, Ryan J.] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University	Ali, A (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	alnurali@cmu.edu; zkolter@cs.cmu.edu; ryantibs@cmu.edu			DOE [DE-FG02-97ER25308]; NSF Expeditions in Computation Award, CompSustNet [CCF-1522054]; NSF [DMS-1309174, DMS-1554123]	DOE(United States Department of Energy (DOE)); NSF Expeditions in Computation Award, CompSustNet; NSF(National Science Foundation (NSF))	AA was supported by DOE Computational Science Graduate Fellowship DE-FG02-97ER25308. JZK was supported by an NSF Expeditions in Computation Award, CompSustNet, CCF-1522054. RJT was supported by NSF Grants DMS-1309174 and DMS-1554123.	Ali Alnur, 2016, TECHNICAL REPORT; Banerjee O, 2008, J MACH LEARN RES, V9, P485; BESAG J, 1974, J ROY STAT SOC B MET, V36, P192; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Centers for Disease Control and Prevention (CDC), 2015, INFL NAT REG LEV GRA; Chen SZ, 2015, BIOMETRIKA, V102, P47, DOI 10.1093/biomet/asu051; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Fan JQ, 2014, ANN STAT, V42, P324, DOI 10.1214/13-AOS1191; Finegold M, 2011, ANN APPL STAT, V5, P1057, DOI 10.1214/10-AOAS410; Friedman J., 2010, TECHNICAL REPORT; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Heckerman D, 2001, J MACH LEARN RES, V1, P49, DOI 10.1162/153244301753344614; Hofling H, 2009, J MACH LEARN RES, V10, P883; Johnson NA, 2013, J COMPUT GRAPH STAT, V22, P246, DOI 10.1080/10618600.2012.681238; Kato Kengo, 2011, TECHNICAL REPORT; Khare K, 2015, J R STAT SOC B, V77, P803, DOI 10.1111/rssb.12088; KOENKER R, 1994, BIOMETRIKA, V81, P673; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Koenker R., 2005, QUANTILE REGRESSION, DOI DOI 10.1017/CBO9780511754098; Koenker R, 2011, BRAZ J PROBAB STAT, V25, P239, DOI 10.1214/10-BJPS131; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Lee J., 2013, J MACH LEARN RES, V31, P388; LIU H., 2012, TECHNICAL REPORT; Liu H, 2012, ANN STAT, V40, P2293, DOI 10.1214/12-AOS1037; Liu H, 2009, J MACH LEARN RES, V10, P2295; Meier L, 2009, ANN STAT, V37, P3779, DOI 10.1214/09-AOS692; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Neville J, 2004, FOURTH IEEE INTERNATIONAL CONFERENCE ON DATA MINING, PROCEEDINGS, P170, DOI 10.1109/ICDM.2004.10101; O'Donoghue Brendan, 2013, TECHNICAL REPORT; Oh S., 2014, NIPS, V27, P667; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Peng J, 2009, J AM STAT ASSOC, V104, P735, DOI 10.1198/jasa.2009.0126; Raskutti G, 2012, J MACH LEARN RES, V13, P389; Rocha G. V., 2008, TECHNICAL REPORT; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; Sohn K.-A., 2012, P 15 INT C ARTIFICIA, P1081; Takeuchi I, 2006, J MACH LEARN RES, V7, P1231; Varin C, 2005, BIOMETRIKA, V92, P519, DOI 10.1093/biomet/92.3.519; Voorman A, 2014, BIOMETRIKA, V101, P85, DOI 10.1093/biomet/ast053; Wang YJ, 2008, BIOMETRIKA, V95, P735, DOI 10.1093/biomet/asn029; Wytock M, 2013, P 30 INT C MACH LEAR, P1265; Yang E., 2012, ADV NEURAL INFORM PR, P1358; Yang EH, 2015, J MACH LEARN RES, V16, P3813; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Yuan XT, 2014, IEEE T INFORM THEORY, V60, P1673, DOI 10.1109/TIT.2013.2296784	45	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700056
C	Alvarez, JM; Salzmann, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Alvarez, Jose M.; Salzmann, Mathieu			Learning the Number of Neurons in Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS	Nowadays, the number of layers and of neurons in each layer of a deep network are typically set manually. While very deep and wide networks have proven effective in general, they come at a high memory and computation cost, thus making them impractical for constrained platforms. These networks, however, are known to have many redundant parameters, and could thus, in principle, be replaced by more compact architectures. In this paper, we introduce an approach to automatically determining the number of neurons in each layer of a deep network during learning. To this end, we propose to make use of a group sparsity regularizer on the parameters of the network, where each group is defined to act on a single neuron. Starting from an overcomplete network, we show that our approach can reduce the number of parameters by up to 80% while retaining or even improving the network accuracy.	[Alvarez, Jose M.] Data61 CSIRO, Canberra, ACT 2601, Australia; [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, CH-1015 Lausanne, Switzerland	Commonwealth Scientific & Industrial Research Organisation (CSIRO); Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Alvarez, JM (corresponding author), Data61 CSIRO, Canberra, ACT 2601, Australia.	jose.alvarez@data61.csiro.au; mathieu.salzmann@epfl.ch						Alvarez J. M., 2016, CORR; [Anonymous], 2014, NIPS P; Ash T., 1989, Connection Science, V1, P365, DOI 10.1080/09540098908915647; Bartlett Peter L., 1996, NIPS; BELLO MG, 1992, IEEE T NEURAL NETWOR, V3, P864, DOI 10.1109/72.165589; Cheng Yu, 2015, ICCV; Collins M. D., 2014, CORR; Collobert R., 2011, BIGLEARN NIPS WORKSH; Denil Misha, 2013, ADV NEURAL INFORM PR; Denton E., 2014, NIPS; Gong Y., 2014, CORR; Greff K., 2015, NIPS; Hassibi B., 1993, ICNN; He K., 2015, CORR; Jaderberg M., 2014, ECCV; Jaderberg M., 2014, BMVC; Ji CY, 1990, NEURAL COMPUT, V2, P188, DOI 10.1162/neco.1990.2.2.188; Krogh A., 1992, NIPS; LeCun Y., 1990, NIPS; Liu B., 2015, CVPR; Montufar G., 2014, NIPS; Mozer M., 1988, NIPS; Murray K., 2015, CORR; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Romero A., 2015, ICLR; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Simon Noah, 2013, J COMPUTATIONAL GRAP; Simonyan K., 2014, 3 INT C LEARN REPR I; Theodoridis S., 2015, MACHINE LEARNING BAY, V8; Wang J., 2014, J FIBER BIOENGINEERI, V7, P603; Warde-Farley D., 2013, ICML; Weigend A. S., 1991, NIPS; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x; Zhou B., 2015, PLACES IMAGE DATABAS; Zhou Hao, 2016, EUR C COMP VIS ECCV	36	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703008
C	Arvanitidis, G; Hansen, LK; Hauberg, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Arvanitidis, Georgios; Hansen, Lars Kai; Hauberg, Soren			A Locally Adaptive Normal Distribution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DIMENSIONALITY REDUCTION	The multivariate normal density is a monotonic function of the distance to the mean, and its ellipsoidal shape is due to the underlying Euclidean metric. We suggest to replace this metric with a locally adaptive, smoothly changing (Riemannian) metric that favors regions of high local density. The resulting locally adaptive normal distribution (LAND) is a generalization of the normal distribution to the "manifold" setting, where data is assumed to lie near a potentially low-dimensional manifold embedded in RD. The LAND is parametric, depending only on a mean and a covariance, and is the maximum entropy distribution under the given metric. The underlying metric is, however, non-parametric. We develop a maximum likelihood algorithm to infer the distribution parameters that relies on a combination of gradient descent and Monte Carlo integration. We further extend the LAND to mixture models, and provide the corresponding EM algorithm. We demonstrate the efficiency of the LAND to fit non-trivial probability distributions over both synthetic data, and EEG measurements of human sleep.	[Arvanitidis, Georgios; Hansen, Lars Kai; Hauberg, Soren] Tech Univ Denmark, DTU Compute, Sect Cognit Syst, Lyngby, Denmark	Technical University of Denmark	Arvanitidis, G (corresponding author), Tech Univ Denmark, DTU Compute, Sect Cognit Syst, Lyngby, Denmark.	gear@dtu.dk; lkai@dtu.dk; sohau@dtu.dk			Novo Nordisk Foundation Interdisciplinary Synergy Program 2014, 'Biophysically adjusted state-informed cortex stimulation (BASICS)'; Danish Council for Independent Research, Natural Sciences	Novo Nordisk Foundation Interdisciplinary Synergy Program 2014, 'Biophysically adjusted state-informed cortex stimulation (BASICS)'; Danish Council for Independent Research, Natural Sciences(Det Frie Forskningsrad (DFF))	LKH was funded in part by the Novo Nordisk Foundation Interdisciplinary Synergy Program 2014, 'Biophysically adjusted state-informed cortex stimulation (BASICS)'. SH was funded in part by the Danish Council for Independent Research, Natural Sciences.	Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Bishop C.M., 2014, ANTIMICROB AGENTS CH, V58, P7250; Boyce R, 2016, SCIENCE, V352, P812, DOI 10.1126/science.aad5252; Carmo M., 1992, MATHEMATICS; Delorme A., 2004, J NEUROSCI METH, P21; Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215; Hauberg S., 2012, ADV NEURAL INFORM PR, P2033; Hauberg S., 2016, IEEE T PATTERN ANAL; Hennig P., 2014, P 17 INT C ART INT S, V33; Imtiaz SA, 2015, IEEE ENG MED BIO, P6014, DOI 10.1109/EMBC.2015.7319762; KARCHER H, 1977, COMMUN PUR APPL MATH, V30, P509, DOI 10.1002/cpa.3160300502; Marxer R., 2008, F MEASURE EVALUATION, P1; Pennec X, 2006, J MATH IMAGING VIS, V25, P127, DOI 10.1007/s10851-006-6228-4; Purves D., 2008, NEUROSCIENCE; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327; Simo-Serra E., 2014, BRIT MACH VIS C BMVC; Straub J., 2015, INT C ART INT STAT A; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tosi A., 2014, C UNC ART INT UAI JU; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Zhang Miaomiao, 2013, ADV NEURAL INFORM PR, V26, P2	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704028
C	Balkanski, E; Rubinstein, A; Singer, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Balkanski, Eric; Rubinstein, Aviad; Singer, Yaron			The Power of Optimization from Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of optimization from samples of monotone submodular functions with bounded curvature. In numerous applications, the function optimized is not known a priori, but instead learned from data. What are the guarantees we have when optimizing functions from sampled data? In this paper we show that for any monotone submodular function with curvature c there is a (1-c)/(1 + c-c(2)) approximation algorithm for maximization under cardinality constraints when polynomially-many samples are drawn from the uniform distribution over feasible sets. Moreover, we show that this algorithm is optimal. That is, for any c < 1, there exists a submodular function with curvature c for which no algorithm can achieve a better approximation. The curvature assumption is crucial as for general monotone submodular functions no algorithm can obtain a constant-factor approximation for maximization under a cardinality constraint when observing polynomially-many samples drawn from any distribution over feasible sets, even when the function is statistically learnable.	[Balkanski, Eric; Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA; [Rubinstein, Aviad] Univ Calif Berkeley, Berkeley, CA 94720 USA	Harvard University; University of California System; University of California Berkeley	Balkanski, E (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	ericbalkanski@g.harvard.edu; aviad@eecs.berkeley.edu; yaron@seas.harvard.edu						Balcan M., 2011, STOC; Balcan M.-F., 2012, COLT; Balcan Maria-Florina, 2015, AAMAS; Balkanski E., 2015, ARXIV151206238; Conforti M., 1984, DISCRETE APPL MATH; Feige U., 2011, SIAM J COMPUTING; Feige U, 1998, JACM; Feldman V., 2013, COLT; Feldman Vitaly, 2014, COLT; Feldman Vitaly, 2015, CORR; Feldman Vitaly, 2013, FOCS; Golovin  D., 2010, IPSN; Gomez Rodriguez M., 2010, SIGKDD; Hang L., 2011, IEICE; Iyer R. K, 2013, NIPS; Jegelka S., 2011, ICML; Jegelka S., 2011, CVPR; Kempe D., 2003, SIGKDD; Leskovec J., 2007, SIGKDD; Lin H., 2011, NAACL HLT; Lin H., 2011, INTERSPEECH; Nemhauser G. L., 1978, MATH PROGRAMMING STU, V8; Rosenfeld N., 2016, ARXIV160504719; Sviridenko M., 2015, SODA; Valiant L. G., 1984, COMMUN ACM; Vondrak J., 2010, RIMS; Yue Yisong, 2008, ICML	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703083
C	Beygelzimer, A; Hsu, D; Langford, J; Zhang, CC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Beygelzimer, Alina; Hsu, Daniel; Langford, John; Zhang, Chicheng			Search Improves Label for Active Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONVERGENCE	We investigate active learning with access to two distinct oracles: LABEL (which is standard) and SEARCH (which is not). The SEARCH oracle models the situation where a human searches a database to seed or counterexample an existing solution. SEARCH is stronger than LABEL while being natural to implement in many situations. We show that an algorithm using both oracles can provide exponentially large problem-dependent improvements over LABEL alone.	[Beygelzimer, Alina] Yahoo Res, New York, NY 10003 USA; [Hsu, Daniel] Columbia Univ, New York, NY USA; [Langford, John] Microsoft Res, New York, NY USA; [Zhang, Chicheng] Univ Calif San Diego, La Jolla, CA USA	Columbia University; Microsoft; University of California System; University of California San Diego	Beygelzimer, A (corresponding author), Yahoo Res, New York, NY 10003 USA.	beygel@yahoo-inc.com; djhsu@cs.columbia.edu; jcl@microsoft.com; chz038@cs.ucsd.edu						Attenberg J., 2010, P 16 ACM SIGKDD INT, P423, DOI DOI 10.1145/1835804.1835859; Balcan M.-F., 2012, COLT; Balcan M.-F., 2006, ICML; Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y; Beygelzimer A, 2010, ADV NEURAL INFORM PR, V23; Beygelzimer Alina, 2011, ICML WORKSH ONL TRAD; COHN D, 1994, MACH LEARN, V15, P201, DOI 10.1007/BF00993277; Dasgupta S., 2005, ADV NEURAL INFORM PR, V18; Dasgupta S., 2007, ADV NEURAL INFORM PR, V20; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843; Hanneke Steve, 2007, ICML, P249; Huang Tzu-Kuo, 2008, ADV NEURAL INFORM PR, V28; Kaariainen M, 2006, LECT NOTES ARTIF INT, V4264, P63; Simard Patrice Y., 2014, ABS14094814 CORR; Vapnik V., 1982, ESTIMATION DEPENDENC; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	18	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701040
C	Bullins, B; Hazan, E; Koren, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bullins, Brian; Hazan, Elad; Koren, Tomer			The Limits of Learning with Missing Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study linear regression and classification in a setting where the learning algorithm is allowed to access only a limited number of attributes per example, known as the limited attribute observation model. In this well-studied model, we provide the first lower bounds giving a limit on the precision attainable by any algorithm for several variants of regression, notably linear regression with the absolute loss and the squared loss, as well as for classification with the hinge loss. We complement these lower bounds with a general purpose algorithm that gives an upper bound on the achievable precision limit in the setting of learning with missing data.	[Bullins, Brian; Hazan, Elad] Princeton Univ, Princeton, NJ 08544 USA; [Koren, Tomer] Google Brain, Mountain View, CA USA	Princeton University; Google Incorporated	Bullins, B (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	bbullins@cs.princeton.edu; ehazan@cs.princeton.edu; tkoren@google.com		Hazan, Elad/0000-0002-1566-3216				Ben-David S, 1998, J COMPUT SYST SCI, V56, P277, DOI 10.1006/jcss.1998.1569; Cesa-Bianchi N., 2010, P 27 INT C MACH LEAR P 27 INT C MACH LEAR; Cesa-Bianchi N, 2011, IEEE T INFORM THEORY, V57, P7907, DOI 10.1109/TIT.2011.2164053; Dekel O, 2010, MACH LEARN, V81, P149, DOI 10.1007/s10994-009-5124-8; HAUSSLER D, 1992, INFORM COMPUT, V100, P78, DOI 10.1016/0890-5401(92)90010-D; Hazan E., 2015, P 32 INT C MACH LEAR; Hazan E., 2012, P 29 INT C MACH LEAR; Kukliansky D., 2015, P 32 INT C MACH LEAR; Little R. J., 2019, STAT ANAL MISSING DA, V793; Loh P.-L., 2011, ADV NEURAL INFORM PR; Rostamizadeh A., 2011, 27 C UNC ART INT; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Zinkevich M., 2003, INT C MACH LEARN ICM	13	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701028
C	Cai, D; Campbell, T; Broderick, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cai, Diana; Campbell, Trevor; Broderick, Tamara			Edge-exchangeable graphs and sparsity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ARRAYS	Many popular network models rely on the assumption of (vertex) exchangeability, in which the distribution of the graph is invariant to relabelings of the vertices. However, the Aldous-Hoover theorem guarantees that these graphs are dense or empty with probability one, whereas many real-world graphs are sparse. We present an alternative notion of exchangeability for random graphs, which we call edge exchangeability, in which the distribution of a graph sequence is invariant to the order of the edges. We demonstrate that edge-exchangeable models, unlike models that are traditionally vertex exchangeable, can exhibit sparsity. To do so, we outline a general framework for graph generative models; by contrast to the pioneering work of Caron and Fox [12], models within our framework are stationary across steps of the graph sequence. In particular, our model grows the graph by instantiating more latent atoms of a single random measure as the dataset size increases, rather than adding new atoms to the measure.	[Cai, Diana] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Campbell, Trevor; Broderick, Tamara] MIT, CSAIL, Cambridge, MA 02139 USA	University of Chicago; Massachusetts Institute of Technology (MIT)	Cai, D (corresponding author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.	dcai@uchicago.edu; tdjc@mit.edu; tbroderick@csail.mit.edu						Aldous D.J., 1985, LECT NOTES MATH, V1117, P1, DOI DOI 10.1007/BFB0099421; ALDOUS DJ, 1981, J MULTIVARIATE ANAL, V11, P581, DOI 10.1016/0047-259X(81)90099-3; [Anonymous], 1990, J THEORET PROBAB; Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168; Borgs C., 2014, ARXIV E PRINTS; Borgs C., 2016, 160107134 ARXIV; Borgs C., 2015, 14011137 ARXIV; Broderick T., 2015, NIPS 2015 WORKSH BAY; Broderick T., 2015, NIPS 2015 WORKSH NET; Broderick T, 2012, BAYESIAN ANAL, V7, P439, DOI 10.1214/12-BA715; Cai Diana, 2015, NIPS 2015 WORKSH NET; Campbell T., 2016, 160300861 ARXIV; Caron F., 2015, 14011137V3 ARXIV; Crane H., 2015, 150908184 ARXIV; Crane H., 2015, 150908185 ARXIV; Dempsey W., 2016, 160304571 ARXIV; HOOVER D, 1979, RELATIONS PROBABILIT; KALLENBERG O, 2005, PROB APPL S; Lloyd J. R., 2012, NIPS 25, V25; Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607; Teh Y. W., 2009, NIPS 23, V23; Veitch V., 2015, 151203099 ARXIV; Williamson RC, 2016, J MACH LEARN RES, V17, P1; Wolfe P. J., 2013, 13095936 ARXIV	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704110
C	Chalk, M; Marre, O; Tkacik, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chalk, Matthew; Marre, Olivier; Tkacik, Gasper			Relevant sparse codes with variational information bottleneck	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In many applications, it is desirable to extract only the relevant aspects of data. A principled way to do this is the information bottleneck (IB) method, where one seeks a code that maximizes information about a 'relevance' variable, Y, while constraining the information encoded about the original data, X. Unfortunately however, the IB method is computationally demanding when data are high-dimensional and/or non-gaussian. Here we propose an approximate variational scheme for maximizing a lower bound on the IB objective, analogous to variational EM. Using this method, we derive an IB algorithm to recover features that are both relevant and sparse. Finally, we demonstrate how kernelized versions of the algorithm can be used to address a broad range of problems with non-linear relation between X and Y	[Chalk, Matthew; Tkacik, Gasper] IST Austria, Campus 1, A-3400 Klosterneuburg, Austria; [Marre, Olivier] Inst Vis, 17 Rue Moreau, F-75012 Paris, France	Institute of Science & Technology - Austria; UDICE-French Research Universities; Sorbonne Universite	Chalk, M (corresponding author), IST Austria, Campus 1, A-3400 Klosterneuburg, Austria.		Marre, Olivier/F-2751-2017	Marre, Olivier/0000-0002-0090-6190				ANDREWS DF, 1974, J ROY STAT SOC B MET, V36, P99; Barber D, 2004, ADV NEUR IN, V16, P201; Bialek W, 2001, NEURAL COMPUT, V13, P2409, DOI 10.1162/089976601753195969; Bialek W, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P659, DOI 10.1109/ISIT.2006.261867; Chechik G., 2002, ADV NEURAL INFORM PR, V15; Doi E., 2005, ADV NEURAL INFORM PR, P377; Doi E, 2012, J NEUROSCI, V32, P16256, DOI 10.1523/JNEUROSCI.4036-12.2012; Eichhorn J, 2009, PLOS COMPUT BIOL, V5, DOI 10.1371/journal.pcbi.1000336; Elidan G., 2002, P UNC ART INT, P200; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hofmann T., 2003, 3 IEEE INT C DAT MIN; Mika S., 1999, NIPS, P526; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Scheffler C., 2008, DERIVATION EM UPDATE; Simoncelli EP, 2001, ANNU REV NEUROSCI, V24, P1193, DOI 10.1146/annurev.neuro.24.1.1193; Slonim N., 2002, ADV NEURAL INFORM PR, P335; Tishby Naftali, 1999, ALL C COMM CONTR COM; Tkacik G, 2010, P NATL ACAD SCI USA, V107, P14419, DOI 10.1073/pnas.1004906107; [No title captured]	20	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700065
C	Chen, H; Xia, HF; Cai, WD; Huang, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Chen, Hong; Xia, Haifeng; Cai, Weidong; Huang, Heng			Error Analysis of Generalized Nystrom Kernel Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				APPROXIMATION; MATRIX	Nystrom method has been successfully used to improve the computational efficiency of kernel ridge regression (KRR). Recently, theoretical analysis of Nystrom KRR, including generalization bound and convergence rate, has been established based on reproducing kernel Hilbert space (RKHS) associated with the symmetric positive semi-definite kernel. However, in real world applications, RKHS is not always optimal and kernel function is not necessary to be symmetric or positive semi-definite. In this paper, we consider the generalized Nystrom kernel regression (GNKR) with l(2) coefficient regularization, where the kernel just requires the continuity and boundedness. Error analysis is provided to characterize its generalization performance and the column norm sampling strategy is introduced to construct the refined hypothesis space. In particular, the fast learning rate with polynomial decay is reached for the GNKR. Experimental analysis demonstrates the satisfactory performance of GNKR with the column norm sampling.	[Chen, Hong; Huang, Heng] Univ Texas Arlington, Comp Sci & Engn, Arlington, TX 76019 USA; [Xia, Haifeng] Huazhong Agr Univ, Math & Stat, Wuhan 430070, Peoples R China; [Cai, Weidong] Univ Sydney, Sch Informat Technol, Sydney, NSW 2006, Australia	University of Texas System; University of Texas Arlington; Huazhong Agricultural University; University of Sydney	Chen, H (corresponding author), Univ Texas Arlington, Comp Sci & Engn, Arlington, TX 76019 USA.	chenh@mail.hzau.edu.cn; haifeng.xia0910@gmail.com; tom.cai@sydney.edu.au; heng@uta.edu			U.S. NSF-IIS [1302675]; NSF-IIS [1633753, 1344152, 1619308]; NSF-DBI [1356628]; NIH [AG049371]; National Natural Science Foundation of China (NSFC) [11671161]	U.S. NSF-IIS; NSF-IIS(National Science Foundation (NSF)); NSF-DBI(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Natural Science Foundation of China (NSFC)(National Natural Science Foundation of China (NSFC))	This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH AG049371, and by National Natural Science Foundation of China (NSFC) 11671161. We thank the anonymous NIPS reviewers for insightful comments.	Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Bach F., 2013, COLT; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Drineas P, 2006, SIAM J COMPUT, V36, P158, DOI 10.1137/S0097539704442696; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Eberts M., 2011, ADV NEURAL INFORM PR, V24, P1539; Feng Y., 2016, NEURAL COMPUT, V28, P1; Gittens A., 2013, INT C MACHINE LEARNI, P567; Hsieh C., 2014, ADV NEURAL INFORM PR, P3689; Jin R, 2013, IEEE T INFORM THEORY, V59, P6939, DOI 10.1109/TIT.2013.2271378; Kumar S, 2012, J MACH LEARN RES, V13, P981; Lim W, 2015, PR MACH LEARN RES, V37, P1367; Liu CJ, 2004, IEEE T PATTERN ANAL, V26, P572, DOI 10.1109/TPAMI.2004.1273927; Pekalska E, 2009, IEEE T PATTERN ANAL, V31, P1017, DOI 10.1109/TPAMI.2008.290; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rudi A., 2015, NIPS, P1657; Scholkopf B., 2001, LEARNING KERNELS SUP; Shi L, 2013, APPL COMPUT HARMON A, V34, P252, DOI 10.1016/j.acha.2012.05.001; Shi L, 2011, APPL COMPUT HARMON A, V31, P286, DOI 10.1016/j.acha.2011.01.001; Sun HW, 2015, IEEE T NEUR NET LEAR, V26, P2576, DOI 10.1109/TNNLS.2014.2375209; Sun HW, 2011, APPL COMPUT HARMON A, V30, P96, DOI 10.1016/j.acha.2010.04.001; Tsang I.W., 2008, ICML ICML 08, P1232, DOI DOI 10.1145/1390156.1390311; Wang Y., 2016, MINIMAX SUBSAMPLING; Williams CKI, 2001, ADV NEUR IN, V13, P682; Yang Tianbao, 2012, NIPS, P485; Yang Yun, 2015, ARXIV150106195; Ying Y., 2009, ADV NEURAL INFORM PR, V22, P2205; Zhu R., 2015, OPTIMAL SUBSAMPLING	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705016
C	Cheng, CA; Boots, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Cheng, Ching-An; Boots, Byron			Incremental Variational Sparse Gaussian Process Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recent work on scaling up Gaussian process regression (GPR) to large datasets has primarily focused on sparse GPR, which leverages a small set of basis functions to approximate the full Gaussian process during inference. However, the majority of these approaches are batch methods that operate on the entire training dataset at once, precluding the use of datasets that are streaming or too large to fit into memory. Although previous work has considered incrementally solving variational sparse GPR, most algorithms fail to update the basis functions and therefore perform suboptimally. We propose a novel incremental learning algorithm for variational sparse GPR based on stochastic mirror ascent of probability densities in reproducing kernel Hilbert space. This new formulation allows our algorithm to update basis functions online in accordance with the manifold structure of probability densities for fast convergence. We conduct several experiments and show that our proposed approach achieves better empirical performance in terms of prediction error than the recent state-of-the-art incremental solutions to variational sparse GPR.	[Cheng, Ching-An; Boots, Byron] Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Cheng, CA (corresponding author), Georgia Inst Technol, Inst Robot & Intelligent Machines, Atlanta, GA 30332 USA.	cacheng@gatech.edu; bboots@cc.gatech.edu	Cheng, Ching-An/AAZ-1802-2020	Cheng, Ching-An/0000-0002-0610-2070				Abdel-Gawad Ahmed H, 2012, ARXIV12033507; Alexander G, 2016, P 19 INT C ART INT S; Alvarez M. A., 2009, ADV NEURAL INFORM PR, P57; Alvarez M. A., 2010, P 13 INT C ART INT S, P25; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Dai Bo, 2015, ARXIV150603101; Hensman J., 2013, P C UNC ART INT UAI, P282; Hoang TN, 2015, PR MACH LEARN RES, V37, P569; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Holmes I, 2015, INFIN DIMENS ANAL QU, V18, DOI 10.1142/S0219025715500198; Khan M. E., 2015, ADV NEURAL INFORM PR, P3384; Lawrence N., 2003, ARTIFICIAL INTELLIGE, V9; Lazaro-Gredilla M., 2009, ADV NEURAL INFORM PR, V22, P1087; Meier F, 2014, ADV NEURAL INFORM PR, P972; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Raskutti G, 2015, IEEE T INFORM THEORY, V61, P1451, DOI 10.1109/TIT.2015.2388583; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sheth R., 2015, P INT C MACH LEARN, P1302; Snelson E., 2005, ADV NEURAL INFORM PR, P1257; Snelson Edward, 2007, P 11 INT C ARTIFICIA, P524; Theis Lucas, 2015, ARXIV150507649; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Walder C., 2008, ICML, P1112	27	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703109
C	Colombo, N; Vlassis, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Colombo, Nicolo; Vlassis, Nikos			A Posteriori Error Bounds for Joint Matrix Decomposition Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SIMULTANEOUS SCHUR DECOMPOSITION; CANONICAL DECOMPOSITION	Joint matrix triangularization is often used for estimating the joint eigenstructure of a set M of matrices, with applications in signal processing and machine learning. We consider the problem of approximate joint matrix triangularization when the matrices in M are jointly diagonalizable and real, but we only observe a set M' of noise perturbed versions of the matrices in M. Our main result is a first-order upper bound on the distance between any approximate joint triangularizer of the matrices in M' and any exact joint triangularizer of the matrices in M. The bound depends only on the observable matrices in M' and the noise level. In particular, it does not depend on optimization specific properties of the triangularizer, such as its proximity to critical points, that are typical of existing bounds in the literature. To our knowledge, this is the first a posteriori bound for joint matrix decomposition. We demonstrate the bound on synthetic data for which the ground truth is known.	[Colombo, Nicolo] UCL, Dept Stat Sci, London, England; [Vlassis, Nikos] Adobe Res, San Jose, CA USA	University of London; University College London; Adobe Systems Inc.	Colombo, N (corresponding author), UCL, Dept Stat Sci, London, England.	nicolo.colombo@ucl.ac.uk; vlassis@adobe.com						ABEDMERAIM K, 1998, ACOUST SPEECH SIG PR, P2541; Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Afsari B, 2008, SIAM J MATRIX ANAL A, V30, P1148, DOI 10.1137/060655997; Anandkumar A, 2014, J MACH LEARN RES, V15, P2773; Balle B, 2011, LECT NOTES ARTIF INT, V6911, P156, DOI 10.1007/978-3-642-23780-5_20; Cardoso J.-F., 1994, 94D023 TEL PAR SIGN; Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546; Colombo N., 2016, P 33 INT C MACH LEAR; Corless R. M., 1997, ISSAC 97. Proceedings of the 1997 International Sympsoium on Symbolic and Algebraic Computation, P133, DOI 10.1145/258726.258767; De Lathauwer L, 2004, SIAM J MATRIX ANAL A, V26, P295, DOI 10.1137/S089547980139786X; De Lathauwer L, 2006, SIAM J MATRIX ANAL A, V28, P642, DOI 10.1137/040608830; Fu T, 2006, INT C COMMUN CIRCUIT, P356, DOI 10.1109/ICCCAS.2006.284653; Haardt M, 1998, IEEE T SIGNAL PROCES, V46, P161, DOI 10.1109/78.651206; Horn R.A., 2013, MATRIX ANAL, VSecond; Iferroudjene R, 2009, APPL MATH COMPUT, V211, P363, DOI 10.1016/j.amc.2009.01.045; KONSTANTINOV MM, 1994, SIAM J MATRIX ANAL A, V15, P383, DOI 10.1137/S089547989120267X; Kuleshov V., 2015, 18 INT C ART INT STA; Luciani X, 2010, LECT NOTES COMPUT SC, V6365, P555, DOI 10.1007/978-3-642-15995-4_69; PANG JS, 1987, MATH OPER RES, V12, P474, DOI 10.1287/moor.12.3.474; Podosinnikova A., 2016, P 33 INT C MACH LEAR; Prudhomme S, 2003, INT J NUMER METH ENG, V56, P1193, DOI 10.1002/nme.609; Sardouie SH, 2013, INT CONF ACOUST SPEE, P4178, DOI 10.1109/ICASSP.2013.6638446; Souloumiac A, 2009, IEEE T SIGNAL PROCES, V57, P2222, DOI 10.1109/TSP.2009.2016997	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703060
C	Davis, D; Udell, M; Edmunds, B		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Davis, Damek; Udell, Madeleine; Edmunds, Brent			The Sound of APALM Clapping: Faster Nonsmooth Nonconvex Optimization with Stochastic Asynchronous PALM	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the Stochastic Asynchronous Proximal Alternating Linearized Minimization (SAPALM) method, a block coordinate stochastic proximal-gradient method for solving nonconvex, nonsmooth optimization problems. SAPALM is the first asynchronous parallel optimization method that provably converges on a large class of nonconvex, nonsmooth problems. We prove that SAPALM matches the best known rates of convergence - among synchronous or asynchronous methods - on this problem class. We provide upper bounds on the number of workers for which we can expect to see a linear speedup, which match the best bounds known for less complex problems, and show that in practice SAPALM achieves this linear speedup. We demonstrate state-of-the-art performance on several matrix factorization problems.	[Davis, Damek; Udell, Madeleine] Cornell Univ, Ithaca, NY 14853 USA; [Edmunds, Brent] Univ Calif Los Angeles, Los Angeles, CA USA	Cornell University; University of California System; University of California Los Angeles	Davis, D (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	dsd95@cornell.edu; mru8@cornell.edu; brent.edmunds@math.ucla.edu						Agarwal A, 2012, IEEE DECIS CONTR P, P5451, DOI 10.1109/CDC.2012.6426626; Bertsekas D. P., PARALLEL DISTRIBUTED, V23; Bolte J, 2014, MATH PROGRAM, V146, P459, DOI 10.1007/s10107-013-0701-9; Davis D, 2016, ARXIV160100698; Davis D., 2016, ARXIV160400526; Dean J., 2012, NIPS 12, V1, P1223; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ghadimi S, 2016, MATH PROGRAM, V155, P267, DOI 10.1007/s10107-014-0846-1; HONG M., 2014, ARXIV14126058; Lian X., 2015, P ADV NEUR INF PROC, P2719; Liu J, 2015, J MACH LEARN RES, V16, P285; Mania H., 2015, ARXIV150706970; Nesterov Y., 2018, APPL OPTIMIZATION; Peng Z., 2015, ARXIV150602396; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Rockafellar R.T., 2009, VARIATIONAL ANAL, V317; Sridhar, 2014, ARXIV14014780; Tseng P, 1991, SIAM J OPTIMIZ, V1, P603, DOI 10.1137/0801036; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Udell Madeleine, 2014, ARXIV14100342; Woodworth J., 2015, ARXIV150402923; Xu YY, 2015, SIAM J OPTIMIZ, V25, P1686, DOI 10.1137/140983938; Yun H, 2014, PROC VLDB ENDOW, V7, P975, DOI 10.14778/2732967.2732973	23	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703064
C	Desir, A; Goyal, V; Jagabathula, S; Segev, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Desir, Antoine; Goyal, Vineet; Jagabathula, Srikanth; Segev, Danny			Assortment Optimization Under the Mallows model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CHOICE; ALGORITHM	We consider the assortment optimization problem when customer preferences follow a mixture of Mallows distributions. The assortment optimization problem focuses on determining the revenue/profit maximizing subset of products from a large universe of products; it is an important decision that is commonly faced by retailers in determining what to offer their customers. There are two key challenges: (a) the Mallows distribution lacks a closed-form expression (and requires summing an exponential number of terms) to compute the choice probability and, hence, the expected revenue/profit per customer; and (b) finding the best subset may require an exhaustive search. Our key contributions are an efficiently computable closed-form expression for the choice probability under the Mallows model and a compact mixed integer linear program (MIP) formulation for the assortment problem.	[Desir, Antoine; Goyal, Vineet] Columbia Univ, IEOR Dept, New York, NY 10027 USA; [Jagabathula, Srikanth] NYU, Stern Sch Business, IOMS Dept, New York, NY 10003 USA; [Segev, Danny] Univ Haifa, Dept Stat, Haifa, Israel	Columbia University; New York University; University of Haifa	Desir, A (corresponding author), Columbia Univ, IEOR Dept, New York, NY 10027 USA.	antoine@ieor.columbia.edu; vgoyal@ieor.columbia.edu; sjagabat@stern.nyu.edu; segevd@stat.haifa.ac.il						Aouad A., 2015, APPROXIMABILITY ASSO; Blanchet J., 2013, EC, P103; Brightwell G., 1991, P ACM S THEORY COMPU, P175, DOI DOI 10.1145/103418.103441; COOLEY JW, 1965, MATH COMPUT, V19, P297, DOI 10.2307/2003354; Davis JM, 2014, OPER RES, V62, P250, DOI 10.1287/opre.2014.1256; Doignon JP, 2004, PSYCHOMETRIKA, V69, P33, DOI 10.1007/BF02295838; Dwork C., 2001, P 10 INT C WORLD WID, P613, DOI [10.1145/371920.372165, DOI 10.1145/371920.372165]; Farias VF, 2013, MANAGE SCI, V59, P305, DOI 10.1287/mnsc.1120.1610; Gallego G, 2014, MANAGE SCI, V60, P2583, DOI 10.1287/mnsc.2014.1931; Guiver J., 2009, P 26 ANN INT C MACHI, P377; Honhon D, 2012, M&SOM-MANUF SERV OP, V14, P279, DOI 10.1287/msom.1110.0365; Jagabathula S., 2014, ASSORTMENT OPTIMIZAT; Lebanon G, 2008, J MACH LEARN RES, V9, P2401; Lu T., 2011, ICML, P145; Luce R, 1959, INDIVIDUAL CHOICE BE; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Marden JI, 1995, ANAL MODELING RANK D; McFadden D., 1978, TRANSP RES REC, V673, P72; Meila M., 2012, ARXIV PREPRINT ARXIV; Bront JJM, 2009, OPER RES, V57, P769, DOI 10.1287/opre.1080.0567; Murphy TB, 2003, COMPUT STAT DATA AN, V41, P645, DOI 10.1016/S0167-9473(02)00165-2; PLACKETT RL, 1975, ROY STAT SOC C-APP, V24, P193; Talluri K, 2004, MANAGE SCI, V50, P15, DOI 10.1287/mnsc.1030.0147; van Ryzin G, 2015, MANAGE SCI, V61, P281, DOI 10.1287/mnsc.2014.2040; YELLOTT JI, 1977, J MATH PSYCHOL, V15, P109, DOI 10.1016/0022-2496(77)90026-8	25	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701081
C	Deza, A; Eckstein, MP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Deza, Arturo; Eckstein, Miguel P.			Can Peripheral Representations Improve Clutter Metrics on Complex Scenes?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Previous studies have proposed image-based clutter measures that correlate with human search times and/or eye movements. However, most models do not take into account the fact that the effects of clutter interact with the foveated nature of the human visual system: visual clutter further from the fovea has an increasing detrimental influence on perception. Here, we introduce a new foveated clutter model to predict the detrimental effects in target search utilizing a forced fixation search task. We use Feature Congestion (Rosenholtz et al.) as our non foveated clutter model, and we stack a peripheral architecture on top of Feature Congestion for our foveated model. We introduce the Peripheral Integration Feature Congestion (PIFC) coefficient, as a fundamental ingredient of our model that modulates clutter as a non-linear gain contingent on eccentricity. We show that Foveated Feature Congestion (FFC) clutter scores (r(44) = 0.82 +/- 0.04, p < 0.0001) correlate better with target detection (hit rate) than regular Feature Congestion (r(44) = -0.19 +/- 0.13,p = 0.0774) in forced fixation search; and we extend foveation to other clutter models showing stronger correlations in all cases. Thus, our model allows us to enrich clutter perception research by computing fixation specific clutter maps. Code for building peripheral representations is availablel	[Deza, Arturo] UC Santa Barbara, Inst Collaborat Biotechnol, Dynam Neurosci, Santa Barbara, CA 93106 USA; [Eckstein, Miguel P.] UC Santa Barbara, Inst Collaborat Biotechnol, Psychol & Brain Sci, Santa Barbara, CA USA	University of California System; University of California Santa Barbara; University of California System; University of California Santa Barbara	Deza, A (corresponding author), UC Santa Barbara, Inst Collaborat Biotechnol, Dynam Neurosci, Santa Barbara, CA 93106 USA.	deza@dyns.ucsb.edu; eckstein@psych.ucsb.edu			U.S. Army Research Office; Regents of the University of California [W911NF-09-0001]	U.S. Army Research Office; Regents of the University of California(University of California System)	We would like to thank Miguel Lago and Aditya Jonnalagadda for useful proof-reads and revisions, as well as Mordechai Juni, N.C. Puneeth, and Emre Akbas for useful suggestions. This work has been sponsored by the U.S. Army Research Office and the Regents of the University of California, through Contract Number W911NF-09-0001 for the Institute for Collaborative Biotechnologies, and that the content of the information does not necessarily reflect the position or the policy of the Government or the Regents of the University of California, and no official endorsement should be inferred.	Achanta R., 2010, TECHNICAL REPORT; Akbas E., 2014, ARXIV14080814; Asher MF, 2013, J VISION, V13, DOI 10.1167/13.5.25; Bravo MJ, 2008, J VISION, V8, DOI 10.1167/8.1.23; Dubey R., 2014, J VISION, V14, P935; Eckstein MP, 2011, J VISION, V11, DOI 10.1167/11.5.14; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Henderson JM, 2009, J VISION, V9, DOI 10.1167/9.1.32; Keshvari S, 2016, J VISION, V16, DOI 10.1167/16.3.39; LANDY MS, 1991, VISION RES, V31, P679, DOI 10.1016/0042-6989(91)90009-T; Levi DM, 2011, CURR BIOL, V21, pR678, DOI 10.1016/j.cub.2011.07.025; Levinshtein A, 2009, IEEE T PATTERN ANAL, V31, P2290, DOI 10.1109/TPAMI.2009.96; Ming-Yu Liu, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2097, DOI 10.1109/CVPR.2011.5995323; Movshon JA, 2014, COLD SH Q B, V79, P115, DOI 10.1101/sqb.2014.79.024844; Oliva A., IDENTIFYING PERCEPTU; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Pramod RT, 2016, PROC CVPR IEEE, P1601, DOI 10.1109/CVPR.2016.177; Rosenholtz R., 2005, P SIGCHI C HUM FACT, P761, DOI [10.1145/1054972.1055078, DOI 10.1145/1054972.1055078]; Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14; Rosenholtz R, 2007, J VISION, V7, DOI 10.1167/7.2.17; Simoncelli E. P, 1995, IM PROC INT C, V3, P3444; van den Berg R, 2009, J VISION, V9, DOI 10.1167/9.4.24; Yu CP, 2014, J VISION, V14, DOI 10.1167/14.7.4; Yu Chen-Ping, 2013, ADV NEURAL INFORM PR, P118	28	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703040
C	Durmus, A; Simsekli, U; Moulines, E; Badeau, R; Richard, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Durmus, Alain; Simsekli, Umut; Moulines, Eric; Badeau, Roland; Richard, Gael			Stochastic Gradient Richardson-Romberg Markov Chain Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Stochastic Gradient Markov Chain Monte Carlo (SG-MCMC) algorithms have become increasingly popular for Bayesian inference in large-scale applications. Even though these methods have proved useful in several scenarios, their performance is often limited by their bias. In this study, we propose a novel sampling algorithm that aims to reduce the bias of SG-MCMC while keeping the variance at a reasonable level. Our approach is based on a numerical sequence acceleration method, namely the Richardson-Romberg extrapolation, which simply boils down to running almost the same SG-MCMC algorithm twice in parallel with different step sizes. We illustrate our framework on the popular Stochastic Gradient Langevin Dynamics (SGLD) algorithm and propose a novel SG-MCMC algorithm referred to as Stochastic Gradient Richardson-Romberg Langevin Dynamics (SGRRLD). We provide formal theoretical analysis and show that SGRRLD is asymptotically consistent, satisfies a central limit theorem, and its non-asymptotic bias and the mean squared-error can be bounded. Our results show that SGRRLD attains higher rates of convergence than SGLD in both finite-time and asymptotically, and it achieves the theoretical accuracy of the methods that are based on higher-order integrators. We support our findings using both synthetic and real data experiments.	[Durmus, Alain; Simsekli, Umut; Badeau, Roland; Richard, Gael] Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France; [Moulines, Eric] Ecole Polytech, UMR 7641, Ctr Math Appl, Palaiseau, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); CNRS - National Institute for Mathematical Sciences (INSMI); Institut Polytechnique de Paris	Durmus, A (corresponding author), Univ Paris Saclay, Telecom ParisTech, CNRS, LTCI, F-75013 Paris, France.				French National Research Agency (ANR) as a part of the EDISON 3D project [ANR-13-CORD-0008-02]	French National Research Agency (ANR) as a part of the EDISON 3D project(French National Research Agency (ANR))	This work is partly supported by the French National Research Agency (ANR) as a part of the EDISON 3D project (ANR-13-CORD-0008-02).	Ahn S., 2012, ICML; Ahn S, 2015, KDD; [Anonymous], 2008, P 25 INT C MACH LEAR; CHEN C., 2015, ADV NEURAL INFORM PR, P2269; Chen T., 2014, ICML; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; Gemulla R., 2011, ACM SIGKDD; Grenander, 1983, DIVISION APPL MATH; Lamberton D, 2002, BERNOULLI, V8, P367; Lamberton D., 2003, STOCH DYNAM, V3, P435; LEMAIRE V., 2005, THESIS; Lemaire V., 2014, ARXIV14011177; Lemaire V, 2015, ANN I H POINCARE-PR, V51, P1562, DOI 10.1214/13-AIHP591; Li Chunyuan, 2016, AAAI C ART INT; Ma Y.A., 2015, ARXIV150604696, P2917; Mattingly JC, 2002, STOCH PROC APPL, V101, P185, DOI 10.1016/S0304-4149(02)00150-3; Pages G, 2007, MONTE CARLO METHODS, V13, P37, DOI 10.1515/MCMA.2007.003; Patterson S., 2013, NIPS; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Sato I, 2014, PR MACH LEARN RES, V32, P982; Shang X., 2015, ADV NEURAL INFORM PR, P37; Simsekli U., 2016, ICML; TALAY D, 1990, STOCH ANAL APPL, V8, P483, DOI 10.1080/07362999008809220; Teh Y. W., 2015, ARXIV150100438; Vollmer SJ, 2016, J MACH LEARN RES, V17, P1; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	27	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704039
C	Falahatgar, M; Ohannessian, MI; Orlitsky, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Falahatgar, Moein; Ohannessian, Mesrob I.; Orlitsky, Alon			Near-Optimal Smoothing of Structured Conditional Probability Matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Utilizing the structure of a probabilistic model can significantly increase its learning speed. Motivated by several recent applications, in particular bigram models in language processing, we consider learning low-rank conditional probability matrices under expected KL-risk. This choice makes smoothing, that is the careful handling of low-probability elements, paramount. We derive an iterative algorithm that extends classical non-negative matrix factorization to naturally incorporate additive smoothing and prove that it converges to the stationary points of a penalized empirical risk. We then derive sample-complexity bounds for the global minimzer of the penalized risk and show that it is within a small factor of the optimal sample complexity. This framework generalizes to more sophisticated smoothing techniques, including absolute-discounting.	[Falahatgar, Moein; Orlitsky, Alon] Univ Calif San Diego, San Diego, CA 92103 USA; [Ohannessian, Mesrob I.] Toyota Technol Inst Chicago, Chicago, IL USA	University of California System; University of California San Diego; Toyota Technological Institute - Chicago	Falahatgar, M (corresponding author), Univ Calif San Diego, San Diego, CA 92103 USA.	moein@ucsd.edu; mesrob@ttic.edu; alon@ucsd.edu			NSF [1065622, 1564355]	NSF(National Science Foundation (NSF))	We would like to thank Venkatadheeraj Pichapati and Ananda Theertha Suresh for many helpful discussions. This work was supported in part by NSF grants 1065622 and 1564355.	Abe, 1991, COLT; Acharya, 2013, COLT; Agarwal, 2013, ARXIV13107991; Arora, 2015, ARXIV150300778; Ben Hamou, 2017, BERNOULLI; Bhojanapalli, 2015, ARXIV150903917; Blei, 2003, JMLR; Borade, 2008, IEEE INT ZUR SEM COM; Brants, 2007, EMNLP; Buntine W., 2004, P 20 C UNC ART INT, P59; Chen SF, 1999, COMPUT SPEECH LANG, V13, P359, DOI 10.1006/csla.1999.0128; Hofmann, 1999, ACM SIGIR; Huang, 2016, ARXIV160206586; Hutchinson, 2015, IEEE T AUDIO SPEECH; Hutchinson, 2011, IEEE SPL; Kamath, 2015, COLT; Kneser, 1995, ICASSP; Lee, 2001, NIPS; Levy, 2014, NIPS; Mikolov, 2011, ICASSP; Ohannessian, 2012, COLT; Orlitsky, 2015, NIPS; Papadimitriou, 1998, ACM SIGACT SIGMOD SI; Parikh, 2013, ARXIV13127077; Shazeer, 2014, ARXIV14121454; Valiant, 2015, ARXIV150405321; VAPNIK, 1998, STAT LEARNING THEORY; Williams, 2015, ARXIV150200512; Zhu, 2013, IM AN	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701056
C	Farajtabar, M; Ye, XJ; Harati, S; Song, L; Zha, HY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Farajtabar, Mehrdad; Ye, Xiaojing; Harati, Sahar; Song, Le; Zha, Hongyuan			Multistage Campaigning in Social Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of how to optimize multi-stage campaigning over social networks. The dynamic programming framework is employed to balance the high present reward and large penalty on low future outcome in the presence of extensive uncertainties. In particular, we establish theoretical foundations of optimal campaigning over social networks where the user activities are modeled as a multivariate Hawkes process, and we derive a time dependent linear relation between the intensity of exogenous events and several commonly used objective functions of campaigning. We further develop a convex dynamic programming framework for determining the optimal intervention policy that prescribes the required level of external drive at each stage for the desired campaigning result. Experiments on both synthetic data and the real-world MemeTracker dataset show that our algorithm can steer the user activities for optimal campaigning much more accurately than baselines.	[Farajtabar, Mehrdad; Song, Le; Zha, Hongyuan] Georgia Inst Technol, Atlanta, GA 30332 USA; [Ye, Xiaojing] Georgia State Univ, Atlanta, GA 30303 USA; [Harati, Sahar] Emory Univ, Atlanta, GA 30322 USA	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia State University; Emory University	Farajtabar, M (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	mehrdad@gatech.edu; xye@gsu.edu; sahar.harati@emory.edu; lsong@cc.gatech.edu; zha@cc.gatech.edu			NSF/NIH BIGDATA [R01 GM108341]; NSF [IIS-1639792, DMS-1620345, DMS-1620342]	NSF/NIH BIGDATA; NSF(National Science Foundation (NSF))	The work is supported in part by NSF/NIH BIGDATA R01 GM108341, NSF IIS-1639792, NSF DMS-1620345, and NSF DMS-1620342.	Aalen OO, 2008, STAT BIOL HEALTH, P1; Al-Mohy AH, 2011, SIAM J SCI COMPUTING; Bertsekas D. P., DYNAMIC PROGRAMMING, V1; Bloembergen D, 2014, ECAI; Blundell Charles, 2012, NIPS; Bracewell R, 1965, FOURIER TRANSFORM II, V5; Chen Pin-Yu, 2014, SYSTEMS MAN CYBERN A; Daley D., 2007, INTRO THEORY POINT P; De A., 2015, ARXIV150605474; Farajtabar M., 2014, NIPS; Folland G. B, 1984, REAL ANAL MODERN TEC; Hanson F. B., 2007, APPL STOCHASTIC PROC, V13; Hawkes A. G., 1971, BIOMETRIKA; Hijab Omar, 2007, INTRO CALCULUS CLASS; Iwata T., 2013, SIGKDD; Kandhway K, 2015, CAMPAIGNING HETEROGE; Kempe D., 2003, SIGKDD; Leskovec J, 2009, SIGKDD; Lian W., 2015, ICML; Linderman S. W, 2014, ICML; Parikh AP, 2012, UAI; Perry PO, 2013, J ROYAL STAT SOC; Vergeer M, 2013, PARTY POLITICS; Wang Yichen, 2016, ARXIV160309021; West D. M., 2013, AIR WARS TELEVISION; Yang S.-H., 2013, INT C MACHINE; Zhou K., 2013, AISTATS	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700066
C	Fawzi, A; Moosayi-Dezfooli, SM; Frossard, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Fawzi, Alhussein; Moosayi-Dezfooli, Seyed-Mohsen; Frossard, Pascal			Robustness of classifiers: from adversarial to random noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Several recent works have shown that state-of-the-art classifiers are vulnerable to worst-case (i.e., adversarial) perturbations of the datapoints. On the other hand, it has been empirically observed that these same classifiers are relatively robust to random noise. In this paper, we propose to study a semi-random noise regime that generalizes both the random and worst-case noise regimes. We propose the first quantitative analysis of the robustness of nonlinear classifiers in this general noise regime. We establish precise theoretical bounds on the robustness of classifiers in this general regime, which depend on the curvature of the classifier's decision boundary. Our bounds confirm and quantify the empirical observations that classifiers satisfying curvature constraints are robust to random noise. Moreover, we quantify the robustness of classifiers in terms of the subspace dimension in the semi-random noise regime, and show that our bounds remarkably interpolate between the worst-case and random noise regimes. We perform experiments and show that the derived bounds provide very accurate estimates when applied to various state-of-the-art deep neural networks and datasets. This result suggests bounds on the curvature of the classifiers' decision boundaries that we support experimentally, and more generally offers important insights onto the geometry of high dimensional classification problems.	[Fawzi, Alhussein; Moosayi-Dezfooli, Seyed-Mohsen; Frossard, Pascal] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Ecole Polytechnique Federale de Lausanne	Fawzi, A (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	alhussein.fawzi@epfl.ch; seyed.moosavi@epfl.ch; pascal.frossard@epfl.ch	Frossard, Pascal/AAF-2268-2019		Hasler Foundation, Switzerland, in the framework of the CORA project	Hasler Foundation, Switzerland, in the framework of the CORA project	We would like to thank the anonymous reviewers for their helpful comments. We thank Omar Fawzi and Louis Merlin for the fruitful discussions. We also gratefully acknowledge the support of NVIDIA Corporation with the donation of the Tesla K40 GPU used for this research. This work has been partly supported by the Hasler Foundation, Switzerland, in the framework of the CORA project.	Chatfield K., 2014, BRIT MACH VIS C; Fawzi A., 2015, ABS150202590 CORR; Fawzi A., 2015, BRIT MACH VIS C BMVC; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Gu S., 2014, ARXIV14125068; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Huang R., 2015, ABS151103034 CORR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726; Lee J.M., 2009, MANIFOLDS DIFFERENTI, V107; Luo Y., 2015, ARXIV PREPRINT ARXIV, DOI [10.48550/arXiv.1511.06292, DOI 10.48550/ARXIV.1511.06292]; Moosavi-Dezfooli Seyed-Mohsen, 2016, IEEE C COMP VIS PATT; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; Sabour S., 2016, INT C LEARN REPR ICL; Simonyan K., 2014, VERY DEEP CONVOLUTIO; Szegedy Christian, 2014, PROC 2 INT C LEARN R; Xu H, 2009, J MACH LEARN RES, V10, P1485; Zhao Q., 2016, ARXIV160305145	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702078
C	Feldman, M; Koren, T; Livni, R; Mansour, Y; Zohar, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Feldman, Michal; Koren, Tomer; Livni, Roi; Mansour, Yishay; Zohar, Aviv			Online Pricing with Strategic and Patient Buyers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MECHANISM DESIGN	We consider a seller with an unlimited supply of a single good, who is faced with a stream of T buyers. Each buyer has a window of time in which she would like to purchase, and would buy at the lowest price in that window, provided that this price is lower than her private value (and otherwise, would not buy at all). In this setting, we give an algorithm that attains O(T-2/3) regret over any sequence of T buyers with respect to the best fixed price in hindsight, and prove that no algorithm can perform better in the worst case.	[Feldman, Michal; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Feldman, Michal] MSR Herzliya, Herzliyya, Israel; [Koren, Tomer] Google Brain, Mountain View, CA USA; [Livni, Roi] Princeton Univ, Princeton, NJ 08544 USA; [Zohar, Aviv] Hebrew Univ Jerusalem, Jerusalem, Israel; [Koren, Tomer; Livni, Roi; Mansour, Yishay; Zohar, Aviv] Microsoft Res, Herzliyya, Israel	Tel Aviv University; Google Incorporated; Princeton University; Hebrew University of Jerusalem	Feldman, M (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.; Feldman, M (corresponding author), MSR Herzliya, Herzliyya, Israel.	michal.feldman@cs.tau.ac.il; tkoren@google.com; rlivni@cs.princeton.edu; mansour@tau.ac.il; avivz@cs.huji.ac.il						Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Arora Raman, 2012, ARXIV12066400; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Balcan MF, 2008, J COMPUT SYST SCI, V74, P1245, DOI 10.1016/j.jcss.2007.08.002; Chawla S, 2010, ACM S THEORY COMPUT, P311; Chawla S, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P243; Chawla Shuchi, 2010, 11 ACM C EL COMM EC; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Dekel O, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P459, DOI 10.1145/2591796.2591868; Huang Zhiyi, 2015, 16 ACM C EC COMP EC, P45; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; MOHRI M, 2015, ADV NEURAL INFORM PR, P2530; Mohri Mehryar, 2014, ADV NEURAL INFORM PR, P1871	13	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703051
C	Fernandez, T; Rivera, N; Teh, YW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Fernandez, Tamara; Rivera, Nicolas; Teh, Yee Whye			Gaussian Processes for Survival Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce a semi-parametric Bayesian model for survival analysis. The model is centred on a parametric baseline hazard, and uses a Gaussian process to model variations away from it nonparametrically, as well as dependence on covariates. As opposed to many other methods in survival analysis, our framework does not impose unnecessary constraints in the hazard rate or in the survival function. Furthermore, our model handles left, right and interval censoring mechanisms common in survival analysis. We propose a MCMC algorithm to perform inference and an approximation scheme based on random Fourier features to make computations faster. We report experimental results on synthetic and real data, showing that our model performs better than competing models such as Cox proportional hazards, ANOVA-DDP and random survival forests.	[Fernandez, Tamara; Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England; [Rivera, Nicolas] Kings Coll London, Dept Informat, London, England	University of Oxford; University of London; King's College London	Fernandez, T (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	fernandez@stats.ox.ac.uk; nicolas.rivera@kcl.ac.uk; y.w.teh@stats.ox.ac.uk	Rivera, Nicolas/AAO-3838-2020		European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant [617071]; Becas CHILE	European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant(European Research Council (ERC)); Becas CHILE	YWT's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071. Tamara Fernandez and Nicolas Rivera were supported by funding from Becas CHILE.	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Barrett J. E., 2013, ARXIV13121591; COX DR, 1972, J R STAT SOC B, V34, P187; De Iorio M, 2009, BIOMETRICS, V65, P762, DOI 10.1111/j.1541-0420.2008.01166.x; DOKSUM K, 1974, ANN PROBAB, V2, P183, DOI 10.1214/aop/1176996703; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; DYKSTRA RL, 1981, ANN STAT, V9, P356, DOI 10.1214/aos/1176345401; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; Hjort N. L., 2010, BAYESIAN NONPARAMETR, V28; Ishwaran H, 2008, ANN APPL STAT, V2, P841, DOI 10.1214/08-AOAS169; Joensuu H, 2014, RADIOLOGY, V271, P96, DOI 10.1148/radiol.13131040; Joensuu H, 2012, LANCET ONCOL, V13, P265, DOI 10.1016/S1470-2045(11)70299-6; KAPLAN EL, 1958, J AM STAT ASSOC, V53, P457, DOI 10.2307/2281868; Martino S, 2011, SCAND J STAT, V38, P514, DOI 10.1111/j.1467-9469.2010.00715.x; Murray I., 2010, JMLR W CP, V9, P541; Murray I., 2010, ADV NEURAL INFORM PR, V23, P1732, DOI DOI 10.5555/2997046.2997089; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rao V. A., 2011, ADV NEURAL INFORM PR, P2474; Therneau TM, 2016, PACKAGE SURVIVAL; Walker S, 1997, ANN STAT, V25, P1762	20	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703079
C	Foster, DJ; Li, ZY; Lykouris, T; Sridharan, K; Tardos, E		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Foster, Dylan J.; Li, Zhiyuan; Lykouris, Thodoris; Sridharan, Karthik; Tardos, Eva			Learning in Games: Robustness of Fast Convergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BOUNDS	We show that learning algorithms satisfying a low approximate regret property experience fast convergence to approximate optimality in a large class of repeated games. Our property, which simply requires that each learner has small regret compared to a (1 + epsilon)-multiplicative approximation to the best action in hindsight, is ubiquitous among learning algorithms; it is satisfied even by the vanilla Hedge forecaster. Our results improve upon recent work of Syrgkanis et al. [28] in a number of ways. We require only that players observe payoffs under other players' realized actions, as opposed to expected payoffs. We further show that convergence occurs with high probability, and show convergence under bandit feedback. Finally, we improve upon the speed of convergence by a factor of n, the number of players. Both the scope of settings and the class of algorithms for which our analysis provides fast convergence are considerably broader than in previous work. Our framework applies to dynamic population games via a low approximate regret property for shifting experts. Here we strengthen the results of Lykouris et al. [19] in two ways: We allow players to select learning algorithms from a larger class, which includes a minor variant of the basic Hedge algorithm, and we increase the maximum churn in players for which approximate optimality is achieved. In the bandit setting we present a new algorithm which provides a "small loss"-type bound with improved dependence on the number of actions in utility settings, and is both simple and efficient. This result may be of independent interest.	[Foster, Dylan J.; Lykouris, Thodoris; Sridharan, Karthik; Tardos, Eva] Cornell Univ, Ithaca, NY 14853 USA; [Li, Zhiyuan] Tsinghua Univ, Beijing, Peoples R China	Cornell University; Tsinghua University	Foster, DJ (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	djfoster@cs.cornell.edu; lizhiyuan13@mails.tsinghua.edu.cn; teddlyk@cs.cornell.edu; sridharan@cs.cornell.edu; eva@cs.cornell.edu	li, zhiyuan/HGD-9581-2022		NSF [CDSE-MSS 1521544, CCF-1563714]; ONR [N00014-08-1-0031]; Google faculty research award; NDSEG fellowship	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Google faculty research award(Google Incorporated); NDSEG fellowship	Work supported in part under NSF grants CDS&E-MSS 1521544, CCF-1563714, ONR grant N00014-08-1-0031, a Google faculty research award, and an NDSEG fellowship.	Abernethy Jacob D, 2009, COMPETING DARK EFFIC; Allenberg Chamy, 2006, HANNAN CONSISTENCY O, P229; [Anonymous], 2005, THESIS; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Christodoulou G., 2005, P 37 ANN ACM S THEOR, P67, DOI DOI 10.1145/1060590.1060600; Daskalakis C, 2015, GAME ECON BEHAV, V92, P327, DOI 10.1016/j.geb.2014.01.003; Foster Dylan J, 2015, ADV NEURAL INFORM PR, P3357; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hazan E., 2009, P 26 ANN INT C MACHI, P393; Hazan E., 2016, INTRO ONLINE CONVEX; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; Kakade SM, 2009, SIAM J COMPUT, V39, P1088, DOI 10.1137/070701704; Koolen Wouter M, 2015, COLT, P1155; Koutsoupias E, 2009, COMPUT SCI REV, V3, P65, DOI 10.1016/j.cosrev.2009.04.003; Lykouris T., 2016, P 27 ANN ACM SIAM S, P120; Neu G., 2015, C LEARN THEOR, V40, P1360; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Roughgarden T, 2002, J ACM, V49, P236, DOI 10.1145/506147.506153; Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883; Roughgarden Tim, 2016, PRICE ANARCHY AUCTIO; Schapire, 2015, P 28 C LEARN THEOR, P1286; Steinhardt Jacob., 2014, ICML, P1593; Syrgkanis V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P211; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; Yaroshinsky R, 2004, MACH LEARN, V55, P271, DOI 10.1023/B:MACH.0000027784.72823.e4	30	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700097
C	Gao, WH; Oh, S; Viswanath, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gao, Weihao; Oh, Sewoong; Viswanath, Pramod			Breaking the Bandwidth Barrier: Geometrical Adaptive Entropy Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				DENSITIES	Estimators of information theoretic measures such as entropy and mutual information are a basic workhorse for many downstream applications in modern data science. State of the art approaches have been either geometric (nearest neighbor (NN) based) or kernel based (with a globally chosen bandwidth). In this paper, we combine both these approaches to design new estimators of entropy and mutual information that outperform state of the art methods. Our estimator uses local bandwidth choices of k-NN distances with a finite k, independent of the sample size. Such a local and data dependent choice improves performance in practice, but the bandwidth is vanishing at a fast rate, leading to a non-vanishing bias. We show that the asymptotic bias of the proposed estimator is universal; it is independent of the underlying distribution. Hence, it can be precomputed and subtracted from the estimate. As a byproduct, we obtain a unified way of obtaining both kernel and NN estimators. The corresponding theoretical contribution relating the asymptotic geometry of nearest neighbors to order statistics is of independent mathematical interest.	[Gao, Weihao; Oh, Sewoong; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA; [Gao, Weihao; Viswanath, Pramod] Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA; [Oh, Sewoong] Univ Illinois, Dept Ind & Enterprise Syst Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign; University of Illinois System; University of Illinois Urbana-Champaign	Gao, WH (corresponding author), Univ Illinois, Coordinated Sci Lab, Urbana, IL 61801 USA.; Gao, WH (corresponding author), Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA.	wgao9@illinois.edu; swoh@illinois.edu; pramodv@illinois.edu			NSF SaTC award [CNS-1527754]; NSF CISE award [CCF-1553452, CCF-1617745]	NSF SaTC award(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CISE award	This work is supported by NSF SaTC award CNS-1527754, NSF CISE award CCF-1553452, NSF CISE award CCF-1617745. We thank the anonymous reviewers for their constructive feedback.	Biau G., 2016, LECT NEAREST NEIHBOR; Galstyan A, 2014, ADV NEURAL INFORM PR, P577; Gao S., 2015, 31 C UNC ART INT UAI; Gao S., 2015, INT C ART INT STAT A; Gao W., 2016, ARXIV160403006; Goria MN, 2005, J NONPARAMETR STAT, V17, P277, DOI 10.1080/104852504200026815; Hjort NL, 1996, ANN STAT, V24, P1619; JOE H, 1989, ANN I STAT MATH, V41, P683, DOI 10.1007/BF00057735; Kozachenko L.F., 1987, PROBL PEREDACHI INF, V23, P9; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689; Leonenko N, 2008, ANN STAT, V36, P2153, DOI 10.1214/07-AOS539; Loader C., 2006, LOCAL REGRESSION LIK; Loader CR, 1996, ANN STAT, V24, P1602; Lombardi D, 2016, PHYS REV E, V93, DOI 10.1103/PhysRevE.93.013310; Manning C. D., 2008, INTRO INFORM RETRIEV, V1; Reiss R.-D., 2012, APPROXIMATE DISTRIBU; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Sheather SJ, 2004, STAT SCI, V19, P588, DOI 10.1214/088342304000000297; Singh S., 2016, ARXIV103008578; Steeg G. Ver, 2016, ICML; Tsybakov AB, 1996, SCAND J STAT, V23, P75; Vincent P., 2003, LOCALLY WEIGHTED FUL; Wang Q, 2008, FOUND TRENDS COMMUN, V5, DOI 10.1561/0100000021; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702046
C	Garg, VK; Jaakkola, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Garg, Vikas K.; Jaakkola, Tommi			Learning Tree Structured Potential Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many real phenomena, including behaviors, involve strategic interactions that can be learned from data. We focus on learning tree structured potential games where equilibria are represented by local maxima of an underlying potential function. We cast the learning problem within a max margin setting and show that the problem is NP-hard even when the strategic interactions form a tree. We develop a variant of dual decomposition to estimate the underlying game and demonstrate with synthetic and real decision/voting data that the game theoretic perspective (carving out local maxima) enables meaningful recovery.	[Garg, Vikas K.; Jaakkola, Tommi] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Garg, VK (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	vgarg@csail.mit.edu; tommi@csail.mit.edu						Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Bradley J., 2010, ICML; Dubey P, 2006, GAME ECON BEHAV, V54, P77, DOI 10.1016/j.geb.2004.10.007; Hoefer M., 2012, LNCS, V7695, P364; Honorio J, 2015, J MACH LEARN RES, V16, P1157; Irfan MT, 2014, ARTIF INTELL, V215, P79, DOI 10.1016/j.artint.2014.06.004; Kearns M., 2001, UAI; LAFFERTY J, 2001, ICML; Martins A. F., 2011, EMNLP; Martins A. F. T., 2010, NIPS; Meshi O., 2013, UAI; Monderer D, 1996, GAME ECON BEHAV, V14, P124, DOI 10.1006/game.1996.0044; Nowozin Sebastian, 2011, FDN TRENDS COMPUTER; Rush AM, 2012, J ARTIF INTELL RES, V45, P305, DOI 10.1613/jair.3680; Rush Alexander M., 2010, EMNLP; Samdani R., 2012, ICML; Song Y., 2011, MOBICOM; Sontag D., 2010, NIPS; Taskar Ben, 2003, NIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Ui T, 2001, ECONOMETRICA, V69, P1373, DOI 10.1111/1468-0262.00246	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701009
C	Greff, K; Rasmus, A; Berglund, M; Hao, TH; Schmidhuber, J; Valpola, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Greff, Klaus; Rasmus, Antti; Berglund, Mathias; Hao, Tele Hotloo; Schmidhuber, Jurgen; Valpola, Harri			Tagger: Deep Unsupervised Perceptual Grouping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				COMPETITION; ATTENTION; BINDING; MODEL	We present a framework for efficient perceptual inference that explicitly reasons about the segmentation of its inputs and features. Rather than being trained for any specific segmentation, our framework learns the grouping process in an unsupervised manner or alongside any supervised task. We enable a neural network to group the representations of different objects in an iterative manner through a differentiable mechanism. We achieve very fast convergence by allowing the system to amortize the joint iterative inference of the groupings and their representations. In contrast to many other recently proposed methods for addressing multi-object scenes, our system does not assume the inputs to be images and can therefore directly handle other modalities. We evaluate our method on multi-digit classification of very cluttered images that require texture segmentation. Remarkably our method achieves improved classification performance over convolutional networks despite being fully connected, by making use of the grouping mechanism. Furthermore, we observe that our system greatly improves upon the semi-supervised result of a baseline Ladder network on our dataset. These results are evidence that grouping is a powerful tool that can help to improve sample efficiency.	[Rasmus, Antti; Berglund, Mathias; Hao, Tele Hotloo; Valpola, Harri] Curious AI Co, Helsinki, Finland; [Greff, Klaus; Schmidhuber, Jurgen] IDSIA, Manno, Switzerland	Universita della Svizzera Italiana	Greff, K (corresponding author), IDSIA, Manno, Switzerland.	klaus@idsia.ch; antti@cai.fi; mathias@cai.fi; hotloo@cai.fi; juergen@idsia.ch; harri@cai.fi			EU [687795]	EU(European Commission)	The authors wish to acknowledge useful discussions with Theofanis Karaletsos, Jaakko Sarela, Tapani Raiko, and Soren Kaae Sonderby. And further acknowledge Rinu Boney, Timo Haanpaa and the rest of the Curious AI Company team for their support, computational infrastructure, and human testing. This research was supported by the EU project "INPUT" (H2020-ICT-2015 grant no. 687795).	[Anonymous], 2010, P INT C MACH LEARN; Ba J. L., 2016, ARXIV160706450CSSTAT; Bahdanau Dzmitry, 2015, NEURAL MACHINE TRANS; Deco G, 2001, LECT NOTES ARTIF INT, V2036, P114; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Eslami S., 2016, ARXIV160308575; Gallinari P., 1987, CESTA AFCET; Goodfellow I. J., 2013, ARXIV13126082; Greff K., 2015, ARXIV151106418CS; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Huang Jonathan, 2015, ICLR WORKSH; Hyvarinen A, 2006, IEEE IJCNN, P4167; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kingma D., 2015, METHOD STOCHASTIC OP; Le Roux N, 2011, NEURAL COMPUT, V23, P593, DOI 10.1162/NECO_a_00086; LeCun Y., 1987, THESIS U P M CURIE P; Meier M, 2014, NEUROCOMPUTING, V141, P76, DOI 10.1016/j.neucom.2014.02.011; RAO AR, 2008, NEURAL NETWORKS IEEE, V19, P168, DOI DOI 10.1109/TNN.2007.905852; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Reichert D. P., 2013, ARXIV13126115CSQBIOS; Reichert DP, 2011, LECT NOTES COMPUT SC, V6791, P18, DOI 10.1007/978-3-642-21735-7_3; Ross DA, 2006, J MACH LEARN RES, V7, P2369; SAUND E, 1995, NEURAL COMPUT, V7, P51, DOI 10.1162/neco.1995.7.1.51; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P131, DOI 10.1162/neco.1992.4.1.131; Schmidhuber J., 1993, INT C ART NEUR NETW, P460; Schmidhuber J., 1991, INT J NEURAL SYST, V2, P125, DOI [10.1142/S012906579100011X, DOI 10.1142/S012906579100011X]; Schmidhuber J., 1993, INT C ART NEUR NETW, P446; Sohn K., 2013, P 30 INT C MACH LEAR, P217; Springenberg J.T., 2014, ARXIV14126806; Srikumar V, 2012, P 2012 JOINT C EMP M, P1114; Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936; Team The Theano Development, 2016, ARXIV160502688CS; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vinh NX, 2010, J MACH LEARN RES, V11, P2837; von der Malsburg C, 1981, CORRELATION THEORY B; VONDERMALSBURG C, 1995, CURR OPIN NEUROBIOL, V5, P520, DOI 10.1016/0959-4388(95)80014-X; Wersing H, 2001, NEURAL COMPUT, V13, P357, DOI 10.1162/089976601300014574; Xu K, 2015, ARXIV150203044; Yli-Krekola A, 2009, LECT NOTES COMPUT SC, V5769, P285, DOI 10.1007/978-3-642-04277-5_29	40	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700031
C	Grosse, RB; Ancha, S; Roy, DM		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Grosse, Roger B.; Ancha, Siddharth; Roy, Daniel M.			Measuring the reliability of MCMC inference with bidirectional Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CONVERGENCE	Markov chain Monte Carlo (MCMC) is one of the main workhorses of probabilistic inference, but it is notoriously hard to measure the quality of approximate posterior samples. This challenge is particularly salient in black box inference methods, which can hide details and obscure inference failures. In this work, we extend the recently introduced bidirectional Monte Carlo [GGA15] technique to evaluate MCMC-based posterior inference algorithms. By running annealed importance sampling (AIS) chains both from prior to posterior and vice versa on simulated data, we upper bound in expectation the symmetrized KL divergence between the true posterior distribution and the distribution of approximate samples. We integrate our method into two probabilistic programming languages, WebPPL [GS] and Stan [CGHL+ p], and validate it on several models and datasets. As an example of how our method be used to guide the design of inference algorithms, we apply it to study the effectiveness of different model representations in WebPPL and Stan.	[Grosse, Roger B.; Ancha, Siddharth] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada; [Roy, Daniel M.] Univ Toronto, Dept Stat, Toronto, ON, Canada	University of Toronto; University of Toronto	Grosse, RB (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.							Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Burda Y., 2015, ARTIFICIAL INTELLIGE; Carpenter B., J STAT SOFTWARE; Cowles MK, 1996, J AM STAT ASSOC, V91, P883, DOI 10.2307/2291683; Cusumano-Towner M. F., 2016, ARXIV160600068; Del Moral P, 2006, J R STAT SOC B, V68, P411, DOI 10.1111/j.1467-9868.2006.00553.x; Efron B, 1998, INTRO BOOTSTRAP; Gelman A, 1992, STAT SCI, V7, P136, DOI 10.1214/ss/1177011136; Gogate V., 2007, C UNC AI; Goodman N. D., 2008, C UNC AI; Goodman N. D., DESIGN IMPLEMENTATIO; Gorham J., 2015, NEURAL INFORM PROCES; Grosse R. B., 2013, NEURAL INFORM PROCES; Grosse R. B., 2015, ARXIV151102543; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Jarzynski C, 1997, PHYS REV E, V56, P5018, DOI 10.1103/PhysRevE.56.5018; Lunn DJ, 2000, STAT COMPUT, V10, P325, DOI 10.1023/A:1008929526011; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal RM, 1996, STAT COMPUT, V6, P353, DOI 10.1007/BF00143556; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Roberts GO, 2001, STAT SCI, V16, P351, DOI 10.1214/ss/1015346320; Stan Development Team, STAN MOD LANG US GUI	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702037
C	Grover, A; Ermon, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Grover, Aditya; Ermon, Stefano			Variational Bayes on Monte Carlo Steroids	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Variational approaches are often used to approximate intractable posteriors or normalization constants in hierarchical latent variable models. While often effective in practice, it is known that the approximation error can be arbitrarily large. We propose a new class of bounds on the marginal log-likelihood of directed latent variable models. Our approach relies on random projections to simplify the posterior. In contrast to standard variational methods, our bounds are guaranteed to be tight with high probability. We provide a new approach for learning latent variable models based on optimizing our new bounds on the log-likelihood. We demonstrate empirical improvements on benchmark datasets in vision and language for sigmoid belief networks, where a neural network is used to approximate the posterior.	[Grover, Aditya; Ermon, Stefano] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Grover, A (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	adityag@cs.stanford.edu; ermon@cs.stanford.edu			NSF [1649208]; Future of Life Institute [2016-158687]	NSF(National Science Foundation (NSF)); Future of Life Institute	This work was supported by grants from the NSF (grant 1649208) and Future of Life Institute (grant 2016-158687).	[Anonymous], 2014, ICLR; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bouchard-Cote A., 2009, UAI; Burda Y., 2016, ICLR; Domingos P, 2011, UAI; Ermon S., 2014, ICML; Ermon S., 2013, UAI; Ermon Stefano, 2013, ICML; Gan Z., 2015, AISTATS; Gershman S., 2014, P ANN M COGN SCI SOC, V36; Gregor K., 2014, ICML; Hadjis Stefan, 2014, UAI; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; HINTON GE, 1995, SCIENCE, V268, P1158, DOI 10.1126/science.7761831; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hsu L.-K., 2016, AISTATS; Kingma D.P., 2015, INT C LEARN REPR, P1; Lee H., 2009, ICML; Mnih A., 2016, ICML; Mnih A., 2014, ICML; NEAL RM, 1992, ARTIF INTELL, V56, P71, DOI 10.1016/0004-3702(92)90065-6; Ranganath R., 2013, AISTATS; REZENDE DJ, 2015, ICML; Salimans T., 2015, ICML; Titsias M., 2015, NIPS; Vadhan S. P., 2011, FDN TRENDS TCS; Zhu M., 2015, ICML	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702006
C	Harris, KD; Mihalas, S; Shea-Brown, E		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Harris, Kameron Decker; Mihalas, Stefan; Shea-Brown, Eric			High resolution neural connectivity from incomplete tracing data using nonnegative spline regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				AREA	Whole-brain neural connectivity data are now available from viral tracing experiments, which reveal the connections between a source injection site and elsewhere in the brain. These hold the promise of revealing spatial patterns of connectivity throughout the mammalian brain. To achieve this goal, we seek to fit a weighted, nonnegative adjacency matrix among 100 mu m brain "voxels" using viral tracer data. Despite a multi-year experimental effort, injections provide incomplete coverage, and the number of voxels in our data is orders of magnitude larger than the number of injections, making the problem severely underdetermined. Furthermore, projection data are missing within the injection site because local connections there are not separable from the injection signal. We use a novel machine-learning algorithm to meet these challenges and develop a spatially explicit, voxel-scale connectivity map of the mouse visual system. Our method combines three features: a matrix completion loss for missing data, a smoothing spline penalty to regularize the problem, and (optionally) a low rank factorization. We demonstrate the consistency of our estimator using synthetic data and then apply it to newly available Allen Mouse Brain Connectivity Atlas data for the visual system. Our algorithm is significantly more predictive than current state of the art approaches which assume regions to be homogeneous. We demonstrate the efficacy of a low rank version on visual cortex data and discuss the possibility of extending this to a whole-brain connectivity matrix at the voxel scale.	[Harris, Kameron Decker] Univ Washington, Appl Math, Seattle, WA 98195 USA; [Mihalas, Stefan; Shea-Brown, Eric] Univ Washington, Appl Math, Allen Inst Brain Sci, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; Allen Institute for Brain Science; University of Washington; University of Washington Seattle	Harris, KD (corresponding author), Univ Washington, Appl Math, Seattle, WA 98195 USA.	kamdh@uw.edu; stefanm@alleninstitute.org; etsb@uw.edu			UW NIH; Boeing Scholarship; NSF [DMS-1122106, 1514743]; Simons Fellowship in Mathematics	UW NIH; Boeing Scholarship; NSF(National Science Foundation (NSF)); Simons Fellowship in Mathematics	We acknowledge the support of the UW NIH Training Grant in Big Data for Neuroscience and Genetics (KDH), Boeing Scholarship (KDH), NSF Grant DMS-1122106 and 1514743 (ESB & KDH), and a Simons Fellowship in Mathematics (ESB). We thank Liam Paninski for helpful insights at the outset of this project. We wish to thank the Allen Institute founders, Paul G. Allen and Jody Allen, for their vision, encouragement, and support. This work was facilitated though the use of advanced computational, storage, and networking infrastructure provided by the Hyak supercomputer system at the University of Washington.	Argyriou A, 2009, J MACH LEARN RES, V10, P2507; Bock DD, 2011, NATURE, V471, P177, DOI 10.1038/nature09802; Boyd S, 2004, CONVEX OPTIMIZATION; BYRD RH, 1995, SIAM J SCI COMPUT, V16, P1190, DOI 10.1137/0916069; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Chaplin TA, 2013, J COMP NEUROL, V521, P1001, DOI 10.1002/cne.23215; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Garrett ME, 2014, J NEUROSCI, V34, P12587, DOI 10.1523/JNEUROSCI.1124-14.2014; Glickfeld L. L., 2013, NATURE NEUROSCIENCE, V16; GOODMAN CS, 1993, CELL, V72, P77, DOI 10.1016/S0092-8674(05)80030-3; Hubel D. H., 1962, J PHYSL, V160; Jenett A, 2012, CELL REP, V2, P991, DOI 10.1016/j.celrep.2012.09.011; Jonas E, 2015, ELIFE, V4, DOI 10.7554/eLife.04250; Kleinfeld D, 2011, J NEUROSCI, V31, P16125, DOI 10.1523/JNEUROSCI.4077-11.2011; Kuan L, 2015, METHODS, V73, P4, DOI 10.1016/j.ymeth.2014.12.013; Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756; LYNCH RE, 1964, B AM MATH SOC, V70, P378, DOI 10.1090/S0002-9904-1964-11105-8; Oh SW, 2014, NATURE, V508, P207, DOI 10.1038/nature13186; Peng H., 2014, NATURE COMMUNICATION, V5; Rosa MGP, 2005, PHILOS T R SOC B, V360, P665, DOI 10.1098/rstb.2005.1626; Sporns O., 2016, NETWORKS BRAIN; UDIN SB, 1988, ANNU REV NEUROSCI, V11, P289, DOI 10.1146/annurev.ne.11.030188.001445; Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9; Wahba G., 1990, SPLINE MODELS OBSERV; Wang QX, 2007, J COMP NEUROL, V502, P339, DOI 10.1002/cne.21286; WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056	26	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701101
C	He, B; De Sa, C; Mitliagkas, I; Re, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		He, Bryan; De Sa, Christopher; Mitliagkas, Ioannis; Re, Christopher			Scan Order in Gibbs Sampling: Models in Which it Matters and Bounds on How Much	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				SYSTEMATIC SCAN	Gibbs sampling is a Markov Chain Monte Carlo sampling technique that iteratively samples variables from their conditional distributions. There are two common scan orders for the variables: random scan and systematic scan. Due to the benefits of locality in hardware, systematic scan is commonly used, even though most statistical guarantees are only for random scan. While it has been conjectured that the mixing times of random scan and systematic scan do not differ by more than a logarithmic factor, we show by counterexample that this is not the case, and we prove that that the mixing times do not differ by more than a polynomial factor under mild conditions. To prove these relative bounds, we introduce a method of augmenting the state space to study systematic scan using conductance.	[He, Bryan; De Sa, Christopher; Mitliagkas, Ioannis; Re, Christopher] Stanford Univ, Stanford, CA 94305 USA	Stanford University	He, B (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	bryanhe@stanford.edu; cdesa@stanford.edu; imit@stanford.edu; chrismre@stanford.edu			DARPA [FA8750-12-2-0335, FA8750-13-2-0039]; NSF [IIS-1247701, CCF-1111943, CCF-1337375, IIS-1353606, DGE-114747]; DOE [108845]; ONR [N000141210041, N000141310129]; NIH [U54EB020405]; DARPA's SIMPLEX program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; Toshiba	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE)); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); DARPA's SIMPLEX program; Oracle; NVIDIA; Huawei(Huawei Technologies); SAP Labs; Sloan Research Fellowship(Alfred P. Sloan Foundation); Moore Foundation(Gordon and Betty Moore Foundation); American Family Insurance; Google(Google Incorporated); Toshiba	The authors acknowledge the support of: DARPA FA8750-12-2-0335; NSF IIS-1247701; NSF CCF-1111943; DOE 108845; NSF CCF-1337375; DARPA FA8750-13-2-0039; NSF IIS-1353606; ONR N000141210041 and N000141310129; NIH U54EB020405; NSF DGE-114747; DARPA's SIMPLEX program; Oracle; NVIDIA; Huawei; SAP Labs; Sloan Research Fellowship; Moore Foundation; American Family Insurance; Google; and Toshiba. The views and conclusions expressed in this material are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of DARPA, AFRL, NSF, ONR, NIH, or the U.S. Government.	[Anonymous], [No title captured]; [Anonymous], ADV NEURAL INFORM PR; Diaconis P, 2000, MICH MATH J, V48, P157; Diaconis P, 2013, BERNOULLI, V19, P1294, DOI 10.3150/12-BEJSP09; Dyer M, 2006, ANN APPL PROBAB, V16, P185, DOI 10.1214/105051605000000683; Dyer M, 2008, COMB PROBAB COMPUT, V17, P761, DOI 10.1017/S0963548308009437; Finkel JR, 2005, P 43 ANN M ASS COMP; GELFAND AE, 1990, J AM STAT ASSOC, V85, P398, DOI 10.2307/2289776; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Gotovos A., 2015, ADV NEURAL INFORM PR; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hayes T. P., 2006, 47 ANN IEEE S FDN CO; Hellerstein JM, 2012, PROC VLDB ENDOW, V5, P1700, DOI 10.14778/2367502.2367510; Levin D. A., 2009, MARKOV CHAINS MIXING; Lunn D, 2009, STAT MED, V28, P3049, DOI 10.1002/sim.3680; McCallum A, 2009, NIPS; Plummer M., 2003, P 3 INT WORKSH DISTR; ROBERTS G. O., 2015, INT J STAT PROBABILI, V5, P51, DOI DOI 10.5539/IJSP.V5N1P51; Smola A, 2010, PROC VLDB ENDOW, V3, P703, DOI 10.14778/1920841.1920931; Zhang C., 2013, P 2013 ACM SIGMOD IN; Zhang YY, 2001, IEEE T MED IMAGING, V20, P45, DOI 10.1109/42.906424; Zhu SC, 1998, INT J COMPUT VISION, V27, P107, DOI 10.1023/A:1007925832420; [No title captured]	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705003
C	Hjelm, RD; Cho, K; Chung, JY; Salakhutdinov, R; Calhoun, V; Jojic, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Hjelm, R. Devon; Cho, Kyunghyun; Chung, Junyoung; Salakhutdinov, Russ; Calhoun, Vince; Jojic, Nebojsa			Iterative Refinement of the Approximate Posterior for Directed Belief Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				VARIATIONAL INFERENCE; MONTE-CARLO	Variational methods that rely on a recognition network to approximate the posterior of directed graphical models offer better inference and learning than previous methods. Recent advances that exploit the capacity and flexibility in this approach have expanded what kinds of models can be trained. However, as a proposal for the posterior, the capacity of the recognition network is limited, which can constrain the representational power of the generative model and increase the variance of Monte Carlo estimates. To address these issues, we introduce an iterative refinement procedure for improving the approximate posterior of the recognition network and show that training with the refined posterior is competitive with state-of-the-art methods. The advantages of refinement are further evident in an increased effective sample size, which implies a lower variance of gradient estimates.	[Hjelm, R. Devon; Calhoun, Vince] Univ New Mexico, Albuquerque, NM 87131 USA; [Hjelm, R. Devon; Calhoun, Vince] Mind Res Network, Albuquerque, NM 87106 USA; [Cho, Kyunghyun] NYU, Courant Inst, New York, NY 10003 USA; [Cho, Kyunghyun] NYU, Ctr Data Sci, New York, NY 10003 USA; [Chung, Junyoung] Univ Montreal, Montreal, PQ, Canada; [Salakhutdinov, Russ] Carnegie Melon Univ, Pittsburgh, PA USA; [Jojic, Nebojsa] Microsoft Res, Redmond, WA USA	University of New Mexico; Lovelace Respiratory Research Institute; New York University; New York University; Universite de Montreal; Carnegie Mellon University; Microsoft	Hjelm, RD (corresponding author), Univ New Mexico, Albuquerque, NM 87131 USA.; Hjelm, RD (corresponding author), Mind Res Network, Albuquerque, NM 87106 USA.	dhjelm@mrn.org; kyunghyun.cho@nyu.edu; junyoung.chung@umontreal.ca; rsalakhu@cs.toronto.edu; vcalhoun@mrn.org; jojic@microsoft.com	Calhoun, Vince D./ACN-9399-2022	Calhoun, Vince D./0000-0001-9058-0747	Microsoft Research; NIH [P20GM103472]; R01 grant [REB020407]; NSF [1539067]; ONR [N000141512791]; ADeLAIDE grant [FA8750-16C-0130-001]; Facebook; Google; NVidia (GPU Center of Excellence); PIBBS	Microsoft Research(Microsoft); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); R01 grant; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ADeLAIDE grant; Facebook(Facebook Inc); Google(Google Incorporated); NVidia (GPU Center of Excellence); PIBBS	This work was supported by Microsoft Research to RDH under NJ; NIH P20GM103472, R01 grant REB020407, and NSF grant 1539067 to VDC; and ONR grant N000141512791 and ADeLAIDE grant FA8750-16C-0130-001 to RS. KC was supported in part by Facebook, Google (Google Faculty Award 2016) and NVidia (GPU Center of Excellence 2015-2016), and RDH was supported in part by PIBBS.	Bornschein Jorg, 2014, ARXIV14062751; Burda Yuri, 2015, ARXIV150900519; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Doucet A, 2001, STAT ENG IN, P3; Goumas Georgios, 2008, ARXIV151105176, P283; Gregor K., 2013, ARXIV PREPRINT ARXIV, V2; Hinton G., 2012, NEURAL NETWORKS MACH, V264, P1; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kingma D. P., 2013, AUTO ENCODING VARIAT; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Neal Radford M, 1992, ARTIFICIAL INTELLIGE, V56; Oh M-S, 1992, J STAT COMPUT SIMUL, V41, P143, DOI [DOI 10.1080/00949659208810398, 10.1080/00949659208810398]; Paige Brooks, 2016, ARXIV160206701; Raiko Tapani, 2014, ARXIV14062989; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans T, 2015, PR MACH LEARN RES, V37, P1218; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; Tang Yichuan, 2013, ADV NEURAL INFORM PR, P530	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702058
C	Huang, RT; Lattimore, T; Gyorgy, A; Szepesvari, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Huang, Ruitong; Lattimore, Tor; Gyorgy, Andras; Szepesvari, Csaba			Following the Leader and Fast Rates in Linear Prediction: Curved Constraint Sets and Other Regularities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The follow the leader (FTL) algorithm, perhaps the simplest of all online learning algorithms, is known to perform well when the loss functions it is used on are positively curved. In this paper we ask whether there are other "lucky" settings when FTL achieves sublinear, "small" regret. In particular, we study the fundamental problem of linear prediction over a non-empty convex, compact domain. Amongst other results, we prove that the curvature of the boundary of the domain can act as if the losses were curved: In this case, we prove that as long as the mean of the loss vectors have positive lengths bounded away from zero, FTL enjoys a logarithmic growth rate of regret, while, e.g., for polyhedral domains and stochastic data it enjoys finite expected regret. Building on a previously known meta-algorithm, we also get an algorithm that simultaneously enjoys the worst-case guarantees and the bound available for FTL.	[Huang, Ruitong; Szepesvari, Csaba] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Lattimore, Tor] Indiana Univ, Sch Informat & Comp, Bloomington, IN 47405 USA; [Gyorgy, Andras] Imperial Coll London, Dept Elect & Elect Engn, London, England	University of Alberta; Indiana University System; Indiana University Bloomington; Imperial College London	Huang, RT (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	ruitong@ualberta.ca; tor.lattimore@gmail.com; a.gyorgy@imperial.ac.uk; szepesva@ualberta.ca			Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning; NSERC	Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was supported in part by the Alberta Innovates Technology Futures through the Alberta Ingenuity Centre for Machine Learning and by NSERC. During part of this work, T. Lattimore was with the Department of Computing Science, University of Alberta.	Abbasi-Yadkori Y., 2010, FORCED EXPLORATION B; Abernethy J., 2008, 21 ANN C LEARN THEOR; Bartlett PL, 2007, P 21 ANN C ADV NEUR, P65; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Foster Dylan J, 2015, ADV NEURAL INFORM PR, P3357; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Gaivoronski AA, 2000, ANN OPER RES, V100, P165, DOI 10.1023/A:1019271201970; Garber D, 2015, PR MACH LEARN RES, V37, P541; Huang R., 2016, FOLLOWING LEADER FAS; Kotlowski W., 2016, ALGORITHMIC LEARNING; Levitin Evgeny S, 1966, USSR COMP MATH MATH, V6, P1, DOI DOI 10.1016/0041-5553(66)90114-5; McMahan H. B., 2010, FOLLOW THE REGULARIZ; Merhav N., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P413, DOI 10.1145/130385.130430; Orabona F., 2014, P 15 INT C ART INT S, P823; Rakhlin A., 2013, C LEARN THEOR, V30, P993; Sani A., 2014, ADV NEURAL INFORM PR, V27, P810; Schneider R, 2014, ENCYCLOP MATH APPL, V151, P1; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shalev-Shwartz Shai, 2009, ADV NEURAL INFORM PR, P1457; van Erven T, 2015, J MACH LEARN RES, V16, P1793	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703091
C	Jain, P; Rao, N; Dhillon, I		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jain, Prateek; Rao, Nikhil; Dhillon, Inderjit			Structured Sparse Regression via Greedy Hard-thresholding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALGORITHMS	Several learning applications require solving high-dimensional regression problems where the relevant features belong to a small number of (overlapping) groups. For very large datasets and under standard sparsity constraints, hard thresholding methods have proven to be extremely efficient, but such methods require NP hard projections when dealing with overlapping groups. In this paper, we show that such NP-hard projections can not only be avoided by appealing to submodular optimization, but such methods come with strong theoretical guarantees even in the presence of poorly conditioned data (i.e. say when two features have correlation >= 0.99), which existing analyses cannot handle. These methods exhibit an interesting computation-accuracy trade-off and can be extended to significantly harder problems such as sparse overlapping groups. Experiments on both real and synthetic data validate our claims and demonstrate that the proposed methods are orders of magnitude faster than other greedy and convex relaxation techniques for learning with group-structured sparsity.	[Jain, Prateek] Microsoft Res India, Bengaluru, Karnataka, India; [Rao, Nikhil] Technicolor, Issy Les Moulineaux, France; [Dhillon, Inderjit] UT Austin, Austin, TX USA	Technicolor SA; University of Texas System; University of Texas Austin	Jain, P (corresponding author), Microsoft Res India, Bengaluru, Karnataka, India.							Bach F., 2010, ARXIV10104207; Baraniuk RG, 2010, IEEE T INFORM THEORY, V56, P1982, DOI 10.1109/TIT.2010.2040894; Bhan N, 2013, IEEE INT SYMP INFO, P1037, DOI 10.1109/ISIT.2013.6620384; Blumensath T, 2010, IEEE J-STSP, V4, P298, DOI 10.1109/JSTSP.2010.2042411; Dhillon Inderjit S., 2011, ADV NEURAL INFORM PR, P882; Hegde C, 2015, IEEE T INFORM THEORY, V61, P5129, DOI 10.1109/TIT.2015.2457939; Huang JZ, 2011, J MACH LEARN RES, V12, P3371; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Jain P., 2011, ADV NEURAL INF PROCE, P1215; Jenatton R., 2010, P 27 INT C MACH LEAR, P487; Kar P., 2014, ADV NEURAL INFORM PR, P685; Kyrillidis A., 2012, Proceedings of the 2012 IEEE International Symposium on Information Theory - ISIT, P2216, DOI 10.1109/ISIT.2012.6283847; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Rao N., 2013, ADV NEURAL INFORM PR, P2202; Rao N., 2012, P 15 INT C ART INT S, P942; Rao N, 2015, IEEE T SIGNAL PROCES, V63, P5798, DOI 10.1109/TSP.2015.2461515; Shah Parikshit, 2011, 2011 49th Annual Allerton Conference on Communication, Control, and Computing (Allerton), P760; Swirszcz G., 2009, ADV NEURAL INF PROCE, P1150; Yuan XT, 2014, PR MACH LEARN RES, V32, P127	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703061
C	Kazemi, SM; Kimmig, A; Van den Broeck, G; Poole, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kazemi, Seyed Mehran; Kimmig, Angelika; Van den Broeck, Guy; Poole, David			New Liftable Classes for First-Order Probabilistic Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Statistical relational models provide compact encodings of probabilistic dependencies in relational domains, but result in highly intractable graphical models. The goal of lifted inference is to carry out probabilistic inference without needing to reason about each individual separately, by instead treating exchangeable, undistinguished objects as a whole. In this paper, we study the domain recursion inference rule, which, despite its central role in early theoretical results on domain-lifted inference, has later been believed redundant. We show that this rule is more powerful than expected, and in fact significantly extends the range of models for which lifted inference runs in time polynomial in the number of individuals in the domain. This includes an open problem called S4, the symmetric transitivity model, and a first-order logic encoding of the birthday paradox. We further identify new classes (SFO2)-F-2 and (SRU)-R-2 of domain-liftable theories, which respectively subsume FO2 and recursively unary theories, the largest classes of domain-liftable theories known so far, and show that using domain recursion can achieve exponential speedup even in theories that cannot fully be lifted with the existing set of inference rules.	[Kazemi, Seyed Mehran; Poole, David] Univ British Columbia, Vancouver, BC, Canada; [Kimmig, Angelika] Katholieke Univ Leuven, Leuven, Belgium; [Van den Broeck, Guy] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	University of British Columbia; KU Leuven; University of California System; University of California Los Angeles	Kazemi, SM (corresponding author), Univ British Columbia, Vancouver, BC, Canada.	smkazemi@cs.ubc.ca; angelika.kimmig@cs.kuleuven.be; guyvdb@cs.ucla.edu; poole@cs.ubc.ca			Research Foundation Flanders (FWO); NSF [IIS-1633857]	Research Foundation Flanders (FWO)(FWO); NSF(National Science Foundation (NSF))	AK is supported by the Research Foundation Flanders (FWO). GVdB is partially supported by NSF (#IIS-1633857).	Ahmadi Babak, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P585, DOI 10.1007/978-3-642-33460-3_43; Ball W. W. Rouse, 1960, MATH RECREATIONS ESS, P45; Beame P, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P313, DOI 10.1145/2745754.2745760; Braz RD, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1319; Bui Hung Hai, 2013, UAI, P132; Choi Jaesik, 2011, AAAI; Dalvi N, 2007, VLDB J, V16, P523, DOI 10.1007/s00778-006-0004-3; de Raedt L., 2016, SYNTHESIS LECT ARTIF, DOI DOI 10.2200/S00692ED1V01Y201601AIM032; De Raedt L., 2007, IJCAI, V7; den Broeck G.V., 2011, IJCAI 2011 P 22 INT, P2178, DOI [10.5591/978-1-57735-516-8/IJCAI11-363, DOI 10.5591/978-1-57735-516-8/IJCAI11-363]; Domingos P., 2011, UAI, P256; Getoor Lise, 2007, INTRO STAT RELATIONA; Jaeger M., 1997, UAI; Jernite Yacine, 2015, ICML; Jha A., 2010, P 24 ANN C NEUR INF, P973; Kazemi S. M., 2016, KR; Kazemi Seyed Mehran, 2016, ARXIV160604512; Kersting K., 2009, P 25 C UNC ART INT, P277; Koch Christoph, 2011, SYNTHESIS LECT DATA, V2, P1, DOI [10.2200/s00362ed1v01y201105dtm016, DOI 10.2200/S00362ED1V01Y201105DTM016, 10.2200/S00362ED1V01Y201105DTM016]; Koller D., 2009, PROBABILISTIC GRAPHI; Kopp T., 2015, NIPS, P1315; Milch B., 2008, AAAI, P1062; Niepert Mathias, 2012, UAI; Poole D., 2011, ARXIV11074035CSAI; Poole D., 2003, P INT JOINT C ART IN, P985; Singla P., 2008, P 23 AAAI C ART INT, V8, P1094; Taghipour N., 2013, P 16 INT C ART INT S, P572; Van den Broeck G., 2014, KR; Van den Broeck Guy, 2012, UAI; Van Haaren Jan, 2015, MACH LEARN, P1; VandenBroeck G, 2011, ADV NEURAL INFORM PR, V24, P1386; Venugopal Deepak, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8726, P258, DOI 10.1007/978-3-662-44845-8_17; Venugopal D., 2014, ADV NEURAL INFORM PR, P2978	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705017
C	Kim, J; Chen, YC; Balakrishnan, S; Rinaldo, A; Wasserman, L		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kim, Jisu; Chen, Yen-Chi; Balakrishnan, Sivaraman; Rinaldo, Alessandro; Wasserman, Larry			Statistical Inference for Cluster Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A cluster tree provides a highly-interpretable summary of a density function by representing the hierarchy of its high-density clusters. It is estimated using the empirical tree, which is the cluster tree constructed from a density estimator. This paper addresses the basic question of quantifying our uncertainty by assessing the statistical significance of topological features of an empirical cluster tree. We first study a variety of metrics that can be used to compare different trees, analyze their properties and assess their suitability for inference. We then propose methods to construct and summarize confidence sets for the unknown true cluster tree. We introduce a partial ordering on cluster trees which we use to prune some of the statistically insignificant features of the empirical tree, yielding interpretable and parsimonious cluster trees. Finally, we illustrate the proposed methods on a variety of synthetic examples and furthermore demonstrate their utility in the analysis of a Graft-versus-Host Disease (GvHD) data set.	[Kim, Jisu; Balakrishnan, Sivaraman; Rinaldo, Alessandro; Wasserman, Larry] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA; [Chen, Yen-Chi] Univ Washington, Dept Stat, Seattle, WA 98195 USA	Carnegie Mellon University; University of Washington; University of Washington Seattle	Kim, J (corresponding author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.	jisuk1@andrew.cmu.edu; yenchic@uw.edu; siva@stat.cmu.edu; arinaldo@stat.cmu.edu; larry@stat.cmu.edu	Chen, Yen-Chi/AAK-5078-2020					[Anonymous], 2010, ADV NEURAL INFORM PR; Balakrishnan Sivaraman, 2012, ADV NEURAL INFORM PR; Brinkman RR, 2007, BIOL BLOOD MARROW TR, V13, P691, DOI 10.1016/j.bbmt.2007.02.002; Chaudhuri K., 2014, IEEE T INFORM THEORY; Chazal F, 2014, ARXIV14127197; Chen Y.-C., 2015, ARXIV150405438; Chernozhukov V., 2016, ANN PROBABILITY; DONOHO DL, 1988, ANN STAT, V16, P1390, DOI 10.1214/aos/1176351045; Efron B., 1996, P NATL ACAD SCI, V93; Einmahl U, 2005, ANN STAT, V33, P1380, DOI 10.1214/009053605000000129; Eldridge J., 2015, P 28 C LEARN THEOR, P588; FELSENSTEIN J, 1985, EVOLUTION, V39, P783, DOI 10.1111/j.1558-5646.1985.tb00420.x; Genovese CR, 2014, ANN STAT, V42, P1511, DOI 10.1214/14-AOS1218; Guibas L, 2013, DISCRETE COMPUT GEOM, V49, P22, DOI 10.1007/s00454-012-9465-x; Hartigan J. A., 1981, J AM STAT ASS; Klemela JS., 2009, SMOOTHING MULTIVARIA, V737; KPOTUFE S., 2011, P 28 INT C MACH LEAR, P225; Scott DW, 2015, WILEY SER PROBAB ST, P1, DOI 10.1002/9781118575574; Silverman B. W., 1986, DENSITY ESTIMATION S, V26; Stuetzle Werner, 2010, J COMPUTATIONAL GRAP, V19; Wang Y, 2015, 31 INT S COMP GEOM L, V34, P461, DOI [10.4230/LIPIcs.SOCG.2015.461, DOI 10.4230/LIPICS.SOCG.2015.461]; Wasserman L, 2010, ALL STAT CONCISE COU; Wellner J., 2013, WEAK CONVERGENCE EMP	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704033
C	Kingravi, HA; Maske, H; Chowdhary, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kingravi, Hassan A.; Maske, Harshal; Chowdhary, Girish			Kernel Observers: Systems-Theoretic Modeling and Inference of Spatiotemporally Evolving Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				NONSTATIONARY COVARIANCE FUNCTIONS	We consider the problem of estimating the latent state of a spatiotemporally evolving continuous function using very few sensor measurements. We show that layering a dynamical systems prior over temporal evolution of weights of a kernel model is a valid approach to spatiotemporal modeling, and that it does not require the design of complex nonstationary kernels. Furthermore, we show that such a differentially constrained predictive model can be utilized to determine sensing locations that guarantee that the hidden state of the phenomena can be recovered with very few measurements. We provide sufficient conditions on the number and spatial location of samples required to guarantee state recovery, and provide a lower bound on the minimum number of samples required to robustly infer the hidden states. Our approach outperforms existing methods in numerical experiments.	[Kingravi, Hassan A.] Pindrop, Atlanta, GA 30308 USA; [Maske, Harshal; Chowdhary, Girish] Univ Illinois, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Kingravi, HA (corresponding author), Pindrop, Atlanta, GA 30308 USA.	hkingravi@pindrop.com; hmaske2@illinois.edu; girishc@illinois.edu			AFOSR [FA9550-15-1-0146]	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported by AFOSR grant #FA9550-15-1-0146.	Brezis H., 2010, FUNCTIONAL ANAL SOBO; Cressie N, 2011, STAT SPATIO TEMPORAL; Csato L, 2002, NEURAL COMPUT, V14, P641, DOI 10.1162/089976602317250933; Garg S., 2012, P 26 AAAI C ART INT; Higdon D, 1998, ENVIRON ECOL STAT, V5, P173, DOI 10.1023/A:1009666805688; Jayasumana S, 2015, IEEE T PATTERN ANAL; Ma CS, 2003, STAT PROBABIL LETT, V61, P411, DOI 10.1016/S0167-7152(02)00401-7; Mardia KV, 1998, TEST, V7, P217, DOI 10.1007/BF02565111; Paciorek CJ, 2004, ADV NEUR IN, V16, P273; Perez-Cruz F, 2013, IEEE SIGNAL PROC MAG, V30, P40, DOI 10.1109/MSP.2013.2250352; Pfingsten T., 2006, NONSTATIONARY GAUSSI; Plagemann C, 2008, LECT NOTES ARTIF INT, V5212, P204, DOI 10.1007/978-3-540-87481-2_14; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Singh Amarjeet, 2010, 2010 IEEE International Conference on Robotics and Automation (ICRA 2010), P5490, DOI 10.1109/ROBOT.2010.5509934; Wikle CK, 2002, STAT MODEL, V2, P299, DOI 10.1191/1471082x02st036oa; Williams CKI, 2001, ADV NEUR IN, V13, P682; Wonham WM., 1974, LINEAR MULTIVARIABLE, V1, DOI [10.1007/978-3-662-22673-5, DOI 10.1007/978-3-662-22673-5]; Zhou K., 1996, ROBUST OPTIMAL CONTR	20	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701046
C	Kontorovich, A; Sabato, S; Urner, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kontorovich, Aryeh; Sabato, Sivan; Urner, Ruth			Active Nearest-Neighbor Learning in Metric Spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				CLASSIFICATION; CONVERGENCE; BOUNDS; RATES	We propose a pool-based non-parametric active learning algorithm for general metric spaces, called MArgin Regularized Metric Active Nearest Neighbor (MARMANN), which outputs a nearest-neighbor classifier. We give prediction error guarantees that depend on the noisy-margin properties of the input sample, and are competitive with those obtained by previously proposed passive learners. We prove that the label complexity of MARMANN is significantly lower than that of any passive learner with similar error guarantees. Our algorithm is based on a generalized sample compression scheme and a new label-efficient active model-selection procedure.	[Kontorovich, Aryeh; Sabato, Sivan] Ben Gurion Univ Negev, Dept Comp Sci, IL-8499000 Beer Sheva, Israel; [Urner, Ruth] Max Planck Inst Intelligent Syst, Dept Empir Inference, D-72076 Tubingen, Germany	Ben Gurion University; Max Planck Society	Kontorovich, A (corresponding author), Ben Gurion Univ Negev, Dept Comp Sci, IL-8499000 Beer Sheva, Israel.		Kontorovich, Aryeh/X-9225-2019; Kontorovich, Aryeh/AAB-4744-2020; Sabato, Sivan/U-4730-2017	Kontorovich, Aryeh/0000-0001-8038-8671; Sabato, Sivan/0000-0002-7975-0044	Israel Science Foundation [1141/12, 755/15]; Yahoo Faculty award	Israel Science Foundation(Israel Science Foundation); Yahoo Faculty award	Sivan Sabato was partially supported by the Israel Science Foundation (grant No. 555/15). Aryeh Kontorovich was partially supported by the Israel Science Foundation (grants No. 1141/12 and 755/15) and a Yahoo Faculty award. We thank Lee-Ad Gottlieb and Dana Ron for helpful discussions.	Awasthi P., 2013, CORR; Balcan M.-F., 2007, COLT; Balcan MF, 2010, MACH LEARN, V80, P111, DOI 10.1007/s10994-010-5174-y; Balcan MF, 2009, J COMPUT SYST SCI, V75, P78, DOI 10.1016/j.jcss.2008.07.003; Berlind C, 2015, PR MACH LEARN RES, V37, P1870; Castro RM, 2007, LECT NOTES COMPUT SC, V4539, P5, DOI 10.1007/978-3-540-72927-3_3; Chaudhuri Kamalika, 2014, NIPS; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Dasgupta S., 2008, P 25 INT C MACH LEAR, P208, DOI DOI 10.1145/1390156.1390183; Dasgupta S., 2004, ADV NEURAL INFORM PR, P337; Dasgupta Sanjoy, 2012, COLT; Devroye L., 1985, WILEY SERIES PROBABI; Devroye L., 1996, APPL MATH, V31; FIX E, 1989, INT STAT REV, V57, P238, DOI 10.2307/1403797; Gonen A, 2013, J MACH LEARN RES, V14, P2583; Gottlieb L., 2016, ARTIFICIAL INTELLIGE; Gottlieb L.-A., 2010, COLT, P433; Gottlieb LA, 2016, THEOR COMPUT SCI, V620, P105, DOI 10.1016/j.tcs.2015.10.040; Gottlieb LA, 2014, IEEE T INFORM THEORY, V60, P5750, DOI 10.1109/TIT.2014.2339840; Gottlieb LA, 2010, LECT NOTES COMPUT SC, V6302, P192, DOI 10.1007/978-3-642-15369-3_15; Gottlieb Lee-Ad, 2014, ADV NEURAL INFORM PR, P370; Graepel T, 2005, MACH LEARN, V59, P55, DOI 10.1007/s10994-005-0462-7; Hanneke S, 2015, J MACH LEARN RES, V16, P3487; Hanneke S, 2011, ANN STAT, V39, P333, DOI 10.1214/10-AOS843; Kontorovich Aryeh, 2015, AISTATS; Kontorovich Aryeh, 2016, CORR; Kpotufe S., 2011, NIPS; Kpotufe Samory, 2015, C LEARN THEOR, P1176; Krauthgamer Robert, 2004, 15 ANN ACM SIAM S DI, P791; KULKARNI SR, 1995, IEEE T INFORM THEORY, V41, P1028, DOI 10.1109/18.391248; Maurer A., 2009, COLT; McCallum A, 1998, ICML; Urner Ruth, 2013, C LEARN THEOR, P376; von Luxburg U, 2004, J MACH LEARN RES, V5, P669	35	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700064
C	Krishnasamy, S; Sen, R; Johari, R; Shakkottai, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Krishnasamy, Subhashini; Sen, Rajat; Johari, Ramesh; Shakkottai, Sanjay			Regret of Queueing Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				ALLOCATION; POLICIES; RULE	We consider a variant of the multiarmed bandit problem where jobs queue for service, and service rates of different servers may be unknown. We study algorithms that minimize queue-regret: the (expected) difference between the queue-lengths obtained by the algorithm, and those obtained by a "genie"-aided matching algorithm that knows exact service rates. A naive view of this problem would suggest that queue-regret should grow logarithmically: since queue-regret cannot be larger than classical regret, results for the standard MAB problem give algorithms that ensure queue-regret increases no more than logarithmically in time. Our paper shows surprisingly more complex behavior. In particular, the naive intuition is correct as long as the bandit algorithm's queues have relatively long regenerative cycles: in this case queue-regret is similar to cumulative regret, and scales (essentially) logarithmically. However, we show that this "early stage" of the queueing bandit eventually gives way to a "late stage", where the optimal queue-regret scaling is O (1/t). We demonstrate an algorithm that (order-wise) achieves this asymptotic queue-regret, and also exhibits close to optimal switching time from the early stage to the late stage.	[Krishnasamy, Subhashini; Sen, Rajat; Shakkottai, Sanjay] Univ Texas Austin, Austin, TX 78712 USA; [Johari, Ramesh] Stanford Univ, Stanford, CA 94305 USA	University of Texas System; University of Texas Austin; Stanford University	Krishnasamy, S (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.				NSF [CNS-1161868, CNS-1343383, CNS-1320175]; ARO [W911NF-16-1-0377, W911NF-15-1-0227, W911NF-14-1-0387]; US DoT	NSF(National Science Foundation (NSF)); ARO; US DoT	This work is partially supported by NSF Grants CNS-1161868, CNS-1343383, CNS-1320175, ARO grants W911NF-16-1-0377, W911NF-15-1-0227, W911NF-14-1-0387 and the US DoT supported D-STOP Tier 1 University Transportation Center.	Agrawal S., 2011, J MACH LEARN RES, V23, P357; Audibert J.-Y., 2010, COLT 23 C LEARN THEO, P13; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck Sebastien, 2013, ARXIV13021611; BUYUKKOC C, 1985, ADV APPL PROBAB, V17, P237, DOI 10.2307/1427064; Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P245, DOI 10.1145/2745844.2745847; Cox D.R., 1961, QUEUES; Garivier A., 2011, ARXIV11022490; GITTINS JC, 1979, J ROY STAT SOC B MET, V41, P148; Jacko P., 2010, MODERN TRENDS CONTRO, P248; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Kushner H, 2013, HEAVY TRAFFIC ANAL C, V47; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Lott C, 2000, PROBAB ENG INFORM SC, V14, P259, DOI 10.1017/S0269964800143013; Mahajan A, 2008, SIGNALS COMMUN TECHN, P121, DOI 10.1007/978-0-387-49819-5_6; Nino-Mora J, 2007, TOP, V15, P161, DOI 10.1007/s11750-007-0025-0; Nino-Mora J, 2006, QUEUEING SYST, V54, P281, DOI 10.1007/s11134-006-0302-x; Perchet V., 2015, ARXIV150500369; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Salomon A, 2013, J MACH LEARN RES, V14, P187; Scott SL, 2010, APPL STOCH MODEL BUS, V26, P639, DOI 10.1002/asmb.874; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; van Mieghem JA, 1995, ANN APPL PROBAB, V5, P809, DOI 10.1214/aoap/1177004706; Whitt Ward., 1974, LECTURE NOTES EC MAT, V98, P307, DOI DOI 10.1007/978-3-642-80838-8_15	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703006
C	Lei, Q; Zhong, K; Dhillon, IS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lei, Qi; Zhong, Kai; Dhillon, Inderjit S.			Coordinate-wise Power Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we propose a coordinate-wise version of the power method from an optimization viewpoint. The vanilla power method simultaneously updates all the coordinates of the iterate, which is essential for its convergence analysis. However, different coordinates converge to the optimal value at different speeds. Our proposed algorithm, which we call coordinate-wise power method, is able to select and update the most important k coordinates in O(kn) time at each iteration, where n is the dimension of the matrix and k <= n is the size of the active set. Inspired by the "greedy" nature of our method, we further propose a greedy coordinate descent algorithm applied on a non-convex objective function specialized for symmetric matrices. We provide convergence analyses for both methods. Experimental results on both synthetic and real data show that our methods achieve up to 23 times speedup over the basic power method. Meanwhile, due to their coordinate-wise nature, our methods are very suitable for the important case when data cannot fit into memory. Finally, we introduce how the coordinate-wise mechanism could be applied to other iterative methods that are used in machine learning.	[Lei, Qi; Zhong, Kai; Dhillon, Inderjit S.] Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA; [Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin	Lei, Q (corresponding author), Univ Texas Austin, Inst Computat Engn & Sci, Austin, TX 78712 USA.	leiqi@ices.utexas.edu; zhongkai@ices.utexas.edu; inderjit@cs.utexas.edu	Lei, Qi/T-2146-2019		NSF [CCF-1320746, IIS-1546452, CCF-1564000]	NSF(National Science Foundation (NSF))	This research was supported by NSF grants CCF-1320746, IIS-1546452 and CCF-1564000.	[Anonymous], 2014, ADV NEURAL INFORM PR, DOI DOI 10.1080/01621459.1963; Dhillon I. S., 2011, ADV NEURAL INFORM PR, V24, P2160; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hardt M, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P331; Hoare C. A. R., 1961, COMMUN ACM, V4, P321, DOI [10.1145/366622.366647, DOI 10.1145/366622.366647, DOI 10.1145/366622.366644]; Hsieh C.J., 2011, P 17 ACM SIGKDD INT, P1064; Ipsen Ilse, 2005, 7 IMACS INT S IT MET, V5; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Journee M, 2010, J MACH LEARN RES, V11, P517; Kwak H, 2010, P 19 INT C WORLD WID, P591, DOI [10.1145/1772690.1772751, DOI 10.1145/1772690.1772751]; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Nutini J, 2015, PR MACH LEARN RES, V37, P1632; Parlett B. N., 1998, SYMMETRIC EIGENVALUE, V20; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Shamir O, 2015, PR MACH LEARN RES, V37, P144; Si S., 2014, NIPS, P2798; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Yuan XT, 2013, J MACH LEARN RES, V14, P899	19	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700067
C	Lepora, NF		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lepora, Nathan F.			Threshold Learning for Optimal Decision Making	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				BASAL GANGLIA; REWARD RATE; REINFORCEMENT; DISCRIMINATION	Decision making under uncertainty is commonly modelled as a process of competitive stochastic evidence accumulation to threshold (the drift-diffusion model). However, it is unknown how animals learn these decision thresholds. We examine threshold learning by constructing a reward function that averages over many trials to Wald's cost function that defines decision optimality. These rewards are highly stochastic and hence challenging to optimize, which we address in two ways: first, a simple two-factor reward-modulated learning rule derived from Williams' REINFORCE method for neural networks; and second, Bayesian optimization of the reward function with a Gaussian process. Bayesian optimization converges in fewer trials than REINFORCE but is slower computationally with greater variance. The REINFORCE method is also a better model of acquisition behaviour in animals and a similar learning rule has been proposed for modelling basal ganglia function.	[Lepora, Nathan F.] Univ Bristol, Dept Engn Math, Bristol, Avon, England	University of Bristol	Lepora, NF (corresponding author), Univ Bristol, Dept Engn Math, Bristol, Avon, England.	n.lepora@bristol.ac.uk						Balci F, 2011, ATTEN PERCEPT PSYCHO, V73, P640, DOI 10.3758/s13414-010-0049-7; Bogacz R, 2007, NEURAL COMPUT, V19, P442, DOI 10.1162/neco.2007.19.2.442; Bogacz R, 2006, PSYCHOL REV, V113, P700, DOI 10.1037/0033-295X.113.4.700; Bogacz R, 2010, Q J EXP PSYCHOL, V63, P863, DOI 10.1080/17470210903091643; Brochu E, 2010, ARXIV PREPRINT ARXIV; Drugowitsch J, 2012, J NEUROSCI, V32, P3612, DOI 10.1523/JNEUROSCI.4010-11.2012; Frank MJ, 2006, PSYCHOL REV, V113, P300, DOI 10.1037/0033-295X.113.2.300; Gold JI, 2002, NEURON, V36, P299, DOI 10.1016/S0896-6273(02)00971-6; Gold JI, 2007, ANNU REV NEUROSCI, V30, P535, DOI 10.1146/annurev.neuro.29.051605.113038; Lepora NF, 2012, NEURAL COMPUT, V24, P2924, DOI 10.1162/NECO_a_00360; Mayrhofer JM, 2013, J NEUROPHYSIOL, V109, P273, DOI 10.1152/jn.00488.2012; Pelikan M., 2005, HIERARCHICAL BAYESIA, P31, DOI [10.1007/978-3-540-32373-06, DOI 10.1007/978-3-540-32373-06, DOI 10.1007/978-3-540-32373-0_3]; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; RATCLIFF R, 1978, PSYCHOL REV, V85, P59, DOI 10.1037//0033-295X.85.2.59; Ratcliff R, 2008, NEURAL COMPUT, V20, P873, DOI 10.1162/neco.2008.12-06-420; Simen P, 2006, NEURAL NETWORKS, V19, P1013, DOI 10.1016/j.neunet.2006.05.038; Simen P, 2009, J EXP PSYCHOL HUMAN, V35, P1865, DOI 10.1037/a0016926; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Stich KP, 2006, J EXP BIOL, V209, P4802, DOI 10.1242/jeb.02574; Sutton RS, 2000, ADV NEUR IN, V12, P1057; WALD A, 1948, ANN MATH STAT, V19, P326, DOI 10.1214/aoms/1177730197; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704019
C	Li, DN; Yang, K; Wong, WH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Li, Dangna; Yang, Kun; Wong, Wing Hung			Density Estimation via Discrepancy Based Adaptive Sequential Partition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				K-MEANS; ALGORITHM; SEARCH	Given iid observations from an unknown absolute continuous distribution defined on some domain Omega, we propose a nonparametric method to learn a piecewise constant function to approximate the underlying probability density function. Our density estimate is a piecewise constant function defined on a binary partition of Omega. The key ingredient of the algorithm is to use discrepancy, a concept originates from Quasi Monte Carlo analysis, to control the partition process. The resulting algorithm is simple, efficient, and has a provable convergence rate. We empirically demonstrate its efficiency as a density estimation method. We also show how it can be utilized to find good initializations for k-means.	[Li, Dangna] Stanford Univ, ICME, Stanford, CA 94305 USA; [Yang, Kun] Google, Mountain View, CA 94043 USA; [Wong, Wing Hung] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	Stanford University; Google Incorporated; Stanford University	Li, DN (corresponding author), Stanford Univ, ICME, Stanford, CA 94305 USA.	dangna@stanford.edu; kunyang@stanford.edu; whwong@stanford.edu			 [NIH-R01GM109836];  [NSF-DMS1330132];  [NSF-DMS1407557]	; ; 	This work was supported by NIH-R01GM109836, NSF-DMS1330132 and NSF-DMS1407557. The second author's work was done when the author was a graduate student at Stanford University.	Aghaeepour N, 2013, NAT METHODS, V10, P228, DOI [10.1038/NMETH.2365, 10.1038/nmeth.2365]; Arthur D., 2007, P 18 ANN ACMS S, P1027, DOI DOI 10.1145/1283383.1283494; Doerr Carola, 2013, PREPRINT; Fraley C, 1998, SIAM J SCI COMPUT, V20, P270, DOI 10.1137/S1064827596311451; Gnewuch M., 2012, SPRINGER P MATH STAT, V23, P43, DOI [10.1007/978-3-642-27440-4, DOI 10.1007/978-3-642-27440-4]; Gnewuch M, 2012, SIAM J NUMER ANAL, V50, P781, DOI 10.1137/110833865; Gray RM, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P172, DOI 10.1109/SEQUEN.1997.666914; Heinrich Stefan, 2000, ACTA ARITHMETICA WAR, V96, P279; Katsavounidis I, 1994, IEEE SIGNAL PROC LET, V1, P144, DOI 10.1109/97.329844; Kaul M, 2013, 2013 IEEE 14TH INTERNATIONAL CONFERENCE ON MOBILE DATA MANAGEMENT (MDM 2013), VOL 1, P137, DOI 10.1109/MDM.2013.24; Kuipers L., 2012, UNIFORM DISTRIBUTION; Liang JJ, 2001, MATH COMPUT, V70, P337, DOI 10.1090/S0025-5718-00-01203-5; Liu H, 2011, J MACH LEARN RES, V12, P907; Lu L, 2013, J AM STAT ASSOC, V108, P1402, DOI 10.1080/01621459.2013.813389; Owen A. B., 2005, CONT MULTIVARIATE AN, P49; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; Ram P., 2011, P 17 ACM SIGKDD INT, P627; Redmond SJ, 2007, PATTERN RECOGN LETT, V28, P965, DOI 10.1016/j.patrec.2007.01.001; Spitzer MH, 2015, SCIENCE, V349, DOI 10.1126/science.1259425; Su T, 2007, INTELL DATA ANAL, V11, P319, DOI 10.3233/IDA-2007-11402; Wong WH, 2010, ANN STAT, V38, P1433, DOI 10.1214/09-AOS755; Xu Q, 2015, PATTERN RECOGN LETT, V54, P50, DOI 10.1016/j.patrec.2014.11.017	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701074
C	Liang, JW; Fadili, JM; Peyre, G		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Liang, Jingwei; Fadili, Jalal M.; Peyre, Gabriel			A Multi-step Inertial Forward-Backward Splitting Method for Non-convex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PROXIMAL METHOD; ALGORITHM; CONVERGENCE	We propose a multi-step inertial Forward-Backward splitting algorithm for minimizing the sum of two non-necessarily convex functions, one of which is proper lower semi-continuous while the other is differentiable with a Lipschitz continuous gradient. We first prove global convergence of the algorithm with the help of the Kurdyka-Lojasiewicz property. Then, when the non-smooth part is also partly smooth relative to a smooth submanifold, we establish finite identification of the latter and provide sharp local linear convergence analysis. The proposed method is illustrated on several problems arising from statistics and machine learning.	[Liang, Jingwei; Fadili, Jalal M.] Normandie Univ, ENSICAEN, CNRS, GREYC, Caen, France; [Peyre, Gabriel] CNRS, DMA, ENS Paris, Paris, France	Centre National de la Recherche Scientifique (CNRS); Universite de Caen Normandie; Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite	Liang, JW (corresponding author), Normandie Univ, ENSICAEN, CNRS, GREYC, Caen, France.	Jingwei.Liang@greyc.ensicaen.fr; Jalal.Fadili@greyc.ensicaen.fr; Gabriel.Peyre@ens.fr			European Research Council (ERC project SIGMA-Vision)	European Research Council (ERC project SIGMA-Vision)(European Research Council (ERC))	This work was partly supported by the European Research Council (ERC project SIGMA-Vision).	Alvarez F, 2000, SIAM J CONTROL OPTIM, V38, P1102, DOI 10.1137/S0363012998335802; Alvarez F, 2001, SET-VALUED ANAL, V9, P3, DOI 10.1023/A:1011253113155; Attouch H, 2014, SIAM J OPTIMIZ, V24, P232, DOI 10.1137/130910294; Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Bolte J, 2010, T AM MATH SOC, V362, P3319, DOI 10.1090/S0002-9947-09-05048-X; Bot R. I., 2014, EURO J COMPUTATIONAL, P1; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chambolle A., 2015, J OPTIMIZATION THEOR, V166, P1; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P6, DOI 10.1109/TIT.2005.860430; Drusvyatskiy D, 2013, MATH PROGRAM, V140, P5, DOI 10.1007/s10107-012-0624-x; Frankel P, 2015, J OPTIMIZ THEORY APP, V165, P874, DOI 10.1007/s10957-014-0642-3; Le HY, 2013, OPTIM LETT, V7, P731, DOI 10.1007/s11590-012-0456-x; Lewis AS, 2013, SIAM J OPTIMIZ, V23, P74, DOI 10.1137/110852103; Lewis AS, 2008, MATH OPER RES, V33, P216, DOI 10.1287/moor.1070.0291; Lewis AS, 2003, SIAM J OPTIMIZ, V13, P702, DOI 10.1137/S1052623401387623; Lewis AS, 2001, SIAM J MATRIX ANAL A, V23, P368, DOI 10.1137/S089547980036838X; LIANG J., 2014, P ADV NEURAL INFORM, P1970; Liang J., 2015, ARXIV150303703; Lorenz DA, 2015, J MATH IMAGING VIS, V51, P311, DOI 10.1007/s10851-014-0523-2; Moudafi A, 2003, J COMPUT APPL MATH, V155, P447, DOI 10.1016/S0377-0427(02)00906-8; Ochs P, 2014, SIAM J IMAGING SCI, V7, P1388, DOI 10.1137/130942954; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; ROCKAFELLAR RT, 1998, GRUND MATH WISS, V317, P1; vandenDries L., 1998, MATH SOC LECT NOTES, V248	26	1	1	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702032
C	Linderman, SW; Adams, RP; Pillow, JW		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Linderman, Scott W.; Adams, Ryan P.; Pillow, Jonathan W.			Bayesian latent structure discovery from multi-neuron recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				MODELS	Neural circuits contain heterogeneous groups of neurons that differ in type, location, connectivity, and basic response properties. However, traditional methods for dimensionality reduction and clustering are ill-suited to recovering the structure underlying the organization of neural circuits. In particular, they do not take advantage of the rich temporal dependencies in multi-neuron recordings and fail to account for the noise in neural spike trains. Here we describe new tools for inferring latent structure from simultaneously recorded spike train data using a hierarchical extension of a multi-neuron point process model commonly known as the generalized linear model (GLM). Our approach combines the GLM with flexible graph-theoretic priors governing the relationship between latent features and neural connectivity patterns. Fully Bayesian inference via Polya-gamma augmentation of the resulting model allows us to classify neurons and infer latent dimensions of circuit organization from correlated spike trains. We demonstrate the effectiveness of our method with applications to synthetic data and multi-neuron recordings in primate retina, revealing latent patterns of neural types and locations from spike trains alone.	[Linderman, Scott W.] Columbia Univ, New York, NY 10027 USA; [Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA; [Adams, Ryan P.] Twitter, San Francisco, CA USA; [Pillow, Jonathan W.] Princeton Univ, Princeton, NJ 08544 USA	Columbia University; Harvard University; Twitter, Inc.; Princeton University	Linderman, SW (corresponding author), Columbia Univ, New York, NY 10027 USA.	swl2133@columbia.edu; rpa@seas.harvard.edu; pillow@princeton.edu			Simons Foundation [SCGB-418011]; NSF [IIS-1421780]; Alfred P. Sloan Foundation; McKnight Foundation; Simons Collaboration on the Global Brain [SCGB AWD1004351]; NSF CAREER Award [IIS-1150186]; NIMH [MH099611]	Simons Foundation; NSF(National Science Foundation (NSF)); Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); McKnight Foundation; Simons Collaboration on the Global Brain; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NIMH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	We thank E. J. Chichilnisky, A. M. Litke, A. Sher and J. Shlens for retinal data. SWL is supported by the Simons Foundation SCGB-418011. RPA is supported by NSF IIS-1421780 and the Alfred P. Sloan Foundation. JWP was supported by grants from the McKnight Foundation, Simons Collaboration on the Global Brain (SCGB AWD1004351), NSF CAREER Award (IIS-1150186), and NIMH grant MH099611.	Ahrens MB, 2013, NAT METHODS, V10, P413, DOI [10.1038/NMETH.2434, 10.1038/nmeth.2434]; BRILLINGER DR, 1976, BIOL CYBERN, V22, P213, DOI 10.1007/BF00365087; Gerhard F, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003138; Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711; Grewe BF, 2010, NAT METHODS, V7, P399, DOI 10.1038/nmeth.1453; Hoff P. D., 2008, ADV NEURAL INFORM PR, V20, P1; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735; Orbanz P, 2015, IEEE T PATTERN ANAL, V37, P437, DOI 10.1109/TPAMI.2014.2334607; Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002; Pillow J., 2012, ADV NEURAL INF PROCE, V25, P1898; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Prevedel R, 2014, NAT METHODS, V11, P727, DOI [10.1038/NMETH.2964, 10.1038/nmeth.2964]; Sanes JR, 2015, ANNU REV NEUROSCI, V38, P221, DOI 10.1146/annurev-neuro-071714-034120; Soudry D., 2013, ARXIV13093724; Stevenson IH, 2009, IEEE T NEUR SYS REH, V17, P203, DOI 10.1109/TNSRE.2008.2010471; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Vidne M, 2012, J COMPUT NEUROSCI, V33, P97, DOI 10.1007/s10827-011-0376-2; Windle J., 2014, ARXIV14050506	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701042
C	Milan, K; Veness, J; Kirkpatrick, J; Hassabis, D; Koop, A; Bowling, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Milan, Kieran; Veness, Joel; Kirkpatrick, James; Hassabis, Demis; Koop, Anna; Bowling, Michael			The Forget-me-not Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the Forget-me-not Process, an efficient, non-parametric meta-algorithm for online probabilistic sequence prediction for piecewise stationary, repeating sources. Our method works by taking a Bayesian approach to partitioning a stream of data into postulated task-specific segments, while simultaneously building a model for each task. We provide regret guarantees with respect to piecewise stationary data sources under the logarithmic loss, and validate the method empirically across a range of sequence prediction and task identification problems.	[Milan, Kieran; Veness, Joel; Kirkpatrick, James; Hassabis, Demis] Google DeepMind, London, England; [Koop, Anna; Bowling, Michael] Univ Alberta, Edmonton, AB, Canada	Google Incorporated; University of Alberta	Milan, K (corresponding author), Google DeepMind, London, England.	kmilan@google.com; aixi@google.com; kirkpatrick@google.com; demishassabis@google.com; anna@cs.ualberta.ca; bowling@cs.ualberta.ca						Adams R.P., 2007, BAYESIAN ONLINE CHAN; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Collins A., 2012, PLOSBIOLOGY, V10, P1, DOI DOI 10.1371/J0URNAL.PBI0.1001293; Collins AGE, 2013, PSYCHOL REV, V120, P190, DOI 10.1037/a0030852; Donoso M, 2014, SCIENCE, V344, P1481, DOI 10.1126/science.1252254; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gyorgy A, 2012, IEEE T INFORM THEORY, V58, P6709, DOI 10.1109/TIT.2012.2209627; Hazan E., 2009, P 26 ANN INT C MACHI, P393; Hutter M, 2007, THEOR COMPUT SCI, V384, P33, DOI 10.1016/j.tcs.2007.05.016; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; Lattimore Tor, 2013, P 24 INT C ALG LEARN, P324; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Veness J, 2015, AAAI CONF ARTIF INTE, P3016; Veness J, 2013, IEEE DATA COMPR CONF, P321, DOI 10.1109/DCC.2013.40; VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165; Willems F., 1997, P 1997 IEEE INT S IN, P68; Willems FMJ, 1996, IEEE T INFORM THEORY, V42, P2210, DOI 10.1109/18.556608; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012	22	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700019
C	Montavon, G; Muller, KR; Cuturi, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Montavon, Gregoire; Mueller, Klaus-Robert; Cuturi, Marco			Wasserstein Training of Restricted Boltzmann Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GRADIENT	Boltzmann machines are able to learn highly complex, multimodal, structured and multiscale real-world data distributions. Parameters of the model are usually learned by minimizing the Kullback-Leibler (KL) divergence from training samples to the learned model. We propose in this work a novel approach for Boltzmann machine training which assumes that a meaningful metric between observations is known. This metric between observations can then be used to define the Wasserstein distance between the distribution induced by the Boltzmann machine on the one hand, and that given by the training sample on the other hand. We derive a gradient of that distance with respect to the model parameters. Minimization of this new objective leads to generative models with different statistical properties. We demonstrate their practical potential on data completion and denoising, for which the metric between observations plays a crucial role.	[Montavon, Gregoire; Mueller, Klaus-Robert] Tech Univ Berlin, Berlin, Germany; [Cuturi, Marco] Univ Paris Saclay, ENSAE, CREST, St Aubin, France; [Mueller, Klaus-Robert] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea	Technical University of Berlin; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Saclay; Korea University	Montavon, G (corresponding author), Tech Univ Berlin, Berlin, Germany.	gregoire.montavon@tu-berlin.de; klaus-robert.mueller@tu-berlin.de; marco.cuturi@ensae.fr	Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685	Brain Korea 21 Plus Program through the National Research Foundation of Korea - Ministry of Education; DFG [MU 987/17-1]; JSPS [26700002]	Brain Korea 21 Plus Program through the National Research Foundation of Korea - Ministry of Education(Ministry of Education (MOE), Republic of KoreaNational Research Foundation of Korea); DFG(German Research Foundation (DFG)); JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	This work was supported by the Brain Korea 21 Plus Program through the National Research Foundation of Korea funded by the Ministry of Education. This work was also supported by the grant DFG (MU 987/17-1). M. Cuturi gratefully acknowledges the support of JSPS young researcher A grant 26700002. Correspondence to GM, KRM and MC.	ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Bassetti F, 2006, STAT PROBABIL LETT, V76, P1298, DOI 10.1016/j.spl.2006.02.001; Cho K, 2013, NEURAL COMPUT, V25, P805, DOI 10.1162/NECO_a_00397; Courty N., 2016, PATTERN ANAL MACHINE; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; Dahl G. E., 2010, ADV NEURAL INFORM PR, V23, P469, DOI DOI 10.5555/2997189.2997242; Frogner Charlie, 2015, ADV NEURAL INF PROCE, V2, P2053; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Huber P. J., 2011, ROBUST STAT; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Marlin BM., 2010, P 13 INT C ART INT S, V9, P509; Montavon Gregoire, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P621, DOI 10.1007/978-3-642-35289-8_33; Rubner Yossi, 1997, IM UND WORKSH, P661; Salakhutdinov R., 2009, P 12 INT C ART INT S, P448; Srivastava N, 2014, J MACH LEARN RES, V15, P2949; Tang YC, 2012, PROC CVPR IEEE, P2264, DOI 10.1109/CVPR.2012.6247936; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; USDA, 2012, PLANTS DAT; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5	20	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701105
C	Newling, J; Fleuret, F		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Newling, James; Fleuret, Francois			Nested Mini-Batch K-Means	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A new algorithm is proposed which accelerates the mini-batch k-means algorithm of Sculley (2010) by using the distance bounding approach of Elkan (2003). We argue that, when incorporating distance bounds into a mini-batch algorithm, already used data should preferentially be reused. To this end we propose using nested mini-batches, whereby data in a mini-batch at iteration t is automatically reused at iteration t + 1. Using nested mini-batches presents two difficulties. The first is that unbalanced use of data can bias estimates, which we resolve by ensuring that each data sample contributes exactly once to centroids. The second is in choosing mini-batch sizes, which we address by balancing premature fine-tuning of centroids with redundancy induced slow-down. Experiments show that the resulting nmbatch algorithm is very effective, often arriving within 1% of the empirical minimum 100x earlier than the standard mini-batch algorithm.	[Newling, James; Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland; [Newling, James; Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Newling, J (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Newling, J (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	james.newling@idiap.ch; francois.fleuret@idiap.ch			Hasler Foundation [13018 MASH2]	Hasler Foundation	James Newling was funded by the Hasler Foundation under the grant 13018 MASH2.	Agarwal P, 2005, COMBINATORIAL COMPUT, P1; Bottou L., 1995, Advances in Neural Information Processing Systems 7, P585; Celebi ME, 2013, EXPERT SYST APPL, V40, P200, DOI 10.1016/j.eswa.2012.07.021; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Ding YF, 2015, PR MACH LEARN RES, V37, P579; Drake J, 2013, FASTER K MEANS CLUST; Elkan C., 2003, P 20 INT C MACHINE L, V20, P147, DOI DOI 10.1016/0026-2714(92)90278-S; Hamerly G, 2010, P 2010 SIAM INT C DA, V2010, P130, DOI DOI 10.1137/1.9781611972801.12; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pelleg D., 1999, PROC 5 ACM SIGKDD IN, P277, DOI [10.1145/312129.312248, DOI 10.1145/312129.312248]; Phillips S, 2002, LECT NOTES COMPUTER, V2409; Sculley D., 2010, P 19 INT C WORLD WID, P1177, DOI [10.1145/1772690.1772862, DOI 10.1145/1772690.1772862]; WANG J, 2012, COMPUTER VISION PATT, P3037	17	1	1	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704006
C	Norouzi-Fard, A; Bazzi, A; El Halabi, M; Bogunovic, I; Hsieh, YP; Cevher, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Norouzi-Fard, Ashkan; Bazzi, Abbas; El Halabi, Marwa; Bogunovic, Ilija; Hsieh, Ya-Ping; Cevher, Volkan			An Efficient Streaming Algorithm for the Submodular Cover Problem	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We initiate the study of the classical Submodular Cover (SC) problem in the data streaming model which we refer to as the Streaming Submodular Cover (SSC). We show that any single pass streaming algorithm using sublinear memory in the size of the stream will fail to provide any non-trivial approximation guarantees for SSC. Hence, we consider a relaxed version of SSC, where we only seek to find a partial cover. We design the first Efficient bicriteria Submodular Cover Streaming (ESC-Streaming) algorithm for this problem, and provide theoretical guarantees for its performance supported by numerical evidence. Our algorithm finds solutions that are competitive with the near-optimal offline greedy algorithm despite requiring only a single pass over the data stream. In our numerical experiments, we evaluate the performance of ESC-Streaming on active set selection and large-scale graph cover problems.	[Norouzi-Fard, Ashkan; Bazzi, Abbas] Ecole Polytech Fed Lausanne, Theory Computat Lab 2, THL2, Lausanne, Switzerland; [El Halabi, Marwa; Bogunovic, Ilija; Hsieh, Ya-Ping; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, LIONS, Lausanne, Switzerland	Ecole Polytechnique Federale de Lausanne; Ecole Polytechnique Federale de Lausanne	Norouzi-Fard, A (corresponding author), Ecole Polytech Fed Lausanne, Theory Computat Lab 2, THL2, Lausanne, Switzerland.	ashkan.norouzifard@epfl.ch; abbas.bazzi@epfl.ch; marwa.elhalabi@epfl.ch; ilija.bogunovic@epfl.ch; ya-ping.hsieh@epfl.ch; volkan.cevher@epfl.ch			European Commission under ERC Future Proof [SNF 200021-146750, SNF CRSII2-147633]; NCCR Marvel; ERC Starting Grant [335288-OptApprox]	European Commission under ERC Future Proof; NCCR Marvel; ERC Starting Grant(European Research Council (ERC))	We would like to thank Michael Kapralov and Ola Svensson for useful discussions. This work was supported in part by the European Commission under ERC Future Proof, SNF 200021-146750, SNF CRSII2-147633, NCCR Marvel, and ERC Starting Grant 335288-OptApprox.	Assadi Sepehr, 2016, ARXIV160305715; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Boldi P, 2011, P 20 INT C WORLD WID, P587, DOI 10.1145/1963405.1963488; Boldi P, 2014, WWW'14 COMPANION: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P227, DOI 10.1145/2567948.2577304; Boldi Paolo, P 13 INT WORLD WID W, P595; Chakrabarti A, 2008, ACM S THEORY COMPUT, P641; Chakrabarti Amit, 2015, ARXIV150704645; Chekuri C, 2015, LECT NOTES COMPUT SC, V9134, P318, DOI 10.1007/978-3-662-47672-7_26; Demaine ED, 2014, LECT NOTES COMPUT SC, V8784, P484, DOI 10.1007/978-3-662-45174-8_33; Dinur I, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P624, DOI 10.1145/2591796.2591884; Emek Y, 2014, LECT NOTES COMPUT SC, V8572, P453; Feige U, 1998, J ACM, V45, P634, DOI 10.1145/285055.285059; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Indyk Piotr, 2015, ARXIV150900118; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kim G, 2011, IEEE I CONF COMP VIS, P169, DOI 10.1109/ICCV.2011.6126239; Krause A., 2012, TRACTABILITY PRACTIC, V3, P19; Kumar R., 2015, TOPC, V2, p14:1, DOI DOI 10.1145/2809814; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mirzasoleiman Baharan, 2015, ADV NEURAL INFORM PR, P2863; Nemhauser G. L., 1978, Mathematics of Operations Research, V3, P177, DOI 10.1287/moor.3.3.177; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rupp M, 2015, INT J QUANTUM CHEM, V115, P1058, DOI 10.1002/qua.24954; Saha B, 2009, P SIAM INT C DAT MIN, P697, DOI [10.1137/1.9781611972795.60, DOI 10.1137/1.9781611972795.60]; Seeger Matthias, 2004, TECHNICAL REPORT; Tschiatschek Sebastian, 2014, ADV NEURAL INFORM PR, P1413; WOLSEY LA, 1982, COMBINATORICA, V2, P385, DOI 10.1007/BF02579435; Yang J, 2012, P ACM SIGKDD WORKSH, P1, DOI [DOI 10.1145/2350190.2350193, 10.1145/2350190.2350193]	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701032
C	Orabona, F; Pal, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Orabona, Francesco; Pal, David			Coin Betting and Parameter-Free Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In the recent years, a number of parameter-free algorithms have been developed for online linear optimization over Hilbert spaces and for learning with expert advice. These algorithms achieve optimal regret bounds that depend on the unknown competitors, without having to tune the learning rates with oracle choices. We present a new intuitive framework to design parameter-free algorithms for both online linear optimization over Hilbert spaces and for learning with expert advice, based on reductions to betting on outcomes of adversarial coins. We instantiate it using a betting algorithm based on the Krichevsky-Trofimov estimator. The resulting algorithms are simple, with no parameters to be tuned, and they improve or match previous results in terms of regret guarantee and per-round complexity.	[Orabona, Francesco] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Pal, David] Yahoo Res, New York, NY USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Orabona, F (corresponding author), SUNY Stony Brook, Stony Brook, NY 11794 USA.	francesco@orabona.com; dpal@yahoo-inc.com						Artin E, 1964, GAMMA FUNCTION; Batir N, 2008, ARCH MATH, V91, P554, DOI 10.1007/s00013-008-2856-9; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; Chaudhuri K., 2009, ADV NEURAL INFORM PR; Chen Ch.-P., 2005, GEN MATH, V13, P65; Chernov A., 2010, P 26 C UNC ART INT; Cover TM, 2006, ELEMENTS INFORM THEO; Foster D. J., 2015, ADV NEURAL INFORM PR, V28, P3375; Foster D. J., 2016, COMMUNICATION; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hoorfar A., 2008, JIPAM J INEQUAL PURE, V9; KELLY JL, 1956, IRE T INFORM THEOR, V2, P185, DOI 10.1109/TIT.1956.1056803; Koolen Wouter M, 2015, COLT, P1155; KRICHEVSKY RE, 1981, IEEE T INFORM THEORY, V27, P199, DOI 10.1109/TIT.1981.1056331; Luo H P, 2014, P ADV NEURAL INFORM, V27, P1368; McAllester D., 2013, ARXIV PREPRINT ARXIV; McMahan Brendan, 2013, ADV NEURAL INFORM PR, V26, P2724; McMahan H. B., 2014, P 27 C LEARN THEOR, P1020; Orabona F., 2013, ADV NEURAL INFORM PR, P1806; Orabona F, 2014, ADV NEUR IN, V27; Schapire, 2015, P 28 C LEARN THEOR, P1286; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Streeter M., 2012, ADV NEURAL INFORM PR; Vovk V, 1998, J COMPUT SYST SCI, V56, P153, DOI 10.1006/jcss.1997.1556; Whittaker E., 1962, COURSE MODERN ANAL; WILLEMS FMJ, 1995, IEEE T INFORM THEORY, V41, P653, DOI 10.1109/18.382012	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701016
C	Ping, W; Liu, Q; Ihler, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ping, Wei; Liu, Qiang; Ihler, Alexander			Learning Infinite RBMs with Frank-Wolfe	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this work, we propose an infinite restricted Boltzmann machine (RBM), whose maximum likelihood estimation (MLE) corresponds to a constrained convex optimization. We consider the Frank-Wolfe algorithm to solve the program, which provides a sparse solution that can be interpreted as inserting a hidden unit at each iteration, so that the optimization process takes the form of a sequence of finite models of increasing complexity. As a side benefit, this can be used to easily and efficiently identify an appropriate number of hidden units during the optimization. The resulting model can also be used as an initialization for typical state-of-the-art RBM training algorithms such as contrastive divergence, leading to models with consistently higher test likelihood than random initialization.	[Ping, Wei; Ihler, Alexander] UC Irvine, Comp Sci, Irvine, CA 92697 USA; [Liu, Qiang] Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA	University of California System; University of California Irvine; Dartmouth College	Ping, W (corresponding author), UC Irvine, Comp Sci, Irvine, CA 92697 USA.	wping@ics.uci.edu; qliu@cs.dartmouth.edu; ihler@ics.uci.edu	Ping, Wei/O-4470-2019		NSF [IIS-1254071, CCF-1331915]; United States Air Force under the DARPA PPAML program [FA8750-14-C-0011]	NSF(National Science Foundation (NSF)); United States Air Force under the DARPA PPAML program	This work is sponsored in part by NSF grants IIS-1254071 and CCF-1331915. It is also funded in part by the United States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program.	Aslan O., 2013, NIPS; Bach F., 2014, ARXIV14128690; Belanger D., 2013, NIPS WORKSH GREED OP; Bengio Yoshua, 2005, NIPS; Beygelzimer A., 2015, NIPS; Bradley D. M., 2009, UAI; Cote M.-A., 2015, NEURAL COMPUTATION; Frank M., 1956, NAVAL RES LOGISTICS; Friedman J., 2001, ANN STAT; Guelat J., 1986, MATH PROGRAMMING; Hinton G, 2002, NEURAL COMPUTATION; Hinton G., 2010, UTML TR; Hinton G.E., 2006, NEURAL COMPUTATION; Jaggi Martin., 2013, ICML; Krishnan R., 2015, NIPS; Krizhevsky A., 2010, AISTATS; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; LeCun Y., 1998, P IEEE; Likas A., 2003, PATTERN RECOGNITION; Marlin B. M., 2010, AISTATS; Nair V., 2010, ICML; Nowozin S., 2008, ICML; Orbanz P., 2011, ENCY MACHINE LEARNIN; Ravi S, 2015, ARXIV151105392; Salakhutdinov R., 2008, ICML; Salakhutdinov R., 2007, ICML; Salakhutdinov R., 2009, AISTATS; Smolensky Paul, 1986, TECHNICAL REPORT, P2; Taylor G. W., 2006, NIPS; Tieleman T., 2008, ICML; Verbeek J. J., 2003, NEURAL COMPUTATION; Welling M., 2002, NIPS	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702089
C	Ramamohan, S; Rajkumar, A; Agarwal, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ramamohan, Siddartha; Rajkumar, Arun; Agarwal, Shivani			Dueling Bandits: Beyond Condorcet Winners to General Tournament Solutions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Recent work on deriving O(log T) anytime regret bounds for stochastic dueling bandit problems has considered mostly Condorcet winners, which do not always exist, and more recently, winners defined by the Copeland set, which do always exist. In this work, we consider a broad notion of winners defined by tournament solutions in social choice theory, which include the Copeland set as a special case but also include several other notions of winners such as the top cycle, uncovered set, and Banks set, and which, like the Copeland set, always exist. We develop a family of UCB-style dueling bandit algorithms for such general tournament solutions, and show O(log T) anytime regret bounds for them. Experiments confirm the ability of our algorithms to achieve low regret relative to the target winning set of interest.	[Ramamohan, Siddartha] Indian Inst Sci, Bangalore 560012, Karnataka, India; [Rajkumar, Arun] Xerox Res, Bangalore 560103, Karnataka, India; [Agarwal, Shivani] Univ Penn, Philadelphia, PA 19104 USA	Indian Institute of Science (IISC) - Bangalore; University of Pennsylvania	Ramamohan, S (corresponding author), Indian Inst Sci, Bangalore 560012, Karnataka, India.	siddartha.yr@csa.iisc.ernet.in; arun_r@csa.iisc.ernet.in; ashivani@seas.upenn.edu			Google	Google(Google Incorporated)	Thanks to the anonymous reviewers for helpful comments and suggestions. SR thanks Google for a travel grant to present this work at the conference.	Ailon Nir, 2014, P 31 INT C MACH LEAR; Brandt F, 2016, HDB COMPUTATIONAL SO, P57; Brandt F, 2015, DISCRETE APPL MATH, V187, P41, DOI 10.1016/j.dam.2015.01.041; Busa-Fekete R., 2013, INT C MACH LEARN; Dudik Miroslav, 2015, P 28 C LEARN THEOR; Hudry O, 1999, SOC CHOICE WELFARE, V16, P137, DOI 10.1007/s003550050135; Jamieson Kevin, 2011, ADV NEURAL INFORM PR; Jamieson Kevin G., 2015, P 18 INT C ART INT S; Komiyama J., 2015, P 28 C LEARN THEOR; SHEPSLE KA, 1984, AM J POLIT SCI, V28, P49, DOI 10.2307/2110787; Urvoy T., 2013, P 30 INT C MACH LEAR, P9199; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Yue Yisong, 2011, 28 INT C MACHINE LEA; Zoghi M, 2015, ADV NEUR IN, V28; Zoghi Masrour, 2014, P 31 INT C MACH LEAR; Zoghi Masrour, 2015, P 8 ACM INT C WEB SE	16	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702084
C	Richardson, E; Herskovitz, R; Ginsburg, B; Zibulevsky, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Richardson, Elad; Herskovitz, Rom; Ginsburg, Boris; Zibulevsky, Michael			SEBOOST - Boosting Stochastic Learning Using Subspace Optimization Techniques	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present SEBOOST, a technique for boosting the performance of existing stochastic optimization methods. SEBOOST applies a secondary optimization process in the subspace spanned by the last steps and descent directions. The method was inspired by the SESOP optimization method, and has been adapted for the stochastic learning. It can be applied on top of any existing optimization method with no need to tweak the internal algorithm. We show that the method is able to boost the performance of different algorithms, and make them more robust to changes in their hyper-parameters. As the boosting steps of SEBOOST are applied between large sets of descent steps, the additional subspace optimization hardly increases the overall computational burden. We introduce hyper-parameters that control the balance between the baseline method and the secondary optimization process. The method was evaluated on several deep learning tasks, demonstrating significant improvement in performance. Video presentation is given in [15]	[Richardson, Elad; Herskovitz, Rom; Zibulevsky, Michael] Technion Israel Inst Technol, Haifa, Israel; [Ginsburg, Boris] Nvidia INC, Santa Clara, CA USA	Technion Israel Institute of Technology; Nvidia Corporation	Richardson, E (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	eladrich@cs.technion.ac.il; fornoch@gmail.com; boris.ginsburg@gmail.com; mzib@cs.technion.ac.il			European Research Council under European Unions Seventh Framework Program, ERC [320649]; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	European Research Council under European Unions Seventh Framework Program, ERC; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	The research leading to these results has received funding from the European Research Council under European Unions Seventh Framework Program, ERC Grant agreement no. 320649 and was supported by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).	[Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; Collobert R., 2011, BIGLEARN NIPS WORKSH; Dean J., 2012, NIPS 12, V1, P1223; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Elad M, 2007, APPL COMPUT HARMON A, V23, P346, DOI 10.1016/j.acha.2007.02.002; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hestenes MR, 1952, METHODS CONJUGATE GR, V49; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Narkiss G., 2005, SEQUENTIAL SUBSPACE; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Zhang S., 2015, NEURAL INFORM PROCES, P685; Zibulevsky M, 2010, IEEE SIGNAL PROC MAG, V27, P76, DOI 10.1109/MSP.2010.936023; Zibulevsky Michael, 2013, ARXIV14010159; Zibulevsky Michael, SESOP SEQUENTIAL SUB	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700073
C	Ritchie, D; Thomas, A; Hanrahan, P; Goodman, ND		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ritchie, Daniel; Thomas, Anna; Hanrahan, Pat; Goodman, Noah D.			Neurally-Guided Procedural Models: Amortized Inference for Procedural Graphics Programs using Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Probabilistic inference algorithms such as Sequential Monte Carlo (SMC) provide powerful tools for constraining procedural models in computer graphics, but they require many samples to produce desirable results. In this paper, we show how to create procedural models which learn how to satisfy constraints. We augment procedural models with neural networks which control how the model makes random choices based on the output it has generated thus far. We call such models neurally-guided procedural models. As a pre-computation, we train these models to maximize the likelihood of example outputs generated via SMC. They are then used as efficient SMC importance samplers, generating high-quality results with very few samples. We evaluate our method on L-system-like models with image-based constraints. Given a desired quality threshold, neurally-guided models can generate satisfactory results up to 10x faster than unguided models.	[Ritchie, Daniel; Thomas, Anna; Hanrahan, Pat; Goodman, Noah D.] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Ritchie, D (corresponding author), Stanford Univ, Stanford, CA 94305 USA.							Benes Bedrich, EUROGRAPHICS 2011; Brooks S, 2011, CH CRC HANDB MOD STA, P1, DOI 10.1201/b10905; Dang Minh, 2015, SIGGRAPH ASIA; Goodman N. D., 2014, DESIGN IMPLEMENTATIO; Gu S., 2015, NIPS; Kingma D. P., ICLR 2014; Kingma D.P., ICLR 2015; Kulkarni T., CVPR 2015; Lorensen W., 1987, SIGGRAPH; MacKay D. J. C., 2002, INFORM THEORY INFERE; Mech Radomir, 1996, SIGGRAPH; Mnih A., ICML 2014; Mnih V., 2014, NIPS; Norman K., AISTATS 2014; Paige B., ICML 2016; Prusinkiewicz P, 1990, ALGORITHMIC BEAUTY P; Prusinkiewicz P., SIGGRAPH 1994; Rezende Danilo Jimenez, ICML 2014; Ritchie D., SIGGRAPH 2015; Ritchie Daniel, EUROGRAPHICS 2015; Stava O, 2014, COMPUT GRAPH FORUM, V33, P118, DOI 10.1111/cgf.12282; Talton JO, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1944846.1944851; Wingate David, NIPS 2012 WORKSH PRO; Wong M. T., SIGGRAPH 1998	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702100
C	Rubin, TN; Koyejo, O; Jones, MN; Yarkoni, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rubin, Timothy N.; Koyejo, Oluwasanmi; Jones, Michael N.; Yarkoni, Tal			Generalized Correspondence-LDA Models (GC-LDA) for Identifying Functional Regions in the Brain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper presents Generalized Correspondence-LDA (GC-LDA), a generalization of the Correspondence-LDA model that allows for variable spatial representations to be associated with topics, and increased flexibility in terms of the strength of the correspondence between data types induced by the model. We present three variants of GC-LDA, each of which associates topics with a different spatial representation, and apply them to a corpus of neuroimaging data. In the context of this dataset, each topic corresponds to a functional brain region, where the region's spatial extent is captured by a probability distribution over neural activity, and the region's cognitive function is captured by a probability distribution over linguistic terms. We illustrate the qualitative improvements offered by GC-LDA in terms of the types of topics extracted with alternative spatial representations, as well as the model's ability to incorporate a-priori knowledge from the neuroimaging literature. We furthermore demonstrate that the novel features of GC-LDA improve predictions for missing data.	[Rubin, Timothy N.] SurveyMonkey, San Mateo, CA 94403 USA; [Koyejo, Oluwasanmi] Univ Illinois, Urbana, IL 61801 USA; [Jones, Michael N.] Indiana Univ, Bloomington, IN 47405 USA; [Yarkoni, Tal] Univ Texas Austin, Austin, TX 78712 USA	University of Illinois System; University of Illinois Urbana-Champaign; Indiana University System; Indiana University Bloomington; University of Texas System; University of Texas Austin	Rubin, TN (corresponding author), SurveyMonkey, San Mateo, CA 94403 USA.							Asuncion A., 2009, P 25 C UNCERTAINTY A, P27, DOI DOI 10.1080/10807030390248483; Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI [10.1145/860435.860460, DOI 10.1145/860435.860460]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Calhoun VD, 2009, NEUROIMAGE, V45, pS163, DOI 10.1016/j.neuroimage.2008.10.057; Manning JR, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0094914; Owen AM, 2005, HUM BRAIN MAPP, V25, P46, DOI 10.1002/hbm.20131; Poldrack RA, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002707; Smith SM, 2009, P NATL ACAD SCI USA, V106, P13040, DOI 10.1073/pnas.0905267106; Thirion B, 2014, FRONT NEUROSCI-SWITZ, V8, DOI 10.3389/fnins.2014.00167; Van Essen DC, 2013, NEUROIMAGE, V80, P62, DOI 10.1016/j.neuroimage.2013.05.041; Vigneau M, 2006, NEUROIMAGE, V30, P1414, DOI 10.1016/j.neuroimage.2005.11.002; Yarkoni T, 2011, NAT METHODS, V8, P665, DOI [10.1038/NMETH.1635, 10.1038/nmeth.1635]; Yeo B. T. T., 2014, CEREB CORTEX	13	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702021
C	Saberian, M; Pereira, JC; Xu, C; Yang, J; Vasconcelos, N		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Saberian, Mohammad; Pereira, Jose Costa; Xu, Can; Yang, Jian; Vasconcelos, Nuno			Large Margin Discriminant Dimensionality Reduction in Prediction Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper we establish a duality between boosting and SVM, and use this to derive a novel discriminant dimensionality reduction algorithm. In particular, using the multiclass formulation of boosting and SVM we note that both use a combination of mapping and linear classification to maximize the multiclass margin. In SVM this is implemented using a pre-defined mapping (induced by the kernel) and optimizing the linear classifiers. In boosting the linear classifiers are pre-defined and the mapping (predictor) is learned through a combination of weak learners. We argue that the intermediate mapping, i.e. boosting predictor, is preserving the discriminant aspects of the data and that by controlling the dimension of this mapping it is possible to obtain discriminant low dimensional representations for the data. We use the aforementioned duality and propose a new method, Large Margin Discriminant Dimensionality Reduction (LADDER) that jointly learns the mapping and the linear classifiers in an efficient manner. This leads to a data-driven mapping which can embed data into any number of dimensions. Experimental results show that this embedding can significantly improve performance on tasks such as hashing and image/scene classification.	[Saberian, Mohammad] Netflix, Los Gatos, CA 95032 USA; [Pereira, Jose Costa] INESCTEC, Porto, Portugal; [Xu, Can] Google, Mountain View, CA USA; [Yang, Jian] Yahoo Res, Washington, DC USA; [Vasconcelos, Nuno] Univ Calif San Diego, San Diego, CA USA	Netflix, Inc.; INESC TEC; Google Incorporated; University of California System; University of California San Diego	Saberian, M (corresponding author), Netflix, Los Gatos, CA 95032 USA.	esaberian@netflix.com; jose.c.pereira@inesctec.pt; canxu@google.com; jianyang@yahoo-inc.com; nvasconcelos@ucsd.edu						[Anonymous], 2008, VISUALIZING HIGH DIM; Bo L, 2012, ISER; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dekel O., 2002, ADV NEURAL INFORM PR, P945; Dollar P., 2009, P BMVC; Freund Y., 1997, J COMP SYS SCI; Gao T., 2011, ICML; Goldberger J., 2004, ADV NEURAL INF PROCE, P1; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Gong Y., 2012, IEEE T PATTERN ANAL, V99; Gong Yunchao, 2014, P ECCV, P2; He X., 2003, ADV NIPS; He X., 2005, P IEEE ICCV; Hsu CW, 2002, IEEE T NEURAL NETWOR, V13, P415, DOI 10.1109/72.991427; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Knuth DE., 1973, ART COMPUTER PROGRAM; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, NEURAL INF PROCESS S, ppp1097; Kulis B, 2012, IEEE T PATTERN ANAL, V34, P1092, DOI 10.1109/TPAMI.2011.219; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Larsson F, 2011, IET COMPUT VIS, V5, P244, DOI 10.1049/iet-cvi.2010.0040; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Mukherjee I., 2010, ADV NIPS; Nocedal S. J., 1999, NUMERICAL OPTIMIZATI; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Parizi S. N., 2012, P IEEE CVPR; Pereira JC, 2014, COMPUT VIS IMAGE UND, V124, P123, DOI 10.1016/j.cviu.2014.03.003; Pereira JC, 2012, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2012.6248041; Perronnin F., 2010, P ECCV; PUDIL P, 1994, PATTERN RECOGN LETT, V15, P1119, DOI 10.1016/0167-8655(94)90127-9; Quattoni A., 2009, P IEEE CVPR; Roweis S., 1998, ADV NIPS; Saberian M., 2011, ADV NIPS; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Sivic J, 2008, PROC CVPR IEEE, P1, DOI 10.1109/CVPRW.2008.4562950; Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531; Sugiyama M, 2007, J MACH LEARN RES, V8, P1027; Vapnik V.N, 1998, STAT LEARNING THEORY; Vasconcelos N., 2012, P ECCV; Weinberger K. Q., 2009, ADV NIPS; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weston J., 1999, 7th European Symposium on Artificial Neural Networks. ESANN'99. Proceedings, P219; Weston J., 2010, P ECML; Zhao B, 2013, PROC CVPR IEEE, P3350, DOI 10.1109/CVPR.2013.430	45	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703094
C	Saxena, S; Verbeek, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Saxena, Shreyas; Verbeek, Jakob			Convolutional Neural Fabrics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Despite the success of CNNs, selecting the optimal architecture for a given task remains an open problem. Instead of aiming to select a single optimal architecture, we propose a "fabric" that embeds an exponentially large number of architectures. The fabric consists of a 3D trellis that connects response maps at different layers, scales, and channels with a sparse homogeneous local connectivity pattern. The only hyper-parameters of a fabric are the number of channels and layers. While individual architectures can be recovered as paths, the fabric can in addition ensemble all embedded architectures together, sharing their weights where their paths overlap. Parameters can be learned using standard methods based on back-propagation, at a cost that scales linearly in the fabric size. We present benchmark results competitive with the state of the art for image classification on MNIST and CIFAR10, and for semantic segmentation on the Part Labels dataset.	[Saxena, Shreyas; Verbeek, Jakob] INRIA Grenoble, Lab Jean Kuntzmann, Grenoble, France	Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Inria	Saxena, S (corresponding author), INRIA Grenoble, Lab Jean Kuntzmann, Grenoble, France.				LabEx PERSYVAL-Lab [ANR-11-LABX-0025-01]	LabEx PERSYVAL-Lab	We would like to thank NVIDIA for the donation of GPUs used in this research. This work has been partially supported by the LabEx PERSYVAL-Lab (ANR-11-LABX-0025-01).	[Anonymous], 2013, ICML; Chen T., 2016, ICLR; Chen Y.-S., 2015, BATCH NORMALIZED MAX; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Graves Alex, 2007, ICANN; He K., 2016, ECCV; Honari S., 2016, CVPR; Huang G.B., 2008, WORKSHOP FACESREAL L; Ioffe S., 2015, P 32 INT C INT C MAC, V37, P448; Kae A., 2013, CVPR; Kalchbrenner N., 2016, P ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kulkarni P., 2015, BMVC; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1989, NIPS; Lin M., 2014, ICLR; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Mairal J., 2014, NIPS; Mishkin Dmytro, 2016, ICLR; Misra I., 2016, CVPR; Noh H., 2015, ICCV; Pfister T., 2015, CVPR; Ronneberger Olaf, 2015, CORR ABS150504597, DOI [DOI 10.48550/ARXIV.1505.04597, DOI 10.1007/978-3-319-24574-4_28], Patent No. [ArXiv150504597Cs, 150504597]; Simonyan Karen, 2015, INT C LEARN REPR; Singh S., 2016, NIPS; Springenberg J.T., 2015, P ICLR WORKSH TRACK, P1; Srivastava N., 2014, JMLR; Tsogkas S., 2015, DEEP LEARNING SEMANT; Warde-Farley D., 2013, ICML; Yang M., 2015, CVPR; Zheng H., 2015, LEARNING HIGH LEVEL; Zhou Y., 2015, INT S NEUR NETW; [No title captured]	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702051
C	Scanagatta, M; Corani, G; de Campos, CP; Zaffalon, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Scanagatta, Mauro; Corani, Giorgio; de Campos, Cassio P.; Zaffalon, Marco			Learning Treewidth-Bounded Bayesian Networks with Thousands of Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a method for learning treewidth-bounded Bayesian networks from data sets containing thousands of variables. Bounding the treewidth of a Bayesian network greatly reduces the complexity of inferences. Yet, being a global property of the graph, it considerably increases the difficulty of the learning process. Our novel algorithm accomplishes this task, scaling both to large domains and to large treewidths. Our novel approach consistently outperforms the state of the art on experiments with up to thousands of variables.	[Scanagatta, Mauro; Corani, Giorgio] USI, SUPSI, IDSIA, Lugano, Switzerland; [de Campos, Cassio P.] Queens Univ Belfast, Belfast, Antrim, North Ireland; [Zaffalon, Marco] IDSIA, Lugano, Switzerland	Universita della Svizzera Italiana; Queens University Belfast; Universita della Svizzera Italiana	Scanagatta, M (corresponding author), USI, SUPSI, IDSIA, Lugano, Switzerland.	mauro@idsia.ch; giorgio@idsia.ch; c.decampos@qub.ac.uk; zaffalon@idsia.ch	Zaffalon, Marco/M-7035-2017	Zaffalon, Marco/0000-0001-8908-1502	Swiss NSF [200021_146606 / 1, IZKSZ2_162188]	Swiss NSF(Swiss National Science Foundation (SNSF))	Work partially supported by the Swiss NSF grants 200021_146606 / 1 and IZKSZ2_162188.	Berg J., 2014, AISTATS 14; Cussens J, 2011, UNCERTAINTY ARTIFICI, P153; Elidan G., 2009, ADV NEURAL INFORM PR, P417; Korhonen J.H., 2013, JMLR W CP, V31, P370; Kwisthout J.H.P., 2010, ECAI 10; Nie S., 2016, AAAI 16; Nie S., 2014, ADV NEURAL INFORM PR, V27, P2285; Nie SQ, 2015, LECT NOTES ARTIF INT, V9161, P387, DOI 10.1007/978-3-319-20807-7_35; Parviainen P, 2014, P 17 INT C ART INT S; Patil H.P., 1986, J COMBINATORICS INFO, V11, P57; Scanagatta M., 2015, ADV NEURAL INFORM PR, P1855; Teyssier Marc, 2012, ABS12071429 CORR	12	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701089
C	Schein, A; Zhou, MY; Wallach, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Schein, Aaron; Zhou, Mingyuan; Wallach, Hanna			Poisson-Gamma Dynamical Systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce a new dynamical system for sequentially observed multivariate count data. This model is based on the gamma-Poisson construction-a natural choice for count data-and relies on a novel Bayesian nonparametric prior that ties and shrinks the model parameters, thus avoiding overfitting. We present an efficient MCMC inference algorithm that advances recent work on augmentation schemes for inference in negative binomial models. Finally, we demonstrate the model's inductive bias using a variety of real-world data sets, showing that it exhibits superior predictive performance over other models and infers highly interpretable latent structure.	[Schein, Aaron] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA; [Zhou, Mingyuan] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA; [Wallach, Hanna] Microsoft Res, 641 Ave Amer, New York, NY 10011 USA	University of Massachusetts System; University of Massachusetts Amherst; University of Texas System; University of Texas Austin; Microsoft	Schein, A (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	aschein@cs.umass.edu; mingyuan.zhou@mccombs.utexas.edu; hanna@dirichlet.net	Zhou, Mingyuan/AAE-8717-2021		UMass Amherst CIIR; NSF [SBE-0965436, IIS-1320219]	UMass Amherst CIIR; NSF(National Science Foundation (NSF))	We thank David Belanger, Roy Adams, Kostis Gourgoulias, Ben Marlin, Dan Sheldon, and Tim Vieira for many helpful conversations. This work was supported in part by the UMass Amherst CIIR and in part by NSF grants SBE-0965436 and IIS-1320219. Any opinions, findings, conclusions, or recommendations are those of the authors and do not necessarily reflect those of the sponsors.	Acharya S, 2015, PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE OF THE ASSOCIATION OF PSYCHOLOGY AND PSYCHIATRY FOR ADULTS AND CHILDREN (A.P.P.A.C 2013); ADELSON RM, 1966, OPER RES QUART, V17, P73, DOI 10.2307/3007241; [Anonymous], 2015, P 31 C UNC ART INT; [Anonymous], 2009, COMPUT INTEL NEUROSC; Basbug M. E., 2016, P 33 INT C MACH LEAR; Blei DM, 2006, INT C MACH LEARN ICM, V148, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859]; BULMER MG, 1974, BIOMETRICS, V30, P101, DOI 10.2307/2529621; Canny J., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P122, DOI 10.1145/1008992.1009016; Charlin L., 2015, RECSYS ACM, P155; Corless RM, 1996, ADV COMPUT MATH, V5, P329, DOI 10.1007/BF02124750; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; Durbin J., 2012, OXFORD STATIST SCI S, Vsecond; FERGUSON TS, 1973, ANN STAT, V1, P209, DOI 10.1214/aos/1176342360; GHAHRAMANI Z, 1998, ADV NEURAL INFORM PR, V11, P431; Haykin S., 2004, KALMAN FILTERING NEU, V47; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kingman J. F. C., 1972, POISSON PROCESSES; Kleinberg J, 2003, DATA MIN KNOWL DISC, V7, P373, DOI 10.1023/A:1024940629314; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; McCullagh P., 1989, GEN LINEAR MODELS, V2nd; Rugh WJ., 1995, LINEAR SYSTEM THEORY; Zhou M, 2012, P 15 INT C ART INT S; Zhou M., ARXIV160407464; Zhou M, 2012, ADV NEURAL INFORM PR; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zhou MY, 2015, IEEE T PATTERN ANAL, V37, P307, DOI 10.1109/TPAMI.2013.211	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700047
C	Schulz, E; Tenenbaum, JB; Duvenaud, D; Speekenbrink, M; Gershman, SJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Schulz, Eric; Tenenbaum, Joshua B.; Duvenaud, David; Speekenbrink, Maarten; Gershman, Samuel J.			Probing the Compositionality of Intuitive Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				EXTRAPOLATION	How do people learn about complex functional structure? Taking inspiration from other areas of cognitive science, we propose that this is accomplished by harnessing compositionality: complex structure is decomposed into simpler building blocks. We formalize this idea within the framework of Bayesian regression using a grammar over Gaussian process kernels. We show that participants prefer compositional over non-compositional function extrapolations, that samples from the human prior over functions are best described by a compositional model, and that people perceive compositional functions as more predictable than their non-compositional but otherwise similar counterparts. We argue that the compositional nature of intuitive functions is consistent with broad principles of human cognition.	[Schulz, Eric; Speekenbrink, Maarten] UCL, London, England; [Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA; [Duvenaud, David] Univ Toronto, Toronto, ON, Canada; [Gershman, Samuel J.] Harvard Univ, Cambridge, MA 02138 USA	University of London; University College London; Massachusetts Institute of Technology (MIT); University of Toronto; Harvard University	Schulz, E (corresponding author), UCL, London, England.	e.schulz@cs.ucl.ac.uk; jbt@mit.edu; duvenaud@cs.toronto.edu; m.speekenbrink@ucl.ac.uk; gershman@fas.harvard.edu	Speekenbrink, Maarten/C-1904-2008	Speekenbrink, Maarten/0000-0003-3221-1091				Bott L, 2004, J EXP PSYCHOL LEARN, V30, P38, DOI 10.1037/0278-7393.30.1.38; Brady TF, 2011, J VISION, V11, DOI 10.1167/11.5.4; BREHMER B, 1974, ORGAN BEHAV HUM PERF, V11, P1, DOI 10.1016/0030-5073(74)90002-6; Carroll J., 1963, FUNCTIONAL LEARNING; DeLosh EL, 1997, J EXP PSYCHOL LEARN, V23, P968, DOI 10.1037/0278-7393.23.4.968; Duvenaud D., 2013, P INT C MACH LEARN I, P1166; Gershman S. J., 2016, VISION RES; Gershman S. J., 2016, DECISION; Gershman SJ, 2010, CURR OPIN NEUROBIOL, V20, P251, DOI 10.1016/j.conb.2010.02.008; Grosse R. B., 2012, UNCERTAINTY ARTIFICI; Kalish, 2009, ADV NEURAL INFORM PR, V21, P553; Kalish ML, 2007, PSYCHON B REV, V14, P288, DOI 10.3758/BF03194066; Kemp C, 2009, PSYCHOL REV, V116, P20, DOI 10.1037/a0014282; KOH KH, 1991, J EXP PSYCHOL LEARN, V17, P811, DOI 10.1037/0278-7393.17.5.811; Lucas CG, 2015, PSYCHON B REV, V22, P1193, DOI 10.3758/s13423-015-0808-5; McDaniel MA, 2005, PSYCHON B REV, V12, P24, DOI 10.3758/BF03196347; Parpart P., 2015, P 37 ANN M COGN SCI, P1829; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sanborn AN, 2010, COGNITIVE PSYCHOL, V60, P63, DOI 10.1016/j.cogpsych.2009.07.001; Schulz E, 2015, P 37 ANN M COGN SCI, P2116; Wilson A., 2013, P 30 INT C MACH LEAR; Wilson A. G., 2015, ADV NEURAL INFORM PR, V28, P2836; Zhao JY, 2016, COGNITION, V146, P217, DOI 10.1016/j.cognition.2015.09.018	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700094
C	Schuurmans, D; Zinkevich, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Schuurmans, Dale; Zinkevich, Martin			Deep Learning Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GRADIENT	We investigate a reduction of supervised learning to game playing that reveals new connections and learning methods. For convex one-layer problems, we demonstrate an equivalence between global minimizers of the training problem and Nash equilibria in a simple game. We then show how the game can be extended to general acyclic neural networks with differentiable convex gates, establishing a bijection between the Nash equilibria and critical (or KKT) points of the deep learning problem. Based on these connections we investigate alternative learning methods, and find that regret matching can achieve competitive training performance while producing sparser models than current deep learning strategies.	[Schuurmans, Dale; Zinkevich, Martin] Google, Edmonton, AB, Canada; [Schuurmans, Dale; Zinkevich, Martin] Google Brain, Edmonton, AB, Canada	Google Incorporated	Schuurmans, D (corresponding author), Google, Edmonton, AB, Canada.	daes@ualberta.ca; martinz@google.com						[Anonymous], 2012, P 26 AAAI C ART INT; [Anonymous], 2013, ICML 3; [Anonymous], 2016, DEEP ONLINE CONVEX O; Balduzzi D, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1265; Bottou Leon, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P421, DOI 10.1007/978-3-642-35289-8_25; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Duchi J., 2008, PROC 25 INT C MACH L, P272; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Gordon G., 2006, NIPS, V19; Gordon G.J, 2005, CMUCALD05112; HAJNAL A, 1993, J COMPUT SYST SCI, V46, P129, DOI 10.1016/0022-0000(93)90001-D; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Hoeffgen K., 1995, JCSS, V52, P114; Karush W., 1939, THESIS; Kearns M, 2006, SCIENCE, V313, P824, DOI 10.1126/science.1127207; King DB, 2015, ACS SYM SER, V1214, P1; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Kuhn H., 1951, P 2 BERK S MATH STAT, P481, DOI DOI 10.1007/BF01582292; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J., 2016, 29 ANN C LEARN THEOR, V49; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Ratliff N., 2006, 22 INT C MACH LEARN; Ratliff Nathan, 2007, 11 INT C ART INT STA; Razborov A., 1992, ALGORITHM THEORY SWA; Shalev-Shwartz S., 2006, NIPS; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Srinivasan N., 2002, ICONIP, V1; Syrgkanis V., 2015, ADV NEURAL INFORM PR, V28, P2971; Tammelin O, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P645; Zinkevich M., 2003, 20 INT C MACH LEARN; Zinkevich M., 2007, NIPS	38	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702062
C	Sener, O; Song, HO; Saxena, A; Savarese, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sener, Ozan; Song, Hyun Oh; Saxena, Ashutosh; Savarese, Silvio			Learning Transferrable Representations for Unsupervised Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Supervised learning with large scale labelled datasets and deep layered models has caused a paradigm shift in diverse areas in learning and recognition. However, this approach still suffers from generalization issues under the presence of a domain shift between the training and the test data distribution. Since unsupervised domain adaptation algorithms directly address this domain shift problem between a labelled source dataset and an unlabelled target dataset, recent papers [11, 33] have shown promising results by fine-tuning the networks with domain adaptation loss functions which try to align the mismatch between the training and testing data distributions. Nevertheless, these recent deep learning based domain adaptation approaches still suffer from issues such as high sensitivity to the gradient reversal hyperparameters [11] and overfitting during the fine-tuning stage. In this paper, we propose a unified deep learning framework where the representation, cross domain transformation, and target label inference are all jointly optimized in an end-to-end fashion for unsupervised domain adaptation. Our experiments show that the proposed method significantly outperforms state-of-the-art algorithms in both object recognition and digit classification experiments by a large margin.	[Sener, Ozan; Song, Hyun Oh; Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA; [Saxena, Ashutosh] Brain Things, Redwood City, CA USA	Stanford University	Sener, O (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ozan@cs.stanford.edu; hsong@cs.stanford.edu; asaxena@cs.stanford.edu; ssilvio@cs.stanford.edu	Sener, Ozan/ABF-9436-2020		Toyota Center grant [1191689-1-UDAWF];  [ONR-N00014-13-1-0761];  [MURI - WF911NF-15-1-0479]	Toyota Center grant; ; 	We acknowledge the support of ONR-N00014-13-1-0761, MURI - WF911NF-15-1-0479 and Toyota Center grant 1191689-1-UDAWF.	[Anonymous], 2014, ABS14123474 CORR; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Berlind C., 2015, ICML; Blum A, 2001, ICML; Boykov Y, 2004, IEEE T PATTERN ANAL, V26, P1124, DOI 10.1109/TPAMI.2004.60; Chopra S., 2013, ICML W; Deng J., 2009, CVPR; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fernando B., 2013, ICCV; Gammerman A., 1998, UAI; Ganin Y., 2015, ICML; Gong B., 2012, CVPR; Gong B., 2013, P INT C MACH LEARN J, V711, P712; Karpathy A., 2014, CVPR; Kodirov E., 2015, ICCV; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lampert, 2014, BMVC; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Long M., 2015, LEARNING TRANSFERABL; Netzer Y., 2011, NIPS W; Palatucci M., 2009, ADV NEURAL INFORM PR, V22; Raina R., 2007, P 24 INT C MACH LEAR; Rohrbach M., 2013, NIPS; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Sindhwani V., 2005, ICML; Sun B., 2016, AAAI; Sun B., 2015, BRIT MACH VIS C; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Thrun S., 2012, LEARNING LEARN; Tommasi T., 2013, ICCV; van der maaten L., 2014, JMLR; Weinberger KQ, 2006, NIPS; Zhu X., 2003, ICML; Zhu Xiaojin, 2002, CMU CALD TECH REPORT	36	1	1	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702107
C	Shrivastava, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shrivastava, Anshumali			Simple and Efficient Weighted Minwise Hashing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Weighted minwise hashing (WMH) is one of the fundamental subroutine, required by many celebrated approximation algorithms, commonly adopted in industrial practice for large - scale search and learning. The resource bottleneck with WMH is the computation of multiple (typically a few hundreds to thousands) independent hashes of the data. We propose a simple rejection type sampling scheme based on a carefully designed red-green map, where we show that the number of rejected sample has exactly the same distribution as weighted minwise sampling. The running time of our method, for many practical datasets, is an order of magnitude smaller than existing methods. Experimental evaluations, on real datasets, show that for computing 500 WMH, our proposal can be 60000x faster than the Ioffe's method without losing any accuracy. Our method is also around 100x faster than approximate heuristics capitalizing on the efficient "densified" one permutation hashing schemes [26, 27]. Given the simplicity of our approach and its significant advantages, we hope that it will replace existing implementations in practice.	[Shrivastava, Anshumali] Rice Univ, Dept Comp Sci, Houston, TX 77005 USA	Rice University	Shrivastava, A (corresponding author), Rice Univ, Dept Comp Sci, Houston, TX 77005 USA.	anshumali@rice.edu			Rice Faculty Initiative Award 2016-17	Rice Faculty Initiative Award 2016-17	This work is supported by Rice Faculty Initiative Award 2016-17. We would like to thank anonymous reviewers, Don Macmillen, and Ryan Moulton for feedbacks on the presentation of the paper.	Bayardo R. J., 2007, WWW, P131, DOI DOI 10.1145/1242572.1242591; BRODER A, 1998, FUN; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; BRODER AZ, 1997, WWW, V29, P1157; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dwork C., ALGORITHMIC FDN DIFF; Gollapudi S., 2006, P 15 ACM INT C INF K, P475; Haeupler  B., 2014, ARXIV14104266; Indyk P, 2001, J ALGORITHM, V38, P84, DOI 10.1006/jagm.2000.1131; Ioffe S., 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P246, DOI 10.1109/ICDM.2010.80; Kleinberg J., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P14, DOI 10.1109/SFFCS.1999.814572; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Li P., 2011, COMMUN ACM; Li Ping, 2015, KDD; Manasse Mark, 2010, MSRTR201073; Mitzenmacher M, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P746; Patrascu M, 2010, LECT NOTES COMPUT SC, V6198, P715; Philbin J, 2007, CVPR; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Rajaraman Anand, MINING MASSIVE DATAS; Rasheed Z., MC MINH METAGENOME C; SADOSKY P, 2015, ARXIV151007714; Shrivastava A., 2014, ICML; Shrivastava A., 2015, THESIS; Shrivastava A, 2014, JMLR WORKSH CONF PRO, V33, P886; Shrivastava  Anshumali, 2014, UAI; Wang J., 1999, D LIB MAGAZINE, V5	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703108
C	Syrgkanis, V; Luo, HP; Krishnamurthy, A; Schapire, RE		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Syrgkanis, Vasilis; Luo, Haipeng; Krishnamurthy, Akshay; Schapire, Robert E.			Improved Regret Bounds for Oracle-Based Adversarial Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a new oracle-based algorithm, BISTRO+, for the adversarial contextual bandit problem, where either contexts are drawn i.i.d. or the sequence of contexts is known a priori, but where the losses are picked adversarially. Our algorithm is computationally efficient, assuming access to an offline optimization oracle, and enjoys a regret of order O((KT)(2/3) (log N)(1/3)), where K is the number of actions, T is the number of iterations, and N is the number of baseline policies. Our result is the first to break the O(T-3/4) barrier achieved by recent algorithms, which was left as a major open problem. Our analysis employs the recent relaxation framework of Rakhlin and Sridharan [7].	[Syrgkanis, Vasilis; Luo, Haipeng; Schapire, Robert E.] Microsoft Res, Cambridge, MA 02142 USA; [Krishnamurthy, Akshay] Univ Massachusetts, Amherst, MA 01003 USA	Microsoft; University of Massachusetts System; University of Massachusetts Amherst	Syrgkanis, V (corresponding author), Microsoft Res, Cambridge, MA 02142 USA.	vasy@microsoft.com; haipeng@microsoft.com; akshay@cs.umass.edu; schapire@microsoft.com						Agarwal Alekh, 2014, INT C MACH LEARN ICM; Auer Peter, 1995, FDN COMPUTER SCI FOC; Cesa-Bianchi Nicolo, 1997, J ACM JACM; Dudik Miroslav, 2011, UNCERTAINTY ARTIFICI; Freund Y., 1997, J COMPUTER SYSTEM SC; Langford John, 2008, ADV NEURAL INFORM PR; Rakhlin Alexander, 2016, INT C MACH LEARN ICM; Syrgkanis Vasilis, 2016, INT C MACH LEARN ICM	8	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703036
C	Wang, G; Giannakis, GB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Gang; Giannakis, Georgios B.			Solving Random Systems of Quadratic Equations via Truncated Generalized Gradient Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				PHASE RETRIEVAL; RECOVERY	This paper puts forth a novel algorithm, termed truncated generalized gradient flow (TGGF), to solve for x is an element of R-n/C-n a system of m quadratic equations y(i) = vertical bar < a(i), x >vertical bar(2), i = 1, 2, . . ., m, which even for{a(i) is an element of R-n/C-n}(i=1)(m) random is known to be NP-hard in general. We prove that as soon as the number of equations m is on the order of the number of unknowns n, TGGF recovers the solution exactly (up to a global unimodular constant) with high probability and complexity growing linearly with the time required to read the data {(a(i), y(i))(i=1)(m). Specifically, TGGF proceeds in two stages. s1) A novel orthogonality-promoting initialization that is obtained with simple power iterations, and, s2) a refinement of the initial estimate by successive updates of scalable truncated generalized gradient iterations. The former is in sharp contrast to the existing spectral initializations, while the latter handles the rather challenging nonconvex and nonsmooth amplitude-based cost function. Empirical results demonstrate that: i) The novel orthogonality-promoting initialization method returns more accurate and robust estimates relative to its spectral counterparts, and, ii) even with the same initialization, our refinement/truncation outperforms Wirtinger-based alternatives, all corroborating the superior performance of TGGF over state-of-the-art algorithms.	[Wang, Gang] Univ Minnesota, ECE Dept, Minneapolis, MN 55455 USA; [Wang, Gang] Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA; [Wang, Gang; Giannakis, Georgios B.] Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China	University of Minnesota System; University of Minnesota Twin Cities; University of Minnesota System; University of Minnesota Twin Cities; Beijing Institute of Technology	Wang, G (corresponding author), Univ Minnesota, ECE Dept, Minneapolis, MN 55455 USA.; Wang, G (corresponding author), Univ Minnesota, Digital Tech Ctr, Minneapolis, MN 55455 USA.; Wang, G (corresponding author), Beijing Inst Technol, Sch Automat, Beijing 100081, Peoples R China.	gangwang@umn.edu; georgios@umn.edu	Giannakis, Georgios/Z-4413-2019; Wang, Gang/I-9061-2019	Giannakis, Georgios/0000-0002-0196-0260; Wang, Gang/0000-0002-7266-2412	NSF [1500713, 1514056]	NSF(National Science Foundation (NSF))	Work in this paper was supported in part by NSF grants 1500713 and 1514056.	Balan R, 2006, APPL COMPUT HARMON A, V20, P345, DOI 10.1016/j.acha.2005.07.001; Berberidis DK, 2015, INT CONF ACOUST SPEE, P5475, DOI 10.1109/ICASSP.2015.7179018; Cai T, 2013, J MACH LEARN RES, V14, P1837; Candes EJ, 2015, SIAM REV, V57, P225, DOI 10.1137/151005099; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candes EJ, 2014, FOUND COMPUT MATH, V14, P1017, DOI 10.1007/s10208-013-9162-z; Chen P., 2015, ARXIV151007379V2; CHEN Y., 2016, COMM PURE APPL MATH; CLARKE FH, 1975, T AM MATH SOC, V205, P247, DOI 10.1090/s0002-9947-1975-0367131-6; Clarke FH, 1990, OPTIMIZATION NONSMOO; Conca A, 2015, APPL COMPUT HARMON A, V38, P346, DOI 10.1016/j.acha.2014.06.005; FIENUP JR, 1982, APPL OPTICS, V21, P2758, DOI 10.1364/AO.21.002758; GERCHBERG RW, 1972, OPTIK, V35, P237; HAUPTMAN HA, 1991, REP PROG PHYS, V54, P1427, DOI 10.1088/0034-4885/54/11/002; MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948; Netrapalli P, 2015, IEEE T SIGNAL PROCES, V63, P4814, DOI 10.1109/TSP.2015.2448516; Pardalos P. M., 1991, J GLOBAL OPTIM, V1, P15; SAHINOGLOU H, 1991, IEEE T CIRCUITS SYST, V38, P954, DOI 10.1109/31.85639; SHOR NZ, 1985, MINIMIZATION METHODS; Waldspurger I, 2015, MATH PROGRAM, V149, P47, DOI 10.1007/s10107-013-0738-9; Wang G, 2014, 2014 IEEE GLOBAL CONFERENCE ON SIGNAL AND INFORMATION PROCESSING (GLOBALSIP), P326, DOI 10.1109/GlobalSIP.2014.7032132; WRIGHT J., 2016, ARXIV160206664; Yeh LH, 2015, OPT EXPRESS, V23, P33214, DOI 10.1364/OE.23.033214	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700025
C	Wei, CY; Hong, YT; Lu, CJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wei, Chen-Yu; Hong, Yi-Te; Lu, Chi-Jen			Tracking the Best Expert in Non-stationary Stochastic Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the dynamic regret of multi-armed bandit and experts problem in non-stationary stochastic environments. We introduce a new parameter Lambda, which measures the total statistical variance of the loss distributions over T rounds of the process, and study how this amount affects the regret. We investigate the interaction between Lambda and Gamma, which counts the number of times the distributions change, as well as Lambda and V, which measures how far the distributions deviates over time. One striking result we find is that even when Gamma, V, and Lambda are all restricted to constant, the regret lower bound in the bandit setting still grows with T. The other highlight is that in the full-information setting, a constant regret becomes achievable with constant Gamma and Lambda, as it can be made independent of T, while with constant V and Lambda, the regret still has a T-1/3 dependency. We not only propose algorithms with upper bound guarantee, but prove their matching lower bounds as well.	[Wei, Chen-Yu; Hong, Yi-Te; Lu, Chi-Jen] Acad Sinica, Inst Informat Sci, Taipei, Taiwan	Academia Sinica - Taiwan	Wei, CY (corresponding author), Acad Sinica, Inst Informat Sci, Taipei, Taiwan.	bahh723@iis.sinica.edu.tw; ted0504@iis.sinica.edu.tw; cjlu@iis.sinica.edu.tw	Lu, Chi-Jen/AAQ-3728-2021					Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Besbes O, 2014, ADV NEUR IN, V27; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chiang Chao-Kai, 2012, 25 C LEARN THEOR COL; Gaillard P, 2014, P 27 ANN C LEARN THE, P176; Garivier A., 2011, 22 INT C ALG LEARN T; Jadbabaie Ali, 2015, P 18 INT C ART INT S; Luo Haipeng, 2015, 28 C LEARN THEOR COL; Maurer Andreas, 2009, 22 C LEARN THEOR COL	11	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704060
C	Xu, Z; Dong, W; Srihari, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Xu, Zhen; Dong, Wen; Srihari, Sargur			Using Social Dynamics to Make Individual Predictions: Variational Inference with a Stochastic Kinetic Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Social dynamics is concerned primarily with interactions among individuals and the resulting group behaviors, modeling the temporal evolution of social systems via the interactions of individuals within these systems. In particular, the availability of large-scale data from social networks and sensor networks offers an unprecedented opportunity to predict state-changing events at the individual level. Examples of such events include disease transmission, opinion transition in elections, and rumor propagation. Unlike previous research focusing on the collective effects of social systems, this study makes efficient inferences at the individual level. In order to cope with dynamic interactions among a large number of individuals, we introduce the stochastic kinetic model to capture adaptive transition probabilities and propose an efficient variational inference algorithm the complexity of which grows linearly-rather than exponentially-with the number of individuals. To validate this method, we have performed epidemic-dynamics experiments on wireless sensor network data collected from more than ten thousand people over three years. The proposed algorithm was used to track disease transmission and predict the probability of infection for each individual. Our results demonstrate that this method is more efficient than sampling while nonetheless achieving high accuracy.	[Xu, Zhen; Dong, Wen; Srihari, Sargur] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Xu, Z (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.	zxu8@buffalo.edu; wendong@buffalo.edu; srihari@buffalo.edu	Dong, Wen/AAQ-6057-2021; Srihari, Sargur N/E-8100-2011	Dong, Wen/0000-0001-8923-2227; 				[Anonymous], 2013, P INT C MACHINE LEAR; Arkin A, 1998, GENETICS, V149, P1633; Brand M, 1997, PROC CVPR IEEE, P994, DOI 10.1109/CVPR.1997.609450; Castellano C, 2009, REV MOD PHYS, V81, P591, DOI 10.1103/RevModPhys.81.591; Cohn I, 2010, J MACH LEARN RES, V11, P2745; Dong W., 2011, P 10 INT C MOBILE UB, P134, DOI [DOI 10.1145/2107596.2107613, 10.1145/2107596.2107613]; Dong W., 2012, PROC 28 C UNCERTAINT, P227; Doucet A., 2009, HDB NONLINEAR FILTER, V12, P3; Durlauf SN, 2004, SOCIAL DYNAMICS, V4; Eubank S, 2004, NATURE, V429, P180, DOI 10.1038/nature02541; Gillespie DT, 2007, ANNU REV PHYS CHEM, V58, P35, DOI 10.1146/annurev.physchem.58.032806.104637; Golightly Andrew, 2011, INTERFACE FOCUS; Heskes T., 2002, UNCERTAINTY ARTIFICI, P216; Keeling MJ., 2008, MODELING INFECT DIS, DOI [DOI 10.1515/9781400841035, 10.1515/9781400841035]; Murphy K, 2001, STAT ENG IN, P499; Nodelman U., 2002, P 18 C UNCERTAINTY A, P378; Opper M., 2008, ADV NEURAL INFORM PR, P1105; Rao V., 2011, P UAI; Robinson JW, 2010, J MACH LEARN RES, V11, P3647; Wen Dong, 2012, Social Computing, Behavioral-Cultural Modeling and Prediction. Proceedings of the 5th International Conference, SBP 2012, P172, DOI 10.1007/978-3-642-29047-3_21; Wilkinson DJ., 2011, STOCHASTIC MODELING; Yedidia J., 2003, EXPLORING ARTIFICIAL, P236	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703089
C	Yang, F; Barber, RF; Jain, P; Lafferty, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yang, Fan; Barber, Rina Foygel; Jain, Prateek; Lafferty, John			Selective inference for group-sparse linear models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				REGRESSION	We develop tools for selective inference in the setting of group sparsity, including the construction of confidence intervals and p-values for testing selected groups of variables. Our main technical result gives the precise distribution of the magnitude of the projection of the data onto a given subspace, and enables us to develop inference procedures for a broad class of group-sparse selection methods, including the group lasso, iterative hard thresholding, and forward stepwise regression. We give numerical results to illustrate these tools on simulated data and on health record data.	[Yang, Fan; Barber, Rina Foygel] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Jain, Prateek] Microsoft Res India, Bengaluru, Karnataka, India; [Lafferty, John] Univ Chicago, Dept Stat, Chicago, IL 60637 USA; [Lafferty, John] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA	University of Chicago; University of Chicago; University of Chicago	Yang, F (corresponding author), Univ Chicago, Dept Stat, Chicago, IL 60637 USA.	fyang1@uchicago.edu; rina@uchicago.edu; prajain@microsoft.com; lafferty@galton.uchicago.edu			ONR [N00014-15-1-2379]; NSF [DMS-1513594, DMS-1547396]	ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	Research supported in part by ONR grant N00014-15-1-2379, and NSF grants DMS-1513594 and DMS-1547396.	[Anonymous], 2015, ARXIV151108866; [Anonymous], CORR; Hastie TJ, 2001, ELEMENTS STAT LEARNI; Jacob L., 2009, P 26 INT C MACH LEAR, P433, DOI DOI 10.1145/1553374.1553431; Lee J. D., 2014, ADV NEURAL INFORM PR, P136; Lee J.D., 2013, ARXIV13116238; Loftus J.R., 2014, ARXIV14053920; Loftus J. R., 2015, ARXIV151101478; Mosci S., 2010, ADV NEURAL INFORM PR, V23, P2604; R Development Core Team, 2021, LANG ENV STAT COMP; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R. J., 2014, ARXIV14013889; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	15	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703073
C	Yen, IEH; Huang, XR; Zhong, K; Zhang, RH; Ravikumar, P; Dhillon, IS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yen, Ian E. H.; Huang, Xiangru; Zhong, Kai; Zhang, Ruohan; Ravikumar, Pradeep; Dhillon, Inderjit S.			Dual Decomposed Learning with Factorwise Oracles for Structural SVMs of Large Output Domain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many applications of machine learning involve structured outputs with large domains, where learning of a structured predictor is prohibitive due to repetitive calls to an expensive inference oracle. In this work, we show that by decomposing training of a Structural Support Vector Machine (SVM) into a series of multiclass SVM problems connected through messages, one can replace an expensive structured oracle with Factorwise Maximization Oracles (FMOs) that allow efficient implementation of complexity sublinear to the factor domain. A Greedy Direction Method of Multiplier (GDMM) algorithm is then proposed to exploit the sparsity of messages while guarantees convergence to epsilon sub-optimality after O (log(1/epsilon)) passes of FMOs over every factor. We conduct experiments on chain-structured and fully-connected problems of large output domains, where the proposed approach is orders-of-magnitude faster than current state-of-the-art algorithms for training Structural SVMs.	[Yen, Ian E. H.; Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Huang, Xiangru; Zhong, Kai; Zhang, Ruohan; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA	Carnegie Mellon University; University of Texas System; University of Texas Austin	Yen, IEH (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.		Huang, Xiangru/ABG-5316-2021		ARO [W911NF-12-1-0390]; NSF [CCF-1320746, CCF-1117055, IIS-1149803, IIS-1546452, IIS-1320894, IIS-1447574, IIS-1546459, CCF-1564000, DMS-1264033]; NIH via as part of the Joint DMS/NIGMS Initiative [R01 GM117594-01]	ARO; NSF(National Science Foundation (NSF)); NIH via as part of the Joint DMS/NIGMS Initiative	We acknowledge the support of ARO via W911NF-12-1-0390, NSF via grants CCF-1320746, CCF-1117055, IIS-1149803, IIS-1546452, IIS-1320894, IIS-1447574, IIS-1546459, CCF-1564000, DMS-1264033, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.	Das D, 2014, COMPUT LINGUIST, V40, P9, DOI 10.1162/COLI_a_00163; Gimpel K, 2012, P 2012 C N AM CHAPT, P221; Hoffman A.J., 1952, J RES NBS; Hofmann T., 2004, P 21 INT C MACH LEAR, P104, DOI 10.1145/1015330.1015341; Hong M., 2012, ARXIV12083922; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kumar M. P., 2007, P ANN C NEUR INF PRO, V20, P1041; Lacoste-Julien S., 2013, P 30 INT C MACH LEAR, P53; Lacoste-Julien S, 2015, ADV NEURAL INFORM PR, V28, P496; Meshi O., 2015, AISTAT; Meshi O., 2010, LEARNING EFFICIENTLY; Meshi O., 2015, ARXIV151101419; Osokin A., 2016, ARXIV160509346; Ravikumar P., 2006, ICML; Samdani R., 2012, ICML; Shalev-Shwartz S., 2011, MATH PROGRAMMING; Su TH, 2007, INT J DOC ANAL RECOG, V10, P27, DOI 10.1007/s10032-006-0037-6; Taskar B., 2003, ADV NEURAL INFORM PR, V16; Taskar B., 2005, ICML; Woodland P., 2000, ASR2000 AUT SPEECH R; Yen I., 2015, NIPS; Yen I. E., 2016, CONVEX ATOMIC NORM A; Yen I. E., 2016, AISTAT	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703058
C	Yu, FX; Suresh, AT; Choromanski, K; Holtmann-Rice, D; Kumar, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yu, Felix Xinnan; Suresh, Ananda Theertha; Choromanski, Krzysztof; Holtmann-Rice, Daniel; Kumar, Sanjiv			Orthogonal Random Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present an intriguing discovery related to Random Fourier Features: in Gaussian kernel approximation, replacing the random Gaussian matrix by a properly scaled random orthogonal matrix significantly decreases kernel approximation error. We call this technique Orthogonal Random Features (ORF), and provide theoretical and empirical justification for this behavior. Motivated by this discovery, we further propose Structured Orthogonal Random Features (SORF), which uses a class of structured discrete orthogonal matrices to speed up the computation. The method reduces the time cost from O (d(2)) to O (d log d), where d is the data dimensionality, with almost no compromise in kernel approximation quality compared to ORE Experiments on several datasets verify the effectiveness of ORF and SORF over the existing methods. We also provide discussions on using the same type of discrete orthogonal structure for a broader range of applications.	[Yu, Felix Xinnan; Suresh, Ananda Theertha; Choromanski, Krzysztof; Holtmann-Rice, Daniel; Kumar, Sanjiv] Google Res, New York, NY 10011 USA	Google Incorporated	Yu, FX (corresponding author), Google Res, New York, NY 10011 USA.	felixyu@google.com; theertha@google.com; kchoro@google.com; dhr@google.com; sanjivk@google.com						Ailon N., 2006, STOC; Andoni A., 2015, PRACTICAL OPTIMAL LS; Bochner S, 1955, HARMONIC ANAL THEORY; Charikar M. S., 2002, STOC; Cheng Yu, 2015, ICCV; Choromanski  K., 2016, TRIPLESPIN A GENERIC; Choromanski  K., 2015, ICML; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; FINO BJ, 1976, IEEE T COMPUT, V25, P1142, DOI 10.1109/TC.1976.1674569; Joachims T., 2006, KDD; Kar P., 2012, AISTATS; Kennedy C, 2016, FAST CROSS POLYTOPE; Le Q., 2013, ICML; Li FX, 2010, LECT NOTES COMPUT SC, V6376, P262; Maji S., 2009, ICCV; Muirhead R. J., 2009, ASPECTS MULTIVARIATE, V197; Niederreiter H., 2010, QUASIMONTE CARLO MET; Pennington Jeffrey, 2015, NIPS; Rahimi A., 2007, NIPS; Rudi Alessandro, 2016, ARXIV160204474; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Sreekanth V., 2010, BMVC; Sriperumbudur B. K., 2015, NIPS; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Yang J., 2014, ICML; Yang T., 2012, NIPS; Yu F. X., 2014, ICML; Yu F. X., 2015, ARXIV150303893; Zhang X., 2015, ICCV	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701103
C	Yun, SY; Proutiere, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yun, Se-Young; Proutiere, Alexandre			Optimal Cluster Recovery in the Labeled Stochastic Block Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider the problem of community detection or clustering in the labeled Stochastic Block Model (LSBM) with a finite number K of clusters of sizes linearly growing with the global population of items n. Every pair of items is labeled independently at random, and label 'appears with probability p (i, j, l) between two items in clusters indexed by i and j, respectively. The objective is to reconstruct the clusters from the observation of these random labels. Clustering under the SBM and their extensions has attracted much attention recently. Most existing work aimed at characterizing the set of parameters such that it is possible to infer clusters either positively correlated with the true clusters, or with a vanishing proportion of misclassified items, or exactly matching the true clusters. We find the set of parameters such that there exists a clustering algorithm with at most s misclassified items in average under the general LSBM and for any s = o(n), which solves one open problem raised in [2]. We further develop an algorithm, based on simple spectral methods, that achieves this fundamental performance limit within O(n polylog(n)) computations and without the a-priori knowledge of the model parameters.	[Yun, Se-Young] Los Alamos Natl Lab, CNLS, Los Alamos, NM 87545 USA; [Proutiere, Alexandre] KTH, Automat Control Dept, S-10044 Stockholm, Sweden	United States Department of Energy (DOE); Los Alamos National Laboratory; Royal Institute of Technology	Yun, SY (corresponding author), Los Alamos Natl Lab, CNLS, Los Alamos, NM 87545 USA.	syun@lanl.gov; alepro@kth.se	Yun, Seyoung/M-6903-2017		U.S. Department of Energy through the LANL/LDRD Program	U.S. Department of Energy through the LANL/LDRD Program(United States Department of Energy (DOE))	We gratefully acknowledge the support of the U.S. Department of Energy through the LANL/LDRD Program for this work.	Abbe E., 2015, NIPS; Abbe Emmanuel, 2014, ABS14053267 CORR; Abbe Emmanuel, 2015, FOCS; Coja-Oghlan A, 2010, COMB PROBAB COMPUT, V19, P227, DOI 10.1017/S0963548309990514; Decelle A., 2011, PHYS REV LETT, V107; Deshpande Y., 2015, ABS150708685 CORR; Gao C., 2015, ABS150503772 CORR; Hajek B., 2015, ABS150907859 CORR; Hajek B., 2014, ABS14126156 CORR; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Heimlicher S., 2012, NIPS WORKSH ALG STAT; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jog V., 2015, ABS150906418 CORR; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Leskovec Jure, 2010, CHI; Massoulie L., 2014, STOC; Mossel E., 2015, STOC; Mossel E., 2015, ABS150903281 CORR; Traag VA, 2009, PHYS REV E, V80, DOI 10.1103/PhysRevE.80.036115; Yun S., 2014, ABS14127335 CORR; Yun S., 2014, COLT; Zhang A., 2015, ABS150705313 CORR; Zhang P., 2015, ABS150900107 CORR	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701053
C	Zheng, S; Yue, YS; Lucey, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zheng, Stephan; Yue, Yisong; Lucey, Patrick			Generating Long-term Trajectories Using Deep Hierarchical Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study the problem of modeling spatiotemporal trajectories over long time horizons using expert demonstrations. For instance, in sports, agents often choose action sequences with long-term goals in mind, such as achieving a certain strategic position. Conventional policy learning approaches, such as those based on Markov decision processes, generally fail at learning cohesive long-term behavior in such high-dimensional state spaces, and are only effective when fairly myopic decision-making yields the desired behavior. The key difficulty is that conventional models are "single-scale" and only learn a single state-action policy. We instead propose a hierarchical policy class that automatically reasons about both long-term and short-term goals, which we instantiate as a hierarchical neural network. We showcase our approach in a case study on learning to imitate demonstrated basketball trajectories, and show that it generates significantly more realistic trajectories compared to non-hierarchical baselines as judged by professional sports analysts.	[Zheng, Stephan; Yue, Yisong] CALTECH, Pasadena, CA 91125 USA; [Lucey, Patrick] STATS, Chicago, IL USA	California Institute of Technology	Zheng, S (corresponding author), CALTECH, Pasadena, CA 91125 USA.	stzheng@caltech.edu; yyue@caltech.edu; plucey@stats.com			NSF [1564330]	NSF(National Science Foundation (NSF))	This research was supported in part by NSF Award #1564330, and a GPU donation (Tesla K40 and Titan X) by NVIDIA.	Bai AJ, 2015, ACM T INTEL SYST TEC, V6, DOI 10.1145/2717316; Byrne RW, 1998, BEHAV BRAIN SCI, V21, P667, DOI 10.1017/S0140525X98001745; Chung J, 2015, ADV NEURAL INFORM PR, P2980; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Evans JST, 2013, PERSPECT PSYCHOL SCI, V8, P223, DOI 10.1177/1745691612460685; Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000; Hausknecht Matthew, P INT C LEARN REPR I; He Ruijie, 2010, 24 AAAI C ART INT JU; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Konidaris G, 2012, INT J ROBOT RES, V31, P360, DOI 10.1177/0278364911428653; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Muelling K, 2014, BIOL CYBERN, V108, P603, DOI 10.1007/s00422-014-0599-1; Panait L, 2005, AUTON AGENT MULTI-AG, V11, P387, DOI 10.1007/s10458-005-2631-2; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Xu K, 2015, ARXIV150203044; Yue Yisong, IEEE INT C DAT MIN I; Ziebart B. D., 2008, AAAI, V8, P1433	17	1	1	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704045
C	Zhou, YX; Spanos, CJ		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhou, Yuxun; Spanos, Costas J.			Causal meets Submodular: Subset Selection with Directed Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We study causal subset selection with Directed Information as the measure of prediction causality. Two typical tasks, causal sensor placement and covariate selection, are correspondingly formulated into cardinality constrained directed information maximizations. To attack the NP-hard problems, we show that the first problem is submodular while not necessarily monotonic. And the second one is "nearly" submodular. To substantiate the idea of approximate submodularity, we introduce a novel quantity, namely submodularity index (SmI), for general set functions. Moreover, we show that based on SmI, greedy algorithm has performance guarantee for the maximization of possibly non-monotonic and non-submodular functions, justifying its usage for a much broader class of problems. We evaluate the theoretical results with several case studies, and also illustrate the application of the subset selection to causal structure learning.	[Zhou, Yuxun; Spanos, Costas J.] UC Berekely, Dept EECS, Berkeley, CA 94720 USA		Zhou, YX (corresponding author), UC Berekely, Dept EECS, Berkeley, CA 94720 USA.	yxzhou@berkeley.edu; spanos@berkeley.edu	Zhou, Yuxun/AAA-1180-2021		Republic of Singapore's National Research Foundation	Republic of Singapore's National Research Foundation(National Research Foundation, Singapore)	This research is funded by the Republic of Singapore's National Research Foundation through a grant to the Berkeley Education Alliance for Research in Singapore (BEARS) for the Singapore-Berkeley Building Efficiency and Sustainability in the Tropics (SinBerBEST) Program. BEARS has been established by the University of California, Berkeley as a center for intellectual excellence in research and education in Singapore. We also thank the reviews for their helpful suggestions.	Abrams Z, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P424; Buchbinder J. S. N. Niv, 2014, ACM SIAM S DISCR ALG; Cevher V, 2011, IEEE J-STSP, V5, P979, DOI 10.1109/JSTSP.2011.2161862; Das D. K. Abhimanyu, 2011, P ICML 2011 SEATTL W; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Feldman M, 2011, ANN IEEE SYMP FOUND, P570, DOI 10.1109/FOCS.2011.46; Galstyan A., 2013, P 6 ACM INT C WEB SE, P3, DOI [10.1145/2433396.2433400, DOI 10.1145/2433396.2433400, /10.1145 /2433396.2433400]; Jiao JT, 2013, IEEE T INFORM THEORY, V59, P6220, DOI 10.1109/TIT.2013.2267934; Kawahara Y., 2011, P 25 ANN C NEUR INF, P2106; KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684; Krause A, 2005, UAI; Krause A, 2008, J MACH LEARN RES, V9, P235; Lin H, 2011, ACL HLT; Lozano A. N.-M. Y. L. C. P. J. H. N. A. A., 2009, ACM SIGKDD C KNOWL D; Mathai P., 2007, P INF THEOR APPL WOR, P274; Murphy K., 2001, COMPUTING SCI STAT, V33, P1024; Narasimhan M., 2012, ARXIV12071404; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Quinn CJ, 2015, IEEE T INFORM THEORY, V61, P6887, DOI 10.1109/TIT.2015.2478440; Quinn CJ, 2011, J COMPUT NEUROSCI, V30, P17, DOI 10.1007/s10827-010-0247-2; Sheehan D. V. B. P. R., 2008, PLOS MED; Yuxun Zhou, 2013, 2013 IEEE International Conference on Automation Science and Engineering (CASE), P593, DOI 10.1109/CoASE.2013.6654000	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703020
C	Acharya, J; Daskalakis, C; Kamath, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Acharya, Jayadev; Daskalakis, Constantinos; Kamath, Gautam			Optimal Testing for Properties of Distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DENSITY	Given samples from an unknown discrete distribution p, is it possible to distinguish whether p belongs to some class of distributions C versus p being far from every distribution in C? This fundamental question has received tremendous attention in statistics, focusing primarily on asymptotic analysis, as well as in information theory and theoretical computer science, where the emphasis has been on small sample size and computational complexity. Nevertheless, even for basic properties of discrete distributions such as monotonicity, independence, log-concavity, unimodality, and monotone-hazard rate, the optimal sample complexity is unknown. We provide a general approach via which we obtain sample-optimal and computationally efficient testers for all these distribution families. At the core of our approach is an algorithm which solves the following problem: Given samples from an unknown distribution p, and a known distribution q, are p and q close in ! 2-distance, or far in total variation distance? The optimality of our testers is established by providing matching lower bounds, up to constant factors. Finally, a necessary building block for our testers and an important byproduct of our work are the first known computationally efficient proper learners for discrete log-concave, monotone hazard rate distributions.	[Acharya, Jayadev; Daskalakis, Constantinos; Kamath, Gautam] MIT, EECS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Acharya, J (corresponding author), MIT, EECS, Cambridge, MA 02139 USA.	jayadev@mit.edu; costis@mit.edu; g@mit.edu						Acharya J., 2015, P 26 ANN ACM SIAM S, P1829, DOI [10.1137/1.9781611973730.122, DOI 10.1137/1.9781611973730.122]; Acharya J., 2014, ISIT; Acharya J., 2012, COLT; Adamaszek M, 2010, PROC APPL MATH, V135, P56; Agresti A., 2011, CATEGORICAL DATA ANA; Alon N., 2007, P STOC; [Anonymous], 2013, J MACH LEARN RES; Balabdaoui F., 2011, MAXIMUM LIKELIHOOD E; Balabdaoui F, 2010, STAT NEERL, V64, P45, DOI 10.1111/j.1467-9574.2009.00438.x; Barlow R. E., 1972, STAT INFERENCE ORDER; Batu T., 2001, P FOCS; Batu T., 2004, P STOC; Bhattacharya B., 2015, NIPS; Bhattacharyya A., 2011, P ITCS, P239; BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488; Canonne C., 2016, STACS; Canonne C. L., 2015, ECCC; Chan S., 2014, P 25 ANN ACM SIAM S, P1193, DOI [10.1137/1.9781611973402.88, DOI 10.1137/1.9781611973402.88]; Chan S.-O., 2013, P SODA; Cule M, 2010, ELECTRON J STAT, V4, P254, DOI 10.1214/09-EJS505; Fischer E., 2001, SCIENCE; Fisher R. A, 1925, STAT METHODS RES WOR; Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865; Hall P, 2005, ANN STAT, V33, P1109, DOI 10.1214/009053605000000039; Huang DY, 2013, IEEE T INFORM THEORY, V59, P8157, DOI 10.1109/TIT.2013.2283266; Jankowski HK, 2009, ELECTRON J STAT, V3, P1567, DOI 10.1214/09-EJS526; Kamath S., 2015, COLT; Lehmann E. L., 2006, SPRINGER TEXTS STAT; Levi R., 2013, THEORY COMPUT, V9, P295; Paninski L., 2008, IEEE T INFORM THEORY, V54; RAO JNK, 1981, J AM STAT ASSOC, V76, P221, DOI 10.2307/2287815; Rubinfeld R., 2006, INT C MATH; Valiant G., 2011, P STOC; Valiant G., 2014, FOCS	36	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101098
C	Alabdulmohsin, I		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Alabdulmohsin, Ibrahim			Algorithmic Stability and Uniform Generalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LEARNABILITY; BOUNDS	One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result. For instance, a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.	[Alabdulmohsin, Ibrahim] King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia	King Abdullah University of Science & Technology	Alabdulmohsin, I (corresponding author), King Abdullah Univ Sci & Technol, Thuwal 23955, Saudi Arabia.	ibrahim.alabdulmohsin@kaust.edu.sa		Alabdulmohsin, Ibrahim/0000-0002-9387-5820				Audibert JY, 2007, J MACH LEARN RES, V8, P863; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Diaconis P., 1991, STAT SCI, V6, P284, DOI 10.1214/ss/1177011699; Downs T, 2002, J MACH LEARN RES, V2, P293, DOI 10.1162/15324430260185637; Elisseeff A., 2002, NATO ASI SERIES LE 3; Kutin S., 2002, P 18 C UNC ART INT U; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Robbins H., 1955, AM MATH MON, V62, P26, DOI [DOI 10.2307/2308012, 10.2307/2308012]; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Stigler S. M., 1986, HIST STAT MEASUREMEN; Vapnik V, 2000, NEURAL COMPUT, V12, P2013, DOI 10.1162/089976600300015042; Vapnik V. N., 1999, NEURAL NETWORKS IEEE, V10; Wager Stefan, 2013, ADV NEURAL INFORM PR, P351; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103057
C	Alistarh, D; Iglesias, J; Vojnovic, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Alistarh, Dan; Iglesias, Jennifer; Vojnovic, Milan			Streaming Min-Max Hypergraph Partitioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In many applications, the data is of rich structure that can be represented by a hypergraph, where the data items are represented by vertices and the associations among items are represented by hyperedges. Equivalently, we are given an input bipartite graph with two types of vertices: items, and associations (which we refer to as topics). We consider the problem of partitioning the set of items into a given number of components such that the maximum number of topics covered by a component is minimized. This is a clustering problem with various applications, e.g. partitioning of a set of information objects such as documents, images, and videos, and load balancing in the context of modern computation platforms. In this paper, we focus on the streaming computation model for this problem, in which items arrive online one at a time and each item must be assigned irrevocably to a component at its arrival time. Motivated by scalability requirements, we focus on the class of streaming computation algorithms with memory limited to be at most linear in the number of components. We show that a greedy assignment strategy is able to recover a hidden co-clustering of items under a natural set of recovery conditions. We also report results of an extensive empirical evaluation, which demonstrate that this greedy strategy yields superior performance when compared with alternative approaches.	[Alistarh, Dan; Vojnovic, Milan] Microsoft Res, Cambridge, England; [Iglesias, Jennifer] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Microsoft; Carnegie Mellon University	Alistarh, D (corresponding author), Microsoft Res, Cambridge, England.	dan.alistarh@microsoft.com; jiglesia@andrew.cmu.edu; milanv@microsoft.com						Bansal N, 2014, SIAM J COMPUT, V43, P872, DOI 10.1137/120873996; Bourse F., 2014, P ACM KDD; Chen Y.-N., 2012, P NIPS; Cheng Y, 2000, Proc Int Conf Intell Syst Mol Biol, V8, P93; Chung F, 2003, ANN COMB, V7, P141; DHILLON I, 2001, P ACM KDD; Dhillon I. S., 2003, P ACM KDD; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Hein M., 2013, P NIPS; Karagiannis T., 2010, P ACM SOCC; Karypis G., 2000, VLSI DESIGN, V11; Krauthgamer R., 2009, PARTITIONING GRAPHS; Li M., 2015, ARXIV PREPRINT ARXIV; Massoulie L., 2014, P ACM STOC; Mitliagkas I., 2013, P NIPS; Mossel E., 2014, PROBABILITY THEORY R, V162, P1; O'Connor L., 2014, P NIPS; Pujol JM, 2012, IEEE ACM T NETWORK, V20, P1162, DOI 10.1109/TNET.2012.2188815; Stanton I., 2014, P ACM SIAM SODA; Stanton I., 2012, P ACM KDD; Svitkina Z, 2004, LECT NOTES COMPUT SC, V3122, P207; Tsourakakis C. E., 2014, P ACM WSDM; Yun S.Y., 2014, ADV NEURAL INF PROCE, V4; Zong B., 2015, P ACM DEBS	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102046
C	Arjevani, Y; Shamir, O		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Arjevani, Yossi; Shamir, Ohad			Communication Complexity of Distributed Convex Learning and Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the fundamental limits to communication-efficient distributed methods for convex learning and optimization, under different assumptions on the information available to individual machines, and the types of functions considered. We identify cases where existing algorithms are already worst-case optimal, as well as cases where room for further improvement is still possible. Among other things, our results indicate that without similarity between the local objective functions (due to statistical data similarity or otherwise) many communication rounds may be required, even if the machines have unbounded computational power.	[Arjevani, Yossi; Shamir, Ohad] Weizmann Inst Sci, IL-7610001 Rehovot, Israel	Weizmann Institute of Science	Arjevani, Y (corresponding author), Weizmann Inst Sci, IL-7610001 Rehovot, Israel.	yossi.arjevani@weizmann.ac.il; ohad.shamir@weizmann.ac.il			FP7 Marie Curie CIG grant; Intel ICRI-CI Institute; Israel Science Foundation [425/13]	FP7 Marie Curie CIG grant; Intel ICRI-CI Institute; Israel Science Foundation(Israel Science Foundation)	This research is supported in part by an FP7 Marie Curie CIG grant, the Intel ICRI-CI Institute, and Israel Science Foundation grant 425/13. We thank Nati Srebro for several helpful discussions and insights.	Agarwal A., 2011, ABS11104198 CORR; Balcan M.-F., 2012, COLT; Balcan Maria-Florina, 2014, NIPS; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Clarkson K. L., 2009, STOC; Cotter A., 2011, NIPS; Dekel O, 2012, J MACH LEARN RES, V13, P165; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Frostig R., 2014, ARXIV14126606; Jaggi M., 2014, NIPS; Lee J., 2015, 150707595 CORR; Mahajan D., 2013, ABS13110636 CORR; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2018, APPL OPTIMIZATION; Niu F., 2011, NIPS 11; Richtarik P., 2013, ABS13102059 CORR; Shamir O., 2014, NIPS; Shamir O., 2014, ALL C COMM CONTR COM; Shamir Ohad, 2014, ICML; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; Tsitsiklis J. N., 1987, Journal of Complexity, V3, P231, DOI 10.1016/0885-064X(87)90013-6; Yang T., 2013, NIPS; Yu Y., 2013, P NEURIPS; ZHANG Y, 2015, ICML; Zhang YC, 2013, J MACH LEARN RES, V14, P3321; Zinkevich M., 2010, NIPS, V4, P4	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100099
C	Babanezhad, R; Ahmed, MO; Virani, A; Schmidt, M; Konecny, J; Sallinen, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Babanezhad, Reza; Ahmed, Mohamed Osama; Virani, Alim; Schmidt, Mark; Konecny, Jakub; Sallinen, Scott			Stop Wasting My Gradients: Practical SVRG	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present and analyze several strategies for improving the performance of stochastic variance-reduced gradient (SVRG) methods. We first show that the convergence rate of these methods can be preserved under a decreasing sequence of errors in the control variate, and use this to derive variants of SVRG that use growing-batch strategies to reduce the number of gradient calculations required in the early iterations. We further (i) show how to exploit support vectors to reduce the number of gradient computations in the later iterations, (ii) prove that the commonly-used regularized SVRG iteration is justified and improves the convergence rate, (iii) consider alternate mini-batch selection strategies, and (iv) consider the generalization error of the method.	[Babanezhad, Reza; Ahmed, Mohamed Osama; Virani, Alim; Schmidt, Mark] Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada; [Konecny, Jakub] Univ Edinburgh, Sch Math, Edinburgh, Midlothian, Scotland; [Sallinen, Scott] Univ British Columbia, Dept Elect & Comp Engn, Vancouver, BC, Canada	University of British Columbia; University of Edinburgh; University of British Columbia	Babanezhad, R (corresponding author), Univ British Columbia, Dept Comp Sci, Vancouver, BC, Canada.	rezababa@cs.ubc.ca; moahmed@cs.ubc.ca; alim.virani@gmail.com; schmidtm@cs.ubc.ca; kubo.konecny@gmail.com; scotts@ece.ubc.ca	Sallinen, Scott/AAG-7640-2020		Natural Sciences and Engineering Research Council of Canada [RGPIN 312176-2010, RGPIN 311661-08, RGPIN-06068-2015]; Google European Doctoral Fellowship	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); Google European Doctoral Fellowship(Google Incorporated)	We would like to thank the reviewers for their helpful comments. This research was supported by the Natural Sciences and Engineering Research Council of Canada (RGPIN 312176-2010, RGPIN 311661-08, RGPIN-06068-2015). Jakub Konecny is supported by a Google European Doctoral Fellowship.	[Anonymous], 1999, ADV KERNEL METHODS S, DOI DOI 10.17877/DE290R-5098; [Anonymous], 2009, SAMPLING DESIGN ANAL; Aravkin A, 2012, MATH PROGRAM, V134, P101, DOI 10.1007/s10107-012-0571-6; Bottou L., 2007, ADV NEURAL INFORM PR; Byrd RH, 2012, MATH PROGRAM, V134, P127, DOI 10.1007/s10107-012-0572-5; Defazio Aaron, 2014, ADV NEURAL INFORM PR; Friedlander MP, 2012, SIAM J SCI COMPUT, V34, pA1380, DOI 10.1137/110830629; Hu C., 2009, ADV NEURAL INFORM PR; Johoson R., 2013, ADV NEURAL INFORM PR; Konecny J., 2014, MS2GD MINI BATCH SEM; Konen J., 2013, SEMISTOCHASTIC GRADI; Le Roux Nicolas, 2012, ADV NEURAL INFORM PR; Mahdavi Mehrdad, 2013, ADV NEURAL INFORM PR; Mairal J., 2013, INT C MACH LEARN ICM; Rosset S, 2007, ANN STAT, V35, P1012, DOI 10.1214/009053606000001370; Usunier N., 2010, INT C ART INT STAT A; van den Doel K, 2012, SIAM J SCI COMPUT, V34, pA185, DOI 10.1137/110826692; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang L., 2013, ADV NEURAL INFORM PR	21	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100079
C	Balsubramani, A; Freund, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Balsubramani, Akshay; Freund, Yoav			Scalable Semi-Supervised Aggregation of Classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present and empirically evaluate an efficient algorithm that learns to aggregate the predictions of an ensemble of binary classifiers. The algorithm uses the structure of the ensemble predictions on unlabeled data to yield significant performance improvements. It does this without making assumptions on the structure or origin of the ensemble, without parameters, and as scalably as linear learning. We empirically demonstrate these performance gains with random forests.	[Balsubramani, Akshay; Freund, Yoav] Univ Calif San Diego, San Diego, CA 92093 USA	University of California System; University of California San Diego	Balsubramani, A (corresponding author), Univ Calif San Diego, San Diego, CA 92093 USA.	abalsubr@cs.ucsd.edu; yfreund@cs.ucsd.edu		freund, yoav/0000-0002-3850-6184	National Science Foundation [IIS-1162581]	National Science Foundation(National Science Foundation (NSF))	The authors acknowledge support from the National Science Foundation under grant IIS-1162581.	[Anonymous], 2012, PREDICTING BIOL RESP; Balsubramani Akshay, 2015, C LEARN THEOR; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.1007/bf00058655; Caruana R., 2008, P 25 INT C MACH LEAR, DOI DOI 10.1145/1390156.1390169; Chapelle O., SEMISUPERVISED LEARN; Dawid A.P., 1979, APPL STAT, V28, P20, DOI [10.2307/2346806, DOI 10.2307/2346806]; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Freund Y., 1996, Proceedings of the Ninth Annual Conference on Computational Learning Theory, P325, DOI 10.1145/238061.238163; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Freund Y., 1997, P 20 9 ANN ACM S THE, P334, DOI [10.1145/258533.258616, DOI 10.1145/258533.258616]; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Koren Y., 2009, NETFLIX PRIZE DOCUME, V81, P1; Lin Y, 2006, J AM STAT ASSOC, V101, P578, DOI 10.1198/016214505000001230; Mallapragada PK, 2009, IEEE T PATTERN ANAL, V31, P2000, DOI 10.1109/TPAMI.2008.235; Moosmann F., 2007, ADV NEURAL INF PROCE, V19, P985; Parisi F, 2014, P NATL ACAD SCI USA, V111, P1253, DOI 10.1073/pnas.1219097111; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schapire RE, 1998, ANN STAT, V26, P1651; Schapire Robert E., 2012, BOOSTING FDN ALGORIT; Weinberger K., 2009, P 26 ANN INT C MACH, P1113, DOI DOI 10.1145/1553374.1553516; Zhu X., 2009, SYNTHESIS LECT ARTIF, V3, P1, DOI [10.2200/S00196ED1V01Y200906AIM006, DOI 10.2200/S00196ED1V01Y200906AIM006]	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102090
C	Borgs, C; Chayes, JT; Smith, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Borgs, Christian; Chayes, Jennifer T.; Smith, Adam			Private Graphon Estimation for Sparse Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				STOCHASTIC BLOCKMODELS; CONVERGENT SEQUENCES	We design algorithms for fitting a high-dimensional statistical model to a large, sparse network without revealing sensitive information of individual members. Given a sparse input graph G, our algorithms output a node-differentially private nonparametric block model approximation. By node-differentially private, we mean that our output hides the insertion or removal of a vertex and all its adjacent edges. If G is an instance of the network obtained from a generative nonparametric model defined in terms of a graphon W, our model guarantees consistency: as the number of vertices tends to infinity, the output of our algorithm converges to W in an appropriate version of the L2 norm. In particular, this means we can estimate the sizes of all multi-way cuts in G. Our results hold as long as W is bounded, the average degree of G grows at least like the log of the number of vertices, and the number of blocks goes to infinity at an appropriate rate. We give explicit error bounds in terms of the parameters of the model; in several settings, our bounds improve on or match known nonprivate results.	[Borgs, Christian; Chayes, Jennifer T.] Microsoft Res New England, Cambridge, MA 02142 USA; [Smith, Adam] Penn State Univ, University Pk, PA 16802 USA	Microsoft; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Borgs, C (corresponding author), Microsoft Res New England, Cambridge, MA 02142 USA.	cborgs@microsoft.com; jchayes@microsoft.com; asmith@psu.edu	Smith, Adam/GPS-8322-2022		NSF [IIS-1447700]; Google Faculty Award	NSF(National Science Foundation (NSF)); Google Faculty Award(Google Incorporated)	A.S. was supported by NSF award IIS-1447700 and a Google Faculty Award. Part of this work was done while visiting Boston University's Hariri Institute for Computation and Harvard University's Center for Research on Computation and Society.	Abbe E., 2015, RECOVERING COM UNPUB; Abbe E., 2015, ARXIV150300609; [Anonymous], 2014, ARXIV14053267; Bickel PJ, 2009, P NATL ACAD SCI USA, V106, P21068, DOI 10.1073/pnas.0907096106; Blocki J., 2013, P 4 C INNOVATIONS TH, P87; Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168; Borgs C, 2012, ANN MATH, V176, P151, DOI 10.4007/annals.2012.176.1.2; BORGS C, 2014, ARXIV14080744; Borgs C, 2014, ARXIV14012906; Borgs C., 2006, ALGORITHMS COMBIN, P315, DOI [DOI 10.1007/3-540-33700-8, 10.1007/3-540-33700-8_18]; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Chen S., 2013, P ACM SIGMOD INT C M, P653; Choi DS, 2012, BIOMETRIKA, V99, P273, DOI 10.1093/biomet/asr053; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Gao C., 2014, ARXIV14105837; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Kasiviswanathan SP, 2013, LECT NOTES COMPUT SC, V7785, P457, DOI 10.1007/978-3-642-36594-2_26; Lovasz L, 2006, J COMB THEORY B, V96, P933, DOI 10.1016/j.jctb.2006.05.002; Lu WT, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P921, DOI 10.1145/2623330.2623683; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Raskhodnikova S., 2015, CORR150407912; Rohe K, 2011, ANN STAT, V39, P1878, DOI 10.1214/11-AOS887; Verzelen, 2015, ARXIV150704118; Wolfe P.J., 2013, ARXIV13095936	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101087
C	Bubeck, S; Eldan, R; Lehec, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bubeck, Sebastien; Eldan, Ronen; Lehec, Joseph			Finite-Time Analysis of Projected Langevin Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We analyze the projected Langevin Monte Carlo (LMC) algorithm, a close cousin of projected Stochastic Gradient Descent (SGD). We show that LMC allows to sample in polynomial time from a posterior distribution restricted to a convex body and with concave log-likelihood. This gives the first Markov chain to sample from a log-concave distribution with a first-order oracle, as the existing chains with provable guarantees (lattice walk, ball walk and hit-and-run) require a zeroth-order oracle. Our proof uses elementary concepts from stochastic calculus which could be useful more generally to understand SGD and its variants.	[Bubeck, Sebastien] Microsoft Res, New York, NY 10011 USA; [Eldan, Ronen] Weizmann Inst Sci, Rehovot, Israel; [Lehec, Joseph] Univ Paris 09, Paris, France	Microsoft; Weizmann Institute of Science; UDICE-French Research Universities; PSL Research University Paris; Universite Paris-Dauphine	Bubeck, S (corresponding author), Microsoft Res, New York, NY 10011 USA.	sebubeck@microsoft.com; roneneldan@gmail.com; lehec@ceremade.dauphine.fr						Ahn S., 2012, ICML 2012; Bach F., ADV NEURAL INFORM PR, V26, P773; Cousins B., 2014, ARXIV14096011; Dalalyan A. S., 2014, ARXIV14127392; DYER M, 1991, J ACM, V38, P1, DOI 10.1145/102782.102783; Kannan R, 2012, MATH OPER RES, V37, P1, DOI 10.1287/moor.1110.0519; Lovasz L, 2006, SIAM J COMPUT, V35, P985, DOI 10.1137/S009753970544727X; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; PFLUG GC, 1986, SIAM J CONTROL OPTIM, V24, P655, DOI 10.1137/0324039; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Roberts G.O., 1996, BERNOULLI, V2, P341, DOI DOI 10.2307/3318418; Skorokhod AV., 1961, THEORY PROBAB APPL, V6, P264, DOI [10.1137/1106035, DOI 10.1137/1106035]; Tanaka H., 1979, HIROSHIMA MATH J, V9, P163; Welling M., 2011, ICML 2011	15	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102082
C	Chakraborty, M; Das, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chakraborty, Mithun; Das, Sanmay			Market Scoring Rules Act As Opinion Pools For Risk-Averse Agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INFORMATION AGGREGATION; PREDICTION	A market scoring rule (MSR) - a popular tool for designing algorithmic prediction markets - is an incentive-compatible mechanism for the aggregation of probabilistic beliefs from myopic risk-neutral agents. In this paper, we add to a growing body of research aimed at understanding the precise manner in which the price process induced by a MSR incorporates private information from agents who deviate from the assumption of risk-neutrality. We first establish that, for a myopic trading agent with a risk-averse utility function, a MSR satisfying mild regularity conditions elicits the agent's risk-neutral probability conditional on the latest market state rather than her true subjective probability. Hence, we show that a MSR under these conditions effectively behaves like a more traditional method of belief aggregation, namely an opinion pool, for agents' true probabilities. In particular, the logarithmic market scoring rule acts as a logarithmic pool for constant absolute risk aversion utility agents, and as a linear pool for an atypical budget-constrained agent utility with decreasing absolute risk aversion. We also point out the interpretation of a market maker under these conditions as a Bayesian learner even when agent beliefs are static.	[Chakraborty, Mithun; Das, Sanmay] Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA	Washington University (WUSTL)	Chakraborty, M (corresponding author), Washington Univ, Dept Comp Sci & Engn, St Louis, MO 63130 USA.	mithunchakraborty@wustl.edu; sanmay@wustl.edu	Chakraborty, Mithun/CAF-8862-2022		NSF IIS [1414452, 1527037]	NSF IIS(National Science Foundation (NSF))	We are grateful for support from NSF IIS awards 1414452 and 1527037.	Abernethy J., 2014, P 15 ACM C EC COMP, P395; Beygelzimer A., 2012, P 11 INT C AUT AG MU, P1317; BORDLEY RF, 1982, MANAGE SCI, V28, P1137, DOI 10.1287/mnsc.28.10.1137; Boucheron S, 2004, LECT NOTES ARTIF INT, V3176, P208; Brahma Aseem, 2012, P 13 ACM C EL COMM, P215; Chen Y., 2010, P 11 ACM C EL COMM, P189; Chen Yiling, 2007, P UAI 07; Cliff Dave, 1997, TECHNICAL REPORT, P105; Cowgill B, 2015, REV ECON STUD, V82, P1309, DOI 10.1093/restud/rdv014; Farmer JD, 2005, P NATL ACAD SCI USA, V102, P2254, DOI 10.1073/pnas.0409157102; Frongillo R., 2012, ADV NEURAL INFORM PR, V25, P3266; Garg A., 2004, P 8 INT S ART INT MA; Genest C., 1986, STAT SCI, P114, DOI [10.1214/ss/1177013825, DOI 10.1214/SS/1177013825]; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Hu Jinli, 2014, P ICML; Iyer K, 2014, MANAGE SCI, V60, P2509, DOI 10.1287/mnsc.2014.1929; Mas-Colell Andreu, 1995, MICROECONOMIC THEORY, V1; Millin Jono, 2012, P 29 INT C MACH LEAR, P1815; Ostrovsky M, 2012, ECONOMETRICA, V80, P2595, DOI 10.3982/ECTA8479; Pennock D. M., 1999, THESIS; Sethi Rajiv, 2015, COMPUTATIONAL EC; Storkey AJ, 2015, LECT NOTES ARTIF INT, V9284, P560, DOI 10.1007/978-3-319-23528-8_35; Surowiecki J, 2004, WISDOM CROWDS; Wolfers J, 2004, J ECON PERSPECT, V18, P107, DOI 10.1257/0895330041371321; Xia L, 2011, C UNC ART INT, P581	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101099
C	Chen, YC; Genovese, CR; Ho, S; Wasserman, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chen, Yen-Chi; Genovese, Christopher R.; Ho, Shirley; Wasserman, Larry			Optimal Ridge Detection using Coverage Risk	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BANDWIDTH SELECTION; MEAN-SHIFT	We introduce the concept of coverage risk as an error measure for density ridge estimation. The coverage risk generalizes the mean integrated square error to set estimation. We propose two risk estimators for the coverage risk and we show that we can select tuning parameters by minimizing the estimated risk. We study the rate of convergence for coverage risk and prove consistency of the risk estimators. We apply our method to three simulated datasets and to cosmology data. In all the examples, the proposed method successfully recover the underlying density structure.	[Chen, Yen-Chi; Genovese, Christopher R.; Wasserman, Larry] Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA; [Ho, Shirley] Carnegie Mellon Univ, Dept Phys, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Chen, YC (corresponding author), Carnegie Mellon Univ, Dept Stat, Pittsburgh, PA 15213 USA.	yenchic@andrew.cmu.edu; genovese@stat.cmu.edu; shirleyh@andrew.cmu.edu; larry@stat.cmu.edu	Chen, Yen-Chi/AAK-5078-2020					Bas E, 2012, J VIS COMMUN IMAGE R, V23, P1260, DOI 10.1016/j.jvcir.2012.09.003; Bas E, 2011, I S BIOMED IMAGING, P1358, DOI 10.1109/ISBI.2011.5872652; Cadre Benoit, 2006, J MULTIVARIATE ANAL; Chen Y.-C., 2015, ARXIV150105303; CHEN Y.-C., 2014, ARXIV14121716; Chen Y.-C., 2014, ARXIV14061803; Chen Yen-Chi, 2014, ARXIV14065663; CHENG YZ, 1995, IEEE T PATTERN ANAL, V17, P790, DOI 10.1109/34.400568; Cuevas A., 2006, AUST NZ J STAT; Eberly D., 1996, RIDGES IMAGE DATA AN; Einbeck J, 2011, J PATTERN RECOGNIT R, V6, P175, DOI 10.13176/11.288; EINMAHL U., 2005, ANN STAT; Evans L. C., 1991, MEASURE THEORY FINE, V5; FUKUNAGA K, 1975, IEEE T INFORM THEORY, V21, P32, DOI 10.1109/TIT.1975.1055330; Genovese CR, 2014, ANN STAT, V42, P1511, DOI 10.1214/14-AOS1218; Gine E., 2002, ANNALES I H POINCARE; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hastie T., 1984, TECHNICAL REPORT; Jones MC, 1996, J AM STAT ASSOC, V91, P401, DOI 10.2307/2291420; Mason DM, 2009, ANN APPL PROBAB, V19, P1108, DOI 10.1214/08-AAP569; Miao Z., 2014, METHOD ACCURATE ROAD; Ozertem U., 2011, J MACHINE LEARNING R; Rinaldo A., 2010, ANN STAT; Scott D. W, 2009, MULTIVARIATE DENSITY, V383; Silverman B.W., 1986, DENSITY ESTIMATION S, V26; SILVERMAN BW, 1987, BIOMETRIKA, V74, P469, DOI 10.2307/2336686; Tibshirani R., 1992, Statistics and Computing, V2, P183, DOI 10.1007/BF01889678	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103034
C	Corneil, D; Gerstner, W		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Corneil, Dane; Gerstner, Wulfram			Attractor Network Dynamics Enable Preplay and Rapid Path Planning in Maze-like Environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PLACE; INTEGRATION; SLOWNESS; MEMORY	Rodents navigating in a well-known environment can rapidly learn and revisit observed reward locations, often after a single trial. While the mechanism for rapid path planning is unknown, the CA3 region in the hippocampus plays an important role, and emerging evidence suggests that place cell activity during hippocampal "preplay" periods may trace out future goal-directed trajectories. Here, we show how a particular mapping of space allows for the immediate generation of trajectories between arbitrary start and goal locations in an environment, based only on the mapped representation of the goal. We show that this representation can be implemented in a neural attractor network model, resulting in bump-like activity profiles resembling those of the CA3 region of hippocampus. Neurons tend to locally excite neurons with similar place field centers, while inhibiting other neurons with distant place field centers, such that stable bumps of activity can form at arbitrary locations in the environment. The network is initialized to represent a point in the environment, then weakly stimulated with an input corresponding to an arbitrary goal location. We show that the resulting activity can be interpreted as a gradient ascent on the value function induced by a reward at the goal location. Indeed, in networks with large place fields, we show that the network properties cause the bump to move smoothly from its initial location to the goal, around obstacles or walls. Our results illustrate that an attractor network with hippocampal-like attributes may be important for rapid path planning.	[Corneil, Dane; Gerstner, Wulfram] Ecole Polytech Fed Lausanne, Lab Computat Neurosci, CH-1015 Lausanne, Switzerland	Ecole Polytechnique Federale de Lausanne	Corneil, D (corresponding author), Ecole Polytech Fed Lausanne, Lab Computat Neurosci, CH-1015 Lausanne, Switzerland.	dane.corneil@epfl.ch; wulfram.gerstner@epfl.ch			Swiss National Science Foundation [200020_147200]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This research was supported by the Swiss National Science Foundation (grant agreement no. 200020_147200). We thank Laureline Logiaco and Johanni Brea for valuable discussions.	[Anonymous], 2014, ADV NEURAL INFORM PR; Bast T, 2009, PLOS BIOL, V7, P730, DOI 10.1371/journal.pbio.1000089; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Conklin J, 2005, J COMPUT NEUROSCI, V18, P183, DOI 10.1007/s10827-005-6558-z; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Drew PJ, 2006, P NATL ACAD SCI USA, V103, P8876, DOI 10.1073/pnas.0600676103; Eliasmith C., 2004, NEURAL ENG COMPUTATI; Franzius M, 2007, PLOS COMPUT BIOL, V3, P1605, DOI 10.1371/journal.pcbi.0030166; Gustafson NJ, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002235; Howard Eichenbaum, 1993, MEMORY AMNESIA HIPPO; Kjelstrup KB, 2008, SCIENCE, V321, P140, DOI 10.1126/science.1157086; Larimer P, 2010, NAT NEUROSCI, V13, P213, DOI 10.1038/nn.2458; Mahadevan Sridhar, 2009, LEARNING REPRESENTAT, V3; Martinet LE, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002045; Nakashiba T, 2008, SCIENCE, V319, P1260, DOI 10.1126/science.1151120; Nakazawa K, 2003, NEURON, V38, P305, DOI 10.1016/S0896-6273(03)00165-X; OKEEFE J, 1971, BRAIN RES, V34, P171, DOI 10.1016/0006-8993(71)90358-1; Pfeiffer BE, 2013, NATURE, V497, P74, DOI 10.1038/nature12112; Ponulak F, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00098; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Schonfeld F, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00051; SCOVILLE WB, 1957, J NEUROL NEUROSUR PS, V20, P11, DOI 10.1136/jnnp.20.1.11; Sprekeler H, 2007, PLOS COMPUT BIOL, V3, P1136, DOI 10.1371/journal.pcbi.0030112; Sprekeler H, 2011, NEURAL COMPUT, V23, P3287, DOI 10.1162/NECO_a_00214; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Urbanczik R, 2014, NEURON, V81, P521, DOI 10.1016/j.neuron.2013.11.030; Wikenheiser Andrew M, 2015, NATURE NEUROSCIENCE	27	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102005
C	Dance, C; Silander, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dance, Christopher; Silander, Tomi			When are Kalman-Filter Restless Bandits Indexable?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the restless bandit associated with an extremely simple scalar Kalman filter model in discrete time. Under certain assumptions, we prove that the problem is indexable in the sense that the Whittle index is a non-decreasing function of the relevant belief state. In spite of the long history of this problem, this appears to be the first such proof. We use results about Schur-convexity and mechanical words, which are particular binary strings intimately related to palindromes.	[Dance, Christopher; Silander, Tomi] Xerox Res Ctr Europe, 6 Chemin Maupertuis, Meylan, Isere, France	Xerox	Dance, C (corresponding author), Xerox Res Ctr Europe, 6 Chemin Maupertuis, Meylan, Isere, France.	dance@xrce.xerox.com; silander@xrce.xerox.com						Altman E, 2000, MATH OPER RES, V25, P324, DOI 10.1287/moor.25.2.324.12230; Altman E, 1995, QUEUEING SYST, V21, P267, DOI 10.1007/BF01149165; Araya M., 2010, ADV NEURAL INFORM PR, V23, P64; Badanidiyuru A, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P671, DOI 10.1145/2623330.2623637; Berstel Jean, 2008, CRM MONOGRAPH SERIES; Bubeck S, 2012, FDN TRENDS MACHINE L, V5; Chen YX, 2014, PR MACH LEARN RES, V32; Gittins J. C., 2011, MULTIARMED BANDIT AL, V2nd; Guha S, 2010, J ACM, V58, DOI 10.1145/1870103.1870106; La Scala BF, 2006, DIGIT SIGNAL PROCESS, V16, P479, DOI 10.1016/j.dsp.2006.04.008; Le Ny J, 2011, IEEE T AUTOMAT CONTR, V56, P1381, DOI 10.1109/TAC.2010.2095970; Lothaire M., 2002, ENCY MATH ITS APPL, V90; Marshall AW, 2011, SPRINGER SER STAT, P3, DOI 10.1007/978-0-387-68276-1; MEIER L, 1967, IEEE T AUTOMAT CONTR, VAC12, P528, DOI 10.1109/TAC.1967.1098668; Nino-Mora J, 2009, IEEE DECIS CONTR P, P2905, DOI 10.1109/CDC.2009.5400949; Ortner Ronald, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P214, DOI 10.1007/978-3-642-34106-9_19; Rajpathak B, 2012, CHAOS, V22, DOI 10.1063/1.4740061; Thiele Thorvald, 1880, COMPENSATION QUELQUE; Verloop I., 2014, HAL00743781 CNRS; Villar S, 2012, THESIS; Vul E., 2009, ADV NEURAL INFORM PR, P1955; WEBER RR, 1990, J APPL PROBAB, V27, P637, DOI 10.2307/3214547; Whittle P., 1988, J APPL PROBAB, V25, P287, DOI DOI 10.2307/3214163	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102070
C	Dekel, O; Eldan, R; Koren, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dekel, Ofer; Eldan, Ronen; Koren, Tomer			Bandit Smooth Convex Optimization: Improving the Bias-Variance Tradeoff	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Bandit convex optimization is one of the fundamental problems in the field of online learning. The best algorithm for the general bandit convex optimization problem guarantees a regret of (O) over tilde (T-5/6), while the best known lower bound is Omega(T-1/2). Many attempts have been made to bridge the huge gap between these bounds. A particularly interesting special case of this problem assumes that the loss functions are smooth. In this case, the best known algorithm guarantees a regret of (O) over tilde (T-2/3). We present an efficient algorithm for the bandit smooth convex optimization problem that guarantees a regret of ($) over tilde (T-5/8). Our result rules out an Omega(T-2/3) lower bound and takes a significant step towards the resolution of this open problem.	[Dekel, Ofer] Microsoft Res, Redmond, WA 98052 USA; [Eldan, Ronen] Weizmann Inst Sci, Rehovot, Israel; [Koren, Tomer] Technion, Haifa, Israel	Microsoft; Weizmann Institute of Science	Dekel, O (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	oferd@microsoft.com; roneneldan@gmail.com; tomerk@technion.ac.il						Abernethy Jacob, 2009, 2009 Information Theory and Applications Workshop (ITA), P280, DOI 10.1109/ITA.2009.5044958; Abernethy Jacob D, 2009, COMPETING DARK EFFIC; Agarwal A., 2011, ADV NEURAL INFORM PR; Agarwal Alekh, 2010, P 23 ANN C LEARN THE; Bubeck S., 2015, ARXIV150706580; Bubeck S., 2015, ARXIV14121587; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck Sebastien, 2015, P 28 ANN C LEARN THE; Dani Varsha, 2008, ADV NEURAL INFORM PR; Dekel O., 2014, P 46 ANN S THEOR COM; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Hazan E., 2014, ADV NEURAL INFORM PR; Nesterov Y., 1994, INTERIOR POINT POLYN, V13; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Saha A, 2011, P 14 INT C ART INT S, P636; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101101
C	Dezfouli, A; Bonilla, EV		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dezfouli, Amir; Bonilla, Edwin V.			Scalable Inference for Gaussian Process Models with Black-Box Likelihoods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a sparse method for scalable automated variational inference (AVI) in a large class of models with Gaussian process (GP) priors, multiple latent functions, multiple outputs and non-linear likelihoods. Our approach maintains the statistical efficiency property of the original AVI method, requiring only expectations over univariate Gaussian distributions to approximate the posterior with a mixture of Gaussians. Experiments on small datasets for various problems including regression, classification, Log Gaussian Cox processes, and warped GPs show that our method can perform as well as the full method under high sparsity levels. On larger experiments using the MNIST and the SARCOS datasets we show that our method can provide superior performance to previously published scalable approaches that have been handcrafted to specific likelihood models.	[Dezfouli, Amir; Bonilla, Edwin V.] Univ New South Wales, Sydney, NSW, Australia	University of New South Wales Sydney	Dezfouli, A (corresponding author), Univ New South Wales, Sydney, NSW, Australia.	akdezfuli@gmail.com; e.bonilla@unsw.edu.au	Bonilla, Edwin V/T-1682-2018	Bonilla, Edwin V/0000-0002-9904-2408	UNSW's Faculty of Engineering Research Grant Program project [PS37866]; AWS in Education Research Grant award; Australian Research Council [DP150104878]	UNSW's Faculty of Engineering Research Grant Program project; AWS in Education Research Grant award; Australian Research Council(Australian Research Council)	This work has been partially supported by UNSW's Faculty of Engineering Research Grant Program project #PS37866 and an AWS in Education Research Grant award. AD was also supported by a grant from the Australian Research Council #DP150104878.	Alvarez M., 2010, EFFICIENT MULTIOUTPU, V9; Alvarez MA, 2011, J MACH LEARN RES, V12, P1459; DOMINGOS P, 2006, AAAI; Gal Y., 2014, NIPS; Goodman N., 2008, UAI; Hensman J., 2015, AISTATS; Hensman J., 2013, UAI; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; JARRETT RG, 1979, BIOMETRIKA, V66, P191, DOI 10.2307/2335266; LAWRENCE N, 2002, NIPS; Lawrence N., 2013, AISTATS; Lichman M, 2013, UCI MACHINE LEARNING; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Murray Iain, 2010, AISTATS; Nguyen T. V., 2014, UAI; Nguyen Trung V., 2014, ICML; Nguyen Trung V., 2014, NIPS; Nickisch Hannes, 2008, JMLR, V9; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Ranganath R., 2014, AISTATS; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Snelson E., 2003, NIPS; Snelson E., 2006, NIPS; Titsias Michalis K., 2009, AISTATS; Vijayakumar S., 2000, ICML; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; Wilson A. G., 2012, ICML; Yang Zichao, 2015, AISTATS	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100033
C	Du, N; Wang, YC; He, N; Song, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Du, Nan; Wang, Yichen; He, Niao; Song, Le			Time-Sensitive Recommendation From Recurrent User Activities	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					By making personalized suggestions, a recommender system is playing a crucial role in improving the engagement of users in modern web-services. However, most recommendation algorithms do not explicitly take into account the temporal behavior and the recurrent activities of users. Two central but less explored questions are how to recommend the most desirable item at the right moment, and how to predict the next returning time of a user to a service. To address these questions, we propose a novel framework which connects self-exciting point processes and low-rank models to capture the recurrent temporal patterns in a large collection of user-item consumption pairs. We show that the parameters of the model can be estimated via a convex optimization, and furthermore, we develop an efficient algorithm that maintains O(1/epsilon) convergence rate, scales up to problems with millions of user-item pairs and hundreds of millions of temporal events. Compared to other state-of-the-arts in both synthetic and real datasets, our model achieves superb predictive performance in the two time-sensitive recommendation tasks. Finally, we point out that our formulation can incorporate other extra context information of users, such as profile, textual and spatial features.	[Du, Nan; Wang, Yichen; Song, Le] Georgia Tech, Coll Comp, Atlanta, GA 30332 USA; [He, Niao] Georgia Tech, H Milton Stewart Sch Ind & Syst Engn, Atlanta, GA USA	University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia Institute of Technology	Du, N (corresponding author), Georgia Tech, Coll Comp, Atlanta, GA 30332 USA.	dunan@gatech.edu; yichen.wang@gatech.edu; nhe6@gatech.edu; lsong@cc.gatech.edu	He, Niao/L-9453-2017	He, Niao/0000-0003-4225-7536	NSF [IIS-1116886]; NSF/NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]	NSF(National Science Foundation (NSF)); NSF/NIH BIGDATA; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	The research was supported in part by NSF IIS-1116886, NSF/NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983.	Aalen OO, 2008, STAT BIOL HEALTH, P1; [Anonymous], 1980, POINT PROCESSES; [Anonymous], SOME NP COMPLETE PRO; Baltrunas L., 2009, TIME DEPENDANT RECOM; Bhargava J. Z. J. L. Preeti, 2015, WWW; Chi E. C., 2012, TENSORS SPARSITY NON; Cox D., 2006, STAT METHODS APPL, V1, P159; Daley D. J., 2007, INTRO THEORY POINT P, V2; Du N., 2012, ADV NEURAL INFORM PR, V4, P2780; Du N., 2015, KDD 15; Du Nan, 2013, ARTIFICIAL INTELLIGE; Harchaoui A. N. Zaid, 2013, MATH PROGRAMMING; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Kapoor K, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1719, DOI 10.1145/2623330.2623348; Kapoor K, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P233, DOI 10.1145/2684822.2685306; Karatzoglou A., 2010, PROC 4 ACM C REC SYS; KINGMAN JFC, 1964, P CAMB PHILOS SOC, V60, P923; Koenigstein N., 2011, P 5 ACM C REC SYST, P165, DOI DOI 10.1145/2043932.2043964; Koren Y, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P447; Lan G., 2012, MATH PROGRAMMING; Lan G., 2014, ARXIV13095550V2; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Ouyang, 2013, ICML; Rendle S., 2011, STUDIES COMPUTATIONA, V330, P137; Wang Yichen, 2015, KDD; Xiong L., 2010, P SDM COL OH, P211; Yu A., 2014, NIPS; Zhou K., 2013, INT C MACH LEARN ICM; ZHOU K., 2013, ARTIFICIAL INTELLIGE	30	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103017
C	Dwork, C; Feldman, V; Hardt, M; Pitassi, T; Reingold, O; Roth, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Dwork, Cynthia; Feldman, Vitaly; Hardt, Moritz; Pitassi, Toniann; Reingold, Omer; Roth, Aaron			Generalization in Adaptive Data Analysis and Holdout Reuse	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				STABILITY	Overfitting is the bane of data analysts, even when data are plentiful. Formal approaches to understanding this problem focus on statistical inference and generalization of individual analysis procedures. Yet the practice of data analysis is an inherently interactive and adaptive process: new analyses and hypotheses are proposed after seeing the results of previous ones, parameters are tuned on the basis of obtained results, and datasets are shared and reused. An investigation of this gap has recently been initiated by the authors in [7], where we focused on the problem of estimating expectations of adaptively chosen functions. In this paper, we give a simple and practical method for reusing a holdout (or testing) set to validate the accuracy of hypotheses produced by a learning algorithm operating on a training set. Reusing a holdout set adaptively multiple times can easily lead to overfitting to the holdout set itself. We give an algorithm that enables the validation of a large number of adaptively chosen hypotheses, while provably avoiding overfitting. We illustrate the advantages of our algorithm over the standard use of the holdout set via a simple synthetic experiment. We also formalize and address the general problem of data reuse in adaptive data analysis. We show how the differential-privacy based approach given in [7] is applicable much more broadly to adaptive data analysis. We then show that a simple approach based on description length can also be used to give guarantees of statistical validity in adaptive settings. Finally, we demonstrate that these incomparable approaches can be unified via the notion of approximate max-information that we introduce. This, in particular, allows the preservation of statistical validity guarantees even when an analyst adaptively composes algorithms which have guarantees based on either of the two approaches.	[Dwork, Cynthia] Microsoft Res, New York, NY 10011 USA; [Feldman, Vitaly] IBM Almaden Res Ctr, San Jose, CA USA; [Hardt, Moritz] Google Res, Mountain View, CA USA; [Pitassi, Toniann] Univ Toronto, Toronto, ON, Canada; [Reingold, Omer] Samsung Res Amer, Mountain View, CA USA; [Roth, Aaron] Univ Penn, Philadelphia, PA 19104 USA	Microsoft; International Business Machines (IBM); Google Incorporated; University of Toronto; Samsung; University of Pennsylvania	Dwork, C (corresponding author), Microsoft Res, New York, NY 10011 USA.							Bassily R., 2015, CORR; Blum Avrim, 2015, CORR; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Cawley GC, 2010, J MACH LEARN RES, V11, P2079; Do C. B., 2007, ADV NEURAL INFORM PR, P377; Dwork C., 2015, CORR; Dwork C, 2006, LECT NOTES COMPUT SC, V4004, P486; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork Cynthia, 2014, CORR; FREEDMAN DA, 1983, AM STAT, V37, P152, DOI 10.2307/2685877; Hardt M, 2014, ANN IEEE SYMP FOUND, P454, DOI 10.1109/FOCS.2014.55; Hastie T., 2009, ELEMENTS STAT LEARNI, V2nd, DOI DOI 10.1007/978-0-387-21606-5; Langford J., 2005, CLEVER METHODS OVERF; Nissim K., 2015, CORR; Rao R. B., 2008, P 2008 SIAM INT C DA, P588, DOI DOI 10.1137/1.9781611972788.54; Reunanen J., 2003, Journal of Machine Learning Research, V3, P1371, DOI 10.1162/153244303322753715; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Steinke T., 2014, ARXIV14101228; Taylor J, 2015, P NATL ACAD SCI USA, V112, P7629, DOI 10.1073/pnas.1507583112; Wang Y., 2015, ARXIV151103376	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103031
C	Ellis, K; Solar-Lezama, A; Tenenbaum, JB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ellis, Kevin; Solar-Lezama, Armando; Tenenbaum, Joshua B.			Unsupervised Learning by Program Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce an unsupervised learning algorithm that combines probabilistic modeling with solver-based techniques for program synthesis. We apply our techniques to both a visual learning domain and a language learning problem, showing that our algorithm can learn many visual concepts from only a few examples and that it can recover some English inflectional morphology. Taken together, these results give both a new approach to unsupervised learning of symbolic compositional structures, and a technique for applying program synthesis tools to noisy data.	[Ellis, Kevin; Tenenbaum, Joshua B.] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA; [Solar-Lezama, Armando] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Ellis, K (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.	ellisk@mit.edu; asolar@csail.mit.edu; jbt@mit.edu			NSF [SHF-1161775]; Center for Minds, Brains and Machines (CBMM) - NSF STC award [CCF-1231216]; ARO MURI contract [W911NF-08-1-0242]	NSF(National Science Foundation (NSF)); Center for Minds, Brains and Machines (CBMM) - NSF STC award; ARO MURI contract(MURI)	We are grateful for discussions with Timothy O'Donnell on morphological rule learners, for advice from Brendan Lake and Tejas Kulkarni on the convolutional network baselines, and for the suggestions of our anonymous reviewers. This material is based upon work supported by funding from NSF award SHF-1161775, from the Center for Minds, Brains and Machines (CBMM) funded by NSF STC award CCF-1231216, and from ARO MURI contract W911NF-08-1-0242.	Albright A, 2003, COGNITION, V90, P119, DOI 10.1016/S0010-0277(03)00146-X; Baayen Harald., 1995, CELEX2 LDC96L14 WEB; Chan Erwin., 2010, RES LANGUAGE COMPUTA, V8, P209; Dehaene S, 2006, SCIENCE, V311, P381, DOI 10.1126/science.1121739; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; Fleuret F, 2011, P NATL ACAD SCI USA, V108, P17621, DOI 10.1073/pnas.1109168108; Goldsmith J, 2001, COMPUT LINGUIST, V27, P153, DOI 10.1162/089120101750300490; Goodman N. D., 2008, P 24 C UNCERTAINTY A, P220; Gregor K., 2015, CORR; Gulwani S, 2011, PLDI 11: PROCEEDINGS OF THE 2011 ACM CONFERENCE ON PROGRAMMING LANGUAGE DESIGN AND IMPLEMENTATION, P62; Gulwani S, 2011, POPL 11: PROCEEDINGS OF THE 38TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P317, DOI 10.1145/1926385.1926423; Gulwani Sumit, 2015, COMMUN ACM; Katz Yarden, 2008, COGSCI, P71; Koza J. R., 1993, GENETIC PROGRAMMING; Lake B.M., 2013, ADV NEURAL INFORM PR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lezama A. S., 2008, THESIS; Liang P., 2010, P 27 INT C MACHINE L, P639; O'Donnell T., 2015, PRODUCTIVITY REUSE L; Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, DOI DOI 10.7551/MITPRESS/5237.001.0001; Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150; Seidenberg MS, 2014, COGNITIVE SCI, V38, P1190, DOI 10.1111/cogs.12147; SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2; Thornburg David D., 1983, COMPUTE          MAR; Torlak Emina, 2013, ONWARD, P135; Virpioja Sami, 2013, TECHNICAL REPORT	27	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101045
C	Foster, DJ; Rakhlin, A; Sridharan, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Foster, Dylan J.; Rakhlin, Alexander; Sridharan, Karthik			Adaptive Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BOUNDS	We propose a general framework for studying adaptive regret bounds in the online learning setting, subsuming model selection and data-dependent bounds. Given a data-or model-dependent bound we ask, "Does there exist some algorithm achieving this bound?" We show that modifications to recently introduced sequential complexity measures can be used to answer this question by providing sufficient conditions under which adaptive rates can be achieved. In particular each adaptive rate induces a set of so-called offset complexity measures, and obtaining small upper bounds on these quantities is sufficient to demonstrate achievability. A cornerstone of our analysis technique is the use of one-sided tail inequalities to bound suprema of offset random processes. Our framework recovers and improves a wide variety of adaptive bounds including quantile bounds, second order data-dependent bounds, and small loss bounds. In addition we derive a new type of adaptive bound for online linear optimization based on the spectral norm, as well as a new online PAC-Bayes theorem.	[Foster, Dylan J.; Sridharan, Karthik] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA; [Rakhlin, Alexander] Univ Penn, Dept Stat, Philadelphia, PA 19104 USA	Cornell University; University of Pennsylvania	Foster, DJ (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.							Bartlett PL, 2002, MACH LEARN, V48, P85, DOI 10.1023/A:1013999503812; Birge L, 1998, BERNOULLI, V4, P329, DOI 10.2307/3318720; Cesa-Bianchi N, 2007, MACH LEARN, V66, P321, DOI 10.1007/s10994-006-5001-7; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Chaudhuri K., 2009, ADV NEURAL INFORM PR; Chiang Chao-Kai, 2012, COLT; Cover T., 1967, T 4 PRAG C INF THEOR, P263; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Even-Dar E, 2008, MACH LEARN, V72, P21, DOI 10.1007/s10994-008-5060-z; Hazan E, 2010, MACH LEARN, V80, P165, DOI 10.1007/s10994-010-5175-x; Koolen Wouter M, 2015, COLT, P1155; Liang Tengyuan, 2015, P 28 C LEARN THEOR; Lugosi G, 1999, ANN STAT, V27, P1830; Luo Haipeng, 2015, CORR; Massart Pascal, 2007, CONCENTRATION INEQUA, V10; Mendelson Shahar, 2014, C LEARN THEOR; Orabona Francesco, 2014, P 27 C LEARN THEOR; Pinelis I., 1994, ANN PROBAB, V22, P1679, DOI [10.1214/aop/1176988477, DOI 10.1214/AOP/1176988477]; Rakhlin A., 2014, P 27 C LEARN THEOR; Rakhlin A., 2012, STAT LEARNING THEORY; Rakhlin A., 2013, P 26 ANN C LEARN THE; Rakhlin Alexander, 2010, ARXIV10113168; Rakhlin Alexander, 2010, ADV NEURAL INFORM PR, V23; Rakhlin Alexander, 2014, PROBABILITY THEORY R; Rakhlin S., 2012, ADV NEURAL INFORM PR, V25, P2150; Srebro N., 2010, ADV NEURAL INFORM PR, P2199	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101103
C	Gao, T; Ji, Q		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gao, Tian; Ji, Qiang			Local Causal Discovery of Direct Causes and Effects	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We focus on the discovery and identification of direct causes and effects of a target variable in a causal network. State-of-the-art causal learning algorithms generally need to find the global causal structures in the form of complete partial directed acyclic graphs (CPDAG) in order to identify direct causes and effects of a target variable. While these algorithms are effective, it is often unnecessary and wasteful to find the global structures when we are only interested in the local structure of one target variable (such as class labels). We propose a new local causal discovery algorithm, called Causal Markov Blanket (CMB), to identify the direct causes and effects of a target variable based on Markov Blanket Discovery. CMB is designed to conduct causal discovery among multiple variables, but focuses only on finding causal relationships between a specific target variable and other variables. Under standard assumptions, we show both theoretically and experimentally that the proposed local causal discovery algorithm can obtain the comparable identification accuracy as global methods but significantly improve their efficiency, often by more than one order of magnitude.	[Gao, Tian; Ji, Qiang] Rensselaer Polytech Inst, Dept ECSE, Troy, NY 12180 USA	Rensselaer Polytechnic Institute	Gao, T (corresponding author), Rensselaer Polytech Inst, Dept ECSE, Troy, NY 12180 USA.	gaot@rpi.edu; jiq@rpi.edu						Aliferis Constantin, 2003, THESIS; [Anonymous], 2007, CAUSAL FEATURE SELEC; CHICKERING DM, 2002, J MACHINE LEARNING R; Cooper GF, 1997, DATA MIN KNOWL DISC, V1, P203, DOI 10.1023/A:1009787925236; Koller D., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P284; Mani S, 2004, ST HEAL T, V107, P731; Mani S, 1999, J AM MED INFORM ASSN, P315; Mani S., 2010, J MACHINE LEARNING R, P121; Margaritis D, 2000, ADV NEUR IN, V12, P505; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403; Niinimki T., 2012, P 28 C UNC ART INT U, P634; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pellet Jean-Philippe, 2009, ADV NEURAL INFORM PR, P1249; Pellet Jean-Philippe, 2008, J MACHINE LEARNING; Pena JM, 2007, INT J APPROX REASON, V45, P211, DOI 10.1016/j.ijar.2006.06.008; Silverstein C, 2000, DATA MIN KNOWL DISC, V4, P163, DOI 10.1023/A:1009891813863; Spirtes P., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P499; Spirtes P, 2000, CONSTRUCTING BAYESIA, DOI DOI 10.1184/R1/6491291.V1; Spirtes P., 2000, CAUSATION PREDICTION; Statnikov Alexander, 2008, CAUSATION PREDICTION; Tsamardinos I, 2003, P 9 ACM SIGKDD INT C, P673, DOI DOI 10.1145/956750.956838; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Zhang JJ, 2008, ARTIF INTELL, V172, P1873, DOI 10.1016/j.artint.2008.08.001	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103012
C	Gao, YJ; Buesing, L; Shenoy, KV; Cunningham, JP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gao, Yuanjun; Buesing, Lars; Shenoy, Krishna V.; Cunningham, John P.			High-dimensional neural spike train analysis with generalized count linear dynamical systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MODELS; REGRESSION	Latent factor models have been widely used to analyze simultaneous recordings of spike trains from large, heterogeneous neural populations. These models assume the signal of interest in the population is a low-dimensional latent intensity that evolves over time, which is observed in high dimension via noisy point-process observations. These techniques have been well used to capture neural correlations across a population and to provide a smooth, denoised, and concise representation of high-dimensional spiking data. One limitation of many current models is that the observation model is assumed to be Poisson, which lacks the flexibility to capture under-and over-dispersion that is common in recorded neural data, thereby introducing bias into estimates of covariance. Here we develop the generalized count linear dynamical system, which relaxes the Poisson assumption by using a more general exponential family for count data. In addition to containing Poisson, Bernoulli, negative binomial, and other common count distributions as special cases, we show that this model can be tractably learned by extending recent advances in variational inference techniques. We apply our model to data from primate motor cortex and demonstrate performance improvements over state-of-the-art methods, both in capturing the variance structure of the data and in held-out prediction.	[Gao, Yuanjun; Buesing, Lars; Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Shenoy, Krishna V.] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Columbia University; Stanford University	Gao, YJ (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.	yg2312@columbia.edu; lars@stat.columbia.edu; shenoy@stanford.edu; jpc2181@columbia.edu			Sloan Research Fellowship; Simons Foundation [325171, 325233]; Grossman Center at Columbia University; Gatsby Charitable Trust	Sloan Research Fellowship(Alfred P. Sloan Foundation); Simons Foundation; Grossman Center at Columbia University; Gatsby Charitable Trust	JPC received funding from a Sloan Research Fellowship, the Simons Foundation (SCGB#325171 and SCGB#325233), the Grossman Center at Columbia University, and the Gatsby Charitable Trust. Thanks to Byron Yu, Gopal Santhanam and Stephen Ryu for providing the cortical data.	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Ananth CV, 1997, INT J EPIDEMIOL, V26, P1323, DOI 10.1093/ije/26.6.1323; Biljana Petreska, 2011, ADV NEURAL INFORM PR, P756; Boyd S., 2009, CONVEX OPTIMIZATION; Buesing L., 2014, ADV NEURAL INFORM PR, P3500; Buesing L., 2015, ADV STATE SPACE METH; Buesing L, 2012, NETWORK-COMP NEURAL, V23, P24, DOI 10.3109/0954898X.2012.677095; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501; Cohen MR, 2011, NAT NEUROSCI, V14, P811, DOI 10.1038/nn.2842; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Cunningham John P, 2007, ADV NEURAL INFORM PR, P329; del Castillo J, 2005, J STAT PLAN INFER, V134, P486, DOI 10.1016/j.jspi.2004.04.019; Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711; Khan Emtiyaz, 2013, P 30 INT C MACH LEAR, P951; Koyama S., 2015, NEURAL COMPUTATION; Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173; LAMBERT D, 1992, TECHNOMETRICS, V34, P1, DOI 10.2307/1269547; Lawhern V, 2010, J NEUROSCI METH, V189, P267, DOI 10.1016/j.jneumeth.2010.03.024; Linderman S. W., 2015, COSYNE; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002; Paninski L., 2013, ADV NEURAL INF PROCE, V26, P2391; Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0; Pillow J., 2012, ADV NEURAL INF PROCE, V25, P1898; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Rao CR., 1965, SANKHY INDIAN J STAT, V27, P311; Sellers KF, 2010, ANN APPL STAT, V4, P943, DOI 10.1214/09-AOAS306; SINGH J, 1978, SIAM J APPL MATH, V34, P545, DOI 10.1137/0134043; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Vidne M, 2012, J COMPUT NEUROSCI, V33, P97, DOI 10.1007/s10827-011-0376-2; Yu Byron M, 2009, ADV NEURAL INFORM PR, P1881, DOI DOI 10.1152/JN.90941	32	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101027
C	Gotovos, A; Hassani, SH; Krause, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gotovos, Alkis; Hassani, S. Hamed; Krause, Andreas			Sampling from Probabilistic Submodular Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Submodular and supermodular functions have found wide applicability in machine learning, capturing notions such as diversity and regularity, respectively. These notions have deep consequences for optimization, and the problem of (approximately) optimizing submodular functions has received much attention. However, beyond optimization, these notions allow specifying expressive probabilistic models that can be used to quantify predictive uncertainty via marginal inference. Prominent, well-studied special cases include Ising models and determinantal point processes, but the general class of log-submodular and log-supermodular models is much richer and little studied. In this paper, we investigate the use of Markov chain Monte Carlo sampling to perform approximate inference in general log-submodular and log-supermodular models. In particular, we consider a simple Gibbs sampling procedure, and establish two sufficient conditions, the first guaranteeing polynomial-time, and the second fast (O(n log n)) mixing. We also evaluate the efficiency of the Gibbs sampler on three examples of such models, and compare against a recently proposed variational approach.	[Gotovos, Alkis; Hassani, S. Hamed; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland	ETH Zurich	Gotovos, A (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	alkisg@inf.ethz.ch; hamed@inf.ethz.ch; krausea@ethz.ch	Gotovos, Alkis/AAS-9761-2021		ERC Starting Grant [307036]	ERC Starting Grant(European Research Council (ERC))	This work was partially supported by ERC Starting Grant 307036.	Aldous D., 1983, SEMINAIRE PROBABILIT; Brooks S, 2011, CH CRC HANDB MOD STA, P1, DOI 10.1201/b10905; Bubley Russ, 1998, S DISCR ALG; Bubley Russ, 1997, S FDN COMP SCI; Conforti Michele, 1984, DISC APP MATH; Diaconis Persi, 1991, ANN APPL PROBABILITY; Djolonga Josip, 2014, NEURAL INFORM PROCES; Dyer Martin, 2009, ANN APPL PROBABILITY; Feige Uriel, 2007, S FDN COMP SCI; Golovin D., 2011, J ARTIFICIAL INTELLI; Greenhill Catherine, 2000, J ALGORITHMS; Hui Lin, 2011, HUMAN LANGUAGE TECHN; Iyer Rishabh, 2015, INT C ART INT STAT; Jerrum M., 1993, SIAM J COMPUTING; JERRUM M, 2003, LEC MATH, P1; Jerrum M, 2004, J ACM; Jerrum Mark, 1995, RANDOM STRUCTURES AL; Jerrum Mark, 1989, SIAM J COMPUTING; Kempe David, 2003, C KNOWL DISC DAT MIN; Koller D., 2009, PROBABILISTIC GRAPHI; Krause Andreas, 2008, J WATER RESOURCES PL; Krause Andreas, 2006, INFORM PROCESSING SE; Kulesza A., 2012, FDN TRENDS MACHINE L; Levin D., 2008, MARKOV CHAINS MIXING; Nemhauser G. L., 1978, MATH PROGRAMMING; Rebeschini P., 2015, C LEARN THEOR; Sinclair Alistair, 1992, COMBINATORICS PROBAB	28	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101004
C	Gunasekar, S; Banerjee, A; Ghosh, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gunasekar, Suriya; Banerjee, Arindam; Ghosh, Joydeep			Unified View of Matrix Completion under General Structural Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Matrix completion problems have been widely studied under special low dimensional structures such as low rank or structure induced by decomposable norms. In this paper, we present a unified analysis of matrix completion under general low-dimensional structural constraints induced by any norm regularization. We consider two estimators for the general problem of structured matrix completion, and provide unified upper bounds on the sample complexity and the estimation error. Our analysis relies on generic chaining, and we establish two intermediate results of independent interest: (a) in characterizing the size or complexity of low dimensional subsets in high dimensional ambient space, a certain partial complexity measure encountered in the analysis of matrix completion problems is characterized in terms of a well understood complexity measure of Gaussian widths, and (b) it is shown that a form of restricted strong convexity holds for matrix completion problems under general norm regularization. Further, we provide several non-trivial examples of structures included in our framework, notably including the recently proposed spectral k-support norm.	[Gunasekar, Suriya; Ghosh, Joydeep] UT Austin, Austin, TX 78712 USA; [Banerjee, Arindam] UMN Twin Cities, Minneapolis, MN USA	University of Texas System; University of Texas Austin; University of Minnesota System; University of Minnesota Twin Cities	Gunasekar, S (corresponding author), UT Austin, Austin, TX 78712 USA.	suriya@utexas.edu; banerjee@cs.umn.edu; ghosh@ece.utexas.edu			NSF [IIS-1421729, IIS-1417697, IIS1116656, IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	We thank the anonymous reviewers for helpful comments and suggestions. S. Gunasekar and J. Ghosh acknowledge funding funding from NSF grants IIS-1421729, IIS-1417697, and IIS1116656. A. Banerjee acknowledges NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and NASA grant NNX12AQ39A.	Amelunxen D., 2014, INFORM INFERENCE; Argyriou A., 2012, NIPS; Banerjee A., 2014, NIPS; Banerjee Arindam, 2005, JMLR; Cai T., 2014, GEOMETRIZING LOCAL R; Candes E. J., 2009, FOCM; Candes E. J., 2011, ACM; Candes E. J., 2010, P IEEE; Candes Emmanuel J, 2005, INFORM THEORY IEEE T; Chandrasekaran V., 2012, FDN COMPUTATIONAL MA; Davenport M. A., 2014, INFORM INFERENCE; Dudley R. M., 1967, J FUNCTIONAL ANAL; Edelman A., 1988, J MATRIX ANAL APPL; Fazel M., 2001, AM CONTR C; Forster J., 2002, J COMPUTER SYSTEM SC; Gunasekar S., 2014, ICML; Jacob Laurent, 2009, NIPS; Keshavan R., 2010, JMLR; Keshavan R. H., 2010, IEEE T IT; Klopp O., 2014, BERNOULLI; Klopp O., 2015, MATRIX COMPLETION SI; Koltchinskii V., 2011, ANN STAT; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Litvak A. E., 2005, ADV MATH; McDonald A. M., 2014, NEW PERSPECTIVES K S; Negahban S., 2012, JMLR; Negahban S., 2009, NIPS; Recht B., 2011, JMLR; Richard E., 2014, ARXIV E PRINTS; Srebro N., 2005, LEARNING THEORY; Talagrand M., 2014, UPPER LOWER BOUNDS S; Talagrand M., 1996, ANN PROBABILITY; Talagrand M., 2001, ANN PROBABILITY; Tropp J. A., 2014, CONVEX RECOVERY STRU; Tropp J. A., 2012, FDN COMPUTATIONAL MA; Vershynin R., 2014, ARXIV E PRINTS; Watson A. G., 1992, LINEAR ALGEBRA ITS A; Yang E., 2013, NIPS	39	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103060
C	Ha, W; Barber, RF		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ha, Wooseok; Barber, Rina Foygel			Robust PCA with compressed data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The robust principal component analysis (RPCA) problem seeks to separate low-rank trends from sparse outliers within a data matrix, that is, to approximate a n x d matrix D as the sum of a low-rank matrix L and a sparse matrix S. We examine the robust principal component analysis (RPCA) problem under data compression, where the data Y is approximately given by (L+S).C, that is, a low-rank + sparse data matrix that has been compressed to size n x m (with m substantially smaller than the original dimension d) via multiplication with a compression matrix C. We give a convex program for recovering the sparse component S along with the compressed low-rank component L. C, along with upper bounds on the error of this reconstruction that scales naturally with the compression dimension m and coincides with existing results for the uncompressed setting m = d. Our results can also handle error introduced through additive noise or through missing data. The scaling of dimension, compression, and signal complexity in our theoretical results is verified empirically through simulations, and we also apply our method to a data set measuring chlorine concentration across a network of sensors to test its performance in practice.	[Ha, Wooseok; Barber, Rina Foygel] Univ Chicago, Chicago, IL 60637 USA	University of Chicago	Ha, W (corresponding author), Univ Chicago, Chicago, IL 60637 USA.	haywse@uchicago.edu; rina@uchicago.edu						Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Foygel R., 2011, ADV NEURAL INFORM PR, V24, P2133; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; He J., 2011, IT 2011; He J, 2012, PROC CVPR IEEE, P1568, DOI 10.1109/CVPR.2012.6247848; Maillard O., 2009, ADV NEURAL INFORM PR, P1213; Netrapalli P., 2014, P ADV NEUR INF PROC, P1107; Wright Y., 2009, ADV NEURAL INFORM PR, V22, DOI DOI 10.5555/2984093.2984326; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Zhou SH, 2009, IEEE T INFORM THEORY, V55, P846, DOI 10.1109/TIT.2008.2009605; Zhou T., 2011, P 28 INT C MACH LEAR, P33	13	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100073
C	Harel, Y; Meir, R; Opper, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Harel, Yuval; Meir, Ron; Opper, Manfred			A Tractable Approximation to Optimal Point Process Filtering: Application to Neural Encoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The process of dynamic state estimation (filtering) based on point process observations is in general intractable. Numerical sampling techniques are often practically useful, but lead to limited conceptual insight about optimal encoding/decoding strategies, which are of significant relevance to Computational Neuroscience. We develop an analytically tractable Bayesian approximation to optimal filtering based on point process observations, which allows us to introduce distributional assumptions about sensory cell properties, that greatly facilitate the analysis of optimal encoding in situations deviating from common assumptions of uniform coding. The analytic framework leads to insights which are difficult to obtain from numerical algorithms, and is consistent with experiments about the distribution of tuning curve centers. Interestingly, we find that the information gained from the absence of spikes may be crucial to performance.	[Harel, Yuval; Meir, Ron] Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel; [Opper, Manfred] Tech Univ Berlin, Dept Artificial Intelligence, D-10587 Berlin, Germany	Technion Israel Institute of Technology; Technical University of Berlin	Harel, Y (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, Haifa, Israel.	yharel@tx.technion.ac.il; rmeir@ee.technion.ac.il; opperm@cs.tu-berlin.de						Ahmadian Y, 2011, NEURAL COMPUT, V23, P46, DOI 10.1162/NECO_a_00059; Anderson B., 2005, P MTS IEEE OC, P1, DOI [10.1109/OCEANS.2005.1639923, DOI 10.1109/OCEANS.2005.1639923]; [Anonymous], 1998, ON LINE LEARNING NEU; Bethge M, 2002, NEURAL COMPUT, V14, P2317, DOI 10.1162/08997660260293247; Bobrowski O, 2009, NEURAL COMPUT, V21, P1277, DOI 10.1162/neco.2008.01-08-692; Brand A, 2002, NATURE, V417, P543, DOI 10.1038/417543a; Bremaud P., 1981, POINT PROCESSES QUEU; Brigo D, 1998, IEEE T AUTOMAT CONTR, V43, P247, DOI 10.1109/9.661075; Dayan P., 2005, THEORETICAL NEUROSCI; Doucet A., 2009, HDB NONLINEAR FILTER, V12, P656; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; Harper NS, 2004, NATURE, V430, P682, DOI 10.1038/nature02768; Kalman R.E., 1961, J BASIC ENG-T ASME, V83, P95, DOI [10.1115/1.3658902, DOI 10.1115/1.3658902]; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Maybeck P. S., 1979, MATH SCI ENG; Minka T.P., 2001, P 17 C UNC ART INT, P362; RHODES IB, 1977, IEEE T AUTOMAT CONTR, V22, P338, DOI 10.1109/TAC.1977.1101534; SNYDER DL, 1972, IEEE T INFORM THEORY, V18, P91, DOI 10.1109/TIT.1972.1054756; Snyder Donald Lee, 1991, RANDOM POINT PROCESS, DOI DOI 10.1007/978-1-4612-3166-0; Susemihl A. K., 2014, ADV NEURAL INFORM PR, P1; Susemihl A. K., 2011, ADV NEURAL INFORM PR, V24, P2303; Susemihl A, 2013, J STAT MECH-THEORY E, DOI 10.1088/1742-5468/2013/03/P03009; Yaeli S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00130	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102012
C	Hashimoto, TB; Sun, Y; Jaakkola, TS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Hashimoto, Tatsunori B.; Sun, Yi; Jaakkola, Tommi S.			From random walks to distances on unweighted graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Large unweighted directed graphs are commonly used to capture relations between entities. A fundamental problem in the analysis of such networks is to properly define the similarity or dissimilarity between any two vertices. Despite the significance of this problem, statistical characterization of the proposed metrics has been limited. We introduce and develop a class of techniques for analyzing random walks on graphs using stochastic calculus. Using these techniques we generalize results on the degeneracy of hitting times and analyze a metric based on the Laplace transformed hitting time (LTHT). The metric serves as a natural, provably well-behaved alternative to the expected hitting time. We establish a general correspondence between hitting times of the Brownian motion and analogous hitting times on the graph. We show that the LTHT is consistent with respect to the underlying metric of a geometric graph, preserves clustering tendency, and remains robust against random addition of non-geometric edges. Tests on simulated and real-world data show that the LTHT matches theoretical predictions and outperforms alternatives.	[Hashimoto, Tatsunori B.; Jaakkola, Tommi S.] MIT EECS, Cambridge, MA 02142 USA; [Sun, Yi] MIT Math, Cambridge, MA USA		Hashimoto, TB (corresponding author), MIT EECS, Cambridge, MA 02142 USA.	thashim@mit.edu; yisun@mit.edu; tommi@mit.edu						Alamgir M., 2012, ICML, P1031; Alamgir Morteza, 2011, ADV NEURAL INFORM PR, P379; Chebotarev P, 2011, DISCRETE APPL MATH, V159, P295, DOI 10.1016/j.dam.2010.11.017; Croydon DA, 2008, POTENTIAL ANAL, V29, P351, DOI 10.1007/s11118-008-9101-9; Gehrke J., 2003, SIGKDD EXPLORATIONS, V5, P149, DOI [10.1145/980972.980992, DOI 10.1145/980972.980992]; Hashimoto TB, 2015, JMLR WORKSH CONF PRO, V38, P342; Kiss G. R., 1973, COMPUTER LIT STUDIES, P153; Kivimaki I, 2014, PHYSICA A, V393, P600, DOI 10.1016/j.physa.2013.09.016; Lu LY, 2011, PHYSICA A, V390, P1150, DOI 10.1016/j.physa.2010.11.027; Oksendal B., 2013, STOCHASTIC DIFFERENT; Sarkar P., 2007, P UAI; Sarkar P., 2011, P 22 INT JOINT C ART, P2722; Shaw B, 2009, INT C MACH LEARN, P937, DOI DOI 10.1145/1553374.1553494; Smith ST, 2014, IEEE T SIGNAL PROCES, V62, P5324, DOI 10.1109/TSP.2014.2336613; Stroock D. W., 1979, MULTIDIMENSIONAL DIF, V233; Tahbaz-Salehi A, 2006, IEEE DECIS CONTR P, P4664, DOI 10.1109/CDC.2006.377308; Ting D., 2010, P ICML; von Luxburg U, 2008, ANN STAT, V36, P555, DOI 10.1214/009053607000000640; von Luxburg U, 2014, J MACH LEARN RES, V15, P1751; Yazdani M., 2013, THESIS; Yen L., 2008, P 14 ACM SIGKDD, P785, DOI DOI 10.1145/1401890.1401984	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103047
C	He, N; Harchaoui, Z		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		He, Niao; Harchaoui, Zaid			Semi-Proximal Mirror-Prox for Nonsmooth Composite Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a new first-order optimization algorithm to solve high-dimensional non-smooth composite minimization problems. Typical examples of such problems have an objective that decomposes into a non-smooth empirical risk part and a non-smooth regularization penalty. The proposed algorithm, called Semi-Proximal Mirror-Prox, leverages the saddle point representation of one part of the objective while handling the other part of the objective via linear minimization over the domain. The algorithm stands in contrast with more classical proximal gradient algorithms with smoothing, which require the computation of proximal operators at each iteration and can therefore be impractical for high-dimensional problems. We establish the theoretical convergence rate of Semi-Proximal Mirror-Prox, which exhibits the optimal complexity bounds, i.e. O (1/epsilon(2)), for the number of calls to linear minimization oracle. We present promising experimental results showing the interest of the approach in comparison to competing methods.	[He, Niao] Georgia Inst Technol, Atlanta, GA 30332 USA; [Harchaoui, Zaid] NYU, INRIA, New York, NY 10003 USA	University System of Georgia; Georgia Institute of Technology; New York University	He, N (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	nhe6@gatech.edu; zaid.harchaoui@nyu.edu	He, Niao/L-9453-2017	He, Niao/0000-0003-4225-7536	NSF [CMMI-1232623]; LabEx Persyval-Lab [ANR-11-LABX-0025]; project "Titan" (CNRS-Mastodons); MSR-Inria joint centre; Moore-Sloan Data Science Environment at NYU; project "Macaron" [ANR-14-CE23-0003-01]	NSF(National Science Foundation (NSF)); LabEx Persyval-Lab; project "Titan" (CNRS-Mastodons); MSR-Inria joint centre; Moore-Sloan Data Science Environment at NYU; project "Macaron"	The authors would like to thank A. Juditsky and A. Nemirovski for fruitful discussions. This work was supported by NSF Grant CMMI-1232623, LabEx Persyval-Lab (ANR-11-LABX-0025), project "Titan" (CNRS-Mastodons), project "Macaron" (ANR-14-CE23-0003-01), the MSR-Inria joint centre, and the Moore-Sloan Data Science Environment at NYU.	Bach F., 2015, SIAM J OPTIMIZATION; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Bertsekas D., 2015, CONVEX OPTIMIZATION; Chen X, 2012, ANN APPL STAT, V6, P719, DOI 10.1214/11-AOAS514; Cox B, 2014, MATH PROGRAM, V148, P143, DOI 10.1007/s10107-013-0725-1; Dudik M., 2012, P 15 INT C ART INT S; Garber D., 2013, ARXIV13014666; Harchaoui Zaid, 2013, MATH PROGRAM, P1; He Niao, 2013, ARXIV13111098; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Juditsky Anatoli, 2013, ARXIV1312107; Koren T, 2012, ICML; Lan G., 2014, CONDITIONAL GRADIENT; Lan Guanghui, 2013, ARXIV; Mu Cun, 2014, ARXIV14037588; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski A, 2010, MATH OPER RES, V35, P52, DOI 10.1287/moor.1090.0427; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2015, TECHNICAL REPORT; Ouyang Y., 2014, ACCELERATED LINEARIZ; Parikh Neal, 2013, FDN TRENDS OPTIMIZAT; Pierucci Federico, 2014, C APPR AUT ACT CAP 1; Schmidt Mark, 2011, ADV NIPS; Zhang X., 2012, NIPS	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101084
C	Khan, ME; Baque, P; Fleuret, F; Fua, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Khan, Mohammad Emtiyaz; Baque, Pierre; Fleuret, Francois; Fua, Pascal			Kullback-Leibler Proximal Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a new variational inference method based on a proximal framework that uses the Kullback-Leibler (KL) divergence as the proximal term. We make two contributions towards exploiting the geometry and structure of the variational bound. First, we propose a KL proximal-point algorithm and show its equivalence to variational inference with natural gradients (e.g., stochastic variational inference). Second, we use the proximal framework to derive efficient variational algorithms for non-conjugate models. We propose a splitting procedure to separate non-conjugate terms from conjugate ones. We linearize the non-conjugate terms to obtain subproblems that admit a closed-form solution. Overall, our approach converts inference in a non-conjugate model to subproblems that involve inference in well-known conjugate models. We show that our method is applicable to a wide variety of models and can result in computationally efficient algorithms. Applications to real-world datasets show comparable performances to existing methods.	[Khan, Mohammad Emtiyaz; Baque, Pierre; Fua, Pascal] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Khan, ME (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	emtiyaz@gmail.com; pierre.baque@epfl.ch; francois.fleuret@idiap.ch; pascal.fua@epfl.ch			Swiss National Science Foundation [CRSII2-147693]	Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	Mohammad Emtiyaz Khan would like to thank Masashi Sugiyama and Akiko Takeda from University of Tokyo, Matthias Grossglauser and Vincent Etter from EPFL, and Hannes Nickisch from Philips Research (Hamburg) for useful discussions and feedback. Pierre Baque was supported in part by the Swiss National Science Foundation, under the grant CRSII2-147693 "Tracking in the Wild".	Babagholami-Mohamadabadi Behnam, 2015, ARXIV150700824; Challis E., 2011, INT C ART INT STAT; Chretien S, 2000, IEEE T INFORM THEORY, V46, P1800, DOI 10.1109/18.857792; Dai Bo, 2015, COMPUTING RES REPOSI; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Honkela A., 2011, J MACHINE LEARNING R, V11, P3235; Honkela Antti, 2004, ADV NEURAL INFORM PR, P593; Khan M. E., 2015, ARXIV151100146; Khan Mohammad Emtiyaz, 2014, ADV NEURAL INFORM PR; Lappalainen H, 2000, PERSP NEURAL COMP, P93; Marlin B., 2011, INT C MACH LEARN; Paquet Ulrich, 2014, NIPS WORKSH VAR INF; Pascanu Razvan, 2013, ARXIV13013584; Polson Nicholas G, 2015, ARXIV150203175; Ranganath R., 2013, ARXIV14010118; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Ravikumar Pradeep, 2008, INT C MACH LEARN; Sato M, 2001, NEURAL COMPUT, V13, P1649, DOI 10.1162/089976601750265045; Seeger MW, 2011, SIAM J IMAGING SCI, V4, P166, DOI 10.1137/090758775; Teboulle M, 1997, SIAM J OPTIMIZ, V7, P1069, DOI 10.1137/S1052623495292130; Theis L., 2015, INT C MACH LEARN; TITSIAS M. K., 2014, INT C MACH LEARN; Tseng P, 2004, MATH OPER RES, V29, P27, DOI 10.1287/moor.1030.0073; Wang C, 2013, J MACH LEARN RES, V14, P1005; Wang H., 2014, ADV NEURAL INFORM PR	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102044
C	Kishimoto, A; Marinescu, R; Botea, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kishimoto, Akihiro; Marinescu, Radu; Botea, Adi			Parallel Recursive Best-First AND/OR Search for Exact MAP Inference in Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The paper presents and evaluates the power of parallel search for exact MAP inference in graphical models. We introduce a new parallel shared-memory recursive best-first AND/OR search algorithm, called SPRBFAOO, that explores the search space in a best-first manner while operating with restricted memory. Our experiments show that SPRBFAOO is often superior to the current state-of-the-art sequential AND/OR search approaches, leading to considerable speed-ups (up to 7-fold with 12 threads), especially on hard problem instances.	[Kishimoto, Akihiro; Marinescu, Radu; Botea, Adi] IBM Res, Dublin, Ireland		Kishimoto, A (corresponding author), IBM Res, Dublin, Ireland.	akihirok@ie.ibm.com; radu.marinescu@ie.ibm.com; adibotea@ie.ibm.com						ALLIS LV, 1994, ARTIF INTELL, V66, P91, DOI 10.1016/0004-3702(94)90004-3; Campbell M, 2002, ARTIF INTELL, V134, P57, DOI 10.1016/S0004-3702(01)00129-1; Chrabakh Wahid, 2003, TECHNICAL REPORT; Dechter R, 2007, ARTIF INTELL, V171, P73, DOI 10.1016/j.artint.2006.11.003; Enzenberger M, 2010, IEEE T COMP INTEL AI, V2, P259, DOI 10.1109/TCIAIG.2010.2083662; Fishelson M., 2002, BIOINFORMATICS, V18, pS189, DOI [10.1093/bioinformatics/18.suppl_1.S189, DOI 10.1093/BIOINFORMATICS/18.SUPPL_1.S189]; Grama A, 1999, IEEE T KNOWL DATA EN, V11, P28, DOI 10.1109/69.755612; Hoki K, 2013, ICGA J, V36, P22, DOI 10.3233/ICG-2013-36103; Kaneko T, 2010, AAAI CONF ARTIF INTE, P95; Kishimoto A, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P400; Kishimoto A, 2012, ICGA J, V35, P131, DOI 10.3233/ICG-2012-35302; Kishimoto A, 2013, ARTIF INTELL, V195, P222, DOI 10.1016/j.artint.2012.10.007; KORF RE, 1993, ARTIF INTELL, V62, P41, DOI 10.1016/0004-3702(93)90045-D; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Marinescu R, 2009, ARTIF INTELL, V173, P1457, DOI 10.1016/j.artint.2009.07.003; Marinescu R, 2009, ARTIF INTELL, V173, P1492, DOI 10.1016/j.artint.2009.07.004; Nagai A, 2002, THESIS; Otten L., 2012, P C UNC ART INT, P665; Pearl Judea, 2014, PROBABILISTIC REASON; Pennock D. M., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P431; Saito JT, 2010, LECT NOTES COMPUT SC, V6048, P75, DOI 10.1007/978-3-642-12993-3_8; Xia Y., 2008, IEEE INT S PAR DISTR; Yanover C, 2008, J COMPUT BIOL, V15, P899, DOI 10.1089/cmb.2007.0158	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103018
C	Korhonen, JH; Parviainen, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Korhonen, Janne H.; Parviainen, Pekka			Tractable Bayesian Network Structure Learning with Bounded Vertex Cover Number	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Both learning and inference tasks on Bayesian networks are NP-hard in general. Bounded tree-width Bayesian networks have recently received a lot of attention as a way to circumvent this complexity issue; however, while inference on bounded tree-width networks is tractable, the learning problem remains NP-hard even for tree-width 2. In this paper, we propose bounded vertex cover number Bayesian networks as an alternative to bounded tree-width networks. In particular, we show that both inference and learning can be done in polynomial time for any fixed vertex cover number bound k, in contrast to the general and bounded tree-width cases; on the other hand, we also show that learning problem is W[1]-hard in parameter k. Furthermore, we give an alternative way to learn bounded vertex cover number Bayesian networks using integer linear programming (ILP), and show this is feasible in practice.	[Korhonen, Janne H.] Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland; [Parviainen, Pekka] Aalto Univ, Dept Comp Sci, HIIT, Espoo, Finland	Aalto University; University of Helsinki; Aalto University	Korhonen, JH (corresponding author), Univ Helsinki, Dept Comp Sci, HIIT, Helsinki, Finland.	janne.h.korhonen@helsinki.fi; pekka.parviainen@aalto.fi			Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN) [251170]	Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN)	We thank James Cussens for fruitful discussions. This research was partially funded by the Academy of Finland (Finnish Centre of Excellence in Computational Inference Research COIN, 251170). The experiments were performed using computing resources within the Aalto University School of Science "Science-IT" project.	Bartlett Mark, 2013, 29 C UNC ART INT UAI; Berg Jeremias, 2014, 17 INT C ART INT STA; Chickering DM, 2004, J MACH LEARN RES, V5, P1287; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; COOPER GF, 1990, ARTIF INTELL, V42, P393, DOI 10.1016/0004-3702(90)90060-D; COOPER GF, 1992, MACH LEARN, V9, P309, DOI 10.1023/A:1022649401552; Cussens James, 2011, 27 C UNC ART INT UAI; Dasgupta Sanjoy, 1999, 15 C UNC ART INT UAI; Downey R.G., 1999, MG COMP SCI; Downey Rodney G., 1994, P 2 CORN WORKSH FEAS, P219; Elidan G, 2008, J MACH LEARN RES, V9, P2699; Grohe M., 2006, TEXT THEORET COMP S; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Jaakkola Tommi, 2010, 13 INT C ART INT STA; Korhonen Janne H., 2013, 16 INT C ART INT STA; Kwisthout Johan H. P., 2010, 19 EUR C ART INT ECA; Meek C, 2001, J ARTIF INTELL RES, V15, P383, DOI 10.1613/jair.914; Nie Siqi, 2014, ADV NEURAL INFORM PR; Niedermeier R., 2006, INVITATION FIXED PAR; Ott Sascha, 2003, Genome Inform, V14, P124; Parviainen Pekka, 2014, 17 INT C ART INT STA; Silander Tomi, 2006, 22 C UNC ART INT UAI	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103041
C	Koyejo, O; Natarajan, N; Ravikumar, P; Dhillon, IS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Koyejo, Oluwasanmi; Natarajan, Nagarajan; Ravikumar, Pradeep; Dhillon, Inderjit S.			Consistent Multilabel Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Multilabel classification is rapidly developing as an important aspect of modern predictive modeling, motivating study of its theoretical aspects. To this end, we propose a framework for constructing and analyzing multilabel classification metrics which reveals novel results on a parametric form for population optimal classifiers, and additional insight into the role of label correlations. In particular, we show that for multilabel metrics constructed as instance-, micro- and macro-averages, the population optimal classifier can be decomposed into binary classifiers based on the marginal instance- conditional distribution of each label, with a weak association between labels via the threshold. Thus, our analysis extends the state of the art from a few known multilabel classification metrics such as Hamming loss, to a general framework applicable to many of the classification metrics in common use. Based on the population-optimal classifier, we propose a computationally efficient and general-purpose plug-in classification algorithm, and prove its consistency with respect to the metric of interest. Empirical results on synthetic and benchmark datasets are supportive of our theoretical findings.	[Koyejo, Oluwasanmi] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA; [Natarajan, Nagarajan; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	Stanford University; University of Texas System; University of Texas Austin	Koyejo, O (corresponding author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.	sanmi@stanford.edu; naga86@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			NSF [CCF-1117055, CCF-1320746, IIS-1320894]; NIH part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences [R01 GM117594-01]	NSF(National Science Foundation (NSF)); NIH part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences	We acknowledge the support of NSF via CCF-1117055, CCF-1320746 and IIS-1320894, and NIH via R01 GM117594-01 as part of the Joint DMS/NIGMS Initiative to Support Research at the Interface of the Biological and Mathematical Sciences.	Cheng W., 2010, P 27 INT C MACH LEAR, P279; Dembczynski K., 2013, P 30 INT C MACH LEAR, P1130; Dembczynski K., 2012, P 29 INT C MACH LEAR, P1319; Dembczynski KJ, 2011, ADV NEUR INF PROC SY; Dembczynski K, 2012, MACH LEARN, V88, P5, DOI 10.1007/s10994-012-5285-8; Devroye Luc P., 1996, PROBABILISTIC THEORY, V31; Gao W, 2013, ARTIF INTELL, V199, P22, DOI 10.1016/j.artint.2013.03.001; Kapoor Ashish, 2012, ADV NEURAL INFORM PR, P2645; Koyejo O., 2014, ADV NEURAL INFORM PR, V3, P2744; Narasimhan H., 2014, ADV NEURAL INF PROC, V27, P1493; Narasimhan H, 2015, PR MACH LEARN RES, V37, P2398; Petterson J., 2011, ADV NEURAL INFORM PR, P1512; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Waegeman W, 2014, J MACH LEARN RES, V15, P3333; Ye N, 2012, P INT C MACH LEARN; Yu H.-F., 2014, INT C MACH LEARN, P593	18	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102032
C	Krishnan, RG; Lacoste-Julien, S; Sontag, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Krishnan, Rahul G.; Lacoste-Julien, Simon; Sontag, David			Barrier Frank-Wolfe for Marginal Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We introduce a globally-convergent algorithm for optimizing the tree-reweighted (TRW) variational objective over the marginal polytope. The algorithm is based on the conditional gradient method (Frank-Wolfe) and moves pseudomarginals within the marginal polytope through repeated maximum a posteriori (MAP) calls. This modular structure enables us to leverage black-box MAP solvers (both exact and approximate) for variational inference, and obtains more accurate results than tree-reweighted algorithms that optimize over the local consistency relaxation. Theoretically, we bound the sub-optimality for the proposed algorithm despite the TRW objective having unbounded gradients at the boundary of the marginal polytope. Empirically, we demonstrate the increased quality of results found by tightening the relaxation over the marginal polytope as well as the spanning tree polytope on synthetic and real-world instances.	[Krishnan, Rahul G.; Sontag, David] NYU, Courant Inst, New York, NY 10003 USA; [Lacoste-Julien, Simon] Ecole Normale Super, Sierra Project Team, INRIA, Paris, France	New York University; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Krishnan, RG (corresponding author), NYU, Courant Inst, New York, NY 10003 USA.				DARPA Probabilistic Programming for Advancing Machine Learning (PPAML) Program under AFRL prime [FA8750-14-C-0005]	DARPA Probabilistic Programming for Advancing Machine Learning (PPAML) Program under AFRL prime	RK and DS gratefully acknowledge the support of the DARPA Probabilistic Programming for Advancing Machine Learning (PPAML) Program under AFRL prime contract no. FA8750-14-C-0005.	Allouche David, 2010, TOULBAR2 OPEN SOURCE; Andres B. T, 2012, OPENGM C PLUS PLUS L; Belanger D., 2013, NIPS WORKSH GREED OP; BESAG J, 1986, J R STAT SOC B; Borenstein E., 2002, ECCV; Boykov Y., 2004, TPAMI; Domke J., 2013, TPAMI; Ermon Stefano, 2013, ICML; Garber D., 2013, ARXIV13014666; Globerson Amir, 2007, UAI; Hazan T., 2012, ICML; I. Gurobi Optimization, 2015, GUROBI OPTIMIZER REF; Jaggi Martin., 2013, ICML; Jancsary J., 2011, AISTATS; Kappes J. H., 2013, CVPR; Kolmogorov V., 2006, TPAMI; Kolmogorov V., 2007, TPAMI; Koo Terry, 2007, EMNLP CONLL; Lacoste-Julien S., 2015, NIPS; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; London B., 2015, ICML; Mooij J., 2010, JMLR; Nowozin S., 2011, ICCV; Papandreou G., 2011, ICCV; Salakhutdinov R., 2008, TECHNICAL REPORT; Shimony S. E., 1994, ARTIF INTELL; Sontag D., 2008, UNCERTAINTY ARTIFICI; SONTAG D, 2007, NIPS; Wainwright M. J., 2005, IEEE T INFORM THEORY; Wang S., 2014, ICLR WORKSH	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102041
C	Kwitt, R; Huber, S; Niethammer, M; Lin, WL; Bauer, U		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kwitt, Roland; Huber, Stefan; Niethammer, Marc; Lin, Weili; Bauer, Ulrich			Statistical Topological Data Analysis - A Kernel Perspective	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the problem of statistical computations with persistence diagrams, a summary representation of topological features in data. These diagrams encode persistent homology, a widely used invariant in topological data analysis. While several avenues towards a statistical treatment of the diagrams have been explored recently, we follow an alternative route that is motivated by the success of methods based on the embedding of probability measures into reproducing kernel Hilbert spaces. In fact, a positive definite kernel on persistence diagrams has recently been proposed, connecting persistent homology to popular kernel-based learning techniques such as support vector machines. However, important properties of that kernel enabling a principled use in the context of probability measure embeddings remain to be explored. Our contribution is to close this gap by proving universality of a variant of the original kernel, and to demonstrate its effective use in two-sample hypothesis testing on synthetic as well as real-world data.	[Kwitt, Roland] Univ Salzburg, Dept Comp Sci, Salzburg, Austria; [Huber, Stefan] IST Austria, Klosterneuburg, Austria; [Niethammer, Marc] Univ N Carolina, Dept Comp Sci, Chapel Hill, NC 27515 USA; [Niethammer, Marc; Lin, Weili] Univ N Carolina, BRIC, Chapel Hill, NC 27515 USA; [Lin, Weili] Univ N Carolina, Dept Radiol, Chapel Hill, NC 27515 USA; [Bauer, Ulrich] Tech Univ Munich, Dept Math, Munich, Germany	Salzburg University; Institute of Science & Technology - Austria; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina; University of North Carolina Chapel Hill; University of North Carolina; University of North Carolina Chapel Hill; Technical University of Munich	Kwitt, R (corresponding author), Univ Salzburg, Dept Comp Sci, Salzburg, Austria.	rkwitt@gmx.at; stefan.huber@ist.ac.at; mn@cs.unc.edu; weili_lin@med.unc.edu; ulrich@bauer.org	Kwitt, Roland/AFS-8639-2022		Austrian Science Fund [KLI 00012]	Austrian Science Fund(Austrian Science Fund (FWF))	This work has been partially supported by the Austrian Science Fund, project no. KLI 00012. We also thank the anonymous reviewers for their valuable comments/suggestions.	Adcock A., 2013, RING ALGEBRAIC FUNCT; Bendich P., 2014, PERSISTENT HOMOLOGY; Bompard L, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0108306; Bubenik P, 2015, J MACH LEARN RES, V16, P77; Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X; Chazal F., 2014, SOCG; Christmann A., 2010, NIPS; Chung M., 2009, IPMI; Dryden I. L., 2016, WILEY SERIES PROBABI, V2nd; Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO; Fasy BT, 2014, ANN STAT, V42, P2301, DOI 10.1214/14-AOS1252; Fukumizu K, 2013, J MACH LEARN RES, V14, P3753; Gretton A, 2012, J MACH LEARN RES, V13, P723; LEDOUX M, 2011, CLASSICS MATH; Lee H., 2014, MICCAI; Li C., 2014, CVPR; Marcus DS, 2010, J COGNITIVE NEUROSCI, V22, P2677, DOI 10.1162/jocn.2009.21407; Mileyko Y, 2011, INVERSE PROBL, V27, DOI 10.1088/0266-5611/27/12/124007; Munch E., 2013, CORR; Reininghaus R., 2015, CVPR; Scholkopf B., 2001, LEARNING KERNELS SUP; Singh N., 2014, MLMI; Smola A., 2007, ALT; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Sun Jian, 2009, SGP; Turner K, 2014, DISCRETE COMPUT GEOM, V52, P44, DOI 10.1007/s00454-014-9604-7	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102036
C	Lacoste-Julien, S; Jaggi, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lacoste-Julien, Simon; Jaggi, Martin			On the Global Linear Convergence of Frank-Wolfe Optimization Variants	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SIMPLICIAL DECOMPOSITION	The Frank-Wolfe (FW) optimization algorithm has lately re-gained popularity thanks in particular to its ability to nicely handle the structured constraints appearing in machine learning applications. However, its convergence rate is known to be slow (sublinear) when the solution lies at the boundary. A simple less-known fix is to add the possibility to take 'away steps' during optimization, an operation that importantly does not require a feasibility oracle. In this paper, we highlight and clarify several variants of the Frank-Wolfe optimization algorithm that have been successfully applied in practice: away-steps FW, pairwise FW, fully-corrective FW and Wolfe's minimum norm point algorithm, and prove for the first time that they all enjoy global linear convergence, under a weaker condition than strong convexity of the objective. The constant in the convergence rate has an elegant interpretation as the product of the (classical) condition number of the function with a novel geometric quantity that plays the role of a 'condition number' of the constraint set. We provide pointers to where these algorithms have made a difference in practice, in particular with the flow polytope, the marginal polytope and the base polytope for submodular optimization.	[Lacoste-Julien, Simon] INRIA, SIERRA Project Team, Ecole Normale Super, Paris, France; [Jaggi, Martin] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Swiss Federal Institutes of Technology Domain; ETH Zurich	Lacoste-Julien, S (corresponding author), INRIA, SIERRA Project Team, Ecole Normale Super, Paris, France.				MSR-Inria Joint Center; Google Research Award	MSR-Inria Joint Center; Google Research Award(Google Incorporated)	We thank J.B. Alayrac, E. Hazan, A. Hubard, A. Osokin and P. Marcotte for helpful discussions. This work was partially supported by the MSR-Inria Joint Center and a Google Research Award.	Ahipasaoglu SD, 2008, OPTIM METHOD SOFTW, V23, P5, DOI 10.1080/10556780701589669; Alexander R., 1977, GEOMETRIAE DEDICATA, V6, P87, DOI [10.1007/BF00181583, DOI 10.1007/BF00181583]; [Anonymous], 2015, NIPS; Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Beck A, 2004, MATH METHOD OPER RES, V59, P235, DOI 10.1007/s001860300327; Beck A., 2015, ARXIV150405002V1; CANON MD, 1968, SIAM J CONTROL, V6, P509, DOI 10.1137/0306032; Chari V, 2015, CVPR; DUNN JC, 1979, SIAM J CONTROL OPTIM, V17, P187, DOI 10.1137/0317015; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garber D., 2013, ARXIV13014666V5; Garber D., 2015, ICML; Guelat J., 1986, MATH PROGRAMMING; HEARN DW, 1987, MATH PROGRAM STUD, V31, P99; Holloway C. A., 1974, Mathematical Programming, V6, P14, DOI 10.1007/BF01580219; Jaggi M., 2013, ICML; Joulin A., 2014, ECCV; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Krishnan R., 2015, NIPS; Kumar P., 2010, INFORMS J COMPUTING; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; Lacoste-Julien S., 2013, ARXIV13127864V2; Lan G., 2013, ARXIV13095550V2; Levitin Evgeny S, 1966, USSR COMP MATH MATH, V6, P1, DOI DOI 10.1016/0041-5553(66)90114-5; Mitchell B., 1974, SIAM J CONTROL, V12; Nanculef R., 2014, INFORM SCI; Nesterov Y., 2004, INTRO LECT CONVEX OP; Pena J., 2015, ARXIV150704073V2; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Robinson S. M., 1982, GEN EQUATIONS THEI 2; VONHOHENBALKEN B, 1977, MATH PROGRAM, V13, P49, DOI 10.1007/BF01584323; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang PW, 2014, J MACH LEARN RES, V15, P1523; WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381; Wolfe P., 1970, INTEGER NONLINEAR PR; Ziegler G. M., 1999, ARXIVMATH9909177V1	36	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102073
C	Lei, YW; Dogan, U; Binder, A; Kloft, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lei, Yunwen; Dogan, Urun; Binder, Alexander; Kloft, Marius			Multi-class SVMs: From Tighter Data-Dependent Generalization Bounds to Novel Algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper studies the generalization performance of multi-class classification algorithms, for which we obtain-for the first time-a data-dependent generalization error bound with a logarithmic dependence on the class size, substantially improving the state-of-the-art linear dependence in the existing data-dependent generalization analysis. The theoretical analysis motivates us to introduce a new multi-class classification machine based on l(p)-norm regularization, where the parameter p controls the complexity of the corresponding bounds. We derive an efficient optimization algorithm based on Fenchel duality theory. Benchmarks on several real-world datasets show that the proposed algorithm can achieve significant accuracy gains over the state of the art.	[Lei, Yunwen] City Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China; [Dogan, Urun] Microsoft Res, Cambridge CB1 2FB, England; [Binder, Alexander] Singapore Univ Technol, ISTD Pillar, Singapore, Singapore; [Binder, Alexander] TU Berlin, Design Machine Learning Grp, Berlin, Germany; [Kloft, Marius] Humboldt Univ, Dept Comp Sci, Berlin, Germany	City University of Hong Kong; Microsoft; Technical University of Berlin; Humboldt University of Berlin	Lei, YW (corresponding author), City Univ Hong Kong, Dept Math, Hong Kong, Hong Kong, Peoples R China.	yunwelei@cityu.edu.hk; udogan@microsoft.com; alexander_binder@sutd.edu.sg; kloft@hu-berlin.de	Lei, Yunwen/V-2782-2018		German Research Foundation (DFG) [KL 2698/2-1]	German Research Foundation (DFG)(German Research Foundation (DFG))	We thank Mehryar Mohri for helpful discussions. This work was partly funded by the German Research Foundation (DFG) award KL 2698/2-1.	Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bengio Samy, 2010, ADV NEURAL INFORM PR, V1, P163, DOI [10.5555/2997189.2997208, DOI 10.5555/2997189.2997208]; Beygelzimer A., 2009, P 25 C UNC ART INT, P51; Cortes Corinna, 2013, INT C MACHINE LEARNI, P46, DOI DOI 10.1007/978-1-4471-4351-2_3; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Dekel O., 2010, P 13 INT C ART INT S, V9, P137; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Glasmachers T., 2010, ADV NEURAL INFORM PR, P739; Guermeur Y, 2002, PATTERN ANAL APPL, V5, P168, DOI 10.1007/s100440200015; Gupta MR, 2014, J MACH LEARN RES, V15, P1461; Hill SI, 2007, J ARTIF INTELL RES, V30, P525, DOI 10.1613/jair.2251; Hofmann T., 2003, NIPS WORKSH SYNT SEM; Jain P, 2009, PROC CVPR IEEE, P762, DOI 10.1109/CVPRW.2009.5206651; Jia Y., P ACM MULT, P675; Keerthi SS, 2008, P 14 ACM SIGKDD INT, P408, DOI DOI 10.1145/1401890.1401942; Kloft M, 2011, J MACH LEARN RES, V12, P953; Koltchinskii V, 2002, ANN STAT, V30, P1; Koltchinskii V, 2000, PROG PROBAB, V47, P443; Kuznetsov V., 2014, ADV NEURAL INFORM PR, P2501; Lang Ken, 1995, MACHINE LEARNING P; Ledoux M., 1991, PROBABILITY BANACH S, V23; Micchelli CA, 2005, J MACH LEARN RES, V6, P1099; Mohri M., 2018, FDN MACHINE LEARNING; Oneto Luca, 2011, ADV NEURAL INFORM PR, P585; RENNIE JDM, 2001, AIM2001026 MIT ART I; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Welinder P., 2010, CNSTR2011001 CAL I T; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Zhang T., 2004, ADV NEURAL INFORM PR, P1625	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103050
C	Li, WY		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Wenye			Estimating Jaccard Index with Missing Observations: A Matrix Calibration Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The Jaccard index is a standard statistics for comparing the pairwise similarity between data samples. This paper investigates the problem of estimating a Jaccard index matrix when there are missing observations in data samples. Starting from a Jaccard index matrix approximated from the incomplete data, our method calibrates the matrix to meet the requirement of positive semi-definiteness and other constraints, through a simple alternating projection algorithm. Compared with conventional approaches that estimate the similarity matrix based on the imputed data, our method has a strong advantage in that the calibrated matrix is guaranteed to be closer to the unknown ground truth in the Frobenius norm than the un-calibrated matrix (except in special cases they are identical). We carried out a series of empirical experiments and the results confirmed our theoretical justification. The evaluation also reported significantly improved results in real learning tasks on benchmark datasets.	[Li, Wenye] Macao Polytech Inst, Macau, Peoples R China	Macao Polytechnic University	Li, WY (corresponding author), Macao Polytech Inst, Macau, Peoples R China.	wyli@ipm.edu.mo			Science and Technology Development Fund, Macao SAR, China [006/2014/A]	Science and Technology Development Fund, Macao SAR, China	The work is supported by The Science and Technology Development Fund (Project No. 006/2014/A), Macao SAR, China.	Bouchard M, 2013, INT J APPROX REASON, V54, P615, DOI 10.1016/j.ijar.2013.01.006; Boyd S, 2005, SIAM J MATRIX ANAL A, V27, P532, DOI 10.1137/040609902; Boyd S, 2004, CONVEX OPTIMIZATION; Broder A. Z., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P327, DOI 10.1145/276698.276781; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deutsch F., 2001, BEST APPROXIMATION I; Duda R.O., 2000, PATTERN CLASSIFICATI; DYKSTRA RL, 1983, J AM STAT ASSOC, V78, P837, DOI 10.2307/2288193; Escalante R, 2011, FUND ALGORITHMS, V8, P1; Ghahramani Z., 1994, P ADV NEUR INF PROC, P120; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Higham NJ, 2002, IMA J NUMER ANAL, V22, P329, DOI 10.1093/imanum/22.3.329; Jaccard P., 1912, NEW PHYTOL, V11, P37, DOI [DOI 10.1111/J.1469-8137.1912.TB05611.X, 10.1111/j.1469-8137.1912.tb05611.x]; KNOL DL, 1989, PSYCHOMETRIKA, V54, P53, DOI 10.1007/BF02294448; Leskovec J, 2014, MINING OF MASSIVE DATASETS, 2ND EDITION, P1; Li P, 2011, COMMUN ACM, V54, P101, DOI 10.1145/1978542.1978566; Li W., 2007, P 24 INT C MACH LEAR, P529; Luenberger D. G., 1969, OPTIMIZATION VECTOR; Malick J, 2004, SIAM J MATRIX ANAL A, V26, P272, DOI 10.1137/S0895479802413856; Qi HD, 2006, SIAM J MATRIX ANAL A, V28, P360, DOI 10.1137/050624509; ROGERS DJ, 1960, SCIENCE, V132, P1115, DOI 10.1126/science.132.3434.1115; SALTON G, 1975, COMMUN ACM, V18, P613, DOI 10.1145/361219.361220; Scholkopf B., 2001, LEARNING KERNELS SUP	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103021
C	Li, X; Ramchandran, K		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Li, Xiao; Ramchandran, Kannan			An Active Learning Framework using Sparse-Graph Codes for Sparse Polynomials and Graph Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DECISION TREES	Let f : {-1, 1}(n) -> R be an n-variate polynomial consisting of 2(n) monomials, in which only s << 2(n) coefficients are non-zero. The goal is to learn the polynomial by querying the values of f. We introduce an active learning framework that is associated with a low query cost and computational runtime. The significant savings are enabled by leveraging sampling strategies based on modern coding theory, specifically, the design and analysis of sparse-graph codes, such as Low-Density-Parity-Check (LDPC) codes, which represent the state-of-the-art of modern packet communications. More significantly, we show how this design perspective leads to exciting, and to the best of our knowledge, largely unexplored intellectual connections between learning and coding. The key is to relax the worst-case assumption with an ensemble-average setting, where the polynomial is assumed to be drawn uniformly at random from the ensemble of all polynomials (of a given size n and sparsity s). Our framework succeeds with high probability with respect to the polynomial ensemble with sparsity up to s = O(2(delta n)) for any delta is an element of(0, 1), where f is exactly learned using O(ns) queries in time O(ns log s), even if the queries are perturbed by Gaussian noise. We further apply the proposed framework to graph sketching, which is the problem of inferring sparse graphs by querying graph cuts. By writing the cut function as a polynomial and exploiting the graph structure, we propose a sketching algorithm to learn the an arbitrary n-node unknown graph using only few cut queries, which scales almost linearly in the number of edges and sub-linearly in the graph size n. Experiments on real datasets show significant reductions in the runtime and query complexity compared with competitive schemes.	[Li, Xiao; Ramchandran, Kannan] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Li, X (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	xiaoli@berkeley.edu; kannanr@berkeley.edu			 [NSF CCF EAGER 1439725]		This work was supported by grant NSF CCF EAGER 1439725.	Angluin D., 1992, Proceedings of the Twenty-Fourth Annual ACM Symposium on the Theory of Computing, P351, DOI 10.1145/129712.129746; Bouvel M, 2005, LECT NOTES COMPUT SC, V3787, P16; Bshouty NH, 2010, LEIBNIZ INT PR INFOR, V5, P143, DOI 10.4230/LIPIcs.STACS.2010.2496; Bshouty NH, 1995, AN S FDN CO, P304, DOI 10.1109/SFCS.1995.492486; Choi SS, 2011, J COMPUT SYST SCI, V77, P1039, DOI 10.1016/j.jcss.2010.08.011; Goldman S. A., 2010, ALGORITHMS THEORY CO, V1, P26; Jackson J., 1994, Proceedings. 35th Annual Symposium on Foundations of Computer Science (Cat. No.94CH35717), P42, DOI 10.1109/SFCS.1994.365706; Kearns M-J., 1990, COMPUTATIONAL COMPLE; Kocaoglu M., 2014, ADV NEURAL INFORM PR, V3122; KUSHILEVITZ E, 1993, SIAM J COMPUT, V22, P1331, DOI 10.1137/0222080; MANSOUR Y, 1995, SIAM J COMPUT, V24, P357, DOI 10.1137/S0097539792239291; Mansour Y., 1994, THEORETICAL ADV NEUR, P391; Mazzawi H., 2011, THESIS; Negahban S, 2012, ANN ALLERTON CONF, P2032, DOI 10.1109/Allerton.2012.6483472; Richardson T., 2008, MODERN CODING THEORY; Scheibler R., 1803, ARXIV13101803; Settles B., 2010, 1648 U WISC MAD DEP, DOI DOI 10.1016/J.MATLET.2010.11.072; Stobbe Peter, 2012, P 15 INT C ART INT S, P1125	18	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100065
C	Liang, M; Hu, XL; Zhang, B		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Liang, Ming; Hu, Xiaolin; Zhang, Bo			Convolutional Neural Networks with Intra-layer Recurrent Connections for Scene Labeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BACKPROPAGATION	Scene labeling is a challenging computer vision task. It requires the use of both local discriminative features and global context information. We adopt a deep recurrent convolutional neural network (RCNN) for this task, which is originally proposed for object recognition. Different from traditional convolutional neural networks (CNN), this model has intra-layer recurrent connections in the convolutional layers. Therefore each convolutional layer becomes a two-dimensional recurrent neural network. The units receive constant feed-forward inputs from the previous layer and recurrent inputs from their neighborhoods. While recurrent iterations proceed, the region of context captured by each unit expands. In this way, feature extraction and context modulation are seamlessly integrated, which is different from typical methods that entail separate modules for the two steps. To further utilize the context, a multi-scale RCNN is proposed. Over two benchmark datasets, Standford Background and Sift Flow, the model outperforms many state-of-the-art models in accuracy and efficiency.	[Liang, Ming; Hu, Xiaolin; Zhang, Bo] Tsinghua Univ, CBICR, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China	Tsinghua University	Liang, M (corresponding author), Tsinghua Univ, CBICR, Dept Comp Sci & Technol, Tsinghua Natl Lab Informat Sci & Technol TNList, Beijing 100084, Peoples R China.	liangm07@mails.tsinghua.edu.cn; xlhu@tsinghua.edu.cn; dcszb@tsinghua.edu.cn			National Basic Research Program (973 Program) of China [2012CB316301, 2013CB329403]; National Natural Science Foundation of China [61273023, 91420201, 61332007]; Natural Science Foundation of Beijing [4132046]	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Beijing(Beijing Natural Science Foundation)	We are grateful to the anonymous reviewers for their valuable comments. This work was supported in part by the National Basic Research Program (973 Program) of China under Grant 2012CB316301 and Grant 2013CB329403, in part by the National Natural Science Foundation of China under Grant 61273023, Grant 91420201, and Grant 61332007, in part by the Natural Science Foundation of Beijing under Grant 4132046.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen LC., ARXIV 2015ABS1412706; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eigen D., 2014, 6 INT C LEARN REPR I; Eigen D., 2014, ICLR; Eigen D, 2012, PROC CVPR IEEE, P2799, DOI 10.1109/CVPR.2012.6248004; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Gould S, 2009, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2009.5459211; Grangier D., 2009, ICML DEEP LEARN WORK, V3; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lempitsky V., 2011, NIPS, V24, P1485; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liu C, 2011, IEEE T PATTERN ANAL, V33, P2368, DOI 10.1109/TPAMI.2011.131; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Mostajabi M., 2015, CVPR; Pinheiro Pedro H. O., 2014, ICML; Sharma A., 2014, ADV NEURAL INFORM PR; Singh G, 2013, PROC CVPR IEEE, P3151, DOI 10.1109/CVPR.2013.405; Socher R., 2011, P 28 INT C INT C MAC, P129; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tighe J, 2013, PROC CVPR IEEE, P3001, DOI 10.1109/CVPR.2013.386; Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Zheng S., 2015, ICCV	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100002
C	Linderman, SW; Johnson, MJ; Adams, RP		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Linderman, Scott W.; Johnson, Matthew J.; Adams, Ryan P.			Dependent Multinomial Models Made Easy: Stick Breaking with the Polya-Gamma Augmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Many practical modeling problems involve discrete data that are best represented as draws from multinomial or categorical distributions. For example, nucleotides in a DNA sequence, children's names in a given state and year, and text documents are all commonly modeled with multinomial distributions. In all of these cases, we expect some form of dependency between the draws: the nucleotide at one position in the DNA strand may depend on the preceding nucleotides, children's names are highly correlated from year to year, and topics in text may be correlated and dynamic. These dependencies are not naturally captured by the typical Dirichlet-multinomial formulation. Here, we leverage a logistic stick-breaking representation and recent innovations in Polya-gamma augmentation to reformulate the multinomial distribution in terms of latent variables with jointly Gaussian likelihoods, enabling us to take advantage of a host of Bayesian inference techniques for Gaussian models with minimal overhead.	[Linderman, Scott W.; Johnson, Matthew J.; Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA; [Adams, Ryan P.] Twitter, Cambridge, MA 02138 USA	Harvard University; Twitter, Inc.	Linderman, SW (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	swl@seas.harvard.edu; mattjj@csail.mit.edu; rpa@seas.harvard.edu			Siebel Scholarship; Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216]; Harvard/MIT Joint Research Grants Program; NSF [IIS-1421780]; Alfred P. Sloan Foundation	Siebel Scholarship; Center for Brains, Minds and Machines (CBMM) - NSF STC; Harvard/MIT Joint Research Grants Program; NSF(National Science Foundation (NSF)); Alfred P. Sloan Foundation(Alfred P. Sloan Foundation)	S.W.L. is supported by a Siebel Scholarship and the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216. M.J.J. is supported by the Harvard/MIT Joint Research Grants Program. R.P.A. is supported by NSF IIS-1421780 as well as the Alfred P. Sloan Foundation.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; [Anonymous], 2012, ARTIF INTELL; Belanger David, 2015, P INT C MACH LEARN; Belanger David, 2014, NIPS 2014 MOD ML NLP; Blei D.M., 2006, P 23 INT C MACH LEAR, P113, DOI [10.1145/1143844.1143859, DOI 10.1145/1143844.1143859, 10.1145/1143844.114385]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chen J, 2013, ADV NEURAL INFORM PR, P2445; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Holmes CC, 2006, BAYESIAN ANAL, V1, P145, DOI 10.1214/06-BA105; Lafferty JohnD., 2006, ADV NEURAL INFORM PR, P147; Lindsten F., 2012, ADV NEURAL INFORM PR, P2591; Murray I., 2010, JMLR W CP, V9, P541; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Thrun S., 2005, PROBABILISTIC ROBOTI; Wan EA, 2000, IEEE 2000 ADAPTIVE SYSTEMS FOR SIGNAL PROCESSING, COMMUNICATIONS, AND CONTROL SYMPOSIUM - PROCEEDINGS, P153, DOI 10.1109/ASSPCC.2000.882463; Wang X., 2008, ADV NEURAL INFORM PR, P1577; Zhou Mingyuan, 2012, Proc Int Conf Mach Learn, V2012, P1343	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100028
C	Liu, Q; Fisher, J; Ihler, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Liu, Qiang; Fisher, John, III; Ihler, Alexander			Probabilistic Variational Bounds for Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				SAMPLING ALGORITHMS	Variational algorithms such as tree-reweighted belief propagation can provide deterministic bounds on the partition function, but are often loose and difficult to use in an "any-time" fashion, expending more computation for tighter bounds. On the other hand, Monte Carlo estimators such as importance sampling have excellent any-time behavior, but depend critically on the proposal distribution. We propose a simple Monte Carlo based inference method that augments convex variational bounds by adding importance sampling (IS). We argue that convex variational methods naturally provide good IS proposals that "cover" the target probability, and reinterpret the variational optimization as designing a proposal to minimize an upper bound on the variance of our IS estimator. This both provides an accurate estimator and enables construction of any-time probabilistic bounds that improve quickly and directly on state-of-the-art variational bounds, and provide certificates of accuracy given enough samples relative to the error in the initial bound.	[Liu, Qiang] Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA; [Fisher, John, III] MIT, CSAIL, Cambridge, MA 02139 USA; [Ihler, Alexander] Univ Calif Irvine, Comp Sci, Irvine, CA USA	Dartmouth College; Massachusetts Institute of Technology (MIT); University of California System; University of California Irvine	Liu, Q (corresponding author), Dartmouth Coll, Comp Sci, Hanover, NH 03755 USA.	qliu@cs.dartmouth.edu; fisher@csail.mit.edu; ihler@ics.uci.edu		Ihler, Alexander/0000-0002-4331-1015	VITALITE under the ARO MURI program [W911NF-11-1-0391]; NSF [IIS-1065618, IIS-1254071]; United States Air Force [FA8750-14-C-0011]	VITALITE under the ARO MURI program; NSF(National Science Foundation (NSF)); United States Air Force(United States Department of Defense)	This work is supported in part by VITALITE, under the ARO MURI program (Award number W911NF-11-1-0391); NSF grants IIS-1065618 and IIS-1254071; and by the United States Air Force under Contract No. FA8750-14-C-0011 under the DARPA PPAML program.	Bengtsson T, 2008, PROBABILITY STAT ESS, V2, P316, DOI [DOI 10.1214/193940307000000518, 10.1214/193940307000000518]; Cheng J, 2001, COMPUTATION STAT, V16, P1, DOI 10.1007/s001800100049; Cheng J., 2000, J ARTIFICIAL INTELLI; Dagum P, 2000, SIAM J COMPUT, V29, P1484, DOI 10.1137/S0097539797315306; Dagum P, 1997, ARTIF INTELL, V93, P1, DOI 10.1016/S0004-3702(97)00013-1; De Freitas N., 2001, UAI; Dechter R, 2003, J ACM, V50, P107, DOI 10.1145/636865.636866; Fung R., 1990, UAI; Globerson A., 2007, ARTIFICIAL INTELLIGE, P130; Gogate V, 2011, INTELL ARTIF, V5, P171, DOI 10.3233/IA-2011-0026; Gogate Vibhav, 2009, THESIS; Hazan T., 2012, ICML; Koller D., 2009, PROBABILISTIC GRAPHI; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Liu J. S., 2008, MONTE CARLO STRATEGI; Liu Q., 2014, THESIS; Liu Q., 2011, ICML; Mateescu R, 2010, J ARTIF INTELL RES, V37, P279, DOI 10.1613/jair.2842; Maurer A, 2009, P 22 ANN C LEARN THE, P115; Mnih Volodymyr, 2008, ICML; Oh M-S, 1992, J STAT COMPUT SIMUL, V41, P143, DOI [DOI 10.1080/00949659208810398, 10.1080/00949659208810398]; ORABONA F, 2014, ICML; Papandreou G., 2011, ICCV; Ruozzi N., 2012, NIPS; Salimans T., 2015, ICML; Shachter R., 1990, UAI; Sudderth E., 2007, P ADV NEUR INF PROC, P1425; Wainwright MJ, 2006, J MACH LEARN RES, V7, P1829; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P2313, DOI 10.1109/TIT.2005.850091; Wexler Y., 2007, UAI; Yuan C., 2002, P 19 C UNC ART INT, P624; Yuan C., 2007, P 22 NAT C ART INT V, V7, P1296; Yuan C, 2007, INT J APPROX REASON, V46, P320, DOI 10.1016/j.ijar.2006.09.006; Yuan CH, 2006, MATH COMPUT MODEL, V43, P1189, DOI 10.1016/j.mcm.2005.05.020	35	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100063
C	Liu, YY; Li, S; Li, FX; Song, L; Rehg, JM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Liu, Yu-Ying; Li, Shuang; Li, Fuxin; Song, Le; Rehg, James M.			Efficient Learning of Continuous-Time Hidden Markov Models for Disease Progression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The Continuous-Time Hidden Markov Model (CT-HMM) is an attractive approach to modeling disease progression due to its ability to describe noisy observations arriving irregularly in time. However, the lack of an efficient parameter learning algorithm for CT-HMM restricts its use to very small models or requires unrealistic constraints on the state transitions. In this paper, we present the first complete characterization of efficient EM-based learning methods for CT-HMM models. We demonstrate that the learning problem consists of two challenges: the estimation of posterior state probabilities and the computation of end-state conditioned statistics. We solve the first challenge by reformulating the estimation problem in terms of an equivalent discrete time-inhomogeneous hidden Markov model. The second challenge is addressed by adapting three approaches from the continuous time Markov chain literature to the CT-HMM domain. We demonstrate the use of CT-HMMs with more than 100 states to visualize and predict disease progression using a glaucoma dataset and an Alzheimer's disease dataset.	[Liu, Yu-Ying; Li, Shuang; Li, Fuxin; Song, Le; Rehg, James M.] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Liu, YY (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA.		Rehg, James/AAM-6888-2020	Rehg, James/0000-0003-1793-5462	NIH [R01 EY13178-15]; National Institute of Biomedical Imaging and Bioengineering through Big Data to Knowledge (BD2K) initiative [U54EB020404]; ADNI under NIH [U01 AG024904]; DOD [W81XWH-12-2-0012]; NSF/NIH [BIGDATA 1R01GM108341]; ONR [N00014-15-1-2340]; NSF [IIS-1218749]; NSF CAREER [IIS-1350983]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Institute of Biomedical Imaging and Bioengineering through Big Data to Knowledge (BD2K) initiative; ADNI under NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); DOD(United States Department of Defense); NSF/NIH(National Science Foundation (NSF)United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	Portions of this work were supported in part by NIH R01 EY13178-15 and by grant U54EB020404 awarded by the National Institute of Biomedical Imaging and Bioengineering through funds provided by the Big Data to Knowledge (BD2K) initiative (www.bd2k.nih.gov).Additionally, the collection and sharing of the Alzheimers data was funded by ADNI under NIH U01 AG024904 and DOD award W81XWH-12-2-0012. The research was also supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, and NSF CAREER IIS-1350983.	Bartolomeo N, 2011, BMC MED RES METHODOL, V11, DOI 10.1186/1471-2288-11-38; Bladt M., 2005, J R STAT SOC B, V39; Cox DR., 2017, THEORY STOCHASTIC PR; Fagan AM, 2009, ANN NEUROL, V65, P176, DOI 10.1002/ana.21559; Higham NJ, 2008, FUNCTIONS MATRICES T; Hobolth A, 2005, STAT APPL GENET MOL, V4; Hobolth A, 2011, J APPL PROBAB, V48, P911, DOI 10.1239/jap/1324046009; Jackson CH, 2011, J STAT SOFTW, V38, P1; Kingman S., 2004, B WHO, V82; Leiva-Murillo J. M., 2011, NIPS; Liu YY, 2013, LECT NOTES COMPUT SC, V8150, P444, DOI 10.1007/978-3-642-40763-5_55; Medeiros FA, 2012, AM J OPHTHALMOL, V153, P1197, DOI 10.1016/j.ajo.2011.11.015; Metzner P., 2007, J COMPUT PHYS, V227; Metzner P, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.066702; Nodelman U., 2005, P UNC AI UAI 05; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Tataru P, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-465; VANLOAN CF, 1978, IEEE T AUTOMAT CONTR, V23, P395, DOI 10.1109/TAC.1978.1101743; Wang X., 2014, SCI J EARTH SCI, V4, P85; Wollstein G, 2005, ARCH OPHTHALMOL-CHIC, V123, P464, DOI 10.1001/archopht.123.4.464; Wollstein G, 2012, BRIT J OPHTHALMOL, V96, P47, DOI 10.1136/bjo.2010.196907	21	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101052
C	Lomeli, M; Favaro, S; Teh, YW		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lomeli, Maria; Favaro, Stefano; Teh, Yee Whye			A hybrid sampler for Poisson-Kingman mixture models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				DENSITY-ESTIMATION; DIRICHLET; REPRESENTATION	This paper concerns the introduction of a new Markov Chain Monte Carlo scheme for posterior sampling in Bayesian nonparametric mixture models with priors that belong to the general Poisson-Kingman class. We present a novel compact way of representing the infinite dimensional component of the model such that while explicitly representing this infinite component it has less memory and storage requirements than previous MCMC schemes. We describe comparative simulation results demonstrating the efficacy of the proposed MCMC algorithm against existing marginal and conditional MCMC samplers.	[Lomeli, Maria] UCL, Gatsby Unit, London, England; [Favaro, Stefano] Univ Torino, Dept Econ & Stat, Turin, Italy; [Favaro, Stefano] Coll Carlo Alberto, Turin, Italy; [Teh, Yee Whye] Univ Oxford, Dept Stat, Oxford, England	University of London; University College London; University of Turin; Collegio Carlo Alberto; University of Oxford	Lomeli, M (corresponding author), UCL, Gatsby Unit, London, England.	mlomeli@gatsby.ucl.ac.uk; stefano.favaro@unito.it; y.w.teh@stats.ox.ac.uk			Gatsby Charitable Foundation; European Research Council [StG N-BNP 306406]; European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013) ERC grant [617071]	Gatsby Charitable Foundation; European Research Council(European Research Council (ERC)European Commission); European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013) ERC grant(European Research Council (ERC))	We thank Konstantina Palla for her insightful comments. Maria Lomeli is funded by the Gatsby Charitable Foundation, Stefano Favaro is supported by the European Research Council through StG N-BNP 306406 and Yee Whye Teh is supported by the European Research Council under the European Unions Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	De Blasi P, 2015, IEEE T PATTERN ANAL, V37, P212, DOI 10.1109/TPAMI.2013.217; Devroye L., 1986, NONUNIFORM RANDOM VA, P61; Devroye L, 2009, ACM T MODEL COMPUT S, V19, DOI 10.1145/1596519.1596523; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; ESCOBAR MD, 1994, J AM STAT ASSOC, V89, P268, DOI 10.2307/2291223; Favaro S, 2014, ELECTRON J STAT, V8, P1063, DOI 10.1214/14-EJS921; Favaro S, 2013, J COMPUT GRAPH STAT, V22, P830, DOI 10.1080/10618600.2012.681211; Favaro S, 2013, STAT SCI, V28, P335, DOI 10.1214/13-STS422; Ghahramani Z, 2015, NATURE, V521, P452, DOI 10.1038/nature14541; GNEDIN A., 2006, J MATH SCI, V138, P5674, DOI DOI 10.1007/S10958-006-0335-Z.698; Hofert M, 2011, COMPUT STAT DATA AN, V55, P57, DOI 10.1016/j.csda.2010.04.025; Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758; James L. F, 2002, ARXIVMATH0205093; KANTER M, 1975, ANN PROBAB, V3, P697, DOI 10.1214/aop/1176996309; KINGMAN JFC, 1967, PAC J MATH, V21, P59, DOI 10.2140/pjm.1967.21.59; KINGMAN JFC, 1978, J LOND MATH SOC, V18, P374, DOI 10.1112/jlms/s2-18.2.374; Lomeli M., 2015, J COMPUTATIONAL GRAP; Neal RM, 2003, ANN STAT, V31, P705, DOI 10.1214/aos/1056562461; Neal RM, 1998, 9815 U TOR DEP STAT; Papaspiliopoulos O, 2008, BIOMETRIKA, V95, P169, DOI 10.1093/biomet/asm086; PERMAN M, 1992, PROBAB THEORY REL, V92, P21, DOI 10.1007/BF01205234; Pitman J, 2003, INST MATH S, V40, P1; Pitman J, 1996, ADV APPL PROBAB, V28, P525, DOI 10.2307/1428070; Pitman J., 2006, LECT NOTES MATH; Regazzini E, 2003, ANN STAT, V31, P560; ROEDER K, 1990, J AM STAT ASSOC, V85, P617, DOI 10.2307/2289993; von Renesse MK, 2008, STOCH PROC APPL, V118, P2038, DOI 10.1016/j.spa.2007.11.008; Walker SG, 2007, COMMUN STAT-SIMUL C, V36, P45, DOI 10.1080/03610910601096262	28	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101059
C	Monfort, M; Lake, BM; Ziebart, BD; Lucey, P; Tenenbaum, JB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Monfort, Mathew; Lake, Brenden M.; Ziebart, Brian D.; Lucey, Patrick; Tenenbaum, Joshua B.			Softstar: Heuristic-Guided Probabilistic Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recent machine learning methods for sequential behavior prediction estimate the motives of behavior rather than the behavior itself. This higher-level abstraction improves generalization in different prediction settings, but computing predictions often becomes intractable in large decision spaces. We propose the Soft-star algorithm, a softened heuristic-guided search technique for the maximum entropy inverse optimal control model of sequential behavior. This approach supports probabilistic search with bounded approximation error at a significantly reduced computational cost when compared to sampling based methods. We present the algorithm, analyze approximation guarantees, and compare performance with simulation-based inference on two distinct complex decision tasks.	[Monfort, Mathew; Ziebart, Brian D.] Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA; [Lake, Brenden M.] NYU, Ctr Data Sci, New York, NY 10003 USA; [Lucey, Patrick] Disney Res Pittsburgh, Pittsburgh, PA 15232 USA; [Tenenbaum, Joshua B.] MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA	University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital; New York University; Massachusetts Institute of Technology (MIT)	Monfort, M (corresponding author), Univ Illinois, Dept Comp Sci, Chicago, IL 60607 USA.	mmonfo2@uic.edu; brenden@nyu.edu; bziebart@uic.edu; patrick.lucey@disneyresearch.com; jbt@mit.edu			National Science Foundation [1227495]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. #1227495, Purposeful Prediction: Co-robot Interaction via Understanding Intent and Goals.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Babes Monica, 2011, INT C MACH LEARN; Baker Chris L., 2007, C COGN SCI SOC; Baum LE, 1972, INEQUALITIES, V3, P1; Boularias A., 2011, INT C ART INT STAT, P182; Byravan Arunkumar, 2015, P INT JOINT C ART IN; Dechter R., 1985, J ACM; Dijkstra E. W., 1959, NUMERISCHE MATH; Green PJ, 1995, BIOMETRIKA, V82, P711, DOI 10.1093/biomet/82.4.711; HART PE, 1968, IEEE T SYST SCI CYB, VSSC4, P100, DOI 10.1109/TSSC.1968.300136; Huang D.-A., 2015, AAAI; Kalman R.E., 1964, J FLUID ENG-T ASME, V86, P51, DOI [10.1115/1.3653115, DOI 10.1115/1.3653115]; Lake B. M., 2013, NIPS; Monfort M., 2015, AAAI; Monfort Mathew, 2013, ICML WORKSH ROB LEAR; Neu G., 2007, P 23 C UNC ART INT, P295; Ng AY, 2000, P INT C MACH LEARN; Ramachandran D, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2586; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Vernaza P., 2012, ADV NEURAL INFORM PR, P575; Ziebart B. D., 2010, INT C MACH LEARN; Ziebart Brian D., 2008, ASS ADV ARTIFICAL IN	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102038
C	Mordatch, I; Lowrey, K; Andrew, G; Popovic, Z; Todorov, E		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Mordatch, Igor; Lowrey, Kendall; Andrew, Galen; Popovic, Zoran; Todorov, Emanuel			Interactive Control of Diverse Complex Characters with Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We present a method for training recurrent neural networks to act as near-optimal feedback controllers. It is able to generate stable and realistic behaviors for a range of dynamical systems and tasks -swimming, flying, biped and quadruped walking with different body morphologies. It does not require motion capture or task-specific features or state machines. The controller is a neural network, having a large number of feed-forward units that learn elaborate state-action mappings, and a small number of recurrent units that implement memory states beyond the physical system state. The action generated by the network is defined as velocity. Thus the network is not learning a control policy, but rather the dynamics under an implicit policy. Essential features of the method include interleaving supervised learning with trajectory optimization, injecting noise during training, training for unexpected changes in the task specification, and using the trajectory optimizer to obtain optimal feedback gains in addition to optimal actions. [GRAPHICS] .	[Mordatch, Igor; Lowrey, Kendall; Andrew, Galen; Popovic, Zoran; Todorov, Emanuel] Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Mordatch, I (corresponding author), Univ Washington, Dept Comp Sci, Seattle, WA 98195 USA.	mordatch@cs.washington.edu; lowrey@cs.washington.edu; galen@cs.washington.edu; zoran@cs.washington.edu; todorov@cs.washington.edu						Chen P, 2011, SIAM J NUMER ANAL, V49, P1417, DOI 10.1137/100799988; Geyer H, 2010, IEEE T NEUR SYS REH, V18, P263, DOI 10.1109/TNSRE.2010.2047592; Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9, DOI 10.1145/280814.280816; Hinton GE, 2012, IMPROVING NEURAL NET; Hoerzer G. M., 2012, CEREBRAL CORTEX; Huh D, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P42; Ijspeert A. J., 2008, CENTRAL PATTERN GENE; Ju E, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2516971.2516976; Levine Sergey, 2014, ICML 14; Mnih V., 2013, ARXIV PREPRINT ARXIV; Mordatch I, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185539; Mordatch Igor, 2014, ROBOT SCI SYST, V4; Rebula JR, 2007, IEEE INT CONF ROBOT, P1467, DOI 10.1109/ROBOT.2007.363191; Schulman J., 2015, CORR; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vukobratovic M., 2004, INT J HUM ROBOT, V1, P157, DOI DOI 10.1142/S0219843604000083; WAGER S., 2013, P 27 ANN C NEUR INF; Wang JM, 2010, ACM T GRAPHIC, V29, DOI 10.1145/1778765.1778810; Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101024
C	Morgenstern, J; Roughgarden, T		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Morgenstern, Jamie; Roughgarden, Tim			The Pseudo-Dimension of Near-Optimal Auctions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MECHANISM DESIGN	This paper develops a general approach, rooted in statistical learning theory, to learning an approximately revenue-maximizing auction from data. We introduce t-level auctions to interpolate between simple auctions, such as welfare maximization with reserve prices, and optimal auctions, thereby balancing the competing demands of expressivity and simplicity. We prove that such auctions have small representation error, in the sense that for every product distribution F over bidders' valuations, there exists a t-level auction with small t and expected revenue close to optimal. We show that the set of t-level auctions has modest pseudo-dimension (for polynomial t) and therefore leads to small learning error. One consequence of our results is that, in arbitrary single-parameter settings, one can learn a mechanism with expected revenue arbitrarily close to optimal from a polynomial number of samples.	[Morgenstern, Jamie] Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA; [Roughgarden, Tim] Stanford Univ, Palo Alto, CA 94304 USA	University of Pennsylvania; Stanford University	Morgenstern, J (corresponding author), Univ Penn, Comp & Informat Sci, Philadelphia, PA 19104 USA.	jamiemor@cis.upenn.edu; tim@cs.stanford.edu			Simons Award for Graduate Students in Theoretical Computer Science; NSF [CCF-1415460]	Simons Award for Graduate Students in Theoretical Computer Science; NSF(National Science Foundation (NSF))	Part of this work done while visiting Stanford University. Partially supported by a Simons Award for Graduate Students in Theoretical Computer Science, as well as NSF grant CCF-1415460.	[Anonymous], 1994, INTRO COMPUTATIONAL; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Babaioff Moshe, 2015, SIGECOM EXCH, V13, P31; Balcan MF, 2008, J COMPUT SYST SCI, V74, P1245, DOI 10.1016/j.jcss.2007.08.002; Balcan Maria-Florina, 2007, TECHNICAL REPORT; Cai Y, 2011, ANN IEEE SYMP FOUND, P522, DOI 10.1109/FOCS.2011.76; Chawla S, 2010, ACM S THEORY COMPUT, P311; Chawla S, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P243; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Devanur Nikhil, 2011, Internet and Network Economics. Proceedings 7th International Workshop, WINE 2011, P122, DOI 10.1007/978-3-642-25510-6_11; Dhangwatnotai P., 2010, P 11 ACM C EL COMM E, P129; Dughmi S, 2014, LECT NOTES COMPUT SC, V8877, P277, DOI 10.1007/978-3-319-13129-0_22; Elkind E, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P736; Hartline Jason, 2015, MECH DESIGN APPROXIM; Hartline JD, 2009, ACM SIGECOM EXCH, V8; Huang Zhiyi, 2014, ABS14072479; Mohri M, 2014, PR MACH LEARN RES, V32; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Pollard David, 1984, CONVERGENCE STOCHAST; Roughgarden T., 2015, IRONING DARK UNPUB; Roughgarden T., 2012, 13 ACM C ELECT COMME, P844; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; Yao A. C.-C., 2015, PROC ACM SIAM SODA 2, P92	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101026
C	Park, MJ; Bohner, G; Macke, JH		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Park, Mijung; Bohner, Gergo; Macke, Jakob H.			Unlocking neural population non-stationarity using a hierarchical dynamics model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				STATE; VARIABILITY	Neural population activity often exhibits rich variability. This variability can arise from single-neuron stochasticity, neural dynamics on short time-scales, as well as from modulations of neural firing properties on long time-scales, often referred to as neural non-stationarity. To better understand the nature of co-variability in neural circuits and their impact on cortical information processing, we introduce a hierarchical dynamics model that is able to capture both slow inter-trial modulations in firing rates as well as neural population dynamics. We derive a Bayesian Laplace propagation algorithm for joint inference of parameters and population states. On neural population recordings from primary visual cortex, we demonstrate that our model provides a better account of the structure of neural firing than stationary dynamics models.	[Park, Mijung; Bohner, Gergo] UCL, Gatsby Computat Neurosci Unit, London, England; [Macke, Jakob H.] Bonn Max Planck Inst Biol Cybernet, Bernstein Ctr Computat Neurosci Tubingen, Res Ctr Caesar, Max Planck Soc, Bonn, Germany	University of London; University College London; Center of Advanced European Studies & Research (CAESAR); Max Planck Society	Park, MJ (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.	mijung@gatsby.ucl.ac.uk; gbohner@gatsby.ucl.ac.uk; jakob.macke@caesar.de			Gatsby Charitable Foundation through BMBF; German Federal Ministry of Education and Research through BMBF;  [FKZ:01GQ1002]	Gatsby Charitable Foundation through BMBF; German Federal Ministry of Education and Research through BMBF; 	We thank Alexander Ecker and the lab of Andreas Tolias for sharing their data with us [5] (see http://toliaslab.org/publications/ecker-et-al-2014/), and for allowing us to use it in this publication, as well as Maneesh Sahani and Alexander Ecker for valuable comments. This work was funded by the Gatsby Charitable Foundation (MP and GB) and the German Federal Ministry of Education and Research (MP and JHM) through BMBF; FKZ: 01GQ1002 (Bernstein Center Tubingen). Code available at http://www.mackelab.org/code.	Beal M.J, 2003, THESIS; Brody CD, 1999, NEURAL COMPUT, V11, P1537, DOI 10.1162/089976699300016133; Brown EN, 2001, P NATL ACAD SCI USA, V98, P12261, DOI 10.1073/pnas.201409398; Czanner G, 2008, J NEUROPHYSIOL, V99, P2672, DOI 10.1152/jn.00343.2007; Destexhe A, 2011, CURR OPIN NEUROBIOL, V21, P717, DOI 10.1016/j.conb.2011.06.002; Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006; Ecker Alexander S, 2015, BIORXIV; Eden UT, 2004, NEURAL COMPUT, V16, P971, DOI 10.1162/089976604773135069; Frank LM, 2002, J NEUROSCI, V22, P3817; Gilbert CD, 2012, NEURON, V75, P250, DOI 10.1016/j.neuron.2012.06.030; Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711; Haefner Ralf M, 2014, ARXIV14090257; Harris KD, 2011, NAT REV NEUROSCI, V12, P509, DOI 10.1038/nrn3084; Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173; Lesica NA, 2005, IEEE T NEUR SYS REH, V13, P194, DOI 10.1109/TNSRE.2005.848339; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Maimon G, 2011, CURR OPIN NEUROBIOL, V21, P559, DOI 10.1016/j.conb.2011.05.001; Murray I., 2010, ADV NEURAL INF PROCE, P1723; Quiroga-Lombard CS, 2013, J NEUROPHYSIOL, V110, P562, DOI 10.1152/jn.00186.2013; Rabinowitz N. C., 2015, ARXIV150701497; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Renart A, 2014, CURR OPIN NEUROBIOL, V25, P211, DOI 10.1016/j.conb.2014.02.013; Sahani M., 2006, P IEEE NONL STAT SIG; Scholvinck ML, 2015, J NEUROSCI, V35, P170, DOI 10.1523/JNEUROSCI.4994-13.2015; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Smola AJ, 2004, ADV NEUR IN, V16, P441; TOMKO GJ, 1974, BRAIN RES, V79, P405, DOI 10.1016/0006-8993(74)90438-7; Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455; vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724; Ventura V., 2005, TRIAL TO TRIAL VARIA; Ypma A, 2005, NEUROCOMPUTING, V69, P85, DOI 10.1016/j.neucom.2005.02.020; Yu, 2009, GAUSSIAN PROCESS FAC, V102, P614; Yu BM, 2006, ADV NEURAL INF PROCE, V18, P1545; ZammitMangion A, 2011, NEURAL COMPUT, V23, P1967, DOI 10.1162/NECO_a_00156	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101050
C	Paul, S; Magdon-Ismail, M; Drineas, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Paul, Saurabh; Magdon-Ismail, Malik; Drineas, Petros			Column Selection via Adaptive Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHMS	Selecting a good column (or row) subset of massive data matrices has found many applications in data analysis and machine learning. We propose a new adaptive sampling algorithm that can be used to improve any relative-error column selection algorithm. Our algorithm delivers a tighter theoretical bound on the approximation error which we also demonstrate empirically using two well known relative-error column subset selection algorithms. Our experimental results on synthetic and real-world data show that our algorithm outperforms non-adaptive sampling as well as prior adaptive sampling approaches.	[Paul, Saurabh] Paypal Inc, Palo Alto, CA 94303 USA; [Magdon-Ismail, Malik; Drineas, Petros] Rensselaer Polytech Inst, CS Dept, Troy, NY 12181 USA	PayPal Holdings, Inc.; Rensselaer Polytechnic Institute	Paul, S (corresponding author), Paypal Inc, Palo Alto, CA 94303 USA.	saupaul@paypal.com; magdon@cs.rpi.edu; drinep@cs.rpi.edu			 [IIS-1447283];  [IIS-1319280]	; 	Most of the work was done when SP was a graduate student at RPI. PD was supported by IIS-1447283 and IIS-1319280.	Boutsidis C., 2013, IEEE T INFORM THEORY, V59; Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X; Boutsidis C, 2014, INFORM PROCESS LETT, V114, P273, DOI 10.1016/j.ipl.2013.11.011; Boutsidis C, 2011, ANN IEEE SYMP FOUND, P305, DOI 10.1109/FOCS.2011.21; Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968; Boutsidis Christos, 2011, P 25 ANN C NEUR INF; CHAN TF, 1987, LINEAR ALGEBRA APPL, V88-9, P67, DOI 10.1016/0024-3795(87)90103-0; CHAN TF, 1992, SIAM J SCI STAT COMP, V13, P727, DOI 10.1137/0913043; Davidov D., 2004, Proceedings of Sheffield SIGIR 2004. The Twenty-Seventh Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P250, DOI 10.1145/1008992.1009036; DESHPANDE A, 2006, THEORY COMPUT, V2, P225; Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681; Deshpande A, 2006, LECT NOTES COMPUT SC, V4110, P292; Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Drineas P, 2007, LINEAR ALGEBRA APPL, V420, P553, DOI 10.1016/j.laa.2006.08.023; Drineas P, 2006, LECT NOTES COMPUT SC, V4110, P316; Drineas Petros, 2002, P 34 ACM S THEORY CO, P82, DOI [10.1145/509907.509922, DOI 10.1145/509907.509922]; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Guruswami V., 2012, P 23 ANN ACM SIAM S, P1207, DOI [10.1137/1.9781611973099.95, 10.1137/1.9781611973099, DOI 10.1137/1.9781611973099]; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Liberty E, 2007, P NATL ACAD SCI USA, V104, P20167, DOI 10.1073/pnas.0709640104; Magdon-Ismail M., 2015, ARXIV150206626; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Maung Crystal, 2013, P ADV NEUR INF PROC, P1628; Papailiopoulos D, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P997, DOI 10.1145/2623330.2623698; Paschou P, 2010, J MED GENET, V47, P835, DOI 10.1136/jmg.2010.078212	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103049
C	Pehlevan, C; Chklovskii, DB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Pehlevan, Cengiz; Chklovskii, Dmitri B.			A Normative Theory of Adaptive Dimensionality Reduction in Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				INFORMATION; REPRESENTATIONS; MODEL; SET	To make sense of the world our brains must analyze high-dimensional datasets streamed by our sensory organs. Because such analysis begins with dimensionality reduction, modeling early sensory processing requires biologically plausible online dimensionality reduction algorithms. Recently, we derived such an algorithm, termed similarity matching, from a Multidimensional Scaling (MDS) objective function. However, in the existing algorithm, the number of output dimensions is set a priori by the number of output neurons and cannot be changed. Because the number of informative dimensions in sensory inputs is variable there is a need for adaptive dimensionality reduction. Here, we derive biologically plausible dimensionality reduction algorithms which adapt the number of output dimensions to the eigenspectrum of the input covariance matrix. We formulate three objective functions which, in the offline setting, are optimized by the projections of the input dataset onto its principal subspace scaled by the eigenvalues of the output covariance matrix. In turn, the output eigenvalues are computed as i) soft-thresholded, ii) hard-thresholded, iii) equalized thresholded eigenvalues of the input covariance matrix. In the online setting, we derive the three corresponding adaptive algorithms and map them onto the dynamics of neuronal activity in networks with biologically plausible local learning rules. Remarkably, in the last two networks, neurons are divided into two classes which we identify with principal neurons and interneurons in biological circuits.	[Pehlevan, Cengiz; Chklovskii, Dmitri B.] Simons Fdn, Simons Ctr Data Anal, New York, NY 10010 USA		Pehlevan, C (corresponding author), Simons Fdn, Simons Ctr Data Anal, New York, NY 10010 USA.	cpehlevan@simonsfoundation.org; dchklovskii@simonsfoundation.org						Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; BARROW HG, 1992, ARTIFICIAL NEURAL NETWORKS, 2, VOLS 1 AND 2, P433; Boyd S, 2004, CONVEX OPTIMIZATION; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Diamantaras K.I., 1996, PRINCIPAL COMPONENT; Doi E, 2012, J NEUROSCI, V32, P16256, DOI 10.1523/JNEUROSCI.4036-12.2012; Druckmann S., 2012, NIPS, P1979; Fairhall AL, 2001, NATURE, V412, P787, DOI 10.1038/35090500; Foldiak P., 1989, IJCNN: International Joint Conference on Neural Networks (Cat. No.89CH2765-6), P401, DOI 10.1109/IJCNN.1989.118615; Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003; Goes J, 2014, JMLR WORKSH CONF PRO, V33, P266; Hu T, 2013, CONF REC ASILOMAR C, P362, DOI 10.1109/ACSSC.2013.6810296; Hubel David H, 1995, EYE BRAIN VISION, P6; Kiani R, 2007, J NEUROPHYSIOL, V97, P4296, DOI 10.1152/jn.00024.2007; King PD, 2013, J NEUROSCI, V33, P5475, DOI 10.1523/JNEUROSCI.4188-12.2013; Koulakov AA, 2011, NEURON, V72, P124, DOI 10.1016/j.neuron.2011.07.031; Kriegeskorte N, 2008, NEURON, V60, P1126, DOI 10.1016/j.neuron.2008.10.043; Leen Todd K, 1990, NIPS, V3; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Mairal J, 2010, J MACH LEARN RES, V11, P19; OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Pehlevan C, 2015, ALL C COMM CONTR COM; Pehlevan C, 2015, NEURAL COMPUT, V27, P1461, DOI 10.1162/NECO_a_00745; Plumbley M. D., 1993, Third International Conference on Artificial Neural Networks (Conf. Publ. No.372), P86; Plumbley MD, 1996, NETWORK-COMP NEURAL, V7, P301, DOI 10.1088/0954-898X/7/2/010; Plumbley MD, 1994, TECH REP; RUBNER J, 1989, EUROPHYS LETT, V10, P693, DOI 10.1209/0295-5075/10/7/015; SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0; Seung HS, 1998, ADV NEUR IN, V10, P329; Tkacik G, 2013, PHYS REV LETT, V110, DOI 10.1103/PhysRevLett.110.058104; Vertechi P., 2014, ADV NEURAL INFORM PR, P3653; YANG B, 1995, IEEE T SIGNAL PROCES, V43, P95, DOI 10.1109/78.365290; Young G, 1938, PSYCHOMETRIKA, V3, P19, DOI 10.1007/BF02287916; Zhu MC, 2015, PLOS COMPUT BIOL, V11, DOI 10.1371/journal.pcbi.1004353	38	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102034
C	Pennington, J; Yu, FX; Kumar, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Pennington, Jeffrey; Yu, Felix X.; Kumar, Sanjiv			Spherical Random Features for Polynomial Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Compact explicit feature maps provide a practical framework to scale kernel methods to large-scale learning, but deriving such maps for many types of kernels remains a challenging open problem. Among the commonly used kernels for nonlinear classification are polynomial kernels, for which low approximation error has thus far necessitated explicit feature maps of large dimensionality, especially for higher-order polynomials. Meanwhile, because polynomial kernels are unbounded, they are frequently applied to data that has been normalized to unit l(2) norm. The question we address in this work is: if we know a priori that data is normalized, can we devise a more compact map? We show that a putative affirmative answer to this question based on Random Fourier Features is impossible in this setting, and introduce a new approximation paradigm, Spherical Random Fourier (SRF) features, which circumvents these issues and delivers a compact approximation to polynomial kernels for data on the unit sphere. Compared to prior work, SRF features are less rank-deficient, more compact, and achieve better kernel approximation, especially for higher-order polynomials. The resulting predictions have lower variance and typically yield better classification accuracy.	[Pennington, Jeffrey; Yu, Felix X.; Kumar, Sanjiv] Google Res, Mountain View, CA 94043 USA	Google Incorporated	Pennington, J (corresponding author), Google Res, Mountain View, CA 94043 USA.	jpennin@google.com; felixyu@google.com; sanjivk@google.com						Bochner S, 1955, HARMONIC ANAL THEORY; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Dai B., 2014, NIPS; Hamid R, 2014, PR MACH LEARN RES, V32, P19; Isozaki H., 2002, PROC 19 INT C COMPUT, P1, DOI [DOI 10.3115/1071884.1071911, DOI 10.3115/1072228.1072282]; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Kar P., 2012, ARTIF INTELL, P583; Kim KI, 2002, IEEE SIGNAL PROC LET, V9, P40, DOI 10.1109/97.991133; Kummer E.E., 2013, J REINE ANGEW MATH, V1837, P228; Li FX, 2010, LECT NOTES COMPUT SC, V6376, P262; Li P, 2006, LECT NOTES ARTIF INT, V4005, P635, DOI 10.1007/11776420_46; Maji S, 2009, IEEE I CONF COMP VIS, P40, DOI 10.1109/ICCV.2009.5459203; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Rahimi A., 2007, ADV NEURAL INFORM PR, P3; Schoenberg IJ, 1938, ANN MATH, V39, P811, DOI 10.2307/1968466; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Sreekanth V, 2010, BRIT MACH VIS C; Storcheus Dmitry, 2015, ARXIV150908880; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Yang JY, 2014, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2014.129; Yu F. X., 2015, ARXIV150303893	22	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102091
C	Pentina, A; Lampert, CH		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Pentina, Anastasia; Lampert, Christoph H.			Lifelong Learning with Non-i.i.d. Tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BOUNDS; INEQUALITIES	In this work we aim at extending the theoretical foundations of lifelong learning. Previous work analyzing this scenario is based on the assumption that learning tasks are sampled i.i.d. from a task environment or limited to strongly constrained data distributions. Instead, we study two scenarios when lifelong learning is possible, even though the observed tasks do not form an i.i.d. sample: first, when they are sampled from the same environment, but possibly with dependencies, and second, when the task environment is allowed to change over time in a consistent way. In the first case we prove a PAC-Bayesian theorem that can be seen as a direct generalization of the analogous previous result for the i.i.d. case. For the second scenario we propose to learn an inductive bias in form of a transfer procedure. We present a generalization bound and show on a toy example how it can be used to identify a beneficial transfer algorithm.	[Pentina, Anastasia; Lampert, Christoph H.] IST Austria, Klosterneuburg, Austria	Institute of Science & Technology - Austria	Pentina, A (corresponding author), IST Austria, Klosterneuburg, Austria.	apentina@ist.ac.at; chl@ist.ac.at			European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant [308036]	European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant(European Research Council (ERC))	This work was in parts funded by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no 308036.	Balcan Maria-Florina, 2015, WORKSH COMP LEARN TH; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Blanchard Gilles, 2011, NEURIPS; DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1, DOI 10.1002/cpa.3160280102; Germain Pascal, 2009, INT C MACH LEAR ICML; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Langford John, 2002, C NEUR INF PROC SYST; Laviolette F, 2007, J MACH LEARN RES, V8, P1461; Maurer A, 2005, J MACH LEARN RES, V6, P967; Maurer A, 2009, MACH LEARN, V75, P327, DOI 10.1007/s10994-009-5109-7; Maurer Andreas, 2013, INT C MACH LEAR ICML; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; McAllester David A., 2003, WORKSH COMP LEARN TH; Pentina Anastasia, 2014, INT C MACH LEAR ICML; Ralaivola Liva, 2010, J MACHINE LEARNING R; Seeger M, 2003, J MACH LEARN RES, V3, P233, DOI 10.1162/153244303765208386; Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334; Thrun S.B., 1993, TECHNICAL REPORT; Ullman Daniel, 1997, WILEY INTERSCIENCE S; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; Vapnik V., 1982, ESTIMATION DEPENDENC	21	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103045
C	Procaccia, AD; Shah, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Procaccia, Ariel D.; Shah, Nisarg			Is Approval Voting Optimal Given Approval Votes?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Some crowdsourcing platforms ask workers to express their opinions by approving a set of k good alternatives. It seems that the only reasonable way to aggregate these k-approval votes is the approval voting rule, which simply counts the number of times each alternative was approved. We challenge this assertion by proposing a probabilistic framework of noisy voting, and asking whether approval voting yields an alternative that is most likely to be the best alternative, given k-approval votes. While the answer is generally positive, our theoretical and empirical results call attention to situations where approval voting is suboptimal.	[Procaccia, Ariel D.; Shah, Nisarg] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Procaccia, AD (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	arielpro@cs.cmu.edu; nkshah@cs.cmu.edu						Alos-Ferrer C, 2006, SOC CHOICE WELFARE, V27, P621, DOI 10.1007/s00355-006-0145-8; [Anonymous], 2013, P 27 AAAI C ART INT; Azari H, 2012, NIPS 12, P126; BARTHOLDI J, 1989, SOC CHOICE WELFARE, V6, P157, DOI 10.1007/BF00303169; Baumeister D, 2010, STUD CHOICE WELF, P199, DOI 10.1007/978-3-642-02839-7_10; Brams SJ., 2007, APPROVAL VOTING; Brams Steven J., 2007, MATH DEMOCRACY DESIG, Vfirst; Caragiannis I, 2013, EC 13, P143, DOI DOI 10.1145/2482540.2482570; Caragiannis I, 2014, AAAI CONF ARTIF INTE, P616; Elkind E, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P182; Erdelyi G, 2009, MATH LOGIC QUART, V55, P425, DOI 10.1002/malq.200810020; FISHBURN PC, 1981, PUBLIC CHOICE, V36, P89; FISHBURN PC, 1978, J ECON THEORY, V19, P180, DOI 10.1016/0022-0531(78)90062-5; Goel A., 2015, P COLL INT; Lee J, 2014, P NATL ACAD SCI USA, V111, P2122, DOI 10.1073/pnas.1313039111; Little Greg, 2010, P 23 ANN ACM S US IN, P57, DOI DOI 10.1145/1866029.1866040; Lu T., 2011, ICML, P145; MALLOWS CL, 1957, BIOMETRIKA, V44, P114, DOI 10.2307/2333244; Procaccia AD, 2012, UAI 12, P695; SERTEL MR, 1988, J ECON THEORY, V45, P207, DOI 10.1016/0022-0531(88)90262-1; Shah NB, 2015, PR MACH LEARN RES, V37, P10; Soufiani HA, 2014, PR MACH LEARN RES, V32; Soufiani Hossein Azari, 2013, P 26 INT C NEUR INF, P2706; YOUNG HP, 1988, AM POLIT SCI REV, V82, P1231, DOI 10.2307/1961757	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102033
C	Qu, C; Xu, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Qu, Chao; Xu, Huan			Subspace Clustering with Irrelevant Features via Robust Dantzig Selector	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MOTION SEGMENTATION	This paper considers the subspace clustering problem where the data contains irrelevant or corrupted features. We propose a method termed "robust Dantzig selector" which can successfully identify the clustering structure even with the presence of irrelevant features. The idea is simple yet powerful: we replace the inner product by its robust counterpart, which is insensitive to the irrelevant features given an upper bound of the number of irrelevant features. We establish theoretical guarantees for the algorithm to identify the correct subspace, and demonstrate the effectiveness of the algorithm via numerical simulations. To the best of our knowledge, this is the first method developed to tackle subspace clustering with irrelevant features.	[Qu, Chao; Xu, Huan] Natl Univ Singapore, Dept Mech Engn, Singapore, Singapore	National University of Singapore	Qu, C (corresponding author), Natl Univ Singapore, Dept Mech Engn, Singapore, Singapore.	A0117143@u.nus.edu; mpexuh@nus.edu.sg			Ministry of Education of Singapore AcRF Tier Two grant [R-265-000-443-112]; A*STAR SERC PSF grant [R-265-000-540-305]	Ministry of Education of Singapore AcRF Tier Two grant(Ministry of Education, Singapore); A*STAR SERC PSF grant(Agency for Science Technology & Research (A*STAR))	This work is partially supported by the Ministry of Education of Singapore AcRF Tier Two grant R-265-000-443-112, and A*STAR SERC PSF grant R-265-000-540-305.	Agarwal P.K., 2004, P 23 ACM SIGMOD SIGA, P155; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Lu L., 2006, P 23 INT C MACHINE L, P593; Ma Y, 2007, IEEE T PATTERN ANAL, V29, P1546, DOI 10.1109/TP'AMI.2007.1085; Rao Shankar R, 2008, CVPR; Soltanolkotabi M, 2014, ANN STAT, V42, P669, DOI 10.1214/13-AOS1199; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; [王应德 Wang Yingde], 2013, [高分子通报, Polymer Bulletin], P89; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Yan JY, 2006, LECT NOTES COMPUT SC, V3954, P94; Zhu H, 2011, IEEE T SIGNAL PROCES, V59, P2002, DOI 10.1109/TSP.2011.2109956	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100075
C	Recasens, A; Khosla, A; Vondrick, C; Torralba, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Recasens, Adria; Khosla, Aditya; Vondrick, Carl; Torralba, Antonio			Where are they looking?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Humans have the remarkable ability to follow the gaze of other people to identify what they are looking at. Following eye gaze, or gaze-following, is an important ability that allows us to understand what other people are thinking, the actions they are performing, and even predict what they might do next. Despite the importance of this topic, this problem has only been studied in limited scenarios within the computer vision community. In this paper, we propose a deep neural network-based approach for gaze-following and a new benchmark dataset, GazeFollow, for thorough evaluation. Given an image and the location of a head, our approach follows the gaze of the person and identifies the object being looked at. Our deep network is able to discover how to extract head pose and gaze orientation, and to select objects in the scene that are in the predicted line of sight and likely to be looked at (such as televisions, balls and food). The quantitative evaluation shows that our approach produces reliable results, even when viewing only the back of the head. While our method outperforms several baseline approaches, we are still far from reaching human performance on this task. Overall, we believe that gaze-following is a challenging and important problem that deserves more attention from the community.	[Recasens, Adria; Khosla, Aditya; Vondrick, Carl; Torralba, Antonio] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Recasens, A (corresponding author), MIT, Cambridge, MA 02139 USA.	recasens@csail.mit.edu; khosla@csail.mit.edu; vondrick@csail.mit.edu; torralba@csail.mit.edu			Obra Social "la Caixa" Fellowship for Post-Graduate Studies; Google PhD Fellowship	Obra Social "la Caixa" Fellowship for Post-Graduate Studies; Google PhD Fellowship(Google Incorporated)	We thank Andrew Owens for helpful discussions. Funding for this research was partially supported by the Obra Social "la Caixa" Fellowship for Post-Graduate Studies to AR and a Google PhD Fellowship to CV.	[Anonymous], 2012, CVPR; BORJI A, 2014, J VIS, V14, P1, DOI DOI 10.1167/14.13.3; Borji A., 2012, ECCV; Emery N., 2000, NEUROSCIENCE BIOBEHA; Everingham M., 2010, IJCV; Fathi A., 2012, ECCV; Fathi Alircza, 2012, CVPR; Hoffman M. W., 2006, NEURAL NETWORKS; Itti L., 2001, NATURE REV NEUROSCIE; Jasso H., 2006, ICDL; Jia Y., 2013, CAFFE OPEN SOURCE CO; Judd T., 2009, CVPR; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Marin M. J., 2014, IJCV; Park H. S., 2013, ICCV; Park Hyun Soo, 2015, CVPR; Parks D., 2014, VISION RES; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Xiao J., 2010, CVPR; Yao B., 2011, ICCV; Zhou B., 2015, ICLR; Zhou B., 2014, NIPS	23	1	1	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101107
C	Ruozzi, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ruozzi, Nicholas			Exactness of Approximate MAP Inference in Continuous MRFs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BELIEF PROPAGATION; CONVERGENCE	Computing the MAP assignment in graphical models is generally intractable. As a result, for discrete graphical models, the MAP problem is often approximated using linear programming relaxations. Much research has focused on characterizing when these LP relaxations are tight, and while they are relatively well-understood in the discrete case, only a few results are known for their continuous analog. In this work, we use graph covers to provide necessary and sufficient conditions for continuous MAP relaxations to be tight. We use this characterization to give simple proofs that the relaxation is tight for log-concave decomposable and log-supermodular decomposable models. We conclude by exploring the relationship between these two seemingly distinct classes of functions and providing specific conditions under which the MAP relaxation can and cannot be tight.	[Ruozzi, Nicholas] Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA	University of Texas System; University of Texas Dallas	Ruozzi, N (corresponding author), Univ Texas Dallas, Dept Comp Sci, Richardson, TX 75080 USA.							Bayati M, 2011, SIAM J DISCRETE MATH, V25, P989, DOI 10.1137/090753115; Globerson A., 2007, P 21 NEUR INF PROC S; Iwata S., 1999, J ACM; Kolmogorov V, 2002, LECT NOTES COMPUT SC, V2352, P65; Malioutov D.M., 2008, THESIS; Malioutov DM, 2006, J MACH LEARN RES, V7, P2031; Minka T.P., 2001, P 17 C UNC ART INT, P362; Moallemi CC, 2010, IEEE T INFORM THEORY, V56, P2041, DOI 10.1109/TIT.2010.2040863; Moallemi CC, 2009, IEEE T INFORM THEORY, V55, P2413, DOI 10.1109/TIT.2009.2016055; Ruozzi N, 2013, J MACH LEARN RES, V14, P2287; Ruozzi N, 2013, IEEE T INFORM THEORY, V59, P5860, DOI 10.1109/TIT.2013.2259576; Sanghavi S, 2011, IEEE T INFORM THEORY, V57, P2203, DOI 10.1109/TIT.2011.2110170; Sanghavi S, 2009, IEEE T INFORM THEORY, V55, P4822, DOI 10.1109/TIT.2009.2030448; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; Vontobel P. O., 2005, ABSCS0512078 CORR; VONTOBEL PO, 2013, INFORM THEORY IEEE T; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wainwright MJ, 2005, IEEE T INFORM THEORY, V51, P3697, DOI 10.1109/TIT.2005.856938; Wald Y., 2014, P 30 UNC ART INT UAI; Weiss Y, 2001, NEURAL COMPUT, V13, P2173, DOI 10.1162/089976601750541769; Weiss Y., 2008, P UAI, P503; Werner T, 2007, IEEE T PATTERN ANAL, V29, P1165, DOI 10.1109/TPAMI.2007.1036	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102080
C	Sadeghi, F; Zitnick, CL; Farhadi, A		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sadeghi, Fereshteh; Zitnick, C. Lawrence; Farhadi, Ali			VISALOGY: Answering Visual Analogy Questions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we study the problem of answering visual analogy questions. These questions take the form of image A is to image B as image C is to what. Answering these questions entails discovering the mapping from image A to image B and then extending the mapping to image C and searching for the image D such that the relation from A to B holds for C to D. We pose this problem as learning an embedding that encourages pairs of analogous images with similar transformations to be close together using convolutional neural networks with a quadruple Siamese architecture. We introduce a dataset of visual analogy questions in natural images, and show first results of its kind on solving analogy questions on natural images.	[Sadeghi, Fereshteh] Univ Washington, Seattle, WA 98195 USA; [Zitnick, C. Lawrence] Microsoft Res, New York, NY USA; [Farhadi, Ali] Univ Washington, Allen Inst AI, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; Microsoft; University of Washington; University of Washington Seattle	Sadeghi, F (corresponding author), Univ Washington, Seattle, WA 98195 USA.	fsadeghi@cs.washington.edu; larryz@microsoft.com; ali@cs.washington.edu			ONR [N00014-13-1-0720]; NSF [IIS-IIS- 1338054, IIS-1218683]; Allen Distinguished Investigator Award	ONR(Office of Naval Research); NSF(National Science Foundation (NSF)); Allen Distinguished Investigator Award	This work was in part supported by ONR N00014-13-1-0720, NSF IIS-1218683, NSF IIS-IIS- 1338054, and Allen Distinguished Investigator Award.	Antol S., 2015, ICCV; Aubry M., 2014, CVPR; Barbella D. M, 2011, AAAI; Baroni M, 2010, COMPUT LINGUIST; Chang M. D, 2014, SPATIAL COGNITION 9; Chopra S., 2005, CVPR; Farhadi A., 2009, CVPR; Forbus K., 2011, TOPICS COGNITIVE SCI; Forbus K. D, 2014, AI MAGAZINE; Forbus K. D, 2005, AAAI; Geman D, 2015, P NATL ACAD SCI, V112; Gentner D, 2001, ANALOGICAL MIND PERS; HERTZMANN A., 2001, SIGGRAPH; Hwang S. J, 2013, ICML; Jia Y., P ACM MULT, P675; Jurgens D. A., 2012, SEMEVAL 2012 TASK 2; Juthe A, 2005, ARGUMENT BY ANALOGY; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Levy Omer, 2014, CONLL; Malinowski M., 2015, P INT C COMP VIS; Malinowski M., 2014, NIPS; Mikolov T, 2013, P 2013 C N AM CHAPTE; Parikh D., 2011, ICCV; Richard S. Zemel, 2014, Arxiv, DOI arXiv:1411.2539; Sadeghi F., 2015, CVPR; Shelley C., 2003, MULTIPLE ANALOGIES S; Tenenbaum J. B., 2000, NEURAL COMPUTATION; Turney P. D, 2010, J ARTIF INT RES; Turney P. D., 2005, CORR; Turney P. D, 2006, COMPUT LINGUIST; Yu L, 2015, IEEE INT C COMP VIS	31	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101037
C	Scaman, K; Lemonnier, R; Vayatis, N		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Scaman, Kevin; Lemonnier, Remi; Vayatis, Nicolas			Anytime Influence Bounds and the Explosive Behavior of Continuous-Time Diffusion Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The paper studies transition phenomena in information cascades observed along a diffusion process over some graph. We introduce the Laplace Hazard matrix and show that its spectral radius fully characterizes the dynamics of the contagion both in terms of influence and of explosion time. Using this concept, we prove tight non-asymptotic bounds for the influence of a set of nodes, and we also provide an in-depth analysis of the critical time after which the contagion becomes super-critical. Our contributions include formal definitions and tight lower bounds of critical explosion time. We illustrate the relevance of our theoretical results through several examples of information cascades used in epidemiology and viral marketing models. Finally, we provide a series of numerical experiments for various types of networks which confirm the tightness of the theoretical bounds.	[Scaman, Kevin; Lemonnier, Remi; Vayatis, Nicolas] Univ Paris Saclay, CNRS, ENS Cachan, CMLA, Paris, France; [Lemonnier, Remi] 1000Mercis, Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Scaman, K (corresponding author), Univ Paris Saclay, CNRS, ENS Cachan, CMLA, Paris, France.	scaman@cmla.ens-cachan.fr; lemonnier@cmla.ens-cachan.fr; vayatis@cmla.ens-cachan.fr			French Government within the program of "Investments for the Future - Big Data"	French Government within the program of "Investments for the Future - Big Data"	This research is part of the SODATECH project funded by the French Government within the program of "Investments for the Future - Big Data".	[Anonymous], 2003, OXFORD STUDIES PROBA; Bollobas B, 2007, RANDOM STRUCT ALGOR, V31, P3, DOI 10.1002/rsa.20168; Chen W., 2010, KDD, P1029, DOI [10.1145/1835804.1835934, DOI 10.1145/1835804.1835934]; Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047; Draief M, 2008, ANN APPL PROBAB, V18, P359, DOI 10.1214/07-AAP470; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; Du Nan, 2013, P 16 INT C ART INT S, P229; Farajtabar Mehrdad, 2014, Adv Neural Inf Process Syst, V27; Gomez-Rodriguez M, 2015, J MACHINE LEARNING R; Gomez-Rodriguez M., 2012, ICML; Gomez-Rodriguez Manuel, 2011, ICML, P561; HAWKES AG, 1974, J APPL PROBAB, V11, P493, DOI 10.2307/3212693; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Kermack WO, 1932, P R SOC LOND A-CONTA, V138, P55, DOI 10.1098/rspa.1932.0171; Lemonnier Remi, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P161, DOI 10.1007/978-3-662-44851-9_11; Lemonnier R, 2014, ADV NEUR IN, V27; Leskovec J, 2007, KDD-2007 PROCEEDINGS OF THE THIRTEENTH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P420; Newman M. E. J., 2010, NETWORKS INTRO OXFOR; Pouget-Abadie J, 2015, PR MACH LEARN RES, V37, P977; Trusov M, 2009, J MARKETING, V73, P90, DOI 10.1509/jmkg.73.5.90; Zhou K, 2013, ICML	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100069
C	Schulam, P; Saria, S		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Schulam, Peter; Saria, Suchi			A Framework for Individualizing Predictions of Disease Trajectories by Exploiting Multi-Resolution Structure	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MODELS	For many complex diseases, there is a wide variety of ways in which an individual can manifest the disease. The challenge of personalized medicine is to develop tools that can accurately predict the trajectory of an individual's disease, which can in turn enable clinicians to optimize treatments. We represent an individual's disease trajectory as a continuous-valued continuous-time function describing the severity of the disease over time. We propose a hierarchical latent variable model that individualizes predictions of disease trajectories. This model shares statistical strength across observations at different resolutions-the population, subpopulation and the individual level. We describe an algorithm for learning population and subpopulation parameters offline, and an online procedure for dynamically learning individual-specific parameters. Finally, we validate our model on the task of predicting the course of interstitial lung disease, a leading cause of death among patients with the autoimmune disease scleroderma. We compare our approach against state-of-the-art and demonstrate significant improvements in predictive accuracy.	[Schulam, Peter; Saria, Suchi] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Johns Hopkins University	Schulam, P (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.	pschulam@jhu.edu; ssaria@cs.jhu.edu						Adomavicius G, 2005, IEEE T KNOWL DATA EN, V17, P734, DOI 10.1109/TKDE.2005.99; Allanore Y, 2015, NAT REV DIS PRIMERS, V1, DOI 10.1038/nrdp.2015.2; Castaldi P. J., 2014, THORAX; CRAIG J, 2008, NAT ED, V1, P184; Gelman A., 2006, DATA ANAL USING REGR, DOI DOI 10.1017/CBO9780511790942.005; Khanna D, 2011, ARTHRITIS RHEUM-US, V63, P3078, DOI 10.1002/art.30467; Lee DS, 2003, JAMA-J AM MED ASSOC, V290, P2581, DOI 10.1001/jama.290.19.2581; Lotvall J, 2011, J ALLERGY CLIN IMMUN, V127, P355, DOI 10.1016/j.jaci.2010.11.037; Marlin B, 2003, ADV NEURAL INFORM PR; Proust-Lima C, 2014, STAT METHODS MED RES, V23, P74, DOI 10.1177/0962280212445839; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rizopoulos D, 2011, BIOMETRICS, V67, P819, DOI 10.1111/j.1541-0420.2010.01546.x; Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550; Ross J., 2013, P 30 INT C MACH LEAR, V28, P1346; Saria S., 2015, IEEE INTELLIGENT SYS, V30; Schulam P, 2015, AAAI CONF ARTIF INTE, P2956; Shi JQ, 2012, STAT MED, V31, P3165, DOI 10.1002/sim.4502; Shi JQ, 2005, STAT COMPUT, V15, P31, DOI 10.1007/s11222-005-4787-7; Sontag David, 2012, P 5 ACM INT C WEB SE, P433, DOI DOI 10.1145/2124295.2124348; Varga J, 2012, SCLERODERMA: FROM PATHOGENESIS TO COMPREHENSIVE MANAGEMENT, P1, DOI 10.1007/978-1-4419-5774-0; Wang H., 2012, ADV NEURAL INFORM PR, P1277, DOI 10.5555/2999134; Wiggins LD, 2012, J AUTISM DEV DISORD, V42, P191, DOI 10.1007/s10803-011-1230-0	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102022
C	Seguy, V; Cuturi, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Seguy, Vivien; Cuturi, Marco			Principal Geodesic Analysis for Probability Measures under the Optimal Transport Metric	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				PROJECTIONS; BARYCENTERS	Given a family of probability measures in P (X), the space of probability measures on a Hilbert space X, our goal in this paper is to highlight one ore more curves in P (X) that summarize efficiently that family. We propose to study this problem under the optimal transport (Wasserstein) geometry, using curves that are restricted to be geodesic segments under that metric. We show that concepts that play a key role in Euclidean PCA, such as data centering or orthogonality of principal directions, find a natural equivalent in the optimal transport geometry, using Wasserstein means and differential geometry. The implementation of these ideas is, however, computationally challenging. To achieve scalable algorithms that can handle thousands of measures, we propose to use a relaxed definition for geodesics and regularized optimal transport distances. The interest of our approach is demonstrated on images seen either as shapes or color histograms.	[Seguy, Vivien; Cuturi, Marco] Kyoto Univ, Grad Sch Informat, Kyoto, Japan	Kyoto University	Seguy, V (corresponding author), Kyoto Univ, Grad Sch Informat, Kyoto, Japan.	vivien.seguy@iip.ist.i.kyoto-u.ac.jp; mcuturi@i.kyoto-u.ac.jp			JSPS [26700002]	JSPS(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of Science)	MC acknowledges the support of JSPS young researcher A grant 26700002.	Agueh M, 2011, SIAM J MATH ANAL, V43, P904, DOI 10.1137/100805741; Ambrosio L., 2006, LECT MATH; Benamou JD, 2015, SIAM J SCI COMPUT, V37, pA1111, DOI 10.1137/141000439; BIGOT J., 2015, ANN I H POINCARE B; Boissard E, 2015, BERNOULLI, V21, P740, DOI 10.3150/13-BEJ585; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Carlier Guillaume, 2015, ESAIM MATH MODELLING; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; FLETCHER PT, 2004, SYSTEMS MAN CYBERN A, V23, P995, DOI DOI 10.1109/TMI.2004.831793; Frechet M., 1948, ANN I H POINCARE, V10, P215; Gramfort A., 2015, INFORM PROCESSING ME; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hunter DR, 2000, J COMPUT GRAPH STAT, V9, P60; McCann RJ, 1997, ADV MATH, V128, P153, DOI 10.1006/aima.1997.1634; Pitie F, 2007, COMPUT VIS IMAGE UND, V107, P123, DOI 10.1016/j.cviu.2006.11.011; Reich S, 2013, SIAM J SCI COMPUT, V35, pA2013, DOI 10.1137/130907367; Scholkopf B., 1997, Artificial Neural Networks - ICANN '97. 7th International Conference Proceedings, P583, DOI 10.1007/BFb0020217; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Srivastava S, 2015, JMLR WORKSH CONF PRO, V38, P912; Verbeek JJ, 2002, PATTERN RECOGN LETT, V23, P1009, DOI 10.1016/S0167-8655(02)00032-6; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang W, 2013, INT J COMPUT VISION, V101, P254, DOI 10.1007/s11263-012-0566-z; Westdickenberg M, 2010, J HYPERBOL DIFFER EQ, V7, P605, DOI 10.1142/S0219891610002244	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100048
C	Shah, NB; Zhou, DY		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shah, Nihar B.; Zhou, Dengyong			Double or Nothing: Multiplicative Incentive Mechanisms for Crowdsourcing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural "no-free-lunch" requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers. Interestingly, this unique mechanism takes a "multiplicative" form. The simplicity of the mechanism is an added benefit. In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.	[Shah, Nihar B.] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Zhou, Dengyong] Microsoft Res, Redmond, WA USA	University of California System; University of California Berkeley; Microsoft	Shah, NB (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	nihar@eecs.berkeley.edu; dengyong.zhou@microsoft.com	Camps-Valls, Gustavo/A-2532-2011; , Gustavo/ABC-1706-2022	Camps-Valls, Gustavo/0000-0003-1683-2138; 				Bohannon J, 2011, SCIENCE, V334, P307, DOI 10.1126/science.334.6054.307; Carlson Andrew, 2010, P 3 ACM INT C WEB SE, P101, DOI [DOI 10.1145/1718487.1718501, 10.1145/ 1718487.1718501]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eickhoff C., 2011, P ACM SIGIR WORKSH C, P21; Gneiting T, 2007, J AM STAT ASSOC, V102, P359, DOI 10.1198/016214506000001437; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Ipeirotis PG, 2014, DATA MIN KNOWL DISC, V28, P402, DOI 10.1007/s10618-013-0306-1; Jagabathula S., 2014, ADV NEURAL INFORM PR, P2492; Karger D. R., 2011, ADV NEURAL INFORM PR, P1953; Kazai G, 2011, PROCEEDINGS OF THE 34TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL (SIGIR'11), P205; Liu Qiang, 2012, ADV NEURAL INFORM PR, V25, P692; Raykar VC, 2010, J MACH LEARN RES, V11, P1297; Shah Nihar B, 2014, ARXIV14081387; Shah Nihar B, 2015, INT C MACH LEARN ICM; Wais P., 2010, NIPS WORKSH COMP SOC; Zhou D., 2015, ARXIV150307240	16	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100045
C	Shang, XC; Zhu, ZX; Leimkuhler, B; Storkey, AJ		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shang, Xiaocheng; Zhu, Zhanxing; Leimkuhler, Benedict; Storkey, Amos J.			Covariance-Controlled Adaptive Langevin Thermostat for Large-Scale Bayesian Sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.	[Shang, Xiaocheng; Zhu, Zhanxing; Leimkuhler, Benedict; Storkey, Amos J.] Univ Edinburgh, Edinburgh, Midlothian, Scotland	University of Edinburgh	Shang, XC (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.	x.shang@ed.ac.uk; zhanxing.zhu@ed.ac.uk; b.leimkuhler@ed.ac.uk; a.storkey@ed.ac.uk	Shang, Xiaocheng/AAY-1527-2021; Zhu, Zhanxing/GQA-7335-2022	Shang, Xiaocheng/0000-0001-8376-6498; 				Abdulle A, 2015, SIAM J NUMER ANAL, V53, P1, DOI 10.1137/140962644; Ahn S., 2012, P 29 INT C MACH LEAR; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Ding N., 2014, ADV NEURAL INFORM PR, V2, P3203; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Frenkel D., 2001, UNDERSTANDING MOL SI; Hoover W. G., 1991, COMPUTATIONAL STAT M; HOROWITZ AM, 1991, PHYS LETT B, V268, P247, DOI 10.1016/0370-2693(91)90812-5; Jones A, 2011, J CHEM PHYS, V135, DOI 10.1063/1.3626941; Leimkuhler B., 2015, ARXIV150506889; Leimkuhler B., 2015, IMA J NUMERICAL ANAL; Leimkuhler B., 2015, INTERDISCIP APPL MAT; Leimkuhler B, 2013, APPL MATH RES EXPRES, P34, DOI 10.1093/amrx/abs010; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; NOSE S, 1984, J CHEM PHYS, V81, P511, DOI 10.1063/1.447334; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Robert C, 2004, MONTE CARLO STAT MET, DOI DOI 10.1007/978-1-4757-4145-2; Vollmer S. J., 2015, ARXIV150100438; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103016
C	Shivanna, R; Chatterjee, B; Sankaran, R; Bhattacharyya, C; Bach, F		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shivanna, Rakesh; Chatterjee, Bibaswan; Sankaran, Raman; Bhattacharyya, Chiranjib; Bach, Francis			Spectral Norm Regularization of Orthonormal Representations for Graph Transduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recent literature [1] suggests that embedding a graph on an unit sphere leads to better generalization for graph transduction. However, the choice of optimal embedding and an efficient algorithm to compute the same remains open. In this paper, we show that orthonormal representations, a class of unit-sphere graph embeddings are PAC learnable. Existing PAC-based analysis do not apply as the VC dimension of the function class is infinite. We propose an alternative PAC-based bound, which do not depend on the VC dimension of the underlying function class, but is related to the famous Lovasz v function. The main contribution of the paper is SPORE, a SPectral regularized ORthonormal Embedding for graph transduction, derived from the PAC bound. SPORE is posed as a non-smooth convex function over an elliptope. These problems are usually solved as semi-definite programs (SDPs) with time complexity O(n(6)). We present, Infeasible Inexact proximal (IIP): an Inexact proximal method which performs subgradient procedure on an approximate projection, not necessarily feasible. IIP is more scalable than SDP, has an O(1/root T) convergence, and is generally applicable whenever a suitable approximate projection is available. We use IIP to compute SPORE where the approximate projection step is computed by FISTA, an accelerated gradient descent procedure. We show that the method has a convergence rate of O(1/root T). The proposed algorithm easily scales to 1000' s of vertices, while the standard SDP computation does not scale beyond few hundred vertices. Furthermore, the analysis presented here easily extends to the multiple graph setting.	[Shivanna, Rakesh] Google Inc, Mountain View, CA 94043 USA; [Chatterjee, Bibaswan; Sankaran, Raman; Bhattacharyya, Chiranjib] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore, Karnataka, India; [Bach, Francis] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France	Google Incorporated; Indian Institute of Science (IISC) - Bangalore; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Shivanna, R (corresponding author), Google Inc, Mountain View, CA 94043 USA.	rakeshshivanna@google.com; bibaswan.chatterjee@csa.iisc.ernet.in; ramans@csa.iisc.ernet.in; chiru@csa.iisc.ernet.in; francis.bach@ens.fr			Indo-French Center for Applied Mathematics (IFCAM)	Indo-French Center for Applied Mathematics (IFCAM)	We acknowledge support from a grant from Indo-French Center for Applied Mathematics (IFCAM).	Ando R. K., 2007, NIPS; Balcan Maria-Florina., 2006, 21 AUGMENTED PAC MOD; Boyle J. P., 1986, ADV ORDER RESTRICTED, P28, DOI DOI 10.1007/978-1-4613-9940-7_3; Cormen Thomas H., 2001, INTRO ALGORITHMS, V2; Erdem A, 2012, NEURAL COMPUT, V24, P700, DOI 10.1162/NECO_a_00233; Goemans MX, 1997, MATH PROGRAM, V79, P143, DOI 10.1007/BF02614315; Jethava V., 2012, NEURAL INFORM PROCES, P1169; Johnson R, 2007, J MACH LEARN RES, V8, P1489; Leordeanu M, 2011, IEEE I CONF COMP VIS, P2274, DOI 10.1109/ICCV.2011.6126507; Lichman M., 2013, UCI MACHINE LEARNING; LOVASZ L, 1979, IEEE T INFORM THEORY, V25, P1, DOI 10.1109/TIT.1979.1055985; Nagy ME, 2014, J COMB THEORY B, V108, P40, DOI 10.1016/j.jctb.2014.02.011; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Schmidt M., 2011, ADV NEURAL INFORM PR, P1458; Shivanna R., 2014, NIPS, P3635; Tran L., 2013, IJBB; Villa S, 2013, SIAM J OPTIMIZ, V23, P1607, DOI 10.1137/110844805; Zhang T, 2005, ADV NEURAL INFORM PR, P1601; Zhou D., 2007, P 24 INT C MACHINE L, P1159, DOI DOI 10.1145/1273496.1273642; Zhou DY, 2004, ADV NEUR IN, V16, P321	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100080
C	Shpitser, I		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Shpitser, Ilya			Segregated Graphs and Marginals of Chain Graph Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MARKOV PROPERTIES	Bayesian networks are a popular representation of asymmetric (for example causal) relationships between random variables. Markov random fields (MRFs) are a complementary model of symmetric relationships used in computer vision, spatial modeling, and social and gene expression networks. A chain graph model under the Lauritzen-Wermuth-Frydenberg interpretation (hereafter a chain graph model) generalizes both Bayesian networks and MRFs, and can represent asymmetric and symmetric relationships together. As in other graphical models, the set of marginals from distributions in a chain graph model induced by the presence of hidden variables forms a complex model. One recent approach to the study of marginal graphical models is to consider a well-behaved supermodel. Such a supermodel of marginals of Bayesian networks, defined only by conditional independences, and termed the ordinary Markov model, was studied at length in [6]. In this paper, we show that special mixed graphs which we call segregated graphs can be associated, via a Markov property, with supermodels of marginals of chain graphs defined only by conditional independences. Special features of segregated graphs imply the existence of a very natural factorization for these supermodels, and imply many existing results on the chain graph model, and the ordinary Markov model carry over. Our results suggest that segregated graphs define an analogue of the ordinary Markov model for marginals of chain graph models. We illustrate the utility of segregated graphs for analyzing outcome interference in causal inference via simulated datasets.	[Shpitser, Ilya] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Johns Hopkins University	Shpitser, I (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.	ilyas@cs.jhu.edu			NIH [R01 AI104459-01A1]	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	The author would like to thank Thomas Richardson for suggesting mixed graphs where -and <-> edges do not meet as interesting objects to think about, and Elizabeth Ogburn and Eric Tchetgen Tchetgen for clarifying discussions of interference. This work was supported in part by an NIH grant R01 AI104459-01A1.	Andersson SA, 1997, ANN STAT, V25, P505; Bell J.S, 1964, PHYS LONG ISL CITY N, V1, P195, DOI [10.1103/Physics-PhysiqueFizika.1.195, DOI 10.1103/PHYSICSPHYSIQUEFIZIKA.1.195]; Burrows H.D, 2014, ANN STAT, P1, DOI 10.1007/978-90-481-3830-2; Cai ZH, 2008, BIOMETRICS, V64, P695, DOI 10.1111/j.1541-0420.2007.00949.x; Drton M, 2009, BERNOULLI, V15, P736, DOI 10.3150/08-BEJ172; Evans R. J., 2010, P 26 C UNC ART INT; Koster JTA, 2002, BERNOULLI, V8, P817; Lauritzen S.L., 1996, OXFORD STAT SCI SERI, V17, P298; Ogburn EL, 2014, STAT SCI, V29, P559, DOI 10.1214/14-STS501; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pearl J, 2009, CAUSALITY MODELS REA, DOI DOI 10.1017/CBO9780511803161; Richardson T, 2003, SCAND J STAT, V30, P145, DOI 10.1111/1467-9469.00323; Richardson T, 2002, ANN STAT, V30, P962; Sadeghi K, 2014, BERNOULLI, V20, P676, DOI 10.3150/12-BEJ502; Shpitser I., 2014, BEHAVIORMETRIKA, V41, P3, DOI DOI 10.2333/BHMK.41.3; Studeny M., 1998, Uncertainty in Artificial Intelligence. Proceedings of the Fourteenth Conference (1998), P496; van der Laan M. J., 2012, WORKING PAPER; Van der Laan MJ, 2014, J CAUSAL INFERENCE, V2, P13, DOI 10.1515/jci-2013-0002; VanderWeele TJ, 2012, EPIDEMIOLOGY, V23, P751, DOI 10.1097/EDE.0b013e31825fb7a0; Verma T, 1990, P 6 C UNC ART INT, P255; Wermuth N, 2011, BERNOULLI, V17, P845, DOI 10.3150/10-BEJ309	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102053
C	Simsek, O; Buckmann, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Simsek, Oezguer; Buckmann, Marcus			Learning From Small Samples: An Analysis of Simple Decision Heuristics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				FRUGAL; MODELS	Simple decision heuristics are models of human and animal behavior that use few pieces of information-perhaps only a single piece of information-and integrate the pieces in simple ways, for example, by considering them sequentially, one at a time, or by giving them equal weight. We focus on three families of heuristics: single-cue decision making, lexicographic decision making, and tallying. It is unknown how quickly these heuristics can be learned from experience. We show, analytically and empirically, that substantial progress in learning can be made with just a few training samples. When training samples are very few, tallying performs substantially better than the alternative methods tested. Our empirical analysis is the most extensive to date, employing 63 natural data sets on diverse subjects.	[Simsek, Oezguer; Buckmann, Marcus] Max Planck Inst Human Dev, Ctr Adapt Behav & Cognit, Lentzeallee 94, D-14195 Berlin, Germany	Max Planck Society	Simsek, O (corresponding author), Max Planck Inst Human Dev, Ctr Adapt Behav & Cognit, Lentzeallee 94, D-14195 Berlin, Germany.	ozgur@mpib-berlin.mpg.de; buckmann@mpib-berlin.mpg.de			Deutsche Forschungsgemeinschaft (DFG) [SI 1732/1-1];  [SPP 1516]	Deutsche Forschungsgemeinschaft (DFG)(German Research Foundation (DFG)); 	Thanks to Gerd Gigerenzer, Konstantinos Katsikopoulos, Malte Lichtenberg, Laura Martignon, Perke Jacobs, and the ABC Research Group for their comments on earlier drafts of this article. This work was supported by Grant SI 1732/1-1 to Ozgur Simsek from the Deutsche Forschungsgemeinschaft (DFG) as part of the priority program "New Frameworks of Rationality" (SPP 1516).	Blewitt ME, 2008, NAT GENET, V40, P663, DOI 10.1038/ng.142; Bottou L., 2005, ADV NEURAL INFORM PR, P1393; Brighton, 2006, AAAI SPRING S COGN S, P17; Brighton H., 2012, EVOLUTION RATIONALIT, P84, DOI DOI 10.1017/CBO9780511792601.006; Brighton H., 2008, PROBABILISTIC MIND P, P189; Chater N, 2003, ORGAN BEHAV HUM DEC, V90, P63, DOI 10.1016/S0749-5978(02)00508-3; Czerlinski Jean, 1999, SIMPLE HEURISTICS MA, P97, DOI DOI 10.1002/(SICI)1099-0771(200004/06)13:2<161::AID-BDM348>3.0.CO;2-P; Dougherty MR, 2008, PSYCHOL REV, V115, P199, DOI 10.1037/0033-295X.115.1.199; Gigerenzer G, 1996, PSYCHOL REV, V103, P650, DOI 10.1037/0033-295X.103.4.650; Gigerenzer G., 1999, SIMPLE HEURISTICS MA, P3; Gigerenzer G.E., 2011, HEURISTICS FDN ADAPT; Hogarth RM, 2005, J MATH PSYCHOL, V49, P115, DOI 10.1016/j.jmp.2005.01.001; Katsikopoulos KV, 2011, DECIS ANAL, V8, P10, DOI 10.1287/deca.1100.0191; Katsikopoulos KV, 2010, PSYCHOL REV, V117, P1259, DOI 10.1037/a0020418; Laskey K., 2014, P 9 INT C TEACH STAT; Luan S, 2014, PSYCHOL REV, V121, P501, DOI 10.1037/a0037025; Martignon L, 2002, THEOR DECIS, V52, P29, DOI 10.1023/A:1015516217425; Martignon L., 1999, BAYESIAN BENCHMARKS, P169; Martignon L, 2008, J MATH PSYCHOL, V52, P352, DOI 10.1016/j.jmp.2008.04.003; Newell BR, 2005, TRENDS COGN SCI, V9, P11, DOI 10.1016/j.tics.2004.11.005; Rose C., TELEVISION PROGRAM; Schmitt M, 2006, J MACH LEARN RES, V7, P55; Schmitt M., 2006, ADV NEURAL INFORM PR, V18, P1177; Simsek, 2013, ADV NEURAL INFORM PR, P2904; Therneau T, 2014, R PACKAGE VERSION; Vul E, 2014, COGNITIVE SCI, V38, P599, DOI 10.1111/cogs.12101	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100011
C	Singer, Y; Vondrak, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Singer, Yaron; Vondrak, Jan			Information-theoretic lower bounds for convex optimization with erroneous oracles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We consider the problem of optimizing convex and concave functions with access to an erroneous zeroth-order oracle. In particular, for a given function x -> f (x) we consider optimization when one is given access to absolute error oracles that return values in [f (x) - epsilon f (x) + epsilon] or relative error oracles that return value in [(1 - epsilon) f (x); (1 + epsilon) f (x)], for some epsilon > 0. We show stark information theoretic impossibility results for minimizing convex functions and maximizing concave functions over polytopes in this model.	[Singer, Yaron] Harvard Univ, Cambridge, MA 02138 USA; [Vondrak, Jan] IBM Almaden Res Ctr, San Jose, CA 95120 USA	Harvard University; International Business Machines (IBM)	Singer, Y (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	yaron@seas.harvard.edu; jvondrak@us.ibm.com			NSF [CCF-1301976, CCF-1452961]; Google Faculty Research Award	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated)	YS was supported by NSF grant CCF-1301976, CAREER CCF-1452961 and a Google Faculty Research Award.	Agarwal A., 2010, P COLT, P28; Agarwal A, 2013, SIAM J OPTIMIZ, V23, P213, DOI 10.1137/110850827; Belloni Alexandre, 2015, COLT 2015; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Dubhashi Devdatt, 1996, MPII961020; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Feige U, 2011, SIAM J COMPUT, V40, P1133, DOI 10.1137/090779346; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y., 2011, 2011001 CORE LIDAM U; RAMDAS A, 2013, P 30 INT C MACH LEAR, P365; Ramdas A, 2014, JMLR WORKSH CONF PRO, V33, P805; Recht K G., 2012, P ADV NIPS SEP, P2681; Shamir O., 2013, P C LEARN THEOR, P3; Stich Sebastian U., 2011, CORR; Vondrak J, 2013, SIAM J COMPUT, V42, P265, DOI 10.1137/110832318	16	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101100
C	Sivakumar, V; Banerjee, A; Ravikumar, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Sivakumar, Vidyashankar; Banerjee, Arindam; Ravikumar, Pradeep			Beyond Sub-Gaussian Measurements: High-Dimensional Structured Estimation with Sub-Exponential Designs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				RECOVERY; LASSO	We consider the problem of high-dimensional structured estimation with norm-regularized estimators, such as Lasso, when the design matrix and noise are drawn from sub-exponential distributions. Existing results only consider sub-Gaussian designs and noise, and both the sample complexity and non-asymptotic estimation error have been shown to depend on the Gaussian width of suitable sets. In contrast, for the sub-exponential setting, we show that the sample complexity and the estimation error will depend on the exponential width of the corresponding sets, and the analysis holds for any norm. Further, using generic chaining, we show that the exponential width for any set will be at most root logp times the Gaussian width of the set, yielding Gaussian width based results even for the sub-exponential case. Further, for certain popular estimators, viz Lasso and Group Lasso, using a VC-dimension based analysis, we show that the sample complexity will in fact be the same order as Gaussian designs. Our general analysis and results are the first in the sub-exponential setting, and are readily applicable to special sub-exponential families such as log-concave and extreme-value distributions.	[Sivakumar, Vidyashankar; Banerjee, Arindam] Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA; [Ravikumar, Pradeep] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Minnesota System; University of Minnesota Twin Cities; University of Texas System; University of Texas Austin	Sivakumar, V (corresponding author), Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	sivakuma@cs.umn.edu; banerjee@cs.umn.edu; pradeepr@cs.utexas.edu			NSF [IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	This work was supported by NSF grants IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.	Adamczak R, 2011, CONSTR APPROX, V34, P61, DOI 10.1007/s00365-010-9117-4; Banerjee A., 2014, NIPS; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P489, DOI 10.1109/TIT.2005.862083; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chatterjee S., 2014, NIPS; Hsu D., 2014, ICML; Koltchinskii V., 2013, ARXIV13123580; Lecue G., 2014, ARXIV14012188; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Meinshausen N, 2009, ANN STAT, V37, P246, DOI 10.1214/07-AOS582; Mendelson S, 2015, J ACM, V62, DOI 10.1145/2699439; Mendelson S, 2012, J FUNCT ANAL, V262, P3775, DOI 10.1016/j.jfa.2012.01.027; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Oliveira R., 2013, ARXIV13122903; Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201; Talagrand M., 2005, THE GENERIC CHAINING; Tropp JA, 2015, APPL NUMER HARMON AN, P67, DOI 10.1007/978-3-319-19749-4_2; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Zhao P, 2006, J MACH LEARN RES, V7, P2541	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102083
C	Thomas, PS; Niekum, S; Theocharous, G; Konidaris, G		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Thomas, Philip S.; Niekum, Scott; Theocharous, Georgios; Konidaris, George			Policy Evaluation Using the Omega-Return	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose the Omega-return as an alternative to the Omega-return currently used by the TD (lambda) family of algorithms. The benefit of the Omega-return is that it accounts for the correlation of different length returns. Because it is difficult to compute exactly, we suggest one way of approximating the Omega-return. We provide empirical studies that suggest that it is superior to the lambda-return and gamma-return for a variety of problems.	[Thomas, Philip S.] Carnegie Mellon Univ, Univ Massachusetts Amherst, Pittsburgh, PA 15213 USA; [Niekum, Scott] Univ Texas Austin, Austin, TX 78712 USA; [Theocharous, Georgios] Adobe Res, San Jose, CA USA; [Konidaris, George] Duke Univ, Durham, NC 27706 USA	Carnegie Mellon University; University of Texas System; University of Texas Austin; Adobe Systems Inc.; Duke University	Thomas, PS (corresponding author), Carnegie Mellon Univ, Univ Massachusetts Amherst, Pittsburgh, PA 15213 USA.							Blana D, 2009, MED BIOL ENG COMPUT, V47, P533, DOI 10.1007/s11517-009-0479-3; Downey C., 2010, ICML, P311; Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398; Jagodnik K., 2007, 12 ANN C INT FES SOC; Kariya T, 2004, GEN LEAST SQUARES; Konidaris G., 2011, AAAI, V380, P5; Konidaris G., 2011, ADV NEURAL INFORM PR, P2402; Mahmood AR, 2014, ADV NEUR IN, V27; Pilarski Patrick M, 2011, IEEE Int Conf Rehabil Robot, V2011, P5975338, DOI 10.1109/ICORR.2011.5975338; Precup D., 2000, INT C MACH LEARN, P759; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tetreault J. R., 2006, P HUM LANG TECHN N A; Theocharous G., 2013, 1 MULT C REINF LEARN; Thomas P., 2015, P 29 C ART INT; Thomas Philip, 2009, Proc Innov Appl Artif Intell Conf, V2009, P165	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101067
C	Tran, D; Blei, DM; Airoldi, EM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Tran, Dustin; Blei, David M.; Airoldi, Edoardo M.			Copula variational inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We develop a general variational inference method that preserves dependency among the latent variables. Our method uses copulas to augment the families of distributions used in mean-field and structured approximations. Copulas model the dependency that is not captured by the original variational distribution, and thus the augmented variational family guarantees better approximations to the posterior. With stochastic optimization, inference on the augmented distribution is scalable. Furthermore, our strategy is generic: it can be applied to any inference procedure that currently uses the mean-field or structured approach. Copula variational inference has many advantages: it reduces bias; it is less sensitive to local optima; it is less sensitive to hyperparameters; and it helps characterize and interpret the dependency among the latent variables.	[Tran, Dustin; Airoldi, Edoardo M.] Harvard Univ, Cambridge, MA 02138 USA; [Blei, David M.] Columbia Univ, New York, NY 10027 USA	Harvard University; Columbia University	Tran, D (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.				NSF [IIS-0745520, IIS-1247664, IIS-1009542]; ONR [N00014-11-1-0651]; DARPA [FA8750-14-2-0009, N66001-15-C-4032]; John Templeton Foundation; Facebook; Adobe; Amazon	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); John Templeton Foundation; Facebook(Facebook Inc); Adobe; Amazon	We thank Luke Bornn, Robin Gong, and Alp Kucukelbir for their insightful comments. This work is supported by NSF IIS-0745520, IIS-1247664, IIS-1009542, ONR N00014-11-1-0651, DARPA FA8750-14-2-0009, N66001-15-C-4032, Facebook, Adobe, Amazon, and the John Templeton Foundation.	[Anonymous], 2014, INT C MACH LEARN; DEMPSTER AP, 1977, J ROYAL STAT SOC B, V39; Dissmann J., 2012, ARXIV12022002; Frechet M., 1960, TRABAJOS ESTADISTICA, V11, P3; Genest C, 2009, INSUR MATH ECON, V44, P143, DOI 10.1016/j.insmatheco.2008.10.005; Giordano R.J., 2015, NEURAL INFORM PROCES; Gruber L., 2015, INT SOC BAYESIAN ANA; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; Hoffman M. D., 2015, ARTIF INTELL; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Joe H., 1996, LECT NOTES MONOGRAPH, P120, DOI DOI 10.1214/LNMS/1215452614; Kappen H. J., 2001, NEURAL INFORM PROCES; Kingma D.P., 2015, INT C LEARN REPR ICL; Kurowicka D., 2006, WILEY SERIES PROBABI; Nelsen R.B, 2006, INTRO COPULAS, Vsecond, DOI DOI 10.1007/0-387-28678-0; Ranganath R, 2014, JMLR WORKSH CONF PRO, V33, P814; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Saul LK, 1996, ADV NEUR IN, V8, P486; Seeger Matthias, 2010, INT C MACH LEARN; Sklar A., 1959, PUBLICATIONS I STAT, V8, P229, DOI DOI 10.1007/BF01544178; Stan Development Team, 2015, STAN C PLUS PLUS LIB; Toulis P., 2014, ARXIV14082923; Tran D., 2015, ARXIV150906459; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	25	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100037
C	Tsiligkaridis, T; Forsythe, KW		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Tsiligkaridis, Theodoros; Forsythe, Keith W.			Adaptive Low-Complexity Sequential Inference for Dirichlet Process Mixture Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				BAYESIAN-INFERENCE	We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a novel adaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and data predictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to other online state-of-the-art methods.	[Tsiligkaridis, Theodoros; Forsythe, Keith W.] MIT, Lincoln Lab, Lexington, MA 02421 USA	Lincoln Laboratory; Massachusetts Institute of Technology (MIT)	Tsiligkaridis, T (corresponding author), MIT, Lincoln Lab, Lexington, MA 02421 USA.	ttsili@ll.mit.edu; forsythe@ll.mit.edu						ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; Batir N, 2008, ARCH MATH, V91, P554, DOI 10.1007/s00013-008-2856-9; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Daume H., 2007, C ART INT STAT; ESCOBAR MD, 1995, J AM STAT ASSOC, V90, P577, DOI 10.2307/2291069; Fearnhead P, 2004, STAT COMPUT, V14, P11, DOI 10.1023/B:STCO.0000009418.04621.cd; Kurihara K., 2006, ADV NEURAL INFORM PR; Lin D., 2013, ADV NEURAL INFORM PR, P395; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; NEAL RM, 1992, FUND THEOR, V50, P197; Rasmussen CE, 2000, ADV NEUR IN, V12, P554; Tsiligkaridis T, 2015, IEEE INT WORKS MACH; Tzikas DG, 2008, IEEE SIGNAL PROC MAG, V25, P131, DOI 10.1109/MSP.2008.929620; Wang LM, 2011, J COMPUT GRAPH STAT, V20, P196, DOI 10.1198/jcgs.2010.07081	14	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103073
C	Valera, I; Ruiz, FJR; Svensson, L; Perez-Cruz, F		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Valera, Isabel; Ruiz, Francisco J. R.; Svensson, Lennart; Perez-Cruz, Fernando			Infinite Factorial Dynamical Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				TRACKING	We propose the infinite factorial dynamic model (iFDM), a general Bayesian non-parametric model for source separation. Our model builds on the Markov Indian buffet process to consider a potentially unbounded number of hidden Markov chains (sources) that evolve independently according to some dynamics, in which the state space can be either discrete or continuous. For posterior inference, we develop an algorithm based on particle Gibbs with ancestor sampling that can be efficiently applied to a wide range of source separation problems. We evaluate the performance of our iFDM on four well-known applications: multitarget tracking, cocktail party, power disaggregation, and multiuser detection. Our experimental results show that our approach for source separation does not only outperform previous approaches, but it can also handle problems that were computationally intractable for existing approaches.	[Valera, Isabel] Max Planck Inst Software Syst, Saarbrucken, Germany; [Ruiz, Francisco J. R.] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Svensson, Lennart] Chalmers Univ Technol, Dept Signals & Syst, Gothenburg, Sweden; [Perez-Cruz, Fernando] Univ Carlos III Madrid, Madrid, Spain; [Perez-Cruz, Fernando] Alcatel Lucent, Bell Labs, Boulogne, France	Max Planck Society; Columbia University; Chalmers University of Technology; Universidad Carlos III de Madrid; Alcatel-Lucent	Valera, I (corresponding author), Max Planck Inst Software Syst, Saarbrucken, Germany.	ivalera@mpi-sws.org; f.ruiz@columbia.edu; lennart.svensson@chalmers.se; fernandop@ieee.org	Svensson, Lennart/AAJ-7677-2020		Plan Regional-Programas I+D of Comunidad de Madrid [AGES-CM S2010/BMD-2422]; FPU fellowship from the Spanish Ministry of Education [AP2010-5333]; Ministerio de Economia of Spain (project COMPREHENSION) [TEC2012-38883-C02-01]; Ministerio de Economia of Spain (project ALCIT) [TEC2012-38800-C03-01]; Comunidad de Madrid (project CASI-CAM-CM) [S2013/ICE-2845]; Office of Naval Research [ONR N00014-11-1-0651]; European Union 7th Framework Programme through the Marie Curie Initial Training Network 'Machine Learning for Personalized Medicine' (MLPM2012) [316861]; Humboldt research fellowship for postdoctoral researchers program	Plan Regional-Programas I+D of Comunidad de Madrid; FPU fellowship from the Spanish Ministry of Education(German Research Foundation (DFG)); Ministerio de Economia of Spain (project COMPREHENSION); Ministerio de Economia of Spain (project ALCIT); Comunidad de Madrid (project CASI-CAM-CM)(Comunidad de Madrid); Office of Naval Research(Office of Naval Research); European Union 7th Framework Programme through the Marie Curie Initial Training Network 'Machine Learning for Personalized Medicine' (MLPM2012); Humboldt research fellowship for postdoctoral researchers program	I. Valera is currently supported by the Humboldt research fellowship for postdoctoral researchers program and acknowledges the support of Plan Regional-Programas I+D of Comunidad de Madrid (AGES-CM S2010/BMD-2422). F. J. R. Ruiz is supported by an FPU fellowship from the Spanish Ministry of Education (AP2010-5333). This work is also partially supported by Ministerio de Economia of Spain (projects COMPREHENSION, id. TEC2012-38883-C02-01, and ALCIT, id. TEC2012-38800-C03-01), by Comunidad de Madrid (project CASI-CAM-CM, id. S2013/ICE-2845), by the Office of Naval Research (ONR N00014-11-1-0651), and by the European Union 7th Framework Programme through the Marie Curie Initial Training Network 'Machine Learning for Personalized Medicine' (MLPM2012, Grant No. 316861).	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Beal M. J., 2002, ADV NEURAL INFORM PR, V14; FORTUNE SJ, 1995, IEEE COMPUT SCI ENG, V2, P58, DOI 10.1109/99.372944; Fox EB, 2011, ANN APPL STAT, V5, P1020, DOI 10.1214/10-AOAS395; Fox EB, 2010, IEEE SIGNAL PROC MAG, V27, P43, DOI 10.1109/MSP.2010.937999; Jiang L., 2014, ARXIV14102046; Johnson MJ, 2013, J MACH LEARN RES, V14, P673; Jordan M. I., 2010, HIERARCHICAL MODELS; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Knowles D, 2011, ANN APPL STAT, V5, P1534, DOI 10.1214/10-AOAS435; Kolter J.Z., 2012, P ARTIFICIAL INTELLI, P1472; Lim J., 2015, INT J DISTRIBUTED SE; Lindsten F, 2014, J MACH LEARN RES, V15, P2145; Lindsten F, 2013, FOUND TRENDS MACH LE, V6, P1, DOI 10.1561/2200000045; Makonin S, 2013, 2013 IEEE ELECTRICAL POWER & ENERGY CONFERENCE (EPEC), DOI 10.1109/EPEC.2013.6802949; Oh S, 2004, IEEE DECIS CONTR P, P735, DOI 10.1109/CDC.2004.1428740; Orbanz P., 2010, ENCY MACHINE LEARNIN, V1; Sarkka S, 2007, INFORM FUSION, V8, P2, DOI 10.1016/j.inffus.2005.09.009; Teh Y. W., 2007, P INT C ART INT STAT, V11; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Thouin F., 2011, 14 INT C INFORM FUSI, P1; Titsias MK, 2014, ADV NEUR IN, V27; Van Gael J, 2009, ADV NEURAL INFORM PR, V21; Vazquez MA, 2013, IEEE T VEH TECHNOL, V62, P3188, DOI 10.1109/TVT.2013.2251024; Whiteley N., 2010, TECHNICAL REPORT, V10, P04	25	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100035
C	Vovk, V; Petej, I; Fedorova, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Vovk, Vladimir; Petej, Ivan; Fedorova, Valentina			Large-scale probabilistic predictors with and without guarantees of validity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This paper studies theoretically and empirically a method of turning machine-learning algorithms into probabilistic predictors that automatically enjoys a property of validity (perfect calibration) and is computationally efficient. The price to pay for perfect calibration is that these probabilistic predictors produce imprecise (in practice, almost precise for large data sets) probabilities. When these imprecise probabilities are merged into precise probabilities, the resulting predictors, while losing the theoretical property of perfect calibration, are consistently more accurate than the existing methods in empirical studies.	[Vovk, Vladimir; Petej, Ivan] Univ London, Dept Comp Sci, Royal Holloway, Egham, Surrey, England; [Fedorova, Valentina] Yandex, Moscow, Russia	University of London; Royal Holloway University London	Vovk, V (corresponding author), Univ London, Dept Comp Sci, Royal Holloway, Egham, Surrey, England.	volodya.vovk@gmail.com; ivan.petej@gmail.com; alushaf@gmail.com			EPSRC [EP/K033344/1]; AFOSR	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	We are grateful to the conference reviewers for numerous helpful comments and observations, to Vladimir Vapnik for sharing his ideas about exploiting synergy between different learning algorithms, and to participants of the conference Machine Learning: Prospects and Applications (October 2015, Berlin) for their questions and comments. The first author has been partially supported by EPSRC (grant EP/K033344/1) and AFOSR (grant "Semantic Completions"). The second and third authors are grateful to their home institutions for funding their trips to Montreal.	AYER M, 1955, ANN MATH STAT, V26, P641, DOI 10.1214/aoms/1177728423; Barlow R. E., 1972, STAT INFERENCE ORDER; Caruana R., 2006, P 23 INT C MACH LEAR, P161; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; HALL M, 2011, SIGKDD EXPLORATIONS, V11, P10; Jiang Xiaoqian, 2011, AMIA Jt Summits Transl Sci Proc, V2011, P16; Lambrou A, 2012, IFIP INT C ARTIFICIA, P182, DOI DOI 10.1007/978-3-642-33412-2_19; LEE CIC, 1983, ANN STAT, V11, P467, DOI 10.1214/aos/1176346153; Murphy A. H., 1973, Journal of Applied Meteorology, V12, P595, DOI 10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2; MURRAY GD, 1983, BIOMETRIKA, V70, P490; Platt JC, 2000, ADV NEUR IN, P61; Vapnik Vladimir N., 2015, COMMUNICATION   1006; Vovk V., 2005, ALGORITHMIC LEARNING, DOI DOI 10.1007/B106715; Vovk V, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P829; Vovk V, 2015, LECT NOTES COMPUT SC, V9300, P307, DOI 10.1007/978-3-319-23534-9_20; Vovk Vladimir, 2015, TECHNICAL REPORT; Zadrozny Bianca, 2001, ICML	17	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101065
C	Wang, J; Trapeznikov, K; Saligrama, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Joseph; Trapeznikov, Kirill; Saligrama, Venkatesh			Efficient Learning by Directed Acyclic Graph For Resource Constrained Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of reducing test-time acquisition costs in classification systems. Our goal is to learn decision rules that adaptively select sensors for each example as necessary to make a confident prediction. We model our system as a directed acyclic graph (DAG) where internal nodes correspond to sensor subsets and decision functions at each node choose whether to acquire a new sensor or classify using the available measurements. This problem can be posed as an empirical risk minimization over training data. Rather than jointly optimizing such a highly coupled and non-convex problem over all decision nodes, we propose an efficient algorithm motivated by dynamic programming. We learn node policies in the DAG by reducing the global objective to a series of cost sensitive learning problems. Our approach is computationally efficient and has proven guarantees of convergence to the optimal system for a fixed architecture. In addition, we present an extension to map other budgeted learning problems with large number of sensors to our DAG architecture and demonstrate empirical performance exceeding state-of-the-art algorithms for data composed of both few and many sensors.	[Wang, Joseph; Saligrama, Venkatesh] Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA; [Trapeznikov, Kirill] Syst & Technol Res, Woburn, MA 01801 USA	Boston University	Wang, J (corresponding author), Boston Univ, Dept Elect & Comp Engn, Boston, MA 02215 USA.	joewang@bu.edu; kirill.trapeznikov@stresearch.com; srv@bu.edu		Saligrama, Venkatesh/0000-0002-0675-2268	U.S. National Science Foundation [1330008]; Department of Homeland Security, Science and Technology Directorate, Office of University Programs [2013-ST-061-ED0001]; ONR [50202168]; US AF [FA8650-14-C-1728]	U.S. National Science Foundation(National Science Foundation (NSF)); Department of Homeland Security, Science and Technology Directorate, Office of University Programs; ONR(Office of Naval Research); US AF	This material is based upon work supported in part by the U.S. National Science Foundation Grant 1330008, by the Department of Homeland Security, Science and Technology Directorate, Office of University Programs, under Grant Award 2013-ST-061-ED0001, by ONR Grant 50202168 and US AF contract FA8650-14-C-1728. The views and conclusions contained in this document are those of the authors and should not be interpreted as necessarily representing the social policies, either expressed or implied, of the U.S. DHS, ONR or AF.	[Anonymous], MULTICLASS CLASSIFIC; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Busa-Fekete Robert, 2012, P 29 INT C MACH LEAR; Chen M., 2012, INT C ART INT STAT; Dulac-Arnold G, 2011, LECT NOTES ARTIF INT, V6911, P375, DOI 10.1007/978-3-642-23780-5_34; Gao T., 2011, NEURIPS 2011, P1062; He H., 2012, NEURIPS 2012, P3158; Ji S., 2007, PATTERN RECOGNITION, V40; Kanani Pallika, 2008, ADV NEURAL INFORM PR; Karayev S., 2013, INT C MACH LEARN WOR; Kusner M., 2014, 28 AAAI C ART INT; Leskovec J., 2007, INT C KNOWL DISC DAT; Maaten L., 2013, P 30 INT C MACH LEAR; Nan F., 2015, P 32 INT C MACH LEAR; Nan F, 2014, INT CONF ACOUST SPEE; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; SHENG V. S., 2006, P 23 INT C MACH LEAR, P809; Steinwart I, 2005, IEEE T INFORM THEORY, V51, P128, DOI 10.1109/TIT.2004.839514; Trapeznikov K, 2013, ARTIF INTELL, P581; Wang J, 2013, ASIAN C MACHINE LEAR, P451; Wang J., 2012, ADV NEURAL INF PROCE, V25, P91; Wang J, 2014, JMLR WORKSH CONF PRO, V33, P987; Wang J, 2014, LECT NOTES COMPUT SC, V8690, P647, DOI 10.1007/978-3-319-10605-2_42; Xu Z., 2012, P 29 INT C MACH LEAR; Xu Z.E., 2013, ICML 2013, P133; Zhang C, 2010, C ELECT INSUL DIEL P	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103020
C	Wang, Y; Dunson, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Ye; Dunson, David			Probabilistic Curve Learning: Coulomb Repulsion and the Electrostatic Gaussian Process	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				ALGORITHM	Learning of low dimensional structure in multidimensional data is a canonical problem in machine learning. One common approach is to suppose that the observed data are close to a lower-dimensional smooth manifold. There are a rich variety of manifold learning methods available, which allow mapping of data points to the manifold. However, there is a clear lack of probabilistic methods that allow learning of the manifold along with the generative distribution of the observed data. The best attempt is the Gaussian process latent variable model (GP-LVM), but identifiability issues lead to poor performance. We solve these issues by proposing a novel Coulomb repulsive process (Corp) for locations of points on the manifold, inspired by physical models of electrostatic interactions among particles. Combining this process with a GP prior for the mapping function yields a novel electrostatic GP (electroGP) process. Focusing on the simple case of a one-dimensional manifold, we develop efficient inference algorithms, and illustrate substantially improved performance in a variety of experiments including filling in missing frames in video.	[Wang, Ye; Dunson, David] Duke Univ, Dept Stat, Durham, NC 27705 USA	Duke University	Wang, Y (corresponding author), Duke Univ, Dept Stat, Durham, NC 27705 USA.	eric.ye.wang@duke.edu; dunson@stat.duke.edu						Belkin M, 2002, ADV NEUR IN, V14, P585; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Chen MH, 2010, IEEE T SIGNAL PROCES, V58, P6140, DOI 10.1109/TSP.2010.2070796; HASTIE T, 1989, J AM STAT ASSOC, V84, P502, DOI 10.2307/2289936; Hough J.B., 2009, ZEROS GAUSSIAN ANAL, V51; Lawrence N, 2005, J MACH LEARN RES, V6, P1783; Lawrence N.D, 2006, P 23 INT C MACH LEAR, V148, P513, DOI DOI 10.1145/1143844.1143909; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Rao V., 2013, ARXIV13081136; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Titsias MK, 2010, P 13 INT C ARTIFICIA, V9, P844; Urtasun R., 2008, P 25 INT C MACHINE L, P1080; Wang Y., 2014, ARXIV14107692; Weinberger K. Q., 2006, AAAI, V6, P1683	15	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101054
C	Yurtsever, A; Quoc, TD; Cevher, V		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Yurtsever, Alp; Quoc Tran-Dinh; Cevher, Volkan			A Universal Primal-Dual Convex Optimization Framework	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose a new primal-dual algorithmic framework for a prototypical constrained convex optimization template. The algorithmic instances of our framework are universal since they can automatically adapt to the unknown Holder continuity degree and constant within the dual formulation. They are also guaranteed to have optimal convergence rates in the objective residual and the feasibility gap for each Holder smoothness degree. In contrast to existing primal-dual algorithms, our framework avoids the proximity operator of the objective function. We instead leverage computationally cheaper, Fenchel-type operators, which are the main workhorses of the generalized conditional gradient (GCG)-type methods. In contrast to the GCG-type methods, our framework does not require the objective function to be differentiable, and can also process additional general linear inclusion constraints, while guarantees the convergence rate on the primal problem.	[Yurtsever, Alp; Cevher, Volkan] Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, Lausanne, Switzerland; [Quoc Tran-Dinh] UNC, Dept Stat & Operat Res, Chapel Hill, NC USA	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of North Carolina; University of North Carolina Chapel Hill	Yurtsever, A (corresponding author), Ecole Polytech Fed Lausanne, Lab Informat & Inference Syst, Lausanne, Switzerland.	alp.yurtsever@epfl.ch; quoctd@email.unc.edu; volkan.cevher@epfl.ch	Yurtsever, Alp/AAB-9053-2020; Tran-Dinh, Quoc/AAX-8950-2020	Tran-Dinh, Quoc/0000-0002-1077-2579	ERC Future Proof; SNF [200021-146750, CRSII2-147633]	ERC Future Proof; SNF	This work was supported in part by ERC Future Proof, SNF 200021-146750 and SNF CRSII2-147633. We would like to thank Dr. Stephen Becker of University of Colorado at Boulder for his support in preparing the numerical experiments.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Candes E, 2012, COMMUN ACM, V55, P111, DOI 10.1145/2184319.2184343; Cevher V, 2014, IEEE SIGNAL PROC MAG, V31, P32, DOI 10.1109/MSP.2014.2329397; Combettes PL, 2008, INVERSE PROBL, V24, DOI 10.1088/0266-5611/24/6/065014; Goldstein T., 2013, ADAPTIVE PRIMAL DUAL; Gross D, 2010, PHYS REV LETT, V105, DOI 10.1103/PhysRevLett.105.150401; Jaggi M., 2010, P 27 INT C MACHINE L; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Juditsky A, 2016, MATH PROGRAM, V156, P221, DOI 10.1007/s10107-015-0876-3; Lan GH, 2016, MATH PROGRAM, V155, P511, DOI 10.1007/s10107-015-0861-x; Larsen R.M, PROPACK SOFTWARE LAR; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2015, TECH REP; Nesterov Y, 2015, MATH PROGRAM, V152, P381, DOI 10.1007/s10107-014-0790-0; Rockafellar RT., 1970, CONVEX ANAL; Shefi R, 2014, SIAM J OPTIMIZ, V24, P269, DOI 10.1137/130910774; Tran-Dinh Q., 2014, ADV NEURAL INFORM PR, V27; Wainwright MJ, 2014, ANNU REV STAT APPL, V1, P233, DOI 10.1146/annurev-statistics-022513-115643; Yu Y, 2014, THESIS	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101085
C	Zhao, T; Wang, ZR; Liu, H		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhao, Tuo; Wang, Zhaoran; Liu, Han			A Nonconvex Optimization Framework for Low Rank Matrix Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				COMPLETION	We study the estimation of low rank matrices via nonconvex optimization. Compared with convex relaxation, nonconvex optimization exhibits superior empirical performance for large scale instances of low rank matrix estimation. However, the understanding of its theoretical guarantees are limited. In this paper, we define the notion of projected oracle divergence based on which we establish sufficient conditions for the success of nonconvex optimization. We illustrate the consequences of this general framework for matrix sensing. In particular, we prove that a broad class of nonconvex optimization algorithms, including alternating minimization and gradient-type methods, geometrically converge to the global optimum and exactly recover the true low rank matrices under standard conditions.	[Zhao, Tuo] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Wang, Zhaoran; Liu, Han] Princeton Univ, Princeton, NJ 08544 USA	Johns Hopkins University; Princeton University	Zhao, T (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.		Wang, Zhaoran/P-7113-2018		NSF [IIS1116730, IIS1332109, IIS1408910, IIS1546482-BIGDATA, DMS1454377-CAREER]; NIH [R01GM083084, R01HG06841, R01MH102339]; FDA [HHSF223201000072C]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); FDA(United States Department of Health & Human Services)	Research supported by NSF IIS1116730, NSF IIS1332109, NSF IIS1408910, NSF IIS1546482-BIGDATA, NSF DMS1454377-CAREER, NIH R01GM083084, NIH R01HG06841, NIH R01MH102339, and FDA HHSF223201000072C.	Arora S., 2015, J MACH LEARN RES, V40; Balakrishnan Sivaraman, 2014, ARXIV14082156; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; Candes EJ, 2010, IEEE T INFORM THEORY, V56, P2053, DOI 10.1109/TIT.2010.2044061; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chen Y., 2013, ARXIV13100154; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Hardt M., 2014, ARXIV14074070; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Hardt Moritz, 2014, ARXIV14022331; Hastie T, 2014, ARXIV14102596; Jain P, 2010, P ADV NEUR INF PROC, P937; Jain P., 2014, ARXIV14111087; Jain P, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P665; Keshavan RH, 2010, IEEE T INFORM THEORY, V56, P2980, DOI 10.1109/TIT.2010.2046205; Koren Yehuda, 2009, NETFLIX PRIZE DOCUME, V81; Lee K, 2010, IEEE T INFORM THEORY, V56, P4402, DOI 10.1109/TIT.2010.2054251; Negahban S, 2011, ANN STAT, V39, P1069, DOI 10.1214/10-AOS850; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Paterek A., 2007, P KDD CUP WORKSH 200, P5; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Rohde A, 2011, ANN STAT, V39, P887, DOI 10.1214/10-AOS860; Stewart G.W., 1990, MATRIX PERTURBATION, V175; Sun Ruoyu, 2014, ARXIV14118003; Takacs G., 2007, ACM SIGKDD EXPLOR NE, V9, P80, DOI DOI 10.1145/1345448.134546	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100101
C	Amin, K; Rostamizadeh, A; Syed, U		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Amin, Kareem; Rostamizadeh, Afshin; Syed, Umar			Repeated Contextual Auctions with Strategic Buyers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Motivated by real-time advertising exchanges, we analyze the problem of pricing inventory in a repeated posted-price auction. We consider both the cases of a truthful and surplus-maximizing buyer, where the former makes decisions myopically on every round, and the latter may strategically react to our algorithm, forgoing short-term surplus in order to trick the algorithm into setting better prices in the future. We further assume a buyer's valuation of a good is a function of a context vector that describes the good being sold. We give the first algorithm attaining sublinear ((O) over tilde (T-2/3)) regret in the contextual setting against a surplus-maximizing buyer. We also extend this result to repeated second-price auctions with multiple buyers.	[Amin, Kareem] Univ Penn, Philadelphia, PA 19104 USA; [Rostamizadeh, Afshin; Syed, Umar] Google Res, Mountain View, CA USA	University of Pennsylvania; Google Incorporated	Amin, K (corresponding author), Univ Penn, Philadelphia, PA 19104 USA.	akareem@cis.upenn.edu; rostami@google.com; usyed@google.com						Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Bar-Yossef Z, 2002, SIAM PROC S, P964; Blum A, 2003, SIAM PROC S, P202; Cary M, 2007, EC'07: PROCEEDINGS OF THE EIGHTH ANNUAL CONFERENCE ON ELECTRONIC COMMERCE, P262; Cesa-Bianchi Nicolo, 2013, P S DISCR ALG; Edelman B, 2007, DECIS SUPPORT SYST, V43, P192, DOI 10.1016/j.dss.2006.08.008; Hajiaghayi M.T., P 5 ACM C EL COMM JA, P71; Kitts B., 2004, Electronic Markets, V14, P186, DOI 10.1080/1019678042000245119; Kitts Brendan, 2005, WORKSH SPONS SEARCH; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Mohri M., 2018, FDN MACHINE LEARNING; Mohri M, 2014, PR MACH LEARN RES, V32; Parkes DC, 2007, ALGORITHMIC GAME THEORY, P411; Rakhlin A., 2012, P 29 INT C MACH LEAR, P449; Rakhlin A, 2011, ARXIV11095647; Scholkopf B., 2001, LEARNING KERNELS SUP; Varian H. R, 2010, INTERMEDIATE MICROEC, V6	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103040
C	Ammar, W; Dyer, C; Smith, NA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ammar, Waleed; Dyer, Chris; Smith, Noah A.			Conditional Random Field Autoencoders for Unsupervised Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We introduce a framework for unsupervised learning of structured predictors with overlapping, global features. Each input's latent representation is predicted conditional on the observed data using a feature-rich conditional random field (CRF). Then a reconstruction of the input is (re)generated, conditional on the latent structure, using a generative model which factorizes similarly to the CRF. The autoencoder formulation enables efficient exact inference without resorting to unrealistic independence assumptions or restricting the kinds of features that can be used. We illustrate connections to traditional autoencoders, posterior regularization, and multi-view learning. We then show competitive results with instantiations of the framework for two canonical tasks in natural language processing: part-of-speech induction and bitext word alignment, and show that training the proposed model can be substantially more efficient than a comparable feature-rich baseline.	[Ammar, Waleed; Dyer, Chris; Smith, Noah A.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Ammar, W (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	wammar@cs.cmu.edu; cdyer@cs.cmu.edu; nasmith@cs.cmu.edu			U.S. Army Research Laboratory; U.S. Army Research Office [W911NF-10-1-0533]	U.S. Army Research Laboratory(United States Department of DefenseUS Army Research Laboratory (ARL)); U.S. Army Research Office	We thank Brendan O'Connor, Dani Yogatama, Jeffrey Flanigan, Manaal Faruqui, Nathan Schneider, Phil Blunsom and the anonymous reviewers for helpful suggestions. We also thank Taylor Berg-Kirkpatrick for providing his implementation of the POS induction baseline, and Phil Blunsom for sharing POS induction evaluation scripts. This work was sponsored by the U.S. Army Research Laboratory and the U.S. Army Research Office under contract/grant number W911NF-10-1-0533. The statements made herein are solely the responsibility of the authors.	Bailey T. L., 1995, MACHINE LEARNING; Bellare K., 2009, P UAI; Berg-Kirkpatrick Taylor, 2010, P NAACL; Blunsom P., 2006, P P ACL; Brown P. F., 1992, COMPUTATIONAL LINGUI; Brown Peter F, 1993, COMPUTATIONAL LINGUI; Buchholz S., 2006, CONLL X; Collobert R, 2008, P ICML; Daum H., 2009, P ICML MONTR, P209; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Druck G., 2010, P ICML; Duchi J., 2011, JMLR; Dyer C., 2011, P ACL HLT; Dyer Chris, 2010, P ACL; Dyer Chris, 2013, P NAACL; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Gao Q., 2008, P ACL WORKSH; Haghighi A., 2006, P NAACL HLT; Jelinek Frederick, 1997, STAT METHODS SPEECH; Johnson M., 2007, P EMNLP; Koehn Philipp, 2003, P NAACL; Koehn Philipp, 2010, STAT MACHINE TRANSLA; Lafferty J., 2001, DEP PAPERS CIS; Liang P., 2005, THESIS MIT; Lin C. - C., 2014, 1 WORKSH COMP APPR C; Lukashin AV, 1998, NUCLEIC ACIDS RES, V26, P1107, DOI 10.1093/nar/26.4.1107; Manning C. D., 2004, P ACL; Merialdo B., 1994, COMP LING; Minka T., 2005, MSRTR2005144; Mnih A., 2012, P ICML; Nivre Joakim, 2007, P CONLL; Och F., 2003, COMPUTATIONAL LINGUI; Papineni K., 2002, P 40 ANN M ASS COMP, P311, DOI [10.3115/1073083.1073135, DOI 10.3115/1073083.1073135]; Petrov Slav, 2012, P LREC MAY; Poon H., 2008, P EMNLP; Reddy S., 2009, P NAM ENT WORKSH; Rosenberg A., 2007, P 2007 EMNLP CONLL; Smith N. A., 2005, P ACL; Socher R., 2010, NIPS WORKSH; Stoyanov V., 2011, P AISTATS; Suzuki J., 2008, P ACL; Swier R., 2004, P EMNLP; Vickrey D., 2010, P ICML; Vincent P., 2008, P ICML; Vogel S., 1996, COLING 1996, V2; Weber M, 2000, UNSUPERVISED LEARNIN; Yamato J., 1992, Proceedings. 1992 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (Cat. No.92CH3168-2), P379, DOI 10.1109/CVPR.1992.223161	47	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101015
C	Archer, E; Koster, U; Pillow, J; Macke, JH		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Archer, Evan; Koster, Urs; Pillow, Jonathan; Macke, Jakob H.			Low-dimensional models of neural population activity in sensory cortical circuits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SPACE; INFORMATION	Neural responses in visual cortex are influenced by visual stimuli and by ongoing spiking activity in local circuits. An important challenge in computational neuroscience is to develop models that can account for both of these features in large multi-neuron recordings and to reveal how stimulus representations interact with and depend on cortical dynamics. Here we introduce a statistical model of neural population activity that integrates a nonlinear receptive field model with a latent dynamical model of ongoing cortical activity. This model captures temporal dynamics and correlations due to shared stimulus drive as well as common noise. Moreover, because the nonlinear stimulus inputs are mixed by the ongoing dynamics, the model can account for a multiple idiosyncratic receptive field shapes with a small number of nonlinear inputs to a low-dimensional dynamical model. We introduce a fast estimation method using online expectation maximization with Laplace approximations, for which inference scales linearly in both population size and recording duration. We test this model to multi-channel recordings from primary visual cortex and show that it accounts for neural tuning properties as well as cross-neural correlations.	[Archer, Evan; Macke, Jakob H.] Max Planck Inst Biol Cybernet, Tubingen, Germany; [Archer, Evan; Macke, Jakob H.] Bernstein Ctr Computat Neurosci, Tubingen, Germany; [Koster, Urs] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA; [Pillow, Jonathan] Princeton Univ, Dept Psychol, Princeton Neurosci Inst, Princeton, NJ 08544 USA	Max Planck Society; University of California System; University of California Berkeley; Princeton University	Archer, E (corresponding author), Max Planck Inst Biol Cybernet, Tubingen, Germany.	evan.archer@tuebingen.mpg.de; urs@nervanasys.com; pillow@princeton.edu; jakob@tuebingen.mpg.de			German Federal Ministry of Education and Research (BMBF, Bernstein Center Tubingen) [FKZ: 01GQ1002]; Max Planck Society; National Eye Institute [EY019965]	German Federal Ministry of Education and Research (BMBF, Bernstein Center Tubingen)(Federal Ministry of Education & Research (BMBF)); Max Planck Society(Max Planck SocietyFoundation CELLEX); National Eye Institute(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI))	We are thankful to Arnulf Graf and the co-authors of [26] for sharing the data used in Fig. 5, and to Memming Park for comments on the manuscript. JHM and EA were funded by the German Federal Ministry of Education and Research (BMBF; FKZ: 01GQ1002, Bernstein Center Tubingen) and the Max Planck Society, and UK by National Eye Institute grant #EY019965. Collaboration between EA, JP and JHM initiated at the 'MCN' Course at the Marine Biological Laboratory, Woods Hole.	ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; Churchland MM, 2007, CURR OPIN NEUROBIOL, V17, P609, DOI 10.1016/j.conb.2007.11.001; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Fairhall A, 2014, CURR OPIN NEUROBIOL, V25, pIX, DOI 10.1016/j.conb.2014.02.001; Ghahramani Z., 1996, U TORONTO TECH REPOR, V6; Graf ABA, 2011, NAT NEUROSCI, V14, P239, DOI 10.1038/nn.2733; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Khan M. Emtiyaz, 2013, P ICML; Koster U, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003684; Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173; Macke J. H., 2012, ADV NEURAL INFO P SY, V24; MACKE JH, 2008, ADV NEURAL INFORM PR, V20, P969; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Mountcastle V., 1957, J NEUROPHYSIOL; Paninski L., 2013, ADV NEURAL INF PROCE, V26, P2391; Paninski L, 2010, J COMPUT NEUROSCI, V29, P107, DOI 10.1007/s10827-009-0179-x; Park IM, 2013, ADV NEURAL INFORM PR, V26, P2454; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Pillow JW, 2006, J VISION, V6, P414, DOI 10.1167/6.4.9; Reich DS, 2001, SCIENCE, V294, P2566, DOI 10.1126/science.1065839; Rust NC, 2005, NEURON, V46, P945, DOI 10.1016/j.neuron.2005.05.021; Sharpee TO, 2013, ANNU REV NEUROSCI, V36, P103, DOI 10.1146/annurev-neuro-062012-170253; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Smith SL, 2010, NAT NEUROSCI, V13, P1144, DOI 10.1038/nn.2620; Truccolo W, 2010, NAT NEUROSCI, V13, P105, DOI 10.1038/nn.2455; Vidne M., 2011, J COMPUT NEUROSCI; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; ZammitMangion A, 2011, NEURAL COMPUT, V23, P1967, DOI 10.1162/NECO_a_00156	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100042
C	Bansal, T; Bhattacharyya, C; Kannan, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bansal, Trapit; Bhattacharyya, C.; Kannan, Ravindran			A provable SVD-based algorithm for learning topics in dominant admixture corpus	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Topic models, such as Latent Dirichlet Allocation (LDA), posit that documents are drawn from admixtures of distributions over words, known as topics. The inference problem of recovering topics from such a collection of documents drawn from admixtures, is NP-hard. Making a strong assumption called separability, [4] gave the first provable algorithm for inference. For the widely used LDA model, [6] gave a provable algorithm using clever tensor-methods. But [4, 6] do not learn topic vectors with bounded l(1) error (a natural measure for probability vectors). Our aim is to develop a model which makes intuitive and empirically supported assumptions and to design an algorithm with natural, simple components such as SVD, which provably solves the inference problem for the model with bounded l1 error. A topic in LDA and other models is essentially characterized by a group of co-occurring words. Motivated by this, we introduce topic specific Catchwords, a group of words which occur with strictly greater frequency in a topic than any other topic individually and are required to have high frequency together rather than individually. A major contribution of the paper is to show that under this more realistic assumption, which is empirically verified on real corpora, a singular value decomposition (SVD) based algorithm with a crucial pre-processing step of thresholding, can provably recover the topics from a collection of documents drawn from Dominant admixtures. Dominant admixtures are convex combination of distributions in which one distribution has a significantly higher contribution than the others. Apart from the simplicity of the algorithm, the sample complexity has near optimal dependence on w(0), the lowest probability that a topic is dominant, and is better than [4]. Empirical evidence shows that on several real world corpora, both Catchwords and Dominant admixture assumptions hold and the proposed algorithm substantially outperforms the state of the art [5].	[Bansal, Trapit; Bhattacharyya, C.] Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India; [Kannan, Ravindran] Microsoft Res, Madras, Tamil Nadu, India	Indian Institute of Science (IISC) - Bangalore	Bansal, T (corresponding author), Indian Inst Sci, Dept Comp Sci & Automat, Bangalore 560012, Karnataka, India.	trapitbansal@gmail.com; chiru@csa.iisc.ernet.in; kannan@microsoft.com			Department of Science and Technology (DST) grant	Department of Science and Technology (DST) grant(Department of Science & Technology (India)Department of Science & Technology (DOST), Philippines)	TB was supported by a Department of Science and Technology (DST) grant.	Anandkumar Animashree, 2012, NEURAL INFORM PROCES; Arora S., 2013, INT C MACH LEARN; Arora S., 2012, FDN COMPUTER SCI; Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Awashti P., 2012, P APPR RAND; Blei DM, 2012, COMMUN ACM, V55, P77, DOI 10.1145/2133806.2133826; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Kumar A., 2010, FDN COMPUTER SCI; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Mimno D, 2011, P C EMP METH NAT LAN, P262, DOI DOI 10.5555/2145432.2145462; Papadimitriou CH, 2000, J COMPUT SYST SCI, V61, P217, DOI 10.1006/jcss.2000.1711	13	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100081
C	Bartz, D; Muller, KR		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Bartz, Daniel; Mueller, Klaus-Robert			Covariance shrinkage for autocorrelated data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The accurate estimation of covariance matrices is essential for many signal processing and machine learning algorithms. In high dimensional settings the sample covariance is known to perform poorly, hence regularization strategies such as analytic shrinkage of Ledoit/Wolf are applied. In the standard setting, i.i.d. data is assumed, however, in practice, time series typically exhibit strong autocorrelation structure, which introduces a pronounced estimation bias. Recent work by Sancetta has extended the shrinkage framework beyond i.i.d. data. We contribute in this work by showing that the Sancetta estimator, while being consistent in the high-dimensional limit, suffers from a high bias in finite sample sizes. We propose an alternative estimator, which is (1) unbiased, (2) less sensitive to hyperparameter choice and (3) yields superior performance in simulations on toy data and on a real world data set from an EEG-based Brain-Computer-Interfacing experiment.	[Bartz, Daniel] TU Berlin, Dept Comp Sci, Berlin, Germany; [Mueller, Klaus-Robert] TU Berlin, Berlin, Germany; [Mueller, Klaus-Robert] Korea Univ, Seoul, South Korea	Technical University of Berlin; Technical University of Berlin; Korea University	Bartz, D (corresponding author), TU Berlin, Dept Comp Sci, Berlin, Germany.	daniel.bartz@tu-berlin.de; klaus-robert.mueller@tu-berlin.de	Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685	National Research Foundation - Korean government [2012-005741]	National Research Foundation - Korean government(Korean Government)	This research was also supported by the National Research Foundation grant (No. 2012-005741) funded by the Korean government. We thank Johannes Hohne, Sebastian Bach and Duncan Blythe for valuable discussions and comments.	ANDREWS DWK, 1991, ECONOMETRICA, V59, P817, DOI 10.2307/2938229; Bartz D, 2013, ADV NEURAL INFORM PR, P1869; Blankertz B., 2007, ADV NEURAL INFORM PR, P113; Blankertz B, 2008, IEEE SIGNAL PROC MAG, V25, P41, DOI 10.1109/MSP.2008.4408441; Blankertz B, 2011, NEUROIMAGE, V56, P814, DOI 10.1016/j.neuroimage.2010.06.048; Blankertz B, 2010, NEUROIMAGE, V51, P1303, DOI 10.1016/j.neuroimage.2010.03.022; Chen YL, 2010, IEEE T SIGNAL PROCES, V58, P5016, DOI 10.1109/TSP.2010.2053029; Edelman A, 2005, ACT NUMERIC, V14, P233, DOI 10.1017/S0962492904000236; Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027; Hastie T., 2008, ELEMENTS STAT LEARNI; Ledoit O, 2004, J MULTIVARIATE ANAL, V88, P365, DOI 10.1016/S0047-259X(03)00096-4; Ledoit O, 2012, ANN STAT, V40, P1024, DOI 10.1214/12-AOS989; Lotte F, 2011, IEEE T BIO-MED ENG, V58, P355, DOI 10.1109/TBME.2010.2082539; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; Samek W., 2013, ADV NEURAL INFORM PR, P1007; Sancetta A, 2008, J MULTIVARIATE ANAL, V99, P949, DOI 10.1016/j.jmva.2007.06.004; Schafer J, 2005, STAT APPL GENET MO B, V4, DOI 10.2202/1544-6115.1175; Silverstein J. W., 2010, SPECTRAL ANAL LARGE; Stein C., 1956, P BERK S MATH STAT P, V1, P197, DOI DOI 10.1525/9780520313880-018; THIEBAUX HJ, 1984, J CLIM APPL METEOROL, V23, P800, DOI 10.1175/1520-0450(1984)023<0800:TIAEOE>2.0.CO;2	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101070
C	Berg-Kirkpatrick, T; Andreas, J; Klein, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Berg-Kirkpatrick, Taylor; Andreas, Jacob; Klein, Dan			Unsupervised Transcription of Piano Music	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present a new probabilistic model for transcribing piano music from audio to a symbolic form. Our model reflects the process by which discrete musical events give rise to acoustic signals that are then superimposed to produce the observed data. As a result, the inference procedure for our model naturally resolves the source separation problem introduced by the the piano's polyphony. In order to adapt to the properties of a new instrument or acoustic environment being transcribed, we learn recording-specific spectral profiles and temporal envelopes in an unsupervised fashion. Our system outperforms the best published approaches on a standard piano transcription task, achieving a 10.6% relative gain in note onset F-1 on real piano audio.	[Berg-Kirkpatrick, Taylor; Andreas, Jacob; Klein, Dan] Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Berg-Kirkpatrick, T (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.	tberg@cs.berkeley.edu; jda@cs.berkeley.edu; klein@cs.berkeley.edu						[Anonymous], 2014, INT MUSIC SCORE LIB; Benetos Emmanouil, 2013, EXPLICIT DURATION HI; Besag Julian, 1986, J ROYAL STAT SOC; Bock Sebastian, 2012, IEEE INT C AC SPEECH; Emiya Valentin, 2010, IEEE T AUDIO SPEECH; Kivinen J., 1997, INFORM COMPUTATION; Levinson Stephen, 1986, COMPUTER SPEECH LANG; Lienhart R., 2009, MULTIMEDIA EXP ICME; Nakano Masahiro, 2012, IEEE INT C AC SPEECH; O'Hanlon K, 2014, INT CONF ACOUST SPEE; Peeling Paul H., 2010, IEEE T AUDIO SPEECH; Poliner GE, 2007, EURASIP J ADV SIG PR, DOI 10.1155/2007/48317; Ryynanen Matti P., 2005, IEEE WORKSH APPL SIG; Vincent Emmanuel, 2010, IEEE T AUDIO SPEECH; Viro Vladimir, 2011, PEACHNOTE MUSIC SCOR; Weninger Felix, 2013, IEEE INT C AC SPEECH	16	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101103
C	Buesing, L; Machado, TA; Cunningham, JP; Paninski, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Buesing, Lars; Machado, Timothy A.; Cunningham, John P.; Paninski, Liam			Clustered factor analysis of multineuronal spike data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CONNECTIVITY; MODEL	High-dimensional, simultaneous recordings of neural spiking activity are often explored, analyzed and visualized with the help of latent variable or factor models. Such models are however ill-equipped to extract structure beyond shared, distributed aspects of firing activity across multiple cells. Here, we extend unstructured factor models by proposing a model that discovers subpopulations or groups of cells from the pool of recorded neurons. The model combines aspects of mixture of factor analyzer models for capturing clustering structure, and aspects of latent dynamical system models for capturing temporal dependencies. In the resulting model, we infer the subpopulations and the latent factors from data using variational inference and model parameters are estimated by Expectation Maximization (EM). We also address the crucial problem of initializing parameters for EM by extending a sparse subspace clustering algorithm to integer-valued spike count observations. We illustrate the merits of the proposed model by applying it to calcium-imaging data from spinal cord neurons, and we show that it uncovers meaningful clustering structure in the data.	[Buesing, Lars; Machado, Timothy A.; Cunningham, John P.; Paninski, Liam] Columbia Univ, Ctr Theoret Neurosci, Dept Stat, New York, NY 10027 USA; [Buesing, Lars; Machado, Timothy A.; Cunningham, John P.; Paninski, Liam] Columbia Univ, Grossman Ctr Stat Mind, New York, NY USA; [Machado, Timothy A.] Columbia Univ, Howard Hughes Med Inst, New York, NY 10032 USA; [Machado, Timothy A.] Columbia Univ, Dept Neurosci, New York, NY USA	Columbia University; Columbia University; Columbia University; Howard Hughes Medical Institute; Columbia University	Buesing, L (corresponding author), Columbia Univ, Ctr Theoret Neurosci, Dept Stat, New York, NY 10027 USA.	lars@stat.columbia.edu; cunningham@stat.columbia.edu; liam@stat.columbia.edu			Simons Foundation (SCGB) [325171, 325233]; Grossman Center at Columbia University; Gatsby Charitable Trust; ARO [MURI W911NF-12-1-0594]; ONR [vN00014-14-1-0243]; DARPA [W91NF-14-1-0269]; NSF CAREER award	Simons Foundation (SCGB); Grossman Center at Columbia University; Gatsby Charitable Trust; ARO; ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work was supported by Simons Foundation (SCGB#325171 and SCGB#325233), Grossman Center at Columbia University, and Gatsby Charitable Trust as well as grants MURI W911NF-12-1-0594 from the ARO, vN00014-14-1-0243 from the ONR, W91NF-14-1-0269 from DARPA and an NSF CAREER award (L.P.).	Arthur D, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1027; Buesing Lars, 2012, NIPS, P1691; Collins M., 2001, ADV NEURAL INFORM PR, V13, P23; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Gerhard F, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003138; Ghahramani Zoubin, 1996, CRGTR961 U TOR; Jones LM, 2007, P NATL ACAD SCI USA, V104, P18772, DOI 10.1073/pnas.0705546104; Keshri S., 2013, ARXIV13093724; Khan Emtiyaz, 2013, P 30 INT C MACH LEAR, P951; Machado Timothy A., 2014, 79 COLD SPRING HARB; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Mishchenko Y, 2011, ANN APPL STAT, V5, P1229, DOI 10.1214/09-AOAS303; Ng AY, 2002, ADV NEUR IN, V14, P849; Okatan M, 2005, NEURAL COMPUT, V17, P1927, DOI 10.1162/0899766054322973; Paninski L., 2013, ADV NEURAL INF PROCE, V26, P2391; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Pnevmatikakis E. A., 2014, ARXIV E PRINTS; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Tipping ME, 1999, NEURAL COMPUT, V11, P443, DOI 10.1162/089976699300016728; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Yu Byron M., 2008, NIPS, P1881	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101010
C	Chakrabarty, D; Jain, P; Kothari, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chakrabarty, Deeparnab; Jain, Prateek; Kothari, Pravesh			Provable Submodular Minimization using Wolfe's Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					submodular function minimization ( SFM) has become a critical problem. Theoretically, unconstrained SFM can be performed in polynomial time [10, 11]. However, these algorithms are typically not practical. In 1976, Wolfe [21] proposed an algorithm to find the minimum Euclidean norm point in a polytope, and in 1980, Fujishige [3] showed how Wolfe's algorithm can be used for SFM. For general submodular functions, this Fujishige-Wolfe minimum norm algorithm seems to have the best empirical performance. Despite its good practical performance, very little is known about Wolfe's minimum norm algorithm theoretically. To our knowledge, the only result is an exponential time analysis due to Wolfe [21] himself. In this paper we give a maiden convergence analysis of Wolfe's algorithm. We prove that in t iterations, Wolfe's algorithm returns an O(1/t)-approximate solution to the min-norm point on any polytope. We also prove a robust version of Fujishige's theorem which shows that an O(1/n(2)) approximate solution to the min-norm point on the base polytope implies exact submodular minimization. As a corollary, we get the first pseudo-polynomial time guarantee for the Fujishige-Wolfe minimum norm algorithm for unconstrained submodular function minimization.	[Chakrabarty, Deeparnab; Jain, Prateek; Kothari, Pravesh] Microsoft Res, 9 Lavelle Rd, Bangalore 560001, Karnataka, India; [Kothari, Pravesh] Univ Texas Austin, Austin, TX 78712 USA	Microsoft; University of Texas System; University of Texas Austin	Chakrabarty, D (corresponding author), Microsoft Res, 9 Lavelle Rd, Bangalore 560001, Karnataka, India.							Bach F., 2010, ABS10104207 CORR; Edmonds Jack, 1970, COMBINATORIAL STRUCT, P69; FUJISHIGE S, 1980, MATH OPER RES, V5, P186, DOI 10.1287/moor.5.2.186; FUJISHIGE S, 2006, MINIMUM NORM POINT A; Fujishige S, 2011, PAC J OPTIM, V7, P3; Fujishige Satoru, 1984, MATH PROGRAMMING STU; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Iwata S, 2001, J ACM, V48, P761, DOI 10.1145/502090.502096; Iwata S, 2000, INT OFFSHORE POLAR E, P97, DOI 10.1145/335305.335317; Iwata S, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1230; Iyer R., 2013, P 30 INT C MACH LEAR, P855; Iyer Rishabh, 2013, ABS13112110 CORR; Iyer Rishabh K., 2013, P 8 C WORKSHOP NEURA, P2436; Jegelka S., 2013, ADV NEURAL INFO PROC, P1313; Jegelka S., 2011, ADV NEURAL INFORM PR, V24, P460; Kohli P, 2010, STUD COMPUT INTELL, V285, P51; Krause A, 2008, J MACH LEARN RES, V9, P235; Schrijver A, 2000, J COMB THEORY B, V80, P346, DOI 10.1006/jctb.2000.1989; Stobbe P., 2010, ADV NEURAL INFO PROC, P2208; WOLFE P, 1976, MATH PROGRAM, V11, P128, DOI 10.1007/BF01580381	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100100
C	Chan, H; Ortiz, LE		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chan, Hau; Ortiz, Luis E.			Computing Nash Equilibria in Generalized Interdependent Security Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study the computational complexity of computing Nash equilibria in generalized interdependent-security (IDS) games. Like traditional IDS games, originally introduced by economists and risk-assessment experts Heal and Kunreuther about a decade ago, generalized IDS games model agents' voluntary investment decisions when facing potential direct risk and transfer-risk exposure from other agents. A distinct feature of generalized IDS games, however, is that full investment can reduce transfer risk. As a result, depending on the transfer-risk reduction level, generalized IDS games may exhibit strategic complementarity (SC) or strategic substitutability (SS). We consider three variants of generalized IDS games in which players exhibit only SC, only SS, and both SC+SS. We show that determining whether there is a pure-strategy Nash equilibrium (PSNE) in SC+SS-type games is NP-complete, while computing a single PSNE in SC-type games takes worst-case polynomial time. As for the problem of computing all mixed-strategy Nash equilibria (MSNE) efficiently, we produce a partial characterization. Whenever each agent in the game is indiscriminate in terms of the transfer-risk exposure to the other agents, a case that Kearns and Ortiz originally studied in the context of traditional IDS games in their NIPS 2003 paper, we can compute all MSNE that satisfy some ordering constraints in polynomial time in all three game variants. Yet, there is a computational barrier in the general (transfer) case: we show that the computational problem is as hard as the Pure-Nash-Extension problem, also originally introduced by Kearns and Ortiz, and that it is NP-complete for all three variants. Finally, we experimentally examine and discuss the practical impact that the additional protection from transfer risk allowed in generalized IDS games has on MSNE by solving several randomly-generated instances of SC+SS-type games with graph structures taken from several real-world datasets.	[Chan, Hau; Ortiz, Luis E.] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook	Chan, H (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	hauchan@cs.stonybrook.edu; leortiz@cs.stonybrook.edu			NSF Graduate Research Fellowship; NSF CAREER Award [IIS-1054541]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	This material is based upon work supported by an NSF Graduate Research Fellowship (first author) and an NSF CAREER Award IIS-1054541 (second author).	[Anonymous], 2001, UAI 01; Chan H., 2012, P 28 C UNC ART INT U, P152; Daskalakis C., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P71, DOI 10.1145/1132516.1132527; Elkind Edith, 2006, P 7 ACM C EL COMM, P100; Fudenberg D., 1998, THEORY LEARNING GAME, V1; Garey M.R., 1979, COMPUTERS INTRACTABI; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; Gkonis KG, 2010, J TRANSP SECUR, V3, P197, DOI 10.1007/s12198-010-0047-y; Gottlob G., 2003, P TARK, P215; Heal G, 2005, J CONFLICT RESOLUT, V49, P201, DOI 10.1177/0022002704272833; Heal G., 2005, WORKING PAPER; Heal Geoffrey, 2004, 10706 NAT BUR EC RES; Kearns M, 2004, ADV NEUR IN, V16, P561; Klimt B., 2004, CEAS; Knuth DE., 1993, STANFORD GRAPH BASE; Laszka A, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2635673; Leskovec J, 2010, CHI2010: PROCEEDINGS OF THE 28TH ANNUAL CHI CONFERENCE ON HUMAN FACTORS IN COMPUTING SYSTEMS, VOLS 1-4, P1361; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Nisan N, 2007, ALGORITHMIC GAME THEORY, P1, DOI 10.1017/CBO9780511800481; Shoham Y., 2009, MULTIAGENT SYSTEMS A; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Xi Chen, 2009, J ACM, V56; ZACHARY WW, 1977, J ANTHROPOL RES, V33, P452, DOI 10.1086/jar.33.4.3629752	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103036
C	Chang, J; Fisher, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chang, Jason; Fisher, John W., III			Parallel Sampling of HDPs using Sub-Cluster Splits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We develop a sampling technique for Hierarchical Dirichlet process models. The parallel algorithm builds upon [1] by proposing large split and merge moves based on learned sub-clusters. The additional global split and merge moves drastically improve convergence in the experimental results. Furthermore, we discover that cross-validation techniques do not adequately determine convergence, and that previous sampling methods converge slower than were previously expected.	[Chang, Jason; Fisher, John W., III] MIT, CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Chang, J (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.	jchang7@csail.mit.edu; fisher@csail.mit.edu			Office of Naval Research Multidisciplinary Research Initiative program [N000141110688]; VITALITE from the Army Research Office Multidisciplinary Research Initiative program [W911NF-11-1-0391]	Office of Naval Research Multidisciplinary Research Initiative program(Office of Naval Research); VITALITE from the Army Research Office Multidisciplinary Research Initiative program	This research was partially supported by the Office of Naval Research Multidisciplinary Research Initiative program, award N000141110688 and by VITALITE, which receives support from Army Research Office Multidisciplinary Research Initiative program, award W911NF-11-1-0391.	ANTONIAK CE, 1974, ANN STAT, V2, P1152, DOI 10.1214/aos/1176342871; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bryant M., 2012, ADV NEURAL INFORM PR; Chang J., 2013, ADV NEURAL INFORM PR; DAHL DB, 2003, TECHNICAL REPORT; Fox E. B., 2008, INT C MACH LEARN; Gal Y., 2013, WORKSH BIG LEARN NIP; Green PJ, 2001, SCAND J STAT, V28, P355, DOI 10.1111/1467-9469.00242; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Ishwaran H, 2002, CAN J STAT, V30, P269, DOI 10.2307/3315951; Jain S, 2004, J COMPUT GRAPH STAT, V13, P158, DOI 10.1198/1061860043001; Jain S, 2007, BAYESIAN ANAL, V2, P445, DOI 10.1214/07-BA219; Johnson MJ, 2013, ADV NEURAL INFORM PR; Lichman M, 2013, UCI MACHINE LEARNING; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Newman D, 2009, J MACH LEARN RES, V10, P1801; Niu F., 2011, P 24 INT C NEUR INF; SETHURAMAN J, 1994, STAT SINICA, V4, P639; Sudderth Erik B, 2006, THESIS; Teh Y. W., 2008, ADV NEURAL INFORM PR, V20; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Wang C., 2012, ARXIV12071657STATML; Williamson S., 2013, INT C MACH LEARN	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100014
C	Chatterjee, S; Chen, S; Banerjee, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chatterjee, Soumyadeep; Chen, Sheng; Banerjee, Arindam			Generalized Dantzig Selector: Application to the k-support norm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SHRINKAGE	We propose a Generalized Dantzig Selector (GDS) for linear models, in which any norm encoding the parameter structure can be leveraged for estimation. We investigate both computational and statistical aspects of the GDS. Based on conjugate proximal operator, a flexible inexact ADMM framework is designed for solving GDS. Thereafter, non-asymptotic high-probability bounds are established on the estimation error, which rely on Gaussian widths of the unit norm ball and the error set. Further, we consider a non-trivial example of the GDS using k-support norm. We derive an efficient method to compute the proximal operator for k-support norm since existing methods are inapplicable in this setting. For statistical analysis, we provide upper bounds for the Gaussian widths needed in the GDS analysis, yielding the first statistical recovery guarantee for estimation with the k-support norm. The experimental results confirm our theoretical analysis.	[Chatterjee, Soumyadeep; Chen, Sheng; Banerjee, Arindam] Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Chatterjee, S (corresponding author), Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.	chatter@cs.umn.edu; shengc@cs.umn.edu; banerjee@cs.umn.edu	Chatterjee, Soumyadeep/AAV-1995-2020		NSF [IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711]; NASA [NNX12AQ39A]	NSF(National Science Foundation (NSF)); NASA(National Aeronautics & Space Administration (NASA))	The research was supported by NSF grants IIS-1447566, IIS-1422557, CCF-1451986, CNS-1314560, IIS-0953274, IIS-1029711, and by NASA grant NNX12AQ39A.	Argyriou A., 2012, ADV NEURAL INFORM PR, P1466; Banerjee A., 2014, NIPS; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Chatterjee Soumyadeep, 2014, ARXIV E PRINTS; Gordon Y, 2007, J APPROX THEORY, V149, P59, DOI 10.1016/j.jat.2007.04.007; James GM, 2009, BIOMETRIKA, V96, P323, DOI 10.1093/biomet/asp013; James GM, 2009, J R STAT SOC B, V71, P127, DOI 10.1111/j.1467-9868.2008.00668.x; Jiu J., 2010, J MACH LEARN RES, V9, P461; Lu ZS, 2012, COMPUT STAT DATA AN, V56, P4037, DOI 10.1016/j.csda.2012.04.019; McDonald Andrew M., 2014, ARXIV E PRINTS; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Parikh Neal, 2014, Foundations and Trends in Optimization, V1, P127, DOI 10.1561/2400000003; Rao N., 2012, P 15 INT C ART INT S, P942; Rudelson M, 2008, COMMUN PUR APPL MATH, V61, P1025, DOI 10.1002/cpa.20227; Stewart Ian, 2003, CHAPMAN HALL CRC MAT; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang H., 2014, NIPS; Wang Xiangfeng, 2012, SIAM J SCI COMPUTING, V34; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	23	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100074
C	Chen, WZ; Wang, ZH; Zhou, JR		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chen, Weizhu; Wang, Zhenghao; Zhou, Jingren			Large-scale L-BFGS using MapReduce	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					L-BFGS has been applied as an effective parameter estimation method for various machine learning algorithms since 1980s. With an increasing demand to deal with massive instances and variables, it is important to scale up and parallelize L-BFGS effectively in a distributed system. In this paper, we study the problem of parallelizing the L-BFGS algorithm in large clusters of tens of thousands of shared-nothing commodity machines. First, we show that a naive implementation of L-BFGS using Map-Reduce requires either a significant amount of memory or a large number of map-reduce steps with negative performance impact. Second, we propose a new L-BFGS algorithm, called Vector-free L-BFGS, which avoids the expensive dot product operations in the two loop recursion and greatly improves computation efficiency with a great degree of parallelism. The algorithm scales very well and enables a variety of machine learning algorithms to handle a massive number of variables over large datasets. We prove the mathematical equivalence of the new Vector-free L-BFGS and demonstrate its excellent performance and scalability using real-world machine learning problems with billions of variables in production clusters.	[Chen, Weizhu; Wang, Zhenghao; Zhou, Jingren] Microsoft, Albuquerque, NM 87107 USA	Microsoft	Chen, WZ (corresponding author), Microsoft, Albuquerque, NM 87107 USA.	wzchen@microsoft.com; zhwang@microsoft.com; jrzhou@microsoft.com	Wang, Zhenghao/HGB-9714-2022; Wang, Zhenghao/HGB-9731-2022					Agarwal A, 2014, J MACH LEARN RES, V15, P1111; Andrew G., 2007, P 24 INT C MACH LEAR, V24, P33, DOI DOI 10.1145/1273496.1273501; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Chu CT., 2007, P 19 INT C NEURAL IN, P281, DOI DOI 10.1234/12345678; Daume III H, 2004, NOTES CG LM BFGS OPT; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Gopal S, 2013, P 30 INT C MACH LEAR, V28, P287; Graepel T., 2010, P 27 INT C MACHINE L, P13; Langford J., 2009, PROC 22 INT C NEURAL, P2331; Ling CX, 2003, P 18 INT C ART INT I, P329; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Low Y, 2010, UNCERTAINTY ARTIFICI; NASH SG, 1989, MATH PROGRAM, V45, P529, DOI 10.1007/BF01589117; Nocedal J, 1999, SPRINGER SERIES OPER, V43; Nocedal Jorge, 1980, UPDATING QUASINEWTON; Schraudolph N. N., 2007, PROC 11 INT C ARTIF, P436; Shanno DF, 1985, J OPTIMIZATION THEOR; Teo C. H., 2007, ACM SIGKDD C KNOWL D; Weinberger K., 2009, INT C MACH LEARN; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	21	1	1	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101004
C	Chwialkowski, K; Sejdinovic, D; Gretton, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Chwialkowski, Kacper; Sejdinovic, Dino; Gretton, Arthur			A Wild Bootstrap for Degenerate Kernel Tests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				U-STATISTICS	A wild bootstrap method for nonparametric hypothesis tests based on kernel distribution embeddings is proposed. This bootstrap method is used to construct provably consistent tests that apply to random processes, for which the naive permutation-based bootstrap fails. It applies to a large group of kernel tests based on V-statistics, which are degenerate under the null hypothesis, and nondegenerate elsewhere. To illustrate this approach, we construct a two-sample test, an instantaneous independence test and a multiple lag independence test for time series. In experiments, the wild bootstrap gives strong performance on synthetic examples, on audio data, and in performance benchmarking for the Gibbs sampler.	[Chwialkowski, Kacper] UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England; [Sejdinovic, Dino; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London; University of London; University College London	Chwialkowski, K (corresponding author), UCL, Dept Comp Sci, Gower St, London WC1E 6BT, England.	kacper.chwialkowski@gmail.com; dino.sejdinovic@gmail.com; arthur.gretton@gmail.com						Arcones MA, 1998, ELECTRON COMMUN PROB, V3, P13, DOI 10.1214/ECP.v3-988; Bauwens L, 2006, J APPL ECONOMET, V21, P79, DOI 10.1002/jae.842; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; BESSERVE M, 2013, NIPS, P2535; Borisov I.S., 2008, SIBERIAN ADV MATH, V11, p[25, 242]; Borovkova S, 2001, T AM MATH SOC, V353, P4261, DOI 10.1090/S0002-9947-01-02819-7; Bradley RC, 2005, PROBAB SURV, V2, P107, DOI 10.1214/154957805100000104; Chwialkowski K., 2014, ICML; Chwialkowski Kacper, 2014, ARXIV14085404; Dedecker J, 2005, PROBAB THEORY REL, V132, P203, DOI 10.1007/s00440-004-0394-3; Dedecker J, 2007, WEAK DEPENDENCE EXAM, V190; Doukhan P., 1994, MIXING; Dudley R. M., 2002, CAMBRIDGE STUDIES AD, V74; Fukumizu K., 2007, ADV NEURAL INF PROCE, P489; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Gretton A., 2007, ADV NEURAL INF PROCE, V20, P585; Gretton A, 2012, J MACH LEARN RES, V13, P723; Harchaoui Z., 2008, NIPS; Hehrmann P., 2011, THESIS; Leucht A, 2013, J MULTIVARIATE ANAL, V117, P257, DOI 10.1016/j.jmva.2013.03.003; Leucht A, 2012, BERNOULLI, V18, P552, DOI 10.3150/11-BEJ354; Lyons R, 2013, ANN PROBAB, V41, P3051; PICKANDS J, 1975, ANN STAT, V3, P119; Sejdinovic D., 2014, ICML; Sejdinovic D., 2013, ADV NEURAL INFORM PR, P1124; Sejdinovic D, 2013, ANN STAT, V41, P2263, DOI 10.1214/13-AOS1140; Serfling R., 1980, APPROXIMATION THEORE; Shao XF, 2010, J AM STAT ASSOC, V105, P218, DOI 10.1198/jasa.2009.tm08744; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Sugiyama M, 2011, NEURAL NETWORKS, V24, P735, DOI 10.1016/j.neunet.2011.04.003; Zhang K., 2011, P C UNCERTAINTY ARTI, P804; Zhang X., 2008, NIPS, V22	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102013
C	Cohen, H; Crammer, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Cohen, Haim; Crammer, Koby			Learning Multiple Tasks in Parallel with a Shared Annotator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	We introduce a new multi-task framework, in which K online learners are sharing a single annotator with limited bandwidth. On each round, each of the K learners receives an input, and makes a prediction about the label of that input. Then, a shared (stochastic) mechanism decides which of the K inputs will be annotated. The learner that receives the feedback (label) may update its prediction rule, and then we proceed to the next round. We develop an online algorithm for multi-task binary classification that learns in this setting, and bound its performance in the worst-case setting. Additionally, we show that our algorithm can be used to solve two bandits problems: contextual bandits, and dueling bandits with context, both allow to decouple exploration and exploitation. Empirical study with OCR data, vowel prediction (VJ project) and document classification, shows that our algorithm outperforms other algorithms, one of which uses uniform allocation, and essentially achieves more (accuracy) for the same labour of the annotator.	[Cohen, Haim; Crammer, Koby] Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Cohen, H (corresponding author), Technion Israel Inst Technol, Dept Elect Engn, IL-32000 Haifa, Israel.	hcohen@tx.technion.ac.il; koby@ee.technion.ac.il						Agarwal A., 2008, UCBEECS2008138; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Avner Orly, 2012, ICML; Cavallanti G, 2010, J MACH LEARN RES, V11, P2901; Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P31; Cesa-Bianchi N, 2006, J MACH LEARN RES, V7, P1205; Cesa-Bianchi Nicolo, 2009, ICML; Crammer K, 2013, MACH LEARN, V90, P347, DOI 10.1007/s10994-012-5321-8; Crammer K, 2012, J MACH LEARN RES, V13, P1891; Crammer Koby, 2012, ADV NEURAL INFORM PR; Daume III Hal, 2010, DANLP 2010; Dekel O., 2010, COLT, P346; Dekel O, 2006, LECT NOTES ARTIF INT, V4005, P453, DOI 10.1007/11776420_34; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Furnkranz J, 2002, J MACH LEARN RES, V2, P721, DOI 10.1162/153244302320884605; Hazan Elad, 2011, ADV NEURAL INFORM PR; Kakade S. M., 2008, ICML 2008, P440; Lin  Hui, 2009, 10 ANN C INT SPEECH; Lugosi Gabor, 2009, COLT; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Shivaswamy Pannagadatta K., 2011, CORR; Tong S., 2000, P 17 INT C MACH LEAR, P999; Yu Jia Yuan, 2009, ICML; Yue Y., 2011, P 28 INT C MACH LEAR, P241; Yue YS, 2012, J COMPUT SYST SCI, V78, P1538, DOI 10.1016/j.jcss.2011.12.028; Yue Yisong, 2009, COLT	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103080
C	Cui, Z; Chang, H; Shan, SG; Chen, XL		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Cui, Zhen; Chang, Hong; Shan, Shiguang; Chen, Xilin			Generalized Unsupervised Manifold Alignment	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				DIMENSIONALITY REDUCTION	In this paper, we propose a Generalized Unsupervised Manifold Alignment (GUMA) method to build the connections between different but correlated datasets without any known correspondences. Based on the assumption that datasets of the same theme usually have similar manifold structures, GUMA is formulated into an explicit integer optimization problem considering the structure matching and preserving criteria, as well as the feature comparability of the corresponding points in the mutual embedding space. The main benefits of this model include: (1) simultaneous discovery and alignment of manifold structures; (2) fully unsupervised matching without any pre-specified correspondences; (3) efficient iterative alignment without computations in all permutation cases. Experimental results on dataset matching and real-world applications demonstrate the effectiveness and the practicability of our manifold alignment method.	[Cui, Zhen; Chang, Hong; Shan, Shiguang; Chen, Xilin] Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing, Peoples R China; [Cui, Zhen] Huaqiao Univ, Sch Comp Sci & Technol, Xiamen, Peoples R China	Chinese Academy of Sciences; Institute of Computing Technology, CAS; Huaqiao University	Cui, Z (corresponding author), Chinese Acad Sci, Key Lab Intelligent Informat Proc, Inst Comp Technol, Beijing, Peoples R China.	zhen.cui@vipl.ict.ac.cn; hong.chang@vipl.ict.ac.cn; sgshan@ict.ac.cn; xlchen@ict.ac.cn			Natural Science Foundation of China [61272319, 61222211, 61202297, 61390510]	Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is partially supported by Natural Science Foundation of China under contracts Nos. 61272319, 61222211, 61202297, and 61390510.	Bay H., 2006, ECCV; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Brualdi Richard A., 2006, COMBINATORIAL MATRIX; Cevikalp H., 2010, CVPR; Cui Z., 2013, CVPR; Cui Z., IEEE T CYBERNETICS; Cui Z., 2012, CVPR; Fernando B., 2013, ICCV; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Gong B., 2012, CVPR; Gopalan R., 2011, P 2011 INT C COMP VI; Griffin G., 2007, CALTECH 256 OBJECT C; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Haghighi A, 2008, ACL 2008, P771; Ham J., 2005, UAI; Hardoon DR, 2004, NEURAL COMPUT, V16, P2639, DOI 10.1162/0899766042321814; Hu Y., 2011, CVPR; Kim T., 2007, T PAMI; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1784, DOI 10.1109/TPAMI.2006.223; MUNKRES J, 1957, J SOC IND APPL MATH, V5, P32, DOI 10.1137/0105003; Pei YR, 2012, IEEE T PATTERN ANAL, V34, P1658, DOI 10.1109/TPAMI.2011.229; Quadrianto N., 2009, NIPS; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Saenko K., 2010, ECCV, P2; Shi Y., 2012, ICML; SHON AP, 2005, NIPS; Sugar C. A., 2003, J AM STAT ASS, V98; Wang C, 2009, IJCAI; Wang C., 2011, IJCAI; Wang Chang, 2008, ICML; Wang R., 2008, CVPR; Wolf L., 2011, CVPR; Xiong L., 2007, ECML; Yamaguchi O., 1998, FGR	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103071
C	De Bock, J; De Campos, CP; Antonucci, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		De Bock, Jasper; De Campos, Cassio P.; Antonucci, Alessandro			Global Sensitivity Analysis for MAP Inference in Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study the sensitivity of a MAP configuration of a discrete probabilistic graphical model with respect to perturbations of its parameters. These perturbations are global, in the sense that simultaneous perturbations of all the parameters (or any chosen subset of them) are allowed. Our main contribution is an exact algorithm that can check whether the MAP configuration is robust with respect to given perturbations. Its complexity is essentially the same as that of obtaining the MAP configuration itself, so it can be promptly used with minimal effort. We use our algorithm to identify the largest global perturbation that does not induce a change in the MAP configuration, and we successfully apply this robustness measure in two practical scenarios: the prediction of facial action units with posed images and the classification of multiple real public data sets. A strong correlation between the proposed robustness measure and accuracy is verified in both scenarios.	[De Bock, Jasper] Univ Ghent, SYSTeMS, Ghent, Belgium; [De Campos, Cassio P.] Queens Univ, Belfast, Antrim, North Ireland; [Antonucci, Alessandro] IDSIA, Lugano, Switzerland	Ghent University; Queens University Belfast; Universita della Svizzera Italiana	De Bock, J (corresponding author), Univ Ghent, SYSTeMS, Ghent, Belgium.	jasper.debock@ugent.be; c.decampos@qub.ac.uk; alessandro@idsia.ch	De Bock, Jasper/AAG-9883-2020		Research Foundation Flanders (FWO); Swiss NSF [200021_146606 / 1]	Research Foundation Flanders (FWO)(FWO); Swiss NSF(Swiss National Science Foundation (SNSF))	J. De Bock is a PhD Fellow of the Research Foundation Flanders (FWO) and he wishes to acknowledge its financial support. The work of C. P. de Campos has been mostly performed while he was with IDSIA and has been partially supported by the Swiss NSF grant 200021_146606 / 1.	Asuncion A, 2007, UCI MACHINE LEARNING; Berger J. O, 1985, STAT DECISION THEORY, P118; BLACKMOND K, 1995, IEEE T SYST MAN CYB, V25, P901, DOI 10.1109/21.384252; Castillo E, 1997, IEEE T SYST MAN CY A, V27, P412, DOI 10.1109/3468.594909; Chan H, 2002, J ARTIF INTELL RES, V17, P265, DOI 10.1613/jair.967; Chan H., 2004, P 20 C UNC ART INT, P67; Chan H, 2005, 19TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI-05), P1300; Chan Hei, 2006, P 22 C UNC ART INT, P63; Dechter R., 2012, P AAAI 2012; Ekman P., 2002, FACIAL ACTION CODING; Kanade Takeo, 2000, P 4 IEEE INT C AUT F, P1, DOI [10.1109/AFGR.2000.840611, DOI 10.1109/AFGR.2000.840611]; Kjarulff U., 2000, P UNCERTAINLY ARTIFI, P317, DOI DOI 10.5555/2073946.2073984; Kwisthout J, 2011, INT J APPROX REASON, V52, P1452, DOI 10.1016/j.ijar.2011.08.003; Kwisthout JHP, 2011, LECT NOTES COMPUT SC, V6543, P356, DOI 10.1007/978-3-642-18381-2_30; Levi I., 1980, ENTERPRISE KNOWLEDGE; Lucey P., P CVPR WORKSH, P94; Pradhan M, 1996, ARTIF INTELL, V85, P363, DOI 10.1016/0004-3702(96)00002-1; Renooij S, 2008, INT J APPROX REASON, V49, P398, DOI 10.1016/j.ijar.2008.02.008; Walley P., 1991, STAT REASONING IMPRE	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102033
C	Dubey, A; Ho, QR; Williamson, S; Xing, EP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Dubey, Avinava; Ho, Qirong; Williamson, Sinead; Xing, Eric P.			Dependent nonparametric trees for dynamic hierarchical clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Hierarchical clustering methods offer an intuitive and powerful way to model a wide variety of data sets. However, the assumption of a fixed hierarchy is often overly restrictive when working with data generated over a period of time: We expect both the structure of our hierarchy, and the parameters of the clusters, to evolve with time. In this paper, we present a distribution over collections of time-dependent, infinite-dimensional trees that can be used to model evolving hierarchies, and present an efficient and scalable algorithm for performing approximate inference in such a model. We demonstrate the efficacy of our model and inference algorithm on both synthetic data and real-world document corpora.	[Dubey, Avinava; Xing, Eric P.] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Ho, Qirong] ASTAR, Inst Infocomm Res, Singapore, Singapore; [Williamson, Sinead] Univ Texas Austin, McCombs Sch Business, Austin, TX 78712 USA	Carnegie Mellon University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R); University of Texas System; University of Texas Austin	Dubey, A (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	akdubey@cs.cmu.edu; hoqirong@gmail.com; sinead.williamson@mccombs.utexas.edu; epxing@cs.cmu.edu	Ho, Qirong/AAB-8152-2022		NSF Big data [IIS1447676]; DARPA XDATA [FA87501220324]; NIH [GWAS R01GM087694]	NSF Big data; DARPA XDATA; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This research was supported by NSF Big data IIS1447676, DARPA XDATA FA87501220324 and NIH GWAS R01GM087694.	Adams R., 2010, ADV NEURAL INFORM PR; Ahmed A., 2008, SDM; Banerjee A., 1995, J MACHINE LEARNING R, V6, P1345; Blei D. M., 2004, ADV NEURAL INFORM PR; Blei DM, 2011, J MACH LEARN RES, V12, P2461; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Caron F., 2007, UAI; Gopal S., 2014, INT C MACH LEARN; Ho Qirong, 2012, P 21 INT C WORLD WID, P739; Kingman J, 1982, J APPL PROB A, V19, P27, DOI DOI 10.2307/3213548; Lin D., 2010, ADV NEURAL INFORM PR; MacEachern S. N., 1999, BAYESIAN STAT SCI; Neal RM, 2003, BAYESIAN STATISTICS 7, P619; Rodriguez A., 2008, J AM STAT ASS, V103; Sudderth E., 2008, ADV NEURAL INFORM PR; Wang X, 2006, KNOWLEDGE DISCOVERY; Williamson S., 2010, ARTIFICIAL INTELLIGE	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100011
C	Eslami, SMA; Tarlow, D; Kohli, P; Winn, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Eslami, S. M. Ali; Tarlow, Daniel; Kohli, Pushmeet; Winn, John			Just-In-Time Learning for Fast and Flexible Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Much of research in machine learning has centered around the search for inference algorithms that are both general-purpose and efficient. The problem is extremely challenging and general inference remains computationally expensive. We seek to address this problem by observing that in most specific applications of a model, we typically only need to perform a small subset of all possible inference computations. Motivated by this, we introduce just-in-time learning, a framework for fast and flexible inference that learns to speed up inference at run-time. Through a series of experiments, we show how this framework can allow us to combine the flexibility of sampling with the efficiency of deterministic message-passing.	[Eslami, S. M. Ali; Tarlow, Daniel; Kohli, Pushmeet; Winn, John] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Eslami, SMA (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	alie@microsoft.com; dtarlow@microsoft.com; pkohli@microsoft.com; jwinn@microsoft.com						[Anonymous], THESIS; Barthelme S., 2011, P 28 INT C MACH LEAR, P289; Caldararu Silvia, 2013, FILZBACH; Chen JM, 2008, PROCEEDINGS OF 2008 INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND CYBERNETICS, VOLS 1-7, P3056, DOI 10.1109/ICMLC.2008.4620932; Criminisi A., 2013, DECISION FORESTCOM; Daniel Munoz J., 2010, EUR C COMP VIS; Goodman N. D., 2008, UAI; Grosse R., 2013, ADV NEURAL INFORM PR, P2769; HEESS N, 2013, ADV NEURAL INFORM PR, V26, P3219; Lewis D. D., 1994, SIGIR '94. Proceedings of the Seventeenth Annual International ACM-SIGIR Conference on Research and Development in Information Retrieval, P3; Lobell DB, 2011, NAT CLIM CHANGE, V1, P42, DOI [10.1038/NCLIMATE1043, 10.1038/nclimate1043]; Minka T., 2001, THESIS MIT CAMBRIDGE; Minka Thomas, 2012, INFER NET; Ross S, 2011, PROC CVPR IEEE; Schlenker W, 2009, P NATL ACAD SCI USA, V106, P15594, DOI 10.1073/pnas.0906865106; Shapovalov R, 2013, PROC CVPR IEEE, P2985, DOI 10.1109/CVPR.2013.384; Smith Matthew J., 2014, B AM METEOROLOGICAL; Stan Development Team, 2014, STAN C LIB PROB SAMP; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, V27	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103046
C	Farajtabar, M; Du, N; Gomez-Rodriguez, M; Valera, I; Zha, HY; Song, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Farajtabar, Mehrdad; Du, Nan; Gomez-Rodriguez, Manuel; Valera, Isabel; Zha, Hongyuan; Song, Le			Shaping Social Activity by Incentivizing Users	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Events in an online social network can be categorized roughly into endogenous events, where users just respond to the actions of their neighbors within the network, or exogenous events, where users take actions due to drives external to the network. How much external drive should be provided to each user, such that the network activity can be steered towards a target state ? In this paper, we model social events using multivariate Hawkes processes, which can capture both endogenous and exogenous event intensities, and derive a time dependent linear relation between the intensity of exogenous events and the overall network activity. Exploiting this connection, we develop a convex optimization framework for determining the required level of external drive in order for the network to reach a desired activity level. We experimented with event data gathered from Twitter, and show that our method can steer the activity of the network more accurately than alternatives.	[Farajtabar, Mehrdad; Du, Nan; Zha, Hongyuan; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Gomez-Rodriguez, Manuel] MPI Software Syst, Saarbrucken, Germany; [Valera, Isabel] Univ Carlos III Madrid, Madrid, Spain	University System of Georgia; Georgia Institute of Technology; Universidad Carlos III de Madrid	Farajtabar, M (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	mehrdad@gatech.edu; dunan@gatech.edu; manuelgr@mpi-sws.org; ivalera@tsc.uc3m.es; fzha@cc.gatech.edu; lsong@cc.gatech.edu			NSF [IIS1116886]; NSF/NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS1350983]; Raytheon Faculty Fellowship; Plan Regional-Programas I+D of Comunidad de Madrid [AGES-CM S2010/BMD-2422]; Ministerio de Ciencia e Innovacion of Spain [TEC2009-14504-C02-00, 2010 CSD2008-00010]	NSF(National Science Foundation (NSF)); NSF/NIH BIGDATA; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Raytheon Faculty Fellowship; Plan Regional-Programas I+D of Comunidad de Madrid; Ministerio de Ciencia e Innovacion of Spain(Ministry of Science and Innovation, Spain (MICINN)Spanish Government)	This project was supported in part by NSF IIS1116886, NSF/NIH BIGDATA 1R01GM108341, NSF CAREER IIS1350983 and Raytheon Faculty Fellowship to Le Song. Isabel Valera acknowledge the support of Plan Regional-Programas I+D of Comunidad de Madrid (AGES-CM S2010/BMD-2422), Ministerio de Ciencia e Innovacion of Spain (project DEIPRO TEC2009-14504-C02-00 and program Consolider-Ingenio 2010 CSD2008-00010 COMONSENS).	Al-Mohy AH, 2011, SIAM J SCI COMPUT, V33, P488, DOI 10.1137/100788860; Blundell Charles, 2012, NIPS; Boyd S, 2004, CONVEX OPTIMIZATION; Cha Meeyoung, 2010, ICWSM; Chen W, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P199, DOI 10.1145/1557019.1557047; Dobson Ian, 2004, P 37 ANN HAW INT C S, P10, DOI DOI 10.1109/HICSS.2004.1265185; Du Nan, 2013, NIPS; Golub G. H., 2012, MATRIX COMPUTATIONS, V3; Gomez-Rodriguez Manuel, 2014, ICWSM; Harris T. E., 2002, THEORY BRANCHING PRO; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Iwata T, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P266; Kempe D., 2003, PROC 9 ACM SIGKDD IN, P137; Kingman John Frank Charles, 1992, POISSON PROCESSES, V3; Linderman Scott W, 2014, ARXIV14020914; Liniger T, 2009, THESIS; Marsan D, 2008, SCIENCE, V319, P1076, DOI 10.1126/science.1148783; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Rasmussen JG, 2013, METHODOL COMPUT APPL, V15, P623, DOI 10.1007/s11009-011-9272-5; Richardson M., 2002, P 9 ACM SIGKDD INT C, P61, DOI [DOI 10.1145/775047.775057, 10.1145/775047.775057]; Rodriguez Manuel G., 2012, ICML; SAAD Y, 1986, SIAM J SCI STAT COMP, V7, P856, DOI 10.1137/0907058; Valera Isabel, 2014, ARXIV14060516; Veen A, 2008, J AM STAT ASSOC, V103, P614, DOI 10.1198/016214508000000148; Yang Shuang-Hong, 2013, P 30 INT C MACH LEAR, V28; Zhou K., 2013, AISTATS; Zhou Ke, 2013, ICML; Zhuang J, 2002, J AM STAT ASSOC, V97, P369, DOI 10.1198/016214502760046925	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101036
C	Festa, D; Hennequin, G; Lengyel, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Festa, Dylan; Hennequin, Guillaume; Lengyel, Mate			Analog Memories in a Balanced Rate-Based Network of E-I Neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				GRADED-RESPONSE; SIMPLE CELLS; DYNAMICS; VARIABILITY; INHIBITION; PLASTICITY; EXCITATION; STABILITY	The persistent and graded activity often observed in cortical circuits is sometimes seen as a signature of autoassociative retrieval of memories stored earlier in synaptic efficacies. However, despite decades of theoretical work on the subject, the mechanisms that support the storage and retrieval of memories remain unclear. Previous proposals concerning the dynamics of memory networks have fallen short of incorporating some key physiological constraints in a unified way. Specifically, some models violate Dale's law (i.e. allow neurons to be both excitatory and inhibitory), while some others restrict the representation of memories to a binary format, or induce recall states in which some neurons fire at rates close to saturation. We propose a novel control-theoretic framework to build functioning attractor networks that satisfy a set of relevant physiological constraints. We directly optimize networks of excitatory and inhibitory neurons to force sets of arbitrary analog patterns to become stable fixed points of the dynamics The resulting networks operate in the balanced regime, are robust to corruptions of the memory cue as well as to ongoing noise, and incidentally explain the reduction of trial-to-trial variability following stimulus onset that is ubiquitously observed in sensory and motor cortices. Our results constitute a step forward in our understanding of the neural substrate of memory.	[Festa, Dylan; Hennequin, Guillaume; Lengyel, Mate] Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Cambridge, England	University of Cambridge	Festa, D (corresponding author), Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Cambridge, England.	df325@cam.ac.uk; gjeh2@cam.ac.uk; m.lengyel@eng.cam.ac.uk	Festa, Dylan/AAQ-9297-2021; Lengyel, Mate/A-6665-2013	Festa, Dylan/0000-0003-3803-1542; Lengyel, Mate/0000-0001-7266-0049	Wellcome Trust; European Union [269921]; Swiss National Science Foundation	Wellcome Trust(Wellcome TrustEuropean Commission); European Union(European Commission); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This work was supported by the Wellcome Trust (GH, ML), the European Union Seventh Framework Programme (FP7/20072013) under grant agreement no. 269921 (Brain-ScaleS) (DF, ML), and the Swiss National Science Foundation (GH).	Ahmadian Y, 2013, NEURAL COMPUT, V25, P1994, DOI 10.1162/NECO_a_00472; Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003; Anderson JS, 2000, SCIENCE, V290, P1968, DOI 10.1126/science.290.5498.1968; Battaglia FP, 1998, NEURAL COMPUT, V10, P431, DOI 10.1162/089976698300017827; BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Churchland MM, 2010, NAT NEUROSCI, V13, P369, DOI 10.1038/nn.2501; Deco G, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002395; Finn IM, 2007, NEURON, V54, P137, DOI 10.1016/j.neuron.2007.02.029; Goldberg JA, 2004, NEURON, V42, P489, DOI 10.1016/S0896-6273(04)00197-7; Hennequin G, 2014, NEURON, V82, P1394, DOI 10.1016/j.neuron.2014.04.045; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Johnson S. G., NLOPT NONLINEAR OPTI; Latham PE, 2004, NEURAL COMPUT, V16, P1385, DOI 10.1162/089976604323057434; Lefort S, 2009, NEURON, V61, P301, DOI 10.1016/j.neuron.2008.12.020; Lengyel M, 2005, NAT NEUROSCI, V8, P1677, DOI 10.1038/nn1561; Lengyel M, 2005, ADV NEURAL INFORM PR, V17, P769; Litwin-Kumar A, 2012, NAT NEUROSCI, V15, P1498, DOI 10.1038/nn.3220; Priebe NJ, 2005, NEURON, V45, P133, DOI 10.1016/j.neuron.2004.12.024; Priebe NJ, 2006, NAT NEUROSCI, V9, P552, DOI 10.1038/nn1660; Roudi Y, 2007, PLOS COMPUT BIOL, V3, P1679, DOI 10.1371/journal.pcbi.0030141; Roxin A, 2011, J NEUROSCI, V31, P16217, DOI 10.1523/JNEUROSCI.1677-11.2011; Song S, 2005, PLOS BIOL, V3, P507, DOI 10.1371/journal.pbio.0030068; TREVES A, 1991, NETWORK-COMP NEURAL, V2, P371, DOI 10.1088/0954-898X/2/4/004; TREVES A, 1990, PHYS REV A, V42, P2418, DOI 10.1103/PhysRevA.42.2418; Vanbiervliet J, 2009, SIAM J OPTIMIZ, V20, P156, DOI 10.1137/070704034; Vogels TP, 2011, SCIENCE, V334, P1569, DOI 10.1126/science.1211095	27	1	1	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101007
C	Feyereisl, J; Kwak, S; Son, J; Han, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Feyereisl, Jan; Kwak, Suha; Son, Jeany; Han, Bohyung			Object Localization based on Structural SVM using Privileged Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a structured prediction algorithm for object localization based on Support Vector Machines (SVMs) using privileged information. Privileged information provides useful high-level knowledge for image understanding and facilitates learning a reliable model even with a small number of training examples. In our setting, we assume that such information is available only at training time since it may be difficult to obtain from visual data accurately without human supervision. Our goal is to improve performance by incorporating privileged information into ordinary learning framework and adjusting model parameters for better generalization. We tackle object localization problem based on a novel structural SVM using privileged information, where an alternating loss-augmented inference procedure is employed to handle the term in the objective function corresponding to privileged information. We apply the proposed algorithm to the Caltech-UCSD Birds 200-2011 dataset, and obtain encouraging results suggesting further investigation into the benefit of privileged information in structured prediction.	[Feyereisl, Jan; Kwak, Suha; Son, Jeany; Han, Bohyung] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea	Pohang University of Science & Technology (POSTECH)	Feyereisl, J (corresponding author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.	thefillm@gmail.com; mercury3@postech.ac.kr; jeany@postech.ac.kr; bhhan@postech.ac.kr	Feyereisl, Jan/P-3740-2019	Feyereisl, Jan/0000-0002-3829-6105	ICT R&D program of MSIP/IITP [14-824-09-006, 14-824-09-014]; IT R&D Program of MKE/KEIT [10040246]	ICT R&D program of MSIP/IITP; IT R&D Program of MKE/KEIT	This work was supported partly by ICT R&D program of MSIP/IITP [14-824-09-006; 14-824-09-014] and IT R&D Program of MKE/KEIT (10040246).	Bay H, 2008, COMPUT VIS IMAGE UND, V110, P346, DOI 10.1016/j.cviu.2007.09.014; Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2; Chen Q, 2012, PROC CVPR IEEE, P3426, DOI 10.1109/CVPR.2012.6248083; Dai QY, 2012, PROC CVPR IEEE, P3322, DOI 10.1109/CVPR.2012.6248070; Duan LX, 2012, IEEE T PATTERN ANAL, V34, P465, DOI 10.1109/TPAMI.2011.114; Elhoseiny M., 2013, ICCV; Farhadi A., 2009, CVPR; Farrell R, 2011, IEEE I CONF COMP VIS, P161, DOI 10.1109/ICCV.2011.6126238; Ionescu C, 2009, IEEE I CONF COMP VIS, P1157, DOI 10.1109/ICCV.2009.5459346; Lacoste-Julien S., 2013, INT C MACH LEARN PML, P53; Lampert C. H., 2009, CVPR; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Li W, 2014, IEEE T PATTERN ANAL, V36, P1134, DOI 10.1109/TPAMI.2013.167; Pechyony D., 2010, ADV NEURAL INFORM PR, P1894; Sharmanska V, 2013, IEEE I CONF COMP VIS, P825, DOI 10.1109/ICCV.2013.107; Socher Richard, 2013, NEURIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vapnik V., 2008, P NATO WORKSH MIN MA, P3; Vapnik V.N., 2006, ESTIMATION DEPENDENC, VVolume 40; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; Wah C., 2011, TECHNICAL REPORT, P5; Wang G, 2013, IEEE T PATTERN ANAL, V35, P2442, DOI 10.1109/TPAMI.2013.58; Wang XY, 2012, INT C PATT RECOG, P3382; Wolf L., 2013, CVPR; Xia H, 2014, IEEE T PATTERN ANAL, V36, P536, DOI 10.1109/TPAMI.2013.149; YANG HM, 2013, IEEE FG, V37, P1	26	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103012
C	Fogel, F; D'Aspremont, A; Vojnovic, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Fogel, Fajwel; D'Aspremont, Alexandre; Vojnovic, Milan			SerialRank: Spectral Ranking using Seriation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ALGORITHMS	We describe a seriation algorithm for ranking a set of n items given pairwise comparisons between these items. Intuitively, the algorithm assigns similar rankings to items that compare similarly with all others. It does so by constructing a similarity matrix from pairwise comparisons, using seriation methods to reorder this matrix and construct a ranking. We first show that this spectral seriation algorithm recovers the true ranking when all pairwise comparisons are observed and consistent with a total order. We then show that ranking reconstruction is still exact even when some pairwise comparisons are corrupted or missing, and that seriation based spectral ranking is more robust to noise than other scoring methods. An additional benefit of the seriation formulation is that it allows us to solve semi-supervised ranking problems. Experiments on both synthetic and real datasets demonstrate that seriation based spectral ranking achieves competitive and in some cases superior performance compared to classical ranking methods.	[Fogel, Fajwel] Ecole Polytech, CMAP, Palaiseau, France; [D'Aspremont, Alexandre] Ecole Normale Super, CNRS & DI, Paris, France; [Vojnovic, Milan] Microsoft Res, Cambridge, England	Institut Polytechnique de Paris; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Microsoft	Fogel, F (corresponding author), Ecole Polytech, CMAP, Palaiseau, France.	fogel@cmap.polytechnique.fr; aspremon@ens.fr; milanv@microsoft.com			European Research Council starting grant (project SIPA); MSR-INRIA joint centre	European Research Council starting grant (project SIPA); MSR-INRIA joint centre	FF, AA and MV would like to acknowledge support from a European Research Council starting grant (project SIPA) and support from the MSR-INRIA joint centre.	Ailon Nir, 2011, ADV NEURAL INFORM PR, P810; Atkins JE, 1998, SIAM J COMPUT, V28, P297, DOI 10.1137/S0097539795285771; Blum A, 2000, THEOR COMPUT SCI, V235, P25, DOI 10.1016/S0304-3975(99)00181-4; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Cohen WW, 1998, ADV NEUR IN, V10, P451; Feige U, 2007, INFORM PROCESS LETT, V101, P26, DOI 10.1016/j.ipl.2006.07.009; Fogel F., 2013, ARXIV13064805; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; Herbrich R, 2006, ADV NEURAL INFORM PR, V19, P569; HUBER PJ, 1963, ANN MATH STAT, V34, P511, DOI 10.1214/aoms/1177704162; Hunter DR, 2004, ANN STAT, V32, P384; Jamieson Kevin G, 2011, ADV NEURAL INFORM PR, P2240; Joachims T., 2002, P 8 ACM SIGKDD INT C, P133, DOI [DOI 10.1145/775047.775067, 10.1145/775047.775067]; KEENER JP, 1993, SIAM REV, V35, P80, DOI 10.1137/1035004; Kendall MG, 1940, BIOMETRIKA, V31, P324; Kenyon-Mathieu C, 2007, ACM S THEORY COMPUT, P95, DOI 10.1145/1250790.1250806; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; KUCZYNSKI J, 1992, SIAM J MATRIX ANAL A, V13, P1094, DOI 10.1137/0613066; Luce R, 1959, INDIVIDUAL CHOICE BE; Negahban Sahand, 2012, NIPS, P2483; Page L., 1998, STANFORD CS TECHNICA; Wauthier Fabian L., 2013, P 30 INT C MACH LEAR	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100002
C	Fragkiadaki, K; Salas, M; Arbelaez, P; Malik, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Fragkiadaki, Katerina; Salas, Marta; Arbelaez, Pablo; Malik, Jitendra			Grouping-Based Low-Rank Trajectory Completion and 3D Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				THRESHOLDING ALGORITHM	Extracting 3D shape of deforming objects in monocular videos, a task known as non-rigid structure-from-motion (NRSfM), has so far been studied only on synthetic datasets and controlled environments. Typically, the objects to reconstruct are pre-segmented, they exhibit limited rotations and occlusions, or full-length trajectories are assumed. In order to integrate NRSfM into current video analysis pipelines, one needs to consider as input realistic -thus incomplete- tracking, and perform spatio-temporal grouping to segment the objects from their surroundings. Furthermore, NRSfM needs to be robust to noise in both segmentation and tracking, e.g., drifting, segmentation "leaking", optical flow "bleeding" etc. In this paper, we make a first attempt towards this goal, and propose a method that combines dense optical flow tracking, motion trajectory clustering and NRSfM for 3D reconstruction of objects in videos. For each trajectory cluster, we compute multiple reconstructions by minimizing the reprojection error and the rank of the 3D shape under different rank bounds of the trajectory matrix. We show that dense 3D shape is extracted and trajectories are completed across occlusions and low textured regions, even under mild relative motion between the object and the camera. We achieve competitive results on a public NRSfM benchmark while using fixed parameters across all sequences and handling incomplete trajectories, in contrast to existing approaches. We further test our approach on popular video segmentation datasets. To the best of our knowledge, our method is the first to extract dense object models from realistic videos, such as those found in Youtube or Hollywood movies, without object-specific priors.	[Fragkiadaki, Katerina; Malik, Jitendra] Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA; [Salas, Marta] Univ Zaragoza, Zaragoza, Spain; [Arbelaez, Pablo] Univ Los Andes, Bogota, Colombia	University of California System; University of California Berkeley; University of Zaragoza; Universidad de los Andes (Colombia)	Fragkiadaki, K (corresponding author), Univ Calif Berkeley, EECS, Berkeley, CA 94720 USA.	katef@berkeley.edu; msalasg@unizar.es; pa.arbelaez@uniandes.edu.co; malik@eecs.berkeley.edu			Direccion General de Investigacion of Spain [DPI2012-32168]; Ministerio de Educacion [FPU-AP2010-2906]	Direccion General de Investigacion of Spain(Spanish Government); Ministerio de Educacion	The authors would like to thank Philipos Modrohai for useful discussions. M.S. acknowledges funding from Direccion General de Investigacion of Spain under project DPI2012-32168 and the Ministerio de Educacion (scholarship FPU-AP2010-2906).	Akhter I., 2011, ACM T GRAPHICS; Akhter I, 2011, IEEE T PATTERN ANAL, V33, P1442, DOI 10.1109/TPAMI.2010.201; Andersen RA, 1998, TRENDS COGN SCI, V2, P222, DOI 10.1016/S1364-6613(98)01181-4; BAO SY, 2013, CVPR, P1264, DOI DOI 10.1109/CVPR.2013.167; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Brand M., 2005, CVPR; Bregler C., 2000, CVPR; Brox T., 2010, ECCV; Cabral R., 2013, ICCV; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Dai Y., 2012, IJCV; Galasso F., 2013, ICCV; Garg R., 2013, DENSE VARIATIONAL RE; Garg R, 2013, INT J COMPUT VISION, V104, P286, DOI 10.1007/s11263-012-0607-7; Gibson J.J., 1951, AM J PSYCHOL, V64, P622, DOI DOI 10.2307/1419017; Gotardo P.F.U., 2011, TPAMI, V33; Lucas B.D., 1981, P 1981 DARPA IM UND; Ma SQ, 2011, MATH PROGRAM, V128, P321, DOI 10.1007/s10107-009-0306-5; Marques M, 2009, COMPUT VIS IMAGE UND, V113, P261, DOI 10.1016/j.cviu.2008.09.004; Paladini M., 2012, IJCV, V96; Park H. S., 2010, ECCV; Russell Chris, 2014, ECCV; Simon T., 2014, ECCV; Sundaram N., 2010, ECCV; Thompson WB, 1998, INT J COMPUT VISION, V30, P163, DOI 10.1023/A:1008026031844; Toh K, 2010, PACIFIC J OPTIMIZATI; Tomasi C., 1991, IJCV; Torresani L., 2008, TPAMI, V30; Torresani L., 2001, CVPR; Torresani Lorenzo, 2002, ECCV; Xiao J., 2003, CMURITR0316; Yu S.X., 2003, ICCV	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101085
C	Frostig, R; Wang, SI; Liang, P; Manning, CD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Frostig, Roy; Wang, Sida, I; Liang, Percy; Manning, Christopher D.			Simple MAP Inference via Low-Rank Relaxations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SEMIDEFINITE; CUT	We focus on the problem of maximum a posteriori (MAP) inference in Markov random fields with binary variables and pairwise interactions. For this common subclass of inference tasks, we consider low-rank relaxations that interpolate between the discrete problem and its full-rank semidefinite relaxation. We develop new theoretical bounds studying the effect of rank, showing that as the rank grows, the relaxed objective increases but saturates, and that the fraction in objective value retained by the rounded discrete solution decreases. In practice, we show two algorithms for optimizing the low-rank objectives which are simple to implement, enjoy ties to the underlying theory, and outperform existing approaches on benchmark MAP inference tasks.	[Frostig, Roy; Wang, Sida, I; Liang, Percy; Manning, Christopher D.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Frostig, R (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	rf@cs.stanford.edu; sidaw@cs.stanford.edu; pliang@cs.stanford.edu; manning@stanford.edu	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X				Alon N, 2006, SIAM J COMPUT, V35, P787, DOI 10.1137/S0097539704441629; BARVINOK AI, 1995, DISCRETE COMPUT GEOM, V13, P189, DOI 10.1007/BF02574037; Briet J, 2010, LECT NOTES COMPUT SC, V6198, P31, DOI 10.1007/978-3-642-14165-2_4; Burer S., 2001, MATH PROGRAM, V95, P329; diaeresis>ahenb <spacing diaeresis>uhl Philipp Kr<spacing, 2013, P 30 INT C MACH LEAR, P513; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Krahenbuhl P, 2011, ADV NEURAL INFORM PR, V24; Kulis B., 2007, INT C ART INT STAT, P235; Nesterov Y, 1998, OPTIM METHOD SOFTW, V9, P141, DOI 10.1080/10556789808805690; Pataki G, 1998, MATH OPER RES, V23, P339, DOI 10.1287/moor.23.2.339; Recht B., 2013, MATH PROGRAMMING COM, V5, P1; Rush Alexander M., 2010, P 2010 C EMP METH NA, P1; Schoenberg I., 1942, DUKE MATH J, V9, P96, DOI DOI 10.1215/S0012-7094-42-00908-6; Sontag D., 2008, ADV NEURAL INFORM PR, V20, P1393; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang S., 2014, INT C LEARN REPR ICL; Weiss Y., 2008, P UAI, P503	21	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100040
C	Garg, A; Ma, TY; Nguyen, HL		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Garg, Ankit; Ma, Tengyu; Nguyen, Huy L.			On Communication Cost of Distributed Statistical Estimation and Dimensionality	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We explore the connection between dimensionality and communication cost in distributed learning problems. Specifically we study the problem of estimating the mean (theta) over right arrow of an unknown d dimensional gaussian distribution in the distributed setting. In this problem, the samples from the unknown distribution are distributed among m different machines. The goal is to estimate the mean (theta) over right arrow at the optimal minimax rate while communicating as few bits as possible. We show that in this setting, the communication cost scales linearly in the number of dimensions i.e. one needs to deal with different dimensions individually. Applying this result to previous lower bounds for one dimension in the interactive setting [1] and to our improved bounds for the simultaneous setting, we prove new lower bounds of Omega(md/log(m)) and Omega(md) for the bits of communication needed to achieve the minimax squared loss, in the interactive and simultaneous settings respectively. To complement, we also demonstrate an interactive protocol achieving the minimax squared loss with O(md) bits of communication, which improves upon the simple simultaneous protocol by a logarithmic factor. Given the strong lower bounds in the general setting, we initiate the study of the distributed parameter estimation problems with structured parameters. Specifically, when the parameter is promised to be s-sparse, we show a simple thresholding based protocol that achieves the same squared loss while saving a d/s factor of communication. We conjecture that the tradeoff between communication and squared loss demonstrated by this protocol is essentially optimal up to logarithmic factor.	[Garg, Ankit; Ma, Tengyu] Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA; [Nguyen, Huy L.] Univ Calif Berkeley, Simons Inst, Berkeley, CA USA	Princeton University; University of California System; University of California Berkeley	Garg, A (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08544 USA.	garg@cs.princeton.edu; tengyu@cs.princeton.edu; hlnguyen@cs.princeton.edu	Nguyen, Huy/GWQ-6433-2022; Nguyen, Huy/AFN-7027-2022					Balcan M, 2012, COLT; Bar-Yossef Z, 2004, J COMPUT SYST SCI, V68, P702, DOI 10.1016/j.jcss.2003.11.006; Barak B, 2013, SIAM J COMPUT, V42, P1327, DOI 10.1137/100811969; Braverman M, 2013, ANN IEEE SYMP FOUND, P668, DOI 10.1109/FOCS.2013.77; Braverman M, 2011, ANN IEEE SYMP FOUND, P748, DOI 10.1109/FOCS.2011.86; Chakrabarti A, 2001, ANN IEEE SYMP FOUND, P270, DOI 10.1109/SFCS.2001.959901; Daume Hal  III, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P154, DOI 10.1007/978-3-642-34106-9_15; Daume III H., 2012, ARTIF INTELL, P282; Duchi J. C., 2014, ABS14050782 CORR; Erkip E, 1998, IEEE T INFORM THEORY, V44, P1026, DOI 10.1109/18.669153; Ganor A., 2014, P EL C COMP COMPL EC, V21, P49; SHANNON CE, 1948, BELL SYST TECH J, V27, P623, DOI 10.1002/j.1538-7305.1948.tb00917.x; Zhang Y., 2013, NEURAL INFORM PROCES, P2328; Zhang YC, 2013, J MACH LEARN RES, V14, P3321	14	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102003
C	Gopalan, P; Charlin, L; Blei, DM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gopalan, Prem; Charlin, Laurent; Blei, David M.			Content-based recommendations with Poisson factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We develop collaborative topic Poisson factorization (CTPF), a generative model of articles and reader preferences. CTPF can be used to build recommender systems by learning from reader histories and content to recommend personalized articles of interest. In detail, CTPF models both reader behavior and article texts with Poisson distributions, connecting the latent topics that represent the texts with the latent preferences that represent the readers. This provides better recommendations than competing methods and gives an interpretable latent space for understanding patterns of readership. Further, we exploit stochastic variational inference to model massive real-world datasets. For example, we can fit CPTF to the full arXiv usage dataset, which contains over 43 million ratings and 42 million word counts, within a day. We demonstrate empirically that our model outperforms several baselines, including the previous state-of-the art approach.	[Gopalan, Prem] Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA; [Charlin, Laurent] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Blei, David M.] Columbia Univ, Dept Stat & Comp Sci, New York, NY 10027 USA	Princeton University; Columbia University; Columbia University	Gopalan, P (corresponding author), Princeton Univ, Dept Comp Sci, Princeton, NJ 08540 USA.	pgopalan@cs.princeton.edu; lcharlin@cs.columbia.edu; david.blei@columbia.edu						Agarwal D., 2010, P 3 ACM INT C WEB SE, P91, DOI DOI 10.1145/1718487.1718499; [Anonymous], 2009, COMPUT INTEL NEUROSC; [Anonymous], 2008, P 25 INT C MACH LEAR; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Canny J, 2004, P 27 ANN INT ACM SIG; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; GEISSER S, 1979, J AM STAT ASSOC, V74, P153, DOI 10.2307/2286745; Ghahramani Z., 2000, NEURAL INFORM PROCES, V12; Hanhuai Shan, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P1025, DOI 10.1109/ICDM.2010.116; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hofman J.M., 2013, ARXIV13111704; HONG L., 2013, P 6 ACM INT C WEB SE, P557, DOI DOI 10.1145/2433396.2433467; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Jack K., 2010, PROCEDIA COMPUTER SC, V1, P1; Johnson NL, 2005, WILEY SER PROBAB ST, P1, DOI 10.1002/0471715816; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Porteous I., 2010, C ASS ADV ART INT; Singh A.P., 2008, PROC PROC 14 ACM SIG, P650; Wang C., 2011, KNOWLEDGE DISCOVERY; Wang Chong, 2011, INT C ART INT STAT, P752; Zhang XianXing, 2012, ADV NEURAL INFORM PR, P1556	22	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101031
C	Henriques, JF; Martins, P; Caseiro, R; Batista, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Henriques, Joao F.; Martins, Pedro; Caseiro, Rui; Batista, Jorge			Fast Training of Pose Detectors in the Fourier Domain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In many datasets, the samples are related by a known image transformation, such as rotation, or a repeatable non-rigid deformation. This applies to both datasets with the same objects under different viewpoints, and datasets augmented with virtual samples. Such datasets possess a high degree of redundancy, because geometrically-induced transformations should preserve intrinsic properties of the objects. Likewise, ensembles of classifiers used for pose estimation should also share many characteristics, since they are related by a geometric transformation. By assuming that this transformation is norm-preserving and cyclic, we propose a closed-form solution in the Fourier domain that can eliminate most redundancies. It can leverage off-the-shelf solvers with no modification (e.g. libsvm), and train several pose classifiers simultaneously at no extra cost. Our experiments show that training a sliding-window object detector and pose estimator can be sped up by orders of magnitude, for transformations as diverse as planar rotation, the walking motion of pedestrians, and out-of-plane rotations of cars.	[Henriques, Joao F.; Martins, Pedro; Caseiro, Rui; Batista, Jorge] Univ Coimbra, Inst Syst & Robot, Coimbra, Portugal	Universidade de Coimbra	Henriques, JF (corresponding author), Univ Coimbra, Inst Syst & Robot, Coimbra, Portugal.	henriques@isr.uc.pt; pedromartins@isr.uc.pt; ruicaseiro@isr.uc.pt; batista@isr.uc.pt	Martins, Pedro/GWC-7702-2022; Batista, Jorge/A-4196-2011	Batista, Jorge/0000-0003-2387-5961	FCT project [PTDC/EEA-CRO/122812/2010, SFRH/BD75459/2010, SFRH/BD74152/2010, SFRH/BPD/90200/2012]	FCT project(Portuguese Foundation for Science and Technology)	The authors would like to thank Joao Carreira for valuable discussions. They also acknowledge support by the FCT project PTDC/EEA-CRO/122812/2010, grants SFRH/BD75459/2010, SFRH/BD74152/2010, and SFRH/BPD/90200/2012.	Boddeti V.N., 2013, CVPR; Chang CY, 2000, IEEE T IMAGE PROCESS, V9, P1937, DOI 10.1109/83.877214; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Davis P. J., 1994, CIRCULANT MATRICES, V2nd; Dosovitskiy A., 2014, INT C LEARN REPR; Felzenszwalb P., 2010, TPAMI; Galoogahi H. K., 2013, ICCV; Geiger A., 2011, ADV NEURAL INFORM PR, V24; Geiger A., 2012, CVPR; Gray R. M., 2006, TOEPLITZ CIRCULANT M; Heitz G., 2008, ECCV; Henriques J. F., 2015, TPAMI; Henriques J. F., 2012, ECCV; Henriques J. F., 2013, ICCV; Jojic N., 2001, ICCV; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Liu K, 2014, INT J COMPUT VISION, V106, P342, DOI 10.1007/s11263-013-0634-z; Malisiewicz T., 2011, ICCV; Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953; Miao X, 2007, NEURAL COMPUT, V19, P2665, DOI 10.1162/neco.2007.19.10.2665; Mundy JL, 2006, LECT NOTES COMPUT SC, V4170, P3; Paulin M., 2014, CVPR; Rifkin R, 2003, NATO SCI SERIES 3, V190, P131, DOI DOI 10.1016/S0072-9752(06)80038-2; Schmidt U., 2012, CVPR; Scholkopf, 2002, ADV NEURAL INFORM PR; Simard P., 1998, LNCS; Tamaki B., 2012, CVPR; Uenohara M, 1998, IEEE T IMAGE PROCESS, V7, P116, DOI 10.1109/83.650856	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101088
C	Hu, HN; Li, ZT; Vetta, A		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Hu, Huining; Li, Zhentao; Vetta, Adrian			Randomized Experimental Design for Causal Graph Discovery	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We examine the number of controlled experiments required to discover a causal graph. Hauser and Buhlmann [1] showed that the number of experiments required is logarithmic in the cardinality of maximum undirected clique in the essential graph. Their lower bounds, however, assume that the experiment designer cannot use randomization in selecting the experiments. We show that significant improvements are possible with the aid of randomization - in an adversarial (worst-case) setting, the designer can then recover the causal graph using at most O(log log n) experiments in expectation. This bound cannot be improved; we show it is tight for some causal graphs. We then show that in a non-adversarial (average-case) setting, even larger improvements are possible: if the causal graph is chosen uniformly at random under a Erdos-Renyi model then the expected number of experiments to discover the causal graph is constant. Finally, we present computer simulations to complement our theoretic results. Our work exploits a structural characterization of essential graphs by Andersson et al. [2]. Their characterization is based upon a set of orientation forcing operations. Our results show a distinction between which forcing operations are most important in worst-case and average-case settings.	[Hu, Huining; Vetta, Adrian] McGill Univ, Sch Comp Sci, Montreal, PQ, Canada; [Li, Zhentao] Ecole Normale Super, LIENS, Paris, France; [Vetta, Adrian] McGill Univ, Dept Math & Stat, Montreal, PQ, Canada	McGill University; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); McGill University	Hu, HN (corresponding author), McGill Univ, Sch Comp Sci, Montreal, PQ, Canada.	huining.hu@mail.mcgill.ca; zhentao.li@ens.fr; vetta@math.mcgill.ca						Andersson SA, 1997, ANN STAT, V25, P505; [Anonymous], 2000, CAUSATION PREDICTION; CAI MC, 1984, DISCRETE MATH, V49, P15; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; Eberhardt F, 2006, STUD FUZZ SOFT COMP, V194, P97; Eberhardt F, 2007, THESIS CARNEGIE MELL, P93; Eberhardt F., 2010, CAUSALITY OBJECTIVES, P87; Eberhardt F., 2008, P 24 C UNC ART INT U, P161; Eberhardt F., 2005, P 21 C UNC ART INT, P178; FRYDENBERG M, 1990, SCAND J STAT, V17, P333; Gavril F., 1972, SIAM Journal on Computing, V1, P180, DOI 10.1137/0201013; Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Meek C., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P403; Renyi A, 1961, ACTA SCI MATH, V22, P4; Williams VV, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P887	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102096
C	Huang, YP; Rao, RPN		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Huang, Yanping; Rao, Rajesh P. N.			Neurons as Monte Carlo Samplers: Bayesian Inference and Learning in Spiking Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a spiking network model capable of performing both approximate inference and learning for any hidden Markov model. The lower layer sensory neurons detect noisy measurements of hidden world states. The higher layer neurons with recurrent connections infer a posterior distribution over world states from spike trains generated by sensory neurons. We show how such a neuronal network with synaptic plasticity can implement a form of Bayesian inference similar to Monte Carlo methods such as particle filtering. Each spike in the population of inference neurons represents a sample of a particular hidden world state. The spiking activity across the neural population approximates the posterior distribution of hidden state. The model provides a functional explanation for the Poisson-like noise commonly observed in cortical responses. Uncertainties in spike times provide the necessary variability for sampling during inference. Unlike previous models, the hidden world state is not observed by the sensory neurons, and the temporal dynamics of the hidden state is unknown. We demonstrate how such networks can sequentially learn hidden Markov models using a spike-timing dependent Hebbian learning rule and achieve power-law convergence rates.	[Huang, Yanping; Rao, Rajesh P. N.] Univ Washington, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Huang, YP (corresponding author), Univ Washington, Seattle, WA 98195 USA.	huangyp@cs.uw.edu; rao@cs.uw.edu						Beck JM, 2007, NEURAL COMPUT, V19, P1344, DOI 10.1162/neco.2007.19.5.1344; Berkes P, 2011, SCIENCE, V331, P83, DOI 10.1126/science.1195870; Bobrowski O, 2009, NEURAL COMPUT, V21, P1277, DOI 10.1162/neco.2008.01-08-692; Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211; Cappe O., 2009, ONLINE ALGORITHM LAT; Chance FS, 2000, NETWORK-COMP NEURAL, V11, P119, DOI 10.1088/0954-898X/11/2/301; Daw N. D., 2007, ADV NEURAL INFORM PR, V19; Deneve S, 2008, NEURAL COMPUT, V20, P91, DOI 10.1162/neco.2008.20.1.91; Deneve S, 2008, NEURAL COMPUT, V20, P118, DOI 10.1162/neco.2008.20.1.118; Doucet A., 2001, SEQUENTIAL MONTE CAR; Doya K., 2007, BAYESIAN BRAIN PROBA; Ecker AS, 2010, SCIENCE, V327, P584, DOI 10.1126/science.1179867; HODGES JL, 1960, ANN MATH STAT, V31, P737, DOI 10.1214/aoms/1177705799; Hoyer P., 2002, ADV NEURAL INFORM PR, V15; Knill DC, 1996, PERCEPTION BAYESIAN; Kording KP, 2004, NATURE, V427, P244, DOI 10.1038/nature02169; Ma WJ, 2006, NAT NEUROSCI, V9, P1432, DOI 10.1038/nn1790; Mongillo G, 2008, NEURAL COMPUT, V20, P1706, DOI 10.1162/neco.2008.10-06-351; Nimchinsky EA, 2004, J NEUROSCI, V24, P2054, DOI 10.1523/JNEUROSCI.5066-03.2004; Paulin MG, 2005, J NEURAL ENG, V2, DOI 10.1088/1741-2560/2/3/S06; Rao RPN, 2004, NEURAL COMPUT, V16, P1, DOI 10.1162/08997660460733976; Wilson R, 2009, ADV NEURAL INFORM PR, V22, P2062; Wu S., 2003, NEURAL COMPUTATION, V15; Zemel R. S., 1999, ADV NEURAL INFORM PR, V11; ZEMEL RS, 2005, ADV NEURAL INFORM PR, V17, P1609; Zhang K., 1998, J NEUROSCIENCE, V16	26	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100052
C	Inouye, DI; Ravikumar, P; Dhillon, IS		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Inouye, David I.; Ravikumar, Pradeep; Dhillon, Inderjit S.			Capturing Semantically Meaningful Word Dependencies with an Admixture of Poisson MRFs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We develop a fast algorithm for the Admixture of Poisson MRFs (APM) topic model [1] and propose a novel metric to directly evaluate this model. The APM topic model recently introduced by Inouye et al. [1] is the first topic model that allows for word dependencies within each topic unlike in previous topic models like LDA that assume independence between words within a topic. Research in both the semantic coherence of a topic models [2, 3, 4, 5] and measures of model fitness [6] provide strong support that explicitly modeling word dependencies-as in APM-could be both semantically meaningful and essential for appropriately modeling real text data. Though APM shows significant promise for providing a better topic model, APM has a high computational complexity because O(p(2)) parameters must be estimated where p is the number of words ([1] could only provide results for datasets with p = 200). In light of this, we develop a parallel alternating Newton-like algorithm for training the APM model that can handle p = 10(4) as an important step towards scaling to large datasets. In addition, Inouye et al. [1] only provided tentative and inconclusive results on the utility of APM. Thus, motivated by simple intuitions and previous evaluations of topic models, we propose a novel evaluation metric based on human evocation scores between word pairs (i.e. how much one word "brings to mind" another word [7]). We provide compelling quantitative and qualitative results on the BNC corpus that demonstrate the superiority of APM over previous topic models for identifying semantically meaningful word dependencies.	[Inouye, David I.; Ravikumar, Pradeep; Dhillon, Inderjit S.] Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Inouye, DI (corresponding author), Univ Texas Austin, Dept Comp Sci, Austin, TX 78712 USA.	dinouye@cs.utexas.edu; pradeepr@cs.utexas.edu; inderjit@cs.utexas.edu			NSF Graduate Research Fellowship [DGE-1110007]; ARO [W911NF-12-1-0390]; NSF [CCF-1117055, IIS-1149803, IIS-1447574, DMS-1264033]	NSF Graduate Research Fellowship(National Science Foundation (NSF)); ARO; NSF(National Science Foundation (NSF))	D. Inouye was supported by the NSF Graduate Research Fellowship via DGE-1110007. P. Ravikumar acknowledges support from ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1447574, and DMS-1264033. I. Dhillon acknowledges support from NSF via CCF-1117055.	Airoldi EM, 2008, J MACH LEARN RES, V9, P1981; Aletras Nikolaos, 2013, P 10 INT C COMP SEM, P13; Blei D., 2006, NIPS, V18, P147, DOI [10.5555/2976248.2976267, DOI 10.5555/2976248.2976267, DOI 10.1145/1143844.1143859]; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boyd-Graber J., 2006, P GLOB WORDNET C; Chang J., 2009, NIPS, V31, P1; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Hinton G. E., 2009, NIPS; Hofmann T, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P289; Hsieh C. - J., 2011, ADV NEURAL INFORM PR, P1; Inouye D. I., 2014, INT C MACH LEARN ICM; Lau J. H., 2012, P 13 C EUR CHAPT ASS, P591; Lau Jey Han, 2011, P 49 ANN M ASS COMP, P1536; Lee DD, 2001, ADV NEUR IN, V13, P556; Lee J., 2013, J MACH LEARN RES, V31, P388; Magatti D., 2009, ISDA; Mao X.-L., 2012, P 21 ACM INT C INF K, P2383, DOI [DOI 10.1145/2396761.2398646, 10.1145/2396761.2398646]; McCallum Andrew Kachites, 2002, MALLET MACHINE LEARN; Mimno D, 2011, P 2011 C EMPIRICAL M, P227, DOI 10.5555/2145432.2145459; Mimno D, 2011, P C EMP METH NAT LAN, P262, DOI DOI 10.5555/2145432.2145462; Newman D, 2010, P 10 ANN JOINT C DIG, P215, DOI DOI 10.1145/1816123.1816156; Nikolova S, 2009, ASSETS'09: PROCEEDINGS OF THE 11TH INTERNATIONAL ACM SIGACCESS CONFERENCE ON COMPUTERS AND ACCESSIBILITY, P171; Reisinger J., 2010, P 27 INT C MACH LEAR, P903; Stevens K., 2012, P 2012 JOINT C EMP M; Teh YW, 2006, J AM STAT ASSOC, V101, P1566, DOI 10.1198/016214506000000302; Varin C, 2011, STAT SINICA, V21, P5; Yang E., 2013, ADV NEURAL INFORM PR, P1718; Yang E., 2012, ADV NEURAL INFO PROC, P1358; Yu HF, 2011, MACH LEARN, V85, P41, DOI 10.1007/s10994-010-5221-8	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100079
C	Knudson, KC; Yates, JL; Huk, AC; Pillow, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Knudson, Karin C.; Yates, Jacob L.; Huk, Alexander C.; Pillow, Jonathan W.			Inferring sparse representations of continuous signals with continuous orthogonal matching pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				RECOVERY	Many signals, such as spike trains recorded in multi-channel electrophysiological recordings, may be represented as the sparse sum of translated and scaled copies of waveforms whose timing and amplitudes are of interest. From the aggregate signal, one may seek to estimate the identities, amplitudes, and translations of the waveforms that compose the signal. Here we present a fast method for recovering these identities, amplitudes, and translations. The method involves greedily selecting component waveforms and then refining estimates of their amplitudes and translations, moving iteratively between these steps in a process analogous to the well-known Orthogonal Matching Pursuit (OMP) algorithm [11]. Our approach for modeling translations borrows from Continuous Basis Pursuit (CBP) [4], which we extend in several ways: by selecting a subspace that optimally captures translated copies of the waveforms, replacing the convex optimization problem with a greedy approach, and moving to the Fourier domain to more precisely estimate time shifts. We test the resulting method, which we call Continuous Orthogonal Matching Pursuit (COMP), on simulated and neural data, where it shows gains over CBP in both speed and accuracy.	[Knudson, Karin C.] Univ Texas Austin, Dept Math, Austin, TX 78712 USA; [Yates, Jacob L.] Univ Texas Austin, Dept Neurosci, Austin, TX 78712 USA; [Huk, Alexander C.] Univ Texas Austin, Ctr Perceptual Syst, Dept Psychol & Neurosci, Austin, TX 78712 USA; [Pillow, Jonathan W.] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton, NJ 08544 USA	University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin; University of Texas System; University of Texas Austin; Princeton University; Princeton University	Knudson, KC (corresponding author), Univ Texas Austin, Dept Math, Austin, TX 78712 USA.	kknudson@math.utexas.edu; jlyates@utexas.edu; huk@utexas.edu; pillow@princeton.edu			McKnight Foundation; NSF CAREER Award [IIS-1150186]; NIH (NEI) [EY017366]; NIH (NIMH) [MH099611]	McKnight Foundation; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NIH (NEI)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); NIH (NIMH)(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Mental Health (NIMH))	This work was supported by the McKnight Foundation (JP), NSF CAREER Award IIS-1150186 (JP), and grants from the NIH (NEI grant EY017366 and NIMH grant MH099611 to AH & JP).	Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006; Ekanadham C, 2014, J NEUROSCI METH, V222, P47, DOI 10.1016/j.jneumeth.2013.10.001; Ekanadham C, 2011, IEEE T SIGNAL PROCES, V59, P4735, DOI 10.1109/TSP.2011.2160058; Ekanadham Chaitanya, 2011, P 25 ANN C NEUR INF, V23; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; Pillow JW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062123; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Vetterli M, 2002, IEEE T SIGNAL PROCES, V50, P1417, DOI 10.1109/TSP.2002.1003065	12	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100043
C	Latimer, KW; Chichilnisky, EJ; Rieke, F; Pillow, JW		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Latimer, Kenneth W.; Chichilnisky, E. J.; Rieke, Fred; Pillow, Jonathan W.			Inferring synaptic conductances from spike trains under a biophysically inspired point process model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FRAMEWORK; INPUT	A popular approach to neural characterization describes neural responses in terms of a cascade of linear and nonlinear stages: a linear filter to describe stimulus integration, followed by a nonlinear function to convert the filter output to spike rate. However, real neurons respond to stimuli in a manner that depends on the nonlinear integration of excitatory and inhibitory synaptic inputs. Here we introduce a biophysically inspired point process model that explicitly incorporates stimulus-induced changes in synaptic conductance in a dynamical model of neuronal membrane potential. Our work makes two important contributions. First, on a theoretical level, it offers a novel interpretation of the popular generalized linear model (GLM) for neural spike trains. We show that the classic GLM is a special case of our conductance-based model in which the stimulus linearly modulates excitatory and inhibitory conductances in an equal and opposite "push-pull" fashion. Our model can therefore be viewed as a direct extension of the GLM in which we relax these constraints; the resulting model can exhibit shunting as well as hyperpolarizing inhibition, and time-varying changes in both gain and membrane time constant. Second, on a practical level, we show that our model provides a tractable model of spike responses in early sensory neurons that is both more accurate and more interpretable than the GLM. Most importantly, we show that we can accurately infer intracellular synaptic conductances from extracellularly recorded spike trains. We validate these estimates using direct intracellular measurements of excitatory and inhibitory conductances in parasol retinal ganglion cells. The stimulus-dependence of both excitatory and inhibitory conductances can be well described by a linear-nonlinear cascade, with the filter driving inhibition exhibiting opposite sign and a slight delay relative to the filter driving excitation. We show that the model fit to extracellular spike trains can predict excitatory and inhibitory conductances elicited by novel stimuli with nearly the same accuracy as a model trained directly with intracellular conductances.	[Latimer, Kenneth W.] Univ Texas Austin, Inst Neurosci, Austin, TX 78712 USA; [Chichilnisky, E. J.] Stanford Univ, Hansen Expt Phys Lab, Dept Neurosurg, Stanford, CA 94305 USA; [Rieke, Fred] Univ Washington, Howard Hughes Med Inst, Dept Physiol & Biophys, Seattle, WA 98195 USA; [Pillow, Jonathan W.] Princeton Univ, Dept Psychol, Princeton Neurosci Inst, Princeton, NJ 08544 USA	University of Texas System; University of Texas Austin; Stanford University; Howard Hughes Medical Institute; University of Washington; University of Washington Seattle; Princeton University	Latimer, KW (corresponding author), Univ Texas Austin, Inst Neurosci, Austin, TX 78712 USA.	latimerk@utexas.edu; ej@stanford.edu; rieke@u.washington.edu; pillow@princeton.edu		Chichilnisky, E.J./0000-0002-5613-0248				Ahrens MB, 2008, NETWORK-COMP NEURAL, V19, P35, DOI 10.1080/09548980701813936; Butts DA, 2011, J NEUROSCI, V31, P11313, DOI 10.1523/JNEUROSCI.0434-11.2011; Chander D, 2001, J NEUROSCI, V21, P9904, DOI 10.1523/JNEUROSCI.21-24-09904.2001; Gerstner W., 2001, HDB BIOL PHYS, P469; Gerwinn S, 2010, FRONTIERS COMPUTATIO; Harris KD, 2003, NATURE, V424, P552, DOI 10.1038/nature01834; McFarland JM, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003143; Mensi S., 2011, ADV NEURAL INF PROCE, P1377; Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002; Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0; Park IM, 2013, ADV NEURAL INFORM PR, V26, P2454; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Pillow JW, 2005, J NEUROSCI, V25, P11003, DOI 10.1523/JNEUROSCI.3305-05.2005; Plesser HE, 2000, NEURAL COMPUT, V12, P367, DOI 10.1162/089976600300015835; Poo C, 2009, NEURON, V62, P850, DOI 10.1016/j.neuron.2009.05.022; Stevenson IH, 2012, PLOS COMPUT BIOL, V8, DOI 10.1371/journal.pcbi.1002775; Theis L., 2010, PLOS COMPUTATIONAL B; Trong PK, 2008, NAT NEUROSCI, V11, P1343, DOI 10.1038/nn.2199; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Vintch B, 2012, NEURAL INFORM PROCES, V25	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100041
C	Lattimore, T; Munos, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lattimore, Tor; Munos, Remi			Bounded Regret for Finite-Armed Structured Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We study a new type of K-armed bandit problem where the expected return of one arm may depend on the returns of other arms. We present a new algorithm for this general class of problems and show that under certain circumstances it is possible to achieve finite expected cumulative regret. We also give problem-dependent lower bounds on the cumulative regret showing that at least in special cases the new algorithm is nearly optimal.	[Lattimore, Tor] Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada; [Munos, Remi] INRIA, Lille, France; [Munos, Remi] Google DeepMind, London, England	University of Alberta; Inria; Google Incorporated	Lattimore, T (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB, Canada.	tlattimo@ualberta.ca; remi.munos@inria.fr			Google Australia Fellowship for Machine Learning; Alberta Innovates Technology Futures, NSERC; European Community [270327]	Google Australia Fellowship for Machine Learning(Google Incorporated); Alberta Innovates Technology Futures, NSERC; European Community(European Commission)	Tor Lattimore was supported by the Google Australia Fellowship for Machine Learning and the Alberta Innovates Technology Futures, NSERC. The majority of this work was completed while Remi Munos was visiting Microsoft Research, New England. This research was partially supported by the European Community's Seventh Framework Programme under grant agreements no. 270327 (project CompLACS).	AGRAWAL R, 1989, IEEE T AUTOMAT CONTR, V34, P1249, DOI 10.1109/9.40770; Agrawal S., 2013, ARTIF INTELL, P99; Agrawal Shipra, 2012, COLT; Amin K., 2011, JMLR WORKSHOP C P, P87; Audibert Jean-Yves, 2007, 0731 CERT EC PONTS; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Auer P, 2010, PERIOD MATH HUNG, V61, P55, DOI 10.1007/s10998-010-3055-6; Bubeck, 2013, ADV NEURAL INFORM PR, P638; Bubeck S, 2012, REGRET ANAL STOCHAST; Bubeck S., 2008, ADV NEURAL INF PROCE, V21, P201; Bubeck Sebastien, 2013, P 26 ANN C LEARN THE; Graves TL, 1997, SIAM J CONTROL OPTIM, V35, P715, DOI 10.1137/S0363012994275440; Kaufmann Emilie, 2012, Algorithmic Learning Theory. 23rd International Conference (ALT 2012). Proceedings, P199, DOI 10.1007/978-3-642-34106-9_18; Korda N, 2013, ADV NEURAL INFORM PR, P1448; Lai T. L., 1984, DESIGN EXPT, P127; LAI TL, 1984, P NATL ACAD SCI USA, V81, P1284, DOI 10.1073/pnas.81.4.1284; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Mersereau AJ, 2009, IEEE T AUTOMAT CONTR, V54, P2787, DOI 10.1109/TAC.2009.2031725; Russo Daniel, 2013, ADV NEURAL INFORM PR, P2256	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102022
C	Li, B; Vorobeychik, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Li, Bo; Vorobeychik, Yevgeniy			Feature Cross-Substitution in Adversarial Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The success of machine learning, particularly in supervised settings, has led to numerous attempts to apply it in adversarial settings such as spam and malware detection. The core challenge in this class of applications is that adversaries are not static data generators, but make a deliberate effort to evade the classifiers deployed to detect them. We investigate both the problem of modeling the objectives of such adversaries, as well as the algorithmic problem of accounting for rational, objective-driven adversaries. In particular, we demonstrate severe shortcomings of feature reduction in adversarial settings using several natural adversarial objective functions, an observation that is particularly pronounced when the adversary is able to substitute across similar features (for example, replace words with synonyms or replace letters in words). We offer a simple heuristic method for making learning more robust to feature cross-substitution attacks. We then present a more general approach based on mixed-integer linear programming with constraint generation, which implicitly trades off overfitting and feature selection in an adversarial setting using a sparse regularizer along with an evasion model. Our approach is the first method for combining an adversarial classification algorithm with a very general class of models of adversarial classifier evasion. We show that our algorithmic approach significantly outperforms state-of-the-art alternatives.	[Li, Bo; Vorobeychik, Yevgeniy] Vanderbilt Univ, Elect Engn & Comp Sci, 221 Kirkland Hall, Nashville, TN 37235 USA	Vanderbilt University	Li, B (corresponding author), Vanderbilt Univ, Elect Engn & Comp Sci, 221 Kirkland Hall, Nashville, TN 37235 USA.	bo.li.2@vanderbilt.edu; yevgeniy.vorobeychik@vanderbilt.edu			Sandia National Laboratories; U.S. Department of Energy's National Nuclear Security Administration [DE-AC04-94AL85000]	Sandia National Laboratories(United States Department of Energy (DOE)); U.S. Department of Energy's National Nuclear Security Administration(National Nuclear Security Administration)	This research was partially supported by Sandia National Laboratories. Sandia National Laboratories is a multi-program laboratory managed and operated by Sandia Corporation, a wholly owned subsidiary of Lockheed Martin Corporation, for the U.S. Department of Energy's National Nuclear Security Administration under contract DE-AC04-94AL85000.	Almeida TA, 2009, EIGHTH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS, PROCEEDINGS, P517, DOI 10.1109/ICMLA.2009.22; Androutsopoulos I., 2000, CS0006013 ARXIV; Androutsopoulos Ion, 2005, CEAS; Barreno M., 2008, P 1 ACM WORKSH WORKS, P19; Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5; Biggio B, 2014, IEEE T KNOWL DATA EN, V26, P984, DOI 10.1109/TKDE.2013.57; Bruckner M., 2011, P 17 ACM SIGKDD INT, P547, DOI DOI 10.1145/2020408.2020495; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; El Ghaoui L., 2003, ROBUST CLASSIFICATIO; Fawcett T., 2003, ACM SIGKDD EXPLORATI, V5, P140, DOI DOI 10.1145/980972.980990; Karlberger C., 2007, P 1 USENIX WORKSH OF, P1; KONG Ying, 2012, INT P COMPUTER SCI I, V49; Laskov P, 2010, MACH LEARN, V81, P115, DOI 10.1007/s10994-010-5207-6; Lichman M, 2013, UCI MACHINE LEARNING; Liu W, 2009, INT CONF DAT MIN WOR, P25, DOI 10.1109/ICDMW.2009.9; Lowd D., 2005, P 11 ACM SIGKDD INT, P641, DOI DOI 10.1145/1081870.1081950; Mahoney M. V., 2002, CS200106, P376, DOI 10.1145/775047.775102; MCCORMICK GP, 1976, MATH PROGRAM, V10, P147, DOI 10.1007/BF01580665; Metsis V, 2006, CEAS, P27; Nelson Blaine, 2011, Privacy and Security Issues in Data Mining and Machine Learning.International ECML/PKDD Workshop, PSDML 2010. Revised Selected Papers, P92, DOI 10.1007/978-3-642-19896-0_8; Nelson B, 2012, J MACH LEARN RES, V13, P1293; Sahami M, 1998, LEARN TEXT CAT PAP 1, V62, P98	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102071
C	Li, N; Jin, R; Zhou, ZH		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Li, Nan; Jin, Rong; Zhou, Zhi-Hua			Top Rank Optimization in Linear Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				AREA	Bipartite ranking aims to learn a real-valued ranking function that orders positive instances before negative instances. Recent efforts of bipartite ranking are focused on optimizing ranking accuracy at the top of the ranked list. Most existing approaches are either to optimize task specific metrics or to extend the rank loss by emphasizing more on the error associated with the top ranked instances, leading to a high computational cost that is super-linear in the number of training instances. We propose a highly efficient approach, titled TopPush, for optimizing accuracy at the top that has computational complexity linear in the number of training instances. We present a novel analysis that bounds the generalization error for the top ranked instances for the proposed approach. Empirical study shows that the proposed approach is highly competitive to the state-of-the-art approaches and is 10-100 times faster.	[Li, Nan; Zhou, Zhi-Hua] Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China; [Jin, Rong] Michigan State Univ, Dept Comp Sci & Engn, E Lansing, MI 48824 USA	Nanjing University; Michigan State University	Li, N (corresponding author), Nanjing Univ, Natl Key Lab Novel Software Technol, Nanjing 210023, Jiangsu, Peoples R China.	lin@lamda.nju.edu.cn; rongjin@cse.msu.edu; zhouzh@lamda.nju.edu.cn			973 Program [2014CB340501]; NSFC [61333014]; NSF [IIS-1251031]; ONR Award [N000141210431]	973 Program(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); NSF(National Science Foundation (NSF)); ONR Award	This research was supported by the 973 Program (2014CB340501), NSFC (61333014), NSF (IIS-1251031), and ONR Award (N000141210431).	Agarwal S, 2005, J MACH LEARN RES, V6, P393; Agarwal Shivani, 2011, P 2011 SIAM INT C DA; [Anonymous], 2009, 26 ANN INT C MACH LE, DOI DOI 10.1145/1553374.1553459; Boyd S, 2004, CONVEX OPTIMIZATION; Boyd Stephen, 2012, P NIPS, V25, P953; Chris Burges T.S., 2005, P 22 INT MACH LEARN, DOI 10.1145/1102351.1102363; Clemencon S, 2008, ANN STAT, V36, P844, DOI 10.1214/009052607000000910; Clemencon S, 2007, J MACH LEARN RES, V8, P2671; Cormen T. H., 2009, INTRO ALGORITHMS, V3rd; CORTES C, 2004, NIPS, V16, P313; Duchi J., 2008, PROC 25 INT C MACH L, P272; Freund Y., 2003, J MACHINE LEARNING R, V4, P933, DOI DOI 10.1162/JMLR.2003.4.6.933; Gao W., 2013, P 30 INT C MACHINE L, P906; HANLEY JA, 1982, RADIOLOGY, V143, P29, DOI 10.1148/radiology.143.1.7063747; Herbrich R, 2000, ADV NEUR IN, P115; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kanamori T, 2013, J MACH LEARN RES, V14, P1461; Kotlowski Wojciech, 2011, P 28 INT C MACH LEAR, P1113; Le Q. V., 2007, ABS07043359 CORR; Li N., 2014, ABS14101462 CORR; Li N, 2013, IEEE T PATTERN ANAL, V35, P1370, DOI 10.1109/TPAMI.2012.172; Liu TY, 2011, LEARNING TO RANK FOR INFORMATION RETRIEVAL, P1, DOI 10.1007/978-3-642-14267-3; Narasimhan H., 2013, KDD, P167; Narasimhan H., 2013, PROC INT C MACH LEAR, P516; Narasimhan Harikrishna, 2013, ADV NEURAL INFORM PR, P2913; NEMIROVSKI A. S., 1994, LECT NOTES; Nesterov Y., 2003, INTRO LECT CONVEX OP; Rakotomamonjy A., 2012, ICML; Rendle S, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P727; Rudin C, 2009, J MACH LEARN RES, V10, P2193; Shalev-Shwartz S, 2006, J MACH LEARN RES, V7, P1567; Sun SL, 2010, J MACH LEARN RES, V11, P2423; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Usunier Nicolas, 2009, ICML; Valizadegan Hamed, 2009, ADV NEURAL INFORM PR, P1883; Xu M., 2013, P 27 AAAI C ART INT, P998; Yang Tianbao, 2012, NIPS, P485; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271; Zhao P., 2011, P 28 INT C MACHINE L, P233	42	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100001
C	Lin, QH; Lu, ZS; Xiao, L		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lin, Qihang; Lu, Zhaosong; Xiao, Lin			An Accelerated Proximal Coordinate Gradient Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We develop an accelerated randomized proximal coordinate gradient (APCG) method, for solving a broad class of composite convex optimization problems. In particular, our method achieves faster linear convergence rates for minimizing strongly convex functions than existing randomized proximal coordinate gradient methods. We show how to apply the APCG method to solve the dual of the regularized empirical risk minimization (ERM) problem, and devise efficient implementations that avoid full-dimensional vector operations. For ill-conditioned ERM problems, our method obtains improved convergence rates than the state-of-the-art stochastic dual coordinate ascent (SDCA) method.	[Lin, Qihang] Univ Iowa, Iowa City, IA 52242 USA; [Lu, Zhaosong] Simon Fraser Univ, Burnaby, BC, Canada; [Xiao, Lin] Microsoft Res, Redmond, WA USA	University of Iowa; Simon Fraser University; Microsoft	Lin, QH (corresponding author), Univ Iowa, Iowa City, IA 52242 USA.	qihang-lin@uiowa.edu; zhaosong@sfu.ca; lin.xiao@microsoft.com	Lu, Zhaosong/Z-5995-2019					Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Chang KW, 2008, J MACH LEARN RES, V9, P1369; Fercoq O., 2013, ARXIV13125799; Hong M., 2013, ARXIV13106957; Hsieh C.-J., 2008, P 25 INT C MACH LEAR, P408, DOI [10.1145/1390156.1390208, DOI 10.1145/1390156.1390208]; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Lee Y. T., ARXIV13051922; Lin Qihang, 2014, NIPS, P3059; Lu Z., 2014, MATH PROGRAMMING A; LUO ZQ, 2002, J OPTIM THEORY APPL, V72, P7; Nesterov Y., 2018, APPL OPTIMIZATION; Nesterov Y, 2012, SIAM J OPTIMIZ, V22, P341, DOI 10.1137/100802001; Richtarik P, 2014, MATH PROGRAM, V144, P1, DOI 10.1007/s10107-012-0614-z; Roux R. N., 2012, ADV NEURAL INFORM PR, P2663; Saha A, 2013, SIAM J OPTIMIZ, V23, P576, DOI 10.1137/110840054; Schmidt Mark, 2013, 00860051 HAL INRIA; Shalev-Shwartz S., 2009, P 26 ANN INT C MACH, P929; Shalev-Shwartz S, 2014, PR MACH LEARN RES, V32; TSENG P., 2001, J OPTIM THEORY APPL, V140, P513; Tseng Paul, 2008, UNPUB; Xiao L., 2014, MSRTR201438	23	1	1	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101027
C	Linderman, SW; Stock, CH; Adams, RP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Linderman, Scott W.; Stock, Christopher H.; Adams, Ryan P.			A framework for studying synaptic plasticity with neural spike train data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				MODELS	Learning and memory in the brain are implemented by complex, time-varying changes in neural circuitry. The computational rules according to which synaptic weights change over time are the subject of much research, and are not precisely understood. Until recently, limitations in experimental methods have made it challenging to test hypotheses about synaptic plasticity on a large scale. However, as such data become available and these barriers are lifted, it becomes necessary to develop analysis techniques to validate plasticity models. Here, we present a highly extensible framework for modeling arbitrary synaptic plasticity rules on spike train data in populations of interconnected neurons. We treat synaptic weights as a (potentially nonlinear) dynamical system embedded in a fully-Bayesian generalized linear model (GLM). In addition, we provide an algorithm for inferring synaptic weight trajectories alongside the parameters of the GLM and of the learning rules. Using this method, we perform model comparison of two proposed variants of the well-known spike-timing-dependent plasticity (STDP) rule, where nonlinear effects play a substantial role. On synthetic data generated from the biophysical simulator NEURON, we show that we can recover the weight trajectories, the pattern of connectivity, and the underlying learning rules.	[Linderman, Scott W.; Adams, Ryan P.] Harvard Univ, Cambridge, MA 02138 USA; [Stock, Christopher H.] Harvard Univ, Cambridge, MA 02138 USA	Harvard University; Harvard University	Linderman, SW (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	swl@seas.harvard.edu; cstock@post.harvard.edu; rpa@seas.harvard.edu			DARPA YFA [N66001-12-1-4219]; NSF [IIS-1421780]; NDSEG fellowship; NSF Center for Brains, Minds, and Machines	DARPA YFA; NSF(National Science Foundation (NSF)); NDSEG fellowship; NSF Center for Brains, Minds, and Machines	This work was partially funded by DARPA YFA N66001-12-1-4219 and NSF IIS-1421780. S.W.L. was supported by an NDSEG fellowship and by the NSF Center for Brains, Minds, and Machines.	Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Biljana Petreska, 2011, ADV NEURAL INFORM PR, P756; Caporale N, 2008, ANNU REV NEUROSCI, V31, P25, DOI 10.1146/annurev.neuro.31.060407.125639; Eldawlatly S, 2010, NEURAL COMPUT, V22, P158, DOI 10.1162/neco.2009.11-08-900; Feldman DE, 2012, NEURON, V75, P556, DOI 10.1016/j.neuron.2012.08.001; Hochbaum Daniel R, 2014, NATURE METHODS; Lindsten F., 2012, ADV NEURAL INFORM PR, V25, P2600; Lloyd James Robert, 2012, ADV NEURAL INFORM PR; MITCHELL TJ, 1988, J AM STAT ASSOC, V83, P1023, DOI 10.2307/2290129; Morrison A, 2008, BIOL CYBERN, V98, P459, DOI 10.1007/s00422-008-0233-1; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Packer AM, 2012, NAT METHODS, V9, P1202, DOI [10.1038/NMETH.2249, 10.1038/nmeth.2249]; Paninski L, 2004, NETWORK-COMP NEURAL, V15, P243, DOI 10.1088/0954-898X/15/4/002; Shababo B., 2013, ADV NEURAL INFORM PR, V26, P1304; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; Stevenson IH, 2011, ADV NEURAL INFORM PR, P2582; Truccolo W, 2005, J NEUROPHYSIOL, V93, P1074, DOI 10.1152/jn.00697.2004; Vogelstein JT, 2009, BIOPHYS J, V97, P636, DOI 10.1016/j.bpj.2008.08.005	20	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100053
C	Liu, GC; Li, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Liu, Guangcan; Li, Ping			Recovery of Coherent Data via Low-Rank Dictionary Pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ROBUST; FACTORIZATION	The recently established RPCA [4] method provides a convenient way to restore low-rank matrices from grossly corrupted observations. While elegant in theory and powerful in reality, RPCA is not an ultimate solution to the low-rank matrix recovery problem. Indeed, its performance may not be perfect even when data are strictly low-rank. This is because RPCA ignores clustering structures of the data which are ubiquitous in applications. As the number of cluster grows, the coherence of data keeps increasing, and accordingly, the recovery performance of RPCA degrades. We show that the challenges raised by coherent data (i.e., data with high coherence) could be alleviated by Low-Rank Representation (LRR) [13], provided that the dictionary in LRR is configured appropriately. More precisely, we mathematically prove that if the dictionary itself is low-rank then LRR is immune to the coherence parameter which increases with the underlying cluster number. This provides an elementary principle for dealing with coherent data and naturally leads to a practical algorithm for obtaining proper dictionaries in unsupervised environments. Experiments on randomly generated matrices and real motion sequences verify our claims.	[Liu, Guangcan; Li, Ping] Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, Piscataway, NJ 08854 USA	Rutgers State University New Brunswick	Liu, GC (corresponding author), Rutgers State Univ, Dept Comp Sci, Dept Stat & Biostat, Piscataway, NJ 08854 USA.	gcliu@rutgers.edu; pingli@rutgers.edu			 [NSF-DMS0808864];  [NSF-SES1131848];  [NSF-EAGER1249316];  [AFOSR-FA9550-13-1-0137];  [ONR-N00014-13-1-0764];  [NSF-III1360971];  [NSF-BIGDATA1419210]	; ; ; ; ; ; 	Guangcan Liu was a Postdoctoral Researcher supported by NSF-DMS0808864, NSF-SES1131848, NSF-EAGER1249316, AFOSR-FA9550-13-1-0137, and ONR-N00014-13-1-0764. Ping Li is also partially supported by NSF-III1360971 and NSF-BIGDATA1419210.	Borcea L., 2012, SYNTHETIC APERTURE R; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2010, P IEEE, V98, P925, DOI 10.1109/JPROC.2009.2035722; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Costeira JP, 1998, INT J COMPUT VISION, V29, P159, DOI 10.1023/A:1008000628999; De la Torre F, 2003, INT J COMPUT VISION, V54, P117, DOI 10.1023/A:1023709501986; Elhamifar Ehsan, 2009, 2009 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2790, DOI 10.1109/CVPRW.2009.5206547; Fazel M., 2002, MATRIX RANK MINIMIZA; FISCHLER MA, 1981, COMMUN ACM, V24, P381, DOI 10.1145/358669.358692; GNANADESIKAN R, 1972, BIOMETRICS, V28, P81, DOI 10.2307/2528963; Gross D, 2011, IEEE T INFORM THEORY, V57, P1548, DOI 10.1109/TIT.2011.2104999; Ke QF, 2005, PROC CVPR IEEE, P739; Liu G., 2010, P 27 INT C MACHINE L, P663, DOI DOI 10.1109/ICDMW.2010.64; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Liu Guangcan, 2012, J MACH LEARN RES P T, P703; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Otazo Ricardo, 2012, LOW RANK SPARSE MATR; Peng YG, 2012, IEEE T PATTERN ANAL, V34, P2233, DOI 10.1109/TPAMI.2011.282; Soltanolkotabi M., 2013, ARXIV13012603; Srebro Nathan, 2005, ADV NEURAL INFORM PR, P5; Tron R, 2007, PROC CVPR IEEE, P41, DOI 10.1109/cvpr.2007.382974; Vidal Rene, 2012, GEN PRINCIPAL COMPON; Weimer Markus, 2007, NEURAL INFORM PROCES; Xu H, 2013, IEEE T INFORM THEORY, V59, P546, DOI 10.1109/TIT.2012.2212415; Xu Huan, 2010, NEURAL INFORM PROCES; Zhang ZD, 2012, INT J COMPUT VISION, V99, P1, DOI 10.1007/s11263-012-0515-x	26	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101005
C	Mandt, S; Blei, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Mandt, Stephan; Blei, David			Smoothed Gradients for Stochastic Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Stochastic variational inference (SVI) lets us scale up Bayesian computation to massive data. It uses stochastic optimization to fit a variational distribution, following easy-to-compute noisy natural gradients. As with most traditional stochastic optimization methods, SVI takes precautions to use unbiased stochastic gradients whose expectations are equal to the true gradients. In this paper, we explore the idea of following biased stochastic gradients in SVI. Our method replaces the natural gradient with a similarly constructed vector that uses a fixed-window moving average of some of its previous terms. We will demonstrate the many advantages of this technique. First, its computational cost is the same as for SVI and storage requirements only multiply by a constant factor. Second, it enjoys significant variance reduction over the unbiased estimates, smaller bias than averaged gradients, and leads to smaller mean-squared error against the full gradient. We test our method on latent Dirichlet allocation with three large corpora.	[Mandt, Stephan] Princeton Univ, Dept Phys, Princeton, NJ 08544 USA; [Blei, David] Columbia Univ, Dept Stat, Dept Comp Sci, New York, NY 10027 USA	Princeton University; Columbia University	Mandt, S (corresponding author), Princeton Univ, Dept Phys, Princeton, NJ 08544 USA.	smandt@princeton.edu; david.blei@columbia.edu			NSF CAREER NSF [IIS-0745520]; NSF BIGDATA NSF [IIS-1247664]; NSF NEURO NSF [IIS-1009542]; ONR [N00014-11-1-0651]; Alfred P. Sloan foundation; DARPA [FA8750-14-2-0009]; NSF MRSEC program through the Princeton Center for Complex Materials Fellowship [DMR-0819860]	NSF CAREER NSF; NSF BIGDATA NSF; NSF NEURO NSF; ONR(Office of Naval Research); Alfred P. Sloan foundation(Alfred P. Sloan Foundation); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF MRSEC program through the Princeton Center for Complex Materials Fellowship	We thank Laurent Charlin, Alp Kucukelbir, Prem Gopolan, Rajesh Ranganath, Linpeng Tang, Neil Houlsby, Marius Kloft, and Matthew Hoffman for discussions. We acknowledge financial support by NSF CAREER NSF IIS-0745520, NSF BIGDATA NSF IIS-1247664, NSF NEURO NSF IIS-1009542, ONR N00014-11-1-0651, the Alfred P. Sloan foundation, DARPA FA8750-14-2-0009 and the NSF MRSEC program through the Princeton Center for Complex Materials Fellowship (DMR-0819860).	Airoldi E. M., 2009, ADV NEURAL INFORM PR, P33; Bishop CM, 2006, PATTERN RECOGNITION, V1; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Gopalan PK, 2013, P NATL ACAD SCI USA, V110, P14534, DOI 10.1073/pnas.1221839110; Hensman J., 2013, UNCERTAINTY ARTIFICI; Hoffman M., 2010, ONLINE LEARNING LATE, P856; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hofman J.M., 2013, ARXIV13111704; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Sato M, 2001, NEURAL COMPUT, V13, P1649, DOI 10.1162/089976601750265045; Schmidt Mark, 2013, 00860051 HAL; Tseng P, 1998, SIAM J OPTIMIZ, V8, P506, DOI 10.1137/S1052623495294797; Wahabzada M, 2011, LECT NOTES ARTIF INT, V6913, P475, DOI 10.1007/978-3-642-23808-6_31; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang C., 2013, ADV NEURAL INFORM PR, P181	17	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103008
C	McDonald, AM; Ponti, M; Stamos, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		McDonald, Andrew M.; Ponti, Massimiliano; Stamos, Dimitris			Spectral k-Support Norm Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				THRESHOLDING ALGORITHM; SHRINKAGE; SELECTION	The k-support norm has successfully been applied to sparse vector prediction problems. We observe that it belongs to a wider class of norms, which we call the box-norms. Within this framework we derive an efficient algorithm to compute the proximity operator of the squared norm, improving upon the original method for the k-support norm. We extend the norms from the vector to the matrix setting and we introduce the spectral k-support norm. We study its properties and show that it is closely related to the multitask learning cluster norm. We apply the norms to real and synthetic matrix completion datasets. Our findings indicate that spectral k-support norm regularization gives state of the art performance, consistently improving over trace norm regularization and the matrix elastic net.	[McDonald, Andrew M.; Ponti, Massimiliano; Stamos, Dimitris] UCL, Dept Comp Sci, London, England	University of London; University College London	McDonald, AM (corresponding author), UCL, Dept Comp Sci, London, England.	a.mcdonald@cs.ucl.ac.uk; m.pontil@cs.ucl.ac.uk; d.stamos@cs.ucl.ac.uk			EPSRC [EP/H027203/1]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to thank Andreas Maurer, Charles Micchelli and especially Andreas Argyriou for useful discussions. Part of this work was supported by EPSRC Grant EP/H027203/1.	Abernethy J, 2009, J MACH LEARN RES, V10, P803; [Anonymous], 1991, FUNCTIONAL ANAL, DOI DOI 10.1070/IM8580; Argyriou A., 2012, ADV NEURAL INFORM PR, P1466; Argyriou A., 2011, ABS11041436 CORR; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D., 2003, CONVEX ANAL OPTIMIZA; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Combettes P.L., 2011, FIXED POINT ALGORITH; Grandvalet Y., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks, P201; Horn R.A., 2013, TOPICS MATRIX ANAL, DOI DOI 10.1017/CBO9780511840371; Jacob L., 2009, ADV NEURAL INFORM PR; Jacob L., 2009, ICML, DOI [10.1145/1553374.1553431, DOI 10.1145/1553374.1553431]; Jaggi M., 2010, ICML; LEWIS A. S., 1995, J CONVEX ANAL, V2, P173; Li H, 2012, IEEE T NEUR NET LEAR, V23, P737, DOI 10.1109/TNNLS.2012.2188906; Marshall A.W, 1979, INEQUALITIES THEORY; Maurer A, 2012, J MACH LEARN RES, V13, P671; Mazumder R, 2010, J MACH LEARN RES, V11, P2287; Micchelli CA, 2005, J MACH LEARN RES, V6, P1099; Micchelli CA, 2013, ADV COMPUT MATH, V38, P455, DOI 10.1007/s10444-011-9245-9; Nesterov Y., 2007, CTR OPERATIONS RES E, V76; Obozinski G., 2012, CORR; Rockafellar R. T., 1970, CONVEX ANAL; Shen ZW, 2011, SIAM J IMAGING SCI, V4, P573, DOI 10.1137/090779437; Srebro N., 2005, ADV NEURAL INFORM PR, V17; Szafranski M., 2007, ADV NEURAL INFORM PR, V21; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Von Neumann J., 1937, TOMSK U REV, VI; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	29	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100076
C	McWilliams, B; Krummenacher, G; Lucic, M; Buhmann, JM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		McWilliams, Brian; Krummenacher, Gabriel; Lucic, Mario; Buhmann, Joachim M.			Fast and Robust Least Squares Estimation in Corrupted Linear Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Subsampling methods have been recently proposed to speed up least squares estimation in large scale settings. However, these algorithms are typically not robust to outliers or corruptions in the observed covariates. The concept of influence that was developed for regression diagnostics can be used to detect such corrupted observations as shown in this paper. This property of influence - for which we also develop a randomized approximation - motivates our proposed subsampling algorithm for large scale corrupted linear regression which limits the influence of data points since highly influential points contribute most to the residual error. Under a general model of corrupted observations, we show theoretically and empirically on a variety of simulated and real datasets that our algorithm improves over the current state-of-the-art approximation schemes for ordinary least squares.	[McWilliams, Brian; Krummenacher, Gabriel; Lucic, Mario; Buhmann, Joachim M.] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	ETH Zurich	McWilliams, B (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	mcbrian@inf.ethz.ch; gabriel.krummenacher@inf.ethz.ch; lucic@inf.ethz.ch; jbuhmann@inf.ethz.ch	Buhmann, Joachim/AAU-4760-2020					Ailon N, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1; Belsley D. A., 1981, REGRESSION DIAGNOSTI; Boutsidis Christos, 2012, ARXIV12040062V4CSDS; Chen Y., 2012, ARXIV12060823; Chen Yudong, 2013, INT C MACH LEARN; Dhillon Paramveer, 2013, ADV NEURAL INFORM PR; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; Drineas Petros, 2011, ARXIV11093843V2CSDS; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1; Loh PL, 2012, ANN STAT, V40, P1637, DOI 10.1214/12-AOS1018; Ma P., 2014, P INT C MACH LEARN; Mahoney Michael W., 2011, ARXIV11045557V3CSDS; McWilliams Brian, 2012, Statistical Analysis and Data Mining, V5, P304, DOI 10.1002/sam.11144; McWilliams B, 2014, DATA MIN KNOWL DISC, V28, P736, DOI 10.1007/s10618-013-0317-y; Tropp Joel A., 2010, ARXIV10111595V4MATHN; Welsch R.E., 1980, EVALUATION ECONOMETR, P153	18	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101099
C	Meier, F; Hennig, P; Schaal, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Meier, Franziska; Hennig, Philipp; Schaal, Stefan			Incremental Local Gaussian Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Locally weighted regression (LWR) was created as a nonparametric method that can approximate a wide range of functions, is computationally efficient, and can learn continually from very large amounts of incrementally collected data. As an interesting feature, LWR can regress on non-stationary functions, a beneficial property, for instance, in control problems. However, it does not provide a proper generative model for function values, and existing algorithms have a variety of manual tuning parameters that strongly influence bias, variance and learning speed of the results. Gaussian (process) regression, on the other hand, does provide a generative model with rather black-box automatic parameter tuning, but it has higher computational cost, especially for big data sets and if a non-stationary model is required. In this paper, we suggest a path from Gaussian (process) regression to locally weighted regression, where we retain the best of both approaches. Using a localizing function basis and approximate inference techniques, we build a Gaussian (process) regression algorithm of increasingly local nature and similar computational complexity to LWR. Empirical evaluations are performed on several synthetic and real robot datasets of increasing complexity and (big) data scale, and demonstrate that we consistently achieve on par or superior performance compared to current state-of-the-art methods while retaining a principled approach to fast incremental regression with minimal manual tuning parameters.	[Meier, Franziska; Schaal, Stefan] Univ Southern Calif, Los Angeles, CA 90089 USA; [Hennig, Philipp; Schaal, Stefan] Max Planck Inst Intelligent Syst, Tubingen, Germany	University of Southern California; Max Planck Society	Meier, F (corresponding author), Univ Southern Calif, Los Angeles, CA 90089 USA.	fmeier@usc.edu; phennig@tue.mpg.de; sschaal@usc.edu						[Anonymous], 2008, ADV NEURAL INFORM PR; Atkeson CG, 1997, ARTIF INTELL REV, V11, P75, DOI 10.1023/A:1006511328852; Broderick T., 2013, ADV NEURAL INFORM PR, V26, P1727; Chalupka K, 2013, J MACH LEARN RES, V14, P333; CSATO L, 2002, NEURAL COMPUTATION; D'Souza Aaron, 2004, ICML; FAN JQ, 1995, J R STAT SOC B, V57, P371; Hastie T., 1993, STAT SCI; Hensman J., 2013, UAI; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Honkela A., 2003, 4 INT S IND COMP AN, P803; Huber MF, 2014, PATTERN RECOGN LETT, V45, P85, DOI 10.1016/j.patrec.2014.03.004; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Luts J, 2014, J COMPUT GRAPH STAT, V23, P589, DOI 10.1080/10618600.2013.810150; Meier Franziska, 2014, P IEEE INT C INT ROB; Moody J., 1988, P 1988 CONN MOD SUMM, P133; Neal R. M., 1996, BAYESIAN LEARNING NE, V118; Quinonero-Candela Joaquin, 2002, NIPS; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A., 2007, NIPS; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Schaal S, 1998, NEURAL COMPUT, V10, P2047, DOI 10.1162/089976698300016963; Schaal S., 2009, TECHNICAL REPORT; Schaal S, 2000, P 17 INT C MACH LEAR, P1079; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Snelson Edward, 2007, P 11 INT C ARTIFICIA, P524; Ting Jo- Anne, 2008, ADV NEURAL INFORM PR, V6, P7; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Wainwright M., 2008, FDN TRENDS MACHINE L	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103045
C	Minh, HQ; San Biagio, M; Murino, V		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Minh, Ha Quang; San Biagio, Marco; Murino, Vittorio			Log-Hilbert-Schmidt metric between positive definite operators on Hilbert spaces	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SIMILARITY; MATRICES	This paper introduces a novel mathematical and computational framework, namely Log-Hilbert-Schmidt metric between positive definite operators on a Hilbert space. This is a generalization of the Log-Euclidean metric on the Riemannian manifold of positive definite matrices to the infinite-dimensional setting. The general framework is applied in particular to compute distances between covariance operators on a Reproducing Kernel Hilbert Space (RKHS), for which we obtain explicit formulas via the corresponding Gram matrices. Empirically, we apply our formulation to the task of multi-category image classification, where each image is represented by an infinite-dimensional RKHS covariance operator. On several challenging datasets, our method significantly outperforms approaches based on covariance matrices computed directly on the original input features, including those using the Log-Euclidean metric, Stein and Jeffreys divergences, achieving new state of the art results.	[Minh, Ha Quang; San Biagio, Marco; Murino, Vittorio] Ist Italiano Tecnol, Via Morego 30, I-16163 Genoa, Italy	Istituto Italiano di Tecnologia - IIT	Minh, HQ (corresponding author), Ist Italiano Tecnol, Via Morego 30, I-16163 Genoa, Italy.	minh.haquang@iit.it; marco.sanbiagio@iit.it; vittorio.murino@iit.it						Andruchow Esteban, 2007, Rev. Unión Mat. Argent., V48, P7; Arsigny V, 2007, SIAM J MATRIX ANAL A, V29, P328, DOI 10.1137/050637996; Bhatia R, 2007, PRINC SER APPL MATH, P1; Bini DA, 2013, LINEAR ALGEBRA APPL, V438, P1700, DOI 10.1016/j.laa.2011.08.052; Boom B. J., 2013, ECOLOGICAL INFORM; Caputo B, 2005, IEEE I CONF COMP VIS, P1597, DOI 10.1109/iccv.2005.54; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Cherian A, 2013, IEEE T PATTERN ANAL, V35, P2161, DOI 10.1109/TPAMI.2012.259; Dryden IL, 2009, ANN APPL STAT, V3, P1102, DOI 10.1214/09-AOAS249; Harandi M, 2014, PROC CVPR IEEE, P1003, DOI 10.1109/CVPR.2014.132; Jayasumana S., 2013, CVPR; Kulis B, 2009, J MACH LEARN RES, V10, P341; Kylberg G., 2011, EXTERNAL REPORT BLUE, V35; Larotonda G., 2005, THESIS; Larotonda G, 2007, DIFFER GEOM APPL, V25, P679, DOI 10.1016/j.difgeo.2007.06.016; Lawson JD, 2001, AM MATH MON, V108, P797, DOI 10.2307/2695553; Li P., 2013, ICCV; Mostow G. D., 1955, MEM AM MATH SOC, V1955, P31; Pennec X, 2006, INT J COMPUT VISION, V66, P41, DOI 10.1007/s11263-005-3222-z; Petryshyn W, 1962, T AM MATH SOC, V105, P136; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Sra S., 2012, NIPS; Tosato D, 2013, IEEE T PATTERN ANAL, V35, P1972, DOI 10.1109/TPAMI.2012.263; Tuzel O, 2008, IEEE T PATTERN ANAL, V30, P1713, DOI 10.1109/TPAMI.2008.75; Wang RP, 2012, PROC CVPR IEEE, P2496, DOI 10.1109/CVPR.2012.6247965; Zhou SK, 2006, IEEE T PATTERN ANAL, V28, P917, DOI 10.1109/TPAMI.2006.120	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102018
C	Moon, KR; Hero, AO		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Moon, Kevin R.; Hero, Alfred O., III			Multivariate f-Divergence EstimationWith Confidence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION; FUNCTIONALS; DEPENDENCY; DENSITIES	The problem of f-divergence estimation is important in the fields of machine learning, information theory, and statistics. While several nonparametric divergence estimators exist, relatively few have known convergence properties. In particular, even for those estimators whose MSE convergence rates are known, the asymptotic distributions are unknown. We establish the asymptotic normality of a recently proposed ensemble estimator of f-divergence between two distributions from a finite number of samples. This estimator has MSE convergence rate of O (1/T), is simple to implement, and performs well in high dimensions. This theory enables us to perform divergence-based inference tasks such as testing equality of pairs of distributions based on empirical samples. We experimentally validate our theoretical results and, as an illustration, use them to empirically bound the best achievable classification error.	[Moon, Kevin R.; Hero, Alfred O., III] Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Moon, KR (corresponding author), Univ Michigan, Dept EECS, Ann Arbor, MI 48109 USA.	krmoon@umich.edu; hero@eecs.umich.edu			NSF [CCF-1217880]; NSF Graduate Research Fellowship [F031543]	NSF(National Science Foundation (NSF)); NSF Graduate Research Fellowship(National Science Foundation (NSF))	This work was partially supported by NSF grant CCF-1217880 and a NSF Graduate Research Fellowship to the first author under Grant No. F031543.	BERLINET A, 1995, STATISTICS, V26, P329, DOI 10.1080/02331889508802500; Berlinet A., 1997, PUBLICATIONS I STAT, V41, P3; BICKEL PJ, 1973, ANN STAT, V1, P1071, DOI 10.1214/aos/1176342558; Blum J.R., 1958, CANADIAN J MATH, V10, P222, DOI DOI 10.4153/CJM-1958-026-0; Carter KM, 2010, IEEE T SIGNAL PROCES, V58, P650, DOI 10.1109/TSP.2009.2031722; Cover TM, 2006, ELEMENTS INFORM THEO; Csiszar I., 1967, STUD SCI MATH HUNG, V2, P299; Dhillon I. S., 2003, Journal of Machine Learning Research, V3, P1265, DOI 10.1162/153244303322753661; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Hero AO, 2002, IEEE SIGNAL PROC MAG, V19, P85, DOI 10.1109/MSP.2002.1028355; Krishnamurthy A., 2014, INT C MACH LEARN, V32; KULLBACK S, 1951, ANN MATH STAT, V22, P79, DOI 10.1214/aoms/1177729694; Lichman M, 2013, UCI MACHINE LEARNING; LOFTSGAARDEN DO, 1965, ANN MATH STAT, V36, P1049, DOI 10.1214/aoms/1177700079; Moon K. R., 2014, CORR; Moon KR, 2014, IEEE INT SYMP INFO, P356, DOI 10.1109/ISIT.2014.6874854; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Oliva J., 2013, PROC 30 INT C MACH L, P1049; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Poczos B., 2011, P 14 INT C ARTIFICIA, P609; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Silva J, 2010, J STAT PLAN INFER, V140, P3180, DOI 10.1016/j.jspi.2010.04.011; Singh S, 2014, PR MACH LEARN RES, V32; Sricharan K., 2012, THESIS; Sricharan K., 2012, ADV NEURAL INFORM PR, P575; Sricharan K, 2012, IEEE T INFORM THEORY, V58, P4135, DOI 10.1109/TIT.2012.2195549; Le TK, 2013, J STAT PLAN INFER, V143, P2089, DOI 10.1016/j.jspi.2013.08.007; Wang Q, 2005, IEEE T INFORM THEORY, V51, P3064, DOI 10.1109/TIT.2005.853314; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060; [No title captured]	34	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102051
C	Muandet, K; Sriperumbudur, B; Scholkopf, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Muandet, Krikamol; Sriperumbudur, Bharath; Schoelkopf, Bernhard			Kernel Mean Estimation via Spectral Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The problem of estimating the kernel mean in a reproducing kernel Hilbert space (RKHS) is central to kernel methods in that it is used by classical approaches (e.g., when centering a kernel PCA matrix), and it also forms the core inference step of modern kernel methods (e.g., kernel-based non-parametric tests) that rely on embedding probability distributions in RKHSs. Previous work [1] has shown that shrinkage can help in constructing "better" estimators of the kernel mean than the empirical estimator. The present paper studies the consistency and admissibility of the estimators in [1], and proposes a wider class of shrinkage estimators that improve upon the empirical estimator by considering appropriate basis functions. Using the kernel PCA basis, we show that some of these estimators can be constructed using spectral filtering algorithms which are shown to be consistent under some technical assumptions. Our theoretical analysis also reveals a fundamental connection to the kernel-based supervised learning framework. The proposed estimators are simple to implement and perform well in practice.	[Muandet, Krikamol; Schoelkopf, Bernhard] MPI IS, Tubingen, Germany; [Sriperumbudur, Bharath] PSU, Dept Stat, University Pk, PA USA		Muandet, K (corresponding author), MPI IS, Tubingen, Germany.	krikamol@tue.mpg.de; bks18@psu.edu; bs@tue.mpg.de	Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				[Anonymous], 2008, SUPPORT VECTOR MACHI; Baldassarre L, 2010, LECT NOTES ARTIF INT, V6321, P56, DOI 10.1007/978-3-642-15880-3_10; Berlinet A., 2004, REPRODUCING KERNEL H, DOI [10.1007/978-1-4419-9096-9, DOI 10.1007/978-1-4419-9096-9]; Engl H., 1996, MATH ITS APPL, V375; Fukumizu K, 2013, J MACH LEARN RES, V14, P3753; GOLUB GH, 1979, TECHNOMETRICS, V21, P215, DOI 10.1080/00401706.1979.10489751; Gretton A., 2007, NIPS; Kim J, 2012, J MACH LEARN RES, V13, P2529; Lepski OV, 1997, ANN STAT, V25, P929; Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517; Muandet K., 2012, PROC 25 INT C NEURAL, P10; Muandet K., 2013, P 29 C UNC ART INT, P449; Muandet K., 2014, P 31 INT C MACH LEAR, P10; Muandet Krikamol, 2013, ICML; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Scholkopf B., 2001, LEARNING KERNELS SUP; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; SONG L, 2013, P ADV NEUR INF PROC, P3228; Song L., 2009, ICML; Song Le, 2008, P 25 INT C MACHINE L, P992; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Vito E.D., 2006, SPECTRAL METHODS REG; [No title captured]	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100018
C	Nakajima, S; Sato, I; Sugiyama, M; Watanabe, K; Kobayashi, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nakajima, Shinichi; Sato, Issei; Sugiyama, Masashi; Watanabe, Kazuho; Kobayashi, Hiroko			Analysis of Variational Bayesian Latent Dirichlet Allocation: Weaker Sparsity than MAP	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				STOCHASTIC COMPLEXITIES; MODELS	Latent Dirichlet allocation (LDA) is a popular generative model of various objects such as texts and images, where an object is expressed as a mixture of latent topics. In this paper, we theoretically investigate variational Bayesian (VB) learning in LDA. More specifically, we analytically derive the leading term of the VB free energy under an asymptotic setup, and show that there exist transition thresholds in Dirichlet hyperparameters around which the sparsity-inducing behavior drastically changes. Then we further theoretically reveal the notable phenomenon that VB tends to induce weaker sparsity than MAP in the LDA model, which is opposed to other models. We experimentally demonstrate the practical validity of our asymptotic theory on real-world Last. FM music data.	[Nakajima, Shinichi] TU Berlin, Berlin Big Data Ctr, D-10587 Berlin, Germany; [Sato, Issei; Sugiyama, Masashi] Univ Tokyo, Tokyo 1130033, Japan; [Watanabe, Kazuho] Toyohashi Univ Technol, Toyohashi, Aichi 4418580, Japan; [Kobayashi, Hiroko] Nikon Inc, Sagamihara, Kanagawa 2448533, Japan	Technical University of Berlin; University of Tokyo; Toyohashi University of Technology; Nikon Corporation	Nakajima, S (corresponding author), TU Berlin, Berlin Big Data Ctr, D-10587 Berlin, Germany.	nakajima@tu-berlin.de; sato@r.dl.itc.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp; wkazuho@cs.tut.ac.jp; hiroko.kobayashi@nikon.com	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	Nikon Corporation; MEXT Kakenhi [23120004]; Berlin Big Data Center project [FKZ 01IS14013A]; JST CREST program; JSPS Kakenhi [23700175, 25120014]	Nikon Corporation; MEXT Kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); Berlin Big Data Center project; JST CREST program(Core Research for Evolutional Science and Technology (CREST)Japan Science & Technology Agency (JST)); JSPS Kakenhi(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	The authors thank the reviewers for helpful comments. Shinichi Nakajima thanks the support from Nikon Corporation, MEXT Kakenhi 23120004, and the Berlin Big Data Center project (FKZ 01IS14013A). Masashi Sugiyama thanks the support from the JST CREST program. Kazuho Watanabe thanks the support from JSPS Kakenhi 23700175 and 25120014.	Alzer H, 1997, MATH COMPUT, V66, P373, DOI 10.1090/S0025-5718-97-00807-7; [Anonymous], 2011, P 28 INT C INT C MAC; Asuncion A., 2009, P 25 C UNCERTAINTY A, P27, DOI DOI 10.1080/10807030390248483; Attias H, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P21; Bicego Manuele, 2010, Proceedings of the 2010 20th International Conference on Pattern Recognition (ICPR 2010), P2728, DOI 10.1109/ICPR.2010.668; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Chen X, 2010, IEEE INT C BIOINFORM, P149, DOI 10.1109/BIBM.2010.5706554; Fei-Fei L, 2005, PROC CVPR IEEE, P524; Ghahramani Z, 2001, NEU INF PRO, P161; Girolami M., 2003, P 26 ANN INT ACM SIG, P433, DOI [10.1145/860435.860537, DOI 10.1145/860435.860537]; Gopalan P., 2013, ARXIV13111704CSIR; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; Hosino T., 2006, IEICE T INF SYST, VJ89-D, P1279; Huynh T., 2008, INT C UB COMP UBICOM; Kaji D., 2010, AUSTR J INTELLIGENT, V35, P35; Krestel R., 2009, ACM C RECOMMENDER SY, P61, DOI [10.1145/1639714.1639726, DOI 10.1145/1639714.1639726]; Mukherjee I., 2008, ADV NIPS; Nakajima S., 2011, J MACHINE LEARNING R, V12, P2579; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Purushotham S, 2012, P ICML; Teh Y. W, 2007, ADV NIPS; Ueda N, 2000, NEURAL COMPUT, V12, P2109, DOI 10.1162/089976600300015088; Watanabe K, 2006, J MACH LEARN RES, V7, P625; Watanabe K, 2007, NEURAL NETWORKS, V20, P210, DOI 10.1016/j.neunet.2006.05.030; Watanabe K, 2009, MACH LEARN, V75, P199, DOI 10.1007/s10994-008-5099-x; Xing Wei, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P178	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103009
C	Nishihara, R; Jegelka, S; Jordan, MI		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Nishihara, Robert; Jegelka, Stefanie; Jordan, Michael, I			On the Convergence Rate of Decomposable Submodular Function Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CYCLIC PROJECTIONS ALGORITHM; SUBSPACES; ANGLES	Submodular functions describe a variety of discrete problems in machine learning, signal processing, and computer vision. However, minimizing submodular functions poses a number of algorithmic challenges. Recent work introduced an easy-to-use, parallelizable algorithm for minimizing submodular functions that decompose as the sum of "simple" submodular functions. Empirically, this algorithm performs extremely well, but no theoretical analysis was given. In this paper, we show that the algorithm converges linearly, and we provide upper and lower bounds on the rate of convergence. Our proof relies on the geometry of submodular polyhedra and draws on results from spectral graph theory.	[Nishihara, Robert; Jegelka, Stefanie; Jordan, Michael, I] Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Nishihara, R (corresponding author), Univ Calif Berkeley, Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	rkn@eecs.berkeley.edu; stefje@eecs.berkeley.edu; jordan@eecs.berkeley.edu	Jordan, Michael I/C-5253-2013		NSF CISE Expeditions Award [CCF-1139158]; LBNL Award [7076018]; DARPA XData Award [FA8750-12-2-0331]; Office of Naval Research [N00014-11-1-0688]; US ARL; US ARO [W911NF-11-1-0391]; NSF [DGE-1106400]	NSF CISE Expeditions Award; LBNL Award; DARPA XData Award; Office of Naval Research(Office of Naval Research); US ARL; US ARO; NSF(National Science Foundation (NSF))	We would like to thank Madalina Persu for suggesting the use of Cheeger's inequality. This research is supported in part by NSF CISE Expeditions Award CCF-1139158, LBNL Award 7076018, and DARPA XData Award FA8750-12-2-0331, and gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!. This work is supported in part by the Office of Naval Research under grant number N00014-11-1-0688, the US ARL and the US ARO under grant number W911NF-11-1-0391, and the NSF under grant number DGE-1106400.	Bach F, 2013, FOUND TRENDS MACH LE, V6, P145, DOI 10.1561/2200000039; Bauschke H. H., 1993, SET VALUED ANAL, V1, P185, DOI DOI 10.1007/BF01027691; Bauschke HH, 2014, J APPROX THEORY, V185, P63, DOI 10.1016/j.jat.2014.06.002; BAUSCHKE HH, 1994, J APPROX THEORY, V79, P418, DOI 10.1006/jath.1994.1136; Bauschke HH, 1996, SIAM REV, V38, P367, DOI 10.1137/S0036144593251710; Beck A, 2013, SIAM J OPTIMIZ, V23, P2037, DOI 10.1137/120887679; Chung F., 1997, AM MATH SOC, DOI 10.1090/cbms/092; DEUTSCH F, 1992, NATO ADV SCI I C-MAT, V356, P105; DEUTSCH F, 1994, NUMER FUNC ANAL OPT, V15, P537, DOI 10.1080/01630569408816580; Deutsch F, 2006, J APPROX THEORY, V142, P36, DOI 10.1016/j.jat.2006.02.005; Diaconis P, 2010, ILLINOIS J MATH, V54, P963, DOI 10.1215/ijm/1336568522; Edmonds J., 1970, SUBMODULAR FUNCTIONS, P69; Fix A., 2013, INT C COMP VIS ICCV; Fujishige S, 2011, PAC J OPTIM, V7, P3; Gray RM, 2006, FOUND TRENDS COMMUN, V2, DOI 10.1561/0100000006; GROTSCHEL M, 1981, COMBINATORICA, V1, P169, DOI 10.1007/BF02579273; Halperin I., 1962, ACTA SCI MATH SZEGED, V23, P96; Hochbaum D., 2009, INT C COMP VIS ICCV; Iwata S, 2003, SIAM J COMPUT, V32, P833, DOI 10.1137/S0097539701397813; Jegelka S., 2011, ADV NEURAL INFORM PR; Jegelka S., 2013, ADV NEURAL INFO PROC, P1313; Jenatton R., 2011, JMLR; Knyazev AV, 2002, SIAM J SCI COMPUT, V23, P2008, DOI 10.1137/S1064827500377332; Kohli P., 2009, INT J COMPUTER VISIO, V82; Kolmogorov V, 2012, DISCRETE APPL MATH, V160, P2246, DOI 10.1016/j.dam.2012.05.025; Komodakis N., 2011, IEEE T PATTERN ANAL; Lin  H., 2011, P INTERSPEECH; McCormick S. T., 2006, HDB DISCRETE OPTIMIZ, P321; Moulines E., 2011, ADV NEURAL INF PROCE, V24; Narasimhan M, 2007, 20TH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P981; Neumann J., 1950, FUNCTIONAL OPERATORS, V2; Orlin JB, 2009, MATH PROGRAM, V118, P237, DOI 10.1007/s10107-007-0189-2; Raik E., 1967, USSR COMP MATH MATH, V7, P1, DOI DOI 10.1016/0041-5553(67)90113-9; Stobbe P., 2010, ADV NEURAL INFORM PR; Tseng P, 1997, SIAM J OPTIMIZ, V7, P951, DOI 10.1137/S1052623495279797; Vicente S., 2009, INT C COMP VIS ICCV	38	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100034
C	O'Connor, L; Feizi, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		O'Connor, Luke; Feizi, Soheil			Biclustering Using Message Passing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Biclustering is the analog of clustering on a bipartite graph. Existent methods infer biclusters through local search strategies that find one cluster at a time; a common technique is to update the row memberships based on the current column memberships, and vice versa. We propose a biclustering algorithm that maximizes a global objective function using message passing. Our objective function closely approximates a general likelihood function, separating a cluster size penalty term into row- and column-count penalties. Because we use a global optimization framework, our approach excels at resolving the overlaps between biclusters, which are important features of biclusters in practice. Moreover, Expectation-Maximization can be used to learn the model parameters if they are unknown. In simulations, we find that our method outperforms two of the best existing biclustering algorithms, ISA and LAS, when the planted clusters overlap. Applied to three gene expression datasets, our method finds coregulated gene clusters that have high quality in terms of cluster size and density.	[O'Connor, Luke] Harvard Univ, Bioinformat & Integrat Genom, Cambridge, MA 02138 USA; [Feizi, Soheil] MIT, Elect Engn & Comp Sci, Cambridge, MA 02139 USA	Harvard University; Massachusetts Institute of Technology (MIT)	O'Connor, L (corresponding author), Harvard Univ, Bioinformat & Integrat Genom, Cambridge, MA 02138 USA.	loconnor@g.harvard.edu; sfeizi@mit.edu			Harvard Division of Medical Sciences	Harvard Division of Medical Sciences	We would like to thank Professor Manolis Kellis and Professor Muriel Medard for their advice and support. We would like to thank the Harvard Division of Medical Sciences for supporting this project.	Bergmann S, 2003, PHYS REV E, V67, DOI 10.1103/PhysRevE.67.031902; Bisson Gilles, 2008, MACH LEARN APPL 2008; Caldas Jose, 2008, MACH LEARN SIGN PROC; Cheng Yizong, 2000, ISMB, V8; Dao P, 2010, BIOINFORMATICS, V26, pi625, DOI 10.1093/bioinformatics/btq393; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dueck Delbert, 2008, RES COMPUTATIONAL MO; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Govaert G, 2008, COMPUT STAT DATA AN, V52, P3233, DOI 10.1016/j.csda.2007.09.007; Krzakala F, 2013, P NATL ACAD SCI USA, V110, P20935, DOI 10.1073/pnas.1312486110; Li L, 2012, BMC BIOPHYS, V5, DOI 10.1186/2046-1682-5-9; Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/NMETH.2016, 10.1038/nmeth.2016]; Nadakuditi RR, 2012, PHYS REV LETT, V108, DOI 10.1103/PhysRevLett.108.188701; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Prelic A, 2006, BIOINFORMATICS, V22, P1122, DOI 10.1093/bioinformatics/btl060; Shabalin AA, 2009, ANN APPL STAT, V3, P985, DOI 10.1214/09-AOAS239; Tanay Amos, 2002, Bioinformatics, V18 Suppl 1, pS136; Yedidia JS, 2005, IEEE T INFORM THEORY, V51, P2282, DOI 10.1109/TIT.2005.850085	19	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103054
C	Park, D; Caramanis, C; Sanghavi, S		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Park, Dohyung; Caramanis, Constantine; Sanghavi, Sujay			Greedy Subspace Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FACE RECOGNITION	We consider the problem of subspace clustering: given points that lie on or near the union of many low-dimensional linear subspaces, recover the subspaces. To this end, one first identifies sets of points close to the same subspace and uses the sets to estimate the subspaces. As the geometric structure of the clusters (linear subspaces) forbids proper performance of general distance based approaches such as K-means, many model-specific methods have been proposed. In this paper, we provide new simple and efficient algorithms for this problem. Our statistical analysis shows that the algorithms are guaranteed exact (perfect) clustering performance under certain conditions on the number of points and the affinity between subspaces. These conditions are weaker than those considered in the standard statistical literature. Experimental results on synthetic data generated from the standard unions of subspaces model demonstrate our theory. We also show that our algorithm performs competitively against state-of-the-art algorithms on real-world applications such as motion segmentation and face clustering, with much simpler implementation and lower computational cost.	[Park, Dohyung; Caramanis, Constantine; Sanghavi, Sujay] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Park, D (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	dhpark@utexas.edu; constantine@utexas.edu; sanghavi@mail.utexas.edu			NSF [1302435, 0954059, 1017525, 1056028]; DTRA [HDTRA1-13-1-0024]; U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center	NSF(National Science Foundation (NSF)); DTRA(United States Department of DefenseDefense Threat Reduction Agency); U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center	The authors would like to acknowledge NSF grants 1302435, 0954059, 1017525, 1056028 and DTRA grant HDTRA1-13-1-0024 for supporting this research. This research was also partially supported by the U.S. Department of Transportation through the Data-Supported Transportation Operations and Planning (D-STOP) Tier 1 University Transportation Center.	[Anonymous], [No title captured]; Bradley PS, 2000, J GLOBAL OPTIM, V16, P23, DOI 10.1023/A:1008324625522; Chen GL, 2009, INT J COMPUT VISION, V81, P317, DOI 10.1007/s11263-008-0178-9; Dyer EL, 2013, J MACH LEARN RES, V14, P2487; Elhamifar E, 2013, IEEE T PATTERN ANAL, V35, P2765, DOI 10.1109/TPAMI.2013.57; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Heckel R., 2014, ARXIV13074891V2; Heckel R., 2013, IEEE INT C AC SPEECH; HO J, 2003, IEEE C COMP VIS PATT; Inglot T, 2010, PROBAB MATH STAT-POL, V30, P339; Kriegel HP, 2009, ACM T KNOWL DISCOV D, V3, DOI 10.1145/1497577.1497578; Kunis S, 2008, FOUND COMPUT MATH, V8, P737, DOI 10.1007/s10208-007-9005-x; Ledoux M., 2005, CONCENTRATION MEASUR, V89; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Liu GC, 2013, IEEE T PATTERN ANAL, V35, P171, DOI 10.1109/TPAMI.2012.88; Milman V. D., 1986, LECT NOTES MATH; Soltanolkotabi M, 2012, ANN STAT, V40, P2195, DOI 10.1214/12-AOS1034; Tron Roberto, 2007, IEEE C COMP VIS PATT; Tropp JA, 2007, IEEE T INFORM THEORY, V53, P4655, DOI 10.1109/TIT.2007.909108; Tseng P, 2000, J OPTIMIZ THEORY APP, V105, P249, DOI 10.1023/A:1004678431677; Vidal R, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P167, DOI 10.1109/cdc.2003.1272554; Vidal R, 2008, INT J COMPUT VISION, V79, P85, DOI 10.1007/s11263-007-0099-z; Vidal R, 2011, IEEE SIGNAL PROC MAG, V28, P52, DOI 10.1109/MSP.2010.939739; Wang Y., 2013, ADV NEURAL INFORM PR; Yang A. Y., 2006, IEEE C COMP VIS PATT; Zhang T, 2012, INT J COMPUT VISION, V100, P217, DOI 10.1007/s11263-012-0535-6	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100087
C	Patrini, G; Nock, R; Rivera, P; Caetano, T		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Patrini, Giorgio; Nock, Richard; Rivera, Paul; Caetano, Tiberio			(Almost) No Label No Cry	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				FRAMEWORK	In Learning with Label Proportions (LLP), the objective is to learn a supervised classifier when, instead of labels, only label proportions for bags of observations are known. This setting has broad practical relevance, in particular for privacy preserving data processing. We first show that the mean operator, a statistic which aggregates all labels, is minimally sufficient for the minimization of many proper scoring losses with linear (or kernelized) classifiers without using labels. We provide a fast learning algorithm that estimates the mean operator via a manifold regularizer with guaranteed approximation bounds. Then, we present an iterative learning algorithm that uses this as initialization. We ground this algorithm in Rademacher-style generalization bounds that fit the LLP setting, introducing a generalization of Rademacher complexity and a Label Proportion Complexity measure. This latter algorithm optimizes tractable bounds for the corresponding bag-empirical risk. Experiments are provided on fourteen domains, whose size ranges up to approximate to 300K observations. They display that our algorithms are scalable and tend to consistently outperform the state of the art in LLP. Moreover, in many cases, our algorithms compete with or are just percents of AUC away from the Oracle that learns knowing all labels. On the largest domains, half a dozen proportions can suffice, i.e. roughly 40K times less than the total number of labels.	[Patrini, Giorgio; Nock, Richard; Rivera, Paul; Caetano, Tiberio] Australian Natl Univ, Sydney, NSW, Australia; [Patrini, Giorgio; Nock, Richard; Rivera, Paul] NICTA, Sydney, NSW, Australia; [Caetano, Tiberio] Univ New South Wales, Sydney, NSW, Australia; [Caetano, Tiberio] Ambiata, Sydney, NSW, Australia	Australian National University; Australian National University; University of New South Wales Sydney	Patrini, G (corresponding author), Australian Natl Univ, Sydney, NSW, Australia.	giorgio.patrini@anu.edu.au; richard.nock@anu.edu.au; paul.rivera@anu.edu.au; tiberio.caetano@anu.edu.au			Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence Program	Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program. The first author would like to acknowledge that part of this research was conducted during his internship at the Commonwealth Bank of Australia. We thank A. Menon and D. Garcia-Garcia for useful discussions.	Altun Y, 2006, LECT NOTES ARTIF INT, V4005, P139, DOI 10.1007/11776420_13; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Beygelzimer A, 2005, P 22 INT C MACH LEAR, P49; Chen B.-C., 2006, 22 INT C DAT ENG ICD, P3; Chen S, 2009, INT CONF DAT MIN WOR, P356, DOI 10.1109/ICDMW.2009.33; de Freitas N., 2005, P 21 C UNC ART INT, P332; Dietterich TG, 1997, ARTIF INTELL, V89, P31, DOI 10.1016/S0004-3702(96)00034-3; Fan K, 2014, NEUROCOMPUTING, V139, P47, DOI 10.1016/j.neucom.2013.09.057; Graca Joao, 2007, ADV NEURAL INFORM PR, P569; Hernandez-Gonzalez J, 2013, PATTERN RECOGN, V46, P3425, DOI 10.1016/j.patcog.2013.05.002; Kearns M., 1996, Proceedings of the Twenty-Eighth Annual ACM Symposium on the Theory of Computing, P459, DOI 10.1145/237814.237994; Koltchinskii V, 2002, ANN STAT, V30, P1; Lai K. -T., 2014, 11 CVPR; Liang Percy, 2009, P 26 ANN INT C MACH, P641; Lichman M, 2013, UCI MACHINE LEARNING; Mann G. -S., 2008, 46 ACL; Musicant DR, 2007, IEEE DATA MINING, P252, DOI 10.1109/ICDM.2007.50; Nock R, 2009, IEEE T PATTERN ANAL, V31, P2048, DOI 10.1109/TPAMI.2008.225; Patrini G., 2014, NIPS 27; Quadrianto N, 2009, J MACH LEARN RES, V10, P2349; Ruping S., 2010, P 27 INT C MACH LEAR, P911; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Stolpe M, 2011, LECT NOTES ARTIF INT, V6913, P349, DOI 10.1007/978-3-642-23808-6_23; Wojtusiak J., 2011, Proceedings of the 2011 Tenth International Conference on Machine Learning and Applications (ICMLA 2011), P84, DOI 10.1109/ICMLA.2011.154; Yu F., 2014, CORR; Yu Felix X., 2013, P 30 INT C MACH LEAR, P504	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102014
C	Prasad, A; Jegelka, S; Batra, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Prasad, Adarsh; Jegelka, Stefanie; Batra, Dhruv			Submodular meets Structured: Finding Diverse Subsets in Exponentially-Large Structured Item Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					To cope with the high level of ambiguity faced in domains such as Computer Vision or Natural Language processing, robust prediction methods often search for a diverse set of high-quality candidate solutions or proposals. In structured prediction problems, this becomes a daunting task, as the solution space (image labelings, sentence parses, etc.) is exponentially large. We study greedy algorithms for finding a diverse subset of solutions in structured-output spaces by drawing new connections between submodular functions over combinatorial item sets and High-Order Potentials (HOPs) studied for graphical models. Specifically, we show via examples that when marginal gains of submodular diversity functions allow structured representations, this enables efficient (sub-linear time) approximate maximization by reducing the greedy augmentation step to inference in a factor graph with appropriately constructed HOPs. We discuss benefits, trade-offs, and show that our constructions lead to significantly better proposals.	[Prasad, Adarsh] UT Austin, Austin, TX 78712 USA; [Jegelka, Stefanie] Univ Calif Berkeley, Berkeley, CA USA; [Batra, Dhruv] Virginia Tech, Blacksburg, VA USA	University of Texas System; University of Texas Austin; University of California System; University of California Berkeley; Virginia Polytechnic Institute & State University	Prasad, A (corresponding author), UT Austin, Austin, TX 78712 USA.	adarsh@cs.utexas.edu; stefje@eecs.berkeley.edu; dbatra@vt.edu			National Science Foundation [IIS-1353694, IIS-1350553]; Army Research Office YIP Award [W911NF-14-1-0180]; Office of Naval Research Award [N00014-14-1-0679]	National Science Foundation(National Science Foundation (NSF)); Army Research Office YIP Award; Office of Naval Research Award(Office of Naval Research)	We thank Xiao Lin for his help. The majority of this work was done while AP was an intern at Virginia Tech. AP and DB were partially supported by the National Science Foundation under Grant No. IIS-1353694 and IIS-1350553, the Army Research Office YIP Award W911NF-14-1-0180, and the Office of Naval Research Award N00014-14-1-0679, awarded to DB. SJ was supported by gifts from Amazon Web Services, Google, SAP, The Thomas and Stacey Siebel Foundation, Apple, C3Energy, Cisco, Cloudera, EMC, Ericsson, Facebook, GameOnTalis, Guavus, HP, Huawei, Intel, Microsoft, NetApp, Pivotal, Splunk, Virdata, VMware, WANdisco, and Yahoo!.	[Anonymous], 2006, IEEE T AUTOM SCI ENG; Batra D., 2012, UAI; Batra D., 2012, ECCV; Boykov Y, 2001, IEEE T PATTERN ANAL, V23, P1222, DOI 10.1109/34.969114; Buchbinder N., 2012, FOCS; Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Chen C., 2013, AISTATS; Chen C., 2014, NIPS; Delong A, 2010, PROC CVPR IEEE, P2173, DOI 10.1109/CVPR.2010.5539897; Dey D, 2012, ROBOTICS SCI SYSTEMS; Everingham M., 2012, PASCAL VISUAL OBJECT; Feige U., 2007, FOCS; Goundan P., 2009, REVISITING GRE UNPUB; Guzman-Rivera A., 2012, P NIPS; Guzman-Rivera A., 2014, AISTATS; Jegelka S., 2011, CVPR; Kempe D., 2003, ACM SIGKDD C KNOWL D; Kohli P., 2013, CVPR; Kohli P., 2010, CVPR; Kolmogorov V, 2004, IEEE T PATTERN ANAL, V26, P147, DOI 10.1109/TPAMI.2004.1262177; Krause A., 2013, ICML TUTORIAL; Krause A, 2008, J MACH LEARN RES, V9, P235; Kulesza A., 2010, P NIPS; LAFFERTY J, 2001, ICML; Lin H., 2011, ACL; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Park D., 2011, ICCV; Streeter M., 2008, NIPS; Tarlow Daniel, 2010, INT C ART INT STAT, P812; Taskar Ben, 2003, NIPS; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Yanover C., 2003, NIPS	34	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100104
C	Putzky, P; Franzen, F; Bassetto, G; Macke, JH		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Putzky, Patrick; Franzen, Florian; Bassetto, Giacomo; Macke, Jakob H.			A Bayesian model for identifying hierarchically organised states in neural population activity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Neural population activity in cortical circuits is not solely driven by external inputs, but is also modulated by endogenous states which vary on multiple time-scales. To understand information processing in cortical circuits, we need to understand the statistical structure of internal states and their interaction with sensory inputs. Here, we present a statistical model for extracting hierarchically organised neural population states from multi-channel recordings of neural spiking activity. Population states are modelled using a hidden Markov decision tree with state-dependent tuning parameters and a generalised linear observation model. We present a variational Bayesian inference algorithm for estimating the posterior distribution over parameters from neural population recordings. On simulated data, we show that we can identify the underlying sequence of population states and reconstruct the ground truth parameters. Using population recordings from visual cortex, we find that a model with two levels of population states outperforms both a one-state and a two-state generalised linear model. Finally, we find that modelling of state-dependence also improves the accuracy with which sensory stimuli can be decoded from the population response.	[Putzky, Patrick; Franzen, Florian; Bassetto, Giacomo; Macke, Jakob H.] Max Planck Inst Biol Cybernet, Tubingen, Germany; [Putzky, Patrick; Franzen, Florian] Univ Tubingen, Grad Training Ctr Neurosci, Tubingen, Germany; [Putzky, Patrick; Franzen, Florian; Bassetto, Giacomo; Macke, Jakob H.] Bernstein Ctr Computat Neurosci, Tubingen, Germany	Max Planck Society; Eberhard Karls University of Tubingen	Putzky, P (corresponding author), Max Planck Inst Biol Cybernet, Tubingen, Germany.	patrick.putzky@gmail.com; florian.franzen@tuebingen.mpg.de; giacomo.bassetto@tuebingen.mpg.de; jakob@tuebingen.mpg.de			German Federal Ministry of Education and Research (BMBF) [FKZ: 01GQ1002]; Max Planck Society	German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); Max Planck Society(Max Planck SocietyFoundation CELLEX)	We are grateful to the authors of [2] for sharing their data (toliaslab.org/publications/ecker-et-al-2014/) and to Alexander Ecker, William McGhee, Marcel Nonnenmacher and David Janssen for comments on the manuscript. This work was funded by the German Federal Ministry of Education and Research (BMBF; FKZ: 01GQ1002, Bernstein Center Tubingen) and the Max Planck Society. Supplementary details and code are available at www.mackelab.org.	Aston-Jones G, 2005, ANNU REV NEUROSCI, V28, P403, DOI 10.1146/annurev.neuro.28.061604.135709; Beal Matthew James, 2003, VARIATIONAL ALGORITH; Bengio Y., 1995, Advances in Neural Information Processing Systems 7, P427; Bezdudnaya T, 2006, NEURON, V49, P421, DOI 10.1016/j.neuron.2006.01.010; Bishop C. M., 2003, P 19 C UNC ART INT, P57; Bishop C.M, 2006, PATTERN RECOGN; Chen Z., 2009, NEURAL COMPUTATION, V21; Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006; Escola S., 2011, NEURAL COMPUTATION, V23; Gerwinn S, 2010, FRONT COMPUT NEUROSC, V4, DOI 10.3389/fncom.2010.00012; Harris K. D., 2011, NATURE REV NEUROSCIE, V12; Jaakkola T. S., 1996, VARIATIONAL APPROACH; Jones L. M., 2007, PNAS, V104; Jordan M. I., 1994, NEURAL COMPUTATION, V6; Jordan MI, 1997, ADV NEUR IN, V9, P501; Kisley MA, 1999, J NEUROSCI, V19, P10451; Lewi J., 2009, NEURAL COMPUTATION, V21; Logothetis NK, 2012, NATURE, V491, P547, DOI 10.1038/nature11618; Mackay D. J. C., 1997, TECH REP; MacKay DJC, 1994, ASHRAE T, V100, P1053; Macke JH, 2011, ADV NEURAL INFORM PR, V24; Paninski L, 2007, PROG BRAIN RES, V165, P493, DOI 10.1016/S0079-6123(06)65031-0; Shababo B., 2013, ADV NEURAL INFORM PR, V26, P1304; Steriade M. M., 2005, BRAIN CONTROL WAKEFU; Zagha E, 2013, NEURON, V79, P567, DOI 10.1016/j.neuron.2013.06.008	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101009
C	Sadowski, P; Baldi, P; Whiteson, D		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sadowski, Peter; Baldi, Pierre; Whiteson, Daniel			Searching for Higgs Boson Decay Modes with Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Particle colliders enable us to probe the fundamental nature of matter by observing exotic particles produced by high-energy collisions. Because the experimental measurements from these collisions are necessarily incomplete and imprecise, machine learning algorithms play a major role in the analysis of experimental data. The high-energy physics community typically relies on standardized machine learning software packages for this analysis, and devotes substantial effort towards improving statistical power by hand-crafting high-level features derived from the raw collider measurements. In this paper, we train artificial neural networks to detect the decay of the Higgs boson to tau leptons on a dataset of 82 million simulated collision events. We demonstrate that deep neural network architectures are particularly well-suited for this task with the ability to automatically discover high-level features from the data and increase discovery significance.	[Sadowski, Peter; Baldi, Pierre] Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92617 USA; [Whiteson, Daniel] Univ Calif Irvine, Dept Phys & Astron, Irvine, CA 92617 USA	University of California System; University of California Irvine; University of California System; University of California Irvine	Sadowski, P (corresponding author), Univ Calif Irvine, Dept Comp Sci, Irvine, CA 92617 USA.	peter.j.sadowski@uci.edu; pfbaldi@ics.uci.edu; daniel@uci.edu						Aad G, 2012, SCIENCE, V338, P1576, DOI 10.1126/science.1232005; Alwall J, 2011, J HIGH ENERGY PHYS, DOI 10.1007/JHEP06(2011)128; [Anonymous], 2010, PYTH SCI COMP C; Chatrchyan S, 2012, SCIENCE, V338, P1569, DOI 10.1126/science.1230816; Cowan G, 2011, EUR PHYS J C, V71, DOI 10.1140/epjc/s10052-011-1554-0; Denby B, 1999, COMPUT PHYS COMMUN, V119, P219, DOI 10.1016/S0010-4655(98)00199-4; Elagin A, 2011, NUCL INSTRUM METH A, V654, P481, DOI 10.1016/j.nima.2011.07.009; Goodfellow I. J., 2013, 13084214 ARXIV; Hocker A., 2007, POS ACAT 040; Ovyn S., 2009, DELPHES FRAMEWORK FA; Sjostrand T, 2006, J HIGH ENERGY PHYS, DOI 10.1088/1126-6708/2006/05/026	11	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101022
C	Semedo, JD; Zandvakili, A; Kohn, A; Machens, CK; Yu, BM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Semedo, Joao D.; Zandvakili, Amin; Kohn, Adam; Machens, Christian K.; Yu, Byron M.			Extracting Latent Structure From Multiple Interacting Neural Populations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				INFORMATION; DYNAMICS	Developments in neural recording technology are rapidly enabling the recording of populations of neurons in multiple brain areas simultaneously, as well as the identification of the types of neurons being recorded (e.g., excitatory vs. inhibitory). There is a growing need for statistical methods to study the interaction among multiple, labeled populations of neurons. Rather than attempting to identify direct interactions between neurons (where the number of interactions grows with the number of neurons squared), we propose to extract a smaller number of latent variables from each population and study how these latent variables interact. Specifically, we propose extensions to probabilistic canonical correlation analysis (pCCA) to capture the temporal structure of the latent variables, as well as to distinguish within-population dynamics from across-population interactions (termed Group Latent Auto-Regressive Analysis, gLARA). We then applied these methods to populations of neurons recorded simultaneously in visual areas V1 and V2, and found that gLARA provides a better description of the recordings than pCCA. This work provides a foundation for studying how multiple populations of neurons interact and how this interaction supports brain function.	[Semedo, Joao D.; Yu, Byron M.] Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA; [Semedo, Joao D.] Inst Super Tecn, Dept Elect & Comp Engn, Lisbon, Portugal; [Semedo, Joao D.; Machens, Christian K.] Champalimaud Ctr Unknown, Champalimaud Neurosci Programme, Lisbon, Portugal; [Zandvakili, Amin; Kohn, Adam] Albert Einstein Coll Med, Dominick Purpura Dept Neurosci, Bronx, NY 10467 USA; [Yu, Byron M.] Carnegie Mellon Univ, Dept Biomed Engn, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Universidade de Lisboa; Instituto Superior Tecnico; Fundacao Champalimaud; Yeshiva University; Albert Einstein College of Medicine; Carnegie Mellon University	Semedo, JD (corresponding author), Carnegie Mellon Univ, Dept Elect & Comp Engn, Pittsburgh, PA 15213 USA.	jsemedo@cmu.edu; amin.zandvakili@einstein.yu.edu; adam.kohn@einstein.yu.edu; christian.machens@neuro.fchampalimaud.org; byronyu@cmu.edu						Ahrens MB, 2012, NATURE, V485, P471, DOI 10.1038/nature11057; Anderson Brian DO, 2012, OPTIMAL FILTERING; Bach F.R., 2005, PROBABILISTIC INTERP; Bock DD, 2011, NATURE, V471, P177, DOI 10.1038/nature09802; Brendel W., 2011, ADV NEURAL INF PROCE, V22, P2654; Crowe DA, 2013, NAT NEUROSCI, V16, P1484, DOI 10.1038/nn.3509; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Everitt B., 1984, INTRO LATENT VARIABL; Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402; Fries P, 2005, TRENDS COGN SCI, V9, P474, DOI 10.1016/j.tics.2005.08.011; Gregoriou GG, 2009, SCIENCE, V324, P1207, DOI 10.1126/science.1171402; Jia XX, 2013, NEURON, V77, P762, DOI 10.1016/j.neuron.2012.12.036; KAMINSKI MJ, 1991, BIOL CYBERN, V65, P203, DOI 10.1007/BF00198091; Kim S, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001110; Kulkarni JE, 2007, NETWORK-COMP NEURAL, V18, P375, DOI 10.1080/09548980701625173; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Mante V, 2013, NATURE, V503, P78, DOI 10.1038/nature12742; Pillow JW, 2008, NATURE, V454, P995, DOI 10.1038/nature07140; Quinn CJ, 2011, J COMPUT NEUROSCI, V30, P17, DOI 10.1007/s10827-010-0247-2; Saalmann YB, 2012, SCIENCE, V337, P753, DOI 10.1126/science.1223082; Salazar RF, 2012, SCIENCE, V338, P1097, DOI 10.1126/science.1224000; Smith AC, 2003, NEURAL COMPUT, V15, P965, DOI 10.1162/089976603765202622; Stopfer M, 2003, NEURON, V39, P991, DOI 10.1016/j.neuron.2003.08.011; Vazquez Y, 2013, P NATL ACAD SCI USA, V110, pE2635, DOI 10.1073/pnas.1309728110; Yu Byron M., 2008, NIPS, P1881	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103076
C	Sharma, A; Tuzel, O; Liu, MY		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sharma, Abhishek; Tuzel, Oncel; Liu, Ming-Yu			Recursive Context Propagation Network for Semantic Scene Labeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose a deep feed-forward neural network architecture for pixel-wise semantic scene labeling. It uses a novel recursive neural network architecture for context propagation, referred to as rCPN. It first maps the local visual features into a semantic space followed by a bottom-up aggregation of local information into a global representation of the entire image. Then a top-down propagation of the aggregated information takes place that enhances the contextual information of each local feature. Therefore, the information from every location in the image is propagated to every other location. Experimental results on Stanford background and SIFT Flow datasets show that the proposed method outperforms previous approaches. It is also orders of magnitude faster than previous methods and takes only 0.07 seconds on a GPU for pixel-wise labeling of a 256 x 256 image starting from raw RGB pixel values, given the super-pixel mask that takes an additional 0.3 seconds using an off-the-shelf implementation.	[Sharma, Abhishek] Univ Maryland, College Pk, MD 20742 USA; [Tuzel, Oncel; Liu, Ming-Yu] MERL, Cambridge, MA USA	University System of Maryland; University of Maryland College Park	Sharma, A (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	bhokaal@cs.umd.edu; oncel@merl.com; mliu@merl.com						Daniel Munoz J., 2010, ECCV; Farabet C., 2013, IEEE TPAMI; Fergus R., 2012, IEEE CVPR; Goller Christoph, 1995, INT C NEUR NETW; Gould S., 2009, IEEE ICCV; Jia Y., 2013, CAFFE OPEN SOURCE CO; Kumar M. Pawan, 2010, IEEE CVPR; Lafferty J, 2001, P 18 INT C MACH LEAR, P282, DOI DOI 10.1038/NPROT.2006.61; Lempitsky V. S., 2011, NIPS; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Ce, 2011, IEEE TPAMI, V33; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Liu MY, 2011, PROC CVPR IEEE; Mottaghi Roozbeh, 2014, IEEE CVPR; Mottaghi Roozbeh, 2013, IEEE CVPR; Pinheiro Pedro H. O., 2014, ICML; Polatkan Gungor, 2011, UAI, P609; Singh Gautam, 2013, IEEE CVPR; Socher R., 2011, ICML; Tighe J, 2013, INT J COMPUT VISION, V101, P329, DOI 10.1007/s11263-012-0574-z; Tighe Joseph, 2013, IEEE CVPR; Torralba A., 2003, IEEE CVPR; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453	23	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100061
C	Sohn, K; Shang, WL; Lee, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Sohn, Kihyuk; Shang, Wenling; Lee, Honglak			Improved Multimodal Deep Learning with Variation of Information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Deep learning has been successfully applied to multimodal representation learning problems, with a common strategy to learning joint representations that are shared across multiple modalities on top of layers of modality-specific networks. Nonetheless, there still remains a question how to learn a good association between data modalities; in particular, a good generative model of multimodal data should be able to reason about missing data modality given the rest of data modalities. In this paper, we propose a novel multimodal representation learning framework that explicitly aims this goal. Rather than learning with maximum likelihood, we train the model to minimize the variation of information. We provide a theoretical insight why the proposed learning objective is sufficient to estimate the data-generating joint distribution of multimodal data. We apply our method to restricted Boltzmann machines and introduce learning methods based on contrastive divergence and multi-prediction training. In addition, we extend to deep networks with recurrent encoding structure to finetune the whole network. In experiments, we demon\strate the state-of-the-art visual recognition performance on MIR-Flickr database and PASCAL VOC 2007 database with and without text features.	[Sohn, Kihyuk; Shang, Wenling; Lee, Honglak] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Sohn, K (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.	kihyuks@umich.edu; shangw@umich.edu; honglak@umich.edu			ONR [N00014-13-1-0762]; Toyota; Google Faculty Research Award	ONR(Office of Naval Research); Toyota; Google Faculty Research Award(Google Incorporated)	This work was supported in part by ONR N00014-13-1-0762, Toyota, and Google Faculty Research Award.	Bengio Y., 2014, ICML; Bengio Y, 2013, NIPS; Bosch A., 2007, ICCV; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Donahue J., 2013, INT C MACH LEARN; Goodfellow I., 2013, NIPS; Guillaumin M., 2010, CVPR; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G. E, 2009, NIPS; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Huang J.-T., 2013, ICASSP; Huiskes M., 2008, ICMIR; Huiskes M. J., 2010, ICMIR; Kim Y., 2013, ICASSP; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lai K., 2013, CONSUMER DEPTH CAMER, P167, DOI [10.1007/978-1-4471-4640-7_9, DOI 10.1007/978-1-4471-4640-7_9]; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lenz I., 2013, RSS; Manjunath BS, 2001, IEEE T CIRC SYST VID, V11, P703, DOI 10.1109/76.927424; Mnih V., 2011, UAI; Neal R. M., 1990, LEARNING STOCHASTIC, V64, P1577; Ngiam J., 2011, ICML; Oliva A, 2001, INT J COMPUT VISION, V42, P145, DOI 10.1023/A:1011139631724; Rao D., 2014, ICRA; Salakhutdinov R., 2009, AISTATS; Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277; Srivastava N., 2012, NIPS; Srivastava N., 2013, NIPS; Tang C., 2013, P ADV NEUR INF PROC, P503; Tieleman T., 2008, ICML; Verbeek J., 2010, ICMIR; Wang A., 2014, ECCV	32	1	1	3	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100058
C	Srikumar, V; Manning, CD		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Srikumar, Vivek; Manning, Christopher D.			Learning Distributed Representations for Structured Output Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In recent years, distributed representations of inputs have led to performance gains in many applications by allowing statistical information to be shared across inputs. However, the predicted outputs (labels, and more generally structures) are still treated as discrete objects even though outputs are often not discrete units of meaning. In this paper, we present a new formulation for structured prediction where we represent individual labels in a structure as dense vectors and allow semantically similar labels to share parameters. We extend this representation to larger structures by defining compositionality using tensor products to give a natural generalization of standard structured prediction approaches. We define a learning objective for jointly learning the model parameters and the label vectors and propose an alternating minimization algorithm for learning. We show that our formulation outperforms structural SVM baselines in two tasks: multiclass document classification and part-of-speech tagging.	[Srikumar, Vivek] Univ Utah, Salt Lake City, UT 84112 USA; [Manning, Christopher D.] Stanford Univ, Stanford, CA 94305 USA	Utah System of Higher Education; University of Utah; Stanford University	Srikumar, V (corresponding author), Univ Utah, Salt Lake City, UT 84112 USA.	svivek@cs.utah.edu; manning@cs.stanford.edu	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X	Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) [FA8750-13-2-0040]	Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL)	We thank the anonymous reviewers for their valuable comments. Stanford University gratefully acknowledges the support of the Defense Advanced Research Projects Agency (DARPA) Deep Exploration and Filtering of Text (DEFT) Program under Air Force Research Laboratory (AFRL) contract no. FA8750-13-2-0040. Any opinions, findings, and conclusion or recommendations expressed in this material are those of the authors and do not necessarily reflect the view of the DARPA, AFRL, or the US government.	Abernethy J, 2006, CS0611124 ARXIV; Amit Y., 2007, INT C MACH LEARN; Argyriou A., 2007, ADV NEURAL INFORM PR; Bengio Y., 2013, IEEE T PATTERN ANAL; Brants T., 2000, C APPL NAT LANG PROC; Cesa- Bianchi N., 2006, INT C MACH LEARN; Coates A., 2011, AISTATS; Collins Michael, 2002, C EMP METH NAT LANG; Collobert R., 2011, J MACHINE LEARNING R, V12; Fazel M., 2004, P AM CONTR C, V4; Hinton G. E., 1988, ANN C COGN SCI SOC; Lafferty J., 2001, MACHINE LEARNING; Lang K., 1995, INT C MACH LEARN; Lei T., 2014, ANN M ASS COMP LING; Mikolov T., 2013, EFFICIENT ESTIMATION; Nivre J., 2007, CONLL SHAR TASK SESS; Parikh Neal, 2013, FDN TRENDS OPTIMIZAT; Petrov Slav, 2011, ARXIV11042086; PLATE TA, 1995, IEEE T NEURAL NETWOR, V6, P623, DOI 10.1109/72.377968; Smolensky P., 1990, ARTIFICIAL INTELLIGE, V46; Socher Richard, 2012, EMPIRICAL METHODS NA; SREBRO N., 2004, ADV NEURAL INFORM PR; Toutanova K., 2003, C N AM CHAPT ASS COM; TSOCHANTARIDIS I, 2005, J MACHINE LEARNING R; Turian J., 2010, ANN M ASS COMP LING; Vandenberghe L, 1996, SIAM REV, V38, P49, DOI 10.1137/1038003	27	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100102
C	Steinberg, DM; Bonilla, EV		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Steinberg, Daniel M.; Bonilla, Edwin, V			Extended and Unscented Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present two new methods for inference in Gaussian process (GP) models with general nonlinear likelihoods. Inference is based on a variational framework where a Gaussian posterior is assumed and the likelihood is linearized about the variational posterior mean using either a Taylor series expansion or statistical linearization. We show that the parameter updates obtained by these algorithms are equivalent to the state update equations in the iterative extended and unscented Kalman filters respectively, hence we refer to our algorithms as extended and unscented GPs. The unscented GP treats the likelihood as a 'black-box' by not requiring its derivative for inference, so it also applies to non-differentiable likelihood models. We evaluate the performance of our algorithms on a number of synthetic inversion problems and a binary classification dataset.	[Steinberg, Daniel M.] NICTA, Sydney, NSW, Australia; [Bonilla, Edwin, V] Univ New South Wales, Sydney, NSW, Australia	Australian National University; University of New South Wales Sydney	Steinberg, DM (corresponding author), NICTA, Sydney, NSW, Australia.	daniel.steinberg@nicta.com.au; e.bonilla@unsw.edu.au			Science Industry Endowment Fund Big Data Knowledge Discovery project [RP 04-174]; Australian Government through the Department of Communications; Australian Research Council through the ICT Centre of Excellence Program	Science Industry Endowment Fund Big Data Knowledge Discovery project; Australian Government through the Department of Communications(Australian Government); Australian Research Council through the ICT Centre of Excellence Program(Australian Research Council)	This research was supported by the Science Industry Endowment Fund (RP 04-174) Big Data Knowledge Discovery project. We thank F. Ramos, L. McCalman, S. O'Callaghan, A. Reid and T. Nguyen for their helpful feedback. NICTA is funded by the Australian Government through the Department of Communications and the Australian Research Council through the ICT Centre of Excellence Program.	BELL BM, 1993, IEEE T AUTOMAT CONTR, V38, P294, DOI 10.1109/9.250476; Geist M., 2010, Proceedings of the 2010 IEEE International Workshop on Machine Learning for Signal Processing (MLSP), P272, DOI 10.1109/MLSP.2010.5589236; Johnson S. G., NLOPT NONLINEAR OPTI; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Julier SJ, 2004, P IEEE, V92, P401, DOI 10.1109/JPROC.2003.823141; Lawrence N.D., 2003, NIPS, V2, P5; MARQUARDT DW, 1963, J SOC IND APPL MATH, V11, P431, DOI 10.1137/0111030; Ming Chai K. A., 2009, ADV NEURAL INF PROCE, V21, P265; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Platt JC, 2000, ADV NEUR IN, P61; Reid A., 2013, P 23 INT JOINT C ART, P2877; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sibley G., 2006, ROBOTICS SCI SYSTEMS, V8, P235; Snelson E., 2003, NIPS; Wang J.M., 2005, NIPS, V18, P3; Wang JM, 2008, IEEE T PATTERN ANAL, V30, P283, DOI 10.1109/TPAMI.2007.1167	18	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102016
C	Stimberg, F; Ruttor, A; Opper, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Stimberg, Florian; Ruttor, Andreas; Opper, Manfred			Poisson Process Jumping between an Unknown Number of Rates: Application to Neural Spike Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				BAYESIAN-INFERENCE; TRAINS; VOICE	We introduce a model where the rate of an inhomogeneous Poisson process is modified by a Chinese restaurant process. Applying a MCMC sampler to this model allows us to do posterior Bayesian inference about the number of states in Poisson-like data. Our sampler is shown to get accurate results for synthetic data and we apply it to V1 neuron spike data to find discrete firing rate states depending on the orientation of a stimulus.	[Stimberg, Florian; Ruttor, Andreas; Opper, Manfred] TU Berlin, Comp Sci, Berlin, Germany	Technical University of Berlin	Stimberg, F (corresponding author), TU Berlin, Comp Sci, Berlin, Germany.	Florian.Stimberg@tu-berlin.de; Andreas.Ruttor@tu-berlin.de; Manfred.Opper@tu-berlin.de						Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; ARJAS E, 1994, STAT SINICA, V4, P505; Barbieri R, 2001, J NEUROSCI METH, V105, P25, DOI 10.1016/S0165-0270(00)00344-7; Blanche TJ, 2005, J NEUROPHYSIOL, V93, P2987, DOI 10.1152/jn.01023.2004; Boys RJ, 2008, STAT COMPUT, V18, P125, DOI 10.1007/s11222-007-9043-x; Chiappalone M, 2005, NEUROCOMPUTING, V65, P653, DOI 10.1016/j.neucom.2004.10.094; Cunningham JP, 2009, NEURAL NETWORKS, V22, P1235, DOI 10.1016/j.neunet.2009.02.004; Fearnhead P, 2006, J ROY STAT SOC B, V68, P767, DOI 10.1111/j.1467-9868.2006.00566.x; GRASSMANN WK, 1977, COMPUT OPER RES, V4, P47, DOI 10.1016/0305-0548(77)90007-7; HEFFES H, 1986, IEEE J SEL AREA COMM, V4, P856, DOI 10.1109/JSAC.1986.1146393; HUTTENLOCHER PR, 1967, EXP NEUROL, V17, P247, DOI 10.1016/0014-4886(67)90104-5; Jager M, 2009, CHEMPHYSCHEM, V10, P2486, DOI 10.1002/cphc.200900331; Kaneoke Y, 1996, J NEUROSCI METH, V68, P211, DOI 10.1016/0165-0270(96)00081-7; Kass RE, 2005, J NEUROPHYSIOL, V94, P8, DOI 10.1152/jn.00648.2004; Kou SC, 2005, J R STAT SOC C-APPL, V54, P469, DOI 10.1111/j.1467-9876.2005.00509.x; LEGENDY CR, 1985, J NEUROPHYSIOL, V53, P926, DOI 10.1152/jn.1985.53.4.926; Maimon G, 2009, NEURON, V62, P426, DOI 10.1016/j.neuron.2009.03.021; MEIERHELLSTERN KS, 1987, EUR J OPER RES, V29, P370, DOI 10.1016/0377-2217(87)90250-5; Nawrot M, 1999, J NEUROSCI METH, V94, P81, DOI 10.1016/S0165-0270(99)00127-2; PERKEL DH, 1967, BIOPHYS J, V7, P391, DOI 10.1016/S0006-3495(67)86596-2; Rao V. A., 2011, ADV NEURAL INFORM PR, P2474; Rao V. A., 2012, THESIS; Rao V, 2013, J MACH LEARN RES, V14, P3295; Saeedi A., 2011, ADV NEURAL INFORM PR, V24; SRIRAM K, 1986, IEEE J SEL AREA COMM, V4, P833, DOI 10.1109/JSAC.1986.1146402; Stimberg F., 2012, ARTIFICIAL INTELLIGE, P1117; Teh Y.W., 2010, ENCY MACHINE LEARNIN; Zhang Xinhua, 2008, TECHNICAL REPORT, V09	28	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102095
C	Tang, YC; Srivastava, N; Salakhutdinov, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tang, Yichuan; Srivastava, Nitish; Salakhutdinov, Ruslan			Learning Generative Models with Visual Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Attention has long been proposed by psychologists to be important for efficiently dealing with the massive amounts of sensory stimulus in the neocortex. Inspired by the attention models in visual neuroscience and the need for object-centered data for generative models, we propose a deep-learning based generative framework using attention. The attentional mechanism propagates signals from the region of interest in a scene to an aligned canonical representation for generative modeling. By ignoring scene background clutter, the generative model can concentrate its resources on the object of interest. A convolutional neural net is employed to provide good initializations during posterior inference which uses Hamiltonian Monte Carlo. Upon learning images of faces, our model can robustly attend to the face region of novel test subjects. More importantly, our model can learn generative models of new faces from a novel dataset of large images where the face locations are not known.	[Tang, Yichuan; Srivastava, Nitish; Salakhutdinov, Ruslan] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Tang, YC (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.	tang@cs.toronto.edu; nitish@cs.toronto.edu; rsalakhu@cs.toronto.edu			Samsung; Google; ONR [N00014-14-1-0232]	Samsung(Samsung); Google(Google Incorporated); ONR(Office of Naval Research)	The authors gratefully acknowledge the support and generosity from Samsung, Google, and ONR grant N00014-14-1-0232.	Alexe B., 2012, NIPS 2012; ANDERSON CH, 1987, P NATL ACAD SCI USA, V84, P6297, DOI 10.1073/pnas.84.17.6297; Baker S, 2004, INT J COMPUT VISION, V56, P221, DOI 10.1023/B:VISI.0000011205.11775.fd; Bengio Yoshua, 2013, ADV NEURAL INFORM PR, V26; Buffalo EA, 2010, P NATL ACAD SCI USA, V107, P361, DOI 10.1073/pnas.0907658106; Chikkerur S, 2010, VISION RES, V50, P2233, DOI 10.1016/j.visres.2010.05.013; Denil M, 2012, NEURAL COMPUT, V24, P2151, DOI 10.1162/NECO_a_00312; DUANE S, 1987, PHYS LETT B, V195, P216, DOI 10.1016/0370-2693(87)91197-X; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Kanwisher N, 2000, NAT REV NEUROSCI, V1, P91, DOI 10.1038/35039043; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Larochelle H., 2010, ADV NEURAL INFORM PR, P1243; Lee H., 2009, P ANN INT C MACH LEA, P609; Lewis J., 1995, FAST NORMALIZED CROS; Mohamed A, 2011, IEEE T AUDIO SPEECH; Neal R. M., 2010, HDB MARKOV CHAIN MON; OLSHAUSEN BA, 1993, J NEUROSCI, V13, P4700; Posner M. I., 1999, P NATL ACAD SCI, V96; Ranzato M. A., 2014, ARXIV14055488; Ranzato Marc'Aurelio, 2011, CVPR; Reichert DP, 2011, LECT NOTES COMPUT SC, V6791, P18, DOI 10.1007/978-3-642-21735-7_3; Salakhutdinov R., 2009, AISTATS; Salakhutdinov R., 2008, P INT C MACH LEARN, V25; Szeliski R, 2011, TEXTS COMPUTER SCI; Tang Y., 2012, ICML; Taylor Graham W., 2010, ECCV 2010; Tieleman T., 2009, ICML, V382, P130; TSOTSOS JK, 1995, ARTIF INTELL, V78, P507, DOI 10.1016/0004-3702(95)00025-9; Wiskott Laurenz, 2004, DOES OUR VISUAL SYST; Zoran Daniel, 2011, ICCV	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101016
C	Tootoonian, S; Lengyel, M		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tootoonian, Sina; Lengyel, Mate			A Dual Algorithm for Olfactory Computation in the Locust Brain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ANTENNAL LOBE; ODOR REPRESENTATIONS	We study the early locust olfactory system in an attempt to explain its well-characterized structure and dynamics. We first propose its computational function as recovery of high-dimensional sparse olfactory signals from a small number of measurements. Detailed experimental knowledge about this system rules out standard algorithmic solutions to this problem. Instead, we show that solving a dual formulation of the corresponding optimisation problem yields structure and dynamics in good agreement with biological data. Further biological constraints lead us to a reduced form of this dual formulation in which the system uses in- dependent component analysis to continuously adapt to its olfactory environment to allow accurate sparse recovery. Our work demonstrates the challenges and rewards of attempting detailed understanding of experimentally well-characterized systems.	[Tootoonian, Sina; Lengyel, Mate] Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England	University of Cambridge	Tootoonian, S (corresponding author), Univ Cambridge, Computat & Biol Learning Lab, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.	st582@eng.cam.ac.uk; m.lengyel@eng.cam.ac.uk	Lengyel, Mate/A-6665-2013	Lengyel, Mate/0000-0001-7266-0049	Wellcome Trust	Wellcome Trust(Wellcome TrustEuropean Commission)	This work was supported by the Wellcome Trust (ST, ML).	Boyd S, 2004, CONVEX OPTIMIZATION; Caron SJC, 2013, NATURE, V497, P113, DOI 10.1038/nature12063; Dayan P., 2005, THEORETICAL NEUROSCI; Eisthen HL, 2002, BRAIN BEHAV EVOLUT, V59, P273, DOI 10.1159/000063564; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; Jortner RA, COMMUNICATION; Jortner RA, 2007, J NEUROSCI, V27, P1659, DOI 10.1523/JNEUROSCI.4171-06.2007; Jouquand C, 2008, J AM SOC HORTIC SCI, V133, P859, DOI 10.21273/JASHS.133.6.859; Leitch B, 1996, J COMP NEUROL, V372, P487; Mazor O, 2005, NEURON, V48, P661, DOI 10.1016/j.neuron.2005.09.032; Murthy M, 2008, NEURON, V59, P1009, DOI 10.1016/j.neuron.2008.07.040; Ng M, 2002, NEURON, V36, P463, DOI 10.1016/S0896-6273(02)00975-3; Papadopoulou M, 2011, SCIENCE, V332, P721, DOI 10.1126/science.1201835; Perez-Orive J, 2002, SCIENCE, V297, P359, DOI 10.1126/science.1070502; Shen K, 2013, NEURON, V80, P1246, DOI 10.1016/j.neuron.2013.08.026; Stopfer M, 1999, NATURE, V402, P664, DOI 10.1038/45244; Stopfer M, 2003, NEURON, V39, P991, DOI 10.1016/j.neuron.2003.08.011	18	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101071
C	Treister, E; Turek, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Treister, Eran; Turek, Javier			A Block-Coordinate Descent Approach for Large-scale Sparse Inverse Covariance Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				OPTIMIZATION; SHRINKAGE; SELECTION	The sparse inverse covariance estimation problem arises in many statistical applications in machine learning and signal processing. In this problem, the inverse of a covariance matrix of a multivariate normal distribution is estimated, assuming that it is sparse. An l(1) regularized log-determinant optimization problem is typically solved to approximate such matrices. Because of memory limitations, most existing algorithms are unable to handle large scale instances of this problem. In this paper we present a new block-coordinate descent approach for solving the problem for large-scale data sets. Our method treats the sought matrix block-by-block using quadratic approximations, and we show that this approach has advantages over existing methods in several aspects. Numerical experiments on both synthetic and real gene expression data demonstrate that our approach outperforms the existing state of the art methods, especially for large-scale problems.	[Treister, Eran] Technion, Comp Sci, Haifa, Israel; [Treister, Eran] Univ British Columbia, Earth & Ocean Sci, Vancouver, BC V6T 1Z2, Canada; [Turek, Javier] Technion Israel Inst Technol, Dept Comp Sci, IL-32000 Haifa, Israel	Technion Israel Institute of Technology; University of British Columbia; Technion Israel Institute of Technology	Treister, E (corresponding author), Technion, Comp Sci, Haifa, Israel.	eran@cs.technion.ac.il; javiert@cs.technion.ac.il	Turek, Javier/Q-9241-2019; Treister, Eran/AAE-1261-2021	Turek, Javier/0000-0003-4027-1529; Treister, Eran/0000-0002-5351-0966	European Union [623212]	European Union(European Commission)	The authors would like to thank Prof. Irad Yavneh for his valuable comments and guidance throughout this work. The research leading to these results has received funding from the European Union's - Seventh Framework Programme (FP7/2007-2013) under grant agreement no 623212 MC Multiscale Inversion.	[Anonymous], 2013, NIPS; ARMIJO L, 1966, PAC J MATH, V16, P1, DOI 10.2140/pjm.1966.16.1; Banerjee O., 2006, P 23 INT C MACH LEAR, P89, DOI DOI 10.1145/1143844.1143856; Banerjee O, 2008, J MACH LEARN RES, V9, P485; DEMPSTER AP, 1972, BIOMETRICS, V28, P157, DOI 10.2307/2528966; Elad M, 2010, SPARSE AND REDUNDANT REPRESENTATIONS, P3, DOI 10.1007/978-1-4419-7011-4_1; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Guillot D., 2012, NIPS; Honorio J., 2013, P 29 C UAI; HSIEH C. J., 2012, NIPS, P2339; Hsieh C.-J., 2011, ADV NEURAL INFORM PR, P2330; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Li L, 2010, MATH PROGRAM COMPUT, V2, P291, DOI 10.1007/s12532-010-0020-6; Mazumder R, 2012, J MACH LEARN RES, V13, P781; Oztoprak F., 2012, P 25 INT C NEUR INF, V25, P755; Saad Y., 2003, ITERATIVE METHODS SP, Vsecond, DOI DOI 10.1137/1.9780898718003; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Treister E, 2012, IEEE T SIGNAL PROCES, V60, P6319, DOI 10.1109/TSP.2012.2218807; Tseng P, 2009, MATH PROGRAM, V117, P387, DOI 10.1007/s10107-007-0170-0; Wen ZW, 2010, SIAM J SCI COMPUT, V32, P1832, DOI 10.1137/090747695; Zibulevsky M, 2010, IEEE SIGNAL PROC MAG, V27, P76, DOI 10.1109/MSP.2010.936023	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102058
C	Trivedi, S; McAllester, D; Shakhnarovich, G		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Trivedi, Shubhendu; McAllester, David; Shakhnarovich, Gregory			Discriminative Metric Learning by Neighborhood Gerrymandering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We formulate the problem of metric learning for k nearest neighbor classification as a large margin structured prediction problem, with a latent variable representing the choice of neighbors and the task loss directly corresponding to classification error. We describe an efficient algorithm for exact loss augmented inference, and a fast gradient descent algorithm for learning in this model. The objective drives the metric to establish neighborhood boundaries that benefit the true class labels for the training points. Our approach, reminiscent of gerrymandering (redrawing of political boundaries to provide advantage to certain parties), is more direct in its handling of optimizing classification accuracy than those previously proposed. In experiments on a variety of data sets our method is shown to achieve excellent results compared to current state of the art in metric learning.	[Trivedi, Shubhendu; McAllester, David; Shakhnarovich, Gregory] Toyota Technol Inst, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Trivedi, S (corresponding author), Toyota Technol Inst, Chicago, IL 60637 USA.	shubhendu@ttic.edu; mcallester@ttic.edu; greg@ttic.edu			NSF [IIS-1409837]	NSF(National Science Foundation (NSF))	This work was partly supported by NSF award IIS-1409837.	[Anonymous], 2006, P 18 INT C NEUR INF; Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348; Beygelzimer A., 2006, P 23 INT C MACH LEAR, P97, DOI DOI 10.1145/1143844.1143857; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Goldberger J., 2004, NIPS; Gong YC, 2011, PROC CVPR IEEE, P817, DOI 10.1109/CVPR.2011.5995432; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Kedem D., 2012, ADV NEURAL INFORM PR, P2582; KIRA K, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P129; Kulis Brian, 2009, ADV NEURAL INFORM PR, P1042; Lanckriet G.R.G, 2010, ICML; Norouzi M., 2011, MINIMAL LOSS HASHING, V1, P2; Norouzi M., 2012, ADV NEURAL INFORM PR, V25; Russell S. J, 2002, ADV NEURAL INFORM PR, P12, DOI DOI 10.5555/2968618.2968683; Stange P., 2008, APP MATH MECH, V8, P10827, DOI DOI 10.1002/PAMM.200810827; Tarlow D., 2013, P 30 INT C MACH LEAR, V28, P199; Wang J., 2012, ECML PKDD; Wang Jun, 2010, ICML; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Weinberger KQ, 2008, P 25 INT C MACH LEAR, P1160, DOI DOI 10.1145/1390156.1390302; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]; Yuille AL, 2002, ADV NEUR IN, V14, P1033	24	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101056
C	Tschiatschek, S; Iyer, R; Wei, HC; Bilmes, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Tschiatschek, Sebastian; Iyer, Rishabh; Wei, Haochen; Bilmes, Jeff			Learning Mixtures of Submodular Functions for Image Collection Summarization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We address the problem of image collection summarization by learning mixtures of submodular functions. Submodularity is useful for this problem since it naturally represents characteristics such as fidelity and diversity, desirable for any summary. Several previously proposed image summarization scoring methodologies, in fact, instinctively arrived at submodularity. We provide classes of submodular component functions (including some which are instantiated via a deep neural network) over which mixtures may be learnt. We formulate the learning of such mixtures as a supervised problem via large-margin structured prediction. As a loss function, and for automatic summary scoring, we introduce a novel summary evaluation method called V-ROUGE, and test both submodular and non-submodular optimization (using the submodular-supermodular procedure) to learn a mixture of submodular functions. Interestingly, using non-submodular optimization to learn submodular functions provides the best results. We also provide a new data set consisting of 14 real-world image collections along with many human-generated ground truth summaries collected using Amazon Mechanical Turk. We compare our method with previous work on this problem and show that our learning approach outperforms all competitors on this new data set. This paper provides, to our knowledge, the first systematic approach for quantifying the problem of image collection summarization, along with a new data set of image collections and human summaries.	[Tschiatschek, Sebastian] Graz Univ Technol, Dept Elect Engn, Graz, Austria; [Iyer, Rishabh; Wei, Haochen; Bilmes, Jeff] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Wei, Haochen] LinkedIn, Mountain View, CA USA	Graz University of Technology; University of Washington; University of Washington Seattle	Tschiatschek, S (corresponding author), Graz Univ Technol, Dept Elect Engn, Graz, Austria.	tschiatschek@tugraz.at; rkiyer@u.washington.edu; weihch90@gmail.com; bilmes@u.washington.edu			National Science Foundation [IIS-1162606]; Austrian Science Fund [P25244-N15]; Google; Microsoft award; Intel Science and Technology Center for Pervasive Computing; Microsoft Research Fellowship award	National Science Foundation(National Science Foundation (NSF)); Austrian Science Fund(Austrian Science Fund (FWF)); Google(Google Incorporated); Microsoft award(Microsoft); Intel Science and Technology Center for Pervasive Computing; Microsoft Research Fellowship award(Microsoft)	This material is based upon work supported by the National Science Foundation under Grant No. (IIS-1162606), the Austrian Science Fund under Grant No. (P25244-N15), a Google and a Microsoft award, and by the Intel Science and Technology Center for Pervasive Computing. Rishabh Iyer is also supported by a Microsoft Research Fellowship award.	Bilmes J., 2012, UAI; Borodin A., 2012, P 31 ACM SIGMOD SIGA, P155; Buchbinder N., 2014, SODA; Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Denton T, 2004, INT C PATT RECOG, P273, DOI 10.1109/ICPR.2004.1334159; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Fergus R, 2003, PROC CVPR IEEE, P264; Hadi Y., 2006, Applied Computing 2006. 21st Annual ACM Symposium on Applied Computing, P1400, DOI 10.1145/1141277.1141601; Hastie T, 2009, ELEMENTS STAT LEARNI, V2, DOI [10.1007/b94608, DOI 10.1007/B94608]; Iyer R., 2013, ICML; Jaffe A., 2006, P 8 ACM INT WORKSH M, P89, DOI DOI 10.1145/1178677.1178692; Khosla A., 2013, C COMP VIS PATT REC; Kirchhoff K., 2014, EMPIRICAL METHODS NA; Kulesza A., 2011, P 28 INT C MACH LEAR; Li X., 2006, P 14 ANN ACM INT C M, P607; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Lin H, 2011, ACL HLT 2011; Lin H., 2012, P 28 C UNC ART INT U, P479; Lin Hui, 2010, NAACL; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Meeker M, 2013, TECHNICAL REPORT; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Narasimhan M., 2005, C UNC ART INT UAI ED; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Ngo CW, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P104, DOI 10.1109/ICCV.2003.1238320; Sermanet P., 2013, ARXIV E PRINTS; Simon I., 2007, ICCV; Sinha P., 2011, INT C MULT EXP ICME, P1; Sivic J, 2005, IEEE I CONF COMP VIS, P370; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Vedaldi A., 2008, VLFEAT OPEN PORTABLE; Wengert C., 2011, P 19 ACM INT C MULT, P1437, DOI [10.1145/2072298.2072034, DOI 10.1145/2072298.2072034]; Yang JC, 2009, PROC CVPR IEEE, P1794, DOI 10.1109/CVPRW.2009.5206757	34	1	1	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101086
C	Valera, I; Ghahramani, Z		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Valera, Isabel; Ghahramani, Zoubin			General Table Completion using a Bayesian Nonparametric Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Even though heterogeneous databases can be found in a broad variety of applications, there exists a lack of tools for estimating missing data in such databases. In this paper, we provide an efficient and robust table completion tool, based on a Bayesian nonparametric latent feature model. In particular, we propose a general observation model for the Indian buffet process (IBP) adapted to mixed continuous (real-valued and positive real-valued) and discrete (categorical, ordinal and count) observations. Then, we propose an inference algorithm that scales linearly with the number of observations. Finally, our experiments over five real databases show that the proposed approach provides more robust and accurate estimates than the standard IBP and the Bayesian probabilistic matrix factorization with Gaussian observations.	[Valera, Isabel] Univ Carlos III Madrid, Dept Signal Proc & Commun, Getafe, Spain; [Ghahramani, Zoubin] Univ Cambridge, Dept Engn, Cambridge, England	Universidad Carlos III de Madrid; University of Cambridge	Valera, I (corresponding author), Univ Carlos III Madrid, Dept Signal Proc & Commun, Getafe, Spain.	ivalera@tsc.uc3m.es; zoubin@eng.cam.ac.uk			Plan Regional-Programas I+D of Comunidad de Madrid [AGES-CM S2010/BMD-2422]; Ministerio de Ciencia e Innovacion of Spain [DEIPRO TEC2009-14504-C02-00, 2010 CSD2008-00010 COMONSENS]; EPSRC [EP/I036575/1]; Google Focused Research Award	Plan Regional-Programas I+D of Comunidad de Madrid; Ministerio de Ciencia e Innovacion of Spain(Ministry of Science and Innovation, Spain (MICINN)Spanish Government); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Google Focused Research Award(Google Incorporated)	Isabel Valera acknowledge the support of Plan Regional-Programas I+D of Comunidad de Madrid (AGES-CM S2010/BMD-2422), Ministerio de Ciencia e Innovacion of Spain (project DEIPRO TEC2009-14504-C02-00 and program Consolider-Ingenio 2010 CSD2008-00010 COMONSENS). Zoubin Ghahramani is supported by the EPSRC grant EP/I036575/1 and a Google Focused Research Award.	[Anonymous], 2008, P 25 INT C MACH LEAR; Blei, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480; Chu W, 2005, J MACH LEARN RES, V6, P1019; Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016; Doshi-Velez F., 2009, P 26 ANN INT C MACH, P273; Eggermont J., 2004, SAC 2004, P1001; Girolami M., 2006, NEURAL COMPUT, V18, P2005; Gopalan P., 2014, NTERN C ART INT STAT; Griffiths TL, 2011, J MACH LEARN RES, V12, P1185; Li X.B., 2009, J DATA INFORM QUALIT, V1, P3; Mansouri K., J CHEM INFORM MODELI; Pew Research Centre, 25 ANN WEB; ROBERT CP, 1995, STAT COMPUT, V5, P121, DOI 10.1007/BF00143942; Ruiz F. J. R., 2012, ADV NEURAL INFORM PR, V25, P1862; Ruiz F. J. R., 2013, J MACHINE LEARNING R; Salakhutdinov R., 2007, ADV NEURAL INFORM PR; Salazar E., 2012, P INT C MACH LEARN J, P1039; Shafto P, 2011, COGNITION, V120, P1, DOI 10.1016/j.cognition.2011.02.010; Singh S., 2013, P ACM INF KNOWL MAN; Titsias M., 2007, ADV NEURAL INFORM PR, V19; Todeschini A., 2013, ADV NEURAL INFORM PR, V26, P845; Williamson S., 2010, P 27 ANN INT C MACH	22	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100010
C	Vinayak, RK; Oymak, S; Hassibi, B		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Vinayak, Ramya Korlakai; Oymak, Samet; Hassibi, Babak			Graph Clustering With Missing Data : Convex Algorithms and Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				CLIQUE	We consider the problem of finding clusters in an unweighted graph, when the graph is partially observed. We analyze two programs, one which works for dense graphs and one which works for both sparse and dense graphs, but requires some a priori knowledge of the total cluster size, that are based on the convex optimization approach for low-rank matrix recovery using nuclear norm minimization. For the commonly used Stochastic Block Model, we obtain explicit bounds on the parameters of the problem (size and sparsity of clusters, the amount of observed data) and the regularization parameter characterize the success and failure of the programs. We corroborate our theoretical findings through extensive simulations. We also run our algorithm on a real data set obtained from crowdsourcing an image classification task on the Amazon Mechanical Turk, and observe significant performance improvement over traditional methods such as k-means.	[Vinayak, Ramya Korlakai; Oymak, Samet; Hassibi, Babak] CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Vinayak, RK (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.	ramya@caltech.edu; soymak@caltech.edu; hassibi@systems.caltech.edu			National Science Foundation [CCF-0729203, CNS-0932428, CIF-1018927]; Office of Naval Research under the MURI [N00014-08-1-0747]; Qualcomm Inc.; Schlumberger Foundation Faculty for the Future Program Grant	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research under the MURI; Qualcomm Inc.; Schlumberger Foundation Faculty for the Future Program Grant(Schlumberger)	The authors thank the anonymous reviewers for their insightful comments. This work was supported in part by the National Science Foundation under grants CCF-0729203, CNS-0932428 and CIF-1018927, by the Office of Naval Research under the MURI grant N00014-08-1-0747, and by a grant from Qualcomm Inc. The first author is also supported by the Schlumberger Foundation Faculty for the Future Program Grant.	Ailon Nir, 2013, CORR; Ames B. P. W., 2013, ROBUST CONVEX RELAXA; Ames BPW, 2014, MATH PROGRAM, V143, P299, DOI 10.1007/s10107-013-0733-1; Ames BPW, 2011, MATH PROGRAM, V129, P69, DOI 10.1007/s10107-011-0459-x; Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V, 2011, SIAM J OPTIMIZ, V21, P572, DOI 10.1137/090761793; Chandrasekaran Venkat, 2012, CORR; Chen Y., 2012, NIPS, P2213; Condon A, 2001, RANDOM STRUCT ALGOR, V18, P116, DOI 10.1002/1098-2418(200103)18:2<116::AID-RSA1001>3.0.CO;2-2; Domingos P., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P57, DOI 10.1145/502512.502525; Ester M., 1995, KDD-95 Proceedings. First International Conference on Knowledge Discovery and Data Mining, P94; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Khosla A., 2011, P WORKSH FIN GRAIN V; Lin Z., 2010, MATH PROGRAMMING; McSherry F, 2001, ANN IEEE SYMP FOUND, P529, DOI 10.1109/SFCS.2001.959929; Mishra N, 2007, LECT NOTES COMPUT SC, V4863, P56; Oymak S., 2011, ARXIV11045186; Schaeffer SE, 2007, COMPUT SCI REV, V1, P27, DOI 10.1016/j.cosrev.2007.05.001; Vinayak RK, 2014, INT CONF ACOUST SPEE; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; Xu XW, 1999, DATA MIN KNOWL DISC, V3, P263, DOI 10.1023/A:1009884809343; Xu Y, 2002, BIOINFORMATICS, V18, P536, DOI 10.1093/bioinformatics/18.4.536; Yang Q., 2005, 2005 IEEE Computational Systems Bioinformatics Conference Workshops and Poster Abstracts, P174	30	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100088
C	Wang, XL; Zhang, LL; Lin, L; Liang, ZJ; Zuo, WM		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Xiaolong; Zhang, Liliang; Lin, Liang; Liang, Zhujin; Zuo, Wangmeng			Deep Joint Task Learning for Generic Object Extraction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					This paper investigates how to extract objects-of-interest without relying on hand-craft features and sliding windows approaches, that aims to jointly solve two sub-tasks: (i) rapidly localizing salient objects from images, and (ii) accurately segmenting the objects based on the localizations. We present a general joint task learning framework, in which each task (either object localization or object segmentation) is tackled via a multi-layer convolutional neural network, and the two networks work collaboratively to boost performance. In particular, we propose to incorporate latent variables bridging the two networks in a joint optimization manner. The first network directly predicts the positions and scales of salient objects from raw images, and the latent variables adjust the object localizations to feed the second network that produces pixelwise object masks. An EM-type method is presented for the optimization, iterating with two steps: (i) by using the two networks, it estimates the latent variables by employing an MCMC-based sampling method; (ii) it optimizes the parameters of the two networks unitedly via back propagation, with the fixed latent variables. Extensive experiments suggest that our framework significantly outperforms other state-of-the-art approaches in both accuracy and efficiency (e.g. 1000 times faster than competing approaches).	[Wang, Xiaolong; Zhang, Liliang; Lin, Liang; Liang, Zhujin] Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China; [Zuo, Wangmeng] Harbin Inst Technol, Sch Comp Sci & Technol, Harbin, Heilongjiang, Peoples R China; [Lin, Liang] SYSU CMU Shunde Int Joint Res Inst, Shunde, Peoples R China; [Wang, Xiaolong] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Sun Yat Sen University; Harbin Institute of Technology; Carnegie Mellon University	Lin, L (corresponding author), Sun Yat Sen Univ, Guangzhou 510006, Guangdong, Peoples R China.	xlwang@cmu.edu; linliang@ieee.org	Zuo, Wangmeng/B-3701-2008		National Natural Science Foundation of China [61173082]; Hi-Tech Research and Development Program of China [2012AA011504]; Guangdong Science and Technology Program [2012B031500006]; Special Project on Integration of Industry, Educationand Research of Guangdong [2012B091000101]; Fundamental Research Funds for the Central Universities [14lgjc11]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Hi-Tech Research and Development Program of China(National High Technology Research and Development Program of China); Guangdong Science and Technology Program; Special Project on Integration of Industry, Educationand Research of Guangdong; Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported by the National Natural Science Foundation of China (no.61173082), the Hi-Tech Research and Development Program of China (no.2012AA011504), Guangdong Science and Technology Program (no. 2012B031500006), Special Project on Integration of Industry, Educationand Research of Guangdong (no.2012B091000101), and Fundamental Research Funds for the Central Universities (no.14lgjc11).	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; [Anonymous], 2012, ECCV; Batra D., 2010, CVPR; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Brox T., 2011, CVPR; Carreira J., 2010, CVPR; Chen X., 2014, CVPR; Cheng MM, 2013, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2013.193; Cheng MM, 2014, VISUAL COMPUT, V30, P443, DOI 10.1007/s00371-013-0867-4; Cheng MM, 2011, PROC CVPR IEEE, P409, DOI 10.1109/CVPR.2011.5995344; Ciresan D., 2012, NIPS; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Eigen D., 2014, 6 INT C LEARN REPR I; Endres I., 2014, IEEE T PATTERN ANAL; Erhan D., 2014, CVPR; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Farabet C., 2012, ICML; Felzenszwalb P. F., 2010, IEEE T PATTERN ANAL; Fidler S., 2013, CVPR; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Huang G. B., 2013, NIPS; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kuettel D., 2012, CVPR; LeCun Y., 1990, NIPS; METROPOLIS N, 1953, J CHEM PHYS, V21, P1087, DOI 10.1063/1.1699114; Ouyang W., 2013, ICCV; Perazzi F., 2012, CVPR; Rosenfeld A., 2011, ICCV; Rubinstein M., 2013, CVPR; Szegedy C., 2013, NIPS; Tu ZW, 2002, IEEE T PATTERN ANAL, V24, P657, DOI 10.1109/34.1000239; Yan Q., 2013, CVPR; Yang Y., 2010, CVPR	33	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102108
C	Wang, ZR; Lu, HR; Liu, H		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Zhaoran; Lu, Huanran; Liu, Han			Tighten after Relax: Minimax-Optimal Sparse PCA in Polynomial Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				PRINCIPAL COMPONENT ANALYSIS; POWER METHOD; RATES	We provide statistical and computational analysis of sparse Principal Component Analysis (PCA) in high dimensions. The sparse PCA problem is highly nonconvex in nature. Consequently, though its global solution attains the optimal statistical rate of convergence, such solution is computationally intractable to obtain. Meanwhile, although its convex relaxations are tractable to compute, they yield estimators with suboptimal statistical rates of convergence. On the other hand, existing nonconvex optimization procedures, such as greedy methods, lack statistical guarantees. In this paper, we propose a two-stage sparse PCA procedure that attains the optimal principal subspace estimator in polynomial time. The main stage employs a novel algorithm named sparse orthogonal iteration pursuit, which iteratively solves the underlying nonconvex problem. However, our analysis shows that this algorithm only has desired computational and statistical guarantees within a restricted region, namely the basin of attraction. To obtain the desired initial estimator that falls into this region, we solve a convex formulation of sparse PCA with early stopping. Under an integrated analytic framework, we simultaneously characterize the computational and statistical performance of this two-stage procedure. Computationally, our procedure converges at the rate of 1/root t within the initialization stage, and at a geometric rate within the main stage. Statistically, the final principal subspace estimator achieves the minimax-optimal statistical rate of convergence with respect to the sparsity level s*, dimension d and sample size n. Our procedure motivates a general paradigm of tackling nonconvex statistical learning problems with provable statistical guarantees.	[Wang, Zhaoran; Lu, Huanran; Liu, Han] Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08540 USA	Princeton University	Wang, ZR (corresponding author), Princeton Univ, Dept Operat Res & Financial Engn, Princeton, NJ 08540 USA.	zhaoran@princeton.edu; huanranl@princeton.edu; hanliu@princeton.edu	Wang, Zhaoran/P-7113-2018		 [NSF IIS1408910];  [NSF IIS1332109];  [NIH R01MH102339];  [NIH R01GM083084];  [NIH R01HG06841]	; ; ; ; 	This research is partially supported by the grants NSF IIS1408910, NSF IIS1332109, NIH R01MH102339, NIH R01GM083084, and NIH R01HG06841.	Amini AA, 2009, ANN STAT, V37, P2877, DOI 10.1214/08-AOS664; Arora R, 2012, ANN ALLERTON CONF, P861, DOI 10.1109/Allerton.2012.6483308; Berthet Q., 2013, C LEARN THEOR, P1046; Berthet Q, 2013, ANN STAT, V41, P1780, DOI 10.1214/13-AOS1127; Birnbaum A, 2013, ANN STAT, V41, P1055, DOI 10.1214/12-AOS1014; Boyd S, 2011, TRENDS MACH LEARN, V3, P1, DOI DOI 10.1561/2200000016; Cai TT, 2013, ANN STAT, V41, P3074, DOI 10.1214/13-AOS1178; d'Aspremont A, 2008, J MACH LEARN RES, V9, P1269; d'Aspremont A, 2007, SIAM REV, V49, P434, DOI 10.1137/050645506; Engelhardt BE, 2010, PLOS GENET, V6, DOI 10.1371/journal.pgen.1001117; Golub G. H., 2012, MATRIX COMPUTATIONS; He BS, 2014, SIAM J OPTIMIZ, V24, P1011, DOI 10.1137/13090849X; Johnstone IM, 2009, J AM STAT ASSOC, V104, P682, DOI 10.1198/jasa.2009.0121; Jolliffe IT, 2003, J COMPUT GRAPH STAT, V12, P531, DOI 10.1198/1061860032148; Journee M, 2010, J MACH LEARN RES, V11, P517; Lei J., 2014, ARXIV14016978; Ma Z., 2013, ANN STAT, P41; Mackey L., 2009, P ADV NEUR INF PROC, V21, P1017; Moghaddam B., 2006, ADV NEURAL INFORM PR, P915; Nadler B, 2008, ANN STAT, V36, P2791, DOI 10.1214/08-AOS618; Shen HP, 2008, J MULTIVARIATE ANAL, V99, P1015, DOI 10.1016/j.jmva.2007.06.007; Vu V., 2012, INT C ARTIFICIAL INT, P1278; Vu VQ, 2013, ANN STAT, V41, P2905, DOI 10.1214/13-AOS1151; Vu VQ, 2013, ADV NEURAL INFORM PR, V26; Wang Z., 2014, ARXIV14085352; Wang ZR, 2014, ANN STAT, V42, P2164, DOI 10.1214/14-AOS1238; Witten DM, 2009, BIOSTATISTICS, V10, P515, DOI 10.1093/biostatistics/kxp008; Yuan XT, 2013, J MACH LEARN RES, V14, P899; Zou H, 2006, J COMPUT GRAPH STAT, V15, P265, DOI 10.1198/106186006X113430	31	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100031
C	Yang, E; Lozano, AC; Ravikumar, P		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Yang, Eunho; Lozano, Aurelie C.; Ravikumar, Pradeep			Elementary Estimators for Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				SELECTION	We propose a class of closed-form estimators for sparsity-structured graphical models, expressed as exponential family distributions, under high-dimensional settings. Our approach builds on observing the precise manner in which the classical graphical model MLE "breaks down" under high-dimensional settings. Our estimator uses a carefully constructed, well-defined and closed-form backward map, and then performs thresholding operations to ensure the desired sparsity structure. We provide a rigorous statistical analysis that shows that surprisingly our simple class of estimators recovers the same asymptotic convergence rates as those of the l(1)-regularized MLEs that are much more difficult to compute. We corroborate this statistical performance, as well as significant computational advantages via simulations of both discrete and Gaussian graphical models.	[Yang, Eunho; Lozano, Aurelie C.] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Ravikumar, Pradeep] Univ Texas Austin, Austin, TX 78712 USA	International Business Machines (IBM); University of Texas System; University of Texas Austin	Yang, E (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	eunhyang@us.ibm.com; aclozano@us.ibm.com; pradeepr@cs.utexas.edu	Yang, Eunho/K-8395-2016		ARO [W911NF-12-1-0390]; NSF [IIS-1149803, IIS-1320894, IIS-1447574, DMS-1264033]	ARO; NSF(National Science Foundation (NSF))	E.Y and P.R. acknowledge the support of ARO via W911NF-12-1-0390 and NSF via IIS-1149803, IIS-1320894, IIS-1447574, and DMS-1264033	Banerjee O, 2008, J MACH LEARN RES, V9, P485; Bickel PJ, 2008, ANN STAT, V36, P2577, DOI 10.1214/08-AOS600; Boyd S, 2004, CONVEX OPTIMIZATION; Cai T, 2011, J AM STAT ASSOC, V106, P594, DOI 10.1198/jasa.2011.tm10155; CROSS GR, 1983, IEEE T PATTERN ANAL, V5, P25, DOI 10.1109/TPAMI.1983.4767341; Friedman J., 2007, BIOSTATISTICS; Hassner M., 1978, Proceedings of the 4th International Joint Conference on Pattern Recognition, P538; Hsieh C-J., 2011, ADV NEURAL INFORM PR, P24; Ising E, 1925, Z PHYS, V31, P253, DOI 10.1007/BF02980577; Jalali A., 2011, INT C AI STAT AISTAT, V14; Li L, 2010, MATH PROGRAM COMPUT, V2, P291, DOI 10.1007/s12532-010-0020-6; Manning CD, 1999, FDN STAT NATURAL LAN; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; Ravikumar P, 2011, ELECTRON J STAT, V5, P935, DOI 10.1214/11-EJS631; Ravikumar P, 2010, ANN STAT, V38, P1287, DOI 10.1214/09-AOS691; Ripley B, 1981, U PENN LAW REV, P252, DOI DOI 10.2307/3313062; Rothman AJ, 2009, J AM STAT ASSOC, V104, P177, DOI 10.1198/jasa.2009.0101; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wainwright M. J., 2003, INT C AI STAT AISTAT; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; WOODS JW, 1978, IEEE T AUTOMAT CONTR, V23, P846, DOI 10.1109/TAC.1978.1101866; Yang E., 2011, INT C MACH LEARN ICM, P28; Yang E., 2014, INT C MACH LEARN ICM, P31; Yedidia JS, 2001, ADV NEUR IN, V13, P689; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018	25	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100097
C	Zhang, HC; Yang, JC		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhang, Haichao; Yang, Jianchao			Scale Adaptive Blind Deblurring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					The presence of noise and small scale structures usually leads to large kernel estimation errors in blind image deblurring empirically, if not a total failure. We present a scale space perspective on blind deblurring algorithms, and introduce a cascaded scale space formulation for blind deblurring. This new formulation suggests a natural approach robust to noise and small scale structures through tying the estimation across multiple scales and balancing the contributions of different scales automatically by learning from data. The proposed formulation also allows to handle non-uniform blur with a straightforward extension. Experiments are conducted on both benchmark dataset and real-world images to validate the effectiveness of the proposed method. One surprising finding based on our approach is that blur kernel estimation is not necessarily best at the finest scale.	[Zhang, Haichao] Duke Univ, Durham, NC 27706 USA; [Yang, Jianchao] Adobe Res, San Jose, CA USA	Duke University; Adobe Systems Inc.	Zhang, HC (corresponding author), Duke Univ, Durham, NC 27706 USA.	hczhang1@gmail.com; jiayang@adobe.com			Adobe Systems	Adobe Systems	The research was supported in part by Adobe Systems.	[Anonymous], 2011, ICCV; Candes E. J., 2012, CORR; Cho S., 2009, ACM SIGGRAPH ASIA 20; Ekanadham C, 2011, NIPS; Elder JH, 1998, IEEE T PATTERN ANAL, V20, P699, DOI 10.1109/34.689301; Farbman Z., 2008, SIGGRAPH; Fergus R., 2006, SIGGRAPH; Gupta A., 2010, ECCV; Harmeling S., 2010, NIPS; Karklin Y., 2011, NIPS; Krishnan D., 2011, CVPR; Levin A., 2007, TECHNICAL REPORT; Levin A, 2011, IEEE T PATTERN ANAL, V33, P2354, DOI 10.1109/TPAMI.2011.148; Nadler Boaz, 2011, CVPR; Shan Q., 2008, SIGGRAPH; Tai YW, 2012, PROC CVPR IEEE, P17, DOI 10.1109/CVPR.2012.6247653; Tai YW, 2011, IEEE T PATTERN ANAL, V33, P1603, DOI 10.1109/TPAMI.2010.222; Tomasi C., 1998, BILATERAL FILTERING; Tschumperle D, 2005, IEEE T PATTERN ANAL, V27, P506, DOI 10.1109/TPAMI.2005.87; Wipf D. P., 2013, CORR; Xu L., 2013, CVPR; Xu L., 2010, ECCV; Xu Y., 2012, ICASSP; Zhang Haichao, 2013, NIPS; Zhong L., 2013, CVPR; [No title captured]	26	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103017
C	Zheng, JG; Jiang, ZL; Chellappa, R; Phillips, PJ		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zheng, Jinging; Jiang, Zhuolin; Chellappa, Rama; Phillips, P. Jonathon			Submodular Attribute Selection for Action Recognition in Video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In real-world action recognition problems, low-level features cannot adequately characterize the rich spatial-temporal structures in action videos. In this work, we encode actions based on attributes that describes actions as high-level concepts e.g., jump forward or motion in the air. We base our analysis on two types of action attributes. One type of action attributes is generated by humans. The second type is data-driven attributes, which are learned from data using dictionary learning methods. Attribute-based representation may exhibit high variance due to noisy and redundant attributes. We propose a discriminative and compact attribute-based representation by selecting a subset of discriminative attributes from a large attribute set. Three attribute selection criteria are proposed and formulated as a submodular optimization problem. A greedy optimization algorithm is presented and guaranteed to be at least (1-1/e)-approximation to the optimum. Experimental results on the Olympic Sports and UCF101 datasets demonstrate that the proposed attribute-based representation can significantly boost the performance of action recognition algorithms and outperform most recently proposed recognition approaches.	[Zheng, Jinging; Chellappa, Rama] Univ Maryland, UMIACS, College Pk, MD 20742 USA; [Jiang, Zhuolin] Huawei Technol, Noahs Ark Lab, Shenzhen, Peoples R China; [Phillips, P. Jonathon] NIST, Gaithersburg, MD 20899 USA	University System of Maryland; University of Maryland College Park; Huawei Technologies; National Institute of Standards & Technology (NIST) - USA	Zheng, JG (corresponding author), Univ Maryland, UMIACS, College Pk, MD 20742 USA.	zjngjng@umiacs.umd.edu; zhuolin.jiang@huawei.com; rama@umiacs.umd.edu; jonathon.phillips@nist.gov	Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012		MURI from the Office of Naval research [1141221258513]	MURI from the Office of Naval research	The identification of any commercial product or trade name does not imply endorsement or recommendation by NIST. This research was partially supported by a MURI from the Office of Naval research under the Grant 1141221258513.	Aharon M., 2006, IEEE T SIGNAL PROCES; Cover TM, 2006, ELEMENTS INFORM THEO; Das A., 2012, NIPS; Dollar P, 2005, BEHAV RECOGNITION VI; Efros A. A., 2003, ICCV; Farhadi A., 2009, CVPR; Fathi Alireza, 2008, CVPR; Gorelick Lena, 2005, ICCV; Iyer R., 2013, ICML; Jain A., 2013, CVPR; Jiang Z., 2013, PAMI; KRAUSE A, 2010, ICML; Krause A., 2005, ICML; Kuipers B., 2011, CVPR; Lampert C. H., 2009, CVPR; Laptev I., 2003, ICCV; Leskovec J., 2007, KDD; Li W., 2012, NIPS; Lin H., 2011, P ACL; Lin Z., 2009, CVPR; Liu J., 2009, CVPR; Liu M.-Y., 2014, PAMI; Liu Y., 2013, ICASSP; Nemhauser G. L., 1978, MATH PROGRAMMING; Niebles J., 2010, ECCV; Perronnin F., 2010, ECCV; Raptis M., 2010, ECCV; Raptis M., 2012, INT C COMP VIS PATT; Sadanand S., 2012, P 2012 IEEE C COMP V; Soomro K., 2012, COMPUT SCI; Streeter M., 2008, NIPS; Tang K., 2012, CVPR; Wang H., 2013, ICCV; Wang H., 2013, INT J COMPUTER VISIO; Wang Y., 2009, CVPR; Wu J., 2013, ICCV; Zhu J., 2013, ICCV	37	1	1	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103016
C	Archer, C; Leen, TK; Baptista, A		Thrun, S; Saul, K; Scholkopf, B		Archer, C; Leen, TK; Baptista, A			Parameterized novelty detection for environmental sensor monitoring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					As part of an environmental observation and forecasting system, sensors deployed in the Columbia RIver Estuary (CORIE) gather information on physical dynamics and changes in estuary habitat. Of these, salinity sensors are particularly susceptible to bio-fouling, which gradually degrades sensor response and corrupts critical data. Automatic fault detectors have the capability to identify bio-fouling early and minimize data loss. Complicating the development of discriminatory classifiers is the scarcity of bio-fouling onset examples and the variability of the bio-fouling signature. To solve these problems, we take a novelty detection approach that incorporates a parameterized bio-fouling model. These detectors identify the occurrence of bio-fouling, and its onset time as reliably as human experts. Real-time detectors installed during the summer of 2001 produced no false alarms, yet detected all episodes of sensor degradation before the field staff scheduled these sensors for cleaning. From this initial deployment through February 2003, our bio-fouling detectors have essentially doubled the amount of useful data coming from the CORTE sensors.	Oregon Hlth Sci Univ, OGI Sch Sci & Engn, Beaverton, OR 97006 USA	Oregon Health & Science University	Archer, C (corresponding author), Oregon Hlth Sci Univ, OGI Sch Sci & Engn, 20000 NW Walker Rd, Beaverton, OR 97006 USA.			Baptista, Antonio/0000-0002-7641-5937				ARCHER C, 2003, WATER RESOURCES RES, V39; BAPTISTA A, 1999, EARTH SYSTEM MONITOR, V9; BASSEVILLE M, 1988, AUTOMATICA, V24, P309, DOI 10.1016/0005-1098(88)90073-8; *USA CORPS ENG, 2001, BIOL ASS COL RIV CHA	4	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						619	626						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500078
C	Aviel, Y; Horn, D; Abeles, M		Thrun, S; Saul, K; Scholkopf, B		Aviel, Y; Horn, D; Abeles, M			The doubly balanced network of spiking neurons: a memory model with high capacity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				ASSOCIATIVE MEMORY; NEURAL NETWORKS; DYNAMICS	A balanced network leads to contradictory constraints on memory models, as exemplified in previous work on accommodation of synfire chains. Here we show that these constraints can be overcome by introducing a 'shadow' inhibitory pattern for each excitatory pattern of the model. This is interpreted as a doublebalance principle, whereby there exists both global balance between average excitatory and inhibitory currents and local balance between the currents carrying coherent activity at any given time frame. This principle can be applied to networks with Hebbian cell assemblies, leading to a high capacity of the associative memory. The number of possible patterns is limited by a combinatorial constraint that turns out to be P=0.06N within the specific model that we employ. This limit is reached by the Hebbian cell assembly network. To the best of our knowledge this is the first time that such high memory capacities are demonstrated in the asynchronous state of models of spiking neurons.	Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Aviel, Y (corresponding author), Hebrew Univ Jerusalem, Interdisciplinary Ctr Neural Computat, IL-91904 Jerusalem, Israel.			Horn, David/0000-0003-2708-186X				Amit DJ, 1997, NETWORK-COMP NEURAL, V8, P373, DOI 10.1088/0954-898X/8/4/003; Aviel Y, 2003, NEURAL COMPUT, V15, P1321, DOI 10.1162/089976603321780290; AVIEL Y, 2003, MEMORY CAPACITY BALA; Brunel N, 2001, J COMPUT NEUROSCI, V11, P63, DOI 10.1023/A:1011204814320; Brunel N, 2000, J COMPUT NEUROSCI, V8, P183, DOI 10.1023/A:1008925309027; GERSTNER W, 1992, NETWORK-COMP NEURAL, V3, P139, DOI 10.1088/0954-898X/3/2/004; HERTZ JA, 1999, NEURONAL INFORMATION; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002; Tuckwell H.C., 1988, INTRO THEORETICAL NE, DOI [10.1017/CBO9780511623202, DOI 10.1017/CBO9780511623202]; van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214; WILLSHAW DJ, 1969, NATURE, V222, P960, DOI 10.1038/222960a0	12	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1247	1254						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500155
C	Bengio, Y; Grandvalet, Y		Thrun, S; Saul, K; Scholkopf, B		Bengio, Y; Grandvalet, Y			No unbiased estimator of the variance of K-fold cross-validation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Most machine learning researchers perform quantitative experiments to estimate generalization error and compare algorithm performances. In order to draw statistically convincing conclusions, it is important to estimate the uncertainty of such estimates. This paper studies the estimation of uncertainty around the K-fold cross-validation estimator. The main theorem shows that there exists no universal unbiased estimator of the variance of K-fold cross-validation. An analysis based on the eigendecomposition of the covariance matrix of errors helps to better understand the nature of the problem and shows that naive estimators may grossly underestimate variance, as conpoundrmed by numerical experiments.	Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Bengio, Y (corresponding author), Univ Montreal, Dept IRO, CP 6128, Montreal, PQ H3C 3J7, Canada.							Bengio Yoshua, 2003, J MACHINE LEARNING R; Breiman L, 1996, ANN STAT, V24, P2350; Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197; Kohavi R., 1995, IJCAI, P1137; Nadeau C, 2003, MACH LEARN, V52, P239, DOI 10.1023/A:1024068626366; STONE M, 1974, J R STAT SOC B, V36, P111, DOI 10.1111/j.2517-6161.1974.tb00994.x; Tibshirani R.J., 1993, MONOGRAPHS STAT APPL, VVolume 57, P1	9	1	1	1	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						513	520						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500065
C	Bofill-i-Petit, A; Murray, AF		Thrun, S; Saul, K; Scholkopf, B		Bofill-i-Petit, A; Murray, AF			Synchrony detection by analogue VLSI neurons with bimodal STDP synapses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We present test results from spike-timing correlation learning experiments carried out with silicon neurons with STDP (Spike Timing Dependent Plasticity) synapses. The weight change scheme of the STDP synapses can be set to either weight-independent or weight-dependent mode. We present results that characterise the learning window implemented for both modes of operation. When presented with spike trains with different types of synchronisation the neurons develop bimodal weight distributions. We also show that a 2-layered network of silicon spiking neurons with STDP synapses can perform hierarchical synchrony detection.	Univ Edinburgh, Edinburgh EH9 3JL, Midlothian, Scotland	University of Edinburgh	Bofill-i-Petit, A (corresponding author), Univ Edinburgh, Edinburgh EH9 3JL, Midlothian, Scotland.	adria.bofill@ee.ed.ac.uk; alan.murray@ee.ed.ac.uk						Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; BOFILL A, 2002, ADV NEURAL INFORMATI, V14; Bofill-i-Petit A, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL V, P817; HALFIGER P, 1996, ADV NEURAL INFORMATI, V9, P692; INDIVERI G, 2003, ADV NEURAL INFORMATI, V15; Kepecs A, 2002, BIOL CYBERN, V87, P446, DOI 10.1007/s00422-002-0358-6; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; van Rossum MCW, 2001, NEUROCOMPUTING, V38, P409, DOI 10.1016/S0925-2312(01)00360-5; VOGELSTEIN RJ, 2003, ADV NEURAL INFORMATI, V15; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	11	1	1	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1027	1034						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500128
C	Chen, H; Fleury, P; Murray, AF		Thrun, S; Saul, K; Scholkopf, B		Chen, H; Fleury, P; Murray, AF			Minimising contrastive divergence in noisy, mixed-mode VLSI neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				CMOS TECHNOLOGY; NETWORKS; PRODUCTS; EXPERTS	This paper presents VLSI circuits with continuous-valued probabilistic behaviour realized by injecting noise into each computing unit (neuron). Interconnecting the noisy neurons forms a Continuous Restricted Boltzmann Machine (CRBM), which has shown promising performance in modelling and classifying noisy biomedical data. The Minimising-Contrastive-Divergence learning algorithm for CRBM is also implemented in mixed-mode VLSI, to adapt the noisy neurons' parameters on-chip.	Univ Edinburgh, Sch Engn & Elect, Edinburgh EH9 3JL, Midlothian, Scotland	University of Edinburgh	Chen, H (corresponding author), Univ Edinburgh, Sch Engn & Elect, Mayfield Rd, Edinburgh EH9 3JL, Midlothian, Scotland.							ALSPECTOR J, 1991, IEEE T CIRCUITS SYST, V38, P109, DOI 10.1109/31.101308; BANU M, 1982, ELECTRON LETT, V18, P678, DOI 10.1049/el:19820461; Cauwenberghs G, 1996, IEEE T NEURAL NETWOR, V7, P346, DOI 10.1109/72.485671; Chen H, 2003, IEE P-VIS IMAGE SIGN, V150, P153, DOI 10.1049/ip-vis:20030362; CHIBLE H, 2000, 7 IEEE INT C EL CIRC, V2, P1004; FLEURY P, 2003, IEEE P INT S CIRC SY, V5, P653; Frey BJ, 1997, ADV NEUR IN, V9, P452; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Murray AF, 2001, NEURAL NETWORKS, V14, P1257, DOI 10.1016/S0893-6080(01)00097-1; TANG T, 2003, P 13 INT C ART NEUR, P638; VITTOZ EA, 1983, IEEE J SOLID-ST CIRC, V18, P273, DOI 10.1109/JSSC.1983.1051939; WEGMANN G, 1990, IEE PROC-G, V137, P95, DOI 10.1049/ip-g-2.1990.0017	12	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1011	1018						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500126
C	Derbeko, P; El-Yaniv, R; Meir, R		Thrun, S; Saul, K; Scholkopf, B		Derbeko, P; El-Yaniv, R; Meir, R			Error bounds for transductive learning via compression and clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					This paper is concerned with transductive learning. Although transduction appears to be an easier task than induction, there have not been many provably useful algorithms and bounds for transduction. We present explicit error bounds for transduction and derive a general technique for devising bounds within this setting. The technique is applied to derive error bounds for compression schemes such as (transductive) SVMs and for transduction algorithms based on clustering.	Technion Israel Inst Technol, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Derbeko, P (corresponding author), Technion Israel Inst Technol, IL-32000 Haifa, Israel.	philip@cs.technion.ac.il; rani@cs.technion.ac.il; rmeir@ee.technion.ac.il						BLUM A, 2003, COLT, P344; Blum PR, 2001, STUD EUROP JUDAISM, V1, P19; BOTTOU L, 1994, EFFECTIVE VC DIMENSI; Dembo A., 1998, LARGE DEVIATION TECH; El-Yaniv R., 2001, ADV NEURAL INFORM PR, V14, P1025; Herbrich R., 2002, LEARNING KERNEL CLAS; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Joachims T., 2003, P 20 INT C MACH LEAR; JOACHIMS T, 1999, EUR C MACH LEARN; Lanckriet G. R. G., 2002, LEARNING KERNEL MATR; McAleese J, 1999, AEROSPACE AM, V37, P3; McAllester D, 2003, LECT NOTES ARTIF INT, V2777, P203, DOI 10.1007/978-3-540-45167-9_16; McAllester DA, 2003, MACH LEARN, V51, P5, DOI 10.1023/A:1021840411064; Vapnik V., 1982, ESTIMATION DEPENDENC; Vapnik V.N, 1998, STAT LEARNING THEORY; WU D, 1999, INT C MACH LEARN	16	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1085	1092						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500135
C	Gruber, A; Weiss, Y		Thrun, S; Saul, K; Scholkopf, B		Gruber, A; Weiss, Y			Factorization with uncertainty and missing data: exploiting temporal coherence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				PERCEPTION	The problem of "Structure From Motion" is a central problem in vision: given the 2D locations of certain points we wish to recover the camera motion and the 3D coordinates of the points. Under simplified camera models, the problem reduces to factorizing a measurement matrix into the product of two low rank matrices. Each element of the measurement matrix contains the position of a point in a particular image. When all elements are observed, the problem can be solved trivially using SVD, but in any realistic situation many elements of the matrix are missing and the ones that are observed have a, different directional uncertainty. Under these conditions, most existing factorization algorithms fail while human perception is relatively unchanged. In this paper we use the well known EM algorithm for factor analysis to perform factorization. This allows us to easily handle missing data and measurement uncertainty and more importantly allows us to place a prior on the temporal trajectory of the latent variables (the camera position). We show that incorporating this prior gives a significant improvement in performance in challenging image sequences.	Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Gruber, A (corresponding author), Hebrew Univ Jerusalem, Sch Comp Sci & Engn, IL-91904 Jerusalem, Israel.	amitg@cs.huji.ac.il; yweiss@cs.huji.ac.il						Andersen RA, 1998, TRENDS COGN SCI, V2, P222, DOI 10.1016/S1364-6613(98)01181-4; Brand M, 2002, LECT NOTES COMPUT SC, V2350, P707; DELLAERT F, 1999, ICCV, P696; Gelb A., 1974, APPL OPTIMAL ESTIMAT; IRANI M, 2000, ECCV, P959; JACOBS D, 1997, CVPR, P206; MORRIS DD, 1999, ICCV, P696; ROWEIS S, 1997, NIPS, P431; SHUM HY, 1995, PRINCIPAL COMPONENT, P854; SOATTO S, 1999, IEEE T PATTERN ANAL, P943; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TREUE S, 1991, VISION RES, V31, P59, DOI 10.1016/0042-6989(91)90074-F; ULLMAN S, 1979, INTERPERTATION VISUA	13	1	1	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1507	1514						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500187
C	Harrison, RR		Thrun, S; Saul, K; Scholkopf, B		Harrison, RR			A low-power analog VLSI visual collision detector	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				LATERAL GENICULATE-NUCLEUS; RESPONSE PROPERTIES; MOTION; COMPUTATION; SYSTEM; CELLS; FLY	We have designed and tested a single-chip analog VLSI sensor that detects imminent collisions by measuring radially expansive optic flow. The design of the chip is based on a model proposed to explain leg-extension behavior in flies during landing approaches. A new elementary motion detector (EMD) circuit was developed to measure optic flow. This EMD circuit models the bandpass nature of large monopolar cells (LMCs) immediately postsynaptic to photoreceptors in the fly visual system. A 16 X 16 array of 2-D motion detectors was fabricated on a 2.24 mm x 2.24 mm die in a standard 0.5-mum CMOS process. The chip consumes 140 muW of power from a 5 V supply. With the addition of wide-angle optics, the sensor is able to detect collisions around 500 ms before impact in complex, real-world scenes.	Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Harrison, RR (corresponding author), Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA.							ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; BORST A, 1988, J COMP PHYSIOL A, V163, P167, DOI 10.1007/BF00612426; DONG DW, 1995, NETWORK-COMP NEURAL, V6, P159, DOI 10.1088/0954-898X/6/2/003; Dror RO, 2001, J OPT SOC AM A, V18, P241, DOI 10.1364/JOSAA.18.000241; Duchon AP, 1998, ADAPT BEHAV, V6, P473, DOI 10.1177/105971239800600306; EGELHAAF M, 1989, J OPT SOC AM A, V6, P116, DOI 10.1364/JOSAA.6.000116; Gabbiani F, 1999, J NEUROSCI, V19, P1122; Harrison RR, 2000, ANALOG INTEGR CIRC S, V24, P213, DOI 10.1023/A:1008361525235; HARRISON RR, UNPUB NIPS 2003; HASSENSTEIN B, 1956, Z NATURFORSCH PT B, V11, P513; LAUGHLIN SB, 1994, PROG RETIN EYE RES, V13, P165, DOI 10.1016/1350-9462(94)90009-4; Mead, 1989, ANALOG VLSI NEURAL S; SAUL AB, 1990, J NEUROPHYSIOL, V64, P206, DOI 10.1152/jn.1990.64.1.206; Sun HJ, 1998, NAT NEUROSCI, V1, P296, DOI 10.1038/1110; Tammero LF, 2002, J EXP BIOL, V205, P2785; VANHATEREN JH, 1992, J COMP PHYSIOL A, V171, P157, DOI 10.1007/BF00188924; VanHateren JH, 1997, VISION RES, V37, P3407, DOI 10.1016/S0042-6989(97)00105-3	17	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						987	994						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500123
C	Hinton, G; Welling, M; Mnih, A		Thrun, S; Saul, K; Scholkopf, B		Hinton, G; Welling, M; Mnih, A			Wormholes improve contrastive divergence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In models that define probabilities via energies, maximum likelihood learning typically involves using Markov Chain Monte Carlo to sample from the model's distribution. If the Markov chain is started at the data distribution, learning often works well even if the chain is only run for a few time steps [3]. But if the data distribution contains modes separated by regions of very low density, brief MCMC will not ensure that different modes have the correct relative energies because it cannot move particles from one mode to another. We show how to improve brief MCMC by allowing long-range moves that are suggested by the data distribution. If the model is approximately correct, these long-range moves have a reasonable acceptance rate.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G5, Canada	University of Toronto	Hinton, G (corresponding author), Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd, Toronto, ON M5S 3G5, Canada.							BENGIO Y, 2001, ADV NEURAL INFORMATI; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; JARZYNSKI C, 2001, LAUR012157; LeCun Y, 1989, P 1988 CONN MOD SUMM, P29; Neal R., 1993, CRGTR931 U TOR COMP; Sminchisescu C., 2003, CSRG478 U TOR; TJELEMELAND H, 1999, 11999 NORW U SCI TEC; VOTER A, 1985, MONTE CARLO METHOD D, V82	8	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						417	424						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500053
C	Hoyle, DC; Rattray, M		Thrun, S; Saul, K; Scholkopf, B		Hoyle, DC; Rattray, M			Limiting form of the sample covariance eigenspectrum in PCA and kernel PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				DISTRIBUTIONS; MATRICES	We derive the limiting form of the eigenvalue spectrum for sample co-variance matrices produced from non-isotropic data. For the analysis of standard PCA we study the case where the data has increased variance along a small number of symmetry-breaking directions. The spectrum depends on the strength of the symmetry-breaking signals and on a parameter a which is the ratio of sample size to data dimension. Results are derived in the limit of large data dimension while keeping a fixed. As a increases there are transitions in which delta functions emerge from the upper end of the bulk spectrum, corresponding to the symmetry-breaking directions in the data, and we calculate the bias in the corresponding eigenvalues. For kernel PCA the covariance matrix in feature space may contain symmetry-breaking structure even when the data components are independently distributed with equal variance. We show examples of phase-transition behaviour analogous to the PCA results in this case.	Univ Manchester, Dept Comp Sci, Manchester M13 9PL, Lancs, England	University of Manchester	Hoyle, DC (corresponding author), Univ Manchester, Dept Comp Sci, Manchester M13 9PL, Lancs, England.	david.c.hoyle@man.ac.uk; magnus@cs.man.ac.uk	Rattray, Magnus/B-4393-2009; Koenig, Matthias/GZN-0200-2022; Peitgen, Heinz-Otto/ABG-2100-2021; Rattray, Magnus/AAE-3297-2021; Rattray, Marcus/C-4088-2009	Rattray, Magnus/0000-0001-8196-5565; Rattray, Magnus/0000-0001-8196-5565; Rattray, Marcus/0000-0002-6620-9830				BAI ZD, 1993, ANN PROBAB, V21, P649, DOI 10.1214/aop/1176989262; Halkjaer S, 1997, ADV NEUR IN, V9, P169; HOYLE DC, IN PRESS PHYS REV E; Johnstone IM, 2001, ANN STAT, V29, P295, DOI 10.1214/aos/1009210544; Jolliffe I.T., 1986, PRINCIPAL COMPONENT; Marenko V. A., 1967, MATH USSR SB, V1, P457, DOI [10.1070/SM1967v001n04ABEH001994, DOI 10.1070/SM1967V001N04ABEH001994]; Minka TP, 2001, ADV NEUR IN, V13, P598; Reimann P, 1996, J PHYS A-MATH GEN, V29, P3521, DOI 10.1088/0305-4470/29/13/021; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Sengupta AM, 1999, PHYS REV E, V60, P3389, DOI 10.1103/PhysRevE.60.3389; WACHTER KW, 1978, ANN PROBAB, V6, P1, DOI 10.1214/aop/1176995607	16	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1181	1188						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500147
C	Kauchak, D; Dasgupta, S		Thrun, S; Saul, K; Scholkopf, B		Kauchak, D; Dasgupta, S			An iterative improvement procedure for hierarchical clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We describe a procedure which finds a hierarchical clustering by hill-climbing. The cost function we use is a hierarchical extension of the k-means cost; our local moves are tree restructurings and node reorderings. We show these can be accomplished efficiently, by exploiting special properties of squared Euclidean distances and by using techniques from scheduling algorithms.	Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Kauchak, D (corresponding author), Univ Calif San Diego, Dept Comp Sci, La Jolla, CA 92093 USA.							FEREA TL, 1999, P NATL ACAD SCI, V97; Hartigan J.A., 1975, CLUSTERING ALGORITHM; HARTIGAN JA, 1985, J CLASSIFICATION; HORN WA, 1972, SIAM J APPL MATH, V23, P189, DOI 10.1137/0123021; KAUCHAK D, 2003, UNPUB	5	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						481	488						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500061
C	Kwok, JT; Mak, B; Ho, S		Thrun, S; Saul, K; Scholkopf, B		Kwok, JT; Mak, B; Ho, S			Eigenvoice speaker adaptation via composite kernel PCA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Eigenvoice speaker adaptation has been shown to be effective when only a small amount of adaptation data is available. At the heart of the method is principal component analysis (PCA) employed to find the most important eigenvoices. In this paper, we postulate that nonlinear PCA, in particular kernel PCA, may be even more effective. One major challenge is to map the feature-space eigenvoices back to the observation space so that the state observation likelihoods can be computed during the estimation of eigenvoice weights and subsequent decoding. Our solution is to compute kernel PCA using composite kernels, and we will call our new method kernel eigenvoice speaker adaptation. On the TIDIGITS corpus, we found that compared with a speaker-independent model, our kernel eigenvoice adaptation method can reduce the word error rate by 28-33% while the standard eigenvoice approach can only match the performance of the speaker-independent model.	Hong Kong Univ Sci & Technol, Dept Comp Sci, Hong Kong, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Kwok, JT (corresponding author), Hong Kong Univ Sci & Technol, Dept Comp Sci, Clear Water Bay, Hong Kong, Hong Kong, Peoples R China.							[Anonymous], 2002, LEARNING KERNELS; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; GENTON MG, 2001, J MACHINE LEARNING R, V2, P299; Kuhn R, 2000, IEEE T SPEECH AUDI P, V8, P695, DOI 10.1109/89.876308; LEONARD RG, 1984, P IEEE INT C AC SPEE, V3, P4211; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; SMOLA A, 1999, 9903 U WISC DAT MIN	7	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1401	1408						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500174
C	Lee, SI; Batzoglou, S		Thrun, S; Saul, K; Scholkopf, B		Lee, SI; Batzoglou, S			ICA-based clustering of genes from microarray expression data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				INDEPENDENT COMPONENT ANALYSIS; CELL-CYCLE; SEPARATION; ALGORITHM	We propose an unsupervised methodology using independent component analysis (ICA) to cluster genes from DNA microarray data. Based on an ICA mixture model of genomic expression patterns, linear and nonlinear ICA finds components that are specific to certain biological processes. Genes that exhibit significant up-regulation or down-regulation within each component are grouped into clusters. We test the statistical significance of enrichment of gene annotations within each cluster. ICA-based clustering outperformed other leading methods in constructing functionally coherent clusters on various datasets. This result supports our model of genomic expression data as composite effect of independent biological processes. Comparison of clustering performance among various ICA algorithms including a kernel-based nonlinear ICA algorithm shows that nonlinear ICA performed the best for small datasets and natural-gradient maximization-likelihood worked well for all the datasets.	Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA	Stanford University	Lee, SI (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.							Alter O, 2000, P NATL ACAD SCI USA, V97, P10101, DOI 10.1073/pnas.97.18.10101; Amari S, 1996, ADV NEUR IN, V8, P757; Ashburner M, 2001, GENOME RES, V11, P1425; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cardoso JF, 1999, NEURAL COMPUT, V11, P157, DOI 10.1162/089976699300016863; Cho RJ, 1998, MOL CELL, V2, P65, DOI 10.1016/S1097-2765(00)80114-8; HARMELING S, ADV NEURAL INFORMATI, V8, P757; Hori G., 2000, P INT WORKSH IND COM, P151; Hsiao LL, 2001, PHYSIOL GENOMICS, V7, P97, DOI 10.1152/physiolgenomics.00040.2001; Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722; KANEHISA M, 2002, CURRENT TOPICS COMPU, P301; Kim SK, 2001, SCIENCE, V293, P2087, DOI 10.1126/science.1061603; Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719; Liebermeister W, 2002, BIOINFORMATICS, V18, P51, DOI 10.1093/bioinformatics/18.1.51; Misra J, 2002, GENOME RES, V12, P1112, DOI 10.1101/gr.225302; Spellman PT, 1998, MOL BIOL CELL, V9, P3273, DOI 10.1091/mbc.9.12.3273; Tavazoie S, 1999, NAT GENET, V22, P281, DOI 10.1038/10343; Troyanskaya O, 2001, BIOINFORMATICS, V17, P520, DOI 10.1093/bioinformatics/17.6.520	18	1	1	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						675	682						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500085
C	Malzahn, D; Opper, M		Thrun, S; Saul, K; Scholkopf, B		Malzahn, D; Opper, M			Approximate analytical bootstrap averages for support vector classifiers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					We compute approximate analytical bootstrap averages for support vector classification using a combination of the replica method of statistical physics and the TAP approach for approximate inference. We test our method on a few datasets and compare it with exact averages obtained by extensive Monte-Carlo sampling.	Tech Univ Denmark, DK-2800 Lyngby, Denmark	Technical University of Denmark	Malzahn, D (corresponding author), Tech Univ Denmark, R Petersens Plads,Bldg 321, DK-2800 Lyngby, Denmark.							[Anonymous], 1995, SPRINGER SERIES STAT; Efron B., 1979, ANN STAT, V7, P1, DOI DOI 10.1214/AOS/1176344552; MALZAHN D, 2003, NIPS, V15; Mezard M., 1987, LECT NOTES PHYS, V9; Opper M, 2000, NEURAL COMPUT, V12, P2655, DOI 10.1162/089976600300014881; Scholkopf B., 1999, ADV KERNEL METHODS S; Sollich P, 1999, IEE CONF PUBL, P91, DOI 10.1049/cp:19991090; Tibshirani R.J., 1993, MONOGRAPHS STAT APPL, VVolume 57, P1	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1189	1196						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500148
C	Marx, Z; Dagan, I; Shamir, E		Thrun, S; Saul, K; Scholkopf, B		Marx, Z; Dagan, I; Shamir, E			Identifying structure across pre-partitioned data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We propose an information-theoretic clustering approach that incorporates a pre-known partition of the data, aiming to identify common clusters that cut across the given partition. In the standard clustering setting the formation of clusters is guided by a single source of feature information. The newly utilized pre-partition factor introduces an additional bias that counterbalances the impact of the features whenever they become correlated with this known partition. The resulting algorithmic framework was applied successfully to synthetic data, as well as to identifying text-based cross-religion correspondences.	Hebrew Univ Jerusalem, Neural Computat Ctr, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Marx, Z (corresponding author), Hebrew Univ Jerusalem, Neural Computat Ctr, IL-91904 Jerusalem, Israel.							CHECHIK G, 2002, ADV NEURAL PROCESSIN, V15; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; DAGAN I, 2002, P 6 C NAT LANG LEARN, P15; FRIEDMAN N, 2002, 17 C UNC ART INT UAI, P152; Globerson A, 2003, Journal of Machine Learning Research, V3, P1307, DOI 10.1162/153244303322753689; Hofmann T, 2001, MACH LEARN, V42, P177, DOI 10.1023/A:1007617005950; MARX Z, 2002, J MACHINE LEARNING R, V3, P747; Tishby N, 1999, P 37 ANN ALL C COMM, V37, P368; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616	9	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						489	496						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500062
C	Nakatani, T; Miyoshi, M; Kinoshita, K		Thrun, S; Saul, K; Scholkopf, B		Nakatani, T; Miyoshi, M; Kinoshita, K			One microphone blind dereverberation based on quasi-periodicity of speech signals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Speech dereverberation is desirable with a view to achieving, for example, robust speech recognition in the real world. However, it is still a challenging problem, especially when using a single microphone. Although blind equalization techniques have been exploited, they cannot deal with speech signals appropriately because their assumptions are not satisfied by speech signals. We propose a new dereverberation principle based on an inherent property of speech signals, namely quasi-periodicity. The present methods learn the dereverberation filter from a lot of speech data with no prior knowledge of the data, and can achieve high quality speech dereverberation especially when the reverberation time is long.	NTT Corp, Commun Sci Labs, Speech Open Lab, Seika, Kyoto, Japan	Nippon Telegraph & Telephone Corporation	Nakatani, T (corresponding author), NTT Corp, Commun Sci Labs, Speech Open Lab, 2-4 Hikaridai, Seika, Kyoto, Japan.	nak@cslab.kecl.ntt.co.jp; miyo@cslab.kecl.ntt.co.jp; kinoshita@cslab.kecl.ntt.co.jp						Amari S, 1997, FIRST IEEE SIGNAL PROCESSING WORKSHOP ON SIGNAL PROCESSING ADVANCES IN WIRELESS COMMUNICATIONS, P101, DOI 10.1109/SPAWC.1997.630083; LEE BA, 2002, P ASJ GEN M AK JAP S, P27; Nakatani T, 2003, 2003 IEEE INTERNATIONAL CONFERENCE ON ACOUSTICS, SPEECH, AND SIGNAL PROCESSING, VOL I, PROCEEDINGS, P92; NAKATANI T, 2003, P IWAENC 2003 SEP; YEGNANARAYANA B, 1975, J ACOUST SOC AM, V58, P853, DOI 10.1121/1.380733; Yegnanarayana B, 2000, IEEE T SPEECH AUDI P, V8, P267, DOI 10.1109/89.841209	6	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1417	1424						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500176
C	Paskin, MA		Thrun, S; Saul, K; Scholkopf, B		Paskin, MA			Sample propagation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MODELS	Rao-Blackwellization is an approximation technique for probabilistic inference that flexibly combines exact inference with sampling. It is useful in models where conditioning on some of the variables leaves a simpler inference problem that can be solved tractably. This paper presents Sample Propagation, an efficient implementation of Rao-Blackwellized approximate inference for a large class of models. Sample Propagation tightly integrates sampling with message passing in a junction tree, and is named for its simple, appealing structure: it walks the clusters of a junction tree, sampling some of the current cluster's variables and then passing a message to one of its neighbors. We discuss the application of Sample Propagation to conditional Gaussian inference problems such as switching linear dynamical systems.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Paskin, MA (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.							BIDYUK B, 2003, P 19 C UNC AI UAI 03; Carter CK, 1996, BIOMETRIKA, V83, P589, DOI 10.1093/biomet/83.3.589; COWELL RG, 1999, PROBABILISTIC NETWOR; DAWID A, 1995, LECT NOTES COMPUTER, V945; DOUCET A, 2000, P 16 C UNC AI UAI 00; JENSEN CS, 1995, INT J HUM-COMPUT ST, V42, P647, DOI 10.1006/ijhc.1995.1029; KJAERULFF U, 1995, P 11 C UNC ART INT U; LAURITZEN SL, 1992, J AM STAT ASSOC, V87, P1098, DOI 10.2307/2290647; LERNER U, 2001, P 17 C UNC AI UAI 01; LERNER U, 2002, THESIS STANFORD U; Neal R. M., 1993, PROBABILISTIC INFERE; Shafer G.R, 1990, ANN MATH ARTIF INTEL, P327, DOI [10.1007/BF01531015, DOI 10.1007/BF01531015]; TEH YW, 2003, P 9 INT WORKSH AI ST	13	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						425	432						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500054
C	Porr, B; Saudargiene, A; Worgotter, F		Thrun, S; Saul, K; Scholkopf, B		Porr, B; Saudargiene, A; Worgotter, F			Analytical solution of spike-timing dependent plasticity based on synaptic biophysics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				ACTION-POTENTIAL INITIATION; NMDA RECEPTORS; CALCIUM INFLUX; MODEL; NEURONS; COINCIDENCE; MECHANISMS; BACKPROPAGATION; SPECIFICITY; DENDRITES	Spike timing plasticity (STDP) is a special form of synaptic plasticity where the relative timing of post- and presynaptic activity determines the change of the synaptic weight. On the postsynaptic side, active back-propagating spikes in dendrites seem to play a crucial role in the induction of spike timing dependent plasticity. We argue that postsynaptically the temporal change of the membrane potential determines the weight change. Coming from the presynaptic side induction of STDP is closely related to the activation of NMDA channels. Therefore, we will calculate analytically the change of the synaptic weight by correlating the derivative of the membrane potential with the activity of the NMDA channel. Thus, for this calculation we utilise biophysical variables of the physiological cell. The final result shows a weight change curve which conforms with measurements from biology. The positive part of the weight change curve is determined by the NMDA activation. The negative part of the weight change curve is determined by the membrane potential change. Therefore, the weight change curve should change its shape depending on the distance from the soma of the postsynaptic cell. We find temporally asymmetric weight change close to the soma and temporally symmetric weight change in the distal dendrite.	Univ Stirling, Stirling FK9 4LR, Scotland	University of Stirling	Porr, B (corresponding author), Univ Stirling, Stirling FK9 4LR, Scotland.							Abarbanel HDI, 2002, P NATL ACAD SCI USA, V99, P10132, DOI 10.1073/pnas.132651299; [Anonymous], [No title captured]; Bi GQ, 2002, BIOL CYBERN, V87, P319, DOI 10.1007/s00422-002-0349-7; Castellani GC, 2001, P NATL ACAD SCI USA, V98, P12772, DOI 10.1073/pnas.201404598; Golding NL, 2001, J NEUROPHYSIOL, V86, P2998, DOI 10.1152/jn.2001.86.6.2998; Hebb D. O., 1949, ORG BEHAV NEUROPHYCH; Johnston D, 2003, PHILOS T ROY SOC B, V358, P667, DOI 10.1098/rstb.2002.1248; Karmarkar UR, 2002, BIOL CYBERN, V87, P373, DOI 10.1007/s00422-002-0351-0; Karmarkar UR, 2002, J NEUROPHYSIOL, V88, P507, DOI 10.1152/jn.2002.88.1.507; Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498; Linden DJ, 1999, NEURON, V22, P661, DOI 10.1016/S0896-6273(00)80726-6; Magee JC, 1997, SCIENCE, V275, P209, DOI 10.1126/science.275.5297.209; Malenka RC, 1999, SCIENCE, V285, P1870, DOI 10.1126/science.285.5435.1870; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Nishiyama M, 2000, NATURE, V408, P584, DOI 10.1038/35046067; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Porr B, 2003, NEURAL COMPUT, V15, P831, DOI 10.1162/08997660360581921; Roberts PD, 1999, J COMPUT NEUROSCI, V7, P235, DOI 10.1023/A:1008910918445; Roberts PD, 2002, BIOL CYBERN, V87, P392, DOI 10.1007/s00422-002-0361-y; Schiller J, 1998, NAT NEUROSCI, V1, P114, DOI 10.1038/363; Senn W, 2001, NEURAL COMPUT, V13, P35, DOI 10.1162/089976601300014628; Shouval HZ, 2002, P NATL ACAD SCI USA, V99, P10831, DOI 10.1073/pnas.152343099; STEWART JL, 1960, FUNDAMENTALS SIGNAL; Stuart G, 1997, TRENDS NEUROSCI, V20, P125, DOI 10.1016/S0166-2236(96)10075-8; Stuart G, 1997, J PHYSIOL-LONDON, V505, P617, DOI 10.1111/j.1469-7793.1997.617ba.x; SUTTON RS, 1981, PSYCHOL REV, V88, P135, DOI 10.1037/0033-295X.88.2.135; Yuste R, 1999, J NEUROSCI, V19, P1976	27	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1343	1350						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500167
C	Scott, C; Nowak, R		Thrun, S; Saul, K; Scholkopf, B		Scott, C; Nowak, R			Near-minimax optimal classification with dyadic classification trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					This paper reports on a family of computationally practical classifiers that converge to the Bayes error at near-minimax optimal rates for a variety of distributions. The classifiers are based on dyadic classification trees (DCTs), which involve adaptively pruned partitions of the feature space. A key aspect of DCTs is their spatial adaptivity, which enables local (rather than global) fitting of the decision boundary. Our risk analysis involves a spatial decomposition of the usual concentration inequalities, leading to a spatially adaptive, data-dependent pruning criterion. For any distribution on (X, Y) whose Bayes decision boundary behaves locally like a Lipschitz smooth function, we show that the DCT error converges to the Bayes error at a rate within a logarithmic factor of the minimax optimal rate. We also study DCTs equipped with polynomial classification rules at each leaf, and show that as the smoothness of the boundary increases their errors converge to the Bayes error at a rate approaching n(-1/2), the parametric rate. We are not aware of any other practical classifiers that provide similar rate of convergence guarantees. Fast algorithms for tree pruning are discussed.	Rice Univ, Houston, TX 77005 USA	Rice University	Scott, C (corresponding author), Rice Univ, Houston, TX 77005 USA.	cscott@rice.edu; nowak@engr.wisc.edu						BARRON AR, 1991, NATO ADV SCI I C-MAT, V335, P561; BENNETT K, 1998, P IEEE INT JOINT C N, V41, P2396; Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808; KEARNS M, 1998, INT C MACH LEARN, P269; Mammen E, 1999, ANN STAT, V27, P1808; Mansour Y., 2000, P 13 ANN C COMP LEAR, P69; OKAMOTO M, 1958, ANN I STAT MATH, V10, P29; SCOTT C, 2002, TREE0201 RIC U; SCOTT C, 2002, ADV NEURAL INFORMATI, V14; SCOTT C, 2003, 0301 TREE RIC U	10	1	1	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1117	1124						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500139
C	Storkey, AJ		Thrun, S; Saul, K; Scholkopf, B		Storkey, AJ			Generalised propagation for fast Fourier transforms with partial or missing data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MODELS	Discrete Fourier transforms and other related Fourier methods have been practically implementable due to the fast Fourier transform (FFT). However there are many situations where doing fast Fourier transforms without complete data would be desirable. In this paper it is recognised that formulating the FFT algorithm as a belief network allows suitable priors to be set for the Fourier coefficients. Furthermore efficient generalised belief propagation methods between clusters of four nodes enable the Fourier coefficients to be inferred and the missing data to be estimated in near to O (n log n) time, where n is the total of the given and missing data points. This method is compared with a number of common approaches such as setting missing data to zero or to interpolation. It is tested on generated data and for a Fourier analysis of a damaged audio signal.	Univ Edinburgh, Sch Informat, Edinburgh EH8 9YL, Midlothian, Scotland	University of Edinburgh	Storkey, AJ (corresponding author), Univ Edinburgh, Sch Informat, 5 Forrest Hill, Edinburgh EH8 9YL, Midlothian, Scotland.							AJI SM, 2000, IEEE T INFORM THEORY, V47, P498; BOUMAN CA, 1994, IEEE T IMAGE PROCESS, V3, P162, DOI 10.1109/83.277898; FREY BJ, 1999, TR991 U WAT COMP SCI; HESKES T, 2003, NIPS15, P343; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; LUETTGEN MR, 1995, IEEE T IMAGE PROCESS, V4, P194, DOI 10.1109/83.342185; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Press WH, 1988, NUMERICAL RECIPES C; Storkey AJ, 1999, IEE CONF PUBL, P55, DOI 10.1049/cp:19991084; Weiss Y, 2000, NEURAL COMPUT, V12, P1, DOI 10.1162/089976600300015880; WEISS Y, 1999, UCBCSD991046 TR; WIEGERINCK W, 1998, P 10 NETH BELG C ART, P177; YEDIDA JS, 1994, TR2003135 MERI; Yedidia JS, 2001, ADV NEUR IN, V13, P689	14	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						433	440						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500055
C	Strohmann, TR; Belitski, A		Thrun, S; Saul, K; Scholkopf, B		Strohmann, TR; Belitski, A			Sparse greedy Minimax Probability Machine Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The Minimax Probability Machine Classification (MPMC) framework [Lanckriet et al., 2002] builds classifiers by minimizing the maximum probability of misclassification, and gives direct estimates of the probabilistic accuracy bound Omega. The only assumptions that MPMC makes is that good estimates of means and covariance matrices of the classes exist. However, as with Support Vector Machines, MPMC is computationally expensive and requires extensive cross validation experiments to choose kernels and kernel parameters that give good performance. In this paper we address the computational cost of MPMC by proposing an algorithm that constructs nonlinear sparse MPMC (SMPMC) models by incrementally adding basis functions (i.e. kernels) one at a time - greedily selecting the next one that maximizes the accuracy bound Omega. SMPMC automatically chooses both kernel parameters and feature weights without using computationally expensive cross validation. Therefore the SMPMC algorithm simultaneously addresses the problem of kernel selection and feature selection (i.e. feature weighting), based solely on maximizing the accuracy bound Omega. Experimental results indicate that we can obtain reliable bounds Omega, as well as test set accuracies that are comparable to state of the art classification algorithms.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Strohmann, TR (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.	strohman@cs.colorado.edu; Andrei.Belitski@colorado.edu						[Anonymous], 2002, LEARNING KERNELS; Beyer W. H., 1987, CRC STANDARD MATH TA, P12; CRAMMER K, 2003, ADV NEURAL INFORMATI, V15; Lanckriet G. R. G., 2003, Journal of Machine Learning Research, V3, P555, DOI 10.1162/153244303321897726; LANCKRIET GRG, 2002, ADV NEURAL INFORMATI, V14; MARSHALL AW, 1960, ANN MATH STAT, V31, P1001, DOI 10.1214/aoms/1177705673; POPESCU I, 2001, TM62 INSEAD DEP MATH	7	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						105	112						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500014
C	Sugita, Y; Tani, J		Thrun, S; Saul, K; Scholkopf, B		Sugita, Y; Tani, J			A holistic approach to compositional semantics: A connectionist model and robot experiments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We present a novel connectionist model for acquiring the semantics of a simple language through the behavioral experiences of a real robot. We focus on the "compositionality" of semantics, a fundamental characteristic of human language, which is the ability to understand the meaning of a sentence as a combination of the meanings of words. We also pay much attention to the "embodiment" of a robot, which means that the robot should acquire semantics which matches its body, or sensory-motor system. The essential claim is that an embodied compositional semantic representation can be self-organized from generalized correspondences between sentences and behavioral patterns. This claim is examined and confirmed through simple experiments in which a robot generates corresponding behaviors from unlearned sentences by analogy with the correspondences between learned sentences and behaviors.	RIKEN, BSI, Wako, Saitama 3510198, Japan	RIKEN	Sugita, Y (corresponding author), RIKEN, BSI, Hirosawa 2-1, Wako, Saitama 3510198, Japan.	sugita@bdc.brain.riken.go.jp; tani@bdc.brain.riken.go.jp	Tani, Jun/AAI-8287-2020					ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Evans G., 1981, WITTGENSTEIN FOLLOW; FODOR J, 1999, 46 RUTG U; HADLEY RF, 1994, MIND LANG, V9, P431, DOI 10.1111/j.1468-0017.1994.tb00225.x; ITO M, 2003, 3 RIKEN BRAIN SCI I; IWAHASHI N, 2003, J JAPANESE SOC ARTIF, V18, P49; JORDAN MI, 1992, COGNITIVE SCI, V16, P307, DOI 10.1207/s15516709cog1603_1; Miikkulainen R, 1993, SUBSYMBOLIC NATURAL; Roy Deb K., 2002, COMPUTER SPEECH LANG, V16; Rumelhart D.E., 1986, PARALLEL DISTRIBUTED, V1, P318; Siskind JM, 2001, J ARTIF INTELL RES, V15, P31, DOI 10.1613/jair.790; Steels L, 2000, FRONT ARTIF INTEL AP, V54, P764; Tani J, 1996, IEEE T SYST MAN CY B, V26, P421, DOI 10.1109/3477.499793; Tani J, 2003, NEURAL NETWORKS, V16, P11, DOI 10.1016/S0893-6080(02)00214-9	16	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						969	976						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500121
C	Tanaka, SC; Doya, K; Okada, G; Ueda, K; Okamoto, Y; Yamawaki, S		Thrun, S; Saul, K; Scholkopf, B		Tanaka, SC; Doya, K; Okada, G; Ueda, K; Okamoto, Y; Yamawaki, S			Different cortico-basal ganglia loops specialize in reward prediction at different time scales	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				MEDIAL PREFRONTAL CORTEX; ORBITOFRONTAL CORTEX; NEURAL RESPONSES; PROBABILISTIC REINFORCEMENT; DECISION-MAKING; HUMAN BRAIN; ANTICIPATION; SENSITIVITY; RECEPTORS	To understand the brain mechanisms involved in reward prediction at different time scales, we developed a Markov decision task that requires prediction of both immediate and future rewards, and analyzed subjects' brain activities using fMRI. We estimated the time course of reward prediction and reward prediction error at different time scales from subjects' performance data, and used them as the explanatory variables for multiple regression analysis with fMRI data. We found topographic maps of different time scales in medial frontal cortex and striatum. The result suggests that different cortico-basal ganglia loops are specialized for reward prediction at different time scales.	Japan Sci & Technol Agcy, Nara Inst Sci & Technol, ATR Computat Neurosci Labs, CREST, Kyoto, Japan	Japan Science & Technology Agency (JST); Nara Institute of Science & Technology	Tanaka, SC (corresponding author), Japan Sci & Technol Agcy, Nara Inst Sci & Technol, ATR Computat Neurosci Labs, CREST, Kyoto, Japan.							Bechara A, 2000, CEREB CORTEX, V10, P295, DOI 10.1093/cercor/10.3.295; Berns GS, 2001, J NEUROSCI, V21, P2793, DOI 10.1523/JNEUROSCI.21-08-02793.2001; Breiter HC, 2001, NEURON, V30, P619, DOI 10.1016/S0896-6273(01)00303-8; Celada P, 2001, J NEUROSCI, V21, P9917, DOI 10.1523/JNEUROSCI.21-24-09917.2001; Doya K, 2000, CURR OPIN NEUROBIOL, V10, P732, DOI 10.1016/S0959-4388(00)00153-7; Doya K, 2002, NEURAL NETWORKS, V15, P495, DOI 10.1016/S0893-6080(02)00044-8; Elliott R, 2000, CEREB CORTEX, V10, P308, DOI 10.1093/cercor/10.3.308; Elliott R, 2000, J NEUROSCI, V20, P6159, DOI 10.1523/JNEUROSCI.20-16-06159.2000; Evenden JL, 1996, PSYCHOPHARMACOLOGY, V128, P161, DOI 10.1007/s002130050121; HABER SN, 1995, J NEUROSCI, V15, P4851; Houk JC, 1995, MODELS INFORM PROCES, P249; Knutson B, 2001, J NEUROSCI, V21, part. no., DOI 10.1523/JNEUROSCI.21-16-j0002.2001; Koepp MJ, 1998, NATURE, V393, P266, DOI 10.1038/30498; Martin-Ruiz R, 2001, J NEUROSCI, V21, P9856, DOI 10.1523/JNEUROSCI.21-24-09856.2001; Mobini S, 2000, PSYCHOPHARMACOLOGY, V152, P390, DOI 10.1007/s002130000542; Mobini S, 2002, PSYCHOPHARMACOLOGY, V160, P290, DOI 10.1007/s00213-001-0983-0; O'Doherty J, 2003, J NEUROSCI, V23, P7931; O'Doherty JP, 2002, NEURON, V33, P815, DOI 10.1016/S0896-6273(02)00603-7; O'Doherty JP, 2003, NEURON, V38, P329, DOI 10.1016/S0896-6273(03)00169-7; Pagnoni G, 2002, NAT NEUROSCI, V5, P97, DOI 10.1038/nn802; Rogers RD, 1999, NEUROPSYCHOPHARMACOL, V20, P322, DOI 10.1016/S0893-133X(98)00091-8; Rogers RD, 1999, J NEUROSCI, V19, P9029, DOI 10.1523/JNEUROSCI.19-20-09029.1999; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TANAKA S, 2002, 8 INT C FUNCT MAPP H	25	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						701	708						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500088
C	Tsang, EKC; Shi, BE		Thrun, S; Saul, K; Scholkopf, B		Tsang, EKC; Shi, BE			A neuromorphic multi-chip model of a disparity selective complex cell	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				BINOCULAR DISPARITY; NEURAL MECHANISMS; VISUAL-CORTEX; DEPTH DISCRIMINATION; RESPONSES; NEURONS; POSITION; PHASE	The relative depth of objects causes small shifts in the left and right retinal positions of these objects, called binocular disparity. Here, we describe a neuromorphic implementation of a disparity selective complex cell using the binocular energy model, which has been proposed to model the response of disparity selective cells in the visual cortex. Our system consists of two silicon chips containing spiking neurons with monocular Gabor-type spatial receptive fields (RF) and circuits that combine the spike outputs to compute a disparity selective complex cell response. The disparity selectivity of the cell can be adjusted by both position and phase shifts between the monocular RF profiles, which are both used in biology. Our neuromorphic system performs better with phase encoding, because the relative responses of neurons tuned to different disparities by phase shifts are better matched than the responses of neurons tuned by position shifts.	Hong Kong Univ Sci & Technol, Dept Elect & Elect Engn, Kowloon, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Tsang, EKC (corresponding author), Hong Kong Univ Sci & Technol, Dept Elect & Elect Engn, Kowloon, Hong Kong, Peoples R China.							ALBRECHT DG, 1991, VISUAL NEUROSCI, V7, P531, DOI 10.1017/S0952523800010336; Anzai A, 1999, J NEUROPHYSIOL, V82, P891, DOI 10.1152/jn.1999.82.2.891; Anzai A, 1999, J NEUROPHYSIOL, V82, P874, DOI 10.1152/jn.1999.82.2.874; Anzai A, 1999, J NEUROPHYSIOL, V82, P909, DOI 10.1152/jn.1999.82.2.909; BARLOW HB, 1967, J PHYSIOL-LONDON, V193, P327, DOI 10.1113/jphysiol.1967.sp008360; Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110; CHOI TYW, 2003, P IEEE INT S CIRC SY, V4, P800; Cumming BG, 1997, NATURE, V389, P280, DOI 10.1038/38487; Fleet DJ, 1996, VISION RES, V36, P1839, DOI 10.1016/0042-6989(95)00313-4; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; OHZAWA I, 1990, SCIENCE, V249, P1037, DOI 10.1126/science.2396096; POGGIO GF, 1985, VISION RES, V25, P397, DOI 10.1016/0042-6989(85)90065-3; Qian N, 1997, VISION RES, V37, P1811, DOI 10.1016/S0042-6989(96)00331-8	13	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1051	1058						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500131
C	Tsuda, K; Ratsch, G		Thrun, S; Saul, K; Scholkopf, B		Tsuda, K; Ratsch, G			Image reconstruction by linear programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					A common way of image denoising is to project a noisy image to the subspace of admissible images made for instance by PCA. However, a major drawback of this method is that all pixels are updated by the projection, even when only a few pixels are corrupted by noise or occlusion. We propose a new method to identify the noisy pixels by l(1)-norm penalization and update the identified pixels only. The, identification and updating of noisy pixels are formulated as one-linear program which can be solved efficiently. Especially, one can apply the v-trick to directly specify the fraction of pixels to be reconstructed. Moreover, we extend the linear program to be able to exploit prior knowledge that occlusions often appear in contiguous blocks (e.g. sunglasses on faces). The basic idea is to penalize boundary points and interior points of the occluded area differently. We are able to show the v-property also for this extended LP leading a method which is easy to use. Experimental results impressively demonstrate the power of our approach.	Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany; AIST CBRC, Koto Ku, 2-43 Aomi, Tokyo 1350064, Japan; Fraunhofer FIRST, Kekulestr 7, D-12489 Berlin, Germany	Max Planck Society; National Institute of Advanced Industrial Science & Technology (AIST); Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Tsuda, K (corresponding author), Max Planck Inst Biol Cybernet, Spemannstr 38, D-72076 Tubingen, Germany.	koji.tsuda@tuebingen.mpg.de; gunnar.raetsch@tuebingen.mpg.de		Ratsch, Gunnar/0000-0001-5486-8532				Ben Hamza A, 2001, IEEE T SIGNAL PROCES, V49, P3045, DOI 10.1109/78.969512; Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; Graf ABA, 2002, LECT NOTES COMPUT SC, V2525, P491; MANGASARIAN O, 1995, 9520 U WISC COMP SCI; Mika S, 1999, ADV NEUR IN, V11, P536; Ratsch G, 2000, ADV NEUR IN, P207; TAKAHASHI T, 2002, LNCS, V2415, P727	9	1	1	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						57	64						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500008
C	Vollgraf, R; Scholz, M; Meinertzhagen, IA; Obermayerl, K		Thrun, S; Saul, K; Scholkopf, B		Vollgraf, R; Scholz, M; Meinertzhagen, IA; Obermayerl, K			Nonlinear filtering of electron micrographs by means of support vector regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Nonlinear filtering can solve very complex problems, but typically involve very time consuming calculations. Here we show that for filters that are constructed as a RBF network with Gaussian basis functions, a decomposition into linear filters exists, which can be computed efficiently in the frequency domain, yielding dramatic improvement in speed. We present an application of this idea to image processing. In electron micrograph images of photoreceptor terminals of the fruit fly, Drosophila, synaptic vesicles containing neurotransmitter should be detected and labeled automatically. We use hand labels, provided by human experts, to learn a RBF filter using Support Vector Regression with Gaussian kernels. We will show that the resulting nonlinear filter solves the task to a degree of accuracy, which is close to what can be achieved by human experts. This allows the very time consuming task of data evaluation to be done efficiently.	Tech Univ Berlin, Dept Elect Engn & Comp Engn, Berlin, Germany	Technical University of Berlin	Vollgraf, R (corresponding author), Tech Univ Berlin, Dept Elect Engn & Comp Engn, Berlin, Germany.							[Anonymous], 2002, LEARNING KERNELS; [Anonymous], 1998, NEURAL NETWORKS COMP; CHANG C.C., 2003, LIBSVM LIB SUPPORT V; FABIANFINE R, 2003, IN PRESS J NEUROSCI; HARVEY LO, 1992, ORGAN BEHAV HUM DEC, V53, P229, DOI 10.1016/0749-5978(92)90063-D; PRESS WH, 1992, RECIPES C; Stowers RS, 1999, GENETICS, V152, P1631; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	10	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						717	724						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500090
C	Welling, M; Agakov, F; Williams, CKI		Thrun, S; Saul, K; Scholkopf, B		Welling, M; Agakov, F; Williams, CKI			Extreme components analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Principal components analysis (PCA) is one of the most widely used techniques in machine learning and data mining. Minor components analysis (MCA) is less well known, but can also play an important role in the presence of constraints on the data distribution. In this paper we present a probabilistic model for "extreme components analysis" (XCA) which at the maximum likelihood solution extracts an optimal combination of principal and minor components. For a given number of components, the log-likelihood of the XCA model is guaranteed to be larger or equal than that of the probabilistic models for PCA and MCA. We describe an efficient algorithm to solve for the globally optimal solution. For log-convex spectra we prove that the solution consists of principal components only, while for log-concave spectra the solution consists of minor components. In general, the solution admits a combination of both. In experiments we explore the properties of XCA on some synthetic and real-world datasets.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G5, Canada	University of Toronto	Welling, M (corresponding author), Univ Toronto, Dept Comp Sci, 10 Kings Coll Rd, Toronto, ON M5S 3G5, Canada.	welling@cs.toronto.edu; ckiw@inf.ed.ac.uk; felixa@inf.ed.ac.uk		Agakov, Felix/0000-0003-4280-9062				Hinton GE, 1999, IEE CONF PUBL, P1, DOI 10.1049/cp:19991075; Proakis J.G., 1996, DIGIT SIGNAL PROCESS; Roweis S, 1998, ADV NEUR IN, V10, P626; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; WELLING M, 2003, IN PRESS P C UNC ART; Williams CKI, 2002, NEURAL COMPUT, V14, P1169, DOI 10.1162/089976602753633439; Zhu H., 1998, NEURAL NETWORKS MACH	7	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						137	144						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500018
C	Welling, M; Teh, YW		Thrun, S; Saul, K; Scholkopf, B		Welling, M; Teh, YW			Linear response for approximate inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				BOLTZMANN MACHINES; ALGORITHMS	Belief propagation on cyclic graphs is an efficient algorithm for computing approximate marginal probability distributions over single nodes and neighboring nodes in the graph. In this paper we propose two new algorithms for approximating joint probabilities of arbitrary pairs of nodes and prove a number of desirable properties that these estimates fulfill. The first algorithm is a propagation algorithm which is shown to converge if belief propagation converges to a stable fixed point. The second algorithm is based on matrix inversion. Experiments compare a number of competing methods.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada	University of Toronto	Welling, M (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.		Teh, Yee Whye/C-3400-2008					HESKES T, 2003, ADV NEURAL INFORMATI, V15; Kappen HJ, 1998, NEURAL COMPUT, V10, P1137, DOI 10.1162/089976698300017386; Kschischang FR, 2001, IEEE T INFORM THEORY, V47, P498, DOI 10.1109/18.910572; Opper M, 2001, ADV MEAN FIELD METHO; Tanaka K, 2003, IEICE T INF SYST, VE86D, P1228; TEH YW, 2001, ADV NEURAL INFORMATI; WAINWRIGHT MJ, 2003, UCBCDS31226; Welling M, 2003, ARTIF INTELL, V143, P19, DOI 10.1016/S0004-3702(02)00361-2; Welling M, 2004, NEURAL COMPUT, V16, P197, DOI 10.1162/08997660460734056; YEDIDIA JS, 2000, ADV NEURAL INFORMATI, V13; Yuille AL, 2002, NEURAL COMPUT, V14, P1691, DOI 10.1162/08997660260028674	11	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						361	368						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500046
C	Brown, AD; Hinton, GE		Dietterich, TG; Becker, S; Ghahramani, Z		Brown, AD; Hinton, GE			Relative density nets: A new way to combine backpropagation with HMM's	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Logistic units in the first hidden layer of a feedforward neural network compute the relative probability of a data point under two Gaussians. This leads us to consider substituting other density models. We present an architecture for performing discriminative learning of Hidden Markov Models using a network of many small HMM's. Experiments on speech data show it to be superior to the standard method of discriminatively training HMM's.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada	University of Toronto	Brown, AD (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3G4, Canada.	andy@cs.utoronto.ca; hinton@gatsby.ucl.ac.uk						BAHL LR, 1986, P IEEE INT C AC SPEE, P49; BAUM LE, 1970, ANN MATH STAT, V41, P164, DOI 10.1214/aoms/1177697196; Bridle J. S., 1990, PROC 2 INT C NEURAL, P211, DOI [10.5555/2969830, DOI 10.5555/2969830]; Brown A., 2001, P ART INT STAT 2001, P3; Jordan M., 1995, 9503 MIT; Rasmussen C.E., 1996, THESIS U TORONTO	6	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1149	1156						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100143
C	Cemgil, AT; Kappen, B		Dietterich, TG; Becker, S; Ghahramani, Z		Cemgil, AT; Kappen, B			Tempo tracking and rhythm quantization by sequential Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We present a probabilistic generative model for timing deviations in expressive music performance. The structure of the proposed model is equivalent to a switching state space model. We formulate two well known music recognition problems, namely tempo tracking and automatic transcription (rhythm quantization) as filtering and maximum a posteriori (MAP) state estimation tasks. The inferences are carried out using sequential Monte Carlo integration (particle filtering) techniques. For this purpose, we have derived a novel Viterbi algorithm for Rao-Blackwellized particle filters, where a subset of the hidden variables is integrated out. The resulting model is suitable for realtime tempo tracking and transcription and hence useful in a number of music applications such as adaptive automatic accompaniment and score typesetting.	Univ Nijmegen, SNN, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Cemgil, AT (corresponding author), Univ Nijmegen, SNN, NL-6525 EZ Nijmegen, Netherlands.		Cemgil, Ali Taylan/A-3068-2016	Cemgil, Ali Taylan/0000-0003-4463-8455				Cemgil AT, 2000, COMPUT MUSIC J, V24, P60, DOI 10.1162/014892600559218; CEMGIL AT, IN PRESS J NEW MUSIC; CHEN R, 2000, J R STAT SOC, V10; Doucet A, 2000, STAT COMPUT, V10, P197, DOI 10.1023/A:1008935410038; DOUCHET A, 2000, UNCERTAINTY ARTIFICI; DOUCHET A, 2000, SEQUENTIAL MONTE CAR; GODSILL S, 2000, ANN I STAT MATH; Murphy K.P., 1998, SWITCHING KALMAN FIL; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; RAPHAEL C, 1999, IN PRESS J COMPUTATI; RAPHAEL C, 2001, P UNCERTAINTY AI; THOM B, 2000, P AAAI2000; Vercoe BL, 1998, P IEEE, V86, P922, DOI 10.1109/5.664280	13	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1361	1368						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100169
C	Franklin, JA		Dietterich, TG; Becker, S; Ghahramani, Z		Franklin, JA			Improvisation and learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				REINFORCEMENT	This article presents a 2-phase computational learning model and application. As a demonstration, a system has been built, called CHIME for Computer Human Interacting Musical Entity. In phase I of training, recurrent back-propagation trains the machine to reproduce 3 jazz melodies. The recurrent network is expanded and is further trained in phase 2 with a reinforcement learning algorithm and a critique produced by a set of basic rules for jazz improvisation. After each phase CHIME can interactively improvise with a human in real time.	Smith Coll, Dept Comp Sci, Northampton, MA 01063 USA	Smith College	Franklin, JA (corresponding author), Smith Coll, Dept Comp Sci, Northampton, MA 01063 USA.							AEBERSOLD J, 1976, YOU CAN PLAY SONNY R, V8; BARTO AG, 1983, IEEE T SYST MAN CYB, V13, P834, DOI 10.1109/TSMC.1983.6313077; Benbrahim H, 1997, ROBOT AUTON SYST, V22, P283, DOI 10.1016/S0921-8890(97)00043-2; Griffith N., 1999, MUSICAL NETWORKS PAR; Gullapalli V., 1994, IEEE CONTROL SYSTEMS; Jordan M.I., 1986, P 8 ANN C COGN SCI S; MESSICK P, 1988, MAXIMUM MIDI; Reeves Scott D., 1995, CREATIVE JAZZ IMPROV; SABATELLA M, 1996, WHOLE APPROACH JAZZ; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Todd PM, 1991, MUSIC CONNECTIONISM; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	12	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1377	1384						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100171
C	Frey, BJ; Jojic, N		Dietterich, TG; Becker, S; Ghahramani, Z		Frey, BJ; Jojic, N			Fast, large-scale transformation-invariant clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In previous work on "transformed mixtures of Gaussians" and "transformed hidden Markov models", we showed how the EM algorithm in a discrete latent variable model can be used to jointly normalize data (e.g., center images, pitch-normalize spectrograms) and learn a mixture model of the normalized data. The only input to the algorithm is the data, a list of possible transformations, and the number of clusters to find. The main criticism of this work was that the exhaustive computation of the posterior probabilities over transformations would make scaling up to large feature vectors and large sets of transformations intractable. Here, we describe how a tremendous speed-up is acheived through the use of a variational technique for decoupling transformations, and a fast Fourier transform method for computing posterior probabilities. For N x N images, learning C clusters under N rotations, N scales, N x-translations and N y-translations takes only (C + 2 log N)N-2 scalar operations per iteration. In contrast, the original algorithm takes CN6 operations to account for these transformations. We give results on learning a 4-component mixture model from a video sequence with frames of size 320 x 240. The model accounts for 360 rotations and 76,800 translations. Each iteration of EM takes only 10 seconds per frame in MATLAB, which is over 5 million times faster than the original algorithm.	Univ Toronto, Machine Learning Grp, Toronto, ON, Canada	University of Toronto	Frey, BJ (corresponding author), Univ Toronto, Machine Learning Grp, Toronto, ON, Canada.							DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; FREY BJ, 2001, IN PRESS IEEE T PATT; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; JOJIC N, 2000, P IEEE C COMP VIS PA; JORDAN M, 1998, LEARNING GRAPHICAL M; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; SIMARD P, 1993, ADV NEURAL INFORMATI, V5; VASCONCELOS N, 1998, ADV NEURAL INFORMATI, V10; WOLBERG G, 2000, P IEEE INT C IM PROC	10	1	1	0	1	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						721	727						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100090
C	Griffiths, TL; Tenenbaum, JB		Dietterich, TG; Becker, S; Ghahramani, Z		Griffiths, TL; Tenenbaum, JB			Using vocabulary knowledge in Bayesian multinomial estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Estimating the parameters of sparse multinomial distributions is an important component of many statistical learning tasks. Recent approaches have used uncertainty over the vocabulary of symbols in a multinomial distribution as a means of accounting for sparsity. We present a Bayesian approach that allows weak prior knowledge, in the form of a small set of approximate candidate vocabularies, to be used to dramatically improve the resulting estimates. We demonstrate these improvements in applications to text compression and estimating distributions over words in newsgroup data.	Stanford Univ, Dept Psychol, Stanford, CA 94305 USA	Stanford University	Griffiths, TL (corresponding author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.							Box G.E.P., 2011, BAYESIAN INFERENCE S; CHEN SF, 1998, TR1098 HARV U CTR RE; FRIEDMAN N, 1998, NEURAL INFORMATION P; JEFFREYS H, 1946, PROC R SOC LON SER-A, V186, P453, DOI 10.1098/rspa.1946.0056; Laplace P.S., 1995, PHILOS ESSAY PROBABI; LIDSTONE GJ, 1920, T FACULTY ACTUARIES, V8, P182; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; PERKS FJA, 1947, J I ACTUARIES, V73, P285; RISTAD ES, 1995, CSTR89595 PRINC U DE	10	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1385	1392						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100172
C	Grudic, GZ; Ungar, LH		Dietterich, TG; Becker, S; Ghahramani, Z		Grudic, GZ; Ungar, LH			Rates of convergence of performance gradient estimates using function approximation and bias in reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We address two open theoretical questions in Policy Gradient Reinforcement Learning. The first concerns the efficacy of using function approximation to represent the state action value function, Q. Theory is presented showing that linear function approximation representations of Q can degrade the rate of convergence of performance gradient estimates by a factor of O(ML) relative to when no function approximation of Q is used, where M is the number of possible actions and L is the number of basis functions in the function approximation representation. The second concerns the use of a bias term in estimating the state action value function. Theory is presented showing that a non-zero bias term can improve the rate of convergence of performance gradient estimates by O(1 - (1/M)), where M is the number of possible actions. Experimental evidence is presented showing that these theoretical results lead to significant improvement in the convergence properties of Policy Gradient Reinforcement Learning algorithms.	Univ Colorado, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Grudic, GZ (corresponding author), Univ Colorado, Boulder, CO 80309 USA.							Bartlett P. L, 2000, P 17 INT C MACH LEAR, P41; GRUDIC GZ, 2000, P 17 INT C MACH LEAR, V17, P343; GRUDIC GZ, 2000, P 17 NAT C ART INT M, V17, P590; KONDA VR, 2000, ADV NEURAL INFORMATI, V12; SUTTON R, 2000, ADV NEURAL INFORMATI, V12; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	6	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1515	1522						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100188
C	Haffner, P		Dietterich, TG; Becker, S; Ghahramani, Z		Haffner, P			Escaping the extrapolated convex hull with vector machines.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Maximum margin classifiers such as Support Vector Machines (SVMs) critically depends upon the convex hulls of the training samples of each class, as they implicitly search for the minimum distance between the convex hulls. We propose Extrapolated Vector Machines (XVMs) which rely on extrapolations outside these convex hulls. XVMs improve SVM generalization very significantly on the MNIST [7] OCR data. They share similarities with the Fisher discriminant: maximize the inter-class margin while minimizing the intra-class disparity.	AT&T Labs Res, Middletown, NJ 07748 USA	AT&T	Haffner, P (corresponding author), AT&T Labs Res, 200 Laurel Ave, Middletown, NJ 07748 USA.							Collobert R., 2000, IDIAPRR0017; Cortes C., 1995, MACH LEARN, P273, DOI [10.1023/A:1022627411411, DOI 10.1007/BF00994018]; CRISP DJ, 2000, ADV NEURAL INFORMATI, V12; DECOSTE D, 2001, MACHINE LEARNING SPE; Keerthi SS, 2000, IEEE T NEURAL NETWOR, V11, P124, DOI 10.1109/72.822516; KOWALCZYK A, 2000, ADV LARGE MARGIN CLA; LeCun Y, 1998, P IEEE, V86; PLATT J, 2000, ADV NEURAL INFORMATI, V12; Vapnik V.N, 1998, STAT LEARNING THEORY	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						753	760						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100094
C	Herbrich, R; Williamson, RC		Dietterich, TG; Becker, S; Ghahramani, Z		Herbrich, R; Williamson, RC			Algorithmic luckiness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In contrast to standard statistical learning theory which studies uniform bounds on the expected error we present a framework that exploits the specific learning algorithm used. Motivated by the luckiness framework [8] we are also able to exploit the serendipity of the training sample. The main difference to previous approaches lies in the complexity measure; rather than covering all hypotheses in a given hypothesis space it is only necessary to cover the functions which could have been learned using the fixed learning algorithm. We show how the resulting framework relates to the VC, luckiness and compression frameworks. Finally, we present an application of this framework to the maximum margin algorithm for linear classifiers which results in a bound that exploits both the margin and the distribution of the data in feature space.	Microsoft Res Ltd, Cambridge CB3 0FB, England	Microsoft	Herbrich, R (corresponding author), Microsoft Res Ltd, Cambridge CB3 0FB, England.							ANTHONY M, 1999, THEORY LEARNING ARTI; Bousquet O, 2001, ADV NEUR IN, V13, P196; CRISTIANINI N, 2001, NC2TR2001087; Herbrich R, 2001, ADV NEUR IN, V13, P224; HERBRICH R, 2002, ALGORITHMIC LUCKINES; LITTLESTONE N, 1986, RELATING DATA COMPRE; Makovoz Y, 1996, J APPROX THEORY, V85, P98, DOI 10.1006/jath.1996.0031; Vapnik V.N, 1998, STAT LEARNING THEORY; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025	10	1	1	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						391	397						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100049
C	Khardon, R; Roth, D; Servedio, R		Dietterich, TG; Becker, S; Ghahramani, Z		Khardon, R; Roth, D; Servedio, R			Efficiency versus convergence of Boolean kernels for on-line learning algorithms	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We study online learning in Boolean domains using kernels which capture feature expansions equivalent to using conjunctions over basic features. We demonstrate a tradeoff between the computational efficiency with which these kernels can be computed and the generalization ability of the resulting classifier. We first describe several kernel functions which capture either limited forms of conjunctions or all conjunctions. We show that these kernels can be used to efficiently run the Perceptron algorithm over an exponential number of conjunctions; however we also prove that using such kernels the Perceptron algorithm can make an exponential number of mistakes even when learning simple functions. We also consider an analogous use of kernel functions to run the multiplicative-update Winnow algorithm over an expanded feature space of exponentially many conjunctions. While known upper bounds imply that Winnow can learn DNF formulae with a polynomial mistake bound in this setting, we prove that it is computationally hard to simulate Winnow's behavior for learning DNF over such a feature set, and thus that such kernel functions for Winnow are not efficiently computable.	Tufts Univ, Medford, MA 02155 USA	Tufts University	Khardon, R (corresponding author), Tufts Univ, Medford, MA 02155 USA.	roni@eecs.tufts.edu; danr@cs.uiuc.edu; rocco@deas.harvard.edu						ANGLUIN D, 1990, MACH LEARN, V5, P121, DOI 10.1007/BF00116034; Carlson A, 1999, UIUCDCSR992101; Cristianini N., 2000, INTRO SUPPORT VECTOR; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; MAASS W, 1998, INFORMATION COMPUTAT, V141, P378; Novikoff, 1962, P S MATH THEOR AUT, P615; Roth D, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P806; SADOHARA K, 2001, LECT NOTES ARTIF INT, V2225, P106; VALIANT LG, 1979, SIAM J COMPUT, V8, P410, DOI 10.1137/0208032; WATKINS C, 1999, CSDTR9807 U LOND COM	10	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						423	430						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100053
C	Lawrence, ND; Rowstron, AIT; Bishop, CM; Taylor, MJ		Dietterich, TG; Becker, S; Ghahramani, Z		Lawrence, ND; Rowstron, AIT; Bishop, CM; Taylor, MJ			Optimising synchronisation times for mobile devices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					With the increasing number of users of mobile computing devices (e.g. personal digital assistants) and the advent of third generation mobile phones, wireless communications are becoming increasingly important. Many applications rely on the device maintaining a replica of a data-structure which is stored on a server, for example news databases, calendars and e-mail. In this paper we explore the question of the optimal strategy for synchronising such replicas. We utilise probabilistic models to represent how the data-structures evolve and to model user behaviour. We then formulate objective functions which can be minimised with respect to the synchronisation timings. We demonstrate, using two real world data-sets, that a user can obtain more up-to-date information using our approach.	Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England	University of Sheffield	Lawrence, ND (corresponding author), Univ Sheffield, Dept Comp Sci, Regent Court,211 Portobello Rd, Sheffield S1 4DP, S Yorkshire, England.			Lawrence, Neil/0000-0001-9258-1030				CHO J, 2000, P 2000 ACM INT C MAN; Kelly FP, 2000, PHILOS T R SOC A, V358, P2335, DOI 10.1098/rsta.2000.0651; Mardia K., 1972, STAT DIRECTIONAL DAT, V37, DOI 10.2307/2984782; Rowstron A., 2001, P 8 WORKSH HOT TOP O; Wills CE, 1999, PROCEEDINGS OF THE EIGHTH INTERNATIONAL WORLD WIDE WEB CONFERENCE, P153; WOLMAN A, 1999, 17 ACM S OP SYST PRI, P16; YU H, 2000, 4 S OP SYST DES IMPL	7	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1401	1408						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100174
C	Leisink, MAR; Kappen, HJ		Dietterich, TG; Becker, S; Ghahramani, Z		Leisink, MAR; Kappen, HJ			Means, correlations and bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The partition function for a Boltzmann machine can be bounded from above and below. We can use this to bound the means and the correlations. For networks with small weights, the values of these statistics can be restricted to non-trivial regions (i.e. a subset of [-1, 1]). Experimental results show that reasonable bounding occurs for weight sizes where mean field expansions generally give good results.	Univ Nijmegen, Dept Biophys, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Leisink, MAR (corresponding author), Univ Nijmegen, Dept Biophys, Geert Grootepl 21, NL-6525 EZ Nijmegen, Netherlands.		Kappen, H.J./L-4425-2015					ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; JAAKKOLA J, 1996, 9604 MIT COMP COGN; Jaakkola T., 1996, P 12 ANN C UNC ART I, P340; MARTIJN AR, 2001, ADV NEURAL INFORMATI, V13, P266; MARTIJN AR, 2001, IN PRESS NEURAL COMP, V13; Peterson C., 1987, Complex Systems, V1, P995; PLEFKA T, 1982, J PHYS A-MATH GEN, V15, P1971, DOI 10.1088/0305-4470/15/6/035; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251; SHERRINGTON D, 1975, PHYS REV LETT, V35, P1793	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						455	462						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100057
C	Matsumoto, N; Okada, M		Dietterich, TG; Becker, S; Ghahramani, Z		Matsumoto, N; Okada, M			Self-regulation mechanism of Temporally Asymmetric Hebbian plasticity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NEURAL-NETWORKS; MEMORY	Recent biological experimental findings have shown that the synaptic plasticity depends on the relative timing of the pre- and postsynaptic spikes which determines whether Long Term Potentiation (LTP) occurs or Long Term Depression (LTD) does. The synaptic plasticity has been called "Temporally Asymmetric Hebbian plasticity (TAH)". Many authors have numerically shown that spatio-temporal patterns can be stored in neural networks. However, the mathematical mechanism for storage of the spatio-temporal patterns is still unknown, especially the effects of LTD. In this paper, we employ a simple neural network model and show that interference of LTP and LTD disappears in a sparse coding scheme. On the other hand, it is known that the covariance learning is indispensable for storing sparse patterns. We also show that TAH qualitatively has the same effect as the covariance learning when spatio-temporal patterns are embedded in the network.	Saitama Univ, RIKEN, Brain Sci Inst, Grad Sch Sci & Engn, Urawa, Saitama 3510198, Japan	RIKEN; Saitama University	Matsumoto, N (corresponding author), Saitama Univ, RIKEN, Brain Sci Inst, Grad Sch Sci & Engn, Urawa, Saitama 3510198, Japan.	xmatumo@brain.riken.go.jp; okada@brain.riken.go.jp						Abbott LF, 1999, ADV NEUR IN, V11, P69; AMARI S, 1989, NEURAL NETWORKS, V2, P451, DOI 10.1016/0893-6080(89)90043-9; AMARI S, 1988, NEURAL NETWORKS, V1, P63, DOI 10.1016/0893-6080(88)90022-6; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Chechik G, 1999, ADV NEUR IN, V11, P97; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498; Kitano K, 1998, J PHYS A-MATH GEN, V31, pL613, DOI 10.1088/0305-4470/31/36/004; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; MIYASHITA Y, 1988, NATURE, V335, P817, DOI 10.1038/335817a0; Munro P, 2000, ADV NEUR IN, V12, P150; OKADA M, 1995, NEURAL NETWORKS, V8, P833, DOI 10.1016/0893-6080(95)00001-G; Okada M, 1996, NEURAL NETWORKS, V9, P1429, DOI 10.1016/S0893-6080(96)00044-5; Perez Vicente C. J., 1989, Journal of Physics A (Mathematical and General), V22, P559, DOI 10.1088/0305-4470/22/5/018; Rao RPN, 2000, ADV NEUR IN, V12, P164; Rubin J, 2001, PHYS REV LETT, V86, P364, DOI 10.1103/PhysRevLett.86.364; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; TSODYKS MV, 1988, EUROPHYS LETT, V6, P101, DOI 10.1209/0295-5075/6/2/002; YOSHIOKA M, 2001, IN PRESS PHYSICAL E; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	20	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						245	252						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100031
C	Nakahara, H; Amari, S		Dietterich, TG; Becker, S; Ghahramani, Z		Nakahara, H; Amari, S			Information-geometric decomposition in spike analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				MODULATION; DYNAMICS; PATTERNS; CORTEX	We present an information-geometric measure to systematically investigate neuronal firing patterns, taking account not only of the second-order but also of higher-order interactions. We begin with the case of two neurons for illustration and show how to test whether or not any pairwise correlation in one period is significantly different from that in the other period. In order to test such a hypothesis of different firing rates, the correlation term needs to be singled out 'orthogonally' to the firing rates, where the null hypothesis might not be of independent firing. This method is also shown to directly associate neural firing with behavior via their mutual information, which is decomposed into two types of information, conveyed by mean firing rate and coincident firing, respectively. Then, we show that these results, using the 'orthogonal' decomposition, are naturally extended to the case of three neurons and n neurons in general.	RIKEN, Brain Sci Inst, Lab Math Neurosci, Wako, Saitama 3510198, Japan	RIKEN	Nakahara, H (corresponding author), RIKEN, Brain Sci Inst, Lab Math Neurosci, 2-1 Hirosawa, Wako, Saitama 3510198, Japan.	hiro@brain.riken.go.jp; amari@brain.riken.go.jp						ABELES M, 1993, J NEUROPHYSIOL, V70, P1629, DOI 10.1152/jn.1993.70.4.1629; AERTSEN AMHJ, 1989, J NEUROPHYSIOL, V61, P900, DOI 10.1152/jn.1989.61.5.900; Amari S, 2001, IEEE T INFORM THEORY, V47, P1701, DOI 10.1109/18.930911; AMARI S, 2000, METHODS INFORMATION; GRUN S, 1996, VERLAG HARRI DTSCH R, V60; Ito H, 2000, NEURAL COMPUT, V12, P195, DOI 10.1162/089976600300015952; Martignon L, 2000, NEURAL COMPUT, V12, P2621, DOI 10.1162/089976600300014872; Nagaoka H., 1982, DIFFERENTIAL GEOMETR; Nakahara H., 2001, Society for Neuroscience Abstracts, V27, P2178; NAKAHARA H, UNPUB INFORMATION GE; Oram MW, 2001, J NEUROPHYSIOL, V86, P1700, DOI 10.1152/jn.2001.86.4.1700; Panzeri S, 2001, NEURAL COMPUT, V13, P1311, DOI 10.1162/08997660152002870; Riehle A, 1997, SCIENCE, V278, P1950, DOI 10.1126/science.278.5345.1950; VAADIA E, 1995, NATURE, V373, P515, DOI 10.1038/373515a0	14	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						253	260						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100032
C	Paskin, MA		Dietterich, TG; Becker, S; Ghahramani, Z		Paskin, MA			Grammatical bigrams	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Unsupervised learning algorithms have been derived for several statistical models of English grammar, but their computational complexity makes applying them to large data sets intractable. This paper presents a probabilistic model of English grammar that is much simpler than conventional models, but which admits an efficient EM training algorithm. The model is based upon grammatical bigrams, i.e., syntactic relationships between pairs of words. We present the results of experiments that quantify the representational adequacy of the grammatical bigram model, its ability to generalize from labelled data, and its ability to induce syntactic structure from large amounts of raw text.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Paskin, MA (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.							Collins M., 1999, THESIS U PENNSYLVANI; DELLAPIETRA S, 1994, LECTURE NOTES ARTIFI, V862, P78; EISNER J, 2000, ADV PROBABILISTIC OT, pCH1; EISNER JM, 1996, ICRS9611 U PENN CIS; Hudson R., 1990, ENGLISH WORD GRAMMAR; Lafferty J, 1992, P AAAI C PROB APPR N; Lari K., 1990, Computer Speech and Language, V4, P35, DOI 10.1016/0885-2308(90)90022-X; PASKIN MA, 2001, CSD011148 U CAL; REYNAR J. C., 1997, P 5 C APPL NAT LANG; Rosenfeld R, 1994, THESIS CARNEGIE MELL; [No title captured]; [No title captured]	13	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						91	97						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100012
C	Ratsch, G; Mika, S; Warmuth, MK		Dietterich, TG; Becker, S; Ghahramani, Z		Ratsch, G; Mika, S; Warmuth, MK			On the convergence of leveraging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				PREDICTION; DESCENT	We give an unified convergence analysis of ensemble learning methods including e.g. AdaBoost, Logistic Regression and the Least-Square-Boost algorithm for regression. These methods have in common that they iteratively call a base learning algorithm which returns hypotheses that are then linearly combined. We show that these methods are related to the Gauss-Southwell method known from numerical optimization and state non-asymptotical convergence results for all these methods. Our analysis includes l(1)-norm regularized cost functions leading to a clean and general way to regularize ensemble learning.	Australian Natl Univ, RSISE, Canberra, ACT 0200, Australia	Australian National University	Ratsch, G (corresponding author), Australian Natl Univ, RSISE, GPO Box 4, Canberra, ACT 0200, Australia.	raetsch@csl.anu.edu.au; mika@first.fhg.de; manfred@cse.ucsc.edu	Rätsch, Gunnar/O-5914-2017	Rätsch, Gunnar/0000-0001-5486-8532				Bauschke H. H., 1997, J CONVEX ANAL, V4, P27; BENNETT KP, 2000, P 17 INT C MACH LEAR, P65; Breiman L, 1999, NEURAL COMPUT, V11, P1493, DOI 10.1162/089976699300016106; CESABIANCHI N, 1994, IEEE T INFORM THEORY, V40, P1215, DOI 10.1109/18.335953; Collins M., 2000, P 13 ANN C COMP LEAR, P158; COPAS JB, 1983, J R STAT SOC B, V45, P311; DELLAPIETRA S, 2001, CMUCS01109 TR; Duffy N, 1999, LECT NOTES ARTIF INT, V1572, P18; Duffy N, 2000, ADV NEUR IN, V12, P258; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman J. H., 1999, GREEDY FUNCTION APPR; HOFFMAN AJ, 1952, J RES NAT BUR STAND, V49, P263, DOI 10.6028/jres.049.027; Kivinen J., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P134, DOI 10.1145/307400.307424; Luenberger D.G, 2016, LINEAR NONLINEAR PRO, DOI 10.1007/978-3-319-18842-3; LUO ZQ, 1992, J OPTIMIZ THEORY APP, V72, P7, DOI 10.1007/BF00939948; Mason L, 2000, ADV NEUR IN, P221; ONODA T, 1998, P INT C ART NEUR NET, P195; Ratsch G, 2002, MACH LEARN, V48, P189, DOI 10.1023/A:1013907905629; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; RATSCH G, 2001, 97 NEUROCOLT2; RATSCH G, 2001, THESIS U POTSDAM; RATSCH G, 2001, 98 ROYAL HOLL COLL; Rockafellar R. T., 1970, CONVEX ANAL; Singer Y, 2000, ADV NEUR IN, V12, P610; TIBSHIRANI R, 1994, REGRESSION SELECTION; ZHANG T, 2002, IN PRESS ADV NEURAL, V14	27	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						487	494						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100061
C	Sabatini, SP; Solari, F; Andreani, G; Bartolozzi, C; Bisio, GM		Dietterich, TG; Becker, S; Ghahramani, Z		Sabatini, SP; Solari, F; Andreani, G; Bartolozzi, C; Bisio, GM			A hierarchical model of complex cells in visual cortex for the binocular perception of motion-in-depth	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				DISPARITY COMPUTATION; PHASE; SPEED	A cortical model for motion-in-depth selectivity of complex cells in the visual cortex is proposed. The model is based on a time extension of the phase-based techniques for disparity estimation. We consider the computation of the total temporal derivative of the time-varying disparity through the combination of the responses of disparity energy units. To take into account the physiological plausibility, the model is based on the combinations of binocular cells characterized by different ocular dominance indices. The resulting cortical units of the model show a sharp selectivity for motion-in-depth that has been compared with that reported in the literature for real cortical cells.	Univ Genoa, Dept Biophys & Elect Engn, I-16145 Genoa, Italy	University of Genoa	Sabatini, SP (corresponding author), Univ Genoa, Dept Biophys & Elect Engn, Via Opera Pia 11A, I-16145 Genoa, Italy.		Sabatini, Silvio P/A-5500-2012; Solari, Fabio/O-4729-2016	Sabatini, Silvio P/0000-0002-0557-7306; Solari, Fabio/0000-0002-8111-0409				ADELSON EH, 1985, J OPT SOC AM A, V2, P284, DOI 10.1364/JOSAA.2.000284; CHEN Y, 2001, J NEUROPHYSIOL, P504; FLEET DJ, 1990, INT J COMPUT VISION, V5, P77, DOI 10.1007/BF00056772; FLEET DJ, 1991, CVGIP-IMAG UNDERSTAN, V53, P198, DOI 10.1016/1049-9660(91)90027-M; FLEET DJ, 1996, VISION RES, V17, P345; HARRIS JM, 1995, VISION RES, V35, P885, DOI 10.1016/0042-6989(94)00194-Q; Ohzawa I, 1997, J NEUROPHYSIOL, V77, P2879, DOI 10.1152/jn.1997.77.6.2879; Qian N, 2000, NEURAL COMPUT, V12, P279, DOI 10.1162/089976600300015781; SANGER TD, 1988, BIOL CYBERN, V59, P405, DOI 10.1007/BF00336114; SPILEERS W, 1990, J NEUROPHYSIOL, V63, P936, DOI 10.1152/jn.1990.63.4.936	10	1	1	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1271	1278						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100158
C	Shamir, M; Sompolinsky, H		Dietterich, TG; Becker, S; Ghahramani, Z		Shamir, M; Sompolinsky, H			Correlation codes in neuronal populations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				VARIABILITY; DISCHARGE; NOISE	Population codes often rely on the tuning of the mean responses to the stimulus parameters. However, this information can be greatly suppressed by long range correlations. Here we study the efficiency of coding information in the second order statistics of the population responses. We show that the Fisher Information of this system grows linearly with the size of the system. We propose a bilinear readout model for extracting information from correlation codes, and evaluate its performance in discrimination and estimation tasks. It is shown that the main source of information in this system is the stimulus dependence of the variances of the single neuron responses.	Hebrew Univ Jerusalem, Racah Inst Phys, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Shamir, M (corresponding author), Hebrew Univ Jerusalem, Racah Inst Phys, IL-91904 Jerusalem, Israel.		Sompolinsky, Haim/ABB-8398-2021; SHAMIR, MAOZ/F-1641-2012	SHAMIR, MAOZ/0000-0002-1489-6704				Abbott LF, 1999, NEURAL COMPUT, V11, P91, DOI 10.1162/089976699300016827; Lee D, 1998, J NEUROSCI, V18, P1161; Maynard EM, 1999, J NEUROSCI, V19, P8083; Salinas E, 1994, J Comput Neurosci, V1, P89, DOI 10.1007/BF00962720; SEUNG S, 1993, P NATL ACAD SCI USA, V90, P10794; Sompolinsky H, 2001, PHYS REV E, V64, DOI 10.1103/PhysRevE.64.051904; Yoon H, 1999, ADV NEUR IN, V11, P167; ZOHARY E, 1994, NATURE, V370, P140, DOI 10.1038/370140a0; [No title captured]	9	1	1	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						277	284						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100035
C	Shon, AP; Hsu, D; Diorio, C		Dietterich, TG; Becker, S; Ghahramani, Z		Shon, AP; Hsu, D; Diorio, C			Learning spike-based correlations and conditional probabilities in silicon	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We have designed and fabricated a VLSI synapse that can learn a conditional probability or correlation between spike-based inputs and feedback signals. The synapse is low power, compact, provides nonvolatile weight storage, and can perform simultaneous multiplication and adaptation. We can calibrate arrays of synapses to ensure uniform adaptation characteristics. Finally, adaptation in our synapse does not necessarily depend on the signals used for computation. Consequently, our synapse can implement learning rules that correlate past and present synaptic activity. We provide analysis and experimental chip results demonstrating the operation in learning and calibration mode, and show how to use our synapse to implement various learning rules in silicon.	Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Shon, AP (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	aaron@cs.washington.edu; hsud@cs.washington.edu; diorio@cs.washington.edu						ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Diorio C, 1997, IEEE T ELECTRON DEV, V44, P2281, DOI 10.1109/16.644652; Figueroa M, 2001, IEEE J SOLID-ST CIRC, V36, P816, DOI 10.1109/4.918920; HAFLIGER P, 1999, THESIS ETH ZURICH; Hasler P, 1999, ANALOG CIRCUITS SIG, V512, P33; LENZLINGER M, 1969, J APPL PHYS, V40, P278, DOI 10.1063/1.1657043; Levy W. B., 1989, COMPUTATIONAL MODELS, P243, DOI DOI 10.1016/S0079-7421(08)60113-9; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Pesavento A, 1999, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON MICROELECTRONICS FOR NEURAL, FUZZY AND BIO-INSPIRED SYSTEMS, MICORNEURO'99, P128, DOI 10.1109/MN.1999.758855; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Takeda E., 1995, HOT CARRIER EFFECTS	11	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1123	1130						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100140
C	Wersing, H		Dietterich, TG; Becker, S; Ghahramani, Z		Wersing, H			Learning lateral interactions for feature binding and sensory segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				NEURAL NETWORKS; CORTEX; MODEL	We present a new approach to the supervised learning of lateral interactions for the competitive layer model (CLM) dynamic feature binding architecture. The method is based on consistency conditions, which were recently shown to characterize the attractor states of this linear threshold recurrent network. For a given set of training examples the learning problem is formulated as a convex quadratic optimization problem in the lateral interaction weights. An efficient dimension reduction of the learning problem can be achieved by using a linear superposition of basis interactions. We show the successful application of the method to a medical image segmentation problem of fluorescence microscope cell images.	HONDA R&D Europe GMBH, D-63073 Offenbach, Germany		Wersing, H (corresponding author), HONDA R&D Europe GMBH, Carl Legien Str 30, D-63073 Offenbach, Germany.							Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072; Hofmann T, 1998, IEEE T PATTERN ANAL, V20, P803, DOI 10.1109/34.709593; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; MOZER MC, 1992, NEURAL COMPUT, V4, P650, DOI 10.1162/neco.1992.4.5.650; Nattkemper TW, 2001, IEEE T INF TECHNOL B, V5, P138, DOI 10.1109/4233.924804; Park J, 1999, NEURAL COMPUT, V11, P1985, DOI 10.1162/089976699300016052; PELILLO M, 1994, IEEE T PATTERN ANAL, V16, P933, DOI 10.1109/34.310691; PERFETTI R, 1995, IEEE T NEURAL NETWOR, V6, P1071, DOI 10.1109/72.410352; RITTER H, 1990, P INT NEUR NETW C PA, V2, P898; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; von der Malsburg C, 1999, NEURON, V24, P95, DOI 10.1016/S0896-6273(00)80825-9; Wang DL, 1997, NEURAL COMPUT, V9, P805, DOI 10.1162/neco.1997.9.4.805; Wersing H, 2001, NEURAL COMPUT, V13, P357, DOI 10.1162/089976601300014574; Wersing H, 2001, NEURAL COMPUT, V13, P1811, DOI 10.1162/08997660152469350; WERSING H, 2000, THESIS U BIELEFELD; XIE X, 2001, ADV NEURAL INFORMATI, V13	16	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1009	1016						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100126
C	Williams, CKI; Agakov, FV; Felderhof, SN		Dietterich, TG; Becker, S; Ghahramani, Z		Williams, CKI; Agakov, FV; Felderhof, SN			Products of Gaussians	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Recently Hinton (1999) has introduced the Products of Experts (PoE) model in which several individual probabilistic models for data are combined to provide an overall model of the data. Below we consider PoE models in which each expert is a Gaussian. Although the product of Gaussians is also a Gaussian, if each Gaussian has a simple structure the product can have a richer structure. We examine (1) Products of Gaussian pancakes which give rise to probabilistic Minor Components Analysis, (2) products of I-factor PPCA models and (3) a products of experts construction for an AR(1) process.	Univ Edinburgh, Div Informat, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Williams, CKI (corresponding author), Univ Edinburgh, Div Informat, Edinburgh EH1 2QL, Midlothian, Scotland.	c.k.i.williams@ed.ac.uk; F.Agakov@lft.uni-erlangen.de; stephenf@dai.ed.ac.uk		Agakov, Felix/0000-0003-4280-9062				AGAKOV F, 2000, THESIS U EDINBURGH; Hinton GE, 1999, IEE CONF PUBL, P1, DOI 10.1049/cp:19991075; Lei Xu, 1991, International Journal of Neural Systems, V2, P169, DOI 10.1142/S0129065791000169; LUETTGEN MR, 1993, IEEE T SIGNAL PROCES, V41, P3377, DOI 10.1109/78.258081; Marks T., 2001, P 3 INT C IND COMP A; OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Proakis J.G., 1996, DIGIT SIGNAL PROCESS; Tipping ME, 1999, J R STAT SOC B, V61, P611, DOI 10.1111/1467-9868.00196; WILLIAMS CKI, 2001, P ICSC S SOFT COMP 2; WILLIAMS CKI, 2001, EDIINFRR0043 U ED DI	11	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1017	1024						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100127
C	Wu, S; Amari, S		Dietterich, TG; Becker, S; Ghahramani, Z		Wu, S; Amari, S			Neural implementation of Bayesian inference in population codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				REPRESENTATION; DYNAMICS	This study investigates a population decoding paradigm, in which the estimation of stimulus in the previous step is used as prior knowledge for consecutive decoding. We analyze the decoding accuracy of such a Bayesian decoder (Maximum a Posteriori Estimate) I and show that it can be implemented by a biologically plausible recurrent network, where the prior knowledge of stimulus is conveyed by the change in recurrent interactions as a result of Hebbian learning.	Univ Sheffield, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England	University of Sheffield	Wu, S (corresponding author), Univ Sheffield, Dept Comp Sci, Sheffield S10 2TN, S Yorkshire, England.							AMARI SI, 1977, BIOL CYBERN, V27, P77, DOI 10.1007/BF00337259; Deneve S, 1999, NAT NEUROSCI, V2, P740, DOI 10.1038/11205; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; Pouget A, 2000, NAT REV NEUROSCI, V1, P125, DOI 10.1038/35039062; POUGET A, 1997, NIPS, V9; Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339; Wu S, 2001, NEURAL COMPUT, V13, P775, DOI 10.1162/089976601300014349; WU S, IN PRESS CNS 01; WU S, IN PRESS NEURAL COMP; Zhang K, 1996, J NEUROSCI, V16, P2112; Zhang KC, 1998, J NEUROPHYSIOL, V79, P1017, DOI 10.1152/jn.1998.79.2.1017	12	1	1	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						317	323						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100040
C	Bourlard, H; Bengio, S; Weber, K		Leen, TK; Dietterich, TG; Tresp, V		Bourlard, H; Bengio, S; Weber, K			New approaches towards robust and adaptive speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In this paper, we discuss some new research directions in automatic speech recognition (ASR), and which somewhat deviate from the usual approaches. More specifically, we will motivate and briefly describe new approaches based on multi-stream and multi/band ASR. These approaches extend the standard hidden Markov model (HMM) based approach by assuming that the different (frequency) channels representing the speech signal are processed by different (independent) "experts", each expert focusing on a different characteristic of the signal, and that the different stream likelihoods (or posteriors) are combined at some (temporal) stage to yield a global recognition output. As a further extension to multi-stream ASR, we will finally introduce a new approach, referred to as HMM2, where the HMM emission probabilities are estimated via state specific feature based HMMs responsible for merging the stream information and modeling their possible correlation.	IDIAP, CH-1920 Martigny, Switzerland		Bourlard, H (corresponding author), IDIAP, POB 592,Rue Simplon 4, CH-1920 Martigny, Switzerland.	bourlard@idiap.ch; bengio@idiap.ch; weber@idiap.ch						BENGIO S, 1990, IDIAPRR0011; BOURLARD H, 1996, P INT C SPOK LANG PR, P422; HAGEN A, 1999, P WORKSH ROB METH SP; HAGEN A, 1998, IDIAPRR9815; Hermansky H, 1999, INT CONF ACOUST SPEE, P289, DOI 10.1109/ICASSP.1999.758119; HERMANSKY H, 1996, P INT C SPOK LANG PR, P458; Lippmann R., 1997, P EUR 97 RHOD GREEC, pKN37; Mirghafori N, 1998, INT CONF ACOUST SPEE, P713, DOI 10.1109/ICASSP.1998.675364; Morris AC, 1998, INT CONF ACOUST SPEE, P737, DOI 10.1109/ICASSP.1998.675370; MORRIS AC, 1999, P EUR 99 BUD SEP; OKAWA S, 1998, P IEEE INT C AC SPEE; Rao S, 1996, IEEE T INFORM THEORY, V42, P1160, DOI 10.1109/18.508839; Tomlinson MJ, 1997, INT CONF ACOUST SPEE, P1247, DOI 10.1109/ICASSP.1997.596171; WEBER K, 2000, IDIAPRR0042	14	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						751	757						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800106
C	Cadez, IV; Smyth, P		Leen, TK; Dietterich, TG; Tresp, V		Cadez, IV; Smyth, P			Model complexity, goodness of fit and diminishing returns	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We investigate a general characteristic of the trade-off in learning problems between goodness-of-fit and model complexity. Specifically we characterize a general class of learning problems where the goodness-of-fit function can be shown to be convex within first-order as a function of model complexity. This general property of "diminishing returns" is illustrated on a number of real data sets and learning problems, including finite mixture modeling and multivariate linear regression.	Univ Calif Irvine, Irvine, CA 92697 USA	University of California System; University of California Irvine	Cadez, IV (corresponding author), Univ Calif Irvine, Irvine, CA 92697 USA.			Smyth, Padhraic/0000-0001-9971-8378				BISHOP C, 1995, NEURAL NETWORKS PATT, P376; CADEZ I, MSTR0018 MICR RES; LI JQ, NIPS 99; Smyth P, 1999, J ATMOS SCI, V56, P3704, DOI 10.1175/1520-0469(1999)056<3704:MRINHH>2.0.CO;2	4	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						388	394						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800055
C	Chechik, G; Tishby, N		Leen, TK; Dietterich, TG; Tresp, V		Chechik, G; Tishby, N			Temporally dependent plasticity: An information theoretic account	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				DEPRESSION	The paradigm of Hebbian learning has recently received a novel interpretation with the discovery of synaptic plasticity that depends on the relative timing of pre and post synaptic spikes. This paper derives a temporally dependent learning rule from the basic principle of mutual information maximization and studies its relation to the experimentally observed plasticity. We find that a supervised spike-dependent learning rule sharing similar structure with the experimentally observed plasticity increases mutual information to a stable near optimal level. Moreover, the analysis reveals how the temporal structure of time-dependent learning rules is determined by the temporal filter applied by neurons over their inputs. These results suggest experimental prediction as to the dependency of the learning rule on neuronal biophysical parameters	Hebrew Univ Jerusalem, Sch Engn & Comp Sci, Jerusalem, Israel	Hebrew University of Jerusalem	Chechik, G (corresponding author), Hebrew Univ Jerusalem, Sch Engn & Comp Sci, Jerusalem, Israel.	ggal@cs.huji.ac.il; tishby@cs.huji.ac.il						Abbott LF, 1999, ADV NEUR IN, V11, P69; BI Q, 1998, J NEUROSCI, V18, P10464; DEBANNE D, 1994, P NATL ACAD SCI USA, V91, P1148, DOI 10.1073/pnas.91.3.1148; Feldman DE, 2000, NEURON, V27, P45, DOI 10.1016/S0896-6273(00)00008-8; Horn D, 2000, ADV NEUR IN, V12, P129; Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498; KEMPTER R, 2000, UNPUB INTRINSIC STAB; LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6; LINSKER R, 1988, COMPUTER, V21, P105, DOI 10.1109/2.36; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; MEHTA M, 1999, COMPUTATIONAL NEUROS; Rao RPN, 2000, ADV NEUR IN, V12, P164; RUBIN J, 2000, IN PRESS PHYS REV D; SHANNON CE, 1948, BELL SYST TECH J, V27, P379, DOI 10.1002/j.1538-7305.1948.tb01338.x; Song S, 2000, NAT NEUROSCI, V3, P919, DOI 10.1038/78829; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	16	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						110	116						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800016
C	Dayan, P		Leen, TK; Dietterich, TG; Tresp, V		Dayan, P			Competition and arbors in ocular dominance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				VISUAL-CORTEX; MODELS	Hebbian and competitive Hebbian algorithms are almost ubiquitous in modeling pattern formation in cortical development. We analyse in theoretical detail a particular model (adapted from Piepenbrock & Obermayer, 1999) for the development of 1d stripe-like patterns, which places competitive and interactive cortical influences, and free and restricted initial arborisation onto a common footing.	UCL, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Dayan, P (corresponding author), UCL, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.	dayan@gatsby.ucl.ac.uk						ERWIN E, 1995, NEURAL COMPUT, V7, P425, DOI 10.1162/neco.1995.7.3.425; Kohonen T., 1995, SELF ORG MAPS, V30, DOI 10.1007/978-3-642-97610-0; Miller K. D., 1996, MODELS NEURAL NETWOR, P55, DOI [10.1007/978-1-4612-0723-8_2, DOI 10.1007/978-1-4612-0723-8_2]; MILLER KD, 1989, SCIENCE, V245, P605, DOI 10.1126/science.2762813; PIEPENBROCK C, 1999, ADV NEURAL INFORMATI, V11; Swindale NV, 1996, NETWORK-COMP NEURAL, V7, P161, DOI 10.1088/0954-898X/7/2/002	6	1	1	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						203	209						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800029
C	Deneve, S; Duhamel, JR; Pouget, A		Leen, TK; Dietterich, TG; Tresp, V		Deneve, S; Duhamel, JR; Pouget, A			A new model of spatial representations in multimodal brain areas	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Most models of spatial representations in the cortex assume cells with limited receptive fields that are defined in a particular egocentric frame of reference. However, cells outside of primary sensory cortex are either gain modulated by postural input or partially shifting. We show that solving classical spatial tasks, like sensory prediction, multi-sensory integration, sensory-motor transformation and motor control requires more complicated intermediate representations that are not invariant in one frame of reference. We present an iterative basis function map that performs these spatial tasks optimally with gain modulated and partially shifting units, and tests it against neurophysiological and neuropsychological data.	Univ Rochester, Dept Brain & Cognit Sci, Rochester, NY 14620 USA	University of Rochester	Deneve, S (corresponding author), Univ Rochester, Dept Brain & Cognit Sci, Rochester, NY 14620 USA.							ANDERSEN RA, 1990, J NEUROSCI, V10, P1176; Duhamel JR, 1997, NATURE, V389, P845, DOI 10.1038/39865; JAY MF, 1987, J NEUROPHYSIOL, V57, P22, DOI 10.1152/jn.1987.57.1.22; POUGET A, 1997, J COGNITIVE NEUROSCI, V9; STRICANNE B, 1993, SOC NEUROSCI, P26	5	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						117	123						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800017
C	Gales, MJF		Leen, TK; Dietterich, TG; Tresp, V		Gales, MJF			Factored semi-tied covariance matrices	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MODELS	A new form of covariance modelling for Gaussian mixture models and hidden Markov models is presented. This is an extension to an efficient form of covariance modelling used in speech recognition, semi-tied covariance matrices. In the standard form of semi-tied covariance matrices the covariance matrix is decomposed into a highly shared decorrelating transform and a component-specific diagonal covariance matrix. The use of a factored decorrelating transform is presented in this paper. This factoring effectively increases the number of possible transforms without increasing the number of free parameters. Maximum likelihood estimation schemes for all the model parameters are presented including the component/transform assignment, transform and component parameters. This new model form is evaluated on a large vocabulary speech recognition task. It is shown that using this factored form of covariance modelling reduces the word error rate.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Gales, MJF (corresponding author), Univ Cambridge, Dept Engn, Trumpington St, Cambridge CB2 1PZ, England.							DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Gales MJF, 1999, IEEE T SPEECH AUDI P, V7, P272, DOI 10.1109/89.759034; GALES MJF, 1999, CUEDFINFENGTR365; GOEL NK, 2001, IN PRESS P ICASSP; Kumar N., 1997, THESIS J HOPKINS U; Roweis S, 1999, NEURAL COMPUT, V11, P305, DOI 10.1162/089976699300016674; Woodland P., 1995, P ARPA WORKSHOP SPOK, P104; YOUNG SJ, 1996, HTK BOOK HTK VERSION; [No title captured]	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						779	785						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800110
C	Hochreiter, S; Mozer, MC		Leen, TK; Dietterich, TG; Tresp, V		Hochreiter, S; Mozer, MC			Beyond maximum likelihood and density estimation: A sample-based criterion for unsupervised learning of complex models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The goal of many unsupervised learning procedures is to bring two probability distributions into alignment. Generative models such as Gaussian mixtures and Boltzmann machines can be cast in this light, as can recoding models such as ICA and projection pursuit. We propose a novel sample-based error measure for these classes of models, which applies even in situations where maximum likelihood (ML) and probability density estimation-based formulations cannot be applied, e.g., models that are nonlinear or have intractable posteriors. Furthermore, our sample-based error measure avoids the difficulties of approximating a density function. We prove that with an unconstrained model, (1) our approach converges on the correct solution as the number of samples goes to infinity, and (2) the expected solution of our approach in the generative framework is the ML solution. Finally, we evaluate our approach via simulations of linear and nonlinear models on mixture of Gaussians and ICA problems. The experiments show the broad applicability and generality of our approach.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Hochreiter, S (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.	hochreit@cs.colorado.edu; mozer@cs.colorado.edu	Hochreiter, Sepp/AAI-5904-2020	Hochreiter, Sepp/0000-0001-7449-2528				[Anonymous], 1999, P 1 INT WORKSH IND C; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; Duda R.O., 1973, J ROYAL STAT SOC SER; ERDOGMUS D, 2000, P 2 INT WORKSH IND C, P75; Everitt B., 1984, INTRO LATENT VARIABL; Ghahramani Z, 1998, ADV NEUR IN, V10, P486; Ghahramani Z., 1996, CRGTR961 U TOR DEP C; Hinton G. E., 1986, PARALLEL DISTRIBUTED, V1, DOI DOI 10.1234/12345678; HINTON GE, 1999, UNSUPERVISED LEARNIN, pR7; HOCHREITER S, 2000, P 2 INT WORKSH IND C, P45; Hyvarinen A., 1999, Neural Computing Surveys, V2; Marques G., 1999, P 1 INT WORKSH IND C, P277; Zhao Y, 1996, IEEE T NEURAL NETWOR, V7, P362, DOI 10.1109/72.485672	13	1	1	0	0	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						535	541						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800076
C	Kali, S; Dayan, P		Leen, TK; Dietterich, TG; Tresp, V		Kali, S; Dayan, P			Hippocampally-dependent consolidation in a hierarchical model of neocortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MEMORY CONSOLIDATION; SYSTEMS; AMNESIA	In memory consolidation, declarative memories which initially require the hippocampus for their recall, ultimately become independent of it. Consolidation has been the focus of numerous experimental and qualitative modeling studies, but only little quantitative exploration. We present a consolidation model in which hierarchical connections in the cortex, that initially instantiate purely semantic information acquired through probabilistic unsupervised learning, come to instantiate episodic information as well. The hippocampus is responsible for helping complete partial input patterns before consolidation is complete, while also training the cortex to perform appropriate completion by itself.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Kali, S (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, Mortimer St,17 Queen Sq, London WC1N 3AR, England.		Káli, Szabolcs/AAW-1138-2020	Káli, Szabolcs/0000-0002-2740-6057				FREUND Y, 1992, ADV NEURAL INFORMATI, V4; HINTON G, 2000, 2000004 GCNU; Hinton Geoffrey E., 1999, UNSUPERVISED LEARNIN; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; McClelland JL, 1986, PARALLEL DISTRIBUTED, V1, P1; Murre JMJ, 1997, MEMORY, V5, P213, DOI 10.1080/741941155; Nadel L, 1997, CURR OPIN NEUROBIOL, V7, P217, DOI 10.1016/S0959-4388(97)80010-4; SQUIRE LR, 1992, PSYCHOL REV, V99, P195, DOI 10.1037/0033-295X.99.2.195; WILSON MA, 1994, SCIENCE, V265, P676, DOI 10.1126/science.8036517	11	1	1	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						24	30						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800004
C	Li, ZP; Dayan, P		Leen, TK; Dietterich, TG; Tresp, V		Li, ZP; Dayan, P			Position variance, recurrence and perceptual learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PATTERN-RECOGNITION; HYPERACUITY; MODELS; CORTEX	Stimulus arrays are inevitably presented at different positions on the retina in visual tasks, even those that nominally require fixation. In particular, this applies to many perceptual learning tasks. We show that perceptual inference or discrimination in the face of positional variance has a structurally different quality from inference about fixed position stimuli, involving a particular, quadratic, non-linearity rather than a purely linear discrimination. We show the advantage taking this non-linearity into account has for discrimination, and suggest it as a role for recurrent connections in area V1, by demonstrating the superior discrimination performance of a recurrent network. We propose that learning the feedforward and recurrent neural connections for these tasks corresponds to the fast and slow components of learning observed in perceptual learning tasks.	Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Li, ZP (corresponding author), Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.	zhaoping@gatsby.ucl.ac.uk; dayan@gatsby.ucl.ac.uk						[Anonymous], [No title captured]; FAHLE M, 1995, VISION RES, V35, P3003, DOI 10.1016/0042-6989(95)00044-Z; FAHLE M, 1994, PERCEPTION, V23, P411, DOI 10.1068/p230411; Fahle M, 1997, VISION RES, V37, P1885, DOI 10.1016/S0042-6989(96)00308-2; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; GILBERT C, 2000, NEUR DYN WORKSH GATS; KARNI A, 1993, NATURE, V365, P250, DOI 10.1038/365250a0; Li ZP, 1999, NETWORK-COMP NEURAL, V10, P187, DOI 10.1088/0954-898X/10/2/305; POGGIO T, 1992, SCIENCE, V256, P1018, DOI 10.1126/science.1589770; Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; SEUNG HS, 1993, P NATL ACAD SCI USA, V90, P10749, DOI 10.1073/pnas.90.22.10749; WEISS Y, 1993, NEURAL COMPUT, V5, P695, DOI 10.1162/neco.1993.5.5.695	13	1	1	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						31	37						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800005
C	Movellan, JR; Mineiro, P; Williams, RJ		Leen, TK; Dietterich, TG; Tresp, V		Movellan, JR; Mineiro, P; Williams, RJ			Partially observable SDE models for image sequence recognition tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					This paper explores a framework for recognition of image sequences using partially observable stochastic differential equation (SDE) models. Monte-Carlo importance sampling techniques are used for efficient estimation of sequence likelihoods and sequence likelihood gradients. Once the network dynamics are learned, we apply the SDE models to sequence recognition tasks in a manner similar to the way Hidden Markov models (HMMs) are commonly applied. The potential advantage of SDEs over HMMS is the use of continuous state dynamics. We present encouraging results for a video sequence recognition task in which SDE models provided excellent performance when compared to hidden Markov models.	Univ Calif San Diego, Inst Neural Computat, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Movellan, JR (corresponding author), Univ Calif San Diego, Inst Neural Computat, La Jolla, CA 92093 USA.							ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; FISHMAN GS, 1996, MONTE CARLO SAMPLING; Karatzas I., 1987, BROWNIAN MOTION STOC; LUETTIN J, 1997, THESIS U SHEFFIELD; MOVELLAN JR, 1995, ADV NEURAL INFORMATI, V7; Oksendal B., 1992, STOCHASTIC DIFFERENT, V3rd	6	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						880	886						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800124
C	Naphade, MR; Kozintsev, I; Huang, T		Leen, TK; Dietterich, TG; Tresp, V		Naphade, MR; Kozintsev, I; Huang, T			Probabilistic semantic video indexing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We propose a novel probabilistic framework for semantic video indexing. We define probabilistic multimedia objects (multijects) to map low-level media features to high-level semantic labels. A graphical network of such multijects (multinet) captures scene context by discovering intra-frame as well as inter-frame dependency relations between the concepts. The main contribution is a novel application of a factor graph framework to model this network. We model relations between semantic concepts in terms of their co-occurrence as well as the temporal dependencies between these concepts within video shots. Using the sum-product algorithm [1] for approximate or exact inference in these factor graph multinets, we attempt to correct errors made during isolated concept detection by forcing high-level constraints. This results in a significant improvement in the overall detection performance.	Univ Illinois, Dept Elect & Comp Engn, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Naphade, MR (corresponding author), Univ Illinois, Dept Elect & Comp Engn, 1406 W Green St, Urbana, IL 61801 USA.	milind@ifp.uiuc.edu; igor@ifp.uiuc.edu; huang@ifp.uiuc.edu						Chang SF, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P531, DOI 10.1109/ICIP.1998.727321; DUDANI SA, 1977, IEEE T COMPUT, V26, P39, DOI 10.1109/TC.1977.5009272; Jain AK, 1998, PATTERN RECOGN, V31, P1369, DOI 10.1016/S0031-3203(97)00131-3; Jain R., 1995, MACHINE VISION; KSCHISCHANG F, 1998, UNPUB IEEE T INF JUL; Naphade MR, 2000, PROC SPIE, V3972, P168; Naphade MR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 3, P536, DOI 10.1109/ICIP.1998.999041; Naphade MR, 1998, 1998 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOL 1, P884, DOI 10.1109/ICIP.1998.723662; NAPHADE MR, 2000, IN PRESS IEEE INT C; ZHONG D, 1997, P IEEE INT C IM PROC, V2, P21	10	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						967	973						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800136
C	Natschlager, T; Maass, W; Sontag, ED; Zador, A		Leen, TK; Dietterich, TG; Tresp, V		Natschlager, T; Maass, W; Sontag, ED; Zador, A			Processing of time series by neural circuits with biologically realistic synaptic dynamics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				NEOCORTICAL PYRAMIDAL NEURONS	Experimental data show that biological synapses behave quite differently from the symbolic synapses in common artificial neural network models. Biological synapses are dynamic, i.e., their "weight" changes on a short time scale by several hundred percent in dependence of the past input to the synapse. In this article we explore the consequences that these synaptic dynamics entail for the computational power of feedforward neural networks. We show that gradient descent suffices to approximate a given (quadratic) filter by a rather small neural system with dynamic synapses, We also compare our network model to artificial neural networks designed for time series processing. Our numerical results are complemented by theoretical analysis which show that even with just a single hidden layer such networks can approximate a surprisingly large large class of nonlinear filters: all filters that can be characterized by Volterra series. This result is robust with regard to various changes in the model for synaptic dynamics.	Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Natschlager, T (corresponding author), Graz Univ Technol, Inst Theoret Comp Sci, A-8010 Graz, Austria.		Sontag, Eduardo D/J-4420-2012	Sontag, Eduardo D/0000-0001-8020-5783; Zador, Anthony/0000-0002-8431-9136				BACK AD, 1993, NEURAL COMPUT, V5, P456, DOI 10.1162/neco.1993.5.3.456; deCharms RC, 1998, SCIENCE, V280, P1439, DOI 10.1126/science.280.5368.1439; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; LITTLE WA, 1975, BEHAV BIOL, V14, P115, DOI 10.1016/S0091-6773(75)90122-4; Maass W, 2000, NEURAL COMPUT, V12, P1743, DOI 10.1162/089976600300015123; Markram H, 1998, P NATL ACAD SCI USA, V95, P5323, DOI 10.1073/pnas.95.9.5323; Markram H, 1996, NATURE, V382, P807, DOI 10.1038/382807a0; Selig DK, 1999, J NEUROSCI, V19, P1236; Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502; Varela JA, 1997, J NEUROSCI, V17, P220; Zador AM, 2000, NAT NEUROSCI, V3, P1167, DOI 10.1038/81432	11	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						145	151						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800021
C	Brown, GJ; Wang, DL		Solla, SA; Leen, TK; Muller, KR		Brown, GJ; Wang, DL			An oscillatory correlation framework for computational auditory scene analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				NEURAL OSCILLATORS; SEGREGATION; NETWORK	A neural model is described which uses oscillatory correlation to segregate speech from interfering sound sources. The core of the model is a two-layer neural oscillator network. A sound stream is represented by a synchronized population of oscillators, and different streams are represented by desynchronized oscillator populations. The model has been evaluated using a corpus of speech mixed with interfering sounds, and produces an improvement in signal-to-noise ratio for every mixture.	Univ Sheffield, Dept Comp Sci, Sheffield S1 4DP, S Yorkshire, England	University of Sheffield	Brown, GJ (corresponding author), Univ Sheffield, Dept Comp Sci, Regent Court,211 Portobello St, Sheffield S1 4DP, S Yorkshire, England.			Brown, Guy/0000-0001-8565-5476				Bregman AS, 1990, AUDITORY SCENE ANAL; Brown GJ, 1997, NEURAL NETWORKS, V10, P1547, DOI 10.1016/S0893-6080(97)00046-4; BROWN GJ, 1994, COMPUT SPEECH LANG, V8, P297, DOI 10.1006/csla.1994.1016; Cooke M, 1993, MODELLING AUDITORY P; ELLIS DPW, 1996, THESIS MIT DEP ELECT; Linsay PS, 1998, IEEE T NEURAL NETWOR, V9, P523, DOI 10.1109/72.668894; TERMAN D, 1995, PHYSICA D, V81, P148, DOI 10.1016/0167-2789(94)00205-5; Wang DL, 1997, NEURAL COMPUT, V9, P1623; Wang DL, 1996, COGNITIVE SCI, V20, P409, DOI 10.1207/s15516709cog2003_3; Wang DL, 1997, NEURAL COMPUT, V9, P805, DOI 10.1162/neco.1997.9.4.805; Wang DL, 1999, NEURAL NETWORKS, V12, P579, DOI 10.1016/S0893-6080(99)00028-3; Wang DLL, 1999, IEEE T NEURAL NETWOR, V10, P684, DOI 10.1109/72.761727	12	1	2	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						747	753						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700106
C	Buhmann, JM; Held, M		Solla, SA; Leen, TK; Muller, KR		Buhmann, JM; Held, M			Model selection in clustering by uniform convergence bounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Unsupervised learning algorithms are designed to extract structure from data samples. Reliable and robust inference requires a guarantee that extracted structures are typical for the data source, i.e., similar structures have to be inferred from a second sample set of the same data source. The overfitting phenomenon in maximum entropy based annealing algorithms is exemplarily studied for a class of histogram clustering models, Bernstein's inequality for large deviations is used to determine the maximally achievable approximation quality parameterized by a minimal temperature. Monte Carlo simulations support the proposed model selection criterion by finite temperature annealing.	Inst Informat 3, D-53117 Bonn, Germany	University of Bonn	Buhmann, JM (corresponding author), Inst Informat 3, Romerstr 164, D-53117 Bonn, Germany.		Buhmann, Joachim/AAU-4760-2020					BUHMANN JM, 1998, 983 IAITR U BONN I I; DEBRUIJN NG, 1981, ASYMPTOTIC METHODS A; Haussler D, 1996, MACH LEARN, V25, P195, DOI 10.1007/BF00114010; HAUSSLER D, 1994, MACH LEARN, V14, P83, DOI 10.1007/BF00993163; HAUSSLER D, 1996, ANN STAT         DEC; HOFMANN T, 1999, ADV NEURAL INFORMATI, V11; van der Vaart AW, 1996, WEAK CONVERGENCE EMP; Vapnik V.N, 1998, STAT LEARNING THEORY	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						216	222						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700031
C	Fisher, JW; Ihler, AT; Viola, PA		Solla, SA; Leen, TK; Muller, KR		Fisher, JW; Ihler, AT; Viola, PA			Learning informative statistics: A nonparametric approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We discuss an information theoretic approach for categorizing and modeling dynamic processes. The approach can learn a compact and informative statistic which summarizes past states to predict future observations. Furthermore, the uncertainty of the prediction is characterized nonparametrically by a joint density over the learned statistic and present observation. We discuss the application of the technique to both noise driven dynamical systems and random processes sampled from a density which is conditioned on the past. In the first case we show results in which both the dynamics of random walk and the statistics of the driving noise are captured. In the second case we present results in which a summarizing statistic is learned on noisy random telegraph waves with differing dependencies on past states. In both cases the algorithm yields a principled approach for discriminating processes with differing dynamics and/or dependencies. The method is grounded in ideas from information theory and nonparametric statistics.	MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Fisher, JW (corresponding author), MIT, 77 Massachusetts Ave,35-421, Cambridge, MA 02139 USA.			Ihler, Alexander/0000-0002-4331-1015				Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; FISHER JW, 1998, P IEEE INT JOINT C N; Kapur J., 1992, ENTROPY OPTIMIZATION; PARZEN E, 1962, ANN MATH STAT, V33, P1065, DOI 10.1214/aoms/1177704472; VIOLA P, 1996, ADV NEURAL INFORMATI	5	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						900	906						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700127
C	Hansen, LK		Solla, SA; Leen, TK; Muller, KR		Hansen, LK			Bayesian averaging is well-temperated	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				INFORMATION	Bayesian predictions are stochastic just like predictions of any other inference scheme that generalize from a finite sample. While a simple variational argument shows that Bayes averaging is generalization optimal given that the prior matches the teacher parameter distribution the situation is less clear if the teacher distribution is unknown. I define a class of averaging procedures, the temperated likelihoods, including both Bayes averaging with a uniform prior and maximum likelihood estimation as special cases. I show that Bayes is generalization optimal in this family for any teacher distribution for two learning problems that are analytically tractable: learning the mean of a Gaussian and asymptotics of smooth learners.	Tech Univ Denmark B321, Dept Math Modelling, DK-2800 Lyngby, Denmark	Technical University of Denmark	Hansen, LK (corresponding author), Tech Univ Denmark B321, Dept Math Modelling, DK-2800 Lyngby, Denmark.		Hansen, Lars/E-3174-2013	Hansen, Lars Kai/0000-0003-0442-5877				BREIMAN L, 1999, 547 STAT DEP; CLEMEN RT, 1989, INT J FORECASTING, V5, P559, DOI 10.1016/0169-2070(89)90012-5; HANSEN LK, 1993, NEURAL NETWORKS, V6, P393, DOI 10.1016/0893-6080(93)90006-I; HANSEN LK, 1990, IEEE T PATTERN ANAL, V12, P993, DOI 10.1109/34.58871; Haussler D, 1997, ANN STAT, V25, P2451; Heskes T, 1998, NEURAL COMPUT, V10, P1425, DOI 10.1162/089976698300017232; Ljung L., 1987, SYSTEM IDENTIFICATIO; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; MOODY JE, 1991, P IEEE WORKSH NEUR N, P1; ROBERT CP, 1994, KENDALLS ADV THEOR B, V2; Vapnik V., 1982, ESTIMATION DEPENDENC; WHITE H, 1981, J AM STAT ASSOC, V76, P419, DOI 10.2307/2287845	13	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						265	271						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700038
C	Jin, CT; Carlile, S		Solla, SA; Leen, TK; Muller, KR		Jin, CT; Carlile, S			Neural system model of human sound localization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				HUMAN LISTENERS	This paper examines the role of biological constraints in the human auditory localization process. A psychophysical and neural system modeling approach was undertaken in which performance comparisons between competing models and a human subject explore the relevant biologically plausible "realism constraints". The directional acoustical cues, upon which sound localization is based, were derived from the human subject's head-related transfer functions(HRTFs). Sound stimuli were generated by convolving bandpass noise with the HRTFs and were presented to both the subject and the model. The input stimuli to the model was processed using the Auditory Image Model of cochlear processing. The cochlear data was then analyzed by a time-delay neural network which integrated temporal and spectral information to determine the spatial location of the sound source. The combined cochlear model and neural network provided a system model of the sound localization process. Human-like localization performance was qualitatively achieved for broadband and bandpass stimuli when the model architecture incorporated frequency division (or tonotopicity), and was trained using variable bandwidth and center-frequency sounds.	Univ Sydney, Dept Physiol, Sydney, NSW 2006, Australia	University of Sydney	Jin, CT (corresponding author), Univ Sydney, Dept Physiol, Sydney, NSW 2006, Australia.			Jin, Craig/0000-0003-4636-753X				Carlile S, 1999, HEARING RES, V128, P175, DOI 10.1016/S0378-5955(98)00205-6; Carlile S, 1997, HEARING RES, V114, P179, DOI 10.1016/S0378-5955(97)00161-5; CARLILE S, 1996, VIRTUAL AUDITORY SPA; Fisher NI., 1987, STAT ANAL SPHERICAL, DOI [10.1017/CBO9780511623059, DOI 10.1017/CBO9780511623059]; GIGUERE C, 1994, J ACOUST SOC AM, V95, P331, DOI 10.1121/1.408366; Gilkey R, 1997, BINAURAL SPATIAL HEA; HYAMS S, 2000, UNPUB J ACOUST SOC A; JIN C, 1999, UNPUB J ACOUST SOC A; MIDDLEBROOKS JC, 1992, J ACOUST SOC AM, V92, P2607, DOI 10.1121/1.404400; NETI C, 1992, J ACOUST SOC AM, V92, P3140, DOI 10.1121/1.404210	10	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						761	767						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700108
C	Lee, DD; Rokni, U; Sompolinsky, H		Solla, SA; Leen, TK; Muller, KR		Lee, DD; Rokni, U; Sompolinsky, H			Algorithms for independent components analysis and higher order statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				BLIND SEPARATION	A latent variable generative model with finite noise is used to describe several different algorithms for Independent Components Analysis (ICA). In particular, the Fixed Point ICA algorithm is shown to be equivalent to the Expectation-Maximization algorithm for maximum likelihood under certain constraints, allowing the conditions for global convergence to be elucidated. The algorithms can also be explained by their generic behavior near a singular point where the size of the optimal generative bases vanishes. An expansion of the likelihood about this singular point indicates the role of higher order correlations in determining the features discovered by ICA. The application and convergence of these algorithms are demonstrated on a simple illustrative example.	Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T	Lee, DD (corresponding author), Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.		Lee, Daniel D./B-5753-2013; Sompolinsky, Haim/ABB-8398-2021					Amari S, 1996, ADV NEUR IN, V8, P757; Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Cardoso JF, 1999, NEURAL COMPUT, V11, P157, DOI 10.1162/089976699300016863; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Haykin S., 1999, NEURAL NETWORKS COMP; Hinton GE, 1997, PHILOS T ROY SOC B, V352, P1177, DOI 10.1098/rstb.1997.0101; Hyvarinen A, 1997, NEURAL COMPUT, V9, P1483, DOI 10.1162/neco.1997.9.7.1483; JUTTEN C, 1991, SIGNAL PROCESS, V24, P1, DOI 10.1016/0165-1684(91)90079-X; Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719; Nadal JP, 1997, NEURAL COMPUT, V9, P1421, DOI 10.1162/neco.1997.9.7.1421; Pearlmutter B. A., 1996, Progress in Neural Information Processing. Proceedings of the International Conference on Neural Information Processing, P151; Roth Z, 1996, IEEE T NEURAL NETWOR, V7, P1291, DOI 10.1109/72.536322	13	1	1	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						491	497						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700070
C	Ohira, R; Sato, Y; Cowan, JD		Solla, SA; Leen, TK; Muller, KR		Ohira, R; Sato, Y; Cowan, JD			Resonance in a stochastic neuron model with delayed interaction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SPIKING NEURONS; RANDOM-WALKS; NETWORKS; NOISE; PROPAGATION	We study here a simple stochastic single neuron model with delayed self-feedback capable of generating spike trains. Simulations show that its spike trains exhibit resonant behavior between "noise" and "delay". In order to gain insight into this resonance, we simplify the model and study a stochastic binary element whose transition probability depends on its state at a fixed interval in the past. With this simplified model we can analytically compute interspike interval histograms, and show how the resonance between noise and delay arises. The resonance is also observed when such elements are coupled through delayed interaction.	Sony Comp Sci Lab, Tokyo 141, Japan	Sony Corporation	Ohira, R (corresponding author), Sony Comp Sci Lab, 3-14-13 Higashi Gotanda, Tokyo 141, Japan.							Bressloff PC, 1999, PHYS REV LETT, V82, P2979, DOI 10.1103/PhysRevLett.82.2979; Foss J, 1996, PHYS REV LETT, V76, P708, DOI 10.1103/PhysRevLett.76.708; Gammaitoni L, 1998, REV MOD PHYS, V70, P223, DOI 10.1103/RevModPhys.70.223; GANG H, 1993, PHYS REV LETT, V71, P807, DOI 10.1103/PhysRevLett.71.807; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; Kim S, 1999, PHYS REV LETT, V82, P1620, DOI 10.1103/PhysRevLett.82.1620; Locher M, 1998, PHYS REV LETT, V80, P5212, DOI 10.1103/PhysRevLett.80.5212; Longtin A, 1997, PHYS REV E, V55, P868, DOI 10.1103/PhysRevE.55.868; Maass W, 1996, NEURAL COMPUT, V8, P1, DOI 10.1162/neco.1996.8.1.1; Maass W, 1997, NEURAL COMPUT, V9, P279, DOI 10.1162/neco.1997.9.2.279; Ohira T, 1999, PHYS REV LETT, V82, P2811, DOI 10.1103/PhysRevLett.82.2811; Ohira T, 1997, PHYS REV E, V55, pR1255, DOI 10.1103/PhysRevE.55.R1255; OHIRA T, 1995, PHYS REV E, V52, P3277, DOI 10.1103/PhysRevE.52.3277; OHIRA T, 1995, NEURAL COMPUT, V7, P518, DOI 10.1162/neco.1995.7.3.518; Pham J, 1998, PHYS REV E, V58, P3610, DOI 10.1103/PhysRevE.58.3610; RAPPEL WJ, 1994, PHYS REV E, V50, P3249, DOI 10.1103/PhysRevE.50.3249	16	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						314	320						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700045
C	Parfitt, S; Tino, P; Dorffner, G		Solla, SA; Leen, TK; Muller, KR		Parfitt, S; Tino, P; Dorffner, G			Graded grammaticality in prediction fractal machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				TIME	We introduce a novel method of constructing language models, which avoids some of the problems associated with recurrent neural networks. The method of creating a Prediction Fractal Machine (PFM) [1] is briefly described and some experiments are presented which demonstrate the suitability of PFMs for language modeling. PFMs distinguish reliably between minimal pairs, and their behavior is consistent with the hypothesis [4] that wellformedness is 'graded' not absolute. A discussion of their potential to offer fresh insights into language acquisition and processing follows.	Austrian Res Inst Artificial Intelligence, A-1010 Vienna, Austria		Parfitt, S (corresponding author), Austrian Res Inst Artificial Intelligence, Schottengasse 3, A-1010 Vienna, Austria.		Dorffner, Georg/AAQ-1455-2020; Tino, Peter/Z-5748-2019	Dorffner, Georg/0000-0002-3181-2576; Tino, Peter/0000-0003-2330-128X				ALLEN J, IN PRESS EMERGENTIST; Barnsley M. F., 2012, FRACTALS EVERYWHERE; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5; Casey M, 1996, NEURAL COMPUT, V8, P1135, DOI 10.1162/neco.1996.8.6.1135; COULSON S, 1998, LANGUAGE COGNITIVE P, V13; Elman J., 1996, RETHINKING INNATENES; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; GROSSBERG S, 1988, NEURAL NETWORKS NATU; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; KRUSCHKE JK, 1991, ADV NEURAL INFORMATI, V3, P649; LAWRENCE S, IN PRESS IEEE T KNOW; McCloskey M., 1989, PSYCHOL LEARNING MOT, V24; PARFITT S, 1997, THESIS IMPERIAL COLL; RON D, 1996, MACH LEARN, P25; Saffran JR, 1996, PROCEEDINGS OF THE EIGHTEENTH ANNUAL CONFERENCE OF THE COGNITIVE SCIENCE SOCIETY, P376; SERVANSCHREIBER D, 1989, ADV NEURAL INFORMATI, V1, P643; TABOR W, IN PRESS COGNITIVE S; Taylor J. R, 1995, LINGUISTICS CATEGORI; TINO P, 1998, TR9818 AUSTR RES I A; TINO P, 1998, NEURAL NETWORKS PATT, P171	20	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						52	58						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700008
C	Patel, GN; Brown, EA; DeWeerth, SP		Solla, SA; Leen, TK; Muller, KR		Patel, GN; Brown, EA; DeWeerth, SP			A neuromorphic VLSI system for modeling the neural control of axial locomotion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				PATTERN	We have developed and tested an analog/digital VLSI system that models the coordination of biological segmental oscillators underlying axial locomotion in animals such as leeches and lampreys. In its current form the system consists of a chain of twelve pattern generating circuits that are capable of arbitrary contralateral inhibitory synaptic coupling. Each pattern generating circuit is implemented with two independent silicon Morris-Lecar neurons with a total of 32 programmable (floating-gate based) inhibitory synapses, and an asynchronous address-event interconnection element that provides synaptic connectivity and implements axonal delay We describe and analyze the data from a set of experiments exploring the system behavior in terms of synaptic coupling.	Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Patel, GN (corresponding author), Georgia Inst Technol, Sch Elect & Comp Engn, Atlanta, GA 30332 USA.	girish@ece.gatech.edu; ebrown@ece.gatech.edu; steved@ece.gatech.edu						BOAHEN KA, 1997, ANALOG INTEGRATED CI; Cohen A., COMMUNICATION; DeWeerth SP, 1997, SEVENTEENTH CONFERENCE ON ADVANCED RESEARCH IN VLSI, PROCEEDINGS, P182, DOI 10.1109/ARVLSI.1997.634854; Hasler P, 1999, 20TH ANNIVERSARY CONFERENCE ON ADVANCED RESEARCH IN VLSI, PROCEEDINGS, P215, DOI 10.1109/ARVLSI.1999.756050; Hirose S., 1993, BIOL INSPIRED ROBOTS, DOI 10.1017/S0263574700017264; KOPELL N, 1988, MATH BIOSCI, V90, P87, DOI 10.1016/0025-5564(88)90059-4; Mahowald M., 1992, THESIS CALTECH PASAD; Marder E, 1996, PHYSIOL REV, V76, P687, DOI 10.1152/physrev.1996.76.3.687; PATEL G, 1999, THESIS GEORGIA I TEC; Somers D, 1995, PHYSICA D, V89, P169, DOI 10.1016/0167-2789(95)00198-0; WILLIAMS TL, 1992, SCIENCE, V258, P662, DOI 10.1126/science.1411575	12	1	1	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						724	730						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700103
C	Ratsch, G; Scholkopf, B; Smola, A; Muller, KR; Onoda, T; Mika, S		Solla, SA; Leen, TK; Muller, KR		Ratsch, G; Scholkopf, B; Smola, A; Muller, KR; Onoda, T; Mika, S			nu-Arc: Ensemble learning in the presence of outliers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	Advances in Neural Information Processing Systems		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					AdaBoost and other ensemble methods have successfully been applied to a number of classification tasks, seemingly defying problems of overfitting. AdaBoost performs gradient descent in an error function with respect to the margin, asymptotically concentrating on the patterns which are hardest to learn. For very noisy problems, however, this can be disadvantageous. Indeed, theoretical analysis has shown that the margin distribution, as opposed to just the minimal margin, plays a crucial role in understanding this phenomenon. Loosely speaking, some outliers should be tolerated if this has the benefit of substantially increasing the margin on the remaining points. We propose a new boosting algorithm which allows for the possibility of a pre-specified fraction of points to lie in the margin area or even on the wrong side of the decision boundary.	GMD FIRST, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Ratsch, G (corresponding author), GMD FIRST, Rudower Chaussee 5, D-12489 Berlin, Germany.	raetsch@first.gmd.de; bsc@microsoft.com; Alex.Smola@anu.edu.au; klaus@first.gmd.de; onoda@criepi.denken.or.jp; mika@first.gmd.de	Rätsch, Gunnar/B-8182-2009; Mueller, Klaus-Robert/Y-3547-2019; Rätsch, Gunnar/O-5914-2017; Schölkopf, Bernhard/A-7570-2013	Mueller, Klaus-Robert/0000-0002-3861-7685; Rätsch, Gunnar/0000-0001-5486-8532; Schölkopf, Bernhard/0000-0002-8177-0925				BREIMAN L, 1997, 504 U CAL STAT DEP; FREAN M, 1998, SIMPLE COST FUNCTION; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J., 1998, ADDITIVE LOGISTIC RE; GOVE A, 1998, P 15 NAT C AI, P692; LeCun Y., 1995, Neural Networks: The Statistical Mechanics Perspective. Proceedings of the CTP-PBSRI. Joint Workshop on Theoretical Physics, P261; MASON L, 1999, IN PRESS MACHINE LEA; Newman C. B. D., 1998, UCI REPOSITORY MACHI; QUINLAN J, LNAI, V1160, P143; Ratsch G., 1999, ADV LARGE MARGIN CLA, P207; RATSCH G, 1998, IN PRESS MACHINE LEA; Schapire R. E., 1998, ANN STAT; Scholkopf B., 1999, ADV KERNEL METHODS S; SCHWENK H, 1998, ADV NEURAL INF PROCE, V10; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	16	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						561	567						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700080
C	Rosca, J; O Ruanaidh, J; Jourjine, A; Rickard, S		Solla, SA; Leen, TK; Muller, KR		Rosca, J; O Ruanaidh, J; Jourjine, A; Rickard, S			Broadband direction-of-arrival estimation based on second order statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					N wideband sources recorded using N closely spaced receivers can feasibly be separated based only on second order statistics when using a physical model of the mixing process. In this case we show that the parameter estimation problem can be essentially reduced to considering directions of arrival and attenuations of each signal. The paper presents two demixing methods operating in the time and frequency domain and experimentally shows that it is always possible to demix signals arriving at different angles. Moreover, one can use spatial cues to solve the channel selection problem and a post-processing Wiener filter to ameliorate the artifacts caused by demixing.	Siemens Corp Res Inc, Princeton, NJ 08540 USA	Siemens AG	Rosca, J (corresponding author), Siemens Corp Res Inc, 755 Coll Rd E, Princeton, NJ 08540 USA.							JOURJINE A, 1999, SCR99TR657; Krim H., 1996, IEEE SIGNAL PROCESSI, V13; Laakso T.I., 1996, IEEE SIGNAL PROCESSI, V13, P30; MOLGEDEY L, 1994, PHYS REV LETT, V72, P3634, DOI 10.1103/PhysRevLett.72.3634; NGO TJ, 1999, 1 INT WORKSH ICA BSS, P257; PARRA L, 1988, NNSP98; TORKOLLA K, 1999, 1 INT WORKSH IND COM, P239; VANVEEN V, 1988, IEEE ASSP MAGAZINE, V5; Weinstein E, 1993, IEEE T SPEECH AUDI P, V1, P405, DOI 10.1109/89.242486	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						775	781						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700110
C	Smith, G; de Freitas, JFG; Robinson, T; Niranjan, M		Solla, SA; Leen, TK; Muller, KR		Smith, G; de Freitas, JFG; Robinson, T; Niranjan, M			Speech modelling using subspace and EM techniques	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The speech waveform can be modelled as a piecewise-stationary linear stochastic state space system, and its parameters can be estimated using an expectation-maximisation (EM) algorithm. One problem is the initialisation of the EM algorithm. Standard initialisation schemes can lead to poor formant trajectories. But these trajectories however are important for vowel intelligibility The aim of this paper is to investigate the suitability of subspace identification methods to initialise EM, The paper compares the subspace state space system identification (4SID) method with the EM algorithm, The 4SID and EM methods are similar in that they both estimate a state sequence (but using Kalman filters and Kalman smoothers respectively), and then estimate parameters (but using least-squares and maximum likelihood respectively). The similarity of 4SID and EM motivates the use of 4SID to initialise EM, Also, 4SID is non-iterative and requires no initialisation, whereas EM is iterative and requires initialisation. However 4SID is sub-optimal compared to EM in a probabilistic sense. During experiments on real speech, 4SID methods compare favourably with conventional initialisation techniques. They produce smoother formant trajectories, have greater frequency resolution, and produce higher likelihoods.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Smith, G (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.			Niranjan, Mahesan/0000-0001-7021-140X				ARUN KS, 1990, SIAM J MATRIX ANAL A, V11, P42, DOI 10.1137/0611003; Gelb A., 1974, APPL OPTIMAL ESTIMAT; Ghahramani Z., 1996, CRGTR962 U TOTR DEP; GRICE M, 1989, MULTILINGUAL SPEECH; GRIVEL E, 1999, ICASSP 99; Ljung L., 1987, SYSTEM IDENTIFICATIO; Ljung L., 1991, SYSTEM IDENTIFICATIO; MACLACHLAN GJ, 1997, EM ALGORITHM EXTENSI; SMITH GA, 2000, CUEDFINFENGTR366 CAM; Van Overschee P., 1996, SUBSPACE IDENTIFICAT; Viberg M, 1997, AUTOMATICA, V33, P1603, DOI 10.1016/S0005-1098(97)00097-6	11	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						796	802						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700113
C	van Schaik, A		Solla, SA; Leen, TK; Muller, KR		van Schaik, A			An analog VLSI model of periodicity extraction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					This paper presents an electronic system that extracts the periodicity of a sound. It uses three analogue VLSI building blocks: a silicon cochlea, two inner-hair-cell circuits and two spiking neuron chips. The silicon cochlea consists of a cascade of filters. Because of the delay between two outputs from the silicon cochlea, spike trains created at these outputs are synchronous only for a narrow range of periodicities. In contrast to traditional bandpass filters, where an increase in selectivity has to be traded off against a decrease in response time, the proposed system responds quickly, independent of selectivity.	Univ Sydney, Comp Engn Lab, Sydney, NSW 2006, Australia	University of Sydney	van Schaik, A (corresponding author), Univ Sydney, Comp Engn Lab, J03, Sydney, NSW 2006, Australia.			van Schaik, Andre/0000-0001-6140-017X				EVANS, 1982, SENSES, P251; LAZZARO J, 1991, IEEE J SOLID-ST CIRC, V26, P772, DOI 10.1109/4.78248; SENEFF S, 1988, J PHONETICS, V16, P55, DOI 10.1016/S0095-4470(19)30466-8; VANSCHAIK, 2000, IN PRESS ANALOG INTE	4	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						738	744						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700105
C	Cho, JW; Lee, SY		Kearns, MS; Solla, SA; Cohn, DA		Cho, JW; Lee, SY			Active noise canceling using analog neuro-chip with on-chip learning capability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORK	A modular analogue neuro-chip set with on-chip learning capability is developed for active noise canceling. The analogue neuro-chip set incorporates the error backpropagation learning rule for practical applications, and allows pin-to-pin interconnections for multi-chip boards. The developed neuro-board demonstrated active noise canceling without any digital signal processor. Multi-path fading of acoustic channels, random noise, and nonlinear distortion of the loud speaker are compensated by the adaptive learning circuits of the neuro-chips. Experimental results are reported for cancellation of car noise in real time.	Korea Adv Inst Sci & Technol, Computat & Neural Syst Lab, Dept Elect Engn, Yusong Gu, Taejon 305701, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Cho, JW (corresponding author), Korea Adv Inst Sci & Technol, Computat & Neural Syst Lab, Dept Elect Engn, Yusong Gu, 373-1 Kusong Dong, Taejon 305701, South Korea.							Allen P. E., 1987, CMOS ANALOG CIRCUIT; ALSPECTOR J, 1992, ADV NEUR IN, V4, P871; Cho IK, 1996, ECON THEORY, V7, P1, DOI 10.1007/BF01212179; CHO J, 1998, P INT JOINT C NEUR N, P581; CHOI JG, 1993, IEEE T NEURAL NETWOR, V4, P484, DOI 10.1109/72.217191; Choi YK, 1996, NEURAL PROCESS LETT, V4, P1, DOI 10.1007/BF00454840; ENOMOTO T, 1982, ELECTRON LETT, V18, P193, DOI 10.1049/el:19820133; GAO F, 1991, P ICASSP, P3589; MORIE T, 1994, IEEE J SOLID-ST CIRC, V29, P1086, DOI 10.1109/4.309904; VANDERSPIEGEL J, 1994, P ICNN, P1830; WATANABE T, 1993, IEEE T NEURAL NETWOR, V4, P387, DOI 10.1109/72.217179; WIDROW B, 1975, P IEEE, V63, P12	12	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						664	670						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700094
C	Darrell, T		Kearns, MS; Solla, SA; Cohn, DA		Darrell, T			Example based image synthesis of articulated figures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We present a method for learning complex appearance mappings, such as occur with images of articulated objects. Traditional interpolation networks fail on this case since appearance is not necessarily a smooth function nor a linear manifold for articulated objects. We define an appearance mapping from examples by constructing a set of independently smooth interpolation networks; these networks can cover overlapping regions of parameter space. A set growing procedure is used to find example clusters which are well-approximated within their convex hull; interpolation then proceeds only within these sets of examples. With this method physically valid images are produced even in regions of parameter space where nearby examples have different appearances. We show results generating both simulated and real arm images.	Interval Res, Palo Alto, CA 94304 USA		Darrell, T (corresponding author), Interval Res, 1801C Page Mill Rd, Palo Alto, CA 94304 USA.	trevor@interval.com						Beymer D, 1996, SCIENCE, V272, P1905, DOI 10.1126/science.272.5270.1905; BREGLER C, 1995, NIPS7; DARRELL T, 1998, P CVPR 98 SANTA BARB; Jagersand M, 1997, PROC CVPR IEEE, P1047, DOI 10.1109/CVPR.1997.609459; Jones MJ, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P683, DOI 10.1109/ICCV.1998.710791; LANITIS A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P368, DOI 10.1109/ICCV.1995.466919; Levoy M., 1996, Computer Graphics Proceedings. SIGGRAPH '96, P31, DOI 10.1145/237170.237199; McMillan L., 1995, Computer Graphics Proceedings. SIGGRAPH 95, P39, DOI 10.1145/218380.218398; Poggio T., 1989, 1140 MIT AI LAB; Rikert TD, 1998, AUTOMATIC FACE AND GESTURE RECOGNITION - THIRD IEEE INTERNATIONAL CONFERENCE PROCEEDINGS, P436, DOI 10.1109/AFGR.1998.670987; SAUL L, 1997, NIPS9 MIT; SEITZ SM, 1996, P SIGGRAPH 96, P21; SHASHUA A, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P920; TENENBAUM J, 1998, NIPS10 MIT	14	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						768	774						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700108
C	de Freitas, JFG; Doucet, A; Niranjan, M; Gee, AH		Kearns, MS; Solla, SA; Cohn, DA		de Freitas, JFG; Doucet, A; Niranjan, M; Gee, AH			Global optimisation of neural network models via sequential sampling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We propose a novel strategy for training neural networks using sequential sampling-importance resampling algorithms. This global optimisation strategy allows us to learn the probability distribution of the network weights in a sequential framework. It is well suited to applications involving on-line, nonlinear, non-Gaussian or non-stationary signal processing.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	de Freitas, JFG (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.			Niranjan, Mahesan/0000-0001-7021-140X; Doucet, Arnaud/0000-0002-7662-419X				CARPENTER J, 1997, IMPROVED PARTICLE FI; DEFREITAS JFG, 1998, ADV NEURAL INFORMATI, V10; DEFREITAS JFG, 1997, 307 CUEDFINFENGTR; DEFREITAS JFG, 1998, 328 CUEDFINFENGTR; Doucet A., 1998, 310 CUEDFINFENGTR; Singhal S., 1989, ADV NEURAL INFORM PR, P133	6	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						410	416						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700058
C	Edwards, RT; Cauwenberghs, G; Pineda, FJ		Kearns, MS; Solla, SA; Cohn, DA		Edwards, RT; Cauwenberghs, G; Pineda, FJ			Optimizing correlation algorithms for hardware-based transient classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The performance of dedicated VLSI neural processing hardware depends critically on the design of the implemented algorithms. We have previously proposed an algorithm for acoustic transient classification [1]. Having implemented and demonstrated this algorithm in a mixed-mode architecture, we now investigate variants on the algorithm, using time and frequency channel differencing, input and output normalization, and schemes to binarize and train the template values, with the goal of achieving optimal classification performance for the chosen hardware.	Johns Hopkins Univ, Baltimore, MD 21218 USA	Johns Hopkins University	Edwards, RT (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.							EDWARDS RT, 1997, INT S CIRC SYST ISCA; EDWARDS RT, 1998, INT S CIRC SYST ISCA; PINEDA FJ, 1996, NEURAL INFORMATION P; PINEDA FJ, 1995, WORLD C NEUR NETW WA; UNNIKRISHNAN KP, 1991, IEEE T SIGNAL PROCES, V39, P698, DOI 10.1109/78.80888; WANG KS, 1995, IEEE T SPEECH AUDI P, V3, P382, DOI 10.1109/89.466657	6	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						678	684						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700096
C	Hayashi, A; Suematsu, N		Kearns, MS; Solla, SA; Cohn, DA		Hayashi, A; Suematsu, N			Viewing classifier systems as model free learning in POMDPs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Classifier systems are now viewed disappointing because of their problems such as the rule strength vs rule set performance problem and the credit assignment problem. In order to solve the problems, we have developed a hybrid classifier system: GLS (Generalization Learning System). In designing GLS, we view CSs as model free learning in POMDPs and take a hybrid approach to finding the best generalization, given the total number of rules. GLS uses the policy improvement procedure by Jaakkola et al. for an locally optimal stochastic policy when a set of rule conditions is given. GLS uses GA to search for the best set of rule conditions.	Hiroshima City Univ, Fac Informat Sci, Asaminami Ku, Hiroshima 7313194, Japan		Hayashi, A (corresponding author), Hiroshima City Univ, Fac Informat Sci, Asaminami Ku, 3-4-1 Ozuka Higashi, Hiroshima 7313194, Japan.	akira@im.hiroshima-cu.ac.jp; suematsu@im.hiroshima-cu.ac.jp						De Jong K., 1988, Machine Learning, V3, P121, DOI 10.1023/A:1022606120092; Goldberg DE, 1989, GENETIC ALGORITHMS S; Grefenstette J. J., 1988, Machine Learning, V3, P225, DOI 10.1007/BF00113898; GREFENSTETTE JJ, 1990, MACH LEARN, V5, P355, DOI 10.1023/A:1022677607120; GREFENSTETTE JJ, 1987, P 2 INT C GEN ALG, P202; Holland J. H., 1978, PATTERN DIRECTED INF; Holland J. H., 1986, MACHINE LEARNING ART; Jaakkola T., 1994, ADV NEURAL INFORM PR, V7, P345; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; McCallum R. A., 1993, P 10 INT C MACH LEAR, P190, DOI DOI 10.1016/B978-1-55860-307-3.50031-9; WESTERDALE TH, 1997, P 2 ANN GEN PROGR C, P529; Wilson SW, 1995, EVOL COMPUT, V3, P149, DOI 10.1162/evco.1995.3.2.149; WILSON SW, 1989, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON GENETIC ALGORITHMS, P244	13	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						989	995						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700139
C	Ioffe, S; Forsyth, D		Kearns, MS; Solla, SA; Cohn, DA		Ioffe, S; Forsyth, D			Learning to find pictures of people	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Finding articulated objects, like people, in pictures presents a particularly difficult object recognition problem. We show how to find people by finding putative body segments, anti then constructing assemblies of those segments that are consistent with the constraints on the appearance of a person that, result from kinematic properties. Since a reasonable model of a person requires at least nine segments, it is not possible to present every group to a classifier. Instead, the search call be pruned by using projected versions of a classifier that accepts groups corresponding to people. We describe an efficient projection algorithm for one popular classifier, and demonstrate that our approach can be used to determine whether images of real scenes contain people.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Ioffe, S (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94720 USA.							BRADY JM, 1984, INT J ROB RES, V3; Enser P. G. B., 1993, Journal of Document and Text Management, V1, P25; Fleck M., 1996, EUR C COMP VIS, VIII, P592; FORSYTH DA, 1996, P 2 INT WORKSH OBJ R; FORSYTH DA, 1997, IEEE C COMP VIS PATT; FREUND Y, 1996, MACHINE LEARNING, V13; GRIMSON WEL, 1987, IEEE T PATTERN ANAL, V9, P469, DOI 10.1109/TPAMI.1987.4767935; HADDON J, 1997, IN PRESS INT C COMP; Rowley HA, 1996, ADV NEUR IN, V8, P875; SHUPPAN E, 1993, POSE FILE, V1; SUNG KK, 1994, 1521 MIT AI MEM	11	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						782	788						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700110
C	Li, ZP; Dayan, P		Kearns, MS; Solla, SA; Cohn, DA		Li, ZP; Dayan, P			Computational differences between asymmetrical and symmetrical networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				VISUAL-CORTEX; NEURAL NETWORKS; OSCILLATORY RESPONSES; MODEL; ORIENTATION; SELECTIVITY; INTEGRATION; BRAIN; CAT	Symmetrically connected recurrent networks have recently been used as models of a host of neural computations. However, because of the separation between excitation and inhibition, biological neural networks are asymmetrical. We study characteristic differences between asymmetrical networks and their symmetrical counterparts, showing that they have dramatically different dynamical behavior and also how the differences can be exploited for computational ends. We illustrate our results in the case of a network that is a selective amplifier.	Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Li, ZP (corresponding author), Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.	zhaoping@gatsby.ucl.ac.uk; dayan@gatsby.ucl.ac.uk						BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; Braun Jochen, 1994, Society for Neuroscience Abstracts, V20, P1665; Carandini M, 1997, VISION RES, V37, P3061, DOI 10.1016/S0042-6989(97)00100-4; COHEN MA, 1983, IEEE T SYST MAN CYB, V13, P815, DOI 10.1109/TSMC.1983.6313075; ECKHORN R, 1988, BIOL CYBERN, V60, P121, DOI 10.1007/BF00202899; ERMENTROUT GB, 1979, J MATH BIOL, V7, P265, DOI 10.1007/BF00275728; Golomb D, 1996, J NEUROPHYSIOL, V75, P750, DOI 10.1152/jn.1996.75.2.750; GRAY CM, 1989, NATURE, V338, P334, DOI 10.1038/338334a0; GROSSBERG S, 1988, NEURAL NETWORKS, V1, P17, DOI 10.1016/0893-6080(88)90021-4; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; KONIG P, 1992, NEURAL COMPUT, V4, P666, DOI 10.1162/neco.1992.4.5.666; LI Z, 1989, BIOL CYBERN, V61, P379, DOI 10.1007/BF00200803; LI Z, 1999, IN PRESS NETWORK COM; LI Z, 1997, THEORETICAL ASPECTS; LI Z, 1995, MODELS NEURAL NETWOR, V2; Li ZP, 1998, NEURAL COMPUT, V10, P903, DOI 10.1162/089976698300017557; Pouget A, 1998, NEURAL COMPUT, V10, P373, DOI 10.1162/089976698300017809; Samsonovich A, 1997, J NEUROSCI, V17, P5900; Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339; SEUNG HS, 1998, NIPS, V10; SOMPOLINSKY H, 1990, P NATL ACAD SCI USA, V87, P7200, DOI 10.1073/pnas.87.18.7200; Stein P. S. G., 1997, NEURONS NETWORKS MOT; STERIADE M, 1993, SCIENCE, V262, P679, DOI 10.1126/science.8235588; SUAREZ H, 1995, J NEUROSCI, V15, P6700; VONDERMALSBURG C, 1988, NEURAL NETWORKS, V1, P141, DOI 10.1016/0893-6080(88)90016-0; Zhang K, 1996, J NEUROSCI, V16, P2112	27	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						274	280						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700039
C	Loch, J		Kearns, MS; Solla, SA; Cohn, DA		Loch, J			The effect of eligibility traces on finding optimal memoryless policies in partially observable Markov decision processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Agents acting in the real world are confronted with the problem of making good decisions with limited knowledge of the environment. Partially observable Markov decision processes (POMDPs) model decision problems in which an agent tries to maximize its reward in the face of limited sensor feedback. Recent work has shown empirically that a reinforcement learning (RL) algorithm called Sarsa(lambda) can efficiently find optimal memoryless policies, which map current observations to actions, for POMDP problems (Loch and Singh 1998). The Sarsa(lambda) algorithm uses a form of short-term memory called an eligibility trace, which distributes temporally delayed rewards to observation-action pairs which lead up to the reward. This paper explores the effect of eligibility traces on the ability of the Sarsa(lambda) algorithm to find optimal memoryless policies. A variant of Sarsa(lambda) called k-step truncated Sarsa(lambda) is applied to four test problems taken from the recent work of Littman, Littman, Cassandra and Kaelbling, Parr and Russell, and Chrisman. The empirical results show that eligibility traces can be significantly truncated without affecting the ability of Sarsa(lambda) to find optimal memoryless policies for POMDPs.	Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Loch, J (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.							Cassandra A.R., 1994, CS9414 BROWN U; Littman M, 1994, CS9440 BROWN U DEP C; LITTMAN M, 1994, ANIMALS ANIMALS, V3; LITTMAN ML, 1995, P 12 INT C MACH LEAR, P362; LOCH J, 1998, IN PRESS P 15 INT C; Lovejoy WS, 1991, ANN OPER RES, V28, P47, DOI 10.1007/BF02055574; PARR R, 1995, P INT JOINT C ART IN; SONDIK EJ, 1978, OPER RES, P26; SUTTON RS, 1990, P 7 INT C MACH LEARN, P216	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						1010	1016						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700142
C	Maass, W; Sontag, ED		Kearns, MS; Solla, SA; Cohn, DA		Maass, W; Sontag, ED			A precise characterization of the class of languages recognized by neural nets under Gaussian and other common noise distributions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS	We consider recurrent analog neural nets where each gate is subject to Gaussian noise, or any other common noise distribution whose probability density function is nonzero on a large set. We show that many regular languages cannot be recognized by networks of this type, for example the language {w is an element of {0, 1}*\ w begins with 0}, and we give a precise characterization of those languages which can be recognized. This result implies severe constraints on possibilities for constructing recurrent analog neural nets that are robust against realistic types of analog noise. On the other hand we present a method for constructing feedforward analog neural nets that are robust with regard to analog noise of this type.	Graz Tech Univ, Inst Theoret Comp Sci, A-8010 Graz, Austria	Graz University of Technology	Maass, W (corresponding author), Graz Tech Univ, Inst Theoret Comp Sci, Klosterwiesgasse 32-2, A-8010 Graz, Austria.		Sontag, Eduardo D/J-4420-2012	Sontag, Eduardo D/0000-0001-8020-5783				Casey M, 1996, NEURAL COMPUT, V8, P1135, DOI 10.1162/neco.1996.8.6.1135; Doeblin W., 1937, B MATH SOC ROUMAINE, V39, P57; Maass W, 1998, NEURAL COMPUT, V10, P1071, DOI 10.1162/089976698300017359; Omlin CW, 1996, J ACM, V43, P937, DOI 10.1145/235809.235811; PAPINICOLAOU G, 1978, STUDIES PROBABILITY, V18, P111; PIPPENGER N, 1989, J ACM, V36, P531, DOI 10.1145/65950.77248; PIPPENGER N, 1985, IEEE S FDN COMP SCI, V26, P30; Pippenger N., 1990, P S PURE MATH, V50, P311; RABIN MO, 1963, INFORM CONTROL, V6, P230, DOI 10.1016/S0019-9958(63)90290-0	9	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						281	287						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700040
C	Neukirchen, C; Rigoll, G		Kearns, MS; Solla, SA; Cohn, DA		Neukirchen, C; Rigoll, G			Controlling the complexity of HMM systems by regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				SPEECH RECOGNITION; NEURAL NETWORKS	This paper introduces a method for regularization of HMM systems that avoids parameter overfitting caused by insufficient training data. Regularization is done by augmenting the EM training method by a penalty term that favors simple and smooth HMM systems. The penalty term is constructed as a mixture model of negative exponential distributions that is assumed to generate the state dependent emission probabilities of the HMMs. This new method is the successful transfer of a well known regularization approach in neural networks to the HMM domain and can be interpreted as a generalization of traditional state-tying for HMM systems. The effect of regularization is demonstrated for continuous speech recognition tasks by improving overfitted triphone models and by speaker adaptation with limited training data.	Gerhard Mercator Univ Duisburg, Dept Comp Sci, D-47057 Duisburg, Germany	University of Duisburg Essen	Neukirchen, C (corresponding author), Gerhard Mercator Univ Duisburg, Dept Comp Sci, D-47057 Duisburg, Germany.	chn@fb9-ti.uni-duisburg.de; rigoll@fb9-ti.uni-duisburg.de						Bahl L., 1991, P DARPA SPEECH NAT L, P264; BAHL LR, 1983, IEEE T PATTERN ANAL, V5, P179, DOI 10.1109/TPAMI.1983.4767370; Baum EB, 1989, NEURAL COMPUT, V1, P151, DOI 10.1162/neco.1989.1.1.151; Gauvain JL, 1994, IEEE T SPEECH AUDI P, V2, P291, DOI 10.1109/89.279278; LANG KJ, 1990, NEURAL NETWORKS, V3, P23, DOI 10.1016/0893-6080(90)90044-L; LECUN Y, 1990, ADV NEURAL INFORMATI, V2; Neukirchen C, 1998, INT CONF ACOUST SPEE, P5, DOI 10.1109/ICASSP.1998.674353; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Plaut D.C, 1986, CMUCS86126; Rigoll G, 1997, ADV NEUR IN, V9, P772; RIGOLL G, 1996, P ICASSP 96, P865; Rottland J, 1998, INT CONF ACOUST SPEE, P465, DOI 10.1109/ICASSP.1998.674468; YOUNG SJ, 1993, P EUROSPEECH, P2203; YOUNG SJ, 1992, P IEEE INT C AC SPEE, V1, P569	14	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						737	743						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700104
C	Rosenholtz, R		Kearns, MS; Solla, SA; Cohn, DA		Rosenholtz, R			General-purpose localization of textured image regions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BLOCK SIZE SEGMENTATION	We suggest a working definition of texture: Texture is stuff that is more compactly represented by its statistics than by specifying the configuration of its parts. This definition suggests that to find texture we look for outliers to the local statistics, and label as texture the regions with no outliers. We present a method, based upon this idea, for labeling points in natural scenes as belonging to texture regions, while simultaneously allowing us to label low-level, bottom-up cues for visual attention. This method is based upon recent psychophysics results on processing of texture and popout.	Xerox PARC, Palo Alto, CA 94304 USA		Rosenholtz, R (corresponding author), Xerox PARC, 3333 Coyote Hill Rd, Palo Alto, CA 94304 USA.			Rosenholtz, Ruth/0000-0001-5299-0331				BERGEN JR, 1991, COMPUTATIONAL MODELS, P252; DUNCAN J, 1989, PSYCHOL REV, V96, P433, DOI 10.1037/0033-295X.96.3.433; FORSYTH D, 1996, ECCV WORKSH OBJ REPR; KINGDOM FAA, 1995, VISION RES, V35, P79, DOI 10.1016/0042-6989(94)E0079-Z; KINGDOM FAA, 1997, INVEST OPHTHALMOL, V38, P636; LEUNG TK, 1996, P 4 EUR C COMP VIS, V1064, P546; MILANESE R, 1993, P IEEE CVPR, P781; ROSENHOLTZ R, 1998, INVEST OPHTH VIS SCI, V39, P629; ROSENHOLTZ R, 1998, INVEST OPHTH VIS SCI, V38, P687; SAUND E, 1998, SCALE SHAPE TEXTURE; SHI J, 1998, P 5 EUR C COMP VIS, V1406, P528; VAISEY J, 1992, IEEE T SIGNAL PROCES, V40, P2040, DOI 10.1109/78.150005; Wolfe Jeremy M, 2010, Curr Biol, V20, pR346, DOI 10.1016/j.cub.2010.02.016; Won CS, 1997, OPT ENG, V36, P2204, DOI 10.1117/1.601441	14	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						817	823						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700115
C	Saul, L; Rahim, M		Kearns, MS; Solla, SA; Cohn, DA		Saul, L; Rahim, M			Markov processes on curves for automatic speech recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					We investigate a probabilistic framework for automatic speech recognition based on the intrinsic geometric properties of curves. In particular, we analyze the setting in which two variables-one continuous (x), one discrete (s)-evolve jointly in time. We suppose that the vector x traces out a smooth multidimensional curve and that the variable s evolves stochastically as a function of the are length traversed along this curve. Since are length does not depend on the rate at which a curve is traversed, this gives rise to a family of Markov processes whose predictions, Pr[s \ x], are invariant to nonlinear warpings of time. We describe the use of such models, known as Markov processes on curves (MPCs), for automatic speech recognition, where x are acoustic feature trajectories and s are phonetic transcriptions. On two tasks-recognizing New Jersey town names and connected alpha-digits-we find that MPCs yield lower word error rates than comparably trained hidden Markov models.	AT&T Labs Res, Shannon Lab, Florham Park, NJ 07932 USA	AT&T	Saul, L (corresponding author), AT&T Labs Res, Shannon Lab, 180 Pk Ave,E-171, Florham Park, NJ 07932 USA.							Do Carmo M.P., 2016, DIFFERENTIAL GEOMETR, Vsecond; Ostendorf M, 1996, IEEE T SPEECH AUDI P, V4, P360, DOI 10.1109/89.536930; Rabiner L., 1993, FUNDAMENTALS SPEECH; SACHS R, 1994, UNPUB AT T; SAUL L, 1998, P 15 INT C MACH LEAR, P506; TISHBY N, 1990, P IEEE INT C AC SPEE, P365; [No title captured]	7	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						751	757						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700106
C	Stemmler, M; Koch, C		Kearns, MS; Solla, SA; Cohn, DA		Stemmler, M; Koch, C			Information maximization in single neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				PYRAMIDAL NEURONS	Information from the senses must be compressed into the limited range of firing rates generated by spiking nerve cells. Optimal compression uses all firing rates equally often, implying that the nerve cell's response matches the statistics of naturally occurring stimuli. Since changing the voltage-dependent ionic conductances in the cell membrane alters the flow of information, an unsupervised, non-Hebbian, developmental learning rule is derived to adapt the conductances in Hodgkin-Huxley model neurons. By maximizing the rate of information transmission, each firing rate within the model neuron's limited dynamic range is used equally often.	CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA	California Institute of Technology	Stemmler, M (corresponding author), CALTECH, Computat & Neural Syst Program, 139-74, Pasadena, CA 91125 USA.			Koch, Christof/0000-0001-6482-8067				Aticky JJ, 2011, NETWORK-COMP NEURAL, V22, P4, DOI 10.3109/0954898X.2011.638888; Avery RB, 1996, J NEUROSCI, V16, P5567; BADDELEY RJ, 1991, P ROY SOC B-BIOL SCI, V246, P219, DOI 10.1098/rspb.1991.0147; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; BELL AJ, 1992, NEURAL INFORMATION P, V4, P59; CONNOR JA, 1977, BIOPHYS J, V18, P81, DOI 10.1016/S0006-3495(77)85598-7; GU XN, 1995, NATURE, V375, P784, DOI 10.1038/375784a0; Hebb D., 1949, ORG BEHAV; Helmchen F, 1996, BIOPHYS J, V70, P1069, DOI 10.1016/S0006-3495(96)79653-4; HOFMANN F, 1994, ANNU REV NEUROSCI, V17, P399, DOI 10.1146/annurev.neuro.17.1.399; LAUGHLIN S, 1981, Z NATURFORSCH C, V36, P910; LEMASSON G, 1993, SCIENCE, V259, P1915, DOI 10.1126/science.8456317; LINSKER R, 1992, NEURAL COMPUT, V4, P691, DOI 10.1162/neco.1992.4.5.691; Purves D, 1994, NEURAL ACTIVITY GROW; RUDERMAN DL, 1995, NETWORK, V5, P517; Smirnakis SM, 1997, NATURE, V386, P69, DOI 10.1038/386069a0; STEIN RB, 1967, BIOPHYS J, V7, P797, DOI 10.1016/S0006-3495(67)86623-2; Tsypkin Y, 1971, ADAPTATION LEARNING	18	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						160	166						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700023
C	Thornber, KK; Williams, LR		Kearns, MS; Solla, SA; Cohn, DA		Thornber, KK; Williams, LR			Orientation, scale, and discontinuity as emergent properties of illusory contour shape	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				STOCHASTIC COMPLETION FIELDS	A recent neural model of illusory contour formation is based on a distribution of natural shapes traced by particles moving with constant speed in directions given by Brownian motions. The input to that model consists of pairs of position and direction constraints and the output consists of the distribution of contours joining all such pairs. In general, these contours will not be closed and their distribution will not be scale-invariant. In this paper, we show how to compute a scale-invariant distribution of closed contours given position constraints alone and use this result to explain a well known illusory contour effect.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Thornber, KK (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							HORN RA, 1985, MATRIX ANAL, P500; KOVACS I, 1993, P NATL ACAD SCI USA, V90, P7495, DOI 10.1073/pnas.90.16.7495; MUMFORD D., 1993, ALGEBRAIC GEOMETRY I; SAMBIN M, 1974, ITALIAN J PSYCHOL, V1, P355; Thornber KK, 1996, BIOL CYBERN, V75, P141, DOI 10.1007/s004220050282; THORNBER KK, 1997, INT WORKSH EN MIN ME; Williams LR, 1997, NEURAL COMPUT, V9, P859, DOI 10.1162/neco.1997.9.4.859	7	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						831	837						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700117
C	Baird, B		Jordan, MI; Kearns, MJ; Solla, SA		Baird, B			Synchronized auditory and cognitive 40 Hz attentional streams and the impact of rhythmic expectation on auditory scene analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We have developed a neural network architecture that implements a theory of attention, learning, and trans-cortical communication based on adaptive synchronization of 5-15 Hz and 30-80 Hz oscillations between cortical areas. Here we present a specific higher order cortical model of attentional networks, rhythmic expectancy, and the interaction of higher-order and primary cortical levels of processing. It accounts for the mismatch negativity" of the auditory ERP and the results of psychological experiments of Jones showing that auditory stream segregation depends on the rhythmic structure of inputs. The timing mechanisms of the model allow us to explain how relative timing information such as the relative order of events between streams is lost when streams are formed. The model suggests how the theories of auditory perception and attention of Jones and Bregman may be reconciled.	Univ Calif Berkeley, Dept Math, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Baird, B (corresponding author), Univ Calif Berkeley, Dept Math, Berkeley, CA 94720 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						3	9						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700001
C	Baluja, S		Jordan, MI; Kearns, MJ; Solla, SA		Baluja, S			Using expectation to guide processing: A study of three real-world applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In many real world tasks, only a small fraction of the available inputs are important at any particular time. This paper presents a method for ascertaining the relevance of inputs by exploiting temporal coherence and predictability. The method proposed in this paper dynamically allocates relevance to inputs by using expectations of their future values. As a model of the task is learned, the model is simultaneously extended to create task-specific predictions of the future values of inputs. Inputs which are either not relevant, and therefore not accounted for in the model, or those which contain noise, will not be predicted accurately These inputs can be de-emphasized, and, in turn, a new, improved, model of the task created. The techniques presented in this paper have yielded significant improvements for the vision-based autonomous control of a land vehicle, vision-based hand tracking in cluttered scenes, and the detection of faults in the etching of semiconductor wafers.	Carnegie Mellon Univ, Justsyst Pittsburgh Res Ctr, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Baluja, S (corresponding author), Carnegie Mellon Univ, Justsyst Pittsburgh Res Ctr, Pittsburgh, PA 15213 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						859	865						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700121
C	Cataltepe, Z; Magdon-Ismail, M		Jordan, MI; Kearns, MJ; Solla, SA		Cataltepe, Z; Magdon-Ismail, M			Incorporating test inputs into learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In many applications, such as credit default prediction and medical image recognition, test inputs are available in addition to the labeled training examples. We propose a method to incorporate the test inputs into learning. Our method results in solutions having smaller test errors than that of simple training solution, especially for noisy problems or small training sets.	CALTECH, Learning Syst Grp, Dept Comp Sci, Pasadena, CA 91125 USA	California Institute of Technology	Cataltepe, Z (corresponding author), CALTECH, Learning Syst Grp, Dept Comp Sci, Pasadena, CA 91125 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						437	443						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700062
C	Chien, S; Stechert, A; Mutz, D		Jordan, MI; Kearns, MJ; Solla, SA		Chien, S; Stechert, A; Mutz, D			On efficient heuristic ranking of hypotheses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper considers the problem of learning the ranking of a set of alternatives based upon incomplete information (e.g., a limited number of observations). We describe two algorithms for hypothesis ranking and their application for probably approximately correct (PAC) and expected loss (EL) learning criteria. Empirical results are provided to demonstrate the effectiveness of these ranking procedures on both synthetic datasets and real-world data from a spacecraft design optimization problem.	CALTECH, Jet Prop Lab, Pasadena, CA 91109 USA	California Institute of Technology; National Aeronautics & Space Administration (NASA); NASA Jet Propulsion Laboratory (JPL)	Chien, S (corresponding author), CALTECH, Jet Prop Lab, 4800 Oak Grove Dr,M-S 525-3660, Pasadena, CA 91109 USA.								0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						444	450						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700063
C	Feraud, R; Bernier, O		Jordan, MI; Kearns, MJ; Solla, SA		Feraud, R; Bernier, O			Ensemble and modular approaches for face detection: a comparison	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A new learning model based on autoassociative neural networks is developped and applied to face detection. To extend the detection ability in orientation and to decrease the number of false alarms, different combinations of networks are tested: ensemble, conditional ensemble and conditional mixture of networks. The use of a conditional mixture of networks allows to obtain state of the art results on different benchmark face databases.	France Telecom, CNET, DTL, DLI, F-22307 Lannion, France	Orange SA	Feraud, R (corresponding author), France Telecom, CNET, DTL, DLI, Technopole Anticipa,2 Ave Pierre Marzin, F-22307 Lannion, France.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						472	478						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700067
C	Foster, DJ; Morris, RGM; Dayan, P		Jordan, MI; Kearns, MJ; Solla, SA		Foster, DJ; Morris, RGM; Dayan, P			Hippocampal model of rat spatial abilities using temporal difference learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We provide a model of the standard watermaze task, and of a more challenging task involving novel platform locations, in which rats exhibit one-trial learning after a few days of training. The model uses hippocampal place cells to support reinforcement learning, and also, in an integrated manner, to build and use allocentric coordinates.	Univ Edinburgh, Ctr Neurosci, Edinburgh EH8 9LE, Midlothian, Scotland	University of Edinburgh	Foster, DJ (corresponding author), Univ Edinburgh, Ctr Neurosci, Crichton St, Edinburgh EH8 9LE, Midlothian, Scotland.		Morris, Richard G M/C-9982-2013						0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						145	151						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700021
C	Freeman, WT; Viola, PA		Jordan, MI; Kearns, MJ; Solla, SA		Freeman, WT; Viola, PA			Bayesian model of surface perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Image intensity variations can result from several different object surface effects, including shading from 3-dimensional relief of the object, or paint on the surface itself. An essential problem in vision, which people solve naturally, is to attribute the proper physical cause, e.g. surface relief or paint, to an observed image. We addressed this problem with an approach combining psychophysical and Bayesian computational methods. We assessed human performance on a set of test images, and found that people made fairly consistent judgements of surface properties. Our computational model assigned simple prior probabilities to different relief or paint explanations for an image, and solved for the most probable interpretation in a Bayesian framework. The ratings of the test images by our algorithm compared surprisingly well with the mean ratings of our subjects.	MERL, Mitsubishi Elect Res Lab, Cambridge, MA 02139 USA		Freeman, WT (corresponding author), MERL, Mitsubishi Elect Res Lab, 201 Broadway, Cambridge, MA 02139 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						787	793						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700111
C	Geiger, D; Rudra, A; Maloney, L		Jordan, MI; Kearns, MJ; Solla, SA		Geiger, D; Rudra, A; Maloney, L			Features as sufficient statistics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					An image is often represented by a set of detected features. We get an enormous compression by representing images in this way. Furthermore, we get a representation which is little affected by small amounts of noise in the image. However, features are typically chosen in an ad hoc manner. Mie show how a good set of features can be obtained using sufficient statistics. The idea of sparse data representation naturally arises. We treat the 1-dimensional and 2-dimensional signal reconstruction problem to make our ideas concrete.	Courant Inst, Dept Comp Sci, New York, NY USA		Geiger, D (corresponding author), Courant Inst, Dept Comp Sci, New York, NY USA.								0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						794	800						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700112
C	Levy, N; Horn, D; Ruppin, E		Jordan, MI; Kearns, MJ; Solla, SA		Levy, N; Horn, D; Ruppin, E			Multi-modular associative memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Motivated by the findings of modular structure in the association cortex, we study a multi-modular model of associative memory that can successfully store memory patterns with different levels of activity. We show that the segregation of synaptic conductances into intra-modular linear and inter-modular nonlinear ones considerably enhances the network's memory retrieval performance. Compared with the conventional, single-module associative memory network, the multi-modular network has two main advantages: It is less susceptible to damage to columnar input, and its response is consistent with the cognitive data pertaining to category specific impairment.	Tel Aviv Univ, Sch Phys & Astron, IL-69978 Tel Aviv, Israel	Tel Aviv University	Levy, N (corresponding author), Tel Aviv Univ, Sch Phys & Astron, IL-69978 Tel Aviv, Israel.		Ruppin, Eytan/R-9698-2017	Ruppin, Eytan/0000-0002-7862-3940; Horn, David/0000-0003-2708-186X					0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						52	58						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700008
C	Movellan, J; Mineiro, P		Jordan, MI; Kearns, MJ; Solla, SA		Movellan, J; Mineiro, P			Bayesian robustification for audio visual fusion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We discuss the problem of catastrophic fusion in multimodal recognition systems. This problem arises in systems that need to fuse different channels in non-stationary environments. Practice shows that when recognition modules within each modality are tested in contexts inconsistent with their assumptions, their influence on the fused product tends to increase, with catastrophic results. We explore a principled solution to this problem based upon Bayesian ideas of competitive models and inference robustification: each sensory channel is provided with simple white-noise context models, and the perceptual hypothesis and context are jointly estimated. Consequently, context deviations are interpreted as changes in white noise contamination strength, automatically adjusting the influence of the module. The approach is tested on a fixed lexicon automatic audiovisual speech recognition problem with very good results.	Univ Calif San Diego, Dept Cognit Sci, San Diego, CA 92092 USA	University of California System; University of California San Diego	Movellan, J (corresponding author), Univ Calif San Diego, Dept Cognit Sci, San Diego, CA 92092 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						742	748						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700105
C	Priel, A; Kanter, I; Kessler, DA		Jordan, MI; Kearns, MJ; Solla, SA		Priel, A; Kanter, I; Kessler, DA			Analytical study of the interplay between architecture and predictability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We study model feed forward networks as time series predictors in the stationary limit. The focus is on complex, yet non-chaotic, behavior. The main question we address is whether the asymptotic behavior is governed by the architecture, regardless the details of the weights. We find hierarchies among classes of architectures with respect to the attractor dimension of the long term sequence they are capable of generating; larger number of hidden units can generate higher dimensional attractors. In the case of a perceptron, we develop the stationary solution for general weights, and show that the flow is typically one dimensional. The relaxation time from an arbitrary initial condition to the stationary solution is found to scale linearly with the size of the network. In multilayer networks, the number of hidden units gives bounds on the number and dimension of the possible attractors. We conclude that long term prediction (in the non-chaotic regime) with such models is governed by attractor dynamics related to the architecture.	Bar Ilan Univ, Minerva Ctr, IL-52900 Ramat Gan, Israel	Bar Ilan University	Priel, A (corresponding author), Bar Ilan Univ, Minerva Ctr, IL-52900 Ramat Gan, Israel.			Priel, Avner/0000-0002-4620-6376					0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						315	321						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700045
C	Sollich, P; Barber, D		Jordan, MI; Kearns, MJ; Solla, SA		Sollich, P; Barber, D			Online learning from finite training sets in nonlinear networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Online learning is one of the most common forms of neural network training. We present an analysis of online learning from finite training sets for non-linear networks (namely, soft-committee machines), advancing the theory to more realistic learning scenarios. Dynamical equations are derived for an appropriate set of order parameters; these are exact in the limiting case of either linear networks or infinite training sets. Preliminary comparisons with simulations suggest that the theory captures some effects of finite training sets, but may not yet account correctly for the presence of local minima.	Univ Edinburgh, Dept Phys, Edinburgh EH9 3JZ, Midlothian, Scotland	University of Edinburgh	Sollich, P (corresponding author), Univ Edinburgh, Dept Phys, Mayfield Rd, Edinburgh EH9 3JZ, Midlothian, Scotland.		Sollich, Peter/H-2174-2011; Sollich, Peter/ABC-2993-2020	Sollich, Peter/0000-0003-0169-7893; 					0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						357	363						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700051
C	Song, XB; Abu-Mostafa, Y; Sill, J; Kasdan, H		Jordan, MI; Kearns, MJ; Solla, SA		Song, XB; Abu-Mostafa, Y; Sill, J; Kasdan, H			Incorporating contextual information in white blood cell identification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this paper we propose a technique to incorporate contextual information into object classification. In the real world there are cases where the identity of an object is ambiguous due to the noise in the measurements based on which the classification should be made. It is helpful to reduce the ambiguity by utilizing extra information referred to as context, which in our case is the identities of the accompanying objects. This technique is applied to white blood cell classification. Comparisons are made against "no context" approach, which demonstrates the superior classification performance achieved by using context. In our particular application, it significantly reduces false alarm rate and thus greatly reduces the cost due to expensive clinical tests.	CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA	California Institute of Technology	Song, XB (corresponding author), CALTECH, Dept Elect Engn, Pasadena, CA 91125 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						950	956						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700134
C	Spangler, RR; Goodman, RM; Hawkins, J		Jordan, MI; Kearns, MJ; Solla, SA		Spangler, RR; Goodman, RM; Hawkins, J			Bach in a box - Real-time harmony	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We describe a system for learning J. S. Bach's rules of musical harmony. These rules are learned from examples and are expressed as rule-based neural networks. The rules are then applied in realtime to generate new accompanying harmony for a live performer. Real-time functionality imposes constraints on the learning and harmonizing processes, including limitations on the types of information the system can use as input and the amount of processing the system can perform. We demonstrate algorithms for generating and refining musical rules from examples which meet these constraints. We describe a method for including a priori knowledge into the rules which yields significant performance gains. We then describe techniques for applying these rules to generate new music in real-time. We conclude the paper with an analysis of experimental results.	CALTECH, Pasadena, CA 91125 USA	California Institute of Technology	Spangler, RR (corresponding author), CALTECH, 136-93, Pasadena, CA 91125 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						957	963						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700135
C	Sykacek, P; Dorffner, G; Rappelsberger, P; Zeitlhofer, J		Jordan, MI; Kearns, MJ; Solla, SA		Sykacek, P; Dorffner, G; Rappelsberger, P; Zeitlhofer, J			Experiences with Bayesian learning in a real world application	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					This paper reports about an application of Bayes' inferred neural network classifiers in the field of automatic sleep staging. The reason for using Bayesian learning for this task is two-fold. First, Bayesian inference is known to embody regularization automatically. Second, a side effect of Bayesian learning leads to larger variance of network outputs in regions without training data. This results in well known moderation effects, which can be used to detect outliers. In a 5 fold cross-validation experiment the full Bayesian solution found with R. Neals hybrid Monte Carlo algorithm, was not better than a single maximum a-posteriori (MAP) solution found with D.J. MacKay's evidence approximation. In a second experiment we studied the properties of both solutions in rejecting classification of movement artefacts.	Austrian Res Inst Artificial Intelligence, A-1010 Vienna, Austria		Sykacek, P (corresponding author), Austrian Res Inst Artificial Intelligence, Schottengasse 3, A-1010 Vienna, Austria.		Dorffner, Georg/AAQ-1455-2020	Dorffner, Georg/0000-0002-3181-2576					0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						964	970						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700136
C	Yang, HH		Jordan, MI; Kearns, MJ; Solla, SA		Yang, HH			Multiplicative updating rule for blind separation derived from the method of scoring	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					For blind source separation, when the Fisher information matrix is used as the Riemannian metric tensor for the parameter space, the steepest descent algorithm to maximize the likelihood function in this Riemannian parameter space becomes the serial updating rule with equivariant property. This algorithm can be further simplified by using the asymptotic form of the Fisher information matrix around the equilibrium.	Oregon Grad Inst, Dept Comp Sci, Portland, OR 97291 USA		Yang, HH (corresponding author), Oregon Grad Inst, Dept Comp Sci, POB 91000, Portland, OR 97291 USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						696	702						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700099
C	Barber, D; Bishop, CM		Mozer, MC; Jordan, MI; Petsche, T		Barber, D; Bishop, CM			Bayesian model comparison by Monte Carlo chaining	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The techniques of Bayesian inference have been applied with great success to many problems in neural computing including evaluation of regression functions, determination of error bars on predictions, and the treatment of hyper-parameters. However, tile problem of model comparison is a much more challenging one for which current techniques have significant limitations. In this paper we show how an extended form of Markov chain Monte Carlo, called chaining, is able to provide effective estimates of the relative probabilities of different models. We present results from the robot arm problem and compare them with the corresponding results obtained using the standard Gaussian approximation framework.			Barber, D (corresponding author), ASTON UNIV,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	1	1	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						333	339						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00047
C	EtienneCummings, R; VanderSpiegel, J; Takahashi, N; Apsel, A; Mueller, P		Mozer, MC; Jordan, MI; Petsche, T		EtienneCummings, R; VanderSpiegel, J; Takahashi, N; Apsel, A; Mueller, P			VLSI implementation of cortical visual motion detection using an analog neural computer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Two dimensional image motion detection neural networks have been implemented using a general purpose analog neural computer. The neural circuits perform spatiotemporal feature extraction based on the cortical motion detection model of Adelson and Bergen. The neural computer provides the neurons, synapses and synaptic time-constants required to realize the model in VLSI hardware. Results show that visual motion estimation can be implemented with simple sum-and-threshold neural hardware with temporal computational capabilities. The neural circuits compute general 2D visual motion in real-time.			EtienneCummings, R (corresponding author), SO ILLINOIS UNIV,CARBONDALE,IL 62901, USA.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						685	691						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00097
C	Iizuka, K; Miyamoto, M; Matsui, H		Mozer, MC; Jordan, MI; Petsche, T		Iizuka, K; Miyamoto, M; Matsui, H			Dynamically adaptable CMOS Winner-Take-All neural network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The major problem that has prevented practical application of analog neuro-LSIs has been poor accuracy due to fluctuating analog device characteristics inherent in each device as a result of manufacturing. This paper proposes a dynamic control architecture that allows analog silicon neural networks to compensate for the fluctuating device characteristics and adapt to a change in input DC level. We have applied this architecture to compensate for input offset voltages of an analog CMOS WTA (Winner-Take-All) chip that we have fabricated. Experimental data show the effectiveness of the architecture.			Iizuka, K (corresponding author), SHARP CO LTD,INFORMAT TECHNOL RES LABS,TENRI,NARA 632,JAPAN.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						713	719						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00101
C	Mathieson, M		Mozer, MC; Jordan, MI; Petsche, T		Mathieson, M			Ordered classes and incomplete examples in classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The classes in classification tasks often have a natural ordering, and the training and testing examples are often incomplete. We propose a nonlinear ordinal model for classification into ordered classes. Predictive, simulation-based approaches are used to learn from past and classify future incomplete examples. These techniques are illustrated by making prognoses for patients who have suffered severe head injuries.			Mathieson, M (corresponding author), UNIV OXFORD,DEPT STAT,1 S PARKS RD,OXFORD OX1 3TG,ENGLAND.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						550	556						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00078
C	Meila, M; Jordan, MI		Mozer, MC; Jordan, MI; Petsche, T		Meila, M; Jordan, MI			Triangulation by continuous embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				When triangulating a belief network we aim to obtain a junction tree of minimum state space. According to (Rose, 1970), searching for the optimal triangulation can be cast as a search over all the permutations of the graph's vertices. Our approach is to embed the discrete set of permutations in a convex continuous domain D. By suitably extending the cost function over D and solving the continous nonlinear optimization task we hope to obtain a good triangulation with respect to the aformentioned cost. This paper presents two ways of embedding the triangulation problem into continuous domain and shows that they perform well compared to the best known heuristic.			Meila, M (corresponding author), MIT,CTR BIOL & COMPUTAT LEARNING,45 CARLETON ST E25-201,CAMBRIDGE,MA 02142, USA.		Jordan, Michael I/C-5253-2013						0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						557	563						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00079
C	Rigoll, G; Neukirchen, C		Mozer, MC; Jordan, MI; Petsche, T		Rigoll, G; Neukirchen, C			A new approach to hybrid HMM/ANN speech recognition using mutual information neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				This paper presents a new approach to speech recognition with hybrid HMM/ANN technology. While the standard approach to hybrid HMM/ANN systems is based on the use of neural networks as posterior probability estimators, the new approach is based on the use of mutual information neural networks trained with a special learning algorithm in order to maximize the mutual information between the input classes of the network and its resulting sequence of firing output neurons during training. It is shown in this paper that such a neural network is an optimal neural vector quantizer for a discrete hidden Markov model system trained on Maximum Likelihood principles. One of the main advantages of this approach is the fact, that such neural networks can be easily combined with HMM's of any complexity with context-dependent capabilities. It is shown that the resulting hybrid system achieves very high recognition rates, which are now already on the same level as the best conventional HMM systems with continuous parameters, and the capabilities of the mutual information neural networks are not yet entirely exploited.			Rigoll, G (corresponding author), GERHARD MERCATOR UNIV DUISBURG,FAC ELECT ENGN,DEPT COMP SCI,BIMARCKSTR 90,DUISBURG,GERMANY.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						772	778						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00109
C	Robel, A		Mozer, MC; Jordan, MI; Petsche, T		Robel, A			Neural network modeling of speech and music signals	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Time series prediction is one of the major applications of neural networks. After a short introduction into the basic theoretical foundations we argue that the iterated prediction of a dynamical system may be interpreted as a model of the system dynamics. By means of RBF neural networks we describe a modeling approach and extend it to be able to model instationary systems. As a practical test for the capabilities of the method we investigate the modeling of musical and speech signals and demonstrate that the model may be used for synthesis of musical and speech signals.			Robel, A (corresponding author), TECH UNIV BERLIN,EINSTEINUFER 17,SEKR EN-8,D-10587 BERLIN,GERMANY.								0	1	1	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						779	785						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00110
C	Rohwer, R; Morciniec, M		Mozer, MC; Jordan, MI; Petsche, T		Rohwer, R; Morciniec, M			The generalisation cost of RAMnets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Given unlimited computational resources, it is best to use a criterion of minimal expected generalisation error to select a model and determine its parameters. However, it may be worthwhile to sacrifice some generalisation performance for higher learning speed. A method for quantifying sub-optimality is set out here, so that this choice can be made intelligently. Furthermore, the method is applicable to a broad class of models, including the ultra-fast memory-based methods such as RAMnets. This brings the added benefit of providing, for the first time, the means to analyse the generalisation properties of such models in a Bayesian framework.			Rohwer, R (corresponding author), ASTON UNIV,NEURAL COMP RES GRP,ASTON TRIANGLE,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						253	259						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00036
C	Saul, LK; Jordan, MI		Mozer, MC; Jordan, MI; Petsche, T		Saul, LK; Jordan, MI			A variational principle for model-based morphing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Given a multidimensional data set and a, model of its density, we consider how to define the optimal interpolation between two points. This is done by assigning a cost to each path through space, based on two competing goals-one to interpolate through regions of high density, the other to minimize are length. From this path functional, we derive the Euler-Lagrange equations for extremal motion; given two points, the desired interpolation is found by solving a boundary value problem. We show that this interpolation can be done efficiently, in high dimensions, for Gaussian, Dirichlet, and mixture models.	MIT,CTR BIOL & COMPUTAT LEARNING,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)	Saul, LK (corresponding author), AT&T BELL LABS,600 MT AVE,MURRAY HILL,NJ 07974, USA.		Jordan, Michael I/C-5253-2013						0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						267	273						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00038
C	Tumer, K; RichardsKortum, R; Ramanujam, N; Ghosh, J		Mozer, MC; Jordan, MI; Petsche, T		Tumer, K; RichardsKortum, R; Ramanujam, N; Ghosh, J			Spectroscopic detection of cervical pre-cancer through radial basis function networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The mortality related to cervical cancer can be substantially reduced through early detection and treatment. However, current detection techniques, such as Pap smear and colposcopy, fail to achieve a concurrently high sensitivity and specificity. In vivo fluorescence spectroscopy is a technique which quickly, noninvasively and quantitatively probes the biochemical and morphological changes that occur in pre-cancerous tissue. RBF ensemble algorithms based on such spectra provide automated, and near realtime implementation of pre-cancer detection in the hands of non-experts. The results are more reliable, direct and accurate than those achieved by either human experts or multivariate statistical algorithms.			Tumer, K (corresponding author), UNIV TEXAS,DEPT ELECT & COMP ENGN,AUSTIN,TX 78712, USA.		Ramanujam, Nimmi/AAA-7575-2021	Ramanujam, Nimmi/0000-0001-7319-8415					0	1	1	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						981	987						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00138
C	Coelho, JA; Sitaraman, R; Grupen, RA		Touretzky, DS; Mozer, MC; Hasselmo, ME		Coelho, JA; Sitaraman, R; Grupen, RA			Parallel optimization of motion controllers via policy iteration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MASSACHUSETTS,DEPT COMP SCI,AMHERST,MA 01003	University of Massachusetts System; University of Massachusetts Amherst									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						996	1002						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00140
C	Coolen, ACC; Laughton, SN; Sherrington, D		Touretzky, DS; Mozer, MC; Hasselmo, ME		Coolen, ACC; Laughton, SN; Sherrington, D			Modern analytic techniques to solve the dynamics of recurrent neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV LONDON KINGS COLL,DEPT MATH,LONDON WC2R 2LS,ENGLAND	University of London; King's College London									0	1	1	0	2	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						253	259						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00036
C	Edwards, RT; Cauwenberghs, G		Touretzky, DS; Mozer, MC; Hasselmo, ME		Edwards, RT; Cauwenberghs, G			Analog VLSI processor implementing the continuous wavelet transform	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						JOHNS HOPKINS UNIV,DEPT ELECT & COMP ENGN,BALTIMORE,MD 21218	Johns Hopkins University									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						692	698						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00098
C	Fry, CL		Touretzky, DS; Mozer, MC; Hasselmo, ME		Fry, CL			How perception guides production in birdsong learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF SAN DIEGO,DEPT COGNIT SCI,LA JOLLA,CA 92093	University of California System; University of California San Diego									0	1	2	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						110	116						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00016
C	Jackson, G; Murray, AF		Touretzky, DS; Mozer, MC; Hasselmo, ME		Jackson, G; Murray, AF			Competence acquisition in an autonomous mobile robot using hardware neural techniques.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV EDINBURGH,DEPT ELECT ENGN,EDINBURGH EH9 3JL,MIDLOTHIAN,SCOTLAND	University of Edinburgh									0	1	1	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1031	1037						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00145
C	Lazzaro, J; Wawrzynek, J		Touretzky, DS; Mozer, MC; Hasselmo, ME		Lazzaro, J; Wawrzynek, J			Silicon models for auditory scene analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV CALIF BERKELEY,CS DIV,BERKELEY,CA 94720	University of California System; University of California Berkeley									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						699	705						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00099
C	Littman, E; Drees, A; Ritter, H		Touretzky, DS; Mozer, MC; Hasselmo, ME		Littman, E; Drees, A; Ritter, H			Visual gesture-based robot guidance with a modular neural system	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV ULM, FAK F INFORMAT, ABT NEUROINFORMAT, D-89069 ULM, GERMANY	Ulm University									0	1	1	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						903	909						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00127
C	McCabe, SL; Denham, MJ		Touretzky, DS; Mozer, MC; Hasselmo, ME		McCabe, SL; Denham, MJ			A model of auditory streaming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV PLYMOUTH,SCH COMP,NEURODYNAM RES GRP,PLYMOUTH PL4 8AA,DEVON,ENGLAND	University of Plymouth									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						52	58						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00008
C	Platt, JC; Allen, TP		Touretzky, DS; Mozer, MC; Hasselmo, ME		Platt, JC; Allen, TP			A neural network classifier for the I1000 OCR chip	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SYNAPT INC,SAN JOSE,CA 95134				Platt, John/GOH-2678-2022						0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						938	944						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00132
C	Snapp, RR; Xu, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Snapp, RR; Xu, T			Estimating the Bayes risk from sample data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV VERMONT,DEPT COMP SCI & ELECT ENGN,BURLINGTON,VT 05405	University of Vermont									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						232	238						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00033
C	Tsioutsias, DI; Mjolsness, E		Touretzky, DS; Mozer, MC; Hasselmo, ME		Tsioutsias, DI; Mjolsness, E			A multiscale attentional framework for relaxation neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						YALE UNIV,DEPT ELECT ENGN,NEW HAVEN,CT 06520	Yale University									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						633	639						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00090
C	VanRoy, B; Tsitsiklis, JN		Touretzky, DS; Mozer, MC; Hasselmo, ME		VanRoy, B; Tsitsiklis, JN			Stable linear approximations to dynamic programming for stochastic control problems with local transitions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,INFORMAT & DECIS SYST LAB,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)									0	1	1	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1045	1051						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00147
C	BONDS, AB		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BONDS, AB			DUAL INHIBITORY MECHANISMS FOR DEFINITION OF RECEPTIVE-FIELD CHARACTERISTICS IN CAT STRIATE CORTEX	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						75	82						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00010
C	BUCHANAN, JT		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BUCHANAN, JT			LOCOMOTION IN A LOWER VERTEBRATE - STUDIES OF THE CELLULAR BASIS OF RHYTHMOGENESIS AND OSCILLATOR COUPLING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						101	108						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00013
C	DARRELL, T; PENTLAND, A		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DARRELL, T; PENTLAND, A			AGAINST EDGES - FUNCTION APPROXIMATION WITH MULTIPLE SUPPORT MAPS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						388	395						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00048
C	GEIGER, D; PEREIRA, RAM		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GEIGER, D; PEREIRA, RAM			LEARNING HOW TO TEACH OR SELECTING MINIMAL SURFACE DATA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						364	371						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00045
C	GISH, SL; BLAUM, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GISH, SL; BLAUM, M			ADAPTIVE DEVELOPMENT OF CONNECTIONIST DECODERS FOR COMPLEX ERROR-CORRECTING CODES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						691	697						7	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00085
C	GLASSMAN, MS		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GLASSMAN, MS			A NETWORK OF LOCALIZED LINEAR DISCRIMINANTS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1102	1109						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00136
C	GOUDREAU, MW; GILES, CL		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GOUDREAU, MW; GILES, CL			NEURAL NETWORK ROUTING FOR RANDOM MULTISTAGE INTERCONNECTION NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						722	729						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00089
C	GRAF, HP; NOHL, CR; BEN, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GRAF, HP; NOHL, CR; BEN, J			IMAGE SEGMENTATION WITH NETWORKS OF VARIABLE SCALES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						480	487						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00059
C	GREBERT, I; STORK, DG; KEESING, R; MIMS, S		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GREBERT, I; STORK, DG; KEESING, R; MIMS, S			NETWORK GENERALIZATION FOR PRODUCTION - LEARNING AND PRODUCING STYLED LETTERFORMS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1118	1124						7	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00138
C	GREENSPAN, HK; GOODMAN, R; CHELLAPPA, R		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GREENSPAN, HK; GOODMAN, R; CHELLAPPA, R			COMBINED NEURAL NETWORK AND RULE-BASED FRAMEWORK FOR PROBABILISTIC PATTERN-RECOGNITION AND DISCOVERY	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Chellappa, Rama/AAJ-1504-2020; Chellappa, Rama/AAV-8690-2020; Chellappa, Rama/B-6573-2012						0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						444	451						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00055
C	HAMEY, LGC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HAMEY, LGC			BENCHMARKING FEEDFORWARD NEURAL NETWORKS - MODELS AND MEASURES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Hamey, Leonard/R-4304-2019	Hamey, Leonard/0000-0001-8594-2176					0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1167	1174						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00144
C	HAMPSHIRE, JB; KUMAR, BVKV		MOODY, JE; HANSON, SJ; LIPPMANN, RP		HAMPSHIRE, JB; KUMAR, BVKV			SHOOTING CRAPS IN SEARCH OF AN OPTIMAL STRATEGY FOR TRAINING CONNECTIONIST PATTERN CLASSIFIERS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1125	1132						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00139
C	JABRI, M; PICKARD, S; LEONG, P; CHI, Z; FLOWER, B; XIE, Y		MOODY, JE; HANSON, SJ; LIPPMANN, RP		JABRI, M; PICKARD, S; LEONG, P; CHI, Z; FLOWER, B; XIE, Y			ANN BASED CLASSIFICATION FOR HEART DEFIBRILLATORS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						637	644						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00078
C	KEESING, R; STORK, DG; SHATZ, CJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		KEESING, R; STORK, DG; SHATZ, CJ			RETINOGENICULATE DEVELOPMENT - THE ROLE OF COMPETITION AND CORRELATED RETINAL ACTIVITY	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						91	97						7	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00012
C	MARTIN, GL; RASHID, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MARTIN, GL; RASHID, M			RECOGNIZING OVERLAPPING HAND-PRINTED CHARACTERS BY CENTERED-OBJECT INTEGRATED SEGMENTATION AND RECOGNITION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						504	511						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00062
C	REDDING, NJ; DOWNS, T		MOODY, JE; HANSON, SJ; LIPPMANN, RP		REDDING, NJ; DOWNS, T			LEARNING IN FEEDFORWARD NETWORKS WITH NONSMOOTH FUNCTIONS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1056	1063						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00130
C	STOLORZ, P		MOODY, JE; HANSON, SJ; LIPPMANN, RP		STOLORZ, P			MERGING CONSTRAINED OPTIMIZATION WITH DETERMINISTIC ANNEALING TO SOLVE COMBINATORIALLY HARD PROBLEMS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1025	1032						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00126
C	VIOLA, PA; LISBERGER, SG; SEJNOWSKI, TJ		MOODY, JE; HANSON, SJ; LIPPMANN, RP		VIOLA, PA; LISBERGER, SG; SEJNOWSKI, TJ			RECURRENT EYE TRACKING NETWORK USING A DISTRIBUTED REPRESENTATION OF IMAGE MOTION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Sejnowski, Terrence/AAV-5558-2021						0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						380	387						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00047
C	WAIBEL, A; JAIN, AN; MCNAIR, A; TEBELSKIS, J; SAITO, H; SCHMIDBAUER, O; OSTERHOLTZ, L; SLOBODA, T; WOSZCZYNA, M		MOODY, JE; HANSON, SJ; LIPPMANN, RP		WAIBEL, A; JAIN, AN; MCNAIR, A; TEBELSKIS, J; SAITO, H; SCHMIDBAUER, O; OSTERHOLTZ, L; SLOBODA, T; WOSZCZYNA, M			JANUS - SPEECH-TO-SPEECH TRANSLATION USING CONNECTIONIST AND NONCONNECTIONIST TECHNIQUES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	1	1	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						183	190						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00023
C	Barde, P; Roy, J; Jeon, W; Pineau, J; Nowrouzezahrai, D; Pal, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Barde, Paul; Roy, Julien; Jeon, Wonseok; Pineau, Joelle; Nowrouzezahrai, Derek; Pal, Christopher			Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Adversarial Imitation Learning alternates between learning a discriminator - which tells apart expert's demonstrations from generated ones - and a generator's policy to produce trajectories that can fool this discriminator. This alternated optimization is known to be delicate in practice since it compounds unstable adversarial training with brittle and sample-inefficient reinforcement learning. We propose to remove the burden of the policy optimization steps by leveraging a novel discriminator formulation. Specifically, our discriminator is explicitly conditioned on two policies: the one from the previous generator's iteration and a learnable policy. When optimized, this discriminator directly learns the optimal generator's policy. Consequently, our discriminator's update solves the generator's optimization problem for free: learning a policy that imitates the expert does not require an additional optimization loop. This formulation effectively cuts by half the implementation and computational burden of Adversarial Imitation Learning algorithms by removing the Reinforcement Learning phase altogether. We show on a variety of tasks that our simpler approach is competitive to prevalent Imitation Learning methods.	[Barde, Paul; Jeon, Wonseok; Pineau, Joelle; Nowrouzezahrai, Derek] McGill Univ, Quebec AI Inst Mila, Montreal, PQ, Canada; [Roy, Julien; Pal, Christopher] Polytech Montreal, Quebec AI Inst Mila, Montreal, PQ, Canada; [Pineau, Joelle] Facebook AI Res, Montreal, PQ, Canada	McGill University; Universite de Montreal; Polytechnique Montreal; Facebook Inc	Barde, P (corresponding author), McGill Univ, Quebec AI Inst Mila, Montreal, PQ, Canada.	bardepau@mila.quebec; julien.roy@mila.quebec; jeonwons@mila.quebec			Fonds de Recherche Nature et Technologies (FRQNT); Ubisoft Montreal; Mitacs' Accelerate Program; NSERC Industrial Research Chair program	Fonds de Recherche Nature et Technologies (FRQNT); Ubisoft Montreal; Mitacs' Accelerate Program; NSERC Industrial Research Chair program(Canada Research Chairs)	We thank Eloi Alonso, Olivier Delalleau, Felix G. Harvey, Maxim Peter and the entire research team at Ubisoft Montreal's La Forge R&D laboratory. Their feedback and comments contributed significantly to this work. Christopher Pal and Derek Nowrouzezahrai acknowledge funding from the Fonds de Recherche Nature et Technologies (FRQNT), Ubisoft Montreal and Mitacs' Accelerate Program in support of our work, as well as Compute Canada for providing computing resources. Derek and Paul also acknowledge support from the NSERC Industrial Research Chair program.	[Anonymous], 2019, P 7 INT C LEARN REPR; Degris T., 2012, ICML12; Finn C., 2016, ARXIV161103852; Finn C, 2016, PR MACH LEARN RES, V48; Fu Justin, 2017, P 5 INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Haarnoja T, 2017, PR MACH LEARN RES, V70; Haarnoja Tuomas, 2018, ARXIV181205905; Hazan E., 2018, ARXIV181202690; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kamyar Seyed, 2019, P 3 C ROB LEARN CORL; Kostrikov Ilya, 2020, P 8 INT C LEARN REPR; Kuefler A, 2017, IEEE INT VEH SYM, P204, DOI 10.1109/IVS.2017.7995721; Lalonde V, 2010, PROCEEDINGS OF THE ASME FLUIDS ENGINEERING DIVISION SUMMER MEETING - 2010 - VOL 3, PTS A AND B, P661; Nachum O., 2018, PROC INT C LEARN REP, P1; Nachum Ofir, 2019, ADV NEURAL INFORM PR, P2318; Ng A., 2004, P 21 INT C MACH LEAR; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Reddy S., 2019, SQIL IMITATION LEARN; Resnick Cinjon, 2018, POMMERMAN NEURIPS 20; Rezende D., 2015, ICML, P1530; Ross St<prime>ephane, 2011, AISTATS; Sasaki Fumihiro, 2018, P 6 INT C LEARN REPR; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Zhou H., 2018, FDG, P1; Ziebart B. D., 2008, AAAI, V8, P1433; Ziebart B. D., 2010, THESIS	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000067
C	Bhaskara, A; Gollapudi, S; Kollias, K; Munagala, K		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Bhaskara, Aditya; Gollapudi, Sreenivas; Kollias, Kostas; Munagala, Kamesh			Adaptive Probing Policies for Shortest Path Routing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Inspired by traffic routing applications, we consider the problem of finding the shortest path from a source s to a destination t in a graph, when the lengths of the edges are unknown. Instead, we are given hints or predictions of the edge lengths from a collection of ML models, trained possibly on historical data and other contexts in the network. Additionally, we assume that the true length of any candidate path can be obtained by probing an up-to-date snapshot of the network. However, each probe introduces a latency, and thus the goal is to minimize the number of probes while finding a near-optimal path with high probability. We formalize this problem and show assumptions under which it admits to efficient approximation algorithms. We verify these assumptions and validate the performance of our algorithms on real data.	[Bhaskara, Aditya] Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA; [Gollapudi, Sreenivas; Kollias, Kostas] Google Res, Mountain View, CA USA; [Munagala, Kamesh] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA	Utah System of Higher Education; University of Utah; Google Incorporated; Duke University	Bhaskara, A (corresponding author), Univ Utah, Sch Comp, Salt Lake City, UT 84112 USA.	bhaskaraaditya@gmail.com; sgollapu@google.com; kostaskollias@google.com; kamesh@cs.duke.edu			NSF [CCF-1637397, CCF-1408784, CCF-2008688]; Google Faculty Research Award; ONR [N00014-19-1-2268]; DARPA [FA8650-18-C-7880]	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	Aditya Bhaskara is partially supported by NSF (CCF-2008688) and by a Google Faculty Research Award. Kamesh Munagala is supported by NSF grants CCF-1637397 and CCF-1408784; ONR award N00014-19-1-2268; and DARPA award FA8650-18-C-7880.	Abraham I, 2010, LECT NOTES COMPUT SC, V6049, P23, DOI 10.1007/978-3-642-13193-6_3; Baum M, 2016, LECT NOTES COMPUT SC, V9685, P33, DOI 10.1007/978-3-319-38851-9_3; Beyhaghi H, 2019, ACM EC '19: PROCEEDINGS OF THE 2019 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P131, DOI 10.1145/3328526.3329626; Bnaya Z., 2009, IJCAI 2009; Cebecauer M, 2018, IET INTELL TRANSP SY, V12, P66, DOI 10.1049/iet-its.2017.0113; de Fabritiis C, 2008, PROCEEDINGS OF THE 11TH INTERNATIONAL IEEE CONFERENCE ON INTELLIGENT TRANSPORTATION SYSTEMS, P197, DOI 10.1109/ITSC.2008.4732534; Delling D, 2018, PROC INT CONF DATA, P1543, DOI 10.1109/ICDE.2018.00172; Delling D, 2017, TRANSPORT SCI, V51, P566, DOI 10.1287/trsc.2014.0579; Deshpande A, 2016, ACM T ALGORITHMS, V12, DOI 10.1145/2876506; Donovan B, 2017, TRANSPORT RES C-EMER, V79, P333, DOI 10.1016/j.trc.2017.03.002; Goel A., 2006, PODS; Goldberg A., 2007, CURRENT TRENDS THEOR; Gollapudi S, 2019, PR MACH LEARN RES, V97; Golovin D, 2011, J ARTIF INTELL RES, V42, P427; Guha S., 2006, Performance Evaluation Review, V34, P381, DOI 10.1145/1140103.1140330; Hsu C.-Y., 2019, ICLR, P1; Lattanzi S, 2020, PROCEEDINGS OF THE THIRTY-FIRST ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA'20), P1859; Liu Z., 2008, P ACM SIGMOD INT C M, P133; Lykouris T, 2018, PR MACH LEARN RES, V80; Min WL, 2011, TRANSPORT RES C-EMER, V19, P606, DOI 10.1016/j.trc.2010.10.002; Mitzenmacher Michael, 2018, P ANN C NEUR INF PRO, P464; Munagala K., 2005, P INT C DAT THEOR; Nikolova E., 2008, PROC AAAI C ARTIF IN, P969; Nikolova E, 2006, LECT NOTES COMPUT SC, V4168, P552; PAPADIMITRIOU CH, 1991, THEOR COMPUT SCI, V84, P127, DOI 10.1016/0304-3975(91)90263-2; Purohit Manish, 2018, ADV NEURAL INFORM PR, P9684; Strano E, 2017, ROY SOC OPEN SCI, V4, DOI 10.1098/rsos.170590; Tchrakian TT, 2012, IEEE T INTELL TRANSP, V13, P519, DOI 10.1109/TITS.2011.2174634; Yang F, 2004, TRANSPORT RES REC, P1, DOI 10.3141/1879-01	29	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000042
C	Boob, D; Deng, Q; Lan, GH; Wang, YL		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Boob, Digvijay; Deng, Qi; Lan, Guanghui; Wang, Yilin			A Feasible Level Proximal Point Method for Nonconvex Sparse Constrained Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				VARIABLE SELECTION; MODEL SELECTION; SHRINKAGE; ALGORITHM; LASSO	Nonconvex sparse models have received significant attention in high-dimensional machine learning. In this paper, we study a new model consisting of a general convex or nonconvex objectives and a variety of continuous nonconvex sparsity-inducing constraints. For this constrained model, we propose a novel proximal point algorithm that solves a sequence of convex subproblems with gradually relaxed constraint levels. Each subproblem, having a proximal point objective and a convex surrogate constraint, can be efficiently solved based on a fast routine for projection onto the surrogate constraint. We establish the asymptotic convergence of the proposed algorithm to the Karush-Kuhn-Tucker (KKT) solutions. We also establish new convergence complexities to achieve an approximate KKT solution when the objective can be smooth/nonsmooth, deterministic/stochastic and convex/nonconvex with complexity that is on a par with gradient descent for unconstrained optimization problems in respective cases. To the best of our knowledge, this is the first study of the first-order methods with complexity guarantee for nonconvex sparse-constrained problems. We perform numerical experiments to demonstrate the effectiveness of our new model and efficiency of the proposed algorithm for large scale problems.	[Boob, Digvijay] Southern Methodist Univ, Dallas, TX 75205 USA; [Deng, Qi; Wang, Yilin] Shanghai Univ Finance & Econ, Shanghai, Peoples R China; [Lan, Guanghui] Georgia Tech, Atlanta, GA USA	Southern Methodist University; Shanghai University of Finance & Economics; University System of Georgia; Georgia Institute of Technology	Boob, D (corresponding author), Southern Methodist Univ, Dallas, TX 75205 USA.	dboob@smu.edu; qideng@sufe.edu.cn; george.lan@isye.gatech.edu; 2017110765@live.sufe.edu.cn			National Science Foundation (NSF) [CCF 1909298]; National Natural Science Foundation of China [11831002]	National Science Foundation (NSF)(National Science Foundation (NSF)National Research Foundation of Korea); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	Most of this work was done while Boob was at Georgia Tech. Boob and Lan gratefully acknowledge the National Science Foundation (NSF) for its support through grant CCF 1909298. Q. Deng acknowledges funding from National Natural Science Foundation of China (Grant 11831002).	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Bertsimas D, 2016, ANN STAT, V44, P813, DOI 10.1214/15-AOS1388; Blumensath T, 2008, J FOURIER ANAL APPL, V14, P629, DOI 10.1007/s00041-008-9035-z; Boob Digvijay, 2019, ARXIV190802734; Bradley P. S., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P82; Candes EJ, 2009, ANN STAT, V37, P2145, DOI 10.1214/08-AOS653; Candes EJ, 2008, J FOURIER ANAL APPL, V14, P877, DOI 10.1007/s00041-008-9045-x; Diamond S, 2016, J MACH LEARN RES, V17; Duchi J., 2008, PROC 25 INT C MACH L, P272; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Foucart S, 2011, SIAM J NUMER ANAL, V49, P2543, DOI 10.1137/100806278; Fu WJJ, 1998, J COMPUT GRAPH STAT, V7, P397, DOI 10.2307/1390712; Gong Pinghua, 2013, JMLR Workshop Conf Proc, V28, P37; Gotoh J, 2018, MATH PROGRAM, V169, P141, DOI 10.1007/s10107-017-1181-0; Thi HAL, 2018, MATH PROGRAM, V169, P5, DOI 10.1007/s10107-018-1235-y; Khamaru Koulik, 2018, INT C MACH LEARN, P2606; Kopsinis Y, 2011, IEEE T SIGNAL PROCES, V59, P936, DOI 10.1109/TSP.2010.2090874; Kyrillidis A, 2012, IEEE INT SYMP INFO; Lan Guanghui, 2019, ADV NEURAL INFORM PR, P10462; Le Thi HA, 2015, EUR J OPER RES, V244, P26, DOI 10.1016/j.ejor.2014.11.031; Lin Qihang, 2019, ARXIV190811518; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Pedregosa F., 2011, J MACH LEARN RES, V12, P2825; Rao BD, 1999, IEEE T SIGNAL PROCES, V47, P187, DOI 10.1109/78.738251; Robbins H., 1971, OPTIMIZING METHODS S, P111; Scutari G, 2017, IEEE T SIGNAL PROCES, V65, P1945, DOI 10.1109/TSP.2016.2637314; Shen XY, 2016, IEEE DECIS CONTR P, P1009, DOI 10.1109/CDC.2016.7798400; Sun Y, 2017, IEEE T SIGNAL PROCES, V65, P794, DOI 10.1109/TSP.2016.2601299; Tibshirani R, 2011, J R STAT SOC B, V73, P273, DOI 10.1111/j.1467-9868.2011.00771.x; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Weston J., 2003, Journal of Machine Learning Research, V3, P1439, DOI 10.1162/153244303322753751; Wright SJ, 2009, IEEE T SIGNAL PROCES, V57, P2479, DOI 10.1109/TSP.2009.2016892; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Yuan XT, 2018, J MACH LEARN RES, V18; Yuan Xiaotong, 2018, P ADV NEUR INF PROC, P1984; Zhang CH, 2008, ANN STAT, V36, P1567, DOI 10.1214/07-AOS520; Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhao P, 2006, J MACH LEARN RES, V7, P2541	42	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000075
C	Bun, M		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Bun, Mark			A Computational Separation between Private Learning and Online Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				SAMPLE COMPLEXITY; BOUNDS	A recent line of work has shown a qualitative equivalence between differentially private PAC learning and online learning: A concept class is privately learnable if and only if it is online learnable with a finite mistake bound. However, both directions of this equivalence incur significant losses in both sample and computational efficiency. Studying a special case of this connection, Gonen, Hazan, and Moran (NeurIPS 2019) showed that uniform or highly sample-efficient pure-private learners can be time-efficiently compiled into online learners. We show that, assuming the existence of one-way functions, such an efficient conversion is impossible even for general pure-private learners with polynomial sample complexity. This resolves a question of Neel, Roth, and Wu (FOCS 2019).	[Bun, Mark] Boston Univ, Dept Comp Sci, Boston, MA 02215 USA	Boston University	Bun, M (corresponding author), Boston Univ, Dept Comp Sci, Boston, MA 02215 USA.	mbun@bu.edu			NSF [CCF-1947889]; Google Research Fellowship at the Simons Institute	NSF(National Science Foundation (NSF)); Google Research Fellowship at the Simons Institute(Google Incorporated)	This work was supported by NSF grant CCF-1947889. Within the 36 months prior to the submission of this work, the author was supported by a Google Research Fellowship at the Simons Institute for the Theory of Computing.	Abernethy Jacob D., 2017, ABS171110019 CORR; Agarwal N, 2017, PR MACH LEARN RES, V70; Alon Noga, 2019, P 51 ANN ACM S THEOR; Balcer Victor, 2018, LIPICS, V94; Beimel A, 2019, J MACH LEARN RES, V20; Beimel A, 2016, THEOR COMPUT, V12, DOI 10.4086/toc.2016.v012a001; Beimel A, 2014, MACH LEARN, V94, P401, DOI 10.1007/s10994-013-5404-1; BLUM AL, 1994, SIAM J COMPUT, V23, P990, DOI 10.1137/S009753979223455X; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Bousquet Olivier, 2019, PASSING TESTS MEMORI; Bun M, 2016, LECT NOTES COMPUT SC, V9562, P176, DOI 10.1007/978-3-662-49096-9_8; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Bun Mark, 2020, ABS200300563 CORR; Bun Mark, 2020, P MACHINE LEARNING R; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Dwork C, 2009, ACM S THEORY COMPUT, P371; Feldman V, 2015, SIAM J COMPUT, V44, P1740, DOI 10.1137/140991844; Ghosh A, 2012, SIAM J COMPUT, V41, P1673, DOI 10.1137/09076828X; GOLDREICH O, 1986, J ACM, V33, P792, DOI 10.1145/6490.6503; Gonen Alon, 2019, P 33 C NEURAL INFORM; Hardt M, 2010, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2010.85; Hazan E, 2016, ACM S THEORY COMPUT, P128, DOI 10.1145/2897518.2897536; Jain Prateek, 2012, JMLR P, V23; Jung Young Hun, 2020, ABS200601980 CORR; Kaplan Haim, 2020, P MACHINE LEARNING R, V125, P2263; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Neel SV, 2019, ANN IEEE SYMP FOUND, P72, DOI 10.1109/FOCS.2019.00014; Roth A, 2010, ACM S THEORY COMPUT, P765; Smith A, 2011, ACM S THEORY COMPUT, P813; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972	33	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000022
C	Chen, JX; Wu, XM; Li, YK; Li, QM; Zhan, LM; Chung, FL		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Chen, Jiaxin; Wu, Xiao-Ming; Li, Yanke; Li, Qimai; Zhan, Li-Ming; Chung, Fu-lai			A Closer Look at the Training Strategy for Modern Meta-Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				STABILITY; LEARNABILITY	The support/query (S/Q) episodic training strategy has been widely used in modern meta-learning algorithms and is believed to improve their generalization ability to test environments. This paper conducts a theoretical investigation of this training strategy on generalization. From a stability perspective, we analyze the generalization error bound of generic meta-learning algorithms trained with such strategy. We show that the S/Q episodic training strategy naturally leads to a counterintuitive generalization bound of O (1/root n), which only depends on the task number n but independent of the inner-task sample size m. Under the common assumption m << n for few-shot learning, the bound of O (1/root n) implies strong generalization guarantees for modern meta-learning algorithms in the few-shot regime. To further explore the influence of training strategies on generalization, we propose a leave-one-out (LOO) training strategy for meta-learning and compare it with S/Q training. Experiments on standard few-shot regression and classification tasks with popular meta-learning algorithms validate our analysis.	[Chen, Jiaxin; Wu, Xiao-Ming; Li, Qimai; Zhan, Li-Ming; Chung, Fu-lai] Hong Kong Polytech Univ, 1Dept Comp, Hong Kong, Peoples R China; [Li, Yanke] Swiss Fed Inst Technol, Dept Math, Zurich, Switzerland	Hong Kong Polytechnic University; Swiss Federal Institutes of Technology Domain; ETH Zurich	Wu, XM; Chung, FL (corresponding author), Hong Kong Polytech Univ, 1Dept Comp, Hong Kong, Peoples R China.	jiax.chen@connect.polyu.hk; xiao-ming.wu@polyu.edu.hk; yankli@student.ethz.ch; qee-mai.li@connect.polyu.hk; lmzhan.zhan@connect.polyu.hk; korris.chung@polyu.edu.hk			PolyU (UGC) [P0030935, P0030970]	PolyU (UGC)(University Grants Commission, India)	The authors would like to thank Junjie Ye for helpful discussion and the anonymous reviewers for their valuable comments. This research was supported by the grants of DaSAIL projects P0030935 and P0030970 funded by PolyU (UGC).	Amit Ron, 2018, ICML; [Anonymous], 2019, ICLR; Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; BLUMER A, 1989, J ACM, V36, P929, DOI 10.1145/76359.76371; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bullins Brian, 2019, ALT; Chen J, 2020, AAAI; Denevi G, 2018, UAI; Denevi Giulia, 2019, ICML; Denevi Giulia, 2018, NEURIPS; Denevi Giulia, 2019, NEURIPS; Elisseeff A, 2005, J MACH LEARN RES, V6, P55; Elisseeff A., 2003, NATO SCI SER SUBSER, V190, P111; Fallah A., 2020, AISTATS; Feldman Vitaly, 2018, NEURIPS; Finn Chelsea, 2017, ICML; Franceschi Luca, 2018, ICML; Hardt Moritz, 2016, ICML; Khodak Mikhail, 2019, NEURIPS; Khodak Mikhail, 2019, ICML; Kuzborskij Ilja, 2018, ICML; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Maurer A, 2005, J MACH LEARN RES, V6, P967; Maurer Andreas, 2017, COLT; Pentina, 2014, ICML; Pentina Anastasia, 2015, NEURIPS; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Snell J, 2017, NEURIPS; Thrun S, 1998, LEARNING TO LEARN, P181; Vapnik V., 2013, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4757-2440-0; Vapnik V.N., 2006, ESTIMATION DEPENDENC, VVolume 40; Vinyals Oriol, 2016, NEURIPS; Yin M., 2020, ICLR; Zhang L., 2018, ARXIV180702872	37	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000020
C	Chen, ZD; Rotskoff, GM; Bruna, J; Vanden-Eijnden, E		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Chen, Zhengdao; Rotskoff, Grant M.; Bruna, Joan; Vanden-Eijnden, Eric			A Dynamical Central Limit Theorem for Shallow Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				FLUCTUATIONS; PROPAGATION; ALGORITHM	Recent theoretical works have characterized the dynamics of wide shallow neural networks trained via gradient descent in an asymptotic mean-field limit when the width tends towards infinity. At initialization, the random sampling of the parameters leads to deviations from the mean-field limit dictated by the classical Central Limit Theorem (CLT). However, since gradient descent induces correlations among the parameters, it is of interest to analyze how these fluctuations evolve. In this work, we derive a dynamical CLT to prove that the asymptotic fluctuations around the mean limit remain bounded in mean square throughout training The upper bound is given by a Monte-Carlo resampling error, with a variance that depends on the 2-norm of the underlying measure, which also controls the generalization error. This motivates the use of this 2-norm as a regularization term during training. Furthermore, if the mean-field dynamics converges to a measure that interpolates the training data, we prove that the asymptotic deviation eventually vanishes in the CLT scaling. We also complement these results with numerical experiments.	[Rotskoff, Grant M.] Stanford Univ, Dept Chem, Stanford, CA 94305 USA; [Chen, Zhengdao; Bruna, Joan; Vanden-Eijnden, Eric] NYU, Courant Inst Math Sci, New York, NY 10003 USA; [Bruna, Joan] NYU, Ctr Data Sci, New York, NY 10003 USA	Stanford University; New York University; New York University	Rotskoff, GM (corresponding author), Stanford Univ, Dept Chem, Stanford, CA 94305 USA.; Chen, ZD; Bruna, J; Vanden-Eijnden, E (corresponding author), NYU, Courant Inst Math Sci, New York, NY 10003 USA.; Bruna, J (corresponding author), NYU, Ctr Data Sci, New York, NY 10003 USA.	zc1216@nyu.edu; rotskoff@stanford.edu; bruna@nyu.edu; eve2@nyu.edu		Rotskoff, Grant/0000-0002-7772-5179	Henry MacCraken Fellowship; James S. McDonnell Foundation; Alfred P. Sloan Foundation; NSF [RI1816753, DMS-1522767]; NSF CAREER [CIF 1845360]; National Science Foundation (NSF) [DMR-1420073]	Henry MacCraken Fellowship; James S. McDonnell Foundation; Alfred P. Sloan Foundation(Alfred P. Sloan Foundation); NSF(National Science Foundation (NSF)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); National Science Foundation (NSF)(National Science Foundation (NSF))	This work benefited from discussions with Lenaic Chizat and Carles Domingo-Enrich. Z.C. acknowledges support from the Henry MacCraken Fellowship. G.M.R. acknowledges support from the James S. McDonnell Foundation. J.B. acknowledges support from the Alfred P. Sloan Foundation, NSF RI1816753, NSF CAREER CIF 1845360, and the Institute for Advanced Study. E. V.-E. acknowledges support by National Science Foundation (NSF) Materials Research Science and Engineering Center Program Grant No. DMR-1420073, and by NSF Grant No. DMS-1522767.	Adlam Ben, NEURAL TANGENT KERNE; Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; [Anonymous], 2018, ARXIV180511387; Araujo Dyego, 2019, MEAN FIELD LIMIT CER; Arora Sanjeev, 2019, ARXIV190108584; Bai Yu, 2019, ARXIV191001619; Baladron Javier, 2011, ARXIV11104294; Belkin Mikhail, 2019, ARXIV181211118CSSTAT; Boyer C, 2019, SIAM J OPTIMIZ, V29, P1260, DOI 10.1137/18M1200750; BRAUN W, 1977, COMMUN MATH PHYS, V56, P101, DOI 10.1007/BF01611497; Chizat L., 2019, ADV NEURAL INFORM PR, P2937; Chizat Lenaic, 2020, P MACHINE LEARNING R; Chizat Lenaic, 2018, ADV NEURAL INFORM PR, P3036; Chizat Lenaic, 2019, SPARSE OPTIMIZATION; Cho Y., 2009, NIPS, P342; Cortez R, 2016, J STAT PHYS, V165, P1102, DOI 10.1007/s10955-016-1674-x; De Bortoli Valentin, 2020, ARXIV200706352; de Dios Jaume, 2020, ARXIV200610225; Diakonikolas Ilias, 2020, P MACHINE LEARNING R, P1514; Du Simon S, 2018, GRADIENT DESCENT FIN; Dyer E., 2019, ARXIV190911304; Fang Cong, 2020, ARXIV200701452; FISHER SD, 1975, J APPROX THEORY, V13, P73, DOI 10.1016/0021-9045(75)90016-7; Geiger M., 2019, ARXIV190608034, p2019b; Geiger M, 2020, J STAT MECH-THEORY E, V2020, DOI 10.1088/1742-5468/ab633c; Ghorbani Behrooz, 2020, ARXIV200613409; Goel Surbhi, 2020, ARXIV200612011; Goldt S., 2020, ARXIV200614709; Gripenberg G., 1990, ENCY MATH ITS APPL; Hanin B, 2019, ARXIV190905989; Huang Jiaoyang, 2019, ARXIV190908156; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Lancellotti C, 2009, J STAT PHYS, V136, P643, DOI 10.1007/s10955-009-9800-7; Lee Jaehoon, 2017, ARXIV171100165; Lee Jason D, 2017, ARXIV171007406; Livni R, 2014, ADV NEUR IN, V27; Lu Yiping, 2020, ARXIV200305508; Luo Tao, 2020, ARXIV200707497; Ma C., 2019, ARXIV190608039; Manurangsi P., 2018, ARXIV181004207; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Neal Brady, 2018, ARXIV181008591; Nguyen Phan- Minh, 2020, ARXIV200111443; Rotskoff Grant, 2019, P INT C MACH LEARN L; Rotskoff GM, 2018, ADV NEUR IN, V31; Rotskoff Grant M, 2018, STAT-US; Safran I, 2018, PR MACH LEARN RES, V80; Salem S., 2018, ARXIV181008946; Salhi J, 2018, STOCHASTICS, V90, P49, DOI 10.1080/17442508.2017.1311898; Serfaty S., 2014, ZUR LECT ADV MATH, DOI DOI 10.4171/152; Sirignano J., 2019, ARXIV190304440; Sirignano J, 2020, SIAM J APPL MATH, V80, P725, DOI 10.1137/18M1192184; Sirignano J, 2020, STOCH PROC APPL, V130, P1820, DOI 10.1016/j.spa.2019.06.003; Sirignano J, 2018, J COMPUT PHYS, V375, P1339, DOI 10.1016/j.jcp.2018.08.029; Smale Stephen, 1963, ANN SCUOLA NORM-SCI, V17, P97; Soltanolkotabi M, 2019, IEEE T INFORM THEORY, V65, P742, DOI 10.1109/TIT.2018.2854560; Spigler S., 2018, ARXIV181009665; Spohn H, 2012, LARGE SCALE DYNAMICS; SZNITMAN AS, 1991, LECT NOTES MATH, V1464, P165; Theodor Misiakiewicz and, 2019, ARXIV190206015; Venturi L, 2019, J MACH LEARN RES, V20; Weinan E, 2019, MACHINE LEARNING CON; Wojtowytsch Stephan, 2020, ARXIV200715623; Woodworth Blake, 2020, C LEARN ING THEORY; Zeng HQ, 2017, PROC INT CONF RECON; Zhao SB, 2013, INT CONF MEASURE, P9, DOI 10.1109/MIC.2013.6757905; Zuhovickii S., 1948, REMARKS PROBLEMS APP	71	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000073
C	Dalton, S; Frosio, I		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Dalton, Steven; Frosio, Iuri			Accelerating Reinforcement Learning through GPU Atari Emulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We introduce CuLE (CUDA Learning Environment), a CUDA port of the Atari Learning Environment (ALE) which is used for the development of deep reinforcement algorithms. CuLE overcomes many limitations of existing CPU-based emulators and scales naturally to multiple GPUs. It leverages GPU parallelization to run thousands of games simultaneously and it renders frames directly on the GPU, to avoid the bottleneck arising from the limited CPU-GPU communication bandwidth. CuLE generates up to 155M frames per hour on a single GPU, a finding previously achieved only through a cluster of CPUs. Beyond highlighting the differences between CPU and GPU emulators in the context of reinforcement learning, we show how to leverage the high throughput of CuLE by effective batching of the training data, and show accelerated convergence for A2C+V-trace.	[Dalton, Steven; Frosio, Iuri] NVIDIA, Santa Clara, CA 95051 USA	Nvidia Corporation	Dalton, S (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.	sdalton@nvidia.com; ifrosio@nvidia.com						Babaeizadeh M, 2016, ABS161106256 CORR; Babaeizadeh Mohammad, 2017, ICLR; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellemare Marc G., 2017, ABS170706887 CORR; Conti E, 2018, ADV NEUR IN, V31; Espeholt L, 2018, PR MACH LEARN RES, V80; Hessel M., 2017, ABS171002298 CORR; Horgan D., 2018, P ICLR; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A., 2015, ARXIV150704296; OpenAI, 2017, OP BAS ACKTR A2C; Paszke A., 2017, AUTOMATIC DIFFERENTI; Petrenko Aleksei, 2020, SAMPLE FACTORY EGOCE; Salimans T., 2017, EVOLUTION STRATEGIES; Stooke A., 2018, ABS180302811 CORR; Such Felipe Petroski, 2018, DEEP NEUROEVOLUTION; Veness J., 2017, ARXIV170906009; Wang Ziyu, 2015, ARXIV151106581	20	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000014
C	Dolatabadi, HM; Erfani, S; Leckie, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Dolatabadi, Hadi M.; Erfani, Sarah; Leckie, Christopher			AdvFlow: Inconspicuous Black-box Adversarial Attacks using Normalizing Flows	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Deep learning classifiers are susceptible to well-crafted, imperceptible variations of their inputs, known as adversarial attacks. In this regard, the study of powerful attack models sheds light on the sources of vulnerability in these classifiers, hopefully leading to more robust ones. In this paper, we introduce AdvFlow: a novel black-box adversarial attack method on image classifiers that exploits the power of normalizing flows to model the density of adversarial examples around a given target image. We see that the proposed method generates adversaries that closely follow the clean data distribution, a property which makes their detection less likely. Also, our experimental results show competitive performance of the proposed approach with some of the existing attack methods on defended classifiers. The code is available at https: //github com/hmdolatabadi/AdvFlow.	[Dolatabadi, Hadi M.; Erfani, Sarah; Leckie, Christopher] Univ Melbourne, Sch Comp & Informat Syst, Parkville, Vic, Australia	University of Melbourne	Dolatabadi, HM (corresponding author), Univ Melbourne, Sch Comp & Informat Syst, Parkville, Vic, Australia.	hadi.mohagheghdolatabadi@student.unimelb.edu.au			 [LE170100200]		This research was undertaken using the LIEF HPC-GPGPU Facility hosted at the University of Melbourne. This Facility was established with the assistance of LIEF Grant LE170100200.	[Anonymous], 2017, NEURIPS ML SYST WORK; [Anonymous], 2017, P 5 INT C LEARN REPR; Ardizzone L., 2019, ABS190702392 CORR; Baluja S, 2018, AAAI CONF ARTIF INTE, P2687; Bhambri S., 2019, ABS191201667 CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Dinh L., 2015, WORKSH TRACK P 3 INT; Dolatabadi H. M., 2020, 2 WORKSH INV NEUR NE; Dolatabadi HM, 2020, PR MACH LEARN RES, V108, P4236; Durkan C., 2019, ADV NEURAL INFORM PR, V32, P7511; Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175; Gidaris S., 2018, P 6 INT C LEARN REPR; Guo C., 2019, P 35 C UNC ART INT U, P411; Harris CR, 2020, NATURE, V585, P357, DOI 10.1038/s41586-020-2649-2; Hendrycks D., 2019, P NEURIPS, P15637; Huang Z., 2020, P 8 INT C LEARN REPR; Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55; Ilyas A., 2019, P 7 INT C LEARN REPR; Ilyas A., 2018, ICML, P2142; Jacobsen J., 2018, P 6 INT C LEARN REPR; Jiang LX, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P864, DOI 10.1145/3343031.3351088; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kingma D.P., 2015, INT C LEARN REPR, P1; Kingma D. P., 2018, P ADV NEURAL INFORM, V31, P10236; Kobyzev Ivan, 2020, IEEE T PATTERN ANAL; Kolter Z., 2018, TUTORIAL ADV NEURAL, V31; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Laidlaw C., 2019, ADV NEURAL INFORM PR, V32, P10408; Li Y, 2019, AAAI CONF ARTIF INTE, P8666; Liang B, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4208; Liu J., 2019, ADV NEURAL INFORM PR, P13556; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Ma X, 2018, P 6 INT C LEARN REPR; Moon S, 2019, PR MACH LEARN RES, V97; Netzer Yuval, 2011, NEURIPS WORKSH DEEP; Papamakarios G., 2019, ABS191202762 CORR; Papernot N., 2016, ABS160507277 CORR; Paszke A., 2017, NEURIPS AUT WORKSH; Rezende D., 2015, ICML, P1530; Rudin W., 1964, PRINCIPLES MATH ANAL, V3; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T., 2017, ABS170303864 CORR; Shafahi A., 2019, 32 ANN C NEUR INF PR, P3353; Shi C., 2020, P 8 INT C LEARN REPR; Simonyan Karen, 2015, INT C LEARN REPR; Song Y., 2018, P 32 INT C NEUR INF, V31, P8322; Szegedy C., 2014, P INT C LEARN REPR; Szegedy C, 2015, P 3 INT C LEARN REPR; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tabak EG, 2013, COMMUN PUR APPL MATH, V66, P145, DOI 10.1002/cpa.21423; Tu CC, 2019, AAAI CONF ARTIF INTE, P742; Vladu A, 2018, P 6 INT C LEARN REPR; Wang H., 2019, P 7 INT C LEARN REPR; Wasserman L., 2013, ALL STAT CONCISE COU; Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255; Wong E., 2020, P 8 INT C LEARN REPR; Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905; Yu XY, 2019, IEEE T NEUR NET LEAR, V30, P2805, DOI 10.1109/TNNLS.2018.2886017; Zagoruyko S., 2016, P BMVC; Zhao H, 2019, CHIN CONT DECIS CONF, P2483, DOI 10.1109/CCDC.2019.8832713; Zisselman Ev, 2020, 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). Proceedings, P13991, DOI 10.1109/CVPR42600.2020.01401; Zugner D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6246, DOI 10.1145/3219819.3220078	65	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000072
C	Dong, YP; Deng, ZJ; Pang, TY; Zhu, J; Su, H		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Dong, Yinpeng; Deng, Zhijie; Pang, Tianyu; Zhu, Jun; Su, Hang			Adversarial Distributional Training for Robust Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Adversarial training (AT) is among the most effective techniques to improve model robustness by augmenting training data with adversarial examples. However, most existing AT methods adopt a specific attack to craft adversarial examples, leading to the unreliable robustness against other unseen attacks. Besides, a single attack algorithm could be insufficient to explore the space of perturbations. In this paper, we introduce adversarial distributional training (ADT), a novel framework for learning robust models. ADT is formulated as a minimax optimization problem, where the inner maximization aims to learn an adversarial distribution to characterize the potential adversarial examples around a natural one under an entropic regularizer, and the outer minimization aims to train robust models by minimizing the expected loss over the worst-case adversarial distributions. Through a theoretical analysis, we develop a general algorithm for solving ADT, and present three approaches for parameterizing the adversarial distributions, ranging from the typical Gaussian distributions to the flexible implicit ones. Empirical results on several benchmarks validate the effectiveness of ADT compared with the state-of-the-art AT methods.	[Dong, Yinpeng; Deng, Zhijie; Pang, Tianyu; Zhu, Jun; Su, Hang] Tsinghua Univ, Tsinghua Bosch Joint ML Ctr, THBI Lab, Dept Comp Sci & Tech,Inst AI,BNRist Ctr, Beijing 100084, Peoples R China	Tsinghua University	Su, H (corresponding author), Tsinghua Univ, Tsinghua Bosch Joint ML Ctr, THBI Lab, Dept Comp Sci & Tech,Inst AI,BNRist Ctr, Beijing 100084, Peoples R China.	dyp17@mails.tsinghua.edu.cn; dzj17@mails.tsinghua.edu.cn; pty17@mails.tsinghua.edu.cn; suhangss@mail.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn			National Key Research and Development Program of China [2020AAA0104304]; NSFC [61620106010, 62076147, U19B2034, U1811461]; Tsinghua-Huawei Joint Research Program; Tsinghua Institute; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; GPU/DGX Acceleration; MSRA fellowship; Baidu fellowship; Beijing Academy of Artificial Intelligence (BAAI)	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Tsinghua-Huawei Joint Research Program; Tsinghua Institute; Tiangong Institute for Intelligent Computing; NVIDIA NVAIL Program; GPU/DGX Acceleration; MSRA fellowship; Baidu fellowship; Beijing Academy of Artificial Intelligence (BAAI)	This work was supported by the National Key Research and Development Program of China (No.2020AAA0104304), NSFC Projects (Nos. 61620106010, 62076147, U19B2034, U1811461), Beijing Academy of Artificial Intelligence (BAAI), Tsinghua-Huawei Joint Research Program, a grant from Tsinghua Institute for Guo Qiang, Tiangong Institute for Intelligent Computing, and the NVIDIA NVAIL Program with GPU/DGX Acceleration. Yinpeng Dong was supported by MSRA and Baidu fellowships.	Alayrac Jean-Baptiste, 2019, ADV NEURAL INFORM PR; [Anonymous], 2019, INT C MACH LEARN ICM; [Anonymous], 2018, INT C LEARN REPR ICL; [Anonymous], 2018, P IEEE C COMP VIS PA; [Anonymous], 2016, P INT C LEARNING REP; Athalye Anish, 2018, INT C MACH LEARN ICM; Baluja S., 2017, ARXIV170309387; Ben-Tal A, 2013, MANAGE SCI, V59, P341, DOI 10.1287/mnsc.1120.1641; Blundell C., 2015, INT C MACH LEARN ICM; Carlini N., 2019, CORR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carmon Yair, 2019, ADV NEURAL INFORM PR; Chen Zhehui, 2018, ARXIV181101213; Cheng Shuyu, 2019, ADV NEURAL INFORM PR; Cohen Jeremy M, 2019, INT C MACH LEARN ICM; Dai Zihang, 2017, INT C LEARN REPR ICL; Dai Zihang, 2017, ADV NEURAL INFORM PR; Danskin J. M., 2012, THEORY MAX MIN ITS A, V5; Dong Y., 2018, P IEEE C COMP VIS PA; Dong Yinpeng, 2020, P IEEE CVF C COMP VI; Engstrom L., 2017, NIPS 2017 WORKSH MAC; Esfahani PM, 2018, MATH PROGRAM, V171, P115, DOI 10.1007/s10107-017-1172-1; Ford Nic, 2019, INT C MACH LEARN ICM; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Haarnoja T., 2018, INT C MACH LEARN ICM INT C MACH LEARN ICM; Hendrycks Dan, 2019, INT C LEARN REPR ICL; Hendrycks Dan, 2019, INT C MACH LEARN ICM; Ilyas Andrew, 2018, INT C MACH LEARN ICM; Isola P, 2017, P 2017 IEEE C COMP V; Jang Yunseok, 2019, P IEEE INT C COMP VI; Johnson J., 2016, EUR C COMP VIS ECCV, P694; Jolliffe I. T., 1986, PRINCIPAL COMPONENT, V87, P513, DOI DOI 10.1007/978-1-4757-1904-8_8; Kannan Harini, 2018, ARXIV180306373; Kingma D.P., 2014, INT C LEARN REPR ICL, Patent No. [1312.6114, 13126114]; Kingma DP, 2015, INT C LEARN REPR ICL; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin A., 2017, INT C LEARN REPR ICL; Li Yandong, 2019, INT C MACH LEARN ICM; Liao F., 2018, P IEEE C COMP VIS PA; Louizos C., 2017, INT C MACH LEARN ICM; Louizos Christos, 2016, INT C MACH LEARN ICM; Madry Aleksander, 2018, 6 INT C LEARN REPR I; Mao Chengzhi, 2019, ADV NEURAL INFORM PR; Netzer Y., 2011, NIPS WORKSH DEEP LEA, P14; Oh Seong Joon, 2017, P IEEE INT C COMP VI; Pang T., 2020, 8 INT C LEARN REPR I; Pang Tianyu, 2019, INT C MACH LEARN ICM; Pang Tianyu, 2018, ADV NEURAL INFORM PR; Qin C., 2019, ADV NEURAL INFORM PR, p13 847; Rice Leslie, 2020, INT C MACH LEARN ICM, P8093; Salman Hadi, 2019, ADV NEURAL INFORM PR; Shafahi Ali, 2019, ADV NEURAL INFORM PR; Shi Jiaxin, 2018, INT C MACH LEARN ICM; Shi Jiaxin, 2018, INT C LEARN REPR ICL; Sinha Aman, 2018, INT C LEARN REPR ICL; Song Chuanbiao, 2019, INT C LEARN REPR ICL; Staib M, 2017, NIPS MACHINE LEARN C; Sun YH, 2017, IEEE ICC; Szegedy Christian, 2014, PROC 2 INT C LEARN R; Tashiro Y., 2020, ARXIV200306878; Tramer F., 2019, ADV NEURAL INFORM PR; Tramer F., 2020, ARXIV200208347; Tsipras Dimitris, 2019, INT C LEARN REPR ICL; Uesato Jonathan, 2018, INT C MACH LEARN ICM; Wang H., 2019, INT C LEARN REPR ICL; Wong E., 2020, INT C LEARN REPR ICL; Wong Eric, 2018, INT C MACH LEARN ICM; Xiao C., 2020, INT C LEARN REPR ICL; Xiao Chaowei, 2018, INT JOINT C ART INT; Xie C., 2019, P IEEE C COMP VIS PA; Yang X., 2020, PRIVACY PROTECTION G; Zagoruyko S., 2016, P BMVC; Zhai R., 2019, ARXIV190600555; Zhang D., 2019, ADV NEURAL INFORM PR; Zhang Haichao, 2019, ADV NEURAL INFORM PR	77	0	0	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													14	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000058
C	El Hanchi, A; Stephens, DA		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		El Hanchi, Ayoub; Stephens, David A.			Adaptive Importance Sampling for Finite-Sum Optimization and Sampling with Decreasing Step-Sizes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				INCLUSION PROBABILITIES	Reducing the variance of the gradient estimator is known to improve the convergence rate of stochastic gradient-based optimization and sampling algorithms. One way of achieving variance reduction is to design importance sampling strategies. Recently, the problem of designing such schemes was formulated as an online learning problem with bandit feedback, and algorithms with sub-linear static regret were designed. In this work, we build on this framework and propose Avare, a simple and efficient algorithm for adaptive importance sampling for finite-sum optimization and sampling with decreasing step-sizes. Under standard technical conditions, we show that Avare achieves O (T-2/3) and O(T-5/6) dynamic regret for SGD and SGLD respectively when run with O(1/t) step sizes. We achieve this dynamic regret bound by leveraging our knowledge of the dynamics defined by the algorithm, and combining ideas from online learning and variance-reduced stochastic optimization. We validate empirically the performance of our algorithm and identify settings in which it leads to significant improvements.	[El Hanchi, Ayoub; Stephens, David A.] McGill Univ, Montreal, PQ, Canada	McGill University	El Hanchi, A (corresponding author), McGill Univ, Montreal, PQ, Canada.	ayoub.elhanchi@mail.mcgill.ca; david.stephens@mcgill.ca			NSERC discovery grant	NSERC discovery grant(Natural Sciences and Engineering Research Council of Canada (NSERC))	This research was supported by an NSERC discovery grant. We would like to thank the anonymous reviewers for their useful comments and suggestions.	Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Besbes O, 2015, OPER RES, V63, P1227, DOI 10.1287/opre.2015.1408; Borsos Z., 2019, INT C MACH LEARN, V97, P705; Borsos Zalan, 2018, P MACHINE LEARNING R, V75, P324; Bouchard Guillaume, 2015, ACCELERATING STOCHAS; Chang C.-C., 2011, ACM T INTEL SYST TEC, V2, P1, DOI [10.1145/1961189.1961199, DOI 10.1145/1961189.1961199]; Chen Xi, 2017, NONSTATIONARY STOCHA; Csiba D, 2018, J MACH LEARN RES, V19; Defazio A, 2016, ADV NEUR IN, V29; Defazio Aaron, 2014, ABS14070202 CORR; Gyorgy Andras, 2016, JMLR WORKSHOP C P, V48, P2943; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Herbster M, 2001, J MACH LEARN RES, V1, P281, DOI 10.1162/153244301753683726; Herbster M., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P286; Hofmann Thomas, 2015, ADV NEURAL INFORM PR, P2305; Johnson Tyler B, 2018, ADV NEURAL INFORM PR, P7276; Katharopoulos A, 2018, PR MACH LEARN RES, V80; Kochar SC, 2001, ANN I STAT MATH, V53, P631, DOI 10.1023/A:1014693702392; MILBRODT H, 1992, STAT PROBABIL LETT, V14, P243, DOI 10.1016/0167-7152(92)90029-5; Mokhtari A, 2016, IEEE DECIS CONTR P, P7195, DOI 10.1109/CDC.2016.7799379; Namkoong H, 2017, PR MACH LEARN RES, V70; Needell D, 2016, MATH PROGRAM, V155, P549, DOI 10.1007/s10107-015-0864-7; Nemirovsky A. S., 1984, J OPERATIONAL RES SO; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Salehi Farnood, 2017, ABS17080 CORR; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Stich Sebastian U., 2019, UNIFIED OPTIMAL ANAL; Stich Sebastian U, 2017, ADV NEURAL INFORM PR, P4381; Stoltz G., 2012, P ADV NEUR INF PROC, P471; Welling Max, 2011, P 28 TH INT C MACH L, P681; Yang TB, 2016, PR MACH LEARN RES, V48; Yu YM, 2012, BERNOULLI, V18, P279, DOI 10.3150/10-BEJ337; Zhang LJ, 2018, ADV NEUR IN, V31; Zhao PL, 2015, PR MACH LEARN RES, V37, P1; Zinkevich Martin, 2003, P INT C MACH LEARN, P928	36	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000038
C	Gamella, JL; Heinze-Deml, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Gamella, Juan L.; Heinze-Deml, Christina			Active Invariant Causal Prediction: Experiment Selection through Stability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				INFERENCE; MODELS	A fundamental difficulty of causal learning is that causal models can generally not be fully identified based on observational data only. Interventional data, that is, data originating from different experimental environments, improves identifiability. However, the improvement depends critically on the target and nature of the interventions carried out in each experiment. Since in real applications experiments tend to be costly, there is a need to perform the right interventions such that as few as possible are required. In this work we propose a new active learning (i.e. experiment selection) framework (A-ICP) based on Invariant Causal Prediction (ICP) [27]. For general structural causal models, we characterize the effect of interventions on so-called stable sets, a notion introduced by [30]. We leverage these results to propose several intervention selection policies for A-ICP which quickly reveal the direct causes of a response variable in the causal graph while maintaining the error control inherent in ICP. Empirically, we analyze the performance of the proposed policies in both population and finite-regime experiments.	[Gamella, Juan L.; Heinze-Deml, Christina] Swiss Fed Inst Technol, Seminar Stat, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Gamella, JL (corresponding author), Swiss Fed Inst Technol, Seminar Stat, Zurich, Switzerland.	gajuan@ethz.ch; heinzedeml@stat.math.ethz.ch		Gamella, Juan L./0000-0001-9728-1125	"la Caixa" Foundation [100010434, LCF/BQ/EU18/11650051]	"la Caixa" Foundation(La Caixa Foundation)	We would like to thank Niklas Pfister, Jonas Peters, Armeen Taeb, Brian McWilliams and Nicolai Meinshausen for valuable discussions and comments on the manuscript. The research leading to these results was supported by a grant from the "la Caixa" Foundation (ID 100010434), with code LCF/BQ/EU18/11650051.	Agrawal R., 2019, P 22 INT C ARTIFICIA, P3400; Bollen K. A, 1989, STRUCTURAL EQUATIONS; Buhlmann P, 2014, ANN STAT, V42, P2526, DOI 10.1214/14-AOS1260; Cho H, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0150611; Colquhoun D, 2014, ROY SOC OPEN SCI, V1, DOI 10.1098/rsos.140216; Eberhardt F., 2008, P 24 C UNC ART INT U, P161; Ghassami A., 2019, ARXIV191005651; Ghassami A, 2018, PR MACH LEARN RES, V80; Haavelmo T, 1943, ECONOMETRICA, V11, P1, DOI 10.2307/1905714; Hauser A, 2014, INT J APPROX REASON, V55, P926, DOI 10.1016/j.ijar.2013.11.007; Hauser A, 2012, J MACH LEARN RES, V13, P2409; He YB, 2008, J MACH LEARN RES, V9, P2523; Heinze-Deml C., 2017, ARXIV171011469; Heinze-Deml C, 2018, J CAUSAL INFERENCE, V6, DOI 10.1515/jci-2017-0016; Hoyer P. O., 2009, ADV NEURAL INFORM PR, V21, P689; Hyttinen A, 2013, J MACH LEARN RES, V14, P3041; Kocaoglu M., 2017, ADV NEURAL INFORM PR, V30, P7018; Kocaoglu M, 2017, PR MACH LEARN RES, V70; Lindgren E., 2018, ADV NEURAL INFORM PR, P5279; Masegosa AR, 2013, INT J APPROX REASON, V54, P1168, DOI 10.1016/j.ijar.2013.03.009; Naik AW, 2016, ELIFE, V5, DOI 10.7554/eLife.10047; Ness RO, 2017, LECT N BIOINFORMAT, V10229, P134, DOI 10.1007/978-3-319-56970-3_9; Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057; Peters J, 2017, ADAPT COMPUT MACH LE; Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043; Peters J., 2011, P 27 C UNC ART INT U, P589; Peters J, 2016, J R STAT SOC B, V78, P947, DOI 10.1111/rssb.12167; Pfister N., 2019, ARXIV191101850STATME; Pfister N, 2019, J AM STAT ASSOC, V114, P1264, DOI 10.1080/01621459.2018.1491403; Rothenhausler D., 2018, ARXIV PREPRINT ARXIV; Settles Burr, 2009, TR1648 U WISC MAD DE, P2; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Subbaswamy A, 2019, PR MACH LEARN RES, V89; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tong S., 2001, INT JOINT C ARTIFICI, P863; Walker SG, 2013, J STAT PLAN INFER, V143, P1621, DOI 10.1016/j.jspi.2013.05.013; Wright S, 1920, J AGRIC RES, V20, P0557	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000027
C	Glynn, P; Johari, R; Rasouli, M		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Glynn, Peter; Johari, Ramesh; Rasouli, Mohammad			Adaptive Experimental Design with Temporal Interference: A Maximum Likelihood Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				CAUSAL INFERENCE; VARIANCE	Suppose an online platform wants to compare a treatment and control policy, e.g., two different matching algorithms in a ridesharing system, or two different inventory management algorithms in an online retail site. Standard experimental approaches to this problem are biased (due to temporal interference between the policies), and not sample efficient. We study optimal experimental design for this setting. We view testing the two policies as the problem of estimating the steady state difference in reward between two unknown Markov chains (i.e., policies). We assume estimation of the steady state reward for each chain proceeds via non-parametric maximum likelihood, and search for consistent (i.e., asymptotically unbiased) experimental designs that are efficient (i.e., asymptotically minimum variance). Characterizing such designs is equivalent to a Markov decision problem with a minimum variance objective; such problems generally do not admit tractable solutions. Remarkably, in our setting, using a novel application of classical martingale analysis of Markov chains via Poisson's equation, we characterize efficient designs via a succinct convex optimization problem. We use this characterization to propose a consistent, efficient online experimental design that adaptively samples the two Markov chains.	[Glynn, Peter; Johari, Ramesh; Rasouli, Mohammad] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Glynn, P (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	glynn@stanford.edu; rjohari@stanford.edu; rasoulim@stanford.edu			National Science Foundation [1931696, 1839229]	National Science Foundation(National Science Foundation (NSF))	The authors gratefully acknowledge helpful feedback from participants at the Workshop on Machine Learning and User Decision-Making, and at the Simons Institute. This work was supported by the National Science Foundation under grants 1931696 and 1839229. In addition, Ramesh Johari was a technical advisor on experimental design for Uber Technologies, Inc., for a part of the period in which this work was completed.	Asmussen S., 2003, APPL PROBABILITY QUE; Athey S, 2018, J AM STAT ASSOC, V113, P230, DOI 10.1080/01621459.2016.1241178; Basse GW, 2016, JMLR WORKSH CONF PRO, V51, P1412; Brandt A., 1938, RES B IOWA AGR HOME, V21, P1; Chamandy N, 2016, EXPERIMENTATION RIDE; Dann C., 2017, ADV NEURAL INFORM PR, P5713; Di Castro D., 2012, PREPRINT; Eckles D, 2017, J CAUSAL INFERENCE, V5, DOI 10.1515/jci-2015-0021; FILAR JA, 1989, MATH OPER RES, V14, P147, DOI 10.1287/moor.14.1.147; Glynn P., 2020, ARXIV200605591; Hall P., 1980, MARTINGALE LIMIT THE; Hudgens MG, 2008, J AM STAT ASSOC, V103, P832, DOI 10.1198/016214508000000292; Iancu DA, 2015, MATH OPER RES, V40, P655, DOI 10.1287/moor.2014.0689; Johari Ramesh, 2020, ARXIV200205670; Kastelman D, 2018, SWITCHBACK TESTS RAN; Kohavi R, 2009, DATA MIN KNOWL DISC, V18, P140, DOI 10.1007/s10618-008-0114-1; LUCAS HL, 1956, J DAIRY SCI, V39, P146, DOI 10.3168/jds.S0022-0302(56)94721-X; Mannor S., 2011, ARXIV11045601; Manski CF, 2013, ECONOMET J, V16, pS1, DOI 10.1111/j.1368-423X.2012.00368.x; Michael Ostrovsky, 2011, P 12 ACM C ELECT COM, P59, DOI DOI 10.1145/1993574.1993585; Putta SR, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P1703; Sobel ME, 2006, J AM STAT ASSOC, V101, P1398, DOI 10.1198/016214506000000636; SOBEL MJ, 1994, OPER RES, V42, P175, DOI 10.1287/opre.42.1.175; SOBEL MJ, 1982, J APPL PROBAB, V19, P794, DOI 10.2307/3213832; Wager S., 2019, ARXIV190302124; Yu PQ, 2018, IEEE T AUTOMAT CONTR, V63, P3135, DOI 10.1109/TAC.2018.2790261	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000035
C	Hassidim, A; Kaplan, H; Mansour, Y; Matias, Y; Stemmer, U		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Hassidim, Avinatan; Kaplan, Haim; Mansour, Yishay; Matias, Yossi; Stemmer, Uri			Adversarially Robust Streaming Algorithms via Differential Privacy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				FREQUENT	A streaming algorithm is said to be adversarially robust if its accuracy guarantees are maintained even when the data stream is chosen maliciously, by an adaptive adversary. We establish a connection between adversarial robustness of streaming algorithms and the notion of differential privacy. This connection allows us to design new adversarially robust streaming algorithms that outperform the current state-of-the-art constructions for many interesting regimes of parameters.	[Hassidim, Avinatan] Bar Ilan Univ, Ramat Gan, Israel; [Hassidim, Avinatan; Kaplan, Haim; Mansour, Yishay; Matias, Yossi; Stemmer, Uri] Google, Tel Aviv, Israel; [Kaplan, Haim; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Stemmer, Uri] Ben Gurion Univ Negev, Beer Sheva, Israel	Bar Ilan University; Google Incorporated; Tel Aviv University; Ben Gurion University	Hassidim, A (corresponding author), Bar Ilan Univ, Ramat Gan, Israel.; Hassidim, A (corresponding author), Google, Tel Aviv, Israel.				Israel Science Foundation [1595/19, 993/17, 1871/19]; German-Israeli Foundation [1367/2017]; Blavatnik Family Foundation; European Research Council (ERC) under the European Union [882396]; Cyber Security Research Center at Ben-Gurion University of the Negev	Israel Science Foundation(Israel Science Foundation); German-Israeli Foundation(German-Israeli Foundation for Scientific Research and Development); Blavatnik Family Foundation; European Research Council (ERC) under the European Union(European Research Council (ERC)); Cyber Security Research Center at Ben-Gurion University of the Negev	Haim Kaplan is partially supported by Israel Science Foundation (grant 1595/19), German-Israeli Foundation (grant 1367/2017), and the Blavatnik Family Foundation. Yishay Mansour has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation program (grant agreement No. 882396), and by the Israel Science Foundation (grant number 993/17). Uri Stemmer is partially supported by the Israel Science Foundation (grant 1871/19), and by the Cyber Security Research Center at Ben-Gurion University of the Negev.	Ahn K. J, 2012, P 23 ANN ACM SLAM S, P459; Ahn K. J., 2012, P PODS; [Anonymous], 2002, VLDB; Bar-Yossef Ziv, 2002, INT WORKSH RAND APPR; Bassily R., 2016, STOC; Beimel A., 2018, FOCS; Beimel A, 2016, THEOR COMPUT, V12, DOI 10.4086/toc.2016.v012a001; Ben-Eliezer O., 2020, PODS; Ben-Eliezer O., 2020, PODS; Blasiok J., 2017, APPROX RANDOM; Bun M, 2018, ACM S THEORY COMPUT, P74, DOI 10.1145/3188745.3188946; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Cormode G, 2005, ACM T DATABASE SYST, V30, P249, DOI 10.1145/1061318.1061325; Datar M, 2002, SIAM J COMPUT, V31, P1794, DOI 10.1137/S0097539701398363; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2015, ACM S THEORY COMPUT, P117, DOI 10.1145/2746539.2746580; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Dwork C, 2009, ACM S THEORY COMPUT, P381; Elkin M, 2006, DISTRIB COMPUT, V18, P375, DOI 10.1007/s00446-005-0147-2; FLAJOLET P, 1985, J COMPUT SYST SCI, V31, P182, DOI 10.1016/0022-0000(85)90041-8; Gilbert A. C., 2012, 2012 Information Theory and Applications Workshop (ITA), P382, DOI 10.1109/ITA.2012.6181772; Gilbert AC, 2012, 2012 IEEE STATISTICAL SIGNAL PROCESSING WORKSHOP (SSP), P536, DOI 10.1109/SSP.2012.6319752; Hardt M., 2013, STOC; Hardt M., 2010, FOCS; Hardt M, 2014, ANN IEEE SYMP FOUND, P454, DOI 10.1109/FOCS.2014.55; Indyk P., 2005, STOC; Jayaram R., 2018, PODS; Kane D. M., 2010, SODA; Kane D. M., 2010, PODS; Kaplan H., 2020, COLT; Kellaris Georgios, 2017, ABS170601552 CORR; McSherry Frank, 2007, FOCS; Mironov I, 2011, SIAM J COMPUT, V40, P1845, DOI 10.1137/080733772; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Nelson J., 2011, THESIS; Nissim K., 2018, NEURIPS; Nissim K., 2019, J PRIV CONFIDENTIALI, V9; Steinke T., 2015, COLT	41	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000063
C	Jamroz, M; Kurdziel, M; Opala, M		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Jamroz, Michal; Kurdziel, Marcin; Opala, Mateusz			A Bayesian Nonparametrics View into Deep Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				NUMBER	We investigate neural network representations from a probabilistic perspective. Specifically, we leverage Bayesian nonparametrics to construct models of neural activations in Convolutional Neural Networks (CNNs) and latent representations in Variational Autoencoders (VAEs). This allows us to formulate a tractable complexity measure for distributions of neural activations and to explore global structure of latent spaces learned by VAEs. We use this machinery to uncover how memorization and two common forms of regularization, i.e. dropout and input augmentation, influence representational complexity in CNNs. We demonstrate that networks that can exploit patterns in data learn vastly less complex representations than networks forced to memorize. We also show marked differences between effects of input augmentation and dropout, with the latter strongly depending on network width. Next, we investigate latent representations learned by standard fi -VAEs and Maximum Mean Discrepancy (MMD) fi-VAEs. We show that aggregated posterior in standard VAEs quickly collapses to the diagonal prior when regularization strength increases. MMD-VAEs, on the other hand, learn more complex posterior distributions, even with strong regularization. While this gives a richer sample space, MMD-VAEs do not exhibit independence of latent dimensions. Finally, we leverage our probabilistic models as an effective sampling strategy for latent codes, improving quality of samples in VAEs with rich posteriors.	[Jamroz, Michal; Kurdziel, Marcin; Opala, Mateusz] AGH Univ Sci & Technol, Krakow, Poland	AGH University of Science & Technology	Jamroz, M (corresponding author), AGH Univ Sci & Technol, Krakow, Poland.	mijamroz@agh.edu.pl; kurdziel@agh.edu.pl; mo@matthewopala.com		Kurdziel, Marcin/0000-0003-2022-7424	Polish Ministry of Science and Higher Education; PL-Grid Infrastructure	Polish Ministry of Science and Higher Education(Ministry of Science and Higher Education, Poland); PL-Grid Infrastructure	Research presented in this work was supported by funds assigned to AGH University of Science and Technology by the Polish Ministry of Science and Higher Education. This research was supported in part by PL-Grid Infrastructure.	Arpit D, 2017, PR MACH LEARN RES, V70; Ashtiani H., 2018, P 32 INT C NEURAL IN, P3416; Burgess CP, 2018, ARXIV180403599; Chen Xinyun, 2017, ARXIV171205526; Galstyan A, 2014, ADV NEURAL INFORM PR, P577; Gao SY, 2019, PR MACH LEARN RES, V89; Gilpin LH, 2018, PR INT CONF DATA SC, P80, DOI 10.1109/DSAA.2018.00018; Gretton A, 2012, J MACH LEARN RES, V13, P723; Higgins I., 2017, P INT C LEARN REPR T; Kingma D.P, P 3 INT C LEARNING R; Kornblith S, 2019, PR MACH LEARN RES, V97; LIU JS, 1994, BIOMETRIKA, V81, P27, DOI 10.1093/biomet/81.1.27; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Miller J. W., 2013, ADV NEURAL INFORM PR, P199; Miller JW, 2018, J AM STAT ASSOC, V113, P340, DOI 10.1080/01621459.2016.1255636; Miller JW, 2014, J MACH LEARN RES, V15, P3333; Montavon G, 2011, J MACH LEARN RES, V12, P2563; Morcos Ari S., 2018, NEURIPS; Neal RM, 2000, J COMPUT GRAPH STAT, V9, P249, DOI 10.2307/1390653; Raghu Maithra, 2017, ADV NEURAL INFORM PR, P6076; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490; Zhao SJ, 2019, AAAI CONF ARTIF INTE, P5885	26	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000005
C	Jeewajee, AK; Kaelbling, LP		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Jeewajee, Adarsh K.; Kaelbling, Leslie P.			Adversarially-learned Inference via an Ensemble of Discrete Undirected Graphical Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Undirected graphical models are compact representations of joint probability distributions over random variables. To solve inference tasks of interest, graphical models of arbitrary topology can be trained using empirical risk minimization However, to solve inference tasks that were not seen during training, these models (EGMs) often need to be re-trained. Instead, we propose an inference-agnostic adversarial training framework which produces an infinitely-large ensemble of graphical models (AGMs). The ensemble is optimized to generate data within the GAN framework, and inference is performed using a finite subset of these models. AGMs perform comparably with EGMs on inference tasks that the latter were specifically optimized for. Most importantly, AGMs show significantly better generalization to unseen inference tasks compared to EGMs, as well as deep neural architectures like GibbsNet and VAEAC which allow arbitrary conditioning. Finally, AGMs allow fast data sampling, competitive with Gibbs sampling from EGMs.	[Jeewajee, Adarsh K.; Kaelbling, Leslie P.] MIT CSAIL, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Jeewajee, AK (corresponding author), MIT CSAIL, Cambridge, MA 02139 USA.	jaks19@mit.edu; lpk@csail.mit.edu			NSF [1723381]; AFOSR [FA9550-17-10165]; ONR [N00014-18-1-2847]; Honda Research Institute; MIT-IBM Watson AI Lab; SUTD Temasek Laboratories	NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); ONR(Office of Naval Research); Honda Research Institute; MIT-IBM Watson AI Lab(International Business Machines (IBM)); SUTD Temasek Laboratories	We gratefully acknowledge support from NSF grant 1723381; from AFOSR grant FA9550-17-10165; from ONR grant N00014-18-1-2847; from the Honda Research Institute; from the MIT-IBM Watson AI Lab; and from SUTD Temasek Laboratories. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of our sponsors.	Ammar Waleed, 2014, ABS14111147 CORR; Antonucci Alessandro, 2013, 23 INT JOINT C ART I; Bahler Dennis, 2000, METHODS COMBINING HE; Baruque B., 2010, STUDIES COMPUTATIONA; Battaglia Peter W, 2018, ARXIV180601261; Belghazi MI, 2019, ADV NEUR IN, V32; Bertinetto Luca, 2016, ABS160605233 CORR; Bixler Reid, 2018, UAI; Bojchevski Aleksandar, 2018, ICML; Camino Ramiro, 2018, ABS180701202 ARXIV; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Choi E., 2017, MLHC; Domke J, 2013, IEEE T PATTERN ANAL, V35, P2454, DOI 10.1109/TPAMI.2013.31; Domke Justin, 2010, NIPS, V23, P523; Donahue J., 2016, ARXIV160509782; Dong Hao-Wen, 2018, ABS181004714 CORR; Douglas Laura, 2017, ABS171100695 CORR; Dumoulin Vincent, 2017, ABS160600704 ARXIV; Fathony Rizal, 2018, DISCRETE WASSERSTEIN; Fathony Rizal, 2018, ADV NEURAL INFORM PR, V31, P8344; Freund Y., 1999, Journal of Japanese Society for Artificial Intelligence, V14, P771; Fumera G, 2005, IEEE T PATTERN ANAL, V27, P942, DOI 10.1109/TPAMI.2005.109; Gens R., 2013, 30 INT C MACHINE LEA, P873; Gilmer J, 2017, PR MACH LEARN RES, V70; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani Ishaan, 2017, ABS170400028 ARXIV; Hinton G., 2015, ARXIV150302531; Hjelm R. Devon, 2017, ABS170208431 ARXIV; Husmeier Dirk, 1999, NEURAL NETWORKS COND; Ivanov Oleg, 2019, VARIATIONAL AUTOENCO; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Jang E., 2017, ABS161101144 ARXIV; Karaletsos Theofanis, 2016, ABS161205048 ARXIV; Koller D., 2009, PROBABILISTIC GRAPHI; Kuleshov Volodymyr, 2017, ABS171102679 CORR; Kulesza Alex, 2008, ADV NEURAL INFORM PR, V20; Lamb A, 2017, ADV NEUR IN, V30; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li C., 2018, P ADV NEUR INF PROC, P6069; Li FF, 2007, COMPUT VIS IMAGE UND, V106, P59, DOI 10.1016/j.cviu.2005.09.012; Liu YH, 2019, PHYS REV LETT, V122, DOI 10.1103/PhysRevLett.122.200501; Mirza M., 2014, ARXIV; Munkhdalai Tsendsuren, 2017, ABS170300837 CORR; Murphy K, 2013, LOOPY BELIEF PROPAGA; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Qu Meng, 2019, ICML; Satorras Victor Garcia, 2020, ABS200301998 ARXIV; Satorras Victor Garcia, 2019, ABS190602547 ARXIV; Situ Haozhen, 2018, QUANTUM GENERATIVE A; Stoyanov Veselin, 2011, P AISTATS; Tompson Jonathan, 2014, ABS14062984 CORR; Tu Lifu, 2018, ABS180303376 CORR; Wainwright Martin, 2002, TREE REWEIGHTED BELI, V21, P12; Wang Hongwei, 2017, ABS171108267 CORR; Xu L., 2019, ABS190700503 CORR; Yoon KiJung, 2018, ABS180307710 CORR; Zhang Zhen, 2019, ABS190600554 CORR; Zheng Shuai, 2015, ABS150203240 CORR; Zhong Zilong, 2018, ABS180203495 CORR	60	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000061
C	Ke, ZQ; Vikalo, H		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Ke, Ziqi; Vikalo, Haris			A Convolutional Auto-Encoder for Haplotype Assembly and Viral Quasispecies Reconstruction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				GENOME SEQUENCE; ALGORITHMS; INFERENCE	Haplotype assembly and viral quasispecies reconstruction are challenging tasks concerned with analysis of genomic mixtures using sequencing data. High-throughput sequencing technologies generate enormous amounts of short fragments (reads) which essentially oversample components of a mixture; the representation redundancy enables reconstruction of the components (haplotypes, viral strains). The reconstruction problem, known to be NP-hard, boils down to grouping together reads originating from the same component in a mixture. Existing methods struggle to solve this problem with required level of accuracy and low runtimes; the problem is becoming increasingly more challenging as the number and length of the components increase. This paper proposes a read clustering method based on a convolutional auto-encoder designed to first project sequenced fragments to a low-dimensional space and then estimate the probability of the read origin using learned embedded features. The components are reconstructed by finding consensus sequences that agglomerate reads from the same origin. Mini-batch stochastic gradient descent and dimension reduction of reads allow the proposed method to efficiently deal with massive numbers of long reads. Experiments on simulated, semi-experimental and experimental data demonstrate the ability of the proposed method to accurately reconstruct haplotypes and viral quasispecies, often demonstrating superior performance compared to state-of-the-art methods. Source codes are available at https://github.com/WuLoli/CAECseq.	[Ke, Ziqi; Vikalo, Haris] Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA	University of Texas System; University of Texas Austin	Ke, ZQ (corresponding author), Univ Texas Austin, Dept Elect & Comp Engn, Austin, TX 78712 USA.	ziqike@utexas.edu; hvikalo@ece.utexas.edu			NSF [CCF 1618427, 2027773]	NSF(National Science Foundation (NSF))	This work was funded in part by the NSF grants CCF 1618427 and 2027773.	Aguiar D, 2012, J COMPUT BIOL, V19, P577, DOI 10.1089/cmb.2012.0084; Ahn S, 2018, BIOINFORMATICS, V34, P23, DOI 10.1093/bioinformatics/bty291; Ahn S, 2017, LECT N BIOINFORMAT, V10229, P353, DOI 10.1007/978-3-319-56970-3_22; Astrovskaya I, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-S6-S1; Bansal V, 2008, GENOME RES, V18, P1336, DOI 10.1101/gr.077065.108; Berger E, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003502; Bonizzoni P, 2016, J COMPUT BIOL, V23, P718, DOI 10.1089/cmb.2015.0220; Cai CX, 2016, IEEE J-STSP, V10, P647, DOI 10.1109/JSTSP.2016.2547860; Chen ZZ, 2013, BIOINFORMATICS, V29, P1938, DOI 10.1093/bioinformatics/btt349; Clark AG, 2004, GENET EPIDEMIOL, V27, P321, DOI 10.1002/gepi.20025; Das S, 2015, BMC GENOMICS, V16, DOI 10.1186/s12864-015-1408-5; Di Giallonardo F, 2014, NUCLEIC ACIDS RES, V42, DOI 10.1093/nar/gku537; Duitama J, 2010, P 1 ACM INT C BIOINF, P160, DOI DOI 10.1145/1854776.1854802; Duitama J, 2012, NUCLEIC ACIDS RES, V40, P2041, DOI 10.1093/nar/gkr1042; Edge P, 2017, GENOME RES, V27, P801, DOI 10.1101/gr.213462.116; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Guo XF, 2017, LECT NOTES COMPUT SC, V10635, P373, DOI 10.1007/978-3-319-70096-0_39; Hashemi A, 2018, BMC GENOMICS, V19, DOI 10.1186/s12864-018-4551-y; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang WC, 2012, BIOINFORMATICS, V28, P593, DOI 10.1093/bioinformatics/btr708; Ke ZQ, 2020, AAAI CONF ARTIF INTE, V34, P719; Kim JH, 2007, GENOME RES, V17, P1101, DOI 10.1101/gr.5894107; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Kuleshov V, 2014, BIOINFORMATICS, V30, pI379, DOI 10.1093/bioinformatics/btu484; Lancia G., 2001, Algorithms - ESA 2001. 9th Annual European Symposium. Proceedings (Lecture Notes in Computer Science Vol.2161), P182; Levy S, 2007, PLOS BIOL, V5, P2113, DOI 10.1371/journal.pbio.0050254; Li H, 2010, BIOINFORMATICS, V26, P589, DOI 10.1093/bioinformatics/btp698; Lippert Ross, 2002, Brief Bioinform, V3, P23, DOI 10.1093/bib/3.1.23; Motazedi E, 2018, BRIEF BIOINFORM, V19, P387, DOI 10.1093/bib/bbw126; Patterson M, 2015, J COMPUT BIOL, V22, P498, DOI 10.1089/cmb.2014.0157; Pirola Y, 2016, BIOINFORMATICS, V32, P1610, DOI 10.1093/bioinformatics/btv495; Prabhakaran S, 2014, IEEE ACM T COMPUT BI, V11, P182, DOI 10.1109/TCBB.2013.145; Prosperi MCF, 2012, BIOINFORMATICS, V28, P132, DOI 10.1093/bioinformatics/btr627; Puljiz Z, 2016, IEEE ACM T COMPUT BI, V13, P518, DOI 10.1109/TCBB.2015.2462367; Sabeti PC, 2002, NATURE, V419, P832, DOI 10.1038/nature01140; Schwartz R, 2010, COMMUN INF SYST, V10, P23; Socher Richard, 2011, P C EMP METH NAT LAN, P151; Topfer A, 2013, J COMPUT BIOL, V20, P113, DOI 10.1089/cmb.2012.0232; Wang RS, 2005, BIOINFORMATICS, V21, P2456, DOI 10.1093/bioinformatics/bti352; Xie MZ, 2016, BIOINFORMATICS, V32, P3735, DOI 10.1093/bioinformatics/btw537; Xu X, 2011, NATURE, V475, P189, DOI 10.1038/nature10158; Zagordi O, 2011, BMC BIOINFORMATICS, V12, DOI 10.1186/1471-2105-12-119; Zhou C, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P665, DOI 10.1145/3097983.3098052	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000026
C	Kilbertus, N; Kusner, MJ; Silva, R		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Kilbertus, Niki; Kusner, Matt J.; Silva, Ricardo			A Class of Algorithms for General Instrumental Variable Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				INFERENCE	Causal treatment effect estimation is a key problem that arises in a variety of real-world settings, from personalized medicine to governmental policy making. There has been a flurry of recent work in machine learning on estimating causal effects when one has access to an instrument. However, to achieve identifiability, they in general require one-size-fits-all assumptions such as an additive error model for the outcome. An alternative is partial identification, which provides bounds on the causal effect. Little exists in terms of bounding methods that can deal with the most general case, where the treatment itself can be continuous. Moreover, bounding methods generally do not allow for a continuum of assumptions on the shape of the causal effect that can smoothly trade off stronger background knowledge for more informative bounds. In this work, we provide a method for causal effect bounding in continuous distributions, leveraging recent advances in gradient-based methods for the optimization of computationally intractable objective functions. We demonstrate on a set of synthetic and real-world data that our bounds capture the causal effect when additive methods fail, providing a useful range of answers compatible with observation as opposed to relying on unwarranted structural assumptions.(1)	[Kilbertus, Niki] Helmholtz AI, Neuherberg, Germany; [Kusner, Matt J.; Silva, Ricardo] UCL, Alan Turing Inst, London, England; [Kilbertus, Niki] Max Planck Inst Intelligent Syst, Stuttgart, Germany; [Kilbertus, Niki] Univ Cambridge, Cambridge, England	University of London; University College London; Max Planck Society; University of Cambridge	Kilbertus, N (corresponding author), Helmholtz AI, Neuherberg, Germany.				Alan Turing Institute under EPSRC [EP/N510129/1]	Alan Turing Institute under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank Florian Gunsilius for useful discussions, providing code for his method and explaining how to prepare the Family Expenditure Survey dataset. We are grateful to Robin Evans, Arthur Gretton, Jiri Hron, Paul Rubenstein, and Rahul Singh for useful discussions and feedback. MK and RS acknowledge support from the The Alan Turing Institute under EPSRC grant EP/N510129/1. This work was partially done while RS was on a sabbatical at the Department of Statistics, University of Oxford.	Acemoglu D, 2001, AM ECON REV, V91, P1369, DOI 10.1257/aer.91.5.1369; Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1; Angrist JD, 1996, J AM STAT ASSOC, V91, P444; Balke A., 1994, Uncertainty in Artificial Intelligence. Proceedings of the Tenth Conference (1994), P46; Bennett A., 2019, ADV NEURAL INFORM PR, P3564; Blundell R, 2007, ECONOMETRICA, V75, P1613, DOI 10.1111/j.1468-0262.2007.00808.x; Bonet B., 2001, P 17 C UNC ART INT S, P48; Bradbury J., 2018, JAX COMPOSABLE TRANS; Chen XH, 2018, QUANT ECON, V9, P39, DOI 10.3982/QE722; Darolles S, 2011, ECONOMETRICA, V79, P1541, DOI 10.3982/ECTA6539; Diamond S, 2016, J MACH LEARN RES, V17; Drton M., 2009, OBERWOLFACH SEMINARS, V39; Evans RJ, 2018, ANN STAT, V46, P2623, DOI 10.1214/17-AOS1631; Gunsilius F., 2018, ARXIV180609517; Gunsilius F., 2020, ARXIV191009502; Gupta MR, 2020, PR MACH LEARN RES, V119; Hartford J, 2017, PR MACH LEARN RES, V70; Hestenes M. R., 1969, Journal of Optimization Theory and Applications, V4, P303, DOI 10.1007/BF00927673; Horowitz JL, 2011, ECONOMETRICA, V79, P347, DOI 10.3982/ECTA8662; Imbens GW, 2009, ECONOMETRICA, V77, P1481, DOI 10.3982/ECTA7108; Imbens GW, 2004, ECONOMETRICA, V72, P1845, DOI 10.1111/j.1468-0262.2004.00555.x; Kilbertus N., 2019, UAI, P213; Kingma P. D., 2014, P INT C LEARN REPR I, V1; Lewis G., 2018, ARXIV180307164; Manski C. F., 2007, IDENTIFICATION PREDI; Muandet K., 2020, ADV NEURAL INFORM PR; Navascues M., 2019, ARXIV170706476; Newey WK, 2003, ECONOMETRICA, V71, P1565, DOI 10.1111/1468-0262.00459; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Palmer TM, 2011, STATA J, V11, P345, DOI 10.1177/1536867X1101100302; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Pearl J., 1995, P 11 C UNC ART INT, P435; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Silva R., 2016, J MACHINE LEARNING R, V17, P1; Singh R., 2019, ADV NEURAL INFORM PR, P4595; Tian J., 2002, P 18 C UNC ART INT, P519; VanderWeele TJ, 2013, J CAUSAL INFERENCE, V1, P1, DOI 10.1515/jci-2012-0002; Wolfe E, 2019, J CAUSAL INFERENCE, V7, DOI 10.1515/jci-2017-0020; Wooldridge JM, 2010, ECONOMETRIC ANALYSIS OF CROSS SECTION AND PANEL DATA, 2ND EDITION, P3; Zhang J., 2020, R61 COL CAUS AI LAB	40	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000018
C	Lipshutz, D; Windolf, C; Golkar, S; Chklovskii, DB		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Lipshutz, David; Windolf, Charlie; Golkar, Siavash; Chklovskii, Dmitri B.			A biologically plausible neural network for Slow Feature Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				INDEPENDENT COMPONENT ANALYSIS; ALGORITHMS; INFORMATION	Learning latent features from time series data is an important problem in both machine learning and brain function. One approach, called Slow Feature Analysis (SFA), leverages the slowness of many salient features relative to the rapidly varying input signals. Furthermore, when trained on naturalistic stimuli, SFA reproduces interesting properties of cells in the primary visual cortex and hippocampus, suggesting that the brain uses temporal slowness as a computational principle for learning latent features. However, despite the potential relevance of SFA for modeling brain function, there is currently no SFA algorithm with a biologically plausible neural network implementation, by which we mean an algorithm operates in the online setting and can be mapped onto a neural network with local synaptic updates. In this work, starting from an SFA objective, we derive an SFA algorithm, called Bio-SFA, with a biologically plausible neural network implementation. We validate Bio-SFA on naturalistic stimuli.	[Lipshutz, David; Windolf, Charlie; Golkar, Siavash; Chklovskii, Dmitri B.] Flatiron Inst, Ctr Computat Neurosci, New York, NY 10010 USA; [Windolf, Charlie] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Chklovskii, Dmitri B.] NYU Med Ctr, Neurosci Inst, New York, NY 10016 USA	Columbia University; New York University	Lipshutz, D (corresponding author), Flatiron Inst, Ctr Computat Neurosci, New York, NY 10010 USA.	dlipshutz@flatironinstitute.org; c.windolf@columbia.edu; sgolkar@flatironinstitute.org; dchklovskii@flatironinstitute.org	Lipshutz, David/AAI-9543-2021	Lipshutz, David/0000-0001-9347-8326	Simons Foundation	Simons Foundation	This work was internally support by the Simons Foundation. We thank Yanis Bahroun, Nicholas Chua, Shiva Farashahi, Johannes Friedrich, Alexander Genkin, Jason Moore, Anirvan Sengupta and Tiberiu Tesileanu for helpful comments and feedback on an earlier draft of this work.	[Anonymous], 1990, ADV NEURAL INFORM PR; Berkes P, 2005, J VISION, V5, P579, DOI 10.1167/5.6.9; Berkes P, 2002, LECT NOTES COMPUT SC, V2415, P81; Blaschke T, 2004, IEEE T SIGNAL PROCES, V52, P1250, DOI 10.1109/TSP.2004.826173; Blaschke T, 2006, NEURAL COMPUT, V18, P2495, DOI 10.1162/neco.2006.18.10.2495; Clark D., 2019, ADV NEURAL INFORM PR, P14267; Cox T., 2000, MULTIDIMENSIONAL SCA; Creutzig F, 2008, NEURAL COMPUT, V20, P1026, DOI 10.1162/neco.2008.01-07-455; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Franzius M, 2007, PLOS COMPUT BIOL, V3, P1605, DOI 10.1371/journal.pcbi.0030166; Hyvarinen A, 1999, IEEE T NEURAL NETWOR, V10, P626, DOI 10.1109/72.761722; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Koch C., 1992, SINGLE NEURON COMPUT, P315; Kompella VR, 2012, NEURAL COMPUT, V24, P2994, DOI 10.1162/NECO_a_00344; Lipshutz David, 2020, ARXIV201000525; Liwicki Stephan, 2013, Computer Vision - ACCV 2012. 11th Asian Conference on Computer Vision. Revised Selected Papers, P162, DOI 10.1007/978-3-642-37444-9_13; Malik ZK, 2014, COGN COMPUT, V6, P595, DOI 10.1007/s12559-014-9257-0; Mitchison G, 1991, NEURAL COMPUT, V3, P312, DOI 10.1162/neco.1991.3.3.312; Noe F, 2015, J CHEM THEORY COMPUT, V11, P5002, DOI 10.1021/acs.jctc.5b00553; Pehlevan C., 2015, P 28 INT C NEUR INF; Perez-Hernandez G, 2013, J CHEM PHYS, V139, DOI 10.1063/1.4811489; Rumelhart D. E., 1986, PARALLEL DISTRIB PRO, V1, P26; Schonfeld F, 2015, FRONT COMPUT NEUROSC, V9, DOI 10.3389/fncom.2015.00051; Schonfeld F, 2013, FRONT COMPUT NEUROSC, V7, DOI 10.3389/fncom.2013.00104; Schwantes CR, 2015, J CHEM THEORY COMPUT, V11, P600, DOI 10.1021/ct5007357; Sultan MM, 2017, J CHEM THEORY COMPUT, V13, P2440, DOI 10.1021/acs.jctc.7b00182; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303; Weghenkel B, 2018, NEURAL COMPUT, V30, P1151, DOI [10.1162/NECO_a_01070, 10.1162/neco_a_01070]; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Wiskott L., 2003, CONDMAT0312317 ARXIV; Yousefi Bardia, 2012, 2012 INT JOINT C NEU; Zhang QF, 2000, IEEE T NEURAL NETWOR, V11, P200, DOI 10.1109/72.822522; Zhang Z, 2012, IEEE T PATTERN ANAL, V34, P436, DOI 10.1109/TPAMI.2011.157; Zito Tiziano, 2008, Front Neuroinform, V2, P8, DOI 10.3389/neuro.11.008.2008	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000008
C	Nandy, P; Basu, K; Chatterjee, S; Tu, Y		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Nandy, Preetam; Basu, Kinjal; Chatterjee, Shaunak; Tu, Ye			A/B Testing in Dense Large-Scale Networks: Design and Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Design of experiments and estimation of treatment effects in large-scale networks, in the presence of strong interference, is a challenging and important problem. Most existing methods' performance deteriorates as the density of the network increases. In this paper, we present a novel strategy for accurately estimating the causal effects of a class of treatments in a dense large-scale network. First, we design an approximate randomized controlled experiment by solving an optimization problem to allocate treatments in the presence of competition among neighboring nodes. Then we apply an importance sampling adjustment to correct for any leftover bias (from the approximation) in estimating average treatment effects. We provide theoretical guarantees, verify robustness in a simulation study, and validate the scalability and usefulness of our procedure in a real-world experiment on a large social network.	[Nandy, Preetam; Basu, Kinjal; Chatterjee, Shaunak; Tu, Ye] LinkedIn Corp, Mountain View, CA 94083 USA		Nandy, P (corresponding author), LinkedIn Corp, Mountain View, CA 94083 USA.	pnandy@linkedin.com; kbasu@linkedin.com; shchatte@linkedin.com; ytu@linkedin.com						[Anonymous], 2013, INT C MACHINE LEARNI; ARELLANO M, 1995, J ECONOMETRICS, V68, P29, DOI 10.1016/0304-4076(94)01642-D; Armitage P., 2002, STAT METHODS MED RES; Aronow PM, 2017, ANN APPL STAT, V11, P1912, DOI 10.1214/16-AOAS1005; Backstrom Lars, 2011, P 20 INT C WORLD WID, P615, DOI DOI 10.1145/1963405.1963492; Banjac G, 2017, IEEE DECIS CONTR P; Banjac G, 2018, 2018 UKACC 12TH INTERNATIONAL CONFERENCE ON CONTROL (CONTROL), P340; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; CRAGG JG, 1993, ECONOMET THEOR, V9, P222, DOI 10.1017/S0266466600007519; Doudchenko N., 2020, ARXIV201002108; Eckles D, 2017, J CAUSAL INFERENCE, V5, DOI 10.1515/jci-2015-0021; EFRON B, 1979, ANN STAT, V7, P1, DOI 10.1214/aos/1176344552; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Gui H, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P399, DOI 10.1145/2736277.2741081; Hernan MA, 2006, J EPIDEMIOL COMMUN H, V60, P578, DOI 10.1136/jech.2004.029496; Hirano K., 2004, APPL BAYESIAN MODELI, P73, DOI DOI 10.1002/0470090456.CH7; Hudgens MG, 2008, J AM STAT ASSOC, V103, P832, DOI 10.1198/016214508000000292; Katzir L., 2012, P 21 INT C WORLD WID, P1029; Kohavi R, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1857, DOI 10.1145/2623330.2623341; Kohavi R, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P1168; MacKinnon DP, 2002, PSYCHOL METHODS, V7, P83, DOI 10.1037/1082-989X.7.1.83; Pouget-Abadie J, 2019, ADV NEURAL INFORM PR, V32, P13309; RUBIN DB, 1974, J EDUC PSYCHOL, V66, P688, DOI 10.1037/h0037350; Saint-Jacques G., 2019, ARXIV190308755; Stellato B, 2020, MATH PROGRAM COMPUT, V12, P637, DOI 10.1007/s12532-020-00179-2; Tang D., 2010, P 16 ACM SIGKDD INT, DOI DOI 10.1145/1835804.1835810; Tu Y, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2241, DOI 10.1145/3292500.3330764; Ugander J, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P329; Xu Y, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2227, DOI 10.1145/2783258.2788602; Zigler C. M., 2018, ARXIV180708660	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000010
C	Thang, NK		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Nguyen Kim Thang			A Bandit Learning Algorithm and Applications to Auction Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				REGRET; OPTIMIZATION; PRICE	We consider online bandit learning in which at every time step, an algorithm has to make a decision and then observe only its reward. The goal is to design efficient (polynomial-time) algorithms that achieve a total reward approximately close to that of the best fixed decision in hindsight. In this paper, we introduce a new notion of (lambda, mu)-concave functions and present a bandit learning algorithm that achieves a performance guarantee which is characterized as a function of the concavity parameters lambda and mu. The algorithm is based on the mirror descent algorithm in which the update directions follow the gradient of the multilinear extensions of the reward functions. The regret bound induced by our algorithm is (O) over tilde(root T) which is nearly optimal. We apply our algorithm to auction design, specifically to welfare maximization, revenue maximization, and no-envy learning in auctions. In welfare maximization, we show that a version of fictitious play in smooth auctions guarantees a competitive regret bound which is determined by the smooth parameters. In revenue maximization, we consider the simultaneous second-price auctions with reserve prices in multi-parameter environments. We give a bandit algorithm which achieves the total revenue at least 1/2 times that of the best fixed reserve prices in hindsight. In no-envy learning, we study the bandit item selection problem where the player valuation is submodular and provide an efficient 1/2-approximation no-envy algorithm.	[Nguyen Kim Thang] Univ Paris Saclay, Univ Evry, IBISC, St Aubin, France	UDICE-French Research Universities; Universite Paris Saclay	Thang, NK (corresponding author), Univ Paris Saclay, Univ Evry, IBISC, St Aubin, France.	kimthang.nguyen@univ-evry.fr			ANR project OATA [ANR-15-CE40-0015-01]	ANR project OATA(French National Research Agency (ANR))	This work is supported by the ANR project OATA no ANR-15-CE40-0015-01.	Abernethy J, 2008, 21 ANN C LEARNING TH, P263; Agarwal Naman, 2019, PROC 32 C LEARNING T, V99, P18; Awerbuch B, 2008, J COMPUT SYST SCI, V74, P97, DOI 10.1016/j.jcss.2007.04.016; Blum A, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1156; Brown G., 1951, ACT ANAL PROD ALLOCA, P374; Bubeck S, 2017, ACM S THEORY COMPUT, P72, DOI 10.1145/3055399.3055403; Bubeck Sebastien, 2012, P ANN C LEARN THEORY, V23; Cesa-Bianchi N, 2015, IEEE T INFORM THEORY, V61, P549, DOI 10.1109/TIT.2014.2365772; Dani V, 2008, ADV NEURAL INFORM PR, V20, P345; Daskalakis C, 2016, ANN IEEE SYMP FOUND, P219, DOI 10.1109/FOCS.2016.31; Dudik M, 2017, ANN IEEE SYMP FOUND, P528, DOI 10.1109/FOCS.2017.55; Dughmi S, 2011, ACM S THEORY COMPUT, P149; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Fudenberg Drew, 1998, THEORY LEARNING GAME; Hazan E., 2016, FDN TRENDS OPTIM, V2, P157, DOI DOI 10.1561/2400000013; Hazan E., 2016, ARXIV PREPRINT ARXIV; Hazan E, 2012, J MACH LEARN RES, V13, P2903; Kalai A, 2005, J COMPUT SYST SCI, V71, P291, DOI 10.1016/j.jcss.2004.10.016; Kleinberg R, 2003, ANN IEEE SYMP FOUND, P594, DOI 10.1109/SFCS.2003.1238232; Kleinberg R, 2005, P 17 INT C NEUR INF, V17, P697; Lykouris T., 2016, P 27 ANN ACM SIAM S, P120; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Narayanan H., 2010, ADV NEURAL INFORM PR, P1777; Roughgarden Tim, 2015, ACM Transactions on Economics and Computation, V3, DOI 10.1145/2737816; Roughgarden T, 2017, J ARTIF INTELL RES, V59, P59, DOI 10.1613/jair.5272; Roughgarden T, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P601; Roughgarden T, 2015, J ACM, V62, DOI 10.1145/2806883; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Syrgkanis V, 2016, PR MACH LEARN RES, V48; Syrgkanis V, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P211	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000004
C	Rast, L; Drugowitsch, J		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Rast, Luke; Drugowitsch, Jan			Adaptation Properties Allow Identification of Optimized Neural Codes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				INFORMATION GEOMETRY; FISHER INFORMATION; MUTUAL INFORMATION; EFFICIENT	The adaptation of neural codes to the statistics of their environment is well captured by efficient coding approaches. Here we solve an inverse problem: characterizing the objective and constraint functions that efficient codes appear to be optimal for, on the basis of how they adapt to different stimulus distributions. We formulate a general efficient coding problem, with flexible objective and constraint functions and minimal parametric assumptions. Solving special cases of this model, we provide solutions to broad classes of Fisher information-based efficient coding problems, generalizing a wide range of previous results. We show that different objective function types impose qualitatively different adaptation behaviors, while constraints enforce characteristic deviations from classic efficient coding signatures. Despite interaction between these effects, clear signatures emerge for both unconstrained optimization problems and information-maximizing objective functions. Asking for a fixed-point of the neural code adaptation, we find an objective-independent characterization of constraints on the neural code. We use this result to propose an experimental paradigm that can characterize both the objective and constraint functions that an observed code appears to be optimized for.	[Rast, Luke] Harvard Univ, Cambridge, MA 02138 USA; [Drugowitsch, Jan] Harvard Med Sch, Dept Neurobiol, Boston, MA 02115 USA	Harvard University; Harvard University; Harvard Medical School	Rast, L (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	lukerast@g.harvard.edu; jan_drugowitsch@hms.harvard.edu			National Institutes of Health [R01MH115554]; James S. McDonnell Foundation [220020462]; Edward R. and Anne G. Lefler Center at Harvard Medical School	National Institutes of Health(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); James S. McDonnell Foundation; Edward R. and Anne G. Lefler Center at Harvard Medical School	This work was supported by funding from the National Institutes of Health (R01MH115554, J.D.), a Scholar Award in Understanding Human Cognition by the James S. McDonnell Foundation (grant#220020462, J.D.), and a Lefler Small Grant from the Edward R. and Anne G. Lefler Center at Harvard Medical School (L.R.).	Amari S, 2001, IEEE T INFORM THEORY, V47, P1701, DOI 10.1109/18.930911; Amari SI, 2016, APPL MATH SCI, V194, P1, DOI 10.1007/978-4-431-55978-8; [Anonymous], 2016, ADV NEURAL INFORM PR, DOI DOI 10.1109/ISVLSI.2016.40; [Anonymous], 2012, ADV NEURAL INFORM PR; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Berry Michael J., 1997, NATURE; Brenner N, 2000, NEURON, V26, P695, DOI 10.1016/S0896-6273(00)81205-2; Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115; Dan Y, 1996, J NEUROSCI, V16, P3351; Daptardar Saurabh, 2019, INVERSE RATIONAL CON; Fairhall Adrienne L, 2000, ADV NEURAL INFORM PR; Fairhall AL, 2001, NATURE, V412, P787, DOI 10.1038/35090500; Ganguli Deep, 2010, Adv Neural Inf Process Syst, V2010, P658; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; LAUGHLIN S, 1981, Z NATURFORSCH C, V36, P910; Liberzon D., 2011, CALCULUS VARIATIONS, DOI DOI 10.2307/J.CTVCM4G0S; Marre O., 2019, 598086 BIORXIV; Morais Michael J, 2018, ADV NEURAL INFORM PR; PARADISO MA, 1988, BIOL CYBERN, V58, P35, DOI 10.1007/BF00363954; Park I. M., 2017, BIORXIV, DOI [10.1101/178418, DOI 10.1101/178418]; Reid MD, 2011, J MACH LEARN RES, V12, P731; SEUNG HS, 1993, P NATL ACAD SCI USA, V90, P10749, DOI 10.1073/pnas.90.22.10749; Todorov Emanuel, 2007, BAYESIAN BRAIN PROBA; Wang Z., 2012, ADV NEURAL INFORM PR, V3, P2168; Wang Zhou, 2016, NEURAL COMPUTATION, V28; Wei XX, 2016, NEURAL COMPUT, V28, P305, DOI 10.1162/NECO_a_00804; Wei XX, 2015, NAT NEUROSCI, V18, P1509, DOI 10.1038/nn.4105; Wei Xue-Xin, 2017, P NATL ACAD SCI, V114; Wu Zhengwei, 2018, INVERSE POMDP INFERR	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000031
C	Romano, Y; Bates, S; Candes, EJ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Romano, Yaniv; Bates, Stephen; Candes, Emmanuel J.			Achieving Equalized Odds by Resampling Sensitive Attributes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					We present a flexible framework for learning predictive models that approximately satisfy the equalized odds notion of fairness. This is achieved by introducing a general discrepancy functional that rigorously quantifies violations of this criterion. This differentiable functional is used as a penalty driving the model parameters towards equalized odds. To rigorously evaluate fitted models, we develop a formal hypothesis test to detect whether a prediction rule violates this property, the first such test in the literature. Both the model fitting and hypothesis testing leverage a resampled version of the sensitive attribute obeying equalized odds, by construction. We demonstrate the applicability and validity of the proposed framework both in regression and multi-class classification problems, reporting improved performance over state-of-the-art methods. Lastly, we show how to incorporate techniques for equitable uncertainty quantification-unbiased for each group under study-to communicate the results of the data analysis in exact terms.	[Romano, Yaniv; Bates, Stephen; Candes, Emmanuel J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Candes, Emmanuel J.] Stanford Univ, Dept Math, Stanford, CA 94305 USA	Stanford University; Stanford University	Romano, Y (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.	yromano@stanford.edu; stephenbates@stanford.edu; candes@stanford.edu			Office of Naval Research grant [N00014-20-12157]; National Science Foundation [DMS 1712800, OAC 1934578]; NSF [DMS 1712800]; Ric Weiland Graduate Fellowship; Army Research Office (ARO) [W911NF-17-1-0304]; Zuckerman Institute; ISEF Foundation; Viterbi Fellowship; Technion; Koret Foundation	Office of Naval Research grant(Office of Naval Research); National Science Foundation(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Ric Weiland Graduate Fellowship; Army Research Office (ARO); Zuckerman Institute; ISEF Foundation; Viterbi Fellowship; Technion; Koret Foundation	E. C. was partially supported by the Office of Naval Research grant N00014-20-12157, and by the National Science Foundation grants DMS 1712800 and OAC 1934578. He thanks Rina Barber and Chiara Sabatti for useful discussions related to this project. S. B. was supported by NSF under grant DMS 1712800 and a Ric Weiland Graduate Fellowship. Y. R. was supported by the Army Research Office (ARO) under grant W911NF-17-1-0304. Y. R. thanks the Zuckerman Institute, ISEF Foundation, the Viterbi Fellowship, Technion, and the Koret Foundation, for providing additional research support.	Barocas Solon, 2017, ADV NEURAL INFORM PR; Candes E, 2018, J R STAT SOC B, V80, P551, DOI 10.1111/rssb.12265; Chen I., 2018, NEURAL INFORM PROCES, P3539; Chouldechova A, 2020, COMMUN ACM, V63, P82, DOI 10.1145/3376898; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Cranmer K, 2017, ADV NEURAL INFORM PR, V30, P981; Donini M., 2018, NEURIPS, P2791; FRIEDMAN JH, 1979, ANN STAT, V7, P697, DOI 10.1214/aos/1176344722; FRIEDMAN JH, 1983, ANN STAT, V11, P377, DOI 10.1214/aos/1176346148; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Henderson M Todd, 2010, ARIZ L REV, V52, P15; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Kleinberg Jon, 2017, LEIBNIZ INT P INFORM; Kusner M.J, 2017, ADV NEURAL INFORM PR, V30, P4066; Loftus Joshua R., 2018, CAUSAL REASONING ALG; Lopez-Paz D, 2017, PROC CVPR IEEE, P58, DOI 10.1109/CVPR.2017.14; Mary Jeremie, 2019, INT C MACH LEARN, P4382; Menon Aditya Krishna, 2018, P C FAIRNESS ACCOUNT, P107; Nabi R., 2018, P 32 AAAI C ART INT; Romano Yaniv, 2020, HARVARD DATA SCI REV; Szekely GJ, 2013, J STAT PLAN INFER, V143, P1249, DOI 10.1016/j.jspi.2013.03.018; Tansey W., 2018, ARXIV181100645; Zhang BH, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P335, DOI 10.1145/3278721.3278779; Zhang J., 2018, P 32 AAAI C ART INT	30	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000017
C	Tasse, GN; James, S; Rosman, B		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Tasse, Geraud Nangue; James, Steven; Rosman, Benjamin			A Boolean Task Algebra For Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					The ability to compose learned skills to solve new tasks is an important property of lifelong-learning agents. In this work, we formalise the logical composition of tasks as a Boolean algebra. This allows us to formulate new tasks in terms of the negation, disjunction and conjunction of a set of base tasks. We then show that by learning goal-oriented value functions and restricting the transition dynamics of the tasks, an agent can solve these new tasks with no further learning. We prove that by composing these value functions in specific ways, we immediately recover the optimal policies for all tasks expressible under the Boolean algebra. We verify our approach in two domains-including a high-dimensional video game environment requiring function approximation-where an agent first learns a set of base skills, and then composes them to solve a super-exponential number of new tasks.	[Tasse, Geraud Nangue; James, Steven; Rosman, Benjamin] Univ Witwatersrand, Sch Comp Sci & Appl Math, Johannesburg, South Africa	University of Witwatersrand	Tasse, GN (corresponding author), Univ Witwatersrand, Sch Comp Sci & Appl Math, Johannesburg, South Africa.	geraudnt@gmail.com; steven.james@wits.ac.za; benjamin.rosman1@wits.ac.za	Rosman, Benjamin/ABF-3933-2020	Rosman, Benjamin/0000-0002-0284-4114	National Research Foundation of South Africa [17808]	National Research Foundation of South Africa(National Research Foundation - South Africa)	The authors wish to thank the anonymous reviewers for their helpful comments, and Pieter Abbeel, Marc Deisenroth and Shakir Mohamed for their assistance in reviewing a final draft of this paper. This work is based on the research supported in part by the National Research Foundation of South Africa (Grant Number: 17808).	Barto AG, 2003, DISCRETE EVENT DYN S, V13, P41, DOI 10.1023/A:1025696116075; BERTSEKAS DP, 1991, MATH OPER RES, V16, P580, DOI 10.1287/moor.16.3.580; Fox R., 2016, 32 C UNC ART INT; Haarnoja T, 2018, IEEE INT CONF ROBOT, P6244; Hunt J., 2019, P INT C MACH LEARN I, V97, P2911; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; James HW, 2006, J APPL PROBAB, V43, P603, DOI 10.1239/jap/1158784933; KAELBLING LP, 1993, IJCAI-93, VOLS 1 AND 2, P1094; Levine S, 2016, J MACH LEARN RES, V17; Mirowski Piotr, 2017, INT C LEARN REPR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Peng X., 2019, ARXIV190509808; Pritzel Alexander, 2016, INT C LEARNING REPRE; Saxe AM, 2017, P 34 INT C MACH LEAR, V70, P3017; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Todorov E., 2009, ADV NEURAL INFORM PR, V22, P1856; Todorov E., 2007, ADV NEURAL INFORM PR, V19, DOI 10.7551/mitpress/7503.003.0176; van Niekerk B, 2019, PR MACH LEARN RES, V97; Veeriah V., 2018, ARXIV180609605; Watkins CJCH., 1989, THESIS	24	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000009
C	Wang, JA; Sezener, E; Budden, D; Hutter, M; Veness, J		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Wang, Jianan; Sezener, Eren; Budden, David; Hutter, Marcus; Veness, Joel			A Combinatorial Perspective on Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Human intelligence is characterized not only by the capacity to learn complex skills, but the ability to rapidly adapt and acquire new skills within an ever-changing environment. In this work we study how the learning of modular solutions can allow for effective generalization to both unseen and potentially differently distributed data. Our main postulate is that the combination of task segmentation, modular learning and memory-based ensembling can give rise to generalization on an exponentially growing number of unseen tasks. We provide a concrete instantiation of this idea using a combination of: (1) the Forget-Me-Not Process, for task segmentation and memory based ensembling; and (2) Gated Linear Networks, which in contrast to contemporary deep learning techniques use a modular and local learning mechanism. We demonstrate that this system exhibits a number of desirable continual learning properties: robustness to catastrophic forgetting, no negative transfer and increasing levels of positive transfer as more tasks are seen. We show competitive performance against both offline and online methods on standard continual learning benchmarks.	[Wang, Jianan; Sezener, Eren; Budden, David; Hutter, Marcus; Veness, Joel] DeepMind, London, England		Wang, JA (corresponding author), DeepMind, London, England.	aixi@google.com						Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9; Aljundi Rahaf, 2019, ADV NEURAL INFORM PR, P11849; Bengio Yoshua, 2019, ARXIV190110912; Bradbury J., 2018, JAX COMPOSABLE TRANS; Budden D., 2020, RLAX REINFORCEMENT L; Budden David, 2020, JAX; Budden David, 2020, GAUSSIAN GATED LINEA; CARPENTER GA, 1988, COMPUTER, V21, P77, DOI 10.1109/2.33; Chaudhry A., 2019, CORR; Chen Yutian, 2019, ARXIV190905557; Chen Z, 2018, SYNTHESIS LECT ARTIF, V12, P1, DOI [10.2200/S00737ED1V01Y201610AIM033, DOI 10.2200/S00737ED1V01Y201610AIM033]; Donahue J, 2013, P 31 INT C MACH LEAR; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Harries M, 1999, ARTIF INTELL; HENNIGAN T., 2020, HAIKU SONNET JAX; Hessel M., 2020, JAX; Hsu Yen-Chang, 2018, NEURIPS CONT LEARN W; Huszar F, 2018, P NATL ACAD SCI USA, V115, pE2496, DOI 10.1073/pnas.1717042115; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kolter JZ, 2007, J MACH LEARN RES, V8, P2755; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Lopez-Paz D, 2017, ADV NEUR IN, V30; Mattern C, 2013, IEEE DATA COMPR CONF, P301, DOI 10.1109/DCC.2013.38; Mattern C, 2012, IEEE DATA COMPR CONF, P337, DOI 10.1109/DCC.2012.40; Milan K., 2016, ADV NEURAL INFORM PR, V29, P3702; Parascandolo Giambattista, 2017, ARXIV171200961; Parisi GI, 2019, NEURAL NETWORKS, V113, P54, DOI 10.1016/j.neunet.2019.01.012; Quinlan J., 2014, C4 5 PROGRAMS MACHIN, DOI DOI 10.1007/BF00993309; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Schwarz J, 2018, PR MACH LEARN RES, V80; Sezener Eren, 2020, ARXIV200211611; Shin H, 2017, ADV NEUR IN, V30; Torrey L., 2009, HDB RES MACHINE LEAR, V11, P242; van de Ven Gido M, 2018, ARXIV180910635; Veness J, 2013, IEEE DATA COMPR CONF, P321, DOI 10.1109/DCC.2013.40; Veness Joel, 2017, ABS171201897 CORR; Veness Joel, 2019, ARXIV191001526; Zenke F, 2017, PR MACH LEARN RES, V70; Zinkevich, 2003, P 20 INT C MACH LEAR, P928	42	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000021
C	Wang, Z; Cheng, XY; Sapiro, G; Qiu, Q		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Wang, Ze; Cheng, Xiuyuan; Sapiro, Guillermo; Qiu, Qiang			A Dictionary Approach to Domain-Invariant Learning in Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					In this paper, we consider domain-invariant deep learning by explicitly modeling domain shifts with only a small amount of domain-specific parameters in a Convolutional Neural Network (CNN). By exploiting the observation that a convolutional filter can be well approximated as a linear combination of a small set of dictionary atoms, we show for the first time, both empirically and theoretically, that domain shifts can be effectively handled by decomposing a convolutional layer into a domain-specific atom layer and a domain-shared coefficient layer, while both remain convolutional. An input channel will now first convolve spatially only with each respective domain-specific dictionary atom to "absorb" domain variations, and then output channels are linearly combined using common decomposition coefficients trained to promote shared semantics across domains. We use toy examples, rigorous analysis, and real-world examples with diverse datasets and architectures, to show the proposed plug-in framework's effectiveness in cross and joint domain performance and domain adaptation. With the proposed architecture, we need only a small set of dictionary atoms to model each additional domain, which brings a negligible amount of additional parameters, typically a few hundred.	[Wang, Ze; Qiu, Qiang] Purdue Univ, W Lafayette, IN 47907 USA; [Cheng, Xiuyuan; Sapiro, Guillermo] Duke Univ, Durham, NC 27706 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Duke University	Wang, Z (corresponding author), Purdue Univ, W Lafayette, IN 47907 USA.	zewang@purdue.edu; xiuyuan.cheng@duke.edu; guillermo.sapiro@duke.edu; qqiu@purdue.edu			NSF; ONR; ARO; NGA; NIH	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); ARO; NGA; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	Work partially supported by NSF, ONR, ARO, NGA, NIH, and gifts from Microsoft, Google, and Amazon.	[Anonymous], 2014, NIPS; [Anonymous], 2014, ABS14123474 CORR; [Anonymous], 2018, ICML; [Anonymous], 2018, CVPR; Bousmalis K., 2016, ADV NEURAL INFORM PR; Buhlmann P., 2018, ARXIV181208233; Chang W.-L., 2019, CVPR; Cordts M., 2016, CVPR; Ganin Y., 2016, JMLR, V17, P2096; Gretton A., 2007, NIPS; Gretton A., 2012, ADV NEURAL INFORM PR, P1214; Hoffman J, 2016, FCNS WILD PIXELLEVEL; Hu L., 2018, CVPR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kang Guoliang, 2019, CVPR; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar A., 2018, P 32 INT C NEUR INF, P9345; Li S. Z., 2013, CVPR WORKSH; Long M., 2016, NIPS; Long MS, 2019, IEEE T PATTERN ANAL, V41, P3071, DOI 10.1109/TPAMI.2018.2868685; Long Mingsheng, 2018, NIPS; Long Mingsheng, 2017, ICML; Lowell U, 2015, ICCV; Qiu Q., 2020, ARXIV200902386; Qiu Qiang, 2018, ICML; Richter S. R., 2016, ECCV; ROZANTSEV A, 2018, IEEE T PATTERN ANAL; Rozantsev A., 2018, CVPR; Saenko Kate, 2010, EECV; Saito Kuniaki, 2017, ICML; Simonyan K., 2014, ICLR; Sulam J, 2018, IEEE T SIGNAL PROCES, V66, P4090, DOI 10.1109/TSP.2018.2846226; Tsai Y.H., 2018, CVPR; Tzeng E., 2017, CVPR; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wu Yifan, 2019, ICML; ZeWang Xiuyuan Cheng, 2020, ICLR; Zhang W., 2018, CVPR, P2; Zhao H., 2019, ICML	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													11	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000049
C	Yang, L; Hajiesmaili, MH; Talebi, MS; Lui, JCS; Wong, WS		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Yang, Lin; Hajiesmaili, Mohammad H.; Talebi, M. Sadegh; Lui, John C. S.; Wong, Wing S.			Adversarial Bandits with Corruptions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					This paper studies adversarial bandits with corruptions. In the basic adversarial bandit setting, the reward of arms is predetermined by an adversary who is oblivious to the learner's policy. In this paper, we consider an extended setting in which an attacker sits in-between the environment and the learner, and is endowed with a limited budget to corrupt the reward of the selected arm. We have two main results. First, we derive a lower bound on the regret of any bandit algorithm that is aware of the budget of the attacker. Also, for budget-agnostic algorithms, we characterize an impossibility result demonstrating that even when the attacker has a sublinear budget, i.e., a budget growing sublinearly with time horizon T, they fail to achieve a sublinear regret. Second, we propose ExpRb, a bandit algorithm that incorporates a biased estimator and a robustness parameter to deal with corruption. We characterize the regret of ExpRb and show that for the case of a known corruption budget, the regret of ExpRb is tight.	[Yang, Lin; Hajiesmaili, Mohammad H.] Univ Massachusetts, Amherst, MA 01003 USA; [Talebi, M. Sadegh] Univ Copenhagen, Copenhagen, Denmark; [Lui, John C. S.; Wong, Wing S.] Chinese Univ Hong Kong, Hong Kong, Peoples R China	University of Massachusetts System; University of Massachusetts Amherst; University of Copenhagen; Chinese University of Hong Kong	Yang, L (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.	linyang@cs.umass.edu; hajiesmaili@cs.umass.edu; m.shahi@di.ku.dk; cslui@cse.cuhk.edu.hk; wswong@ie.cuhk.edu.hk			Schneider Electric; Hong Kong Innovation and Technology Fund under the HKUST-MIT Research Alliance Consortium [ITS/066/17FP]; NSF [CNS-1908298]; Department of Computer Science, University of Copenhagen; Lenovo Group (China) Limited;  [GRF 14201819]	Schneider Electric; Hong Kong Innovation and Technology Fund under the HKUST-MIT Research Alliance Consortium; NSF(National Science Foundation (NSF)); Department of Computer Science, University of Copenhagen; Lenovo Group (China) Limited; 	Lin Yang and Wing Shing Wong acknowledge the support from Schneider Electric, Lenovo Group (China) Limited and the Hong Kong Innovation and Technology Fund (ITS/066/17FP) under the HKUST-MIT Research Alliance Consortium. Mohammad Hajiesmaili's research is supported by NSF CNS-1908298. The work of John C.S. Lui is supported in part by the GRF 14201819. Sadegh Talebi's research is supported by Department of Computer Science, University of Copenhagen.	Audibert J.-Y., 2009, P 22 ANN C LEARN THE, P217; Audibert JY, 2010, J MACH LEARN RES, V11, P2785; Auer P, 2003, SIAM J COMPUT, V32, P48, DOI 10.1137/S0097539701398375; Awerbuch B, 2004, P 36 ANN ACM S THEOR, P45; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Cormack Gordon V., 2006, Foundations and Trends in Information Retrieval, V1, P1, DOI 10.1561/1500000006; Even-Dar E, 2009, MATH OPER RES, V34, P726, DOI 10.1287/moor.1090.0396; Feng Z., 2020, INT C MACH LEARN, P3092; Gupta A., 2019, P MACH LEARN RES, P1562; Gyorgy A, 2006, LECT NOTES ARTIF INT, V4005, P468, DOI 10.1007/11776420_35; Heydari A, 2015, EXPERT SYST APPL, V42, P3634, DOI 10.1016/j.eswa.2014.12.029; Jun KS, 2018, ADV NEUR IN, V31; Khan W. Z., IEEE COMMUNICATIONS, V17; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Li Yingkai, 2019, ARXIV190902109; Luca M, 2016, MANAGE SCI, V62, P3412, DOI 10.1287/mnsc.2015.2304; Lykouris T, 2018, ACM S THEORY COMPUT, P114, DOI 10.1145/3188745.3188918; Neu G., 2015, ADV NEURAL INFORM PR, P3168; NEU G, 2012, ARTIF INTELL, P805; Ozisik P., 2020, ADV NEURAL INFORM PR; Qian B, 2019, PROCEEDINGS OF 2019 IEEE 8TH JOINT INTERNATIONAL INFORMATION TECHNOLOGY AND ARTIFICIAL INTELLIGENCE CONFERENCE (ITAIC 2019), P440, DOI 10.1109/ITAIC.2019.8785608; ROBBINS H, 1952, B AM MATH SOC, V58, P527, DOI 10.1090/S0002-9904-1952-09620-8; Slivkins A, 2019, FOUND TRENDS MACH LE, V12, P1, DOI 10.1561/2200000068; Talebi MS, 2018, IEEE T AUTOMAT CONTR, V63, P915, DOI 10.1109/TAC.2017.2747409; Wilbur KC, 2009, MARKET SCI, V28, P293, DOI 10.1287/mksc.1080.0397; Wu X, 2017, IEEE INT CONF BIG DA, P494; Zhang LF, 2019, 2019 IEEE 4TH INTERNATIONAL CONFERENCE ON ADVANCED ROBOTICS AND MECHATRONICS (ICARM 2019), P118, DOI 10.1109/ICARM.2019.8833708; Zhang X., 2019, ARXIV190301666; Zhou P, 2019, IEEE ACM T NETWORK, V27, P1815, DOI 10.1109/TNET.2019.2930464; Zimmert J., 2020, ARXIV180707623V4; Zimmert J, 2019, PR MACH LEARN RES, V89, P467	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000054
C	Zhang, MJ; He, YX		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Zhang, Minjia; He, Yuxiong			Accelerating Training of Transformer-Based Language Models with Progressive Layer Dropping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Recently, Transformer-based language models have demonstrated remarkable performance across many NLP domains. However, the unsupervised pre-training step of these models suffers from unbearable overall computational expenses. Current methods for accelerating the pre-training either rely on massive parallelism with advanced hardware or are not applicable to language modeling. In this work, we propose a method based on progressive layer dropping that speeds the training of Transformer-based language models, not at the cost of excessive hardware resources but from model architecture change and training technique boosted efficiency. Extensive experiments on BERT show that the proposed method achieves a 24% time reduction on average per sample and allows the pre-training to be 2.5x faster than the baseline to get a similar accuracy on downstream tasks. While being faster, our pre-trained models are equipped with strong knowledge transferability, achieving comparable and sometimes higher GLUE score than the baseline when pre-trained with the same number of samples.	[Zhang, Minjia; He, Yuxiong] Microsoft Corp, Redmond, WA 98052 USA	Microsoft	Zhang, MJ (corresponding author), Microsoft Corp, Redmond, WA 98052 USA.	minjiaz@microsoft.com; yuxhe@microsoft.com						Ba L.J., 2016, ABS160706450 CORR; Baevski Alexei, 2019, 7 INT C LEARN REPR; Baydin AG, 2018, J MACH LEARN RES, V18; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Brown Tom B., 2020, ABS200514165 CORR; Child Rewon, 2019, ABS190410509 CORR; Clark Kevin, 2020, 8 INT C LEARN REPR I; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Ding M, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2694; Fan Angela, 2020, 8 INT CLEARN REPR IC; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Gong LY, 2019, PR MACH LEARN RES, V97; Greff Klaus, 2017, 5 INT C LEARN REPR I; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Huang YP, 2019, ADV NEUR IN, V32; Jiang X., 2019, ABS190910351 CORR; Joshi Mandar, 2019, ABS190710529 CORR; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Keskar N. S., 2017, 5 INT C LEARN REPR I; Lan Z., 2020, ARXIV; Lee J, 2019, CORR ABS190108746, DOI DOI 10.1044/2019_AJSLP-CAC48-18-0220; Liu Liyuan, 2020, ABS200408249 CORR; Liu Y, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5070; Markidis S, 2018, IEEE SYM PARA DISTR, P522, DOI 10.1109/IPDPSW.2018.00091; Micikevicius Paulius, 2018, 6 INT C LEARN REPR I; Morerio P, 2017, IEEE I CONF COMP VIS, P3564, DOI 10.1109/ICCV.2017.383; Nguyen Toan Q., 2019, ABS191005895 CORR; Radford Alec, 2019, LANGUAGE MODELS ARE; Raffel C, 2020, J MACH LEARN RES, V21; Sanh Victor, 2019, ABS191001108 CORR; Shoeybi Mohammad, 2019, ABS190908053 CORR; Sra S, 2016, JMLR WORKSH CONF PRO, V51, P957; Vaswani A, 2017, ADV NEUR IN, V30; Wang Alex, 2019, 7 INT C LEARN REPR; Wang Q, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P1810; Xiong Ruibin, 2020, ARXIV200204745; Yang W, 2019, NAACL HLT 2019: THE 2019 CONFERENCE OF THE NORTH AMERICAN CHAPTER OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: HUMAN LANGUAGE TECHNOLOGIES: PROCEEDINGS OF THE DEMONSTRATIONS SESSION, P72; Yang ZW, 2021, ANIM BIOTECHNOL, V32, P67, DOI 10.1080/10495398.2019.1653901; You Yang, 2019, ABS190400962 CORR; Zaeemzadeh Alireza, 2018, ABS180507477 CORR	44	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													13	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000015
C	Zhu, YZ		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Zhu, Yunzhang			A convex optimization formulation for multivariate regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK				PRECISION MATRIX ESTIMATION; VARIABLE SELECTION; SPARSE; LIKELIHOOD; LASSO	Multivariate regression (or multi-task learning) concerns the task of predicting the value of multiple responses from a set of covariates. In this article, we propose a convex optimization formulation for high-dimensional multivariate linear regression under a general error covariance structure. The main difficulty with simultaneous estimation of the regression coefficients and the error covariance matrix lies in the fact that the negative log-likelihood function is not convex. To overcome this difficulty, a new parameterization is proposed, under which the negative log-likelihood function is proved to be convex. For faster computation, two other alternative loss functions are also considered, and proved to be convex under the proposed parameterization. This new parameterization is also useful for covariate-adjusted Gaussian graphical modeling in which the inverse of the error covariance matrix is of interest. A joint non-asymptotic analysis of the regression coefficients and the error covariance matrix is carried out under the new parameterization. In particular, we show that the proposed method recovers the oracle estimator under sharp scaling conditions, and rates of convergence in terms of vector l(infinity) norm are also established. Empirically, the proposed methods outperform existing high-dimensional multivariate linear regression methods that are based on either minimizing certain non-convex criteria or certain two-step procedures.	[Zhu, Yunzhang] Ohio State Univ, Dept Stat, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Zhu, YZ (corresponding author), Ohio State Univ, Dept Stat, Columbus, OH 43210 USA.	zhu.219@osu.edu			US National Science Foundation (NSF) [NSF-DMS-1721445, NSF-DMS-1712580, NSF-DMS-2015490]	US National Science Foundation (NSF)(National Science Foundation (NSF))	The author is supported in part by the US National Science Foundation (NSF) under grants NSF-DMS-1721445, NSF-DMS-1712580, and NSF-DMS-2015490. The author thanks the reviewers for valuable comments and suggestions that improved the article.	ANDERSON TW, 1951, ANN MATH STAT, V22, P327, DOI 10.1214/aoms/1177729580; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; BREGMAN LM, 1966, DOKL AKAD NAUK SSSR+, V171, P1019; Fan JQ, 2009, ANN APPL STAT, V3, P521, DOI 10.1214/08-AOAS215; Fan JQ, 2001, J AM STAT ASSOC, V96, P1348, DOI 10.1198/016214501753382273; Izenman A. J., 1975, Journal of Multivariate Analysis, V5, P248, DOI 10.1016/0047-259X(75)90042-1; Khare K, 2015, J R STAT SOC B, V77, P803, DOI 10.1111/rssb.12088; Lee JD, 2015, J COMPUT GRAPH STAT, V24, P230, DOI 10.1080/10618600.2014.900500; Lee JD, 2014, SIAM J OPTIMIZ, V24, P1420, DOI 10.1137/130921428; Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013; Li B, 2012, J AM STAT ASSOC, V107, P152, DOI 10.1080/01621459.2011.644498; Liu H., 2014, ADV NEURAL INFORM PR, P127; Luo ZQ, 2007, SIAM J OPTIMIZ, V18, P1, DOI 10.1137/050642691; Molstad AJ, 2016, BIOMETRIKA, V103, P595, DOI 10.1093/biomet/asw034; Nesterov Y, 2007, GRADIENT METHODS MIN; Obozinski G, 2011, ANN STAT, V39, P1, DOI 10.1214/09-AOS776; Peng J, 2010, ANN APPL STAT, V4, P53, DOI 10.1214/09-AOAS271; Peng J, 2009, J AM STAT ASSOC, V104, P735, DOI 10.1198/jasa.2009.0126; Reinsel G. C., 1998, MULTIVARIATE REDUCED; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Rothman AJ, 2008, ELECTRON J STAT, V2, P494, DOI 10.1214/08-EJS176; She YY, 2015, IEEE T SIGNAL PROCES, V63, P389, DOI 10.1109/TSP.2014.2373315; Shen XT, 2013, ANN I STAT MATH, V65, P807, DOI 10.1007/s10463-012-0396-3; Shen XT, 2012, J AM STAT ASSOC, V107, P223, DOI 10.1080/01621459.2011.645783; Sofer T., 2012, VARIABLE SELEC UNPUB; Turlach BA, 2005, TECHNOMETRICS, V47, P349, DOI 10.1198/004017005000000139; Wang JH, 2015, STAT SINICA, V25, P831, DOI 10.5705/ss.2013.192; Yin JX, 2013, J MULTIVARIATE ANAL, V116, P365, DOI 10.1016/j.jmva.2013.01.005; Yuan M, 2007, J ROY STAT SOC B, V69, P329, DOI 10.1111/j.1467-9868.2007.00591.x; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang T, 2014, BIOMETRIKA, V101, P103, DOI 10.1093/biomet/ast059; Zhu Y., 2018, J ROYAL STAT SOC B	33	0	0	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													10	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000025
C	Abernethy, J; Cummings, R; Kumar, B; Morgenstern, J; Taggart, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Abernethy, Jacob; Cummings, Rachel; Kumar, Bhuvesh; Morgenstern, Jamie; Taggart, Samuel			Learning Auctions with Robust Incentive Guarantees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DYNAMIC MECHANISM; DESIGN	We study the problem of learning Bayesian-optimal revenue-maximizing auctions. The classical approach to maximizing revenue requires a known prior distribution on the demand of the bidders, although recent work has shown how to replace the knowledge of a prior distribution with a polynomial sample. However, in an online setting, when buyers can participate in multiple rounds, standard learning techniques are susceptible to strategic overfitting: bidders can improve their long-term wellbeing by manipulating the trajectory of the learning algorithm through bidding. For example, they may be able to strategically adjust their behavior in earlier rounds to achieve lower, more favorable future prices. Such non-truthful behavior can hinder learning and harm revenue. In this paper, we combine tools from differential privacy, mechanism design, and sample complexity to give a repeated auction that (1) learns bidder demand from past data, (2) is approximately revenue-optimal, and (3) strategically robust, as it incentivizes bidders to behave truthfully.	[Abernethy, Jacob; Cummings, Rachel; Kumar, Bhuvesh; Morgenstern, Jamie] Georgia Tech, Atlanta, GA 30332 USA; [Taggart, Samuel] Oberlin Coll, Oberlin, OH 44074 USA	University System of Georgia; Georgia Institute of Technology; Oberlin College	Abernethy, J (corresponding author), Georgia Tech, Atlanta, GA 30332 USA.	prof@gatech.edu; rachelc@gatech.edu; bhuvesh@gatech.edu; jamiemmtcs@gatech.edu; sam.taggart@oberlin.edu						Amin K., 2013, ADV NEURAL INFORM PR, V1, P1169; Archer Aaron, 2004, INTERNET MATH, V1, P129; Ashlagi I, 2016, EC'16: PROCEEDINGS OF THE 2016 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P213, DOI 10.1145/2940716.2940775; Aydin K, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P387, DOI 10.1145/2835776.2835829; Balcan Maria-Florina F, 2016, P 29 C NEURAL INFORM, P2083; Balcan MF, 2005, ANN IEEE SYMP FOUND, P605, DOI 10.1109/SFCS.2005.50; Blum A, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1156; Braverman M, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P523, DOI 10.1145/3219166.3219233; Bubeck S, 2017, EC'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P497, DOI 10.1145/3033274.3085145; Cesa-Bianchi N, 2015, IEEE T INFORM THEORY, V61, P549, DOI 10.1109/TIT.2014.2365772; Chan THH, 2011, ACM T INFORM SYST SE, V14, DOI 10.1145/2043621.2043626; Cole R, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P243, DOI 10.1145/2591796.2591867; Cummings Rachel, 2015, Web and Internet Economics. 11th International Conference, WINE 2015. Proceedings: LNCS 9470, P286, DOI 10.1007/978-3-662-48995-6_21; Cummings Rachel, 2015, P 28 C LEARNING THEO, P448; Cummings Rachel, 2017, XRDS, V24, P34; Devanur NR, 2016, ACM S THEORY COMPUT, P426, DOI 10.1145/2897518.2897553; Dhangwatnotai P, 2015, GAME ECON BEHAV, V91, P318, DOI 10.1016/j.geb.2014.03.011; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2010, ACM S THEORY COMPUT, P715; Elkind E, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P736; Epasto A, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P1369, DOI 10.1145/3178876.3186042; Gonczarowski YA, 2017, ACM S THEORY COMPUT, P856, DOI 10.1145/3055399.3055427; Hartline Jason, 2013, BOOK DRAFT OCTOBER, V122; Hartline Jason, 2016, ARXIV160801875; Hsu J, 2016, SIAM J COMPUT, V45, P1953, DOI 10.1137/15100271X; Hu P, 2016, ADV BIO SCI RES, V3, P138; Kakade SM, 2013, OPER RES, V61, P837, DOI 10.1287/opre.2013.1194; Kannan Sampath, 2015, P 16 ACM C EC COMP A, P261; Kearns M, 2014, P 5 C INN THEOR COMP, P403; Leme RP, 2016, PROCEEDINGS OF THE 25TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'16), P1093, DOI 10.1145/2872427.2883071; Liu J., 2018, ADV NEURAL INFORM PR, P2038; Liu SQ, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2008; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Mohri M, 2014, PR MACH LEARN RES, V32; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR; MYERSON RB, 1981, MATH OPER RES, V6, P58, DOI 10.1287/moor.6.1.58; Nissim Kobbi, 2012, 13 ACM C EC COMP EC, P774; Papadimitriou C., 2016, P 27 ANN ACM SIAM S, P1458; Pavan A, 2014, ECONOMETRICA, V82, P601, DOI 10.3982/ECTA10269; Xiao David, 2013, 4 C INN THEOR COMP S, P67	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903025
C	Acharya, J; Bhadane, S; Indyk, P; Sun, ZT		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Acharya, Jayadev; Bhadane, Sourbh; Indyk, Piotr; Sun, Ziteng			Estimating Entropy of Distributions in Constant Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FREQUENT; UNSEEN	We consider the task of estimating the entropy of k-ary distributions from samples in the streaming model, where space is limited. Our main contribution is an algorithm that requires O(k log(1/epsilon)(2)/epsilon(3)) samples and a constant O(1) memory words of space and outputs a +/-epsilon estimate of H(p). Without space limitations, the sample complexity has been established as S(k, epsilon) = Theta(k/epsilon log k + log(2) k/epsilon(2)), which is sub-linear in the domain size k, and the current algorithms that achieve optimal sample complexity also require nearly-linear space in k. Our algorithm partitions [0, 1] into intervals and estimates the entropy contribution of probability values in each interval. The intervals are designed to trade off the bias and variance of these estimates.	[Acharya, Jayadev; Bhadane, Sourbh; Sun, Ziteng] Cornell Univ, Ithaca, NY 14853 USA; [Indyk, Piotr] MIT, Cambridge, MA 02139 USA	Cornell University; Massachusetts Institute of Technology (MIT)	Acharya, J (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	acharya@cornell.edu; snb62@cornell.edu; indyk@mit.edu; zs335@cornell.edu			MIT-Shell Energy Research Fellowship [NSF-CCF-1657471]	MIT-Shell Energy Research Fellowship	This work is supported by NSF-CCF-1657471. This research started with the support of MIT-Shell Energy Research Fellowship to JA and PI, while JA was at MIT.	Acharya J, 2017, PR MACH LEARN RES, V70; Acharya J, 2018, IEEE INT SYMP INFO, P1400; Acharya J, 2017, IEEE T INFORM THEORY, V63, P38, DOI 10.1109/TIT.2016.2620435; Alon N., 1996, P 28 ANN ACM S THEOR; Antos A, 2001, RANDOM STRUCT ALGOR, V19, P163, DOI 10.1002/rsa.10019; Bar-Yossef Z., 2002, Randomization and Approximation Techniques in Computer Science. 6th International Workshop, RANDOM 2002. Proceedings (Lecture Notes in Computer Science Vol.2483), P1; Bar-Yossef Z., 2001, STOC, P266; Basharin Georgij P., 1959, THEORY PROBABILITY E, V4, P333, DOI [10.1137/1104033, DOI 10.1137/1104033]; Bhattacharyya A, 2016, PODS'16: PROCEEDINGS OF THE 35TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P385, DOI 10.1145/2902251.2902284; Bhuvanagiri L, 2006, LECT NOTES COMPUT SC, V4168, P148; Chakrabarti A, 2006, INTERNET MATH, V3, P63, DOI 10.1080/15427951.2006.10129117; Chakrabarti A, 2010, ACM T ALGORITHMS, V6, DOI 10.1145/1798596.1798604; Chakrabarti A, 2008, PROCEEDINGS OF THE NINETEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P720; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Chien Steve, 2010, SPACE EFFICIENT ESTI; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; COVER TM, 1969, ANN MATH STAT, V40, P828, DOI 10.1214/aoms/1177697590; Crouch Michael, 2016, LIPICS LEIBNIZ INT P, V57; Dagan Yuval, 2018, ARXIV180301420; Diakonikolas Ilias, 2019, P 29 ANN C LEARN THE; EFRON B, 1976, BIOMETRIKA, V63, P435, DOI 10.2307/2335721; FLAJOLET P, 1985, J COMPUT SYST SCI, V31, P182, DOI 10.1016/0022-0000(85)90041-8; Fukuchi K, 2017, IEEE INT SYMP INFO, P2103, DOI 10.1109/ISIT.2017.8006900; Goldreich O., 2000, ELECT C COMPUT COMPL, V7, P20; Guha Sudipto, 2009, ACM T ALGORITHMS, V5; GUHA SUDIPTO, 2007, JMLR P, V2, P171; Han Y, 2016, PROCEEDINGS OF 2016 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA 2016), P256; Hao Y., 2018, ADV NEURAL INFORM PR, P8834; Hao Yi, 2019, ARXIV190603794; Harvey Nicholas J. A., 2006, P 49 ANN IEEE S FDN, P489; HELLMAN ME, 1970, ANN MATH STAT, V41, P765, DOI 10.1214/aoms/1177696958; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Indyk P, 2000, ANN IEEE SYMP FOUND, P189, DOI 10.1109/SFCS.2000.892082; Indyk Piotr, 2005, P 37 ANN ACM S THEOR, P202; Jain Ayush, 2018, P 2018 IEEE INT S IN; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Kane DM, 2010, PODS 2010: PROCEEDINGS OF THE TWENTY-NINTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P41, DOI 10.1145/1807085.1807094; Lall A., 2006, Performance Evaluation Review, V34, P145, DOI 10.1145/1140103.1140295; LEIGHTON FT, 1986, IEEE T INFORM THEORY, V32, P733, DOI 10.1109/TIT.1986.1057250; Metwally A, 2005, LECT NOTES COMPUT SC, V3363, P398, DOI 10.1007/978-3-540-30570-5_27; Miller G., 1955, INFORM THEORY PSYCHO, P95; Moshkovitz Dana, 2017, C LEARN THEOR, P1516; Obremski Maciej, 2017, P 20 INT WORKSH APPR; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Paninski L, 2004, IEEE T INFORM THEORY, V50, P2200, DOI 10.1109/TIT.2004.833360; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Raz Ran, 2016, P IEEE 57 ANN S FDN; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Steinhardt Jacob, 2016, P 29 C LEARNING THEO, P1490; Valiant G, 2011, ACM S THEORY COMPUT, P685; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468	53	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305019
C	Agarwal, A; Peng, JH; Milenkovic, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Agarwal, Abhishek; Peng, Jianhao; Milenkovic, Olgica			Online Convex Matrix Factorization with Representative Regions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS; COMPLEXITY; MODEL	Matrix factorization (MF) is a versatile learning method that has found wide applications in various data-driven disciplines. Still, many MF algorithms do not adequately scale with the size of available datasets and/or lack interpretability. To improve the computational efficiency of the method, an online (streaming) MF algorithm was proposed in [1]. To enable data interpretability, a constrained version of MF, termed convex MF, was introduced in [2]. In the latter work, the basis vectors are required to lie in the convex hull of the data samples, thereby ensuring that every basis can be interpreted as a weighted combination of data samples. No current algorithmic solutions for online convex MF are known as it is challenging to find adequate convex bases without having access to the complete dataset. We address both problems by proposing the first online convex MF algorithm that maintains a collection of constant-size sets of representative data samples needed for interpreting each of the basis [2] and has the same almost sure convergence guarantees as the online learning algorithm of [1]. Our proof techniques combine random coordinate descent algorithms with specialized quasi-martingale convergence analysis. Experiments on synthetic and real world datasets show significant computational savings of the proposed online convex MF method compared to classical convex MF. Since the proposed method maintains small representative sets of data samples needed for convex interpretations, it is related to a body of work in theoretical computer science, pertaining to generating point sets [3], and in computer vision, pertaining to archetypal analysis [4]. Nevertheless, it differs from these lines of work both in terms of the objective and algorithmic implementations.	[Agarwal, Abhishek; Peng, Jianhao; Milenkovic, Olgica] Univ Illinois, Elect & Comp Engn, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Agarwal, A (corresponding author), Univ Illinois, Elect & Comp Engn, Champaign, IL 61820 USA.	abhiag@illinois.edu; jianhao2@illinois.edu; milenkov@illinois.edu			DB2K NIH [3U01CA198943-02S 1]; NSF/IUCR CCBGM Center; SVCF [CZI 2018-182799 2018-182797]	DB2K NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF/IUCR CCBGM Center; SVCF	The authors are grateful to Prof. Bruce Hajek for valuable discussions. This work was funded by the DB2K NIH 3U01CA198943-02S 1, NSF/IUCR CCBGM Center, and the SVCF CZI 2018-182799 2018-182797.	[Anonymous], 2016, ARXIV161209296; [Anonymous], 2017, 31 AAAI C ART INT; Bach Francis, 1869, ARXIV08121869; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Blum A., 2016, SODA, P548; Bouchard G, 2013, AISTATS, V31, P144; Dai W, 2012, IEEE T SIGNAL PROCES, V60, P6340, DOI 10.1109/TSP.2012.2215026; Ding C, 2010, IEEE T PATTERN ANAL, V32, P45, DOI 10.1109/TPAMI.2008.277; Dua D., 2017, UCI MACHINE LEARNING; Esser E, 2012, IEEE T IMAGE PROCESS, V21, P3239, DOI 10.1109/TIP.2012.2190081; Fevotte C, 2015, IEEE T IMAGE PROCESS, V24, P4810, DOI 10.1109/TIP.2015.2468177; Gribonval R, 2015, IEEE T INFORM THEORY, V61, P3469, DOI 10.1109/TIT.2015.2424238; Kasiviswanathan SP, 2012, P ADV NEUR INF PROC, P2258; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee DD, 2001, ADV NEUR IN, V13, P556; Lee H., 2007, ADV NEURAL INF PROCE, P801; Leonardis A, 2000, COMPUT VIS IMAGE UND, V78, P99, DOI 10.1006/cviu.1999.0830; Lin CJ, 2007, NEURAL COMPUT, V19, P2756, DOI 10.1162/neco.2007.19.10.2756; Liu HF, 2012, IEEE T PATTERN ANAL, V34, P1299, DOI 10.1109/TPAMI.2011.217; Mairal J., 2009, ADV NEURAL INFORM PR, P1033; Mairal J, 2010, J MACH LEARN RES, V11, P19; Mei J., 2018, P EUR C COMP VIS, P486; PAATERO P, 1994, ENVIRONMETRICS, V5, P111, DOI 10.1002/env.3170050203; Rubinstein R, 2010, IEEE T SIGNAL PROCES, V58, P1553, DOI 10.1109/TSP.2009.2036477; Srebro N., 2005, P ADV NEURAL INFORM; Tang J, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2564638; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xia Rui, 2019, ARXIV190306500; Zhao RB, 2016, INT CONF ACOUST SPEE, P2662, DOI 10.1109/ICASSP.2016.7472160; Zhao Renbo, 2016, ARXIV160800075; Zheng GXY, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14049	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904083
C	Aglietti, V; Bonilla, EV; Damoulas, T; Cripps, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aglietti, Virginia; Bonilla, Edwin V.; Damoulas, Theodoros; Cripps, Sally			Structured Variational Inference in Continuous Cox Process Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a scalable framework for inference in a continuous sigmoidal Cox process that assumes the corresponding intensity function is given by a Gaussian process (GP) prior transformed with a scaled logistic sigmoid function. We present a tractable representation of the likelihood through augmentation with a superposition of Poisson processes. This view enables a structured variational approximation capturing dependencies across variables in the model. Our framework avoids discretization of the domain, does not require accurate numerical integration over the input space and is not limited to GPs with squared exponential kernels. We evaluate our approach on synthetic and real-world data showing that its benefits are particularly pronounced on multivariate input settings where it overcomes the limitations of mean-field methods and sampling schemes. We provide the state of-the-art in terms of speed, accuracy and uncertainty quantification trade-offs.	[Aglietti, Virginia; Damoulas, Theodoros] Univ Warwick, Alan Turing Inst, Coventry, W Midlands, England; [Bonilla, Edwin V.] CSIROs Data6l, Paris, France; [Cripps, Sally] Univ Sydney, Ctr Translat Data Sci, Sydney, NSW, Australia	University of Warwick; Commonwealth Scientific & Industrial Research Organisation (CSIRO); University of Sydney	Aglietti, V (corresponding author), Univ Warwick, Alan Turing Inst, Coventry, W Midlands, England.	V.Aglietti@warwick.ac.uk; Edwin.Bonilla@data61.csiro.au; T.Damoulas@warwick.ac.uk; Sally.Cripps@sydney.edu.au			EPSRC [EP/L0167 10/1]; Alan Turing Institute under EPSRC [EP/N510129/1]; Lloyds Register Foundation programme on Data Centric Engineering; University of Sydney's Centre for Translational Data Science; Australian Research Council ARC [FT140101266]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Lloyds Register Foundation programme on Data Centric Engineering; University of Sydney's Centre for Translational Data Science; Australian Research Council ARC(Australian Research Council)	This work was supported by the EPSRC grant EP/L0167 10/1, The Alan Turing Institute under EPSRC grant EP/N510129/1, the Lloyds Register Foundation programme on Data Centric Engineering, the University of Sydney's Centre for Translational Data Science and the Australian Research Council ARC FT140101266.	Adams R. P., 2009, P 26 ANN INT C MACH, P9, DOI DOI 10.1145/1553374.1553376; Aglietti V, 2019, PR MACH LEARN RES, V89, P537; Bonilla EV, 2019, J MACH LEARN RES, V20; Brix A, 2001, J R STAT SOC B, V63, P823, DOI 10.1111/1467-9868.00315; Centre For The Biology Of Memory and Sargolini E, 2014, GRID CELL DAT SARG; COX DR, 1955, J ROY STAT SOC B, V17, P129; Cunningham J. P., 2008, P 25 INT C MACH LEAR, P192, DOI DOI 10.1145/1390156.1390181; Diggle PJ, 2013, STAT SCI, V28, P542, DOI 10.1214/13-STS441; Donner C., 2018, J MACHINE LEARNING R, V19, P2710; Grubesic TH, 2008, J QUANT CRIMINOL, V24, P285, DOI 10.1007/s10940-008-9047-5; Gunter T, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P310; John S. T., 2018, P 35 INT C MACH LEAR, V80, P2362; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Lasko TA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P469; LEWIS PAW, 1979, NAV RES LOG, V26, P403, DOI 10.1002/nav.3800260304; Lian WZ, 2015, PR MACH LEARN RES, V37, P2030; Lloyd C, 2016, JMLR WORKSH CONF PRO, V51, P389; Lloyd C, 2015, PR MACH LEARN RES, V37, P1814; Lopera A. F., 2019, ARTIFICIAL INTELLIGE; Marsan D, 2008, SCIENCE, V319, P1076, DOI 10.1126/science.1148783; Moller J, 1998, SCAND J STAT, V25, P451, DOI 10.1111/1467-9469.00115; Moller J., 2003, STA INFERENCE SIMULA; Nguyen Trung V., 2014, NIPS; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Sargolini F, 2006, SCIENCE, V312, P758, DOI 10.1126/science.1125572; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Vere-Jones, 2003, INTRO THEORY POINT P; Walder C. J., 2017, P MACHINE LEARNING R, P3579	28	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904014
C	Ainsworth, S; Barnes, M; Srinivasa, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ainsworth, Samuel; Barnes, Matt; Srinivasa, Siddhartha			Mo' States Mo' Problems: Emergency Stop Mechanisms from Observation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In many environments, only a relatively small subset of the complete state space is necessary in order to accomplish a given task. We develop a simple technique using emergency stops (e-stops) to exploit this phenomenon. Using e-stops significantly improves sample complexity by reducing the amount of required exploration, while retaining a performance bound that efficiently trades off the rate of convergence with a small asymptotic sub-optimality gap. We analyze the regret behavior of e-stops and present empirical results in discrete and continuous settings demonstrating that our reset mechanism can provide order-of-magnitude speedups on top of existing reinforcement learning methods.	[Ainsworth, Samuel; Barnes, Matt; Srinivasa, Siddhartha] Univ Washington, Sch Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Ainsworth, S (corresponding author), Univ Washington, Sch Comp Sci & Engn, Seattle, WA 98195 USA.	skainswo@cs.washington.edu; mbarnes@cs.washington.edu; siddh@cs.washington.edu			National Science Foundation TRIPODS+X:RES [A135918]; National Institute of Health R01 [R01EB019335]; National Science Foundation CPS [1544797]; National Science Foundation NRI [1637748]; Office of Naval Research; RCTA; Amazon; Honda Research Institute USA	National Science Foundation TRIPODS+X:RES; National Institute of Health R01(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); National Science Foundation CPS; National Science Foundation NRI(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); RCTA; Amazon; Honda Research Institute USA	The authors would like to thank The Notorious B.I.G. and Justin Fu for their contributions to music, pop culture, and our implementation of DDPG. This work was (partially) funded by the National Science Foundation TRIPODS+X:RES (#A135918), National Institute of Health R01 (#R01EB019335), National Science Foundation CPS (#1544797), National Science Foundation NRI (#1637748), the Office of Naval Research, the RCTA, Amazon, and Honda Research Institute USA.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Abbeel P., 2005, P 22 INT C MACH LEAR; Alshiekh M, 2018, AAAI CONF ARTIF INTE, P2669; Azar MG, 2017, PR MACH LEARN RES, V70; Bagnell J Andrew, 2015, TECHNICAL REPORT; Bradbury J., 2018, JAX COMPOSABLE TRANS; Brockman Greg, 2016, OPENAL GYM; Eysenbach Benjamin, 2018, LEAVE NO TRACE LEARN; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Geramifard A, 2011, P AMER CONTR CONF, P3393; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kahn G., 2017, UNCERTAINTY AWARE RE; Kakade Sham, 2018, ARXIV180209184; Laskey Michael, 2016, INT C ROB AUT; Maire Frederic, 2005, IJCAI WORKSH PLANN L; Nagabandi Anusha, 2018, INT C ROB AUT; Richter C, 2017, ROBOTICS: SCIENCE AND SYSTEMS XIII; Ross S., 2011, P INT C ART INT STAT; Ross S, 2013, IEEE INT CONF ROBOT, P1765, DOI 10.1109/ICRA.2013.6630809; Smart William D, 2000, P 17 INT C MACH LEAR; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Torabi Faraz, 2018, ARXIV180706158; Widrow Bernard, 1964, COMPUTER INFORM SCI, P288	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906080
C	Ajmera, S; Rajagopal, S; Rehman, RU; Sridharan, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ajmera, Shagun; Rajagopal, Shreya; Rehman, Razi Ur; Sridharan, Devarajan			Infra-slow brain dynamics as a marker for cognitive function and decline	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CONNECTIVITY; FLUCTUATIONS; CORTEX; BEHAVIOR; SIGNALS; DRIVEN	Functional magnetic resonance imaging (fMRI) enables measuring human brain activity, in vivo. Yet, the fMRI hemodynamic response unfolds over very slow timescales (<0.1-1 Hz), orders of magnitude slower than millisecond timescales of neural spiking. It is unclear, therefore, if slow dynamics as measured with fMRI are relevant for cognitive function. We investigated this question with a novel application of Gaussian Process Factor Analysis (GPFA) and machine learning to fMRI data. We analyzed slowly sampled (1.4 Hz) fMRI data from 1000 healthy human participants (Human Connectome Project database), and applied GPFA to reduce dimensionality and extract smooth latent dynamics. GPFA dimensions with slow (<1 Hz) characteristic timescales identified, with high accuracy (>95%), the specific task that each subject was performing inside the fMRI scanner. Moreover, functional connectivity between slow GPFA latents accurately predicted inter-individual differences in behavioral scores across a range of cognitive tasks. Finally, infra-slow (<0.1 Hz) latent dynamics predicted CDR (Clinical Dementia Rating) scores of individual patients, and identified patients with mild cognitive impairment (MCI) who would progress to develop Alzheimer's dementia (AD). Slow and infra-slow brain dynamics may be relevant for understanding the neural basis of cognitive function, in health and disease.	[Ajmera, Shagun; Rajagopal, Shreya; Sridharan, Devarajan] Indian Inst Sci, Ctr Neurosci, Bangalore, Karnataka, India; [Rehman, Razi Ur; Sridharan, Devarajan] Indian Inst Sci, Comp Sci & Automat, Bangalore, Karnataka, India	Indian Institute of Science (IISC) - Bangalore; Indian Institute of Science (IISC) - Bangalore	Sridharan, D (corresponding author), Indian Inst Sci, Ctr Neurosci, Bangalore, Karnataka, India.; Sridharan, D (corresponding author), Indian Inst Sci, Comp Sci & Automat, Bangalore, Karnataka, India.	ajmerashagun@gmail.com; shreyakr96@gmail.com; razirmp@gmail.com; sridhar@iisc.ac.in			Wellcome Trust-Department of Biotechnology India Alliance Intermediate fellowship [IA/I/15/2/502089]; Science and Engineering Research Board Early Career award [ECR/2016/000403]; Pratiksha Trust Young Investigator award; Department of BiotechnologyIndian Institute of Science Partnership Program grant; Sonata Software grant; Tata Trusts grant	Wellcome Trust-Department of Biotechnology India Alliance Intermediate fellowship(Wellcome Trust DBT India Alliance); Science and Engineering Research Board Early Career award; Pratiksha Trust Young Investigator award; Department of BiotechnologyIndian Institute of Science Partnership Program grant; Sonata Software grant; Tata Trusts grant	We wish to thank Byron Yu and Abhijit Chinchani for guidance with GPFA analysis. This research was funded by a Wellcome Trust-Department of Biotechnology India Alliance Intermediate fellowship [IA/I/15/2/502089], a Science and Engineering Research Board Early Career award [ECR/2016/000403], a Pratiksha Trust Young Investigator award, a Department of BiotechnologyIndian Institute of Science Partnership Program grant, a Sonata Software grant and a Tata Trusts grant (to DS). We would like to thank the Human Connectome Project (HCP) and Alzheimers Disease Neuroimaging Initiative (ADNI) for access to fMRI data; details regarding their funding sources are available in their respective websites (http://www.humanconnectomeproject.org/; https://adni.loni.usc.edu).	Allen EA, 2014, CEREB CORTEX, V24, P663, DOI 10.1093/cercor/bhs352; Beckmann CF, 2005, PHILOS T R SOC B, V360, P1001, DOI 10.1098/rstb.2005.1634; Bokil H, 2010, J NEUROSCI METH, V192, P146, DOI 10.1016/j.jneumeth.2010.06.020; Bolt T, 2017, HUM BRAIN MAPP, V38, P1992, DOI 10.1002/hbm.23500; Chan AW, 2015, NAT COMMUN, V6, DOI 10.1038/ncomms8738; De Martino F, 2008, NEUROIMAGE, V43, P44, DOI 10.1016/j.neuroimage.2008.06.037; Fox MD, 2007, NAT REV NEUROSCI, V8, P700, DOI 10.1038/nrn2201; Glasser MF, 2013, NEUROIMAGE, V80, P105, DOI 10.1016/j.neuroimage.2013.04.127; Glover GH, 2011, NEUROSURG CLIN N AM, V22, P133, DOI 10.1016/j.nec.2010.11.001; Greicius M, 2008, CURR OPIN NEUROL, V21, P424, DOI 10.1097/WCO.0b013e328306f2c5; Greicius MD, 2009, CEREB CORTEX, V19, P72, DOI 10.1093/cercor/bhn059; Hasson U, 2004, SCIENCE, V303, P1634, DOI 10.1126/science.1089506; Hiltunen T, 2014, J NEUROSCI, V34, P356, DOI 10.1523/JNEUROSCI.0276-13.2014; Hodge MR, 2016, NEUROIMAGE, V124, P1102, DOI 10.1016/j.neuroimage.2015.04.046; Kashyap R, 2019, NEUROIMAGE, V189, P804, DOI 10.1016/j.neuroimage.2019.01.069; Ko AL, 2011, J NEUROSCI, V31, P11728, DOI 10.1523/JNEUROSCI.5730-10.2011; Lankinen K, 2014, NEUROIMAGE, V92, P217, DOI 10.1016/j.neuroimage.2014.02.004; Lee JH, 2010, NATURE, V465, P788, DOI 10.1038/nature09108; Leopold DA, 2003, CEREB CORTEX, V13, P422, DOI 10.1093/cercor/13.4.422; Liegeois R, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-10317-7; Linkenkaer-Hansen K, 2001, J NEUROSCI, V21, P1370; Logothetis NK, 2001, NATURE, V412, P150, DOI 10.1038/35084005; Mitra A, 2018, NEURON, V98, P297, DOI 10.1016/j.neuron.2018.03.015; Mitra A, 2015, ELIFE, V4, DOI 10.7554/eLife.10781; Monto S, 2007, CEREB CORTEX, V17, P1386, DOI 10.1093/cercor/bhl049; Murphy K, 2013, NEUROIMAGE, V80, P349, DOI 10.1016/j.neuroimage.2013.04.001; O'Bryant SE, 2008, ARCH NEUROL-CHICAGO, V65, P1091, DOI 10.1001/archneur.65.8.1091; Power JD, 2011, NEURON, V72, P665, DOI 10.1016/j.neuron.2011.09.006; Rasmussen CE, 2010, J MACH LEARN RES, V11, P3011; Sacre P, 2019, P NATL ACAD SCI USA, V116, P1404, DOI 10.1073/pnas.1811259115; Shen XL, 2017, NAT PROTOC, V12, P506, DOI 10.1038/nprot.2016.178; Shirer WR, 2012, CEREB CORTEX, V22, P158, DOI 10.1093/cercor/bhr099; Sridharan D, 2008, P NATL ACAD SCI USA, V105, P12569, DOI 10.1073/pnas.0800005105; Sridharan D, 2007, NEURON, V55, P521, DOI 10.1016/j.neuron.2007.07.003; Sun FT, 2004, NEUROIMAGE, V21, P647, DOI 10.1016/j.neuroimage.2003.09.056; Van Essen DC, 2012, NEUROIMAGE, V62, P2222, DOI 10.1016/j.neuroimage.2012.02.018; Viswanathan A, 2007, NAT NEUROSCI, V10, P1308, DOI 10.1038/nn1977; Wen HG, 2016, BRAIN TOPOGR, V29, P13, DOI 10.1007/s10548-015-0448-0; Yu Byron M, 2009, ADV NEURAL INFORM PR, P1881, DOI DOI 10.1152/JN.90941; Zacks JM, 2001, NAT NEUROSCI, V4, P651, DOI 10.1038/88486	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID	32231426				2022-12-19	WOS:000534424307001
C	Allen, C; Balazevic, I; Hospedales, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Allen, Carl; Balazevic, Ivana; Hospedales, Timothy			What the Vec? Towards Probabilistically Grounded Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Word2Vec (W2V) and GloVe are popular, fast and efficient word embedding algorithms. Their embeddings are widely used and perform well on a variety of natural language processing tasks. Moreover, W2V has recently been adopted in the field of graph embedding, where it underpins several leading algorithms. However, despite their ubiquity and relatively simple model architecture, a theoretical understanding of what the embedding parameters of W2V and GloVe learn and why that is useful in downstream tasks has been lacking. We show that different interactions between PMI vectors reflect semantic word relationships, such as similarity and paraphrasing, that are encoded in low dimensional word embeddings under a suitable projection, theoretically explaining why embeddings of W2V and GloVe work. As a consequence, we also reveal an interesting mathematical interconnection between the considered semantic relationships themselves.	[Allen, Carl; Balazevic, Ivana; Hospedales, Timothy] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland; [Hospedales, Timothy] Samsung AI Ctr, Cambridge, England	University of Edinburgh	Allen, C (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	carl.allen@ed.ac.uk; ivana.balazevic@ed.ac.uk; t.hospedales@ed.ac.uk		Allen, Carl/0000-0002-1536-657X	Centre for Doctoral Training in Data Science - EPSRC [EP/L016427/1]; University of Edinburgh	Centre for Doctoral Training in Data Science - EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh	We thank Ivan Titov, Jonathan Mallinson and the anonymous reviewers for helpful comments. Carl Allen and Ivana Balazevi ' c were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh.	Agirre E., 2009, STUDY SIMILARITY REL; Allen Carl, 2019, ARXIV190911611; Allen Carl, 2019, INT C MACH LEARN; Arora Sanjeev, 2016, T ASS COMPUTATIONAL; Asr Fatemeh Torabi, 2018, LONG PAPERS, V1; Bullinaria JA, 2007, BEHAV RES METHODS, V39, P510, DOI 10.3758/BF03193020; Collins M, 2002, ADV NEURAL INFORM PR, V14; CONNEAU A, 2017, EMPIRICAL METHODS NA; Cotterell Ryan, 2017, P 15 C EUR CHAPT ASS, V2, P175; Ethayarajh Kawin, 2019, ASS COMPUTATIONAL LI; Finkelstein Lev, 2001, INT C WORLD WID WEB; Gittens Alex, 2017, ASS COMPUTATIONAL LI; Grover A, 2016, INT C KNOWL DISC DAT; Hashimoto Tatsunori B, 2016, T ASS COMPUTATIONAL; Islam Aminul, 2012, INT C COMP LING; Jaiswal Amit Kumar, 2018, LERNEN WISSEN DATEN; Kiros Ryan, 2015, P NIPS; Landgraf AJ, 2017, ARXIV170509755; Levy O., 2015, T ASS COMPUTATIONAL; Levy Omer, 2014, COMPUTATIONAL NATURA; Levy Omer, 2014, NEURIPS; Li Qiuchi, 2018, WORKSH REPR LEARN NL; Linzen Tal, 2016, 1 WORKSH EV VECT SPA; Mahoney M., 2011, TEXT8 WIKIPEDIA DUMP; Mikolov T, 2013, 1 INT C LEARN REPR I; Mikolov T., 2013, 27 ANN C NEUR INF PR, P3111; Mikolov Tomas, 2013, LINGUISTIC REGULARIT; Mimno David, 2017, EMPIRICAL METHODS NA; Muller Klaus-Robert, 1997, INT C ART NEUR NETW; Pennington Jeffrey, 2014, P 2014 EMNLP; Perozzi Bryan, 2014, INT C KNOWL DISC DAT; Qiu Jiezhong, 2018, INT C WEB SEARCH DAT; Rehurek R., 2010, WORKSH NEW CHALL NLP; Schnabel Tobias, 2015, EMPIRICAL METHODS NA; Srebro Nathan, 2003, INT C MACH LEARN; Turney PD, 2010, J ARTIF INTELL RES, V37, P141, DOI 10.1613/jair.2934; Turney Peter D, 2001, EUR C MACH LEARN; Wieting John, 2019, INT C LEARN REPR	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307048
C	Alon, N; Bassily, R; Moran, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alon, Noga; Bassily, Raef; Moran, Shay			Limits of Private Learning with Access to Public Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider learning problems where the training set consists of two types of examples: private and public. The goal is to design a learning algorithm that satisfies differential privacy only with respect to the private examples. This setting interpolates between private learning (where all examples are private) and classical learning (where all examples are public). We study the limits of learning in this setting in terms of private and public sample complexities. We show that any hypothesis class of VC-dimension d can be agnostically learned up to an excess error of a using only (roughly) d/alpha public examples and d/alpha(2) private labeled examples. This result holds even when the public examples are unlabeled. This gives a quadratic improvement over the standard d/alpha(2) upper bound on the public sample complexity (where private examples can be ignored altogether if the public examples are labeled). Furthermore, we give a nearly matching lower bound, which we prove via a generic reduction from this setting to the one of private learning without public data.	[Alon, Noga] Princeton Univ, Dept Math, Princeton, NJ 08544 USA; [Bassily, Raef] Ohio State Univ, Dept Comp Sci & Engn, Columbus, OH 43210 USA; [Moran, Shay] Google AI, Princeton, NJ USA	Princeton University; University System of Ohio; Ohio State University	Alon, N (corresponding author), Princeton Univ, Dept Math, Princeton, NJ 08544 USA.	nalon@math.princeton.edu; bassily.l@osu.edu; shaymoran1@gmail.com	Alon, Noga/GOV-5970-2022	Alon, Noga/0000-0003-1332-4883	NSF [DMS-1855464, AF-1908281, SHF-1907715]; BSF [2018267]; Simons Foundation; Google Faculty Research Award; OSU faculty start-up	NSF(National Science Foundation (NSF)); BSF(US-Israel Binational Science Foundation); Simons Foundation; Google Faculty Research Award(Google Incorporated); OSU faculty start-up	N. Alon's research is supported in part by NSF grant DMS-1855464, BSF grant 2018267, and the Simons Foundation. R. Bassily's research is supported by NSF Awards AF-1908281, SHF-1907715, Google Faculty Research Award, and OSU faculty start-up support.	Alon N, 2019, ACM S THEORY COMPUT, P852, DOI 10.1145/3313276.3316312; Alon Noga, 2019, ARXIV191011519CSLG; Bassily R., 2018, ADV NEURAL INFORM PR, P7102; Beimel Amos, 2013, Approximation, Randomization, and Combinatorial Optimization. Algorithms and Techniques. Algorithms and Techniques. 16th International Workshop, APPROX 2013 and 17th International Workshop, RANDOM 2013. Proceedings: LNCS 8096, P363, DOI 10.1007/978-3-642-40328-6_26; Beimel A., 2015, SODA, P461, DOI DOI 10.1137/1.9781611973730.32; BenDavid Ddivid Shai, 2009, COLT 2009 22 C LEARN; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Chaudhuri Kamalika, 2011, JMLR Workshop Conf Proc, V2011, P155; Dwork C., 2010, FOCS; Dwork C, 2006, EUROCRYPT; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Hamm J, 2016, PR MACH LEARN RES, V48; Kasiviswanathan SP, 2008, ANN IEEE SYMP FOUND, P531, DOI 10.1109/FOCS.2008.27; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; Papernot N., 2018, ICLR P, P1; Papernot Nicolas, 2017, STAT, P1050; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019	18	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902002
C	Alt, B; Sosic, A; Koeppl, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Alt, Bastian; Sosic, Adrian; Koeppl, Heinz			Correlation Priors for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE	Many decision-making problems naturally exhibit pronounced structures inherited from the characteristics of the underlying environment. In a Markov decision process model, for example, two distinct states can have inherently related semantics or encode resembling physical state configurations. This often implies locally correlated transition dynamics among the states. In order to complete a certain task in such environments, the operating agent usually needs to execute a series of temporally and spatially correlated actions. Though there exists a variety of approaches to capture these correlations in continuous state-action domains, a principled solution for discrete environments is missing. In this work, we present a Bayesian learning framework based on Polya-Gamma augmentation that enables an analogous reasoning in such cases. We demonstrate the framework on a number of common decision-making related problems, such as imitation learning, subgoal extraction, system identification and Bayesian reinforcement learning. By explicitly modeling the underlying correlation structures of these problems, the proposed approach yields superior predictive performance compared to correlation-agnostic models, even when trained on data sets that are an order of magnitude smaller in size.	[Alt, Bastian; Sosic, Adrian; Koeppl, Heinz] Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany	Technical University of Darmstadt	Alt, B (corresponding author), Tech Univ Darmstadt, Dept Elect Engn & Informat Technol, Darmstadt, Germany.	bastian.alt@bcs.tu-darmstadt.de; adrian.sosic@bcs.tu-darmstadt.de; heinz.koeppl@bcs.tu-darmstadt.de		Alt, Bastian/0000-0002-1522-5400	German Research Foundation (DFG) within the Collaborative Research Center (CRC) [1053 - MAKI]	German Research Foundation (DFG) within the Collaborative Research Center (CRC)(German Research Foundation (DFG))	This work has been funded by the German Research Foundation (DFG) as part of the projects B4 and C3 within the Collaborative Research Center (CRC) 1053 - MAKI.	Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Chalkiadakis Georgios, 2003, P 2 INT JOINT C AUT, P709; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Engel Y., 2003, P 20 INT C MACH LEAR, P154; Ghavamzadeh M, 2015, FOUND TRENDS MACH LE, V8, P360, DOI 10.1561/2200000049; Guez A., 2012, ADV NEURAL INFORM PR; Gupta S., 2018, ARXIV180805904; Hoffman MW, 2014, JMLR WORKSH CONF PRO, V33, P365; Ishwaran H, 2001, J AM STAT ASSOC, V96, P161, DOI 10.1198/016214501750332758; Killian Taylor, 2017, Adv Neural Inf Process Syst, V30, P6250; Lafferty JohnD., 2006, ADV NEURAL INFORM PR, P147; Linderman S.W., 2015, ADV NEURAL INFORM PR, P3456; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Paraschos A., 2013, ADV NEURAL INFORM PR; Pavlov M., 2008, NIPS WORKSH MOD UNC; Polson NG, 2013, J AM STAT ASSOC, V108, P1339, DOI 10.1080/01621459.2013.829001; Poupart P., 2006, ICML, P697; Ranchod P, 2015, IEEE INT C INT ROBOT, P471, DOI 10.1109/IROS.2015.7353414; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rothkopf CA, 2011, LECT NOTES ARTIF INT, V6913, P34, DOI 10.1007/978-3-642-23808-6_3; Sosic A, 2018, J MACH LEARN RES, V19; Sosic A, 2018, IEEE T PATTERN ANAL, V40, P1295, DOI 10.1109/TPAMI.2017.2711024; Sosic A, 2016, INT CONF ACOUST SPEE, P4801, DOI 10.1109/ICASSP.2016.7472589; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Titsias Michalis, 2008, ADV NEURAL INFORM PR; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Wenzel F, 2019, AAAI CONF ARTIF INTE, P5417; Zhu Y., 2018, ARXV180209564	31	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905079
C	Amrollahi, A; Zandieh, A; Kapralov, M; Krause, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Amrollahi, Andisheh; Zandieh, Amir; Kapralov, Michael; Krause, Andreas			Efficiently Learning Fourier Sparse Set Functions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning set functions is a key challenge arising in many domains, ranging from sketching graphs to black-box optimization with discrete parameters. In this paper we consider the problem of efficiently learning set functions that are defined over a ground set of size n and that are sparse (say k-sparse) in the Fourier domain. This is a wide class, that includes graph and hypergraph cut functions, decision trees and more. Our central contribution is the first algorithm that allows learning functions whose Fourier support only contains low degree (say degree d = o(n)) polynomials using O(kd log n) sample complexity and runtime O(kn log(2) k log n log d). This implies that sparse graphs with k edges can, for the first time, be learned from O(k log n) observations of cut values and in linear time in the number of vertices. Our algorithm can also efficiently learn (sums of) decision trees of small depth. The algorithm exploits techniques from the sparse Fourier transform literature and is easily implementable. Lastly, we also develop an efficient robust version of our algorithm and prove l(2)/l(2) approximation guarantees without any statistical assumptions on the noise.	[Amrollahi, Andisheh; Krause, Andreas] Swiss Fed Inst Technol, Zurich, Switzerland; [Zandieh, Amir; Kapralov, Michael] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	ETH Zurich; Ecole Polytechnique Federale de Lausanne	Amrollahi, A (corresponding author), Swiss Fed Inst Technol, Zurich, Switzerland.	amrollaa@ethz.ch; amir.zandieh@epfl.ch; michael.kapralov@epfl.ch; krausea@ethz.ch	Zandieh, Amir/AAX-4763-2020	Zandieh, Amir/0000-0002-1294-9390; Krause, Andreas/0000-0001-7260-9673	ERC	ERC(European Research Council (ERC)European Commission)	Supported by ERC Starting Grant SUBLINEAR.	Das AK, 2013, INT CONF ACOUST SPEE, P5890, DOI 10.1109/ICASSP.2013.6638794; Goldreich O., 1989, Proceedings of the Twenty First Annual ACM Symposium on Theory of Computing, P25, DOI 10.1145/73007.73010; Hassanieh H, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P563; Haviv I, 2017, LECT NOTES MATH, V2169, P163, DOI 10.1007/978-3-319-45282-1_11; Hazan Elad, 2018, INT C LEARN REPR; Indyk P., 2014, P 25 ACM SIAM S DISC, P480; KUSHILEVITZ E, 1993, SIAM J COMPUT, V22, P1331, DOI 10.1137/0222080; Mansour Y., 1994, THEORETICAL ADV NEUR, P391; Price E, 2011, ANN IEEE SYMP FOUND, P295, DOI 10.1109/FOCS.2011.92; Rudelson M, 2008, COMMUN PUR APPL MATH, V61, P1025, DOI 10.1002/cpa.20227; Scheibler R, 2015, IEEE T INFORM THEORY, V61, P2115, DOI 10.1109/TIT.2015.2404441; Stobbe Peter, 2012, P 15 INT C ART INT S, P1125	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906074
C	Andriushchenko, M; Hein, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Andriushchenko, Maksym; Hein, Matthias			Provably Robust Boosted Decision Stumps and Trees against Adversarial Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The problem of adversarial robustness has been studied extensively for neural networks. However, for boosted decision trees and decision stumps there are almost no results, even though they are widely used in practice (e.g. XGBoost) due to their accuracy, interpretability, and efficiency. We show in this paper that for boosted decision stumps the exact min-max robust loss and test error for an 1(infinity)-attack can be computed in O(T log T) time per input, where T is the number of decision stumps and the optimal update step of the ensemble can be done in O(n(2) T log T), where P. is the number of data points. For boosted trees we show how to efficiently calculate and optimize an upper bound on the robust loss, which leads to state-of-the-art robust test error for boosted trees on MNIST (12.5% for epsilon(infinity) = 0.3), FMNIST (23.2% for epsilon(infinity) = 0.1), and CIFAR-10 (74.7% for epsilon(infinity) = 8/255). Moreover, the robust test error rates we achieve are competitive to the ones of provably robust convolutional networks. The code of all our experiments is available at http://github.com/max-andr/provably-robust-boosting.	[Andriushchenko, Maksym; Hein, Matthias] Univ Tubingen, Tubingen, Germany	Eberhard Karls University of Tubingen	Andriushchenko, M (corresponding author), Univ Tubingen, Tubingen, Germany.	maksym.andriushchenko@uni-tuebingen.de; matthias.hein@uni-tuebingen.de			German Federal Ministry of Education and Research (BMBF) through the Tubingen Al Center [FKZ: 01IS 18039A]; DFG Cluster of Excellence "Machine Learning - New Perspectives for Science" [EXC 2064/1, 390727645]; DFG [389792660, TRR 248]	German Federal Ministry of Education and Research (BMBF) through the Tubingen Al Center(Federal Ministry of Education & Research (BMBF)); DFG Cluster of Excellence "Machine Learning - New Perspectives for Science"(German Research Foundation (DFG)); DFG(German Research Foundation (DFG))	We thank the anonymous reviewers for very helpful and thoughtful comments. We acknowledge the support from the German Federal Ministry of Education and Research (BMBF) through the Tubingen Al Center (FKZ: 01IS 18039A). This work was also supported by the DFG Cluster of Excellence "Machine Learning - New Perspectives for Science", EXC 2064/1, project number 390727645, and by DFG grant 389792660 as part of TRR 248.	Athalye Anish, 2018, ICML; Bertsimas D., 2018, INFORMS J OPTIMIZATI, V1, P2; Bertsimas Dimitris, 2017, MACHINE LEARNING; Boyd S, 2004, CONVEX OPTIMIZATION; Brendel Wieland, 2018, ICLR; Buckman Jacob, 2018, P ICLR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen Hongge, 2019, ICML; Chen Tianqi, 2016, KDD; Cheng M., 2019, P ICLR; Cohen Jeremy M, 2019, ICML; CROCE F, 2019, AISTATS; Droste Stefan, 2002, THEORETICAL COMPUTER; Dua D., 2017, UCI MACHINE LEARNING; Dvijotham K, 2018, ARXIV180510265; Ebrahimi Javid, 2018, ACL; Engstrom Logan, 2018, NEURIPS2018 WORKSH S; Freund Yoav, 1996, ICML; Freund Yoav, 2009, ARXIV09052138V1; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Friedman JH, 2002, COMPUT STAT DATA AN, V38, P367, DOI 10.1016/S0167-9473(01)00065-2; Goodfellow I. J., 2015, ICLR; Gowal Sven, 2018, NEUR WORKSH SEC MACH; Guo Chuan, 2019, ICML; Hein Matthias, 2019, CVPR; Hein Matthias, 2017, NEURIPS; Ilyas Andrew, 2018, ICML; Kannan Harini, 2018, ARXIV180306373; Kantchelian Alex, 2016, ICML; Katz Guy, 2017, ICCAV; Ke G., 2017, NEURIPS; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kulynych Bogdan, 2018, NEUR WORKSH SEC MACH; Lapin Maksim, 2016, PAMI, V40, P1533; Lapin Maksim, 2016, CVPR; Laurent Hyafil, 1976, INFORM PROCESSINGLET; Lecuyer M, 2019, P IEEE S SECUR PRIV, P656, DOI 10.1109/SP.2019.00044; Li B, 2021, J NEUROL, V268, P2042, DOI 10.1007/s00415-019-09596-3; LLC Gurobi Optimization, 2019, GUROBI OPTIMIZER REF; Lu Jiajun, 2017, ARXIV PREPRINT ARXIV; Lutz RW, 2008, COMPUT STAT DATA AN, V52, P3331, DOI 10.1016/j.csda.2007.11.006; Madry Aleksander, 2018, ICLR; Moon Seungyong, 2019, ICML; Moosavi-Dezfooli Seyed-Mohsen, 2016, CVPR; Mosbach Marius, 2018, NEURIPS 2018 WORKSH; Nalisnick Eric, 2019, ICLR; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Nguyen A., 2015, CVPR; Olshen R., 1984, CLASSIFICATION REGRE; Papernot Nicolas, 2016, ARXIV180903008; Papernot Nicolas, 2016, IEEE EURO S P; Raghunathan A., 2018, INT C LEARN REPR; Rifkin R, 2004, J MACH LEARN RES, V5, P101; Russu Paolo, 2016, ACM WORKSH AL SEC; Salman Hadi, 2019, NEURLPS; Schapire R. E., 1999, MACHINE LEARNING; Shaham U., 2018, NEUROCOMPUTING; Smith Jack W, 1988, ANN S COMP APPL MED; Stallkamp Johannes, 2012, NEURAL NETWORKS; Szegedy Christian, 2014, ICLR; TJENG V, 2019, ICLR; Tsipras Dimitris, 2019, ICLR; Uzilov AV, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-173; Viola P, 2004, INT J COMPUT VISION, V57, P137, DOI 10.1023/B:VISI.0000013087.49260.fb; Viola P., 2001, P 2001 IEEE COMP SOC, pI, DOI [10.1109/CVPR.2001.990517, DOI 10.1109/CVPR.2001.990517]; Wang Shiqi, 2018, NEURIPS; Wang Y., 2018, ICML; Warmuth Manfred K., 2007, NEURLPS; Weng Tsui-Wei, 2018, ICML; WONG E, 2018, NEURIPS; Wong Eric, 2018, ICML; Xiao Han, 2017, FASHIONMNIST NOVEL I; Xiao Kai Y, 2019, ICLR; Xu H, 2009, J MACH LEARN RES, V10, P1485; Zhang Huan, 2018, NEURIPS	77	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904063
C	Antonakopoulos, K; Belmega, EV; Mertikopoulos, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Antonakopoulos, Kimon; Belmega, E. Veronica; Mertikopoulos, Panayotis			An Adaptive Mirror-Prox Algorithm for Variational Inequalities with Singular Operators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LIPSCHITZ GRADIENT CONTINUITY; MONOTONE-OPERATORS; 1ST-ORDER METHODS; CONVERGENCE; OPTIMIZATION; FLOWS; GAMES	Lipschitz continuity is a central requirement for achieving the optimal O(1/T) rate of convergence in monotone, deterministic variational inequalities (a setting that includes convex minimization, convex-concave optimization, nonatomic games, and many other problems). However, in many cases of practical interest, the operator defining the variational inequality may exhibit singularities at the boundary of the feasible region, precluding in this way the use of fast gradient methods that attain this optimal rate (such as Nemirovski's mirror-prox algorithm and its variants). To address this issue, we consider a regularity condition which relates the variation of the operator to that of a suitably chosen Bregman function. Leveraging this Bregman continuity condition, we derive an adaptive mirror-prox algorithm which attains the optimal O(1/T) rate of convergence in problems with possibly singular operators, without any prior knowledge of the degree of smoothness (the Bregman analogue of the Lipschitz constant). We also show that, under Bregman continuity, the mirror-prox algorithm achieves a O(1/root T) convergence rate in stochastic variational inequalities.	[Antonakopoulos, Kimon; Mertikopoulos, Panayotis] Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LIG, F-38000 Grenoble, France; [Belmega, E. Veronica] Univ Cergy Pontoise, CNRS, ETIS, ENSEA, Cergy, France	Inria; UDICE-French Research Universities; Communaute Universite Grenoble Alpes; Institut National Polytechnique de Grenoble; Universite Grenoble Alpes (UGA); Centre National de la Recherche Scientifique (CNRS); Centre National de la Recherche Scientifique (CNRS); CY Cergy Paris Universite	Antonakopoulos, K (corresponding author), Univ Grenoble Alpes, CNRS, INRIA, Grenoble INP,LIG, F-38000 Grenoble, France.	kimon.antonakopoulos@inria.fr; belmega@ensea.fr; panayotis.mertikopoulos@imag.fr			French National Research Agency (ANR) [ANR-16-CE33-0004-01, ANR-18-CE40-0030]	French National Research Agency (ANR)(French National Research Agency (ANR))	The authors gratefully acknowledge financial support from the French National Research Agency (ANR) under grants ORACLESS (ANR-16-CE33-0004-01) and ELIOT (ANR-18-CE40-0030).	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Alvarez F, 2004, SIAM J CONTROL OPTIM, V43, P477, DOI 10.1137/S0363012902419977; [Anonymous], 2007, ALGORITHMIC GAME THE; Bach F., 2019, COLT 19; BAILLON JB, 1977, ISRAEL J MATH, V26, P137, DOI 10.1007/BF03007664; Bao D., 2000, GRAD TEXT M, V200, DOI 10.1007/978-1-4612-1268-3; Bauschke HH, 2017, CONVEX ANAL MONOTONE, DOI 10.1007/978-3-319-48311-5; Bolte J, 2018, SIAM J OPTIMIZ, V28, P2131, DOI 10.1137/17M1138558; Bot R.I., 2018, FORWARD BACKWARD FOR; Bravo M., 2018, P 32 INT C NEUR INF, P5666; Bravo M, 2017, GAME ECON BEHAV, V103, P41, DOI 10.1016/j.geb.2016.06.004; BRUCK RE, 1977, J MATH ANAL APPL, V61, P159, DOI 10.1016/0022-247X(77)90152-4; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Combettes PL, 2005, MULTISCALE MODEL SIM, V4, P1168, DOI 10.1137/050626090; Daskalakis C., 2018, ICLR 18; Facchinei F., 2003, SPRINGER SERIES OPER; Gidel G., 2019, ICLR 19; Goodfellow I. J., 2014, NIPS 14; Hanzely F., 2018, ACCELERATED BREGMAN; Juditsky A., 2011, STOCHASTIC SYST, V1, P17, DOI 10.1287/10-SSY011; Kakade SM, 2012, J MACH LEARN RES, V13, P1865; Kleinrock L., 1975, QUEUEING SYST; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Liang T., 2019, AISTATS 19; Lu H., 2017, RELATIVE CONTINUITY; Lu HH, 2018, SIAM J OPTIMIZ, V28, P333, DOI 10.1137/16M1099546; Malitsky Y., 2018, GOLDEN RATIO ALGORIT; Malitsky Y, 2015, SIAM J OPTIMIZ, V25, P502, DOI 10.1137/14097238X; Mertikopoulos P., 2018, SODA 18; Mertikopoulos P., 2019, ICLR 19; Mertikopoulos P, 2019, MATH PROGRAM, V173, P465, DOI 10.1007/s10107-018-1254-8; Mertikopoulos P, 2018, J ECON THEORY, V177, P315, DOI 10.1016/j.jet.2018.06.002; Mertikopoulos P, 2018, SIAM J OPTIMIZ, V28, P163, DOI 10.1137/16M1105682; Mertikopoulos P, 2017, IEEE T SIGNAL PROCES, V65, P2277, DOI 10.1109/TSP.2017.2656847; Mertikopoulos P, 2016, MATH OPER RES, V41, P1297, DOI 10.1287/moor.2016.0778; Mokhtari A., 2019, UNIFIED ANAL EXTRA G; Nemirovski A, 2004, SIAM J OPTIMIZ, V15, P229, DOI 10.1137/S1052623403425629; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y, 2009, MATH PROGRAM, V120, P221, DOI 10.1007/s10107-007-0149-x; Nesterov Y, 2007, MATH PROGRAM, V109, P319, DOI 10.1007/s10107-006-0034-z; Odena A., 2016, CONDITIONAL IMAGE SY; PASSTY GB, 1979, J MATH ANAL APPL, V72, P383, DOI 10.1016/0022-247X(79)90234-8; Rakhlin A., 2013, NIPS 13; Rakhlin A., 2013, COLT 13; Shahshahani S, 1979, MEMOIRS AM MATH SOC, V211; Sra S, 2012, OPTIMIZATION FOR MACHINE LEARNING, P1; Stampacchia G., 1964, COMPTES RENDUS HEBDO; Wardrop J, 1952, P I CIVIL ENG, V1, P767, DOI 10.1680/ipeds.1952.11362; Yadav A., 2018, ICLR 18	51	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900009
C	Antoniou, A; Storkey, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Antoniou, Antreas; Storkey, Amos			Learning to Learn via Self-Critique	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In few-shot learning, a machine learning system learns from a small set of labelled examples relating to a specific task, such that it can generalize to new examples of the same task. Given the limited availability of labelled examples in such tasks, we wish to make use of all the information we can. Usually a model learns task-specific information from a small training-set (support-set) to predict on an unlabelled validation set (target-set). The target-set contains additional task-specific information which is not utilized by existing few-shot learning methods. Making use of the target-set examples via transductive learning requires approaches beyond the current methods; at inference time, the target-set contains only unlabelled input data-points, and so discriminative learning cannot be used. In this paper, we propose a framework called Self-Critique and Adapt or SCA, which learns to learn an label-free loss function, parameterized as a neural network. A base-model learns on a support-set using existing methods (e.g. stochastic gradient descent combined with the cross-entropy loss), and then is updated for the incoming target-task using the learnt loss function. The label-free loss function is learned such that the target-set-updated model achieves higher generalization performance. Experiments demonstrate that SCA offers substantially reduced errorrates compared to baselines which only adapt on the support-set, and results in state of the art benchmark performance on Mini-ImageNet and Caltech-UCSD Birds 200.	[Antoniou, Antreas; Storkey, Amos] Univ Edinburgh, Edinburgh, Midlothian, Scotland	University of Edinburgh	Antoniou, A (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.	a.antoniou@sms.ed.ac.uk; a.storkey@ed.ac.uk			UK Engineering and Physical Sciences Research Council [EP/L016427/1]; University of Edinburgh; Huawei DDMPLab Innovation Research Grant; EPSRC Centre for Doctoral Training in Data Science	UK Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh; Huawei DDMPLab Innovation Research Grant; EPSRC Centre for Doctoral Training in Data Science(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We would like to thank our colleagues Elliot Crowley, Paul Micaelli, James Owers and Joseph Mellor for reviewing this work and providing useful suggestions/comments. Furthermore, we'd like to thank Harri Edwards for the useful discussions at the beginning of this project and the review and comments he provided. This work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh as well as a Huawei DDMPLab Innovation Research Grant.	Antoniou A., 2019, INT C LEARN REPR, P1; Chen Wei-Yu, 2019, INT C LEARN REPR, P12; Edwards H., 2017, INT C LEARN REPR ICL; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ADV NEURAL INFORM PR, P9516; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Grant Erin, 2018, INT C LEARN REPR; Houthooft R, 2018, ADV NEUR IN, V31; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Li Ke, 2016, ARXIV160601885; Li Zhenguo, 2017, METASGD LEARNING LEA; Liu Yanbin, 2018, ARXIV180510002; Mishra Nikhil, 2017, ARXIV170703141; Munkhdalai T., 2017, ARXIV170300837; Nichol Alex, 2018, ARXIV180302999; Oreshkin BN, 2018, ADV NEUR IN, V31; Qiao SY, 2018, PROC CVPR IEEE, P7229, DOI 10.1109/CVPR.2018.00755; Ravi Sachin, 2016, OPTIMIZATION MODEL F, P1; Rinu Boney A. I., 2018, ICLR2018 MET WORKSH; Rusu Andrei A, 2018, ARXIV180705960; Santoro A, 2017, ADV NEUR IN, V30; Santurkar S, 2018, ADV NEUR IN, V31; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F., 2017, ARXIV170609529; Vapnik V, 2006, 24 TRANSDUCTIVE INFE; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Yu Y., 2018, SAMPLE EFFICIENT REI	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901055
C	Aragam, B; Amini, AA; Zhou, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aragam, Bryon; Amini, Arash A.; Zhou, Qing			Globally optimal score-based learning of directed acyclic graphs in high-dimensions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BAYESIAN NETWORK STRUCTURE; PENALIZED ESTIMATION; VARIABLE SELECTION; LASSO; LIKELIHOOD; SPARSITY; MODELS; RISK	We prove that Omega(s log p) samples suffice to learn a sparse Gaussian directed acyclic graph (DAG) from data, where s is the maximum Markov blanket size. This improves upon recent results that require Omega(s(4) log p) samples in the equal variance case. To prove this, we analyze a popular score-based estimator that has been the subject of extensive empirical inquiry in recent years and is known to achieve state-of-the-art results. Furthermore, the approach we study does not require strong assumptions such as faithfulness that existing theory for score-based learning crucially relies on. The resulting estimator is based around a difficult nonconvex optimization problem, and its analysis may be of independent interest given recent interest in nonconvex optimization in machine learning. Our analysis overcomes the drawbacks of existing theoretical analyses, which either fail to guarantee structure consistency in high-dimensions (i.e. learning the correct graph with high probability), or rely on restrictive assumptions. In contrast, we give explicit finite-sample bounds that are valid in the important p >> n regime.	[Aragam, Bryon] Univ Chicago, Chicago, IL 60637 USA; [Amini, Arash A.; Zhou, Qing] Univ Calif Los Angeles, Los Angeles, CA USA	University of Chicago; University of California System; University of California Los Angeles	Aragam, B (corresponding author), Univ Chicago, Chicago, IL 60637 USA.	bryon@chicagobooth.edu; aaamini@stat.ucla.edu; zhou@stat.ucla.edu			NSF [IIS-1546098]	NSF(National Science Foundation (NSF))	We thank the anonymous reviewers for their feedback. The authors acknowledge the support of the NSF via IIS-1546098.	[Anonymous], 2017, ADV NEURAL INF PROCE; [Anonymous], 2016, ADV NEURAL INFORM PR; Aragam B., 2015, ARXIV151108963; Aragam B., 2017, J STAT SOFTWARE; Aragam B, 2015, J MACH LEARN RES, V16, P2273; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Chickering DM, 2004, J MACH LEARN RES, V5, P1287; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Cussens J., 2011, P 27 C UNC ART INT; Cussens J, 2017, MATH PROGRAM, V164, P285, DOI 10.1007/s10107-016-1087-2; Drton M., 2018, ARXIV180703419; Du Simon S., 2018, INT C LEARN REPR; Evans R. J., 2018, ARXIV180108364; Finale Doshi-Velez, 2017, RIGOROUS SCI INTERPR; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Geiger D, 2002, ANN STAT, V30, P1412; Ghoshal A., 2017, LEARNING IDENTIFIABL; Ghoshal A., 2017, LEARNING LINEAR STRU; Ghoshal A, 2017, PR MACH LEARN RES, V54, P767; GORDON Y, 1988, LECT NOTES MATH, V1317, P84; Gu JY, 2019, STAT COMPUT, V29, P161, DOI 10.1007/s11222-018-9801-y; Han SW, 2016, J AM STAT ASSOC, V111, P1004, DOI 10.1080/01621459.2016.1142880; Huang Jian, 2012, STAT SCI REV J I MAT, V27, P4; Jin C., 2016, LOCAL MAXIMA LIKELIH, P4123; Jin C., 2018, MINIMIZING NONCONVEX; Jin Chi, 2017, ARXIV170300887; Kalisch M, 2007, J MACH LEARN RES, V8, P613; Karimi Hamed, 2016, ECML; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Lin SW, 2014, FOUND COMPUT MATH, V14, P1079, DOI 10.1007/s10208-014-9205-0; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Loh P.-L., 2014, ARXIV14125632; Loh PL, 2015, J MACH LEARN RES, V16, P559; Loh PL, 2014, J MACH LEARN RES, V15, P3065; Meinshausen N, 2006, ANN STAT, V34, P1436, DOI 10.1214/009053606000000281; .Nandy P, 2018, ARXIV150702608; Nicholson A., 2014, INT J APPROX REASON, V55, P59, DOI [10.1016/j.ijar.2013.03.016, DOI 10.1016/J.IJAR.2013.03.016]; Ordyniak S, 2013, J ARTIF INTELL RES, V46, P263, DOI 10.1613/jair.3744; Ott S, 2003, PACIFIC SYMPOSIUM ON BIOCOMPUTING 2004, P557; Ott Sascha, 2003, Genome Inform, V14, P124; Park G., 2017, LEARNING QUADRATIC V; Perrier E, 2008, J MACH LEARN RES, V9, P2251; Peters J, 2014, BIOMETRIKA, V101, P219, DOI 10.1093/biomet/ast043; Raskutti G, 2011, IEEE T INFORM THEORY, V57, P6976, DOI 10.1109/TIT.2011.2165799; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Sanford AD, 2012, J OPER RES SOC, V63, P431, DOI 10.1057/jors.2011.7; Schmidt M., 2007, AAAI, V2, P1278; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Shimizu S, 2011, J MACH LEARN RES, V12, P1225; Shojaie A, 2010, BIOMETRIKA, V97, P519, DOI 10.1093/biomet/asq038; Silander T., 2006, P 22 C UNC ART INT; Singh A. P., 2005, FINDING OPTIMAL BAYE; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Spirtes P., 1991, Social Science Computer Review, V9, P62, DOI 10.1177/089443939100900106; Sun J., 2015, ARXIV151006096; Sun J, 2015, PR MACH LEARN RES, V37, P2351; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Tian JH, 2018, BIOMED PHARMACOTHER, V106, P109, DOI 10.1016/j.biopha.2018.06.070; Uhler C, 2013, ANN STAT, V41, P436, DOI 10.1214/12-AOS1080; van de Geer S, 2014, ELECTRON J STAT, V8, P543, DOI 10.1214/14-EJS894; Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085; Varshney K. R., 2018, ARXIV180609710; Wachter S., 2018, COUNTERFACTUAL EXPLA, V31; Wainwright MJ, 2009, IEEE T INFORM THEORY, V55, P2183, DOI 10.1109/TIT.2009.2016018; Wang Y. S., 2018, HIGH DIMENSIONAL CAU; Xiang J., 2013, PROC 26 INT C ADV NE, P2418; Yang EH, 2015, J MACH LEARN RES, V16, P3813; Ye Q., 2019, ARXIV190412360; Zhang B, 2013, CELL, V153, P707, DOI 10.1016/j.cell.2013.03.030; Zhang CH, 2008, ANN STAT, V36, P1567, DOI 10.1214/07-AOS520; Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhao P, 2006, J MACH LEARN RES, V7, P2541; Zheng Xun, 2018, ADV NEURAL INFORM PR, P9472	77	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304045
C	Araya, E; De Castro, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Araya, Ernesto; De Castro, Yohann			Latent Distance Estimation for Random Geometric Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				VERTEX CLASSIFICATION; CONVERGENT SEQUENCES; SPARSE	Random geometric graphs are a popular choice for a latent points generative model for networks. Their definition is based on a sample of n points X1,X2, . . ., X-n on the Euclidean sphere Sd-1 which represents the latent positions of nodes of the network. The connection probabilities between the nodes are determined by an unknown function (referred to as the "link" function) evaluated at the distance between the latent points. We introduce a spectral estimator of the pairwise distance between latent points and we prove that its rate of convergence is the same as the nonparametric estimation of a function on Sd-1, up to a logarithmic factor. In addition, we provide an efficient spectral algorithm to compute this estimator without any knowledge on the nonparametric link function. As a byproduct, our method can also consistently estimate the dimension d of the latent space.	[Araya, Ernesto] Univ Paris Sud, Lab Math Orsay LMO, Paris, France; [De Castro, Yohann] Ecole Cent Lyon, Inst Camille Jordan, F-69134 Ecully, France	UDICE-French Research Universities; Universite Paris Saclay; UDICE-French Research Universities; Universite Claude Bernard Lyon 1; Centre National de la Recherche Scientifique (CNRS); Ecole Centrale de Lyon; Institut National des Sciences Appliquees de Lyon - INSA Lyon; Universite Jean Monnet	Araya, E (corresponding author), Univ Paris Sud, Lab Math Orsay LMO, Paris, France.	ernesto.araya-valdivia@u-psud.fr; yohann.de-castro@ec-lyon.fr						Arias-Castro E., 2018, ARXIV180410611; Bandeira AS, 2016, ANN PROBAB, V44, P2479, DOI 10.1214/15-AOP1025; Borgs C, 2012, ANN MATH, V176, P151, DOI 10.4007/annals.2012.176.1.2; Bubeck S, 2016, RANDOM STRUCT ALGOR, V49, P503, DOI 10.1002/rsa.20633; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; Dai F., 2013, SPRINGER VERLAG MONO; De Castro Y., ARXIVORGPDF170802107; Devroye L, 2011, ELECTRON J PROBAB, V16, P2481, DOI 10.1214/EJP.v16-967; Diaz J., 2018, ARXIV180410611; Emery M., 1998, LECT PROBABILITY THE; ERDOS P, 1960, B INT STATIST INST, V38, P343; GILBERT EN, 1961, J SOC IND APPL MATH, V9, P533, DOI 10.1137/0109045; Gupta R, 2016, SIAM J COMPUT, V45, P197, DOI 10.1137/140955331; Higham DJ, 2008, BIOINFORMATICS, V24, P1093, DOI 10.1093/bioinformatics/btn079; Hirsch F., 2012, ELEMENTS FUNCTIONAL, V192; Hoff PD, 2002, J AM STAT ASSOC, V97, P1090, DOI 10.1198/016214502388618906; Jia XD, 2004, I-SPAN 2004: 7TH INTERNATIONAL SYMPOSIUM ON PARALLEL ARCHITECTURES, ALGORITHMS AND NETWORKS, PROCEEDINGS, P575; Karger D, 1998, J ACM, V45, P246, DOI 10.1145/274787.274791; Koltchinskii VI., 1998, HIGH DIMENSIONAL PRO, P191; Levin K, 2017, IEEE T SIGNAL PROCES, V65, P1988, DOI 10.1109/TSP.2016.2645517; Lovasz L., 2012, LARGE NETWORKS GRAPH; Lovasz L., 2006, J COMB THEORY B, V96, P197; Nicaise S, 2000, MATH NACHR, V213, P117, DOI 10.1002/(SICI)1522-2616(200005)213:1<117::AID-MANA117>3.0.CO;2-A; Penrose M.D., 2003, RANDOM GEOMETRIC GRA; Sussman DL, 2014, IEEE T PATTERN ANAL, V36, P48, DOI 10.1109/TPAMI.2013.135; Tang M, 2013, ANN STAT, V41, P1406, DOI 10.1214/13-AOS1112; Walters M., 2011, SURVEYS COMBINATORIC, V392, P365	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900033
C	Arbel, M; Korba, A; Salim, A; Gretton, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arbel, Michael; Korba, Anna; Salim, Adil; Gretton, Arthur			Maximum Mean Discrepancy Gradient Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				METRICS; SPACE	We construct a Wasserstein gradient flow of the maximum mean discrepancy (MMD) and study its convergence properties. The MMD is an integral probability metric defined for a reproducing kernel Hilbert space (RKHS), and serves as a metric on probability measures for a sufficiently rich RKHS. We obtain conditions for convergence of the gradient flow towards a global optimum, that can be related to particle transport when optimizing neural networks. We also propose a way to regularize this MMD flow, based on an injection of noise in the gradient. This algorithmic fix comes with theoretical and empirical evidence. The practical implementation of the flow is straightforward, since both the MMD and its gradient have simple closed-form expressions, which can be easily estimated with samples.	[Arbel, Michael; Korba, Anna; Gretton, Arthur] UCL, Gatsby Computat Neurosci Unit, London, England; [Salim, Adil] KAUST, Visual Comp Ctr, Thuwal, Saudi Arabia	University of London; University College London; King Abdullah University of Science & Technology	Arbel, M (corresponding author), UCL, Gatsby Computat Neurosci Unit, London, England.	michael.n.arbel@gmail.com; a.korba@ucl.ac.uk; adil.salim@kaust.edu.sa; arthur.gretton@gmail.com						Ambrosio L., 2008, LECT MATH ETH ZURICH, VSecond; [Anonymous], 2008, INFORM SCI STAT; Arbel M., 2018, NIPS; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Bellemare MG, 2017, ARXIV; Blanchet A, 2018, J FUNCT ANAL, V275, P1650, DOI 10.1016/j.jfa.2018.06.014; Bottou L, 2018, LECT NOTES ARTIF INT, V11100, P229, DOI 10.1007/978-3-319-99492-5_11; Carrillo JA, 2006, ARCH RATION MECH AN, V179, P217, DOI 10.1007/s00205-005-0386-1; Carrillo JA, 2019, CALC VAR PARTIAL DIF, V58, DOI 10.1007/s00526-019-1486-3; Chaudhari P., 2017, ARXIV170404932CSMATH; Chizat L., 2018, ARXIV181207956CSMATH; Chizat L., 2018, NIPS; Craig K, 2016, MATH COMPUT, V85, P1681, DOI 10.1090/mcom3033; Dalalyan A., 2019, STOCHASTIC PROCESSES; Dougal J., 2018, ICLR; Durmus  Alain, 2018, ARXIV180209188; Dziugaite G. K., 2015, UAI; Genevay A., 2018, AISTATS; Goodfellow I.J., 2014, PROC NEURAL INFORM P, DOI DOI 10.1002/RCS.128; Gretton A., 2012, J MACHINE LEARNING R; Gulcehre C, 2016, ARXIV160804980; Gulcehre C., 2016, ICML; Gulrajani Ishaan, 2017, NIPS; Hazan E., 2016, ICML; Jordan R, 1998, SIAM J MATH ANAL, V29, P1, DOI 10.1137/S0036141096303359; Jourdain B., 2007, ARXIV07072723; Kac M., 1956, P 3 BERK S MATH STAT, VVolume 3, P171; Li C.-L., 2017, ARXIV170508584CSSTAT; Li Y., 2015, ARXIV150202761; Liu Q., 2017, ADV NEURAL INFORM PR, P3115; MCKEAN HP, 1966, P NATL ACAD SCI USA, V56, P1907, DOI 10.1073/pnas.56.6.1907; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Mroueh Youssef, 2019, AISTATS; Oguntuase J. A., 2001, J INEQUALITIES PURE; Otto F, 2000, J FUNCT ANAL, V173, P361, DOI 10.1006/jfan.1999.3557; Peyre R, 2018, ESAIM CONTR OPTIM CA, V24, P1489, DOI 10.1051/cocv/2017050; Rotskoff G., 2019, ICML; Rotskoff Grant M, 2018, STAT-US; Santambrogio F, 2015, PROG NONLINEAR DIFFE, V87, P59, DOI 10.1007/978-3-319-20828-2_2; Simsekli U., 2019, ICML; Sirignano J, 2018, ARXIV180809372; Smola A.J., 1998, LEARNING KERNELS, V4; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5	45	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306048
C	Arora, R; Upadhyay, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arora, Raman; Upadhyay, Jalaj			On Differentially Private Graph Sparsification and Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMPROVED APPROXIMATION ALGORITHMS; CUT	In this paper, we study private sparsification of graphs. In particular, we give an algorithm that given an input graph, returns a sparse graph which approximates the spectrum of the input graph while ensuring differential privacy. This allows one to solve many graph problems privately yet efficiently and accurately. This is exemplified with application of the proposed meta-algorithm to graph algorithms for privately answering cut-queries, as well as practical algorithms for computing MAX-CUT and SPARSEST-CUT with better accuracy than previously known. We also give an efficient private algorithm to learn Laplacian eigenmap on a graph.	[Arora, Raman] Johns Hopkins Univ, Baltimore, MD 21218 USA; [Upadhyay, Jalaj] Rutgers State Univ, New Brunswick, NJ USA	Johns Hopkins University; Rutgers State University New Brunswick	Arora, R (corresponding author), Johns Hopkins Univ, Baltimore, MD 21218 USA.	arora@cs.jhu.edu; jalaj.kumar.upadhyay@gmail.com			NSF BIGDATA [IIS-1546482, IIS-1838139]; DARPA [W911NF1820267]	NSF BIGDATA; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This research was supported, in part, by NSF BIGDATA grants IIS-1546482 and IIS-1838139, and DARPA award W911NF1820267. This work was done when Jalaj Upadhyay was working as a postdoctoral researcher with Raman Arora at the Johns Hopkins University. Authors would like to thank Adam Smith, Lorenzo Orecchia, Cynthia Steinhardt, and Sarvagya Upadhyay for insightful discussions during the early stages of the project.	Achlioptas D, 2007, J ACM, V54, DOI 10.1145/1219092.1219097; Allen-Zhu Z, 2015, ACM S THEORY COMPUT, P237, DOI 10.1145/2746539.2746610; Alon N, 2006, INVENT MATH, V163, P499, DOI 10.1007/s00222-005-0465-9; Arora Raman, 2018, ADV NEURAL INFORM PR, P4137; Arora S, 2009, J ACM, V56, DOI 10.1145/1502793.1502794; Bach Francis R., 2017, J MACHINE LEARNING R, V18, P714; Batson J, 2012, SIAM J COMPUT, V41, P1704, DOI 10.1137/090772873; Bencztir Andr6s A, 1996, STOC, P47; Blocki J., 2013, P 4 C INNOVATIONS TH, P87; Blocki J, 2012, ANN IEEE SYMP FOUND, P410, DOI 10.1109/FOCS.2012.67; Chen S., 2013, P ACM SIGMOD INT C M, P653; Cheng D., 2015, JMLR WORKSHOP C P, P364; Cohen MB, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P181, DOI 10.1145/2688073.2688113; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Costello JC, 2009, GENOME BIOL, V10, DOI 10.1186/gb-2009-10-9-r97; Drineas P., 2010, ARXIV10053097; Dwork C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P11, DOI 10.1145/2591796.2591883; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; FIEDLER M, 1973, CZECH MATH J, V23, P298; Goemans MX, 1995, J ACM, V42, P1115, DOI 10.1145/227683.227684; Gupta A, 2012, LECT NOTES COMPUT SC, V7194, P339, DOI 10.1007/978-3-642-28914-9_19; Gupta A, 2010, PROC APPL MATH, V135, P1106; Har-Peled Sariel, 2005, APPROXIMATE MAX CUT; Hardt M, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1255; Karwa V, 2011, PROC VLDB ENDOW, V4, P1146; Kasiviswanathan SP, 2013, LECT NOTES COMPUT SC, V7785, P457, DOI 10.1007/978-3-642-36594-2_26; Kelner J. A., 2014, P 25 ANN ACM SIAM S, P217; Khot S, 2007, SIAM J COMPUT, V37, P319, DOI 10.1137/S0097539705447372; Khot Subhash, 2002, P 34 ANN ACM S THEOR, P767, DOI [DOI 10.1145/509907.510017, 10.1145/509907.510017]; Kyng K., 2015, PROC MACH LEARN RES, V40, P1190; Lee YT, 2017, ACM S THEORY COMPUT, P678, DOI 10.1145/3055399.3055477; Lee YT, 2014, ANN IEEE SYMP FOUND, P424, DOI 10.1109/FOCS.2014.52; Lee Yin Tat, 2015, ARXIV150608204; LEE YT, 2015, FDN COMP SCI FOCS 20, P250, DOI DOI 10.1109/FOCS.2015.24; Marcus Adam, 2013, ARXIV13063969; MEYER CD, 1973, SIAM J APPL MATH, V24, P315; Mir D., 2012, P JOINT EDBT ICDT WO, P167; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Ogata H, 1999, NUCLEIC ACIDS RES, V27, P29, DOI 10.1093/nar/27.1.29; Orecchia L, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P1141; Peng R., 2016, P 27 ANN ACM SIAM S, P1862; PROVAN JS, 1983, SIAM J COMPUT, V12, P777, DOI 10.1137/0212053; Raskhodnikova S, 2016, ANN IEEE SYMP FOUND, P495, DOI 10.1109/FOCS.2016.60; Rastogi V, 2009, PODS'09: PROCEEDINGS OF THE TWENTY-EIGHTH ACM SIGMOD-SIGACT-SIGART SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P107, DOI 10.1145/1559795.1559812; Rother C, 2004, ACM T GRAPHIC, V23, P309, DOI 10.1145/1015706.1015720; Rudi A, 2017, ADV NEUR IN, V30; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Sherman J, 2013, ANN IEEE SYMP FOUND, P263, DOI 10.1109/FOCS.2013.36; Spielman DA, 2012, ISR J MATH, V190, P83, DOI 10.1007/s11856-011-0194-2; Spielman DA, 2011, SIAM J COMPUT, V40, P1913, DOI 10.1137/080734029; Spielman DA, 2011, SIAM J COMPUT, V40, P981, DOI 10.1137/08074489X; Tropp JA, 2012, FOUND COMPUT MATH, V12, P389, DOI 10.1007/s10208-011-9099-z; Upadhyay J, 2013, LECT NOTES COMPUT SC, V8269, P276, DOI 10.1007/978-3-642-42033-7_15; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	55	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905009
C	Arora, S; Cohen, N; Hu, W; Luo, YP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Arora, Sanjeev; Cohen, Nadav; Hu, Wei; Luo, Yuping			Implicit Regularization in Deep Matrix Factorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Efforts to understand the generalization mystery in deep learning have led to the belief that gradient-based optimization induces a form of implicit regularization, a bias towards models of low "complexity." We study the implicit regularization of gradient descent over deep linear neural networks for matrix completion and sensing, a model referred to as deep matrix factorization. Our first finding, supported by theory and experiments, is that adding depth to a matrix factorization enhances an implicit tendency towards low-rank solutions, oftentimes leading to more accurate recovery. Secondly, we present theoretical and empirical arguments questioning a nascent view by which implicit regularization in matrix factorization can be captured using simple mathematical norms. Our results point to the possibility that the language of standard regularizers may not be rich enough to fully encompass the implicit regularization brought forth by gradient-based optimization.	[Arora, Sanjeev; Hu, Wei; Luo, Yuping] Princeton Univ, Princeton, NJ 08544 USA; [Arora, Sanjeev] Inst Adv Study, Princeton, NJ 08540 USA; [Cohen, Nadav] Tel Aviv Univ, Tel Aviv, Israel	Princeton University; Institute for Advanced Study - USA; Tel Aviv University	Arora, S (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.; Arora, S (corresponding author), Inst Adv Study, Princeton, NJ 08540 USA.	arora@cs.princeton.edu; cohennadav@cs.tau.ac.il; huwei@cs.princeton.edu; yupingl@cs.princeton.edu	Hu, Wei/AHE-7065-2022		NSF; ONR; Simons Foundation; Schmidt Foundation; Mozilla Research; Amazon Research; DARPA; SRC; Zuckerman Israeli Postdoctoral Scholars Program	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Simons Foundation; Schmidt Foundation; Mozilla Research; Amazon Research; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); SRC; Zuckerman Israeli Postdoctoral Scholars Program	This work was supported by NSF, ONR, Simons Foundation, Schmidt Foundation, Mozilla Research, Amazon Research, DARPA and SRC. Nadav Cohen was a member at the Institute for Advanced Study, and was additionally supported by the Zuckerman Israeli Postdoctoral Scholars Program. The authors thank Nathan Srebro for illuminating discussions which helped improve the paper.	Advani M. S., 2017, ARXIV171003667; Agrawal A., 2018, J CONTROL DECIS, V5, P42, DOI [10.1080/23307706.2017.1397554, DOI 10.1080/23307706.2017.1397554]; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2016, ADV NEURAL INFORM PR; [Anonymous], 2014, INT C LEARN REPR; Arora S., 2019, INT C LEARN REPR; Arora S, 2018, PR MACH LEARN RES, V80; Bartlett P., 2018, INT C MACH LEARN, P520; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; BUNSEGERSTNER A, 1991, NUMER MATH, V60, P1, DOI 10.1007/BF01385712; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chi Y., 2018, ARXIV180909573; Davenport MA, 2016, IEEE J-STSP, V10, P608, DOI 10.1109/JSTSP.2016.2539100; De Moor B, 1989, 281989 KATH U; Diamond S, 2016, J MACH LEARN RES, V17; Du Simon S, 2019, ARXIV190108572; Du Simon S, 2018, ARXIV180600900; Fan JC, 2018, NEURAL NETWORKS, V98, P34, DOI 10.1016/j.neunet.2017.10.007; Ge R, 2017, PR MACH LEARN RES, V70; Gidel Gauthier, 2019, ARXIV190413262; Gunasekar S., 2017, ADV NEURAL INFORM PR, P6151; Gunasekar S, 2018, PR MACH LEARN RES, V80; Gunasekar Suriya, 2018, ADV NEURAL INFORM PR, P9461; Hardt Moritz, 2016, INT C LEARN REPR; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; Hochreiter S, 1997, NEURAL COMPUT, V9, P1, DOI 10.1162/neco.1997.9.1.1; Ilyashenko Yulij, 2008, LECT ANAL DIFFERENTI, V86; Ji Z., 2019, INT C LEARN REPR ICL; Keskar Nitish Shirish, 2017, INT C LEARN REPR, DOI [10.48550/arxiv.1609.04836, DOI 10.48550/ARXIV.1609.04836]; Krantz S., 2002, PRIMER REAL ANAL FUN, DOI DOI 10.1007/978-0-8176-8134-0; Lampinen AK., 2019, INT C LEARN REPR ICL; Li Y., 2018, C LEARN THEOR, V75, P2; Lin JH, 2016, PR MACH LEARN RES, V48; Ma C, 2018, INT SYM QUAL ELECT, P335; Nacson M. S., 2019, P MACHINE LEARNING R, V89, P3420; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Neyshabur Behnam, 2014, ARXIV14126614; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Rahaman N, 2018, ARXIV180608734; Recht B, 2010, SIAM REV, V52, P471, DOI 10.1137/070697835; Roy Olivier, 2007, 2007 15th European Signal Processing Conference (EUSIPCO), P606; Soudry D, 2018, J MACH LEARN RES, V19; Townsend J., 2016, TECHNICAL REPORT; Trigeorgis G, 2017, IEEE T PATTERN ANAL, V39, P417, DOI 10.1109/TPAMI.2016.2554555; Wang Q, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1155, DOI 10.1145/3097983.3098164; Xue HJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3203; You Y., 2017, LARGE BATCH TRAINING; Z Li, 2015, NAT COMMUN, V6, P8248, DOI DOI 10.1038/NC0MMS9850; Zeng HQ, 2017, PROC INT CONF RECON; Zhao HD, 2017, AAAI CONF ARTIF INTE, P2921	53	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307043
C	Assadi, S; Balkanski, E; Leme, RP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Assadi, Sepehr; Balkanski, Eric; Leme, Renato Paes			Secretary Ranking with Minimal Inversions	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a secretary problem which captures the task of ranking in online settings. We term this problem the secretary ranking problem: elements from an ordered set arrive in random order and instead of picking the maximum element, the algorithm is asked to assign a rank, or position, to each of the elements. The rank assigned is irrevocable and is given knowing only the pairwise comparisons with elements previously arrived. The goal is to minimize the distance of the rank produced to the true rank of the elements measured by the Kendall-Tau distance, which corresponds to the number of pairs that are inverted with respect to the true order. Our main result is a matching upper and lower bound for the secretary ranking problem. We present an algorithm that ranks n elements with only O(n(3/2)) inversions in expectation, and show that any algorithm necessarily suffers Omega(n(3/2)) inversions when there are n available positions. In terms of techniques, the analysis of our algorithm draws connections to linear probing in the hashing literature, while our lower bound result relies on a general anti -concentration bound for a generic balls and bins sampling process. We also consider the case where the number of positions m can be larger than the number of secretaries n and provide an improved bound by showing a connection of this problem with random binary trees.	[Assadi, Sepehr] Rutgers State Univ, New Brunswick, NJ 08901 USA; [Balkanski, Eric] Harvard Univ, Cambridge, MA 02138 USA; [Leme, Renato Paes] Google Res, Mountain View, CA USA	Rutgers State University New Brunswick; Harvard University; Google Incorporated	Assadi, S (corresponding author), Rutgers State Univ, New Brunswick, NJ 08901 USA.	sepehr.assadi@rutgers.edu; ebalkans@gmail.com; renatoppl@google.com						Agarwal A., 2017, COLT, V65, P39; Agrawal S, 2014, OPER RES, V62, P876, DOI 10.1287/opre.2014.1289; [Anonymous], 2013, P 22 INT C WORLD WID; Azar P.D., 2014, P 25 ANN ACM SIAM S, P1358; Babaioff M, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P434; Babaioff M, 2007, LECT NOTES COMPUT SC, V4627, P16; Babichenko Yakov, 2017, P 18 C EC COMP EC, P243; Bateni M, 2010, LECT NOTES COMPUT SC, V6302, P39, DOI 10.1007/978-3-642-15369-3_4; Braverman M, 2016, ACM S THEORY COMPUT, P851, DOI 10.1145/2897518.2897642; Burges C. J., 2007, ADV NEURAL INFORM PR, V19, P193, DOI DOI 10.1007/S10994-010-5185-8; Busa-Fekete R., 2013, ICML; Cao H, 2007, PROCEEDINGS OF THE 26TH CHINESE CONTROL CONFERENCE, VOL 4, P129; Chen YX, 2015, PR MACH LEARN RES, V37, P371; CHOW YS, 1964, ISRAEL J MATH, V2, P81, DOI 10.1007/BF02759948; Chris Burges T.S., 2005, P 22 INT MACH LEARN, DOI 10.1145/1102351.1102363; Cohen Ilan R., 2014, P 26 ACM SIAM S DISC, P73; Davidson S, 2014, ACM T DATABASE SYST, V39, DOI 10.1145/2684066; DIACONIS P, 1977, J ROY STAT SOC B MET, V39, P262, DOI 10.1111/j.2517-6161.1977.tb01624.x; Diaconis P., 1988, LECT NOTES MONOGRAPH, V11; DYNKIN E. B., 1963, SOV MATH DOKL, V4, P627; Esfandiari H, 2017, SIAM J DISCRETE MATH, V31, P1685, DOI 10.1137/15M1029394; FEIGE U, 1994, SIAM J COMPUT, V23, P1001, DOI 10.1137/S0097539791195877; Feldman J, 2010, LECT NOTES COMPUT SC, V6346, P182, DOI 10.1007/978-3-642-15775-2_16; Ferguson TS., 1989, STAT SCI, V4, P282, DOI DOI 10.1214/SS/1177012493; FREEMAN PR, 1983, INT STAT REV, V51, P189, DOI 10.2307/1402748; Gautham BP, 2011, PROCEEDINGS OF THE 1ST WORLD CONGRESS ON INTEGRATED COMPUTATIONAL MATERIALS ENGINEERING (ICME), P35; Girdhar Y, 2009, 2009 CANADIAN CONFERENCE ON COMPUTER AND ROBOT VISION, P292, DOI 10.1109/CRV.2009.30; Gobel O, 2015, LECT NOTES COMPUT SC, V9294, P680, DOI 10.1007/978-3-662-48350-3_57; Heckel R., 2016, ARXIV160608842; Jang M., 2016, ARXIV160304153; Jun Xu, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P391; Kesselheim T, 2015, ACM S THEORY COMPUT, P879, DOI 10.1145/2746539.2746602; Kleinberg R, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P630; Knuth D., 1963, NOTES OPEN ADDRESSIN; Korula N, 2009, LECT NOTES COMPUT SC, V5556, P508, DOI 10.1007/978-3-642-02930-1_42; LINDLEY DV, 1961, ROY STAT SOC C-APP, V10, P39; Liu N. N., 2008, P 31 ANN INT ACM SIG, P83, DOI DOI 10.1145/1390334.1390351; Liu T.Y., 2007, P SIGIR 2007 WORKSH, V310; Mosteller, 2006, SELECTED PAPERS F MO, P355; Pagh A, 2007, ACM S THEORY COMPUT, P318, DOI 10.1145/1250790.1250839; Patrascu M, 2010, LECT NOTES COMPUT SC, V6198, P715; Radlinski F., 2005, QUERY CHAINS LEARNIN, P239, DOI DOI 10.1145/1081870.1081899; Rubinstein A, 2016, ACM S THEORY COMPUT, P324, DOI 10.1145/2897518.2897540; Sakaguchi Minoru, 1995, MATH JAPONICAE, V42, P343; SCHMIDT JP, 1990, PROCEEDINGS OF THE TWENTY SECOND ANNUAL ACM SYMPOSIUM ON THEORY OF COMPUTING, P224, DOI 10.1145/100216.100245; Shi Y., 2010, P 4 ACM C RECOMMENDE, P269; Shi Y, 2012, 2012 10TH INTERNATIONAL SYMPOSIUM ON ANTENNAS, PROPAGATION & EM THEORY (ISAPE), P137, DOI 10.1109/ISAPE.2012.6408727; SINGH JA, 2017, J MACHINE LEARNING R, V18; Tie-Yan Liu, 2009, Foundations and Trends in Information Retrieval, V3, P225, DOI 10.1561/1500000016; Wang J, 2008, INFORM RETRIEVAL, V11, P477, DOI 10.1007/s10791-008-9060-1; Yunbo Cao, 2006, Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P186; Zhang QL, 2008, CHEM COMMUN, P1199, DOI 10.1039/b716681h	52	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301009
C	Axelrod, B; Diakonikolas, I; Sidiropoulos, A; Stewart, A; Valiant, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Axelrod, Brian; Diakonikolas, Ilias; Sidiropoulos, Anastasios; Stewart, Alistair; Valiant, Gregory			A Polynomial Time Algorithm for Log-Concave Maximum Likelihood via Locally Exponential Families	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LIMIT DISTRIBUTION-THEORY; K-MONOTONE DENSITY; LOGCONCAVE FUNCTIONS; GLOBAL RATES; CONVERGENCE; CONSISTENCY; INFERENCE	We consider the problem of computing the maximum likelihood multivariate log-concave distribution for a set of points. Specifically, we present an algorithm which, given n points in R-d and an accuracy parameter epsilon > 0, runs in time poly(n, d, 1/epsilon), and returns a log-concave distribution which, with high probability, has the property that the likelihood of the n points under the returned distribution is at most an additive. less than the maximum likelihood that could be achieved via any log-concave distribution. This is the first computationally efficient (polynomial time) algorithm for this fundamental and practically important task. Our algorithm rests on a novel connection with exponential families: the maximum likelihood log-concave distribution belongs to a class of structured distributions which, while not an exponential family, "locally" possesses key properties of exponential families. This connection then allows the problem of computing the log-concave maximum likelihood distribution to be formulated as a convex optimization problem, and solved via an approximate first-order method. Efficiently approximating the (sub) gradients of the objective function is a main technical challenge in this work.	[Axelrod, Brian; Valiant, Gregory] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Diakonikolas, Ilias] Univ Wisconsin, Dept Comp Sci, Madison, WI USA; [Sidiropoulos, Anastasios] Univ Illinois, Dept Comp Sci, Chicago, IL USA; [Stewart, Alistair] Web3 Fdn, Zug, Switzerland	Stanford University; University of Wisconsin System; University of Wisconsin Madison; University of Illinois System; University of Illinois Chicago; University of Illinois Chicago Hospital	Axelrod, B (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	baxelrod@cs.stanford.edu; ilias.diakonikolas@gmail.com; sidiropo@gmail.com; stewart.al@gmail.com; gvaliant@stanford.edu			Sloan Research Fellowship; USC startup grant; NSF [CCF-1704417, CCF-1453472, CCF-1934915, CCF-1423230, CCF-1815145, DGE-1656518, CCF-1763299, CCF-1652862]; Finch family fellowship; ONR Young Investigator Award [N00014-18-1-2295]	Sloan Research Fellowship(Alfred P. Sloan Foundation); USC startup grant; NSF(National Science Foundation (NSF)); Finch family fellowship; ONR Young Investigator Award	Ilias Diakonikolas was supported by NSF Award CCF-1652862 (CAREER) and a Sloan Research Fellowship. Alistair Stewart was supported by a USC startup grant. Anastasios Sidiropoulos was supported by NSF awards CCF-1453472 (CAREER) and CCF-1934915 (TRIPODS), and NSF grants CCF-1423230 and CCF-1815145. Brian Axelrod was supported by NSF Fellowship grant DGE-1656518, NSF award CCF-1763299, and a Finch family fellowship. Brian Axelrod and Gregory Valiant were supported by NSF awards CCF-1704417, and an ONR Young Investigator Award (N00014-18-1-2295).	Acharya J, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1278; Acharya J, 2015, PODS'15: PROCEEDINGS OF THE 33RD ACM SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P249, DOI 10.1145/2745754.2745772; An M. Yu., 1995, 9503 DUK U DEP EC, DOI [10.2139/ssrn.1933, DOI 10.2139/SSRN.1933]; Axelrod B., 2018, ABS181103204 CORR; Bagnoli M, 2005, ECON THEOR, V26, P445, DOI 10.1007/s00199-004-0514-4; Balabdaoui F, 2007, ANN STAT, V35, P2536, DOI 10.1214/009053607000000262; Balabdaoui F, 2018, BERNOULLI, V24, P1053, DOI 10.3150/16-BEJ864; Balabdaoui F, 2010, STAT NEERL, V64, P45, DOI 10.1111/j.1467-9574.2009.00438.x; Balabdaoui F, 2009, ANN STAT, V37, P1299, DOI 10.1214/08-AOS609; Barlow R. E., 1972, STAT INFERENCE ORDER; BIRGE L, 1987, ANN STAT, V15, P1013, DOI 10.1214/aos/1176350489; BIRGE L, 1987, ANN STAT, V15, P995, DOI 10.1214/aos/1176350488; Boyd S, 2004, CONVEX OPTIMIZATION; BRUNK HD, 1958, ANN MATH STAT, V29, P437, DOI 10.1214/aoms/1177706621; Canonne C. L., 2016, STACS; Carpenter T., 2018, PROC 31 C LEARN THEO, V75, P1234; Chan KS, 2004, BIOMETRIKA, V91, P113, DOI 10.1093/biomet/91.1.113; Chan SO, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P604, DOI 10.1145/2591796.2591848; Chan SO, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1380; Chan Siu-on, 2014, ADV NEURAL INFORM PR, P1844; Chen YN, 2013, STAT SINICA, V23, P1373, DOI 10.5705/ss.2011.224; Cule M, 2010, J R STAT SOC B, V72, P545, DOI 10.1111/j.1467-9868.2010.00753.x; Dagan Y., 2019, ABS190305315 CORR; Daskalakis C., 2012, SODA, P1371; Daskalakis C., 2016, P 48 ANN ACM S THEOR; Daskalakis C, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P709; Daskalakis C, 2013, ANN IEEE SYMP FOUND, P217, DOI 10.1109/FOCS.2013.31; De A., 2018, ABS181103744 CORR; Diakonikolas I., 2016, P 29 C LEARN THEOR C, P850; Diakonikolas I., 2018, ABS181205524 CORR; Diakonikolas I., 2017, PROC 2017 C LEARN TH, V65, P711; Diakonikolas I., 2016, EFFICIENT ROBUST PRO; Diakonikolas I., 2016, P STOC 16; Diakonikolas I., 2016, P 29 C LEARN THEOR C, P831; Diakonikolas Ilias, 2018, P 31 C LEARNING THEO, P819; Doss CR, 2016, ANN STAT, V44, P954, DOI 10.1214/15-AOS1394; Duchi J. C., 2016, GRADUATE SUMMER SCH; Dumbgen L, 2011, J STAT SOFTW, V39, P1, DOI 10.18637/jss.v039.i06; Dumbgen L, 2009, BERNOULLI, V15, P40, DOI 10.3150/08-BEJ141; Fougeres AL, 1997, CAN J STAT, V25, P375, DOI 10.2307/3315785; Gao FC, 2009, SCI CHINA SER A, V52, P1525, DOI 10.1007/s11425-009-0102-y; Grenander U, 1956, SKAND AKTUARIETIDSK, V1956, P125; Groeneboom P., 1985, P BERK C HON JERZ NE, VII, P539; Groeneboom P., 2014, NONPARAMETRIC ESTIMA; Han QY, 2016, ANN STAT, V44, P1332, DOI 10.1214/15-AOS1408; HANSON DL, 1976, ANN STAT, V4, P1038, DOI 10.1214/aos/1176343640; Jankowski HK, 2009, ELECTRON J STAT, V3, P1567, DOI 10.1214/09-EJS526; Kannan R, 1997, RANDOM STRUCT ALGOR, V11, P1, DOI 10.1002/(SICI)1098-2418(199708)11:1<1::AID-RSA1>3.0.CO;2-X; Kim AKH, 2016, ANN STAT, V44, P2756, DOI 10.1214/16-AOS1480; Koenker R, 2010, ANN STAT, V38, P2998, DOI 10.1214/10-AOS814; Lovasz L, 2006, SIAM J COMPUT, V35, P985, DOI 10.1137/S009753970544727X; Lovasz L, 2006, J COMPUT SYST SCI, V72, P392, DOI 10.1016/j.jcss.2005.08.004; Lovasz L, 2006, ANN IEEE SYMP FOUND, P57; Lovasz L, 2007, RANDOM STRUCT ALGOR, V30, P307, DOI 10.1002/rsa.20135; RAO BLSP, 1969, SANKHYA SER A, V31, P23; Rathke F., 2018, ABS180507272 CORR; Robeva E., 2017, GEOMETRY LOG CONCAVE; Samworth R. J., 2017, RECENT PROGR LOG CON; STANLEY RP, 1989, ANN NY ACAD SCI, V576, P500, DOI 10.1111/j.1749-6632.1989.tb16434.x; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Walther G, 2009, STAT SCI, V24, P319, DOI 10.1214/09-STS303; WEGMAN EJ, 1970, ANN MATH STAT, V41, P457, DOI 10.1214/aoms/1177697085; WEGMAN EJ, 1970, ANN MATH STAT, V41, P2169, DOI 10.1214/aoms/1177696724	64	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307071
C	Aydore, S; Zhu, TH; Foster, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aydore, Sergul; Zhu, Tianhao; Foster, Dean			Dynamic Local Regret for Non-convex Online Forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider online forecasting problems for non-convex machine learning models. Forecasting introduces several challenges such as (i) frequent updates are necessary to deal with concept drift issues since the dynamics of the environment change over time, and (ii) the state of the art models are non-convex models. We address these challenges with a novel regret framework. Standard regret measures commonly do not consider both dynamic environment and non-convex models. We introduce a local regret for non-convex models in a dynamic environment. We present an update rule incurring a cost, according to our proposed local regret, which is sublinear in time T. Our update uses time-smoothed gradients. Using a real-world dataset we show that our time-smoothed approach yields several benefits when compared with state-of-the-art competitors: results are more stable against new data; training is more robust to hyperparameter selection; and our approach is more computationally efficient than the alternatives.	[Aydore, Sergul; Zhu, Tianhao] Stevens Inst Technol, Dept ECE, Hoboken, NJ 07030 USA; [Foster, Dean] Amazon, New York, NY USA	Stevens Institute of Technology; Amazon.com	Aydore, S (corresponding author), Stevens Inst Technol, Dept ECE, Hoboken, NJ 07030 USA.	sergulaydore@gmail.com; romeo.zhuth@gmail.com; foster@amazon.com			AWS Machine Learning Research Awards	AWS Machine Learning Research Awards	This project has been supported by AWS Machine Learning Research Awards.	Anava O., 2013, P 26 ANN C LEARNING, P172; [Anonymous], 2013, ICML 3; [Anonymous], 2018, ADV NEURAL INFORM PR; Barta Gergo, 2017, INT C INT DEC TECHN, P67; Daniely A, 2015, PR MACH LEARN RES, V37, P1405; Flunkert V., 2017, ARXIV170404110; Foster DP, 1998, BIOMETRIKA, V85, P379, DOI 10.1093/biomet/85.2.379; Gultekin S, 2019, IEEE T SIGNAL PROCES, V67, P1223, DOI 10.1109/TSP.2018.2889982; Hazan E., 2009, P 26 ANN INT C MACHI, P393; Hazan  E., 2017, ARXIV170800075; Hazan E., 2018, ADV NEURAL INFORM PR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hsu D, 2012, J COMPUT SYST SCI, V78, P1460, DOI 10.1016/j.jcss.2011.12.025; Koolen Wouter M, 2015, ADV NEURAL INFORM PR, P2557; Kuznetsov Vitaly, 2016, P MACHINE LEARNING R, P1190; Liu CH, 2016, AAAI CONF ARTIF INTE, P1867; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Toubeau JF, 2019, IEEE T POWER SYST, V34, P1203, DOI 10.1109/TPWRS.2018.2870041; Wang Guanghui, 2018, P 27 INT JOINT C ART, P2762; Zhang LJ, 2018, PR MACH LEARN RES, V80; Zinkevich Martin, 2003, P INT C MACH LEARN, P928; Zliobaite I, 2016, STUD BIG DATA, V16, P91, DOI 10.1007/978-3-319-26989-4_4	22	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308005
C	Bahroun, Y; Sengupta, AM; Chklovskii, DB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bahroun, Yanis; Sengupta, Anirvan M.; Chklovskii, Dmitri B.			A Similarity-preserving Neural Network Trained on Transformed Images Recapitulates Salient Features of the Fly Motion Detection Circuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIRECTIONAL SELECTIVITY; LIE-GROUPS; SPARSE; INTEGRATION; COMPONENTS; NEURONS	Learning to detect content-independent transformations from data is one of the central problems in biological and artificial intelligence. An example of such problem is unsupervised learning of a visual motion detector from pairs of consecutive video frames. Rao and Ruderman formulated this problem in terms of learning infinitesimal transformation operators (Lie group generators) via minimizing image reconstruction error. Unfortunately, it is difficult to map their model onto a biologically plausible neural network (NN) with local learning rules. Here we propose a biologically plausible model of motion detection. We also adopt the transformation-operator approach but, instead of reconstruction-error minimization, start with a similarity-preserving objective function. An online algorithm that optimizes such an objective function naturally maps onto an NN with biologically plausible learning rules. The trained NN recapitulates major features of the well-studied motion detector in the fly. In particular, it is consistent with the experimental observation that local motion detectors combine information from at least three adjacent pixels, something that contradicts the celebrated Hassenstein-Reichardt model.	[Bahroun, Yanis; Sengupta, Anirvan M.; Chklovskii, Dmitri B.] Flatiron Inst, New York, NY 10010 USA; [Sengupta, Anirvan M.] Rutgers State Univ, New Brunswick, NJ USA; [Chklovskii, Dmitri B.] NYU, Langone Med Ctr, New York, NY 10003 USA	Rutgers State University New Brunswick; New York University; NYU Langone Medical Center	Bahroun, Y (corresponding author), Flatiron Inst, New York, NY 10010 USA.	ybahroun@flatironinstitute.org; anirvans@physics.rutgers.edu; dchklovskii@flatironinstitute.org						Arenz A, 2017, CURR BIOL, V27, P929, DOI 10.1016/j.cub.2017.01.051; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Bethge Matthias, 2007, HUMAN VISION ELECT I, V6492; BUCHNER E, 1976, BIOL CYBERN, V24, P85, DOI 10.1007/BF00360648; Cai Q, 2009, J CHEM PHYS, V130, DOI 10.1063/1.3099708; Cox T., 2000, MULTIDIMENSIONAL SCA; Egelhaaf Martin, 1993, J NEUROSCI, V13; FERMI G, 1963, KYBERNETIK, V2, P15, DOI 10.1007/BF00292106; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Gotz KG, 1964, KYBERNETIK, V2, P77; Grimes DB, 2005, NEURAL COMPUT, V17, P47, DOI 10.1162/0899766052530893; Gruntman E, 2018, NAT NEUROSCI, V21, P250, DOI 10.1038/s41593-017-0046-4; Haag J, 2004, P NATL ACAD SCI USA, V101, P16333, DOI 10.1073/pnas.0407368101; Haag J, 2017, ELIFE, V6, DOI 10.7554/eLife.29044; Hausser M, 2003, CURR OPIN NEUROBIOL, V13, P372, DOI 10.1016/S0959-4388(03)00075-8; Hinton G.F., 1981, P 7 INT JOINT C ART, V2, P683; KOCH C, 1983, P NATL ACAD SCI-BIOL, V80, P2799, DOI 10.1073/pnas.80.9.2799; Koulakov AA, 2011, NEURON, V72, P124, DOI 10.1016/j.neuron.2011.07.031; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Mardia K.V, 1980, MULTIVARIATE ANAL; Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53; Memisevic Roland, 2011, ARXIV11100107; Miao X, 2007, NEURAL COMPUT, V19, P2665, DOI 10.1162/neco.2007.19.10.2665; OJA E, 1992, NEURAL NETWORKS, V5, P927, DOI 10.1016/S0893-6080(05)80089-9; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Olshausen BA, 2007, PROC SPIE, V6492, DOI 10.1117/12.715515; Pehlevan C., 2015, P 28 INT C NEUR INF; Pehlevan C, 2018, NEURAL COMPUT, V30, P84, DOI [10.1162/neco_a_01018, 10.1162/NECO_a_01018]; Pehlevan C, 2014, CONF REC ASILOMAR C, P769, DOI 10.1109/ACSSC.2014.7094553; Pehlevan C, 2015, NEURAL COMPUT, V27, P1461, DOI 10.1162/NECO_a_00745; Rao RPN, 1999, ADV NEUR IN, V11, P810; Reichardt W., 1961, SENSORY COMMUNICATIO, P303; Salazar-Gatzimas E, 2018, CURR BIOL, V28, P3748, DOI 10.1016/j.cub.2018.10.007; Sengupta A., 2018, ADV NEURAL INFORM PR, V31, P7080; Single S, 1998, SCIENCE, V281, P1848, DOI 10.1126/science.281.5384.1848; Sinha S. R., 2018, ARXIV181211878; Spruston N, 2008, NAT REV NEUROSCI, V9, P206, DOI 10.1038/nrn2286; Stuart G, 1997, TRENDS NEUROSCI, V20, P125, DOI 10.1016/S0166-2236(96)10075-8; Takemura S, 2017, ELIFE, V6, DOI 10.7554/eLife.26975; TORRE V, 1978, PROC R SOC SER B-BIO, V202, P409, DOI 10.1098/rspb.1978.0075; von der Malsburg C., 1994, MODELS NEURAL NETWOR, P95, DOI [10.1007/978-1-4612-4320-5_2, DOI 10.1007/978-1-4612-4320-5_2]; Wienecke CFR, 2018, NEURON, V99, P680, DOI 10.1016/j.neuron.2018.07.005; Williams CKI, 2001, ADV NEUR IN, V13, P675	44	0	0	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905081
C	Bai, XY; Guan, J; Wang, HN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bai, Xueying; Guan, Jian; Wang, Hongning			Model-Based Reinforcement Learning with Adversarial Training for Online Recommendation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reinforcement learning is well suited for optimizing policies of recommender systems. Current solutions mostly focus on model-free approaches, which require frequent interactions with the real environment, and thus are expensive in model learning. Offline evaluation methods, such as importance sampling, can alleviate such limitations, but usually request a large amount of logged data and do not work well when the action space is large. In this work, we propose a model-based reinforcement learning solution which models user-agent interaction for offline policy learning via a generative adversarial network. To reduce bias in the learned model and policy, we use a discriminator to evaluate the quality of generated data and scale the generated rewards. Our theoretical analysis and empirical evaluations demonstrate the effectiveness of our solution in learning policies from the offline and generated data.	[Bai, Xueying] SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA; [Guan, Jian] Tsinghua Univ, Dept Comp Sci & Technol, Beijing, Peoples R China; [Wang, Hongning] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Tsinghua University; University of Virginia	Bai, XY (corresponding author), SUNY Stony Brook, Dept Comp Sci, Stony Brook, NY 11794 USA.	xubai@cs.stonybrook.edu; j-guan19@mails.tsinghua.edu.cn; hwbx@virginia.edu		Wang, Hongning/0000-0002-6524-9195				Achiam J, 2017, PR MACH LEARN RES, V70; [Anonymous], 1998, REINFORCEMENT LEARNI; Chen M, 2019, PROCEEDINGS OF THE TWELFTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'19), P456, DOI 10.1145/3289600.3290999; Chen XS, 2019, PR MACH LEARN RES, V97; Chung J., 2014, ARXIV14123555; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth MP, 2011, LEARNING CONTROL LOW; Gilotte A, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P198, DOI 10.1145/3159652.3159687; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gu SX, 2016, PR MACH LEARN RES, V48; He XN, 2016, SIGIR'16: PROCEEDINGS OF THE 39TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P549, DOI 10.1145/2911451.2911489; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Liebman E, 2015, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS (AAMAS'15), P591; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Lu Zhongqi, 2016, ARXIV160807793; Meger D, 2015, IEEE INT CONF ROBOT, P2332, DOI 10.1109/ICRA.2015.7139509; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Morimoto J., 2003, ADV NEURAL INFORMATI, P1563; Oh J, 2017, ADV NEUR IN, V30; Peng Baolin, 2018, ARXIV180106176; Precup D., 2001, ICML, P417; Precup D., 2000, P 17 INT C MACH LEAR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Shani G, 2005, J MACH LEARN RES, V6, P1265; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Swaminathan A, 2015, ADV NEUR IN, V28; Swaminathan A, 2015, J MACH LEARN RES, V16, P1731; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wu QY, 2017, CIKM'17: PROCEEDINGS OF THE 2017 ACM CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P1927, DOI 10.1145/3132847.3133025; Yu LQ, 2017, AAAI CONF ARTIF INTE, P66; Zhao X., 2019, ARXIV190203987; Zheng GJ, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P167, DOI 10.1145/3178876.3185994	38	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902037
C	Bailey, JP; Piliouras, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bailey, James P.; Piliouras, Georgios			Fast and Furious Learning in Zero-Sum Games: Vanishing Regret with Non-Vanishing Step Sizes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We show for the first time that it is possible to reconcile in online learning in zero-sum games two seemingly contradictory objectives: vanishing time-average regret and non-vanishing step sizes. This phenomenon, that we coin "fast and furious" learning in games, sets a new benchmark about what is possible both in max-min optimization as well as in multi-agent systems. Our analysis does not depend on introducing a carefully tailored dynamic. Instead we focus on the most well studied online dynamic, gradient descent. Similarly, we focus on the simplest textbook class of games, two-agent two-strategy zero-sum games, such as Matching Pennies. Even for this simplest of benchmarks the best known bound for total regret, prior to our work, was the trivial one of O(T), which is immediately applicable even to a non-learning agent. Based on a tight understanding of the geometry of the non-equilibrating trajectories in the dual space we prove a regret bound of Theta(v root T) matching the well known optimal bound for adaptive step sizes in the online setting. This guarantee holds for all fixed step-sizes without having to know the time horizon in advance and adapt the fixed step-size accordingly.As a corollary, we establish that even with fixed learning rates the time-average of mixed strategies, utilities converge to their exact Nash equilibrium values. We also provide experimental evidence suggesting the stronger regret bound holds for all zero-sum games. [GRAPHICS] .	[Bailey, James P.] Texas A&M Univ, College Stn, TX 77843 USA; [Piliouras, Georgios] Singapore Univ Technol & Design, Singapore, Singapore	Texas A&M University System; Texas A&M University College Station; Singapore University of Technology & Design	Bailey, JP (corresponding author), Texas A&M Univ, College Stn, TX 77843 USA.	jamespbailey@tamu.edu; georgios@sutd.edu.sg		Bailey, James P.`/0000-0002-7207-512X	MOE AcRF Tier 2 Grant [2016-T2-1-170, PIE-SGP-AI-2018-01]; NRF 2018 Fellowship [NRF-NRFF2018-07]	MOE AcRF Tier 2 Grant; NRF 2018 Fellowship	James P. Bailey and Georgios Piliouras acknowledge MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and NRF 2018 Fellowship NRF-NRFF2018-07.	[Anonymous], 2019, ARXIV190108106; Bailey J.P., 2019, AAMAS; Bailey J. P., 2018, ACM C EC COMP; Bailey James P., 2019, FINITE REGRET CYCLES; Balduzzi D., 2018, NIPS; Balduzzi D., 2018, ICML; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING, DOI DOI 10.1017/CBO9780511546921; Cheung Yun Kuen, 2019, COLT; Chotibut Thiparat, 2018, ARXIV180706831; Chotibut Thiparat, 2019, ARXIV190602486; Daskalakis C., 2018, ICLR; Daskalakis C, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P235; Daskalakis Constantinos, 2019, LIPICS, V124, DOI [10.4230/LIPIcs.ITCS.2019.27, DOI 10.4230/LIPICS.ITCS.2019.27]; Foster D. J., 2016, ADV NEURAL INFORM PR, P4727; Fudenberg Drew, 1998, THEORY LEARNING GAME; Gidel G., 2019, INT C LEARNING REPRE; Kakade S. M., 2009, DUALITY STRONG CONVE; KLEINBERG R., 2011, S INN COMP SCI ICS; Mertikopoulos P, 2019, P 2019 INT C LEARN R; Mertikopoulos Panayotis, 2018, ACM SIAM S DISCR ALG; Nagarajan SG, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P685; Omidshafiei S., 2019, ARXIV190301373; PALAIOPANOS  G., 2017, ADV NEURAL INFORM PR, V30, P5872, DOI DOI 10.5555/3295222.3295337; Pangallo Marco, 2017, TAXONOMY LEARNING DY; Papadimitriou C, 2018, ENTROPY-SWITZ, V20, DOI 10.3390/e20100782; Piliouras G., 2018, ITCS; Piliouras G., 2014, P 25 ANN ACM SIAM S, P861; Piliouras G, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P181; Rakhlin S., 2013, ADV NEURAL INFORM PR, P3066; Syrgkanis Vasilis, 2015, ADV NEURAL INFORM PR, P2989; Yazici Yasin, 2019, ICLR; Young H.P., 2004, STRATEGIC LEARNING I	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904061
C	Balcan, MF; Dick, T; Noothigattu, R; Procaccia, AD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balcan, Maria-Florina; Dick, Travis; Noothigattu, Ritesh; Procaccia, Ariel D.			Envy-Free Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In classic fair division problems such as cake cutting and rent division, envy-freeness requires that each individual (weakly) prefer his allocation to anyone else's. On a conceptual level, we argue that envy-freeness also provides a compelling notion of fairness for classification tasks, especially when individuals have heterogeneous preferences. Our technical focus is the generalizability of envy-free classification, i.e., understanding whether a classifier that is envy free on a sample would be almost envy free with respect to the underlying distribution with high probability. Our main result establishes that a small sample is sufficient to achieve such guarantees, when the classifier in question is a mixture of deterministic classifiers that belong to a family of low Natarajan dimension.	[Balcan, Maria-Florina; Noothigattu, Ritesh] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Dick, Travis; Procaccia, Ariel D.] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Balcan, MF (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	ninamf@cs.cmu.edu; tdick@cs.cmu.edu; riteshn@cmu.edu; arielpro@cs.cmu.edu			National Science Foundation [IIS-1350598, IIS-1714140, IIS-1618714, IIS-1901403, CCF-1525932, CCF-1733556, CCF-1535967, CCF-1910321]; Office of Naval Research [N00014-16-1-3075, N00014-17-1-2428]; J.P. Morgan AI Research Award; Amazon Research Award; Microsoft Research Faculty Fellowship; Bloomberg Data Science research grant; Guggenheim Fellowship; Block Center for Technology and Society	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); J.P. Morgan AI Research Award; Amazon Research Award; Microsoft Research Faculty Fellowship(Microsoft); Bloomberg Data Science research grant; Guggenheim Fellowship; Block Center for Technology and Society	This work was partially supported by the National Science Foundation under grants IIS-1350598, IIS-1714140, IIS-1618714, IIS-1901403, CCF-1525932, CCF-1733556, CCF-1535967, CCF-1910321; by the Office of Naval Research under grants N00014-16-1-3075 and N00014-17-1-2428; and by a J.P. Morgan AI Research Award, an Amazon Research Award, a Microsoft Research Faculty Fellowship, a Bloomberg Data Science research grant, a Guggenheim Fellowship, and a grant from the Block Center for Technology and Society.	Agrawal A., 2018, J CONTROL DECIS, V5, P42, DOI [10.1080/23307706.2017.1397554, DOI 10.1080/23307706.2017.1397554]; Balcan Maria-Florina, 2012, P 25 COLT, P41; Brams Steven J., 1996, FAIR DIVISION CAKE C; Chajewska U, 2001, P 18 INT C MACH LEAR, P35; Chi-Chih Yao A., 1977, 18th Annual Symposium on Foundations of Computer Science, P222; Daniely A., 2012, NIPS, V25, P485; Datta Amit, 2015, Proceedings on Privacy Enhancing Technologies, V1, P92, DOI 10.1515/popets-2015-0007; Diamond S, 2016, J MACH LEARN RES, V17; Donini Michele, 2018, ARXIV180208626; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Foley D. K, 1967, YALE ECON ESSAYS, V7, P45; Freedman R, 2018, AAAI CONF ARTIF INTE, P1636; Gal Y, 2017, J ACM, V64, DOI 10.1145/3131361; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hebert-Johnson U., 2018, P MACHINE LEARNING R, P1939; Joseph Matthew, 2016, NIPS, P325; Kearns M., 2018, 35 INT C MACH LEARN, V6, P4008; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Luong Binh Thanh, 2011, P 17 ACM SIGKDD INT, P502; Manurangsi P, 2017, MATH SOC SCI, V89, P100, DOI 10.1016/j.mathsocsci.2017.05.006; Moulin HJ, 2003, FAIR DIVISION AND COLLECTIVE WELFARE, P1; Natarajan B. K., 1989, Machine Learning, V4, P67, DOI 10.1023/A:1022605311895; Nielsen TD, 2004, ARTIF INTELL, V160, P53, DOI 10.1016/j.artint.2004.08.003; Noothigattu R, 2018, AAAI CONF ARTIF INTE, P1587; Procaccia AD, 2013, COMMUN ACM, V56, P78, DOI 10.1145/2483852.2483870; Robertson Jack M., 1998, CAKE CUTTING ALGORIT; Rothblum Guy N, 2018, ARXIV180303242; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Su FE, 1999, AM MATH MON, V106, P930, DOI 10.2307/2589747; Sweeney L, 2013, COMMUN ACM, V56, P44, DOI 10.1145/2447976.2447990; VAPNIK VN, 1971, THEOR PROBAB APPL+, V16, P264, DOI 10.1137/1116025; VARIAN HR, 1974, J ECON THEORY, V9, P63, DOI 10.1016/0022-0531(74)90075-1; Woodworth B., 2017, ARXIV PREPRINT ARXIV; Zafar M. B., 2017, ADV NEURAL INFORM PR, P228; Zemel R., 2013, P INT C MACH LEARN, P325	35	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301026
C	Balestriero, R; Cosentino, R; Aazhang, B; Baraniuk, RG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balestriero, Randall; Cosentino, Romain; Aazhang, Behnaam; Baraniuk, Richard G.			The Geometry of Deep Networks: Power Diagram Subdivision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the geometry of deep (neural) networks (DNs) with piecewise affine and convex nonlinearities. The layers of such DNs have been shown to be max-affine spline operators (MASOs) that partition their input space and apply a regiondependent affine mapping to their input to produce their output. We demonstrate that each MASO layer's input space partition corresponds to a power diagram (an extension of the classical Voronoi tiling) with a number of regions that grows exponentially with respect to the number of units (neurons). We further show that a composition of MASO layers (e.g., the entire DN) produces a progressively subdivided power diagram and provide its analytical form. The subdivision process constrains the affine maps on the potentially exponentially many power diagram regions with respect to the number of neurons to greatly reduce their complexity. For classification problems, we obtain a formula for the DN's decision boundary in the input space plus a measure of its curvature that depends on the DN's architecture, nonlinearities, and weights. Numerous numerical experiments support and extend our theoretical results.	[Balestriero, Randall; Cosentino, Romain; Aazhang, Behnaam; Baraniuk, Richard G.] Rice Univ, Houston, TX 77005 USA	Rice University	Balestriero, R (corresponding author), Rice Univ, Houston, TX 77005 USA.		Baraniuk, Richard/ABA-1743-2020		NSF [SCH-1838873, CCF-1911094, IIS-1838177, IIS-1730574]; ONR [N00014-18-12571, N00014-17-1-2551]; AFOSR [FA9550-18-1-0478]; DARPA [G001534-7500]; Vannevar Bush Faculty Fellowship, ONR [N00014-18-1-2047]; NIH [R01HL144683-CFDA]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Vannevar Bush Faculty Fellowship, ONR; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	RB and RGB were supported by NSF grants CCF-1911094, IIS-1838177, and IIS-1730574; ONR grants N00014-18-12571 and N00014-17-1-2551; AFOSR grant FA9550-18-1-0478; DARPA grant G001534-7500; and a Vannevar Bush Faculty Fellowship, ONR grant N00014-18-1-2047. RC and BA were supported by NSF grant SCH-1838873 and NIH grant R01HL144683-CFDA.	Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348; Aurenhammer Franz, GEOMETRIAE DEDICATA; Balestriero R., 2018, INT C MACH LEARN, V80, P374; Baraniuk R, 2018, ARXIV180506576; Georgescu B, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P456; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hanin B., 2019, ARXIV190109021; Hannah LA, 2013, J MACH LEARN RES, V14, P3261; IMAI H, 1985, SIAM J COMPUT, V14, P93, DOI 10.1137/0214006; JOHNSON RA, 1960, ADV EUCLIDEAN GEOMET; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; KERN WF, 1938, SOLID MENSURATION PR; Magnani A, 2009, OPTIM ENG, V10, P1, DOI 10.1007/s11081-008-9045-3; Montufar G.F., 2014, ADV NEURAL INF PROCE, V27, P2924, DOI DOI 10.5555/2969033.2969153; Muja M, 2009, VISAPP 2009: PROCEEDINGS OF THE FOURTH INTERNATIONAL CONFERENCE ON COMPUTER VISION THEORY AND APPLICATIONS, VOL 1, P331; Pach J., 2011, COMBINATORIAL GEOMET, V37; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; Raghu M, 2017, PR MACH LEARN RES, V70; Wang Zichao, 2019, INT C LEARN REPR ICL; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Chiyuan, 2016, ARXIV161103530	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907048
C	Ban, F; Woodruff, D; Zhang, QY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ban, Frank; Woodruff, David; Zhang, Qiuyi (Richard)			Regularized Weighted Low Rank Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPLEXITY	The classical low rank approximation problem is to find a rank k matrix UV (where U has k columns and V has k rows) that minimizes the Frobenius norm of A - UV. Although this problem can be solved efficiently, we study an NP-hard variant of this problem that involves weights and regularization. A previous paper of [Razenshteyn et al. '16] derived a polynomial time algorithm for weighted low rank approximation with constant rank. We derive provably sharper guarantees for the regularized version by obtaining parameterized complexity bounds in terms of the statistical dimension rather than the rank, allowing for a rank-independent runtime that can be significantly faster. Our improvement comes from applying sharper matrix concentration bounds, using a novel conditioning technique, and proving structural theorems for regularized low rank problems.	[Ban, Frank; Zhang, Qiuyi (Richard)] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Ban, Frank; Zhang, Qiuyi (Richard)] Google, Mountain View, CA 94043 USA; [Woodruff, David] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	University of California System; University of California Berkeley; Google Incorporated; Carnegie Mellon University	Ban, F (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.; Ban, F (corresponding author), Google, Mountain View, CA 94043 USA.	fban@berkeley.edu; dwoodruff@cs.cmu.edu; qiuyi@berkeley.edu			Office of Naval Research (ONR) [N00014-18-1-2562]	Office of Naval Research (ONR)(Office of Naval Research)	Part of this work was done while D. Woodruff was visiting Google Mountain View as well as the Simons Institute for the Theory of Computing, and was supported in part by an Office of Naval Research (ONR) grant N00014-18-1-2562.	Avron H., 2017, APPROXIMATION RANDOM; Basu S, 1996, J ACM, V43, P1002, DOI 10.1145/235809.235813; Chen Z, 2008, ABS08100800 CORR; Cohen G, 2016, LEIBNIZ INT PR INFOR, V50, DOI 10.4230/LIPIcs.CCC.2016.8; Cohen MB, 2017, ACM S THEORY COMPUT, P410, DOI 10.1145/3055399.3055463; Das Saptarshi, 2011, PREPRINT; Davis TA, 2011, ACM T MATH SOFTWARE, V38, DOI [10.1145/2049662.2049670, 10.1145/2049662.2049663]; Gillis N, 2011, SIAM J MATRIX ANAL A, V32, P1149, DOI 10.1137/110820361; Li YZ, 2016, EUR SIGNAL PR CONF, P2235, DOI 10.1109/EUSIPCO.2016.7760646; Lu WS, 1997, IEEE T CIRCUITS-I, V44, P650, DOI 10.1109/81.596949; Lu WS, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL III, P694; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Razenshteyn I, 2016, ACM S THEORY COMPUT, P250, DOI 10.1145/2897518.2897639; RENEGAR J, 1992, J SYMB COMPUT, V13, P301, DOI 10.1016/S0747-7171(10)80005-7; Shpak D., 1990, IEEE 33 MIDW S CIRC; Srebro N., 2003, P 20 INT C MACHINE L, P720; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	19	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304010
C	Banerjee, A; Gu, QL; Sivakumar, V; Wu, ZS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Banerjee, Arindam; Gu, Qilong; Sivakumar, Vidyashankar; Wu, Zhiwei Steven			Random Quadratic Forms with Dependence: Applications to Restricted Isometry and Beyond	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Several important families of computational and statistical results in machine learning and randomized algorithms rely on uniform bounds on quadratic forms of random vectors or matrices. Such results include the Johnson-Lindenstrauss (J-L) Lemma, the Restricted Isometry Property (RIP), randomized sketching algorithms, and approximate linear algebra. The existing results critically depend on statistical independence, e.g., independent entries for random vectors, independent rows for random matrices, etc., which prevent their usage in dependent or adaptive modeling settings. In this paper, we show that such independence is in fact not needed for such results which continue to hold under fairly general dependence structures. In particular, we present uniform bounds on random quadratic forms of stochastic processes which are conditionally independent and sub-Gaussian given another (latent) process. Our setup allows general dependencies of the stochastic process on the history of the latent process and the latent process to be influenced by realizations of the stochastic process. The results are thus applicable to adaptive modeling settings and also allows for sequential design of random vectors and matrices. We also discuss stochastic process based forms of J-L, RIP, and sketching, to illustrate the generality of the results.(1)	[Banerjee, Arindam; Gu, Qilong; Sivakumar, Vidyashankar; Wu, Zhiwei Steven] Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Banerjee, A (corresponding author), Univ Minnesota Twin Cities, Dept Comp Sci & Engn, Minneapolis, MN 55455 USA.		Gao, Tiegang/AAT-9599-2021		NSF [OAC-1934634, IIS-1908104, IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986]; Google Faculty Research Award; J.P. Morgan Faculty Award; Mozilla research grant	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); J.P. Morgan Faculty Award; Mozilla research grant	The research was supported by NSF grants OAC-1934634, IIS-1908104, IIS-1563950, IIS-1447566, IIS-1447574, IIS-1422557, CCF-1451986, a Google Faculty Research Award, a J.P. Morgan Faculty Award, and a Mozilla research grant. Part of this work completed while ZSW was visiting the Simons Institute for the Theory of Computing at UC Berkeley.	Achlioptas D., 2001, J COMPUT SYST SCI, V66, P671; Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P557, DOI 10.1145/1132516.1132597; Arcones M.A., 1993, J THEORET PROBAB, V6, P101; Auer P., 2003, Journal of Machine Learning Research, V3, P397, DOI 10.1162/153244303321897663; Banerjee A., 2014, ADV NEURAL INFORM PR; Banerjee Arindam, 2019, ARXIV191004930; Barber D., 2012, BAYESIAN REASONING M; Boucheron S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Chen Sheng, 2015, ADV NEURAL INFORM PR, V28; Dasgupta Anirban, 2010, STOC; Dasgupta S., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P634, DOI 10.1109/SFFCS.1999.814639; dela Pea V.H., 1999, DECOUPLING PROBABILI; Dereziliski Michal, 2018, ADV NEURAL INFORM PR, V31, P2510; Derezinski ML, 2018, PR MACH LEARN RES, V84; Deshpande Y, 2018, PR MACH LEARN RES, V80; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Drineas P, 2006, SIAM J COMPUT, V36, P132, DOI 10.1137/S0097539704442684; Hanneke S, 2014, FOUND TRENDS MACH LE, V7, DOI 10.1561/2200000037; HANSON DL, 1971, ANN MATH STAT, V42, P1079, DOI 10.1214/aoms/1177693335; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Indyk Piotr, 1997, STOC; Johnson W. B., 1984, CONT MATH, V26, P189, DOI DOI 10.1090/CONM/026/737400; Kane DM, 2014, J ACM, V61, DOI 10.1145/2559902; Kannan Sampath, 2018, CORR; Krahmer F, 2014, COMMUN PUR APPL MATH, V67, P1877, DOI 10.1002/cpa.21504; Ledaux M., 1991, PROBABILITY BANACH S; Ledoux M., 2013, PROBABILITY BANACH S, P86; Lutkepohl H., 2005, NEW INTRO MULTIPLE T, DOI DOI 10.1007/978-3-540-27752-1; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mendelson S, 2018, ANN APPL PROBAB, V28, P3491, DOI 10.1214/18-AAP1391; Neel Seth, 2018, P 35 INT C MACH LEAR; Negahban SN, 2012, STAT SCI, V27, P538, DOI 10.1214/12-STS400; Nie XK, 2018, PR MACH LEARN RES, V84; Oymak S, 2018, IEEE T INFORM THEORY, V64, P4129, DOI 10.1109/TIT.2017.2773497; Rudelson M, 2013, ELECTRON COMMUN PROB, V18, P1, DOI 10.1214/ECP.v18-2865; Settles B., 2012, SYNTHESIS LECT ARTIF, V6, P1, DOI [10.2200/s00429ed1v01y201207aim018, DOI 10.2200/S00429ED1V01Y201207AIM018]; Sivakumar Vidyashankar, 2019, STRUCTURED LINEAR CO; Talagrand M., 2014, UPPER LOWER BOUNDS S; Talagrand M., 2005, SPRINGER MG MATH, P222, DOI 10.1007/3-540-27499-5; Vershynin R., 2014, ESTIMATION HIGH DIME, P3; Wainwright MJ, 2019, CA ST PR MA, P1, DOI 10.1017/9781108627771; Williams D., 1991, PROBABILITY MARTINGA; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	49	0	0	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904027
C	Bartal, Y; Fandina, N; Neiman, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bartal, Yair; Fandina, Nova; Neiman, Ofer			Dimensionality reduction: theoretical perspective on practical measures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM; SELECTION	Dimensionality reduction plays a central role in real world applications for Machine Learning, among many fields. In particular, metric dimensionalityreduction, where data from a general metric is mapped into low dimensional space, is often used as a first step before applying machine learning algorithms. In almost all these applications the quality of the embedding is measured by various average case criteria. Metric dimensionality reduction has also been studied in Math and TCS, within the extremely fruitful and influential field of metric embedding. Yet, the vast majority of theoretical research has been devoted to analyzing the worst case behavior of embeddings, and therefore has little relevance to practical settings. The goal of this paper is to bridge the gap between theory and practice view-points of metric dimensionalityreduction, laying the foundation for a theoretical study of more practically oriented analysis. This paper can be viewed as providing a comprehensive theoretical framework for analyzing different distortion measurement criteria, with the lens of practical applicability, and in particular for Machine Learning. The need for this line of research was recently raised by Chennuru Vankadara and von Luxburg in (13)[NeurIPS' 18], who emphasized the importance of pursuing it from both theoretical and practical perspectives. We consider some important and vastly used average case criteria, some of which originated within the well-known Multi-Dimensional Scaling framework. While often studied in practice, no theoretical studies have thus far attempted at providing rigorous analysis of these criteria. In this paper we provide the first analysis of these, as well as the new distortion measure developed in (13) designed to posses Machine Learning desired properties. Moreover, we show that all measures considered can be adapted to posses similar qualities. The main consequences of our work are nearly tight bounds on the absolute values of all distortion criteria, as well as first approximation algorithms with provable guarantees. All our theoretical results are backed by empirical experiments.	[Bartal, Yair; Fandina, Nova] Hebrew Univ Jerusalem, Dept Comp Sci, Jerusalem, Israel; [Neiman, Ofer] Ben Gurion Univ Negev, Dept Comp Sci, Beer Sheva, Israel	Hebrew University of Jerusalem; Ben Gurion University	Bartal, Y (corresponding author), Hebrew Univ Jerusalem, Dept Comp Sci, Jerusalem, Israel.	yair@cs.huji.ac.il; fandina@cs.huji.ac.il; neimano@cs.bgu.ac.il			ISF [1817/17]; BSF [2015813]	ISF(Israel Science Foundation); BSF(US-Israel Binational Science Foundation)	This work is supported by ISF grant #1817/17 and BSF grant #2015813.	Abraham I, 2011, ADV MATH, V228, P3026, DOI 10.1016/j.aim.2011.08.003; Achlioptas D, 2003, J COMPUT SYST SCI, V66, P671, DOI 10.1016/S0022-0000(03)00025-4; Ailon N, 2013, ACM T ALGORITHMS, V9, DOI 10.1145/2483699.2483701; Alon N, 2009, COMB PROBAB COMPUT, V18, P3, DOI 10.1017/S0963548307008917; Athitsos V, 2003, LECT NOTES ARTIF INT, V2915, P288; Badoiu M, 2003, SIAM PROC S, P434; Basalaj W., 2001, UCAMCLTR509; Borg I., 2005, SPRINGER SERIES STAT, DOI 10.1007/0-387-28981-X; Bronstein AM, 2006, P NATL ACAD SCI USA, V103, P1168, DOI 10.1073/pnas.0508601103; Cayton L., 2006, P 23 INT C MACH LEAR, P169; Censi A, 2013, IEEE T PATTERN ANAL, V35, P2357, DOI 10.1109/TPAMI.2013.34; Costa M, 2004, INT CON DISTR COMP S, P178, DOI 10.1109/ICDCS.2004.1281582; Cox R, 2004, ACM SIGCOMM COMP COM, V34, P113, DOI 10.1145/972374.972394; Cox TF, 2000, MONOGRAPHS STAT APPL; Dabek F, 2004, ACM SIGCOMM COMP COM, V34, P15, DOI 10.1145/1030194.1015471; Dasgupta A, 2010, ACM S THEORY COMPUT, P341; Dhamdhere K, 2004, LECT NOTES COMPUT SC, V3122, P96; GROENEN PJF, 1995, J CLASSIF, V12, P3, DOI 10.1007/BF01202265; Halperin E, 2003, BIOINFORMATICS, V19, pi122, DOI 10.1093/bioinformatics/btg1016; Hastad J, 2003, J ALGORITHM, V49, P42, DOI 10.1016/S0196-6774(03)00083-X; Heiser W. J., 1988, Classification and Related Methods of Data Analysis. Proceedings of the First Conference of the International Federation of Classification Societies (IFCS), P455; Hjaltason GR, 2003, IEEE T PATTERN ANAL, V25, P530, DOI 10.1109/TPAMI.2003.1195989; Hristescu G, 2000, LECT NOTES COMPUT SC, V1874, P358; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Indyk P, 2001, ANN IEEE SYMP FOUND, P10, DOI 10.1109/SFCS.2001.959878; Indyk Piotr, LOW DISTORTION EMBED; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Larsen K. G., 2016, ARXIV160902094; Lee S, 2007, LECT NOTES COMPUT SC, V4479, P890; Lee S, 2010, IEEE ACM T NETWORK, V18, P27, DOI 10.1109/TNET.2009.2023322; LINIAL N, 1995, COMBINATORICA, V15, P215, DOI 10.1007/BF01200757; Linial N., 2002, P ICM; LUA EK, 2005, P 5 ACM SIGCOMM C IN, P11; Lumezanu C, 2008, INT CON DISTR COMP S, P361, DOI 10.1109/ICDCS.2008.27; Mardia K. V., 1979, PROBABILITY MATH STA; MATOUSEK J, 1990, COMMENTATIONES MATH, V31, P589; Matousek J, 2008, ANN IEEE SYMP FOUND, P405, DOI 10.1109/FOCS.2008.21; Ng T. S. Eugene, 2002, P IEEE INFOCOM 2002; Quist M, 2004, J MACH LEARN RES, V5, P399; SAMMON JW, 1969, IEEE T COMPUT, VC 18, P401, DOI 10.1109/T-C.1969.222678; Sharma P, 2006, ACM SIGCOMM COMP COM, V36, P41, DOI 10.1145/1140086.1140092; Shavitt Y, 2004, IEEE ACM T NETWORK, V12, P993, DOI 10.1109/TNET.2004.838597; SPENCE I, 1989, PSYCHOMETRIKA, V54, P501, DOI 10.1007/BF02294632; Sundman D, 2011, EUR SIGNAL PR CONF, P368; Suri Sahaana, 2017, ABS170800183 CORR; Tang LY, 2004, LECT NOTES COMPUT SC, V3015, P63; Vankadara Leena Chennuru, 2018, ADV NEURAL INFORM PR, V31, P4891; Vera JF, 2007, J CLASSIF, V24, P277, DOI 10.1007/s00357-007-0020-1; Wang JTL, 2005, IEEE T SYST MAN CY B, V35, P973, DOI 10.1109/TSMCB.2005.848489; Zhang Rongmei, 2006, P 26 IEEE INT C DIST, P73, DOI 10.1109/ICDCS. 2006.7	52	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902023
C	Basu, S; Sen, R; Sanghavi, S; Shakkottai, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Basu, Soumya; Sen, Rajat; Sanghavi, Sujay; Shakkottai, Sanjay			Blocking Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider a novel stochastic multi-armed bandit setting, where playing an arm makes it unavailable for a fixed number of time slots thereafter. This models situations where reusing an arm too often is undesirable (e.g. making the same product recommendation repeatedly) or infeasible (e.g. compute job scheduling on machines). We show that with prior knowledge of the rewards and delays of all the arms, the problem of optimizing cumulative reward does not admit any pseudo-polynomial time algorithm (in the number of arms) unless randomized exponential time hypothesis is false, by mapping to the PINWHEEL scheduling problem. Subsequently, we show that a simple greedy algorithm that plays the available arm with the highest reward is asymptotically (1 - 1/e) optimal. When the rewards are unknown, we design a UCB based algorithm which is shown to have c log T + o(log T) cumulative regret against the greedy algorithm, leveraging the free exploration of arms due to the unavailability. Finally, when all the delays are equal the problem reduces to Combinatorial Semi-bandits providing us with a lower bound of c' log T + omega(log T).	[Basu, Soumya; Shakkottai, Sanjay] UT Austin, Austin, TX 78712 USA; [Sen, Rajat] Amazon, Seattle, WA USA; [Sanghavi, Sujay] UT Austin, Amazon, Austin, TX USA	University of Texas System; University of Texas Austin; Amazon.com; Amazon.com; University of Texas System; University of Texas Austin	Basu, S (corresponding author), UT Austin, Austin, TX 78712 USA.				NSF [1826320]; ARO [W911NF-17-1-0359]; Wireless Networking and Communications Group Industrial Affiliates Program; US DoT	NSF(National Science Foundation (NSF)); ARO; Wireless Networking and Communications Group Industrial Affiliates Program; US DoT	This research was partially supported by NSF Grant 1826320, ARO grant W911NF-17-1-0359, the Wireless Networking and Communications Group Industrial Affiliates Program, and the the US DoT supported D-STOP Tier 1 University Transportation Center.	ANANTHARAM V, 1987, IEEE T AUTOMAT CONTR, V32, P968, DOI 10.1109/TAC.1987.1104491; Bosman T, 2018, LECT NOTES COMPUT SC, V10807, P217, DOI 10.1007/978-3-319-77404-6_17; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Calabro C, 2008, J COMPUT SYST SCI, V74, P386, DOI 10.1016/j.jcss.2007.06.015; Cella L., 2019, STOCHASTIC BANDITS D; Cesa-Bianchi N., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P100; Cesa-Bianchi N, 2012, J COMPUT SYST SCI, V78, P1404, DOI 10.1016/j.jcss.2012.01.001; Chen S, 2013, IMBALANCED LEARNING: FOUNDATIONS, ALGORITHMS, AND APPLICATIONS, P151; Combes Richard, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P245, DOI 10.1145/2745844.2745847; Combes R., 2015, P 28 INT C NEUR INF, P2116; Fishburn PC, 2002, ALGORITHMICA, V34, P14, DOI 10.1007/s00453-002-0938-9; Gai Y, 2012, IEEE ACM T NETWORK, V20, P1466, DOI 10.1109/TNET.2011.2181864; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; Gopalan A., 2015, P 28 C LEARNING THEO, P861; Goulden IP, 2018, RAMANUJAN J, P1; Gyorgy A, 2007, J MACH LEARN RES, V8, P2369; Holte R., 1989, Proceedings of the Twenty-Second Annual Hawaii International Conference on System Sciences. Vol.II: Software Track (IEEE Cat. No.89TH0243-6), P693, DOI 10.1109/HICSS.1989.48075; Immorlica N, 2018, ANN IEEE SYMP FOUND, P309, DOI 10.1109/FOCS.2018.00037; Immorlica Nicole, 2019, IEEE 60 ANN S FDN CO; Jacobs T., 2014, ARXIV14107237; Kale S., 2016, ADV NEURAL INFORM PR, V29, P2181; Kleinberg R, 2010, MACH LEARN, V80, P245, DOI 10.1007/s10994-010-5178-7; Kveton B., 2014, ARXIV14100949; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Ortner P., 2007, ADV NEURAL INFORM PR, V19, P49; Sankararaman K. A., 2018, INT C ART INT STAT A, P1760; TEWARI A, 2008, ADV NEURAL INFORM PR, P1505; Zhou D. P., 2018, 32 AAAI C ART INT	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304075
C	Baydin, AG; Heinrich, L; Bhimji, W; Shao, L; Naderiparizi, S; Munk, A; Liu, JL; Gram-Hansen, B; Louppe, G; Meadows, L; Torr, P; Lee, V; Prabhat; Cranmer, K; Wood, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Baydin, Atilim Gunes; Heinrich, Lukas; Bhimji, Wahid; Shao, Lei; Naderiparizi, Saeid; Munk, Andreas; Liu, Jialin; Gram-Hansen, Bradley; Louppe, Gilles; Meadows, Lawrence; Torr, Philip; Lee, Victor; Prabhat; Cranmer, Kyle; Wood, Frank			Efficient Probabilistic Inference in the Quest for Physics Beyond the Standard Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SIMULATION; BOSON; MASS	We present a novel probabilistic programming framework that couples directly to existing large-scale simulators through a cross-platform probabilistic execution protocol, which allows general-purpose inference engines to record and control random number draws within simulators in a language-agnostic way. The execution of existing simulators as probabilistic programs enables highly interpretable posterior inference in the structured model defined by the simulator code base. We demonstrate the technique in particle physics, on a scientifically accurate simulation of the tau (tau) lepton decay, which is a key ingredient in establishing the properties of the Higgs boson. Inference efficiency is achieved via inference compilation where a deep recurrent neural network is trained to parameterize proposal distributions and control the stochastic simulator in a sequential importance sampling scheme, at a fraction of the computational cost of a Markov chain Monte Carlo baseline.	[Baydin, Atilim Gunes; Gram-Hansen, Bradley; Torr, Philip] Univ Oxford, Oxford, England; [Heinrich, Lukas] CERN, Meyrin, Switzerland; [Bhimji, Wahid; Liu, Jialin; Prabhat] Lawrence Berkeley Natl Lab, Berkeley, CA USA; [Shao, Lei; Meadows, Lawrence; Lee, Victor] Intel Corp, Santa Clara, CA USA; [Naderiparizi, Saeid; Munk, Andreas; Wood, Frank] Univ British Columbia, Vancouver, BC, Canada; [Louppe, Gilles] Univ Liege, Liege, Belgium; [Cranmer, Kyle] NYU, New York, NY 10003 USA	University of Oxford; European Organization for Nuclear Research (CERN); United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; Intel Corporation; University of British Columbia; University of Liege; New York University	Baydin, AG (corresponding author), Univ Oxford, Oxford, England.		Louppe, Gilles/D-1923-2017	Louppe, Gilles/0000-0002-2082-3106	National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility [DE-AC02-05CH11231]; NERSC Big Data Center; Intel; National Science Foundation [ACI-1450310, OAC-1836650]; EPRSC Autonomous Intelligent Machines and Systems grant; EPSRC/MURI [EP/N019474/1]; Lawrence Berkeley National Lab.; DARPA D3M [FA8750-17-2-0093]; Intel under its LBNL NERSC Big Data Center; NSERC	National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility; NERSC Big Data Center; Intel(Intel Corporation); National Science Foundation(National Science Foundation (NSF)); EPRSC Autonomous Intelligent Machines and Systems grant; EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Lawrence Berkeley National Lab.; DARPA D3M; Intel under its LBNL NERSC Big Data Center; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We thank the anonymous reviewers for their constructive comments that helped us improve this paper significantly. This research used resources of the National Energy Research Scientific Computing Center (NERSC), a U.S. Department of Energy Office of Science User Facility operated under Contract No. DE-AC02-05CH11231. This work was partially supported by the NERSC Big Data Center; we acknowledge Intel for their funding support. KC, LH, and GL were supported by the National Science Foundation under the awards ACI-1450310. Additionally, KC was supported by the National Science Foundation award OAC-1836650. BGH is supported by the EPRSC Autonomous Intelligent Machines and Systems grant. AGB and PT are supported by EPSRC/MURI grant EP/N019474/1 and AGB is also supported by Lawrence Berkeley National Lab. FW is supported by DARPA D3M, under Cooperative Agreement FA8750-17-2-0093, Intel under its LBNL NERSC Big Data Center, and an NSERC Discovery grant.	Aad G, 2016, EUR PHYS J C, V76, DOI 10.1140/epjc/s10052-016-4110-0; Aad G, 2012, PHYS LETT B, V716, P122, DOI 10.1016/j.physletb.2012.08.009; Abadi M, 2015, P 12 USENIX S OPERAT; Abazov VM, 2004, NATURE, V429, P638, DOI 10.1038/nature02589; Allison J, 2016, NUCL INSTRUM METH A, V835, P186, DOI 10.1016/j.nima.2016.06.125; Alwall J, 2014, J HIGH ENERGY PHYS, DOI 10.1007/JHEP07(2014)079; Alwall J, 2011, PHYS REV D, V84, DOI 10.1103/PhysRevD.84.074010; Andersen JR, 2013, PHYS REV D, V87, DOI 10.1103/PhysRevD.87.015019; Artoisenet P., 2008, POS CHARGED, V2008; Artoisenet P, 2013, PHYS REV LETT, V111, DOI 10.1103/PhysRevLett.111.091802; Arulampalam MS, 2002, IEEE T SIGNAL PROCES, V50, P174, DOI 10.1109/78.978374; Asquith L., 2018, JET SUBSTRUCTURE LAR; Aurisano A, 2016, J INSTRUM, V11, DOI 10.1088/1748-0221/11/09/P09001; Avery P, 2013, PHYS REV D, V87, DOI 10.1103/PhysRevD.87.055006; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Baydin AG, 2019, PROCEEDINGS OF SC19: THE INTERNATIONAL CONFERENCE FOR HIGH PERFORMANCE COMPUTING, NETWORKING, STORAGE AND ANALYSIS, DOI 10.1145/3295500.3356180; Bingham Eli, 2018, J MACHINE LEARNING R; Bishop C.M., 1994, MIXTURE DENSITY NETW; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Brehmer J, 2018, PHYS REV D, V98, DOI 10.1103/PhysRevD.98.052004; Brooks SP, 1998, J COMPUT GRAPH STAT, V7, P434, DOI 10.2307/1390675; Brooks S, 2011, CH CRC HANDB MOD STA, pXIX; Campbell JM, 2013, PHYS REV D, V87, DOI 10.1103/PhysRevD.87.073005; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Casado Mario Lezcano, 2017, NEUR INF PROC SYST N; CMS Collaboration, 2012, Physics Letters B, V716, P30, DOI 10.1016/j.physletb.2012.08.021; Cranmer K., 2015, ARXIV150602169; de Oliveira L, 2016, J HIGH ENERGY PHYS, DOI 10.1007/JHEP07(2016)069; Dillon J.V., 2017, TENSORFLOW DISTRIBUT; Djouadi A, 2008, PHYS REP, V457, P1, DOI 10.1016/j.physrep.2007.10.004; Doucet A., 2009, OXFORD HDB NONLINEAR, P656; Dutta R, 2016, ARXIV161110242; Endeve E, 2012, ASTROPHYS J, V751, DOI 10.1088/0004-637X/751/1/26; Gainer J S, 2013, P 2013 COMM SUMM STU; Gelman A, 2015, J EDUC BEHAV STAT, V40, P530, DOI 10.3102/1076998615606113; Gershman SJ, 2014, P 36 ANN C COGN SCI; GILKS WR, 1992, J R STAT SOC C-APPL, V41, P337; Gleisberg T, 2009, J HIGH ENERGY PHYS, DOI 10.1088/1126-6708/2009/02/007; Goodman N, 2012, ARXIV PREPRINT ARXIV; Gordon A, 2014, P C FUTURE SOFTWARE, DOI DOI 10.1145/2593882.2593900; Gritsan AV, 2016, PHYS REV D, V94, DOI 10.1103/PhysRevD.94.055023; GRZADKOWSKI B, 1995, PHYS LETT B, V350, P218, DOI 10.1016/0370-2693(95)00369-V; Harnik R, 2013, PHYS REV D, V88, DOI 10.1103/PhysRevD.88.076009; Heinecke A, 2014, INT CONF HIGH PERFOR, P3, DOI 10.1109/SC.2014.6; Hintjens P., 2013, ZEROMQ MESSAGING MAN; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hoffman MD, 2014, J MACH LEARN RES, V15, P1593; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Hooberman B., 2017, DEEP LEARN PHYS SCI; Kasieczka G., 2018, 10 INT WORKSH TOP QU; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Koller D., 2009, PROBABILISTIC GRAPHI; KONDO K, 1988, J PHYS SOC JPN, V57, P4126, DOI 10.1143/JPSJ.57.4126; Krauss F., 2002, J HIGH ENERGY PHYS, V2002; Lampl W., 2008, ATLLARGPUB2008002 CE; Le T. D., 2015, THESIS; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Martini T., 2017, MATRIX ELEMENT METHO; Martini T, 2015, J HIGH ENERGY PHYS, DOI 10.1007/JHEP09(2015)083; Naderiparizi S., 2019, ARXIV191009056; Neal Radford M, 2011, HDB MARKOV CHAIN MON, V2; Papamakarios George, 2017, ARXIV170507057; Perdikaris P, 2016, PHYS FLUIDS, V28, DOI 10.1063/1.4941315; Raberto M, 2001, PHYSICA A, V299, P319, DOI 10.1016/S0378-4371(01)00312-0; Racah E, 2016, 2016 15TH IEEE INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA 2016), P892, DOI [10.1109/ICMLA.2016.0160, 10.1109/ICMLA.2016.151]; Rainforth T., 2018, C UNC ART INT UAI; Rainforth Tom, 2018, ARXIV180204537; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; Schouten D, 2015, COMPUT PHYS COMMUN, V192, P54, DOI 10.1016/j.cpc.2015.02.020; Sjostrand T, 2006, J HIGH ENERGY PHYS, DOI 10.1088/1126-6708/2006/05/026; Soper DE, 2011, PHYS REV D, V84, DOI 10.1103/PhysRevD.84.074002; Sunnaker M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002803; Tran D, 2018, ADV NEUR IN, V31; Tran D, 2017, ADV NEUR IN, V30; Tran Dustin, 2016, ARXIV161009787; Le TA, 2017, PR MACH LEARN RES, V54, P1338; Uria B, 2016, J MACH LEARN RES, V17; van de Meent Jan-Willem, 2018, ARXIV E PRINTS; Wilkinson R. D., STAT APPL GENETICS M, V12, P129; Williams D., 1991, PROBABILITY MARTINGA; Wingate David, 2011, JMLR P, V15, P770; Wood F, 2014, JMLR WORKSH CONF PRO, V33, P1024	90	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305045
C	Bellec, PC; Kuchibhotla, AK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bellec, Pierre C.; Kuchibhotla, Arun K.			First order expansion of convex regularized estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SHARP ORACLE INEQUALITIES; LEAST-SQUARES; CONFIDENCE-INTERVALS; LASSO; SELECTION; INFERENCE; BOUNDS; REGRESSION; SPARSITY	We consider first order expansions of convex penalized estimators in high-dimensional regression problems with random designs. Our setting includes linear regression and logistic regression as special cases. For a given penalty function h and the corresponding penalized estimator (beta) over cap, we construct a quantity eta, the first order expansion of (beta) over cap, such that the distance between (beta) over cap and eta is an order of magnitude smaller than the estimation error parallel to(beta) over cap - beta*parallel to. In this sense, the first order expansion eta can be thought of as a generalization of influence functions from the mathematical statistics literature to regularized estimators in high-dimensions. Such first order expansion implies that the risk of (beta) over cap is asymptotically the same as the risk of eta which leads to a precise characterization of the MSE of (beta) over cap; this characterization takes a particularly simple form for isotropic design. Such first order expansion also leads to inference results based on (beta) over cap. We provide sufficient conditions for the existence of such first order expansion for three regularizers: the Lasso in its constrained form, the lasso in its penalized form, and the Group-Lasso. The results apply to general loss functions under some conditions and those conditions are satisfied for the squared loss in linear regression and for the logistic loss in the logistic model.	[Bellec, Pierre C.] Rutgers State Univ, Dept Stat, 501 Hill Ctr, Piscataway, NJ 08854 USA; [Kuchibhotla, Arun K.] Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA	Rutgers State University New Brunswick; University of Pennsylvania	Bellec, PC (corresponding author), Rutgers State Univ, Dept Stat, 501 Hill Ctr, Piscataway, NJ 08854 USA.	pierre.bellec@rutgers.edu; arunku@upenn.edu						Alquier P, 2019, ANN STAT, V47, P2117, DOI 10.1214/18-AOS1742; [Anonymous], 2019, STAT LEARNING SPARSI; BELLEC P., 2018, ARXIV180401230; Bellec PC, 2018, ELECTRON J STAT, V12, P3443, DOI 10.1214/18-EJS1457; Bellec PC, 2018, ANN STAT, V46, P3603, DOI 10.1214/17-AOS1670; Bellec PC, 2018, ANN STAT, V46, P745, DOI 10.1214/17-AOS1566; Bellec Pierre C, 2019, ARXIV190208885; Bellec Pierre C, 2017, SEMINAIRE CONGRES; Belloni A, 2016, J BUS ECON STAT, V34, P606, DOI 10.1080/07350015.2016.1166116; Belloni A, 2014, ANN STAT, V42, P757, DOI 10.1214/14-AOS1204; Belloni A, 2013, BERNOULLI, V19, P521, DOI 10.3150/11-BEJ410; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Cai TT, 2009, ANN STAT, V37, P569, DOI 10.1214/07-AOS538; Candes EJ, 2006, ACT NUMERIC, V15, P257, DOI 10.1017/S0962492906230010; Dedieu Antoine, 2018, ARXIV181003081; Dirksen S, 2015, ELECTRON J PROBAB, V20, DOI 10.1214/EJP.v20-3760; HSU D, 2012, ELECTRON COMMUN PROB, V17, P1, DOI DOI 10.1214/ECP.V17-2079.URL; ICHIMURA H, 1993, J ECONOMETRICS, V58, P71, DOI 10.1016/0304-4076(93)90114-K; Javanmard A, 2014, J MACH LEARN RES, V15, P2869; Javanmard A, 2014, IEEE T INFORM THEORY, V60, P6522, DOI 10.1109/TIT.2014.2343629; Javanmard Adel, 2015, ANN STAT; Knight K, 2000, ANN STAT, V28, P1356; Koltchinskii V, 2009, ANN I H POINCARE-PR, V45, P7, DOI 10.1214/07-AIHP146; Kuchibhotla A. K., 2018, 180905172 ARXIV; Lee JD, 2014, SIAM J OPTIMIZ, V24, P1420, DOI 10.1137/130921428; Liaw C, 2017, LECT NOTES MATH, V2169, P277, DOI 10.1007/978-3-319-45282-1_18; Liu H., 2009, ARTIF INTELL, VVolume 5, P376; Lounici K, 2011, ANN STAT, V39, P2164, DOI 10.1214/11-AOS896; Mendelson S, 2016, STOCH PROC APPL, V126, P3652, DOI 10.1016/j.spa.2016.04.019; Mitra R, 2016, ELECTRON J STAT, V10, P1829, DOI 10.1214/16-EJS1120; Rigollet P, 2011, ANN STAT, V39, P731, DOI 10.1214/10-AOS854; Sun TN, 2013, J MACH LEARN RES, V14, P3385; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Van de Geer S, 2014, ANN STAT, V42, P1166, DOI 10.1214/14-AOS1221; van de Geer S, 2014, SCAND J STAT, V41, P72, DOI 10.1111/sjos.12032; Van der Vaart A.W, 2000, CAMBRIDGE SERIES STA, V3; Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47; Yu B., 2009, ADV NEURAL INFORM PR, P1348; Zhang CH, 2014, J R STAT SOC B, V76, P217, DOI 10.1111/rssb.12026; Zhang CH, 2012, STAT SCI, V27, P576, DOI 10.1214/12-STS399; Zhang CH, 2010, ANN STAT, V38, P894, DOI 10.1214/09-AOS729; Zhang L, 2011, MANAG SCI ENG MANAG, P28, DOI 10.1109/RADECS.2011.6131380	47	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303045
C	Bellemare, MG; Dabney, W; Dadashi, R; Taiga, AA; Castro, PS; Le Roux, N; Schuurmans, D; Lattimore, T; Lyle, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bellemare, Marc G.; Dabney, Will; Dadashi, Robert; Taiga, Adrien Ali; Castro, Pablo Samuel; Le Roux, Nicolas; Schuurmans, Dale; Lattimore, Tor; Lyle, Clare			A Geometric Perspective on Optimal Representations for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new perspective on representation learning in reinforcement learning based on geometric properties of the space of value functions. We leverage this perspective to provide formal evidence regarding the usefulness of value functions as auxiliary tasks. Our formulation considers adapting the representation to minimize the (linear) approximation of the value function of all stationary policies for a given environment. We show that this optimization reduces to making accurate predictions regarding a special class of value functions which we call adversarial value functions (AVFs). We demonstrate that using value functions as auxiliary tasks corresponds to an expected-error relaxation of our formulation, with AVFs a natural candidate, and identify a close relationship with proto-value functions (Mahadevan, 2005). We highlight characteristics of AVFs and their usefulness as auxiliary tasks in a series of experiments on the four-room domain.	[Bellemare, Marc G.; Dadashi, Robert; Taiga, Adrien Ali; Castro, Pablo Samuel; Le Roux, Nicolas; Schuurmans, Dale] Google Res, Montreal, PQ, Canada; [Dabney, Will; Lattimore, Tor] DeepMind, London, England; [Taiga, Adrien Ali] Univ Montreal, Mila, Montreal, PQ, Canada; [Schuurmans, Dale] Univ Alberta, Edmonton, AB, Canada; [Lyle, Clare] Univ Oxford, Oxford, England	Universite de Montreal; University of Alberta; University of Oxford	Bellemare, MG (corresponding author), Google Res, Montreal, PQ, Canada.							Abadi M., 2016, S OP SYST DES IMPL; Abel D., 2016, P INT C MACH LEARN; Andrychowicz M., 2017, NIPS; [Anonymous], P INT C LEARN REPR; [Anonymous], P INT C LEARN REPR; Barreto A, 2017, ADV NEURAL INFORM PR; Behzadian B., 2018, P ICML PRED GEN MOD; Bellemare M. G., 2017, P INT C MACH LEARN; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellman RE, 1957, DYNAMIC PROGRAMMING; Bernhard Korte, 2005, COMBINATORIAL OPTIMI; Bertsekas D. P., 2018, TECHNICAL REPORT; Bertsekas D. P., 2012, APPROXIMATE DYNAMIC, V2; Bhatnagar S, 2013, IEEE J SELECTED TOPI; Boyd S, 2004, CONVEX OPTIMIZATION; Chung W., 2019, 7 INT C LEARN REPR I; Dadashi R, 2019, VALUE FUNCTION POLYT; Dayan P., 1993, NEURAL COMPUTATION; Ernst D, 2005, J MACH LEARN RES, V6, P503; Farahmand A, 2016, J MACHINE LEARNING R; Foster D., 2002, MACHINE LEARNING; Francois-Lavet V, 2018, COMBINED REINFORCEME; Gelada C, 2019, P INT C MACH LEARN; Hutter M., 2009, J ARTIFICIAL GEN INT; Keller P W, 2006, P INT C MACH LEARN; Kumar Saurabh, 2018, DOPAMINE RES FRAMEWO; Lagoudakis M., 2003, J MACHINE LEARNING R; Levine N., 2017, ADV NEURAL INFORM PR; Li L., 2006, ISAIM; Liang Y, 2016, P INT C AUT AG MULT; Littman M., 2002, ADV NEURAL INFORM PR; Machado M. C, 2018, P INT C LEARN REPR; Machado M. C, 2017, P INT C MACH LEARN; Mahadevan S, 2010, ADV NEURAL INFORM PR; Mahadevan Sridhar, 2007, J MACHINE LEARNING R; Menache I., 2005, ANN OPERATIONS RES; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R., 2003, P INT C MACH LEARN; Nachum O., 2019, P INT C LEARN REPR; Parr R, 2008, P INT C MACH LEARN; Parr R, 2007, P INT C MACH LEARN; Petrik M, 2011, J MACHINE LEARNING R; Petrik M, 2007, P INT JOINT C ART IN; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Ratitch B, 2004, P EUROPEAN C MACHINE; Rockafellar R. T., 2009, VARIATIONAL ANAL, DOI DOI 10.1007/978-3-030-63416-2_683; Ruan S. S, 2015, P AAAI C ART INT; Rummery G. A., 1994, TECHNICAL REPORT; Samuel AL, 1959, IBM J RES DEV; Schaul T, 2015, P INT C MACH LEARN; Selfridge O., 1959, PANDEMONIUM PARADIGM; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Solway A, 2014, PLOS COMPUTATIONAL B; Song Z, 2016, ADV NEURAL INFORM PR; Stachenfeld K. L, 2014, ADV NEURAL INFORM PR; Such F. P, 2019, P INT JOINT C ART IN; Sutton R, 2011, P INT C AUT AG MULT; Sutton R. S., 1999, ARTIFICIAL INTELLIGE; Sutton R. S., 1996, ADV NEURAL INFORM PR; Sutton R. S., 2016, J MACHINE LEARNING R; Sutton R. S., 2000, ADV NEURAL INFORM PR; Sutton Richard S, 1995, P INT C MACH LEARN; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; TESAURO G, 1995, COMMUNICATIONS ACM, V38; Tieleman T., 2012, COURSERA NEURAL NETW; Tosatto S, 2017, P INT C MACH LEARN; van den Oord Aaron, 2018, ADV NEURAL INFORM PR, P4; Van Hasselt H., 2018, DEEP REINFORCEMENT L; Veeriah V, 2019, ADV NEURAL INFORM PR; Wu Y, 2019, P INT C LEARN REPR; Yu H., 2009, P IEEE S AD DYN PROG	72	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304037
C	Bello, K; Honorio, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bello, Kevin; Honorio, Jean			Exact inference in structured prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Structured prediction can be thought of as a simultaneous prediction of multiple labels. This is often done by maximizing a score function on the space of labels, which decomposes as a sum of pairwise and unary potentials. The above is naturally modeled with a graph, where edges and vertices are related to pairwise and unary potentials, respectively. We consider the generative process proposed by Globerson et al. (2015) and apply it to general connected graphs. We analyze the structural conditions of the graph that allow for the exact recovery of the labels. Our results show that exact recovery is possible and achievable in polynomial time for a large class of graphs. In particular, we show that graphs that are bad expanders can be exactly recovered by adding small edge perturbations coming from the Erdos-Renyi model. Finally, as a byproduct of our analysis, we provide an extension of Cheeger's inequality.	[Bello, Kevin; Honorio, Jean] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Bello, K (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47906 USA.	kbellome@purdue.edu; jhonorio@purdue.edu			National Science Foundation [1716609-IIS]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1716609-IIS.	ABBE E., 2014, IEEE T NETW SCI ENG, V1, P10; Abbe E, 2016, IEEE T INFORM THEORY, V62, P471, DOI 10.1109/TIT.2015.2490670; Altun Y., 2003, EUR C SPEECH COMM TE; BARAHONA F, 1982, J PHYS A-MATH GEN, V15, P3241, DOI 10.1088/0305-4470/15/10/028; Bello K, 2018, NEURIPS; Boyd S, 2004, CONVEX OPTIMIZATION; Boykov Y, 2006, HANDBOOK OF MATHEMATICAL MODELS IN COMPUTER VISION, P79, DOI 10.1007/0-387-28831-7_5; Chandrasekaran V, 2013, P NATL ACAD SCI USA; Chandrasekaran V., 2008, P 24 C UNC ART INT, P70; Cheeger J, 1969, P PRINC C S BOCHN; CHEN Y., 2016, INT C MACH LEARN, P689; Daume H, 2009, MACH LEARN, V75, P297, DOI 10.1007/s10994-009-5106-x; Foster D., 2018, INT C ART INT STAT, P1810; Globerson A, 2015, PR MACH LEARN RES, V37, P2181; Hoory S, 2006, B AM MATH SOC, V43, P439, DOI 10.1090/S0273-0979-06-01126-8; Koo Terry, 2010, P 2010 C EMP METH NA, P1288; Krivelevich M, 2015, SIAM J DISCRETE MATH, V29, P1654, DOI 10.1137/151002496; Kulesza A., 2007, ADV NEURAL INFORM PR, P785; Lafferty John, 2001, P IEEE; Schraudolph N. N., 2009, ADV NEURAL INFORM PR, P1417; Sontag D., 2012, UNCERTAINTY ARTIFICI; Taskar B, 2004, ADV NEUR IN, V16, P25; TROPP JA, 2012, FOUND COMPUT MATH, V12, P389, DOI DOI 10.1007/s10208-011-9099-z; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303066
C	Bellot, A; van der Schaar, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bellot, Alexis; van der Schaar, Mihaela			Conditional Independence Testing using Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFERENCE	We consider the hypothesis testing problem of detecting conditional dependence, with a focus on high-dimensional feature spaces. Our contribution is a new test statistic based on samples from a generative adversarial network designed to approximate directly a conditional distribution that encodes the null hypothesis, in a manner that maximizes power (the rate of true negatives). We show that such an approach requires only that density approximation be viable in order to ensure that we control type I error (the rate of false positives); in particular, no assumptions need to be made on the form of the distributions or feature dependencies. Using synthetic simulations with high-dimensional data we demonstrate significant gains in power over competing methods. In addition, we illustrate the use of our test to discover causal markers of disease in genetic data.	[Bellot, Alexis; van der Schaar, Mihaela] Univ Cambridge, Cambridge, England; [Bellot, Alexis; van der Schaar, Mihaela] Alan Turing Inst, London, England; [van der Schaar, Mihaela] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	University of Cambridge; University of California System; University of California Los Angeles	Bellot, A (corresponding author), Univ Cambridge, Cambridge, England.; Bellot, A (corresponding author), Alan Turing Inst, London, England.	abellot@turing.ac.uk; mschaar@turing.ac.uk			Alan Turing Institute under the EPSRC [EP/N510129/1]; ONR; NSF [1462245, 1533983]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	We thank the anonymous reviewers for valuable feedback. This work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1, the ONR and the NSF grants number 1462245 and number 1533983.	[Anonymous], 2017, ICLR; Barretina J, 2012, NATURE, V483, P603, DOI 10.1038/nature11003; Belghazi Ishmael, 2018, INT C MACH LEARN; Berrett Thomas B, 2018, ARXIV180705405; Candes E, 2018, J R STAT SOC B, V80, P551, DOI 10.1111/rssb.12265; Chatterjee A, 2014, CELL REP, V9, P1333, DOI 10.1016/j.celrep.2014.10.039; Doran G, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P132; Garnett MJ, 2012, NATURE, V483, P570, DOI 10.1038/nature11005; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Jordon J., 2019, ICLR; Khera AV, 2017, NAT REV GENET, V18, DOI 10.1038/nrg.2016.160; Kun Zhang, 2012, UAI; Lauritzen Steffen L., 1996, OXFORD STAT SCI SERI, V17; Lopez-Paz D., 2013, ADV NEURAL INFORM PR, V26, P1, DOI DOI 10.1214/A0S/1176345528; Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057; Prickett TD, 2014, J INVEST DERMATOL, V134, P452, DOI 10.1038/jid.2013.365; Ramdas A, 2015, AAAI CONF ARTIF INTE, P3571; Runge Jakob, 2018, AISTATS; Sen R., 2017, ADV NEURAL INFORM PR, P2951; Sen Rajat, 2018, ARXIV180609708; Shah R.D., 2018, ARXIV180407203; Strobl E. V., 2017, ARXIV170203877; Tansey W., 2018, ARXIV181100645; Thomas Joy A, ELEMENTS INFORM THEO; Toni T, 2009, J R SOC INTERFACE, V6, P187, DOI 10.1098/rsif.2008.0172; Zhu ZH, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02317-2	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302022
C	Bennett, A; Kallus, N; Schnabel, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bennett, Andrew; Kallus, Nathan; Schnabel, Tobias			Deep Generalized Method of Moments for Instrumental Variable Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SAMPLE PROPERTIES; MODELS	Instrumental variable analysis is a powerful tool for estimating causal effects when randomization or full control of confounders is not possible. The application of standard methods such as 2SLS, GMM, and more recent variants are significantly impeded when the causal effects are complex, the instruments are high-dimensional, and/or the treatment is high-dimensional. In this paper, we propose the DeepGMM algorithm to overcome this. Our algorithm is based on a new variational reformulation ofGMMwith optimal inverse-covariance weighting that allows us to efficiently control very many moment conditions. We further develop practical techniques for optimization and model selection that make it particularly successful in practice. Our algorithm is also computationally tractable and can handle large-scale datasets. Numerical results show our algorithm matches the performance of the best tuned methods in standard settings and continues to work in high-dimensional settings where even recent methods break.	[Bennett, Andrew; Kallus, Nathan] Cornell Univ, Ithaca, NY 14853 USA; [Schnabel, Tobias] Microsoft Res, Redmond, WA USA	Cornell University; Microsoft	Bennett, A (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	awb222@cornell.edu; kallus@cornell.edu; tbs49@cornell.edu			National Science Foundation [1846210]	National Science Foundation(National Science Foundation (NSF))	This material is based upon work supported by the National Science Foundation under Grant No. 1846210.	Ai CR, 2003, ECONOMETRICA, V71, P1795, DOI 10.1111/1468-0262.00470; Angrist JD, 2009, MOSTLY HARMLESS ECONOMETRICS: AN EMPIRICISTS COMPANION, P1; Angrist JD, 2001, J ECON PERSPECT, V15, P69, DOI 10.1257/jep.15.4.69; Arjovsky M., 2017, ARXIV170107875; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett PL, 2019, J MACH LEARN RES, V20, P1; BERRY S, 1995, ECONOMETRICA, V63, P841, DOI 10.2307/2171802; Blundell R, 2007, ECONOMETRICA, V75, P1613, DOI 10.1111/j.1468-0262.2007.00808.x; CHAMBERLAIN G, 1987, J ECONOMETRICS, V34, P305, DOI 10.1016/0304-4076(87)90015-7; Chen X, 2016, ADV NEUR IN, V29; Chen XH, 2018, QUANT ECON, V9, P39, DOI 10.3982/QE722; Cole JA, 2006, PHARMACOTHERAPY, V26, P1157, DOI 10.1592/phco.26.8.1157; Darolles S, 2011, ECONOMETRICA, V79, P1541, DOI 10.3982/ECTA6539; Daskalakis Constantinos, 2017, ARXIV171100141; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HANSEN LP, 1982, ECONOMETRICA, V50, P1029, DOI 10.2307/1912775; HANSEN LP, 1980, J ECON DYN CONTROL, V2, P7, DOI 10.1016/0165-1889(80)90049-4; Hansen LP, 1996, J BUS ECON STAT, V14, P262, DOI 10.2307/1392442; HANSEN LP, 1982, ECONOMETRICA, V50, P1269, DOI 10.2307/1911873; Hartford J, 2017, PR MACH LEARN RES, V70; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johansson F. D., 2018, ARXIV180208598; Kallus, 2016, ARXIV PREPRINT ARXIV; Kallus Nathan, 2018, ARXIV180205664; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Ledoux M., 2013, PROBABILITY BANACH S, P86; Lewis G., 2018, ARXIV180307164; Newey WK, 2003, ECONOMETRICA, V71, P1565, DOI 10.1111/1468-0262.00459; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Ravuri S., 2018, ARXIV180611006; RUBIN DB, 1973, BIOMETRICS, V29, P159, DOI 10.2307/2529684; Shalit U, 2017, PR MACH LEARN RES, V70	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303054
C	Bennett, A; Kallus, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bennett, Andrew; Kallus, Nathan			Policy Evaluation with Latent Confounders via Optimal Balance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PROXY VARIABLES; PERFORMANCE; BIAS	Evaluating novel contextual bandit policies using logged data is crucial in applications where exploration is costly, such as medicine. But it usually relies on the assumption of no unobserved confounders, which is bound to fail in practice. We study the question of policy evaluation when we instead have proxies for the latent confounders and develop an importance weighting method that avoids fitting a latent outcome regression model. We show that unlike the unconfounded case no single set of weights can give unbiased evaluation for all outcome models, yet we propose a new algorithm that can still provably guarantee consistency by instead minimizing an adversarial balance objective. We further develop tractable algorithms for optimizing this objective and demonstrate empirically the power of our method when confounders are latent.	[Bennett, Andrew; Kallus, Nathan] Cornell Univ, Ithaca, NY 14853 USA	Cornell University	Bennett, A (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.	awb222@cornell.edu; kallus@cornell.edu			National Science Foundation [1846210]; JPMorgan Chase Co.	National Science Foundation(National Science Foundation (NSF)); JPMorgan Chase Co.	This material is based upon work supported by the National Science Foundation under Grant No. 1846210. This research was funded in part by JPMorgan Chase & Co. Any views or opinions expressed herein are solely those of the authors listed, and may differ from the views and opinions expressed by JPMorgan Chase & Co. or its affiliates. This material is not a product of the Research Department of J.P. Morgan Securities LLC. This material should not be construed as an individual recommendation for any particular client and is not intended as a recommendation of particular securities, financial instruments or strategies for a particular client. This material does not constitute a solicitation or offer in any jurisdiction.	[Anonymous], 2017, ARXIV170508821; Athey S, 2018, J R STAT SOC B, V80, P597, DOI 10.1111/rssb.12268; Austin PC, 2015, STAT MED, V34, P3661, DOI 10.1002/sim.6607; Bertsimas D, 2017, DIABETES CARE, V40, P210, DOI 10.2337/dc16-0826; Beygelzimer A, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P129; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Edwards JK, 2015, INT J EPIDEMIOL, V44, P1452, DOI 10.1093/ije/dyu272; Elliott MR, 2008, J OFF STAT, V24, P517; FROST PA, 1979, REV ECON STAT, V61, P323, DOI 10.2307/1924606; Ionides EL, 2008, J COMPUT GRAPH STAT, V17, P295, DOI 10.1198/106186008X320456; Kallus, 2016, ARXIV PREPRINT ARXIV; Kallus N., 2019, STAT SINICA, V29, P1697; Kallus N., 2018, ADV NEURAL INFORM PR, P9269; Kallus N., 2018, ADV NEURAL INFORM PR, P8895; Kallus N., 2018, ADV NEURAL INFORM PR, P6921; Kallus N., 2019, 22 INT C ART INT STA, P2281; Kallus N, 2018, P 21 INT C ART INT S, P1243; Kallus N, 2017, PR MACH LEARN RES, V54, P372; Kallus N, 2017, PR MACH LEARN RES, V70; Kallus N, 2018, J R STAT SOC B, V80, P85, DOI 10.1111/rssb.12240; Kallus Nathan, 2019, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; Kube A., 2019, P AAAI C ART INT; Kuroki M, 2014, BIOMETRIKA, V101, P423, DOI 10.1093/biomet/ast066; Ledoux M., 2013, PROBABILITY BANACH S, P86; Li LJ, 2011, METAGENOMICS OF THE HUMAN BODY, P297, DOI 10.1007/978-1-4419-7089-3_14; Lunceford JK, 2004, STAT MED, V23, P2937, DOI 10.1002/SIM.1903; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Mendelson S, 2004, J MACH LEARN RES, V4, P759, DOI 10.1162/1532443041424337; Miao W, 2016, ARXIV160908816; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pearl J., 2012, ARXIV12033504; Qian M, 2011, ANN STAT, V39, P1180, DOI 10.1214/10-AOS864; ROBINS JM, 1994, J AM STAT ASSOC, V89, P846, DOI 10.2307/2290910; Swaminathan A, 2015, PR MACH LEARN RES, V37, P814; Swaminathan A, 2015, ADV NEUR IN, V28; Wang JX, 2008, INT CONF BIOMED, P62, DOI 10.1109/BMEI.2008.123; Wickens M. R., 1972, ECONOMETRICA J ECONO, P759; Wooldridge JM, 2009, ECON LETT, V104, P112, DOI 10.1016/j.econlet.2009.04.026; Zhou DX, 2002, J COMPLEXITY, V18, P739, DOI 10.1006/jcom.2002.0635	43	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304079
C	Berner, J; Elbrachter, D; Grohs, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Berner, Julius; Elbraechter, Dennis; Grohs, Philipp			How degenerate is the parametrization of neural networks with the ReLU activation function?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ERROR-BOUNDS	Neural network training is usually accomplished by solving a non-convex optimization problem using stochastic gradient descent. Although one optimizes over the networks parameters, the main loss function generally only depends on the realization of the neural network, i.e. the function it computes. Studying the optimization problem over the space of realizations opens up new ways to understand neural network training. In particular, usual loss functions like mean squared error and categorical cross entropy are convex on spaces of neural network realizations, which themselves are non-convex. Approximation capabilities of neural networks can be used to deal with the latter non-convexity, which allows us to establish that for sufficiently large networks local minima of a regularized optimization problem on the realization space are almost optimal. Note, however, that each realization has many different, possibly degenerate, parametrizations. In particular, a local minimum in the parametrization space needs not correspond to a local minimum in the realization space. To establish such a connection, inverse stability of the realization map is required, meaning that proximity of realizations must imply proximity of corresponding parametrizations. We present pathologies which prevent inverse stability in general, and, for shallow networks, proceed to establish a restricted space of parametrizations on which we have inverse stability w.r.t. to a Sobolev norm. Furthermore, we show that by optimizing over such restricted sets, it is still possible to learn any function which can be learned by optimization over unrestricted sets.	[Berner, Julius; Elbraechter, Dennis] Univ Vienna, Fac Math, Oskar Morgenstern Pl 1, A-1090 Vienna, Austria; [Grohs, Philipp] Univ Vienna, UniVienna, Fac Math & Res Platform DataSci, Oskar Morgenstern Pl 1, A-1090 Vienna, Austria	University of Vienna; University of Vienna	Berner, J (corresponding author), Univ Vienna, Fac Math, Oskar Morgenstern Pl 1, A-1090 Vienna, Austria.	julius.berner@univie.ac.at; dennis.elbraechter@univie.ac.at; philipp.grohs@univie.ac.at			Austrian Science Fund (FWF) [I3403-N32, P 30148]	Austrian Science Fund (FWF)(Austrian Science Fund (FWF))	The research of JB and DE was supported by the Austrian Science Fund (FWF) under grants I3403-N32 and P 30148. The authors would like to thank Pavol Harar for helpful comments.	[Anonymous], ARXIV181103962; [Anonymous], 2018, ARXIV180903062; [Anonymous], 2017, ARXIV171206541; Arora S, 2018, PR MACH LEARN RES, V80; Bansal N., 2018, ADV NEURAL INFORM PR, P4261; Bartlett P. L., 2017, ARXIV170608498; Bartlett P. L., 2017, ARXIV; Berner J., 2019, ARXIV190504992; Bolcskei Helmut, 2017, ARXIV170501714; Burger M, 2001, J APPROX THEORY, V112, P235, DOI 10.1006/jath.2001.3613; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Du Simon S, 2018, GRADIENT DESCENT FIN; Evans L.C., 2015, TXB MATH; Evans LC, 2010, GRADUATE STUDIES MAT, V19; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Goodfellow IJ, 2014, 3 INT C LEARN REPR I; Guhring I., 2019, ARXIV190207896 ARXIV190207896; Hinton G.E., 2012, ARXIV; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kutyniok G., 2019, ARXIV; Li H, 2018, ADV NEUR IN, V31; Liu H., 2018, ARXIV180609055; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Pennington J, 2017, PR MACH LEARN RES, V70; Perekrestenko D., 2018, ARXIV180601528; Petersen P, 2017, ARXIV170905289; Petersen P., 2018, ARXIV180608459; Nguyen Q, 2017, PR MACH LEARN RES, V70; Rodr<prime>iguez Pau, 2016, ARXIV161101967; Safran I, 2016, PR MACH LEARN RES, V48; Shaham U, 2018, APPL COMPUT HARMON A, V44, P537, DOI 10.1016/j.acha.2016.04.003; Shamir O., 2013, P INT C MACH LEARN A, P71; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Wan WF, 2018, IEEE IND ELEC, P3815, DOI 10.1109/IECON.2018.8591522; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	40	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307077
C	Bertrand, Q; Massias, M; Gramfort, A; Salmon, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bertrand, Quentin; Massias, Mathurin; Gramfort, Alexandre; Salmon, Joseph			Handling correlated and repeated measurements with the smoothed multivariate square-root Lasso	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COVARIANCE ESTIMATION; DESCENT METHOD; REGRESSION; SELECTION; RULES; MEG	A limitation of Lasso-type estimators is that the optimal regularization parameter depends on the unknown noise level. Estimators such as the concomitant Lasso address this dependence by jointly estimating the noise level and the regression coefficients. Additionally, in many applications, the data is obtained by averaging multiple measurements: this reduces the noise variance, but it dramatically reduces sample sizes and prevents refined noise modeling. In this work, we propose a concomitant estimator that can cope with complex noise structure by using non-averaged measurements, its data-fitting term arising as a smoothing of the nuclear norm. The resulting optimization problem is convex and amenable, thanks to smoothing theory, to state-of-the-art optimization techniques that leverage the sparsity of the solutions. Practical benefits are demonstrated on toy datasets, realistic simulated data and real neuroimaging data.	[Bertrand, Quentin; Massias, Mathurin; Gramfort, Alexandre] Univ Paris Saclay, INRIA, CEA, F-91120 Palaiseau, France; [Salmon, Joseph] Univ Montpellier, CNRS, Montpellier, France	CEA; Inria; UDICE-French Research Universities; Universite Paris Saclay; Centre National de la Recherche Scientifique (CNRS); Universite de Montpellier	Bertrand, Q (corresponding author), Univ Paris Saclay, INRIA, CEA, F-91120 Palaiseau, France.	quentin.bertrand@inria.fr; mathurin.massias@inria.fr; alexandre.gramfort@inria.fr; joseph.salmon@umontpellier.fr			ERC Starting Grant SLAB [ERC-YStG-676943]	ERC Starting Grant SLAB	This work was funded by ERC Starting Grant SLAB ERC-YStG-676943.	Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bach F, 2012, FOUND TRENDS MACH LE, V4, P1, DOI 10.1561/2200000015; Beck A., 2017, 1 ORDERMETHODSIN OPT, V25; Belloni A, 2011, BIOMETRIKA, V98, P791, DOI 10.1093/biomet/asr043; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Chambolle A, 2011, J MATH IMAGING VIS, V40, P120, DOI 10.1007/s10851-010-0251-1; Chen S., 2017, NEURIPS, P2838; Dalalyan A. S., 2013, ICML; Daye ZJ, 2012, BIOMETRICS, V68, P316, DOI 10.1111/j.1541-0420.2011.01652.x; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Engemann DA, 2015, NEUROIMAGE, V108, P328, DOI 10.1016/j.neuroimage.2014.12.040; Fan JQ, 2008, J R STAT SOC B, V70, P849, DOI 10.1111/j.1467-9868.2008.00674.x; Fercoq O, 2015, PR MACH LEARN RES, V37, P333; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Gramfort A, 2013, NEUROIMAGE, V70, P410, DOI 10.1016/j.neuroimage.2012.12.051; Gramfort A, 2014, NEUROIMAGE, V86, P446, DOI 10.1016/j.neuroimage.2013.10.027; Huber P. J., 1974, Proceedings on computational statistics, P165; Huber PJ, 1981, ROBUST STAT; Johnson TB, 2015, PR MACH LEARN RES, V37, P1171; Kolar M., 2012, P 29 INT C MACH LEAR, P1447; Lam S.K., 2015, P 2 WORKSHOP LLVM CO, P1, DOI [10.1145/2833157.2833162, DOI 10.1145/2833157.2833162]; Lee W, 2012, J MULTIVARIATE ANAL, V111, P241, DOI 10.1016/j.jmva.2012.03.013; Massias M., 2018, PROC INT C ARTIF INT, P998; Massias M., 2018, ICML; Molstad A. J., 2019, ARXIV190905041; Ndiaye E, 2015, ADV NEUR IN, V28; Ndiaye E, 2017, J PHYS CONF SER, V904, DOI 10.1088/1742-6596/904/1/012006; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Obozinski G, 2010, STAT COMPUT, V20, P231, DOI 10.1007/s11222-008-9111-x; Ou WM, 2009, NEUROIMAGE, V44, P932, DOI 10.1016/j.neuroimage.2008.05.063; Owen AB, 2007, CONTEMP MATH, V443, P59; Parikh Neal, 2013, FDN TRENDS OPTIMIZAT; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rai P., 2012, P ADV NEUR INF PROC, P3185; Rothman AJ, 2010, J COMPUT GRAPH STAT, V19, P947, DOI 10.1198/jcgs.2010.09188; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Tseng P, 2009, J OPTIMIZ THEORY APP, V140, P513, DOI 10.1007/s10957-008-9458-3; Tseng P, 2001, J OPTIMIZ THEORY APP, V109, P475, DOI 10.1023/A:1017501703105; Van de Geer S., 2016, STAT ANAL HIGH DIMEN, P279; van de Geer S., 2016, LECT NOTES MATH, V2159; Wagener J, 2012, MATH METHODS STAT, V21, P109, DOI 10.3103/S1066530712020032	46	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304001
C	Bilos, M; Charpentier, B; Gunnemann, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bilos, Marin; Charpentier, Bertrand; Guennemann, Stephan			Uncertainty on Asynchronous Time Event Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Asynchronous event sequences are the basis of many applications throughout different industries. In this work, we tackle the task of predicting the next event (given a history), and how this prediction changes with the passage of time. Since at some time points (e.g. predictions far into the future) we might not be able to predict anything with confidence, capturing uncertainty in the predictions is crucial. We present two new architectures, WGP-LN and FD-Dir, modelling the evolution of the distribution on the probability simplex with time-dependent logistic normal and Dirichlet distributions. In both cases, the combination of RNNs with either Gaussian process or function decomposition allows to express rich temporal evolution of the distribution parameters, and naturally captures uncertainty. Experiments on class prediction, time prediction and anomaly detection demonstrate the high performances of our models on various datasets compared to other approaches.	[Bilos, Marin; Charpentier, Bertrand; Guennemann, Stephan] Tech Univ Munich, Munich, Germany	Technical University of Munich	Bilos, M (corresponding author), Tech Univ Munich, Munich, Germany.	bilos@in.tum.de; charpent@in.tum.de; guennemann@in.tum.de			German Federal Ministry of Education and Research (BMBF) [01IS 18036B]; BMW AG	German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); BMW AG	This research was supported by the German Federal Ministry of Education and Research (BMBF), grant no. 01IS 18036B, and by the BMW AG. The authors would like to thank Bernhard Schlegel for helpful discussion and comments. The authors of this work take full responsibilities for its content.	[Anonymous], 2010, ACTIVITY RECOGNITION; Begleiter R, 2004, J ARTIF INTELL RES, V22, P385, DOI 10.1613/jair.1491; Blundell Charles, 2015, ARXIV E PRINTS; Calcaterra Craig, 2008, ARXIV E PRINTS; Chung J., 2014, ARXIV14123555; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Eleftheriadis Stefanos, 2017, NIPS; Eswaran Dhivya, 2017, POWER CERTAINTY DIRI, P144; Fortunato Meire, 2017, ARXIV170402798; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Isham V., 1979, Stochastic Processes & their Applications, V8, P335, DOI 10.1016/0304-4149(79)90008-5; Kingma D.P, P 3 INT C LEARNING R; Lakshminarayanan Balaji, 2016, ARXIV E PRINTS; Li Yang, 2018, TIME DEPENDENT REPRE; Malinin Andrey, 2018, ARXIV E PRINTS; McDermott Patrick L., 2017, ARXIV E PRINTS; Mei HY, 2017, ADV NEUR IN, V30; Neil Daniel, 2016, CORR; Ritter Hippolyt, 2018, ICLR; Sadowski Peter, 2019, NEURAL NETWORK REGRE; Snelson E., 2006, ADV NEURAL INFORM PR, V18, P1259; Turner Ryan, 2010, AISTATS; Wen Junfeng, 2018, WEIGHTED GAUSSIAN PR; Yeo Kyongmin, 2018, LEARNING TEMPORAL EV	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904048
C	Birnbaum, S; Kuleshov, V; Enam, SZ; Koh, PW; Ermon, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Birnbaum, Sawyer; Kuleshov, Volodymyr; Enam, S. Zayd; Koh, Pang Wei; Ermon, Stefano			Temporal FiLM: Capturing Long-Range Sequence Dependencies with Feature-Wise Modulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				NARROW-BAND; SPEECH	Learning representations that accurately capture long-range dependencies in sequential inputs-including text, audio, and genomic data-is a key problem in deep learning. Feed-forward convolutional models capture only feature interactions within finite receptive fields while recurrent architectures can be slow and difficult to train due to vanishing gradients. Here, we propose Temporal Feature-Wise Linear Modulation (TFiLM)-a novel architectural component inspired by adaptive batch normalization and its extensions-that uses a recurrent neural network to alter the activations of a convolutional model. This approach expands the receptive field of convolutional sequence models with minimal computational overhead. Empirically, we find that TFiLM significantly improves the learning speed and accuracy of feed-forward neural networks on a range of generative and discriminative learning tasks, including text classification and audio super-resolution.	[Birnbaum, Sawyer; Kuleshov, Volodymyr; Enam, S. Zayd; Koh, Pang Wei; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA; [Birnbaum, Sawyer; Kuleshov, Volodymyr] Afresh Technol, San Francisco, CA 94110 USA	Stanford University	Birnbaum, S (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Birnbaum, S (corresponding author), Afresh Technol, San Francisco, CA 94110 USA.	sawyerb@cs.stanford.edu; kuleshov@cs.stanford.edu; zayd@cs.stanford.edu; pangwei@cs.stanford.edu; ermon@cs.stanford.edu			NSF [1651565, 1522054, 1733686]; ONR [N00014-19-1-2145]; AFOSR [FA9550-19-1-0024]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This research was supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), and AFOSR (FA9550-19-1-0024)	Alipanahi B, 2015, NAT BIOTECHNOL, V33, P831, DOI 10.1038/nbt.3300; [Anonymous], 2016, ABS160601781 CORR; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bradbury Jeremy, 2000, LINEAR PREDICTIVE CO; Cheng YM, 1994, IEEE T SPEECH AUDI P, V2, P544, DOI 10.1109/89.326637; de Vries Harm, 2017, MODULATING EARLY VIS; Dhingra B., 2016, ABS160601549 CORR; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dumoulin Vincent, 2018, DISTILL; Dumoulin Vincent, 2016, ARXIV161007629; Eiband M, 2018, IUI 2018: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON INTELLIGENT USER INTERFACES, P211, DOI 10.1145/3172944.3172961; Ekstrand P., 2002, P 1 IEEE BEN WORKSH; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Ernst J, 2015, NAT BIOTECHNOL, V33, P364, DOI 10.1038/nbt.3157; Gehring J., 2017, P ICML; Gers FA, 2001, LECT NOTES COMPUT SC, V2130, P669; GRAY AH, 1976, IEEE T ACOUST SPEECH, V24, P380, DOI 10.1109/TASSP.1976.1162849; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jie H., 2017, P IEEE C COMP VIS PA, P99; Johnson R, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P562, DOI 10.18653/v1/P17-1052; Joulin A., 2017, P 15 C EUR CHAPT ASS, P427, DOI DOI 10.18653/V1/E17-2068; Jurafsky Dan, 2014, SPEECH LANGUAGE PROC, V3; Kaggle, CORP FAV GROC SAL FO; Kasowski M, 2013, SCIENCE, V342, P750, DOI 10.1126/science.1242510; Kim T, 2017, INTERSPEECH, P2411, DOI 10.21437/Interspeech.2017-556; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Koh Pang Wei, 2016, BIORXIV; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuleshov V, 2014, NAT BIOTECHNOL, V32, P261, DOI 10.1038/nbt.2833; Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li KH, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P2578; Lin Z., 2017, ARXIV PREPRINT ARXIV; Maas A. L., 2012, INTERSPEECH; Macartney C., 2018, IMPROVED SPEECH ENHA; Mehri S., 2016, ARXIV161207837; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Oord A.V.D., 2016, SSW; Park KY, 2000, INT CONF ACOUST SPEE, P1843, DOI 10.1109/ICASSP.2000.862114; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perez Ethan, 2017, ABS170907871 CORR; Pulakka H, 2011, INT CONF ACOUST SPEE, P5100; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Sun Chi, 2019, ABS190505583 CORR; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wang M, 2018, 2018 11TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING (ISCSLP), P260, DOI 10.1109/ISCSLP.2018.8706637; Wang SY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4468; Yamagishi Junichi, 2012, ENGLISH MULTISPEAKER; Yang ZW, 2021, ANIM BIOTECHNOL, V32, P67, DOI 10.1080/10495398.2019.1653901; Yogatama D., 2017, GENERATIVE DISCRIMIN; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang Xiang, 2015, ADV NEURAL INFORM PR, P649, DOI DOI 10.5555/2969239.2969312	57	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901087
C	Blondel, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Blondel, Mathieu			Structured Prediction with Projection Oracles	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CLASSIFICATION; CONSISTENCY; REGRESSION	We propose in this paper a general framework for deriving loss functions for structured prediction. In our framework, the user chooses a convex set including the output space and provides an oracle for projecting onto that set. Given that oracle, our framework automatically generates a corresponding convex and smooth loss function. As we show, adding a projection as output layer provably makes the loss smaller. We identify the marginal polytope, the output space's convex hull, as the best convex set on which to project. However, because the projection onto the marginal polytope can sometimes be expensive to compute, we allow to use any convex superset instead, with potentially cheaper-to-compute projection. Since efficient projection algorithms are available for numerous convex sets, this allows us to construct loss functions for a variety of tasks. On the theoretical side, when combined with calibrated decoding, we prove that our loss functions can be used as a consistent surrogate for a (potentially non-convex) target loss function of interest. We demonstrate our losses on label ranking, ordinal regression and multilabel classification, confirming the improved accuracy enabled by projections.	[Blondel, Mathieu] NTT Commun Sci Labs, Kyoto, Japan	Nippon Telegraph & Telephone Corporation	Blondel, M (corresponding author), NTT Commun Sci Labs, Kyoto, Japan.	mathieu@mblondel.org						Ailon N, 2016, THEOR COMPUT SCI, V650, P92, DOI 10.1016/j.tcs.2016.07.033; Almeida Miguel, 2013, P 51 ANN M ASS COMPU, P196; Amos B., 2019, ARXIV190608707; Gutierrez PA, 2016, IEEE T KNOWL DATA EN, V28, P127, DOI 10.1109/TKDE.2015.2457911; BakIr G., 2007, PREDICTING STRUCTURE; Banerjee A, 2005, J MACH LEARN RES, V6, P1705; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Belanger D., 2013, NIPS WORKSH GREED OP; Belanger D, 2017, PR MACH LEARN RES, V70; BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873; Birkhoff G., 1946, U NAC TACUMAN REV SE, V5, P147; Blondel M., 2018, P MACHINE LEARNING R, P880; Blondel Mathieu, 2019, P AISTATS; Blondel Mathieu, 2019, ARXIV190102324; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Collins Michael, 2002, P EMNLP; Cortes CE, 2005, TRANSPORT RES REC, P153, DOI 10.3141/1923-17; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dessein Arnaud, 2016, ARXIV161006447; Domke Justin, 2012, INT C ARTIFICIAL INT; Duchi John C., 2010, P 27 INT C MACH LEAR, P327; GROTZINGER SJ, 1984, APPL MATH OPT, V12, P247, DOI 10.1007/BF01449044; Helmbold DP, 2009, J MACH LEARN RES, V10, P1705; Kadri Hachem, 2013, INT C MACH LEARN ICM, P471; Korba A., 2018, P NEURIPS, P8994; Krishnan R. G., 2015, P NEURIPS, P532, DOI DOI 10.5555/2969239.2969299; Kuhn H.W., 1955, NAV RES LOGIST Q, V2, P83, DOI [10.1002/nav.3800020109, DOI 10.1002/NAV.3800020109]; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lim CH, 2016, JMLR WORKSH CONF PRO, V51, P1205; Luise G., 2019, P ICML; Martins AFT, 2016, PR MACH LEARN RES, V48; Martins FT, 2017, P 2017 C EMP METH NA, P349, DOI DOI 10.18653/V1/D17-1036; Mensch A., 2019, P ICML; Mroueh Y., 2012, ADV NEURAL INFORM PR, P2789; Negrinho R., 2014, P NEURIPS; Niculae V, 2018, PR MACH LEARN RES, V80; Nowak-Vila A., 2019, P AISTATS; Nowak-Vila Alex, 2019, ARXIV190201958; Osokin A, 2017, ADV NEUR IN, V30; PARDALOS PM, 1990, MATH PROGRAM, V46, P321, DOI 10.1007/BF01585748; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pedregosa Fabian, 2017, J MACHINE LEARNING R, V18, P1769; Petterson James, 2009, ADV NEURAL INFORM PR, P1455; Ravikumar P., 2011, JMLR WORKSHOP C P, P618; Rockafellar R. T., 1970, CONVEX ANAL; SINKHORN R, 1967, PAC J MATH, V21, P343, DOI 10.2140/pjm.1967.21.343; Stoyanov Veselin, 2011, P AISTATS; Suehiro D., 2012, P ALT; Taskar Ben, 2004, THESIS; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Valiant L. G., 1979, Theoretical Computer Science, V8, P189, DOI 10.1016/0304-3975(79)90044-6; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Weston J., 2003, ADV NEURAL INFORM PR, V15, P897; Yasutake S, 2011, LECT NOTES COMPUT SC, V7074, P534, DOI 10.1007/978-3-642-25591-5_55; Zeng X., 2015, P SPARS; Zhang T, 2004, ANN STAT, V32, P56	58	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903075
C	Boney, R; Di Palo, N; Berglund, M; Ilin, A; Kannala, J; Rasmus, A; Valpola, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Boney, Rinu; Di Palo, Norman; Berglund, Mathias; Ilin, Alexander; Kannala, Juho; Rasmus, Antti; Valpola, Harri			Regularizing Trajectory Optimization with Denoising Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICTIVE CONTROL; NETWORK	Trajectory optimization using a learned model of the environment is one of the core elements of model-based reinforcement learning. This procedure often suffers from exploiting inaccuracies of the learned model. We propose to regularize trajectory optimization by means of a denoising autoencoder that is trained on the same trajectories as the model of the environment. We show that the proposed regularization leads to improved planning with both gradient-based and gradient-free optimizers. We also demonstrate that using regularized trajectory optimization leads to rapid initial learning in a set of popular motor control tasks, which suggests that the proposed approach can be a useful tool for improving sample efficiency.	[Boney, Rinu; Ilin, Alexander; Kannala, Juho] Aalto Univ, Espoo, Finland; [Boney, Rinu; Berglund, Mathias; Ilin, Alexander; Rasmus, Antti; Valpola, Harri] Curious AI, Helsinki, Finland; [Di Palo, Norman] Sapienza Univ Rome, Rome, Italy	Aalto University; Sapienza University Rome	Boney, R (corresponding author), Aalto Univ, Espoo, Finland.; Boney, R (corresponding author), Curious AI, Helsinki, Finland.	rinu.boney@aalto.fi; normandipalo@gmail.com						Akhtar N, 2018, IEEE ACCESS, V6, P14410, DOI 10.1109/ACCESS.2018.2807385; Arulkumaran K., 2017, ARXIV PREPRINT ARXIV; Botev ZI, 2013, HANDB STAT, V31, P35, DOI 10.1016/B978-0-444-53859-8.00003-5; Brockman G., 2016, OPENAI GYM; Clavera I., 2018, ARXIV180905214; Dalvi N, 2004, 10 ACM SIGKDD INT C, DOI DOI 10.1145/1014052.1014066; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Di Palo Norman, 2018, ARXIV181203955; Duan Y, 2016, INT C MACH LEARN, P1329; Finn C, 2017, PR MACH LEARN RES, V70; Huang S., 2017, 5 INT C LEARN REPR I; Kingma D.P., 2015, INT C LEARN REPR ICL; Ko J, 2007, IEEE INT CONF ROBOT, P742, DOI 10.1109/ROBOT.2007.363075; Kouvaritakis, 2001, NONLINEAR PREDICTIVE, V61; Kumar V, 2016, IEEE INT CONF ROBOT, P378, DOI 10.1109/ICRA.2016.7487156; Kurutach T., 2018, INT C LEARN REPR; Levine S, 2014, ADV NEUR IN, V27; Levine Sergey, 2013, ICML; Lowrey Kendall, 2018, ARXIV181101848; Mayne DQ, 2000, AUTOMATICA, V36, P789, DOI 10.1016/S0005-1098(99)00214-9; Miyasawa K., 1961, B I INT STAT, V38, P1; Nagabandi A, 2018, IEEE INT CONF ROBOT, P7579; Pong V., 2018, INT C LEARN REPR ICL; Ramachandran P., 2017, SEARCHING ACTIVATION; Raphan M, 2011, NEURAL COMPUT, V23, P374, DOI 10.1162/NECO_a_00076; Ricker N. L., 1993, Journal of Process Control, V3, P109, DOI 10.1016/0959-1524(93)80006-W; Rommel Cedric, 2019, J GUIDANCE CONTROL D, P1; Rossiter J.A., 2003, CRC P CONTROL SER; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Tassa Y, 2014, IEEE INT CONF ROBOT, P1168, DOI 10.1109/ICRA.2014.6907001; Tassa Y, 2012, IEEE INT C INT ROBOT, P4906, DOI 10.1109/IROS.2012.6386025; Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142; Vincent P, 2010, J MACH LEARN RES, V11, P3371	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302081
C	Bravo-Hermsdorff, G; Gunderson, LM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bravo-Hermsdorff, Gecia; Gunderson, Lee M.			A Unifying Framework for Spectrum-Preserving Graph Sparsification and Coarsening	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					How might one "reduce" a graph? That is, generate a smaller graph that preserves the global structure at the expense of discarding local details? There has been extensive work on both graph sparsification (removing edges) and graph coarsening (merging nodes, often by edge contraction); however, these operations are currently treated separately. Interestingly, for a planar graph, edge deletion corresponds to edge contraction in its planar dual (and more generally, for a graphical matroid and its dual). Moreover, with respect to the dynamics induced by the graph Laplacian (e.g., diffusion), deletion and contraction are physical manifestations of two reciprocal limits: edge weights of 0 and 1, respectively. In this work, we provide a unifying framework that captures both of these operations, allowing one to simultaneously sparsify and coarsen a graph while preserving its large-scale structure. The limit of infinite edge weight is rarely considered, as many classical notions of graph similarity diverge. However, its algebraic, geometric, and physical interpretations are reflected in the Laplacian pseudoinverse L-dagger, which remains finite in this limit. Motivated by this insight, we provide a probabilistic algorithm that reduces graphs while preserving L-dagger, using an unbiased procedure that minimizes its variance. We compare our algorithm with several existing sparsification and coarsening algorithms using real-world datasets, and demonstrate that it more accurately preserves the large-scale structure.	[Bravo-Hermsdorff, Gecia] Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA; [Gunderson, Lee M.] Princeton Univ, Dept Astrophys Sci, Princeton, NJ 08544 USA	Princeton University; Princeton University	Bravo-Hermsdorff, G (corresponding author), Princeton Univ, Princeton Neurosci Inst, Princeton, NJ 08544 USA.	geciah@princeton.edu; leeg@princeton.edu						Ahn KJ, 2012, P PODS, P5, DOI [10.1145/2213556.2213560, DOI 10.1145/2213556.2213560]; [Anonymous], 2015, DEEP CONVOLUTIONAL N; [Anonymous], 2015, J EXP ALGORITHMICS; Ausiello Giorgio, 2012, COMPLEXITY APPROXIMA; Batson J, 2013, COMMUN ACM, V56, P87, DOI 10.1145/2492007.2492029; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chandra A. K., 1996, Computational Complexity, V6, P312, DOI 10.1007/BF01270385; Chazelle B., 2000, DISCREPANCY METHOD R; Chen HC, 2018, AAAI CONF ARTIF INTE, P2127; Christiano P, 2011, ACM S THEORY COMPUT, P273; Cohen MB, 2017, ACM S THEORY COMPUT, P410, DOI 10.1145/3055399.3055463; Cohen Michael B., 2014, P 46 ANN ACM S THEOR; Estrach J. B., 2014, P 2 INT C LEARN REPR; FIEDLER M, 1973, CZECH MATH J, V23, P298; Fouss F, 2007, IEEE T KNOWL DATA EN, V19, P355, DOI 10.1109/TKDE.2007.46; Fung WS, 2019, SIAM J COMPUT, V48, P1196, DOI 10.1137/16M1091666; Gentile C., 2013, P C LEARN THEOR PRIN, P662; Gleiser PM, 2003, ADV COMPLEX SYST, V6, P565, DOI 10.1142/S0219525903001067; HENDRICKSON B, 1995, P ACM IEEE C SUP, P1; Herbster M, 2005, ACM INT C PROCEEDING, V119, P305; Hirani AN, 2015, 2015 IEEE 29TH INTERNATIONAL PARALLEL AND DISTRIBUTED PROCESSING SYMPOSIUM WORKSHOPS, P812, DOI 10.1109/IPDPSW.2015.73; Isella L, 2011, J THEOR BIOL, V271, P166, DOI 10.1016/j.jtbi.2010.11.033; Jarrell TA, 2012, SCIENCE, V337, P437, DOI 10.1126/science.1221762; Jin Y, 2018, ARXIV180204447; Kapralov M, 2017, SIAM J COMPUT, V46, P456, DOI 10.1137/141002281; Karypis G, 1998, SIAM J SCI COMPUT, V20, P359, DOI 10.1137/S1064827595287997; Koren Y, 2002, INFOVIS 2002: IEEE SYMPOSIUM ON INFORMATION VISUALIZATION 2002, P137, DOI 10.1109/INFVIS.2002.1173159; Koren Y., 2000, LNCS, P183, DOI DOI 10.1007/3-540-44541-2_18; Koutis I., 2016, ACM TRANS PARALLEL C, V3, P14, DOI [10.1145/2948062, DOI 10.1145/2948062]; Kyng R, 2015, C LEARN THEOR; Kyng R, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2032; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184; Lee YT, 2017, ACM S THEORY COMPUT, P678, DOI 10.1145/3055399.3055477; LeGall F., 2014, ISSAC 2014 P 39 INT; Loukas A, 2018, ARXIV180810650V2; Loukas Andreas, 2018, P MACHINE LEARNING R; MEYER CD, 1973, SIAM J APPL MATH, V24, P315; Monnig ND, 2018, DISCRETE APPL MATH, V236, P347, DOI 10.1016/j.dam.2017.10.007; Negahban S., 2012, ADV NEURAL INFORM PR, P2474; NICHOLSON AC, 2015, PYAMG ALGEBRAIC MULT, V3; Oxley J.G., 2006, MATROID THEORY, V3; Purohit M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P1296, DOI 10.1145/2623330.2623701; Ranjan G, 2014, LECT NOTES COMPUT SC, V8881, P729, DOI 10.1007/978-3-319-12691-3_54; Ranjan G, 2013, PHYSICA A, V392, P3833, DOI 10.1016/j.physa.2013.04.013; RIEDEL KS, 1992, SIAM J MATRIX ANAL A, V13, P659, DOI 10.1137/0613040; Ron D, 2011, MULTISCALE MODEL SIM, V9, P407, DOI 10.1137/100791142; Saerens M, 2004, LECT NOTES COMPUT SC, V3201, P371; Satuluri V., 2011, P INT C MAN DAT SIGM, P721, DOI DOI 10.1145/1989323.1989399; Simonovsky M., 2017, P IEEE C COMP VIS PA, P3693; Solis R, 2006, IEEE DECIS CONTR P, P2737; Spielman DA, 2011, SIAM J COMPUT, V40, P1913, DOI 10.1137/080734029; Spielman DA, 2011, SIAM J COMPUT, V40, P981, DOI 10.1137/08074489X; Stehle J, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0023176; Subelj L, 2011, EUR PHYS J B, V81, P353, DOI 10.1140/epjb/e2011-10979-2; Teng SH, 2010, LECT NOTES COMPUT SC, V6108, P2; Van Mieghem P, 2017, PHYS REV E, V96, DOI 10.1103/PhysRevE.96.032311; Wang Y., 2018, ARXIV181208942; Woodbury M. A., 1950, 42 PRINC U STAT RES	58	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307072
C	Bressan, M; Cesa-Bianchi, N; Paudice, A; Vitale, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bressan, Marco; Cesa-Bianchi, Nicolo; Paudice, Andrea; Vitale, Fabio			Correlation Clustering with Adaptive Similarity Queries	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In correlation clustering, we are given P. objects together with a binary similarity score between each pair of them. The goal is to partition the objects into clusters so to minimise the disagreements with the scores. In this work we investigate correlation clustering as an active learning problem: each similarity score can be learned by making a query, and the goal is to minimise both the disagreements and the total number of queries. On the one hand, we describe simple active learning algorithms, which provably achieve an almost optimal trade-off while giving cluster recovery guarantees, and we test them on different datasets. On the other hand, we prove information-theoretical bounds on the number of queries necessary to guarantee a prescribed disagreement bound. These results give a rich characterization of the trade-off between queries and clustering error.	[Bressan, Marco] Univ Rome Sapienza, Dept Comp Sci, Rome, Italy; [Cesa-Bianchi, Nicolo; Paudice, Andrea] Univ Milan, Dept Comp Sci, Milan, Italy; [Cesa-Bianchi, Nicolo] Univ Milan, DSRC, Milan, Italy; [Paudice, Andrea] IIT, Chicago, IL 60616 USA; [Vitale, Fabio] Univ Lille, Dept Comp Sci, Lille, France; [Vitale, Fabio] INRIA, Paris, France	Sapienza University Rome; University of Milan; University of Milan; Illinois Institute of Technology; Universite de Lille - ISITE; Universite de Lille; Inria	Bressan, M (corresponding author), Univ Rome Sapienza, Dept Comp Sci, Rome, Italy.		Paudice, Andrea/CAH-2786-2022		Google Focused Award "Algorithms and Learning for Al" (ALL4AI); ERC [DMAP 680153]; "Dipartimenti di Eccellenza 2018-2022" grant; MIUR PRIN grant Algorithms, Games, and DigitalMarkets (ALGADIMAR)	Google Focused Award "Algorithms and Learning for Al" (ALL4AI)(Google Incorporated); ERC(European Research Council (ERC)European Commission); "Dipartimenti di Eccellenza 2018-2022" grant; MIUR PRIN grant Algorithms, Games, and DigitalMarkets (ALGADIMAR)	The authors gratefully acknowledge partial support by the Google Focused Award "Algorithms and Learning for Al" (ALL4AI). Marco Bressan and Fabio Vitale are also supported in part by the ERC Starting Grant DMAP 680153 and by the "Dipartimenti di Eccellenza 2018-2022" grant awarded to the Department of Computer Science of the Sapienza University of Rome. Nicolo Cesa-Bianchi is also supported by the MIUR PRIN grant Algorithms, Games, and DigitalMarkets (ALGADIMAR).	Abbe E, 2015, ANN IEEE SYMP FOUND, P670, DOI 10.1109/FOCS.2015.47; Ailon N, 2018, LECT NOTES COMPUT SC, V10807, P14, DOI 10.1007/978-3-319-77404-6_2; Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513; Bansal N, 2004, MACH LEARN, V56, P89, DOI 10.1023/B:MACH.0000033116.57574.95; Ben-Dor A, 1999, J COMPUT BIOL, V6, P281, DOI 10.1089/106652799318274; Bonchi Francesco, 2013, CORR; Cesa-Bianchi N., 2015, P C LEARN THEOR, P297; Cesa-Bianchi Nicol6, 2012, P COLT; Charikar M, 2005, J COMPUT SYST SCI, V71, P360, DOI 10.1016/j.jcss.2004.10.012; Chawla S, 2015, ACM S THEORY COMPUT, P219, DOI 10.1145/2746539.2746604; Chen YD, 2014, J MACH LEARN RES, V15, P2213; Chiang KY, 2014, J MACH LEARN RES, V15, P1177; Cohen WW, 2002, P 8 ACM SIGKDD INT C, P475, DOI DOI 10.1145/775047.775116; Demaine ED, 2006, THEOR COMPUT SCI, V361, P172, DOI 10.1016/j.tcs.2006.05.008; Getoor L, 2012, PROC VLDB ENDOW, V5, P2018, DOI 10.14778/2367502.2367564; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Kim Sungwoong, 2011, ADV NEURAL INFORM PR; Massoulie L, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P694, DOI 10.1145/2591796.2591857; Mazumdar A., 2017, ADV NEURAL INFORM PR, P5788; Mazumdar A., 2017, ADV NEURAL INFORM PR, P4682; Mossel E, 2018, COMBINATORICA, V38, P665, DOI 10.1007/s00493-016-3238-8; Qu XB, 2016, INT CONF ACOUST SPEE, P689, DOI 10.1109/ICASSP.2016.7471763; Saha Barna, 2019, P ESA; Shalev-Shwartz Shai, 2014, UNDERSTANDINGMACHINE; Tang J, 2016, ACM COMPUT SURV, V49, DOI 10.1145/2906153; Wirth A., 2010, ENCY MACHINE LEARNIN, DOI 10.1007/978-0-387-30164-8176	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904021
C	Bun, M; Steinke, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bun, Mark; Steinke, Thomas			Average-Case Averages: Private Algorithms for Smooth Sensitivity and Mean Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The simplest and most widely applied method for guaranteeing differential privacy is to add instance-independent noise to a statistic of interest that is scaled to its global sensitivity. However, global sensitivity is a worst-case notion that is often too conservative for realized dataset instances. We provide methods for scaling noise in an instance-dependent way and demonstrate that they provide greater accuracy under average-case distributional assumptions. Specifically, we consider the basic problem of privately estimating the mean of a real distribution from i.i.d. samples. The standard empirical mean estimator can have arbitrarily-high global sensitivity. We propose the trimmed mean estimator, which interpolates between the mean and the median, as a way of attaining much lower sensitivity on average while losing very little in terms of statistical accuracy. To privately estimate the trimmed mean, we revisit the smooth sensitivity framework of Nissim, Raskhodnikova, and Smith (STOC 2007), which provides a framework for using instance-dependent sensitivity. We propose three new additive noise distributions which provide concentrated differential privacy when scaled to smooth sensitivity. We provide theoretical and experimental evidence showing that our noise distributions compare favorably to others in the literature, in particular, when applied to the mean estimation problem.	[Bun, Mark] Boston Univ, Boston, MA 02215 USA; [Steinke, Thomas] IBM Res Almaden, Almaden, CA USA	Boston University; International Business Machines (IBM)	Bun, M (corresponding author), Boston Univ, Boston, MA 02215 USA.	mbun@bu.edu; smooth@thomas-steinke.net						Abadi Martin, 2016, P 2016 ACM SIGSAC C; [Anonymous], [No title captured]; [Anonymous], 2019, ARXIV190200582; [Anonymous], 2018, ARXIV181108382; Avella-Medina Marco, 2019, ARXIV190611923; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Bun Mark, 2018, STOC; Bun Mark, 2016, TCC; Bun Mark, 2019, ARXIV190602830; Campbell Z, 2018, 2018 1ST INTERNATIONAL CONFERENCE ON DATA INTELLIGENCE AND SECURITY (ICDIS 2018), P281, DOI 10.1109/ICDIS.2018.00052; Canonne C. L., 2019, ARXIV190511947; Canonne CL, 2019, ACM S THEORY COMPUT, P310, DOI 10.1145/3313276.3316336; Couch Simon, 2019, ARXIV190309364; Dwork C., 2006, TCC; Dwork C, 2006, EUROCRYPT; Dwork Cynthia, 2016, CORR; Dwork Cynthia, 2009, STOC; Feldman Vitaly, 2018, COLT; Fletcher S, 2017, EXPERT SYST APPL, V78, P16, DOI 10.1016/j.eswa.2017.01.034; Gaboardi Marco, 2016, ICML 16 P 33 INT C I, V48; Gonen Alon, 2017, ARXIV171010556; Kamath Gautam, 2019, COLT; Karwa V, 2011, PROC VLDB ENDOW, V4, P1146; Karwa Vishesh, 2018, 9 INN THEOR COMP SCI; Kasiviswanathan SP, 2013, LECT NOTES COMPUT SC, V7785, P457, DOI 10.1007/978-3-642-36594-2_26; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Nissim K, 2007, ACM S THEORY COMPUT, P75, DOI 10.1145/1250790.1250803; Okada R, 2015, LECT NOTES ARTIF INT, V9285, P458, DOI 10.1007/978-3-319-23525-7_28; Papernot N., 2018, INT C LEARN REPR ICL; Sealfon Adam, 2019, NEURIPS; Serfling R., 2011, INT ENCY STAT SCI, V23, P68; Smith Adam, 2008, ARXIV08094794; Wang Y, 2015, SPRINGER THESES-RECO, P1, DOI 10.1007/978-3-662-47175-3; Wang Y, 2013, TRANS DATA PRIV, V6, P127	34	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300017
C	Cai, RC; Xie, F; Glymour, C; Hao, ZF; Zhang, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cai, Ruichu; Xie, Feng; Glymour, Clark; Hao, Zhifeng; Zhang, Kun			Triad Constraints for Learning Causal Structure of Latent Variables	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Learning causal structure from observational data has attracted much attention, and it is notoriously challenging to find the underlying structure in the presence of confounders (hidden direct common causes of two variables). In this paper, by properly leveraging the non-Gaussianity of the data, we propose to estimate the structure over latent variables with the so-called Triad constraints: we design a form of "pseudo-residual" from three variables, and show that when causal relations are linear and noise terms are non-Gaussian, the causal direction between the latent variables for the three observed variables is identifiable by checking a certain kind of independence relationship. In other words, the Triad constraints help us to locate latent confounders and determine the causal direction between them. This goes far beyond the Tetrad constraints and reveals more information about the underlying structure from non-Gaussian data. Finally, based on the Triad constraints, we develop a two-step algorithm to learn the causal structure corresponding to measurement models. Experimental results on both synthetic and real data demonstrate the effectiveness and reliability of our method.	[Cai, Ruichu; Xie, Feng; Hao, Zhifeng] Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China; [Glymour, Clark; Zhang, Kun] Carnegie Mellon Univ, Dept Philosophy, Pittsburgh, PA 15213 USA; [Hao, Zhifeng] Foshan Univ, Sch Math & Big Data, Foshan, Peoples R China	Guangdong University of Technology; Carnegie Mellon University; Foshan University	Cai, RC (corresponding author), Guangdong Univ Technol, Sch Comp Sci, Guangzhou, Peoples R China.	cairuichu@gdut.edu.cn; xiefeng009@gmail.com; cg09@andrew.cmu.edu; zfhao@gdut.edu.cn; kunz1@cmu.edu	cai, ruichu/AAX-7200-2021	Xie, Feng/0000-0001-7229-3955	NSFC-Guangdong Joint Found [U 1501254]; Natural Science Foundation of China [61876043]; Natural Science Foundation of Guangdong [2014A030306004, 2014A030308008]; Guangdong High-level Personnel of Special Support Program [2015TQ01X140]; Science and Technology Planning Project of Guangzhou [201902010058]; Outstanding Young Scientific Research Talents International Cultivation Project Fund of Department of Education of Guangdong Province [40190001]; NIH [NIH-iRO1EB022858-01, FAINRO1EB022858, NIH-iROiLM012087, NIH-5U54HG008540-02, FAINU54HG008540]; United States Air Force [FA8650-17-C-7715]; NSF EAGER Grant [IIS-182968 1]; Living Analytics Research Center; Singapore Management University	NSFC-Guangdong Joint Found; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Natural Science Foundation of Guangdong(National Natural Science Foundation of Guangdong Province); Guangdong High-level Personnel of Special Support Program; Science and Technology Planning Project of Guangzhou; Outstanding Young Scientific Research Talents International Cultivation Project Fund of Department of Education of Guangdong Province; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); United States Air Force(United States Department of Defense); NSF EAGER Grant(National Science Foundation (NSF)); Living Analytics Research Center; Singapore Management University(Singapore Management University)	This research was supported in part by NSFC-Guangdong Joint Found (U 1501254), Natural Science Foundation of China (61876043), Natural Science Foundation of Guangdong (2014A030306004, 2014A030308008), Guangdong High-level Personnel of Special Support Program (2015TQ01X140), Science and Technology Planning Project of Guangzhou(201902010058) and Outstanding Young Scientific Research Talents International Cultivation Project Fund of Department of Education of Guangdong Province(40190001). KZ would like to acknowledge the support by NIH under Contract No. NIH-iRO1EB022858-01, FAINRO1EB022858, NIH-iROiLM012087, NIH-5U54HG008540-02, and FAINU54HG008540, by the United States Air Force under Contract No. FA8650-17-C-7715, and by NSF EAGER Grant No. IIS-182968 1. The NIH, the U.S. Air Force, and the NSF are not responsible for the views reported here. KZ also benefited from funding from Living Analytics Research Center and Singapore Management University. Feng would like to thank Shohei Shimizu for his insightful discussions and suggestions on the original draft. We appreciate the comments from anonymous reviewers, which greatly helped to improve the paper.	Bartholomew David, 2008, ANAL INTERPRETATION; Chen Bryant, 2017, P 34 INT C MACH LEAR, V70, P757; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Colombo D, 2012, ANN STAT, V40, P294, DOI 10.1214/11-AOS940; CRAMER H, 1962, RANDOM VARIABLES PRO; Drton Mathias, P 20 C UNC ART INT, P130; Gretton A., 2008, ADV NEURAL INFORM PR, P585; Hoyer PO, 2008, INT J APPROX REASON, V49, P362, DOI 10.1016/j.ijar.2008.02.006; Huang BW, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1551, DOI 10.1145/3219819.3220104; Huang BW, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3561; Kagan A.M., 1973, CHARACTERIZATION PRO; Kummerfeld E, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1655, DOI 10.1145/2939672.2939838; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Shimizu S, 2009, NEUROCOMPUTING, V72, P2024, DOI 10.1016/j.neucom.2008.11.018; Silva R., 2005, P 22 INT C MACH LEAR, P808; Spearman C, 1928, B J PSYCHOL-GEN SECT, V19, P95, DOI 10.1111/j.2044-8295.1928.tb00500.x; Spirtes P., 1991, Social Science Computer Review, V9, P62, DOI 10.1177/089443939100900106; Spirtes P., 1995, Uncertainty in Artificial Intelligence. Proceedings of the Eleventh Conference (1995), P499; Spirtes P., 2013, P 29 C UNC ART INT B, P606; Spirtes P., 2000, CAUSATION PREDICTION; Spirtes Peter, 2016, Appl Inform (Berl), V3, P3; Sullivant S, 2010, ANN STAT, V38, P1665, DOI 10.1214/09-AOS760; Zhang K, 2017, IEEE ANTENNAS PROP, P1341, DOI 10.1109/APUSNCURSINRSM.2017.8072713; Zhang K, 2008, J MACH LEARN RES, V9, P2455	25	0	0	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904051
C	Cao, Y; Gu, QQ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cao, Yuan; Gu, Quanquan			Generalization Bounds of Stochastic Gradient Descent for Wide and Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the training and generalization of deep neural networks (DNNs) in the over-parameterized regime, where the network width (i.e., number of hidden nodes per layer) is much larger than the number of training data points. We show that, the expected 0-1 loss of a wide enough ReLU network trained with stochastic gradient descent (SGD) and random initialization can be bounded by the training loss of a random feature model induced by the network gradient at initialization, which we call a neural tangent randomfeature (NTRF) model. For data distributions that can be classified by NTRF model with sufficiently small error, our result yields a generalization error bound in the order of (O) over bar (n(-1/2)) that is independent of the network width. Our result is more general and sharper than many existing generalization error bounds for over-parameterized neural networks. In addition, we establish a strong connection between our generalization error bound and the neural tangent kernel (NTK) proposed in recent work.	[Cao, Yuan; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of California System; University of California Los Angeles	Cao, Y (corresponding author), Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA.	yuancao@cs.ucla.edu; qgu@cs.ucla.edu			National Science Foundation CAREER Award [IIS-1906169, IIS-1903202]; Salesforce Deep Learning Research Award	National Science Foundation CAREER Award(National Science Foundation (NSF)); Salesforce Deep Learning Research Award	We would like to thank Peter Bartlett for a valuable discussion, and Simon S. Du for pointing out a related work [3]. We also thank the anonymous reviewers and area chair for their helpful comments. This research was sponsored in part by the National Science Foundation CAREER Award IIS-1906169, IIS-1903202, and Salesforce Deep Learning Research Award. The views and conclusions contained in this paper are those of the authors and should not be interpreted as representing any funding agencies.	Allen-Zhu Zeyuan, 2018, ARXIV181104918; [Anonymous], ARXIV181103962; [Anonymous], 2018, ARXIV180801204; [Anonymous], 2017, ARXIV171206541; Arora Sanjeev, 2018, ARXIV180205296; Arora Sanjeev, 2019, ARXIV190108584; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Bartlett P. L., 2017, ADV NEURAL INFORM PR; Bengio Y, 2009, P NIPS 08 P 21 INT C; Brutzkus Alon, 2017, ARXIV PREPRINT ARXIV; CAO Y., 2019, ARXIV190201384; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; DANIELY A., 2017, ADV NEURAL INFORM PR; Du Simon S, 2018, GRADIENT DESCENT FIN; Du SS., 2019, P 7 INT C LEARN REPR; Dziugaite Gintare Karolina, 2017, ARXIV170311008; He K., 2016, IEEE C COMPUTER VISI; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Jacot A., 2018, ARXIV180607572; Jain P., 2019, ARXIV190412443; KRIZHEVSKY A., 2012, ADV NEURAL INFORMATI; LANGFORD J., 2002, ADV NEURAL INFORM PR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J., 2019, ARXIV190206720; Li X., 2018, ARXIV180605159; Long Philip M, 2019, ARXIV190512600; NEYSHABUR B., 2018, ROLE PARAMETRIZATION; Neyshabur B, 2015, C LEARN THEOR; Neyshabur Behnam, 2017, ARXIV170709564; Oymak S., 2019, ARXIV190204674; Rahimi A, 2007, NEURAL INFORM PROCES; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Wei Colin, 2018, ARXIV181005369; Wu L., 2019, ARXIV190404326; Yang G., 2019, ARXIV190204760; Yehudai Gilad, 2019, ARXIV190400687; Zhang Chiyuan, 2016, ARXIV161103530; Zou D, 2018, ARXIV181108888	39	0	0	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902046
C	Carrington, R; Bharath, K; Preston, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Carrington, Rachel; Bharath, Karthik; Preston, Simon			Invariance and identifiability issues for word embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Word embeddings are commonly obtained as optimizers of a criterion function f of a text corpus, but assessed on word-task performance using a different evaluation function. of the test data. We contend that a possible source of disparity in performance on tasks is the incompatibility between classes of transformations that leave f and g invariant. In particular, word embeddings defined by f are not unique; they are defined only up to a class of transformations to which f is invariant, and this class is larger than the class to which g is invariant. One implication of this is that the apparent superiority of one word embedding over another, as measured by word task performance, may largely be a consequence of the arbitrary elements selected from the respective solution sets. We provide a formal treatment of the above identifiability issue, present some numerical examples, and discuss possible resolutions.	[Carrington, Rachel; Bharath, Karthik; Preston, Simon] Univ Nottingham, Sch Math Sci, Nottingham, England	University of Nottingham	Carrington, R (corresponding author), Univ Nottingham, Sch Math Sci, Nottingham, England.	rachel.carrington@nottingham.ac.uk; karthik.bharath@nottingham.ac.uk; simon.preston@nottingham.ac.uk		Carrington, Rachel/0000-0002-1537-609X	NSF [DMS 1613054]; NIH [RO1 CA214955]; Bloomberg Data Science Research Grant; EPSRC PhD studentship	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Bloomberg Data Science Research Grant; EPSRC PhD studentship(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors gratefully acknowledge support for this work from grants NSF DMS 1613054 and NIH RO1 CA214955 (KB), a Bloomberg Data Science Research Grant (KB & SP), and an EPSRC PhD studentship (RC).	Bullinaria JA, 2012, BEHAV RES METHODS, V44, P890, DOI 10.3758/s13428-011-0183-8; Davies M, 2012, CORPORA, V7, P121, DOI 10.3366/cor.2012.0024; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; Eckart C, 1936, PSYCHOMETRIKA, V1, P211, DOI 10.1007/BF02288367; Finkelstein L, 2002, ACM T INFORM SYST, V20, P116, DOI 10.1145/503104.503110; Hill F, 2015, COMPUT LINGUIST, V41, P665, DOI 10.1162/COLI_a_00237; Levy Omer, T ASS COMPUTATIONAL, V3, P211; Mikolov T., 2013, ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mu Cun, 2019, ARXIV180400306V2; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Turney P.D., 2013, T ASSOC COMPUT LING, V1, P353, DOI [10.1162/tacl_a_00233, DOI 10.1162/tacl_a_00233]; Yin Z, 2018, ADV NEUR IN, V31	13	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906076
C	Casgrain, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Casgrain, Philippe			A Latent Variational Framework for Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUBGRADIENT METHODS	This paper provides a unifying theoretical framework for stochastic optimization algorithms by means of a latent stochastic variational problem. Using techniques from stochastic control, the solution to the variational problem is shown to be equivalent to that of a Forward Backward Stochastic Differential Equation (FBSDE). By solving these equations, we recover a variety of existing adaptive stochastic gradient descent methods. This framework establishes a direct connection between stochastic optimization algorithms and a secondary latent inference problem on gradients, where a prior measure on gradient observations determines the resulting algorithm.	[Casgrain, Philippe] Univ Toronto, Dept Stat Sci, Toronto, ON, Canada	University of Toronto	Casgrain, P (corresponding author), Univ Toronto, Dept Stat Sci, Toronto, ON, Canada.	p.casgrain@mail.utoronto.ca		Casgrain, Philippe/0000-0002-3230-950X				Aitchison L, 2018, ARXIV180707540; Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Bensoussan A., 2004, STOCHASTIC CONTROL P; Carmona R, 2016, FINANC MATH, P1; Casgrain P., 2018, ARXIV180304094; Casgrain Philippe, 2018, ARXIV180604472; Casgrain Philippe, 2018, ARXIV181006101; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Da Silva A. B., 2018, ARXIV181013108; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gupta V., 2017, ARXIV170606569; Jacod Jean, 2013, LIMIT THEOREMS STOCH, V288; Jankovic S, 2012, MATH COMPUT MODEL, V55, P1734, DOI 10.1016/j.mcm.2011.11.018; Kingma D.P, P 3 INT C LEARNING R; Krichene W., 2015, ADV NEURAL INFORM PR, P2845; Krichene W., 2017, ADV NEURAL INFORM PR, V30, P6796; Lucas J., 2018, P 6 INT C LEARN REPR, P6; Ma J., 1999, FORWARD BACKWARD STO; Mertikopoulos P, 2018, SIAM J OPTIMIZ, V28, P163, DOI 10.1137/16M1105682; Nemirovskij Arkadij Semenovic, 1983, PROBLEM COMPLEXITY M; Nesterov Y., SOV MATH DOKL, V27; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Raginsky M, 2012, IEEE DECIS CONTR P, P6793, DOI 10.1109/CDC.2012.6426639; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Ruder S., 2016, ARXIV160904747; Van Handel R., 2007, COURSE NOTES; Vuckovic J., 2018, ARXIV181012273; Walrand J, 2006, RANDOM PROCESSES SYS; Wibisono Andre, 2014, P NATL ACAD SCI USA, V113, pE7351; Wilson A. C., 2016, ARXIV161102635; Xu Pan, 2018, INT C ART INT STAT, P1087; Yin FL, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON MECHANICAL, CONTROL AND COMPUTER ENGINEERING (ICMCCE), P488, DOI 10.1109/ICMCCE.2018.00109	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305062
C	Chaabouni, R; Kharitonov, E; Dupoux, E; Baroni, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chaabouni, Rahma; Kharitonov, Eugene; Dupoux, Emmanuel; Baroni, Marco			Anti-efficient encoding in emergent communication	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMPRESSION; PRINCIPLE	Despite renewed interest in emergent language simulations with neural networks, little is known about the basic properties of the induced code, and how they compare to human language. One fundamental characteristic of the latter, known as Zipf's Law of Abbreviation (ZLA), is that more frequent words are efficiently associated to shorter strings. We study whether the same pattern emerges when two neural networks, a "speaker" and a "listener", are trained to play a signaling game. Surprisingly, we find that networks develop an anti-efficient encoding scheme, in which the most frequent inputs are associated to the longest messages, and messages in general are skewed towards the maximum length threshold. This anti-efficient code appears easier to discriminate for the listener, and, unlike in human communication, the speaker does not impose a contrasting least-effort pressure towards brevity. Indeed, when the cost function includes a penalty for longer messages, the resulting message distribution starts respecting ZLA. Our analysis stresses the importance of studying the basic features of emergent communication in a highly controlled setup, to ensure the latter will not depart too far from human language. Moreover, we present a concrete illustration of how different functional pressures can lead to successful communication codes that lack basic properties of human language, thus highlighting the role such pressures play in the latter.	[Chaabouni, Rahma; Kharitonov, Eugene; Dupoux, Emmanuel; Baroni, Marco] Facebook AI Res, Menlo Pk, CA 94025 USA; [Chaabouni, Rahma; Dupoux, Emmanuel] PSL Res Univ, Cognit Machine Learning ENS, EHESS, CNRS,INRIA, Paris, France; [Baroni, Marco] ICREA, Barcelona, Spain	Facebook Inc; Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Universite Paris Cite; ICREA	Chaabouni, R (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.; Chaabouni, R (corresponding author), PSL Res Univ, Cognit Machine Learning ENS, EHESS, CNRS,INRIA, Paris, France.	rchaabouni@fb.com; kharitonov@fb.com; dpx@fb.com; mbaroni@fb.com		Dupoux, Emmanuel/0000-0002-7814-2952				Baayen Harald., 1993, WORD FREQUENCY DISTR; Bouchacourt D., 2018, P 2018 C EMP METH NA, P981, DOI DOI 10.18653/V1/D18-1119; Chaabouni R, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5166; Cover TM, 2006, ELEMENTS INFORM THEO; David Lewis, 1969, CONVENTION PHILOS ST; Evtimova Katrina, 2018, P ICLR C TRACK VANC; FERRER I, 2011, J STAT MECH-THEORY E, V2011; Ferrer i Cancho Ramon, 2007, J STAT MECH-THEORY E, V2007; Ferrer-i-Cancho R, 2013, COGNITIVE SCI, V37, P1565, DOI 10.1111/cogs.12061; George Zipf, 1949, HUMAN BEHAV PRINCIPL; Gibson Edward, 2019, TRENDS COGNITIVE SCI; Graesser Laura, 2019, EMERGENT LINGUISTIC; Havrylov S., 2017, ADV SYSTEMS NEURAL I, P2149; Heesen R, 2019, P ROY SOC B-BIOL SCI, V286, DOI 10.1098/rspb.2018.2900; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Jang Eric, 2017, P 5 INT C LEARN REPR; Kanwal J, 2017, COGNITION, V165, P45, DOI 10.1016/j.cognition.2017.05.001; Kharitonov E., 2019, ARXIV190700852; Kingma D.P, P 3 INT C LEARNING R; Kirby S, 2001, IEEE T EVOLUT COMPUT, V5, P102, DOI 10.1109/4235.918430; Kottur Satwik, 2017, P 2017 C EMP METH NA, P2962, DOI DOI 10.18653/V1/D17-1321.URL; Lazaridou Angeliki, 2017, P ICLR C TRACK TOUL; Lazaridou Angeliki, 2018, P ICLR C TRACK VANC; Lee Jason, 2018, P ICLR C TRACK VANC; Liou CY, 2014, NEUROCOMPUTING, V139, P84, DOI 10.1016/j.neucom.2013.09.055; Lowe R, 2019, AAMAS '19: PROCEEDINGS OF THE 18TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P693; Maddison Chris J, 2016, ARXIV161100712; Mahowald K, 2018, COGNITIVE SCI, V42, P3116, DOI 10.1111/cogs.12689; Mandelbrot B., 1954, T IRE, V3, P124; Martin Fermin Moscoso del Prado, 2013, P ANN M COGN SCI SOC, V35; MILLER GA, 1957, AM J PSYCHOL, V70, P311, DOI 10.2307/1419346; Piantadosi ST, 2011, P NATL ACAD SCI USA, V108, P3526, DOI 10.1073/pnas.1012551108; Schulman J., 2015, ARXIV PREPRINT ARXIV; Sigurd B, 2004, STUD LINGUISTICA, V58, P37, DOI 10.1111/j.0039-3193.2004.00109.x; SIMON HA, 1955, BIOMETRIKA, V42, P425; Strauss U., 2007, CONTRIBUTIONS SCI TE, ppp. 277, DOI DOI 10.1007/978-1-4020-4068-9; Teahan WJ, 2000, COMPUT LINGUIST, V26, P375, DOI 10.1162/089120100561746; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	39	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306031
C	Chang, YC; Roohi, N; Gao, SC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chang, Ya-Chien; Roohi, Nima; Gao, Sicun			Neural Lyapunov Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose new methods for learning control policies and neural network Lyapunov functions for nonlinear control problems, with provable guarantee of stability. The framework consists of a learner that attempts to find the control and Lyapunov functions, and a falsifier that finds counterexamples to quickly guide the learner towards solutions. The procedure terminates when no counterexample is found by the falsifier, in which case the controlled nonlinear system is provably stable. The approach significantly simplifies the process of Lyapunov control design, provides end-to-end correctness guarantee, and can obtain much larger regions of attraction than existing methods such as LQR and SOS/SDP. We show experiments on how the new methods obtain high-quality solutions for challenging robot control problems such as path tracking for wheeled vehicles and humanoid robot balancing.	[Chang, Ya-Chien; Roohi, Nima; Gao, Sicun] UCSD, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Chang, YC (corresponding author), UCSD, La Jolla, CA 92093 USA.	yac021@eng.ucsd.edu; nroohi@eng.ucsd.edu; sicung@eng.ucsd.edu						Ahmadi Amir A., 2013, ABS13086833 CORR; Ahmadi Amir A., 2015, ABS150403761 CORR; Ahmadi Amir A., 2011 50 IEEE C DEC C; Ahmadi AA, 2012, P AMER CONTR CONF, P3334; Amodei D., 2016, ABS160606565 CORR; Berkenkamp F, 2016, IEEE DECIS CONTR P, P4661, DOI 10.1109/CDC.2016.7798979; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Chang Ya-Chien, NEURAL LYAPUNOV CONT; Chesi G, 2009, IEEE T AUTOMAT CONTR, V54, P935, DOI 10.1109/TAC.2009.2015979; Chow Y., 2018, ADV NEURAL INFORM PR, P8092; Gao L, 2012, 2012 IEEE INTERNATIONAL SYMPOSIUM ON MULTIMEDIA (ISM), P286, DOI 10.1109/ISM.2012.60; Haddad Wassim, 2008, NONLINEAR DYNAMICAL, V1; Henrion D., 2005, LECT NOTES CONTROL I, V312; Jarvis-Wloszek Z, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P4676, DOI 10.1109/CDC.2003.1272309; KAPINSKI J, 2014, P 17 INT C HYBR SYST, P133, DOI DOI 10.1145/2562059.2562139; Kwakernaak H., 1972, LINEAR OPTIMAL CONTR; Lofberg J, 2009, IEEE T AUTOMAT CONTR, V54, P1007, DOI 10.1109/TAC.2009.2017144; Majumdar A, 2017, INT J ROBOT RES, V36, P947, DOI 10.1177/0278364917712421; Mania Horia, 2018, ADV NEURAL INFORM PR, V31, P1805; Parrilo PA., 2000, THESIS CALTECH; Rasmussen C., 2006, ADAPTATIVE COMPUTATI; Ravanbakhsh H, 2019, AUTON ROBOT, V43, P275, DOI 10.1007/s10514-018-9791-9; Richards SM, 2018, P 2 C ROBOT LEARNING, V87, P466; Sicun Gao, 2013, Automated Deduction - CADE-24. 24th International Conference on Automated Deduction. Proceedings: LNCS 7898, P208, DOI 10.1007/978-3-642-38574-2_14; Tedrake Russ, 2019, COURSE NOTES MIT	25	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303026
C	Char, I; Chung, Y; Neiswanger, W; Kandasamy, K; Nelson, AO; Boyer, MD; Kolemen, E; Schneider, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Char, Ian; Chung, Youngseog; Neiswanger, Willie; Kandasamy, Kirthevasan; Nelson, Andrew Oakleigh; Boyer, Mark D.; Kolemen, Egemen; Schneider, Jeff			Offline Contextual Bayesian Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In black-box optimization, an agent repeatedly chooses a configuration to test, so as to find an optimal configuration. In many practical problems of interest, one would like to optimize several systems, or "tasks", simultaneously; however, in most of these scenarios the current task is determined by nature. In this work, we explore the "offline" case in which one is able to bypass nature and choose the next task to evaluate (e.g. via a simulator). Because some tasks may be easier to optimize and others may be more critical, it is crucial to leverage algorithms that not only consider which configurations to try next, but also which tasks to make evaluations for. In this work, we describe a theoretically grounded Bayesian optimization method to tackle this problem. We also demonstrate that if the model of the reward structure does a poor job of capturing variation in difficulty between tasks, then algorithms that actively pick tasks for evaluation may end up doing more harm than good. Following this, we show how our approach can be used for real world applications in science and engineering, including optimizing tokamak controls for nuclear fusion.	[Char, Ian; Chung, Youngseog; Neiswanger, Willie; Schneider, Jeff] Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA; [Kandasamy, Kirthevasan] Univ Calif Berkeley, Dept EECS, Berkeley, CA 94720 USA; [Nelson, Andrew Oakleigh; Boyer, Mark D.; Kolemen, Egemen] Princeton Plasma Phys Lab, POB 451, Princeton, NJ 08543 USA	Carnegie Mellon University; University of California System; University of California Berkeley; Princeton University; United States Department of Energy (DOE); Princeton Plasma Physics Laboratory	Char, I (corresponding author), Carnegie Mellon Univ, Dept Machine Learning, Pittsburgh, PA 15213 USA.	ichar@cs.cnu.edu; youngsec@cs.cnu.edu; willie@cs.cnu.edu; kandasany@eecs.berkeley.edu; anelson@pppl.gov; nboyer@pppl.gov; ekolenen@pppl.gov; schneide@cs.cnu.edu	Boyer, Mark/AAW-1372-2021; Nelson, Andrew Oakleigh/AAA-2582-2022	Boyer, Mark/0000-0002-6845-9155; Nelson, Andrew Oakleigh/0000-0002-9612-1936	National Science Foundation Graduate Research Fellowship Program [DGE1252522, DGE1745016]; NSF [CCF1629559, IIS1563887]; Kwanjeong Educational Foundation	National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); Kwanjeong Educational Foundation	This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1252522 and DGE1745016. Willie Neiswanger is also supported by NSF grants CCF1629559 and IIS1563887. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.; Youngseog Chung is supported by the Kwanjeong Educational Foundation.	Agrawal S., 2013, ICML 3, P127, DOI DOI 10.5555/3042817.3043073; [Anonymous], 2009, ARXIV PREPRINT ARXIV; [Anonymous], 2000, MUSTERERKENNUNG 2000; Auer P., 2002, J MACHINE LEARNING R, V3, P397, DOI [10.5555/944919.944941, DOI 10.4271/610369]; Baltz EA, 2017, SCI REP-UK, V7, DOI 10.1038/s41598-017-06645-7; BRANIN FH, 1972, IBM J RES DEV, V16, P504, DOI 10.1147/rd.165.0504; Bubeck S, 2011, J MACH LEARN RES, V12, P1655; Cannas B, 2013, NUCL FUSION, V53, DOI 10.1088/0029-5515/53/9/093023; Carpenter B, 2017, J STAT SOFTW, V76, P1, DOI 10.18637/jss.v076.i01; Chung Y., 2020, ARXIV200101793; Fabisch A, 2015, KUNSTL INTELL, V29, P369, DOI 10.1007/s13218-015-0363-2; Fabisch A, 2014, J MACH LEARN RES, V15, P3371; Frazier P.I., 2018, ARXIV, DOI 10.1287/educ.2018.0188; Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314; Gibbs MN, 1998, THESIS; Ginsbourger D, 2014, SIAM-ASA J UNCERTAIN, V2, P490, DOI 10.1137/130949555; Golovin Daniel, 1998, J ARTIFICIAL INTELLI, V42, P427; Grierson BA, 2018, FUSION SCI TECHNOL, V74, P101, DOI 10.1080/15361055.2017.1398585; Guestrin C., 2005, P 22 INT C MACH LEAR, P265, DOI DOI 10.1145/1102351.1102385; Kandasamy K., 2019, ARXIV190306694; Kandasamy K, 2018, PR MACH LEARN RES, V84; Kandasamy Kirthevasan, 2019, P 36 INT C MACH LEAR; Kates-Harbeck Julian, 2019, NATURE, P1; Metzen J. H., 2015, ARXIV151104211; Montes Kevin Joseph, 2019, NUCL FUSION; Neiswanger Willie, 2019, ARXIV PREPRINT ARXIV; Ong C. S., 2011, ADV NEURAL INFORM PR, P2447; Pasolli E, 2011, INT GEOSCI REMOTE SE, P3574, DOI 10.1109/IGARSS.2011.6049994; Pearce M, 2018, EUR J OPER RES, V270, P1074, DOI 10.1016/j.ejor.2018.03.017; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Russo D, 2016, J MACH LEARN RES, V17; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Tang W., 2016, 26 IAEA FUS EN C IAE; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Toscano-Palmerin S., 2018, ARXIV180308661; Van Aken D, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P1009, DOI 10.1145/3035918.3064029	39	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304061
C	Charikar, M; Shiragur, K; Sidford, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Charikar, Moses; Shiragur, Kirankumar; Sidford, Aaron			A General Framework for Symmetric Property Estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MINIMAX ESTIMATION; SUPPORT SIZE; UNSEEN	In this paper we provide a general framework for estimating symmetric properties of distributions from i.i.d. samples. For a broad class of symmetric properties we identify the easy region where empirical estimation works and the difficult region where more complex estimators are required. We show that by approximately computing the profile maximum likelihood (PML) distribution [ADOS16] in this difficult region we obtain a symmetric property estimation framework that is sample complexity optimal for many properties in a broader parameter regime than previous universal estimation approaches based on PML. The resulting algorithms based on these pseudo PML distributionsare also more practical.	[Charikar, Moses; Shiragur, Kirankumar; Sidford, Aaron] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Charikar, M (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	moses@cs.stanford.edu; shiragur@stanford.edu; sidford@stanford.edu			Simons Investigator Award; Google Faculty Research Award; Amazon Research Award; NSF CAREER Award [CCF-1844855]	Simons Investigator Award; Google Faculty Research Award(Google Incorporated); Amazon Research Award; NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD))	We thank the reviewers for the helpful comments, great suggestions, and positive feedback. Moses Charikar was supported by a Simons Investigator Award, a Google Faculty Research Award and an Amazon Research Award. Aaron Sidford was partially supported by NSF CAREER Award CCF-1844855.	Acharya Jayadev, 2016, ABS161102960 CORR; [Anonymous], 2014, P 26 ANN ACM SIAM S, DOI DOI 10.1137/1.9781611973730.124; Cai TT, 2011, ANN STAT, V39, P1012, DOI 10.1214/10-AOS849; Charikar Moses, 2019, ARXIV E PRINTS; EFRON B, 1976, BIOMETRIKA, V63, P435, DOI 10.2307/2335721; HAN Y, 2018, ARXIV180208405; Han Yanjun, 2017, ARXIV E PRINTS; Han Yanjun, 2017, ARXIV E PRINTS; Hao Y., 2018, ADV NEURAL INFORM PR, P8834; Hao Yi, 2019, BROAD OPTIMALITY PRO; Hao Yi, 2019, DATA AMPLIFICATION I; Jiao JT, 2016, IEEE INT SYMP INFO, P750; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Jiao Jiantao, 2017, ARXIV E PRINTS; Nemirovski A, 2003, 42ND IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-6, PROCEEDINGS, P2419; Orlitsky A, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P306; Orlitsky A, 2016, P NATL ACAD SCI USA, V113, P13283, DOI 10.1073/pnas.1607774113; Pavlichin D. S., 2017, ARXIV E PRINTS; Raghunathan Aditi, 2017, ABS170703854 CORR; Raskhodnikova S, 2007, ANN IEEE SYMP FOUND, P559, DOI 10.1109/FOCS.2007.47; Timan A. F., 2014, THEORY APPROXIMATION, V34; Valiant G, 2011, ACM S THEORY COMPUT, P685; Valiant G, 2011, ANN IEEE SYMP FOUND, P403, DOI 10.1109/FOCS.2011.81; Wu Y., 2015, ARXIV E PRINTS; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468; Wu Yihong, 2016, ARXIV E PRINTS; Zou James, 2016, NATURE COMMUNICATION, V7, P13293	27	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904013
C	Chen, L; Esfandiari, H; Fu, T; Mirrokni, VS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Lin; Esfandiari, Hossein; Fu, Thomas; Mirrokni, Vahab S.			Locality-Sensitive Hashing for f-Divergences and Kre.in Kernels: Mutual Information Loss and Beyond	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Computing approximate nearest neighbors in high dimensional spaces is a central problem in large-scale data mining with a wide range of applications in machine learning and data science. A popular and effective technique in computing nearest neighbors approximately is the locality-sensitive hashing (LSH) scheme. In this paper, we aim to develop LSH schemes for distance functions that measure the distance between two probability distributions, particularly for f-divergences as well as a generalization to capture mutual information loss. First, we provide a general framework to design LHS schemes for f-divergence distance functions and develop LSH schemes for the generalized Jensen-Shannon divergence and triangular discrimination in this framework. We show a two-sided approximation result for approximation of the generalized Jensen-Shannon divergence by the Hellinger distance, which may be of independent interest. Next, we show a general method of reducing the problem of designing an LSH scheme for a Kre.in kernel (which can be expressed as the difference of two positive definite kernels) to the problem of maximum inner product search. We exemplify this method by applying it to the mutual information loss, due to its several important applications such as model compression.	[Chen, Lin] Yale Univ, New Haven, CT 06520 USA; [Chen, Lin; Esfandiari, Hossein; Fu, Thomas; Mirrokni, Vahab S.] Google Res, Mountain View, CA 94043 USA	Yale University; Google Incorporated	Chen, L (corresponding author), Yale Univ, New Haven, CT 06520 USA.; Chen, L (corresponding author), Google Res, Mountain View, CA 94043 USA.	lin.chen@yale.edu; esfandiari@google.com; thomasfu@google.com; mirrokni@google.com	Chen, Lin/CAH-1961-2022	Chen, Lin/0000-0003-0349-6577				Abdelkader Ahmed, 2019, SODA, P355; Abdullah A, 2016, JMLR WORKSH CONF PRO, V51, P948; Abdullah A, 2015, ACM S THEORY COMPUT, P509, DOI 10.1145/2746539.2746595; Abdullah Amirali, 2012, P 28 ANN S COMP GEOM, P31; ALI SM, 1966, J ROY STAT SOC B, V28, P131; [Anonymous], 1981, CONTRIBUTIONS PROBAB; Bateni MohammadHossein, 2019, ICML; Bhaskara A, 2018, PR MACH LEARN RES, V80; Bishop C.M, 2006, PATTERN RECOGN; Bochner S., 1959, ANN MATH STUDIES; Broder AZ, 1998, COMPRESSION AND COMPLEXITY OF SEQUENCES 1997 - PROCEEDINGS, P21, DOI 10.1109/SEQUEN.1997.666900; Charikar M.S., 2002, P 34 ANN ACM S THEOR, V34, P380, DOI DOI 10.1145/509907.509965; Chaudhuri K., 2008, COLT, V8, P10; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Daskalakis Constantinos, 2017, P 30 C LEARNING THEO, P697; Datar M., 2004, P ACM 20 ANN S COMP, P253; Dhillon I. S., 2003, Journal of Machine Learning Research, V3, P1265, DOI 10.1162/153244303322753661; Gorisse D, 2012, IEEE T PATTERN ANAL, V34, P402, DOI 10.1109/TPAMI.2011.193; Han X., 2017, ARXIVCSLGCSLG1708077; Indyk P., 1998, Proceedings of the Thirtieth Annual ACM Symposium on Theory of Computing, P604, DOI 10.1145/276698.276876; Kartowsky A, 2019, IEEE T INFORM THEORY, V65, P917, DOI 10.1109/TIT.2018.2879802; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P6; Le Cam L., 2012, ASYMPTOTIC METHODS S; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li P, 2014, PR MACH LEARN RES, V32, P676; LIN JH, 1991, IEEE T INFORM THEORY, V37, P145, DOI 10.1109/18.61115; Mao XL, 2017, AAAI CONF ARTIF INTE, P3244; Mu YD, 2010, AAAI CONF ARTIF INTE, P539; Neyshabur B, 2015, PR MACH LEARN RES, V37, P1926; Ong CS, 2004, P 21 INT C MACH LEAR, P81; Sakai Y, 2014, 2014 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA), P120; Sason I, 2016, IEEE T INFORM THEORY, V62, P5973, DOI 10.1109/TIT.2016.2603151; Shrivastava Anshumali, 2014, ADV NEURAL INFORM PR, P2321; Terasawa K, 2007, LECT NOTES COMPUT SC, V4619, P27; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wang J., 2014, HASHING SIMILARITY S; Wang JD, 2018, IEEE T PATTERN ANAL, V40, P769, DOI 10.1109/TPAMI.2017.2699960; Yagnik J, 2011, IEEE I CONF COMP VIS, P2431, DOI 10.1109/ICCV.2011.6126527; Yan X., 2018, ADV NEURAL INFORM PR, V31, P2952; Zhang J, 2016, PROCEEDINGS OF 2016 INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY AND ITS APPLICATIONS (ISITA 2016), P448; [No title captured]	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901065
C	Chen, RD; Paschalidis, IC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Ruidi; Paschalidis, Ioannis Ch.			Selecting Optimal Decisions via Distributionally Robust Nearest-Neighbor Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper develops a prediction-based prescriptive model for optimal decision making that (i) predicts the outcome under each action using a robust nonlinear model, and (ii) adopts a randomized prescriptive policy determined by the predicted outcomes. The predictive model combines a new regularized regression technique, which was developed using Distributionally Robust Optimization (DRO) with an ambiguity set constructed from the Wasserstein metric, with the K-Nearest Neighbors (K-NN) regression, which helps to capture the nonlinearity embedded in the data. We show theoretical results that guarantee the out-of-sample performance of the predictive model, and prove the optimality of the randomized policy in terms of the expected true future outcome. We demonstrate the proposed methodology on a hypertension dataset, showing that our prescribed treatment leads to a larger reduction in the systolic blood pressure compared to a series of alternatives. A clinically meaningful threshold level used to activate the randomized policy is also derived under a sub-Gaussian assumption on the predicted outcome.	[Chen, Ruidi] Boston Univ, Div Syst Engn, Boston, MA 02215 USA; [Paschalidis, Ioannis Ch.] Boston Univ, Dept Elect & Comp Engn, Div Syst Engn, Boston, MA 02215 USA; [Paschalidis, Ioannis Ch.] Boston Univ, Dept Biomed Engn, Boston, MA 02215 USA	Boston University; Boston University; Boston University	Chen, RD (corresponding author), Boston Univ, Div Syst Engn, Boston, MA 02215 USA.	rchen15@bu.edu; yannisp@bu.edu	Chen, Ruidi/AAZ-3412-2021		NSF [IIS-1914792, DMS-1664644, CNS-1645681]; ONR [N00014-19-1-2571]; NIH [1R01GM135930]; Boston University Data Science Initiative; BU Center for Information and Systems Engineering	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Boston University Data Science Initiative; BU Center for Information and Systems Engineering	The research was partially supported by the NSF under grants IIS-1914792, DMS-1664644, and CNS-1645681, by the ONR under grant N00014-19-1-2571, by the NIH under grant 1R01GM135930, by the Boston University Data Science Initiative, and by the BU Center for Information and Systems Engineering.	ALTMAN NS, 1992, AM STAT, V46, P175, DOI 10.2307/2685209; [Anonymous], 2014, ARXIV14025481; [Anonymous], 2016, FDN TRENDS IN OPTIMI; Bastani H, 2015, ONLINE DECISION MAKI; Bertsimas D., 2017, 171109974 ARXIV; Bertsimas Dimitris, 2018, INFORM J OPTIMIZATIO; Bertsimas Dimitris, 2018, ARXIV180704183; Bertsimas Dimitris, 2016, DIABETES CARE; Biggs M, 2018, OPTIMIZING OBJECTIVE; Bravo Fernanda, 2017, DISCOVERING OPTIMAL; Breiman L., 2017, CLASSIFICATION REGRE, DOI DOI 10.1201/9781315139470; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Chen RD, 2018, J MACH LEARN RES, V19; Chu W., 2011, P 14 INT C ART INT S, V15, P208; CLEVELAND WS, 1988, J AM STAT ASSOC, V83, P596, DOI 10.2307/2289282; den Hertog D., 2016, BRIDGING GAP PREDICT, DOI DOI 10.1016/j.neucom.2017.01.026; Dunn J. W., 2018, THESIS; Esfahani P. M., 2017, MATH PROGRAM, V171, P1; Gao R., 2016, MATH OPER RES; Gottlieb LA, 2017, IEEE T INFORM THEORY, V63, P4838, DOI 10.1109/TIT.2017.2713820; Sen Suvrajeet, 2018, INFORM J OPTIM UNPUB; Tewari A., 2017, MOBILE HLTH, P495; Wu HS, 2015, ADV NEUR IN, V28; Xia Isaac, 2018, THESIS; Zhu FY, 2018, ACM-BCB'18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON BIOINFORMATICS, COMPUTATIONAL BIOLOGY, AND HEALTH INFORMATICS, P492, DOI 10.1145/3233547.3233553	27	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300068
C	Chen, S; Luo, L; Yang, J; Gong, C; Li, J; Huang, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Shuo; Luo, Lei; Yang, Jian; Gong, Chen; Li, Jun; Huang, Heng			Curvilinear Distance Metric Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Distance Metric Learning aims to learn an appropriate metric that faithfully measures the distance between two data points. Traditional metric learning methods usually calculate the pairwise distance with fixed distance functions (e.g., Euclidean distance) in the projected feature spaces. However, they fail to learn the underlying geometries of the sample space, and thus cannot exactly predict the intrinsic distances between data points. To address this issue, we first reveal that the traditional linear distance metric is equivalent to the cumulative arc length between the data pair's nearest points on the learned straight measurer lines. After that, by extending such straight lines to general curved forms, we propose a Curvilinear Distance Metric Learning (CDML) method, which adaptively learns the nonlinear geometries of the training data. By virtue of Weierstrass theorem, the proposed CDML is equivalently parameterized with a 3-order tensor, and the optimization algorithm is designed to learn the tensor parameter. Theoretical analysis is derived to guarantee the effectiveness and soundness of CDML. Extensive experiments on the synthetic and real-world datasets validate the superiority of our method over the state-of-the-art metric learning models.	[Chen, Shuo; Yang, Jian; Gong, Chen] Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Sch Comp Sci & Engn,PCA Lab, Nanjing 210094, Peoples R China; [Chen, Shuo; Yang, Jian; Gong, Chen] Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China; [Luo, Lei; Huang, Heng] Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA; [Luo, Lei; Huang, Heng] JD Finance Amer Corp, Mountain View, CA USA; [Li, Jun] MIT, Inst Med Engn & Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Nanjing University of Science & Technology; Nanjing University of Science & Technology; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Massachusetts Institute of Technology (MIT)	Yang, J (corresponding author), Nanjing Univ Sci & Technol, Key Lab Intelligent Percept & Syst High Dimens In, Minist Educ, Sch Comp Sci & Engn,PCA Lab, Nanjing 210094, Peoples R China.; Yang, J (corresponding author), Nanjing Univ Sci & Technol, Sch Comp Sci & Engn, Jiangsu Key Lab Image & Video Understanding Socia, Nanjing 210094, Peoples R China.; Luo, L (corresponding author), Univ Pittsburgh, Elect & Comp Engn, Pittsburgh, PA 15260 USA.	shuochen@njust.edu.cn; le194@pitt.edu; csjyang@njust.edu.cn; chen.gong@njust.edu.cn; junli@mit.edu; henghuanghh@gmail.com			National Science Fund (NSF) of China [U1713208, 61602246, 61973162]; "111" Program [AH92005]; Fundamental Research Funds for the Central Universities [30918011319]; NSF of Jiangsu Province [BK20171430]; "Young Elite Scientists Sponsorship Program" by Jiangsu Province; "Young Elite Scientists Sponsorship Program" by CAST [2018QNRC001]; Program for Changjiang Scholars; U.S. NSF-IIS [1836945]; NSF-IIS [1836938, 1845666, 1852606, 1838627, 1837956]; NSF-DBI [1836866]	National Science Fund (NSF) of China; "111" Program(Ministry of Education, China - 111 Project); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities); NSF of Jiangsu Province; "Young Elite Scientists Sponsorship Program" by Jiangsu Province; "Young Elite Scientists Sponsorship Program" by CAST; Program for Changjiang Scholars(Program for Changjiang Scholars & Innovative Research Team in University (PCSIRT)); U.S. NSF-IIS; NSF-IIS(National Science Foundation (NSF)); NSF-DBI(National Science Foundation (NSF))	S.C., J.Y., and C.G. were supported by the National Science Fund (NSF) of China under Grant (Nos: U1713208, 61602246, and 61973162), Program for Changjiang Scholars, "111" Program AH92005, the Fundamental Research Funds for the Central Universities (No: 30918011319), NSF of Jiangsu Province (No: BK20171430), the "Young Elite Scientists Sponsorship Program" by Jiangsu Province, and the "Young Elite Scientists Sponsorship Program" by CAST (No: 2018QNRC001).; L.L. and H.H. were supported by U.S. NSF-IIS 1836945, NSF-IIS 1836938, NSF-DBI 1836866, NSF-IIS 1845666, NSF-IIS 1852606, NSF-IIS 1838627, and NSF-IIS 1837956.	[Anonymous], 2016, ICML; Asuncion A, 2007, UCI MACHINE LEARNING; Balasubramanian Krishnakumar, 2018, NEURIPS; Brown M, 2011, IEEE T PATTERN ANAL, V33, P43, DOI 10.1109/TPAMI.2010.54; Chen Shuo, 2018, IJCAI; Chen Tian Qi, 2018, NEURIPS; Cheung W, 2009, IEEE T IMAGE PROCESS, V18, P2012, DOI 10.1109/TIP.2009.2024578; cNamee John M, 2013, NEWNES; Dhillon I. S., 2007, ICML; Duan YZ, 2019, CONSERV GENET RESOUR, V11, P249, DOI 10.1007/s12686-017-0979-7; Gangopadhyay A, 2002, ADV TOP GLO, V1, P1; Hauberg Soren, 2012, NEURIPS; Hu J, 2014, CVPR; Huang Feihu, 2019, AAAI; Huang Feihu, 2019, ICML; Huang ZR, 2017, CEREB CORTEX, V27, P1037, DOI 10.1093/cercor/bhv288; Huo Z., 2016, KDD; Landwehr Niels, 2005, MACHINE LEARNING; Meyer C.D., 2000, MATRIX ANAL APPL LIN, V71; PaaBen Benjamin, 2018, ICML; Predd Joel B, 2009, IEEE T INFORM THEORY; Rudin W, 1964, PRINCIPLES MATH ANAL, V2nd; Sohn Kihyuk, 2016, NEURIPS; Sudrez Juan Luis, 2018, ARXIV181205944; Wang F., 2015, CVPR, V3, P4; Wang Jian, 2017, ICCV; Wang Jun, 2011, NEURIPS; Weinberger Kilian Q, 2006, NEURIPS; Xie PT, 2018, PR MACH LEARN RES, V80; Xu J., 2018, NEURIPS; Ye Han-Jia, 2016, NEURIPS; Zagoruyko Sergey, 2015, ICCV; Zhu Pengfei, 2018, IJCAI	33	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304025
C	Chen, WD; Wu, SH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Wei-Da; Wu, Shan-Hung			CNN2: Viewpoint Generalization via a Binocular Vision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				V2; CONNECTIONS	The Convolutional Neural Networks (CNNs) have laid the foundation for many techniques in various applications. Despite achieving remarkable performance in some tasks, the 3D viewpoint generalizability of CNNs is still far behind humans visual capabilities. Although recent efforts, such as the Capsule Networks, have been made to address this issue, these new models are either hard to train and/or incompatible with existing CNN-based techniques specialized for different applications. Observing that humans use binocular vision to understand the world, we study in this paper whether the 3D viewpoint generalizability of CNNs can be achieved via a binocular vision. We propose CNN2, a CNN that takes two images as input, which resembles the process of an object being viewed from the left eye and the right eye. CNN2 uses novel augmentation, pooling, and convolutional layers to learn a sense of three-dimensionality in a recursive manner. Empirical evaluation shows that CNN2 has improved viewpoint generalizability compared to vanilla CNNs. Furthermore, CNN2 is easy to implement and train, and is compatible with existing CNN-based specialized techniques for different applications.	[Chen, Wei-Da; Wu, Shan-Hung] Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan	National Tsing Hua University	Chen, WD (corresponding author), Natl Tsing Hua Univ, Dept Comp Sci, Hsinchu, Taiwan.	wdchen@datalab.cs.nthu.edu.tw; shwu@cs.nthu.edu.tw			MOST Joint Research Center for AI Technology; All Vista Healthcare, Taiwan [MOST 108-2634-F-007-003-]	MOST Joint Research Center for AI Technology; All Vista Healthcare, Taiwan	This work is supported by the MOST Joint Research Center for AI Technology and All Vista Healthcare, Taiwan (MOST 108-2634-F-007-003-). We also thank the anonymous reviewers for their insightful feedbacks.	An C, 2017, IEEE INT CONF MICROW, P176; [Anonymous], 2017, P ICML; [Anonymous], 2006, VISUAL BRAIN ACTION; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Anzai A, 2007, NAT NEUROSCI, V10, P1313, DOI 10.1038/nn1975; Ba LJ, 2015, 2015 IEEE International Conference on Applied Superconductivity and Electromagnetic Devices (ASEMD), P3, DOI 10.1109/ASEMD.2015.7453438; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Bojarski M., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1109/IVS.2017.7995975; Cai Z., 2016, P ECCV; Chen C-F, 2019, BIG LITTLE NET EFFIC; Chen Z., 2016, P 12 USENIX S OP SYS, P265, DOI 10.5555/ 3026877.3026899; Cheng Gong, 2016, P CVPR; Cheng Xiuyuan, 2019, P ICLR; Codevilla Felipe, 2018, P ICRA; Cohen T., 2016, P ICML; Dai ZY, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P126, DOI 10.1145/3159652.3159659; Ecker Alexander S., 2019, P ICLR; Esteva Andre, 2019, NATURE MED, V25; FUKUSHIMA K., 1982, COMPETITION COOPERAT, P267; Gehring Jonas, 2017, P ACL; Geirhos R., 2019, P ICLR; Giraldo L. G. S., 2018, ARXIV180602888; Girshick R., 2017, P CVPR; Godard C, 2017, PROC CVPR IEEE, P6602, DOI 10.1109/CVPR.2017.699; Gong Y., 2014, MULTISCALE ORDERLESS; Gotts Stephen J, 2013, P NATL ACAD SCI USA; He K., 2014, P ECCV; Hinton G. E., 2011, INT C ART NEUR NETW; Hinton G.E., 2018, P ICLR; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Jaderberg M., 2015, P NIPS; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kheradpisheh SR, 2016, SCI REP-UK, V6, DOI 10.1038/srep32672; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kyrkou Christos, 2018, DES AUT TEST EUR C E; Lai Kevin, 2011, LARGE SCALE HIERARCH; Laptev D., 2016, P CVPR; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Yann, 2004, P CVPR, V2; Lin Tsung-Yi, 2020, IEEE Trans Pattern Anal Mach Intell, V42, P318, DOI 10.1109/TPAMI.2018.2858826; LIU MY, 2017, P ASM 36 INT C OC; Long Bria, 2018, ROLE TEXTURAL STAT V; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maninis K.-K., 2016, P ECCV; Maruko I, 2008, J NEUROPHYSIOL, V100, P2486, DOI 10.1152/jn.90397.2008; Maturana Daniel, 2015, P IEEE RSJ INT C INT; McDonald Ryan, 2018, P 2018 C EMP METH NA, P1849, DOI DOI 10.18653/V1/D18-1211; Moura Thiago DO, 2014, P MICR TECHN DEV SBM, P1; Murphy PC, 1999, SCIENCE, V286, P1552, DOI 10.1126/science.286.5444.1552; Peer D., 2018, ARXIV181209707; Qi C. R., 2017, POINTNET DEEP LEARNI; Qi C. R., 2016, P CVPR; Qi Kunlun, 2018, REMOTE SENSING; Qiu FTT, 2005, NEURON, V47, P155, DOI 10.1016/j.neuron.2005.05.028; Real E., 2018, ARXIV180201548; Redmon J, 2016, PROC CVPR IEEE, P779, DOI 10.1109/CVPR.2016.91; REID RC, 1995, NATURE, V378, P281, DOI 10.1038/378281a0; Sabour S., 2017, P NIPS; Su H., 2015, P ICCV; von der Heydt R, 2000, VISION RES, V40, P1955, DOI 10.1016/S0042-6989(00)00044-4; VONDERHEYDT R, 1984, SCIENCE, V224, P1260, DOI 10.1126/science.6539501; Wallis TSA, 2017, J VISION, V17, DOI 10.1167/17.12.5; Worrall D. E., 2017, P CVPR; Wurtz R. H., 2000, PRINCIPLES NEURAL SC, V4, P523; Yang S., 2015, P ICCV; Zhu J., 2017, P ICCV	69	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302003
C	Chen, XY; Wang, SN; Fu, B; Long, MS; Wang, JM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Xinyang; Wang, Sinan; Fu, Bo; Long, Mingsheng; Wang, Jianmin			Catastrophic Forgetting Meets Negative Transfer: Batch Spectral Shrinkage for Safe Transfer Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Before sufficient training data is available, fine-tuning neural networks pre-trained on large-scale datasets substantially outperforms training from random initialization. However, fine-tuning methods suffer from a dilemma across catastrophic forgetting and negative transfer. While several methods with explicit attempts to overcome catastrophic forgetting have been proposed, negative transfer is rarely delved into. In this paper, we launch an in-depth empirical investigation into negative transfer in fine-tuning and find that, for the weight parameters and feature representations, transferability of their spectral components is diverse. For safe transfer learning, we present Batch Spectral Shrinkage (BSS), a novel regularization approach to penalizing smaller singular values so that untransferable spectral components are suppressed. BSS is orthogonal to existing fine-tuning methods and is readily pluggable into them. Experimental results show that BSS can significantly enhance the performance of state-of-the-art methods, especially in few training data regime.	[Long, Mingsheng] Tsinghua Univ, BNRist, Sch Software, Beijing, Peoples R China; Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China; Natl Engn Lab Big Data Software, Beijing, Peoples R China	Tsinghua University; Tsinghua University	Long, MS (corresponding author), Tsinghua Univ, BNRist, Sch Software, Beijing, Peoples R China.	chenxiny17@mails.tsinghua.edu.cn; wang-sn17@mails.tsinghua.edu.cn; mingsheng@tsinghua.edu.cn; jimwang@tsinghua.edu.cn	wang, jian/GVS-0711-2022		National Key R&D Program of China [2017YFC1502003]; Natural Science Foundation of China [61772299, 71690231]	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank Dr. Yuchen Zhang at Tsinghua University for helpful discussions. This work was supported by the National Key R&D Program of China (2017YFC1502003) and Natural Science Foundation of China (61772299 and 71690231).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aygun M, 2017, IEEE INT CONF COMP V, P2674, DOI 10.1109/ICCVW.2017.309; Barati E, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P811, DOI 10.1145/3343031.3351037; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Cui Y, 2018, PROC CVPR IEEE, P4109, DOI 10.1109/CVPR.2018.00432; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Donahue J, 2014, PR MACH LEARN RES, V32; Ge WF, 2017, PROC CVPR IEEE, P10, DOI 10.1109/CVPR.2017.9; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2019, IEEE I CONF COMP VIS, P4917, DOI 10.1109/ICCV.2019.00502; Hinton G., 2015, ARXIV150302531; Huh Minyoung, 2016, ARXIV160808614; Khosla Aditya, 2011, 1 WORKSH FIN GRAIN V, P6; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; Krause J, 2013, 2013 IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION WORKSHOPS (ICCVW), P554, DOI 10.1109/ICCVW.2013.77; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee SW, 2017, ADV NEUR IN, V30; Li Xiang, 2019, INT C LEARN REPR, P1; Li Xuhong, 2018, P MACHINE LEARNING R, V80, P2830; Li ZZ, 2018, IEEE T PATTERN ANAL, V40, P2935, DOI 10.1109/TPAMI.2017.2773081; Liu J., 2017, AAAI C ART INT AAAI; Maji S., 2013, TECHNICAL REPORT, P6; MIAO J, 1992, LINEAR ALGEBRA APPL, V171, P81, DOI 10.1016/0024-3795(92)90251-5; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; PARKHI OM, 2012, PROC CVPR IEEE, P3498, DOI [DOI 10.1007/978-3-030-28954-6_10, 10.1109/CVPR.2012.6248092, DOI 10.1109/CVPR.2012.6248092]; Quattoni A, 2009, PROC CVPR IEEE, P413, DOI 10.1109/CVPRW.2009.5206537; Romero Adriana, 2014, ARXIV14126550; Rusu A. A., 2016, ARXIV160604671; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Serra J, 2018, PR MACH LEARN RES, V80; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Thrun S., 1995, INTELLIGENT ROBOTS S, P201, DOI DOI 10.1016/B978-044482250-5/50015-3; Torrey L., 2010, HDB RES MACHINE LEAR, P242, DOI DOI 10.4018/978-1-60566-766-9.CH011; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang Alex, 2018, ARXIV180407461, DOI DOI 10.18653/V1/W18-5446; Wang ZHW, 2019, IEEE INT CONF COMP V, P4320, DOI 10.1109/ICCVW.2019.00532; Welinder P., 2010, CNSTR2010001 CALTECH; Yang SM, 2016, CHIN CONT DECIS CONF, P3443, DOI 10.1109/CCDC.2016.7531578; Ye W., 2019, AAAI C ART INT AAAI; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Yoon Jaehong, 2018, INT C LEARN REPR ICL; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zagoruyko S., 2017, P INT C LEARN REPR, DOI DOI 10.1109/CVPR.2019.00271	46	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301085
C	Chen, YC; Meila, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Yu-Chia; Meila, Marina			Selecting the independent coordinates of manifolds with large aspect ratios	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many manifold embedding algorithms fail apparently when the data manifold has a large aspect ratio (such as a long, thin strip). Here, we formulate success and failure in terms of finding a smooth embedding, showing also that the problem is pervasive and more complex than previously recognized. Mathematically, success is possible under very broad conditions, provided that embedding is done by carefully selected eigenfunctions of the Laplace-Beltrami operator M. Hence, we propose a bicriterial Independent Eigencoordinate Selection (IES) algorithm that selects smooth embeddings with few eigenvectors. The algorithm is grounded in theory, has low computational overhead, and is successful on synthetic and large real data.	[Chen, Yu-Chia] Univ Washington, Dept Elect & Comp Engn, Seattle, WA 98195 USA; [Meila, Marina] Univ Washington, Dept Stat, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle; University of Washington; University of Washington Seattle	Chen, YC (corresponding author), Univ Washington, Dept Elect & Comp Engn, Seattle, WA 98195 USA.	yuchaz@uw.edu; mmp2@uw.edu		Chen, Yu-Chia/0000-0003-2308-3630	U.S. Department of Energy, Solar Energy Technology Office award [DE-EE0008563]; NSF [IIS-0313339, DMS PD 08-1269]	U.S. Department of Energy, Solar Energy Technology Office award(United States Department of Energy (DOE)); NSF(National Science Foundation (NSF))	The authors acknowledge partial support from the U.S. Department of Energy, Solar Energy Technology Office award DE-EE0008563 and from the NSF DMS PD 08-1269 and NSF IIS-0313339 awards. They are grateful to the Tkatchenko and Pfaendtner labs and in particular to Stefan Chmiela and Chris Fu for providing the molecular dynamics data and for many hours of brainstorming and advice.	Abazajian KN, 2009, ASTROPHYS J SUPPL S, V182, P543, DOI 10.1088/0067-0049/182/2/543; Bates J, 2014, APPL COMPUT HARMON A, V37, P516, DOI 10.1016/j.acha.2014.03.002; Belkin M., 2007, NIPS PROCESSING, V19, P129; Bilmes, 2012, P 28 C UNC ART INT, P407; Blau Yochai, 2017, LECT NOTES ARTIF I I, P256, DOI DOI 10.1007/978-3-319-71249-9_16; Chmiela S, 2017, SCI ADV, V3, DOI 10.1126/sciadv.1603015; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Dasgupta S, 2013, EDGAR ALLAN POE IN CONTEXT, P313; Dsilva CJ, 2018, APPL COMPUT HARMON A, V44, P759, DOI 10.1016/j.acha.2015.06.008; Fleming KL, 2016, J PHYS CHEM A, V120, P299, DOI 10.1021/acs.jpca.5b10667; Goldberg Y, 2008, J MACH LEARN RES, V9, P1909; Harville D.A., 1998, MATRIX ALGEBRA STAT; Hein M, 2005, LECT NOTES COMPUT SC, V3559, P470, DOI 10.1007/11503415_32; Hein M, 2007, J MACH LEARN RES, V8, P1325; Horn R.A., 2013, MATRIX ANAL, P321; Joncas D., 2017, ADV NEURAL INFORM PR, P4457; Lee J.M, 2012, INTRO SMOOTH MANIFOL; Levina E., 2005, ADV NEURAL INFORM PR, V17, P777, DOI DOI 10.5555/2976040.2976138; Mardia KV., 2016, WILEY SERIES PROBABI, VSecond Edition; McQueen J, 2016, J MACH LEARN RES, V17; Nadler B., 2006, ADV NEURAL INFORM PR, P955; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Perraul-Joncas D., 2013, NONLINEAR DIMENSIONA; Portegies JW, 2016, COMMUN PUR APPL MATH, V69, P478, DOI 10.1002/cpa.21565; Strauss W. A., 2007, PARTIAL DIFFERENTIAL; Ting D., 2010, P ICML	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301012
C	Cheung, WC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cheung, Wang Chi			Regret Minimization for Reinforcement Learning with Vectorial Feedback and Complex Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider an agent who is involved in an online Markov Decision Process, and receives a vector of outcomes every round. The agent optimizes an aggregate reward function on the multi-dimensional outcomes. Due to state transitions, it is challenging to balance the contribution from each dimension for achieving near-optimality. Contrary to the single objective case, stationary policies are generally sub-optimal. We propose a no-regret algorithm based on the Frank-Wolfe algorithm (Frank and Wolfe 1956, Agrawal and Devanur 2014), UCRL2 (Jaksch et al. 2010), as well as a crucial and novel Gradient Threshold Procedure (GTP). GTP involves carefully delaying gradient updates, and returns a non-stationary policy that diversifies the outcomes for optimizing the aggregate reward.	[Cheung, Wang Chi] Natl Univ Singapore, Dept Ind Syst Engn & Management, Singapore, Singapore	National University of Singapore	Cheung, WC (corresponding author), Natl Univ Singapore, Dept Ind Syst Engn & Management, Singapore, Singapore.	isecwc@nus.edu.sg						Achiam J, 2017, PR MACH LEARN RES, V70; Agrawal S., 2016, ADV NEURAL INFORM PR, V30, P1184; Agrawal S., 2016, ADV NEURAL INFORM PR, V29, P3450; Agrawal S., 2014, ACM C EC COMP; Agrawal S, 2016, P 29 ANN C LEARN THE, P4; Altman E., 1999, STOCH MODEL SER; [Anonymous], 2015, FOUND TRENDS MACH LE, V8, P232, DOI 10.1561/2200000050; Audibert JY, 2009, THEOR COMPUT SCI, V410, P1876, DOI 10.1016/j.tcs.2009.01.016; Auer P, 2006, ADV NEURAL INFORM PR, V19, P49; Azar Yossi, 2014, P 5 C INN THER COMP, P195; Badanidiyuru A, 2014, P C LEARN THEOR, V35, P1109; Badanidiyuru A, 2013, ANN IEEE SYMP FOUND, P207, DOI 10.1109/FOCS.2013.30; Barrett L., 2008, P 25 INT C MACHINE L, P41, DOI DOI 10.1145/1390156.1390162; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; Berthet Q., 2017, ADV NEURAL INFORM PR, P2225; Bertsekas D.P., 2005, DYNAMIC PROGRAMMING; Busa-Fekete R, 2017, PR MACH LEARN RES, V70; Cheung W. C., 2019, CORR; Even-Dar E., 2009, 22 ANN C LEARN THEOR; Ferreira KJ, 2018, OPER RES, V66, P1586, DOI 10.1287/opre.2018.1755; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Fruit R., 2018, P MACHINE LEARNING R, V80, P1578; Fruit Ronan, 2018, P 32 INT C NEUR INF, P2998; Hazan E., 2018, CORR; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Immorlica N., 2018, CORR; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Le HM, 2019, PR MACH LEARN RES, V97; Lee L., 2019, CORR; Liu CM, 2015, IEEE T SYST MAN CY-S, V45, P385, DOI 10.1109/TSMC.2014.2358639; Mannor S, 2004, J MACH LEARN RES, V5, P325; Miryoosefi S., 2019, NEURIPS 2019; NEMIROVSKI A, 1983, WILEY INTERSCIENCE S; Norris J., 1998, CAMBRIDGE SERIES STA; Ortner R., 2018, CORR; OUYANG Y, 2017, ADV NEURAL INFORM PR, V30, P1333; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Roijers DM, 2013, J ARTIF INTELL RES, V48, P67, DOI 10.1613/jair.3987; Rosenberg A, 2019, PR MACH LEARN RES, V97; Tarbouriech J., 2019, P 22 INT C ART INT S, P974; Tessler C., 2019, INT C LEARN REPR	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300066
C	Chowdhury, SR; Gopalan, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chowdhury, Sayak Ray; Gopalan, Aditya			Bayesian Optimization under Heavy-tailed Payoffs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider black box optimization of an unknown function in the nonparametric Gaussian process setting when the noise in the observed function values can be heavy tailed. This is in contrast to existing literature that typically assumes sub Gaussian noise distributions for queries. Under the assumption that the unknown function belongs to the Reproducing Kernel Hilbert Space (RKHS) induced by a kernel, we first show that an adaptation of the well-known GP-UCB algorithm 2 -Po:with reward truncation enjoys sublinear (O) over tilde (T2+alpha/2(1+alpha)) regret even with only the (1 + alpha)-th moments, alpha is an element of (0, 1], of the reward distribution being bounded ((O) over tilde hides logarithmic factors). However, for the common squared exponential (SE) and Matern kernels, this is seen to be significantly larger than a fundamental Omega(T1/1+alpha) lower bound on regret. We resolve this gap by developing novel Bayesian optimization algorithms, based on kernel approximation techniques, with regret bounds matching the lower bound in order for the SE kernel. We numerically benchmark the algorithms on environments based on both synthetic models and real-world data sets.	[Chowdhury, Sayak Ray; Gopalan, Aditya] Indian Inst Sci, Dept ECE, Bangalore 560012, Karnataka, India	Indian Institute of Science (IISC) - Bangalore	Chowdhury, SR (corresponding author), Indian Inst Sci, Dept ECE, Bangalore 560012, Karnataka, India.	sayak@iisc.ac.in; aditya@iisc.ac.in			Google India PhD fellowship grant; Tata Trusts travel grant; DST INSPIRE faculty grant [IFA13- ENG-69]	Google India PhD fellowship grant(Google Incorporated); Tata Trusts travel grant; DST INSPIRE faculty grant	The authors are grateful to the anonymous reviewers for their valuable comments. S. R. Chowdhury is supported by the Google India PhD fellowship grant and the Tata Trusts travel grant. A. Gopalan is grateful for support from the DST INSPIRE faculty grant IFA13- ENG-69.	Alaoui A., 2015, P 28 INT C NEURAL IN, P775; [Anonymous], 2018, ADV NEURAL INFORM PR; Azar MG, 2014, PR MACH LEARN RES, V32, P1557; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bochner S., 1959, LECT FOURIER INTEGRA; Bogunovic I., 2018, ADV NEURAL INFORM PR, P5760; Brochu E, 2010, ARXIV PREPRINT ARXIV; Bubeck S, 2011, J MACH LEARN RES, V12, P1655; Calandriello D., 2019, C LEARN THEORY COLT; Carpentier A, 2014, ADV NEUR IN, V27; Chowdhury SR, 2017, PR MACH LEARN RES, V70; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Durand A, 2018, J MACH LEARN RES, V19; Garnett R, 2010, PROCEEDINGS OF THE 9TH ACM/IEEE INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS, P209, DOI 10.1145/1791212.1791238; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hildebrand F.B., 1987, INTRO NUMERICAL ANAL, V2nd ed.; Hsu D, 2014, PR MACH LEARN RES, V32, P37; Jagannathan K, 2014, IEEE T INFORM THEORY, V60, P2896, DOI 10.1109/TIT.2014.2311125; Joaquin QC, 2007, LARGE SCALE KERNEL M, P203, DOI DOI 10.7551/MITPRESS/7496.003.0011; Kandasamy K, 2015, PR MACH LEARN RES, V37, P295; Kleinberg R, 2008, ACM S THEORY COMPUT, P681; Lattimore T., 2017, ADV NEURAL INFORM PR, P1584; Lawrence ND., 2015, ARXIV150501627; Medina AM, 2016, PR MACH LEARN RES, V48; Mokus J., 1975, LECT NOTES COMPUTER, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55, DOI 10.1007/978-3-662-38527-2_55, 10.1007/3-540-07165-2, DOI 10.1007/3-540-07165-2]; Mutny M., 2018, ADV NEURAL INFORM PR, P9005; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Resnick SI, 2007, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-45024-7; Rolland P., 2018, ARXIV180207028; Scarlett Jonathan, 2017, C LEARN THEOR, V65, P1723; Seldin Y, 2012, IEEE T INFORM THEORY, V58, P7086, DOI 10.1109/TIT.2012.2211334; Sen R., 2019, PROC 22 INT C ARTIF, P2096; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Sriperumbudur Bharath, 2015, ADV NEURAL INFORM PR, P1144; Strogatz SH, 2001, NATURE, V410, P268, DOI 10.1038/35065725; Vakili S, 2013, IEEE J-STSP, V7, P759, DOI 10.1109/JSTSP.2013.2263494; Wang Z, 2017, PR MACH LEARN RES, V70; Wang Z, 2016, JMLR WORKSH CONF PRO, V51, P1022; Wang Zi, 2017, ARXIV170601445; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yu XT, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P937	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905046
C	Ciliberto, C; Bach, F; Rudi, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ciliberto, Carlo; Bach, Francis; Rudi, Alessandro			Localized Structured Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OBJECTS	Key to structured prediction is exploiting the problem's structure to simplify the learning process. A major challenge arises when data exhibit a local structure (i.e., are made "by parts") that can be leveraged to better approximate the relation between (parts of) the input and (parts of) the output. Recent literature on signal processing, and in particular computer vision, shows that capturing these aspects is indeed essential to achieve state-of-the-art performance. However, in this context algorithms are typically derived on a case-by-case basis. In this work we propose the first theoretical framework to deal with part-based data from a general perspective and study a novel method within the setting of statistical learning theory. Our analysis is novel in that it explicitly quantifies the benefits of leveraging the part-based structure of a problem on the learning rates of the proposed estimator.	[Ciliberto, Carlo] Imperial Coll, Dept Elect & Elect Engn, London, England; [Bach, Francis; Rudi, Alessandro] PSL Res Univ, Ecole Normale Super, Dept Informat, INRIA, Paris, France	Imperial College London; Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Ciliberto, C (corresponding author), Imperial Coll, Dept Elect & Elect Engn, London, England.	c.ciliberto@imperial.ac.uk; francis.bach@inria.fr; alessandro.rudi@inria.fr						Alahari K, 2008, PROC CVPR IEEE, P472; Aliprantis CD, 2006, INFINITE DIMENSIONAL; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bahl L. R., 1986, ICASSP 86 Proceedings. IEEE-IECEJ-ASJ International Conference on Acoustics, Speech and Signal Processing (Cat. No.86CH2243-4), P49; BakIr G., 2007, PREDICTING STRUCTURE; Blaschko MB, 2008, LECT NOTES COMPUT SC, V5302, P2, DOI 10.1007/978-3-540-88682-2_2; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Ciliberto C, 2017, ADV NEUR IN, V30; Ciliberto Carlo, 2016, ADV NEURAL INFORM PR, V29, P4412; Collins M, 2004, TEXT SPEECH LANG TEC, P19; Cortes C, 2014, PR MACH LEARN RES, V32, P1134; Cortes Corinna, 2016, ADV NEURAL INFORM PR, P2514; Dalal N., 2005, INT J INFORM SYSTEM, P886, DOI [10.1109/icnc.2013.6818189, DOI 10.1109/ICNC.2013.6818189]; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dieuleveut A, 2017, J MACH LEARN RES, V18; Djerrab M, 2018, MACH LEARN, V107, P1229, DOI 10.1007/s10994-018-5698-0; Duchi John C., 2010, P 27 INT C MACH LEAR, P327; Joachims T, 2009, COMMUN ACM, V52, P97, DOI 10.1145/1592761.1592783; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Korba A., 2018, P NEURIPS; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lampert CH, 2009, IEEE T PATTERN ANAL, V31, P2129, DOI 10.1109/TPAMI.2009.144; Lever Guy, 2012, INT C MACH LEARN ICM, V5; Luise G, 2019, PR MACH LEARN RES, V97; Luise Giulia, 2018, ADV NEUR IN, V31, P5859; Meyn S. P., 2012, MARKOV CHAINS STOCHA; Micchelli Charles A, 2004, ADV NEURAL INFORM PR, P921; Nowak-Vila Alex, 2018, ARTIFICIAL INTALLIGE; Nowozin Sebastian, 2011, FDN TRENDS COMPUTER; Osokin A, 2017, ADV NEUR IN, V30; Ratliff N. D., 2006, P 23 INT C MACH LEAR, P729, DOI DOI 10.1145/1143844.1143936; Rudi A, 2017, ADV NEUR IN, V30; Rudi Alessandro, 2018, ADV NEURAL INFORM PR, P5610; Singh R, 2019, ADV NEUR IN, V32; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Song L, 2013, IEEE SIGNAL PROC MAG, V30, P98, DOI 10.1109/MSP.2013.2252713; Steinke F, 2010, SIAM J IMAGING SCI, V3, P527, DOI 10.1137/080744189; Struminsky K, 2018, ADV NEUR IN, V31; Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013; Szummer M, 2008, LECT NOTES COMPUT SC, V5303, P582, DOI 10.1007/978-3-540-88688-4_43; Taskar B, 2004, ADV NEUR IN, V16, P25; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Tuia D, 2011, J SIGNAL PROCESS SYS, V65, P301, DOI 10.1007/s11265-010-0483-8; Vedaldi A., 2009, P ADV NEUR INF PROC, P1928	45	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307033
C	Clark, DG; Livezey, JA; Bouchard, KE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Clark, David G.; Livezey, Jesse A.; Bouchard, Kristofer E.			Unsupervised Discovery of Temporal Structure in Noisy Data with Dynamical Components Analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIMENSIONALITY REDUCTION; INFORMATION; COMPLEXITY; ENTROPY	Linear dimensionality reduction methods are commonly used to extract low-dimensional structure from high-dimensional data. However, popular methods disregard temporal structure, rendering them prone to extracting noise rather than meaningful dynamics when applied to time series data. At the same time, many successful unsupervised learning methods for temporal, sequential and spatial data extract features which are predictive of their surrounding context. Combining these approaches, we introduce Dynamical Components Analysis (DCA), a linear dimensionality reduction method which discovers a subspace of high-dimensional time series data with maximal predictive information, defined as the mutual information between the past and future. We test DCA on synthetic examples and demonstrate its superior ability to extract dynamical structure compared to commonly used linear methods. We also apply DCA to several real-world datasets, showing that the dimensions extracted by DCA are more useful than those extracted by other methods for predicting future states and decoding auxiliary variables. Overall, DCA robustly extracts dynamical structure in noisy, high-dimensional data while retaining the computational efficiency and geometric interpretability of linear dimensionality reduction methods.	[Clark, David G.] Columbia Univ, Ctr Theoret Neurosci, New York, NY 10027 USA; [Clark, David G.; Livezey, Jesse A.; Bouchard, Kristofer E.] Lawrence Berkeley Natl Lab, Biol Syst & Engn Div, Berkeley, CA USA; [Livezey, Jesse A.; Bouchard, Kristofer E.] Univ Calif Berkeley, Redwood Ctr Theoret Neurosci, Berkeley, CA 94720 USA; [Bouchard, Kristofer E.] Univ Calif Berkeley, Helen Wills Neurosci Inst, Berkeley, CA 94720 USA	Columbia University; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; University of California System; University of California Berkeley; University of California System; University of California Berkeley	Clark, DG (corresponding author), Columbia Univ, Ctr Theoret Neurosci, New York, NY 10027 USA.; Clark, DG (corresponding author), Lawrence Berkeley Natl Lab, Biol Syst & Engn Div, Berkeley, CA USA.	dgc2138@cumc.columbia.edu; jlivezey@lbl.gov; kebouchard@lbl.gov			LBNL Laboratory Directed Research and Development	LBNL Laboratory Directed Research and Development	D.G.C. and K.E.B. were funded by LBNL Laboratory Directed Research and Development. We thank the Neural Systems and Data Science Lab and Laurenz Wiskott for helpful discussion.	Bethge Matthias, 2007, HUMAN VISION ELECT I, V6492; Bialek W, 2001, NEURAL COMPUT, V13, P2409, DOI 10.1162/089976601753195969; Borga M., 2001, CANONICAL CORRELATIO, V4, P5; Bouchard KE, 2016, P NATL ACAD SCI USA, V113, P9641, DOI 10.1073/pnas.1606725113; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Costa AC, 2019, P NATL ACAD SCI USA, V116, P1501, DOI 10.1073/pnas.1813476116; Creutzig F, 2009, PHYS REV E, V79, DOI 10.1103/PhysRevE.79.041925; Cunningham JP, 2015, J MACH LEARN RES, V16, P2859; Cunningham JP, 2014, NAT NEUROSCI, V17, P1500, DOI 10.1038/nn.3776; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Gao PR, 2015, CURR OPIN NEUROBIOL, V32, P148, DOI 10.1016/j.conb.2015.04.003; Glaser J. I., 2017, MACHINE LEARNING NEU; Goerg G., 2013, INT C MACHINE LEARNI, P64; Golub MD, 2018, NAT NEUROSCI, V21, P607, DOI 10.1038/s41593-018-0095-3; Hotelling H, 1933, J EDUC PSYCHOL, V24, P417, DOI 10.1037/h0071325; Hyvarinen A, 2001, NEURAL COMPUT, V13, P883, DOI 10.1162/089976601300014394; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Kim Y, 2016, AAAI CONF ARTIF INTE, P2741; Kolchinsky A, 2017, ENTROPY-SWITZ, V19, DOI 10.3390/e19070361; Korn K, 2019, INT CONF ACOUST SPEE, P1100, DOI 10.1109/ICASSP.2019.8682601; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Larsen R, 2002, J CHEMOMETR, V16, P427, DOI 10.1002/cem.743; Li Ming, 2013, INTRO KOLMOGOROV COM; Mahoney MW, 2009, P NATL ACAD SCI USA, V106, P697, DOI [10.1073/pnas.0803205105, 10.1073/pnas.0803205106]; Malekzadeh M, 2015, PROCEEDINGS OF THE WORKSHOP ON PRIVACY BY DESIGN IN DISTRIBUTED SYSTEMS (P2DS'18), DOI 10.1145/3195258.3195260; Marzen SE, 2017, PHYS REV E, V95, DOI 10.1103/PhysRevE.95.051301; McAllester David, 2018, ARXIV180207572; McAllester David, 2018, ARXIV181104251; Mikolov T., 2013, ARXIV; Mizuseki K, 2009, MULTIUNIT RECORDINGS; ODoherty Joseph E., 2017, NONHUMAN PRIMATE REA, DOI [10.5281/zenodo.583331, DOI 10.5281/ZENODO.583331.]; Palmer SE, 2015, P NATL ACAD SCI USA, V112, P6908, DOI 10.1073/pnas.1506855112; Pandarinath C, 2018, NAT METHODS, V15, P805, DOI 10.1038/s41592-018-0109-9; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Richthofer S, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P190, DOI 10.1109/ICMLA.2015.158; Selfish Gene, 2017, HIST HOURL WEATH DAT; Stogbauer H, 2004, PHYS REV E, V70, DOI 10.1103/PhysRevE.70.066123; Strong SP, 1998, PHYS REV LETT, V80, P197, DOI 10.1103/PhysRevLett.80.197; Tegmark Max, 2019, ARXIV190203364; Thirion B, 2003, NEUROIMAGE, V20, P34, DOI 10.1016/S1053-8119(03)00316-1; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tishby Naftali, 2000, PHYSICS0004057 ARXIV; TONG L, 1990, 1990 IEEE INTERNATIONAL SYMP ON CIRCUITS AND SYSTEMS, VOLS 1-4, P1784, DOI 10.1109/ISCAS.1990.111981; van den Oord Aaron, 2018, ARXIV180703748; Weghenkel B, 2017, MACH LEARN, V106, P1359, DOI 10.1007/s10994-017-5632-x; WILSON MA, 1993, SCIENCE, V261, P1055, DOI 10.1126/science.8351520; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938; Xie Z., 1996, J TIME SER ANAL, V17, P65; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; Zeng XL, 2018, P NATL ACAD SCI USA, V115, P9956, DOI 10.1073/pnas.1715593115; Zheng YJ, 2018, INT CONF MANIP MANU, P6, DOI 10.1109/3M-NANO.2018.8552232; Ziehe A., 1998, ICANN 98. Proceedings of the 8th International Conference on Artificial Neural Networks, P675	53	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905089
C	Cohen, A; Hassidim, A; Kaplan, H; Mansour, Y; Moran, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cohen, Alon; Hassidim, Avinatan; Kaplan, Haim; Mansour, Yishay; Moran, Shay			Learning to Screen	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Imagine a large firm with multiple departments that plans a large recruitment. Candidates arrive one-by-one, and for each candidate the firm decides, based on her data (CV, skills, experience, etc), whether to summon her for an interview. The firm wants to recruit the best candidates while minimizing the number of interviews. We model such scenarios as an assignment problem between items (candidates) and categories (departments): the items arrive one-by-one in an online manner, and upon processing each item the algorithm decides, based on its value and the categories it can be matched with, whether to retain or discard it (this decision is irrevocable). The goal is to retain as few items as possible while guaranteeing that the set of retained items contains an optimal matching. We consider two variants of this problem: (i) in the first variant it is assumed that the n items are drawn independently from an unknown distribution D. (ii) In the second variant it is assumed that before the process starts, the algorithm has an access to a training set of n items drawn independently from the same unknown distribution (e.g. data of candidates from previous recruitment seasons). We give near-optimal bounds on the best-possible number of retained items in each of these variants. These results demonstrate that one can retain exponentially less items in the second variant (with the training set). Our algorithms and analysis utilize ideas and techniques from statistical learning theory and from discrete algorithms.	[Cohen, Alon] Technion Israel Inst Technol, Haifa, Israel; [Cohen, Alon; Hassidim, Avinatan; Kaplan, Haim; Mansour, Yishay; Moran, Shay] Google Res, Mountain View, CA 94043 USA; [Hassidim, Avinatan] Bar Ilan Univ, Ramat Gan, Israel; [Kaplan, Haim; Mansour, Yishay] Tel Aviv Univ, Tel Aviv, Israel; [Moran, Shay] Princeton Univ, Princeton, NJ 08544 USA	Technion Israel Institute of Technology; Google Incorporated; Bar Ilan University; Tel Aviv University; Princeton University	Cohen, A (corresponding author), Technion Israel Inst Technol, Haifa, Israel.; Cohen, A (corresponding author), Google Res, Mountain View, CA 94043 USA.	aloncohen@technion.ac.il; avinatanh@gmail.com; haimk@tau.ac.il; mansour.yishay@gmail.com; shaymoran1@gmail.com	Cohen, Alon/AAW-5887-2021					Babaioff M, 2007, PROCEEDINGS OF THE EIGHTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P434; Balcan MF, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P173, DOI 10.1145/3219166.3219217; Blum A, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2351; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Bousquet O., 2004, LECT NOTES COMPUTER, V3176; Celis LE, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P144; Celis L Elisa, 2017, ARXIV170406840; Correa J. R., 2018, CORR; Ezra T, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P319, DOI 10.1141/3219166.3219211; Ferguson TS., 1989, STAT SCI, V4, P282, DOI DOI 10.1214/SS/1177012493; Greenberg S., 2013, CORR; Gupta A, 2016, MATH OPER RES, V41, P1404, DOI 10.1287/moor.2016.0782; Har-Peled S, 2011, DISCRETE COMPUT GEOM, V45, P462, DOI 10.1007/s00454-010-9248-1; Hsu J, 2016, ACM S THEORY COMPUT, P440, DOI 10.1145/2897518.2897559; LAWLER E, 2001, COMBINATORIAL OPTIMI; McDiarmid C, 1989, SURVEYS COMBINATORIC; Mehta A, 2012, FOUND TRENDS THEOR C, V8, P265, DOI 10.1561/0400000057; MORAN S, 1985, J ACM, V32, P938, DOI 10.1145/4221.4259; MORGENSTERN J., 2016, PROC MACH LEARN RES, V49, P1298; Morgenstern Jamie H, 2015, ADV NEURAL INFORM PR; Sauer N., 1972, J COMB THEORY A, V13, P145, DOI [10.1016/0097-3165(72)90019-2, DOI 10.1016/0097-3165(72)90019-2]; Talagrand M, 1995, PUBL MATH-PARIS, P73; van Handel Ramon, 2014, TECHNICAL REPORT; Vardi S, 2015, LEIBNIZ INT PR INFOR, V30, P716, DOI 10.4230/LIPIcs.STACS.2015.716; Vondrak J, 2010, CORR	25	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900023
C	Cohen, E; Geri, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cohen, Edith; Geri, Ofir			Sampling Sketches for Concave Sublinear Functions of Frequencies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ASYMPTOTIC THEORY	We consider massive distributed datasets that consist of elements modeled as key value pairs and the task of computing statistics or aggregates where the contribution of each key is weighted by a function of its frequency (sum of values of its elements). This fundamental problem has a wealth of applications in data analytics and machine learning, in particular, with concave sublinear functions of the frequencies that mitigate the disproportionate effect of keys with high frequency. The family of concave sublinear functions includes low frequency moments (p < 1), capping, logarithms, and their compositions. A common approach is to sample keys, ideally, proportionally to their contributions and estimate statistics from the sample. A simple but costly way to do this is by aggregating the data to produce a table of keys and their frequencies, apply our function to the frequency values, and then apply a weighted sampling scheme. Our main contribution is the design of composable sampling sketches that can be tailored to any concave sublinear function of the frequencies. Our sketch structure size is very close to the desired sample size and our samples provide statistical guarantees on the estimation quality that are very close to that of an ideal sample of the same size computed over aggregated data. Finally, we demonstrate experimentally the simplicity and effectiveness of our methods.	[Cohen, Edith] Google Res, Mountain View, CA 94043 USA; [Cohen, Edith] Tel Aviv Univ, Tel Aviv, Israel; [Geri, Ofir] Stanford Univ, Stanford, CA 94305 USA	Google Incorporated; Tel Aviv University; Stanford University	Cohen, E (corresponding author), Google Res, Mountain View, CA 94043 USA.; Cohen, E (corresponding author), Tel Aviv Univ, Tel Aviv, Israel.	edith@cohenwang.com; ofirgeri@cs.stanford.edu			NSF [CCF-1617577]; Simons Investigator Award; Google Graduate Fellowship in Computer Science in the School of Engineering at Stanford University; Stanford University; Stanford Research Computing Center	NSF(National Science Foundation (NSF)); Simons Investigator Award; Google Graduate Fellowship in Computer Science in the School of Engineering at Stanford University(Google Incorporated); Stanford University(Stanford University); Stanford Research Computing Center	Ofir Geri was supported by NSF grant CCF-1617577, a Simons Investigator Award for Moses Charikar, and the Google Graduate Fellowship in Computer Science in the School of Engineering at Stanford University. The computing for this project was performed on the Sherlock cluster. We would like to thank Stanford University and the Stanford Research Computing Center for providing computational resources and support that contributed to these research results.	Andoni A, 2011, ANN IEEE SYMP FOUND, P363, DOI 10.1109/FOCS.2011.82; Braverman V., 2018, ABS181200241 CORR; BRAVERMAN V, 2010, STOC; Braverman V., 2016, PODS; CHAO MT, 1982, BIOMETRIKA, V69, P653; Cohen E., 2017, KDD; Cohen E., 2007, ACM PODC; Cohen E., 2012, P ACM SIGMETRICS PER; Cohen E., 2014, J COMPUT SYSTEM SCI, V80; Cohen E., 2015, KDD; Cohen E, 2011, SIAM J COMPUT, V40, P1402, DOI 10.1137/10079817X; Duffield N, 2007, J ACM, V54, DOI 10.1145/1314690.1314696; Estan C., 2002, SIGCOMM; FLAJOLET P, 1985, J COMPUT SYST SCI, V31, P182, DOI 10.1016/0022-0000(85)90041-8; Flajolet P., 2007, ANAL ALGORITHMS AOFA; Frahling G, 2008, INT J COMPUT GEOM AP, V18, P3, DOI 10.1142/S0218195908002520; GIBBONS P, 1998, SIGMOD; Google, 2014, FREQ CAPP ADWORDS HE; Jayaram R, 2018, ANN IEEE SYMP FOUND, P544, DOI 10.1109/FOCS.2018.00058; Jowhari H., 2011, P 30 ACM SIGMOD SIGA, P49; Knuth D.E., 1968, ART COMPUTER PROGRAM, V2; Kulkarni R., 2017, MILLION NEWS HEADLIN; Lerman K., 2010, P 16 ACM SIGKDD INT, P949; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Misra J., 1982, TECHNICAL REPORT; Monemizadeh M., 2010, P 21 ACM SIAM S DISC; Ohlsson E., 1998, J OFF STAT, V14, P149; Osborne M., 2014, FACEBOOK REACH FREQU; Pennington Jeffrey, 2014, EMNLP; Rosen B, 1997, J STAT PLAN INFER, V62, P135, DOI 10.1016/S0378-3758(96)00185-1; ROSEN B, 1972, ANN MATH STAT, V43, P373, DOI 10.1214/aoms/1177692620; VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165	34	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301036
C	Cohen-Addad, V; Hjuler, N; Parotsidis, N; Saulpic, D; Schwiegelshohn, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cohen-Addad, Vincent; Hjuler, Niklas; Parotsidis, Nikos; Saulpic, David; Schwiegelshohn, Chris			Fully Dynamic Consistent Facility Location	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION ALGORITHM	We consider classic clustering problems in fully dynamic data streams, where data elements can be both inserted and deleted. In this context, there are several important parameters: (1) the quality of the solution after each insertion or deletion, (2) the time it takes to update the solution, and (3) how different consecutive solutions are. The question of obtaining efficient algorithms in this context for facility location, k-median and k-means has been raised in a recent paper by Hubert-Chan et al. [WWW' 18] and also appears as a natural follow-up on the online model with recourse studied by Lattanzi and Vassilvitskii [ICML'17] (i.e.: in insertion-only streams). In this paper, we focus on general metric spaces and mainly on the facility location problem. We give an arguably simple algorithm that maintains a constant factor approximation, with O (n log n) update time, and total recourse O (n). This improves over the naive algorithm which consists in recomputing a solution after each update and that can take up to O (n(2)) update time, and O (n(2)) total recourse. Our bounds are nearly optimal: in general metric space, inserting a point takes O (n) times to describe the distances to other points, and we give a simple lower bound of O(n) for the recourse. Moreover, we generalize this result for the 1-medians and k-means problems: our algorithms maintain a constant factor approximation in (O) over tilde (n + k(2)) time per update. We complement our analysis with experiments showing that the cost of the solution maintained by our algorithm at any time t is very close to the cost of a solution obtained by quickly recomputing a solution from scratch at time t while having a much better running time.	[Cohen-Addad, Vincent] CNRS, Paris, France; [Cohen-Addad, Vincent] Sorbonne Univ, Paris, France; [Hjuler, Niklas; Parotsidis, Nikos] Univ Copenhagen, Copenhagen, Denmark; [Saulpic, David] Sorbonne Univ, Ecole Normale Super, Paris, France; [Schwiegelshohn, Chris] Sapienza Univ Rome, Rome, Italy	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Universite Paris Cite; UDICE-French Research Universities; Sorbonne Universite; University of Copenhagen; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Sorbonne Universite; Sapienza University Rome	Cohen-Addad, V (corresponding author), CNRS, Paris, France.; Cohen-Addad, V (corresponding author), Sorbonne Univ, Paris, France.	vcohen@di.ens.fr; hjuler@di.ku.dk; nipa@di.ku.dk; david.saulpic@lip6.fr; schwiegelshohn@diag.uniroma1.it			VILLUM Foundation [16582]; Ce projet a beneficie d'une aide de l'Etat geree par l'Agence Nationale de la Recherche au titre du Programme FOCAL portant la reference suivante [ANR-18-CE40-0004-01]	VILLUM Foundation(Villum Fonden); Ce projet a beneficie d'une aide de l'Etat geree par l'Agence Nationale de la Recherche au titre du Programme FOCAL portant la reference suivante(French National Research Agency (ANR))	Nikos Parotsidis is supported by Grant Number 16582, Basic Algorithms Research Copenhagen (BARC), from the VILLUM Foundation. Ce projet a beneficie d'une aide de l'Etat geree par l'Agence Nationale de la Recherche au titre du Programme FOCAL portant la reference suivante : ANR-18-CE40-0004-01.	Ahmadian S, 2017, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2017.15; Anagnostopoulos A, 2004, INFORM COMPUT, V194, P175, DOI 10.1016/j.ic.2004.06.002; Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Bachem O, 2017, PR MACH LEARN RES, V70; Braverman V., 2017, P 34 INT C MACH LEAR, P576; Braverman V, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P26; Braverman Vladimir, 2016, P 27 ANN ACM SIAM S, P1374, DOI [10.1137/1.9781611974331.ch95, DOI 10.1137/1.9781611974331.CH95]; Chan THH, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P579, DOI 10.1145/3178876.3186124; Charikar M, 2004, SIAM J COMPUT, V33, P1417, DOI 10.1137/S0097539702418498; Charikar M., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P378, DOI 10.1109/SFFCS.1999.814609; Charikar M., 2003, P 35 ANN ACM S THEOR, P30, DOI DOI 10.1145/780542.780548; Cygan M., 2018, 26 ANN EUR S ALG ESA; Czumaj A, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1710; Fotakis D, 2007, J DISCRET ALGORITHMS, V5, P141, DOI 10.1016/j.jda.2006.03.001; Fotakis D, 2008, ALGORITHMICA, V50, P1, DOI 10.1007/s00453-007-9049-y; Fotakis D, 2006, THEOR COMPUT SCI, V361, P275, DOI 10.1016/j.tcs.2006.05.015; Frahling G., 2005, P 37 ANN ACM S THEOR, P209, DOI DOI 10.1145/1060590.1060622; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Goranci G., 2018, 26 ANN EUR S ALG ESA; Guha S, 1999, J ALGORITHMS, V31, P228, DOI 10.1006/jagm.1998.0993; Gupta A., 2008, ABS08092554 CORR; Henzinger M., 2017, 25 ANN EUR S ALG ESA; Hubert Chan T., TWITTER DATA SET; INDYK P, 2004, P 36 ANN ACM S THEOR, P373, DOI DOI 10.1145/1007352.1007413; Jain K, 2001, J ACM, V48, P274, DOI 10.1145/375827.375845; Kanungo T, 2004, COMP GEOM-THEOR APPL, V28, P89, DOI 10.1016/j.comgeo.2004.03.003; Lammersen C, 2008, LECT NOTES COMPUT SC, V5193, P660, DOI 10.1007/978-3-540-87744-8_55; Lang H, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1002; Lattanzi S, 2017, PR MACH LEARN RES, V70; Li S, 2013, INFORM COMPUT, V222, P45, DOI 10.1016/j.ic.2012.01.007; Meek C., 1990, US CENSUS DATA; Mettu RR, 2004, MACH LEARN, V56, P35, DOI 10.1023/B:MACH.0000033114.18632.e0; Mettu RR, 2003, SIAM J COMPUT, V32, P816, DOI 10.1137/S0097539701383443; Meyerson A, 2001, ANN IEEE SYMP FOUND, P426, DOI 10.1109/SFCS.2001.959917; Munteanu A, 2018, KUNSTL INTELL, V32, P37, DOI 10.1007/s13218-017-0519-3; Shindler M., 2011, ADV NEURAL INFORM PR, P2375	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303027
C	Colin, I; Dos Santos, L; Scaman, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Colin, Igor; Dos Santos, Ludovic; Scaman, Kevin			Theoretical Limits of Pipeline Parallel Optimization and Application to Distributed Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We investigate the theoretical limits of pipeline parallel learning of deep learning architectures, a distributed setup in which the computation is distributed per layer instead of per example. For smooth convex and non-convex objective functions, we provide matching lower and upper complexity bounds and show that a naive pipeline parallelization of Nesterov's accelerated gradient descent is optimal. For non-smooth convex functions, we provide a novel algorithm coined Pipeline ParallelRandom Smoothing (PPRS) that is within a d(1/4) multiplicative factor of the optimal convergence rate, where d is the underlying dimension. While the convergence rate still obeys a slow epsilon(-2) convergence rate, the depth-dependent part is accelerated, resulting in a near-linear speed-up and convergence time that only slightly depends on the depth of the deep learning architecture. Finally, we perform an empirical analysis of the non-smooth non-convex case and show that, for difficult and highly non-smooth problems, PPRS outperforms more traditional optimization algorithms such as gradient descent and Nesterov's accelerated gradient descent for problems where the sample size is limited, such as few-shot or adversarial learning.	[Colin, Igor; Dos Santos, Ludovic; Scaman, Kevin] Huawei Noahs Ark Lab, Hong Kong, Peoples R China	Huawei Technologies	Colin, I (corresponding author), Huawei Noahs Ark Lab, Hong Kong, Peoples R China.							Ahmed A., 2014, P 11 USENIX C OP SYS, P583; Amos B., 2017, ICML, V70, P146; [Anonymous], 2015, FOUND TRENDS MACH LE, V8, P232, DOI 10.1561/2200000050; [Anonymous], 2018, P ADV NEUR INF PROC; Ben-Nun Tal, 2018, ARXIV E PRINTS; Bengio Y., 2006, ADV NEURAL INFORM PR, P123; Burke JV, 2005, SIAM J OPTIMIZ, V15, P751, DOI 10.1137/030601296; Carmon Yair, 2017, ARXIV E PRINTS; Chen C.-C., 2018, ARXIV180902839; Chen Yutian, 2019, ICLR; Duchi JC, 2012, SIAM J OPTIMIZ, V22, P674, DOI 10.1137/110831659; Huang Y., 2018, CORR; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2014, ARXIV; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee S, 2014, ADV NEUR IN, V27; Li H, 2018, ADV NEUR IN, V31; Nesterov Y., 2018, APPL OPTIMIZATION; PETROWSKI A, 1993, IEEE T NEURAL NETWOR, V4, P970, DOI 10.1109/72.286892; Que Xiaocun, 2016, RANDOMIZED ALGORITHM; VALIANT LG, 1990, COMMUN ACM, V33, P103, DOI 10.1145/79173.79181; Wu Y., 2016, GOOGLES NEURAL MACHI	22	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904006
C	Combettes, CW; Pokutta, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Combettes, Cyrille W.; Pokutta, Sebastian			Blended Matching Pursuit	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GREEDY ALGORITHMS; OPTIMIZATION; SELECTION	Matching pursuit algorithms are an important class of algorithms in signal processing and machine learning. We present a blended matching pursuit algorithm, combining coordinate descent-like steps with stronger gradient descent steps, for minimizing a smooth convex function over a linear space spanned by a set of atoms. We derive sublinear to linear convergence rates according to the smoothness and sharpness orders of the function and demonstrate computational superiority of our approach. In particular, we derive linear rates for a large class of non-strongly convex functions, and we demonstrate in experiments that our algorithm enjoys very fast rates of convergence and wall-clock speed while maintaining a sparsity of iterates very comparable to that of the (much slower) orthogonal matching pursuit.	[Combettes, Cyrille W.] Georgia Inst Technol, Atlanta, GA 30332 USA; [Pokutta, Sebastian] Zuse Inst Berlin, Berlin, Germany; [Pokutta, Sebastian] TU Berlin, Berlin, Germany	University System of Georgia; Georgia Institute of Technology; Zuse Institute Berlin; Technical University of Berlin	Combettes, CW (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	cyrille@gatech.edu; pokutta@zib.de			NSF CAREER award [CMMI-1452463]	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	Research reported in this paper was partially supported by NSF CAREER award CMMI-1452463.	Bolte J, 2007, SIAM J OPTIMIZ, V17, P1205, DOI 10.1137/050644641; Braun G, 2019, PROC 36 INTERNAT C M, V97, P735; Braun G, 2017, PR MACH LEARN RES, V70; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; CHEN S, 1989, INT J CONTROL, V50, P1873, DOI 10.1080/00207178908953472; Condat L, 2016, MATH PROGRAM, V158, P575, DOI 10.1007/s10107-015-0946-6; Dai W, 2009, IEEE T INFORM THEORY, V55, P2230, DOI 10.1109/TIT.2009.2016006; Davenport MA, 2010, IEEE T INFORM THEORY, V56, P4395, DOI 10.1109/TIT.2010.2054653; DAVIS G, 1994, OPT ENG, V33, P2183, DOI 10.1117/12.173207; Fazel M, 2001, P AMER CONTR CONF, P4734, DOI 10.1109/ACC.2001.945730; Frank M., 1956, NAVAL RES LOGISTICS, V3, P95, DOI [DOI 10.1002/NAV.3800030109, 10.1002/nav.3800030109]; Garg R, 2009, 2009 INTERNATIONAL CONFERENCE ON COMPUTER AND AUTOMATION ENGINEERING, PROCEEDINGS, P336, DOI 10.1109/ICCAE.2009.28; Gribonval R, 2006, IEEE T INFORM THEORY, V52, P255, DOI 10.1109/TIT.2005.860474; Guyon I., 2005, ADV NEURAL INFORM PR, V17, P545; Nguyen H, 2017, CALCOLO, V54, P207, DOI 10.1007/s10092-016-0183-2; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Jaggi M., 2013, P 30 INT C MACHINE L, P427; Kerdreux T., 2019, 22 INT C ART INT STA, P1275; Lan G., 2017, P 34 INT C MACH LEAR, P1965; Levitin Evgeny S, 1966, USSR COMP MATH MATH, V6, P1, DOI DOI 10.1016/0041-5553(66)90114-5; Locatello F., 2018, P 35 INT C MACH LEAR, V80, P3198; Locatello F, 2017, PR MACH LEARN RES, V54, P860; Lojasiewicz S., 1963, EQUATIONS DERIVEES P, P87, DOI DOI 10.1006/JDEQ.1997.3393; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; NATARAJAN BK, 1995, SIAM J COMPUT, V24, P227, DOI 10.1137/S0097539792240406; Needell D, 2009, APPL COMPUT HARMON A, V26, P301, DOI 10.1016/j.acha.2008.07.002; NEMIROVSKII AS, 1985, USSR COMP MATH MATH+, V25, P21, DOI 10.1016/0041-5553(85)90100-4; PATI YC, 1993, CONFERENCE RECORD OF THE TWENTY-SEVENTH ASILOMAR CONFERENCE ON SIGNALS, SYSTEMS & COMPUTERS, VOLS 1 AND 2, P40, DOI 10.1109/ACSSC.1993.342465; Rao N, 2015, IEEE T SIGNAL PROCES, V63, P5798, DOI 10.1109/TSP.2015.2461515; Roulet V., 2017, ADV NEURAL INFORM PR, V30, P1119; Shalev-Shwartz S, 2010, SIAM J OPTIMIZ, V20, P2807, DOI 10.1137/090759574; Temlyakov V., 2013, ARXIV13121244; Temlyakov VN, 2015, CONSTR APPROX, V41, P269, DOI 10.1007/s00365-014-9272-0; Temlyakov V, 2014, CONF REC ASILOMAR C, P1331, DOI 10.1109/ACSSC.2014.7094676; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani RJ, 2015, J MACH LEARN RES, V16, P2543; Tropp JA, 2004, IEEE T INFORM THEORY, V50, P2231, DOI 10.1109/TIT.2004.834793; Yao Quanming, 2016, P 25 INT JOINT C ART, P2294; Yuan Xiao-Tong, 2013, J MACHINE LEARNING R, V14, P899; Zhang T, 2009, J MACH LEARN RES, V10, P555	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302008
C	Cui, H; Khardon, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cui, Hao; Khardon, Roni			Sampling Networks and Aggregate Simulation for Online POMDP Planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The paper introduces a new algorithm for planning in partially observable Markov decision processes (POMDP) based on the idea of aggregate simulation. The algorithm uses product distributions to approximate the belief state and shows how to build a representation graph of an approximate action-value function over belief space. The graph captures the result of simulating the model in aggregate under independence assumptions, giving a symbolic representation of the value function. The algorithm supports large observation spaces using sampling networks, a representation of the process of sampling values of observations, which is integrated into the graph representation. Following previous work in MDPs this approach enables action selection in POMDPs through gradient optimization over the graph representation. This approach complements recent algorithms for POMDPs which are based on particle representations of belief states and an explicit search for action selection. Our approach enables scaling to large factored action spaces in addition to large state spaces and observation spaces. An experimental evaluation demonstrates that the algorithm provides excellent performance relative to state of the art in large POMDP problems.	[Cui, Hao] Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA; [Khardon, Roni] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA	Tufts University; Indiana University System; Indiana University Bloomington	Cui, H (corresponding author), Tufts Univ, Dept Comp Sci, Medford, MA 02155 USA.	hao.cui@tufts.edu; rkhardon@iu.edu			NSF [IIS-1616280]; Adobe Data Science Research Award; Tufts Technology Services	NSF(National Science Foundation (NSF)); Adobe Data Science Research Award; Tufts Technology Services	This work was partly supported by NSF under grant IIS-1616280 and by an Adobe Data Science Research Award. Some of the experiments in this paper were performed on the Tufts Linux Research Cluster supported by Tufts Technology Services.	Bakker B, 2001, INT C NEURAL INF PRO, V14, P1475; Cassandra A., 1997, P 13 C UNC ART INT, P54; Cui H., 2016, P INT JOINT C ART IN, P3075; Cui H, 2015, AAAI CONF ARTIF INTE, P3261; Cui Hao, 2018, P ADV NEUR INF PROC, P3085; Cui Hao, 2019, P INT C AUT PLANN SC; Garg Neha Priyadarshini, 2019, ROBOTICS SCI SYSTEMS; Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Kearns M, 2002, MACH LEARN, V49, P193, DOI 10.1023/A:1017932429737; Kurniawati H., 2008, ROBOTICS SCI SYSTEMS; Littman M. L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P362; McAllester DA, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P409; Pajarinen J, 2010, LECT NOTES ARTIF INT, V6323, P1, DOI 10.1007/978-3-642-15939-8_1; PAQUET S, 2005, P 4 INT JOINT C AUT, P970; Pearl J., 1989, MORGAN KAUFMANN SERI; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Sanner S., 2010, RELATIONAL DYN UNPUB; Shani G, 2013, AUTON AGENT MULTI-AG, V27, P1, DOI 10.1007/s10458-012-9200-2; Silver D., 2010, NIPS, P2164; SMALLWOOD RD, 1973, OPER RES, V21, P1071, DOI 10.1287/opre.21.5.1071; Sunberg ZN, 2018, P I C AUTOMAT PLAN S, P259; Tesauro G., 1996, P ADV NEUR INF PROC; Ye Nan, 2017, J ARTIFICIAL INTELLI, V58	25	0	0	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900077
C	Cuturi, M; Teboul, O; Vert, JP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cuturi, Marco; Teboul, Olivier; Vert, Jean-Philippe			Differentiable Ranks and Sorting using Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Sorting is used pervasively in machine learning, either to define elementary algorithms, such as k-nearest neighbors (k-NN) rules, or to define test-time metrics, such as top-k classification accuracy or ranking losses. Sorting is however a poor match for the end-to-end, automatically differentiable pipelines of deep learning. Indeed, sorting procedures output two vectors, neither of which is differentiable: the vector of sorted values is piecewise linear, while the sorting permutation itself (or its inverse, the vector of ranks) has no differentiable properties to speak of, since it is integer-valued. We propose in this paper to replace the usual sort procedure with a differentiable proxy. Our proxy builds upon the fact that sorting can be seen as an optimal assignment problem, one in which the n values to be sorted are matched to an auxiliary probability measure supported on any increasing family of n target values. From this observation, we propose extended rank and sort operators by considering optimal transport (OT) problems (the natural relaxation for assignments) where the auxiliary measure can be any weighted measure supported on m increasing values, where m not equal n. We recover differentiable operators by regularizing these OT problems with an entropic penalty, and solve them by applying Sinkhorn iterations. Using these smoothed rank and sort operators, we propose differentiable proxies for the classification 0/1 loss as well as for the quantile regression loss.	[Cuturi, Marco; Teboul, Olivier; Vert, Jean-Philippe] Google Res, Brain Team, Mountain View, CA 94043 USA	Google Incorporated	Cuturi, M (corresponding author), Google Res, Brain Team, Mountain View, CA 94043 USA.	cuturi@google.com; oliviert@google.com; jpvert@google.com						[Anonymous], 2013, P INT C MACH LEARN I; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Birkhoff G., 1946, U NAC TACUMAN REV SE, V5, P147; Bonneel N, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818107; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Boyd S, 2012, ADV NEURAL INFORM PR, P953; Brenier Y, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2012.0343; Burges C., 2011, P LEARN RANK CHALL, P25; Burges C. J., 2007, ADV NEURAL INFORM PR, V19, P193, DOI DOI 10.1007/S10994-010-5185-8; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi M, 2014, PR MACH LEARN RES, V32, P685; David H.A., 2004, ENCY STAT SCI; Delon J, 2012, SIAM J DISCRETE MATH, V26, P801, DOI 10.1137/110823304; Flamary R, 2018, MACH LEARN, V107, P1923, DOI 10.1007/s10994-018-5717-1; FRANKLIN J, 1989, LINEAR ALGEBRA APPL, V114, P717, DOI 10.1016/0024-3795(89)90490-4; Galichon Alfred, 2009, TECHNICAL REPORT; Grover Aditya, 2019, P ICLR 2019; Hashimoto T., 2016, INT C MACH LEARN, P2417; Huber P. J., 2011, ROBUST STAT; Jarvelin K, 2002, ACM T INFORM SYST, V20, P422, DOI 10.1145/582415.582418; Jiang Ray, 2019, ARXIV190712059; Kolouri S, 2016, PROC CVPR IEEE, P5258, DOI 10.1109/CVPR.2016.568; KOSOWSKY JJ, 1994, NEURAL NETWORKS, V7, P477, DOI 10.1016/0893-6080(94)90081-7; Lecue G., 2019, ANN STAT; Lugosi G, 2019, BERNOULLI, V25, P2075, DOI 10.3150/18-BEJ1046; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Qin T, 2010, INFORM RETRIEVAL, V13, P375, DOI 10.1007/s10791-009-9124-x; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Romano Y., 2019, ARXIV190503222; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; Schmitzer B., 2016, ARXIV161006519; Tarjan RE, 1997, MATH PROGRAM, V78, P169, DOI 10.1007/BF02614369; Taylor M., 2008, P 2008 INT C WEB SEA, P77, DOI DOI 10.1145/1341531.1341544; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; WILSON AG, 1969, J TRANSP ECON POLICY, V3, P108; Zhai Shaodan, 2013, ADV NEURAL INFORM PR, V26, P872; Zisserman A., 2018, ARXIV180207595	39	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306082
C	Dai, B; Liu, Z; Dai, HJ; He, N; Gretton, A; Song, L; Schuurinans, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dai, Bo; Liu, Zhen; Dai, Hanjun; He, Niao; Gretton, Arthur; Song, Le; Schuurinans, Dale			Exponential Family Estimation via Adversarial Dynamics Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present an efficient algorithm for maximum likelihood estimation (MLE) of exponential family models, with a general parametrization of the energy function that includes neural networks. We exploit the primal-dual view of the MLE with a kinetics augmented model to obtain an estimate associated with an adversarial dual sampler. To represent this sampler, we introduce a novel neural architecture, dynamics embedding, that generalizes Hamiltonian Monte-Carlo (HMC). The proposed approach inherits the flexibility of HMC while enabling tractable entropy estimation for the augmented model. By learning both a dual sampler and the primal model simultaneously, and sharing parameters between them, we obviate the requirement to design a separate sampling procedure once the model has been trained, leading to more effective learning. We show that many existing estimators, such as contrastive divergence, pseudo/composite-likelihood, score matching, minimum Stein discrepancy estimator, non-local contrastive objectives, noise-contrastive estimation, and minimum probability flow, are special cases of the proposed approach, each expressed by a different (fixed) dual sampler. An empirical investigation shows that adapting the sampler during MLE can significantly improve on state-of-the-art estimators(1).	[Dai, Bo; Dai, Hanjun; Schuurinans, Dale] Google Res, Brain Team, Redmond, WA 02514 USA; [Liu, Zhen] Univ Montreal, Mila, Montreal, PQ, Canada; [He, Niao] Univ Illinois, Urbana, IL USA; [Gretton, Arthur] UCL, London, England; [Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Song, Le] Ant Financial, Atlanta, GA USA; [Schuurinans, Dale] Univ Alberta, Edmonton, AB, Canada	Google Incorporated; Universite de Montreal; University of Illinois System; University of Illinois Urbana-Champaign; University of London; University College London; University System of Georgia; Georgia Institute of Technology; University of Alberta	Dai, B (corresponding author), Google Res, Brain Team, Redmond, WA 02514 USA.	bodai@google.com; zhen.liu.2@umontreal.ca; hadai@google.com	Dai, Hanjun/AAQ-8943-2021		NSF [CDSE-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983]	NSF(National Science Foundation (NSF))	We thank David Duvenaud, Arnaud Doucet, George Tucker and the Google Brain team, as well as the anonymous reviewers for their insightful comments and suggestions. L.S. was supported in part by NSF grants CDS&E-1900017 D3SC, CCF-1836936 FMitF, IIS-1841351, CAREER IIS-1350983.	Barp A., 2019, ARXIV190608283; BERTSEKAS D, 1995, NONLINEAR PROGRAMMIN; BESAG J, 1975, J ROY STAT SOC D-STA, V24, P179, DOI 10.2307/2987782; Bottou L., 2017, ARXIV170107875; Boutsidis C, 2017, LINEAR ALGEBRA APPL, V533, P95, DOI 10.1016/j.laa.2017.07.004; Brown L. D, 1986, LECT NOTES MONOGRAPH, V9; Caterini A. L., 2018, ADV NEURAL INFORM PR; Dai B, 2016, JMLR WORKSH CONF PRO, V51, P985; Dai Bo, 2019, P 19 INT C ART INT S, P2321; Dai Bo, 2018, ADV NEURAL INFORM PR; Dai Zihang, 2017, INT C LEARN REPR; Dinh Laurent., 2017, INT C LEARN REPR; Du Yilun, 2019, ARXIV190308689; Feng YH, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); GONG W., 2019, INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grathwohl W., 2019, P INT C LEARN REPR; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gretton Arthur, 2019, INT C MACH LEARN; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Han I, 2015, PR MACH LEARN RES, V37, P908; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hoffman M.D, 2018, INT C LEARN REPR; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, IEEE T NEURAL NETWOR, V18, P1529, DOI 10.1109/TNN.2007.895819; Kim T., 2016, ARXIV160603439; Kindermann R., 1980, MARKOV RANDOM FIELDS, DOI [10.1090/conm/001, DOI 10.1090/CONM/001]; Kingma D., 2010, NIPS; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma Diederik P, 2018, ADV NEURAL INFORM PR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; LeCun Yann, 2006, PREDICTING STRUCTURE, V1; Lindsay BG, 1988, CONT MATH, V80, P221, DOI DOI 10.1090/CONM/080/999014; Liu Qiang, 2017, ARXIV170700797; Ma Y.-A., 2015, P 28 INT C NEURAL IN, P2917; Miyato Takeru, 2018, 6 INT C LEARNING REP, P8; Mnih Andriy, 2012, P 29 INT C INT C MAC; Neal RM, 2011, HDB MARKOV CHAIN MON, V2, P2; Rezende D.J., 2015, INT C MACH LEARN; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rockafellar R. T., 1970, PRINCETON MATH SERIE, V28; Sohl-Dickstein J., 2011, P 28 INT C INT C MAC, P905; Song J., 2017, ADV NEURAL INFORM PR, P5140; Sriperumbudur Bharath, 2017, J MACHINE LEARNING R, V18, P1830; Tieleman T., 2008, P INT C MACH LEARN; Tieleman Tijmen, 2009, P 26 ANN INT C MACH, P1033, DOI DOI 10.1145/1553374.1553506; Vickrey David, 2010, P INT C MACH LEARN; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wu YN, 2018, ANN MATH SCI APPL, V3, P211; Zhang L., 2018, ARXIV180910188; Zhu SC, 1998, SIXTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, P847, DOI 10.1109/ICCV.1998.710816	53	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902059
C	Dai, FZ; Walter, MR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dai, Falcon Z.; Walter, Matthew R.			Maximum Expected Hitting Cost of a Markov Decision Process and Informativeness of Rewards	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new complexity measure for Markov decision processes (MDPs), the maximum expected hitting cost (MEHC). This measure tightens the closely related notion of diameter [JOA10] by accounting for the reward structure. We show that this parameter replaces diameter in the upper bound on the optimal value span of an extended MDP, thus refining the associated upper bounds on the regret of several UCRL2-like algorithms. Furthermore, we show that potential-based reward shaping [NHR99] can induce equivalent reward functions with varying informativeness, as measured by MEHC. We further establish that shaping can reduce or increase MEHC by at most a factor of two in a large class of MDPs with finite MEHC and unsaturated optimal average rewards.	[Dai, Falcon Z.; Walter, Matthew R.] Toyota Technol Inst Chicago, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Dai, FZ (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	dai@ttic.edu; mwalter@ttic.edu		Walter, Matthew/0000-0003-1425-6050	National Science Foundation [1830660]	National Science Foundation(National Science Foundation (NSF))	This work was supported in part by the National Science Foundation under Grant No. 1830660. We thank Avrim Blum for many insightful comments. In particular, his challenge to finding a better example has led to Theorem 2. We also thank Ronan Fruit for a discussion on a concept similar to the proposed maximum expected hitting cost that he independently developed in his thesis draft.	Asmuth J, 2008, P 23 AAAI C ARTIFICI, P604; Bartlett RA, 2009, 2009 ICSE WORKSHOP ON SOFTWARE ENGINEERING FOR COMPUTATIONAL SCIENCE AND ENGINEERING, P35, DOI 10.1109/SECSE.2009.5069160; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Clark J., 2016, FAULTY REWARD FUNCTI; Fruit Ronan, 2018, ADV NEURAL INFORM PR, P2994; Fruit Ronan, 2019, ALG LEARN THEOR C; Fruit Ronan, 2018, JMLR WORKSHOP C P, P1573; Givan R, 2000, ARTIF INTELL, V122, P71, DOI 10.1016/S0004-3702(00)00047-3; Grzes M, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P565; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kakade Sham M., 2003, THESIS; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kober J, 2013, INT J ROBOT RES, V32, P1238, DOI 10.1177/0278364913495721; Levin D., 2008, MARKOV CHAINS MIXING; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Strehl Alexander L., 2005, P 22 INT C MACH LEAR, P856, DOI [DOI 10.1145/1102351.1102459, 10.1145/1102351.1102459]; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tewari A, 2007, LECT NOTES COMPUT SC, V4539, P263, DOI 10.1007/978-3-540-72927-3_20; Wiewiora E, 2003, J ARTIF INTELL RES, V19, P205, DOI 10.1613/jair.1190; Wiewiora E, 2003, P 20 INT C MACH LEAR, P792	21	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307067
C	Dai, HJ; Li, YJ; Wang, CL; Singh, R; Huang, PS; Kohli, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dai, Hanjun; Li, Yujia; Wang, Chenglong; Singh, Rishabh; Huang, Po-Sen; Kohli, Pushmeet			Learning Transferable Graph Exploration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper considers the problem of efficient exploration of unseen environments, a key challenge in AI. We propose a 'learning to explore' framework where we learn a policy from a distribution of environments. At test time, presented with an unseen environment from the same distribution, the policy aims to generalize the exploration strategy to visit the maximum number of unique states in a limited number of steps. We particularly focus on environments with graph-structured state-spaces that are encountered in many important real-world applications like software testing and map building. We formulate this task as a reinforcement learning problem where the 'exploration' agent is rewarded for transitioning to previously unseen environment states and employ a graph-structured memory to encode the agent's past trajectory. Experimental results demonstrate that our approach is extremely effective for exploration of spatial maps; and when applied on the challenging problems of coverage-guided software-testing of domain-specific programs and real-world mobile applications, it outperforms methods that have been hand-engineered by human experts.	[Dai, Hanjun] Georgia Inst Technol, Atlanta, GA 30332 USA; [Dai, Hanjun; Singh, Rishabh] Google Brain, Mountain View, CA 94043 USA; [Wang, Chenglong] Univ Washington, Seattle, WA 98195 USA; [Dai, Hanjun; Li, Yujia; Huang, Po-Sen; Kohli, Pushmeet] DeepMind, London, England	University System of Georgia; Georgia Institute of Technology; Google Incorporated; University of Washington; University of Washington Seattle	Dai, HJ (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.; Dai, HJ (corresponding author), Google Brain, Mountain View, CA 94043 USA.; Dai, HJ (corresponding author), DeepMind, London, England.	hadai@google.com; yujiali@google.com; clwang@cs.washington.edu; rising@google.com; posenhuang@google.com; pushmeet@google.com	Dai, Hanjun/AAQ-8943-2021					Allamanis M, 2017, LEARNING REPRESENT P; [Anonymous], 2018, CORR; Azim T, 2013, ACM SIGPLAN NOTICES, V48, P641, DOI [10.1145/2544173.2509549, 10.1145/2509136.2509549]; Battaglia Peter W, 2018, ARXIV 180601261; Bellemare M., 2016, NEURIPS; Bello I., 2016, ARXIV PREPRINT ARXIV; Bunel Rudy, 2018, CORR; Burnim J, 2009, 7TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING, P3, DOI 10.1145/1595696.1595700; Cadar C, 2013, COMMUN ACM, V56, P82, DOI 10.1145/2408776.2408795; CAI H, 2018, ADV NEURAL INFORM PR, P1537; Carlone L, 2014, J INTELL ROBOT SYST, V75, P291, DOI 10.1007/s10846-013-9981-9; Carrillo H, 2012, IEEE INT CONF ROBOT, P2080, DOI 10.1109/ICRA.2012.6224890; Dai HJ, 2016, PR MACH LEARN RES, V48; Devlin J., 2017, ARXIV170307469; Duan Y., 2016, RL2 FAST REINFORCEME; Durrant-Whyte H, 2006, IEEE ROBOT AUTOM MAG, V13, P99, DOI 10.1109/MRA.2006.1638022; EYSENBACH B, 2018, 6 INT C LEARN REPR I, P1; Gilmer Justin, 2017, ARXIV170401212; Godefroid P., 2008, NDSS; Godefroid P, 2017, IEEE INT CONF AUTOM, P50, DOI 10.1109/ASE.2017.8115618; Guez Arthur, 2018, ARXIV180204697; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Jia S., 2018, INT C LEARNING REPRE; Johnson D.D., 2016, ICLR 2017; Khalil E., 2017, ADV NEURAL INFORM PR, P6348; Kipf TN, 2016, P INT C LEARN REPR; Lei Tao, 2017, ARXIV170509037; Lemieux C, 2018, IEEE INT CONF AUTOM, P475, DOI 10.1145/3238147.3238176; Liu Xiao, 2019, DEEPFUZZ AUTOMATIC G; Mao K., 2016, P 25 INT S SOFTWARE, P94, DOI 10.1145/2931037.2931054; MILLER BP, 1990, COMMUN ACM, V33, P32, DOI 10.1145/96267.96279; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mu B, 2016, IEEE DECIS CONTR P, P5583, DOI 10.1109/CDC.2016.7799127; Nowak A., 2017, ARXIV170607450; Ostrovski G., 2017, ARXIV170301310; Pathak D., 2017, ICML; Pu Yewen, 2018, INT C MACH LEARN, V80, P4161; Rajpal M, 2017, CORR; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Selsam D., 2018, ARXIV180203685; Sen K., 2005, P 10 EUROPEAN SOFTWA, P263, DOI [DOI 10.1145/1081706.1081750, 10.1145/1081706.1081750]; She D., 2018, ARXIV180705620; Shin Richard, 2018, SYNTHETIC DATASETS N; Sutton M., 2007, FUZZING BRUTE FORCE; Velickovic P., 2017, STAT-US, V1050, P20; Wang J.X., 2016, ARXIV161105763	49	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302051
C	Dalalyan, AS; Thompson, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dalalyan, Arnak S.; Thompson, Philip			Outlier-robust estimation of a sparse linear mode using l(1)-penalized Huber's M-estimator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUB-GAUSSIAN ESTIMATORS; MATRIX; LASSO; GUARANTEES	We study the problem of estimating a p-dimensional s-sparse vector in a linear model with Gaussian design and additive noise. In the case where the labels are contaminated by at most o adversarial outliers, we prove that the l(1)-penalized Huber's M-estimator based on n samples attains the optimal rate of convergence (s/n)(1/2) + (o/n) up to a logarithmic factor. For more general design matrices, our results highlight the importance of two properties: the transfer principle and the incoherence property. These properties with suitable constants are shown to yield the optimal rates, up to log-factors, of robust estimation with adversarial contamination.	[Dalalyan, Arnak S.; Thompson, Philip] ENSAE Paristech, CREST, Palaiseau, France	Institut Polytechnique de Paris	Dalalyan, AS (corresponding author), ENSAE Paristech, CREST, Palaiseau, France.	arnak.dalalyan@ensae.fr; philipthomp@gmail.com			Investissements d'Avenir/Labex Ecodec, Labex LMH [ANR-1 lIDEX-0003/Labex Ecodec/ANR 1-LABX-0047, ANR-1 1-LABX-0056-LMH]	Investissements d'Avenir/Labex Ecodec, Labex LMH(French National Research Agency (ANR))	We would like to thank the reviewers for the careful reading of the paper and for helpful and thoughtful remarks. This work was supported by the grants Investissements d'Avenir ANR-1 lIDEX-0003/Labex Ecodec/ANR 1-LABX-0047 and ANR-1 1-LABX-0056-LMH, Labex LMH.	Adcock B, 2018, SIAM-ASA J UNCERTAIN, V6, P1424, DOI 10.1137/17M112590X; Balakrishnan S., 2017, C LEARN THEOR PMLR, P169; Balmand S., 2015, ARXIV151204734; Bellec P. C., 2017, ARXIV E PRINTS; Bellec PC, 2018, ANN STAT, V46, P3603, DOI 10.1214/17-AOS1670; Belloni A, 2016, ELECTRON J STAT, V10, P1729, DOI 10.1214/15-EJS1095; Bhatia K, 2017, ADV NEUR IN, V30; Bickel PJ, 2009, ANN STAT, V37, P1705, DOI 10.1214/08-AOS620; BOUCHERON S., 2013, CONCENTRATION INEQUA, DOI [10.1093/acprof:oso/9780199535255.001.0001, DOI 10.1093/ACPROF:OSO/9780199535255.001.0001]; Candes E, 2007, ANN STAT, V35, P2313, DOI 10.1214/009053606000001523; Candes EJ, 2008, IEEE T INFORM THEORY, V54, P2829, DOI 10.1109/TIT.2008.924688; Catoni O, 2012, ANN I H POINCARE-PR, V48, P1148, DOI 10.1214/11-AIHP454; Chen MJ, 2016, ELECTRON J STAT, V10, P3752, DOI 10.1214/16-EJS1216; Cheng Yu, 2019, P 30 ANN ACM SIAM S, P2755; Dalalyan A., 2012, ADV NEURAL INFORM PR, V25, P1268; Dalalyan A. S., 2017, ARXIV E PRINTS; Devroye L, 2016, ANN STAT, V44, P2695, DOI 10.1214/16-AOS1440; Diakonikolas I, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2683; Donoho D, 2016, PROBAB THEORY REL, V166, P935, DOI 10.1007/s00440-015-0675-z; DONOHO DL, 1992, ANN STAT, V20, P1803, DOI 10.1214/aos/1176348890; Foygel R, 2014, IEEE T INFORM THEORY, V60, P1223, DOI 10.1109/TIT.2013.2293654; Hampel E, 2005, WILEY SERIES PROBABI; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Huber PJ, 2009, WILEY SERIES PROBABI; Juditsky A, 2011, IEEE T INFORM THEORY, V57, P7818, DOI 10.1109/TIT.2011.2162569; Karmalkar S., 2018, 2 S SIMPL ALG SOSA 2; Katiyar A., 2019, ARXIV E PRINTS; Koltchinskii Vladimir, 2011, LECT NOTES MATH, V2033, DOI [10.1007/978-3-642-22147-7, DOI 10.1007/978-3-642-22147-7]; Lafferty J., 2017, ArXiv e-prints; Lai KA, 2016, ANN IEEE SYMP FOUND, P665, DOI 10.1109/FOCS.2016.76; Laska Jason N., 2009, 2009 43rd Asilomar Conference on Signals, Systems and Computers, P1556, DOI 10.1109/ACSSC.2009.5470141; Lee Y, 2012, STAT SCI, V27, P350, DOI 10.1214/11-STS377; Lerasle M., 2017, ARXIV E PRINTS; Liu L, 2020, INT J COMPUT VISION, V128, P261, DOI 10.1007/s11263-019-01247-4; Liu Liyuan, 2019, CORR; Lugosi G, 2019, ANN STAT, V47, P783, DOI 10.1214/17-AOS1639; Minsker S, 2018, ANN STAT, V46, P2871, DOI 10.1214/17-AOS1642; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2036, DOI 10.1109/TIT.2012.2232347; Oliveira R., 2013, ARXIV13122903; Raskutti G, 2010, J MACH LEARN RES, V11, P2241; Rudelson M, 2013, IEEE T INFORM THEORY, V59, P3434, DOI 10.1109/TIT.2013.2243201; Sardy S, 2001, IEEE T SIGNAL PROCES, V49, P1146, DOI 10.1109/78.923297; She YY, 2011, J AM STAT ASSOC, V106, P626, DOI 10.1198/jasa.2011.tm10390; Suggala A. S., 2019, CORR; Sun TN, 2013, J MACH LEARN RES, V14, P3385; Tukey J.W., 1960, CONTRIBUTIONS PROBAB, P448; YATRACOS YG, 1985, ANN STAT, V13, P768, DOI 10.1214/aos/1176349553; Yu C, 2017, COMMUN STAT-SIMUL C, V46, P6261, DOI 10.1080/03610918.2016.1202271	56	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904080
C	Dan, C; Wang, H; Zhang, HY; Zhou, YC; Ravikumar, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dan, Chen; Wang, Hong; Zhang, Hongyang; Zhou, Yuchen; Ravikumar, Pradeep			Optimal Analysis of Subset-Selection Based l(p) Low-Rank Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PRINCIPAL COMPONENT ANALYSIS; ROBUST MATRIX FACTORIZATION	We study the low rank approximation problem of any given matrix A over l """ and C'm in entry -wise fp loss, that is, finding a rank -1 matrix X such that is minimized Unlike the traditional,e2 setting, this particular variant is NP -Hard. We show that the algorithm of column subset selection, which was an algorithmic foundation of many existing algorithms, enjoys approximation ratio (k 1)1/P for 1 < p < 2 and (k -11)1-1/P for p > 2. This improves upon the previous 0(k + 1) bound for p > 1 W. We complement our analysis with lower bounds; these bounds match our upper bounds up to constant 1 when p > 2. At the core of our techniques is an application of Riesz-Thorin interpolation theorem from harmonic analysis, which might be of independent interest to other algorithmic designs and analysis more broadly. As a consequence of our analysis, we provide better approximation guarantees for several other algorithms with various time complexity. For example, to make the algorithm of column subset selection computationally efficient, we analyze a polynomial time bi-criteria algorithm which selects 0(k log m) columns. We show that this algorithm has an approximation ratio of 0((k 1)14) for 1 < p < 2 and 0((k 1)1-1/P) for p > 2. This improves over the best-known bound with an 0(k + 1) approximation ratio. Our bi-criteria algorithm also implies an exact -rank method in polynomial time with a slightly larger approximation ratio.	[Dan, Chen] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Wang, Hong] Princeton Univ, Princeton, NJ 08544 USA; [Zhang, Hongyang] Toyota Technol Inst, Chicago, IL USA; [Zhou, Yuchen] Univ Wisconsin, Madison, WI USA; [Ravikumar, Pradeep] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Princeton University; Toyota Technological Institute - Chicago; University of Wisconsin System; University of Wisconsin Madison; Carnegie Mellon University	Dan, C (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	cdan@cs.cmu.edu; Hong.Wang1991@gmail.com; honyanz@ttic.edu; yuchenzhou@stat.wisc.edu; pradeepr@cs.cmu.edu			Rakuten Inc.; NSF [IIS1909816]	Rakuten Inc.; NSF(National Science Foundation (NSF))	C.D. and P.R. acknowledge the support of Rakuten Inc., and NSF via IIS1909816. The authors would also like to acknowledge two MathOverflow users, known to us only by their usernames, 'fedja' and 'Mahdi', for informing us the Riesz-Thorin interpolation theorem.	BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Ban F, 2019, ANN IEEE SYMP FOUND, P745, DOI 10.1109/FOCS.2019.00050; Bhaskara Aditya, 2018, LIPICS LEIBNIZ INT P, V94; Boutsidis C, 2017, SIAM J COMPUT, V46, P543, DOI 10.1137/140977898; Boutsidis C, 2014, SIAM J COMPUT, V43, P687, DOI 10.1137/12086755X; Boutsidis C, 2009, PROCEEDINGS OF THE TWENTIETH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P968; Bringmann K., 2017, P NIPS C, P6648; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Caramanis C., 2016, P ADV NEUR INF PROC, P4152, DOI DOI 10.5555/3157382.3157562; Chiang KY, 2016, PR MACH LEARN RES, V48; Chierichetti Flavio, 2017, ARXIV170506730; Dan Chen, 2018, LEIBNIZ INT P INFORM, V117, DOI 10.4230/LIPIcs.MFCS.2018.41; Deshpande A, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1117, DOI 10.1145/1109557.1109681; Deshpande A, 2006, LECT NOTES COMPUT SC, V4110, P292; Deshpande A, 2011, PROCEEDINGS OF THE TWENTY-SECOND ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P482; Deshpande A, 2010, ANN IEEE SYMP FOUND, P329, DOI 10.1109/FOCS.2010.38; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Frieze A, 2004, J ACM, V51, P1025, DOI 10.1145/1039488.1039494; Gillis N, 2018, MATH OPERATIONS RES; Gillis Nicolas, 2017, ARXIV170600078; Huber P. J., 2011, INT ENCY STAT SCI, P1248, DOI [10.1007/978-3-642-04898-2_594, DOI 10.1007/978-3-642-04898-2_594]; Jin C, 2017, PR MACH LEARN RES, V70; Kannan R, 2008, FOUND TRENDS THEOR C, V4, P157, DOI 10.1561/0400000025; Ke QF, 2005, PROC CVPR IEEE, P739; Ke Qifa, 2005, COMP VIS PATT REC 20, V1, P739; Kyrillidis Anastasios, 2018, ARXIV180509464; Liang Xiong, 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P844, DOI 10.1109/ICDM.2011.52; Mashreghi Javad, 2009, REPRESENTATION THEOR, V74; Meng DY, 2013, IEEE I CONF COMP VIS, P1337, DOI 10.1109/ICCV.2013.169; Netrapalli P., 2014, P ADV NEUR INF PROC, P1107; Nie FP, 2014, PR MACH LEARN RES, V32, P1062; Otazo R, 2015, MAGN RESON MED, V73, P1125, DOI 10.1002/mrm.25240; Song Z., 2019, P 30 ANN ACM SIAM S, P2772, DOI DOI 10.1137/1.9781611975482.172; Song Z, 2017, ACM S THEORY COMPUT, P688, DOI 10.1145/3055399.3055431; Song Zhao, 2018, ARXIV181101442; Wang NY, 2013, IEEE I CONF COMP VIS, P1785, DOI 10.1109/ICCV.2013.224; Wang YN, 2015, JMLR WORKSH CONF PRO, V38, P1033; Wang Yining, 2015, ARXIV150504343; Xu H, 2012, IEEE T INFORM THEORY, V58, P3047, DOI 10.1109/TIT.2011.2173156; XU L, 1995, IEEE T NEURAL NETWOR, V6, P131, DOI 10.1109/72.363442	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302053
C	Daniely, A; Granot, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Daniely, Amit; Granot, Elad			Generalization Bounds for Neural Networks via Approximate Description Length	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We investigate the sample complexity of networks with bounds on the magnitude of its weights. In particular, we consider the class N = {Wt circle rho circle Wt-1 circle rho ... circle rho circle W-1 : W-1,W- ..., Wt-1 is an element of M-1,M-d} where the spectral norm of each W-i is bounded by O(1), the Frobenius norm is bounded by R, and rho is the sigmoid function e(x)/1 + e(x) or the smoothened ReLU function in (1 + e(x)). We show that for any depth t, if the inputs are in [-1, 1](d), the sample complexity of N is (O) over tilde (dR(2)/epsilon(2)). This bound is optimal up to log-factors, and substantially improves over the previous state of the art of (O) over tilde (dR(2)/epsilon(2)), that was established in a recent line of work [9, 4, 7, 5, 2, 8]. We furthermore show that this bound remains valid if instead of considering the magnitude of the W-i's, we consider the magnitude of W-i - W-i(0), where W-i(0) are some reference matrices, with spectral norm of O(1). By taking the W-i(0) to be the matrices at the onset of the training process, we get sample complexity bounds that are sub-linear in the number of parameters, in many typical regimes of parameters. To establish our results we develop a new technique to analyze the sample complexity of families W- of predictors. We start by defining a new notion of a randomized approximate description of functions f : X -> R-d. We then show that if there is a way to approximately describe functions in a class W-using d bits, then T examples suffices to guarantee uniform convergence. Namely, that the empirical loss of all the functions in the class is epsilon-close to the true loss. Finally, we develop a set of tools for calculating the approximate description length of classes of functions that can be presented as a composition of linear function classes and non-linear functions.	[Daniely, Amit; Granot, Elad] Hebrew Univ Jerusalem, Jerusalem, Israel; [Daniely, Amit] Google Res Tel Aviv, Tel Aviv, Israel	Hebrew University of Jerusalem	Daniely, A (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.; Daniely, A (corresponding author), Google Res Tel Aviv, Tel Aviv, Israel.	amit.daniely@mail.huji.ac.il; elad.granot@mail.huji.ac.il						[Anonymous], 2013, ICML 3; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Arora S., 2018, ICML; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; GOLOWICH N., 2018, COLT; Nagarajan V, 2019, ARXIV190101672; Neyshabur B., 2019, ICLR; Neyshabur Behnam, 2018, P 6 INT C LEARN REPR; Schapire R.E., 1997, P 14 INT C MACHINE L, P322; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019	12	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904062
C	Daniely, A; Feldman, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Daniely, Amit; Feldman, Vitaly			Locally Private Learning without Interaction Requires Separation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				COMMUNICATION; COMPLEXITY	We consider learning under the constraint of local differential privacy (LDP). For many learning problems known efficient algorithms in this model require many rounds of communication between the server and the clients holding the data points. Yet multi-round protocols are prohibitively slow in practice due to network latency and, as a result, currently deployed large-scale systems are limited to a single round. Despite significant research interest, very little is known about which learning problems can be solved by such non-interactive systems. The only lower bound we are aware of is for PAC learning an artificial class of functions with respect to a uniform distribution [39]. We show that the margin complexity of a class of Boolean functions is a lower bound on the complexity of any non-interactive LDP algorithm for distribution-independent PAC learning of the class. In particular, the classes of linear separators and decision lists require exponential number of samples to learn non-interactively even though they can be learned in polynomial time by an interactive LDP algorithm. This gives the first example of a natural problem that is significantly harder to solve without interaction and also resolves an open problem of Kasiviswanathan et al. [39]. We complement this lower bound with a new efficient learning algorithm whose complexity is polynomial in the margin complexity of the class. Our algorithm is non-interactive on labeled samples but still needs interactive access to unlabeled samples. All of our results also apply to the statistical query model and any model in which the number of bits communicated about each data point is constrained.	[Daniely, Amit] Hebrew Univ Jerusalem, Jerusalem, Israel; [Daniely, Amit; Feldman, Vitaly] Google Res, Mountain View, CA 94043 USA	Hebrew University of Jerusalem; Google Incorporated	Daniely, A (corresponding author), Hebrew Univ Jerusalem, Jerusalem, Israel.; Daniely, A (corresponding author), Google Res, Mountain View, CA 94043 USA.							Acharya J., 2018, ARXIV181211476; Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; [Anonymous], 1994, INTRO COMPUTATIONAL; [Anonymous], 2019, ARXIV190200582; Apple Differential Privacy Team, 2017, APPLE MACHINE LEARNI, V1; Arriaga R. I., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P616, DOI 10.1109/SFFCS.1999.814637; Balcan MF, 2015, ALGORITHMICA, V72, P282, DOI 10.1007/s00453-014-9954-9; Balkanski E, 2019, P SODA, P283; Balkanski E., 2017, STOC; Balkanski E, 2018, ACM S THEORY COMPUT, P1138, DOI 10.1145/3188745.3188752; Balkanski Eric, 2018, ABS180803880 CORR; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Ben-David S, 1998, J COMPUT SYST SCI, V56, P277, DOI 10.1006/jcss.1998.1569; Ben-David S, 2002, J MACHINE LEARNING R, V3, P441; Boser B. E., 1992, Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, P144, DOI 10.1145/130385.130401; Bshouty NH, 2002, J MACH LEARN RES, V2, P359, DOI 10.1162/153244302760200669; Buhrman H, 2007, ANN IEEE CONF COMPUT, P24, DOI 10.1109/CCC.2007.18; Daniely Amit, 2019, COLT, P3180; Daniely Amit, 2018, ARXIV180909165; Diakonikolas Jelena, 2018, ABS181101903 CORR; Ding Bolin, 2017, 31 C NEUR INF PROC S, P3574; Duchi J., 2013, ADV NEURAL INFORM PR, V26, P1529; Duchi J. C., 2018, P 31 C LEARNING THEO, P3065; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; DUNAGAN J, 2004, P 36 ANN ACM S THEOR, P315; Dwork C., 2014, ALGORITHMIC FDN DIFF, V9, DOI [10.1561/ 0400000042., DOI 10.1561/0400000042, 10.1561/0400000042]; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork Cynthia, 2014, ABS14112664 CORR; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Evfimievski A., 2003, P 22 ACM SIGACT SIGM, P211, DOI DOI 10.1145/773153.773174; Feldman V., 2015, ABS151209170 CORR; Feldman V, 2008, ACM S THEORY COMPUT, P619; Feldman Vitaly, 2012, ABS12011214 ARXIV CO; Forster J, 2001, LECT NOTES ARTIF INT, V2111, P402; GOLDMANN M, 1992, COMPUT COMPLEX, V2, P277, DOI [10.1007/BF01200426, DOI 10.1007/BF01200426]; Hardt M, 2014, ANN IEEE SYMP FOUND, P454, DOI 10.1109/FOCS.2014.55; Joseph Matthew, 2019, EXPONENTIAL SEPARATI; Joseph Matthew, 2019, ABS190403564 CORR; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Kearns M, 1998, J ACM, V45, P983, DOI 10.1145/293347.293351; Linial N, 2009, COMB PROBAB COMPUT, V18, P227, DOI 10.1017/S0963548308009656; Luo ZQ, 2005, IEEE T INFORM THEORY, V51, P2210, DOI 10.1109/TIT.2005.847692; Novikoff, 1962, P S MATH THEOR AUT, P615; Rajagopal R, 2006, 2006 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, VOLS 1-6, PROCEEDINGS, P836, DOI 10.1109/ISIT.2006.261731; Ribeiro A, 2006, IEEE T SIGNAL PROCES, V54, P1131, DOI 10.1109/TSP.2005.863009; Rivest R. L., 1987, Machine Learning, V2, P229, DOI 10.1023/A:1022607331053; Sherstov AA, 2008, COMPUT COMPLEX, V17, P149, DOI 10.1007/s00037-008-0242-4; Simon, 2011, COLT, P437; Smith A, 2017, P IEEE S SECUR PRIV, P58, DOI 10.1109/SP.2017.35; Steinhardt Jacob., 2015, C LEARN THEOR, P1564; Steinhardt Jacob, 2016, P 29 C LEARNING THEO, P1490; Suresh Ananda Theertha, 2016, ARXIV161100429; Ullman, 2015, COLT, P1588; VALIANT LG, 1984, COMMUN ACM, V27, P1134, DOI 10.1145/1968.1972; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137; Woodworth Blake E., 2018, NEURIPS, P8505; Zhang Y., 2013, NEURAL INFORM PROCES, P2328	57	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906063
C	Dauphin, YN; Schoenholz, SS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dauphin, Yann N.; Schoenholz, Samuel S.			Metalnit: Initializing learning by learning to initialize	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				TERM	Deep learning models frequently trade handcrafted features for deep features learned with much less human intervention using gradient descent. While this paradigm has been enormously successful, deep networks are often difficult to train and performance can depend crucially on the initial choice of parameters. In this work, we introduce an algorithm called Metalnit as a step towards automating the search for good initializations using meta-learning. Our approach is based on a hypothesis that good initializations make gradient descent easier by starting in regions that look locally linear with minimal second order effects. We formalize this notion via a quantity that we call the gradient quotient, which can be computed with any architecture or dataset. Metalnit minimizes this quantity efficiently by using gradient descent to tune the norms of the initial weight matrices. We conduct experiments on plain and residual networks and show that the algorithm can automatically recover from a class of bad initializations. Metalnit allows us to train networks and achieve performance competitive with the state-of-the-art without batch normalization or residual connections. In particular, we find that this approach outperforms normalization for networks without skip connections on CIFAR-10 and can scale to Resnet-50 models on Imagenet.	[Dauphin, Yann N.; Schoenholz, Samuel S.] Google AI, Cambridge, MA 02139 USA		Dauphin, YN (corresponding author), Google AI, Cambridge, MA 02139 USA.	ynd@google.com; schsam@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Abr?moff MD., 2016, INVEST OPHTHALMOL, V57, P5206; Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Baydin A. G., 2017, ARXIV170304782; Bernstein J., 2018, ARXIV180204434, V80, P560; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Carter Shan, 2019, DISTILL, V1, P2, DOI [10.23915/distill.00015, DOI 10.23915/DISTILL.00015]; Dauphin YN, 2017, PR MACH LEARN RES, V70; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Djork-Arn, ICLR 2016; Dozat T., 2016, ICLR WORKSHOP; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Finn C, 2017, PR MACH LEARN RES, V70; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Geiger M., 2019, ARXIV190101608, p2019a; Ghorbani B, 2019, PR MACH LEARN RES, V97; Gilboa D., 2019, ARXIV190108987; Gilmer Justin, 2016, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Gur-Ari Guy, 2018, ARXIV181204754; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S., GRADIENT FLOW RECURR; Jaderberg Max, 2017, ABS171109846 CORR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le, 2017, ARXIV PREPRINT ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lee J., 2019, ARXIV190206720; Loshchilov I., 2017, P INT C LEARNING REP; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Martens James, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P479, DOI 10.1007/978-3-642-35289-8_27; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Metz L, 2019, PR MACH LEARN RES, V97; Metz Luke, 2019, METALEARNING UPDATE; Mishkin Dmytro, 2015, ICLR; Nesterov Yu. E., 1983, Doklady Akademii Nauk SSSR, V269, P543; Olah Chris, 2017, DISTILL, P4, DOI DOI 10.23915/DISTILL.00007; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Philipp G., 2018, ARXIV180600179; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Saxe Andrew M, 2013, ARXIV13126120; Schoenholz S. S., 2019, ARXIV190208129; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wilson AC, 2017, ADV NEUR IN, V30; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Wu YX, 2020, INT J COMPUT VISION, V128, P742, DOI 10.1007/s11263-019-01198-w; Xiao LC, 2018, PR MACH LEARN RES, V80; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zeiler Matthew D, 2012, ARXIV12125701; Zhang Han, 2019, ICLR; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31	63	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904031
C	Degenne, R; Koolen, WM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Degenne, Remy; Koolen, Wouter M.			Pure Exploration with Multiple Correct Answers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTIARMED BANDIT	We determine the sample complexity of pure exploration bandit problems with multiple good answers. We derive a lower bound using a new game equilibrium argument. We show how continuity and convexity properties of single-answer problems ensure that the existing Track-and-Stop algorithm has asymptotically optimal sample complexity. However, that convexity is lost when going to the multiple-answer setting. We present a new algorithm which extends Track-and-Stop to the multiple-answer case and has asymptotic sample complexity matching the lower bound.	[Degenne, Remy; Koolen, Wouter M.] Ctr Wiskunde & Informat, Sci Pk 123, Amsterdam, Netherlands		Degenne, R (corresponding author), Ctr Wiskunde & Informat, Sci Pk 123, Amsterdam, Netherlands.	remy.degenne@cwi.nl; wmkoolen@cwi.nl						Audibert J.-Y., 2010, P 23 C LEARN THEOR; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Berry D.A., 1985, BANDIT PROBLEMS SEQU; Blackwell D., 1954, THEORY GAMES STAT DE, pXI, 355; Boyd S, 2004, CONVEX OPTIMIZATION; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Bubeck S, 2011, THEOR COMPUT SCI, V412, P1832, DOI 10.1016/j.tcs.2010.12.059; Castro RM, 2014, BERNOULLI, V20, P2217, DOI 10.3150/13-BEJ555; Chen L., 2017, C LEARN THEOR, V65, P482; Chen S., 2014, ADV NEURAL INFORM PR; Combes R., 2017, ADV NEURAL INFORM PR, P1763; Even-Dar E., 2002, Computational Learning Theory. 15th Annual Conference on Computational Learning Theory, COLT 2002. Proceedings (Lecture Notes in Artificial Intelligence Vol.2375), P255; Even-Dar E, 2006, J MACH LEARN RES, V7, P1079; Feinberg EA, 2014, J MATH ANAL APPL, V413, P1040, DOI 10.1016/j.jmaa.2013.12.011; Gabillon V, 2016, JMLR WORKSH CONF PRO, V51, P1004; Gabillon Victor, 2012, ADV NEURAL INFORM PR; Garivier A., 2019, ARXIV190503495; Garivier A., 2016, P 29 C LEARN THEOR C; Garivier A., 2016, C LEARN THEOR, P1028; Huang R., 2017, P MACHINE LEARING RE, V76, P593; Kalyanakrishnan S., 2012, P ICML; Kalyanakrishnan Shivaram, 2010, P 27 INT C MACHINE L; Karnin Z., 2013, ICML; Katariya S, 2017, PR MACH LEARN RES, V54, P392; Kaufmann E., 2013, P 26 C LEARN THEOR; Kaufmann E, 2017, ADV NEUR IN, V30; Kaufmann E, 2016, J MACH LEARN RES, V17; Kaufmann Emilie, 2018, P 32 C NEUR INF PROC, V31, P6333; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Locatelli A, 2016, PR MACH LEARN RES, V48; Magureanu S., 2014, P C LEARN THEOR COLT, P975; Maron O., 1997, ARTIF INTELL, V11, P113; Russo D., 2016, ABS160208448 CORR; Teraoka K, 2014, IEICE T INF SYST, VE97D, P392, DOI 10.1587/transinf.E97.D.392; Xiong J, 2014, PROCEEDINGS OF THE 2014 CONFERENCE ON EMERGING NETWORKING EXPERIMENTS AND TECHNOLOGIES (CONEXT'14), P29, DOI 10.1145/2674005.2675014; Zhou YC, 2017, PR MACH LEARN RES, V70	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906027
C	Deng, Y; Schneider, J; Sivan, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Deng, Yuan; Schneider, Jon; Sivan, Balasubramanian			Strategizing against No-regret Learners	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					How should a player who repeatedly plays a game against a no-regret learner strategize to maximize his utility? We study this question and show that under some mild assumptions, the player can always guarantee himself a utility of at least what he would get in a Stackelberg equilibrium of the game. When the no-regret learner has only two actions, we show that the player cannot get any higher utility than the Stackelberg equilibrium utility. But when the no-regret learner has more than two actions and plays a mean-based no-regret strategy, we show that the player can get strictly higher than the Stackelberg equilibrium utility. We provide a characterization of the optimal game-play for the player against a mean-based no-regret learner as a solution to a control problem. When the no-regret learner's strategy also guarantees him a no-swap regret, we show that the player cannot get anything higher than a Stackelberg equilibrium utility.	[Deng, Yuan] Duke Univ, Durham, NC 27706 USA; [Schneider, Jon; Sivan, Balasubramanian] Google Res, Mountain View, CA USA	Duke University; Google Incorporated	Deng, Y (corresponding author), Duke Univ, Durham, NC 27706 USA.	ericdy@cs.duke.edu; jschnei@google.com; balusivan@google.com						Agrawal S, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P171, DOI 10.1145/3219166.3219234; Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Blum Avrim, 2005, LEARNING THEORY; Braverman M, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P523, DOI 10.1145/3219166.3219233; Cesa-Bianchi N, 2003, MACH LEARN, V51, P239, DOI 10.1023/A:1022901500417; CesaBianchi N, 1997, J ACM, V44, P427, DOI 10.1145/258128.258179; Farkas J, 1902, J REINE ANGEW MATH, V124, P1; Foster DP, 1998, BIOMETRIKA, V85, P379, DOI 10.1093/biomet/85.2.379; Foster DP, 1997, GAME ECON BEHAV, V21, P40, DOI 10.1006/game.1997.0595; FOSTER DP, 1993, OPER RES, V41, P704, DOI 10.1287/opre.41.4.704; Foster DP, 1999, GAME ECON BEHAV, V29, P7, DOI 10.1006/game.1999.0740; Freund Y, 1999, GAME ECON BEHAV, V29, P79, DOI 10.1006/game.1999.0738; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Hannan J., 1957, CONTRIBUTIONS THEORY, P97; Hart S, 2000, ECONOMETRICA, V68, P1127, DOI 10.1111/1468-0262.00153; Von Stackelberg H., 2010, MARKET STRUCTURE EQU	18	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301055
C	Dereznski, M; Calandriello, D; Valko, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dereznski, Michal; Calandriello, Daniele; Valko, Michal			Exact sampling of dete mirtarttal point processes with sublinear time preprocessing	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				SUCCESSES; NUMBER	We study the complexity of sampling from a distribution over all index subsets of he set {1 n} with the probability of a subset S proportional to the determinant of the submatrix L5 of some n x a positive semidefinite matrix L, where Ls corresponds to the entries of L indexed by S. Known as a determinantal point process (DPP), this distribution is used in machine learning to induce diversity in subset selection. When sampling from DDPs, we often wish to sample multiple subsets S with small expected size k = E[ SII n from a very large matrix L, so it is important to minimize the preprocessing cost of the procedure (performed once) as well as the sampling cost (performed repeatedly). For this purpose we provide DPP-VFX, a new algorithm which, given access only to L. samples -actly from a determinantal point process While satisfying the folllywing two properties: (I) its preprocessing cost is n poly( k), i.e., sublinear in the size of L. and (2) its sampling cost is poly. (k), i.e., independent of the size of L. Prior to our results, state-of-the-art exact samplers required 0(03) preprocessirig time and sampling time linear in n or dependent on the spectral properties of L. We furthermore give a reduction which allows using our algorithm for exact saznpiing from cardinality constrained determinantal point processes with 71, poly(k) time preprocessing. Our implementation of DPP- VFX is provided at ht tps figithub comiguilgautier/DPPyf.	[Dereznski, Michal] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Calandriello, Daniele] Ist Italiano Tecnol, LCSL, Genoa, Italy; [Valko, Michal] DeepMind Paris, Paris, France	University of California System; University of California Berkeley; Istituto Italiano di Tecnologia - IIT	Dereznski, M (corresponding author), Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA.	mderezin@berkeley.edu; daniele.calandriello@iit.it; valkom@deepmind.com			NSF via the NSF TRIPODS program; Center for Brains, Minds and Machines (CBMM) - NSF STC [CCF-1231216]; Italian Institute of Technology	NSF via the NSF TRIPODS program; Center for Brains, Minds and Machines (CBMM) - NSF STC; Italian Institute of Technology(Istituto Italiano di Tecnologia - IIT)	MD thanks the NSF for funding via the NSF TRIPODS program. This material is based upon work supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216, and the Italian Institute of Technology. We gratefully acknowledge the support of NVIDIA Corporation for the donation of the Titan Xp GPUs and the Tesla k40 GPU used for this research.	Affandi R. H., 2013, P INT C ART INT STAT; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; ALDOUS DJ, 1990, SIAM J DISCRETE MATH, V3, P450, DOI 10.1137/0403039; Anari N., 2016, JMLR WORKSHOP C P, V49, P103; Bardenet Remi, 2017, ESAIM: Proceedings and Surveys, V60, P180, DOI 10.1051/proc/201760180; Ben Hough J, 2006, PROBAB SURV, V3, P206, DOI 10.1214/154957806000000078; Branden Petter, 2014, HDB ENUMERATIVE COMB, V10; Bringmann K, 2012, LECT NOTES COMPUT SC, V7391, P133, DOI 10.1007/978-3-642-31594-7_12; BRODER A, 1989, ANN IEEE SYMP FOUND, P442, DOI 10.1109/SFCS.1989.63516; Brunel Victor-Emmanuel, 2018, ADV NEURAL INFORM PR, V31, P7365; Burt DR, 2019, PR MACH LEARN RES, V97; Calandriello D., 2019, C LEARN THEOR, P533; Calandriello Daniele, 2017, AISTATS; Celis E., 2018, P 35 INT C MACH LEAR, P716; Chen L., 2018, ADV NEURAL INFORM PR, P5622; DARROCH JN, 1964, ANN MATH STAT, V35, P1317, DOI 10.1214/aoms/1177703287; Derezi6ski Michal, 2019, P 32 C LEARN THEOR; Derezifiski Michal, 2017, ADV NEURAL INFORM PR, V30, P3087; Dereziiski Michal, 2019, P 22 INT C ART INT S; Dereziliski Michal, 2018, ADV NEURAL INFORM PR, V31, P2510; Diaconis Persi, 1991, ANN APPL PROBABILITY; Erraqabi Akram, 2016, INT C MACH LEARN; Gautier G., 2019, J MACHINE LEARNING R; Gautier Guillaume, 2017, INT C MACH LEARN; Gillenwater J., 2012, P EMNLP CONLL, V1404, P710; Gillenwater J., 2014, APPROXIMATE INFERENC; Gillenwater Jennifer A, 2018, ADV NEURAL INFORM PR, V31, P6911; GUENOCHE A, 1983, J ALGORITHM, V4, P214, DOI 10.1016/0196-6774(83)90022-6; Kang B., 2013, ADV NEURAL INFORM PR, P2319; Kulesza Alex, 2011, ICML; Kulesza Alex, 2012, DETERMINANTAL POINT; Launay Claire, 2018, ARXIV180208429; Li CT, 2016, JMLR WORKSH CONF PRO, V51, P1328; Li Chengtao, 2016, P 30 INT C NEUR INF, P4195; Loosli Gaelle, 2007, LARGE SCALE KERNEL M, V6; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mariet Z., 2016, ADV NEURAL INFORM PR, V29, P2694; METROPOLIS N, 1949, J AM STAT ASSOC, V44, P335, DOI 10.2307/2280232; Pemantle R, 2014, COMB PROBAB COMPUT, V23, P140, DOI 10.1017/S0963548313000345; Poulson Jack, 2019, ARXIVE190500165V1; Propp JG, 1998, J ALGORITHMS, V27, P170, DOI 10.1006/jagm.1997.0917; Rebeschini Patrick, 2015, P MACHINE LEARNING R, V40, P1480; Rudi Alessandro, 2018, ADV NEURAL INFORM PR, P5672; Zhang C, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Zhang Chi, 2019, AAAI	46	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903021
C	Detlefsen, NS; Jorgensen, M; Hauberg, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Detlefsen, Nicki S.; Jorgensen, Martin; Hauberg, Soren			Reliable training and estimation of variance networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose and investigate new complementary methodologies for estimating predictive variance networks in regression neural networks. We derive a locally aware mini-batching scheme that results in sparse robust gradients, and we show how to make unbiased weight updates to a variance network. Further, we formulate a heuristic for robustly fitting both the mean and variance networks post hoc. Finally, we take inspiration from posterior Gaussian processes and propose a network architecture with similar extrapolation properties to Gaussian processes. The proposed methodologies are complementary, and improve upon baseline methods individually. Experimentally, we investigate the impact of predictive uncertainty on multiple datasets and tasks ranging from regression, active learning and generative modeling. Experiments consistently show significant improvements in predictive uncertainty estimation over state-of-the-art methods across tasks and datasets.	[Detlefsen, Nicki S.; Jorgensen, Martin; Hauberg, Soren] Tech Univ Denmark, Sect Cognit Syst, Lyngby, Denmark	Technical University of Denmark	Detlefsen, NS (corresponding author), Tech Univ Denmark, Sect Cognit Syst, Lyngby, Denmark.	nsde@dtu.dk; marjor@dtu.dk; sohau@dtu.dk			European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [757360]; VILLUM FONDEN [15334]	European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); VILLUM FONDEN(Villum Fonden)	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement no 757360). NSD, MJ and SH were supported in part by a research grant (15334) from VILLUM FONDEN. We gratefully acknowledge the support of NVIDIA Corporation with the donation of GPU hardware used for this research.	Amodei D., 2016, CONCRETE PROBLEMS AI; [Anonymous], 2009, TECHNICAL REPORT; Arvanitidis G, 2018, INT C LEARN REPR; Bishop Christopher M, 1994, TECH REP; Damianou A. C, 2013, P 16 INT C ART INT S; Frazier P.I., 2018, ARXIV, DOI 10.1287/educ.2018.0188; Fu C., 2016, EFANNA EXTREMELY FAS; Gal Y, 2016, PR MACH LEARN RES, V48; Gelman A., 2021, BAYESIAN DATA ANAL; Guo CA, 2017, PR MACH LEARN RES, V70; Hauberg S., 2018, ARXIV180604994; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1861; Hinton G., 2012, ARXIV PREPRINT ARXIV; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; HORVITZ DG, 1952, J AM STAT ASSOC, V47, P663, DOI 10.2307/2280784; Houlsby N., 2012, NIPS, V3, P2096; Huang SJ, 2014, IEEE T PATTERN ANAL, V36, P1936, DOI 10.1109/TPAMI.2014.2307881; Kendall A, 2017, ADV NEURAL INFORM PR, P5574; Kingma D.P., 2013, ICLR; Kiureghian AD, 2009, STRUCT SAF, V31, P105, DOI 10.1016/j.strusafe.2008.06.020; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; LANGE KL, 1989, J AM STAT ASSOC, V84, P881, DOI 10.2307/2290063; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Loader C, 1999, LOCFIT LOCAL REGRESS; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Maddox W., 2019, CORR; Mattei P., 2018, P ANN C NEUR INF PRO, P3859; NIX DA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P55, DOI 10.1109/ICNN.1994.374138; Pearce T, 2018, P 35 INT C MACH LEAR; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Snelson Edward, 2006, ADV NEURAL INFORM PR, V3; Szepesv?ri, 2010, SYNTHESIS LECT ARTIF, V4, DOI [10.2200/S00268ED1V01Y201005AIM009, DOI 10.2200/S00268ED1V01Y201005AIM009]; Takahashi H, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2696; TIBSHIRANI R, 1987, J AM STAT ASSOC, V82, P559, DOI 10.2307/2289465	35	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306034
C	Detorakis, G; Dutta, S; Khanna, A; Jerry, M; Datta, S; Neftci, E		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Detorakis, Georgios; Dutta, Sourav; Khanna, Abhishek; Jerry, Matthew; Datta, Suman; Neftci, Emre			Inherent Weight Normalization in Stochastic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				STATISTICAL FLUCTUATIONS	Multiplicative stochasticity such as Dropout improves the robustness and generalizability of deep neural networks. Here, we further demonstrate that always-on multiplicative stochasticity combined with simple threshold neurons are sufficient operations for deep neural networks. We call such models Neural Sampling Machines (NSM). We find that the probability of activation of the NSM exhibits a self-normalizing property that mirrors Weight Normalization, a previously studied mechanism that fulfills many of the features of Batch Normalization in an online fashion. The normalization of activities during training speeds up convergence by preventing internal covariate shift caused by changes in the input distribution. The always-on stochasticity of the NSM confers the following advantages: the network is identical in the inference and learning phases, making the NSM suitable for online learning, it can exploit stochasticity inherent to a physical substrate such as analog non-volatile memories for in-memory computing, and it is suitable for Monte Carlo sampling, while requiring almost exclusively addition and comparison operations. We demonstrate NSMs on standard classification benchmarks (MNIST and CIFAR) and event-based classification benchmarks (N-MNIST and DVS Gestures). Our results show that NSMs perform comparably or better than conventional artificial neural networks with the same architecture.	[Detorakis, Georgios] Univ Calif Irvine, Dept Cognit Sci, Irvine, CA 92697 USA; [Dutta, Sourav; Khanna, Abhishek; Jerry, Matthew; Datta, Suman] Univ Notre Dame, Dept Elect Engn, Notre Dame, IN 46556 USA; [Neftci, Emre] Univ Calif Irvine, Dept Comp Sci, Dept Cognit Sci, Irvine, CA 92697 USA	University of California System; University of California Irvine; University of Notre Dame; University of California System; University of California Irvine	Detorakis, G (corresponding author), Univ Calif Irvine, Dept Cognit Sci, Irvine, CA 92697 USA.	gdetorak@uci.edu; sdutta4@nd.edu; akhanna@nd.edu; mjerry@alumni.nd.edu; sdatta@nd.edu; eneftci@uci.edu		Neftci, Emre/0000-0002-0332-3273				ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Al-Shedivat M, 2015, IEEE J EM SEL TOP C, V5, P242, DOI 10.1109/JETCAS.2015.2435512; Ambrogio S, 2014, IEEE T ELECTRON DEV, V61, P2920, DOI 10.1109/TED.2014.2330202; Ambrogio S, 2014, IEEE T ELECTRON DEV, V61, P2912, DOI 10.1109/TED.2014.2330200; AMIR A, 2017, P IEEE C COMP VIS PA, P7243, DOI DOI 10.1109/CVPR.2017.781; Andrychowicz M, 2016, ADV NEUR IN, V29; Bengio Yoshua, 2013, ARXIV; Branco T, 2009, NAT REV NEUROSCI, V10, P373, DOI 10.1038/nrn2634; Buesing L, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002211; Carandini Matteo, 2011, NATURE REV NEUROSCIE, V13; Cohen G, 2017, IEEE IJCNN, P2921, DOI 10.1109/IJCNN.2017.7966217; Cohn D. A., 1995, Advances in Neural Information Processing Systems 7, P705; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Eryilmaz SB, 2016, IEEE T ELECTRON DEV, V63, P5004, DOI 10.1109/TED.2016.2616483; Gal Y., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goumas Georgios, 2008, ARXIV151105176, P283; Harris JJ, 2012, NEURON, V75, P762, DOI 10.1016/j.neuron.2012.08.019; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Jerry M, 2017, INT EL DEVICES MEET; Kingma D.P, P 3 INT C LEARNING R; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee D-H, 2014, ARXIV14127525; Levy WB, 2002, J NEUROSCI, V22, P4746, DOI 10.1523/JNEUROSCI.22-11-04746.2002; Liu SC, 2010, CURR OPIN NEUROBIOL, V20, P288, DOI 10.1016/j.conb.2010.03.007; Maddison Chris J, 2016, ARXIV161100712; McDonnell MD, 2011, NAT REV NEUROSCI, V12, DOI 10.1038/nrn3061-c2; Moreno-Bote R, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003522; Mulaosmanovic H, 2018, IEEE ELECTR DEVICE L, V39, P135, DOI 10.1109/LED.2017.2771818; Naous R, 2016, AIP ADV, V6, DOI 10.1063/1.4967352; Neal Radford M, 1990, LEARNING STOCHASTIC, P64; Neftci E, 2017, IEEE INT SYMP CIRC S; Neftci E, 2014, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00272; Neftci EO, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00241; Orchard G, 2015, FRONT NEUROSCI-SWITZ, V9, DOI [10.3389/fhins.2015.00437, 10.3389/fnins.2015.00437]; Paszke A., 2017, AUTOMATIC DIFFERENTI; Puri M, 2013, PROC INT CONF DOC, P1320, DOI 10.1109/ICDAR.2013.267; Querlioz D, 2015, P IEEE, V103, P1398, DOI 10.1109/JPROC.2015.2437616; Raiko Tapani, 2014, ARXIV14062989; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Rolfe J. T., 2016, ARXIV160902200; Shayer O., 2017, ARXIV171007739; Springenberg J.T., 2014, ARXIV14126806; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; WALMSLEY B, 1987, J NEUROSCI, V7, P1037; Wan L., 2013, P INT C MACHINE LEAR, P1058; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yu S., 2016, ELECTRON DEVICES MEE; Zemel R.S., 2016, P 4 INT C LEARN REPR	52	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303030
C	Devin, C; Geng, D; Abbeel, P; Darrell, T; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Devin, Coline; Geng, Daniel; Abbeel, Pieter; Darrell, Trevor; Levine, Sergey			Plan Arithmetic: Compositional Plan Vectors for Multi-Task Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Autonomous agents situated in real-world environments must be able to master large repertoires of skills. While a single short skill can be learned quickly, it would be impractical to learn every task independently. Instead, the agent should share knowledge across behaviors such that each task can be learned efficiently, and such that the resulting model can generalize to new tasks, especially ones that are compositions or subsets of tasks seen previously. A policy conditioned on a goal or demonstration has the potential to share knowledge between tasks if it sees enough diversity of inputs. However, these methods may not generalize to a more complex task at test time. We introduce compositional plan vectors (CPVs) to enable a policy to perform compositions of tasks without additional supervision. CPVs represent trajectories as the sum of the subtasks within them. We show that CPVs can be learned within a one-shot imitation learning framework without any additional supervision or information about task hierarchy, and enable a demonstration-conditioned policy to generalize to tasks that sequence twice as many skills as the tasks seen during training. Analogously to embeddings such as word2vec in NLP, CPVs can also support simple arithmetic operations - for example, we can add the CPVs for two different tasks to command an agent to compose both tasks, without any additional training.	[Devin, Coline; Geng, Daniel; Abbeel, Pieter; Darrell, Trevor; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Devin, C (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.				National Science Foundation Graduate Research Fellowship Program [DGE 1752814]	National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF))	We thank Kate Rakelly for insightful discussions and Hexiang Hu for writing the initial version of the 3D simulated environment. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE 1752814.	Andreas J, 2017, PR MACH LEARN RES, V70; [Anonymous], 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, ADV NEURAL INFORM PR; Bacon Pierre-Luc, 2016, AAAI; BI JS, 2017, ADV NEURAL INFORM PR, P108; Bin Peng X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925881; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Coros S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618516; Da Silva B, 2012, ARXIV12066398; Deisenroth MP, 2014, IEEE INT CONF ROBOT, P3876, DOI 10.1109/ICRA.2014.6907421; Devin Coline, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2169, DOI 10.1109/ICRA.2017.7989250; Devin C, 2018, IEEE INT CONF ROBOT, P7111; Finn Chelsea, 2017, C ROB LEARN PMLR, P357; Frans K., 2017, ABS171009767 CORR; Haarnoja T, 2018, PR MACH LEARN RES, V80; Haarnoja T, 2018, IEEE INT CONF ROBOT, P6244; Hausman K., 2018, INT C LEARN REPR; Heess N., 2016, ABS161005182 CORR; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; James S., 2018, ARXIV181003237; Johnson M., 2016, P 25 INT JOINT C ART, P4246; Kiros R., 2015, ADV NEURAL INF PROCE, P3294; Kupcsik Andras Gabor, 2013, 27 AAA C ART INT; Levy O, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P302, DOI 10.3115/v1/p14-2050; Liu LB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083723; Merel J., 2019, P INT C LEARN REPR; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Nagarajan T., 2018, P EUR C COMP VIS ECC, P169; Oh J, 2017, PR MACH LEARN RES, V70; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Simo-Serra E, 2015, IEEE I CONF COMP VIS, P118, DOI 10.1109/ICCV.2015.22; Singh S.P., 1992, ADV NEURAL INFORM PR, P251; Sohn Sungryull, 2018, P ADV NEUR INF PROC, P7156; Stulp F, 2013, IEEE-RAS INT C HUMAN, P417, DOI 10.1109/HUMANOIDS.2013.7030008; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Xu DC., 2018, BIOMED RES INT, V2018, P1; Zico Kolter J., 2007, P ROB SCI SYST	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906062
C	Diao, HA; Jayaram, R; Song, Z; Sun, W; Woodruff, DP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Diao, Huaian; Jayaram, Rajesh; Song, Zhao; Sun, Wen; Woodruff, David P.			Optimal Sketching for Kronecker Product Regression and Low Rank Approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EMBEDDINGS	We study the Kronecker product regression problem, in which the design matrix is a Kronecker product of two or more matrices. Formally, given Az E for i = 1, 2,..., q where 77, >> d, for each i, and b E IR.n'n2""nq, let A = Al 0 A2 0 " " " 0 Aq. Then for p E [1, 2], the goal is to find x E IR.d' dg that approximately minimizes Ax p. Recently, Diao, Song, Sun, and Woodruff (AISTATS, 2018) gave an algorithm which is faster than forming the Kronecker product A E x d,...dq Specifically, for p = 2 they achieve a running time of 0(ELi nnz(A,) + nnz (b)), where nnz(A,) is the number of non-zero entries in Az. Note that nnz(b) can be as large as e(pi " " " rig). For p = 1, q = 2 and nl = 772, they achieve a worse bound of 0(p3/2poly(did2) nnz(b)). In this work, we provide significantly faster algorithms. For p = 2, our running time is 0(ELi nnz(A,)), which has no dependence on nnz(b). For p < 2, our running time is 0(ELi nnz(A,) nnz(b)), which matches the prior best running time for p = 2. We also consider the related all -pairs regression problem, where given A E R." d, b E Rn, we want to solve minXERd Ax p, where A E 1n2 xd, b E Rn2 consist of all pairwise differences of the rows of A, b. We give an O(nnz(A)) time algorithm for p E [1, 2], improving the S2(172) time required to form A. Finally, we initiate the study of Kronecker product low rank and low t -rank approximation. For input A as above, we give 0(ELi nnz(A,)) time algorithms, which is much faster than computing A.	[Diao, Huaian] Northeast Normal Univ, Key Lab Appl Stat MOE, Changchun, Jilin, Peoples R China; [Diao, Huaian] Northeast Normal Univ, Sch Math & Stat, Changchun, Jilin, Peoples R China; [Jayaram, Rajesh; Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Jayaram, Rajesh; Song, Zhao; Woodruff, David P.] Simons Inst Theory Computing, Berkeley, CA USA; [Song, Zhao] Univ Washington, Seattle, WA 98195 USA; [Sun, Wen] Microsoft Res New York, New York, NY USA	Northeast Normal University - China; Northeast Normal University - China; Carnegie Mellon University; University of Washington; University of Washington Seattle	Diao, HA (corresponding author), Northeast Normal Univ, Key Lab Appl Stat MOE, Changchun, Jilin, Peoples R China.; Diao, HA (corresponding author), Northeast Normal Univ, Sch Math & Stat, Changchun, Jilin, Peoples R China.	hadiao@nenu.edu.cn; rkjayara@cs.cmu.edu; zhaosong@uw.edu; sun.wen@microsoft.com; dwoodruf@cs.cmu.edu	Diao, Huaian/E-2996-2018	Diao, Huaian/0000-0002-3787-9608	Office of Naval Research (ONR) [N00014-18-1-2562]	Office of Naval Research (ONR)(Office of Naval Research)	David Woodruff would like to thank support from the Office of Naval Research (ONR) grant N00014-18-1-2562. This work was also partly done while David Woodruff was visiting the Simons Institute for the Theory of Computing.	Adil Deeksha, 2019, P 30 ANN ACM SIAM S, P1405, DOI DOI 10.1137/1.9781611975482.86; Ahle Thomas D, 2020, SODA; Andoni A, 2011, ANN IEEE SYMP FOUND, P363, DOI 10.1109/FOCS.2011.82; Bakshi A., 2018, ADV NEURAL INFORM PR, P3782; Ban F, 2019, ANN IEEE SYMP FOUND, P745, DOI 10.1109/FOCS.2019.00050; Batson J, 2012, SIAM J COMPUT, V41, P1704, DOI 10.1137/090772873; Boutsidis C, 2016, ACM S THEORY COMPUT, P236, DOI 10.1145/2897518.2897646; Boutsidis C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P353, DOI 10.1145/2591796.2591819; Bubeck S, 2018, ACM S THEORY COMPUT, P1130, DOI 10.1145/3188745.3188776; Chierichetti Flavio, 2017, ICML; Clarkson KL, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P257; Clarkson KL, 2015, ANN IEEE SYMP FOUND, P310, DOI 10.1109/FOCS.2015.27; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Clarkson KL, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P466; Clarkson Kenneth L, 2015, P 26 ANN ACM SIAM S, P921; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Dasgupta A, 2009, SIAM J COMPUT, V38, P2060, DOI 10.1137/070696507; Diao Huaian, 2018, AISTATS 2018; Eilers P. H., 2006, P 21 INT WORKSH STAT; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gurobi Optimization LLC, 2020, GUROBI OPTIMIZER REF; Indyk P., 2019, COLT; Indyk P, 2006, J ACM, V53, P307, DOI 10.1145/1147954.1147955; Kane DM, 2011, ACM S THEORY COMPUT, P745; Kane DM, 2010, PROC APPL MATH, V135, P1161; Lan Wang, 2019, COMMUNICATION; Larsen KG, 2016, ANN IEEE SYMP FOUND, P61, DOI 10.1109/FOCS.2016.16; Lee Yin Tat, COLT; Li X, 2017, PRO INT CONF SCI INF, P1346; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Musco C, 2017, ANN IEEE SYMP FOUND, P672, DOI 10.1109/FOCS.2017.68; Nakos Vasileios, 2019, P 51 ANN ACM S THEOR; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Nolan J. P., 2007, STABLE DISTRIBUTIONS; Oh SY, 2005, J APPL MATHE COMPUT, V19, P151, DOI 10.1007/BF02935795; Razenshteyn I., 2016, P 48 ANN S THEOR COM; Sohler C, 2011, ACM S THEORY COMPUT, P755; Song Z., 2017, P 49 ANN S THEOR COM; Song Zhao, 2019, SODA 2019; Song Zhao, 2019, NEURIPS; Song Zhao, 2018, ARXIV181101442; Song Zhao, 2016, ADV NEURAL INFORM PR, V29, P793; Van Loan C., 1992, FRONTIERS APPL MATH, V10; Van Loan CF, 2000, J COMPUT APPL MATH, V123, P85, DOI 10.1016/S0377-0427(00)00393-9; VANLOAN CF, 1993, NATO ADV SCI INST SE, V232, P293; Wang L, 2009, J AM STAT ASSOC, V104, P1631, DOI 10.1198/jasa.2009.tm09055; Wang L, 2009, BIOMETRICS, V65, P564, DOI 10.1111/j.1541-0420.2008.01099.x; Wang Lan, 2019, NEW TUNING FREE APPR; Wang Lan, 2018, TECHNICAL REPORT; Wang Ruosong, 2019, SODA; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	52	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304071
C	Diao, HA; Song, Z; Woodruff, DP; Yang, X		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Diao, Huaian; Song, Zhao; Woodruff, David P.; Yang, Xin			Total Least Squares Regression in Input Sparsity Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In the total least squares problem, one is given an m x n matrix A, and an m x d matrix B, and one seeks to "correct" both A and B, obtaining matrices A and B, so that there exists an X satisfying the equation AX = B. Typically the problem is overconstrained, meaning that _m max(n, d). The cost of the solution A, is given by 11A A112F 11B -B112F. We give an algorithm for finding a solution X to the linear system AX = B for which the cost 11A A112F + 11B A1 2F is at most a multiplicative (1 + 6) factor times the optimal cost, up to an additive error j that may be an arbitrarily small function of n. Importantly, our running time is 0(nnz (A) + nnz(B)) poly(n/c) " d, where for a matrix C, nnz (C) denotes its number of non -zero entries. Importantly, our running time does not directly depend on the large parameter m. As total least squares regression is known to be solvable via low rank approximation, a natural approach is to invoke fast algorithms for approximate low rank approximation, obtaining matrices A and B from this low rank approximation, and then solving for X so that AX = B. However, existing algorithms do not apply since in total least squares the rank of the low rank approximation needs to be n, and so the running time of known methods would be at least mn2. In contrast, we are able to achieve a much faster running time for finding X by never explicitly forming the equation Ax = B, but instead solving for an X which is a solution to an implicit such equation. Finally, we generalize our algorithm to the total least squares problem with regularization.	[Diao, Huaian] Northeast Normal Univ, Changchun, Peoples R China; [Diao, Huaian] KLAS MOE, Beijing, Peoples R China; [Song, Zhao; Yang, Xin] Univ Washington, Seattle, WA 98195 USA; [Woodruff, David P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Northeast Normal University - China; University of Washington; University of Washington Seattle; Carnegie Mellon University	Diao, HA (corresponding author), Northeast Normal Univ, Changchun, Peoples R China.; Diao, HA (corresponding author), KLAS MOE, Beijing, Peoples R China.	hadiao@nenu.edu.cn; zhaosong@uw.edu; dwoodruf@cs.cmu.edu; yx1992@cs.washington.edu	Diao, Huaian/E-2996-2018	Diao, Huaian/0000-0002-3787-9608				Andoni Alexandr, 2018, ICML; AVRON H., APPROXIMATION RANDOM; Boutsidis C, 2016, ACM S THEORY COMPUT, P236, DOI 10.1145/2897518.2897646; Charikar M, 2002, LECT NOTES COMPUT SC, V2380, P693; Clarkson K. L., 2019, P INT C MACH LEARN, P1262; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Cohen M. B., 2019, COLT; Cohen Michael B, 2019, P 51 ANN ACM S THEOR; COPPERSMITH D, 1990, J SYMB COMPUT, V9, P251, DOI 10.1016/S0747-7171(08)80013-2; Cortez P, 2009, DECIS SUPPORT SYST, V47, P547, DOI 10.1016/j.dss.2009.05.016; Diao H., 2019, NEURIPS; Diao Huaian, 2018, AISTATS; Drineas P, 2006, LECT NOTES COMPUT SC, V4168, P304; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; Dua D., 2017, UCI MACHINE LEARNING; Friedland S, 2007, SIAM J MATRIX ANAL A, V29, P656, DOI 10.1137/06065551; Lampe J, 2014, ELECTRON T NUMER ANA, V42, P13; Lampe J, 2010, TAIWAN J MATH, V14, P885, DOI 10.11650/twjm/1500405873; Lee Yin Tat, 2019, COLT; Li X, 2017, PRO INT CONF SCI INF, P1346; Lu S, 2009, SIAM J MATRIX ANAL A, V31, P918, DOI 10.1137/070709086; Markovsky I, 2007, SIGNAL PROCESS, V87, P2283, DOI 10.1016/j.sigpro.2007.04.004; Meng XR, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P91; Nelson J, 2013, ANN IEEE SYMP FOUND, P117, DOI 10.1109/FOCS.2013.21; Price Eric, 2017, ICALP; Razenshteyn I., 2016, P 48 ANN S THEOR COM; Renaut RA, 2004, SIAM J MATRIX ANAL A, V26, P457, DOI 10.1137/S0895479802419889; Song Z., 2019, SODA 19 P 30 ANN ACM; Song Z., 2017, P 49 ANN S THEOR COM; Song Zhao, 2019, NEURIPS; Thorup M, 2012, SIAM J COMPUT, V41, P293, DOI 10.1137/100800774; van der Putten P., TECHNICAL REPORT; Van Huffel S., 1991, TOTAL LEAST SQUARES; Williams VV, 2012, STOC'12: PROCEEDINGS OF THE 2012 ACM SYMPOSIUM ON THEORY OF COMPUTING, P887; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060	36	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302048
C	Ding, J; Calderbank, R; Tarokh, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ding, Jie; Calderbank, Robert; Tarokh, Vahid			Gradient Information for Representation and Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				FISHER INFORMATION; DISTRIBUTIONS; SELECTION	Motivated by Fisher divergence, in this paper we present a new set of information quantities which we refer to as gradient information. These measures serve as surrogates for classical information measures such as those based on logarithmic loss, Kullback-Leibler divergence, directed Shannon information, etc. in many data-processing scenarios of interest, and often provide significant computational advantage, improved stability, and robustness. As an example, we apply these measures to the Chow-Liu tree algorithm, and demonstrate remarkable performance and significant computational reduction using both synthetic and real data.	[Ding, Jie] Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA; [Calderbank, Robert; Tarokh, Vahid] Duke Univ, Dept Elect & Comp Engn, Durham, NC 27708 USA	University of Minnesota System; University of Minnesota Twin Cities; Duke University	Ding, J (corresponding author), Univ Minnesota, Sch Stat, Minneapolis, MN 55455 USA.	dingj@umn.edu; robert.calderbank@duke.edu; vahid.tarokh@duke.edu	Poor, H. Vincent/S-5027-2016	Poor, H. Vincent/0000-0002-2062-131X	DARPA [HR00111890040]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	This work was supported by DARPA Grant No. HR00111890040.	Bercher JF, 2009, INFORM SCIENCES, V179, P3832, DOI 10.1016/j.ins.2009.07.013; BLACHMAN NM, 1965, IEEE T INFORM THEORY, V11, P267, DOI 10.1109/TIT.1965.1053768; Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; BROWNLEES C. T., 2017, COMMUNITY DETECTION; Carreira-Perpinan M.A., 2005, P ARTIFICIAL INTELLI, VR5:, P33; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover T.M., 2012, ELEMENTS INFORM THEO, DOI DOI 10.1002/047174882X; Dawid AP, 2015, BAYESIAN ANAL, V10, P479, DOI 10.1214/15-BA942; Dawid P, 2012, ANN STAT, V40, P593, DOI 10.1214/12-AOS972; Ding J., 2017, IEEE T INF THEORY; Ding J, 2018, IEEE SIGNAL PROC MAG, V35, P16, DOI 10.1109/MSP.2018.2867638; FRIEDEN BR, 1990, PHYS REV A, V41, P4265, DOI 10.1103/PhysRevA.41.4265; FRIEDMAN JH, 1979, ANN STAT, V7, P697, DOI 10.1214/aos/1176344722; GRANGER CWJ, 1988, J ECONOMETRICS, V39, P199, DOI 10.1016/0304-4076(88)90045-0; GRANGER CWJ, 1969, ECONOMETRICA, V37, P424, DOI 10.2307/1912791; Hamilton JD, 2012, REV ECON STAT, V94, P935, DOI 10.1162/REST_a_00197; HENDRICK.AD, 1971, ANN MATH STAT, V42, P1916, DOI 10.1214/aoms/1177693057; Holmes CC, 2017, BIOMETRIKA, V104, P497, DOI 10.1093/biomet/asx010; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Hyvarinen A, 2007, COMPUT STAT DATA AN, V51, P2499, DOI 10.1016/j.csda.2006.09.003; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Liu Q., 2016, NEURIPS; Liu Q, 2016, PR MACH LEARN RES, V48; Lyu S., 2009, P 25 C UNC ART INT, P359; Massey J., 1990, P INT S INF THEOR AP, P303; Parry M, 2012, ANN STAT, V40, P561, DOI 10.1214/12-AOS971; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Ribeiro AS, 2008, PHYS REV E, V77, DOI 10.1103/PhysRevE.77.011901; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Yu M., 2016, ADV NEURAL INFORM PR, V29, P2829; Zou H, 2005, J R STAT SOC B, V67, P301, DOI 10.1111/j.1467-9868.2005.00503.x	35	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302040
C	Ding, LZ; Yu, MY; Liu, L; Zhu, F; Liu, Y; Li, Y; Shao, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ding, Lizhong; Yu, Mengyang; Liu, Li; Zhu, Fan; Liu, Yong; Li, Yu; Shao, Ling			Two Generator Game: Learning to Sample via Linear Goodness-of-Fit Test	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				KERNEL; DIMENSIONALITY; CONSISTENCY; METRICS	Learning the probability distribution of high-dimensional data is a challenging problem. To solve this problem, we formulate a deep energy adversarial network (DEAN), which casts the energy model learned from real data into an optimization of a goodness-of-fit (GOF) test statistic. DEAN can be interpreted as a GOF game between two generative networks, where one explicit generative network learns an energy-based distribution that fits the real data, and the other implicit generative network is trained by minimizing a GOF test statistic between the energy-based distribution and the generated data, such that the underlying distribution of the generated data is close to the energy-based distribution. We design a two-level alternative optimization procedure to train the explicit and implicit generative networks, such that the hyper-parameters can also be automatically learned. Experimental results show that DEAN achieves high quality generations compared to the state-of-the-art approaches.	[Ding, Lizhong; Yu, Mengyang; Liu, Li; Zhu, Fan; Shao, Ling] IIAI, Abu Dhabi, U Arab Emirates; [Liu, Yong] Chinese Acad Sci, Inst Informat Engn, Beijing, Peoples R China; [Li, Yu] KAUST, Thuwal, Saudi Arabia	Chinese Academy of Sciences; Institute of Information Engineering, CAS; King Abdullah University of Science & Technology	Ding, LZ (corresponding author), IIAI, Abu Dhabi, U Arab Emirates.				National Natural Science Foundation of China [61703396]; CCF-Tencent Open Fund; Shenzhen Government [GJHZ20180419190732022]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CCF-Tencent Open Fund; Shenzhen Government	This work was supported in part by National Natural Science Foundation of China (No. 61703396), the CCF-Tencent Open Fund and Shenzhen Government (GJHZ20180419190732022).	Andrieu C, 2003, MACH LEARN, V50, P5, DOI 10.1023/A:1020281327116; Arbel Michael, 2018, NEURIPS 31, V31; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Brock A., 2018, ICLR, P1; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Dai Zihang, 2017, ICLR; Ding LZ, 2019, AAAI CONF ARTIF INTE, P3454; Ding LZ, 2018, AAAI CONF ARTIF INTE, P2910; Ding LZ, 2019, AAAI CONF ARTIF INTE, P3462; Ding LZ, 2017, IEEE T CYBERNETICS, V47, P554, DOI 10.1109/TCYB.2016.2520582; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Feng YH, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Fukumizu K., 2009, ADV NEURAL INFORM PR, P1750; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; GOODFELLOW IJ, 2014, NIPS 27, V27; Gorham J, 2017, PR MACH LEARN RES, V70; Gretton A, 2012, ADV NEURAL INF PROCE; Gretton A., 2008, ADV NEURAL INFORM PR, P585; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hu Z., 2018, ICLR; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jin Yanghua, 2017, ARXIV170805509; Jitkrittum W., 2017, ADV NEURAL INFORM PR, P262; Karras T., 2017, PROGR GROWING GANS I; KIEFER J, 1956, ANN MATH STAT, V27, P887, DOI 10.1214/aoms/1177728066; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liao S., 2014, P 23 ACM INT C INF K, P1159; Liu Q., 2016, ADV NEURAL INFORM PR, V29, P2378; Liu Q, 2016, PR MACH LEARN RES, V48; Liu Qiang, 2018, NEURIPS 30, V30, P8854; Liu Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2497; Liu Y, 2019, IEEE T PATTERN ANAL, V41, P1939, DOI 10.1109/TPAMI.2018.2878849; Lizhong Ding, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8724, P354, DOI 10.1007/978-3-662-44848-9_23; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Micchelli CA, 2006, J MACH LEARN RES, V7, P2651; Mirza M., 2014, ARXIV; Mroueh Youssef, 2017, ICLR; Ngiam J., 2011, P 28 INT C MACHINE L, P1105; Nowozin S, 2016, ADV NEUR IN, V29; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Palmer A, 2018, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P1009; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Saremi Saeed, 2018, ARXIV180508306; Schonborn S, 2017, INT J COMPUT VISION, V123, P160, DOI 10.1007/s11263-016-0967-5; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Stein C., 1972, PROC 6 BERKELEY S MA, VII, p583?602; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Sutherland D. J., 2017, ICLR; Sutherland Danica J, 2018, ICLR; Wang D., 2016, ARXIV161101722; Wang Ruohan, 2017, ARXIV170403817; Wang Wei, 2019, ICLR; Xie JW, 2016, PR MACH LEARN RES, V48; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; ZAREMBA W, 2013, ADV NEURAL INFORM PR, V26, P755; Zhao J., 2017, ICLR; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	76	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902084
C	Dobriban, E; Liu, SF		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dobriban, Edgar; Liu, Sifan			Asymptotics for Sketching in Least Squares	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION; ALGORITHMS	We consider a least squares regression problem where the data has been generated from a linear model, and we are interested to learn the unknown regression parameters. We consider "sketch-and-solve" methods that randomly project the data first, and do regression after. Previous works have analyzed the statistical and computational performance of such methods. However, the existing analysis is not fine-grained enough to show the fundamental differences between various methods, such as the Subsampled Randomized Hadamard Transform (SRHT) and Gaussian projections. In this paper, we make progress on this problem, working in an asymptotic framework where the number of datapoints and dimension of features goes to infinity. We find the limits of the accuracy loss (for estimation and test error) incurred by popular sketching methods. We show separation between different methods, so that SRHT is better than Gaussian projections. Our theoretical results are verified on both real and synthetic data. The analysis of SRHT relies on novel methods from random matrix theory that may be of independent interest.	[Dobriban, Edgar] Univ Penn, Dept Stat, Philadelphia, PA 19104 USA; [Liu, Sifan] Stanford Univ, Dept Stat, Stanford, CA 94305 USA	University of Pennsylvania; Stanford University	Dobriban, E (corresponding author), Univ Penn, Dept Stat, Philadelphia, PA 19104 USA.	dobriban@wharton.upenn.edu; sfliu@stanford.edu			NSF BIGDATA [IIS 1837992]; Tsinghua University Summer Research award	NSF BIGDATA; Tsinghua University Summer Research award	The authors thank Ken Clarkson, Miles Lopes, Michael Mahoney, Mert Pilanci, Garvesh Raskutti, David Woodruff for helpful discussions. ED was partially supported by NSF BIGDATA grant IIS 1837992. SL was partially supported by a Tsinghua University Summer Research award. A version of our manuscript is available on arxiv at https://arxiv.org/abs/1810.06089.	Achlioptas D., 2001, P TWENTIETHACMSIGMOD, P274, DOI DOI 10.1145/375551.375608; Ahfock D., 2017, ARXIV170603665; Ailon N., 2006, STOC'06. Proceedings of the 38th Annual ACM Symposium on Theory of Computing, P557, DOI 10.1145/1132516.1132597; Anderson GW, 2014, ADV MATH, V255, P381, DOI 10.1016/j.aim.2013.12.026; Avron H, 2014, ADV NEUR IN, V27; Bai Z, 2010, SPRINGER SER STAT, P1, DOI 10.1007/978-1-4419-0661-8; Bertin-Mahieux Thierry, 2011, P 12 INT C MUS INF R, DOI DOI 10.7916/D8NZ8J07; Cannings TI, 2017, J R STAT SOC B, V79, P959, DOI 10.1111/rssb.12228; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Couillet R., 2011, RANDOM MATRIX METHOD; Dhillon P S, 2013, ADV NEURAL INFORM PR, P360; Diao Huaian, 2017, ARXIV171209473; Drineas P, 2006, PROCEEDINGS OF THE SEVENTHEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1127, DOI 10.1145/1109557.1109682; Drineas P, 2016, COMMUN ACM, V59, P80, DOI 10.1145/2842602; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; Drineas Petros, 2017, ARXIV171208880; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hastie T, 2009, ELEMENTS STAT LEARNI; Huang ZF, 2018, PR MACH LEARN RES, V80; Kaban A, 2014, JMLR WORKSH CONF PRO, V33, P448; Kuzborskij Ilja, 2018, ARXIV180911033; Liberty E, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P581, DOI 10.1145/2487575.2487623; Lopes M., 2011, ADV NEURAL INFORM PR, P1206; Ma P, 2015, J MACH LEARN RES, V16, P861; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Maillard O., 2009, ADV NEURAL INFORM PR, P1213; Malik OA, 2018, ADV NEUR IN, V31; Mardia K.V., 1979, MULTIVARIATE ANAL; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Oymak Samet, 2017, INFORM INFERENCE J I; Papailiopoulos D, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P997, DOI 10.1145/2623330.2623698; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Raskutti G, 2016, J MACH LEARN RES, V17, P1; Sarlos T, 2006, ANN IEEE SYMP FOUND, P143; Thanei GA, 2017, CONTRIB STAT, P51, DOI 10.1007/978-3-319-41573-4_3; Tulino A. M., 2004, Foundations and Trends in Communications and Information Theory, V1, P1, DOI 10.1561/0100000001; Tulino AM, 2010, IEEE T INFORM THEORY, V56, P1187, DOI 10.1109/TIT.2009.2039041; van der Vaart A.W., 1998, ASYMPTOTIC STAT, P87, DOI [10.1017/cbo9780511802256, DOI 10.1017/CBO9780511802256, 10.1017/CBO9780511802256]; Vempala SS, 2005, RANDOM PROJECTION ME, V65; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yao J., 2015, LARGE SAMPLE COVARIA, V39	42	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303064
C	Domke, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Domke, Justin			Provable Gradient Variance Guarantees for Black-Box Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION	Recent variational inference methods use stochastic gradient estimators whose variance is not well understood. Theoretical guarantees for these estimators are important to understand when these methods will or will not work. This paper gives bounds for the common "reparameterization" estimators when the target is smooth and the variational family is a location-scale distribution. These bounds are unimprovable and thus provide the best possible guarantees under the stated assumptions.	[Domke, Justin] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Domke, J (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	domke@cs.umass.edu						Blei DM, 2017, J AM STAT ASSOC, V112, P859, DOI 10.1080/01621459.2017.1285773; BUCHHOLZ A, 2018, ICML; Domke Justin, 2019, ARXIV190108431CSSTAT; FAN K, 2015, NEURIPS; Geffner Tomas, 2018, NEURIPS; GHAHRAMANI Z, 2001, NEURIPS; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Miller Andrew, 2017, NEURIPS; Ranganath R., 2014, AISTATS; REGIER J, 2017, NEURIPS, P10; Regier Jeffrey, 2016, ARXIV161103404ASTROP; Roeder Geoffrey, 2017, NEURIPS; Ruiz Francisco J. R., 2016, ARXIV160301140STAT; Ruiz Francisco J. R., 2016, NEURIPS; Tan LSL, 2018, STAT COMPUT, V28, P259, DOI 10.1007/s11222-017-9729-7; Titsias M., 2014, ICML; Titsias Michalis K., 2015, NEURIPS; WINGATE D, 2013, ARXIV13011299CSSTAT; Winn J, 2005, J MACH LEARN RES, V6, P661; Xu Ming, 2019, AISTATS; YAO Y, 2018, ICML	22	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300030
C	Duan, YQ; Ke, ZT; Wang, MD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Duan, Yaqi; Ke, Zheng Tracy; Wang, Mengdi			State Aggregation Learning from Markov Transition Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM; BOUNDS	State aggregation is a popular model reduction method rooted in optimal control. It reduces the complexity of engineering systems by mapping the system's states into a small number of meta-states. The choice of aggregation map often depends on the data analysts' knowledge and is largely ad hoc. In this paper, we propose a tractable algorithm that estimates the probabilistic aggregation map from the system's trajectory. We adopt a soft-aggregation model, where each meta-state has a signature raw state, called an anchor state. This model includes several common state aggregation models as special cases. Our proposed method is a simple two-step algorithm: The first step is spectral decomposition of empirical transition matrix, and the second step conducts a linear transformation of singular vectors to find their approximate convex hull. It outputs the aggregation distributions and disaggregation distributions for each meta-state in explicit forms, which are not obtainable by classical spectral methods. On the theoretical side, we prove sharp error bounds for estimating the aggregation and disaggregation distributions and for identifying anchor states. The analysis relies on a new entry-wise deviation bound for singular vectors of the empirical transition matrix of a Markov process, which is of independent interest and cannot be deduced from existing literature. The application of our method to Manhattan traffic data successfully generates a data-driven state aggregation map with nice interpretations.	[Duan, Yaqi; Wang, Mengdi] Princeton Univ, Princeton, NJ 08544 USA; [Ke, Zheng Tracy] Harvard Univ, Cambridge, MA 02138 USA	Princeton University; Harvard University	Duan, YQ (corresponding author), Princeton Univ, Princeton, NJ 08544 USA.	yaqid@princeton.edu; zke@fas.harvard.edu; mengdiw@princeton.edu						[Anonymous], 2017, ARXIV PREPRINT ARXIV; Araujo MCU, 2001, CHEMOMETR INTELL LAB, V57, P65, DOI 10.1016/S0169-7439(01)00119-8; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boardman J.W., 1995, MAPPING TARGET SIGNA; Chang CI, 2006, IEEE T GEOSCI REMOTE, V44, P2804, DOI 10.1109/TGRS.2006.881803; Chen Y., 2017, ARXIV170709971; Chen YM, 2018, INT J E-PLAN RES, V7, P35, DOI 10.4018/IJEPR.2018040103; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Eldridge Justin, 2017, ARXIV170606516; Falahatgar M, 2016, IEEE INT SYMP INFO, P2689, DOI 10.1109/ISIT.2016.7541787; Gillis N., 2014, REGULARIZATION OPTIM, V12, P257; Han YJ, 2015, IEEE T INFORM THEORY, V61, P6343, DOI 10.1109/TIT.2015.2478816; Javadi H., 2019, J AM STAT ASS, P1; Jin J., 2017, ARXIV170807852; Jin JS, 2015, ANN STAT, V43, P57, DOI 10.1214/14-AOS1265; Ke Z. T., 2017, ARXIV170407016; Koltchinskii V, 2016, PROG PROBAB, V71, P397, DOI 10.1007/978-3-319-40519-3_18; Koltchinskii V, 2016, ANN I H POINCARE-PR, V52, P1976, DOI 10.1214/15-AIHP705; Kontorovich Aryeh, 2013, INT C MACH LEARN P 30 INT C MACH LEAR, P702; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Li J, 2015, ACM S THEORY COMPUT, P743, DOI 10.1145/2746539.2746584; Li X, 2018, METHOD CELL BIOL, V148, P35, DOI 10.1016/bs.mcb.2018.09.010; Nascimento JMP, 2005, IEEE T GEOSCI REMOTE, V43, P898, DOI 10.1109/TGRS.2005.844293; Rabani Y., 2014, INNOVATIONS THEORETI, P207; ROGERS DF, 1991, OPER RES, V39, P553, DOI 10.1287/opre.39.4.553; Tsitsiklis JN, 1996, MACH LEARN, V22, P59, DOI 10.1007/BF00114724; Weber M, 2005, LECT NOTES COMPUT SC, V3695, P57; Wolfer Geoffrey, 2019, INT C ALGORITHMIC LE; Yang Lin F, 2019, P 36 INT C MACH LEAR; Yi HAO, 2018, ADV NEURAL INFORM PR, V31, P648; Zhang A, 2018, ARXIV180202920; Zhong YQ, 2018, SIAM J OPTIMIZ, V28, P989, DOI 10.1137/17M1122025	37	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304048
C	Edmondson, LR; Jimenez-Rodriguez, A; Saal, HP		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Edmondson, Laura R.; Jimenez-Rodriguez, Alejandro; Saal, Hannes P.			Nonlinear scaling of resource allocation in sensory bottlenecks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INFORMATION; DENSITY; EXPLAIN	In many sensory systems, information transmission is constrained by a bottleneck, where the number of output neurons is vastly smaller than the number of input neurons. Efficient coding theory predicts that in these scenarios the brain should allocate its limited resources by removing redundant information. Previous work has typically assumed that receptors are uniformly distributed across the sensory sheet, when in reality these vary in density, often by an order of magnitude. How, then, should the brain efficiently allocate output neurons when the density of input neurons is nonuniform? Here, we show analytically and numerically that resource allocation scales nonlinearly in efficient coding models that maximize information transfer, when inputs arise from separate regions with different receptor densities. Importantly, the proportion of output neurons allocated to a given input region changes depending on the width of the bottleneck, and thus cannot be predicted from input density or region size alone. Narrow bottlenecks favor magnification of high density input regions, while wider bottlenecks often cause contraction. Our results demonstrate that both expansion and contraction of sensory input regions can arise in efficient coding models and that the final allocation crucially depends on the neural resources made available.	[Edmondson, Laura R.; Saal, Hannes P.] Univ Sheffield, Dept Psychol, Sheffield, S Yorkshire, England; [Jimenez-Rodriguez, Alejandro] Univ Sheffield, Dept Comp Sci, Sheffield, S Yorkshire, England; [Edmondson, Laura R.; Jimenez-Rodriguez, Alejandro; Saal, Hannes P.] Univ Sheffield, Sheffield Robot, Sheffield, S Yorkshire, England	University of Sheffield; University of Sheffield; University of Sheffield	Edmondson, LR (corresponding author), Univ Sheffield, Dept Psychol, Sheffield, S Yorkshire, England.; Edmondson, LR (corresponding author), Univ Sheffield, Sheffield Robot, Sheffield, S Yorkshire, England.	lredmondson1@sheffield.ac.uk; a.jimenez-rodriguez@sheffield.ac.uk; h.saal@sheffield.ac.uk		Edmondson, Laura R./0000-0001-9886-1121; Saal, Hannes/0000-0002-7544-0196	Wellcome Trust [209998/Z/17/Z]; EU Horizon 2020 program as part of the Human Brain Project [HBP-SGA2] [785907]	Wellcome Trust(Wellcome TrustEuropean Commission); EU Horizon 2020 program as part of the Human Brain Project [HBP-SGA2]	We would like to thank Mark Humphries for comments on an earlier version of this manuscript. This work was supported by the Wellcome Trust [209998/Z/17/Z] and by the EU Horizon 2020 program as part of the Human Brain Project [HBP-SGA2, 785907].	Atick JJ, 1990, NEURAL COMPUT, V2, P308, DOI 10.1162/neco.1990.2.3.308; Aticky JJ, 2011, NETWORK-COMP NEURAL, V22, P4, DOI 10.3109/0954898X.2011.638888; ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Doi E, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003761; Doi E, 2012, J NEUROSCI, V32, P16256, DOI 10.1523/JNEUROSCI.4036-12.2012; Ganguli D, 2014, NEURAL COMPUT, V26, P2103, DOI 10.1162/NECO_a_00638; Goodchild AK, 1996, J COMP NEUROL, V366, P55, DOI 10.1002/(SICI)1096-9861(19960226)366:1<55::AID-CNE5>3.0.CO;2-J; Graham DJ, 2006, VISION RES, V46, P2901, DOI 10.1016/j.visres.2006.03.008; Hyvarinen A, 2000, NEURAL NETWORKS, V13, P411, DOI 10.1016/S0893-6080(00)00026-5; Hyvarinen A, 2009, COMPUT IMAGING VIS, V39, P1; Jiang M, 2015, PROC CVPR IEEE, P1072, DOI 10.1109/CVPR.2015.7298710; JOHANSSON RS, 1979, J PHYSIOL-LONDON, V286, P283, DOI 10.1113/jphysiol.1979.sp012619; Kessy A, 2018, AM STAT, V72, P309, DOI 10.1080/00031305.2016.1277159; KOVACSTRIKO J, 1991, LINEAR ALGEBRA APPL, V145, P221, DOI 10.1016/0024-3795(91)90298-B; Lindsey J, 2019, INT C LEARN REPR ICL; Stevens CF, 2002, NEURON, V36, P139, DOI 10.1016/S0896-6273(02)00902-9; Tesileanu T, 2019, ELIFE, V8, DOI 10.7554/eLife.39279; Tishby Naftali, 2000, P 37 ANN ALL C COMM, Patent No. [physics/0004057, 0004057]; Wei XX, 2015, NAT NEUROSCI, V18, P1509, DOI 10.1038/nn.4105; Wells-Gray EM, 2016, EYE, V30, P1135, DOI 10.1038/eye.2016.107	22	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307055
C	Eiben, E; Ganian, R; Kanj, I; Szeider, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Eiben, Eduard; Ganian, Robert; Kanj, Iyad; Szeider, Stefan			The Parameterized Complexity of Cascading Portfolio Scheduling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM	Cascading portfolio scheduling is a static algorithm selection strategy which uses a sample of test instances to compute an optimal ordering (a cascading schedule) of a portfolio of available algorithms. The algorithms are then applied to each future instance according to this cascading schedule, until some algorithm in the schedule succeeds. Cascading scheduling has proven to be effective in several applications, including QBF solving and generation of ImageNet classification models. It is known that the computation of an optimal cascading schedule in the offline phase is NP-hard. In this paper we study the parameterized complexity of this problem and establish its fixed-parameter tractability by utilizing structural properties of the success relation between algorithms and test instances. Our findings are significant as they reveal that in spite of the intractability of the problem in its general form, one can indeed exploit sparseness or density of the success relation to obtain non-trivial runtime guarantees for finding an optimal cascading schedule.	[Eiben, Eduard] Royal Holloway Univ London, Dept CS, London, England; [Ganian, Robert; Szeider, Stefan] TU Wien, Algorithms & Complex Grp, Vienna, Austria; [Kanj, Iyad] Depaul Univ, Sch Comp, Chicago, IL 60604 USA	University of London; Royal Holloway University London; Technische Universitat Wien; DePaul University	Eiben, E (corresponding author), Royal Holloway Univ London, Dept CS, London, England.				Austrian Science Fund (FWF) [P 32441, P 31336]	Austrian Science Fund (FWF)(Austrian Science Fund (FWF))	Robert Ganian acknowledges the support by the Austrian Science Fund (FWF), Project P 31336, and is also affiliated with FI MUNI, Brno, Czech Republic. Stefan Szeider acknowledges the support by the Austrian Science Fund (FWF), Project P 32441.	Bodlaender HL, 2016, SIAM J COMPUT, V45, P317, DOI 10.1137/130947374; Bodlaender HL, 2005, LECT NOTES COMPUT SC, V3381, P1; Cygan M., 2015, PARAMETERIZED ALGORI; Downey R. G., 2013, TEXTS COMPUTER SCI; Feige U, 2004, ALGORITHMICA, V40, P219, DOI 10.1007/s00453-004-1110-5; Flum Jorg, 2006, TEXTS THEORETICAL CO, VXIV; Ganian R, 2018, PR MACH LEARN RES, V80; Gottlob G, 2008, COMPUT J, V51, P303, DOI 10.1093/comjnl/bxm056; Gottlob G, 2010, ARTIF INTELL, V174, P105, DOI 10.1016/j.artint.2009.10.003; Hoos HH, 2018, LECT NOTES COMPUT SC, V11008, P195, DOI 10.1007/978-3-319-98334-9_13; Ito Shinji, 2018, ADV NEURAL INFORM PR, V31, P10611; Kerschke P, 2019, EVOL COMPUT, V27, P577, DOI 10.1162/evco_a_00234; Kotthoff L, 2014, AI MAG, V35, P48, DOI 10.1609/aimag.v35i3.2460; Lindauer M., 2018, HDB PARALLEL CONSTRA, P583, DOI [10.1007/978-3-319-63516-315, DOI 10.1007/978-3-319-63516-315]; Lindauer M, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P5025; Luo Haipeng, 2018, ADV NEURAL INFORM PR, V31, P8245; NIEDERMEIER R., 2006, OXFORD LECT SERIES M; Pulina L, 2009, CONSTRAINTS, V14, P80, DOI 10.1007/s10601-008-9051-2; Rice J. R., 1976, Advances in computers, vol.15, P65, DOI 10.1016/S0065-2458(08)60520-3; Rizzini M, 2017, INT J ARTIF INTELL T, V26, DOI 10.1142/S0218213017600065; ROBERTSON N, 1984, J COMB THEORY B, V36, P49, DOI 10.1016/0095-8956(84)90013-3; Roussel Olivier, 2012, P SAT CHALL 2012, P47; Streeter Matthew, 2018, JMLR WORKSHOP C P, V80, P4759	23	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307066
C	Emami, M; Sahraee-Ardakan, M; Rangan, S; Fletcher, AK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Emami, Melikasadat; Sahraee-Ardakan, Mojtaba; Rangan, Sundeep; Fletcher, Alyson K.			Input-Output Equivalence of Unitary and Contractive RNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unitary recurrent neural networks (URNNs) have been proposed as a method to overcome the vanishing and exploding gradient problem in modeling data with long-term dependencies. A basic question is how restrictive is the unitary constraint on the possible input-output mappings of such a network? This work shows that for any contractive RNN with ReLU activations, there is a URNN with at most twice the number of hidden states and the identical input-output mapping. Hence, with ReLU activations, URNNs are as expressive as general RNNs. In contrast, for certain smooth activations, it is shown that the input-output mapping of an RNN cannot be matched with a URNN, even with an arbitrary number of states. The theoretical results are supported by experiments on modeling of slowly-varying dynamical systems.	[Emami, Melikasadat; Sahraee-Ardakan, Mojtaba] Univ Calif Los Angeles, Dept ECE, Los Angeles, CA 90095 USA; [Rangan, Sundeep] NYU, Dept ECE, New York, NY USA; [Fletcher, Alyson K.] Univ Calif Los Angeles, Dept Stat, Los Angeles, CA USA	University of California System; University of California Los Angeles; New York University; University of California System; University of California Los Angeles	Emami, M (corresponding author), Univ Calif Los Angeles, Dept ECE, Los Angeles, CA 90095 USA.	emami@ucla.edu; msahraee@ucla.edu; srangan@nyu.edu; akfletcher@ucla.edu			National Science Foundation [1116589, 1302336, 1547332, 1254204, 1738286]; Office of Naval Research [N00014-15-1-2677]; NIST; industrial affiliates of NYU WIRELESS; SRC	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); NIST(National Institute of Standards & Technology (NIST) - USA); industrial affiliates of NYU WIRELESS; SRC	The work of M. Emami, M. Sahraee-Ardakan, A. K. Fletcher was supported in part by the National Science Foundation under Grants 1254204 and 1738286, and the Office of Naval Research under Grant N00014-15-1-2677. S. Rangan was supported in part by the National Science Foundation under Grants 1116589, 1302336, and 1547332, NIST, the industrial affiliates of NYU WIRELESS, and the SRC.	Arjovsky M, 2016, PR MACH LEARN RES, V48; BENGIO Y, 1993, 1993 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOLS 1-3, P1183, DOI 10.1109/ICNN.1993.298725; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Fletcher AK, 2018, IEEE INT SYMP INFO, P1884; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jing L., 2017, INT C MACHINE LEARNI, P1733; Jing L, 2019, NEURAL COMPUT, V31, P765, DOI 10.1162/neco_a_01174; Kingma D.P, P 3 INT C LEARNING R; Mhammedi Z, 2017, PR MACH LEARN RES, V70; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Perez-Cruz F, 2018, INT C ART INT STAT, V84, P1924; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; STEWART GW, 1980, SIAM J NUMER ANAL, V17, P403, DOI 10.1137/0717034; Strang G., 1993, INTRO LINEAR ALGEBRA, V3; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vidyasagar M., 2002, NONLINEAR SYSTEMS AN, V42; Vorontsov E, 2017, PR MACH LEARN RES, V70; White OL, 2004, PHYS REV LETT, V92, DOI 10.1103/PhysRevLett.92.148102; Wisdom Scott, 2016, ADV NEURAL INFORM PR	28	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907007
C	Emelianenko, D; Voita, E; Serdyukov, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Emelianenko, Dmitrii; Voita, Elena; Serdyukov, Pavel			Sequence Modeling with Unconstrained Generation Order	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The dominant approach to sequence generation is to produce a sequence in some predefined order, e.g. left to right. In contrast, we propose a more general model that can generate the output sequence by inserting tokens in any arbitrary order. Our model learns decoding order as a result of its training procedure. Our experiments show that this model is superior to fixed order models on a number of sequence generation tasks, such as Machine Translation, Image-to-LaTeX and Image Captioning.(1)	[Emelianenko, Dmitrii; Voita, Elena; Serdyukov, Pavel] Yandex, Moscow, Russia; [Emelianenko, Dmitrii] Natl Res Univ Higher Sch Econ, Moscow, Russia; [Voita, Elena] Univ Amsterdam, Amsterdam, Netherlands	HSE University (National Research University Higher School of Economics); University of Amsterdam	Emelianenko, D (corresponding author), Yandex, Moscow, Russia.; Emelianenko, D (corresponding author), Natl Res Univ Higher Sch Econ, Moscow, Russia.	dimdi-y@yandex-team.ru; lena-voita@yandex-team.ru; pavser@yandex-team.ru						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2014, ABS14093215 CORR; [Anonymous], 2018, ARXIV180803867; Bahdanau D., 2015, P 3 INT C LEARNING R; Barrault L., 2019, 4 C MACH TRANSL WMT, V2, P1; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Berg T., 2011, STRUCTURE LANGUAGE D; Briot J.-P, 2017, ABS170901620 CORR; Chen X, 2015, CORR, V1504, P325; Dehghani M., 2018, ABS180703819 CORR; Deng YT, 2017, PR MACH LEARN RES, V70; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Federmann Christian, 2018, P 3 C MACH TRANSL SH, P272, DOI DOI 10.18653/V1/W18-6401; Ford Nicolas, 2018, P 2018 C EMPIRICAL M, P2942; Gehring Jonas, 2017, ABS170503122 CORR; Geng Xinwei, 2018, P 2018 C EMP METH NA, P523, DOI DOI 10.18653/V1/D18-1048; Gu Jiatao, 2019, INSERTION BASED DECO; Gu Jiatao, 2018, P ICLR; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Koehn P., 2004, P EMNLP, P8014; Manning CD, 2014, PROCEEDINGS OF 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS: SYSTEM DEMONSTRATIONS, P55, DOI 10.3115/v1/p14-5010; Mehri Shikib, 2018, ADV NEURAL INFORM PR, V31, P5518; Mikolov T., 2010, INTERSPEECH; Nakazawa T, 2016, LREC; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Post M, 2018, P 3 C MACH TRANSL RE, P186; Qian Q, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4279; Riloff Ellen, 2018, P 2018 C EMP METH NA; Sennrich R., 2016, ACL; Singh S. S., 2018, TEACHING MACHINES CO; Stern Mitchell, 2019, INSERTION TRANSFORME; Vaswani Ashish, 2017, ADV NEURAL INFORM PR, P5998; Vinyals O., 2016, P ICLR; Watanabe T, 2002, COLING 2002 19 INT C, V1, P1; Welleck Sean, 2019, NONMONOTONIC SEQUENT; Wu L, 2018, INT J CHEM REACT ENG, V16, DOI 10.1515/ijcre-2018-0018; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xia YC, 2017, ADV NEUR IN, V30; Xu Kelvin, 2015, ABS150203044 CORR	41	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307069
C	Esencayi, Y; Gaboardi, M; Li, S; Wang, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Esencayi, Yunus; Gaboardi, Marco; Li, Shi; Wang, Di			Facility Location Problem in Differential Privacy Model Revisited	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper we study the uncapacitated facility location problem in the model of differential privacy (DP) with uniform facility cost. Specifically, we first show that, under the hierarchically well-separated tree (HST) metrics and the super-set output setting that was introduced in [8], there is an epsilon-DP algorithm that achieves an O(1/epsilon) (expected multiplicative) approximation ratio; this implies an O( log n/epsilon) approximation ratio for the general metric case, where n is the size of the input metric. These bounds improve the best-known results given by [8]. In particular, our approximation ratio for HST-metrics is independent of n, and the ratio for general metrics is independent of the aspect ratio of the input metric. On the negative side, we show that the approximation ratio of any epsilon-DP algorithm is lower bounded by Omega( 1/root epsilon), even for instances on HST metrics with uniform facility cost, under the super-set output setting. The lower bound shows that the dependence of the approximation ratio for HST metrics on epsilon can not be removed or greatly improved. Our novel methods and techniques for both the upper and lower bound may find additional applications.	[Esencayi, Yunus; Li, Shi; Wang, Di] SUNY Buffalo, Buffalo, NY 14260 USA; [Gaboardi, Marco] Boston Univ, Boston, MA 02215 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; Boston University	Esencayi, Y (corresponding author), SUNY Buffalo, Buffalo, NY 14260 USA.	yunusese@buffalo.edu; gaboardi@bu.edu; shil@buffalo.edu; dwang45@buffalo.edu			NSF [CCF-1566356, CCF-1717138, CCF-1844890, CCF-1718220, CCF-1716400]	NSF(National Science Foundation (NSF))	Yunus Esencayi is supported in part by NSF grants CCF-1566356. Part of the work was done when Di Wang and Marco Gaboardi were visiting the Simons Institute of the Theory for Computing. Marco Gaboardi is supported in part by NSF through grant CCF-1718220. Di Wang is supported in part by NSF through grant CCF-1716400. Shi Li is supported in part by NSF grants CCF-1566356, CCF-1717138 and CCF-1844890.	Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Balcan MF, 2017, PR MACH LEARN RES, V70; Cardoso A. R., 2019, 22 INT C ART INT STA, P1650; Charikar M., 1999, 40th Annual Symposium on Foundations of Computer Science (Cat. No.99CB37039), P378, DOI 10.1109/SFFCS.1999.814609; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Fakcharoenphol J, 2004, J COMPUT SYST SCI, V69, P485, DOI 10.1016/j.jcss.2004.04.01; Feldman D, 2009, ACM S THEORY COMPUT, P361; Gupta A, 2010, PROC APPL MATH, V135, P1106; Hsu J, 2016, SIAM J COMPUT, V45, P1953, DOI 10.1137/15100271X; Hsu Justin, 2016, P 27 ANN ACM SIAM S, P580; Huang ZY, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P343; Huang Zhiyi, 2019, ARXIV190500767; Jain K, 2001, J ACM, V48, P274, DOI 10.1145/375827.375845; Kearns M, 2014, P 5 C INN THEOR COMP, P403; Li S, 2011, LECT NOTES COMPUT SC, V6756, P77, DOI 10.1007/978-3-642-22012-8_5; Li Shi, 2019, P 30 ANN ACM SIAM S, P2279, DOI DOI 10.1137/1.9781611975482.138; Mitrovic M, 2017, PR MACH LEARN RES, V70; Nissim Kobbi, 2012, 3 INN THEOR COMP SCI, P203; Shmoys D.B., 1997, APPROXIMATION ALGORI, V1997; THIZY JM, 1994, INFOR, V32, P1	20	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900012
C	Fan, XH; Li, B; Sisson, SA; Li, CY; Chen, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fan, Xuhui; Li, Bin; Sisson, Scott A.; Li, Caoyuan; Chen, Ling			Scalable Deep Generative Relational Models with High-Order Node Dependence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a probabilistic framework for modelling and exploring the latent structure of relational data. Given feature information for the nodes in a network, the scalable deep generative relational model (SDREM) builds a deep network architecture that can approximate potential nonlinear mappings between nodes' feature information and the nodes' latent representations. Our contribution is two-fold: (1) We incorporate high-order neighbourhood structure information to generate the latent representations at each node, which vary smoothly over the network. (2) Due to the Dirichlet random variable structure of the latent representations, we introduce a novel data augmentation trick which permits efficient Gibbs sampling. The SDREM can be used for large sparse networks as its computational cost scales with the number of positive links. We demonstrate its competitive performance through improved link prediction performance on a range of real-world datasets.	[Fan, Xuhui; Sisson, Scott A.] Univ New South Wales, Sch Math & Stat, Sydney, NSW, Australia; [Li, Bin] Fudan Univ, Shanghai Key Lab IlP, Shanghai, Peoples R China; [Li, Bin] Fudan Univ, Sch Comp Sci, Shanghai, Peoples R China; [Li, Caoyuan; Chen, Ling] Univ Technol, Fac Engn & IT, Sydney, NSW, Australia	University of New South Wales Sydney; Fudan University; Fudan University; University of Technology Sydney	Fan, XH (corresponding author), Univ New South Wales, Sch Math & Stat, Sydney, NSW, Australia.	xuhui.fan@unsw.edu.au; libin@fudan.edu.cn; scott.sisson@unsw.edu.au	Fan, Xu/GSE-2196-2022	Chen, Ling/0000-0002-6468-5729; Fan, Xuhui/0000-0002-7558-7200	Australian Research Council through the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS) [CE140100049]; Shanghai Municipal Science & Technology Commission [16JC1420401]; Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning;  [DP160102544]	Australian Research Council through the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS)(Australian Research Council); Shanghai Municipal Science & Technology Commission(Science & Technology Commission of Shanghai Municipality (STCSM)); Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning; 	Xuhui Fan and Scott A. Sisson are supported by the Australian Research Council through the Australian Centre of Excellence in Mathematical and Statistical Frontiers (ACEMS, CE140100049), and Scott A. Sisson through the Discovery Project Scheme (DP160102544). Bin Li is supported by Shanghai Municipal Science & Technology Commission (16JC1420401) and the Program for Professor of Special Appointment (Eastern Scholar) at Shanghai Institutions of Higher Learning.	Airoldi Edo M, 2009, ADV NEURAL INFORM PR, P33; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Dai HJ, 2018, PR MACH LEARN RES, V80; Dunson DB, 2005, BIOSTATISTICS, V6, P11, DOI 10.1093/biostatistics/kxh025; Duvenaud David K, 2015, P NIPS; Fan X., 2018, AISTATS, P1859; Fan XH, 2019, PR MACH LEARN RES, V89; Fan XH, 2016, AAAI CONF ARTIF INTE, P1547; Fan XH, 2018, ADV NEUR IN, V31; Gan Z, 2015, JMLR WORKSH CONF PRO, V38, P268; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Ho QR, 2012, J AM STAT ASSOC, V107, P916, DOI 10.1080/01621459.2012.682530; Hu CW, 2017, PR MACH LEARN RES, V70; Hu CW, 2016, JMLR WORKSH CONF PRO, V51, P1124; Huopaniemi I, 2010, BIOINFORMATICS, V26, pi391, DOI 10.1093/bioinformatics/btq174; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kemp Charles, 2006, AAAI, DOI DOI 10.1145/1837026.1837061; Kim Dae Il, 2012, 29 INT C MACH LEARN, P1559; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Kipf TN, 2016, P INT C LEARN REPR; Li B., 2009, P 26 ANN INT C MACH, P617, DOI DOI 10.1145/1553374.1553454; Mehta N, 2019, PR MACH LEARN RES, V97; Miller Kurt T., 2009, NONPARAMETRIC LATENT, P1276; Newman MEJ, 2004, PHYS REV E, V70, DOI [10.1103/PhysRevE.70.056131, 10.1103/PhysRevE.69.026113]; Nowicki K, 2001, J AM STAT ASSOC, V96, P1077, DOI 10.1198/016214501753208735; Palla K., 2012, P 29 INT C MACH LEAR, P1607; Phaedon-Stelios Koutsourelakis, 2008, AAAI; Porteous I., 2008, AAAI, V3, P1487; Rai P., 2015, ADV NEURAL INF PROCE, V28, P1805; ROMAN SM, 1978, ADV MATH, V27, P95, DOI 10.1016/0001-8708(78)90087-7; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Zhang Yingxue, 2018, ARXIV181111103; Zhao H, 2017, PR MACH LEARN RES, V70; Zhao He, 2018, NEURIPS, P7966; Zhou MY, 2015, JMLR WORKSH CONF PRO, V38, P1135; Zhou MY, 2016, J MACH LEARN RES, V17, P1; Zitnik M, 2017, BIOINFORMATICS, V33, pI190, DOI 10.1093/bioinformatics/btx252	38	0	0	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904032
C	Fan, YF; Gao, TR; Zhao, ZZ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fan, Yifeng; Gao, Tingran; Zhao, Zhizhen			Unsupervised Co-Learning on G-Manifolds Across Irreducible Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				GEOMETRIC DIFFUSIONS; STRUCTURE DEFINITION; THEORETIC PATTERNS; HARMONIC-ANALYSIS; CLASSIFICATION; MODELS; TOOL	We introduce a novel co-learning paradigm for manifolds naturally admitting an action of a transformation group G, motivated by recent developments on learning a manifold from attached fibre bundle structures. We utilize a representation theoretic mechanism that canonically associates multiple independent vector bundles over a common base manifold, which provides multiple views for the geometry of the underlying manifold. The consistency across these fibre bundles provide a common base for performing unsupervised manifold co-learning through the redundancy created artificially across irreducible representations of the transformation group. We demonstrate the efficacy of our proposed algorithmic paradigm through drastically improved robust nearest neighbor identification in cryo-electron microscopy image analysis and the clustering accuracy in community detection.	[Fan, Yifeng; Zhao, Zhizhen] Univ Illinois, Champaign, IL 61820 USA; [Gao, Tingran] Univ Chicago, Chicago, IL 60637 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Chicago	Fan, YF (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	yifengf2@illinois.edu; tingrangao@galton.uchicago.edu; zhizhenz@illinois.edu			National Science Foundation [DMS-185479, DMS-1854831]	National Science Foundation(National Science Foundation (NSF))	This work is supported in part by the National Science Foundation DMS-185479 and DMS-1854831.	ALEKSEEVSKY AV, 1993, ANN GLOB ANAL GEOM, V11, P197, DOI [10.1007/BF00773366, DOI 10.1007/BF00773366]; Belkin M, 2002, ADV NEUR IN, V14, P585; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Berline N., 2003, HEAT KERNELS DIRAC O; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Breiding P, 2018, REV MAT COMPLUT, V31, P545, DOI 10.1007/s13163-018-0273-6; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Chandrasekaran V, 2009, ANN ALLERTON CONF, P962, DOI 10.1109/ALLERTON.2009.5394889; Chen M., 2011, ADV NEURAL INFORM PR, P2456, DOI DOI 10.1016/B978-012545025-6/50150-7; Coifman RR, 2006, APPL COMPUT HARMON A, V21, P5, DOI 10.1016/j.acha.2006.04.006; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7426, DOI 10.1073/pnas.0500334102; Coifman RR, 2005, P NATL ACAD SCI USA, V102, P7432, DOI 10.1073/pnas.0500896102; Donoho DL, 2003, P NATL ACAD SCI USA, V100, P5591, DOI 10.1073/pnas.1031596100; Fan YF, 2019, PR MACH LEARN RES, V97; Farquhar J., 2006, ADV NEURAL INFORM PR, P355; Gao T., 2016, ARXIV160202330; Gao TR, 2019, PR MACH LEARN RES, V97; Guth E., 1932, MONATSHEFTE MATH PHY, V39, pA51; Minh HQ, 2016, J MACH LEARN RES, V17; Hadani R, 2011, FOUND COMPUT MATH, V11, P589, DOI 10.1007/s10208-011-9095-3; Hadani R, 2011, ANN MATH, V174, P1219, DOI 10.4007/annals.2011.174.2.11; Hall B., 2015, LIE GROUPS LIE ALGEB, V222; Kakarala R, 2010, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2010.5540222; Kendall David G, 1989, STAT SCI, P6, DOI DOI 10.1214/SS/1177012582; KETTENRING JR, 1971, BIOMETRIKA, V58, P433, DOI 10.1093/biomet/58.3.433; Kobayashi, 2012, TRANSFORMATION GROUP; Kondor Imre Risi, 2008, THESIS; Kondor R., 2007, ARXIV0701127; Kondor R, 2018, PR MACH LEARN RES, V80; Kumar A., 2011, P 28 INT C MACH LEAR, P393, DOI DOI 10.5555/3104482.3104532; Kumar Abhishek, 2011, NEURIPS, P2, DOI DOI 10.5555/2986459.2986617; Landa B, 2018, SIAM J IMAGING SCI, V11, P2254, DOI 10.1137/18M1169394; Li YM, 2019, IEEE T KNOWL DATA EN, V31, P1863, DOI 10.1109/TKDE.2018.2872063; Lin C.-Y., 2018, FRONT APPL MATH STAT, V4, P21; Maslen D. K., 1997, GROUPS COMPUTATION 2, V28, P183; Michor P. W., 2008, TOPICS DIFFERENTIAL, V93; Mumford D., 1994, GEOMETRIC INVARIANT, V3rd, P34; Muslea I, 2006, J ARTIF INTELL RES, V27, P203, DOI 10.1613/jair.2005; Nadler B., 2006, ADV NEURAL INFORM PR, P955; Ng AY, 2002, ADV NEUR IN, V14, P849; Ongie G, 2017, PR MACH LEARN RES, V70; Penczek PA, 1996, ULTRAMICROSCOPY, V63, P205, DOI 10.1016/0304-3991(96)00037-X; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Raskutti G, 2012, J MACH LEARN RES, V13, P389; Richardson K, 2001, ILLINOIS J MATH, V45, P517, DOI 10.1215/ijm/1258138353; Rokhlin V, 2009, SIAM J MATRIX ANAL A, V31, P1100, DOI 10.1137/080736417; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Schmitt AlexanderHW, 2008, GEOMETRIC INVARIANT, V11; Serre J-P, 1977, GRADUATE TEXTS MATH, V42; Sindhwani V., 2005, P ICML WORKSH LEARN, P74; Sindhwani V., 2008, INT C MACH LEARN, V307, P976, DOI DOI 10.1145/1390156.1390279; Singer A, 2011, SIAM J IMAGING SCI, V4, P723, DOI 10.1137/090778390; Singer A, 2017, INF INFERENCE, V6, P58, DOI 10.1093/imaiai/iaw016; Sun SL, 2013, NEURAL COMPUT APPL, V23, P2031, DOI 10.1007/s00521-013-1362-6; Sun SL, 2010, NEUROCOMPUTING, V73, P2980, DOI 10.1016/j.neucom.2010.07.007; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Tibshirani R., 2015, STAT LEARNING SPARSI; tom Dieck T., 2013, REPRESENTATIONS COMP, V98; Vershynin R., 2018, HIGH DIMENSIONAL PRO, V47; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Vural E, 2018, J MACH LEARN RES, V18, P1; Wainwright M.J., 2019, HIGH DIMENSIONAL STA, V48; Xu C., 2013, ARXIV13045634; Zhao J, 2017, INFORM FUSION, V38, P43, DOI 10.1016/j.inffus.2017.02.007; Zhao Z., 2019, ARXIV190407772; Zhao ZZ, 2016, IEEE T COMPUT IMAG, V2, P1, DOI 10.1109/TCI.2016.2514700; Zhao ZZ, 2014, J STRUCT BIOL, V186, P153, DOI 10.1016/j.jsb.2014.03.003	71	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900061
C	Farahmand, AM		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Farahmand, Amir-massoud			Value Function in Frequency Domain and the Characteristic Value Iteration Algorithm	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				EMPIRICAL CHARACTERISTIC FUNCTION	This paper considers the problem of estimating the distribution of returns in reinforcement learning, i.e., distributional RL problem. It presents a new representational framework to maintain the uncertainty of returns and provides mathematical tools to compute it. We show that instead of representing a probability distribution function of returns, one can represent their characteristic function, the Fourier transform of their distribution. We call the new representation Characteristic Value Function (CVF). The CVF satisfies a Bellman-like equation, and its corresponding Bellman operator is contraction with respect to certain metrics. The contraction property allows us to devise an iterative procedure to compute the CVF, which we call Characteristic Value Iteration (CVI). We analyze CVI and its approximate variant and show how approximation errors affect the quality of the computed CVF	[Farahmand, Amir-massoud] Vector Inst, Toronto, ON, Canada; [Farahmand, Amir-massoud] Univ Toronto, Toronto, ON, Canada	University of Toronto	Farahmand, AM (corresponding author), Vector Inst, Toronto, ON, Canada.; Farahmand, AM (corresponding author), Univ Toronto, Toronto, ON, Canada.	farahmand@vectorinstitute.ai			Canada CIFAR AI Chairs program	Canada CIFAR AI Chairs program	I would like to thank the anonymous reviewers for their helpful feedback, particularly Reviewer #4. I acknowledge the funding from the Canada CIFAR AI Chairs program.	Arras Benjamin, 2017, ARXIV160506819V2; Barth-Maron G., 2018, PROC INT C LEARN REP, P1; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bertsekas D. P., 2013, ABSTRACT DYNAMIC PRO; Bertsekas DP., 1978, STOCHASTIC OPTIMAL C; Carrillo JA, 2007, RIV MAT UNIV PARMA, V6, P75; Chen JL, 2019, PR MACH LEARN RES, V97; Chow Y, 2018, J MACH LEARN RES, V18; Culotta A., 2010, ADV NEURAL INFORM PR, V23, P568; Dabney Will, 2018, P 35 INT C MACH LEAR, P1; Engel Y., 2005, P 22 INT C MACH LEAR, P201, DOI DOI 10.1145/1102351.1102377; Ernst D, 2005, J MACH LEARN RES, V6, P503; Farahmand AM, 2009, P AMER CONTR CONF, P725, DOI 10.1109/ACC.2009.5160611; FEUERVERGER A, 1990, CAN J STAT, V18, P155, DOI 10.2307/3315564; FEUERVERGER A, 1981, J AM STAT ASSOC, V76, P379, DOI 10.2307/2287839; FEUERVERGER A, 1977, ANN STAT, V5, P88, DOI 10.1214/aos/1176343742; Garcia J, 2015, J MACH LEARN RES, V16, P1437; Gibbs AL, 2002, INT STAT REV, V70, P419, DOI 10.2307/1403865; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hastie T, 2009, ELEMENTS STAT LEARNI; Hunter J.K., 2001, APPL ANAL; Knight JL, 2002, ECONOMET THEOR, V18, P691, DOI 10.1017/S026646660218306X; Kohler Michael, 2002, DISTRIBUTION FREETHE, P7; Lyle Clare, 2019, P 31 AAAI C ART INT, P1; Morimura T., 2010, P 26 C UNC ART INT, P368; Morimura Tetsuro, 2010, P 27 INT C MACH LEAR, P1; Munos R, 2008, J MACH LEARN RES, V9, P815; Prashanth L., 2013, ADV NEURAL INFORM PR; Rowland Mark, 2018, P 21 INT C ART INT S, P1; Rowland Mark, 2019, P 36 INT C MACH LEAR, P1; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R.S, 2019, REINFORCEMENT LEARNI; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Tamar Aviv, 2012, P 29 INT C MACH LEAR, P1; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wasserman L. L., 2007, ALL NONPARAMETRIC ST; Williams D., 1991, PROBABILITY MARTINGA; Yu J., 2004, ECONOMET REV, V23, P93, DOI [DOI 10.1081/ETC-120039605, https://doi.org/10.1081/ETC-120039605]	40	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906046
C	Farina, G; Ling, CK; Fang, F; Sandholm, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Farina, Gabriele; Ling, Chun Kai; Fang, Fei; Sandholm, Tuomas			Correlation in Extensive-Form Games: Saddle-Point Formulation and Benchmarks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					While Nash equilibrium in extensive-form games is well understood, very little is known about the properties of extensive-form correlated equilibrium (EFCE), both from a behavioral and from a computational point of view. In this setting, the strategic behavior of players is complemented by an external device that privately recommends moves to agents as the game progresses; players are free to deviate at any time, but will then not receive future recommendations. Our contributions are threefold. First, we show that an EFCE can be formulated as the solution to a bilinear saddle-point problem. To showcase how this novel formulation can inspire new algorithms to compute EFCEs, we propose a simple subgradient descent method which exploits this formulation and structural properties of EFCEs. Our method has better scalability than the prior approach based on linear programming. Second, we propose two benchmark games, which we hope will serve as the basis for future evaluation of EFCE solvers. These games were chosen so as to cover two natural application domains for EFCE: conflict resolution via a mediator, and bargaining and negotiation. Third, we document the qualitative behavior of EFCE in our proposed games. We show that the social-welfare-maximizing equilibria in these games are highly nontrivial and exhibit surprisingly subtle sequential behavior that so far has not received attention in the literature.	[Farina, Gabriele; Ling, Chun Kai; Sandholm, Tuomas] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Fang, Fei] Carnegie Mellon Univ, Inst Software Res, Pittsburgh, PA 15213 USA; [Sandholm, Tuomas] Strateg Machine Inc, CMU, Dept Comp Sci, Pittsburgh, PA USA; [Sandholm, Tuomas] Strategy Robot Inc, CMU, Dept Comp Sci, Pittsburgh, PA USA; [Sandholm, Tuomas] Optimized Markets Inc, CMU, Dept Comp Sci, Pittsburgh, PA USA	Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University; Carnegie Mellon University	Farina, G (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	gfarina@cs.cmu.edu; chunkail@cs.cmu.edu; feif@cs.cmu.edu; sandholm@cs.cmu.edu			National Science Foundation [IIS-1718457, IIS-1617590, CCF-1733556]; ARO [W911NF-17-1-0082]; Facebook fellowship; Lockheed Martin	National Science Foundation(National Science Foundation (NSF)); ARO; Facebook fellowship(Facebook Inc); Lockheed Martin	This material is based on work supported by the National Science Foundation under grants IIS-1718457, IIS-1617590, and CCF-1733556, and the ARO under award W911NF-17-1-0082. Gabriele Farina is supported by a Facebook fellowship. Co-authors Ling and Fang are supported in part by a research grant from Lockheed Martin.	Ashlagi I, 2008, J ARTIF INTELL RES, V33, P575, DOI 10.1613/jair.2588; Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Bowling M., 2015, SCIENCE; Brown N., 2017, SCIENCE; Brown N, 2019, SCIENCE, V365, P885, DOI 10.1126/science.aay2400; Brown Noam, 2019, AAAI; CRAWFORD VP, 1982, ECONOMETRICA, V50, P1431, DOI 10.2307/1913390; Dodis Y, 2000, LECT NOTES COMPUT SC, V1880, P112; Dudik M., 2009, UAI, P151; Farina Gabriele, 2019, AAAI C ART INT; Gordon G. J., 2008, P 25 INT C MACH LEAR, P360; Hazan E., 2016, INTRO ONLINE CONVEX; Huang W, 2008, LECT NOTES COMPUT SC, V5385, P506, DOI 10.1007/978-3-540-92185-1_56; Jiang AX, 2015, GAME ECON BEHAV, V91, P347, DOI 10.1016/j.geb.2013.02.002; Koutsoupias E., 1999, S THEOR ASP COMP SCI; LLC Gurobi Optimization, 2018, GUROBI OPTIMIZER REF; Moravcik M., 2017, ARXIV170101724V3; NASH JF, 1950, P NATL ACAD SCI USA, V36, P48, DOI 10.1073/pnas.36.1.48; Nemirovski Arkadi, 2004, SIAM J OPTIMIZATION; Nesterov Yurii, 2005, SIAM J OPTIMIZATION; Papadimitriou CH, 2008, J ACM, V55, DOI 10.1145/1379759.1379762; Ray Indrajit, 2009, TECHNICAL REPORT; Roughgarden T, 2002, J ACM, V49, P236, DOI 10.1145/506147.506153; Shoham Y., 2009, MULTIAGENT SYSTEMS A; von Stengel B, 2008, MATH OPER RES, V33, P1002, DOI 10.1287/moor.1080.0340; von Stengel Bernhard, 1996, GAMES EC BEHAV; von Stengel Bernhard, 2002, HDB GAME THEORY, V3; Wan Huang, 2011, THESIS; Wang Y, 2013, J OPTIM, V2013, DOI 10.1155/2013/356420; Zinkevich M., 2007, NIPS	30	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900078
C	Farquhar, G; Whiteson, S; Foerster, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Farquhar, Gregory; Whiteson, Shimon; Foerster, Jakob			Loaded DiCE: Trading off Bias and Variance in Any-Order Score Function Estimators for Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Gradient-based methods for optimisation of objectives in stochastic settings with unknown or intractable dynamics require estimators of derivatives. We derive an objective that, under automatic differentiation, produces low-variance unbiased estimators of derivatives at any order. Our objective is compatible with arbitrary advantage estimators, which allows the control of the bias and variance of any-order derivatives when using function approximation. Furthermore, we propose a method to trade off bias and variance of higher order derivatives by discounting the impact of more distant causal dependencies. We demonstrate the correctness and utility of our objective in analytically tractable MDPs and in meta-reinforcement-learning for continuous control.	[Farquhar, Gregory; Whiteson, Shimon] Univ Oxford, Oxford, England; [Foerster, Jakob] Facebook AI Res, London, England	University of Oxford; Facebook Inc	Foerster, J (corresponding author), Facebook AI Res, London, England.	gregory.farquhar@cs.ox.ac.uk			UK EPSRC CDT in Autonomous Intelligent Machines and Systems; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [637713]	UK EPSRC CDT in Autonomous Intelligent Machines and Systems(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	We thank Maruan Al-Shedivat and Minqi Jiang for valuable discussions. This work was supported by the UK EPSRC CDT in Autonomous Intelligent Machines and Systems. This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713).	Cuenca JA, 2014, ASIA PACIF MICROWAVE, P441; Finn C, 2017, PR MACH LEARN RES, V70; Foerster J., 2018, INT C MACH LEARN, P1524; Foerster J, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P122; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Francois-Lavet V, 2018, FOUND TRENDS MACH LE, V11, P219, DOI 10.1561/2200000071; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; Furmston Thomas, 2016, J MACHINE LEARNING R, V17, P8055; Liu H, 2019, PR MACH LEARN RES, V97; Mao JK, 2019, PR MACH LEARN RES, V97; Mohamed S., 2019, ARXIV190610652; Rothfuss J., 2018, ARXIV181006784; Schulman J., 2015, ARXIV PREPRINT ARXIV; Schulman John, 2015, ARXIV150602438; Stadie Bradly C, 2018, ARXIV180301118; Weaver Lex., 2001, P 17 C UNC ART INT U, P538; Weber Theophane, 2019, ARXIV190101761; Zintgraf Luisa M, 2019, INT C MACH LEARN	18	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308020
C	Fawzi, A; Malinowski, M; Fawzi, H; Fawzi, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fawzi, Alhussein; Malinowski, Mateusz; Fawzi, Hamza; Fawzi, Omar			Learning dynamic polynomial proofs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				RELAXATIONS; HIERARCHY	Polynomial inequalities lie at the heart of many mathematical disciplines. In this paper, we consider the fundamental computational task of automatically searching for proofs of polynomial inequalities. We adopt the framework of semi-algebraic proof systems that manipulate polynomial inequalities via elementary inference rules that infer new inequalities from the premises. These proof systems are known to be very powerful, but searching for proofs remains a major difficulty. In this work, we introduce a machine learning based method to search for a dynamic proof within these proof systems. We propose a deep reinforcement learning framework that learns an embedding of the polynomials and guides the choice of inference rules, taking the inherent symmetries of the problem as an inductive bias. We compare our approach with powerful and widely-studied linear programming hierarchies based on static proof systems, and show that our method reduces the size of the linear program by several orders of magnitude while also improving performance. These results hence pave the way towards augmenting powerful and well-studied semi-algebraic proof systems with machine learning guiding strategies for enhancing the expressivity of such proof systems.	[Fawzi, Alhussein; Malinowski, Mateusz] DeepMind, London, England; [Fawzi, Hamza] Univ Cambridge, Cambridge, England; [Fawzi, Omar] ENS Lyon, Lyon, France	University of Cambridge; Ecole Normale Superieure de Lyon (ENS de LYON)	Fawzi, A (corresponding author), DeepMind, London, England.	afawzi@google.com; mateuszm@google.com; hf323@cam.ac.uk; omar.fawzi@ens-lyon.fr		Malinowski, Mateusz/0000-0003-3909-0149				Bansal K., 2019, ABS190403241 CORR; Barak B, 2014, PROCEEDINGS OF THE INTERNATIONAL CONGRESS OF MATHEMATICIANS (ICM 2014), VOL IV, P509; Bengio Y., 2018, ARXIV181106128; Chlamtac E, 2012, INT SER OPER RES MAN, V166, P139, DOI 10.1007/978-1-4614-0769-0_6; Gauthier Thibault, 2018, ARXIV180400596; Grigoriev D, 2002, MOSC MATH J, V2, P647, DOI 10.17323/1609-4514-2002-2-4-647-679; Huang Daniel, 2018, ARXIV180600608; Kaliszyk C, 2018, ADV NEUR IN, V31, P8836; KRIVINE JL, 1964, CR HEBD ACAD SCI, V258, P3417; Lasserre JB, 2001, SIAM J OPTIMIZ, V11, P796, DOI 10.1137/S1052623400366802; Lasserre JB, 2008, SIAM J CONTROL OPTIM, V47, P1643, DOI 10.1137/070685051; Laurent M, 2003, MATH OPER RES, V28, P470, DOI 10.1287/moor.28.3.470.16391; Laurent M, 2014, J GLOBAL OPTIM, V60, P393, DOI 10.1007/s10898-013-0123-5; LIN LJ, 1992, MACH LEARN, V8, P293, DOI 10.1007/BF00992699; LOVASZ L, 1979, IEEE T INFORM THEORY, V25, P1, DOI 10.1109/TIT.1979.1055985; Lovasz L, 1991, SIAM J OPTIMIZ, V1, P166, DOI 10.1137/0801013; Majumdar A, 2013, IEEE INT CONF ROBOT, P4054, DOI 10.1109/ICRA.2013.6631149; Marechal A, 2016, LECT NOTES COMPUT SC, V9583, P166, DOI 10.1007/978-3-662-49122-5_8; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Papachristodoulou A, 2002, IEEE DECIS CONTR P, P3482, DOI 10.1109/CDC.2002.1184414; Parrilo P.A., 2000, THESIS; Parrilo PA, 2004, DISCRETE COMPUT GEOM, V31, P357, DOI 10.1007/s00454-003-2880-2; SCHRIJVER A., 2003, COMBINATORIAL OPTIMI, V24; Selsam D., 2018, ARXIV180203685; SHERALI HD, 1990, SIAM J DISCRETE MATH, V3, P411, DOI 10.1137/0403036; STENGLE G, 1974, MATH ANN, V207, P87, DOI 10.1007/BF01362149	26	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304021
C	Feng, YH; Li, LH; Liu, Q		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Feng, Yihao; Li, Lihong; Liu, Qiang			A Kernel Loss for Solving the Bellman Equation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				POLICY EVALUATION; ITERATION	Value function learning plays a central role in many state-of-the-art reinforcement-learning algorithms. Many popular algorithms like Q-learning do not optimize any objective function, but are fixed-point iterations of some variants of Bellman operator that are not necessarily a contraction. As a result, they may easily lose convergence guarantees, as can be observed in practice. In this paper, we propose a novel loss function, which can be optimized using standard gradient-based methods with guaranteed convergence. The key advantage is that its gradient can be easily approximated using sampled transitions, avoiding the need for double samples required by prior algorithms like residual gradient. Our approach may be combined with general function classes such as neural networks, using either on- or off-policy data, and is shown to work reliably and effectively in several benchmarks, including classic problems where standard algorithms are known to diverge.	[Feng, Yihao; Liu, Qiang] UT Austin, Austin, TX 78712 USA; [Li, Lihong] Google Res, Mountain View, CA USA	University of Texas System; University of Texas Austin; Google Incorporated	Feng, YH (corresponding author), UT Austin, Austin, TX 78712 USA.	yihao@cs.utexas.edu; lihong@google.com; lqiang@cs.utexas.edu			NSF CRII [1830161]; NSF CAREER [1846421]; Google Cloud; Amazon Web Services (AWS)	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); Google Cloud(Google Incorporated); Amazon Web Services (AWS)	and and Amazon Web Services (AWS) This work is supported in part by NSF CRII 1830161 and NSF CAREER 1846421. We would like to acknowledge Google Cloud and and Amazon Web Services (AWS) for their support. We also thank an anonymous reviewer and Bo Dai for helpful suggestions on related work that improved the paper.	[Anonymous], 2016, P INT C LEARN REPR; [Anonymous], 2018, P INT C LEARN REPR I; Antos A, 2008, MACH LEARN, V71, P89, DOI 10.1007/s10994-007-5038-2; Baird L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P30; Berlinet A., 2011, REPRODUCING KERNEL H; Bertsekas D.P., 2016, NONLINEAR PROGRAMMIN, V3; Beutner E., 2012, DERIVING ASYMPTOTIC, P803; Bhatnagar Shalabh, 2009, ADV NEURAL INFORM PR, V22, P1204; Boyan J. A., 1995, Advances in Neural Information Processing Systems 7, P369; Boyan JA, 1999, MACHINE LEARNING, PROCEEDINGS, P49; Chen Y., 2018, P 35 INT C MACH LEAR, P833; Chwialkowski K., 2016, P 33 INT C MACH LEAR; Dai B, 2017, PR MACH LEARN RES, V54, P1458; DENKER M, 1983, Z WAHRSCHEINLICHKEIT, V64, P505, DOI 10.1007/BF00534953; Farahmand A. M., 2008, ADV NEURAL INFORM PR, P441; Farahmand AM, 2016, J MACH LEARN RES, V17; Fox R., 2016, P 32 C UNC ART INT; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gu SX, 2016, PR MACH LEARN RES, V48; Hoffman M. W., 2011, LECT NOTES COMPUTER, P102; Huang GP, 2019, INT CONF ACOUST SPEE, P426, DOI 10.1109/ICASSP.2019.8683585; Kumar P, 1986, STOCHASTIC SYSTEMS E; Lazaric A, 2012, J MACH LEARN RES, V13, P3041; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Liu Q, 2016, PR MACH LEARN RES, V48; Maei H.R., 2010, 27 ICML, P719; Maei Hamid Reza, 2011, THESIS; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Muandet K, 2017, FOUND TRENDS MACH LE, V10, P1, DOI 10.1561/2200000060; Munos R, 2008, J MACH LEARN RES, V9, P815; Nachum O., 2018, INT C LEARN REPR; Ormoneit D, 2002, MACH LEARN, V49, P161, DOI 10.1023/A:1017928328829; Parr R., 2008, P 25 INT C MACH LEAR, P752; Puterman Martin L., 1994, MARKOV DECISION PROC, V1st, DOI DOI 10.1002/9780470316887; Scholkopf B., 2001, LEARNING KERNELS SUP; Serfling R. J., 2009, APPROXIMATION THEORE, V162; Sriperumbudur BK, 2010, J MACH LEARN RES, V11, P1517; Stewart J., 1976, ROCKY MT J MATH, V6, P409, DOI [10.1216/RMJ-1976-6-3-409, DOI 10.1216/RMJ-1976-6-3-409]; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R. S., 2009, P INT C MACH LEARN I, P125, DOI DOI 10.1145/1553374.1553501; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szepesvari C, 2010, ALGORITHMS REINFORCE, V4; Taylor Gavin, 2009, P 26 ANN INT C MACHI, P1017; Tsitsiklis JN, 1997, IEEE T AUTOMAT CONTR, V42, P674, DOI 10.1109/9.580874; Macua SV, 2015, IEEE T AUTOMAT CONTR, V60, P1260, DOI 10.1109/TAC.2014.2368731; Wang M., 2017, CORR; Wang ZY, 2016, PR MACH LEARN RES, V48; Wu Y., 2017, NIPS 2017, P5279; Xu X., 2005, INT J INFORM TECHNOL, V11, P54; Xu X, 2007, IEEE T NEURAL NETWOR, V18, P973, DOI 10.1109/TNN.2007.899161	56	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907015
C	Finocchiaro, J; Frongillo, R; Waggoner, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Finocchiaro, Jessie; Frongillo, Rafael; Waggoner, Bo			An Embedding Framework for Consistent Polyhedral Surrogates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				MULTICLASS CLASSIFICATION; INFORMATION; ALGORITHMS	We formalize and study the natural approach of designing convex surrogate loss functions via embeddings for problems such as classification or ranking. In this approach, one embeds each of the finitely many predictions (e.g. classes) as a point in R-d, assigns the original loss values to these points, and convexifies the loss in some way to obtain a surrogate. We prove that this approach is equivalent, in a strong sense, to working with polyhedral (piecewise linear convex) losses. More-over, given any polyhedral loss L, we give a construction of a link function through which L is a consistent surrogate for the loss it embeds. We go on to illustrate the power of this embedding framework with succinct proofs of consistency or inconsistency of various polyhedral surrogates in the literature.	[Finocchiaro, Jessie; Frongillo, Rafael; Waggoner, Bo] CU Boulder, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Finocchiaro, J (corresponding author), CU Boulder, Boulder, CO 80309 USA.	jefi8453@colorado.edu; raf@colorado.edu; bwag@colorado.edu		Finocchiaro, Jessica/0000-0002-3222-0089; Waggoner, Bo/0000-0002-1366-1065; Frongillo, Rafael/0000-0002-0170-7572	National Science Foundation [1657598]	National Science Foundation(National Science Foundation (NSF))	We thank Arpit Agarwal and Peter Bartlett for many early discussions, which led to several important insights. We thank Eric Balkanski for help with a lemma about submodular functions. This material is based upon work supported by the National Science Foundation under Grant No. 1657598.	Abernethy J., 2013, ACM T EC COMPUTATION, V1, P12; Agarwal A., 2015, JMLR WORKSHOP C P, V40, P1; Bartlett PL, 2008, J MACH LEARN RES, V9, P1823; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Boyd S, 2004, CONVEX OPTIMIZATION; Crammer K, 2002, J MACH LEARN RES, V2, P265, DOI 10.1162/15324430260185628; Duchi J, 2018, ANN STAT, V46, P3246, DOI 10.1214/17-AOS1657; Finocchiaro Jessica, 2019, ADV NEURAL INFORM PR; Fissler T, 2016, ANN STAT, V44, P1680, DOI 10.1214/16-AOS1439; Frongillo R., 2015, ADV NEURAL INFORM PR, V29; FRONGILLO R., 2015, JMLR WORKSHOP C P, V40, P1; Frongillo R, 2014, LECT NOTES COMPUT SC, V8877, P354, DOI 10.1007/978-3-319-13129-0_29; Gao W, 2011, INT C COMP AID IND D, P344; Gneiting T, 2011, J AM STAT ASSOC, V106, P746, DOI 10.1198/jasa.2011.r10138; Hazan Tamir, 2010, NEURIPS; Lambert N.S., 2018, ELICITATION EVALUATI; Lambert N, 2008, EC'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON ELECTRONIC COMMERCE, P129; LAPIN M, 2015, ADV NEURAL INFORM PR, P325; Lapin M, 2018, IEEE T PATTERN ANAL, V40, P1533, DOI 10.1109/TPAMI.2017.2751607; Lapin M, 2016, PROC CVPR IEEE, P1468, DOI 10.1109/CVPR.2016.163; OSBAND K, 1985, J PUBLIC ECON, V27, P107, DOI 10.1016/0047-2727(85)90031-3; Osokin Anton, 2017, ADV NEURAL INFORM PR, P302; Ramaswamy HG, 2018, ELECTRON J STAT, V12, P530, DOI 10.1214/17-EJS1388; Ramaswamy Harish G, 2016, J MACHINE LEARNING R, V17, P397; Reid MD, 2010, J MACH LEARN RES, V11, P2387; Rockafellar R.T., 1997, PRINCETON MATH SERIE, V28; SAVAGE LJ, 1971, J AM STAT ASSOC, V66, P783, DOI 10.2307/2284229; Steinwart Ingo, 2014, P 27 C LEARN THEOR, P482; Williamson RC, 2016, J MACH LEARN RES, V17, P1; Yang Forest, 2019, ABS190111141 CORR; Yu Jiaqian, 2018, IEEE T PATTERN ANAL; Yuan M, 2010, J MACH LEARN RES, V11, P111; Zhang C, 2018, J AM STAT ASSOC, V113, P730, DOI 10.1080/01621459.2017.1282372	34	0	0	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902041
C	Flokas, L; Vlatakis-Gkaragkounis, EV; Piliouras, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Flokas, Lampros; Vlatakis-Gkaragkounis, Emmanouil V.; Piliouras, Georgios			Efficiently avoiding saddle points with zero order methods: No gradients required	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the case of derivative-free algorithms for non-convex optimization, also known as zero order algorithms, that use only function evaluations rather than gradients. For a wide variety of gradient approximators based on finite differences, we establish asymptotic convergence to second order stationary points using a carefully tailored application of the Stable Manifold Theorem. Regarding efficiency, we introduce a noisy zero-order method that converges to second order stationary points, i.e avoids saddle points. Our algorithm uses only O (1/epsilon(2)) approximate gradient calculations and, thus, it matches the converge rate guarantees of their exact gradient counterparts up to constants. In contrast to previous work, our convergence rate analysis avoids imposing additional dimension dependent slowdowns in the number of iterations required for non-convex zero order optimization.	[Flokas, Lampros; Vlatakis-Gkaragkounis, Emmanouil V.] Columbia Univ, Dept Comp Sci, New York, NY 10025 USA; [Piliouras, Georgios] Singapore Univ Technol & Design, Engn Syst & Design, Singapore, Singapore	Columbia University; Singapore University of Technology & Design	Flokas, L (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10025 USA.	lamflokas@cs.columbia.edu; emvlatakis@cs.columbia.edu; georgios@sutd.edu.sg			MOE AcRF Tier 2 Grant [2016-T2-1-170, PIE-SGP-AI-2018-01]; NRF [NRF-NRFF2018-07]; NSF [CCF-1763970, CCF-1563155, CCF-1814873, CCF-1703925]; Onassis Foundation [F ZN 010-1/2017-2018]	MOE AcRF Tier 2 Grant; NRF; NSF(National Science Foundation (NSF)); Onassis Foundation	Georgios Piliouras acknowledges MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-2018-01 and NRF 2018 Fellowship NRF-NRFF2018-07. Emmanouil-Vasileios Vlatakis-Gkaragkounis was supported by NSF CCF-1563155, NSF CCF-1814873, NSF CCF-1703925, NSF CCF-1763970. We are grateful to Alexandros Potamianos for bringing this problem to our attention, and for helpful discussions at an early stage of this project for its connection to Natural Language Processing tasks. Finally, this work was supported by the Onassis Foundation - Scholarship ID: F ZN 010-1/2017-2018.	Agarwal A., 2010, P COLT, P28; Agarwal N, 2017, ACM S THEORY COMPUT, P1195, DOI 10.1145/3055399.3055464; [Anonymous], 2017, CORR; Balasubramanian K., 2018, ADV NEURAL INFORM PR, P3459; Carmon Yair, 2016, CORR; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen TY, 2019, IEEE INTERNET THINGS, V6, P1276, DOI 10.1109/JIOT.2018.2839563; Choromanska A., 2015, P 18 INT C ART INT S; Choromanski K, 2018, PR MACH LEARN RES, V80; Conn AR, 2009, SIAM J OPTIMIZ, V20, P387, DOI 10.1137/060673424; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Du S.S., 2017, ADV NEURAL INFORM PR, P1067; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gu B., 2016, ARXIV161201425; Hajinezhad D, 2018, IEEE DECIS CONTR P, P4939, DOI 10.1109/CDC.2018.8619333; JIN C., 2018, ADV NEURAL INFORM PR, V31, P4901; Jin C., 2018, P 31 C LEARNING THEO, P1042; Jin C, 2017, PR MACH LEARN RES, V70; Lee JD, 2019, MATH PROGRAM, V176, P311, DOI 10.1007/s10107-019-01374-3; Levy Kfir Y., 2016, CORR; Lian Xiangru, 2016, P C NEUR INF PROC SY, P3054; Liu S., 2018, INT C ART INT STAT, P288; Madry A., 2018, INT C LEARN REPR ICL; MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Rubinstein Reuven Y., 2016, SIMULATION MONTE CAR, V10; Shub M., 1987, ASTE RISQUE; Snoek J., 2012, P NIPS, V12, P2960; Spall J.C., 2003, INTRO STOCHASTIC SEA; Ting P., 2018, ADV NEURAL INFORM PR, P3731; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang YF, 2018, IEEE ANN INT CONF CY, P1356, DOI 10.1109/CYBER.2018.8688170; Zeyuan Allen-Zhu, 2018, ADV NEURAL INFORM PR, P3720	38	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901067
C	Flokas, L; Vlatakis-Gkaragkounis, EV; Piliouras, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Flokas, Lampros; Vlatakis-Gkaragkounis, Emmanouil V.; Piliouras, Georgios			Poincare Recurrence, Cycles and Spurious Equilibria in Gradient-Descent-Ascent for Non-Convex Non-Concave Zero-Sum Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study a wide class of non-convex non-concave min-max games that generalizes over standard bilinear zero-sum games. In this class, players control the inputs of a smooth function whose output is being applied to a bilinear zero-sum game. This class of games is motivated by the indirect nature of the competition in Generative Adversarial Networks, where players control the parameters of a neural network while the actual competition happens between the distributions that the generator and discriminator capture. We establish theoretically, that depending on the specific instance of the problem gradient-descent-ascent dynamics can exhibit a variety of behaviors antithetical to convergence to the game theoretically meaningful min-max solution. Specifically, different forms of recurrent behavior (including periodicity and Poincare recurrence) are possible as well as convergence to spurious (non-min-max) equilibria for a positive measure of initial conditions. At the technical level, our analysis combines tools from optimization theory, game theory and dynamical systems.	[Flokas, Lampros; Vlatakis-Gkaragkounis, Emmanouil V.] Columbia Univ, Dept Comp Sci, New York, NY 10025 USA; [Piliouras, Georgios] Singapore Univ Technol & Design, Engn Syst & Design, Singapore, Singapore	Columbia University; Singapore University of Technology & Design	Flokas, L (corresponding author), Columbia Univ, Dept Comp Sci, New York, NY 10025 USA.	lamflokas@cs.columbia.edu; emvlatakis@cs.columbia.edu; georgios@sutd.edu.sg			MOE AcRF Tier 2 Grant [2016-T2-1-170, PIE-SGP-AI-201801]; NRF 2018 Fellowship [NRF-NRFF2018-07]; NSF [CCF-1563155, CCF-1814873, CCF-1703925, CCF-1763970]; Onassis Foundation [F ZN 010-1/2017-2018]	MOE AcRF Tier 2 Grant; NRF 2018 Fellowship; NSF(National Science Foundation (NSF)); Onassis Foundation	Georgios Piliouras acknowledges MOE AcRF Tier 2 Grant 2016-T2-1-170, grant PIE-SGP-AI-201801 and NRF 2018 Fellowship NRF-NRFF2018-07. Emmanouil-Vasileios Vlatakis-Gkaragkounis was supported by NSF CCF-1563155, NSF CCF-1814873, NSF CCF-1703925, NSF CCF-1763970. Finally this work was supported by the Onassis Foundation -Scholarship ID: F ZN 010-1/2017-2018.	Abernethy Jacob, 2019, ABS190602027 CORR; Adolphs L, 2019, PR MACH LEARN RES, V89, P486; [Anonymous], 2018, ICML; [Anonymous], 2017, ARXIV PREPRINT ARXIV; Bailey J.P, 2019, NEURIPS; Bailey J. P., 2019, ABS190704392 CORR; Bailey JP, 2018, ACM EC'18: PROCEEDINGS OF THE 2018 ACM CONFERENCE ON ECONOMICS AND COMPUTATION, P321, DOI 10.1145/3219166.3219235; Bailey James P., 2019, 18 INT C AUT AG MULT; Balduzzi D, 2018, PR MACH LEARN RES, V80; Berthelot D., 2017, ABS170310717 CORR; Cheung Yun Kuen, 2019, COLT; Chhabra A, 2017, 2017 IEEE 3RD INTERNATIONAL CONFERENCE ON COLLABORATION AND INTERNET COMPUTING (CIC), P243, DOI 10.1109/CIC.2017.00040; Chintala S., 2017, ABS170107875 CORR; Daskalakis C., 2018, P 32 INT C NEURAL IN, V31, P9256; Daskalakis Constantinos, 2019, LIPICS, V124, DOI [10.4230/LIPIcs.ITCS.2019.27, DOI 10.4230/LIPICS.ITCS.2019.27]; DOBZINSKI S, 2011, INNOVATIONS COMPUTER, P129; Du Simon S., 2018, ABS181103804 CORR; Ge H, 2018, LECT NOTES COMPUT SC, V11205, P122, DOI 10.1007/978-3-030-01246-5_8; Gidel G., 2019, INT C LEARNING REPRE; Gidel G., 2018, ABS180210551 CORR, Vabs/1802.10551; Gidel G., 2019, AISTATS; Gidel Gauthier, 2019, P MACHINE LEARNING R, P1802; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jin Chi, 2019, ABS190200618 CORR; Kodali Naveen, 2017, ABS170507215 CORR; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; LI TY, 1975, AM MATH MON, V82, P985, DOI 10.2307/2318254; Lin Qihang, 2018, ABS181010207 CORR; Ma TY, 2018, ACM S THEORY COMPUT, P2, DOI 10.1145/3188745.3232194; Mai Tung, 2017, ABS171011249 CORR; Mazumdar Eric, 2018, ABS180405464 CORR; Mertikopoulos P, 2019, P 2019 INT C LEARN R; Mertikopoulos P, 2018, SODA'18: PROCEEDINGS OF THE TWENTY-NINTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2703; Mertikopoulos Panayotis, 2019, 7 INT C LEARN REPR I; Metz L., 2017, 5 INT C LEARN REPR I; Nagarajan SG, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P685; Oliehoek Frans A., 2018, ABS180607268 CORR; PALAIOPANOS  G., 2017, ADV NEURAL INFORM PR, V30, P5872, DOI DOI 10.5555/3295222.3295337; Pfau David, 2016, ABS161001945 CORR; Piliouras G., 2014, P 25 ANN ACM SIAM S, P861; Piliouras Georgios, 2018, 9 INN THEOR COMP SCI, P59; Radford A., 2016, 4 INT C LEARN REPR I; Salimans T, 2016, ADV NEUR IN, V29; Sandholm WilliamH., 2010, POPULATION GAMES EVO; Sanjabi Maziar, 2018, ABS181202878 CORR; Yazici Yasin, 2019, ICLR; Zeng H., 2018, 6 INT C LEARN REPR I; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629	50	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902012
C	Foster, DJ; Greenberg, S; Kale, S; Luo, HP; Mohri, M; Sridharan, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Foster, Dylan J.; Greenberg, Spencer; Kale, Satyen; Luo, Haipeng; Mohri, Mehryar; Sridharan, Karthik			Hypothesis Set Stability and Generalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BOUNDS	We present a study of generalization for data-dependent hypothesis sets. We give a general learning guarantee for data-dependent hypothesis sets based on a notion of transductive Rademacher complexity. Our main result is a generalization bound for data-dependent hypothesis sets expressed in terms of a notion of hypothesis set stability and a notion of Rademacher complexity for data-dependent hypothesis sets that we introduce. This bound admits as special cases both standard Rademacher complexity bounds and algorithm-dependent uniform stability bounds. We also illustrate the use of these learning bounds in the analysis of several scenarios.	[Foster, Dylan J.] MIT, Cambridge, MA 02139 USA; [Greenberg, Spencer] Spark Wave, Hyderabad, Telangana, India; [Kale, Satyen] Google Res, Mountain View, CA USA; [Luo, Haipeng] Univ Southern Calif, Los Angeles, CA USA; [Mohri, Mehryar] Courant Inst, New York, NY USA; [Sridharan, Karthik] Cornell Univ, Ithaca, NY 14853 USA	Massachusetts Institute of Technology (MIT); Google Incorporated; University of Southern California; Cornell University	Foster, DJ (corresponding author), MIT, Cambridge, MA 02139 USA.	dylanf@mit.edu; admin@sparkwave.tech; satyen@satyenkale.com; haipengl@usc.edu; mohri@google.com; sridharan@cs.cornell.edu			NSF [IIS-1618662, IIS-1755781, CCF-1535987]; Google Research Award; NSF CAREER Award [1750575]; Sloan Research Fellowship	NSF(National Science Foundation (NSF)); Google Research Award(Google Incorporated); NSF CAREER Award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Research Fellowship(Alfred P. Sloan Foundation)	HL is supported by NSF IIS-1755781. The work of SG and MM was partly supported by NSF CCF-1535987, NSF IIS-1618662, and a Google Research Award. KS would like to acknowledge NSF CAREER Award 1750575 and Sloan Research Fellowship.	Bartlett P. L., 2002, COLT, V2375, P79; Bartlett Peter L., 2002, J MACHINE LEARNING, V3; Bassily R, 2016, ACM S THEORY COMPUT, P1046, DOI 10.1145/2897518.2897566; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bousquet O., 2019, CORR; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Cannon A, 2002, J MACH LEARN RES, V2, P335, DOI 10.1162/153244302760200650; Catoni O., 2007, PAC BAYESIAN SUPERVI; Cesa-Bianchi N., 2001, P 15 ANN C NEUR INF, P359; Cortes C., 2008, P 25 INT C MACH LEAR, P176; DEVROYE LP, 1979, IEEE T INFORM THEORY, V25, P202, DOI 10.1109/TIT.1979.1056032; Dziugaite GK, 2018, PR MACH LEARN RES, V80; Dziugaite Gintare Karolina, 2018, ADV NEURAL INFORM PR, P8440; El-Yaniv R, 2007, LECT NOTES COMPUT SC, V4539, P157, DOI 10.1007/978-3-540-72927-3_13; Elisseeff A, 2005, J MACH LEARN RES, V6, P55; Feldman Vitaly, 2019, P COLT; Gat Y, 2001, MACH LEARN, V42, P233, DOI 10.1023/A:1007605716762; Herbrich R, 2003, J MACH LEARN RES, V3, P175, DOI 10.1162/153244303765208368; Kakade Sham M., 2008, NIPS; Kale S., 2011, P 2 S INN COMP SCI I, P487; Kearns M., 1997, Proceedings of the Tenth Annual Conference on Computational Learning Theory, P152, DOI 10.1145/267460.267491; Koltchinskii V, 2002, ANN STAT, V30, P1; Kutin S, 2002, P 18 C UNC ART INT, P275; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; Lever G, 2013, THEOR COMPUT SCI, V473, P4, DOI 10.1016/j.tcs.2012.10.013; Maurer A., 2017, C LEARN THEOR, P1461; McAllester D, 2003, LECT NOTES ARTIF INT, V2777, P203, DOI 10.1007/978-3-540-45167-9_16; McSherry F, 2007, ANN IEEE SYMP FOUND, P94, DOI 10.1109/FOCS.2007.66; Miyaguchi Kohei, 2019, CORR; Mohri M., 2018, FDN MACHINE LEARNING; Mohri M, 2010, J MACH LEARN RES, V11, P789; Neyshabur Behnam, 2018, P ICLR; Parrado-Hernandez Emilio, 2015, J MACHINE LEARNING R, V13, P3507; Philips Petra, 2005, THESIS; Pollard David, 1984, CONVERGENCE STOCHAST; ROGERS WH, 1978, ANN STAT, V6, P506, DOI 10.1214/aos/1176344196; Rudelson M, 2007, J ACM, V54, DOI 10.1145/1255443.1255449; Shalev-Shwartz S, 2010, J MACH LEARN RES, V11, P2635; Srebro Nathan, 2010, NIPS; Steinke Thomas, 2017, CORR; Stewart G., 1998, MATRIX ALGORITHMS, V1; Vapnik V.N, 1998, STAT LEARNING THEORY; Vondrak J., 2018, ADV NEURAL INFORM PR, P9770; Zhang T, 2002, J MACH LEARN RES, V2, P527, DOI 10.1162/153244302760200713	46	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306070
C	Fostert, A; Jankowiak, M; Bingham, E; Horsfall, P; Teh, YW; Rainforth, T; Goodman, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fostert, Adam; Jankowiak, Martin; Bingham, Eli; Horsfall, Paul; Teh, Yee Whye; Rainforth, Tom; Goodman, Noah			Variational Bayesian Optimal Experimental Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bayesian optimal experimental design (BOED) is a principled framework for making efficient use of limited experimental resources. Unfortunately, its applicability is hampered by the difficulty of obtaining accurate estimates of the expected information gain (EIG) of an experiment. To address this, we introduce several classes of fast EIG estimators by building on ideas from amortized variational inference. We show theoretically and empirically that these estimators can provide significant gains in speed and accuracy over previous approaches. We further demonstrate the practicality of our approach on a number of end-to-end experiments.	[Fostert, Adam; Teh, Yee Whye; Rainforth, Tom] Univ Oxford, Dept Stat, Oxford, England; [Jankowiak, Martin; Bingham, Eli; Horsfall, Paul; Goodman, Noah] Uber Technol Inc, Uber AI Labs, San Francisco, CA USA; [Goodman, Noah] Stanford Univ, Stanford, CA 94305 USA	University of Oxford; Uber Technologies, Inc.; Stanford University	Fostert, A (corresponding author), Univ Oxford, Dept Stat, Oxford, England.	adam.foster@stats.ox.ac.uk			Uber Al Labs; EPSRC [EP/N509711/1]; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC [617071]	Uber Al Labs; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC(European Research Council (ERC))	We gratefully acknowledge research funding from Uber Al Labs. MJ would like to thank Paul Szerlip for help generating the sprites used in the Mechanical Turk experiment. AF would like to thank Patrick Rebeschini, Dominic Richards and Emile Mathieu for their help and support. AF gratefully acknowledges funding from EPSRC grant no. EP/N509711/1. YWT's and TR's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	Amzal B, 2006, J AM STAT ASSOC, V101, P773, DOI 10.1198/016214505000001159; [Anonymous], 2018, ICML P MACH LEARN RE; [Anonymous], 2005, STAT EXPT; [Anonymous], 2014, ICLR; ARROW KJ, 1961, REV ECON STAT, V43, P225, DOI 10.2307/1927286; BACH F., 2011, ADV NEURAL INFORM PR, P451; Bingham E, 2019, J MACH LEARN RES, V20; Burda Yuri, 2016, 4 INT C LEARN REPR I; Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939; Cook AR, 2008, BIOMETRICS, V64, P860, DOI 10.1111/j.1541-0420.2007.00931.x; DAYAN P, 1995, NEURAL COMPUT, V7, P889, DOI 10.1162/neco.1995.7.5.889; DONSKER MD, 1975, COMMUN PUR APPL MATH, V28, P1, DOI 10.1002/cpa.3160280102; EHRENFELD S, 1962, J AM STAT ASSOC, V57, P668, DOI 10.2307/2282404; Embretson S. E., 2013, ITEM RESPONSE THEORY; Gelman A., 2013, TEXTS STAT SCI SERIE, Vthird, DOI 10.1201/b16018; Golovin D, 2010, ARON CULOTTA ADV NEU, P766; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Kleinegesse S, 2019, PR MACH LEARN RES, V89, P476; Kohavi R, 2009, DATA MIN KNOWL DISC, V18, P140, DOI 10.1007/s10618-008-0114-1; Kruschke JK, 2014, JAGS STAN, V2nd, DOI DOI 10.1016/B978-0-12-405888-0.09996-7; Le T. A., 2018, INT C LEARN REPR ICL; Lewi J, 2009, NEURAL COMPUT, V21, P619, DOI 10.1162/neco.2008.08-07-594; Lindley D.V., 1972, BAYESIAN STAT REV, V2; LINDLEY DV, 1956, ANN MATH STAT, V27, P986, DOI 10.1214/aoms/1177728069; Long Q, 2013, COMPUT METHOD APPL M, V259, P24, DOI 10.1016/j.cma.2013.02.017; Ma C., 2018, ARXIV180911142; MACKAY DJC, 1992, NEURAL COMPUT, V4, P590, DOI 10.1162/neco.1992.4.4.590; Muller P, 2005, HANDB STAT, V25, P509, DOI 10.1016/S0169-7161(05)25017-4; Myung JI, 2013, J MATH PSYCHOL, V57, P53, DOI 10.1016/j.jmp.2013.05.005; Poole B., 2018, NEURIPS WORKSH BAYES; Rainforth Tom, 2017, THESIS; Rainforth Tom, 2018, INT C MACH LEARN, P4264; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Samuelson PA, 1948, ECONOMICA-NEW SER, V15, P243, DOI 10.2307/2549561; Sebastiani Paola, 2000, J ROYAL STAT SOC B, V62; Shababo B., 2013, ADV NEURAL INFORM PR, V26, P1304; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Stuhlmuller Andreas, 2013, ADV NEURAL INFORM PR, P3048; Thomas Owen, 2016, ARXIV161110242; Vanlier J, 2012, BIOINFORMATICS, V28, P1136, DOI 10.1093/bioinformatics/bts092; Vincent B. T., 2017, DARC TOOLBOX AUTOMAT	43	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905068
C	Gala, R; Gouwens, N; Yao, ZZ; Budzillo, A; Penn, O; Tasic, B; Murphy, G; Zeng, HK; Sumbul, U		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gala, Rohan; Gouwens, Nathan; Yao, Zizhen; Budzillo, Agata; Penn, Osnat; Tasic, Bosiljka; Murphy, Gabe; Zeng, Hongkui; Sumbul, Uygar			A coupled autoencoder approach for multi-modal analysis of cell types	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent developments in high throughput profiling of individual neurons have spurred data driven exploration of the idea that there exist natural groupings of neurons referred to as cell types. The promise of this idea is that the immense complexity of brain circuits can be reduced, and effectively studied by means of interactions between cell types. While clustering of neuron populations based on a particular data modality can be used to define cell types, such definitions are often inconsistent across different characterization modalities. We pose this issue of cross-modal alignment as an optimization problem and develop an approach based on coupled training of autoencoders as a framework for such analyses. We apply this framework to a Patch-seq dataset consisting of transcriptomic and electrophysiological profiles for the same set of neurons to study consistency of representations across modalities, and evaluate cross-modal data prediction ability. We explore the problem where only a subset of neurons is characterized with more than one modality, and demonstrate that representations learned by coupled autoencoders can be used to identify types sampled only by a single modality.	[Gala, Rohan; Gouwens, Nathan; Yao, Zizhen; Budzillo, Agata; Penn, Osnat; Tasic, Bosiljka; Murphy, Gabe; Zeng, Hongkui; Sumbul, Uygar] Allen Inst, Seattle, WA 98109 USA	Allen Institute for Brain Science	Gala, R (corresponding author), Allen Inst, Seattle, WA 98109 USA.	rohang@alleninstitute.org; uygars@alleninstitute.org		Sumbul, Uygar/0000-0001-7134-8897				Azizi E., 2017, GENOM COMPUT BIOL, V3, P46, DOI [10.18547/gcb.2017.vol3.iss1.e46, DOI 10.18547/GCB.2017.VOL3.ISS1.E46]; Baglama J, 2003, SIAM J SCI COMPUT, V24, P1650, DOI 10.1137/S1064827501397949; Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607; Cadwell CR, 2016, NAT BIOTECHNOL, V34, P199, DOI 10.1038/nbt.3445; Chen KH, 2015, SCIENCE, V348, DOI 10.1126/science.aaa6090; Dilokthanakul Nat, 2016, ARXIV161102648; Feng FX, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P7, DOI 10.1145/2647868.2654902; Gronbech CH, 2018, BIORXIV; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lopez Romain, 2018, BAYESIAN INFERENCE G, DOI [10.1101/292037, DOI 10.1101/292037]; Markram H, 2015, CELL, V163, P456, DOI 10.1016/j.cell.2015.09.029; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pierson E, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0805-z; Prabhakaran S, 2016, PR MACH LEARN RES, V48; Risso D, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02554-5; Risso Davide, 2017, BIORXIV; Seung HS, 2014, NEURON, V83, P1262, DOI 10.1016/j.neuron.2014.08.054; Smith Stephen J, 2019, BIORXIV; Sorensen Staci A, 2018, CLASSIFICATION ELECT; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tasic B, 2018, NATURE, V563, P72, DOI 10.1038/s41586-018-0654-5; Wang S, 2015, INT CONF MACH LEARN, P883, DOI 10.1109/ICMLC.2015.7340670; Zeisel A, 2018, CELL, V174, P999, DOI 10.1016/j.cell.2018.06.021; Zeng HK, 2017, NAT REV NEUROSCI, V18, P530, DOI 10.1038/nrn.2017.85; Zhao Dazhi, 2019, NEURAL NETWORKS; Zurauskiene J, 2016, BMC BIOINFORMATICS, V17, DOI 10.1186/s12859-016-0984-y	30	0	0	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900081
C	Gamarnik, D; Gaudio, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gamarnik, David; Gaudio, Julia			Sparse High-Dimensional Isotonic Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION; ALGORITHM; FIT	We consider the problem of estimating an unknown coordinate-wise monotone function given noisy measurements, known as the isotonic regression problem. Often, only a small subset of the features affects the output. This motivates the sparse isotonic regression setting, which we consider here. We provide an upper bound on the expected VC entropy of the space of sparse coordinate-wise monotone functions, and identify the regime of statistical consistency of our estimator. We also propose a linear program to recover the active coordinates, and provide theoretical recovery guarantees. We close with experiments on cancer classification, and show that our method significantly outperforms several standard methods.	[Gamarnik, David] MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Gaudio, Julia] MIT, Operat Res Ctr, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Gamarnik, D (corresponding author), MIT, Sloan Sch Management, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	gamarnik@mit.edu; jgaudio@mit.edu	Gaudio, Julia/AAA-6833-2020					Barlow R.E., 1973, STAT INFERENCE ORDER; Bertsimas D, 1999, MACH LEARN, V35, P225, DOI 10.1023/A:1007586831473; BEST MJ, 1990, MATH PROGRAM, V47, P425, DOI 10.1007/BF01580873; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Cohen JD, 2018, SCIENCE, V359, P926, DOI 10.1126/science.aar3247; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; de Leeuw J, 2009, J STAT SOFTW, V32, P1; DYKSTRA RL, 1982, ANN STAT, V10, P708, DOI 10.1214/aos/1176345866; Fix E., 1951, 2149004 USAF SCH AV; Forbes SA, 2011, NUCLEIC ACIDS RES, V39, pD945, DOI 10.1093/nar/gkq929; Foucart S., 2013, MATH INTRO COMPRESSI, P1, DOI DOI 10.1007/978-0-8176-4948-7; KRUSKAL JB, 1964, PSYCHOMETRIKA, V29, P1, DOI 10.1007/BF02289565; Luss R, 2012, ANN APPL STAT, V6, P253, DOI 10.1214/11-AOAS504; Moshkovitz G, 2014, ADV MATH, V262, P1107, DOI 10.1016/j.aim.2014.06.008; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Robertson T, 1988, ORDER RESTRICTED STA; SASABUCHI S, 1983, BIOMETRIKA, V70, P465; SASABUCHI S, 1992, HIROSHIMA MATH J, V22, P551; Schell MJ, 1997, J AM STAT ASSOC, V92, P128, DOI 10.2307/2291456; [No title captured]; [No title captured]; [No title captured]; [No title captured]	24	0	0	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904050
C	Gangrade, A; Venkatesh, P; Nazer, B; Saligrama, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gangrade, Aditya; Venkatesh, Praveen; Nazer, Bobak; Saligrama, Venkatesh			Efficient Near-Optimal Testing of Community Changes in Balanced Stochastic Block Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				REGULARIZATION	We propose and analyze the problems of community goodness-of-fit and two-sample testing for stochastic block models (SBM), where changes arise due to modification in community memberships of nodes. Motivated by practical applications, we consider the challenging sparse regime, where expected node degrees are constant, and the inter-community mean degree (b) scales proportionally to intracommunity mean degree (alpha). Prior work has sharply characterized partial or full community recovery in terms of a "signal-to-noise ratio" (SNR) based on a and b. For both problems, we propose computationally-efficient tests that can succeed far beyond the regime where recovery of community membership is even possible. Overall, for large changes, s >> root n wewe need only SNR = O(1) whereas a naive test based on community recovery with O(s) errors requires SNR = Theta(log n). Conversely, in the small change regime, s << root n, via an information theoretic lower bound, we show that, surprisingly, no algorithm can do better than the na ve algorithm that first estimates the community up to O(s) errors and then detects changes. We validate these phenomena numerically on SBMS and on real -world datasets as well as Markov Random Fields where we only observe node data rather than the existence of links.	[Gangrade, Aditya; Nazer, Bobak; Saligrama, Venkatesh] Boston Univ, Boston, MA 02215 USA; [Venkatesh, Praveen] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Boston University; Carnegie Mellon University	Gangrade, A (corresponding author), Boston Univ, Boston, MA 02215 USA.	gangrade@bu.edu; vpraveen@cmu.edu; bobak@bu.edu; srv@bu.edu	Venkatesh, Praveen/AAZ-4522-2021	Venkatesh, Praveen/0000-0003-0752-1506				Abbe E, 2018, FOUND TRENDS COMMUN, V14, P1, DOI 10.1561/0100000067; Adamic LA, 2005, INT WORKSHOP LINK DI, P36, DOI DOI 10.1145/1134271.1134277; Banks J., 2016, P C LEARN THEOR, P383; Bassett DS, 2008, J NEUROSCI, V28, P9239, DOI 10.1523/JNEUROSCI.1929-08.2008; Chen JC, 2006, BIOINFORMATICS, V22, P2283, DOI 10.1093/bioinformatics/btl370; Chin P., 2015, C LEARN THEOR, P391; Chung F., 2010, NOTICES AMS, V57, P726; Chung Fan, CBMS REGIONAL C SERI; Dembo A, 2010, BRAZ J PROBAB STAT, V24, P137, DOI 10.1214/09-BJPS027; Fei YJ, 2019, IEEE T INFORM THEORY, V65, P551, DOI 10.1109/TIT.2018.2839677; Fortunato S, 2010, PHYS REP, V486, P75, DOI 10.1016/j.physrep.2009.11.002; Gallager R. G, 1968, INFORM THEORY RELIAB, V588; Gao C., 2017, ARXIV170406742; Ghoshdastidar D, 2018, ADV NEUR IN, V31; Ghoshdastidar Debarghya, 2017, P MACHINE LEARNING R, P954; Ghoshdastidar Debarghya, 2017, ARXIV170700833; Girvan M, 2002, P NATL ACAD SCI USA, V99, P7821, DOI 10.1073/pnas.122653799; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Hunter JD, 2007, COMPUT SCI ENG, V9, P90, DOI 10.1109/MCSE.2007.55; Jiang DX, 2004, IEEE T KNOWL DATA EN, V16, P1370, DOI 10.1109/TKDE.2004.68; Jones E., 2001, SCIPY OPEN SOURCE SC; Joseph A, 2016, ANN STAT, V44, P1765, DOI 10.1214/16-AOS1447; Le CM, 2017, RANDOM STRUCT ALGOR, V51, P538, DOI 10.1002/rsa.20713; Lehmann Erich L, 2006, TESTING STATISTICALH; Li Yezheng, 2018, ARXIV181112593; Newman MEJ, 2006, P NATL ACAD SCI USA, V103, P8577, DOI 10.1073/pnas.0601602103; Oliphant TE., 2006, A GUIDE TO NUMPY; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pnevmatikakis EA, 2016, NEURON, V89, P285, DOI 10.1016/j.neuron.2015.11.037; Tang M, 2017, J COMPUT GRAPH STAT, V26, P344, DOI 10.1080/10618600.2016.1193505; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Wang W, 2010, IEEE INT SYMP INFO, P1373, DOI 10.1109/ISIT.2010.5513573; Wu Yihong, 2018, ARXIV180600118; Zhang AY, 2016, ANN STAT, V44, P2252, DOI 10.1214/15-AOS1428; Zhang B, 2009, BIOINFORMATICS, V25, P526, DOI 10.1093/bioinformatics/btn660	42	0	0	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902004
C	Garcia, FM; Thomas, PS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Garcia, Francisco M.; Thomas, Philip S.			A Meta-MDP Approach to Exploration for Lifelong Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHMS	In this paper we consider the problem of how a reinforcement learning agent that is tasked with solving a sequence of reinforcement learning problems (a sequence of Markov decision processes) can use knowledge acquired early in its lifetime to improve its ability to solve new problems. We argue that previous experience with similar problems can provide an agent with information about how it should explore when facing a new but related problem. We show that the search for an optimal exploration strategy can be formulated as a reinforcement learning problem itself and demonstrate that such strategy can leverage patterns found in the structure of related problems. We conclude with experiments that show the benefits of optimizing an exploration strategy using our proposed framework.	[Garcia, Francisco M.; Thomas, Philip S.] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Garcia, FM (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	fmgarcia@cs.umass.edu; pthomas@cs.umass.edu						[Anonymous], 2020, REINFORCEMENT LEARNI; Azar MG, 2017, PR MACH LEARN RES, V70; Brockman G., 2016, OPENAI GYM; Fernandez F., 2006, P 5 INT JOINT C AUT, P720, DOI DOI 10.1145/1160633.1160762; Finn C, 2017, PR MACH LEARN RES, V70; Garivier A, 2011, LECT NOTES ARTIF INT, V6925, P174, DOI 10.1007/978-3-642-24412-4_16; Houthooft Rein, 2016, CORR; Koch William, 2018, CORR; Laroche Romain, 2017, MULTIADVISOR REINFOR; Lemtiri-Chlieh F, 2011, ABIOTIC STRESS RESPONSE IN PLANTS - PHYSIOLOGICAL, BIOCHEMICAL AND GENETIC PERSPECTIVES, P137; Li Q, 2012, 2012 5TH INTERNATIONAL CONGRESS ON IMAGE AND SIGNAL PROCESSING (CISP), P1; Martin Jarryd, 2017, CORR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pathak D., 2017, CORR; PHANSALKAR VV, 1995, NEURAL COMPUT, V7, P950, DOI 10.1162/neco.1995.7.5.950; Schmidhuber J, 1998, LEARNING TO LEARN, P293; Schmidhuber Jurgen, 1995, TECHNICAL REPORT; Schulman J, 2016, INT C LEARNING REPRE; Schulman J., 2017, CORR; Strehl Alexander L., 2008, ISAIM; Tang H., 2016, CORR; Tianhao Zhang, 2015, CORR; Van Seijen H., 2017, HYBRID REWARD ARCHIT; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; WHITEHEAD SD, 1991, MACHINE LEARNING, P363; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yang QM, 2012, IEEE T SYST MAN CY B, V42, P377, DOI 10.1109/TSMCB.2011.2166384	27	0	0	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305066
