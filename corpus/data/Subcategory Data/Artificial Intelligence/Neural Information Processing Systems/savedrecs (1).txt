PT	AU	BA	BE	GP	AF	BF	CA	TI	SO	SE	BS	LA	DT	CT	CY	CL	SP	HO	DE	ID	AB	C1	C3	RP	EM	RI	OI	FU	FP	FX	CR	NR	TC	Z9	U1	U2	PU	PI	PA	SN	EI	BN	J9	JI	PD	PY	VL	IS	PN	SU	SI	MA	BP	EP	AR	DI	DL	D2	EA	PG	WC	WE	SC	GA	PM	OA	HC	HP	DA	UT
C	Achlioptas, D; McSherry, F; Scholkopf, B		Dietterich, TG; Becker, S; Ghahramani, Z		Achlioptas, D; McSherry, F; Scholkopf, B			Sampling techniques for kernel methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				ALGORITHM	We propose randomized techniques for speeding up Kernel Principal Component Analysis on three levels: sampling and quantization of the Gram matrix in training, randomized rounding in evaluating the kernel expansions, and random projections in evaluating the kernel itself. In all three cases, we give sharp bounds on the accuracy of the obtained approximations. Rather intriguingly, all three techniques can be viewed as instantiations of the following idea: replace the kernel function k by a "randomized kernel" which behaves like k in expectation.					Schölkopf, Bernhard/A-7570-2013; Achlioptas, Dimitris/AAJ-6358-2020	Schölkopf, Bernhard/0000-0002-8177-0925; 				Achlioptas D., 2001, P TWENTIETHACMSIGMOD, P274, DOI DOI 10.1145/375551.375608; BURGES CJC, 1996, P 13 INT C MACH LEAR, P71; CRISTIANINI N, 2001, P 18 INT C MACH LEAR; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; FUREDI Z, 1981, COMBINATORICA, V1, P233, DOI 10.1007/BF02579329; GOULD NIM, 1991, IMA J NUMER ANAL, V11, P299, DOI 10.1093/imanum/11.3.299; HOEFFDING W, 1963, J AM STAT ASSOC, V58, P13, DOI 10.2307/2282952; Johnson W.B., 1984, CONTEMP MATH, V26, P1, DOI [10.1090/conm/026/737400, DOI 10.1090/CONM/026/737400]; NICKEL RH, 1989, J OPTIMIZ THEORY APP, V60, P453, DOI 10.1007/BF00940348; OSUNA E, 1997, P 1997 IEEE WORKSH N, V7, P276; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; WILLIAMS CK, 2001, ADV NEURAL INFORMATI	14	66	66	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						335	342						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100042
C	Tang, YC; Salakhutdinov, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tang, Yichuan Charlie; Salakhutdinov, Ruslan			Multiple Futures Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Temporal prediction is critical for making intelligent and robust decisions in complex dynamic environments. Motion prediction needs to model the inherently uncertain future which often contains multiple potential outcomes, due to multi-agent interactions and the latent goals of others. Towards these goals, we introduce a probabilistic framework that efficiently learns latent variables to jointly model the multi-step future motions of agents in a scene. Our framework is data-driven and learns semantically meaningful latent variables to represent the multimodal future, without requiring explicit labels. Using a dynamic attention-based state encoder, we learn to encode the past as well as the future interactions among agents, efficiently scaling to any number of agents. Finally, our model can be used for planning via computing a conditional probability density over the trajectories of other agents given a hypothetical rollout of the 'self' agent. We demonstrate our algorithms by predicting vehicle trajectories of both simulated and real data, demonstrating the state-of-the-art results on several vehicle trajectory datasets.	[Tang, Yichuan Charlie; Salakhutdinov, Ruslan] Apple Inc, Cupertino, CA 95014 USA	Apple Inc	Tang, YC (corresponding author), Apple Inc, Cupertino, CA 95014 USA.	yichuan_tang@apple.com; rsalakhutdinov@apple.com						Alahi A, 2016, PROC CVPR IEEE, P961, DOI 10.1109/CVPR.2016.110; Bansal Mayank, 2018, CORR; Bayer J, 2014, ARXIV14117610; Bengio S, 2015, ADV NEURAL INFORM PR, V1, P1171; Burda Yuri, 2015, ARXIV150900519; Casas Sergio, 2018, C ROB LEARN, P2; Chai Yuning, 2019, C ROB LEARN CORL; Chang MF, 2019, PROC CVPR IEEE, P8740, DOI 10.1109/CVPR.2019.00895; Chung J., 2014, ARXIV14123555; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Colyar James, 2007, FHWAHRT07030, P2; Cui Henggang, 2018, CORR; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Deo N., 2018, CORR; Deo Nachiket, 2018, IEEE T INTELL VEHICL, V3, P129; Dosovitskiy A., 2017, C ROBOT LEARNING, P1; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Haarnoja Tuomas, 2016, CORR; Howard TM, 2014, IEEE ROBOT AUTOM MAG, V21, P64, DOI 10.1109/MRA.2013.2294914; Kuefler A, 2017, IEEE INT VEH SYM, P204, DOI 10.1109/IVS.2017.7995721; Lefevre S., 2014, ROBOMECH J, V1, P1, DOI DOI 10.1186/S40648-014-0001-Z; Ma WC, 2017, PROC CVPR IEEE, P4636, DOI 10.1109/CVPR.2017.493; Ma Y., 2018, CORR; Paden B, 2016, IEEE T INTELL VEHICL, V1, P33, DOI 10.1109/TIV.2016.2578706; Park S. H., 2018, CORR, Vabs/1802.06338; Rhinehart Nicholas, 2019, CORR; Sun C, 2019, IEEE I CONF COMP VIS, P7463, DOI 10.1109/ICCV.2019.00756; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Thrun S, 2006, J FIELD ROBOT, V23, P661, DOI 10.1002/rob.20147; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Watters N, 2017, ADV NEUR IN, V30; Weber T., 2017, CORR; Zhao Tianyang, 2019, CORR	39	65	65	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907012
C	Gonzalez-Garcia, A; van de Weijer, J; Bengio, Y		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gonzalez-Garcia, Abel; van de Weijer, Joost; Bengio, Yoshua			Image-to-image translation for cross-domain disentanglement	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep image translation methods have recently shown excellent results, outputting high-quality images covering multiple modes of the data distribution. There has also been increased interest in disentangling the internal representations learned by deep methods to further improve their performance and achieve a finer control. In this paper, we bridge these two objectives and introduce the concept of cross-domain disentanglement. We aim to separate the internal representation into three parts. The shared part contains information for both domains. The exclusive parts, on the other hand, contain only factors of variation that are particular to each domain. We achieve this through bidirectional image translation based on Generative Adversarial Networks and cross-domain autoencoders, a novel network component. Our model offers multiple advantages. We can output diverse samples covering multiple modes of the distributions of both domains, perform domain-specific image transfer and interpolation, and cross-domain retrieval without the need of labeled data, only paired images. We compare our model to the state-of-the-art in multi-modal image translation and achieve better results for translation on challenging datasets as well as for cross-domain retrieval on realistic datasets.	[Gonzalez-Garcia, Abel] Comp Vis Ctr, Barcelona, Spain; [van de Weijer, Joost] Univ Autonoma Barcelona, Comp Vis Ctr, Barcelona, Spain; [Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada	Centre de Visio per Computador (CVC); Autonomous University of Barcelona; Centre de Visio per Computador (CVC); Universite de Montreal	Gonzalez-Garcia, A (corresponding author), Comp Vis Ctr, Barcelona, Spain.	agonzalez@cvc.uab.es		van de Weijer, Joost/0000-0002-9656-9706	CHISTERA project M2CR [PCIN2015-251];  [TIN2016-79717-R]	CHISTERA project M2CR; 	We acknowledge the Spanish project TIN2016-79717-R and the CHISTERA project M2CR (PCIN2015-251).	[Anonymous], 2017, NIPS; [Anonymous], 2016, ADV NEURAL INFORM PR; [Anonymous], 2018, ECCV; [Anonymous], 2018, P IEEE C COMP VIS PA; [Anonymous], 2018, CVPR; [Anonymous], 2018, ICML; [Anonymous], 2011, ICANN; Aubry M., 2014, CVPR; Aytar Y., 2017, IEEE T PAMI; Barrow H. G., 1978, COMPUT VIS SYST; Bengio Y., 2013, IEEE T PAMI; Bottou L., 2017, ICML; Bousmalis K., 2016, ADV NEURAL INFORM PR; Bousmalis Konstantinos, 2017, CVPR; Chen X, 2016, ADV NEUR IN, V29; Darrell T, 2017, ICLR; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Eigen D., 2015, ICCV; Feutry C., 2018, ARXIV180209386V1; Fidler S, 2012, NIPS, P620; Ganin Y., 2015, ICML; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani Ishaan, 2017, NIPS; Huang Xun, 2018, ECCV; Iizuka S., 2016, ACM TOG; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Isola P., 2017, CVPR; Ji X., 2017, ACM MULTIMEDIA; Kingma D. P., 2014, NIPS; Li C., 2016, ECCV; Liu Y., 2018, CVPR; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Ma Liqian, 2018, CVPR; Mathieu Michael, 2016, P INT C LEARN REPR I; Pang K., 2017, BRIT MACH VIS C BMVC; Radford A., 2015, ICLR; Reed S., 2014, ICML; Reed S. E., 2015, P ADV NEUR INF PROC, P1; Rifai S., 2012, ECCV; Ronneberger O., 2015, ICMICCAI; Salimans Tim, 2016, ADV NEURAL INFORM PR; Tappen M.F., 2003, NIPS; Tenenbaum J., 1997, NIPS; Tran L., 2017, CVPR; Tylecek R., 2013, GCPR; Wang Y., 2018, CVPR; Zhang R., 2016, ECCV; Zhu J.Y., 2017, CVPR	50	65	67	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301029
C	Raghunathan, A; Steinhardt, J; Liang, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy			Semidefinite relaxations for certifying robustness to adversarial examples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite their impressive performance on diverse tasks, neural networks fail catastrophically in the presence of adversarial inputs-imperceptibly but adversarially perturbed versions of natural inputs. We have witnessed an arms race between defenders who attempt to train robust networks and attackers who try to construct adversarial examples. One promise of ending the arms race is developing certified defenses, ones which are provably robust against all attackers in some family. These certified defenses are based on convex relaxations which construct an upper bound on the worst case loss over all attackers in the family. Previous relaxations are loose on networks that are not trained against the respective relaxation. In this paper, we propose a new semidefinite relaxation for certifying robustness that applies to arbitrary ReLU networks. We show that our proposed relaxation is tighter than previous relaxations and produces meaningful robustness guarantees on three different foreign networks whose training objectives are agnostic to our proposed relaxation.	[Raghunathan, Aditi; Steinhardt, Jacob; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Raghunathan, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	aditir@cs.stanford.edu; jsteinhardt@cs.stanford.edu; pliang@cs.stanford.edu		Steinhardt, Jacob Noah/0000-0002-0257-3860	Future of Life Institute Research Award; Open Philanthrophy Project Award; Fannie & John Hertz Foundation Fellowship; NSF Graduate Research Fellowship	Future of Life Institute Research Award; Open Philanthrophy Project Award; Fannie & John Hertz Foundation Fellowship; NSF Graduate Research Fellowship(National Science Foundation (NSF))	This work was partially supported by a Future of Life Institute Research Award and Open Philanthrophy Project Award. JS was supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Research Fellowship. We thank Eric Wong for providing relevant experimental results. We are also grateful to Moses Charikar, Zico Kolter and Eric Wong for several helpful discussions and anonymous reviewers for useful feedback.	Ahmadi A. A., 2017, ARXIV170602586; [Anonymous], 2017, GROUND TRUTH ADVERSA; Athalye A., 2018, P 35 INT C MACH LEAR; Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Brown T. B., 2017, P NIPS WORKSH; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Dvijotham K, 2018, ARXIV180510265; DVIJOTHAM K, 2018, ARXIV180306567; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; Evtimov I., 2017, ROBUST PHYS WORLD AT; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Hein M, 2017, NIPS 17; Huang S., 2017, ADVERSARIAL ATTACKS; Jia R., 2017, EMPIRICAL METHODS NA; Katz G., 2017, PROVING ADVERSARIAL; Katz G., 2017, ARXIV170201135; Kolter J. Z., 2018, ICML 2018; Lofberg J., 2004, CACSD; Lu Jiajun, 2017, ARXIV PREPRINT ARXIV; Madry Aleksander, 2018, 6 INT C LEARN REPR I; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Raghunathan Aditi, 2018, INT C LEARNING REPRE; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Szegedy Christian, 2014, PROC 2 INT C LEARN R; Tjeng V., 2017, INT C LEARN REPR; Vershynin R., 2010, INTRO NONASYMPTOTIC; Wong E., 2018, ARXIV180512514; Wong Eric, 2018, INT C MACH LEARN ICM; Zhang Huan, 2018, ARXIV180409699	30	65	67	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005046
C	Triantafillou, E; Zemel, R; Urtasun, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Triantafillou, Eleni; Zemel, Richard; Urtasun, Raquel			Few-Shot Learning Through an Information Retrieval Lens	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Few-shot learning refers to understanding new concepts from only a few examples. We propose an information retrieval-inspired approach for this problem that is motivated by the increased importance of maximally leveraging all the available information in this low-data regime. We define a training objective that aims to extract as much information as possible from each training batch by effectively optimizing over all relative orderings of the batch points simultaneously. In particular, we view each batch point as a 'query' that ranks the remaining ones based on its predicted relevance to them and we define a model within the framework of structured prediction to optimize mean Average Precision over these rankings. Our method achieves impressive results on the standard few-shot classification benchmarks while is also capable of few-shot retrieval.	[Triantafillou, Eleni; Zemel, Richard] Univ Toronto, Vector Inst, Toronto, ON, Canada; [Urtasun, Raquel] Univ Toronto, Vector Inst, Uber ATG, Toronto, ON, Canada	University of Toronto; University of Toronto	Triantafillou, E (corresponding author), Univ Toronto, Vector Inst, Toronto, ON, Canada.		Jeong, Yongwook/N-7413-2016					Bellet A., 2013, ARXIV; Bromley J., 1993, International Journal of Pattern Recognition and Artificial Intelligence, V7, P669, DOI 10.1142/S0218001493000339; Chopra S, 2005, PROC CVPR IEEE, P539, DOI 10.1109/cvpr.2005.202; Finn C, 2017, ARXIV170303400; Goldberger Jacob, 2005, ADV NEURAL INFORM PR, V17, P8, DOI DOI 10.1109/TCSVT.2013.2242640; Hazan Tamir, 2010, NEURIPS; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kaiser O., 2017, P INT C LEARN REPR; Kingma D.P, P 3 INT C LEARNING R; Koch Gregory, 2015, ICML DEEP LEARN WORK; Lake B.M., 2011, P 33 ANN C COGN SCI, V172, P2; Min RQ, 2009, IEEE DATA MINING, P357, DOI 10.1109/ICDM.2009.27; Mohapatra P., 2014, P ADV NEUR INF PROC, P2312; Ravi Sachin, 2017, INT C LEARN REPR, V1, P6; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov R., 2007, AISTATS, V11; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shyam Pranav, 2017, ARXIV170300767; Snell J., 2017, ARXIV170305175; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Song Hyun Oh, 2016, ARXIV161201213; Song Y, 2016, PR MACH LEARN RES, V48; Tsochantaridis I, 2005, J MACH LEARN RES, V6, P1453; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Weinberger K.Q., P ADV NEURAL INFORM, P1473; Yisong Yue, 2007, 30th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P271	27	65	67	0	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402030
C	Shental, N; Bar-Hillel, A; Hertz, T; Weinshall, D		Thrun, S; Saul, K; Scholkopf, B		Shental, N; Bar-Hillel, A; Hertz, T; Weinshall, D			Computing Gaussian mixture models with EM using equivalence constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Density estimation with Gaussian Mixture Models is a popular generative technique used also for clustering. We develop a framework to incorporate side information in the form of equivalence constraints into the model estimation procedure. Equivalence constraints are defined on pairs of data points, indicating whether the points arise from the same source (positive constraints) or from different sources (negative constraints). Such constraints can be gathered automatically in some learning problems, and are a natural form of supervision in others. For the estimation of model parameters we present a closed form EM procedure which handles positive constraints, and a Generalized EM procedure using a Markov net which handles negative constraints. Using publicly available data sets we demonstrate that such side information can lead to considerable improvement in clustering tasks, and that our algorithm is preferable to two other suggested methods using the same type of side information.	Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel	Hebrew University of Jerusalem	Shental, N (corresponding author), Hebrew Univ Jerusalem, Ctr Neural Computat, IL-91904 Jerusalem, Israel.	fenoam@cs.huji.ac-il; aharonbh@cs.huji.ac.il; tomboy@cs.huji.ac.il; daphna@cs.huji.ac.il	Hertz, Tomer/S-5744-2016; Hillel, Aharon Bar/R-2656-2016	Hertz, Tomer/0000-0002-0561-1578; Bar-Hillel, Aharon/0000-0002-7303-0687				DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Georghiades A. S., 2000, Proceedings Fourth IEEE International Conference on Automatic Face and Gesture Recognition (Cat. No. PR00580), P277, DOI 10.1109/AFGR.2000.840647; KLEIN D, 2002, ICML; Miller DJ, 1997, ADV NEUR IN, V9, P571; Nigam K, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P792; PHILLIPS PJ, 1998, NIPS, V11, P803; Shental N, 2002, LECT NOTES COMPUT SC, V2353, P776; Szummer M., 2001, ADV NEURAL INFORM PR; Wagstaff K., 2001, ICML, V1, P577, DOI DOI 10.1109/TPAMI.2002.1017616; XING E, 2002, ADV NEURAL INFORMATI, V15	10	65	67	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						465	472						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500059
C	Taskar, B; Wong, MF; Abbeel, P; Koller, D		Thrun, S; Saul, K; Scholkopf, B		Taskar, B; Wong, MF; Abbeel, P; Koller, D			Link prediction in relational data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Many real-world domains are relational in nature, consisting of a set of objects related to each other in complex ways. This paper focuses on predicting the existence and the type of links between entities in such domains. We apply the relational Markov network framework of Taskar et al. to define a joint probabilistic model over the entire link graph - entity attributes and links. The application of the RMN algorithm to this task requires the definition of probabilistic patterns over subgraph structures. We apply this method to two new relational datasets, one involving university webpages, and the other a social network. We show that the collective classification approach of RMNs, and the introduction of subgraph patterns over link labels, provide significant improvements in accuracy over flat classification, which attempts to predict each link in isolation.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	Taskar, B (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	btaskar@cs.stanford.edu; mingfai.wong@cs.stanford.edu; abbeel@cs.stanford.edu; koller@cs.stanford.edu						ADAMIC L, 2002, SOCIAL NETWORK CAUGH; [Anonymous], 1994, INDUCTIVE LOGIC PROG; CRAVEN M, 1998, P AAAI; DellaPietra S, 1997, IEEE T PATTERN ANAL, V19, P380, DOI 10.1109/34.588021; Egghe L., 1990, INTRO INFORMETRICS; GETOOR L, 2001, P ICML; GETOOR L, 2001, IJCAI WORKSH TEXT LE; GHANI R, 2001, P ICML; Kleinberg JM, 1999, J ACM, V46, P604, DOI 10.1145/324133.324140; Koller D, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P580; Neville J., 2000, AAAI WORKSH LEARN ST; Page L., 1999, TECHNICAL REPORT 199; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; TASKAR B, 2002, P UAI; Taskar B., 2001, P 17 INT JOINT C ART, P870; Wasserman S, 1996, PSYCHOMETRIKA, V61, P401, DOI 10.1007/BF02294547; YEDIDIA I, 2000, P NIPS	17	65	70	0	5	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						659	666						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500083
C	Hershey, J; Movellan, J		Solla, SA; Leen, TK; Muller, KR		Hershey, J; Movellan, J			Audio-vision: Using audio-visual synchrony to locate sounds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					Psychophysical and physiological evidence shows that sound localization of acoustic signals is strongly influenced by their synchrony with visual signals. This effect, known as ventriloquism, is at work when sound coming from the side of a TV set feels as if it were coming from the mouth of the actors. The ventriloquism effect suggests that there is important information about sound location encoded in the synchrony between the audio and video signals. In spite of this evidence, audiovisual synchrony is rarely used as a source of information in computer vision tasks. In this paper we explore the use of audio visual synchrony to locate sound sources. We developed a system that searches for regions of the visual landscape that correlate highly with the acoustic signals and tags them as likely to contain an acoustic source. We discuss our experience implementing the system, present results on a speaker localization task and discuss potential applications of the approach.	Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA	University of California System; University of California San Diego	Hershey, J (corresponding author), Univ Calif San Diego, Dept Cognit Sci, La Jolla, CA 92093 USA.							Bertelson P., 1994, P 1994 INT C SPOK LA, V2, P559; Driver J, 1996, NATURE, V381, P66, DOI 10.1038/381066a0; Feldman DE, 1997, J NEUROSCI, V17, P6820; GOODALL C, 1983, WILEY SERIES PROBABI; RADEAU M, 1977, PERCEPT PSYCHOPHYS, V22, P137, DOI 10.3758/BF03198746; Recanzone GH, 1998, P NATL ACAD SCI USA, V95, P869, DOI 10.1073/pnas.95.3.869; Stryker MP, 1999, SCIENCE, V284, P925, DOI 10.1126/science.284.5416.925; ZHENG W, 1999, FUNCTIONAL SELECTION, P962	8	65	71	1	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						813	819						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700115
C	Sill, J		Jordan, MI; Kearns, MJ; Solla, SA		Sill, J			Monotonic networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Monotonicity is a constraint which arises in many application domains. We present a machine learning model, the monotonic network, for which monotonicity can be enforced exactly, i.e., by virtue of functional form. A straightforward method for implementing and training a monotonic network is described. Monotonic networks are proven to be universal approximators of continuous, differentiable monotonic functions. We apply monotonic networks to a real-world task in corporate bond rating prediction and compare them to other approaches.	CALTECH, Computat & Neural Syst Program, Pasadena, CA 91125 USA	California Institute of Technology	Sill, J (corresponding author), CALTECH, Computat & Neural Syst Program, MC 136-93, Pasadena, CA 91125 USA.								0	65	69	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						661	667						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700094
C	Tesauro, G; Galperin, GR		Mozer, MC; Jordan, MI; Petsche, T		Tesauro, G; Galperin, GR			On-line policy improvement using Monte-Carlo search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present a Monte-Carlo simulation algorithm for real-time policy improvement of an adaptive controller. In the Monte-Carlo simulation, the long-term expected reward of each possible action is statistically measured, using the initial policy to make decisions in each step of the simulation. The action maximizing the measured expected reward is then taken, resulting in an improved policy. Our algorithm is easily parallelizable and has been implemented on the IBM SP1 and SP2 parallel-RISC supercomputers. We have obtained promising initial results in applying this algorithm to the domain of backgammon. Results are reported for a wide variety of initial policies, ranging from a random policy to TD-Gammon, an extremely strong multi-layer neural network. fn each case, the Monte-Carlo algorithm gives a substantial reduction, by as much as a factor of 5 or more, in the error rate of the base players. The algorithm is also potentially useful in many other adaptive control applications in which it is possible to simulate the environment.			Tesauro, G (corresponding author), IBM CORP,THOMAS J WATSON RES CTR,POB 704,YORKTOWN HTS,NY 10598, USA.								0	65	70	1	7	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1068	1074						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00150
C	Tsitsiklis, JN; VanRoy, B		Mozer, MC; Jordan, MI; Petsche, T		Tsitsiklis, JN; VanRoy, B			Analysis of temporal-difference learning with function approximation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We present new results about the temporal-difference learning algorithm, as applied to approximating the cost-to-go function of a Markov chain using linear function approximators. The algorithm we analyze performs on-line updating of a parameter vector during a single endless trajectory of an aperiodic irreducible finite state Markov chain. Results include convergence (with probability 1), a characterization of the limit of convergence, and a bound on the resulting approximation error. In addition to establishing new and stronger results than those previously available, our analysis is based on a new line of reasoning that provides new intuition about the dynamics of temporal-difference learning. Furthermore, we discuss the implications of two counter-examples with regards to the significance of on-line updating and linearly parameterized function approximators.			Tsitsiklis, JN (corresponding author), MIT,INFORMAT & DECIS SYST LAB,77 MASSACHUSETTS AVE,CAMBRIDGE,MA 02139, USA.								0	65	65	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						1075	1081						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00151
C	Kim, J; Park, S; Kwak, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kim, Jangho; Park, SeongUk; Kwak, Nojun			Paraphrasing Complex Network: Network Compression via Factor Transfer	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				AUTOENCODERS	Many researchers have sought ways of model compression to reduce the size of a deep neural network (DNN) with minimal performance degradation in order to use DNNs in embedded systems. Among the model compression methods, a method called knowledge transfer is to train a student network with a stronger teacher network. In this paper, we propose a novel knowledge transfer method which uses convolutional operations to paraphrase teacher's knowledge and to translate it for the student. This is done by two convolutional modules, which are called a paraphraser and a translator. The paraphraser is trained in an unsupervised manner to extract the teacher factors which are defined as paraphrased information of the teacher network. The translator located at the student network extracts the student factors and helps to translate the teacher factors by mimicking them. We observed that our student network trained with the proposed factor transfer method outperforms the ones trained with conventional knowledge transfer methods.	[Kim, Jangho; Park, SeongUk; Kwak, Nojun] Seoul Natl Univ, Seoul, South Korea	Seoul National University (SNU)	Kim, J (corresponding author), Seoul Natl Univ, Seoul, South Korea.	kjh91@snu.ac.kr; swpark0703@snu.ac.kr; nojunk@snu.ac.kr		Kim, Jangho/0000-0003-1334-4649	Next-Generation Information Computing Development Program through the NRF of Korea [2017M3C4A7077582]; ICT R&D program of MSIP/IITP, Korean Government [2017-0-00306]	Next-Generation Information Computing Development Program through the NRF of Korea; ICT R&D program of MSIP/IITP, Korean Government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea)	This work was supported by Next-Generation Information Computing Development Program through the NRF of Korea (2017M3C4A7077582) and ICT R&D program of MSIP/IITP, Korean Government (2017-0-00306).	Alain G, 2014, J MACH LEARN RES, V15, P3563; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Everingham M., PASCAL VISUAL OBJECT; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang Gao, 2017, ARXIV PREPRINT ARXIV; Iandola F.N., 2016, ARXIV; Krizhevsky A., CIFAR10; Lebedev V, 2016, PROC CVPR IEEE, P2554, DOI 10.1109/CVPR.2016.280; Masci J, 2011, LECT NOTES COMPUT SC, V6791, P52, DOI 10.1007/978-3-642-21735-7_7; Ng WWY, 2016, PATTERN RECOGN, V60, P875, DOI 10.1016/j.patcog.2016.06.013; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Romero Adriana, 2014, ARXIV14126550; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shin HC, 2013, IEEE T PATTERN ANAL, V35, P1930, DOI 10.1109/TPAMI.2012.277; Srinivas S., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.5244/C.29.31; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Zoph B., 2016, ARXIV161101578	31	64	68	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302075
C	Rocco, I; Cimpoi, M; Arandjelovic, R; Torii, A; Pajdla, T; Sivic, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Rocco, Ignacio; Cimpoi, Mircea; Arandjelovic, Relja; Torii, Akihiko; Pajdla, Tomas; Sivic, Josef			Neighbourhood Consensus Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We address the problem of finding reliable dense correspondences between a pair of images. This is a challenging task due to strong appearance differences between the corresponding scene elements and ambiguities generated by repetitive patterns. The contributions of this work are threefold. First, inspired by the classic idea of disambiguating feature matches using semi-local constraints, we develop an end-to-end trainable convolutional neural network architecture that identifies sets of spatially consistent matches by analyzing neighbourhood consensus patterns in the 4D space of all possible correspondences between a pair of images without the need for a global geometric model. Second, we demonstrate that the model can be trained effectively from weak supervision in the form of matching and non-matching image pairs without the need for costly manual annotation of point to point correspondences. Third, we show the proposed neighbourhood consensus network can be applied to a range of matching tasks including both category- and instance-level matching, obtaining the state-of-the-art results on the PF Pascal dataset and the InLoc indoor visual localization benchmark.	[Rocco, Ignacio; Sivic, Josef] PSL Res Univ, CNRS, INRIA, Dept Informat,ENS,UMR 8548, Paris, France; [Cimpoi, Mircea; Pajdla, Tomas; Sivic, Josef] CTU, CIIRC, Prague, Czech Republic; [Arandjelovic, Relja] DeepMind, London, England; [Torii, Akihiko] Tokyo Inst Technol, Tokyo, Japan	Centre National de la Recherche Scientifique (CNRS); Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Universite Paris Cite; Czech Technical University Prague; Tokyo Institute of Technology	Rocco, I (corresponding author), PSL Res Univ, CNRS, INRIA, Dept Informat,ENS,UMR 8548, Paris, France.		Rocco, Ignacio/AAH-5435-2019; Cimpoi, Mircea/ABE-9462-2020	Rocco, Ignacio/0000-0002-9638-0508; Cimpoi, Mircea/0000-0002-5624-7593; Pajdla, Tomas/0000-0001-6325-0072	JSPS KAKENHI [15H05313, 16KK0002]; EU-H2020 project LADIO [731970]; ERC grant LEAP [336845]; CIFAR Learning in Machines Brains program; European Regional Development Fund under the project IMPACT [CZ.02.1.01/0.0/0.0/15 003/0000468]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); EU-H2020 project LADIO; ERC grant LEAP; CIFAR Learning in Machines Brains program; European Regional Development Fund under the project IMPACT	This work was partially supported by JSPS KAKENHI Grant Numbers 15H05313, 16KK0002, EU-H2020 project LADIO No. 731970, ERC grant LEAP No. 336845, CIFAR Learning in Machines & Brains program and the European Regional Development Fund under the project IMPACT (reg. no. CZ.02.1.01/0.0/0.0/15 003/0000468).	Agarwal S, 2011, COMMUN ACM, V54, P105, DOI 10.1145/2001269.2001293; Balntas V., 2016, ARXIV160105030; Balntas V., 2016, BMVC, V1, P3; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Everingham M., 2012, PASCAL VISUAL OBJECT; Fischer P, 2014, ARXIV14055769; Ham B., 2017, IEEE T PAMI; Han K., 2017, ICCV; Han X., 2015, P IEEE C COMP VIS PA; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Jahrer M., 2008, COMP VIS WINT WORKSH; Kanazawa Angjoo, 2018, ECCV; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kingma D.P, P 3 INT C LEARNING R; Liu C., 2008, P ECCV; Long J.L., 2014, P C NEUR INF PROC SY, V27, P1601; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Mikolajczyk K, 2002, LECT NOTES COMPUT SC, V2350, P128, DOI 10.1007/3-540-47969-4_9; Noh H, 2017, IEEE I CONF COMP VIS, P3476, DOI 10.1109/ICCV.2017.374; Rocco I., 2018, CVPR; Rocco I, 2018, ADV NEUR IN, V31; Rocco I, 2017, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2017.12; Sattler T., 2018, CVPR; Sattler T., 2009, P ICCV; Savinov N., 2017, NIPS; SCHAFFALITZKY F, 2002, INT C IM VID RETR; Schmid C., 1997, IEEE PAMI; Schonberger J. L., 2017, P CVPR; Simo-Serra E., 2015, ICCV; Simonyan Karen, 2014, IEEE PAMI; Sivic J, 2003, NINTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOLS I AND II, PROCEEDINGS, P1470, DOI 10.1109/iccv.2003.1238663; Sun D., 2010, P CVPR; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Taira Hajime, 2018, CVPR; Tuytelaars T, 2007, FOUND TRENDS COMPUT, V3, P177, DOI 10.1561/0600000017; Yi KM, 2016, LECT NOTES COMPUT SC, V9910, P467, DOI 10.1007/978-3-319-46466-4_28; Zagoruyko S., 2015, CVPR; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; ZHANG ZY, 1995, ARTIF INTELL, V78, P87, DOI 10.1016/0004-3702(95)00022-4	44	64	66	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301062
C	Tran, T; Pham, T; Carneiro, G; Palmer, L; Reid, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Toan Tran; Trung Pham; Carneiro, Gustavo; Palmer, Lyle; Reid, Ian			A Bayesian Data Augmentation Approach for Learning Deep Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISTRIBUTIONS; NETWORKS	Data augmentation is an essential part of the training process applied to deep learning models. The motivation is that a robust training process for deep learning models depends on large annotated datasets, which are expensive to be acquired, stored and processed. Therefore a reasonable alternative is to be able to automatically generate new annotated training samples using a process known as data augmentation. The dominant data augmentation approach in the field assumes that new training samples can be obtained via random geometric or appearance transformations applied to annotated training samples, but this is a strong assumption because it is unclear if this is a reliable generative model for producing new training samples. In this paper, we provide a novel Bayesian formulation to data augmentation, where new annotated training points are treated as missing variables and generated based on the distribution learned from the training set. For learning, we introduce a theoretically sound algorithm - generalised Monte Carlo expectation maximisation, and demonstrate one possible implementation via an extension of the Generative Adversarial Network (GAN). Classification results on MNIST, CIFAR-10 and CIFAR-100 show the better performance of our proposed method compared to the current dominant data augmentation approach mentioned above - the results also show that our approach produces better classification results than similar GAN models.	[Toan Tran; Trung Pham; Carneiro, Gustavo; Reid, Ian] Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia; [Palmer, Lyle] Univ Adelaide, Sch Publ Hlth, Adelaide, SA, Australia	University of Adelaide; University of Adelaide	Tran, T (corresponding author), Univ Adelaide, Sch Comp Sci, Adelaide, SA, Australia.	toan.m.tran@adelaide.edu.au; trung.pham@adelaide.edu.au; gustavo.carneiro@adelaide.edu.au; lyle.palmer@adelaide.edu.au; ian.reid@adelaide.edu.au	Palmer, Lyle John/K-3196-2014; Jeong, Yongwook/N-7413-2016; Tran, Toan/P-8774-2019	Palmer, Lyle John/0000-0002-1628-3055; Reid, Ian/0000-0001-7790-6423; Tran, Toan/0000-0001-7182-7548; Carneiro, Gustavo/0000-0002-5571-6220	Vietnam International Education Development (VIED); Australian Research Council through the Centre of Excellence for Robotic Vision [CE140100016]; Australian Research Council through Laureate Fellowship [FL130100102]	Vietnam International Education Development (VIED); Australian Research Council through the Centre of Excellence for Robotic Vision(Australian Research Council); Australian Research Council through Laureate Fellowship(Australian Research Council)	TT gratefully acknowledges the support by Vietnam International Education Development (VIED). TP, GC and IR gratefully acknowledge the support of the Australian Research Council through the Centre of Excellence for Robotic Vision (project number CE140100016) and Laureate Fellowship FL130100102 to IR.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bishop C. M., 2006, J ELECT IMAG, V16, P140; Carreira-Perpinan M.A., 2005, P ARTIFICIAL INTELLI, VR5:, P33; Chen X, 2016, ADV NEUR IN, V29; Collobert R., 2008, AUNIFIED ARCHITECTUR, P160; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Cui XD, 2015, IEEE-ACM T AUDIO SPE, V23, P1469, DOI 10.1109/TASLP.2015.2438544; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Fawzi A, 2016, IEEE IMAGE PROC, P3688, DOI 10.1109/ICIP.2016.7533048; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hauberg S, 2016, JMLR WORKSH CONF PRO, V51, P342; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Chao, 2017, NONLINEAR OPTICS PRI; Mirza M., 2014, ARXIV PREPRINT ARXIV; Odena A., 2016, ARXIV161009585; Odena Augustus, 2016, CONDITIONAL IMAGE SY; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Simard P. Y., 2003, P 7 INT C DOC AN REC, V2; Socher R., 2009, CVPR09; Tanner M. A., 1991, LECT NOTES STAT, V67; TANNER MA, 1987, J AM STAT ASSOC, V82, P528, DOI 10.2307/2289457; Yaeger L., 1996, NIPS, V9, P807; Zhang X., 2015, ARXIV150201710	31	64	64	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402082
C	Nguyen, TD; Le, T; Vu, H; Phung, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tu Dinh Nguyen; Trung Le; Hung Vu; Dinh Phung			Dual Discriminator Generative Adversarial Nets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.	[Tu Dinh Nguyen; Trung Le; Hung Vu; Dinh Phung] Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia	Deakin University	Nguyen, TD (corresponding author), Deakin Univ, Ctr Pattern Recognit & Data Analyt, Geelong, Vic, Australia.	tu.nguyen@deakin.edu.au; trung.1@deakin.edu.au; hungv@deakin.edu.au; dinh.phung@deakin.edu.au	Jeong, Yongwook/N-7413-2016	Phung, Dinh/0000-0002-9977-8247	Australian Research Council (ARC) [DP160109394]	Australian Research Council (ARC)(Australian Research Council)	This work was partially supported by the Australian Research Council (ARC) Discovery Grant Project DP160109394.	Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M., 2017, ARXIV170107875; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Burges, 1998, MNIST DATABASE HANDW; Che Tong, 2016, ARXIV161202136; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Denton Emily L, 2015, NEURIPS, V2, P4; Dumoulin Vincent, 2016, ARXIV E PRINTS; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I., 2016, ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gupta Somesh Das, 2000, MATH STAT; Hoang Q., 2017, P INT C LEARN REPR; Huszar Ferenc, 2015, ABS151105101 CORR; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lemarechal Claude, 2012, FUNDAMENTALS CONVEX; Metz Luke, 2016, ARXIV161102163; Mirza M., 2014, ARXIV; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Nowozin S, 2016, ADV NEUR IN, V29; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Theis Lucas, 2015, ARXIV151101844; Wang Ruohan, 2017, ARXIV170403817; Warde-Farley D, 2017, ICLR SUBMISSIONS, V8	31	64	69	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402070
C	Vandat, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Vandat, Arash			Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Collecting large training datasets, annotated with high-quality labels, is costly and time-consuming. This paper proposes a novel framework for training deep convolutional neural networks from noisy labeled datasets that can be obtained cheaply. The problem is formulated using an undirected graphical model that represents the relationship between noisy and clean labels, trained in a semi-supervised setting. In our formulation, the inference over latent clean labels is tractable and is regularized during training using auxiliary sources of information. The proposed model is applied to the image labeling problem and is shown to be effective in labeling unseen images as well as reducing label noise in training on CIFAR-10 and MS COCO datasets.	[Vandat, Arash] D Wave Syst Inc, Burnaby, BC, Canada		Vandat, A (corresponding author), D Wave Syst Inc, Burnaby, BC, Canada.	avandat@dwavesys.com	Jeong, Yongwook/N-7413-2016					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen LC, 2015, PR MACH LEARN RES, V37, P1785; Chen XL, 2015, IEEE I CONF COMP VIS, P1431, DOI 10.1109/ICCV.2015.168; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Do T., 2010, INT C ART INT STAT, P177; Fang H., 2015, C COMP VIS PATT REC; Fergus R., 2009, ADV NEURAL INFORM PR, P522; Frenay B, 2014, IEEE T NEUR NET LEAR, V25, P845, DOI 10.1109/TNNLS.2013.2292894; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang S, 2016, IEEE IPCCC; Kingma D.P, P 3 INT C LEARNING R; Kirillov A., 2015, ARXIV151105067; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Lin G., 2016, COMPUTER VISION PATT; Maaten L., 2011, P 14 INT C ARTIFICIA, P479; Misra I, 2016, COMPUTER VISION PATT; Mnih V., 2012, P 29 INT C MACHINE L, P567, DOI DOI 10.5555/3042573.3042603; Natarajan Nagarajan, 2013, ADV NEURAL INFORM PR; Neal R. M., 1998, LEARNING GRAPHICAL M; Patrini G, 2017, PROC CVPR IEEE, P2233, DOI 10.1109/CVPR.2017.240; Prabhavalkar R, 2010, INT CONF ACOUST SPEE, P5534, DOI 10.1109/ICASSP.2010.5495222; Quattoni A, 2007, IEEE T PATTERN ANAL, V29, P1848, DOI 10.1109/TPAMI.2007.1124; Reed Scott, 2014, ARXIV14126596; Rohrbach M., 2010, CVPR; Ross S., 2011, CVPR; Schwing A. G., 2015, ARXIV PREPRINT ARXIV; Sukhbaatar Sainbayar, 2014, ICLR; Tieleman T., 2008, P 25 INT C MACHINE L, P1064, DOI DOI 10.1145/1390156.1390290; Veit A, 2017, PROC CVPR IEEE, P6575, DOI 10.1109/CVPR.2017.696; Xiao T., 2015, CVPR; Younes L., 1989, PROBABILITY THEORY R; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhu X., 2003, P ICML WORKSH CONT L, V3	36	64	66	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405066
C	Haarnoja, T; Ajay, A; Levine, S; Abbeel, P		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Haarnoja, Tuomas; Ajay, Anurag; Levine, Sergey; Abbeel, Pieter			Backprop KF: Learning Discriminative Deterministic State Estimators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Generative state estimators based on probabilistic filters and smoothers are one of the most popular classes of state estimators for robots and autonomous vehicles. However, generative models have limited capacity to handle rich sensory observations, such as camera images, since they must model the entire distribution over sensor readings. Discriminative models do not suffer from this limitation, but are typically more complex to train as latent variable models for state estimation. We present an alternative approach where the parameters of the latent state distribution are directly optimized as a deterministic computation graph, resulting in a simple and effective gradient descent algorithm for training discriminative state estimators. We show that this procedure can be used to train state estimators that use complex input, such as raw camera images, which must be processed using expressive nonlinear function approximators such as convolutional neural networks. Our model can be viewed as a type of recurrent neural network, and the connection to probabilistic filtering allows us to design a network architecture that is particularly well suited for state estimation. We evaluate our approach on synthetic tracking task with raw image inputs and on the visual odometry task in the KITTI dataset. The results show significant improvement over both standard generative approaches and regular recurrent neural networks.	[Haarnoja, Tuomas; Ajay, Anurag; Levine, Sergey; Abbeel, Pieter] Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Haarnoja, T (corresponding author), Univ Calif Berkeley, Dept Comp Sci, Berkeley, CA 94720 USA.	haarnoja@berkeley.edu; anuragajay@berkeley.edu; svlevine@berkeley.edu; pabbeel@berkeley.edu			ONR; Army Research Office; Berkeley DeepDrive Center	ONR(Office of Naval Research); Army Research Office; Berkeley DeepDrive Center	This research was funded in part by ONR through a Young Investigator Program award, by the Army Research Office through the MAST program, and by the Berkeley DeepDrive Center.	Abbeel P., 2005, ROBOTICS SCI SYSTEMS, V2, P1; Baccouche Moez, 2011, Human Behavior Unterstanding. Proceedings Second International Workshop, HBU 2011, P29, DOI 10.1007/978-3-642-25446-8_4; Bengio Y., 2014, ARXIV14061078; Bobrowski O., 2007, ADV NEURAL INFORM PR; Dahl GE, 2012, IEEE T AUDIO SPEECH, V20, P30, DOI 10.1109/TASL.2011.2134090; Do T., 2010, INT C ART INT STAT, P177; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Hess R, 2009, PROC CVPR IEEE, P240, DOI 10.1109/CVPRW.2009.5206801; Hinton, 2016, ARXIV PREPRINT ARXIV; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kim M., 2007, COMP VIS 2007 ICCV 2, P1; Kingma D.P, P 3 INT C LEARNING R; Ko J, 2009, AUTON ROBOT, V27, P75, DOI 10.1007/s10514-009-9119-x; Lafferty J., 2001, P 18 INT C MACHINE L, P282, DOI DOI 10.5555/645530.655813; Limketkai B., 2007, INT C ROB AUT; Morency L. -P., 2007, P I C COMP VI PATT R, P1, DOI DOI 10.1109/CVPR.2007.383299; Ondruska P., 2016, P 30 AAAI C ART INT; Ross D. A., 2006, P 23 INT C MACH LEAR, P761; Sminchisescu C, 2005, PROC CVPR IEEE, P390; Sontag D., 2015, ARXIV151105121; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thrun S, 2002, COMMUN ACM, V45, P52, DOI 10.1145/504729.504754; Wilson Robert, 2009, ADV NEURAL INFORM PR, V22, P2062; Yadaiah N., 2006, INT JOINT C NEUR NET	24	64	66	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700054
C	Choi, JH; Vishwanathan, SVN		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Choi, Joon Hee; Vishwanathan, S. V. N.			DFacTo: Distributed Factorization of Tensors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We present a technique for significantly speeding up Alternating Least Squares (ALS) and Gradient Descent (GD), two widely used algorithms for tensor factorization. By exploiting properties of the Khatri-Rao product, we show how to efficiently address a computationally challenging sub-step of both algorithms. Our algorithm, DFacTo, only requires two sparse matrix-vector products and is easy to parallelize. DFacTo is not only scalable but also on average 4 to 10 times faster than competing algorithms on a variety of datasets. For instance, DFacTo only takes 480 seconds on 4 machines to perform one iteration of the ALS algorithm and 1,143 seconds to perform one iteration of the GD algorithm on a 6.5 million x 2.5 million x 1.5 million dimensional tensor with 1.2 billion non-zero entries.	[Choi, Joon Hee] Purdue Univ, Elect & Comp Engn, W Lafayette, IN 47907 USA; [Vishwanathan, S. V. N.] Purdue Univ, Stat & Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus; Purdue University System; Purdue University; Purdue University West Lafayette Campus	Choi, JH (corresponding author), Purdue Univ, Elect & Comp Engn, W Lafayette, IN 47907 USA.	choi240@purdue.edu; vishy@stat.purdue.edu						Acar E, 2011, J CHEMOMETR, V25, P67, DOI 10.1002/cem.1335; Acar Evrim, 2011, MLG 11 P MIN LEARN G; Bader BW, 2007, SIAM J SCI COMPUT, V30, P205, DOI 10.1137/060676489; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; BERNSTEIN D. S., 2005, MATRIX MATH; Carlson Andrew, 2010, AAAI; Horn R.A., 2012, MATRIX ANAL, DOI [DOI 10.1017/CBO9780511810817, 10.1017/CBO9780511810817]; Kang U, 2012, P 18 ACM SIGKDD INT, P316, DOI 10.1145/2339530.2339583; Karatzoglou A, 2010, P 4 ACM C REC SYST, P79, DOI DOI 10.1145/1864708.1864727; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893; McAuley Julian, 2013, P 7 ACM C REC SYST A, DOI DOI 10.1145/2507157.2507163; Porter MF, 2006, PROGRAM-ELECTRON LIB, V40, P211, DOI 10.1108/eb046814; Smilde A, 2004, MULTIWAY ANAL APPL C	14	64	65	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101066
C	Stober, S; Cameron, DJ; Grahn, JA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Stober, Sebastian; Cameron, Daniel J.; Grahn, Jessica A.			Using Convolutional Neural Networks to Recognize Rhythm Stimuli from Electroencephalography Recordings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				NEURONAL ENTRAINMENT; METER; BEAT; BETA	Electroencephalography ( EEG) recordings of rhythm perception might contain enough information to distinguish different rhythm types/genres or even identify the rhythms themselves. We apply convolutional neural networks (CNNs) to analyze and classify EEG data recorded within a rhythm perception study in Kigali, Rwanda which comprises 12 East African and 12 Western rhythmic stimuli - each presented in a loop for 32 seconds to 13 participants. We investigate the impact of the data representation and the pre-processing steps for this classification tasks and compare different network structures. Using CNNs, we are able to recognize individual rhythms from the EEG with a mean classification accuracy of 24.4% (chance level 4.17%) over all subjects by looking at less than three seconds from a single channel. Aggregating predictions for multiple channels, a mean accuracy of up to 50% can be achieved for individual subjects.	[Stober, Sebastian; Cameron, Daniel J.; Grahn, Jessica A.] Western Univ, Brain & Mind Inst, Dept Psychol, London, ON N6A 5B7, Canada	Western University (University of Western Ontario)	Stober, S (corresponding author), Western Univ, Brain & Mind Inst, Dept Psychol, London, ON N6A 5B7, Canada.	sstober@uwo.ca; dcamer25@uwo.ca; jgrahn@uwo.ca	Grahn, Jessica/A-1371-2010	Grahn, Jessica/0000-0001-7270-2114	Postdoc-Program of the German Academic Exchange Service (DAAD); Natural Sciences and Engineering Research Council of Canada (NSERC) through the Western International Research Award [R4911A07]; AUCC Students for Development Award	Postdoc-Program of the German Academic Exchange Service (DAAD)(Deutscher Akademischer Austausch Dienst (DAAD)); Natural Sciences and Engineering Research Council of Canada (NSERC) through the Western International Research Award(Natural Sciences and Engineering Research Council of Canada (NSERC)); AUCC Students for Development Award	This work was supported by a fellowship within the Postdoc-Program of the German Academic Exchange Service (DAAD), by the Natural Sciences and Engineering Research Council of Canada (NSERC), through the Western International Research Award R4911A07, and by an AUCC Students for Development Award.	Barz G. F., 2004, MUSIC E AFRICA EXPER; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Cameron D. J., FRONTIERS HUMAN NEUR; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Fujioka T, 2012, J NEUROSCI, V32, P1791, DOI 10.1523/JNEUROSCI.4107-11.2012; Fujioka T, 2009, ANN NY ACAD SCI, V1169, P89, DOI 10.1111/j.1749-6632.2009.04779.x; Geiser E, 2009, CORTEX, V45, P93, DOI 10.1016/j.cortex.2007.09.010; Glorot X., 2010, NIPS 2010 WORKSH DEE; Goodfellow I.J., 2013, ARXIV13084214; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Iversen JR, 2009, ANN NY ACAD SCI, V1169, P58, DOI 10.1111/j.1749-6632.2009.04579.x; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ladinig O, 2009, MUSIC PERCEPT, V26, P377, DOI 10.1525/MP.2009.26.4.377; Langkvist M., 2012, ADV ARTIFICIAL NEURA, V2012; Nozaradan S, 2012, J NEUROSCI, V32, P17572, DOI 10.1523/JNEUROSCI.3203-12.2012; Nozaradan S, 2011, J NEUROSCI, V31, P10234, DOI 10.1523/JNEUROSCI.0411-11.2011; Plis SM, 2013, ARXIV13125847; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Snyder JS, 2005, COGNITIVE BRAIN RES, V24, P117, DOI 10.1016/j.cogbrainres.2004.12.014; Stober S., 2014, ISMIR, P649; Tang Yichuan, 2013, ARXIV13060239; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Vlek RJ, 2011, CLIN NEUROPHYSIOL, V122, P1526, DOI 10.1016/j.clinph.2011.01.042; Will U, 2007, NEUROSCI LETT, V424, P55, DOI 10.1016/j.neulet.2007.07.036; Wulsin DF, 2011, J NEURAL ENG, V8, DOI 10.1088/1741-2560/8/3/036015	25	64	66	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100051
C	Moore, AW		Kearns, MS; Solla, SA; Cohn, DA		Moore, AW			Very fast EM-based mixture model clustering using multiresolution kd-trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Clustering is important in many fields including manufacturing, biology, finance, and astronomy. Mixture models are a popular approach due to their statistical foundations. and EM is a very popular method for finding: mixture models. EM, however, requires many accesses of the data, and thus has been dismissed as impractical (e.g. [9]) for data milling of enormous datasets. We present a new algorithm, based on the multiresolution. kd-trees of [5], which dramatically reduces the cost of EM-based clustering, with savings rising linearly with the number of datapoints. Although presented here for maximum likelihood estimation of Gaussian mixture models, it is also applicable to non-Gaussian models (provided class densities are monotonic in Mahalanobis distance?), mixed categorical/numeric clusters, and Bayesian methods such as Autoclass [1].	Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Moore, AW (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.							ANDREW W, 1998, J ARTIFICIAL INTELLI, V8; CHEESEMAN P, 1994, LECT NOTES STAT, V89; DENG K, 1995, P IJCAI 95; Duda R.O., 1973, J ROYAL STAT SOC SER; ESTER M, 1995, P 1 INT C KNOWL DISC; MOORE AW, 1997, P 1997 INT MACH LEAR; Omohundro S. M., 1987, Complex Systems, V1, P273; OMOHUNDRO SM, 1991, ADV NEURAL INFORMATI, V3; ZHANG T, 1996, P 15 ACM SIGACT SIGM	9	64	68	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						543	549						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700077
C	Ghahramani, Z; Jordan, MI		Touretzky, DS; Mozer, MC; Hasselmo, ME		Ghahramani, Z; Jordan, MI			Factorial hidden Markov models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TORONTO,DEPT COMP SCI,TORONTO,ON M5S 1A4,CANADA	University of Toronto			Jordan, Michael I/C-5253-2013						0	64	64	0	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						472	478						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00067
C	MEL, BW		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MEL, BW			THE CLUSTERON - TOWARD A SIMPLE ABSTRACTION FOR A COMPLEX NEURON	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	64	65	0	2	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						35	42						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00005
C	WETTSCHERECK, D; DIETTERICH, T		MOODY, JE; HANSON, SJ; LIPPMANN, RP		WETTSCHERECK, D; DIETTERICH, T			IMPROVING THE PERFORMANCE OF RADIAL BASIS FUNCTION NETWORKS BY LEARNING-CENTER LOCATIONS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	64	67	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1133	1140						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00140
C	Gaier, A; Ha, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gaier, Adam; Ha, David			Weight Agnostic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ALGORITHM; SYNAPTOGENESIS; RECOGNITION; TOPOLOGY; SYSTEMS; CORTEX	Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights.	[Gaier, Adam] Univ Lorraine, CNRS, INRIA, Bonn Rhein Sieg Univ Appl Sci, Metz, France; [Ha, David] Google Brain, Tokyo, Japan	Centre National de la Recherche Scientifique (CNRS); Inria; Universite de Lorraine	Gaier, A (corresponding author), Univ Lorraine, CNRS, INRIA, Bonn Rhein Sieg Univ Appl Sci, Metz, France.	adam.gaier@h-brs.de; hadavid@google.com	Galer, Adam/GLR-5448-2022					Ackley D., 1991, SFI STUDIES SCI COMP, V10, P487; ANGELINE PJ, 1994, IEEE T NEURAL NETWOR, V5, P54, DOI 10.1109/72.265960; Baldwin J. M., 1896, AM NAT, V30, P441, DOI DOI 10.1086/276408; Barber D., 1998, Neural Networks and Machine Learning. Proceedings, P215; Bishop CM, 2006, PATTERN RECOGNITION; BLACK JE, 1990, P NATL ACAD SCI USA, V87, P5568, DOI 10.1073/pnas.87.14.5568; Blier L, 2018, ADV NEUR IN, V31; Braun H., 1993, P ANNGA93 INT C ART, P25; Brock A., 2018, ICLR, P1; Brockman G., 2016, OPENAI GYM; Bruer JT, 1999, PHI DELTA KAPPAN, V81, P264; Bullmore ET, 2009, NAT REV NEUROSCI, V10, P186, DOI 10.1038/nrn2575; Burger J, 1998, ANIM BEHAV, V56, P547, DOI 10.1006/anbe.1998.0809; Burges, 1998, MNIST DATABASE HANDW; Byoung-Tzk Zhang, 1993, Complex Systems, V7, P199; Chollet F., 2015, KERAS; Clune J, 2013, P ROY SOC B-BIOL SCI, V280, DOI 10.1098/rspb.2012.2863; Cohen N, 2017, INT C LEARN REPR ICL, P1; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dasgupta D., 1992, COGANN-92. International Workshop on Combinations of Genetic Algorithms and Neural Networks (Cat. No.92TH0435-8), P87, DOI 10.1109/COGANN.1992.273946; Dayan E, 2011, NEURON, V72, P443, DOI 10.1016/j.neuron.2011.10.008; Deb K, 2002, IEEE T EVOLUT COMPUT, V6, P182, DOI 10.1109/4235.996017; Eichler K, 2017, NATURE, V548, P175, DOI 10.1038/nature23455; Finn C, 2017, PR MACH LEARN RES, V70; FUKUSHIMA K, 1982, PATTERN RECOGN, V15, P455, DOI 10.1016/0031-3203(82)90024-3; Fullmer B., 1992, Toward a Practice of Autonomous Systems. Proceedings of the First European Conference on Artificial Life, P255; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gal Y., 2016, THESIS, V1, P3; Goth A, 2001, BEHAVIOUR, V138, P117, DOI 10.1163/156853901750077826; Graves A., 2014, ARXIV14105401; Graves A., 2011, ADV NEURAL INFORM PR, P2348, DOI DOI 10.5555/2986459.2986721; Graves Alex, 2016, ARXIV160308983; Gruau F., 1996, Genetic Programming. Proceedings of the First Annual Conference 1996, P81; Grunwald P. D., 2007, MINIMUM DESCRIPTION; Guo YW, 2016, ADV NEUR IN, V29; Ha D., 2018, ARXIV181003779; Ha D, 2018, ADV NEUR IN, V31; Hanson S. J., 1990, ADV NEURAL INFORMATI, P533; HANSON SJ, 1990, PHYSICA D, V42, P265, DOI 10.1016/0167-2789(90)90081-Y; Harp S., 1990, ADV NEURAL INFORMATI, V2, P447; He K, 2016, ADV NEUR IN, V29; He Y, 2010, CURR OPIN NEUROL, V23, P341, DOI 10.1097/WCO.0b013e32833aa567; Hinton G. E., 1993, Proceeding of the Sixth Annual ACM Conference on Computational Learning Theory, P5, DOI 10.1145/168304.168306; Hinton G.E., 1996, ADAPTIVE INDIVIDUALS, P447; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Huang XW, 2015, ACTA POLYM SIN, P1133; HUTTENLOCHER PR, 1990, NEUROPSYCHOLOGIA, V28, P517, DOI 10.1016/0028-3932(90)90031-I; Jaeger H, 2004, SCIENCE, V304, P78, DOI 10.1126/science.1091277; Jang E., 2017, ICLR; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kingma D.P, P 3 INT C LEARNING R; Kleim JA, 2002, NEUROBIOL LEARN MEM, V77, P63, DOI 10.1006/nlme.2000.4004; Kolmogorov A. N., 1968, International Journal of Computer Mathematics, V2, P157, DOI 10.1080/00207166808803030; Konak A, 2006, RELIAB ENG SYST SAFE, V91, P992, DOI 10.1016/j.ress.2005.11.018; Krishnan R., 1994, DELTA GANN NEW APPRO; Krueger D., 2017, ARXIV PREPRINT ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 1998, CONVOLUTIONAL NETWOR, V3361, P255, DOI DOI 10.1109/IJCNN.2004.1381049; Lee CH, 1996, IEEE C EVOL COMPUTAT, P665, DOI 10.1109/ICEC.1996.542680; Lee Namhoon, 2019, INT C LEARN REPR; Lehman J., 2008, ALIFE, P329; Li Chunyuan, 2018, INT C LEARN REPR ICL; Li H., 2017, P INT C LEARN REPR I, P1; Li Liam, 2019, UAI; Liu Hanxiao, 2018, ICLR; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu Z, 2019, PATTERN ANAL APPL, V22, P1527, DOI 10.1007/s10044-019-00792-5; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Mallya A, 2018, LECT NOTES COMPUT SC, V11208, P72, DOI 10.1007/978-3-030-01225-0_5; Mandischer M., 1993, Artificial Neural Nets and Genetic Algorithms. Proceedings of the International Conference, P643; MANIEZZO V, 1994, IEEE T NEURAL NETWOR, V5, P39, DOI 10.1109/72.265959; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; MILES DB, 1995, OECOLOGIA, V103, P261, DOI 10.1007/BF00329089; Miller B. L., 1995, Complex Systems, V9, P193; Molchanov P., 2017, P INT C LEARN REPR I, P1; Mori A, 2000, J COMP PSYCHOL, V114, P408, DOI 10.1037/0735-7036.114.4.408; Mouret JB, 2011, STUD COMPUT INTELL, V341, P139; Neal RM, 1996, LECT NOTES STAT, V118; Neklyudov K., 2019, INT C LEARN REPR ICL; NOWLAN SJ, 1992, NEURAL COMPUT, V4, P473, DOI 10.1162/neco.1992.4.4.473; Opitz DW, 1997, J ARTIF INTELL RES, V6, P177, DOI 10.1613/jair.368; Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271; Parisi G. I., 2018, ARXIV180207569; Pathak Deepak, 2017, P IEEE C COMP VIS PA, P16; Pham H, 2018, PR MACH LEARN RES, V80; Pujol JCF, 1998, APPL INTELL, V8, P73, DOI 10.1023/A:1008272615525; Raiko T., 2009, NEUROCOMPUTING; Real E, 2017, PR MACH LEARN RES, V70; Real Esteban, 2019, P AAAI C ART INT AAA, DOI DOI 10.1609/AAAI.V33I01.33014780; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; RISSANEN J, 1978, AUTOMATICA, V14, P465, DOI 10.1016/0005-1098(78)90005-5; Rissanen J., 2007, INFORM COMPLEXITY ST; Roli A., 2014, INTRO RESERVOIR COMP; Sabour Sara, 2017, PROC 31 INT C NEURAL; Schmichuber J, 2007, NEURAL COMPUT, V19, P757, DOI 10.1162/neco.2007.19.3.757; Schmidhuber J, 1997, NEURAL NETWORKS, V10, P857, DOI 10.1016/S0893-6080(96)00127-X; SCHMIDHUBER J, 1991, IEEE IJCNN, P1458, DOI 10.1109/IJCNN.1991.170605; Seung H. S., 2012, CONNECTOME BRAINS WI; SMITH JM, 1987, NATURE, V329, P761, DOI 10.1038/329761a0; So DR, 2019, PR MACH LEARN RES, V97; SOLOMONOFF RJ, 1964, INFORM CONTROL, V7, P1, DOI 10.1016/S0019-9958(64)90223-2; Sporns O, 2005, PLOS COMPUT BIOL, V1, P245, DOI 10.1371/journal.pcbi.0010042; Stanley KO, 2002, EVOL COMPUT, V10, P99, DOI 10.1162/106365602320169811; Starck J. Matthias, 1998, Oxford Ornithology Series, V8, P3; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Takemura S, 2017, ELIFE, V6, DOI 10.7554/eLife.26975; Tedrake R., 2009, UNDERACTUATED ROBOTI, V3; Tierney Adrienne L, 2009, Zero Three, V30, P9; Trask A, 2018, ADV NEUR IN, V31; Turing A.M, 1948, INTELLIGENT MACHINER; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; van den Heuvel MP, 2011, J NEUROSCI, V31, P15775, DOI 10.1523/JNEUROSCI.3539-11.2011; Varshney LR, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1001066; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; WHITE JG, 1986, PHILOS T R SOC B, V314, P1, DOI 10.1098/rstb.1986.0056; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yao X, 1998, APPL MATH COMPUT, V91, P83, DOI 10.1016/S0096-3003(97)10005-4; Yu Kaicheng, 2019, INT C LEARN REPR, P2; Zador AM, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-11786-6; Zoph B., 2017, P1; Zuo Xingdong, 2018, PYTORCH IMPLEMENTATI	124	63	65	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													15	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305037
C	Li, H; De, S; Xu, Z; Studer, C; Samet, H; Goldstein, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Hao; De, Soham; Xu, Zheng; Studer, Christoph; Samet, Hanan; Goldstein, Tom			Training Quantized Nets: A Deeper Understanding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Currently, deep neural networks are deployed on low-power portable devices by first training a full-precision model using powerful hardware, and then deriving a corresponding low-precision model for efficient inference on such systems. However, training models directly with coarsely quantized weights is a key step towards learning on embedded platforms that have limited computing resources, memory capacity, and power consumption. Numerous recent publications have studied methods for training quantized networks, but these studies have mostly been empirical. In this work, we investigate training methods for quantized neural networks from a theoretical viewpoint. We first explore accuracy guarantees for training methods under convexity assumptions. We then look at the behavior of these algorithms for non-convex problems, and show that training algorithms that exploit high-precision representations have an important greedy search phase that purely quantized training methods lack, which explains the difficulty of training using low-precision arithmetic.	[Li, Hao; De, Soham; Xu, Zheng; Samet, Hanan; Goldstein, Tom] Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA; [Studer, Christoph] Cornell Univ, Sch Elect & Comp Engn, Ithaca, NY 14853 USA	University System of Maryland; University of Maryland College Park; Cornell University	De, S (corresponding author), Univ Maryland, Dept Comp Sci, College Pk, MD 20742 USA.	haoli@cs.umd.edu; sohamde@cs.umd.edu; xuzh@cs.umd.edu; studer@cornell.edu; hjs@cs.umd.edu; tomg@cs.umd.edu	Jeong, Yongwook/N-7413-2016; Li, Hao/X-1188-2019		US National Science Foundation (NSF) [CCF-1535902]; US Office of Naval Research [N00014-17-1-2078]; Sloan Foundation; Xilinx, Inc.; US NSF [ECCS-1408006, CCF-1535897, CAREER CCF-1652065, IIS-13-20791]	US National Science Foundation (NSF)(National Science Foundation (NSF)); US Office of Naval Research(Office of Naval Research); Sloan Foundation(Alfred P. Sloan Foundation); Xilinx, Inc.; US NSF(National Science Foundation (NSF))	T. Goldstein was supported in part by the US National Science Foundation (NSF) under grant CCF-1535902, by the US Office of Naval Research under grant N00014-17-1-2078, and by the Sloan Foundation. C. Studer was supported in part by Xilinx, Inc. and by the US NSF under grants ECCS-1408006, CCF-1535897, and CAREER CCF-1652065. H. Samet was supported in part by the US NSF under grant IIS-13-20791.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, QUANTIZED NEURAL NET; Anwar S, 2015, INT CONF ACOUST SPEE, P1131, DOI 10.1109/ICASSP.2015.7178146; Baldassi C, 2015, PHYS REV LETT, V115, DOI 10.1103/PhysRevLett.115.128101; Cheng Zhiyong, 2015, ARXIV150303562; Collobert R., 2011, NIPS; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; De S., 2016, ARXIV PREPRINT ARXIV; Goyal P., 2017, LARGE MINIBATCH SGD; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; HOHFELD M, 1992, NEUROCOMPUTING, V4, P291, DOI 10.1016/0925-2312(92)90014-G; Hwang K, 2014, IEEE WRK SIG PRO SYS, P174; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lan GH, 2012, MATH PROGRAM, V134, P425, DOI 10.1007/s10107-011-0442-6; Lax PD, 2007, LINEAR ALGEBRA ITS A; Levin D.A., 2009, MARKOV CHAINS MIXING; Li F., 2016, ARXIV160504711V1; Lin DD, 2016, PR MACH LEARN RES, V48; Lin Z., 2016, ARXIV151003009; MARCHESI M, 1993, IEEE T NEURAL NETWOR, V4, P53, DOI 10.1109/72.182695; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Min Ju Kim, 2015, 2015 11th Conference on Lasers and Electro-Optics Pacific Rim (CLEO-PR). Proceedings, P1, DOI 10.1109/CLEOPR.2015.7376173; Miyashita Daisuke, 2016, ARXIV160301025; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Soudry D., 2014, NIPS; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhou A, 2017, INCREMENTAL NETWORK; Zhou Shuchang, 2016, P IEEE C COMP VIS PA; Zhu Chenzhuo, 2017, ICLR	34	63	64	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405086
C	Yao, SR; Huang, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yao, Sirui; Huang, Bert			Beyond Parity: Fairness Objectives for Collaborative Filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study fairness in collaborative-filtering recommender systems, which are sensitive to discrimination that exists in historical data. Biased data can lead collaborative-filtering methods to make unfair predictions for users from minority groups. We identify the insufficiency of existing fairness metrics and propose four new metrics that address different forms of unfairness. These fairness metrics can be optimized by adding fairness terms to the learning objective. Experiments on synthetic and real data show that our new metrics can better measure fairness than the baseline, and that the fairness objectives effectively help reduce unfairness.	[Yao, Sirui] Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA	Virginia Polytechnic Institute & State University	Yao, SR (corresponding author), Virginia Tech, Dept Comp Sci, Blacksburg, VA 24061 USA.	ysirui@vt.edu; bhuang@vt.edu	Jeong, Yongwook/N-7413-2016; Huang, Bert/E-2576-2016	Huang, Bert/0000-0002-8548-7246				Beede DN., 2011, WOMEN STEM GENDER GA, DOI DOI 10.2139/SSRN.1964782; Broad S., 2014, ASS SUPPORTING COMPU, P29; Chausson O., 2010, WHO WATCHES WHAT ASS; Dascalu MI, 2016, BEHAV INFORM TECHNOL, V35, P290, DOI 10.1080/0144929X.2015.1128977; DAYMONT TN, 1984, J HUM RESOUR, V19, P408, DOI 10.2307/145880; Ekstrand Michael D., 2010, Foundations and Trends in Human-Computer Interaction, V4, P81, DOI 10.1561/1100000009; Griffith AL, 2010, ECON EDUC REV, V29, P911, DOI 10.1016/j.econedurev.2010.06.010; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; HUBER PJ, 1964, ANN MATH STAT, V35, P73, DOI 10.1214/aoms/1177703732; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kamishima T, 2014, 8 ACM C REC SYST RES; Kamishima T, 2016, INT CONF DAT MIN WOR, P860, DOI [10.1109/ICDMW.2016.23, 10.1109/ICDMW.2016.0127]; Kamishima Toshihiro, 2012, P WORKSHOP HUMAN DEC, P8; Kamishima Toshihiro, 2013, DECISIONS RECSYS, P1; Kingma D.P, P 3 INT C LEARNING R; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Lum K., 2016, ARXIV161008077; Marlin B., 2012, ARXIV12065267; Marlin Benjamin M, 2009, RECSYS, P5, DOI DOI 10.1145/1639714.1639717; Nguyen TN, 2010, PROCEDIA COMPUT SCI, V1, P2811, DOI 10.1016/j.procs.2010.08.006; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Sacin C. V., 2009, ED DATA MINING 2009; Sahebi S., 2015, P 9 ACM C REC SYST, P131; Smith E, 2011, BRIT EDUC RES J, V37, P993, DOI 10.1080/01411926.2010.515019; Zafar M. B., 2017, ARXIV150705259; Zemel R., 2013, P INT C MACH LEARN, P325	29	63	65	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402095
C	Grefenstette, E; Hermann, KM; Suleyman, M; Blunsom, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Grefenstette, Edward; Hermann, Karl Moritz; Suleyman, Mustafa; Blunsom, Phil			Learning to Transduce with Unbounded Memory	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.	[Grefenstette, Edward; Hermann, Karl Moritz; Suleyman, Mustafa; Blunsom, Phil] Google DeepMind, London, England; [Blunsom, Phil] Univ Oxford, Oxford, England	Google Incorporated; University of Oxford	Grefenstette, E (corresponding author), Google DeepMind, London, England.	etg@google.com; kmh@google.com; mustafasul@google.com; pblunsom@google.com						Aho A.V., 1972, THEORY PARSING TRANS; Allauzen C, 2007, LECT NOTES COMPUT SC, V4783, P11; Bengio Y., 2014, ARXIV14061078; Das Sreerupa, 1993, ADV NEURAL INFORM PR; Das Sreerupa, 1992, P 14 ANN C COGN SCI; Dreyer M, 2008, P 2008 C EMPIRICAL M, P1080; Graves A., 2012, REPR LEARN WORKSH IC; Graves A., 2014, ARXIV14105401; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Joulin A, 2015, ADV NEUR IN, V28; Nair V, 2010, P 27 INT C MACHINE L, P807; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Sukhbaatar S., 2015, ABS150308895 CORR; Sun GZ, 1998, NEURAL NETWORK PUSHD; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wu D, 1997, COMPUT LINGUIST, V23, P377; Wu D., 1998, COLING ACL 98 36 ANN, P1408; Zaremba Wojciech, 2015, ARXIV150500521; Zhou ZH, 2002, ARTIF INTELL, V137, P239, DOI 10.1016/S0004-3702(02)00190-X	20	63	63	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100016
C	Rowley, HA; Baluja, S; Kanade, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Rowley, HA; Baluja, S; Kanade, T			Human face detection in visual scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,SCH COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	63	67	0	3	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						875	881						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00123
C	BOAHEN, KA; ANDREOU, AG		MOODY, JE; HANSON, SJ; LIPPMANN, RP		BOAHEN, KA; ANDREOU, AG			A CONTRAST SENSITIVE SILICON RETINA WITH RECIPROCAL SYNAPSES	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Andreou, Andreas G./A-3271-2010	Andreou, Andreas G./0000-0003-3826-600X					0	63	65	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						764	772						9	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00094
C	Dong, XY; Yang, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dong, Xuanyi; Yang, Yi			Network Pruning via Transformable Architecture Search	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Network pruning reduces the computation costs of an over-parameterized network without performance damage. Prevailing pruning algorithms pre-define the width and depth of the pruned networks, and then transfer parameters from the unpruned network to pruned networks. To break the structure limitation of the pruned networks, we propose to apply neural architecture search to search directly for a network with flexible channel and layer sizes. The number of the channels/layers is learned by minimizing the loss of the pruned networks. The feature map of the pruned network is an aggregation of K feature map fragments (generated by K networks of different sizes), which are sampled based on the probability distribution. The loss can be back-propagated not only to the network weights, but also to the parameterized distribution to explicitly tune the size of the channels/layers. Specifically, we apply channel-wise interpolation to keep the feature map with different channel sizes aligned in the aggregation procedure. The maximum probability for the size in each distribution serves as the width and depth of the pruned network, whose parameters are learned by knowledge transfer, e.g., knowledge distillation, from the original networks. Experiments on CIFAR-10, CIFAR-100 and ImageNet demonstrate the effectiveness of our new perspective of network pruning compared to traditional network pruning algorithms. Various searching and knowledge transfer approaches are conducted to show the effectiveness of the two components. Code is at: https://github.com/D-X-Y/NAS-Projects.	[Dong, Xuanyi; Yang, Yi] Univ Technol Sydney, ReLER, CAI, Sydney, NSW, Australia; [Dong, Xuanyi] Baidu Res, Sunnyvale, CA 94089 USA	University of Technology Sydney; Baidu	Dong, XY (corresponding author), Baidu Res, Sunnyvale, CA 94089 USA.	xuanyi.dong@student.uts.edu.au; yi.yang@uts.edu.au	yang, yang/HGT-7999-2022; Dong, Xuanyi/Q-5434-2019; Yang, Yi/B-9273-2017; yang, yang/GWB-9426-2022; yang, yang/GVT-5210-2022	Dong, Xuanyi/0000-0001-9272-1590; Yang, Yi/0000-0002-0512-880X; 				Alizadeh Milad, 2019, INT C LEARN REPR ICL; Alvarez Jose M, 2016, ADV NEURAL INFORM PR, P2270; Cai H, 2018, AAAI CONF ARTIF INTE, P2787; Cai Han, 2019, INT C LEARN REPR; Chen Tianqi, 2016, ICLR; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong XT, 2019, PROCEEDINGS OF 2019 IEEE 3RD INFORMATION TECHNOLOGY, NETWORKING, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (ITNEC 2019), P2176, DOI 10.1109/ITNEC.2019.8729549; Figurnov M, 2017, PROC CVPR IEEE, P1790, DOI 10.1109/CVPR.2017.194; Gordon A, 2018, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2018.00171; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; He Y, 2017, I IEEE EMBS C NEUR E, P138, DOI 10.1109/NER.2017.8008311; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; Hinton G., 2015, NIPS WORKSH, P1; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang XW, 2015, ACTA POLYM SIN, P1133; Hubara I, 2018, J MACH LEARN RES, V18; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jang E., 2017, ICLR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li H., 2017, P INT C LEARN REPR I, P1; Li K, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON COMPUTATIONAL INTELLIGENCE AND APPLICATIONS (ICCIA), P186, DOI 10.1109/ICCIA.2018.00042; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu Z., 2018, INT C LEARN REPR ICL; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Loshchilov I., 2017, P INT C LEARNING REP; Louizos Christos, 2018, INT C LEARN REPR ICL; Maddison Chris J, 2017, ICLR; Minnehan B, 2019, PROC CVPR IEEE, P10707, DOI 10.1109/CVPR.2019.01097; Molchanov P, 2019, PROC CVPR IEEE, P11256, DOI 10.1109/CVPR.2019.01152; Pham H, 2018, PR MACH LEARN RES, V80; Real Esteban, 2019, AAAI C ART INT AAAI; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tartaglione E, 2018, ADV NEUR IN, V31; Wen W, 2016, ADV NEUR IN, V29; Ye J., 2018, RETHINKING SMALLERNO; Yim J, 2017, PROC CVPR IEEE, P7130, DOI 10.1109/CVPR.2017.754; Yu J., 2019, AUTOSLIM ONE SHOT AR; Zagoruyko S., 2017, P INT C LEARN REPR, DOI DOI 10.1109/CVPR.2019.00271; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579; Zoph B., 2017, P1	48	62	63	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300069
C	Belbute-Peres, FD; Smith, KA; Allen, KR; Tenenbaum, JB; Kolter, JZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Belbute-Peres, Filipe de A.; Smith, Kevin A.; Allen, Kelsey R.; Tenenbaum, Joshua B.; Kolter, J. Zico			End-to-End Differentiable Physics for Learning and Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a differentiable physics engine that can be integrated as a module in deep neural networks for end-to-end learning. As a result, structured physics knowledge can be embedded into larger systems, allowing them, for example, to match observations by performing precise simulations, while achieves high sample efficiency. Specifically, in this paper we demonstrate how to perform backpropagation analytically through a physical simulator defined via a linear complementarity problem. Unlike traditional finite difference methods, such gradients can be computed analytically, which allows for greater flexibility of the engine. Through experiments in diverse domains, we highlight the system's ability to learn physical parameters from data, efficiently match and simulate observed visual behavior, and readily enable control via gradient-based planning methods. Code for the engine and experiments is included with the paper.(1)	[Belbute-Peres, Filipe de A.; Kolter, J. Zico] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Smith, Kevin A.; Allen, Kelsey R.; Tenenbaum, Joshua B.] MIT, Brain & Cognit Sci, Cambridge, MA 02139 USA; [Kolter, J. Zico] Bosch Ctr Artificial Intelligence, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Massachusetts Institute of Technology (MIT)	Belbute-Peres, FD (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	filiped@cs.cmu.edu; k2smith@mit.edu; krallen@mit.edu; jbt@mit.edu; zkolter@cs.cmu.edu						Abbeel P., 2005, NIPS, P1; Al-Rfou Rami, 2016, ARXIV E PRINTS; Amos Brandon, 2017, ARXIV170300443CSMATH; Anitescu M, 1997, NONLINEAR DYNAM, V14, P231, DOI 10.1023/A:1008292328909; [Anonymous], 2017, ARXIV170802596; Atkeson CG, 1997, IEEE INT CONF ROBOT, P3557, DOI 10.1109/ROBOT.1997.606886; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W., 2016, ARXIV161200222CS; Brockman G., 2016, OPENAI GYM; Catto Erin, 2010, GDC; Chang M.B., 2016, ARXIV161200341CS; Chun Kai Ling, 2018, ARXIV180502777; Cline Michael Bradley, 2002, THESIS; Cottle Richard W., 2008, ENCY OPTIMIZATION, P1873; Coumans E., 2013, OPEN SOURCE BULLETPH, V15, P5; Degrave Jonas, 2016, ARXIV161101652CS; Djolonga Josip, 2017, NEURAL INFORM PROCES; Featherstone R., 1984, ROBOT DYNAMICS ALGOR; Fragkiadaki Katerina, 2015, ARXIV151107404CS; Garstenauer Helmut, 2006, THESIS; Gregorius Dirk, 2015, GDC; Gregorius Dirk, 2013, GDC; Hamrick J.B., 2015, P 37 ANN C COGN SCI, P886; Heess N., 2015, ARXIV151009142CS; Hermans M, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0086696; Hesse C., 2017, OPENAI BASELINES; Kurutach T., 2018, INT C LEARN REPR; Lee Jeongseok, 2018, J OPEN SOURCE SOFTWA, V3, P500, DOI DOI 10.21105/JOSS.00500; Lerer Adam, 2016, ARXIV160301312CS; Ljung L., 1999, SYSTEM IDENTIFICATIO, V2nd; Mensch Arthur, 2018, ARXIV180203676; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Ranjan Anurag, 2016, ARXIV161100850CS; Smith KA, 2013, TOP COGN SCI, V5, P185, DOI 10.1111/tops.12009; SONG J, 2004, ICINCO, P2223; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van den Bergen G., 2004, M KAUFMANN SERIES IN; van Hasselt H, 2015, ARXIV150804582; WERBOS PJ, 1989, PROCEEDINGS OF THE 28TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-3, P260, DOI 10.1109/CDC.1989.70114	43	62	62	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001070
C	Conti, E; Madhavan, V; Such, FP; Lehman, J; Stanley, KO; Clune, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Conti, Edoardo; Madhavan, Vashisht; Such, Felipe Petroski; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff			Improving Exploration in Evolution Strategies for Deep Reinforcement Learning via a Population of Novelty-Seeking Agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Evolution strategies (ES) are a family of black-box optimization algorithms able to train deep neural networks roughly as well as Q-learning and policy gradient methods on challenging deep reinforcement learning (RL) problems, but are much faster (e.g. hours vs. days) because they parallelize better. However, many RL problems require directed exploration because they have reward functions that are sparse or deceptive (i.e. contain local optima), and it is unknown how to encourage such exploration with ES. Here we show that algorithms that have been invented to promote directed exploration in small-scale evolved neural networks via populations of exploring agents, specifically novelty search (NS) and quality diversity (QD) algorithms, can be hybridized with ES to improve its performance on sparse or deceptive deep RL tasks, while retaining scalability. Our experiments confirm that the resultant new algorithms, NS-ES and two QD algorithms, NSR-ES and NSRA-ES, avoid local optima encountered by ES to achieve higher performance on Atari and simulated robots learning to walk around a deceptive trap. This paper thus introduces a family of fast, scalable algorithms for reinforcement learning that are capable of directed exploration. It also adds this new family of exploration algorithms to the RL toolbox and raises the interesting possibility that analogous algorithms with multiple simultaneous paths of exploration might also combine well with existing RL algorithms outside ES.	[Conti, Edoardo; Madhavan, Vashisht; Such, Felipe Petroski; Lehman, Joel; Stanley, Kenneth O.; Clune, Jeff] Uber AI Labs, San Francisco, CA 94107 USA		Conti, E; Madhavan, V (corresponding author), Uber AI Labs, San Francisco, CA 94107 USA.	edoardo.conti@gmail.com; vashisht@uber.com	Lehman, Joel/AAH-9977-2019	Lehman, Joel/0000-0002-9535-1123				Bellemare M., 2016, NEURIPS; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Brockman G., 2016, OPENAI GYM; Cuccu G, 2011, LECT NOTES COMPUT SC, V6624, P234, DOI 10.1007/978-3-642-20525-5_24; Cuccu G, 2011, IEEE C EVOL COMPUTAT, P158; Cully A, 2015, NATURE, V521, P503, DOI 10.1038/nature14422; Cully A, 2013, GECCO'13: PROCEEDINGS OF THE 2013 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P175; Dauphin Yann, 2014, ABS14062572 ARXIV; Fortunato M., 2017, NOISY NETWORKS EXPLO; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Gomes Jorge, 2014, ARXIV14070577; Hansen N, 2003, EVOL COMPUT, V11, P1, DOI 10.1162/106365603321828970; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Huizinga J, 2016, GECCO'16: PROCEEDINGS OF THE 2016 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P125, DOI 10.1145/2908812.2908836; Jaderberg Max, 2017, ABS171109846 CORR; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lange S., 2010, PROC INT JOINT C NEU, P1; Lehman J, 2011, GENET EVOL COMPUT, P37; Lehman J, 2011, GECCO-2011: PROCEEDINGS OF THE 13TH ANNUAL GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P211; Lehman J, 2011, EVOL COMPUT, V19, P189, DOI 10.1162/EVCO_a_00025; Liepins Gunar E., 1990, CONF90071751 ORNL TE; Liu Y, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Meyerson Elliot, 2016, P GEN EV COMP C GECC; Miikkulainen R, 2019, ARTIFICIAL INTELLIGENCE IN THE AGE OF NEURAL NETWORKS AND BRAIN COMPUTING, P293, DOI 10.1016/B978-0-12-815480-9.00015-3; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mouret Jean-Baptiste, 2015, ARXIV; Naddaf Yavar, 2010, GAME INDEPENDENT AGE; Ostrovski G, 2017, PR MACH LEARN RES, V70; Oudeyer Pierre-Yves, 2007, Front Neurorobot, V1, P6, DOI 10.3389/neuro.12.006.2007; Paquette Phillip, 2016, OPENAI GYM; Pathak D., 2017, ARXIV170505363; Plappert Matthias, 2017, ARXIV170601905; Pugh JK, 2015, GECCO'15: PROCEEDINGS OF THE 2015 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P967, DOI 10.1145/2739480.2754664; Pugh Justin K, 2016, QUALITY DIVERSITY NE, V3; Rechenberg I, 1978, SIMULATIONSMETHODEN, P83; Rusu A. A., 2015, ARXIV151106295; Salimans T., 2017, ARXIV170303864; Salimans T, 2016, ADV NEUR IN, V29; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004; Stadie B. C., 2015, ARXIV150700814; Stanton Christopher, 2016, PLOS ONE; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang HL, 2017, INT CONF MEAS, P1, DOI [10.1109/ICMTMA.2017.0009, 10.1109/ICMTMA.2017.8]; van den Oord Aaron, 2016, ARXIV160605328; Velez R, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P737, DOI 10.1145/2576768.2598225; Velez Roby, 2017, ARXIV170507241; Wierstra D, 2008, IEEE C EVOL COMPUTAT, P3381, DOI 10.1109/CEC.2008.4631255; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	55	62	65	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305007
C	Norcliffe-Brown, W; Vafeias, E; Parisot, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Norcliffe-Brown, Will; Vafeias, Efstathios; Parisot, Sarah			Learning Conditioned Graph Structures for Interpretable Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Visual Question answering is a challenging problem requiring a combination of concepts from Computer Vision and Natural Language Processing. Most existing approaches use a two streams strategy, computing image and question features that are consequently merged using a variety of techniques. Nonetheless, very few rely on higher level image representations, which can capture semantic and spatial relationships. In this paper, we propose a novel graph-based approach for Visual Question Answering. Our method combines a graph learner module, which learns a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions. We test our approach on the VQA v2 dataset using a simple baseline architecture enhanced by the proposed graph learner module. We obtain promising results with 66.18% accuracy and demonstrate the interpretability of the proposed method. Code can be found at github.com/aimbrain/vqa-project.	[Norcliffe-Brown, Will; Vafeias, Efstathios; Parisot, Sarah] AimBrain Ltd, London, England		Norcliffe-Brown, W (corresponding author), AimBrain Ltd, London, England.	will.norcliffe@aimbrain.com; stathis@aimbrain.com; sarah@aimbrain.com						Anderson P., 2017, ABS170707998 CORR; [Anonymous], 2016, CORR; Ba J., 2017, P 3 INT C LEARN REPR; Boscaini Davide, 2016, P 30 INT C NEUR INF, P2; Cho K., 2014, P 2014 C EMP METH NA, P1724; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Fukui Akira, 2016, ABS160601847 CORR; Goyal Y., 2017, CVPR; Ilievski I, 2017, ADV NEUR IN, V30; Kafle K, 2017, COMPUT VIS IMAGE UND, V163, P3, DOI 10.1016/j.cviu.2017.06.005; Li Y., 2015, CORR; Lu JS, 2016, ADV NEUR IN, V29; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Pennington J, 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/d14-1162, DOI 10.3115/V1/D14-1162, 10.3115/v1/D14-1162]; Schwartz I., 2017, ADV NEURAL INFORM PR, P3667; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Teney D., 2017, ARXIV170802711; Teney Damien, 2016, CORR, V3; Trott A., 2018, ICLR; Velickovic P., 2018, P 6 INT C LEARN REPR; Wu Q, 2017, COMPUT VIS IMAGE UND, V163, P21, DOI 10.1016/j.cviu.2017.05.001; Xu D., 2017, CVPR, V2; Zhang Y., 2018, ICLR	23	62	65	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002084
C	Wang, HY; Sievert, S; Charles, Z; Liu, SC; Wright, S; Papailiopoulos, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Hongyi; Sievert, Scott; Charles, Zachary; Liu, Shengchao; Wright, Stephen; Papailiopoulos, Dimitris			ATOMO: Communication-efficient Learning via Atomic Sparsification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes. To mitigate these overheads, several studies propose the use of sparsified stochastic gradients. We argue that these are facets of a general sparsification method that can operate on any possible atomic decomposition. Notable examples include element-wise, singular value, and Fourier decompositions. We present ATOMO, a general framework for atomic sparsification of stochastic gradients. Given a gradient, an atomic decomposition, and a sparsity budget, ATOMO gives a random unbiased sparsification of the atoms minimizing variance. We show that recent methods such as QSGD and TernGrad are special cases of ATOMO and that sparsifiying the singular value decomposition of neural networks gradients, rather than their coordinates, can lead to significantly faster distributed training.	[Wang, Hongyi; Liu, Shengchao; Wright, Stephen] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA; [Sievert, Scott; Charles, Zachary; Papailiopoulos, Dimitris] Univ Wisconsin, Dept Elect & Comp Engn, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison; University of Wisconsin System; University of Wisconsin Madison	Wang, HY (corresponding author), Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA.			Sievert, Scott/0000-0002-4275-3452				Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Aji A. F., 2017, P 2017 C EMP METH NA, P440; ALEXANDER ST, 1987, IEEE T ACOUST SPEECH, V35, P1250, DOI 10.1109/TASSP.1987.1165279; Alistarh D, 2017, ADV NEUR IN, V30; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2018, ARXIV180208021; [Anonymous], 2016, ABS161107555 CORR; [Anonymous], 2015, ARXIV151201274; Bermudez JCM, 1996, IEEE T SIGNAL PROCES, V44, P1175, DOI 10.1109/78.502330; Bernstein J., 2018, ARXIV180204434; Bubeck S., 2015, FDN TRENDS MACHINE L; Chen C.-Y., 2017, ARXIV171202679; Cotter A., 2011, P NEURIPS GRAN SPAIN; Dalcin LD, 2011, ADV WATER RESOUR, V34, P1124, DOI 10.1016/j.advwatres.2011.04.013; De S., 2016, ARXIV PREPRINT ARXIV; De Sa C., 2015, NIPS, P2674; De Sa C., 2018, ARXIV180303383; De Sa C, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P561, DOI 10.1145/3079856.3080248; De Sa C, 2015, PR MACH LEARN RES, V37, P2332; Dean J., 2012, NIPS 12, V1, P1223; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; GITLIN RD, 1973, IEEE T CIRCUITS SYST, VCT20, P125, DOI 10.1109/TCT.1973.1083627; Grubic Demjan, 2018, SYNCHRONOUS MULTIGPU; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jaderberg M., 2014, ARXIV14053866; Jia Y., P ACM MULT, P675; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Konecny J., 2016, ARXIV161005492; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Leblond R., 2016, ARXIV160604809; Li JL, 2017, INT CONF MACH LEARN, P35; Lin Y., 2017, ARXIV171201887; Mania H., 2015, ARXIV150706970; McMahan H. Brendan, 2016, ARXIV160205629; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Qi H., 2016, P INT C LEARN REPR I; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Reddi SJ, 2016, PR MACH LEARN RES, V48; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Seide F, 2014, INTERSPEECH, P1058; Strom N., 2015, 16 ANN C INT SPEECH; Suresh Ananda Theertha, 2016, ARXIV161100429; Tsuzuku Y., 2018, ARXIV180206058; Wangni J., 2017, ARXIV171009854; Wen W., 2017, NIPS 17, V30, P1509; Wiesler Simon, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P180, DOI 10.1109/ICASSP.2014.6853582; Xue J, 2013, INTERSPEECH, P2364; Yin D., 2018, P 21 INT C ART INT S, P1998; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	53	62	65	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004041
C	Wang, SQ; Pei, KX; Whitehouse, J; Yang, JF; Jana, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Shiqi; Pei, Kexin; Whitehouse, Justin; Yang, Junfeng; Jana, Suman			Efficient Formal Safety Analysis of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural networks are increasingly deployed in real-world safety-critical domains such as autonomous driving, aircraft collision avoidance, and malware detection. However, these networks have been shown to often mispredict on inputs with minor adversarial or even accidental perturbations. Consequences of such errors can be disastrous and even potentially fatal as shown by the recent Tesla autopilot crashes. Thus, there is an urgent need for formal analysis systems that can rigorously check neural networks for violations of different safety properties such as robustness against adversarial perturbations within a certain L-norm of a given image. An effective safety analysis system for a neural network must be able to either ensure that a safety property is satisfied by the network or find a counterexample, i.e., an input for which the network will violate the property. Unfortunately, most existing techniques for performing such analysis struggle to scale beyond very small networks and the ones that can scale to larger networks suffer from high false positives and cannot produce concrete counterexamples in case of a property violation. In this paper, we present a new efficient approach for rigorously checking different safety properties of neural networks that significantly outperforms existing approaches by multiple orders of magnitude. Our approach can check different safety properties and find concrete counterexamples for networks that are 10x larger than the ones supported by existing analysis techniques. We believe that our approach to estimating tight output bounds of a network for a given input range can also help improve the explainability of neural networks and guide the training process of more robust neural networks.	[Wang, Shiqi; Pei, Kexin; Whitehouse, Justin; Yang, Junfeng; Jana, Suman] Columbia Univ, New York, NY 10027 USA	Columbia University	Wang, SQ (corresponding author), Columbia Univ, New York, NY 10027 USA.	tcwangshiqi@cs.columbia.edu; kpei@cs.columbia.edu; jaw2228@cs.columbia.edu; junfeng@cs.columbia.edu; suman@cs.columbia.edu	Wang, Shiqi/AAR-5013-2020	Wang, Shiqi/0000-0002-6338-1432	NSF [CNS-16-17670, CNS-15-63843, CNS-15-64055]; ONR [N00014-17-1-2010, N00014-16-1-2263, N00014-17-1-2788]; Google Faculty Fellowship	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Google Faculty Fellowship(Google Incorporated)	We thank the anonymous reviewers for their constructive and valuable feedback. This work is sponsored in part by NSF grants CNS-16-17670, CNS-15-63843, and CNS-15-64055; ONR grants N00014-17-1-2010, N00014-16-1-2263, and N00014-17-1-2788; and a Google Faculty Fellowship. Any opinions, findings, conclusions, or recommendations expressed herein are those of the authors, and do not necessarily reflect those of the US Government, ONR, or NSF.	[Anonymous], 2018, INT C LEARN REPR; Arp D, 2014, 21ST ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2014), DOI 10.14722/ndss.2014.23247; Bojarski M., 2017, IEEE INT VEH S GOLD; Dvijotham K, 2018, ARXIV180510265; Dvijotham K., 2018, C UNC ART INT; Ehlers R., 2017, 15 INT S AUT TECHN V; Eldan R, 2011, DISCRETE COMPUT GEOM, V46, P29, DOI 10.1007/s00454-011-9328-x; Fischetti Matteo, 2017, ARXIV171206174; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Goodfellow I. J., 2015, P ICLR; Hoffmann J, 2013, P 28 ANN ACM S APPL, P1808, DOI DOI 10.1145/2480362.2480701; Huang XW, 2017, LECT NOTES COMPUT SC, V10426, P3, DOI 10.1007/978-3-319-63387-9_1; Jana Suman, 2017, ARXIV PREPRINT ARXIV; Julian K. D., 2016, DAS 16; Justin W., 2018, 27 USENIX SEC S; Katz G., 2017, INT C COMP AID VER; Katz G., 2017, 1 WORKSH FORM VER AU; KOCHENDERFER MJ, 2012, TECHNICAL REPORT; Koh P. W., 2017, INT C MACH LEARN; Lecuyer M., 2018, ARXIV180203471; Li Jiwei, 2016, ARXIV161208220; M. T. Notes, 2015, AIRB COLL AV SYST X; Mirman M, 2018, PR MACH LEARN RES, V80; Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924; Pascanu R., 2013, NUMBER RESPONSE REGI; Peck J, 2017, ADV NEURAL INFORM PR, P804; Pei KX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P1, DOI 10.1145/3132747.3132785; Pulina L, 2010, LECT NOTES COMPUT SC, V6174, P243, DOI 10.1007/978-3-642-14295-6_24; RAMON MJC, 2009, INTRO INTERVAL ANAL; Shrikumar A., 2017, INT C MACH LEARN; Szegedy C., 2013, INT C LEARN REPR; Tian Y., 2018, 40 INT C SOFTW ENG; Tjeng V., 2017, INT C LEARN REPR; Weng T.-W., 2018, INT C LEARN REPR; Wong E., 2018, INT C MACH LEARN; Wong E., 2018, ADV NEURAL INFORM PR; Zhang H., 2018, ADV NEURAL INFORM PR; Zhang Huan, 2018, ARXIV180409699	40	62	65	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000083
C	Long, MS; Cao, ZJ; Wang, JM; Yu, PS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Long, Mingsheng; Cao, Zhangjie; Wang, Jianmin; Yu, Philip S.			Learning Multiple Tasks with Multilinear Relationship Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep networks trained on large-scale data can learn transferable features to promote learning multiple tasks. Since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the task relatedness underlying parameter tensors and improve feature transferability in the multiple task-specific layers. This paper presents Multilinear Relationship Networks (MRN) that discover the task relationships based on novel tensor normal priors over parameter tensors of multiple task-specific layers in deep convolutional networks. By jointly learning transferable features and multilinear relationships of tasks and features, MRN is able to alleviate the dilemma of negative-transfer in the feature layers and under-transfer in the classifier layer. Experiments show that MRN yields state-of-the-art results on three multi-task learning datasets.	[Long, Mingsheng; Cao, Zhangjie; Wang, Jianmin; Yu, Philip S.] Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China	Tsinghua University	Long, MS (corresponding author), Tsinghua Univ, Sch Software, Beijing 100084, Peoples R China.	mingsheng@tsinghua.edu.cn; caozhangjie14@gmail.com; jimwang@tsinghua.edu.cn; psyu@uic.edu	Jeong, Yongwook/N-7413-2016; wang, jian/GVS-0711-2022		National Key R&D Program of China [2016YFB1000701]; National Natural Science Foundation of China [61772299, 61325008, 61502265, 61672313]; TNList Fund	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); TNList Fund	This work was supported by the National Key R&D Program of China (2016YFB1000701), National Natural Science Foundation of China (61772299, 61325008, 61502265, 61672313) and TNList Fund.	Ando RK, 2005, J MACH LEARN RES, V6, P1817; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chen J., 2011, KDD; Chen JH, 2013, IEEE T PATTERN ANAL, V35, P1025, DOI 10.1109/TPAMI.2012.189; Chu X, 2015, IEEE I CONF COMP VIS, P3352, DOI 10.1109/ICCV.2015.383; Ciliberto C., 2015, ICML; Donahue J, 2014, PR MACH LEARN RES, V32; Evgeniou T., 2004, P 10 ACM SIGKDD INT, P109; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gupta A. K., 1999, MATRIX VARIATE DISTR; Jacob L., 2009, P 21TH NIPS, P745; Kang Z., 2011, P INT C MACH LEARN, V2, P4; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar A., 2012, INT C MACH LEARN; Long MS, 2015, PR MACH LEARN RES, V37, P97; Maurer A, 2016, J MACH LEARN RES, V17; Misra I, 2016, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2016.320; Ohlson M, 2013, J MULTIVARIATE ANAL, V113, P37, DOI 10.1016/j.jmva.2011.05.015; Ouyang WL, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.299; Romera- Paredes B., 2013, ICML; Srivastava Nitish, 2013, NIPS; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Yang Yongxin, 2017, P INT C MACH REPR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Y., 2010, NIPS; Zhang Y, 2010, PROCEEDINGS OF THE ASME 29TH INTERNATIONAL CONFERENCE ON OCEAN, OFFSHORE AND ARCTIC ENGINEERING, 2010, VOL 6, P733; Zhang Yu, 2017, ARXIV170708114, DOI DOI 10.1109/TKDE.2021.3070203; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7	32	62	63	1	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401061
C	Tucker, G; Mnih, A; Maddison, CJ; Lawson, D; Sohl-Dickstein, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Tucker, George; Mnih, Andriy; Maddison, Chris J.; Lawson, Dieterich; Sohl-Dickstein, Jascha			REBAR: Low-variance, unbiased gradient estimates for discrete latent variable models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VARIATIONAL INFERENCE	Learning in models with discrete latent variables is challenging due to high variance gradient estimators. Generally, approaches have relied on control variates to reduce the variance of the REINFORCE estimator. Recent work (Jang et al., 2016; Maddison et al., 2016) has taken a different approach, introducing a continuous relaxation of discrete variables to produce low-variance, but biased, gradient estimates. In this work, we combine the two approaches through a novel control variate that produces low-variance, unbiased gradient estimates. Then, we introduce a modification to the continuous relaxation and show that the tightness of the relaxation can be adapted online, removing it as a hyperparameter. We show state-of-the-art variance reduction on several benchmark generative modeling tasks, generally leading to faster convergence to a better final log-likelihood.	[Tucker, George; Lawson, Dieterich; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA 94040 USA; [Mnih, Andriy; Maddison, Chris J.] DeepMind, London, England; [Maddison, Chris J.] Univ Oxford, Oxford, England	Google Incorporated; University of Oxford	Tucker, G (corresponding author), Google Brain, Mountain View, CA 94040 USA.	gjt@google.com; amnih@google.com; cmaddis@stats.ox.ac.uk; dieterichl@google.com; jaschasd@google.com	Jeong, Yongwook/N-7413-2016					Burda Yuri, 2015, ARXIV150900519; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Goumas Georgios, 2008, ARXIV151105176, P283; Jang E., 2016, ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Maddison CJ, 2014, ADV NEUR IN, V27; Maddison Chris J, 2016, ARXIV161100712; Mnih A, 2016, PR MACH LEARN RES, V48; Mnih A, 2014, PR MACH LEARN RES, V32, P1791; Mnih V, 2014, ADV NEUR IN, V27; Paisley J., 2012, ARXIV12066430; Raiko Tapani, 2014, ARXIV14062989; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Roeder Geoffrey, 2017, ARXIV170309194; Ruiz F.J.R., 2016, UNCERTAINTY ARTIFICI, P647; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Titsias MK, 2015, ADV NEUR IN, V28; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Wingate D., 2013, ARXIV13011299; Zaremba Wojciech, 2015, ARXIV150500521	24	62	62	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402066
C	Kulkarni, TD; Narasimhan, KR; Saeedi, A; Tenenbaum, JB		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kulkarni, Tejas D.; Narasimhan, Karthik R.; Saeedi, Ardavan; Tenenbaum, Joshua B.			Hierarchical Deep Reinforcement Learning: Integrating Temporal Abstraction and Intrinsic Motivation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Learning goal-directed behavior in environments with sparse feedback is a major challenge for reinforcement learning algorithms. One of the key difficulties is insufficient exploration, resulting in an agent being unable to learn robust policies. Intrinsically motivated agents can explore new behavior for their own sake rather than to directly solve external goals. Such intrinsic behaviors could eventually help the agent solve tasks posed by the environment. We present hierarchical-DQN (h-DQN), a framework to integrate hierarchical action-value functions, operating at different temporal scales, with goal-driven intrinsically motivated deep reinforcement learning. A top-level q-value function learns a policy over intrinsic goals, while a lower-level function learns a policy over atomic actions to satisfy the given goals. h-DQN allows for flexible goal specifications, such as functions over entities and relations. This provides an efficient space for exploration in complicated environments. We demonstrate the strength of our approach on two problems with very sparse and delayed feedback: (1) a complex discrete stochastic decision process with stochastic transitions, and (2) the classic ATARI game - 'Montezuma's Revenge'.	[Kulkarni, Tejas D.] DeepMind, London, England; [Narasimhan, Karthik R.; Saeedi, Ardavan] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Tenenbaum, Joshua B.] MIT, BCS, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Kulkarni, Tejas D.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Kulkarni, TD (corresponding author), DeepMind, London, England.; Kulkarni, TD (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	tejasdkulkarni@gmail.com; karthikn@mit.edu; ardavans@mit.edu; jbt@mit.edu						[Anonymous], 2005, IDENTIFYING USEFUL S, DOI [DOI 10.1145/1102351.1102454, 10.1145/1102351.1102454]; [Anonymous], 2003, FLORIDA AI RES SOC C; [Anonymous], 2014, ADV NEURAL INFORM PR; [Anonymous], 2003, IJCAI; [Anonymous], 2020, REINFORCEMENT LEARNI; Baranes A, 2013, ROBOT AUTON SYST, V61, P49, DOI 10.1016/j.robot.2012.05.008; Barto AG, 2003, DISCRETE EVENT DYN S, V13, P343; Bellemare M.G., 2012, J ARTIFICIAL INTELLI; Chentanez N., 2004, ADV NEURAL INF PROCE, V17, P1281, DOI DOI 10.1109/TAMD.2010.2051031; Cobo L. C., 2013, P 2013 INT C AUT AG, P1061; DAYAN P, 1993, NEURAL COMPUT, V5, P613, DOI 10.1162/neco.1993.5.4.613; Dietterich TG, 2000, J ARTIF INTELL RES, V13, P227, DOI 10.1613/jair.639; Diuk C., 2008, ICML, DOI [10.1145/1390156.1390187, DOI 10.1145/1390156.1390187]; Eslami S., 2016, ARXIV160308575; Frank M., 2015, INTRINSIC MOTIVATION, P245; Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000; Hernandez-Gardiol N, 2001, ADV NEUR IN, V13, P1047; Koutnik J, 2014, GECCO'14: PROCEEDINGS OF THE 2014 GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, P541, DOI 10.1145/2576768.2598358; Mnih V, 2016, ASYNCHRONOUS METHODS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mohamed S., 2015, ADV NEURAL INFORM PR, V28, P2116; Nair A., 2015, ARXIV150704296; Osband I., 2016, ARXIV160204621; Oudeyer P.-Y., 2009, FRONT NUEROROBOT, V1, P6, DOI DOI 10.3389/NEUR0.12.006.2007; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schmidhuber J, 2010, IEEE T AUTON MENT DE, V2, P230, DOI 10.1109/TAMD.2010.2056368; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Singh S., 2009, P ANN C COGNITIVE SC, P2601; Singh S, 2010, IEEE T AUTON MENT DE, V2, P70, DOI 10.1109/TAMD.2010.2051031; Sorg J., 2010, P 9 INT C AUT AG MUL, P31; Spelke ES, 2007, DEVELOPMENTAL SCI, V10, P89, DOI 10.1111/j.1467-7687.2007.00569.x; Stadie B. C., 2015, ARXIV150700814; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Yao H., 2014, ADV NEURAL INFORM PR, P990	35	62	62	4	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701090
C	Csato, L; Opper, M		Leen, TK; Dietterich, TG; Tresp, V		Csato, L; Opper, M			Sparse representation for Gaussian process models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We develop an approach for a sparse representation for Gaussian Process (GP) models in order to overcome the limitations of GPs caused by large data sets. The method is based on a combination of a Bayesian online algorithm together with a sequential construction of a relevant subsample of the data which fully specifies the prediction of the model. Experimental results on toy examples and large real-world datasets indicate the efficiency of the approach.	Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England	Aston University	Csato, L (corresponding author), Sch Engn & Appl Sci, Neural Comp Res Grp, Birmingham B4 7ET, W Midlands, England.	csatol@aston.ac.uk; opperm@aston.ac.uk		Csato, Lehel/0000-0003-1052-1849				[Anonymous], 1998, ON LINE LEARNING NEU; Bernardo J. M., 1994, BAYESIAN THEORY; Csato L, 2000, ADV NEUR IN, V12, P251; GIBBS M, 1999, EFFICIENT IMPLEMENTA; Lemm JC, 2000, PHYS REV LETT, V84, P2068, DOI 10.1103/PhysRevLett.84.2068; Platt J., ADV KERNEL METHODS S; Saad D., 1998, ON LINE LEARNING NEU; Scholkopf B, 1999, IEEE T NEURAL NETWOR, V10, P1000, DOI 10.1109/72.788641; SEEGER M, 2000, NIPS, V12; TIPPING M, 2000, NIPS, V12; TRECATE GF, 1999, NIPS, V11; TRESP V, IN PRESS NEURAL COMP; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Williams C., 1999, LEARNING GRAPHICAL M; WILLIAMS CKI, 1996, NIPS, V8	15	62	63	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						444	450						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800063
C	Cohen, WW; Schapire, RE; Singer, Y		Jordan, MI; Kearns, MJ; Solla, SA		Cohen, WW; Schapire, RE; Singer, Y			Learning to order things	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					There are many applications in which it is desirable to order rather than classify instances. Here we consider the problem of learning how to order, given feedback in the form of preference judgments, i.e., statements to the effect that one instance should be ranked ahead of another. We outline a two-stage approach in which one first learns by conventional means a preference-function, of the form PREF(u, v), which indicates whether it is advisable to rank u, before v. New instances are then ordered so as to maximize agreements with the learned preference function. We show that the problem of finding the ordering that agrees best with a preference function is NP-complete, even under very restrictive assumptions. Nevertheless, we describe a simple greedy algorithm that is guaranteed to find a good approximation. We then discuss an on-line learning algorithm, based on the "Hedge" algorithm, for finding a good linear combination of ranking "experts." We use the ordering algorithm combined with the on-line learning algorithm to find a combination of "search experts" each of which is a domain-specific query expansion strategy for a WWW search engine, and present experimental results that demonstrate the merits of our approach.	AT&T Bell Labs, Florham Park, NJ 07932 USA	AT&T	Cohen, WW (corresponding author), AT&T Bell Labs, 180 Pk Ave, Florham Park, NJ 07932 USA.								0	62	63	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						451	457						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700064
C	Amari, S		Mozer, MC; Jordan, MI; Petsche, T		Amari, S			Neural learning in structured parameter spaces - Natural Riemannian gradient	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The parameter space of neural networks has a Riemannian metric structure. The natural Riemannian gradient should be used instead of the conventional gradient, since the former denotes the true steepest descent direction of a loss function in the Riemannian space. The behavior of the stochastic gradient learning algorithm is much more effective if the natural gradient is used. The present paper studies the information-geometrical structure of perceptrons and other networks, and prove that the on-line learning method based on the natural gradient is asymptotically as efficient as the optimal batch algorithm. Adaptive modification of the learning constant is proposed and analyzed in terms of the Riemannian measure and is shown to be efficient. The natural gradient is finally applied to blind separation of mixtured independent signal sources.			Amari, S (corresponding author), RIKEN,FRONTIER RES PROGRAM,HIROSAWA 2-1,WAKO,SAITAMA 35101,JAPAN.								0	62	67	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						127	133						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00018
C	Wang, Y; Solomon, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Yue; Solomon, Justin			PRNet: Self-Supervised Learning for Partial-to-Partial Registration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a simple, flexible, and general framework titled Partial Registration Network (PRNet), for partial-to-partial point cloud registration. Inspired by recently-proposed learning-based methods for registration, we use deep networks to tackle non-convexity of the alignment and partial correspondence problems. While previous learning-based methods assume the entire shape is visible, PRNet is suitable for partial-to-partial registration, outperforming PointNetLK, DCP, and non-learning methods on synthetic data. PRNet is self-supervised, jointly learning an appropriate geometric representation, a keypoint detector that finds points in common between partial views, and keypoint-to-keypoint correspondences. We show PRNet predicts keypoints and correspondences consistently across views and objects. Furthermore, the learned representation is transferable to classification.	[Wang, Yue; Solomon, Justin] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Wang, Y (corresponding author), MIT, Cambridge, MA 02139 USA.	yuewangx@mit.edu; jsolomon@mit.edu			Army Research Office [W911NF1710068]; Air Force Office of Scientific Research [FA9550-19-1-031]; National Science Foundation [IIS-1838071]; Amazon Research Award; MIT-IBM Watson AI Laboratory; Toyota-CSAIL Joint Research Center; Skoltech-MIT Next Generation Program	Army Research Office; Air Force Office of Scientific Research(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); National Science Foundation(National Science Foundation (NSF)); Amazon Research Award; MIT-IBM Watson AI Laboratory(International Business Machines (IBM)); Toyota-CSAIL Joint Research Center; Skoltech-MIT Next Generation Program	The authors acknowledge the generous support of Army Research Office grant W911NF1710068, Air Force Office of Scientific Research award FA9550-19-1-031, of National Science Foundation grant IIS-1838071, from an Amazon Research Award, from the MIT-IBM Watson AI Laboratory, from the Toyota-CSAIL Joint Research Center, from a gift from Adobe Systems, and from the Skoltech-MIT Next Generation Program. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of these organizations. The authors also thank members of MIT Geometric Data Processing group for helpful discussion and feedback on the paper.	Agamennoni Gabriel, 2016, INT C INT ROB SYST I; Atzmon M, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201301; Ba LJ, 2015, 2015 IEEE International Conference on Applied Superconductivity and Electromagnetic Devices (ASEMD), P3, DOI 10.1109/ASEMD.2015.7453438; Bahdanau Dzmitry, 2017, 5 INT C LEARN REPR I; Bengio Yoshua, 2013, ESTIMATING PROPAGATI, P4; BESL PJ, 1992, IEEE T PATTERN ANAL, V14, P239, DOI 10.1109/34.121791; Bouaziz Sofien, 2013, P S GEOM PROC; Bronstein Alexander M, 2006, P NATL ACAD SCI US; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Castrejon L., 2016, COMPUTER VISION PATT; Chang A, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P53; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Duvenaud David K, 2015, P NIPS; Ephrat A, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201357; Esteves Carlos, 2017, EUR C COMP VIS ECCV; Ezuz D, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3202660; Ezuz Danielle, 2017, COMPUTER GRAPHICS FO; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; Fitzgibbon Andrew W., 2001, BRIT MACH VIS C BMVC; GOFORTH H, 2019, IEEE C COMP VIS PATT; Hahnel Dirk, 2002, P VDI C ROB ROB; Halimi Oshri, 2019, IEEE C COMP VIS PATT; He K. M, 2019, COMPUTER VISION PATT; Hinzmann Timo, 2016, INT S ROB RES; Huang Qi-Xing, 2008, P S GEOM PROC; Izatt G., 2017, P INT S ROB RES ISRR; Jang E., 2017, ICLR; Kanezaki A, 2018, PROC CVPR IEEE, P5010, DOI 10.1109/CVPR.2018.00526; Kim Vladimir G., 2011, ACM SIGGRAPH; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Lipman Yaron, 2009, ACM SIGGRAPH; Litany Or, 2017, INT C COMP VIS ICCV; Maron H, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925913; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mena G, 2018, INT C LEARN REPR ICL; Mnih V, 2016, PR MACH LEARN RES, V48; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Ovsjanikov M, 2012, ACM T GRAPHIC, V31, DOI 10.1145/2185520.2185526; Oweppns Andrew, 2018, EUR C COMP VIS ECCV; Pfau David, 2016, NEURIPS WORKSH ADV T; Pomerleau F., 2015, FDN TRENDS ROBOT, V4, P1, DOI DOI 10.1561/2300000035; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qi Charles R, 2017, ARXIV170602413; Rosen David M., 2016, WORKSH ALG FDN ROB W; Rusinkiewicz S, 2001, THIRD INTERNATIONAL CONFERENCE ON 3-D DIGITAL IMAGING AND MODELING, PROCEEDINGS, P145, DOI 10.1109/IM.2001.924423; Segal A., 2009, ROBOTICS SCI SYSTEMS; Solomon J, 2016, CYTOTHERAPY, V18, P1, DOI 10.1016/j.jcyt.2015.09.010; Solomon J, 2012, COMPUT GRAPH FORUM, V31, P1617, DOI 10.1111/j.1467-8659.2012.03167.x; Solomon Justin, 2013, P S GEOM PROC; Su H, 2018, PROC CVPR IEEE, P2530, DOI 10.1109/CVPR.2018.00268; Su H, 2015, IEEE I CONF COMP VIS, P2686, DOI 10.1109/ICCV.2015.308; Suwajanakorn S, 2018, ADV NEUR IN, V31; Tang CR, 2020, PHARMACOLOGY, V105, P339, DOI 10.1159/000503865; Thomas H, 2019, IEEE I CONF COMP VIS, P6420, DOI 10.1109/ICCV.2019.00651; Turk G., 1994, Computer Graphics Proceedings. Annual Conference Series 1994. SIGGRAPH 94 Conference Proceedings, P311, DOI 10.1145/192161.192241; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang YR, 2019, IEEE I CONF COMP VIS, P5016, DOI 10.1109/ICCV.2019.00512; Wang Yu, 2019, REPR LEARN GRAPHS MA; Wang Y, 2019, ACM T GRAPHIC, V38, DOI 10.1145/3326362; Yang H, 2019, ROBOTICS: SCIENCE AND SYSTEMS XV; Yang JL, 2016, IEEE T PATTERN ANAL, V38, P2241, DOI 10.1109/TPAMI.2015.2513405; Yi L, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3272127.3275027; Zaheer Manzil, 2017, ADV NEURAL INFORM PR, V2, P8; Zhang R, 2017, PROC CVPR IEEE, P645, DOI 10.1109/CVPR.2017.76; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhao HS, 2018, LECT NOTES COMPUT SC, V11207, P418, DOI 10.1007/978-3-030-01219-9_25; Zhou QY, 2016, LECT NOTES COMPUT SC, V9906, P766, DOI 10.1007/978-3-319-46475-6_47	71	61	63	4	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900041
C	Wong, E; Schmidt, FR; Metzen, JH; Kolter, JZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wong, Eric; Schmidt, Frank R.; Metzen, Jan Hendrik; Kolter, J. Zico			Scaling provable adversarial defenses	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent work has developed methods for learning deep network classifiers that are provably robust to norm-bounded adversarial perturbation; however, these methods are currently only possible for relatively small feedforward networks. In this paper, in an effort to scale these approaches to substantially larger models, we extend previous work in three main directions. First, we present a technique for extending these training procedures to much more general networks, with skip connections (such as ResNets) and general nonlinearities; the approach is fully modular, and can be implemented automatically (analogous to automatic differentiation). Second, in the specific case of l(infinity), adversarial perturbations and networks with ReLU nonlinearities, we adopt a nonlinear random projection for training, which scales linearly in the number of hidden units (previous approaches scaled quadratically). Third, we show how to further improve robust error through cascade models. On both MNIST and CIFAR data sets, we train classifiers that improve substantially on the state of the art in provable robust adversarial error bounds: from 5.8% to 3.1% on MNIST (with l(infinity) perturbations of epsilon = 0.1), and from 80% to 36.4% on CIFAR (with l(infinity) perturbations of epsilon = 2/255). Code for all experiments in the paper is available at https : //github. com/locuslab/ convex_adversarial/.	[Wong, Eric] Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA; [Schmidt, Frank R.; Metzen, Jan Hendrik] Bosch Ctr Artificial Intelligence, Renningen, Germany; [Kolter, J. Zico] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Kolter, J. Zico] Bosch Ctr Artificial Intelligence, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Wong, E (corresponding author), Carnegie Mellon Univ, Machine Learning Dept, Pittsburgh, PA 15213 USA.	ericwong@cs.cmu.edu; frank.r.schmidt@de.bosch.com; janhendrik.metzen@de.bosch.com; zkolter@cs.cmu.edu						[Anonymous], 2018, INT C LEARN REPR; Athalye A., 2018, P 35 INT C MACH LEAR; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; DVIJOTHAM K, 2018, ARXIV180306567; Ehlers Ruediger, 2017, INT S AUT TECHN VER; FENCHEL W, 1949, CAN J MATH, V1, P73, DOI 10.4153/CJM-1949-007-x; Gehr Timon, 2018, IEEE C SEC PRIV; Goodfellow I. J., 2015, P ICLR; Hein Matthias, 2017, ADV NEURAL INFORM PR; Huang Xiaowei, INT C COMP AID VER, P3; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kurakin Alexey, 2018, ARXIV180400097; Kurakin Alexey, 2017, INT C LEARNING REPRE; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li P, 2007, J MACH LEARN RES, V8, P2497; Lomuscio A., 2017, CORR; Madry A., 2018, ARXIV PREPRINT ARXIV; Metzen J H, 2017, INT C LEARN REPR; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Sinha A., 2018, ICLR; Tjeng V., 2017, ABS171107356 CORR; Vempala SS, 2005, RANDOM PROJECTION ME, V65; Wong Eric, 2017, ARXIV PREPRINT ARXIV; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]	25	61	62	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002090
C	Ba, LJ; Caruana, R		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Ba, Lei Jimmy; Caruana, Rich			Do Deep Nets Really Need to be Deep?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Currently, deep neural networks are the state of the art on problems such as speech recognition and computer vision. In this paper we empirically demonstrate that shallow feed-forward nets can learn the complex functions previously learned by deep nets and achieve accuracies previously only achievable with deep models. Moreover, in some cases the shallow nets can learn these deep functions using the same number of parameters as the original deep models. On the TIMIT phoneme recognition and CIFAR-10 image recognition tasks, shallow nets can be trained that perform similarly to complex, well-engineered, deeper convolutional models.	[Ba, Lei Jimmy] Univ Toronto, Toronto, ON, Canada; [Caruana, Rich] Microsoft Res, Redmond, WA USA	University of Toronto; Microsoft	Ba, LJ (corresponding author), Univ Toronto, Toronto, ON, Canada.	jimmy@psi.utoronto.ca; rcaruana@microsoft.com						Abdel-Hamid O, 2012, INT CONF ACOUST SPEE, P4277, DOI 10.1109/ICASSP.2012.6288864; Abdel-Hamid Ossama, 2013, INTERSPEECH, V2013; [Anonymous], 2013, ARXIV PREPRINT ARXIV; [Anonymous], 2013, ARXIV201313013557; Bucilua Cristian, 2006, P 12 ACM SIGKDD INT, P535, DOI [10.1145/1150402.1150464, DOI 10.1145/1150402.1150464]; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dauphin Yann N, 2013, ARXIV13013583; Erhan D, 2010, J MACH LEARN RES, V11, P625; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Krizhevsky Alex, 2009, LEARNING MULTIPLE LA, P6; LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546; Li Deng, 2013, ICASSP 2013; Mohamed AR, 2012, IEEE T AUDIO SPEECH, V20, P14, DOI 10.1109/TASL.2011.2109382; Nair V., 2010, ICML, P807; Razavian A. S., 2014, INTERSPEECH; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Xue J., 2013, P INTERSPEECH	21	61	61	6	18	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102045
C	Barber, D; Agakov, F		Thrun, S; Saul, K; Scholkopf, B		Barber, D; Agakov, F			The IM algorithm: A variational approach to information maximization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					The maximisation of information transmission over noisy channels is a common, albeit generally computationally difficult problem. We approach the difficulty of computing the mutual information for noisy channels by using a variational approximation. The resulting IM algorithm is analagous to the EM algorithm, yet maximises mutual information, as opposed to likelihood. We apply the method to several practical examples, including linear compression, population encoding and CDMA.	Univ Edinburgh, Inst Adapt & Neural Computac, Edinburgh EH1 2QL, Midlothian, Scotland	University of Edinburgh	Barber, D (corresponding author), Univ Edinburgh, Inst Adapt & Neural Computac, Edinburgh EH1 2QL, Midlothian, Scotland.							Barber D, 2001, NEU INF PRO, P197; Barlow HB, 1989, NEURAL COMPUT, V1, P295, DOI 10.1162/neco.1989.1.3.295; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Brunel N, 1998, NEURAL COMPUT, V10, P1731, DOI 10.1162/089976698300017115; JAAKKOLA T, 1997, P NATO ASI LEARNING; LINSKER R, 1993, ADV NEURAL INFORMATI, V5; MIKA S, 1999, ADV NEURAL INFORMATI, V11; Neal R. M., 1998, LEARNING GRAPHICAL M; SAAD D, 2001, ADV MEAN FIELD METHO; Tanaka T, 2001, ADV NEUR IN, V13, P315; TORKKOLA K, 2000, P 17 INT C MACH LEAR; Wainwright M. J., 2002, UNCERTAINTY ARTIFICI; [No title captured]	13	61	61	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						201	208						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500026
C	Rabinovich, A; Agarwal, S; Laris, C; Price, JH; Belongie, S		Thrun, S; Saul, K; Scholkopf, B		Rabinovich, A; Agarwal, S; Laris, C; Price, JH; Belongie, S			Unsupervised color decomposition of histologically stained tissue samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Accurate spectral decomposition is essential for the analysis and diagnosis of histologically stained tissue sections. In this paper we present the first automated system for performing this decomposition. We compare the performance of our system with ground truth data and report favorable results.	Univ Calif San Diego, Dept Bioengn, San Diego, CA 92103 USA	University of California System; University of California San Diego	Rabinovich, A (corresponding author), Univ Calif San Diego, Dept Bioengn, San Diego, CA 92103 USA.			Belongie, Serge/0000-0002-0388-5217				[Anonymous], [No title captured]; Bravo-Zanoguera M, 1998, REV SCI INSTRUM, V69, P3966, DOI 10.1063/1.1149207; Cardoso J.-F., 1993, IEE P F; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Krajewski S, 1999, P NATL ACAD SCI USA, V96, P5752, DOI 10.1073/pnas.96.10.5752; KRAJEWSKI S, 1994, AM J PATHOL, V145, P1323; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Marr D, 1983, VISION COMPUTATIONAL; Parra L, 2000, ADV NEUR IN, V12, P942; Price JH, 1996, CYTOMETRY, V25, P303, DOI 10.1002/(SICI)1097-0320(19961201)25:4<303::AID-CYTO1>3.3.CO;2-T; PRICE JH, 1990, THESIS U CALIFORNIA; Ruifrok AC, 2001, ANAL QUANT CYTOL, V23, P291; SHI J, 1994, P IEEE C COMP VIS PA, P593, DOI DOI 10.1109/CVPR.1994.323794; WORDINGER RJ, 1985, MANUAL IMMUNOPEROXID	14	61	62	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						667	673						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500084
C	Yue, ZS; Yong, HW; Zhao, Q; Zhang, L; Meng, DY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yue, Zongsheng; Yong, Hongwei; Zhao, Qian; Zhang, Lei; Meng, Deyu			Variational Denoising Network: Toward Blind Noise Modeling and Removal	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				IMAGE; ALGORITHM	Blind image denoising is an important yet very challenging problem in computer vision due to the complicated acquisition process of real images. In this work we propose a new variational inference method, which integrates both noise estimation and image denoising into a unique Bayesian framework, for blind image denoising. Specifically, an approximate posterior, parameterized by deep neural networks, is presented by taking the intrinsic clean image and noise variances as latent variables conditioned on the input noisy image. This posterior provides explicit parametric forms for all its involved hyper-parameters, and thus can be easily implemented for blind image denoising with automatic noise estimation for the test noisy image. On one hand, as other data-driven deep learning methods, our method, namely variational denoising network (VDN), can perform denoising efficiently due to its explicit form of posterior expression. On the other hand, VDN inherits the advantages of traditional model-driven approaches, especially the good generalization capability of generative models. VDN has good interpretability and can be flexibly utilized to estimate and remove complicated non-i.i.d. noise collected in real scenarios. Comprehensive experiments are performed to substantiate the superiority of our method in blind image denoising.	[Yue, Zongsheng; Zhao, Qian; Meng, Deyu] Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Shaanxi, Peoples R China; [Yue, Zongsheng; Yong, Hongwei; Zhang, Lei] Hong Kong Polytech Univ, Dept Comp, Kowloon, Hong Kong, Peoples R China; [Zhang, Lei] Alibaba Grp, DAMO Acad, Shenzhen, Peoples R China; [Meng, Deyu] Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China	Xi'an Jiaotong University; Hong Kong Polytechnic University; Alibaba Group; Macau University of Science & Technology	Meng, DY (corresponding author), Xi An Jiao Tong Univ, Sch Math & Stat, Xian, Shaanxi, Peoples R China.; Meng, DY (corresponding author), Macau Univ Sci & Technol, Fac Informat Technol, Macau, Peoples R China.	dymeng@mail.xjtu.edu.cn			National Key R&D Program of China [2018YFB1004300]; China NSFC [61661166011, 11690011, 61603292, 61721002, U1811461]; Kong Kong RGC General Research Fund [PolyU 152216/18E]	National Key R&D Program of China; China NSFC(National Natural Science Foundation of China (NSFC)); Kong Kong RGC General Research Fund	This research was supported by National Key R&D Program of China (2018YFB1004300), the China NSFC project under contract 61661166011, 11690011, 61603292, 61721002 and U1811461, and Kong Kong RGC General Research Fund (PolyU 152216/18E).	Abdelhamed A, 2018, PROC CVPR IEEE, P1692, DOI 10.1109/CVPR.2018.00182; Aharon M, 2006, IEEE T SIGNAL PROCES, V54, P4311, DOI 10.1109/TSP.2006.881199; Anaya J., 2014, ARXIV14098230; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bishop CM, 2006, PATTERN RECOGNITION; Blei DM, 2006, BAYESIAN ANAL, V1, P121, DOI 10.1214/06-BA104; Brooks T., 2018, ARXIV181111127; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; BURGER HC, 2012, PROC CVPR IEEE, P2392, DOI DOI 10.1109/CVPR.2012.6247952; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Deng Jia, 2014, ACM C HUM FACT COMP; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P1618, DOI 10.1109/TIP.2012.2235847; Dong WS, 2013, IEEE T IMAGE PROCESS, V22, P700, DOI 10.1109/TIP.2012.2221729; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Guo Shi, 2018, ARXIV180704686; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lebrun M, 2015, IEEE T IMAGE PROCESS, V24, P3149, DOI 10.1109/TIP.2015.2439041; Lefkimmiatis S, 2018, PROC CVPR IEEE, P3204, DOI 10.1109/CVPR.2018.00338; Liu D., 2018, P 32 C NEURAL INFORM, P1673; Ma KD, 2017, IEEE T IMAGE PROCESS, V26, P1004, DOI 10.1109/TIP.2016.2631888; Maggioni M, 2013, IEEE T IMAGE PROCESS, V22, P119, DOI 10.1109/TIP.2012.2210725; Mairal J, 2008, IEEE T IMAGE PROCESS, V17, P53, DOI 10.1109/TIP.2007.911828; Meng Deyu, 2013, IEEE INT C COMP VIS; PERONA P, 1990, IEEE T PATTERN ANAL, V12, P629, DOI 10.1109/34.56205; Plotz T, 2017, PROC CVPR IEEE, P2750, DOI 10.1109/CVPR.2017.294; Plotz Tobias, 2018, ADV NEURAL INFORM PR, V31, P1087; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roth S, 2009, INT J COMPUT VISION, V82, P205, DOI 10.1007/s11263-008-0197-6; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Simoncelli EP, 1996, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, PROCEEDINGS - VOL I, P379; Su F, 2016, 2016 16TH INTERNATIONAL SYMPOSIUM ON COMMUNICATIONS AND INFORMATION TECHNOLOGIES (ISCIT), P280, DOI 10.1109/ISCIT.2016.7751636; Tai Y, 2017, IEEE I CONF COMP VIS, P4549, DOI 10.1109/ICCV.2017.486; Tsin Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL I, PROCEEDINGS, P480, DOI 10.1109/ICCV.2001.937555; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Xiong RQ, 2017, IEEE T IMAGE PROCESS, V26, DOI [10.1109/TIP.2016.2621478, 10.1109/TIP.2017.2689999]; Xu Jun, 2018, EUR C COMP VIS ECCV; Yong HW, 2018, IEEE T PATTERN ANAL, V40, P1726, DOI 10.1109/TPAMI.2017.2732350; Yue ZS, 2018, REMOTE SENS-BASEL, V10, DOI 10.3390/rs10101631; Zhang K, 2018, PROC CVPR IEEE, P3262, DOI 10.1109/CVPR.2018.00344; Zhang K, 2018, IEEE T IMAGE PROCESS, V27, P4608, DOI 10.1109/TIP.2018.2839891; Zhao Q, 2014, PR MACH LEARN RES, V32, P55; Zhu FY, 2017, IEEE T PATTERN ANAL, V39, P1518, DOI 10.1109/TPAMI.2016.2604816; Zhu FY, 2016, PROC CVPR IEEE, P420, DOI 10.1109/CVPR.2016.52	49	60	63	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301065
C	Zhou, H; Lan, J; Liu, R; Yosinski, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhou, Hattie; Lan, Janice; Liu, Rosanne; Yosinski, Jason			Deconstructing Lottery Tickets: Zeros, Signs, and the Supermask	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The recent "Lottery Ticket Hypothesis" paper by Frankle & Carbin showed that a simple approach to creating sparse networks (keeping the large weights) results in models that are trainable from scratch, but only when starting from the same initial weights. The performance of these networks often exceeds the performance of the non-sparse base model, but for reasons that were not well understood. In this paper we study the three critical components of the Lottery Ticket (LT) algorithm, showing that each may be varied significantly without impacting the overall results. Ablating these factors leads to new insights for why LT networks perform as well as they do. We show why setting weights to zero is important, how signs are all you need to make the reinitialized network train, and why masking behaves like training. Finally, we discover the existence of Supermasks, masks that can be applied to an untrained, randomly initialized network to produce a model with performance far better than chance (86% on MNIST, 41% on CIFAR-10).	[Zhou, Hattie] Uber, San Francisco, CA 94103 USA; [Lan, Janice; Liu, Rosanne; Yosinski, Jason] Uber AI, San Francisco, CA USA	Uber Technologies, Inc.	Zhou, H (corresponding author), Uber, San Francisco, CA 94103 USA.	hattie@uber.com; janlan@uber.com; rosanne@uber.com; yosinski@uber.com		Yosinski, Jason/0000-0002-4701-0199				Carbin M, 2019, INT C LEARN REPR ICL; Courbariaux M., 2015, ADV NEUR IN, P3123; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dauphin Yann, 2013, ABS13013583 CORR; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Frankle Jonathan, 2019, ABS190301611 CORR; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Han S., 2015, 4 INT C LEARN REPR; Han Song, 2015, ADV NEURAL INFORM PR, P1135, DOI DOI 10.5555/2969239.2969366; Kadev A., 2017, INT C LEARNING REPRE; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Chunyuan, 2018, INT C LEARN REPR APR; Li H., 2016, INT C LEARNING REPRE; Louizos C., 2017, BAYESIAN COMPRESSION, V30; Mallya Arun, 2018, P EUR C COMP VIS ECC, P67; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Tyree S., 2017, INT C LEARN REPR ICL; Wen Wei, 2016, ABS160803665 CORR; Yang T.-J., 2017, P IEEE C COMP VIS PA, P5687; Yang ZC, 2015, IEEE I CONF COMP VIS, P1476, DOI 10.1109/ICCV.2015.173	23	60	60	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303057
C	Chen, IY; Johansson, FD; Sontag, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Irene Y.; Johansson, Fredrik D.; Sontag, David			Why Is My Classifier Discriminatory?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Recent attempts to achieve fairness in predictive models focus on the balance between fairness and accuracy. In sensitive applications such as healthcare or criminal justice, this trade-off is often undesirable as any increase in prediction error could have devastating consequences. In this work, we argue that the fairness of predictions should be evaluated in context of the data, and that unfairness induced by inadequate samples sizes or unmeasured predictive variables should be addressed through data collection, rather than by constraining the model. We decompose cost-based metrics of discrimination into bias, variance, and noise, and propose actions aimed at estimating and reducing each term. Finally, we perform case-studies on prediction of income, mortality, and review ratings, confirming the value of this analysis. We find that data collection is often a means to reduce discrimination without sacrificing accuracy.	[Chen, Irene Y.; Johansson, Fredrik D.; Sontag, David] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Chen, IY (corresponding author), MIT, Cambridge, MA 02139 USA.	iychen@mit.edu; fredrikj@mit.edu; dsontag@csail.mit.edu		Johansson, Fredrik/0000-0002-4323-3715	Office of Naval Research Award [N00014-17-1-2791]; NSF CAREER award [1350965]	Office of Naval Research Award(Office of Naval Research); NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD))	The authors would like to thank Yoni Halpern and Hunter Lang for helpful comments, and Zeshan Hussain for clinical guidance. This work was partially supported by Office of Naval Research Award No. N00014-17-1-2791 and NSF CAREER award #1350965.	AMARI SI, 1993, NEURAL NETWORKS, V6, P161, DOI 10.1016/0893-6080(93)90013-M; Angwin J., 2016, PROPUBLICA, P254; [Anonymous], ACTES RECHERCHE SCI; Antos A, 1999, IEEE T PATTERN ANAL, V21, P643, DOI 10.1109/34.777375; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Bechavod Y., 2017, ARXIV PREPRINT ARXIV; Bhattacharyya A., 1943, BULL CALCUTTA MATH S, V35, P99; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Calmon Flavio, 2017, ADV NEURAL INFORM PR, V30, P3995; Chouldechova A, 2017, ARXIV170300056; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; COVER TM, 1967, IEEE T INFORM THEORY, V13, P21, DOI 10.1109/TIT.1967.1053964; Devijver PA, 1982, PATTERN RECOGNITION; Domhan T., 2015, 24 INT JOINT C ART I; Domingos P., 2000, P 17 INT C MACH LEAR, P231; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Efron B., 1992, BREAKTHROUGHS STAT, P569, DOI DOI 10.1007/978-1-4612-4380-9_41; Ensign Danielle, 2017, CORR; Fish B., 2016, P 2016 SIAM INT C DA, P144; Fisher R. A, 1925, STAT METHODS RES WOR; Friedler S., 2018, ARXIV180204422; FUKUNAGA K, 1987, IEEE T PATTERN ANAL, V9, P634, DOI 10.1109/TPAMI.1987.4767958; Ghassemi M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P75, DOI 10.1145/2623330.2623742; Gnanesh, 2017, GOODREADS BOOK REV; Hardt M., 2018, ARXIV180304383; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Hebert-Johnson U., 2017, ARXIV171108513; Hestness J, 2017, ARXIV171200409; Johnson AEW, 2016, SCI DATA, V3, DOI 10.1038/sdata.2016.35; Kamiran Faisal, 2010, Proceedings 2010 10th IEEE International Conference on Data Mining (ICDM 2010), P869, DOI 10.1109/ICDM.2010.50; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Koepke Hoyt, 2012, ARXIV12064680; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Lichman M., 2013, UCI MACHINE LEARNING; Mahalanobis P.C., 1936, 2010 IEEE INT; Marlin B.M., 2012, P 2 ACM SIGHIT INT H, P389; Mukherjee S, 2003, J COMPUT BIOL, V10, P119, DOI 10.1089/106652703321825928; Pleiss G., 2017, ADV NEURAL INFORM PR, P5684; Ruggieri S, 2010, ACM T KNOWL DISCOV D, V4, DOI 10.1145/1754428.1754432; TUKEY JW, 1949, BIOMETRICS, V5, P99, DOI 10.2307/3001913; Tumer K., 1996, Proceedings of the 13th International Conference on Pattern Recognition, P695, DOI 10.1109/ICPR.1996.546912; Woodworth Blake, 2017, C LEARN THEOR; Zafar M. B., 2017, ARXIV150705259; Zemel R., 2013, P INT C MACH LEARN, P325	51	60	61	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303053
C	Kusner, M; Loftus, J; Russell, C; Silva, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kusner, Matt; Loftus, Joshua; Russell, Chris; Silva, Ricardo			Counterfactual Fairness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RISK	Machine learning can impact people with legal or ethical consequences when it is used to automate decisions in areas such as insurance, lending, hiring, and predictive policing. In many of these scenarios, previous decisions have been made that are unfairly biased against certain subpopulations, for example those of a particular race, gender, or sexual orientation. Since this past data may be biased, machine learning predictors must account for this to avoid perpetuating or creating discriminatory practices. In this paper, we develop a framework for modeling fairness using tools from causal inference. Our definition of counterfactual fairness captures the intuition that a decision is fair towards an individual if it the same in (a) the actual world and (b) a counterfactual world where the individual belonged to a different demographic group. We demonstrate our framework on a real-world problem of fair prediction of success in law school.	[Kusner, Matt; Russell, Chris; Silva, Ricardo] Alan Turing Inst, London, England; [Kusner, Matt] Univ Warwick, Coventry, W Midlands, England; [Loftus, Joshua] NYU, New York, NY 10003 USA; [Russell, Chris] Univ Surrey, Guildford, Surrey, England; [Silva, Ricardo] UCL, London, England	University of Warwick; New York University; University of Surrey; University of London; University College London	Kusner, M (corresponding author), Alan Turing Inst, London, England.; Kusner, M (corresponding author), Univ Warwick, Coventry, W Midlands, England.	mkusner@turing.ac.uk; loftus@nyu.edu; crussell@turing.ac.uk; ricardo@stats.ucl.ac.uk	Jeong, Yongwook/N-7413-2016		Alan Turing Institute under the EPSRC [EP/N510129/1]; EPSRC Platform Grant [EP/P022529/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Platform Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1. CR acknowledges additional support under the EPSRC Platform Grant EP/P022529/1. We thank Adrian Weller for insightful feedback, and the anonymous reviewers for helpful comments.	[Anonymous], 2016, ACTUAL CAUSALITY; Berk R., 2017, ARXIV170309207V1; Bollen K., 1989, STRUCTURAL EQUATIONS, V3, P7; Bollen K. A., 1993, TESTING STRUCTURAL E; Brennan T, 2009, CRIM JUSTICE BEHAV, V36, P21, DOI 10.1177/0093854808326545; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Dawid AP, 2000, J AM STAT ASSOC, V95, P407, DOI 10.2307/2669377; DeDeo S., 2014, ARXIV14124643; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Glymour C, 2014, EPIDEMIOLOGY, V25, P488, DOI 10.1097/EDE.0000000000000122; Grgic-Hlaca N, 2016, CASE PROCESS FAIRNES; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Johnson K.D., 2016, ARXIV160800528; Joseph M., 2016, P 3 WORKSH FAIRN ACC; Kamiran F., 2009, COMPUTER CONTROL COM, P1, DOI 10.1109/IC4.2009.4909197; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Khandani AE, 2010, J BANK FINANC, V34, P2767, DOI 10.1016/j.jbankfin.2010.06.001; Kleinberg J., 2017, P 8 INNOVATIONS THEO; Lewis D., 1973, COUNTERFACTUALS; Louizos C., 2015, ARXIV PREPRINT ARXIV; Mahoney JF, 2007, Patent No. [US7287008 B1, 7287008]; Mooij J.M., 2009, P 26 ANN INT C MACH, P745; Nabi R., 2017, ARXIV170510378V1; Pearl J., 2009, CAUSALITY MODELS REA, DOI [DOI 10.1017/CBO9780511803161, 10.1017/CBO9780511803161]; Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021; Pearl J, 2009, STAT SURV, V3, P96, DOI 10.1214/09-SS057; Peters J, 2014, J MACH LEARN RES, V15, P2009; Russell C., 2017, ADV NEURAL INFORM PR, V31; Silva R., 2016, J MACHINE LEARNING R, V17, P1; Stan Development Team, 2020, RSTAN R INT STAN; Wightman, 1998, LSAC RES REPORT SERI; Zafar Muhammad Bilal, 2016, ARXIV161008452; Zafar Muhammad Bilal, 2015, ARXIV150705259; Zemel R., 2013, P INT C MACH LEARN, P325; Zliobaite Indre, 2015, ARXIV151100148	39	60	61	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404014
C	Goh, G; Cotter, A; Gupta, M; Friedlander, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Goh, Gabriel; Cotter, Andrew; Gupta, Maya; Friedlander, Michael			Satisfying Real-world Goals with Dataset Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The goal of minimizing misclassification error on a training set is often just one of several real-world goals that might be defined on different datasets. For example, one may require a classifier to also make positive predictions at some specified rate for some subpopulation (fairness), or to achieve a specified empirical recall. Other real-world goals include reducing churn with respect to a previously deployed model, or stabilizing online training In this paper we propose handling multiple goals on multiple datasets by training with dataset constraints, using the ramp penalty to accurately quantify costs, and present an efficient algorithm to approximately optimize the resulting non-convex constrained optimization problem. Experiments on both benchmark and real-world industry datasets demonstrate the effectiveness of our approach.	[Goh, Gabriel] Univ Calif Davis, Dept Math, Davis, CA 95616 USA; [Cotter, Andrew; Gupta, Maya] Google Inc, 1600 Amphitheatre Pkwy, Mountain View, CA 94043 USA; [Friedlander, Michael] Univ British Columbia, Dept Comp Sci, Vancouver, BC V6T 1Z4, Canada	University of California System; University of California Davis; Google Incorporated; University of British Columbia	Goh, G (corresponding author), Univ Calif Davis, Dept Math, Davis, CA 95616 USA.	ggoh@math.ucdavis.edu; acotter@google.com; mayagupta@google.com; mpf@cs.ubc.ca						Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Biddle D, 2005, ADVERSE IMPACT TEST; BLAND RG, 1981, OPER RES, V29, P1039, DOI 10.1287/opre.29.6.1039; Boyd S., 2011, STANFORD EE 364B LEC; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Collobert R., 2006, ICML; Cotter A., 2013, PROC INT C MACH LEAR, P266; Davenport M., 2010, IEEE T PATTERN ANAL; Eban E. E., 2016, LARGE SCALE LEARNING; Fard M. M., 2016, NIPS; Gasso G., 2011, ACM T INTELLIGENT SY; Gruinbaum B., 1960, PACIFIC J MATH, V10, P1257; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Mann G. S., 2007, P 24 INT C MACH LEAR, P593, DOI DOI 10.1145/1273496.1273571; Miettinen K., 1999, NONLINEAR MULTIOBJEC; Narasimhan H, 2015, PR MACH LEARN RES, V37, P199; Nemirovski A., 1994, LECT NOTES EFFICIENT; Rademacher LA, 2007, S COMP GEOM, P302, DOI [10.1145/1247069.1247123, DOI 10.1145/1247069.1247123]; Rockafellar R., 2000, J RISK, V2, P21, DOI 10.21314/JOR.2000.038; Scott C. D., 2005, IEEE T INFORM THEORY; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Vuolo M. S., 2013, NEW YORK LAW J; Zafar M. B., 2015, ICML WORKSH FAIRN AC	27	60	60	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702063
C	Chung, J; Kastner, K; Dinh, L; Goel, K; Courville, A; Bengio, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Chung, Junyoung; Kastner, Kyle; Dinh, Laurent; Goel, Kratarth; Courville, Aaron; Bengio, Yoshua			A Recurrent Latent Variable Model for Sequential Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					In this paper, we explore the inclusion of latent random variables into the hidden state of a recurrent neural network (RNN) by combining the elements of the variational autoencoder. We argue that through the use of high-level latent random variables, the variational RNN (VRNN) 1 can model the kind of variability observed in highly structured sequential data such as natural speech. We empirically evaluate the proposed model against other related sequential models on four speech datasets and one handwriting dataset. Our results show the important roles that latent random variables can play in the RNN dynamics.	[Chung, Junyoung; Kastner, Kyle; Dinh, Laurent; Goel, Kratarth; Courville, Aaron; Bengio, Yoshua] Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ, Canada	Universite de Montreal	Chung, J (corresponding author), Univ Montreal, Dept Comp Sci & Operat Res, Montreal, PQ, Canada.	junyoung.chung@umontreal.ca; kyle.kastner@umontreal.ca; laurent.dinh@umontreal.ca; kratarth.goel@umontreal.ca; aaron.courville@umontreal.ca; yoshua.bengio@umontreal.ca			CIFAR; Ubisoft; NSERC; Calcul Quebec; Canada Research Chairs; Compute Canada	CIFAR(Canadian Institute for Advanced Research (CIFAR)); Ubisoft; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Calcul Quebec; Canada Research Chairs(Canada Research ChairsCGIAR); Compute Canada	The authors would like to thank the developers of Theano [1]. Also, the authors thank Kyunghyun Cho, Kelvin Xu and Sungjin Ahn for insightful comments and discussion. We acknowledge the support of the following agencies for research funding and computing support: Ubisoft, NSERC, Calcul Quebec, Compute Canada, the Canada Research Chairs and CIFAR.	Bastien F., 2012, DEEP LEARN UNS FEAT; Bayer J, 2014, ARXIV14117610; Bertrand A, 2008, INT CONF ACOUST SPEE, P4713, DOI 10.1109/ICASSP.2008.4518709; Boulanger-Lewandowski N., 2012, P 29 INT C MACH LEAR, P1159, DOI DOI 10.32604/CSSE.2021.014030; Cho K., 2014, P 2014 C EMP METH NA, P1724; Graves Alex, 2013, ARXIV13080850 CORR; Gregor K., 2015, P 32 INT C MACH LEAR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; King S., 2013, 9 ANN BLIZZARD CHALL; Kingma D., P INT C LEARN REPR I, DOI DOI 10.1145/1830483.1830503; Kingma DP., 2015, INT C LEARN REPR ICL; Kusy B, 2009, 2009 INTERNATIONAL CONFERENCE ON INFORMATION PROCESSING IN SENSOR NETWORKS (IPSN 2009), P109; Liwicki M, 2005, PROC INT CONF DOC, P956, DOI 10.1109/ICDAR.2005.132; Nair V., 2010, ICML, P807; Pachitariu M., 2012, ADV NEURAL INFORM PR, P1322; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Tokuda K, 2013, P IEEE, V101, P1234, DOI 10.1109/JPROC.2013.2251852; van Amersfoort J. R., 2014, ARXIV14126581	18	60	60	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100021
C	Ng, AY; Kim, HJ; Jordan, MI; Sastry, S		Thrun, S; Saul, K; Scholkopf, B		Ng, AY; Kim, HJ; Jordan, MI; Sastry, S			Autonomous helicopter flight via reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Autonomous helicopter flight represents a challenging control problem, with complex, noisy, dynamics. In this paper, we describe a successful application of reinforcement learning to autonomous helicopter flight. We first fit a stochastic, nonlinear model of the helicopter dynamics. We then use the model to learn to hover in place, and to fly a number of maneuvers taken from an RC helicopter competition.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	Ng, AY (corresponding author), Stanford Univ, Stanford, CA 94305 USA.		Jordan, Michael I/C-5253-2013					ATKESON C, 1997, AI REV, V11; BAGNELL J, 2001, INT C ROB AUT IEEE; Balas G.J., 1995, MU ANAL SYNTHESIS TO; FRANKLIN GF, 1995, FEEDBACK CONTROL DYN; Jordan M. I., 2003, THESIS U CALIFORNIA; KIEFER J, 1952, ANN MATH STAT, V23, P462, DOI 10.1214/aoms/1177729392; LEISHMAN J.G., 2000, CAMBRIDGE AEROSPACE, P55; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; NG AY, 2000, P 16 C UNC ART INT; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; SHIM H, 2000, THESIS UC BERKELEY; [No title captured]	12	60	60	0	5	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						799	806						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500100
C	Gray, AG; Moore, AW		Leen, TK; Dietterich, TG; Tresp, V		Gray, AG; Moore, AW			'N-body' problems in statistical learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present efficient algorithms for all-point-pairs problems, or 'N-body'-like problems, which are ubiquitous in statistical learning. We focus on six examples, including nearest-neighbor classification, kernel density estimation, outlier detection, and the two-point correlation. These include any problem which abstractly requires a comparison of each of the N points in a dataset with each other point and would naively be solved using N-2 distance computations. In practice N is often large enough to make this infeasible. We present a suite of new geometric techniques which are applicable in principle to any 'N-body' computation including large-scale mixtures of Gaussians, RBF neural networks, and HMM's. Our algorithms exhibit favorable asymptotic scaling and are empirically several orders of magnitude faster than the naive computation, even for small datasets. We are aware of no exact algorithms for these problems which are more efficient either empirically or theoretically. In addition, our framework yields simple and elegant algorithms. It also permits two important generalizations beyond the standard all-point-pairs problems, which are more difficult. These are represented by our final examples, the multiple two-point correlation and the notorious n-point correlation.	Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Gray, AG (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA.	agray@cs.cmu.edu; awm@cs.cmu.edu						BARNES J, 1986, NATURE, V324, P446, DOI 10.1038/324446a0; DENG K, 1995, P 12 INT JOINT C ART, P1233; Friedman J. H., 1977, ACM Transactions on Mathematical Software, V3, P209, DOI 10.1145/355744.355745; GREENGARD L, 1987, J COMPUTATIONAL PHYS, V73; Knuth, 1997, SORTING SEARCHING; Moore AW, 1999, ADV NEUR IN, V11, P543; MOORE AW, 2000, IN PRESS 12 C UNC AR; MOORE AW, 1998, J ARTIFICIAL INTELLI, V8; PELLEG D, 1999, P 5 INT C KNOWL DISC; Preparata F.P., 1985, COMPUTATIONAL GEOMET, V1; SZALAY A, 2000, COMMUNICATION; SZAPUDI I, 1999, MONTHLY NOTICES ROYA; SZAPUDI I, 1997, ASTROPHYSICAL J; ZHANG T, 1996, P 15 ACM SIGACT SIGM	14	60	60	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						521	527						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800074
C	Tenenbaum, JB; Freeman, WT		Mozer, MC; Jordan, MI; Petsche, T		Tenenbaum, JB; Freeman, WT			Separating style and content	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We seek to analyze and manipulate two factors, which we call style and content, underlying a set of observations. We fit training data with bilinear models which explicitly represent the two-factor structure. These models can adapt easily during testing to new styles or content, allowing us to solve three general tasks: extrapolation of a new style to unobserved content; classification of content observed in a new style; and translation of new content observed in a new style. For classification, we embed bilinear models in a probabilistic framework, Separable Mixture Models (SMMs), which generalizes earlier work on factorial mixture models [7, 3]. Significant performance improvement on a benchmark speech dataset shows the benefits of our approach.			Tenenbaum, JB (corresponding author), MIT,DEPT BRAIN & COGNIT SCI,E25-618,CAMBRIDGE,MA 02139, USA.								0	60	60	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						662	668						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00094
C	Carmon, Y; Raghunathan, A; Schmidt, L; Liang, P; Duchi, JC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Carmon, Yair; Raghunathan, Aditi; Schmidt, Ludwig; Liang, Percy; Duchi, John C.			Unlabeled Data Improves Adversarial Robustness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We demonstrate, theoretically and empirically, that adversarial robustness can significantly benefit from semisupervised learning. Theoretically, we revisit the simple Gaussian model of Schmidt et al. [41] that shows a sample complexity gap between standard and robust classification. We prove that unlabeled data bridges this gap: a simple semisupervised learning procedure (self-training) achieves high robust accuracy using the same number of labels required for achieving high standard accuracy. Empirically, we augment CIFAR-10 with 500K unlabeled images sourced from 80 Million Tiny Images and use robust self-training to outperform state-of-the-art robust accuracies by over 5 points in (i) l(infinity) robustness against several strong attacks via adversarial training and (ii) certified l(2) and l(infinity) robustness via randomized smoothing. On SVHN, adding the dataset's own extra training set with the labels removed provides gains of 4 to 10 points, within 1 point of the gain from using the extra labels.	[Carmon, Yair; Raghunathan, Aditi; Liang, Percy; Duchi, John C.] Stanford Univ, Stanford, CA 94305 USA; [Schmidt, Ludwig] Univ Calif Berkeley, Berkeley, CA USA	Stanford University; University of California System; University of California Berkeley	Carmon, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yairc@stanford.edu; aditir@stanford.edu; ludwig@berkeley.edu; pliang@csstanford.edu; jduchi@stanford.edu		Duchi, John/0000-0003-0045-7185	Stanford Graduate Fellowship; Google Fellowship; Open Philanthropy AI Fellowship; Open Philanthropy Project Award; NSF CAREER award [1553086]; Sloan Foundation; ONR-YIP [N00014-19-1-2288]	Stanford Graduate Fellowship(Stanford University); Google Fellowship(Google Incorporated); Open Philanthropy AI Fellowship; Open Philanthropy Project Award; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Sloan Foundation(Alfred P. Sloan Foundation); ONR-YIP(Office of Naval Research)	The authors would like to thank an anonymous reviewer for proposing the label amount experiment in Appendix C.5. YC was supported by the Stanford Graduate Fellowship. AR was supported by the Google Fellowship and Open Philanthropy AI Fellowship. PL was supported by the Open Philanthropy Project Award. JCD was supported by the NSF CAREER award 1553086, the Sloan Foundation and ONR-YIP N00014-19-1-2288.	Athalye A, 2018, PR MACH LEARN RES, V80; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Biggio B, 2018, PATTERN RECOGN, V84, P317, DOI 10.1016/j.patcog.2018.07.023; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bubeck S, 2019, PR MACH LEARN RES, V97; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cohen J, 2019, PR MACH LEARN RES, V97; Cubuk ED, 2019, PROC CVPR IEEE, P113, DOI 10.1109/CVPR.2019.00020; Das TK, 2007, J COSMOL ASTROPART P, DOI 10.1088/1475-7516/2007/06/009; Degwekar A., 2019, ARXIV190201086; Engstrom L., 2018, ARXIV180710272; Gilmer Justin, 2018, ARXIV180102774; Gowal Sven, 2018, EFFECTIVENESS INTERV; Hendrycks D, 2019, PR MACH LEARN RES, V97; Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525; Kannan Harini, 2018, ARXIV180306373; Khim J., 2018, ARXIV181009519; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Lecuyer M., 2019, 2019 IEEE S SEC PRIV; Lee D., 2013, INT C MACH LEARN ICM; Li B., 2018, ARXIV180903113, P1; Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274; Loshchilov I., 2017, P INT C LEARNING REP; Madry Aleksander, 2017, ARXIV; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Montasser Omar, 2019, ARXIV190204217, V99, P2512; Najafi A., 2019, ARXIV190513021; Netzer Yuval, 2011, NEURIPS WORKSH, V2, P6; Oliver A, 2018, ADV NEUR IN, V31; Papernot N., 2018, ARXIV161000768; Raghunathan A., 2019, ARXIV190606032; Raghunathan A, 2018, ADV NEUR IN, V31; Raghunathan Aditi, 2018, ARXIV180109344; Recht B, 2018, ARXIV180600451; Sajjadi Mehdi, 2016, NEURIPS; Schmidt L, 2018, ADV NEUR IN, V31; SCUDDER HJ, 1965, IEEE T INFORM THEORY, V11, P363, DOI 10.1109/tit.1965.1053799; Sinha A., 2018, ICLR; Tarvainen Antti, 2017, CORR, Vabs/1703; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Uesato J., 2019, NEURIPS; Wong E, 2018, ADV NEUR IN, V31; Wong E, 2018, PR MACH LEARN RES, V80; Xie Qizhe, 2019, ARXIV190412848, P2; Yin D, 2019, PR MACH LEARN RES, V97; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhai R., 2019, ARXIV190600555; Zhang H, 2019, PR MACH LEARN RES, V97; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485; Zhu X., 2003, INT C MACH LEARN; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_; Zou Y, 2019, IEEE I CONF COMP VIS, P5981, DOI 10.1109/ICCV.2019.00608	60	59	59	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902078
C	Dong, L; Yang, N; Wang, WH; Wei, FR; Liu, XD; Wang, Y; Gao, JF; Zhou, M; Hon, HW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dong, Li; Yang, Nan; Wang, Wenhui; Wei, Furu; Liu, Xiaodong; Wang, Yu; Gao, Jianfeng; Zhou, Ming; Hon, Hsiao-Wuen			Unified Language Model Pre-training for Natural Language Understanding and Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper presents a new UNIfied pre-trained Language Model (UNILM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UNILM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UNILM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https //github.com/microsoft/unilm.	[Dong, Li; Yang, Nan; Wang, Wenhui; Wei, Furu; Liu, Xiaodong; Wang, Yu; Gao, Jianfeng; Zhou, Ming; Hon, Hsiao-Wuen] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Dong, L (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	lidongl@microsoft.com; nanya@microsoft.com; wenwan@microsoft.com; fuwei@microsoft.com; xiaodl@microsoft.com; yuwan@microsoft.com; jfgao@microsoft.com; mingzhou@microsoft.com; hon@microsoft.com						Baevski A, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5360; Bar-Haim Roy, 2006, P 2 PASC CHALL WORKS; Bentivogli Luisa, 2009, P TEXT AN C TA C 09; Cao ZQ, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P152; Cer Daniel, 2017, ARXIV170800055; Chi Zewen, 2019, ABS190910481 ARXIV; Dagan I, 2006, LECT NOTES ARTIF INT, V3944, P177; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Dolan William B., 2005, P 3 INT WORKSH PAR I; Du XY, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1342, DOI 10.18653/v1/P17-1123; Du XY, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P1907; Edunov Sergey, 2019, P C N AM CHAPT ASS C, P4052, DOI DOI 10.18653/V1/N19-1409; Galley Michel, 2019, AAAI DIAL SYST TECHN; Giampiccolo Danilo, 2007, P ACL PASCAL WORKSH, P1; Hendrycks Dan, 2016, ARXIV160608415; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Hu Minghao, 2018, ARXIV180807644; Kingma D.P, P 3 INT C LEARNING R; Klein G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P67, DOI 10.18653/v1/P17-4012; Levesque Hector, 2012, 13 INT C PRINC KNWOL; Lin CH, 2012, 2012 8TH INTERNATIONAL SYMPOSIUM ON CHINESE SPOKEN LANGUAGE PROCESSING, P74, DOI 10.1109/ISCSLP.2012.6423469; Liu Xiaodong, 2019, CORR; Liu Y., 2019, ARXIV; Paulus Romain, 2018, CORR; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Qin LH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P5427; Radford A., 2018, P 2018 C N AM ASS CO, DOI 10.48550/ARXIV.1802.05365; RAJPURKAR P, 2016, P 2016 C EMP METH NA, V2016, P2383, DOI DOI 10.18653/V1/D16-1264; Rajpurkar P, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 2, P784; Reddy S, 2019, T ASSOC COMPUT LING, V7, P249, DOI 10.1162/tacl_a_00266; See A, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1073, DOI 10.18653/v1/P17-1099; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Song Kaitao, 2019, ARXIV190502450; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tam Y, 2019, AAAI DIAL SYST TECHN; Taylor WL, 1953, JOURNALISM QUART, V30, P415, DOI 10.1177/107769905303000401; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang Alex, 2018, ARXIV180407461, DOI DOI 10.18653/V1/W18-5446; Wang Alex, 2019, CORR; Warstadt Alex, 2018, ARXIV180512471; Williams A., 2018, P C N AM CHAPT ASS C, V1, P1112, DOI DOI 10.18653/V1/N18-1101; Wu YH, 2018, VIS COMPUT IND BIOME, V1, DOI 10.1186/s42492-018-0008-z; Yogatama Dani, 2019, ARXIV190111373; Zhang S., 2019, CORR; Zhao Yao, 2018, P 2018 C EMP METH NA, P3901, DOI DOI 10.18653/V1/D18-1424; Zhou Q, 2018, J ENG-JOE, P669, DOI 10.1049/joe.2018.0096; Zhu YK, 2015, IEEE I CONF COMP VIS, P19, DOI 10.1109/ICCV.2015.11	51	59	60	1	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904067
C	Jiang, P; Agrawal, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jiang, Peng; Agrawal, Gagan			A Linear Speedup Analysis of Distributed Deep Learning with Sparse and Quantized Communication	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The large communication overhead has imposed a bottleneck on the performance of distributed Stochastic Gradient Descent (SGD) for training deep neural networks. Previous works have demonstrated the potential of using gradient sparsification and quantization to reduce the communication cost. However, there is still a lack of understanding about how sparse and quantized communication affects the convergence rate of the training algorithm. In this paper, we study the convergence rate of distributed SGD for non-convex optimization with two communication reducing strategies: sparse parameter averaging and gradient quantization. We show that O(1/root MK) convergence rate can be achieved if the sparsification and quantization hyperparameters are configured properly. We also propose a strategy called periodic quantized averaging (PQASGD) that further reduces the communication cost while preserving the O(1/root pMK) convergence rate. Our evaluation validates our theoretical results and shows that our PQASGD can converge as fast as full-communication SGD with only 3% - 5% communication data size.	[Jiang, Peng; Agrawal, Gagan] Ohio State Univ, Columbus, OH 43210 USA	University System of Ohio; Ohio State University	Jiang, P (corresponding author), Ohio State Univ, Columbus, OH 43210 USA.	jiang.952@osu.edu; agrawal@cse.ohio-state.edu		Jiang, Peng/0000-0001-7743-6062				Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Acero A., 1990, ACOUSTICAL ENV ROBUS; Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Aji A.F., 2017, P 2017 C EMP METH NA, DOI [10.18653/v1/D17-1045, DOI 10.18653/V1/D17-1045]; Alistarh D, 2017, ADV NEUR IN, V30; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Awan AA, 2017, ACM SIGPLAN NOTICES, V52, P193, DOI [10.1145/3018743.3018769, 10.1145/3155284.3018769]; Chen C.-Y., 2017, CORR; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; CHILIMBI TM, 2014, P OSDI, V14, P571; Coates A., 2013, INT C MACHINE LEARNI, P1337; Collobert R, 2002, TORCH MODULAR MACHIN; De Sa C., 2015, NIPS, P2674; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Dekel O, 2012, J MACH LEARN RES, V13, P165; Dinh L, 2017, PR MACH LEARN RES, V70; Garg R., 2009, P 26 ANN INT C MACHI, P337; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223; Hoffer Elad, 2017, ADV NEURAL INFORM PR, P1729; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Keskar N.S., 2016, ABS160904836; Krizhevsky A, 2009, LEARNING MULTIPLE LA; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Li H, 2017, ADV NEUR IN, V30; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Li M, 2015, GPS SOLUT, V19, P27, DOI 10.1007/s10291-013-0362-4; Lian X, 2017, P ADV NEUR INF PROC, P5330; Lian X., 2016, ADV NEURAL INFORM PR, P3054; Lin YL, 2018, INT CONF SYST SCI EN; Povey D, 2014, PARALLEL TRAINING DE; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Seide F, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2135, DOI 10.1145/2939672.2945397; Seide Frank, 2014, 1 BIT STOCHASTIC GRA; Shi R, 2018, 2018 IEEE 26 INT S M; Smith Samuel L., 2018, INT C LEARN REPR; Strom N, 2015, 16TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION (INTERSPEECH 2015), VOLS 1-5, P1488; Su H, 2015, CORR; Wang LN, 2018, ACM SIGPLAN NOTICES, V53, P41, DOI 10.1145/3200691.3178491; Wangni Jianqiao, 2017, ARXIV171009854; Wen W., 2017, NIPS 17, V30, P1509; Zhang S, 2015, ONCOL TRANSLAT MED, V1, P15; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	47	59	60	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302053
C	Nam, H; Kim, HE		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nam, Hyeonseob; Kim, Hyo-Eun			Batch-Instance Normalization for Adaptively Style-Invariant Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Real-world image recognition is often challenged by the variability of visual styles including object textures, lighting conditions, filter effects, etc. Although these variations have been deemed to be implicitly handled by more training data and deeper networks, recent advances in image style transfer suggest that it is also possible to explicitly manipulate the style information. Extending this idea to general visual recognition problems, we present Batch-Instance Normalization (BIN) to explicitly normalize unnecessary styles from images. Considering certain style features play an essential role in discriminative tasks, BIN learns to selectively normalize only disturbing styles while preserving useful styles. The proposed normalization module is easily incorporated into existing network architectures such as Residual Networks, and surprisingly improves the recognition performance in various scenarios. Furthermore, experiments verify that BIN effectively adapts to completely different tasks like object classification and style transfer, by controlling the tradeoff between preserving and removing style variations. BIN can be implemented with only a few lines of code using popular deep learning frameworks.(1)	[Nam, Hyeonseob; Kim, Hyo-Eun] Lunit Inc, Seoul, South Korea		Nam, H (corresponding author), Lunit Inc, Seoul, South Korea.	hsnam@lunit.io; hekim@lunit.io						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba J. L., 2016, ARXIV PREPRINT ARXIV; Cimpoi M, 2015, PROC CVPR IEEE, P3828, DOI 10.1109/CVPR.2015.7299007; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Ganin Y., 2016, JMLR, V17, P2096; Gatys L. A., 2015, ADV NEURAL INFORM PR, V28, P262, DOI DOI 10.1016/0014-5793(76)80724-7; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang Xun, 2017, ICCV; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Ioffe Sergey, 2017, NIPS, P3; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Johnson Justin, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9906, P694, DOI 10.1007/978-3-319-46475-6_43; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lempitsky V., 2016, ARXIV160708022V3; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Ulyanov D., 2017, CVPR, P6924; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Weinberger L, 2017, CVPR; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	27	59	60	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302056
C	Song, Y; Shu, R; Kushman, N; Ermon, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Song, Yang; Shu, Rui; Kushman, Nate; Ermon, Stefano			Constructing Unrestricted Adversarial Examples with Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Adversarial examples are typically constructed by perturbing an existing data point within a small matrix norm, and current defense methods are focused on guarding against this type of attack. In this paper, we propose unrestricted adversarial examples, a new threat model where the attackers are not restricted to small norm-bounded perturbations. Different from perturbation-based attacks, we propose to synthesize unrestricted adversarial examples entirely from scratch using conditional generative models. Specifically, we first train an Auxiliary Classifier Generative Adversarial Network (AC-GAN) to model the class-conditional distribution over data samples. Then, conditioned on a desired class, we search over the AC-GAN latent space to find images that are likely under the generative model and are misclassified by a target classifier. We demonstrate through human evaluation that unrestricted adversarial examples generated this way are legitimate and belong to the desired class. Our empirical results on the MNIST, SVHN, and CelebA datasets show that unrestricted adversarial examples can bypass strong adversarial training and certified defense methods designed for traditional adversarial attacks.	[Song, Yang; Shu, Rui; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA; [Kushman, Nate] Microsoft Res, Bengaluru, Karnataka, India	Stanford University	Song, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yangsong@cs.stanford.edu; ruishu@cs.stanford.edu; nkushman@microsoft.com; ermon@cs.stanford.edu	Song, Yang/X-5282-2019		Intel Corporation; NSF [1651565, 1522054, 1733686]; TRI; FLI [2017-158687]	Intel Corporation(Intel Corporation); NSF(National Science Foundation (NSF)); TRI; FLI	The authors would like to thank Shengjia Zhao for reviewing an early draft of this paper. We also thank Ian Goodfellow, Ben Poole, Anish Athalye and Sumanth Dathathri for helpful online discussions. This research was supported by Intel Corporation, TRI, NSF (#1651565, #1522054, #1733686) and FLI (#2017-158687).	Anderson HS, 2016, AISEC'16: PROCEEDINGS OF THE 2016 ACM WORKSHOP ON ARTIFICIAL INTELLIGENCE AND SECURITY, P13, DOI 10.1145/2996758.2996767; Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Arjovsky M, 2017, PR MACH LEARN RES, V70; Athalye A, 2018, PR MACH LEARN RES, V80; Baluja S., 2017, ARXIV170309387; Brown Tom B, 2017, ARXIV171209665; Buhrmester M, 2011, PERSPECT PSYCHOL SCI, V6, P3, DOI 10.1177/1745691610393980; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini N, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P513; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Cisse Moustapha, 2017, P ADV NEUR INF PROC; Eykholt K, 2018, PROC CVPR IEEE, P1625, DOI 10.1109/CVPR.2018.00175; Fawzi A., 2016, BRIT MACHINE VISION; Goodfellow I., 2018, ARXIV180908352; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Grover A, 2018, AAAI CONF ARTIF INTE, P3069; Gu S., 2014, ARXIV14125068; Gulrajani I, 2017, P NIPS 2017; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hosseini Hossein, 2018, ARXIV180400499; Kolter J.Z., 2017, CORR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A, 2016, INT C LEARN REPR SAN; Kurakin A., 2016, ARXIV PREPRINT ARXIV; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Lee H., 2017, GENERATIVE ADVERSARI; Liu Yanpei, 2017, INT C LEARN REPR; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Madry Aleksander, 2017, MNIST ADVERSARIAL EX; Madry Aleksander, 2017, ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; NOCEDAL J, 1980, MATH COMPUT, V35, P773, DOI 10.1090/S0025-5718-1980-0572855-7; Odena A, 2017, PR MACH LEARN RES, V70; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Raghunathan Aditi, 2018, ARXIV180109344; Samangouei P., 2018, P INT C LEARN REPR; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shu Rui, 2018, P 6 INT C LEARN REPR, P2; Sinha A, 2017, ARXIV171010571; Song J., 2018, MULTIAGENT GENERATIV; Tao T, 2012, RANDOM MATRICES-THEO, V1, DOI 10.1142/S2010326311500018; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd; Xiao CW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3905; Xiao H., 2017, ARXIV 170807747; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Yang Song, 2018, INT C LEARN REPR; Zhang GM, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P103, DOI 10.1145/3133956.3134052; Zhao ZF, 2018, ACSR ADV COMPUT, V78, P1	54	59	60	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002082
C	Lei, LH; Ju, C; Chen, JB; Jordan, MI		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lei, Lihua; Ju, Cheng; Chen, Jianbo; Jordan, Michael, I			Non-Convex Finite-Sum Optimization Via SCSG Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We develop a class of algorithms, as variants of the stochastically controlled stochastic gradient (SCSG) methods [21], for the smooth non-convex finite-sum optimization problem. Assuming the smoothness of each component, the complexity of SCSG to reach a stationary point with E parallel to del f(x)parallel to(2) <= epsilon is O (min{epsilon(-5/3), epsilon(-1)n(2/3)}), which strictly outperforms the stochastic gradient descent. Moreover, SCSG is never worse than the state-of-the-art methods based on variance reduction and it significantly outperforms them when the target accuracy is low. A similar acceleration is also achieved when the functions satisfy the Polyak-Lojasiewicz condition. Empirical experiments demonstrate that SCSG outperforms stochastic gradient methods on training multi-layers neural networks in terms of both training and validation loss.	[Lei, Lihua; Ju, Cheng; Chen, Jianbo; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Lei, LH (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	lihua.lei@berkeley.edu; cju@berkeley.edu; jianbochen@berkeley.edu; jordan@stat.berkeley.edu	Jordan, Michael I/C-5253-2013; Jeong, Yongwook/N-7413-2016; Ju, Cheng/O-8988-2019	Jordan, Michael/0000-0001-8935-817X				Agarwal Alekh, 2014, ABS14100723 ARXIV; AGARWAL N, 2016, ARXIV161101146; Allen-Zhu Z., 2014, ARXIV14071537; Allen-Zhu Z, 2017, PR MACH LEARN RES, V70; Allen-Zhu Zeyuan, 2016, ABS160305643 ARXIV; Allen-Zhu Zeyuan, 2015, ABS150601972 ARXIV; Babanezhad Harikandeh R., 2015, ADV NEURAL INFORM PR, V28, P2251; Bertsekas DP, 1997, SIAM J OPTIMIZ, V7, P913, DOI 10.1137/S1052623495287022; CARMON Y, 2016, ARXIV161100756; Carmon Y, 2017, PR MACH LEARN RES, V70; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Gaivoronski AA., 1994, OPTIMIZATION METHODS, V4, P117, DOI [10.1080/10556789408805582, DOI 10.1080/10556789408805582]; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lei L., 2016, ARXIV160903261; NELDER JA, 1972, J R STAT SOC SER A-G, V135, P370, DOI 10.2307/2344614; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nesterov Y., 2004, APPL OPTIM; Reddi S., 2016, ARXIV160306159; Reddi SJ, 2016, PR MACH LEARN RES, V48; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tseng P, 1998, SIAM J OPTIMIZ, V8, P506, DOI 10.1137/S1052623495294797; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Zhang, 2013, ADV NEURAL INFORM PR, P315	30	59	60	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402039
C	Choi, SPM; Yeung, DY		Touretzky, DS; Mozer, MC; Hasselmo, ME		Choi, SPM; Yeung, DY			Predictive Q-routing: A memory-based reinforcement learning approach to adaptive traffic control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						HONG KONG UNIV SCI & TECHNOL,DEPT COMP SCI,KOWLOON,HONG KONG	Hong Kong University of Science & Technology									0	59	63	1	1	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						945	951						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00133
C	Hastie, T; Tibshirani, R		Touretzky, DS; Mozer, MC; Hasselmo, ME		Hastie, T; Tibshirani, R			Discriminant adaptive nearest neighbor classification and regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						STANFORD UNIV,DEPT STAT,STANFORD,CA 94305	Stanford University				Hastie, Trevor/0000-0002-0164-3142					0	59	60	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						409	415						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00058
C	Huang, HB; Li, ZH; He, R; Sun, ZN; Tan, TN		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huang, Huaibo; Li, Zhihang; He, Ran; Sun, Zhenan; Tan, Tieniu			IntroVAE: Introspective Variational Autoencoders for Photographic Image Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel introspective variational autoencoder (IntroVAE) model for synthesizing high-resolution photographic images. IntroVAE is capable of self-evaluating the quality of its generated samples and improving itself accordingly. Its inference and generator models are jointly trained in an introspective way. On one hand, the generator is required to reconstruct the input images from the noisy outputs of the inference model as normal VAEs. On the other hand, the inference model is encouraged to classify between the generated and real samples while the generator tries to fool it as GANs. These two famous generative frameworks are integrated in a simple yet efficient single-stream architecture that can be trained in a single stage. IntroVAE preserves the advantages of VAEs, such as stable training and nice latent manifold. Unlike most other hybrid models of VAEs and GANs, IntroVAE requires no extra discriminators, because the inference model itself serves as a discriminator to distinguish between the generated and real samples. Experiments demonstrate that our method produces high-resolution photo-realistic images (e.g., CELEBA images at 1024(2)), which are comparable to or better than the state-of-the-art GANs.	[He, Ran] Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China; CASIA, Ctr Res Intelligent Percept & Comp, Beijing, Peoples R China; CASIA, Natl Lab Pattern Recognit, Beijing, Peoples R China; Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences	He, R (corresponding author), Univ Chinese Acad Sci, Sch Artificial Intelligence, Beijing, Peoples R China.	huaibo.huang@cripac.ia.ac.cn; zhihang.li@nlpr.ia.ac.cn; rhe@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn; tnt@nlpr.ia.ac.cn			State Key Development Program [2016YFB1001001]; National Natural Science Foundation of China [61622310, 61427811]	State Key Development Program; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is partially funded by the State Key Development Program (Grant No. 2016YFB1001001) and National Natural Science Foundation of China (Grant No. 61622310, 61427811).	[Anonymous], 2017, ICLR; [Anonymous], 2017, ICLR; [Anonymous], 2014, ICLR; Berthelot D., 2017, BEGAN BOUNDARY EQUIL; Bottou L., 2017, ARXIV170107875STATML; Brock A., 2017, ICLR; Chen X., 2017, ICLR; Dahl R., 2017, ICCV; Darrell T, 2017, ICLR; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Diamos G., 2017, ADV NEURAL INFORM PR, V30, P2966; Dosovitskiy Alexey, 2016, NEURIPS; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Durugkar Ishan, 2017, ICLR; Goodfellow I, 2016, DEEP LEARNING; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Huang HB, 2017, IEEE I CONF COMP VIS, P1698, DOI 10.1109/ICCV.2017.187; Karras Tero, 2018, ICLR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lample Guillaume, 2017, ARXIV170600409; Larsen ABL, 2016, PR MACH LEARN RES, V48; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu MY, 2017, ADV NEUR IN, V30; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Ma Liqian, 2017, P NEUR INF PROC SYST, P405; Makhzani A., 2015, ICLR WORKSH, DOI DOI 10.3389/FPHAR.2020.565644; Odena A, 2017, PR MACH LEARN RES, V70; Radford A., 2016, ICLR; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salimans T., 2016, ADV NEUR IN, P2234; Sonderby CK, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Nguyen TD, 2017, ADV NEUR IN, V30; Ulyanov D., 2018, AAAI; van den Oord A, 2016, PR MACH LEARN RES, V48; Wang T.-C., 2018, CVPR; Yu F., 2015, ARXIVABS150603365 CO; Zhang H., 2017, ICCV; Zhang Han, 2017, ARXIV171010916V2; Zhang Z., 2018, ARXIV180209178; Zhao SS, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3490	45	58	59	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300006
C	Watters, N; Tacchetti, A; Weber, T; Pascanu, R; Battaglia, P; Zoran, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Watters, Nicholas; Tacchetti, Andrea; Weber, Theophane; Pascanu, Razvan; Battaglia, Peter; Zoran, Daniel			Visual Interaction Networks: Learning a Physics Simulator from Video	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					From just a glance, humans can make rich predictions about the future of a wide range of physical systems. On the other hand, modern approaches from engineering, robotics, and graphics are often restricted to narrow domains or require information about the underlying state. We introduce the Visual Interaction Network, a general-purpose model for learning the dynamics of a physical system from raw visual observations. Our model consists of a perceptual front-end based on convolutional neural networks and a dynamics predictor based on interaction networks. Through joint training, the perceptual front-end learns to parse a dynamic visual scene into a set of factored latent object representations. The dynamics predictor learns to roll these states forward in time by computing their interactions, producing a predicted physical trajectory of arbitrary length. We found that from just six input video frames the Visual Interaction Network can generate accurate future trajectories of hundreds of time steps on a wide range of physical systems. Our model can also be applied to scenes with invisible objects, inferring their future states from their effects on the visible objects, and can implicitly infer the unknown mass of objects. This work opens new opportunities for model-based decision-making and planning from raw sensory observations in complex physical environments.	[Watters, Nicholas; Tacchetti, Andrea; Weber, Theophane; Pascanu, Razvan; Battaglia, Peter; Zoran, Daniel] DeepMind, London, England		Watters, N (corresponding author), DeepMind, London, England.	nwatters@google.com; atacchet@google.com; theophane@google.com; razp@google.com; peterbattaglia@google.com; danielzoran@google.com	Jeong, Yongwook/N-7413-2016					Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Battaglia Peter W, 2016, ARXIV161200222; Bhat KS, 2002, LECT NOTES COMPUT SC, V2350, P551; Bhattacharyya Apratim, 2016, ARXIV161108841; Brubaker MA, 2009, IEEE I CONF COMP VIS, P2389, DOI 10.1109/ICCV.2009.5459407; Chang Michael B, 2016, ARXIV161200341; Ehrhardt S., 2017, ARXIV170300247; Fragkiadaki K., 2015, ARXIV151107404; Gerstenberg Tobias, 2012, P 34 CIT; Gerstenberg Tobias, 2014, COGSCI; Grzeszczuk R., 1998, Computer Graphics. Proceedings. SIGGRAPH 98 Conference Proceedings, P9, DOI 10.1145/280814.280816; Hamrick JB, 2016, COGNITION, V157, P61, DOI 10.1016/j.cognition.2016.08.012; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kingma D.P, P 3 INT C LEARNING R; Ladicky L, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2816795.2818129; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lerer A, 2016, PR MACH LEARN RES, V48; Li Wenbin, 2016, ARXIV160400066; Mottaghi R, 2016, PROC CVPR IEEE, P3521, DOI 10.1109/CVPR.2016.383; Mottaghi R, 2016, LECT NOTES COMPUT SC, V9908, P269, DOI 10.1007/978-3-319-46493-0_17; Spelke ES, 2007, DEVELOPMENTAL SCI, V10, P89, DOI 10.1111/j.1467-7687.2007.00569.x; Stewart Russell, 2016, ARXIV160905566; Winograd Terry, 1971, TECHNICAL REPORT; Winston P. H, 1970, MITLCSTR76 MIT; Wu J., 2015, ADV NEURAL INF PROCE, V28, P1, DOI DOI 10.1007/978-3-319-26532-2_15; Wu Jiajun, 2016, PSYCHOL SCI, V13, P89	27	58	59	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404059
C	Kriege, NM; Giscard, PL; Wilson, RC		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kriege, Nils M.; Giscard, Pierre-Louis; Wilson, Richard C.			On Valid Optimal Assignment Kernels and Applications to Graph Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					The success of kernel methods has initiated the design of novel positive semidefinite functions, in particular for structured data. A leading design paradigm for this is the convolution kernel, which decomposes structured objects into their parts and sums over all pairs of parts. Assignment kernels, in contrast, are obtained from an optimal bijection between parts, which can provide a more valid notion of similarity. In general however, optimal assignments yield indefinite functions, which complicates their use in kernel methods. We characterize a class of base kernels used to compare parts that guarantees positive semidefinite optimal assignment kernels. These base kernels give rise to hierarchies from which the optimal assignment kernels are computed in linear time by histogram intersection. We apply these results by developing the Weisfeiler-Lehman optimal assignment kernel for graphs. It provides high classification accuracy on widely-used benchmark data sets improving over the original Weisfeiler-Lehman kernel.	[Kriege, Nils M.] TU Dortmund, Dept Comp Sci, Dortmund, Germany; [Giscard, Pierre-Louis; Wilson, Richard C.] Univ York, Dept Comp Sci, York, N Yorkshire, England	Dortmund University of Technology; University of York - UK	Kriege, NM (corresponding author), TU Dortmund, Dept Comp Sci, Dortmund, Germany.	nils.kriege@tu-dortmund.de; pierre-louis.giscard@york.ac.uk; richard.wilson@york.ac.uk		Kriege, Nils/0000-0003-2645-947X	German Science Foundation (DFG) within the Collaborative Research Center [SFB 876]; Royal Commission for the Exhibition of 1851	German Science Foundation (DFG) within the Collaborative Research Center(German Research Foundation (DFG)); Royal Commission for the Exhibition of 1851	N. M. Kriege is supported by the German Science Foundation (DFG) within the Collaborative Research Center SFB 876 "Providing Information by Resource-Constrained Data Analysis", project A6 "Resource-efficient Graph Mining". P.-L. Giscard is grateful for the financial support provided by the Royal Commission for the Exhibition of 1851.	Bai L, 2015, PR MACH LEARN RES, V37, P30; Barla A, 2003, 2003 INTERNATIONAL CONFERENCE ON IMAGE PROCESSING, VOL 3, PROCEEDINGS, P513; Borgwardt K. M., 2005, 5 IEEE INT C DAT MIN, DOI DOI 10.1109/ICDM.2005.132; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Boughorbel S., 2005, INT C IM P ICIP 2005; Burkard R., 2012, ASSIGNMENT PROBLEMS; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Frohlich H., 2005, P 22 INT C MACH LEAR, P225, DOI DOI 10.1145/1102351.1102380; Gori M, 2005, IEEE T PATTERN ANAL, V27, P1100, DOI 10.1109/TPAMI.2005.138; Grauman K., 2007, NIPS, V19, P505, DOI DOI 10.7551/MITPRESS/7503.003.0068; Grauman K, 2007, J MACH LEARN RES, V8, P725; Haussler D., 1999, CONVOLUTION KERNELS; Ismagilov RS, 1997, MATH NOTES+, V62, P186, DOI 10.1007/BF02355907; Johansson FD, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P467, DOI 10.1145/2783258.2783341; Loosli G., 2015, IEEE T PATTERN ANAL, VPP, P1; Pachauri D., 2013, ADV NEURAL INFORM PR, V26, P1860; Riesen K, 2009, IMAGE VISION COMPUT, V27, P950, DOI 10.1016/j.imavis.2008.04.004; Schiavinato M, 2015, LECT NOTES COMPUT SC, V9370, P146, DOI 10.1007/978-3-319-24261-3_12; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; SWAIN MJ, 1991, INT J COMPUT VISION, V7, P11, DOI 10.1007/BF00130487; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Vert J.P., 2008, CORR; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Yanardag P., 2015, KDD, P1365	24	58	58	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701023
C	Saul, LK; Jordan, MI		Touretzky, DS; Mozer, MC; Hasselmo, ME		Saul, LK; Jordan, MI			Exploiting tractable substructures in intractable networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						MIT,CTR BIOL & COMP LEARNING,CAMBRIDGE,MA 02139	Massachusetts Institute of Technology (MIT)			Jordan, Michael I/C-5253-2013						0	58	60	0	5	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						486	492						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00069
C	Allen-Zhu, Z; Li, YZ; Liang, YY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Allen-Zhu, Zeyuan; Li, Yuanzhi; Liang, Yingyu			Learning and Generalization in Overparameterized Neural Networks, Going Beyond Two Layers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The fundamental learning theory behind neural networks remains largely open. What classes of functions can neural networks actually learn? Why doesn't the trained network overfit when it is overparameterized? In this work, we prove that overparameterized neural networks can learn some notable concept classes, including two and three-layer networks with fewer parameters and smooth activations. Moreover, the learning can be simply done by SGD (stochastic gradient descent) or its variants in polynomial time using polynomially many samples. The sample complexity can also be almost independent of the number of parameters in the network. On the technique side, our analysis goes beyond the so-called NTK (neural tangent kernel) linearization of neural networks in prior works. We establish a new notion of quadratic approximation of the neural network, and connect it to the SGD theory of escaping saddle points.	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA; [Li, Yuanzhi] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Liang, Yingyu] Univ Wisconsin, Madison, WI USA	Carnegie Mellon University; University of Wisconsin System; University of Wisconsin Madison	Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu; yuanzhil@andrew.cmu.edu; yliang@cs.wisc.edu	Li, Yuan/GXV-1310-2022		Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin-Madison; Wisconsin Alumni Research Foundation;  [FA9550-18-1-0166]	Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin-Madison; Wisconsin Alumni Research Foundation; 	This work was supported in part by FA9550-18-1-0166. Y. Liang would also like to acknowledge that support for this research was provided by the Office of the Vice Chancellor for Research and Graduate Education at the University of Wisconsin-Madison with funding from the Wisconsin Alumni Research Foundation.	Allen-Zhu Zeyuan, 2019, NEURIPS; Allen-Zhu Zeyuan, 2019, ICML; Andoni A, 2014, PR MACH LEARN RES, V32, P1908; Arora Sanjeev, 2018, ARXIV180205296; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; BAKSHI A., 2018, ARXIV181101885; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Boob D., 2017, ARXIV171011241; Brutzkus A., 2018, 6 INT C LEARN REPR I; Brutzkus A., 2017, P 34 INT C MACH LEAR, V70, P605; Daniely Amit, 2017, ARXIV PREPRINT ARXIV; Du SS., 2019, P 7 INT C LEARN REPR; Eldan R., 2018, ARXIV180609087; Ge R., 2017, ARXIV PREPRINT ARXIV; Ge Rong, 2019, INT C LEARN REPR; Goel S., 2017, C LEARNING THEORY, P1004; Goel S., 2018, ARXIV170906010V4; Golowich Noah, 2018, P C LEARN THEOR; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Huang JW, 2017, 2017 17TH IEEE INTERNATIONAL CONFERENCE ON COMMUNICATION TECHNOLOGY (ICCT 2017), P598; Jacot Arthur, 2018, ADV NEURAL INFORM PR, P8571; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Kleinberg R, 2018, PR MACH LEARN RES, V80; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Lee J., 2019, ARXIV190206720; Li Yuanzhi, 2017, ARXIV171209203; Liang Percy, 2016, CS229T STAT231 STAT; Liang Yingyu, 2018, ADV NEURAL INFORM PR; Liu DN, 2015, INT CONF MEAS, P798, DOI 10.1109/ICMTMA.2015.197; Livni R., 2014, NIPS, V1, P855; Maurer A, 2016, LECT NOTES ARTIF INT, V9925, P3, DOI 10.1007/978-3-319-46379-7_1; Neyshabur Behnam, 2017, ARXIV170709564; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Siuly S, 2016, HEALTH INFOR SCI, P99, DOI 10.1007/978-3-319-47653-7_6; Soltanolkotabi M., 2017, ARXIV170704926; Soudry D., 2016, ARXIV PREPRINT ARXIV; Spielman Daniel, 2001, P 33 ANN ACM S THEOR, P296, DOI 10.1145/380752.380813; Sridharan Karthik, 2014, MACHINE LEARNING THE; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Tian Yuandong, 2017, ARXIV170300560; Valiant L. G., 1984, COMMUNICATIONS ACM; Vempala S., 2018, ARXIV180502677; Wainwright M., 2015, BASIC TAIL CONCENTRA; Xie B., 2016, ARXIV161103131; Yang G., 2019, ARXIV190204760; Zhai A, 2018, PROBAB THEORY REL, V170, P821, DOI 10.1007/s00440-017-0771-3; Zhang C., 2017, ICLR; Zhang YC, 2017, PR MACH LEARN RES, V54, P83; Zhong  Kai, 2017, ARXIV170603175	53	57	57	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306019
C	Barbu, A; Mayo, D; Alverio, J; Luo, W; Wang, C; Gutfreund, D; Tenenbaum, J; Katz, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Barbu, Andrei; Mayo, David; Alverio, Julian; Luo, William; Wang, Christopher; Gutfreund, Dan; Tenenbaum, Joshua; Katz, Boris			ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet - objects are largely centered and unoccluded - and harder, due to the controls. Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.	[Barbu, Andrei; Mayo, David; Alverio, Julian; Luo, William; Wang, Christopher; Katz, Boris] MIT, CSAIL, Cambridge, MA 02139 USA; [Barbu, Andrei; Mayo, David; Tenenbaum, Joshua; Katz, Boris] CBMM, Cambridge, MA 02139 USA; [Gutfreund, Dan] MIT IBM Watson AI, Cambridge, MA USA; [Tenenbaum, Joshua] MIT, BCS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Barbu, A (corresponding author), MIT, CSAIL, Cambridge, MA 02139 USA.; Barbu, A (corresponding author), CBMM, Cambridge, MA 02139 USA.	abarbu@csail.mit.edu		Wang, Christopher/0000-0002-5349-7861	Center for Brains, Minds and Machines, CBMM; NSF STC [CCF-1231216]; MIT-IBM Brain-Inspired Multimedia Comprehension project; Toyota Research Institute; SystemsThatLearn@CSAILinitiative	Center for Brains, Minds and Machines, CBMM; NSF STC(National Science Foundation (NSF)); MIT-IBM Brain-Inspired Multimedia Comprehension project(International Business Machines (IBM)); Toyota Research Institute; SystemsThatLearn@CSAILinitiative	This work was supported, in part by, the Center for Brains, Minds and Machines, CBMM, NSF STC award CCF-1231216, the MIT-IBM Brain-Inspired Multimedia Comprehension project, the Toyota Research Institute, and the SystemsThatLearn@CSAILinitiative.We would like to thank the members of CBMM, particularly the postdoc group, for many wonderful and productive discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Gu CH, 2018, PROC CVPR IEEE, P6047, DOI 10.1109/CVPR.2018.00633; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huh Minyoung, 2016, ARXIV160808614; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Kornblith Simon, 2018, ARXIV180508974; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kuznetsova Alina, 2018, ARXIV181100982; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu C., 2018, P EUR C COMP VIS ECC, P19, DOI DOI 10.1007/978-3-030-01246-5_2; McCoy R Thomas, 2019, ACL; Myanganbayar Battushig, 2018, P 14 AS C COMP VIS A; Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207; Rohrbach M, 2012, PROC CVPR IEEE, P1194, DOI 10.1109/CVPR.2012.6247801; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sigurdsson GA, 2016, LECT NOTES COMPUT SC, V9905, P510, DOI 10.1007/978-3-319-46448-0_31; Sun C, 2017, IEEE I CONF COMP VIS, P843, DOI 10.1109/ICCV.2017.97; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Wah C., 2011, TECH REP; Zhu Z, 2017, INT JOINT C ART INT; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	25	57	58	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901012
C	Jeong, J; Lee, S; Kim, J; Kwak, N		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Jeong, Jisoo; Lee, Seungeui; Kim, Jeesoo; Kwak, Nojun			Consistency-based Semi-supervised Learning for Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Making a precise annotation in a large dataset is crucial to the performance of object detection. While the object detection task requires a huge number of annotated samples to guarantee its performance, placing bounding boxes for every object in each sample is time-consuming and costs a lot. To alleviate this problem, we propose a Consistency-based Semi-supervised learning method for object Detection (CSD), which is a way of using consistency constraints as a tool for enhancing detection performance by making full use of available unlabeled data. Specifically, the consistency constraint is applied not only for object classification but also for the localization. We also proposed Background Elimination (BE) to avoid the negative effect of the predominant backgrounds on the detection performance. We have evaluated the proposed CSD both in single-stage(2) and two-stage detectors(3) and the results show the effectiveness of our method.	[Jeong, Jisoo; Lee, Seungeui; Kim, Jeesoo; Kwak, Nojun] Seoul Natl Univ, Grad Sch Convergence Sci & Technol, Dept Transdisciplinary Studies, Seoul, South Korea	Seoul National University (SNU)	Jeong, J (corresponding author), Seoul Natl Univ, Grad Sch Convergence Sci & Technol, Dept Transdisciplinary Studies, Seoul, South Korea.	soo3553@snu.ac.kr; seungeui.lee@snu.ac.kr; kimjiss0305@snu.ac.kr; nojunk@snu.ac.kr		Jeong, Jisoo/0000-0003-1154-4532	IITP - Korea government (MSIT) [2019-0-01367]; Next-Generation Information Computing Development Program through the NRF of Korea [2017M3C4A7077582]	IITP - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea); Next-Generation Information Computing Development Program through the NRF of Korea	This work was supported by IITP grant funded by the Korea government (MSIT) (No.2019-0-01367) and Next-Generation Information Computing Development Program through the NRF of Korea (2017M3C4A7077582).	Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Chapelle O., 2006, IEEE T NEURAL NETWOR, V20, P542; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Jie ZQ, 2017, PROC CVPR IEEE, P4294, DOI 10.1109/CVPR.2017.457; Kumar A, 2010, ASIA PACIF MICROWAVE, P1189; Laine Samuli, 2016, ARXIV161002242; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Nguyen Nhu-Van, 2019, INT C COMP VIS THEOR; Oliver A, 2018, ADV NEUR IN, V31; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rosenberg Chuck, 2005, SEMISUPERVISED SELF; Russakovsky O., 2015, PROC CVPR IEEE, DOI [DOI 10.1109/CVPR.2015.7298824, 10.1109/cvpr.2015.7298824]; Shi MJ, 2017, IEEE I CONF COMP VIS, P3401, DOI 10.1109/ICCV.2017.366; Tang YX, 2016, PROC CVPR IEEE, P2119, DOI 10.1109/CVPR.2016.233; Tarvainen Antti, 2017, CORR, Vabs/1703; Wang JJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P971; Wang KZ, 2018, PROC CVPR IEEE, P1605, DOI 10.1109/CVPR.2018.00173; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Yan Ziang, 2017, ARXIV170208740; Zhu Y, 2017, IEEE I CONF COMP VIS, P1859, DOI 10.1109/ICCV.2017.204	24	57	57	2	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902039
C	Xing, C; Rostamzadeh, N; Oreshkin, BN; Pinheiro, PO		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xing, Chen; Rostamzadeh, Negar; Oreshkin, Boris N.; Pinheiro, Pedro O.			Adaptive Cross-Modal Few-shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Metric-based meta-learning techniques have successfully been applied to few-shot classification problems. In this paper, we propose to leverage cross-modal information to enhance metric-based few-shot learning methods. Visual and semantic feature spaces have different structures by definition. For certain concepts, visual features might be richer and more discriminative than text ones. While for others, the inverse might be true. Moreover, when the support from visual information is limited in image classification, semantic representations (learned from unsupervised text corpora) can provide strong prior knowledge and context to help learning. Based on these two intuitions, we propose a mechanism that can adaptively combine information from both modalities according to new image categories to be learned. Through a series of experiments, we show that by this adaptive combination of the two modalities, our model outperforms current uni-modality few-shot learning methods and modality-alignment methods by a large margin on all benchmarks and few-shot scenarios tested. Experiments also show that our model can effectively adjust its focus on the two modalities. The improvement in performance is particularly large when the number of shots is very small.	[Xing, Chen] Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China; [Xing, Chen; Rostamzadeh, Negar; Oreshkin, Boris N.; Pinheiro, Pedro O.] Element AI, Montreal, PQ, Canada	Nankai University	Xing, C (corresponding author), Nankai Univ, Coll Comp Sci, Tianjin, Peoples R China.; Xing, C (corresponding author), Element AI, Montreal, PQ, Canada.	xingchen1113@gmail.com						[Anonymous], 2018, PAMI; Bart E., 2005, CVPR; Bauer M., 2017, ARXIV170600326; Bengio Samy, 1992, OPTIMIZATION SYNAPTI; Bridle John S, 1990, NEUROCOMPUTING, P227, DOI DOI 10.1007/978-3-642-76153-9_28; Dumoulin Vincent, 2018, DISTILL; Fink M., 2005, NIPS; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ADV NEURAL INFORM PR, P9516; Frome Andrea, 2013, NEURIPS; Glorot X., 2011, P 14 INT C ART INT S, P315; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S., 2001, INT C ART NEUR NETW; Jackendoff R, 1987, COGNITION; Jiang X., 2019, P INT C LEARN REPR; Joulin Armand, 2016, ARXIV161203651; Kim Taesup, 2018, ADV NEURAL INFORM PR; Kingma D. P, 2014, ARXIV13126114; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lacoste A., 2017, ARXIV171205016; Lake Brenden, 2011, C COGN SCI SOC, P6; Larochelle H., 2008, AAAI, V1, P3; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Liu Y., 2019, P ICLR; Lu Zhiwu, 2018, ARXIV181008332; Markman EM, 1991, CATEGORIZATION NAMIN; Mishra N., 2018, INT C LEARN REPR, P1; Nichol Alex, 2018, ABS180302999 ARXIV; Norouzi Mohammad, 2014, ICLR; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Ravi S., 2017, INT C LEARN REPR, P12; Ren M., 2018, ICLR; Rezaee M, 2018, IAPR WORKS PATTERN; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Rusu Andrei A, 2019, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J., 1987, THESIS TU MUNICH MUC; Schonfeld E, 2019, PROC CVPR IEEE, P8239, DOI 10.1109/CVPR.2019.00844; Smith Linda, 2005, ARTIF LIFE; Smith LB, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.02124; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Socher Richard, 2013, NEURIPS; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Thrun S., 1998, LIFELONG LEARNING AL; Tsai Y.-H. H., 2017, ICCV; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang YX, 2018, PROC CVPR IEEE, P7278, DOI 10.1109/CVPR.2018.00760; Welinder P., 2010, CNSTR2010001 CALTECH; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhu Yizhe, 2018, CVPR	60	57	58	1	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304081
C	Camburu, OM; Rocktaschel, T; Lukasiewicz, T; Blunsom, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Camburu, Oana-Maria; Rocktaschel, Tim; Lukasiewicz, Thomas; Blunsom, Phil			e-SNLI: Natural Language Inference with Natural Language Explanations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In order for machine learning to garner widespread public adoption, models must be able to provide interpretable and robust explanations for their decisions, as well as learn from human-provided explanations at train time. In this work, we extend the Stanford Natural Language Inference dataset with an additional layer of human-annotated natural language explanations of the entailment relations. We further implement models that incorporate these explanations into their training process and output them at test time. We show how our corpus of explanations, which we call e-SNLI, can be used for various goals, such as obtaining full sentence justifications of a model's decisions, improving universal sentence representations and transferring to out-of-domain NLI datasets. Our dataset(1) thus opens up a range of research directions for using natural language explanations, both for improving models and for asserting their trust.	[Camburu, Oana-Maria; Lukasiewicz, Thomas; Blunsom, Phil] Univ Oxford, Dept Comp Sci, Oxford, England; [Rocktaschel, Tim] UCL, Dept Comp Sci, London, England; [Lukasiewicz, Thomas] Alan Turing Inst, London, England; [Blunsom, Phil] DeepMind, London, England	University of Oxford; University of London; University College London	Camburu, OM (corresponding author), Univ Oxford, Dept Comp Sci, Oxford, England.	oana-maria.camburu@cs.ox.ac.uk; t.rocktaschel@ucl.ac.uk; thomas.lukasiewicz@cs.ox.ac.uk; phil.blunsom@cs.ox.ac.uk			Alan Turing Institute under the EPSRC [EP/N510129/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1. We would also like to thank Jakob Foerster for the valuable discussions.	Alvarez-Melis D., 2017, ABS170701943 CORR; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Chen Q., 2016, ABS160906038 CORR; Chen Q., 2017, ABS171104289 CORR; Conneau Alexis, 2017, ABS170502364 CORR; Das A., 2016, ABS160603556 CORR; Dasgupta Ishita, 2018, EVALUATING COMPOSITI; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gong Yichen, 2017, ABS170904348 CORR; Gururangan Suchin, 2018, P 2018 C N AM CHAPT, P107, DOI [DOI 10.18653/V1/N18-2017, 10.18653/v1/N18-2017]; Hill Felix, 2016, ABS160203483 CORR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jansen P. A., 2018, ABS180203052 CORR; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Ling Wang, 2017, ABS170504146 CORR; Liu P., 2016, ABS160505573 CORR; Marelli M., 2014, SICK CURE EVALUATION; Nie Yixin, 2017, P 2 WORKSH EV VECT S, P41; Park Dong Huk, 2018, ABS180208129 CORR; Ribeiro M.T., 2016, ABS160204938 CORR; Rocktaschel Tim, 2015, ABS150906664 CORR; T_ackstr_om O, 2016, P 2016 C EMP METH NA; Williams Adina, 2017, ARXIV170405426; Xu K, 2015, PR MACH LEARN RES, V37, P2048	27	57	59	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004013
C	Hsieh, JT; Liu, BB; Huang, DA; Fei-Fei, L; Niebles, JC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hsieh, Jun-Ting; Liu, Bingbin; Huang, De-An; Fei-Fei, Li; Niebles, Juan Carlos			Learning to Decompose and Disentangle Representations for Video Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Our goal is to predict future video frames given a sequence of input frames. Despite large amounts of video data, this remains a challenging task because of the high-dimensionality of video frames. We address this challenge by proposing the Decompositional Disentangled Predictive Auto-Encoder (DDPAE), a framework that combines structured probabilistic models and deep networks to automatically (i) decompose the high-dimensional video that we aim to predict into components, and (ii) disentangle each component to have low-dimensional temporal dynamics that are easier to predict. Crucially, with an appropriately specified generative model of video frames, our DDPAE is able to learn both the latent decomposition and disentanglement without explicit supervision. For the Moving MNIST dataset, we show that DDPAE is able to recover the underlying components (individual digits) and disentanglement (appearance and location) as we intuitively would do. We further demonstrate that DDPAE can be applied to the Bouncing Balls dataset involving complex interactions between multiple objects to predict the video frame directly from the pixels and recover physical states without explicit supervision.	[Hsieh, Jun-Ting; Liu, Bingbin; Huang, De-An; Fei-Fei, Li; Niebles, Juan Carlos] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Hsieh, JT (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	junting@stanford.edu; bingbin@stanford.edu; dahuang@cs.stanford.edu; feifeili@cs.stanford.edu; jniebles@cs.stanford.edu	Niebles, Juan Carlos/AAT-5882-2021	Niebles, Juan Carlos/0000-0001-8225-9793	Panasonic; Oppo	Panasonic; Oppo	This work was partially funded by Panasonic and Oppo. We thank our anonymous reviewers, John Emmons, Kuan Fang, Michelle Guo, and Jingwei Ji for their helpful feedback and suggestions.	Alahi A., 2016, CVPR; Amersfoort J.V., 2017, ABS170108435 CORR; [Anonymous], 2016, ARXIV160305106; [Anonymous], 2017, NIPS; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2016, NIPS; [Anonymous], 2014, ICLR; Battaglia P, 2016, NIPS; Chang MB., 2017, 5 INT C LEARN REPR I; Cho K., 2014, P 2014 C EMP METH NA, P1724; Eslami S. M. A., 2016, NIPS 2016, P3; Finn Chelsea, 2016, NIPS; Fragkiadaki K., 2016, ICLR; Gao D. J. R., 2016, ACCV; Ghosh  A., 2017, AAAI; Greff Klaus, 2017, NIPS; Greff Klaus, 2016, NIPS; Gregor K., 2015, ARXIV150204623; Jia X., 2016, NIPS; Kalchbrenner N., 2017, ICML; Karl  M., 2016, ICLR; Kitani K. M, 2012, ECCV; Kosiorek Adam R., 2018, NIPS; Lan T., 2014, EUR C COMP VIS; Liu M.-Y., 2017, ARXIV170704993; Lotter W., 2016, INT C LEARN REPR ICL; Mathieu Michael, 2016, P INT C LEARN REPR I; Oh J., 2015, NIPS; Oliu Marc, 2018, ECCV; Patraucean Viorica, 2015, ARXIV151106309; Paxton C., 2018, ARXIV180400062; Ranzato MarcAurelio, 2014, ARXIV14126604; Soran B., 2015, ICCV; Srivastava Nitish, 2015, ICML; Sutskever I., 2014, NEURIPS; Tulyakov  S., 2017, ARXIV171111566; Villegas R., 2017, P 34 INT C MACH LEAR; Villegas Ruben, 2017, ICLR; Vondrick C, 2017, CVPR; Walker J., 2017, ICCV; Walker Jacob, 2016, P EUR C COMP VIS ECC; Xingjian S., 2015, NIPS; Xue T., 2016, NIPS; Zhou T., 2016, ECCV	45	57	57	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300048
C	Tang, HL; Gan, SD; Zhang, C; Zhang, T; Liu, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tang, Hanlin; Gan, Shaoduo; Zhang, Ce; Zhang, Tong; Liu, Ji			Communication Compression for Decentralized Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Optimizing distributed learning systems is an art of balancing between computation and communication. There have been two lines of research that try to deal with slower networks: communication compression for low bandwidth networks, and decentralization for high latency networks. In this paper, We explore a natural question: can the combination of both techniques lead to a system that is robust to both bandwidth and latency? Although the system implication of such combination is trivial, the underlying theoretical principle and algorithm design is challenging: unlike centralized algorithms, simply compressing exchanged information, even in an unbiased stochastic way, within the decentralized network would accumulate the error and fail to converge. In this paper, we develop a framework of compressed, decentralized training and propose two different strategies, which we call extrapolation compression and difference compression. We analyze both algorithms and prove both converge at the rate of O(1/root nT) where n is the number of workers and T is the number of iterations, matching the convergence rate for full precision, centralized training. We validate our algorithms and find that our proposed algorithm outperforms the best of merely decentralized and merely quantized algorithm significantly for networks with both high latency and low bandwidth.	[Tang, Hanlin; Liu, Ji] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; [Gan, Shaoduo; Zhang, Ce] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Zhang, Tong; Liu, Ji] Tencent AI Lab, Bellevue, WA USA	University of Rochester; Swiss Federal Institutes of Technology Domain; ETH Zurich	Tang, HL (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.	htang14@ur.rochester.edu; sgan@inf.ethz.ch; ce.zhang@inf.ethz.ch; tongzhang@tongzhang-ml.org; ji.liu.uwisc@gmail.com	Zhang, Tong/HGC-1090-2022	Zhang, Ce/0000-0002-8105-7505	NSF [CCF1718513]; IBM; NEC; Mercedes-Benz Research AMP; Development North America; Oracle Labs; Swisscom; Zurich Insurance; Chinese Scholarship Council; Department of Computer Science at ETH Zurich	NSF(National Science Foundation (NSF)); IBM(International Business Machines (IBM)); NEC; Mercedes-Benz Research AMP; Development North America; Oracle Labs; Swisscom; Zurich Insurance; Chinese Scholarship Council(China Scholarship Council); Department of Computer Science at ETH Zurich	Hanlin Tang and Ji Liu are in part supported by NSF CCF1718513, IBM faculty award, and NEC fellowship. CZ and the DS3Lab gratefully acknowledge the support from Mercedes-Benz Research & Development North America, Oracle Labs, Swisscom, Zurich Insurance, Chinese Scholarship Council, and the Department of Computer Science at ETH Zurich.	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Alistarh D., 2017, NIPS; Alistarh D, 2017, ADV NEUR IN, V30; [Anonymous], 2016, ABS161107555 CORR; [Anonymous], 2015, ARXIV151201274; BACH F., 2011, ADV NEURAL INFORM PR, P451; Bottou Leon, 2010, P INT C COMP STAT CO; Boyd S, 2006, IEEE T INFORM THEORY, V52, P2508, DOI 10.1109/TIT.2006.874516; De Sa C, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P561, DOI 10.1145/3079856.3080248; Dekel O, 2012, J MACH LEARN RES, V13, P165; Dobbe R., 2017, ADV NEURAL INFORM PR, P2945; Drumond M, 2018, ADV NEUR IN, V31; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He L., 2018, P ADV NEUR INF PROC, P4536; Kashyap A, 2007, AUTOMATICA, V43, P1192, DOI 10.1016/j.automatica.2007.01.002; Lan G., 2017, COMMUNICATION EFFICI, V01; Lavaei J, 2012, IEEE T AUTOMAT CONTR, V57, P19, DOI 10.1109/TAC.2011.2160593; Li JL, 2017, INT CONF MACH LEARN, P35; Li Z., 2017, ARXIV170407807; Lian X., 2017, ASYNCHRONOUS DECENTR, V10; Lian X., 2017, CAN DECENTRALIZED AL, V05; Lin T., 2018, CORR; Mhamdi E. M. E., 2017, TECHNICAL REPORT; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P2506, DOI 10.1109/TAC.2009.2031203; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Omidshafiei S., 2017, ARXIV170306182; Recht B., 2011, ADV NEURAL INFORM PR, V24, P693; Seide F, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2135, DOI 10.1145/2939672.2945397; Shi W, 2015, IEEE T SIGNAL PROCES, V63, P6013, DOI 10.1109/TSP.2015.2461520; Sirb B, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P76, DOI 10.1109/BigData.2016.7840591; Stich S. U., 2018, P 32 INT C NEUR INF, P4452; Stich S. U., 2018, ARXIV180500982; Suresh AT, 2017, PR MACH LEARN RES, V70; Wangni J., 2017, ARXIV171009854; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zhang WP, 2017, PR MACH LEARN RES, V70; Zhao LY, 2017, INT J HYPERTHER, V33, P343, DOI 10.1080/02656736.2016.1255918; Zheng S., 2016, ARXIV160908326CSLG; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	44	57	56	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002022
C	Zheng, X; Aragam, B; Ravikumar, P; Xing, EP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zheng, Xun; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.			DAGs with NO TEARS: Continuous Optimization for Structure Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BAYESIAN NETWORKS; MAXIMUM-LIKELIHOOD; MODEL; SELECTION	Estimating the structure of directed acyclic graphs (DAGs, also known as Bayesian networks) is a challenging problem since the search space of DAGs is combinatorial and scales superexponentially with the number of nodes. Existing approaches rely on various local heuristics for enforcing the acyclicity constraint. In this paper, we introduce a fundamentally different strategy: we formulate the structure learning problem as a purely continuous optimization problem over real matrices that avoids this combinatorial constraint entirely. This is achieved by a novel characterization of acyclicity that is not only smooth but also exact. The resulting problem can be efficiently solved by standard numerical algorithms, which also makes implementation effortless. The proposed method outperforms existing ones, without imposing any structural assumptions on the graph such as bounded treewidth or in-degree.	[Zheng, Xun; Aragam, Bryon; Ravikumar, Pradeep; Xing, Eric P.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Xing, Eric P.] Petuum Inc, Pittsburgh, PA USA	Carnegie Mellon University	Zheng, X (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	xunzheng@cs.cmu.edu; naragam@cs.cmu.edu; pradeepr@cs.cmu.edu; epxing@cs.cmu.edu			NSF [IIS-1149803, IIS-1664720, IIS1563887]; NIH [R01GM114311, P30DA035778]; Dept of Health [BD4BH4100070287]; AFRL/DARPA [FA87501720152]	NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Dept of Health; AFRL/DARPA	We thank the anonymous reviewers for valuable feedback. P.R. acknowledges the support of NSF via IIS-1149803, IIS-1664720. E.X. and B.A. acknowledge the support of NIH R01GM114311, P30DA035778. X.Z. acknowledges the support of Dept of Health BD4BH4100070287, NSF IIS1563887, AFRL/DARPA FA87501720152.	Al-Mohy Awad H., 2009, SIAM J MATRIX ANAL A; Aragam B, 2016, ARXIV151108963; Aragam B, 2015, J MACH LEARN RES, V16, P2273; Banerjee O, 2008, J MACH LEARN RES, V9, P485; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Bouckaert R. R., 1993, Symbolic and Quantitative Approaches to Reasoning and Uncertainty. European Conference ECSQARU '93 Proceedings, P41, DOI 10.1007/BFb0028180; Byrd R. H., 1995, SIAM J SCI COMPUTING; Chickering D. M., 2003, Journal of Machine Learning Research, V3, P507, DOI 10.1162/153244303321897717; Chickering D. M., 1996, LEARNING FROM DATA; Chickering DM, 2004, J MACH LEARN RES, V5, P1287; Chickering DM, 1997, MACH LEARN, V29, P181, DOI 10.1023/A:1007469629108; Cussens J., 2011, P 27 C UNC ART INT; Cussens J, 2017, MATH PROGRAM, V164, P285, DOI 10.1007/s10107-016-1087-2; Ellis Byron, 2008, J AM STAT ASS, V103; Friedman J, 2008, BIOSTATISTICS, V9, P432, DOI 10.1093/biostatistics/kxm045; Fu F, 2013, J AM STAT ASSOC, V108, P288, DOI 10.1080/01621459.2012.754359; Gamez JA, 2011, DATA MIN KNOWL DISC, V22, P106, DOI 10.1007/s10618-010-0178-6; Garnett, 2016, ADV NEURAL INFORM PR, P1462; Gu JY, 2019, STAT COMPUT, V29, P161, DOI 10.1007/s11222-018-9801-y; Harary Frank, 1971, MATEMATICKY CASOPIS; HECKERMAN D, 1995, MACH LEARN, V20, P197, DOI 10.1023/A:1022623210503; Hsieh CJ, 2014, J MACH LEARN RES, V15, P2911; Koller D., 2009, PROBABILISTIC GRAPHI; Kuipers J., 2014, ANN STAT, V42, P1689; Loh PL, 2014, J MACH LEARN RES, V15, P3065; Nemirovski A.i., 1999, OPTIMIZATION 2 STAND; Nesterov Yuri, 2005, MATH PROGRAMMING; Niinimaki T, 2016, J MACH LEARN RES, V17; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Ott Sascha, 2003, Genome Inform, V14, P124; Park Young Woong, 2017, J MACHINE LEARNING R; Ramsey J, 2016, INT J DATA SCI ANAL, P1; Robinson Robert W., 1977, COMBINATORIAL MATH; Sachs K, 2005, SCIENCE, V308, P523, DOI 10.1126/science.1105809; Scanagatta M., 2015, ADV NEURAL INFORM PR, P1864; Schmidt M., 2007, AAAI, V2, P1278; SCHMIDT M., 2009, ARTIF INTELL, P456; Shimizu S, 2006, J MACH LEARN RES, V7, P2003; Silander T., 2006, P 22 C UNC ART INT; Singh A. P., 2005, FINDING OPTIMAL BAYE; Spirtes P., 1991, Social Science Computer Review, V9, P62, DOI 10.1177/089443939100900106; Spirtes P., 2000, CAUSATION PREDICTION, V81; Teyssier Marc, 2005, UNCERTAINTY ARTIFICA; Tsamardinos I, 2006, MACH LEARN, V65, P31, DOI 10.1007/s10994-006-6889-7; Van de Geer S, 2013, ANN STAT, V41, P536, DOI 10.1214/13-AOS1085; Wang XY, 2016, PR MACH LEARN RES, V48; Xiang J., 2013, PROC 26 INT C ADV NE, P2418; Yuan M, 2007, BIOMETRIKA, V94, P19, DOI 10.1093/biomet/asm018; Zhang B, 2013, CELL, V153, P707, DOI 10.1016/j.cell.2013.03.030; Zhong K., 2014, ADV NEURAL INFORM PR, P2375; Zhou Q, 2011, J AM STAT ASSOC, V106, P1317, DOI 10.1198/jasa.2011.ap10346; Zhou S, 2009, P 23 ANN C NEUR INF, V22, P2304	52	57	58	7	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004007
C	Calmon, FP; Wei, D; Vinzamuri, B; Ramamurthy, KN; Varshney, KR		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Calmon, Flavio P.; Wei, Dennis; Vinzamuri, Bhanukiran; Ramamurthy, Karthikeyan Natesan; Varshney, Kush R.			Optimized Pre-Processing for Discrimination Prevention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Non-discrimination is a recognized objective in algorithmic decision making. In this paper, we introduce a novel probabilistic formulation of data pre-processing for reducing discrimination. We propose a convex optimization for learning a data transformation with three goals: controlling discrimination, limiting distortion in individual data samples, and preserving utility. We characterize the impact of limited sample size in accomplishing this objective. Two instances of the proposed optimization are applied to datasets, including one on real-world criminal recidivism. Results show that discrimination can be greatly reduced at a small cost in classification accuracy.	[Calmon, Flavio P.] Harvard Univ, Cambridge, MA 02138 USA; [Wei, Dennis; Vinzamuri, Bhanukiran; Ramamurthy, Karthikeyan Natesan; Varshney, Kush R.] IBM Res AI, Yorktown Hts, NY USA	Harvard University	Calmon, FP (corresponding author), Harvard Univ, Cambridge, MA 02138 USA.	flavio@seas.harvard.edu; dwei@us.ibm.com; bhanu.vinzamuri@ibm.com; knatesa@us.ibm.com; krvarshn@us.ibm.com						[Anonymous], ARXIV170306856; [Anonymous], 2012, P 27 ANN ACM S APPL; Calders Toon, 2013, DISCRIMINATION PRIVA, V3, P43, DOI [DOI 10.1007/978-3-642-30487-3_3, 10.1007/978-3-642-30487-3_3]; Chouldechova A, 2016, ARXIV161007524; Corbett-Davies Sam, 2017, ARXIV170108230; Diamond S, 2016, J MACH LEARN RES, V17; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Fish B., 2016, P 2016 SIAM INT C DA, P144; Friedler S.A., 2016, IM POSSIBILITY FAIRN; Hajian S, 2013, THESIS; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Johnson K.D., 2016, ARXIV160800528; Kamiran F, 2012, KNOWL INF SYST, V33, P1, DOI 10.1007/s10115-011-0463-8; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; Kleinberg Jon, 2017, INNOVATIONS THEORETI; Li N, 2007, INT CONF NANO MICRO, P692, DOI 10.1109/icde.2007.367856; Lichman M., 2013, UCI MACHINE LEARNING; Pearl J, 2014, AM STAT, V68, P8, DOI 10.1080/00031305.2014.876829; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; ProPublica, COMPAS REC RISK SCOR; Ruggieri S, 2014, TRANS DATA PRIV, V7, P99; T. U. EEOC, 1979, UN GUID EMPL SEL PRO; Zafar Muhammad Bilal, 2016, ARXIV161008452; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang Z, 2016, P NIPS WORKSH INT MA; Zliobaite I., 2011, Proceedings of the 2011 IEEE 11th International Conference on Data Mining (ICDM 2011), P992, DOI 10.1109/ICDM.2011.72	28	57	57	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404007
C	Xie, QZ; Dai, ZH; Du, YL; Hovy, E; Neubig, G		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xie, Qizhe; Dai, Zihang; Du, Yulun; Hovy, Eduard; Neubig, Graham			Controllable Invariance through Adversarial Feature Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.	[Xie, Qizhe; Dai, Zihang; Du, Yulun; Hovy, Eduard; Neubig, Graham] Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Xie, QZ (corresponding author), Carnegie Mellon Univ, Language Technol Inst, Pittsburgh, PA 15213 USA.	qizhex@cs.cmu.edu; dzihang@cs.cmu.edu; yulund@cs.cmu.edu; hovy@cs.cmu.edu; gneubig@cs.cmu.edu	Jeong, Yongwook/N-7413-2016	Hovy, Eduard/0000-0002-3270-7903	DARPA [FA8750-12-2-0342]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We thank Shi Feng, Di Wang and Zhilin Yang for insightful discussions. This research was supported in part by DARPA grant FA8750-12-2-0342 funded under the DEFT program.	Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Cettolo Mauro, 2012, P 16 C EUR ASS MACH, P261; Chen Xilun, 2016, ARXIV160601614; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Frank A., 2010, UCI MACHINE LEARNING; Ganin Y, 2016, J MACH LEARN RES, V17; Ganin Yaroslav, 2015, ICML; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Johnson M., 2017, T ASSOC COMPUT LING, DOI 10.1162/tacl_a_00065; Kingma D.P, P 3 INT C LEARNING R; Klein G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P67, DOI 10.18653/v1/P17-4012; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Yujia, 2014, ARXIV14125244; Louizos C., 2016, 4 INT C LEARN REPR; Louppe G., 2016, ARXIV161101046; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Saminger-Platz S., 2017, P INT C LEARN REPR I; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tenenbaum J., 1997, NIPS; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xu Ruochen, 2017, ACL; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414	31	57	59	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400056
C	Nadeau, C; Bengio, Y		Solla, SA; Leen, TK; Muller, KR		Nadeau, C; Bengio, Y			Inference for the generalization error	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					In order to to compare learning algorithms, experimental results reported in the machine learning litterature often use statistical tests of significance. Unfortunately, most of these tests do not take into account the variability due to the choice of training set. We perform a theoretical investigation of the variance of the cross-validation estimate of the generalization error that takes into account the variability due to the choice of training sets. This allows us to propose two new ways to estimate this variance. We show, via simulations, that these new statistics perform well relative to the statistics considered by Dietterich (Dietterich, 1998).	CIRANO, Montreal, PQ H3A 2A5, Canada	Universite de Montreal	Nadeau, C (corresponding author), CIRANO, 2020 Univ, Montreal, PQ H3A 2A5, Canada.	jcnadeau@altavista.net; bengioy@iro.umontreal.ca						Breiman L, 1996, ANN STAT, V24, P2350; Dietterich TG, 1998, NEURAL COMPUT, V10, P1895, DOI 10.1162/089976698300017197; HINTON G, 1995, ASSESSING LEARNING P; NADEAU C, 1999, INFERENCE GEN ERROR	4	57	57	0	4	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						307	313						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700044
C	Rao, RPN; Sejnowski, TJ		Solla, SA; Leen, TK; Muller, KR		Rao, RPN; Sejnowski, TJ			Predictive sequence learning in recurrent neocortical circuits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				DIRECTION SELECTIVITY; VISUAL-CORTEX; SIMPLE CELLS; POTENTIATION; NEURONS; MODEL; HIPPOCAMPUS; COINCIDENCE; DEPRESSION; MECHANISMS	Neocortical circuits are dominated by massive excitatory feedback: more than eighty percent of the synapses made by excitatory cortical neurons are onto other excitatory cortical neurons. Why is there such massive recurrent excitation in the neocortex and what is its role in cortical computation? Recent neurophysiological experiments have shown that the plasticity of recurrent neocortical synapses is governed by a temporally asymmetric Hebbian learning rule. We describe how such a rule may allow the cortex to modify recurrent synapses for prediction of input sequences. The goal is to predict the next cortical input from the recent past based on previous experience of similar input sequences. We show that a temporal difference learning rule for prediction used in conjunction with dendritic back-propagating action potentials reproduces the temporally asymmetric Hebbian plasticity observed physiologically. Biophysical simulations demonstrate that a network of cortical neurons can learn to predict moving stimuli and develop direction selective responses as a consequence of learning. The space-time response properties of model neurons are shown to be similar to those of direction selective cells in alert monkey V1.	Salk Inst, Computat Neurobiol Lab, La Jolla, CA 92037 USA	Salk Institute	Rao, RPN (corresponding author), Salk Inst, Computat Neurobiol Lab, La Jolla, CA 92037 USA.		Sejnowski, Terrence/AAV-5558-2021	Rao, Rajesh P. N./0000-0003-0682-8952				Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406; Abbott LF, 1999, ADV NEUR IN, V11, P69; Barlow H, 1998, PERCEPTION, V27, P885, DOI 10.1068/p270885; Berry MJ, 1999, NATURE, V398, P334, DOI 10.1038/18678; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Chance FS, 1999, NAT NEUROSCI, V2, P277, DOI 10.1038/6381; DEBANNE D, 1994, P NATL ACAD SCI USA, V91, P1148, DOI 10.1073/pnas.91.3.1148; DEXTEXHE A, 1998, METHODS NEURONAL MOD; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; Gerstner W, 1997, J COMPUT NEUROSCI, V4, P79, DOI 10.1023/A:1008820728122; Kempter R, 1999, ADV NEUR IN, V11, P125; LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6; Livingstone MS, 1998, NEURON, V20, P509, DOI 10.1016/S0896-6273(00)80991-5; Maex R, 1996, J NEUROPHYSIOL, V75, P1515, DOI 10.1152/jn.1996.75.4.1515; Mainen ZF, 1996, NATURE, V382, P363, DOI 10.1038/382363a0; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Mehta MR, 1997, P NATL ACAD SCI USA, V94, P8918, DOI 10.1073/pnas.94.16.8918; MINAI AA, 1993, P 1993 INNS WORLD C, V2, P505; Mineiro P, 1998, NEURAL COMPUT, V10, P353, DOI 10.1162/089976698300017791; MONTAGUE PR, 1995, NATURE, V377, P725, DOI 10.1038/377725a0; MONTAGUE PR, 1994, LEARN MEMORY, V1, P1; Rao RPN, 1999, NAT NEUROSCI, V2, P79, DOI 10.1038/4580; Rao RPN, 1997, NEURAL COMPUT, V9, P721, DOI 10.1162/neco.1997.9.4.721; Schultz W, 1997, SCIENCE, V275, P1593, DOI 10.1126/science.275.5306.1593; SUAREZ H, 1995, J NEUROSCI, V15, P6700; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton R.S., 1990, LEARNING COMPUTATION; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	29	57	57	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						164	170						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700024
C	Caruana, R; Baluja, S; Mitchell, T		Touretzky, DS; Mozer, MC; Hasselmo, ME		Caruana, R; Baluja, S; Mitchell, T			Using the future to ''sort out'' the present: Rankprop and multitask learning for medical risk evaluation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						CARNEGIE MELLON UNIV,SCH COMP SCI,PITTSBURGH,PA 15213	Carnegie Mellon University									0	57	62	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						959	965						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00135
C	Wang, XM; Jin, Y; Long, MS; Wang, JM; Jordan, MI		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Ximei; Jin, Ying; Long, Mingsheng; Wang, Jianmin; Jordan, Michael I.			Transferable Normalization: Towards Improving Transferability of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep neural networks (DNNs) excel at learning representations when trained on large-scale datasets. Pre-trained DNNs also show strong transferability when fine-tuned to other labeled datasets. However, such transferability becomes weak when the target dataset is fully unlabeled as in Unsupervised Domain Adaptation (UDA). We envision that the loss of transferability mainly stems from the intrinsic limitation of the architecture design of DNNs. In this paper, we delve into the components of DNN architectures and propose Transferable Normalization (TransNorm) in place of existing normalization techniques. TransNorm is an end-to-end trainable layer to make DNNs more transferable across domains. As a general method, TransNorm can be easily applied to various deep neural networks and domain adaption methods, without introducing any extra hyper-parameters or learnable parameters. Empirical results justify that TransNorm not only improves classification accuracies but also accelerates convergence for mainstream DNN-based domain adaptation methods.	[Long, Mingsheng] Tsinghua Univ, BNRist, Sch Software, Beijing, Peoples R China; Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China; Natl Engn Lab Big Data Software, Beijing, Peoples R China; [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Tsinghua University; Tsinghua University; University of California System; University of California Berkeley	Long, MS (corresponding author), Tsinghua Univ, BNRist, Sch Software, Beijing, Peoples R China.	wxm17@mails.tsinghua.edu.cn; jiny18@mails.tsinghua.edu.cn; mingsheng@tsinghua.edu.cn; jimwang@tsinghua.edu.cn; jordan@cs.berkeley.edu	wang, jian/GVS-0711-2022		National Key R&D Program of China [2017YFC1502003]; Natural Science Foundation of China [61772299, 71690231]	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by the National Key R&D Program of China (2017YFC1502003) and the Natural Science Foundation of China (61772299 and 71690231).	[Anonymous], 1908, BIOMETRIKA, V6, P1; Ba L. J., 2016, CORRABS160706450; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Carlucci Fabio Maria, 2017, ICCV; Donahue J, 2014, PR MACH LEARN RES, V32; Ganin Y, 2016, J MACH LEARN RES, V17; Gong B., 2013, P INT C MACH LEARN J, V711, P712; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hoffman J, 2018, PR MACH LEARN RES, V80; Huang Jiayuan, 2006, NEURIPS; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kornblith S, 2019, PROC CVPR IEEE, P2656, DOI 10.1109/CVPR.2019.00277; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Li YH, 2018, PATTERN RECOGN, V80, P109, DOI 10.1016/j.patcog.2018.03.005; Long M., 2018, IEEE T PATTERN ANAL, P1; Long MS, 2018, ADV NEUR IN, V31; Long MS, 2017, PR MACH LEARN RES, V70; Long MS, 2016, ADV NEUR IN, V29; Long Mingsheng, 2015, ARXIV150202791; Mirza M., 2014, ARXIV; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934; Peng Xingchao, 2017, VISDA VISUAL DOMAIN; Pinheiro Pedro O., 2018, CVPR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; SAITO K, 2018, CVPR; Sankaranarayanan S., 2018, CVPR; Santurkar S, 2018, ADV NEUR IN, V31; Strathmann H., 2012, P ADV NEUR INF PROC, P1205, DOI DOI 10.5555/2999134.2999269; Tzeng E., 2014, ARXIV PREPRINT ARXIV; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Venkateswara H, 2017, PROC CVPR IEEE, P5385, DOI 10.1109/CVPR.2017.572; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P93; Wang XM, 2019, AAAI CONF ARTIF INTE, P5345; Wiesler Simon, 2011, NEURIPS; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Yosinski J, 2014, ADV NEUR IN, V27; You KC, 2019, PR MACH LEARN RES, V97; Zhang YC, 2019, PR MACH LEARN RES, V97; Zhao H., 2019, ICML	47	56	57	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301089
C	Abu-El-Haija, S; Perozzi, B; Al-Rfou, R; Alemi, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Abu-El-Haija, Sami; Perozzi, Bryan; Al-Rfou, Rami; Alemi, Alex			Watch Your Step: Learning Node Embeddings via Graph Attention	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Graph embedding methods represent nodes in a continuous vector space, preserving different types of relational information from the graph. There are many hyperparameters to these methods (e.g. the length of a random walk) which have to be manually tuned for every graph. In this paper, we replace previously fixed hyperparameters with trainable ones that we automatically learn via backpropagation. In particular, we propose a novel attention model on the power series of the transition matrix, which guides the random walk to optimize an upstream objective. Unlike previous approaches to attention models, the method that we propose utilizes attention parameters exclusively on the data itself (e.g. on the random walk), and are not used by the model for inference. We experiment on link prediction tasks, as we aim to produce embeddings that best-preserve the graph structure, generalizing to unseen information. We improve state-of-the-art results on a comprehensive suite of real-world graph datasets including social, collaboration, and biological networks, where we observe that our graph attention model can reduce the error by up to 20%-40%. We show that our automatically-learned attention parameters can vary significantly per graph, and correspond to the optimal choice of hyperparameter if we manually tune existing methods.	[Abu-El-Haija, Sami] Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA; [Perozzi, Bryan] Google AI, New York, NY USA; [Abu-El-Haija, Sami; Al-Rfou, Rami; Alemi, Alex] Google AI, Mountain View, CA USA	University of Southern California	Abu-El-Haija, S (corresponding author), Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.; Abu-El-Haija, S (corresponding author), Google AI, Mountain View, CA USA.	haija@isi.edu; bperozzi@acm.org; rmyeid@google.com; alemi@google.com						Abu-El-Haija S., 2017, CIKM; Abu-El-Haija Sami, 2017, PROPORTIONATE GRADIE; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Cao SS, 2016, AAAI CONF ARTIF INTE, P1145; Chen H., 2018, ARXIV PREPRINT ARXIV; Chen HC, 2018, AAAI CONF ARTIF INTE, P2127; Dai HJ, 2016, PR MACH LEARN RES, V48; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Duvenaud David K, 2015, P NIPS; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Gupta  M., 2016, P 25 INT C COMP WORL; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hamilton WL, 2017, REPRESENTATION LEARN; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Levy O., 2015, T ASSOC COMPUT LING, V3, P211, DOI [10.1162/tacl_a_00134, DOI 10.1162/TACL_A_00134]; Li Yujia, 2016, P INT C LEARN REPR I, P2; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Luo Y., 2015, P 2015 C EMP METH NA, P1656, DOI [10.18653/v1/D15-1191, DOI 10.18653/V1/D15-1191]; Mikolov T., 2013, ARXIV; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Niepert M, 2016, PR MACH LEARN RES, V48; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Perozzi B., 2017, P 2017 IEEEACM INT C, P258; Ramanathan V, 2016, PROC CVPR IEEE, P3043, DOI 10.1109/CVPR.2016.332; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Stark C, 2006, NUCLEIC ACIDS RES, V34, pD535, DOI 10.1093/nar/gkj109; Tong HH, 2006, IEEE DATA MINING, P613; Velickovic P., 2018, P INT C LEARN REPR; Wang DX, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1225, DOI 10.1145/2939672.2939753; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Yang Z, 2016, PR MACH LEARN RES, V48	38	56	59	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003071
C	Amos, B; Rodriguez, IDJ; Sacks, J; Boots, B; Kolter, JZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Amos, Brandon; Rodriguez, Ivan Dario Jimenez; Sacks, Jacob; Boots, Byron; Kolter, J. Zico			Differentiable MPC for End-to-end Planning and Control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PATH-INTEGRAL CONTROL	We present foundations for using Model Predictive Control (MPC) as a differentiable policy class for reinforcement learning in continuous state and action spaces. This provides one way of leveraging and combining the advantages of model-free and model-based approaches. Specifically, we differentiate through MPC by using the KKT conditions of the convex approximation at a fixed point of the controller. Using this strategy, we are able to learn the cost and dynamics of a controller via end-to-end learning. Our experiments focus on imitation learning in the pendulum and cartpole domains, where we learn the cost and dynamics terms of an MPC policy class. We show that our MPC policies are significantly more data-efficient than a generic neural network and that our method is superior to traditional system identification in a setting where the expert is unrealizable.	[Amos, Brandon; Kolter, J. Zico] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Rodriguez, Ivan Dario Jimenez; Sacks, Jacob; Boots, Byron] Georgia Tech, Atlanta, GA USA; [Kolter, J. Zico] Bosch Ctr AI, Renningen, Germany	Carnegie Mellon University; University System of Georgia; Georgia Institute of Technology	Amos, B (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.				National Science Foundation Graduate Research Fellowship Program [DGE1252522]	National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF))	BA is supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1252522. We thank Alfredo Canziani, Shane Gu, and Yuval Tassa for insightful discussions.	Abbeel P., 2006, MACHINE LEARNING P 2, P1, DOI DOI 10.1145/1143844.1143845; Alexis Kostas, 2011, 2011 19th Mediterranean Conference on Control & Automation (MED 2011), P1247; Amos B., 2017, P INT C MACH LEARN; [Anonymous], 2017, ARXIV170802596; [Anonymous], 2017, ARXIV170303078; Ba J., 2017, P 3 INT C LEARN REPR; Bagnell, 2016, INT S EXP ROB, P703; Bansal S., 2017, ARXIV170903153; Boedecker Joschika, 2014, IEEE S AD DYN PROGR; Bouffard P, 2012, IEEE INT CONF ROBOT, P279, DOI 10.1109/ICRA.2012.6225035; Boyd Stephen, 2008, STANFORD EE 363 LINE; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Erez  T., 2012, INT C INT ROB SYST; Farquhar G., 2017, ARXIV171011417; Gonzalez R, 2011, ROBOT AUTON SYST, V59, P711, DOI 10.1016/j.robot.2011.05.006; Gu Shixiang, 2016, ARXIV161102247; Gu Shixiang, 2016, P INT C MACH LEARN; Heess N., 2015, NIPS; Kamel M, 2015, IEEE INTL CONF CONTR, P1160, DOI 10.1109/CCA.2015.7320769; Karkus P., 2017, ADV NEURAL INFORM PR, P4697; Lenz I, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Levine S, 2016, J MACH LEARN RES, V17; Levine S, 2014, ADV NEUR IN, V27; Levine Sergey, 2017, 294112 BERK CS; Li W., 2004, ITERATIVE LINEAR QUA; Lillicrap TP, 2016, 4 INT C LEARN REPR; Liniger A, 2015, OPTIM CONTR APPL MET, V36, P628, DOI 10.1002/oca.2123; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V., 2013, PLAYING ATARI DEEP R, P1; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neunert  M., 2016, ICRA; Oh J, 2017, ADV NEUR IN, V30; Oh Junhyuk, 2016, P 33 INT C MACH LEAR; Okada M., 2017, ARXIV170609597; Pascanu R., 2017, ARXIV170706170; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pathak Deepak, 2018, ARXIV180408606; Pereira M., 2018, ARXIV180205803; Pong V., 2018, ARXIV180209081; Racaniere S., 2017, P 31 INT C NEUR INF, P5694; Schneider JG, 1997, ADV NEUR IN, V9, P1047; Schulman J., 2016, INT C LEARN REPR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D., 2016, ARXIV161208810; Srinivas A, 2018, PR MACH LEARN RES, V80; Sun L., 2017, ARXIV170702515; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Tamar Aviv, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P336, DOI 10.1109/ICRA.2017.7989043; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Tassa Y, 2014, IEEE INT CONF ROBOT, P1168, DOI 10.1109/ICRA.2014.6907001; Theodorou EA, 2010, J MACH LEARN RES, V11, P3137; Tieleman Tijmen, 2012, LECT 65 RMSPROP DIVI, V4; Watter Manuel, 2015, ADV NEURAL INFORM PR, V2, P2746; Williams G, 2017, J GUID CONTROL DYNAM, V40, P344, DOI 10.2514/1.G001921; Williams G, 2016, IEEE INT CONF ROBOT, P1433, DOI 10.1109/ICRA.2016.7487277; Xie Zhaoming, 2017, INT C ROB AUT ICRA	56	56	56	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002080
C	Manhaeve, R; Dumancic, S; Kimmig, A; Demeester, T; De Raedt, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Manhaeve, Robin; Dumancic, Sebastijan; Kimmig, Angelika; Demeester, Thomas; De Raedt, Luc			DeepProbLog: Neural Probabilistic Logic Programming	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce DeepProbLog, a probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques can be adapted for the new language. Our experiments demonstrate that DeepProbLog supports (i) both symbolic and sub-symbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples.	[Manhaeve, Robin; Dumancic, Sebastijan; De Raedt, Luc] Katholieke Univ Leuven, Leuven, Belgium; [Kimmig, Angelika] Cardiff Univ, Cardiff, S Glam, Wales; [Demeester, Thomas] Univ Ghent, IMEC, Ghent, Belgium	KU Leuven; Cardiff University; Ghent University; IMEC	Manhaeve, R (corresponding author), Katholieke Univ Leuven, Leuven, Belgium.	robin.manhaeve@cs.kuleuven.be; sebastijan.dumancic@cs.kuleuven.be; KimmigA@cardiff.ac.uk; thomas.demeester@ugent.be; luc.deraedt@cs.kuleuven.be	De Raedt, Luc/AAX-1544-2021; Dumancic, Sebastijan/X-3698-2018; Manhaeve, Robin/AAE-4378-2022	De Raedt, Luc/0000-0002-6860-6303; Dumancic, Sebastijan/0000-0003-0915-8034; Manhaeve, Robin/0000-0001-9907-7486; Demeester, Thomas/0000-0002-9901-5768	FWO [1S61718N]; Research Fund KU Leuven [GOA/13/010]; Research Foundation - Flanders [G079416N]	FWO(FWO); Research Fund KU Leuven(KU Leuven); Research Foundation - Flanders(FWO)	RM is a SB PhD fellow at FWO (1S61718N). SD is supported by the Research Fund KU Leuven (GOA/13/010) and Research Foundation - Flanders (G079416N)	Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; [Anonymous], 2018, AAAI; Bosnjak Matko, 2017, P 34 INT C MACH LEAR, V70, P547; Darwiche A, 2002, J ARTIF INTELL RES, V17, P229, DOI 10.1613/jair.989; Darwiche A., 2011, P 22 INT JOINT C ART, P819, DOI DOI 10.5591/978-1-57735-516-8/IJCAI11-143; de Raedt L., 2016, SYNTHESIS LECT ARTIF, DOI DOI 10.2200/S00692ED1V01Y201601AIM032; De Raedt L., IJCAI 2007, V7, P2462; De Raedt L, 2015, MACH LEARN, V100, P5, DOI 10.1007/s10994-015-5494-z; DeRaedt Luc, 2014, PROBABILISTIC PROGRA; Dries Anton, 2015, JOINT EUR C MACH LEA, P312; Dumancic S, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1631; Eisner J, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P1; Evans R, 2018, J ARTIF INTELL RES, V61, P1; Fierens D, 2015, THEOR PRACT LOG PROG, V15, P358, DOI 10.1017/S1471068414000076; Garcez A. D., 2017, P 26 INT JOINT C ART, P1596; Garcez A.S.d., 2012, NEURAL SYMBOLIC LEAR; Getoor Lise, 2007, INTRO STAT RELATIONA; Gutmann B, 2008, LECT NOTES ARTIF INT, V5211, P473, DOI 10.1007/978-3-540-87479-9_49; Hammer B., 2007, PERSPECTIVES NEURAL, V8; Holldobler S, 1999, APPL INTELL, V11, P45, DOI 10.1023/A:1008376514077; Kimmig A., 2011, 25 AAAI C ART INT; King DB, 2015, ACS SYM SER, V1214, P1; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Paszke Adam, 2017, P WORKSH FUT GRAD BA; Reed Scott, 2016, NEURAL PROGRAMMER IN; Rocktaschel T, 2017, ADV NEUR IN, V30; Roy S., 2015, P 2015 C EMP METH NA, P1743, DOI 10.18653/v1/D15-1202; Santoro A, 2017, ADV NEUR IN, V30; Sourek G., 2018, J ARTIFICIAL INTELLI; William W, 2018, J ARTIFICIAL INTELLI, V1, P1; Xu JY, 2018, PR MACH LEARN RES, V80; Zhou Z.-H., 2018, ARXIV180201173	33	56	56	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303072
C	Yue, KY; Sun, M; Yuan, YC; Zhou, F; Ding, ER; Xu, FX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yue, Kaiyu; Sun, Ming; Yuan, Yuchen; Zhou, Feng; Ding, Errui; Xu, Fuxin			Compact Generalized Non-local Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The non-local module [27] is designed for capturing long-range spatio-temporal dependencies in images and videos. Although having shown excellent performance, it lacks the mechanism to model the interactions between positions across channels, which are of vital importance in recognizing fine-grained objects and actions. To address this limitation, we generalize the non-local module and take the correlations between the positions of any two channels into account. This extension utilizes the compact representation for multiple kernel functions with Taylor expansion that makes the generalized non-local module in a fast and low-complexity computation flow. Moreover, we implement our generalized non-local method within channel groups to ease the optimization. Experimental results illustrate the clear-cut improvements and practical applicability of the generalized non-local module on both fine-grained object recognition and video classification. Code is available at: https://github.com/KaiyuYue/cgnl-network.pytorch.	[Yue, Kaiyu; Sun, Ming; Yuan, Yuchen; Ding, Errui] Baidu VIS, Beijing, Peoples R China; [Zhou, Feng] Baidu Res, Beijing, Peoples R China; [Yue, Kaiyu; Xu, Fuxin] Cent South Univ, Changsha, Hunan, Peoples R China	Baidu; Central South University	Yue, KY (corresponding author), Cent South Univ, Changsha, Hunan, Peoples R China.	yuekaiyu@baidu.com; sunming05@baidu.com; yuanyuchen02@baidu.com; zhoufeng09@baidu.com; dingerrui@baidu.com; fxxu@csu.edu.cn						Ashish Vaswani N. P. J. U. L. J. A. N. G. L. K., 2017, NIPS; Carreira J, 2012, LECT NOTES COMPUT SC, V7578, P430, DOI 10.1007/978-3-642-33786-4_32; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cui Y, 2017, PROC CVPR IEEE, P3049, DOI 10.1109/CVPR.2017.325; Gao Y, 2016, PROC CVPR IEEE, P317, DOI 10.1109/CVPR.2016.41; Goyal Priya, 2017, ARXIV170602677; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; Howard A.G., 2017, MOBILENETS EFFICIENT; Jie H., 2017, P IEEE C COMP VIS PA, P99; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Luo WJ, 2016, ADV NEUR IN, V29; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scholkopf B., 2001, LEARNING KERNELS SUP; Soomro K., 2012, ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Ustyuzhaninov I., 2017, ICLR; Wah C., 2011, TECH REP; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xie Saining, 2017, ARXIV171204851, V1, P5; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716	31	56	56	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001008
C	Lin, XF; Zhao, C; Pan, W		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lin, Xiaofan; Zhao, Cong; Pan, Wei			Towards Accurate Binary Convolutional Neural Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We introduce a novel scheme to train binary convolutional neural networks (CNNs) - CNNs with weights and activations constrained to {-1,+1} at run-time. It has been known that using binary weights and activations drastically reduce memory size and accesses, and can replace arithmetic operations with more efficient bitwise operations, leading to much faster test-time inference and lower power consumption. However, previous works on binarizing CNNs usually result in severe prediction accuracy degradation. In this paper, we address this issue with two major innovations: (1) approximating full-precision weights with the linear combination of multiple binary weight bases; (2) employing multiple binary activations to alleviate information loss. The implementation of the resulting binary CNN, denoted as ABC-Net, is shown to achieve much closer performance to its full-precision counterpart, and even reach the comparable prediction accuracy on ImageNet and forest trail datasets, given adequate binary weight bases and activations.	[Lin, Xiaofan; Zhao, Cong; Pan, Wei] DJI Innovat Inc, Shenzhen, Peoples R China		Pan, W (corresponding author), DJI Innovat Inc, Shenzhen, Peoples R China.	xiaofan.lin@dji.com; cong.zhao@dji.com; wei.pan@dji.com	Jeong, Yongwook/N-7413-2016					Bengio Yoshua, 2013, ARXIV; CAI Z, 2017, ARXIV170200953; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Esser S. K, 2016, P NATL ACAD SCI USA; Giusti A., 2016, IEEE ROBOTICS AUTOMA; Govindu G., PAR DISTR PROC S 200, P149; Grabbe C, 2003, PROCEEDINGS OF THE 2003 IEEE INTERNATIONAL SYMPOSIUM ON CIRCUITS AND SYSTEMS, VOL II, P268; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hubara I., 2016, ARXIV160907061; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Liu SL, 2016, CONF PROC INT SYMP C, P393, DOI 10.1109/ISCA.2016.42; Qian N, 1999, NEURAL NETWORKS, V12, P145, DOI 10.1016/S0893-6080(98)00116-6; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; TOMS DJ, 1990, ELECTRON LETT, V26, P1745, DOI 10.1049/el:19901121; Venkatesh G., 2016, ARXIV161000324; Zhou Shuchang, 2016, P IEEE C COMP VIS PA	19	56	56	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400033
C	Jaitly, N; Sussillo, D; Le, QV; Vinyals, O; Sutskever, I; Bengio, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Jaitly, Navdeep; Sussillo, David; Le, Quoc V.; Vinyals, Oriol; Sutskever, Ilya; Bengio, Samy			An Online Sequence-to-Sequence Model Using Partial Conditioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Sequence-to-sequence models have achieved impressive results on various tasks. However, they are unsuitable for tasks that require incremental predictions to be made as more data arrives or tasks that have long input sequences and output sequences. This is because they generate an output sequence conditioned on an entire input sequence. In this paper, we present a Neural Transducer that can make incremental predictions as more input arrives, without redoing the entire computation. Unlike sequence-to-sequence models, the Neural Transducer computes the next-step distribution conditioned on the partially observed input sequence and the partially generated sequence. At each time step, the transducer can decide to emit zero to many output symbols. The data can be processed using an encoder and presented as input to the transducer. The discrete decision to emit a symbol at every time step makes it difficult to learn with conventional backpropagation. It is however possible to train the transducer by using a dynamic programming algorithm to generate target discrete decisions. Our experiments show that the Neural Transducer works well in settings where it is required to produce output predictions as data come in. We also find that the Neural Transducer performs well for long sequences even when attention mechanisms are not used.	[Jaitly, Navdeep; Sussillo, David; Le, Quoc V.; Sutskever, Ilya; Bengio, Samy] Google Brain, Mountain View, CA 94043 USA; [Vinyals, Oriol] Google DeepMind, London, England; [Sutskever, Ilya] Open AI, San Francisco, CA USA	Google Incorporated; Google Incorporated	Jaitly, N (corresponding author), Google Brain, Mountain View, CA 94043 USA.	ndjaitly@google.com; sussillo@google.com; qvl@google.com; vinyals@google.com; ilyasu@openai.com; bengio@google.com						Bahdanau Dzmitry, 2015, END TO END ATTENTION; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chorowski Jan, 2015, NEURAL INFORM PROCES; Chorowski Jan, 2014, NEUR INF PROC SYST W; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Graves A., 2014, ARXIV14105401; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Graves Alex, 2012, COMPUT SCI; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Neelakantan Arvind, 2015, ARXIV151104834; Reed Scott, 2015, ICLR, V1, P5; Sordoni Alessandro, 2015, P 2015 C N AM CHAPT, P196, DOI DOI 10.3115/V1/N15-1020; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vinyals O, 2015, ICML DEEP LEARN WORK; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Vinyals Oriol, 2015, NIPS; Zaremba Wojciech, 2015, ARXIV150500521	21	56	56	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973705008
C	Wang, H; Shi, XJ; Yeung, DY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Hao; Shi, Xingjian; Yeung, Dit-Yan			Collaborative Recurrent Autoencoder: Recommend while Learning to Fill in the Blanks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Hybrid methods that utilize both content and rating information are commonly used in many recommender systems. However, most of them use either handcrafted features or the bag-of-words representation as a surrogate for the content information but they are neither effective nor natural enough. To address this problem, we develop a collaborative recurrent autoencoder (CRAE) which is a denoising recurrent autoencoder (DRAE) that models the generation of content sequences in the collaborative filtering (CF) setting. The model generalizes recent advances in recurrent deep learning from i.i.d. input to non-i.i.d. (CF-based) input and provides a new denoising scheme along with a novel learnable pooling scheme for the recurrent autoencoder. To do this, we first develop a hierarchical Bayesian model for the DRAE and then generalize it to the CF setting. The synergy between denoising and CF enables CRAE to make accurate recommendations while learning to fill in the blanks in sequences. Experiments on real-world datasets from different domains (CiteULike and Netflix) show that, by jointly modeling the order-aware generation of sequences for the content information and performing CF for the ratings, CRAE is able to significantly outperform the state of the art on both the recommendation task based on ratings and the sequence generation task based on content information.	[Wang, Hao; Shi, Xingjian; Yeung, Dit-Yan] Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China	Hong Kong University of Science & Technology	Wang, H (corresponding author), Hong Kong Univ Sci & Technol, Hong Kong, Peoples R China.	hwangaz@cse.ust.hk; xshiab@cse.ust.hk; dyyeung@cse.ust.hk						Bengio Y., 2015, DEEP LEARNING UNPUB; Blei, 2011, P 17 ACM SIGKDD INT, P448, DOI DOI 10.1145/2020408.2020480; Chen TQ, 2012, J MACH LEARN RES, V13, P3619; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Georgiev K., 2013, ICML; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Oord A., 2013, NIPS; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Rendle Steffen, 2009, UAI; Ricci F, 2011, INTRO RECOMMENDER SY; Sainath TN, 2013, INT CONF ACOUST SPEE, P8614, DOI 10.1109/ICASSP.2013.6639347; Salakhutdinov R., 2007, P 24 INT C MACHINE L, V227, P791, DOI [DOI 10.1145/1273496.1273596, 10.1145/1273496.1273596]; Salakhutdinov R., 2007, ADV NEURAL INF PROCE, V20, P1257; Singh A.P., 2008, PROC PROC 14 ACM SIG, P650; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tikk D, 2015, ARXIV151106939; van Amersfoort J. R., 2014, ARXIV14126581; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang H., 2015, AAAI; Wang H., 2016, TKDE; Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273; Wang Hao, 2013, IJCAI; Wang X., 2014, ACM MM	25	56	56	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701020
C	Padgett, C; Cottrell, G		Mozer, MC; Jordan, MI; Petsche, T		Padgett, C; Cottrell, G			Representing face images for emotion classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We compare the generalization performance of three distinct representation schemes for facial emotions using a single classification strategy (neural network). The face images presented to the classifiers are represented as: full face projections of the dataset onto their eigenvectors (eigenfaces); a similar projection constrained to eye and mouth areas (eigenfeatures); and finally a projection of the eye and mouth areas onto the eigenvectors obtained from 32x32 random image patches from the dataset. The latter system achieves 86% generalization on novel face images (individuals the networks were not trained on) drawn from a database in which human subjects consistently identify a single emotion for the face.			Padgett, C (corresponding author), UNIV CALIF SAN DIEGO,DEPT COMP SCI,LA JOLLA,CA 92034, USA.			Cottrell, Garrison/0000-0001-7538-1715					0	56	57	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						894	900						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00126
C	JORDAN, MI; JACOBS, RA		MOODY, JE; HANSON, SJ; LIPPMANN, RP		JORDAN, MI; JACOBS, RA			HIERARCHIES OF ADAPTIVE EXPERTS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO										Jordan, Michael I/C-5253-2013						0	56	57	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						985	992						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00121
C	SIMARD, P; LECUN, Y; VICTORRI, B; DENKER, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		SIMARD, P; LECUN, Y; VICTORRI, B; DENKER, J			TANGENT PROP - A FORMALISM FOR SPECIFYING SELECTED INVARIANCES IN AN ADAPTIVE NETWORK	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	56	56	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						895	903						9	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00110
C	Bai, SJ; Kolter, JZ; Koltun, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bai, Shaojie; Kolter, J. Zico; Koltun, Vladlen			Deep Equilibrium Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We present a new approach to modeling sequential data: the deep equilibrium model (DEQ). Motivated by an observation that the hidden layers of many existing deep sequence models converge towards some fixed point, we propose the DEQ approach that directly finds these equilibrium points via root-finding. Such a method is equivalent to running an infinite depth (weight-tied) feedforward network, but has the notable advantage that we can analytically backpropagate through the equilibrium point using implicit differentiation. Using this approach, training and prediction in these networks require only constant memory, regardless of the effective "depth" of the network. We demonstrate how DEQs can be applied to two state-of-the-art deep sequence models: self-attention transformers and trellis networks. On large-scale language modeling tasks, such as the WikiText-103 benchmark, we show that DEQs 1) often improve performance over these stateof-the-art models (for similar parameter counts); 2) have similar computational requirements to existing models; and 3) vastly reduce memory consumption (often the bottleneck for training large sequence models), demonstrating an up-to 88% memory reduction in our experiments. The code is available at https://github.com/locuslab/deq.	[Bai, Shaojie; Kolter, J. Zico] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Kolter, J. Zico] Bosch Ctr AI, Gerlingen, Germany; [Koltun, Vladlen] Intel Labs, Santa Clara, CA USA	Carnegie Mellon University; Intel Corporation	Bai, SJ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.							Al-Rfou Rami, 2018, ARXIV180804444; Almeida L.B., 1990, ARTIFICIAL NEURAL NE; Amos B, 2017, PR MACH LEARN RES, V70; [Anonymous], 2017, ADV NEURAL INFORM PR; Arjovsky M, 2016, PR MACH LEARN RES, V48; Ba L. J., 2016, CORRABS160706450; Baevski A., 2019, INT C LEARN REPR ICL; Bai S., 2018, ARXIV PREPRINT ARXIV; Bai S., 2019, INT C LEARN REPR, V103, P1; Bradbury J., 2017, P INT C LEARN REPR I, P1; Broyden C.G., 1965, MATH COMPUT, V19, P577, DOI [DOI 10.1090/S0025-5718-1965-0198670-6, 10.1090/s0025-5718-1965-0198670-6]; Chen T.Q., 2018, ADV NEURAL INFORM PR; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Child R., 2019, ARXIV190410509; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Dabre Raj, 2019, AAAI C ART INT; Dai ZH, 2019, 57TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2019), P2978; Dauphin YN, 2017, PR MACH LEARN RES, V70; Dehghani Mostafa, 2019, ICLR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; El Ghaoui L., 2019, ARXIV190806315; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kazi Michaeel, 2017, ANN M ASS COMP LING; Liao RJ, 2018, PR MACH LEARN RES, V80; Liu Hanxiao, 2019, INTERNATIONAL CONFER; MacKay M, 2018, ADV NEUR IN, V31; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Melis G, 2018, INT C LEARN REPR ICL; Merity S., 2018, 6 INT C LEARN REPR I; Merity Stephen, 2018, ARXIV180308240; Miller J, 2018, ARXIV180510369; Niculae V, 2018, PR MACH LEARN RES, V80; Oord A.V.D., 2016, SSW; Paszke A, 2019, ADV NEURAL INF PROCE, DOI DOI 10.48550/ARXIV.1912.01703; Pineda F. J., 1988, NEURAL INFORM PROCES, P602; Santoro A, 2018, ADV NEUR IN, V31; Scellier Benjamin, 2017, FRONTIERS COMPUTATIO; SHERMAN J, 1950, ANN MATH STAT, V21, P124, DOI 10.1214/aoms/1177729893; Simard Patrice Y, 1989, NEURAL INFORM PROCES; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Trinh TH, 2018, PR MACH LEARN RES, V80; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; WAIBEL A, 1989, IEEE T ACOUST SPEECH, V37, P328, DOI 10.1109/29.21701; Wang PW, 2019, PR MACH LEARN RES, V97; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Yang Zhilin, 2018, ICLR; Zhang SZ, 2016, ADV NEUR IN, V29; Zhang Ziming, 2019, ARXIV190300755; Zoph B., 2017, P1	55	55	55	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300063
C	Liu, Q; Nickel, M; Kiela, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Qi; Nickel, Maximilian; Kiela, Douwe			Hyperbolic Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ZINC; DATABASE; MODEL	Learning from graph-structured data is an important task in machine learning and artificial intelligence, for which Graph Neural Networks (GNNs) have shown great promise. Motivated by recent advances in geometric representation learning, we propose a novel GNN architecture for learning representations on Riemannian manifolds with differentiable exponential and logarithmic maps. We develop a scalable algorithm for modeling the structural properties of graphs, comparing Euclidean and hyperbolic geometry. In our experiments, we show that hyperbolic GNNs can lead to substantial improvements on various benchmark datasets.	[Liu, Qi; Nickel, Maximilian; Kiela, Douwe] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Liu, Q (corresponding author), Facebook AI Res, New York, NY 10003 USA.	qiliu@fb.com; maxn@fb.com; dkiela@fb.com						Aldecoa R, 2015, COMPUT PHYS COMMUN, V196, P492, DOI 10.1016/j.cpc.2015.05.028; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Battaglia Peter W, 2018, ARXIV180601261; Becigneul G., 2019, PROC INT C LEARN REP, P1; Begusic S, 2018, PHYSICA A, V510, P400, DOI 10.1016/j.physa.2018.06.131; Bengio Y., 2014, ARXIV14061078; Boguna Marian, 2010, Nat Commun, V1, P62, DOI 10.1038/ncomms1063; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Broomhead D. S., 1988, Complex Systems, V2, P321; Bruna J., 2014, P ICLR; Chami Ines, 2019, 32 C NEUR INF PROC S; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Duvenaud David K, 2015, P NIPS; Erdos P., 1959, PUBL MATH-DEBRECEN, V6, P290, DOI DOI 10.2307/1999405; Ganea Octavian, 2018, ADV NEURAL INFORM PR, P5345; Ganea OE, 2018, PR MACH LEARN RES, V80; Gilmer J, 2017, PR MACH LEARN RES, V70; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Gulcehre C., 2019, ICLR; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Irwin JJ, 2005, J CHEM INF MODEL, V45, P177, DOI 10.1021/ci049714+; Irwin JJ, 2012, J CHEM INF MODEL, V52, P1757, DOI 10.1021/ci3001277; Jin WG, 2018, PR MACH LEARN RES, V80; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kleinberg R, 2007, IEEE INFOCOM SER, P1902, DOI 10.1109/INFCOM.2007.221; Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106; Li Yujia, 2016, P INT C LEARN REPR I, P2; Liang JQ, 2018, PLOS ONE, V13, DOI 10.1371/journal.pone.0202202; Liu Q, 2018, ADV NEUR IN, V31; Nickel M, 2018, PR MACH LEARN RES, V80; Nickel M, 2017, ADV NEUR IN, V30; Niepert M, 2016, PR MACH LEARN RES, V48; Papadopoulos F, 2012, NATURE, V489, P537, DOI 10.1038/nature11459; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Ruddigkeit L, 2012, J CHEM INF MODEL, V52, P2864, DOI 10.1021/ci300415d; Sala F, 2018, PR MACH LEARN RES, V80; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schlichtkrull M, 2018, LECT NOTES COMPUT SC, V10843, P593, DOI 10.1007/978-3-319-93417-4_38; Schutt KT, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13890; Sterling T, 2015, J CHEM INF MODEL, V55, P2324, DOI 10.1021/acs.jcim.5b00559; Velickovic P., 2018, P INT C LEARN REPR; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Wheatley Spencer, 2018, 1822 SWISS FIN I, P18; Wood G., 2014, ETHEREUM PROJ YELLOW, V151, P1; Wu Ke, 2018, TECHNICAL REPORT; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890; Zhang MH, 2018, ADV NEUR IN, V31; Zitnik M, 2018, BIOINFORMATICS, V34, P457, DOI 10.1093/bioinformatics/bty294	51	55	55	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308027
C	Li, Y; Gupta, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Li, Yin; Gupta, Abhinav			Beyond Grids: Learning Graph Representations for Visual Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose learning graph representations from 2D feature maps for visual recognition. Our method draws inspiration from region based recognition, and learns to transform a 2D image into a graph structure. The vertices of the graph define clusters of pixels ("regions"), and the edges measure the similarity between these clusters in a feature space. Our method further learns to propagate information across all vertices on the graph, and is able to project the learned graph representation back into 2D grids. Our graph representation facilitates reasoning beyond regular grids and can capture long range dependencies among regions. We demonstrate that our model can be trained from end-to-end, and is easily integrated into existing networks. Finally, we evaluate our method on three challenging recognition tasks: semantic segmentation, object detection and object instance segmentation. For all tasks, our method outperforms state-of-the-art methods.	[Li, Yin] Univ Wisconsin, Dept Comp Sci, Dept Biostat & Med Informat, 1210 W Dayton St, Madison, WI 53706 USA; [Gupta, Abhinav] Carnegie Mellon Univ, Sch Comp Sci, Robot Inst, Pittsburgh, PA 15213 USA; [Li, Yin] CMU, Pittsburgh, PA 15213 USA	University of Wisconsin System; University of Wisconsin Madison; Carnegie Mellon University; Carnegie Mellon University	Li, Y (corresponding author), Univ Wisconsin, Dept Comp Sci, Dept Biostat & Med Informat, 1210 W Dayton St, Madison, WI 53706 USA.; Li, Y (corresponding author), CMU, Pittsburgh, PA 15213 USA.	yin.li@wisc.edu; abhinavg@cs.cmu.edu			ONR MURI [N000141612007]; Sloan Fellowship; ONR Young Investigator Award; Okawa Fellowship	ONR MURI(MURIOffice of Naval Research); Sloan Fellowship(Alfred P. Sloan Foundation); ONR Young Investigator Award; Okawa Fellowship	This work was supported by ONR MURI N000141612007, Sloan Fellowship, Okawa Fellowship and ONR Young Investigator Award to AG. The authors thank Xiaolong Wang for many helpful discussions, and Jianping Shi for sharing implementation details of PSPNet.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arandjelovic R, 2013, PROC CVPR IEEE, P1578, DOI 10.1109/CVPR.2013.207; Badrinarayanan V., 2015, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Berg A.C., 2015, ARXIV150604579; Carreira  Joao, 2012, IJCV; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Duvenaud David K, 2015, P NIPS; Farabet C, 2013, IEEE T PATTERN ANAL, V35, P1915, DOI 10.1109/TPAMI.2012.231; Fidler S., 2013, CVPR; Fulkerson  Brian, 2009, CVPR; Gadde R, 2016, LECT NOTES COMPUT SC, V9905, P597, DOI 10.1007/978-3-319-46448-0_36; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gould S., 2009, NIPS, V1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hoiem D, 2005, IEEE I CONF COMP VIS, P654; Jegou H, 2010, PROC CVPR IEEE, P3304, DOI 10.1109/CVPR.2010.5540039; Kalayeh Mahdi M, 2018, ARXIV180602892; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kohli P, 2009, INT J COMPUT VISION, V82, P302, DOI 10.1007/s11263-008-0202-0; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Ladicky L, 2009, IEEE I CONF COMP VIS, P739, DOI 10.1109/ICCV.2009.5459248; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo WJ, 2016, ADV NEUR IN, V29; Malisiewicz T., 2009, NIPS; Munoz D, 2010, LECT NOTES COMPUT SC, V6316, P57, DOI 10.1007/978-3-642-15567-3_5; Perronnin F, 2010, LECT NOTES COMPUT SC, V6314, P143, DOI 10.1007/978-3-642-15561-1_11; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Yan JC, 2015, PROC CVPR IEEE, P1520, DOI 10.1109/CVPR.2015.7298759; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang Hang, 2018, CVPR; Zhang Hanwang, 2017, PROC CVPR IEEE, P5532, DOI [DOI 10.1109/CVPR.2017.331, DOI 10.1109/CVPR.2018.00611]	45	55	56	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003075
C	Noh, H; You, T; Mun, J; Han, B		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Noh, Hyeonwoo; You, Tackgeun; Mun, Jonghwan; Han, Bohyung			Regularizing Deep Neural Networks by Noise: Its Interpretation and Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Overfitting is one of the most critical challenges in deep neural networks, and there are various types of regularization methods to improve generalization performance. Injecting noises to hidden units during training, e.g., dropout, is known as a successful regularizer, but it is still not clear enough why such training techniques work well in practice and how we can maximize their benefit in the presence of two conflicting objectives-optimizing to true data distribution and preventing overfitting by regularization. This paper addresses the above issues by 1) interpreting that the conventional training methods with regularization by noise injection optimize the lower bound of the true objective and 2) proposing a technique to achieve a tighter lower bound using multiple noise samples per training example in a stochastic gradient descent iteration. We demonstrate the effectiveness of our idea in several computer vision applications.	[Noh, Hyeonwoo; You, Tackgeun; Mun, Jonghwan; Han, Bohyung] POSTECH, Dept Comp Sci & Engn, Pohang, South Korea	Pohang University of Science & Technology (POSTECH)	Noh, H (corresponding author), POSTECH, Dept Comp Sci & Engn, Pohang, South Korea.	shgusdngogo@postech.ac.kr; tackgeun.you@postech.ac.kr; choco1916@postech.ac.kr; bhhan@postech.ac.kr	Jeong, Yongwook/N-7413-2016		IITP grant - Korea government (MSIT) [2017-0-01778, 2017-0-01780]	IITP grant - Korea government (MSIT)	This work was supported by the IITP grant funded by the Korea government (MSIT) [2017-0-01778, Development of Explainable Human-level Deep Machine Learning Inference Framework; 2017-0-01780, The Technology Development for Event Recognition/Relational Reasoning and Learning Knowledge based System for Video Understanding].	Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ba J., 2013, ADV NEURAL INFORM PR, P3084; Ba J, 2015, ADV NEUR IN, V28; Bornschein J., 2015, ICLR; Bulo SR, 2016, PR MACH LEARN RES, V48; Burda Y., 2016, ICLR; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Gal Y, 2016, PR MACH LEARN RES, V48; Han B., 2016, P CVPR; Han B., 2017, CVPR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang G, 2016, LECT NOTES COMPUT SC, V9908, P646, DOI 10.1007/978-3-319-46493-0_39; Jain Prateek, 2015, ARXIV150302031; Kingma D. P, 2014, ARXIV13126114; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Larsson G., 2017, INT C LEARN REPR ICL; Li Z., 2016, ARXIV PREPRINT ARXIV; Liang PH., 2013, ADV NEURAL INFORM PR, V26; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ma X., 2016, ARXIV160908017; Mao HZ, 2017, IEEE COMPUT SOC CONF, P1927, DOI 10.1109/CVPRW.2017.241; Mnih Andriy, 2016, P ICML; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nam H, 2016, PROC CVPR IEEE, P4293, DOI 10.1109/CVPR.2016.465; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Raiko T., 2015, ICLR; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Soomro K., 2012, ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wan L., 2013, P INT C MACHINE LEAR, P1058; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	40	55	56	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405019
C	Joseph, M; Kearns, M; Morgenstern, J; Roth, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Joseph, Matthew; Kearns, Michael; Morgenstern, Jamie; Roth, Aaron			Fairness in Learning: Classic and Contextual Bandits	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce the study of fairness in multi-armed bandit problems. Our fairness definition demands that, given a pool of applicants, a worse applicant is never favored over a better one, despite a learning algorithm's uncertainty over the true payoffs. In the classic stochastic bandits problem we provide a provably fair algorithm based on "chained" confidence intervals, and prove a cumulative regret bound with a cubic dependence on the number of arms. We further show that any fair algorithm must have such a dependence, providing a strong separation between fair and unfair learning that extends to the general contextual case. In the general contextual case, we prove a tight connection between fairness and the KWIK (Knows What It Knows) learning model: a KWIK algorithm for a class of functions can be transformed into a provably fair contextual bandit algorithm and vice versa. This tight connection allows us to provide a provably fair algorithm for the linear contextual bandit problem with a polynomial dependence on the dimension, and to show (for a different class of functions) a worst-case exponential gap in regret between fair and non-fair learning algorithms.	[Joseph, Matthew; Kearns, Michael; Morgenstern, Jamie; Roth, Aaron] Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA	University of Pennsylvania	Joseph, M (corresponding author), Univ Penn, Dept Comp & Informat Sci, Philadelphia, PA 19104 USA.	majos@cis.upenn.edu; mkearns@cis.upenn.edu; jamiemor@cis.upenn.edu; aaroth@cis.upenn.edu						Abernethy J., 2013, P ICML, P588; Adler Philip, 2016, CORR; Amin Kareem, 2012, ARXIV12023782; Auer P, 2002, MACH LEARN, V47, P235, DOI 10.1023/A:1013689704352; Barocas S., 2016, CALIFORNIA LAW REV, V104; Barry-Jester Anna Maria, 2015, THE MARSHALL PROJECT; Beygelzimer A, 2011, INT C ART INT STAT, V15, P19; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Byrnes Nanette, 2016, MIT TECHNOLOGY REV; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Coglianese Cary, 2016, GEORGETOWN LAW J; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Fish Benjamin, 2016, SIAM INT S DAT MIN; Joseph M., 2016, CORR; Kamishima T., 2011, 2011 IEEE International Conference on Data Mining Workshops, P643, DOI 10.1109/ICDMW.2011.83; KATEHAKIS MN, 1995, P NATL ACAD SCI USA, V92, P8584, DOI 10.1073/pnas.92.19.8584; LAI TL, 1985, ADV APPL MATH, V6, P4, DOI 10.1016/0196-8858(85)90002-8; Li L., 2009, THESIS; Li LH, 2011, MACH LEARN, V82, P399, DOI 10.1007/s10994-010-5225-4; Luong Binh Thanh, 2011, P 17 ACM SIGKDD INT, P502; Miller Claire Cain, 2015, NEW YORK TIMES; Rudin C., 2013, WIRED MAGAZINE; Strehl AL, 2008, ADV NEURAL INFORM PR, V20, P1417; Zemel R., 2013, P INT C MACH LEARN, P325	27	55	55	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702102
C	Novikov, A; Podoprikhin, D; Osokin, A; Vetrov, D		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Novikov, Alexander; Podoprikhin, Dmitry; Osokin, Anton; Vetrov, Dmitry			Tensorizing Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Deep neural networks currently demonstrate state-of-the-art performance in several domains. At the same time, models of this class are very demanding in terms of computational resources. In particular, a large amount of memory is required by commonly used fully-connected layers, making it hard to use the models on low-end devices and stopping the further increase of the model size. In this paper we convert the dense weight matrices of the fully-connected layers to the Tensor Train [17] format such that the number of parameters is reduced by a huge factor and at the same time the expressive power of the layer is preserved. In particular, for the Very Deep VGG networks [21] we report the compression factor of the dense weight matrix of a fully-connected layer up to 200000 times leading to the compression factor of the whole network up to 7 times.	[Novikov, Alexander; Podoprikhin, Dmitry; Vetrov, Dmitry] Skolkovo Inst Sci & Technol, Moscow, Russia; [Osokin, Anton] INRIA, SIERRA Project Team, Paris, France; [Vetrov, Dmitry] Natl Res Univ, Higher Sch Econ, Moscow, Russia; [Novikov, Alexander] Russian Acad Sci, Inst Numer Math, Moscow, Russia	Skolkovo Institute of Science & Technology; Inria; HSE University (National Research University Higher School of Economics); Russian Academy of Sciences	Novikov, A (corresponding author), Skolkovo Inst Sci & Technol, Moscow, Russia.	novikov@bayesgroup.ru; podoprikhin.dmitry@gmail.com; anton.osokin@inria.fr; vetrovd@yandex.ru	Osokin, Anton/D-7398-2012	Osokin, Anton/0000-0002-8807-5132	RFBR [15-31-20596 (mol-a-ved)]; Microsoft: Moscow State University Joint Research Center [RPD 1053945]; MSR-INRIA Joint Center; Russian Science Foundation [14-11-00659]	RFBR(Russian Foundation for Basic Research (RFBR)); Microsoft: Moscow State University Joint Research Center; MSR-INRIA Joint Center; Russian Science Foundation(Russian Science Foundation (RSF))	We would like to thank Ivan Oseledets for valuable discussions. A. Novikov, D. Podoprikhin, D. Vetrov were supported by RFBR project No. 15-31-20596 (mol-a-ved) and by Microsoft: Moscow State University Joint Research Center (RPD 1053945). A. Osokin was supported by the MSR-INRIA Joint Center. The results of the tensor toolbox application (in Sec. 6) are supported by Russian Science Foundation No. 14-11-00659.	Asanovi K., 1991, TECH REP; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Chen WL, 2015, PR MACH LEARN RES, V37, P2285; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Denil Misha, 2013, NIPS, DOI DOI 10.5555/2999792.2999852; Dentinel Zarembaw, 2014, NEURIPS, P1269; Gilboa E., 2012, 12094120 ARXIV; Gong Y., 2014, 14126115 ARXIV; Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Krizhevsky Alex., 2009, LEARNING MULTIPLE LA, P6; Lebedev V., 2014, INT C LEARN REPR ICL; Novikov A, 2014, PR MACH LEARN RES, V32, P811; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Simonyan K., 2015, INT C LEARN REPR ICL; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Vedaldi A., P ACM INT C MULT; Xue J, 2013, INTERSPEECH, P2364; Yang Z., 2014, 14127149 ARXIV; Zhang Z., 2014, CLOUD BASED DESIGN M, P63	27	55	55	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101047
C	Ren, MY; Kiros, R; Zemel, RS		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ren, Mengye; Kiros, Ryan; Zemel, Richard S.			Exploring Models and Data for Image Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					This work aims to address the problem of image-based question-answering (QA) with new models and datasets. In our work, we propose to use neural networks and visual semantic embeddings, without intermediate stages such as object detection and image segmentation, to predict answers to simple questions about images. Our model performs 1.8 times better than the only published results on an existing image QA dataset. We also present a question generation algorithm that converts image descriptions, which are widely available, into QA form. We used this algorithm to produce an order-of-magnitude larger dataset, with more evenly distributed answers. A suite of baseline results on this new dataset are also presented.	[Ren, Mengye; Kiros, Ryan; Zemel, Richard S.] Univ Toronto, Toronto, ON, Canada; [Zemel, Richard S.] Canadian Inst Adv Res, Quebec City, PQ, Canada	University of Toronto; Canadian Institute for Advanced Research (CIFAR)	Ren, MY (corresponding author), Univ Toronto, Toronto, ON, Canada.	mren@cs.toronto.edu; rkiros@cs.toronto.edu; zemel@cs.toronto.edu						[Anonymous], NIPS DEEP LEARN WORK; Antol Stanislaw, 2015, CORR; Bird Steven, 2006, ACL; Chen X, 2015, CORR, V1504, P325; Chen X., 2014, CORR; Chomsky N., 1973, CONDITIONS TRANSFORM; Devlin J., 2015, CORR; Donahue J., 2014, CVPR; Fang H., 2015, C COMP VIS PATT REC; Fellbaum Christiane, 1998, WORDNET ELECT DATABA; Frome A., NIPS2013; Gao H., 2015, CORR; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Karpathy A., 2013, NIPS; Kiros Ryan, 2015, ARXIV14112539; Klein B., 2015, CVPR; Klein D., 2003, ACL; Lebret R., 2015, ICML; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Ma L., 2015, CORR; Malinowski M., 2014, NIPS; Malinowski M., 2014, NIPS WORKSH LEARN SE; Malinowski M., 2015, CORR; Mikolov T., 2013, EFFICIENT ESTIMATION; Ordonez V., 2011, NIPS; Palmer M., 1994, ACL; Russakovsky O., 2015, INT J COMPUT VISION, V115, P211; Silberman N., 2012, ECCV; Simonyan Karen, 2015, INT C LEARN REPR; Vinyals O., 2015, CVPR; Xu K., 2015, ICML	32	55	56	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100008
C	Tesauro, G		Thrun, S; Saul, K; Scholkopf, B		Tesauro, G			Extending Q-Learning to general adaptive multi-agent systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					Recent multi-agent extensions of Q-Learning require knowledge of other agents' payoffs and Q-functions, and assume game-theoretic play at all times by all other agents. This paper proposes a fundamentally different approach, dubbed "Hyper-Q" Learning, in which values of mixed strategies rather than base actions are learned, and in which other agents' strategies are estimated from observed actions via Bayesian inference. Hyper-Q may be effective against many different types of adaptive agents, even if they are persistently dynamic. Against certain broad categories of adaptation, it is argued that Hyper-Q may converge to exact optimal time-varying policies. In tests using Rock-Paper-Scissors, Hyper-Q learns to significantly exploit an Infinitesimal Gradient Ascent (IGA) player, as well as a Policy Hill Climber (PHC) player. Preliminary analysis of Hyper-Q against itself is also presented.	IBM Thomas J Watson Res Ctr, Hawthorne, NY 10532 USA	International Business Machines (IBM)	Tesauro, G (corresponding author), IBM Thomas J Watson Res Ctr, 19 Skyline Dr, Hawthorne, NY 10532 USA.	tesauro@watson.ibm.com						Bowling M, 2002, ARTIF INTELL, V136, P215, DOI 10.1016/S0004-3702(02)00121-2; Bowling M., 2000, P 17 INT C MACH LEAR, P89; CHANG YH, 2002, P NIPS 2001; HONG SJ, 2002, RC22393 IBM RES; HU J, 1998, P 15 INT C MACH LEAR, P242; Kearns M, 2002, P 18 C UNC ART INT, P259; LITTMAN ML, 2001, P ICML 01; LITTMAN ML, 1994, P 11 INT C MACH LEAR, P157; Munos R, 1997, INT JOINT CONF ARTIF, P826; Singh S. P., 2000, P 16 C UNC ART INT S, P541; Smart W. D., 2000, P ICML, P903; Uther WTB, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P769; Watkins C.J.C.H., 1989, LEARNING DELAYED REW; Weibull J.W., 1997, EVOLUTIONARY GAME TH	14	55	56	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						871	878						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500109
C	Pavlovic, V; Rehg, JM; MacCormick, J		Leen, TK; Dietterich, TG; Tresp, V		Pavlovic, V; Rehg, JM; MacCormick, J			Learning switching linear models of human motion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					The human figure exhibits complex and rich dynamic behavior that is both nonlinear and time-varying. Effective models of human dynamics can be learned from motion capture data using switching linear dynamic system (SLDS) models. We present results for human motion synthesis, classification, and visual tracking using learned SLDS models. Since exact inference in SLDS is intractable, we present three approximate inference algorithms and compare their performance. In particular, a new variational inference algorithm is obtained by casting the SLDS model as a Dynamic Bayesian Network. Classification experiments show the superiority of SLDS over conventional HMM's for our problem domain.	Compaq Cambridge Res Lab, Cambridge, MA 02139 USA		Pavlovic, V (corresponding author), Compaq Cambridge Res Lab, Cambridge, MA 02139 USA.		Rehg, James/AAM-6888-2020	Rehg, James/0000-0003-1793-5462				BARSHALOM, 1998, ESTIMATION TRACKING; BLAKE A, 1998, NIPS 98; BOYEN X, 1999, P UNC ART INT; BRAND M, 1998, NIPS 98; Bregler C., 1997, P INT C COMP VIS PAT; GHAHRAMANI Z, 1998, SWITCHING STATE SPAC; Howe N, 1999, NIPS 99; JORDAN M, 1998, LEARNING GRAPHICAL M; KIM CJ, 1994, J ECONOMETRICS, V60; Lerner U., 2000, P AAAI AUST TX; Morris D., 1998, CVPR; MURPHY KP, 1998, 9810 TR COMP CRL; PAVLOVIC V, 1999, P INT C COMP VIS	13	55	55	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						981	987						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800138
C	Cristianini, N; Campbell, C; Shawe-Taylor, J		Kearns, MS; Solla, SA; Cohn, DA		Cristianini, N; Campbell, C; Shawe-Taylor, J			Dynamically adapting kernels in support vector machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					The kernel-parameter is one of the few tunable parameters in Support Vector machines, controlling the complexity of the resulting hypothesis. Its choice amounts to model selection and its value is usually found by means of a validation set. We present an algorithm which can automatically perform model selection with little additional computational cost and with no need of a validation set. In this procedure model selection and learning are not separate, but kernels are dynamically adjusted during the learning process to find the kernel parameter which provides the best possible upper bound on the generalisation error. Theoretical results motivating the approach and experimental results confirming its validity are presented.	Univ Bristol, Dept Engn Math, Bristol, Avon, England	University of Bristol	Cristianini, N (corresponding author), Univ Bristol, Dept Engn Math, Bristol, Avon, England.			Shawe-Taylor, John/0000-0002-2030-0073				Aizerman M. A., 1964, AUTOMAT REM CONTR, V25, P821, DOI DOI 10.1234/12345678; Bartlett P., 1998, ADV KERNEL METHODS S; BURGES CJC, 1998, DATA MIN KNOWL DISC, V2, P1, DOI DOI 10.1023/A:1009715923555; FRIESS T, 1995, MACHINE LEARNING; GORMAN RP, 1988, NEURAL NETWORKS, V1, P75, DOI 10.1016/0893-6080(88)90023-8; James R., 1966, ADV CALCULUS; LECUN Y, 1995, COMPARISON LEARNING, P53; SHAWTAYLER J, 1996, NCTR96053; STER B, 1996, P INT C ENG APPL NEU, P427; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	10	55	61	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						204	210						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700029
C	Ghorbani, A; Wexler, J; Zou, J; Kim, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ghorbani, Amirata; Wexler, James; Zou, James; Kim, Been			Towards Automatic Concept-based Explanations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Interpretability has become an important topic of research as more machine learning (ML) models are deployed and widely used to make important decisions. Most of the current explanation methods provide explanations through feature importance scores, which identify features that are important for each individual input. However, how to systematically summarize and interpret such per sample feature importance scores itself is challenging. In this work, we propose principles and desiderata for concept based explanation, which goes beyond per-sample features to identify higher level human-understandable concepts that apply across the entire dataset. We develop a new algorithm, ACE, to automatically extract visual concepts. Our systematic experiments demonstrate that ACE discovers concepts that are human-meaningful, coherent and important for the neural network's predictions.	[Ghorbani, Amirata; Zou, James] Stanford Univ, Stanford, CA 94305 USA; [Ghorbani, Amirata; Wexler, James; Kim, Been] Google Brain, Mountain View, CA 94043 USA	Stanford University; Google Incorporated	Ghorbani, A (corresponding author), Stanford Univ, Stanford, CA 94305 USA.; Ghorbani, A (corresponding author), Google Brain, Mountain View, CA 94043 USA.	amiratag@stanford.edu; jwexler@google.com; jamesz@stanford.edu; beenkim@google.com	Ghorbani, Amirata/AFU-2782-2022		Stanford Graduate Fellowship; NSF CCF [1763191]; NIH [R21 MD012867-01, P30AG059307]; Silicon Valley Foundation; Chan-Zuckerberg Initiative	Stanford Graduate Fellowship(Stanford University); NSF CCF; NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Silicon Valley Foundation; Chan-Zuckerberg Initiative	A.G. is supported by a Stanford Graduate Fellowship (Robert Bosch Fellow). J.Z. is supported by NSF CCF 1763191, NIH R21 MD012867-01, NIH P30AG059307, and grants from the Silicon Valley Foundation and the Chan-Zuckerberg Initiative.	Achanta R, 2012, IEEE T PATTERN ANAL, V34, P2274, DOI 10.1109/TPAMI.2012.120; Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Adebayo M. M. I. G. M. H. B. K. Julius, 2018, NIPS; Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Berg A.C., 2015, ARXIV150604579; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Brendel Wieland, 2019, INT C LEARN REPR; Chang J, 2018, WOODH PUBL SER BIOM, P119, DOI 10.1016/B978-0-08-100936-9.00008-3; Chang Jonathan., 2009, P 22 INT C NEURAL IN, P288, DOI DOI 10.5555/2984093.2984126; Dabkowski P, 2017, ADV NEUR IN, V30; Ester M., 1996, P 2 INT C KNOWL DISC, P226; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Frey BJ, 2007, SCIENCE, V315, P972, DOI 10.1126/science.1136800; Geirhos Robert., 2018, ARXIV PREPRINT ARXIV; Genone J, 2012, PHILOS PSYCHOL, V25, P717, DOI 10.1080/09515089.2011.627538; Ghorbani A, 2017, ARXIV171010547; Gimenez J. R., 2018, ARXIV180706214; Goodman B, 2017, AI MAG, V38, P50, DOI 10.1609/aimag.v38i3.2741; Gulshan V, 2016, JAMA-J AM MED ASSOC, V316, P2402, DOI 10.1001/jama.2016.17216; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kim H, 2014, IEEE INT SYMP INFO, P1952, DOI 10.1109/ISIT.2014.6875174; Koh PW, 2017, PR MACH LEARN RES, V70; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Neubert P, 2014, INT C PATT RECOG, P996, DOI 10.1109/ICPR.2014.181; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Poursabzi-Sangdeh Forough, 2018, ARXIV180207810; Ribeiro M.T., 2016, ARXIV; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Rosch E., 1999, CONCEPTS CORE READIN, P189, DOI [DOI 10.1016/B978-1-4832-1446-7.50028-5, 10.1016/b978-l-4832-1446-7.50028-5]; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Smilkov D, 2017, ARXIV; Sundararajan M, 2017, PR MACH LEARN RES, V70; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tariyal S, 2016, IEEE ACCESS, V4, P10096, DOI 10.1109/ACCESS.2016.2611583; Tintarev N, 2011, RECOMMENDER SYSTEMS HANDBOOK, P479, DOI 10.1007/978-0-387-85820-3_15; Ulyanov D, 2020, INT J COMPUT VISION, V128, P1867, DOI 10.1007/s11263-020-01303-4; Ustun B., 2014, ARXIV14054047; Vedaldi A, 2008, LECT NOTES COMPUT SC, V5305, P705, DOI 10.1007/978-3-540-88693-8_52; Wang F, 2015, JMLR WORKSH CONF PRO, V38, P1013; Wei X, 2018, IEEE T IMAGE PROCESS, V27, P4838, DOI 10.1109/TIP.2018.2836300; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang R, 2018, PROC CVPR IEEE, P586, DOI 10.1109/CVPR.2018.00068	45	54	54	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900082
C	Zhang, QM; Zhang, J; Liu, W; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Qiming; Zhang, Jing; Liu, Wei; Tao, Dacheng			Category Anchor-Guided Unsupervised Domain Adaptation for Semantic Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unsupervised domain adaptation (UDA) aims to enhance the generalization capability of a certain model from a source domain to a target domain. UDA is of particular significance since no extra effort is devoted to annotating target domain samples. However, the different data distributions in the two domains, or domain shift/discrepancy, inevitably compromise the UDA performance. Although there has been a progress in matching the marginal distributions between two domains, the classifier favors the source domain features and makes incorrect predictions on the target domain due to category-agnostic feature alignment. In this paper, we propose a novel category anchor-guided (CAG) UDA model for semantic segmentation, which explicitly enforces category-aware feature alignment to learn shared discriminative features and classifiers simultaneously. First, the category-wise centroids of the source domain features are used as guided anchors to identify the active features in the target domain and also assign them pseudo-labels. Then, we leverage an anchor-based pixel-level distance loss and a discriminative loss to drive the intra-category features closer and the inter-category features further apart, respectively. Finally, we devise a stagewise training mechanism to reduce the error accumulation and adapt the proposed model progressively. Experiments on both the GTA5!Cityscapes and SYNTHIA!Cityscapes scenarios demonstrate the superiority of our CAG-UDA model over the state-of-the-art methods. The code is available at https://github.com/RogerZhangzz/CAG_UDA.	[Zhang, Qiming; Zhang, Jing; Tao, Dacheng] Univ Sydney, Fac Engn, Sch Comp Sci, UBTECH Sydney AI Ctr, Darlington, NSW 2008, Australia; [Liu, Wei] Tencent AI Lab, Bellevue, WA USA	University of Sydney	Zhang, QM (corresponding author), Univ Sydney, Fac Engn, Sch Comp Sci, UBTECH Sydney AI Ctr, Darlington, NSW 2008, Australia.	qzha2506@uni.sydney.edu.au; jing.zhang1@sydney.edu.au; wl2223@columbia.edu; dacheng.tao@sydney.edu.au		Liu, Wei/0000-0002-3865-8145; ZHANG, JING/0000-0001-6595-7661	Australian Research Council [FL-170100117]; National Natural Science Foundation of China [61806062]	Australian Research Council(Australian Research Council); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by the Australian Research Council Project FL-170100117 and the National Natural Science Foundation of China Project 61806062.	[Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Chang W.C., 2019, ARXIV190502331; Chen Chaoqi, 2018, ARXIV181108585; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen M., 2011, ADV NEURAL INF PROCE, P2456; Chen Y.-H., 2017, P IEEE INT C COMP VI; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Hoffman J, 2016, FCNS WILD PIXELLEVEL; Hoffman J., 2018, INT C MACH LEARN ICM; Hong WX, 2018, PROC CVPR IEEE, P1335, DOI 10.1109/CVPR.2018.00145; Inoue N, 2018, PROC CVPR IEEE, P5001, DOI 10.1109/CVPR.2018.00525; Jiang WH, 2019, IEEE T KNOWL DATA EN, V31, P561, DOI 10.1109/TKDE.2018.2837085; Jiang WH, 2018, PATTERN RECOGN, V81, P484, DOI 10.1016/j.patcog.2018.04.018; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kang G., 2019, ARXIV190100976; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Y., 2019, ARXIV190410620; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu M. -Y., 2016, ADV NEURAL INFORM PR, P469; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long M., 2015, P 32 INT C MACH LEAR, V1, P97; Luo Y., 2018, ARXIV180909478; Pinheiro PO, 2018, PROC CVPR IEEE, P8004, DOI 10.1109/CVPR.2018.00835; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Sankaranarayanan S, 2018, PROC CVPR IEEE, P3752, DOI 10.1109/CVPR.2018.00395; Simonyan Karen, 2015, INT C LEARN REPR; Tao Y, 2017, CHIN CONTR CONF, P4288, DOI 10.23919/ChiCC.2017.8028032; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Vu T.-H., 2018, ARXIV181112833; Wu Z., 2018, EUR C COMP VIS, P518; Xie S., 2018, ICML, P5419; Zhang J, 2017, P IEEE C COMP VIS PA, P7418; Zhang J, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P984, DOI 10.1145/3240508.3240653; Zhang J, 2020, IEEE T IMAGE PROCESS, V29, P72, DOI 10.1109/TIP.2019.2922837; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zou Y., 2018, EUR C COMP VIS, P289	47	54	54	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300040
C	Jakab, T; Gupta, A; Bilen, H; Vedaldi, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jakab, Tomas; Gupta, Ankush; Bilen, Hakan; Vedaldi, Andrea			Unsupervised Learning of Object Landmarks through Conditional Image Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a method for learning landmark detectors for visual objects (such as the eyes and the nose in a face) without any manual supervision. We cast this as the problem of generating images that combine the appearance of the object as seen in a first example image with the geometry of the object as seen in a second example image, where the two examples differ by a viewpoint change and/or an object deformation. In order to factorize appearance and geometry, we introduce a tight bottleneck in the geometry-extraction process that selects and distils geometry-related features. Compared to standard image generation problems, which often use generative adversarial networks, our generation task is conditioned on both appearance and geometry and thus is significantly less ambiguous, to the point that adopting a simple perceptual loss formulation is sufficient. We demonstrate that our approach can learn object landmarks from synthetic image deformations or videos, all without manual supervision, while outperforming state-of-the-art unsupervised landmark detectors. We further show that our method is applicable to a large variety of datasets - faces, people, 3D objects, and digits - without any modifications.	[Jakab, Tomas; Gupta, Ankush; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England; [Bilen, Hakan] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Oxford; University of Edinburgh	Jakab, T (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.	tomj@robots.ox.ac.uk; ankush@robots.ox.ac.uk; hbilen@ed.ac.uk; vedaldi@robots.ox.ac.uk	Bilen, Hakan/AAG-3202-2022; Bilen, Hakan/ACY-3128-2022; Bilen, Hakan/H-9130-2016	Bilen, Hakan/0000-0002-6947-6918	EPSRC AIMS CDT; Clarendon Fund scholarship; ERC [638009-IDIU]	EPSRC AIMS CDT(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Clarendon Fund scholarship; ERC(European Research Council (ERC)European Commission)	We are grateful for the support provided by EPSRC AIMS CDT, ERC 638009-IDIU, and the Clarendon Fund scholarship. We would like to thank James Thewlis for suggestions and support with code and data, and David Novotny and Triantafyllos Afouras for helpful advice.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Nguyen A, 2017, PROC CVPR IEEE, P3510, DOI 10.1109/CVPR.2017.374; [Anonymous], 2015, P ICCV; Bruna Joan, 2016, ICLR; Burgos-Artizzu XP, 2013, IEEE I CONF COMP VIS, P1513, DOI 10.1109/ICCV.2013.191; Carl V., 2016, ADV NEURAL INFORM PR, V29, P613, DOI DOI 10.13016/M26GIH-TNYZ; Charles J., 2013, P BRIT MACH VIS C BR, P1; CHEN Q, 2017, P ICCV, V1; Chen X., 2014, P NIPS; Chung J. S., 2018, ARXIV180605622; Denton E., 2017, P NIPS; Dosovitskiy Alexey, 2016, NEURIPS; Duchon J., 1977, CONSTRUCTIVE THEORY, P85; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 2006, NEURAL COMPUT, V18, P1527, DOI 10.1162/neco.2006.18.7.1527; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kalchbrenner N, 2016, ARXIV161000527; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koestinger M., 2011, ICCV WORKSH, DOI [10.1109/ICCVW.2011.6130513, DOI 10.1109/ICCVW.2011.6130513]; Larsson G., 2016, P ECCV; LeCun Y., 2004, P CVPR; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Nagrani A, 2017, INTERSPEECH, P2616, DOI 10.21437/Interspeech.2017-950; Netzer Y., 2011, NIPS DLW, V2011; Nguyen A., 2016, P NIPS; Patraucean V., 2015, ICLR WORKSH; Pfister T, 2015, IEEE I CONF COMP VIS, P1913, DOI 10.1109/ICCV.2015.222; Pfister T, 2015, LECT NOTES COMPUT SC, V9003, P538, DOI 10.1007/978-3-319-16865-4_35; Pfister Tomas, 2013, P BMVC; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed Scott E, 2015, P NIPS; Shu Z., 2018, P ECCV; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sun Y, 2013, PROC CVPR IEEE, P3476, DOI 10.1109/CVPR.2013.446; Sutskever Ilya, 2009, ADV NEURAL INFORM PR, P2; Suwajanakorn S, 2018, ADV NEUR IN, V31; Thewlis J., 2017, P NIPS; Thewlis J, 2017, IEEE I CONF COMP VIS, P3229, DOI 10.1109/ICCV.2017.348; Villegas R, 2017, PR MACH LEARN RES, V70; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Wahba G., 1990, SPLINE MODELS OBSERV, V59, DOI [10.1137/1.9781611970128, DOI 10.1137/1.9781611970128]; Whitney William F., 2016, ICLR WORKSH; Wiles Olivia, 2018, P BMVC; Xue T., 2016, P NIPS; Yang Y, 2011, PROC CVPR IEEE, P1385, DOI 10.1109/CVPR.2011.5995741; Zhang JM, 2014, LECT NOTES COMPUT SC, V8694, P188, DOI 10.1007/978-3-319-10599-4_13; Zhang YL, 2018, PROC CVPR IEEE, P2472, DOI 10.1109/CVPR.2018.00262; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7; Zhang Zhanpeng, 2016, PAMI	56	54	55	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304006
C	Le, L; Patterson, A; White, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Le, Lei; Patterson, Andrew; White, Martha			Supervised autoencoders: Improving generalization performance with unsupervised regularizers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generalization performance is a central goal in machine learning, with explicit generalization strategies needed when training over-parametrized models, like large neural networks. There is growing interest in using multiple, potentially auxiliary tasks, as one strategy towards this goal. In this work, we theoretically and empirically analyze one such model, called a supervised auto-encoder: a neural network that jointly predicts targets and inputs (reconstruction). We provide a novel generalization result for linear auto-encoders, proving uniform stability based on the inclusion of the reconstruction error-particularly as an improvement on simplistic regularization such as norms. We then demonstrate empirically that, across an array of architectures with a different number of hidden units and activation functions, the supervised auto-encoder compared to the corresponding standard neural network never harms performance and can improve generalization.	[Le, Lei] Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA; [Patterson, Andrew; White, Martha] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 2E8, Canada	Indiana University System; Indiana University Bloomington; University of Alberta	Le, L (corresponding author), Indiana Univ, Dept Comp Sci, Bloomington, IN 47405 USA.	leile@iu.edu; ap3@ualberta.ca; whitem@ualberta.ca	White, Martha/AAF-7066-2020; Patterson, Andrew/AAG-7631-2021	White, Martha/0000-0002-5356-2950; 				Abu-Mostafa Yaser S, 1990, J COMPLEXITY; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Baldi  Pierre, 2014, ARXIV150901240V2; Baldi  Pierre, 1989, NEURAL NETWORKS; Bartlett Peter L, 2002, J MACHINE LEARNING R; Baxter  Jonathan, 2000, J ARTIFICIAL INTELLI; Baxter  Jonathan, 1995, ANN C LEARN THEOR; Ben-David  S, 2003, LECT NOTES COMPUTER; Bengio Y., 2007, ADV NEURAL INFORM PR, DOI 10.7551/mitpress/7503.003.0024; Bousquet Olivier, 2002, J MACHINE LEARNING R; Carreira-Perpinan Miguel A, 2014, INT C ART INT STAT; Caruana R., 1997, MACHINE LEARNING; CARUANA R, 1997, ADV NEURAL INFORM PR; Deterding D. H, 1990, THESIS; Gao  Bin-Bin, 2017, IEEE T IMAGE PROCESS; Gogna  Anupriya, 2016, NEURAL INFORM PROCES; Gottlieb  Lee-Ad, 2016, THEORETICAL COMPUTER; Graham  Benjamin, 2014, ARXIV14114000V2CSLG; Kakade S., 2008, ADV NEURAL INFORM PR; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR; LeCun Y., 1998, P IEEE; Lee Giles  C, 2015, EMNLP; Liu TL, 2017, IEEE T PATTERN ANAL, V39, P227, DOI 10.1109/TPAMI.2016.2544314; Maurer  Andreas, 2015, ARXIV150901240V2; Maurer  Andreas, 2013, ANN C LEARN THEOR; Maurer  Andreas, 2006, J MACHINE LEARNING R; Mohri M., 2018, FDN MACHINE LEARNING; Mohri  Mehryar, 2015, NIPS WORKSH FEAT EXT; Moosavi-Dezfooli Seyed-Mohsen, 2016, IEEE C COMP VIS PATT; Morgan N., 1990, ADV NEURAL INFORM PR; Noh H., 2017, ADV NEURAL INFORM PR; Ranzato M., 2008, INT C MACH LEARN; Ranzato M.A, 2006, ADV NEURAL INFORM PR; Rasmus A., 2015, ADV NEURAL INFORM PR; Srivastava N., 2014, J MACHINE LEARNING R; Vincent P., 2010, J MACHINE LEARNING R; WAGER S., 2013, P 27 ANN C NEUR INF; Weston J., 2008, INT C MACH LEARN; Yaeger  Larry, 1997, ADV NEURAL INFORM PR; Zhang  Tong, 2002, J MACHINE LEARNING R; Zhang YT, 2016, PR MACH LEARN RES, V48	42	54	54	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300011
C	Liu, SC; Long, MS; Wang, JM; Jordan, MI		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Shichen; Long, Mingsheng; Wang, Jianmin; Jordan, Michael I.			Generalized Zero-Shot Learning with Deep Calibration Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ATTRIBUTES	A technical challenge of deep learning is recognizing target classes without seen data. Zero-shot learning leverages semantic representations such as attributes or class prototypes to bridge source and target classes. Existing standard zero-shot learning methods may be prone to overfitting the seen data of source classes as they are blind to the semantic representations of target classes. In this paper, we study generalized zero-shot learning that assumes accessible to target classes for unseen data during training, and prediction on unseen data is made by searching on both source and target classes. We propose a novel Deep Calibration Network (DCN) approach towards this generalized zero-shot learning paradigm, which enables simultaneous calibration of deep networks on the confidence of source classes and uncertainty of target classes. Our approach maps visual features of images and semantic representations of class prototypes to a common embedding space such that the compatibility of seen data to both source and target classes are maximized. We show superior accuracy of our approach over the state of the art on benchmark datasets for generalized zero-shot learning, including AwA, CUB, SUN, and aPY.	[Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, Sch Software, Beijing, Peoples R China; [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China; [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, BNRist, Beijing, Peoples R China; [Liu, Shichen; Long, Mingsheng; Wang, Jianmin] Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China; [Jordan, Michael I.] Univ Calif Berkeley, Berkeley, CA 94720 USA	Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; University of California System; University of California Berkeley	Long, MS (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.; Long, MS (corresponding author), Tsinghua Univ, MOE, KLiss, Beijing, Peoples R China.; Long, MS (corresponding author), Tsinghua Univ, BNRist, Beijing, Peoples R China.; Long, MS (corresponding author), Tsinghua Univ, Res Ctr Big Data, Beijing, Peoples R China.	liushichen95@gmail.com; mingsheng@tsinghua.edu.cn; jimwang@tsinghua.edu.cn; jordan@berkeley.edu	wang, jian/GVS-0711-2022; Jordan, Michael I/C-5253-2013	Jordan, Michael/0000-0001-8935-817X	National Key R&D Program of China [2016YFB1000701]; Natural Science Foundation of China [61772299, 71690231, 61502265]; DARPA Program on Lifelong Learning Machines	National Key R&D Program of China; Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); DARPA Program on Lifelong Learning Machines	This work was supported by the National Key R&D Program of China (2016YFB1000701), the Natural Science Foundation of China (61772299, 71690231, 61502265) and the DARPA Program on Lifelong Learning Machines.	Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Akata Z, 2013, PROC CVPR IEEE, P819, DOI 10.1109/CVPR.2013.111; Al-Halah Z, 2015, IEEE WINT CONF APPL, P837, DOI 10.1109/WACV.2015.116; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Atzmon Yuval, 2018, UAI; Ba JL, 2015, IEEE I CONF COMP VIS, P4247, DOI 10.1109/ICCV.2015.483; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Changpinyo S, 2017, IEEE I CONF COMP VIS, P3496, DOI 10.1109/ICCV.2017.376; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; David E., 1986, PARALLEL DISTRIBUTED, P318, DOI DOI 10.5555/104279.104293; Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Frome Andrea, 2013, NEURIPS; Fu YW, 2016, PROC CVPR IEEE, P5337, DOI 10.1109/CVPR.2016.576; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Fu YW, 2014, IEEE T PATTERN ANAL, V36, P303, DOI 10.1109/TPAMI.2013.128; FU ZY, 2015, PROC CVPR IEEE, P2635, DOI DOI 10.1109/CVPR.2015.7298879; Gavves E, 2015, IEEE I CONF COMP VIS, P2731, DOI 10.1109/ICCV.2015.313; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo CA, 2017, PR MACH LEARN RES, V70; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Kodirov E, 2015, IEEE I CONF COMP VIS, P2452, DOI 10.1109/ICCV.2015.282; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Li FF, 2006, IEEE T PATTERN ANAL, V28, P594, DOI 10.1109/TPAMI.2006.79; Li ZY, 2014, LECT NOTES COMPUT SC, V8694, P350, DOI 10.1007/978-3-319-10599-4_23; Mensink T, 2014, PROC CVPR IEEE, P2441, DOI 10.1109/CVPR.2014.313; Mikolov T., 2013, 9 INT C LEARN REPR, P1; Palatucci Mark, 2009, ADV NEURAL INFORM PR, P1410; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Patterson G, 2012, PROC CVPR IEEE, P2751, DOI 10.1109/CVPR.2012.6247998; Rohrbach M, 2011, PROC CVPR IEEE, P1641, DOI 10.1109/CVPR.2011.5995627; Rohrbach M, 2010, PROC CVPR IEEE, P910, DOI 10.1109/CVPR.2010.5540121; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Scheirer WJ, 2013, IEEE T PATTERN ANAL, V35, P1757, DOI 10.1109/TPAMI.2012.256; Socher Richard, 2013, NEURIPS; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Tsai YHH, 2017, IEEE I CONF COMP VIS, P3591, DOI 10.1109/ICCV.2017.386; Verma V.K., 2018, CVPR; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Wang XY, 2013, IEEE I CONF COMP VIS, P2120, DOI 10.1109/ICCV.2013.264; Wu ZX, 2016, PROC CVPR IEEE, P3112, DOI 10.1109/CVPR.2016.339; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2017, PROC CVPR IEEE, P3077, DOI 10.1109/CVPR.2017.328; Yang Y., 2015, ICLR; Yu FLX, 2013, PROC CVPR IEEE, P771, DOI 10.1109/CVPR.2013.105; Zhang H, 2018, PROC CVPR IEEE, P695, DOI 10.1109/CVPR.2018.00079; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhang Z., 2015, ARXIV151104512; Zhang ZM, 2015, IEEE I CONF COMP VIS, P4166, DOI 10.1109/ICCV.2015.474; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	59	54	55	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302005
C	Narasimhan, M; Lazebnik, S; Schwing, AG		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Narasimhan, Medhini; Lazebnik, Svetlana; Schwing, Alexander G.			Out of the Box: Reasoning with Graph Convolution Nets for Factual Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Accurately answering a question about a given image requires combining observations with general knowledge. While this is effortless for humans, reasoning with general knowledge remains an algorithmic challenge. To advance research in this direction a novel 'fact-based' visual question answering (FVQA) task has been introduced recently along with a large set of curated facts which link two entities, i.e., two possible answers, via a relation. Given a question-image pair, deep network techniques have been employed to successively reduce the large set of facts until one of the two entities of the final remaining fact is predicted as the answer. We observe that a successive process which considers one fact at a time to form a local decision is sub-optimal. Instead, we develop an entity graph and use a graph convolutional network to 'reason' about the correct answer by jointly considering all entities. We show on the challenging FVQA dataset that this leads to an improvement in accuracy of around 7% compared to the state of the art.	[Narasimhan, Medhini; Lazebnik, Svetlana; Schwing, Alexander G.] Univ Illinois, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Narasimhan, M (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	medhini2@illinois.edu; slazebni@illinois.edu; aschwing@illinois.edu		Narasimhan, Medhini/0000-0002-6288-5022	National Science Foundation [1718221, 1563727]; Samsung; 3M; IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR); Amazon Research Award; AWS Machine Learning Research Award	National Science Foundation(National Science Foundation (NSF)); Samsung(Samsung); 3M(3M); IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR)(International Business Machines (IBM)); Amazon Research Award; AWS Machine Learning Research Award	This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221 and Grant No. 1563727, Samsung, 3M, IBM-ILLINOIS Center for Cognitive Computing Systems Research (C3SR), Amazon Research Award, and AWS Machine Learning Research Award. We thank NVIDIA for providing the GPUs used for this research. We also thank Arun Mallya and Aditya Deshpande for their help.	Andreas J., 2016, CVPR; [Anonymous], 2018, CVPR; [Anonymous], 2017, ARXIV170306585; [Anonymous], 2018, ESWC; Antol S., 2015, ICCV; Auer Soren, 2007, ISWC ASWC; Ben-Younes H., 2017, ICCV; Berant J, 2013, EMNLP; Berant J., 2014, ACL; Bordes A., 2014, EMNLP; Bordes A., 2015, ICLR; Bordes A., 2014, ECML; Cai Q., 2013, ACL; Das A., 2016, EMNLP; Das A., 2017, CVPR; Dong L., 2015, ACL; Fader A., 2014, KDD; Fukui A., 2016, EMNLP; Gao H., 2015, NEURIPS; Gordon D., 2018, CVPR; Jabri A., 2016, ECCV; Jain U., 2017, CVPR; Jain U., 2018, ARXIV180311186; Jiang A, 2015, ARXIV151105676; Johnson J., 2017, CVPR; Kim Jin- Hwa, 2016, NEURIPS; Kipf TN, 2016, P INT C LEARN REPR; Kolomiyets O., 2011, INFORM SCI; Krishnamurthy J., 2013, ACL; Kwiatkowski T., 2013, EMNLP; LeCun Y., 1998, IEEE; Li Y., 2017, ARXIV170907192; Liang P., 2013, COMPUTATIONAL LINGUI; Lu J., 2016, NEURIPS; Ma L., 2016, AAAI; Malinowski M., 2015, P INT C COMP VIS; Malinowski M., 2014, NEURIPS; Mostafazadeh N., 2016, ARXIV160306059; Narasimhan K., 2016, EMNLP; Narasimhan M., 2018, ECCV; Pennington Jeffrey, 2014, EMNLP; Reddy S., 2016, ACL; Ren M., 2015, NEURIPS; Schwartz I., 2017, NEURIPS; Shih K. J., 2016, CVPR; Speer Robyn, 2017, AAAI; Tandon N., 2014, WSDM; Unger C., 2012, WWW; Vijayakumar Ashwin K, 2016, ARXIV161002424; Wang P., 2018, TPAMI; Wang P., 2017, IJCAI; Wu Q., 2016, P CVPR; Wu Q., 2016, ARXIV160302814; Xiao C., 2016, ACL; Xiong C., 2016, P INT C MACHINE LEAR; Xu H., 2016, P EUR C COMP VIS ECC; Yang Z., 2016, CVPR; Yao X., 2014, ACL; Yih Scott Wen-tau, 2015, ACL IJCNLP; Yu L, 2015, IEEE INT C COMP VIS; Zettlemoyer L.S., 2005, ACL; Zettlemoyer L.S., 2005, UAI; Zhang P., 2015, ARXIV151105099; Zhang Y., 2016, ARXIV160600979; Zhou Bolei, 2015, ARXIV151202167; Zhu Y., 2016, CVPR; Zhu Y., 2015, CORR; Zitnick C. L., 2016, AI MAGAZINE	68	54	57	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302065
C	Chierichetti, F; Kumar, R; Lattanzi, S; Vassilvitskii, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chierichetti, Flavio; Kumar, Ravi; Lattanzi, Silvio; Vassilvitskii, Sergei			Fair Clustering Through Fairlets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the question of fair clustering under the disparate impact doctrine, where each protected class must have approximately equal representation in every cluster. We formulate the fair clustering problem under both the k-center and the k-median objectives, and show that even with two protected classes the problem is challenging, as the optimum solution can violate common conventions-for instance a point may no longer be assigned to its nearest cluster center! En route we introduce the concept of fairlets, which are minimal sets that satisfy fair representation while approximately preserving the clustering objective. We show that any fair clustering problem can be decomposed into first finding good fairlets, and then using existing machinery for traditional clustering algorithms. While finding good fairlets can be NP-hard, we proceed to obtain efficient approximation algorithms based on minimum cost flow. We empirically demonstrate the price of fairness by quantifying the value of fair clustering on real-world datasets with sensitive attributes.	[Chierichetti, Flavio] Sapienza Univ, Dipartimento Informat, Rome, Italy; [Kumar, Ravi] Google Res, 1600 Amphitheater Pkwy, Mountain View, CA 94043 USA; [Lattanzi, Silvio; Vassilvitskii, Sergei] Google Res, 76 9th Ave, New York, NY 10011 USA	Sapienza University Rome; Google Incorporated; Google Incorporated	Chierichetti, F (corresponding author), Sapienza Univ, Dipartimento Informat, Rome, Italy.		Jeong, Yongwook/N-7413-2016		ERC [DMAP 680153]; Google Focused Research Award; SIR Grant [RBSI14Q743]	ERC(European Research Council (ERC)European Commission); Google Focused Research Award(Google Incorporated); SIR Grant	Flavio Chierichetti was supported in part by the ERC Starting Grant DMAP 680153, by a Google Focused Research Award, and by the SIR Grant RBSI14Q743.	Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1; [Anonymous], 2017, 23180 NBER; Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Biddle, 2006, ADVERSE IMPACT TEST; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Dwork C., 2012, P 3 INN THEOR COMP S, P214; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Joseph Matthew, 2016, NIPS, P325; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Kanungo T, 2002, IEEE T PATTERN ANAL, V24, P881, DOI 10.1109/TPAMI.2002.1017616; Kleinberg Jon M., 2017, ITCS; Kohavi R., 1996, KDD-96 Proceedings. Second International Conference on Knowledge Discovery and Data Mining, P202; Li S, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P901; Lichman M., 2013, UCI MACHINE LEARNING; Luong Binh Thanh, 2011, P 17 ACM SIGKDD INT, P502; Moro S, 2014, DECIS SUPPORT SYST, V62, P22, DOI 10.1016/j.dss.2014.03.001; Xu R., 2009, CLUSTERING; Zafar Muhammad Bilal, 2017, AISTATS, P259; Zemel R., 2013, P INT C MACH LEARN, P325	21	54	54	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405011
C	MOODY, J; UTANS, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MOODY, J; UTANS, J			PRINCIPLED ARCHITECTURE SELECTION FOR NEURAL NETWORKS - APPLICATION TO CORPORATE BOND RATING PREDICTION	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	54	59	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						683	690						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00084
C	Cadene, R; Dancette, C; Ben-Younes, H; Cord, M; Parikh, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cadene, Remi; Dancette, Corentin; Ben-Younes, Hedi; Cord, Matthieu; Parikh, Devi			RUBi: Reducing Unimodal Biases for Visual Question Answering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Visual Question Answering (VQA) is the task of answering questions about an image. Some VQA models often exploit unimodal biases to provide the correct answer without using the image information. As a result, they suffer from a huge drop in performance when evaluated on data outside their training set distribution. This critical issue makes them unsuitable for real-world settings. We propose RUBi, a new learning strategy to reduce biases in any VQA model. It reduces the importance of the most biased examples, i.e. examples that can be correctly classified without looking at the image. It implicitly forces the VQA model to use the two input modalities instead of relying on statistical regularities between the question and the answer. We leverage a question-only model that captures the language biases by identifying when these unwanted regularities are used. It prevents the base VQA model from learning them by influencing its predictions. This leads to dynamically adjusting the loss in order to compensate for biases. We validate our contributions by surpassing the current state-of-the-art results on VQA-CP v2. This dataset is specifically designed to assess the robustness of VQA models when exposed to different question biases at test time than what was seen during training. Our code is available: github.com/cdancette/rubi.bootstrap.pytorch	[Cadene, Remi; Dancette, Corentin; Ben-Younes, Hedi; Cord, Matthieu] Sorbonne Univ, CNRS, LIP6, 4 Pl Jussieu, F-75005 Paris, France; [Parikh, Devi] Facebook AI Res, Menlo Pk, CA 94025 USA; [Parikh, Devi] Georgia Inst Technol, Atlanta, GA 30332 USA	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Facebook Inc; University System of Georgia; Georgia Institute of Technology	Cadene, R (corresponding author), Sorbonne Univ, CNRS, LIP6, 4 Pl Jussieu, F-75005 Paris, France.	remi.cadene@lip6.fr; corentin.dancette@lip6.fr; hedi.ben-younes@lip6.fr; matthieu.cord@lip6.fr; parkih@gatech.edu			ANR [ANR-11-LABX-65]	ANR(French National Research Agency (ANR))	The effort from Sorbonne University was supported within the Labex SMART supported by French state funds managed by the ANR within the Investissements d'Avenir programme under reference ANR-11-LABX-65.	Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Agrawal Aishwarya, 2016, ARXIV160607356; Anand A., 2018, ARXIV181105013; Anderson P, 2018, PROC CVPR IEEE, P3674, DOI 10.1109/CVPR.2018.00387; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Ben-Younes H, 2019, AAAI CONF ARTIF INTE, P8102; Ben-younes H, 2017, IEEE I CONF COMP VIS, P2631, DOI 10.1109/ICCV.2017.285; Cadene R, 2019, PROC CVPR IEEE, P1989, DOI 10.1109/CVPR.2019.00209; Chao Wei-Lun, 2018, NAACL; Das Abhishek, 2016, C EMP MATH NAT LANG; Das A, 2017, IEEE INT C ELECTR TA; de Vries Harm, 2017, C COMP VIS PATT REC; Gordon J., 2013, P 2013 WORKSH AUT KN, V13, P25, DOI DOI 10.1145/2509558.2509563; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Gurari D, 2018, PROC CVPR IEEE, P3608, DOI 10.1109/CVPR.2018.00380; Hendricks LA, 2018, LECT NOTES COMPUT SC, V11207, P793, DOI 10.1007/978-3-030-01219-9_47; Hu RH, 2018, LECT NOTES COMPUT SC, V11211, P55, DOI 10.1007/978-3-030-01234-2_4; Hudson DA, 2019, PROC CVPR IEEE, P6693, DOI 10.1109/CVPR.2019.00686; Jabri A, 2016, LECT NOTES COMPUT SC, V9912, P727, DOI 10.1007/978-3-319-46484-8_44; Jia S, 2018, LECT NOTES COMPUT SC, V11191, P164, DOI 10.1007/978-3-030-01768-2_14; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Kafle K, 2017, IEEE I CONF COMP VIS, P1983, DOI 10.1109/ICCV.2017.217; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kim JH, 2018, ADV NEUR IN, V31; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lu CW, 2016, LECT NOTES COMPUT SC, V9905, P852, DOI 10.1007/978-3-319-46448-0_51; Lu JH, 2016, PROCEEDINGS OF THE 2015 INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND ENGINEERING TECHNOLOGY (CSET2015), MEDICAL SCIENCE AND BIOLOGICAL ENGINEERING (MSBE2015), P289; Manjunatha Varun, 2019, CVPR; Mikolov T., 2013, ARXIV; Misra I, 2016, PROC CVPR IEEE, P2930, DOI 10.1109/CVPR.2016.320; Peng Gao, 2019, CVPR; Ramakrishnan S, 2018, ADV NEUR IN, V31; Rohrbach Anna, 2018, EMNLP; Shi JX, 2019, PROC CVPR IEEE, P8368, DOI 10.1109/CVPR.2019.00857; Shrestha R, 2019, PROC CVPR IEEE, P10464, DOI 10.1109/CVPR.2019.01072; Stock P, 2018, LECT NOTES COMPUT SC, V11210, P504, DOI 10.1007/978-3-030-01231-1_31; Thomason Jesse, 2019, NACL; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Wu CF, 2018, ADV NEUR IN, V31; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu Z, 2018, IEEE T NEUR NET LEAR, V29, P5947, DOI 10.1109/TNNLS.2018.2817340; Zellers R, 2019, PROC CVPR IEEE, P6713, DOI 10.1109/CVPR.2019.00688; Zhao Jieyu, 2017, P 2017 C EMP METH NA, P2941, DOI [10.18653/v1/D17-1323, DOI 10.18653/V1/D17-1323]	45	53	54	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300076
C	You, ZH; Yan, K; Ye, JM; Ma, M; Wang, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		You, Zhonghui; Yan, Kun; Ye, Jinmian; Ma, Meng; Wang, Ping			Gate Decorator: Global Filter Pruning Method for Accelerating Deep Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Filter pruning is one of the most effective ways to accelerate and compress convolutional neural networks (CNNs). In this work, we propose a global filter pruning algorithm called Gate Decorator, which transforms a vanilla CNN module by multiplying its output by the channel-wise scaling factors (i.e. gate). When the scaling factor is set to zero, it is equivalent to removing the corresponding filter. We use Taylor expansion to estimate the change in the loss function caused by setting the scaling factor to zero and use the estimation for the global filter importance ranking. Then we prune the network by removing those unimportant filters. After pruning, we merge all the scaling factors into its original module, so no special operations or structures are introduced. Moreover, we propose an iterative pruning framework called Tick-Tock to improve pruning accuracy. The extensive experiments demonstrate the effectiveness of our approaches. For example, we achieve the state-of-the-art pruning ratio on ResNet-56 by reducing 70% FLOPs without noticeable loss in accuracy. For ResNet-50 on ImageNet, our pruned model with 40% FLOPs reduction outperforms the baseline model by 0.31% in top-1 accuracy. Various datasets are used, including CIFAR-10, CIFAR-100, CUB-200, ImageNet ILSVRC-12 and PASCAL VOC 2011.	[You, Zhonghui; Yan, Kun; Wang, Ping] Peking Univ, Sch Software & Microelect, Beijing, Peoples R China; [Ye, Jinmian] Momenta, Beijing, Peoples R China; [Ma, Meng; Wang, Ping] Peking Univ, Natl Engn Res Ctr Software Engn, Beijing, Peoples R China; [Wang, Ping] Minist Educ, Key Lab High Confidence Software Technol PKU, Beijing, Peoples R China	Peking University; Peking University	Wang, P (corresponding author), Peking Univ, Sch Software & Microelect, Beijing, Peoples R China.; Ma, M; Wang, P (corresponding author), Peking Univ, Natl Engn Res Ctr Software Engn, Beijing, Peoples R China.; Wang, P (corresponding author), Minist Educ, Key Lab High Confidence Software Technol PKU, Beijing, Peoples R China.	zhonghui@pku.edu.cn; kyan2018@pku.edu.cn; jinmian.y@gmail.com; mameng@pku.edu.cn; pwang@pku.edu.cn			National Key RAMP;D Program of China [2017YFB1200700]; Peking University Medical Cross Research Seed Fund; National Natural Science Foundation of China [61701007]	National Key RAMP;D Program of China; Peking University Medical Cross Research Seed Fund; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported by National Key R&D Program of China (Grant no.2017YFB1200700), Peking University Medical Cross Research Seed Fund and National Natural Science Foundation of China (Grant no.61701007).	Alvarez Jose M, 2016, ADV NEURAL INFORM PR, P2270; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen Yunpeng, 2019, ABS190405049 ARXIV; Courbariaux M., 2016, ABS160202830 ARXIV; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dentinel Zarembaw, 2014, NEURIPS, P1269; Ding Xiaohan, 2019, ABS190403837 ARXIV; Everingham M., PASCAL VISUAL OBJECT; Guo YW, 2016, ADV NEUR IN, V29; Han S., 2016, P 4 INT C LEARN REPR, P1; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He Y, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2234; He Y, 2019, PROC CVPR IEEE, P4335, DOI 10.1109/CVPR.2019.00447; He YH, 2018, LECT NOTES COMPUT SC, V11211, P815, DOI 10.1007/978-3-030-01234-2_48; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Howard A.G., 2017, MOBILENETS EFFICIENT; Hu Hengyuan, 2016, ARXIV; Hu J, 2018, PROC CVPR IEEE, P7132, DOI [10.1109/CVPR.2018.00745, 10.1109/TPAMI.2019.2913372]; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li H., 2017, P INT C LEARN REPR I, P1; Lin SH, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2425; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu Z, 2019, PATTERN ANAL APPL, V22, P1527, DOI 10.1007/s10044-019-00792-5; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Ma Ningning, 2018, P EUR C COMP VIS ECC; MEENA N, 2018, ADV NEURAL INFORM PR, P333; Molchanov P., 2017, P INT C LEARN REPR I, P1; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Real E., 2019, ASS ADVANCEMENT ARTI; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Singh Pravendra, 2019, ABS190304120 ARXIV; Smith L.N., 2017, ABS170807120 CORR; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tesauro G, 1994, ADV NEURAL INFORM PR, V6, P263; Wada Kentaro, PYTORCH IMPLEMENTATI; Wah C., 2011, TECH REP; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Xie Sirui, 2019, ICLR, V1, P13; Yang W, 2017, IEEE I CONF COMP VIS, P1290, DOI 10.1109/ICCV.2017.144; Ye J., 2018, RETHINKING SMALLERNO; Ye JM, 2018, PROC CVPR IEEE, P9378, DOI 10.1109/CVPR.2018.00977; Yu CQ, 2018, LECT NOTES COMPUT SC, V11217, P334, DOI 10.1007/978-3-030-01261-8_20; Yu RC, 2018, PROC CVPR IEEE, P9194, DOI 10.1109/CVPR.2018.00958; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhou A, 2017, INCREMENTAL NETWORK; Zhuang BH, 2018, PROC CVPR IEEE, P7920, DOI 10.1109/CVPR.2018.00826; Zhuang ZW, 2018, ADV NEUR IN, V31; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	55	53	53	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302016
C	Zaheer, M; Reddi, SJ; Sachan, D; Kale, S; Kumar, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zaheer, Manzil; Reddi, Sashank J.; Sachan, Devendra; Kale, Satyen; Kumar, Sanjiv			Adaptive Methods for Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Adaptive gradient methods that rely on scaling gradients down by the square root of exponential moving averages of past squared gradients, such RMS PROP, ADAM, ADADELTA have found wide application in optimizing the nonconvex problems that arise in deep learning. However, it has been recently demonstrated that such methods can fail to converge even in simple convex optimization settings. In this work, we provide a new analysis of such methods applied to nonconvex stochastic optimization problems, characterizing the effect of increasing minibatch size. Our analysis shows that under this scenario such methods do converge to stationarity up to the statistical limit of variance in the stochastic gradients (scaled by a constant factor). In particular, our result implies that increasing minibatch sizes enables convergence, thus providing a way to circumvent the nonconvergence issues. Furthermore, we provide a new adaptive optimization algorithm, YOGI, which controls the increase in effective learning rate, leading to even better performance with similar theoretical guarantees on convergence. Extensive experiments show that YOGI with very little hyperparameter tuning outperforms methods such as ADAM in several challenging machine learning tasks.	[Zaheer, Manzil; Reddi, Sashank J.; Kale, Satyen; Kumar, Sanjiv] Google Res, Mountain View, CA 94043 USA; [Sachan, Devendra] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Google Incorporated; Carnegie Mellon University	Zaheer, M (corresponding author), Google Res, Mountain View, CA 94043 USA.	manzilzaheer@google.com; sashank@google.com; dsachan@andrew.cmu.edu; satyenkale@google.com; sanjivk@google.com	Zaheer, Manzil/ABG-6249-2021					Agarwal Naman, 2016, CORR; Allen Zhu Z., 2016, CORR; Bernstein J., 2018, ARXIV180204434, V80, P560; Carmon Yair, 2016, CORR; Chiu J.P., 2016, T ASS COMPUTATIONAL, V4, P357, DOI DOI 10.1162/TACL_A_00104; Clark Kevin, SEMISUPERVISED LEARN; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Ghadimi Saeed, 2014, MATH PROGRAM, V155, P267; Hazan Elad, 2015, ADV NEURAL INFORM PR, P1585; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton, 2016, ARXIV PREPRINT ARXIV; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Jin Chi, 2017, CORR; Keskar N. S., 2017, ARXIV171207628; Kingma D.P, P 3 INT C LEARNING R; Li Xingguo, 2016, ICML; Luong Minh-Thang, 2015, INT WORKSH SPOK LANG; Ma XZ, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1064; Martens J., 2010, P 27 INT C MACH LEAR, P735; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Nesterov Y., 2018, APPL OPTIMIZATION; Reddi SJ, 2018, PR MACH LEARN RES, V84; Reddi SJ, 2016, PR MACH LEARN RES, V48; Reddi Sashank J., 2016, CORR; Reddi Sashank J., 2018, INT C LEARN REPR; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; Schweter Stefan, 2018, NEURAL MACHINE TRANS; Sennrich Rico, 2015, ARXIV150807909; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals Oriol, 2012, ARTIF INTELL, P1261; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Zaheer Manzil, 2017, P ADV NEUR INF PROC, P3394; Zeiler Matthew D, 2012, ARXIV12125701	40	53	54	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004036
C	Karakus, C; Sun, YF; Diggavi, S; Yin, WT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Karakus, Can; Sun, Yifan; Diggavi, Suhas; Yin, Wotao			Straggler Mitigation in Distributed Optimization Through Data Encoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Slow running or straggler tasks can significantly reduce computation speed in distributed computation. Recently, coding-theory-inspired approaches have been applied to mitigate the effect of straggling, through embedding redundancy in certain linear computational steps of the optimization algorithm, thus completing the computation without waiting for the stragglers. In this paper, we propose an alternate approach where we embed the redundancy directly in the data itself, and allow the computation to proceed completely oblivious to encoding. We propose several encoding schemes, and demonstrate that popular batch algorithms, such as gradient descent and L-BFGS, applied in a coding-oblivious manner, deterministically achieve sample path linear convergence to an approximate solution of the original problem, using an arbitrarily varying subset of the nodes at each iteration. Moreover, this approximation can be controlled by the amount of redundancy and the number of nodes used in each iteration. We provide experimental results demonstrating the advantage of the approach over uncoded and data replication strategies.	[Karakus, Can; Diggavi, Suhas; Yin, Wotao] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [Sun, Yifan] Technicolor Res, Los Altos, CA USA	University of California System; University of California Los Angeles; Technicolor SA	Karakus, C (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.	karakus@ucla.edu; Yifan.Sun@technicolor.com; suhasdiggavi@ucla.edu; wotaoyin@math.ucla.edu	Jeong, Yongwook/N-7413-2016		NSF [1314937, 1423271]	NSF(National Science Foundation (NSF))	This work was supported in part by NSF grants 1314937 and 1423271.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Berahas A.L., 2016, ADV NEURAL INFORM PR, P1055; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Candes EJ, 2006, IEEE T INFORM THEORY, V52, P5406, DOI 10.1109/TIT.2006.885507; Da Wang, 2015, ACM SIGMETRICS Performance Evaluation Review, V43, P7, DOI 10.1145/2847220.2847223; Drineas P, 2011, NUMER MATH, V117, P219, DOI 10.1007/s00211-010-0331-6; Dutta S., 2016, P ADV NEUR INF PROC; Fickus M, 2012, LINEAR ALGEBRA APPL, V436, P1014, DOI 10.1016/j.laa.2011.06.027; GEMAN S, 1980, ANN PROBAB, V8, P252, DOI 10.1214/aop/1176994775; Goethals J. M., 1967, CANAD J MATH; Karakus C., 2017, STRAGGLER MITIGATION; Karakus C, 2017, IEEE INT SYMP INFO, P2890, DOI 10.1109/ISIT.2017.8007058; Lee K, 2016, IEEE INT SYMP INFO, P1143, DOI 10.1109/ISIT.2016.7541478; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Mokhtari A, 2015, J MACH LEARN RES, V16, P3151; Paley R.E.A.C., 1933, J MATH PHYS, V12, P311, DOI DOI 10.1002/SAPM1933121311; Pilanci M, 2015, IEEE T INFORM THEORY, V61, P5096, DOI 10.1109/TIT.2015.2450722; Riedl J, 1998, MOVIELENS DATASET; Shenker S., 2013, 10 USENIX S NETWORKE, P185; Szollosi F, 2013, LINEAR ALGEBRA APPL, V438, P1962, DOI 10.1016/j.laa.2011.05.034; Tandon Rashish, 2016, ML SYST WORKSH MLSYS; WELCH LR, 1974, IEEE T INFORM THEORY, V20, P397, DOI 10.1109/tit.1974.1055219; Yadwadkar N. J., 2016, J MACH LEARN RES, V17, P1	24	53	53	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405050
C	Choy, CB; Gwak, J; Savarese, S; Chandraker, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Choy, Christopher B.; Gwak, JunYoung; Savarese, Silvio; Chandraker, Manmohan			Universal Correspondence Network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present a deep learning framework for accurate visual correspondences and demonstrate its effectiveness for both geometric and semantic matching, spanning across rigid motions to intra-class shape or appearance variations. In contrast to previous CNN-based approaches that optimize a surrogate patch similarity objective, we use deep metric learning to directly learn a feature space that preserves either geometric or semantic similarity. Our fully convolutional architecture, along with a novel correspondence contrastive loss allows faster training by effective reuse of computations, accurate gradient computation through the use of thousands of examples per image pair and faster testing with O(n) feed forward passes for n keypoints, instead of O(n(2)) for typical patch similarity methods. We propose a convolutional spatial transformer to mimic patch normalization in traditional features like SIFT, which is shown to dramatically boost accuracy for semantic correspondences across intra-class shape variations. Extensive experiments on KITTI, PASCAL, and CUB-2011 datasets demonstrate the significant advantages of our features over prior works that use either hand-constructed or learned features.	[Choy, Christopher B.; Gwak, JunYoung; Savarese, Silvio] Stanford Univ, Stanford, CA 94305 USA; [Chandraker, Manmohan] NEC Labs Amer Inc, Princeton, NJ USA	Stanford University; NEC Corporation	Choy, CB (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	chrischoy@ai.stanford.edu; jgwak@ai.stanford.edu; ssilvio@stanford.edu; manu@nec-labs.com	Chandraker, Manmohan/AAU-4762-2021		Korea Foundation of Advanced Studies; Toyota [122282]; ONR [N00014-13-1-0761]; MURI [WF911NF-15-1-0479]	Korea Foundation of Advanced Studies; Toyota; ONR(Office of Naval Research); MURI(MURI)	This work was part of C. Choy's internship at NEC Labs. We acknowledge the support of Korea Foundation of Advanced Studies, Toyota Award #122282, ONR N00014-13-1-0761, and MURI WF911NF-15-1-0479.	Agrawal Pulkit, 2015, CVPR, V2; Alcantarilla P. F., 2012, ECCV; [Anonymous], 2015, P 28 INT C NEUR INF; [Anonymous], 2010, CALIFORNIA I TECHNOL; Bay H., 2008, CVIU; Bourdev L., 2009, ICCV; Bromley J., 1994, NIPS; Butler D. J., 2012, ECCV; Chopra S., 2005, CVPR, V1; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Everingham M., PASCAL VISUAL OBJECT; Fua P., 2016, ECCV; Garcia V., 2010, ICIP; Girshick R, 2015, ARXIV E PRINTS; Hadsell R., 2006, P CVPR; He Kaiming, 2014, ECCV; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Kanazawa A., 2014, DEEP LEARN REPR LEAR; Kanazawa A., 2016, ARXIV E PRINTS; Kim J., 2013, CVPR; Liu C, 2011, IEEE T PATTERN ANAL, V33, P978, DOI 10.1109/TPAMI.2010.147; Long J., 2015, PROC CVPR IEEE, DOI 10.1109/TPAMI.2016.2572683TPAMI.2016.2572683; Long J. L., 2014, NIPS; Lowe D. G., 2004, IJCV; Matas J., 2002, BMVC; Menze Moritz, 2015, CVPR; Revaud J., 2015, DEEPMATCHING HIERARC; Schroff F., 2015, CVPR; Song H. O., 2016, COMPUTER VISION PATT; Tola E., 2010, PAMI; Wang J, 2014, CVPR; Yang H., 2014, CVPR; Yang Y., 2013, PAMI; Zagoruyko S., 2015, CVPR; Zbontar J., 2015, CVPR; Zhou T., 2015, CVPR	36	53	53	0	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704012
C	Tenenbaum, JB; Griffiths, TL		Leen, TK; Dietterich, TG; Tresp, V		Tenenbaum, JB; Griffiths, TL			Structure learning in human causal induction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				POWER	We use graphical models to explore the question of how people learn simple causal relationships from data. The two leading psychological theories can both be seen as estimating the parameters of a fixed graph. We argue that a complete account of causal induction should also consider how people learn the underlying causal graph structure, and we propose to model this inductive process as a Bayesian inference. Our argument is supported through the discussion of three data sets.	Stanford Univ, Dept Psychol, Stanford, CA 94305 USA	Stanford University	Tenenbaum, JB (corresponding author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.							Anderson J. R., 1990, ADAPTIVE CHARACTER T; [Anonymous], 2000, CAUSALITY; BUEHNER MJ, 1997, P 19 ANN C COGN SCI; Cheng PW, 1997, PSYCHOL REV, V104, P367, DOI 10.1037/0033-295X.104.2.367; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Glymour C, 1998, MIND MACH, V8, P39, DOI 10.1023/A:1008234330618; Lober K, 2000, PSYCHOL REV, V107, P195, DOI 10.1037/0033-295X.107.1.195	7	53	53	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						59	65						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800009
C	Hofmann, T; Puzicha, J; Jordan, MI		Kearns, MS; Solla, SA; Cohn, DA		Hofmann, T; Puzicha, J; Jordan, MI			Learning from dyadic data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				EM ALGORITHM	Dyadic data refers to a domain with two finite sets of objects in which observations are made for dyads, i.e., pairs with one clement front either set. This type of data arises naturally in many application ranging from computational linguistics and information retrieval to preference analysis and computer vision. In this paper, we present a systematic, domain-independent. framework of learning from dyadic data by statistical mixture models. Our approach covers different models with nat and hierarchical latent class structures. We propose an annealed version of the standard Ehl algorithm for model fitting which is empirically evaluated on a variety of data sets from different, domains.	MIT, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Hofmann, T (corresponding author), MIT, Ctr Biol & Computat Learning, 77 Massachusetts Ave, Cambridge, MA 02139 USA.		Jordan, Michael I/C-5253-2013					DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Hofmann T, 1998, IEEE T PATTERN ANAL, V20, P803, DOI 10.1109/34.709593; HOFMANN T, 1998, 1625 MIT ART INT LAB; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; MEILA M, 1998, ADV NEURAL INFORMATI, V10; Pereira Fernando, 1993, P 31 ANN M ASS COMP, P183, DOI DOI 10.3115/981574.981598; ROSE K, 1990, PHYS REV LETT, V65, P945, DOI 10.1103/PhysRevLett.65.945; SAUL L, 1997, P 2 INT C EMP METH N	8	53	56	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						466	472						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700066
C	Lewicki, MS; Sejnowski, TJ		Kearns, MS; Solla, SA; Cohn, DA		Lewicki, MS; Sejnowski, TJ			Coding time-varying signals using sparse, shift-invariant representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				FREQUENCY	A common way to represent a time series is to divide it into short duration blocks, each of which is then represented by a set of basis functions. A limitation of this approach, however, is that the temporal alignment of the basis functions with the underlying structure in the time series is arbitrary. We present an algorithm for encoding a time series that does not require blocking the data. The algorithm finds an efficient representation by inferring the best temporal positions for functions in a kernel basis. These can have arbitrary temporal extent and are not constrained to be orthogonal. This allows the model to capture structure in the signal that may occur at arbitrary temporal positions and preserves the relative temporal structure of underlying events. The model is shown to be equivalent to a very sparse and highly overcomplete basis. Under this model, the mapping from the data to the representation is nonlinear, but can be computed efficiently. This form also allows the use of existing methods for adapting the basis itself to data. This approach is applied to speech data and results in a shift invariant, spike-like representation that resembles coding in the cochlear nerve.	Salk Inst Biol Studies, Howard Hughes Med Inst, Computat Neurobiol Lab, La Jolla, CA 92037 USA	Howard Hughes Medical Institute; Salk Institute	Lewicki, MS (corresponding author), Salk Inst Biol Studies, Howard Hughes Med Inst, Computat Neurobiol Lab, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.		Sejnowski, Terrence/AAV-5558-2021					CARDOSO JF, 1997, IEEE SIGNAL PROCESS, V4, P109; CHEN S, 1996, ATOMIC DECOMPOSITION; COIFMAN RR, 1992, IEEE T INFORM THEORY, V38, P713, DOI 10.1109/18.119732; DAUBECHIES I, 1990, IEEE T INFORM THEORY, V36, P961, DOI 10.1109/18.57199; LEWICKI M, 1998, UNPUB NEURAL COMPUTA; MALLAT SG, 1993, IEEE T SIGNAL PROCES, V41, P3397, DOI 10.1109/78.258082; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725	7	53	55	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						730	736						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700103
C	DARKEN, C; MOODY, J		MOODY, JE; HANSON, SJ; LIPPMANN, RP		DARKEN, C; MOODY, J			TOWARDS FASTER STOCHASTIC GRADIENT SEARCH	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	53	54	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						1009	1016						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00124
C	Bagdasaryan, E; Poursaeed, O; Shmatikov, V		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bagdasaryan, Eugene; Poursaeed, Omid; Shmatikov, Vitaly			Differential Privacy Has Disparate Impact on Model Accuracy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Differential privacy (DP) is a popular mechanism for training machine learning models with bounded leakage about the presence of specific points in the training data. The cost of differential privacy is a reduction in the model's accuracy. We demonstrate that in the neural networks trained using differentially private stochastic gradient descent (DP-SGD), this cost is not borne equally: accuracy of DP models drops much more for the underrepresented classes and subgroups. For example, a gender classification model trained using DP-SGD exhibits much lower accuracy for black faces than for white faces. Critically, this gap is bigger in the DP model than in the non-DP model, i.e., if the original model is unfair, the unfairness becomes worse once DP is applied. We demonstrate this effect for a variety of tasks and models, including sentiment analysis of text and image classification. We then explain why DP training mechanisms such as gradient clipping and noise addition have disproportionate effect on the underrepresented and more complex subgroups, resulting in a disparate reduction of model accuracy.	[Bagdasaryan, Eugene; Poursaeed, Omid; Shmatikov, Vitaly] Cornell Tech, New York, NY 10044 USA		Bagdasaryan, E (corresponding author), Cornell Tech, New York, NY 10044 USA.	eugene@cs.cornell.edu; op63@cornell.edu; shmat@cs.cornell.edu		Poursaeed, Omid/0000-0001-6073-8777	NSF [1611770, 1704296, 1700832, 1642120]; Google Faculty Research Award; Eric and Wendy Schmidt by recommendation of the Schmidt Futures program	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); Eric and Wendy Schmidt by recommendation of the Schmidt Futures program	This research was supported in part by the NSF grants 1611770, 1704296, 1700832, and 1642120, the generosity of Eric and Wendy Schmidt by recommendation of the Schmidt Futures program, and a Google Faculty Research Award.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Agarwal A, 2018, PR MACH LEARN RES, V80; Beutel A., 2017, FAT ML; Blodgett S. L., 2018, ACL; Blodgett S. L., 2016, EMNLP; Buda M, 2018, NEURAL NETWORKS, V106, P249, DOI 10.1016/j.neunet.2018.07.011; Buolamwini J., 2018, C FAIRN ACC TRANSP, P77; Chawla NV, 2002, J ARTIF INTELL RES, V16, P321, DOI 10.1613/jair.953; Cui Yin, 2019, CVPR, P9268; Cummings R., 2019, FAIR UMAP; Douzas G, 2018, EXPERT SYST APPL, V91, P464, DOI 10.1016/j.eswa.2017.09.030; Dwork C., 2011, ENCY CRYPTOGRAPHY SE, P338, DOI DOI 10.1007/978-1-4419-5906-5_752; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2011, COMMUN ACM, V54, P86, DOI 10.1145/1866739.1866758; Elazar Y., 2018, P 2018 C EMP METH NA, P11; Geyer R. C., 2018, NEURIPS; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Horn G.V., 2018, IEEE C COMP VIS PATT; Jagielski M., 2019, ICML; Kearns M, 2018, PR MACH LEARN RES, V80; KEARNS MJ, 1994, MACH LEARN, V17, P115, DOI 10.1007/BF00993468; Kuppam S., 2019, ARXIV190512744; McMahan B., 2017, P 20 INT C ART INT S, P1273; McMahan H. B., 2018, P PRIV PRES MACH LEA; McMahan H. B., 2018, PROC INT C LEARN REP, P1; Merler Michele, 2019, ARXIV190110436; Mironov I., 2017, CSF; Mohri M, 2019, PR MACH LEARN RES, V97; Neelakantan A., 2015, ARXIV151106807; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Yaghini M., 2019, ARXIV190600389; Yeom S, 2018, P IEEE COMPUT SECUR, P268, DOI 10.1109/CSF.2018.00027; Zhang ZZ, 2017, PROC CVPR IEEE, P3549, DOI 10.1109/CVPR.2017.378; Zhu W., 2019, ARXIV190208534	38	52	53	3	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907017
C	Laine, S; Karras, T; Lehtinen, J; Aila, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Laine, Samuli; Karras, Tero; Lehtinen, Jaakko; Aila, Timo			High-Quality Self-Supervised Deep Image Denoising	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We describe a novel method for training high-quality image denoising models based on unorganized collections of corrupted images. The training does not need access to clean reference images, or explicit pairs of corrupted images, and can thus be applied in situations where such data is unacceptably expensive or impossible to acquire. We build on a recent technique that removes the need for reference data by employing networks with a "blind spot" in the receptive field, and significantly improve two key aspects: image quality and training efficiency. Our result quality is on par with state-of-the-art neural network denoisers in the case of i.i.d. additive Gaussian noise, and not far behind with Poisson and impulse noise. We also successfully handle cases where parameters of the noise model are variable and/or unknown in both training and evaluation data.	[Laine, Samuli; Karras, Tero; Lehtinen, Jaakko; Aila, Timo] NVIDIA, Santa Clara, CA 95051 USA; [Lehtinen, Jaakko] Aalto Univ, Espoo, Finland	Nvidia Corporation; Aalto University	Laine, S (corresponding author), NVIDIA, Santa Clara, CA 95051 USA.	slaine@nvidia.com; tkarras@nvidia.com; jlehtinen@nvidia.com; taila@nvidia.com	Lehtinen, Jaakko/G-2328-2013					Batson J., 2019, P 36 INT C MACH LEAR, P524; Bromiley P. A., 2003, 2003003 U MANCH MED, V3; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Cha S., 2018, ABS180707569 CORR; Chen GY, 2015, IEEE I CONF COMP VIS, P477, DOI 10.1109/ICCV.2015.62; Dabov K, 2007, IEEE IMAGE PROC, P313, DOI 10.1109/icip.2007.4378954; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Gu SH, 2014, PROC CVPR IEEE, P2862, DOI 10.1109/CVPR.2014.366; Guo S, 2019, PROC CVPR IEEE, P1712, DOI 10.1109/CVPR.2019.00181; Hasinoff Samuel W, 2014, PHOTON POISSON NOISE; Jena R., 2019, ABS190412323 CORR; Kendall A, 2017, ADV NEURAL INFORM PR, P5574; Krull A., 2019, CVPR, P2129; Krull A., 2019, ABS190600651 CORR; Le Q.V., 2005, P 22 INT C MACHINE L, P489, DOI [10.1145/1102351.1102413, DOI 10.1145/1102351.1102413]; Lehtinen J., 2018, P INT C MACH LEARN I; Liu B., 2018, ABS181107268 CORR; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Makitalo M, 2011, IEEE T IMAGE PROCESS, V20, P99, DOI 10.1109/TIP.2010.2056693; Mao Xiao-Jiao, 2016, ADV NEURAL INFORM PR, P2802; NIX DA, 1994, 1994 IEEE INTERNATIONAL CONFERENCE ON NEURAL NETWORKS, VOL 1-7, P55, DOI 10.1109/ICNN.1994.374138; Qu CC, 2018, IEEE WINT CONF APPL, P9, DOI 10.1109/WACVW.2018.00007; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Salimans Tim, 2017, P INT C LEARN REPR I, P10; Shocher A, 2018, PROC CVPR IEEE, P3118, DOI 10.1109/CVPR.2018.00329; Soltanayev S, 2018, ADV NEUR IN, V31; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; van den Oord A, 2016, PR MACH LEARN RES, V48; Xu J, 2017, IEEE I CONF COMP VIS, P1105, DOI 10.1109/ICCV.2017.125; Zhang Y., 2018, ABS181210477 CORR	31	52	51	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307003
C	Song, Y; Ermon, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Song, Yang; Ermon, Stefano			Generative Modeling by Estimating Gradients of the Data Distribution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				DIMENSIONALITY REDUCTION	We introduce a new generative model where samples are produced via Langevin dynamics using gradients of the data distribution estimated with score matching. Because gradients can be ill-defined and hard to estimate when the data resides on low-dimensional manifolds, we perturb the data with different levels of Gaussian noise, and jointly estimate the corresponding scores, i.e., the vector fields of gradients of the perturbed data distribution for all noise levels. For sampling, we propose an annealed Langevin dynamics where we use gradients corresponding to gradually decreasing noise levels as the sampling process gets closer to the data manifold. Our framework allows flexible model architectures, requires no sampling during training or the use of adversarial methods, and provides a learning objective that can be used for principled model comparisons. Our models produce samples comparable to GANs on MNIST, CelebA and CIFAR-10 datasets, achieving a new state-of-the-art inception score of 8.87 on CIFAR-10. Additionally, we demonstrate that our models learn effective representations via image inpainting experiments.	[Song, Yang; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Song, Y (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	yangsong@cs.stanford.edu; ermon@cs.stanford.edu			Toyota Research Institute ("TRI"); NSF [1651565, 1522054, 1733686]; ONR [N00014-19-1-2145]; AFOSR [FA9550-19-1-0024]	Toyota Research Institute ("TRI"); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	Toyota Research Institute ("TRI") provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. This research was also supported by NSF (#1651565, #1522054, #1733686), ONR (N00014-19-1-2145), AFOSR (FA9550-19-1-0024).	Alain G., 2016, INFORM INFERENCE; Arjovsky M, 2017, PR MACH LEARN RES, V70; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Bengio Y., 2013, P 26 INT C NEUR INF, P899; Bordes F., 2017, ARXIV170306975; Brock A., 2019, INT C LEARNING REPRE; Chandra B, 2014, LECT NOTES COMPUT SC, V8834, P535, DOI 10.1007/978-3-319-12637-1_67; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen TQ, 2014, PR MACH LEARN RES, V32, P1683; Dai Z., 2017, ADV NEURAL INFORM PR, P6510; Dinh Laurent, 2014, ARXIV14108516; Du Y., 2019, ADV NEURAL INFORM PR, V6; Dumoulin V., 2017, INT C LEARN REPR 201; Geras K. J., 2014, ARXIV14063269; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal A, 2017, ADV NEUR IN, V30; Graves A, 2013, ARXIV13080850; Gulrajani I, 2017, P NIPS 2017; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; Hensel M, 2017, ADV NEUR IN, V30; Hinton GE, 2002, NEURAL COMPUT, V14, P1771, DOI 10.1162/089976602760128018; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hoffman T., 2007, ADV NEURAL INFORM PR, V19, P1145; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Karras T, 2019, PROC CVPR IEEE, P4396, DOI 10.1109/CVPR.2019.00453; Kingma D., 2010, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lin G., 2017, P IEEE C COMP VIS PA, p1925~1934; Liu Q, 2016, PR MACH LEARN RES, V48; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Luo Q, 2017, INT CONF MEAS, P150, DOI 10.1109/ICMTMA.2017.44; Miyasawa K., 1961, B I INT STAT, V38, P1; Neal RM, 2011, CH CRC HANDB MOD STA, P113; Neal RM, 2001, STAT COMPUT, V11, P125, DOI 10.1023/A:1008923215028; Nijkamp Erik, 2019, ARXIV190312370; Nowozin S, 2016, ADV NEUR IN, V29; Oord A.V.D., 2016, SSW; Ostrovski G., 2018, ICML, P3933; Ostrovski G, 2017, PR MACH LEARN RES, V70; Raphan M, 2011, NEURAL COMPUT, V23, P374, DOI 10.1162/NECO_a_00076; Ravuri S., 2018, P MACHINE LEARNING R, V80, P4314; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Salimans T, 2016, ADV NEUR IN, V29; Saremi Saeed, 2018, ARXIV180508306; Sohl-Dickstein J., 2009, ARXIV09064779; Sohl-Dickstein J, 2015, PR MACH LEARN RES, V37, P2256; Song JM, 2017, ADV NEUR IN, V30; Song Y, 2019, P COMBUST INST, V37, P667, DOI 10.1016/j.proci.2018.06.115; Song Y., 2019, P 35 C UNC ART INT U, P204; Sriperumbudur B. K., 2009, ARXIV PREPRINT ARXIV; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Ulyanov D., 2016, ARXIV160708022; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3; Wenliang LK, 2019, PR MACH LEARN RES, V97; Yu F., 2016, P ICLR 2016; Yu F, 2017, PROC CVPR IEEE, P636, DOI 10.1109/CVPR.2017.75; Zhang QJ, 2018, FRONT COMPUT SCI-CHI, V12, P1140, DOI 10.1007/s11704-016-6107-0	66	52	53	5	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903052
C	Bansal, N; Chen, XH; Wang, ZY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bansal, Nitin; Chen, Xiaohan; Wang, Zhangyang			Can We Gain More from Orthogonality Regularizations in Training Deep CNNs?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper seeks to answer the question: as the (near-) orthogonality of weights is found to be a favorable property for training deep convolutional neural networks, how can we enforce it in more effective and easy-to-use ways? We develop novel orthogonality regularizations on training deep CNNs, utilizing various advanced analytical tools such as mutual coherence and restricted isometry property. These plug-and-play regularizations can be conveniently incorporated into training almost any CNN without extra hassle. We then benchmark their effects on state-of-the-art models: ResNet, WideResNet, and ResNeXt, on several most popular computer vision datasets: CIFAR-10, CIFAR-100, SVHN and ImageNet. We observe consistent performance gains after applying those proposed regularizations, in terms of both the final accuracies achieved, and faster and more stable convergences. We have made our codes and pre-trained models publicly	[Bansal, Nitin; Chen, Xiaohan; Wang, Zhangyang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA	Texas A&M University System; Texas A&M University College Station	Bansal, N (corresponding author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.	bansa01@tamu.edu; chernxh@tamu.edu; atlaswang@tamu.edu	Bansal, Nitin/GLS-6901-2022	Bansal, Nitin/0000-0002-1818-5387; Wang, Zhangyang/0000-0002-2050-5693	NSF [RI-1755701]	NSF(National Science Foundation (NSF))	The work by N. Bansal, X. Chen and Z. Wang is supported in part by NSF RI-1755701. We would also like to thank all anonymous reviewers for their tremendously useful comments to help improve our work.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; [Anonymous], 2016, ADV NEURAL INFORM PR; Arjovsky M, 2016, PR MACH LEARN RES, V48; Balestriero Randall, 2018, P 35 INT C MACH LEAR; Baraniuk R, 2018, ARXIV180506576; Candes EJ, 2005, IEEE T INFORM THEORY, V51, P4203, DOI 10.1109/TIT.2005.858979; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Desjardins G., 2015, P 28 C NEUR PROC SYS, P2071; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Dorobantu V., 2016, ARXIV161204035; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Harandi M., 2016, ARXIV161105927; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Jia K., 2016, CORR; Keskar N.S., 2016, ABS160904836; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lei Huang, 2017, ARXIV170906079; Lin  Z., 2015, ARXIV150803117; Mhammedi Zakaria, 2016, ARXIV161200188; Mishkin Dmytro, 2015, ICLR; Miyato T., 2018, INT C LEARN REPR, P2; Ozay M., 2016, ARXIV161007008; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Rodr<prime>iguez Pau, 2016, ARXIV161101967; Saxe A., 2014, INT C LEARNING REPRE; Sokolic J, 2017, IEEE T SIGNAL PROCES, V65, P4265, DOI 10.1109/TSP.2017.2708039; Sun Yifan, 2017, ARXIV PREPRINT; Vorontsov E., 2017, ARXIV170200071; Wang S., 2016, P 33 INT C MACH LEAR, P718; WANG Z, 2016, SPARSE CODING ITS AP, P1; Wang Zhangyang, 2018, ARXIV180405515; Xie Di, 2017, ARXIV170301827; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yoshida Y, 2017, ARXIV PREPRINT ARXIV; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhang T, 2011, IEEE T INFORM THEORY, V57, P6215, DOI 10.1109/TIT.2011.2162263; Zhou JP, 2006, IEEE T IMAGE PROCESS, V15, P511, DOI 10.1109/TIP.2005.863046	39	52	52	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304029
C	Liu, AH; Liu, YC; Yeh, YY; Wang, YCF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Alexander H.; Liu, Yen-Cheng; Yeh, Yu-Ying; Wang, Yu-Chiang Frank			A Unified Feature Disentangler for Multi-Domain Image Translation and Manipulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel and unified deep learning framework which is capable of learning domain-invariant representation from data across multiple domains. Realized by adversarial training with additional ability to exploit domain-specific information, the proposed network is able to perform continuous cross-domain image translation and manipulation, and produces desirable output images accordingly. In addition, the resulting feature representation exhibits superior performance of unsupervised domain adaptation, which also verifies the effectiveness of the proposed model in learning disentangled features for describing cross-domain data.	[Liu, Alexander H.; Wang, Yu-Chiang Frank] Natl Taiwan Univ, Taipei, Taiwan; [Liu, Yen-Cheng] Georgia Inst Technol, Atlanta, GA 30332 USA; [Yeh, Yu-Ying] Univ Calif San Diego, La Jolla, CA 92093 USA; [Wang, Yu-Chiang Frank] MOST Joint Res Ctr AI Technol & All Vista Healthc, Taipei, Taiwan	National Taiwan University; University System of Georgia; Georgia Institute of Technology; University of California System; University of California San Diego	Liu, AH (corresponding author), Natl Taiwan Univ, Taipei, Taiwan.	b03902034@ntu.edu.tw; ycliu@gatech.edu; yuyeh@eng.ucsd.edu; ycwang@ntu.edu.tw	Wang, Yu-Chiang Frank/AAE-7478-2021	Wang, Yu-Chiang Frank/0000-0002-2333-157X				Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bousmalis  K., 2016, ADV NEURAL INFORM PR, P343; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Ghifary  M., 2016, P EUR C COMP VIS ECC; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Higgins  I., 2017, PROCEEDINGS OF THE I; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Kim T, 2017, PR MACH LEARN RES, V70; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; Lample Guillaume, 2017, ARXIV170600409; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Liu Y, 2018, PROCEEDINGS OF THE 2ND INTERNATIONAL CONFERENCE ON COMPUTER SCIENCE AND APPLICATION ENGINEERING (CSAE2018), DOI 10.1145/3207677.3278091; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Odena A, 2017, PR MACH LEARN RES, V70; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tran L, 2017, PROC CVPR IEEE, P1283, DOI 10.1109/CVPR.2017.141; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Zhao  S., 2017, P INT C MACH LEARN I; Zhu Jun-Yan, 2017, ICCV	30	52	53	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302059
C	Song, GC; Chai, W		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Song, Guocong; Chai, Wei			Collaborative Learning for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce collaborative learning in which multiple classifier heads of the same network are simultaneously trained on the same training data to improve generalization and robustness to label noise with no extra inference cost. It acquires the strengths from auxiliary training, multi-task learning and knowledge distillation. There are two important mechanisms involved in collaborative learning. First, the consensus of multiple views from different classifier heads on the same example provides supplementary information as well as regularization to each classifier, thereby improving generalization. Second, intermediate-level representation (ILR) sharing with backpropagation resealing aggregates the gradient flows from all heads, which not only reduces training computational complexity, but also facilitates supervision to the shared layers. The empirical results on CIFAR and ImageNet datasets demonstrate that deep neural networks learned as a group in a collaborative way significantly reduce the generalization error and increase the robustness to label noise.	[Song, Guocong] Playground Global, Palo Alto, CA 94306 USA; [Chai, Wei] Google, Mountain View, CA 94043 USA	Google Incorporated	Song, GC (corresponding author), Playground Global, Palo Alto, CA 94306 USA.	songgc@gmail.com; chaiwei@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Anil R., 2018, INT C LEARN REPR ICL; Baxter J., 1995, Proceedings of the Eighth Annual Conference on Computational Learning Theory, P311, DOI 10.1145/225298.225336; Caruana R., 1993, ICML, DOI [DOI 10.1016/B978-1-55860-307-3.50012-5, 10.1016/b978-1-55860-307-3.50012-5]; Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, ARXIV150302531; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang Jonathan, 2017, P IEEE C COMP VIS PA, P7310, DOI DOI 10.1109/CVPR.2017.351; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Laine Samuli, 2017, P INT C LEARN REPR I, P3; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Nagarajan V, 2017, ADV NEUR IN, V30; Rhu M, 2016, INT SYMP MICROARCH; Romero Adriana, 2015, ICLR; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Yang Yongxin, 2017, P INT C MACH REPR; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang Y., 2017, ABS170600384 ARXIV	23	52	54	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301079
C	Xu, ZW; van Hasselt, H; Silver, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Zhongwen; van Hasselt, Hado; Silver, David			Meta-Gradient Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The goal of reinforcement learning algorithms is to estimate and/or optimise the value function. However, unlike supervised learning, no teacher or oracle is available to provide the true value function. Instead, the majority of reinforcement learning algorithms estimate and/or optimise a proxy for the value function. This proxy is typically based on a sampled and bootstrapped approximation to the true value function, known as a return. The particular choice of return is one of the chief components determining the nature of the algorithm: the rate at which future rewards are discounted; when and how values should be bootstrapped; or even the nature of the rewards themselves. It is well-known that these decisions are crucial to the overall success of RL algorithms. We discuss a gradient-based meta-learning algorithm that is able to adapt the nature of the return, online, whilst interacting and learning from the environment. When applied to 57 games on the Atari 2600 environment over 200 million frames, our algorithm achieved a new state-of-the-art performance.	[Xu, Zhongwen; van Hasselt, Hado; Silver, David] DeepMind, London, England		Xu, ZW (corresponding author), DeepMind, London, England.	zhongwen@google.com; hado@google.com; davidsilver@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; Al-Shedivat Maruan, 2018, P INT C LEARN REPR I; Andrychowicz M, 2016, ADV NEUR IN, V29; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Doina Precup, 2000, P 17 INT C MACH LEAR, P80; Downey C., 2010, ICML, P311; Duan Y., 2016, RL2 FAST REINFORCEME; Elfwing S., 2017, CORR; Espeholt L, 2018, PR MACH LEARN RES, V80; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, ICLR; Fortunato M., 2018, ICLR; Franceschi L, 2017, PR MACH LEARN RES, V70; Grant Erin, 2018, INT C LEARN REPR; Harutyunyan A., 2016, ALG LEARN THEOR 27 I, P305; Hasselt H, 2010, ADV NEURAL INFORM PR, V23, P2613, DOI DOI 10.5555/2997046.2997187; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hessel M, 2018, AAAI CONF ARTIF INTE, P3215; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 2001, LECT NOTES COMPUT SC, V2130, P87; Jaderberg Max, 2017, ICLR; Jaderberg Max, 2017, ABS171109846 CORR; Kearns M.J., 2000, P 13THANNUAL C COMPU, P142; Kingma D.P, P 3 INT C LEARNING R; Konidaris G., 2011, ADV NEURAL INFORM PR, P2402; Maclaurin D, 2015, PR MACH LEARN RES, V37, P2113; Mahmood A., 2017, THESIS; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair A., 2015, ARXIV150704296; Pedregosa F, 2016, PR MACH LEARN RES, V48; Randlov J., 1998, Machine Learning. Proceedings of the Fifteenth International Conference (ICML'98), P463; Rummery GA, 1994, 166 CUEDFINFENGTR CA; Schaul T., 2016, ABS151105952 CORR; Schaul T, 2015, PR MACH LEARN RES, V37, P1312; Schmidhuber J, 1987, THESIS; Schraudolph N. N., 1999, ICANN; Singh S, 1998, MACH LEARN, V32, P5, DOI 10.1023/A:1007495401240; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton RS, 2014, PR MACH LEARN RES, V32, P568; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; SUTTON RS, 1992, AAAI-92 PROCEEDINGS : TENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE, P171; Sutton RS., 2016, J MACH LEARN RES, V17, P2603; Thrun S, 1998, LEARNING TO LEARN, P181; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Van Hasselt Hado, 2016, P AAAI C ART INT, V30; van Seijen H, 2009, ADPRL: 2009 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P177; Wang JW, 2017, ACTA OPHTHALMOL, V95, pE10, DOI 10.1111/aos.13227; Wang ZY, 2016, PR MACH LEARN RES, V48; White M, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P557; Wichrowska O, 2017, PR MACH LEARN RES, V70; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Williams RJ, 1989, NEURAL COMPUT, V1, P270, DOI 10.1162/neco.1989.1.2.270; Zheng Z., 2018, NEURIPS	62	52	52	24	62	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF	30284463				2022-12-19	WOS:000461823302041
C	Hayton, P; Scholkopf, B; Tarassenko, L; Anuzis, P		Leen, TK; Dietterich, TG; Tresp, V		Hayton, P; Scholkopf, B; Tarassenko, L; Anuzis, P			Support vector novelty detection applied to Jet Engine vibration spectra	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					A system has been developed to extract diagnostic information from jet engine carcass vibration data. Support Vector Machines applied to novelty detection provide a measure of how unusual the shape of a vibration signature is, by learning a representation of normality. We describe a novel method for Support Vector Machines of including information from a second class for novelty detection and give results from the application to Jet Engine vibration analysis.	Univ Oxford, Dept Engn Sci, Oxford OX1 3PJ, England	University of Oxford	Hayton, P (corresponding author), Univ Oxford, Dept Engn Sci, Parks Rd, Oxford OX1 3PJ, England.		Schölkopf, Bernhard/A-7570-2013	Schölkopf, Bernhard/0000-0002-8177-0925				BOSER B, 1992, P 5 ANN WORKSH COMP, V5, P144; Nairac A, 1999, INTEGR COMPUT-AID E, V6, P53; Scholkopf B, 2000, ADV NEUR IN, V12, P582; SCHOLKOPF B, 1999, 9987 TR MSR; SCHOLKOPF B, 2000, 200022 TR MSR; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	6	52	52	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						946	952						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800133
C	Teh, YW; Hinton, GE		Leen, TK; Dietterich, TG; Tresp, V		Teh, YW; Hinton, GE			Rate-coded restricted Boltzmann machines for face recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We describe a neurally-inspired, unsupervised learning algorithm that builds a non-linear generative model for pairs of face images from the same individual. Individuals are then recognized by finding the highest relative probability pair among all pairs that consist of a test image and an image whose identity is known. Our method compares favorably with other methods in the literature. The generative model consists of a single layer of rate-coded, non-linear feature detectors and it has the property that, given a data vector, the true posterior probability distribution over the feature detector activities can be inferred rapidly without iteration or approximation. The weights of the feature detectors are learned by comparing the correlations of pixel intensities and feature activations in two phases: When the network is observing real data and when it is observing reconstructions of real data generated from the feature activations.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Hinton, GE (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England.		Teh, Yee Whye/C-3400-2008					BELMUMEUR PN, 1996, EUR C COMP VIS; Hinton G. E., 2000, 2000004 GCNU TR U CO; LEE D, 1999, NATURE, V401; McClelland JL, 1986, PARALLEL DISTRIBUTED, V1, P1; Moghaddam B, 1997, IEEE T PATTERN ANAL, V19, P696, DOI 10.1109/34.598227; MOGHADDAM B, 1998, IEEE INT C AUT FAC G; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; PHILLIPS PJ, 1997, INT C AUD VID BAS BI; Smolensky P, 1986, PARALLEL DISTRIBUTED, V1; TIPPING M, 1997, NCRG97010 AST U; TURK M, 1991, J COGNITIVE NEUROSCI, V3, P71, DOI 10.1162/jocn.1991.3.1.71	11	52	55	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						908	914						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800128
C	Bell, AJ; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Bell, AJ; Sejnowski, TJ			Edges are the 'independent components' of natural scenes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Field (1994) has suggested that neurons with line and edge selectivities found in primary visual cortex of cats and monkeys form a sparse, distributed representation of natural scenes, and Barlow (1989) has reasoned that such responses should emerge from an unsupervised learning algorithm that attempts to find a factorial code of independent visual features. We show here that non-linear 'infomax', when applied to an ensemble of natural scenes, produces sets of visual filters that are localised and oriented. Some of these filters are Gabor-like and resemble those produced by the sparseness-maximisation network of Olshausen & Field (1996). In addition, the outputs of these filters are as independent as possible, since the infomax network is able to perform Independent Components Analysis (ICA). We compare the resulting ICA filters and their associated basis functions, with other decorrelating filters produced by Principal Components Analysis (PCA) and zero-phase whitening filters (ZCA). The ICA filters have more sparsely distributed (kurtotic) outputs on natural scenes. They also resemble the receptive fields of simple cells in visual cortex, which suggests that these neurons form an information-theoretic co-ordinate system for images.			Bell, AJ (corresponding author), SALK INST BIOL STUDIES,COMPUTAT NEUROBIOL LAB,10010 N TORREY PINES RD,LA JOLLA,CA 92037, USA.		Sejnowski, Terrence/AAV-5558-2021						0	52	54	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						831	837						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00117
C	Ma, JX; Zhou, C; Cui, P; Yang, HX; Zhu, WW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ma, Jianxin; Zhou, Chang; Cui, Peng; Yang, Hongxia; Zhu, Wenwu			Learning Disentangled Representations for Recommendation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					User behavior data in recommender systems are driven by the complex interactions of many latent factors behind the users' decision making processes. The factors are highly entangled, and may range from high-level ones that govern user intentions, to low-level ones that characterize a user's preference when executing an intention. Learning representations that uncover and disentangle these latent factors can bring enhanced robustness, interpretability, and controllability. However, learning such disentangled representations from user behavior is challenging, and remains largely neglected by the existing literature. In this paper, we present the MACRo-mIcro Disentangled Variational Auto-Encoder (MacridVAE) for learning disentangled representations from user behavior. Our approach achieves macro disentanglement by inferring the high-level concepts associated with user intentions (e.g., to buy a shirt or a cellphone), while capturing the preference of a user regarding the different concepts separately. A micro-disentanglement regularizer, stemming from an information-theoretic interpretation of VAEs, then forces each dimension of the representations to independently reflect an isolated low-level factor (e.g., the size or the color of a shirt). Empirical results show that our approach can achieve substantial improvement over the state-of-the-art baselines. We further demonstrate that the learned representations are interpretable and controllable, which can potentially lead to a new paradigm for recommendation where users are given fine-grained control over targeted aspects of the recommendation lists.	[Ma, Jianxin; Zhou, Chang; Yang, Hongxia] Alibaba Grp, Hangzhou, Peoples R China; [Ma, Jianxin; Cui, Peng; Zhu, Wenwu] Tsinghua Univ, Beijing, Peoples R China; [Ma, Jianxin; Zhou, Chang] Alibaba, Hangzhou, Peoples R China	Alibaba Group; Tsinghua University; Alibaba Group	Ma, JX (corresponding author), Alibaba Grp, Hangzhou, Peoples R China.; Ma, JX (corresponding author), Tsinghua Univ, Beijing, Peoples R China.; Ma, JX (corresponding author), Alibaba, Hangzhou, Peoples R China.	majx13fromthu@gmail.com; ericzhou.zc@alibaba-inc.com; cuip@tsinghua.edu.cn; yang.yhx@alibaba-inc.com; wwzhu@tsinghua.edu.cn			National Program on Key Basic Research Project [2015CB352300]; National Key Research and Development Project [2018AAA0102004]; National Natural Science Foundation of China [61772304, 61521002, 61531006, U1611461]; Beijing Academy of Artificial Intelligence (BAAI); Young Elite Scientist Sponsorship Program by CAST	National Program on Key Basic Research Project(National Basic Research Program of China); National Key Research and Development Project; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Academy of Artificial Intelligence (BAAI); Young Elite Scientist Sponsorship Program by CAST	The authors from Tsinghua University are supported in part by National Program on Key Basic Research Project (No. 2015CB352300), National Key Research and Development Project (No. 2018AAA0102004), National Natural Science Foundation of China (No. 61772304, No. 61521002, No. 61531006, No. U1611461), Beijing Academy of Artificial Intelligence (BAAI), and the Young Elite Scientist Sponsorship Program by CAST. All opinions, findings, and conclusions in this paper are those of the authors and do not necessarily reflect the views of the funding agencies.	Achille A, 2018, IEEE T PATTERN ANAL, V40, P2897, DOI 10.1109/TPAMI.2017.2784440; Alemi Alexander A, 2015, INT C LEARN REPR; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Berger J, 2014, PHILOS PSYCHOL, V27, P829, DOI 10.1080/09515089.2013.771241; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Bouchacourt D, 2018, AAAI CONF ARTIF INTE, P2095; Burgess CP, 2018, ARXIV180403599; Chen T.Q., 2018, NEURIPS, P2610; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; Deshpande M, 2004, ACM T INFORM SYST, V22, P143, DOI 10.1145/963770.963776; Dilokthanakul Nat, 2016, ARXIV161102648; Dupont Emilien, 2018, ARXIV180400104; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Harper FM, 2016, ACM T INTERACT INTEL, V5, DOI 10.1145/2827872; He XN, 2017, PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW'17), P173, DOI 10.1145/3038912.3052569; He Xiangnan, 2015, P 24 ACM INT C INFOR, P1661; Higgins M, 2017, PALGR COMMUN, V3, DOI 10.1057/s41599-017-0005-4; Hsieh JT, 2018, ADV NEUR IN, V31; Hu YF, 2008, IEEE DATA MINING, P263, DOI 10.1109/ICDM.2008.22; Jang E., 2016, ARXIV; Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1; Jiang ZX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1965; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Komodakis Nikos, 2018, INT C LEARN REPR ICL; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Kosiorek AR, 2018, ADV NEUR IN, V31; Li XP, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P305, DOI 10.1145/3097983.3098077; Liang DW, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P689, DOI 10.1145/3178876.3186150; Liu B., 2013, P 12 ACM WORKSH HOT; Locatello F, 2019, ARXIV190501258; Locatello F, 2019, PR MACH LEARN RES, V97; LOWERE B, 1976, THESIS; Ma JX, 2019, PR MACH LEARN RES, V97; Ma LQ, 2018, PROC CVPR IEEE, P99, DOI 10.1109/CVPR.2018.00018; Maddison Chris J, 2016, ARXIV161100712; Rendle Steffen, 2009, UAI; Resnick P., 1994, P ACM C COMP SUPP CO, P175, DOI DOI 10.1145/192844.192905; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Salakhutdinov R., 2011, NIPS, V20, P1; Sarwar B., 2001, P 10 INT C WORLD WID, P285, DOI DOI 10.1145/371920.372071; Tishby N, 2015, 2015 IEEE INFORMATION THEORY WORKSHOP (ITW); Tishby Naftali, 2000, PHYSICS0004057 ARXIV; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang H, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1235, DOI 10.1145/2783258.2783273; Wiskel JB, 2018, PR INT PIPELINE CONF; ZHANG Y, 2007, P 30 ANN INT ACM SIG; Zhang YF, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P83, DOI 10.1145/2600428.2609579; Zhou CF, 2018, INT C INDOOR POSIT; ZHU JY, 2018, ADV NEURAL INFORM PR	53	51	52	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305068
C	Elsayed, GF; Shankar, S; Cheung, B; Papernot, N; Kurakin, A; Goodfellow, I; Sohl-Dickstein, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Elsayed, Gamaleldin F.; Shankar, Shreya; Cheung, Brian; Papernot, Nicolas; Kurakin, Alexey; Goodfellow, Ian; Sohl-Dickstein, Jascha			Adversarial Examples that Fool both Computer Vision and Time-Limited Humans	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MODELS	Machine learning models are vulnerable to adversarial examples: small changes to images can cause computer vision models to make mistakes such as identifying a school bus as an ostrich. However, it is still an open question whether humans are prone to similar mistakes. Here, we address this question by leveraging recent techniques that transfer adversarial examples from computer vision models with known parameters and architecture to other models with unknown parameters and architecture, and by matching the initial processing of the human visual system. We find that adversarial examples that strongly transfer across computer vision models influence the classifications made by time-limited human observers.	[Elsayed, Gamaleldin F.; Kurakin, Alexey; Goodfellow, Ian; Sohl-Dickstein, Jascha] Google Brain, Mountain View, CA USA; [Shankar, Shreya] Stanford Univ, Stanford, CA 94305 USA; [Cheung, Brian] Univ Calif Berkeley, Berkeley, CA USA; [Papernot, Nicolas] Penn State Univ, University Pk, PA 16802 USA	Google Incorporated; Stanford University; University of California System; University of California Berkeley; Pennsylvania Commonwealth System of Higher Education (PCSHE); Pennsylvania State University; Pennsylvania State University - University Park	Elsayed, GF (corresponding author), Google Brain, Mountain View, CA USA.	gamaleldin.elsayed@gmail.com; jaschasd@google.com						Athalye Anish, 2017, ROBUST ADVERSARIAL E; Athalye Anish, 2017, ARXIV PREPRINT ARXIV; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Brown T. B., 2017, P NIPS WORKSH; Buckman J., 2018, INT C LEARN REPR; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eckstein MP, 2017, CURR BIOL, V27, P2827, DOI 10.1016/j.cub.2017.07.068; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; Gatys LeonA., 2015, ARXIV, DOI 10.1167/16.12.326; Geirhos R., 2017, ARXIV170606969; Goodfellow I., 2017, ATTACKING MACHINE LE; Goodfellow I. J., 2014, ARXIV14126572; Hassabis D, 2017, NEURON, V95, P245, DOI 10.1016/j.neuron.2017.06.011; He K., 2016, ARXIV E PRINTS; Hillis JM, 2002, SCIENCE, V298, P1627, DOI 10.1126/science.1075396; Ibbotson M, 2011, CURR OPIN NEUROBIOL, V21, P553, DOI 10.1016/j.conb.2011.05.012; Jha S., 2015, ABS151107528 CORR; KOLTER JZ, 2017, ARXIV171100851; KOVACS G, 1995, P NATL ACAD SCI USA, V92, P5587, DOI 10.1073/pnas.92.12.5587; Kummerer M., 2014, CORR; Kummerer M., 2017, J VISUAL-JAPAN, V17, P1147, DOI [10.1167/17.10.1147, DOI 10.1167/17.10.1147]; Kurakin A., 2016, ICLR 2017 WORKSH; Kurakin Alexey, 2017, ARXIV E PRINTS; Kurakin Alexey, 2016, ARXIV E PRINTS; Land M.F., 2012, ANIMAL EYES, P1; Liu Yanpei, 2016, ARXIV161102770; Madry A., 2018, ARXIV PREPRINT ARXIV; McIntosh LT, 2016, ADV NEUR IN, V29; Olshausen B. A., 2013, 20 YEARS COMPUTATION, P243, DOI [10.1007/978-1-4614-1424-7_12, DOI 10.1007/978-1-4614-1424-7_12]; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Potter MC, 2014, ATTEN PERCEPT PSYCHO, V76, P270, DOI 10.3758/s13414-013-0605-z; Rajalingham R., 2018, BIORXIV; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Szegedy C., 2015, ARXIV E PRINTS; Szegedy C., 2016, ARXIV E PRINTS; Szegedy C, 2013, 2 INT C LEARNING REP; Van Essen D. C., 1995, INTRO NEURAL ELECT N, V2nd, P45; Xu W., 2017, P 25 ANN NETW DISTR, DOI DOI 10.14722/NDSS.2018.23198; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244	43	51	51	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303087
C	Yang, T; Zhang, XY; Li, ZM; Zhang, WQ; Sun, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yang, Tong; Zhang, Xiangyu; Li, Zeming; Zhang, Wenqiang; Sun, Jian			MetaAnchor: Learning to Detect Objects with Customized Anchors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a novel and flexible anchor mechanism named MetaAnchor for object detection frameworks. Unlike many previous detectors model anchors via a predefined manner, in MetaAnchor anchor functions could be dynamically generated from the arbitrary customized prior boxes. Taking advantage of weight prediction, MetaAnchor is able to work with most of the anchor-based object detection systems such as RetinaNet. Compared with the predefined anchor scheme, we empirically find that MetaAnchor is more robust to anchor settings and bounding box distributions; in addition, it also shows the potential on transfer tasks. Our experiment on COCO detection task shows that MetaAnchor consistently outperforms the counterparts in various scenarios.	[Yang, Tong; Zhang, Xiangyu; Li, Zeming; Sun, Jian] Megvii Inc Face, Beijing, Peoples R China; [Yang, Tong; Zhang, Wenqiang] Fudan Univ, Shanghai, Peoples R China	Fudan University	Yang, T (corresponding author), Megvii Inc Face, Beijing, Peoples R China.; Yang, T (corresponding author), Fudan Univ, Shanghai, Peoples R China.	yangtong@megvii.com; zhangxiangyu@megvii.com; lizeming@megvii.com; wqzhang@fudan.edu.cn; sunjian@megvii.com		Zhang, Xiangyu/0000-0003-2138-4608	National Key RD Program [2017YFA0700800]	National Key RD Program	This work is supported by National Key R&D Program No. 2017YFA0700800, China.	Andrychowicz M, 2016, ADV NEUR IN, V29; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Dollar P, 2012, IEEE T PATTERN ANAL, V34, P743, DOI 10.1109/TPAMI.2011.155; Elhoseiny M, 2013, IEEE I CONF COMP VIS, P2584, DOI 10.1109/ICCV.2013.321; Erhan D, 2014, PROC CVPR IEEE, P2155, DOI 10.1109/CVPR.2014.276; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Fu C. -Y., 2017, ARXIV170106659; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Ha David, 2016, ARXIV160909106; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hoffman Judy, 2014, NIPS; Hu R, 2017, INT C COMP VIS ICCV; Huang L., 2015, ARXIV150904874; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Li Z, 2017, ARXIV171200960; Li Zeming, 2017, ARXIV171107264; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lin Tsung-Yi, 2017, ARXIV170802002, P2980, DOI [DOI 10.1109/ICCV.2017.324, DOI 10.1109/TPAMI.2018.2858826]; Mao JY, 2017, PROC CVPR IEEE, P6034, DOI 10.1109/CVPR.2017.639; Misra  I., 2017, IEEE C COMP VIS PATT, V3; Najibi M, 2017, IEEE I CONF COMP VIS, P4885, DOI 10.1109/ICCV.2017.522; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Peng C., 2018, ARXIV180406215; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rezatofighi S. H., 2018, ARXIV PREPRINT ARXIV; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen ZQ, 2017, IEEE I CONF COMP VIS, P1937, DOI 10.1109/ICCV.2017.212; Song  G., 2018, ARXIV180405197; Stewart R, 2016, PROC CVPR IEEE, P2325, DOI 10.1109/CVPR.2016.255; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wang  J., 2018, ARXIV180406559; Wang Xinlong, 2017, ARXIV171107752; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Zhang LL, 2016, LECT NOTES COMPUT SC, V9906, P443, DOI 10.1007/978-3-319-46475-6_28; Zhang SF, 2017, IEEE I CONF COMP VIS, P192, DOI 10.1109/ICCV.2017.30; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360	45	51	57	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300030
C	Hoshen, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hoshen, Yedid			VAIN: Attentional Multi-agent Predictive Modeling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Multi-agent predictive modeling is an essential step for understanding physical, social and team-play systems. Recently, Interaction Networks (INs) were proposed for the task of modeling multi-agent physical systems. One of the drawbacks of INs is scaling with the number of interactions in the system (typically quadratic or higher order in the number of agents). In this paper we introduce VAIN, a novel attentional architecture for multi-agent predictive modeling that scales linearly with the number of agents. We show that VAIN is effective for multi-agent predictive modeling. Our method is evaluated on tasks from challenging multi-agent prediction domains: chess and soccer, and outperforms competing multi-agent approaches.	[Hoshen, Yedid] Facebook AI Res, NYC, New York, NY 10003 USA	Facebook Inc	Hoshen, Y (corresponding author), Facebook AI Res, NYC, New York, NY 10003 USA.	yedidh@fb.com	Jeong, Yongwook/N-7413-2016					Alahi A., 2016, CVPR; Amodei Dario, 2016, ICML; [Anonymous], 2014, ICLR; [Anonymous], 2015, NIPS; [Anonymous], 2015, NIPS; Battaglia P, 2016, NIPS; Campbell Murray, 2002, ARTIFICIAL INTELLIGE; David Omid E, 2016, ICANN; Gori Marco, 2005, IJCNN; Graves A, 2014, NEURAL TURING MACHIN; Hinton G., 2012, IEEE SIGNAL PROCESSI; Hochreiter S, 1997, NEURAL COMPUTATION; Johnson J., 2016, ARXIV161206890; Kaiser Lukasz, 2016, ICLR; Kingma D.P., 2015, INT C LEARN REPR, P1; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lai M., 2015, ARXIV150901549; LeCun Y., 1989, NEURAL COMPUTATION; Li Y., 2016, P 4 INT C LEARN REPR; Peng P., 2017, ARXIV170310069; Pettersen Svein Arne, 2014, P 5 ACM MULT SYST C, P18; Qi C. R., 2017, CVPR; Santoro A., 2017, ARXIV170601427; Scarselli F., 2009, IEEE T NEURAL NETWOR; Schroff F., 2015, CVPR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sukhbaatar S., 2016, NIPS; Taigman Y., 2014, CVPR; Tesauro Gerald, 1990, IJCNN; Tian Yuandong, 2016, ICLR; Usunier N, 2017, INT C LEARN REPR; Vaswani A., 2017, P 31 INT C NEURAL IN, P6000, DOI DOI 10.5555/3295222.3295349; Weston J., 2014, ARXIV14103916; Wu Y., 2016, GOOGLES NEURAL MACHI	34	51	51	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402073
C	Racah, E; Beckham, C; Maharaj, T; Kahou, SE; Prabhat; Pa, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Racah, Evan; Beckham, Christopher; Maharaj, Tegan; Kahou, Samira Ebrahimi; Prabhat; Pa, Christopher			ExtremeWeather: A large-scale climate dataset for semi-supervised detection, localization, and understanding of extreme weather events	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Then detection and identification of extreme weather events in large-scale climate simulations is an important problem for risk management, informing governmental policy decisions and advancing our basic understanding of the climate system. Recent work has shown that fully supervised convolutional neural networks (CNNs) can yield acceptable accuracy for classifying well-known types of extreme weather events when large amounts of labeled data are available. However, many different types of spatially localized climate patterns are of interest including hurricanes, extra-tropical cyclones, weather fronts, and blocking events among others. Existing labeled data for these patterns can be incomplete in various ways, such as covering only certain years or geographic areas and having false negatives. This type of climate data therefore poses a number of interesting machine learning challenges. We present a multichannel spatiotemporal CNN architecture for semi-supervised bounding box prediction and exploratory data analysis. We demonstrate that our approach is able to leverage temporal information and unlabeled data to improve the localization of extreme weather events. Further, we explore the representations learned by our model in order to better understand this important data. We present a dataset, ExtremeWeather, to encourage machine learning research in this area and to help facilitate further work in understanding and mitigating the effects of climate change. The dataset is available at extremeweatherdataset.github.io and the code is available at https: //github.com/eracah/hur-detect.	[Racah, Evan; Beckham, Christopher; Maharaj, Tegan; Pa, Christopher] Univ Montreal, MILA, Montreal, PQ, Canada; [Racah, Evan; Prabhat] Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA; [Beckham, Christopher; Maharaj, Tegan; Pa, Christopher] Ecole Polytech Montreal, Montreal, PQ, Canada; [Kahou, Samira Ebrahimi] Microsoft Maluuba, Montreal, PQ, Canada	Universite de Montreal; United States Department of Energy (DOE); Lawrence Berkeley National Laboratory; Universite de Montreal; Polytechnique Montreal	Racah, E (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.; Racah, E (corresponding author), Lawrence Berkeley Natl Lab, Berkeley, CA 94720 USA.	evan.racah@umontreal.ca; christopher.beckham@polymtl.ca; tegan.maharaj@polymtl.ca; samira.ebrahimi@microsoft.com; Prabhat@lbl.gov; christopher.pal@polymtl.ca	Jeong, Yongwook/N-7413-2016		Office of Science of the U.S. Department of Energy [DE-AC02-05CH11231]; Samsung; Google	Office of Science of the U.S. Department of Energy(United States Department of Energy (DOE)); Samsung(Samsung); Google(Google Incorporated)	This research used resources of the National Energy Research Scientific Computing Center (NERSC), a DOE Office of Science User Facility supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231. Code relies on open-source deep learning frameworks Theano (Bergstra et al.; Team et al., 2016) and Lasagne (Team, 2016), whose developers we gratefully acknowledge. We thank Samsung and Google for support that helped make this research possible. We would also like to thank Yunjie Liu and Michael Wehner for providing access to the climate datasets; Alex Lamb and Thorsten Kurth for helpful discussions.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ARXIV170508421; Ballas Nicolas, 2016, ICLR; Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI [10.1109/ICCV.2015.104, 10.1109/ICCV.2015.312]; Collins W., 2016, ARXIV160501156; Conley A.J., 2012, DESCRIPTION NCAR COM; Dettinger MD, 2011, WATER-SUI, V3, P445, DOI 10.3390/w3020445; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal R., 2017, ARXIV170604261; Hannun Awni Y., 2013, ICML WORKSH DEEP LEA; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hosseini-Asl E., 2016, ALZHEIMERS DIS DIAGN; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Kahou SE, 2015, ICMI'15: PROCEEDINGS OF THE 2015 ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P467, DOI 10.1145/2818346.2830596; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kay W., 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavers DA, 2012, J GEOPHYS RES-ATMOS, V117, DOI 10.1029/2012JD018027; Makhzani A., 2015, ARXIV151105644; Misra Ishan, 2015, ABS150505769 CORR; Monahan AH, 2009, J CLIMATE, V22, P6501, DOI 10.1175/2009JCLI3062.1; Neu U, 2013, B AM METEOROL SOC, V94, P529, DOI 10.1175/BAMS-D-11-00154.1; Parkhi Omkar M., 2015, BRIT MACH VIS C; Prabhat Oliver Rubel, 2012, ICCS; Prabhat Surendra Byna, 2015, CAIP; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2017, IMAGENET LARGE SCALE; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Springenberg Jost Tobias, 2015, ARXIV151106390; Srivastava Nitish, 2015, ABS150204681 CORR, V2; Steinhaeuser Karsten, 2011, Advances in Spatial and Temporal Databases. Proceedings 12th International Symposium (SSTD 2011), P39, DOI 10.1007/978-3-642-22922-0_4; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tran D, 2014, LEARNING SPATIOTEMPO; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wehner M, 2015, J CLIMATE, V28, P3905, DOI 10.1175/JCLI-D-14-00311.1; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Whitney William F., 2016, UNDERSTANDING VISUAL; Xie Jianwen, 2016, SYNTHESIZING DYNAMIC; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Yao L, 2015, IEEE I CONF COMP VIS, P4507, DOI 10.1109/ICCV.2015.512; Zhang Yuting, 2016, ARXIV160606582V1; Zhao J., 2015, ARXIV150602351	49	51	51	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403046
C	Kandasamy, K; Krishnamurthy, A; Poczos, B; Wasserman, L; Robins, JM		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Kandasamy, Kirthevasan; Krishnamurthy, Akshay; Poczos, Barnabas; Wasserman, Larry; Robins, James M.			Nonparametric von Mises Estimators for Entropies, Divergences and Mutual Informations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We propose and analyse estimators for statistical functionals of one or more distributions under nonparametric assumptions. Our estimators are derived from the von Mises expansion and are based on the theory of influence functions, which appear in the semiparametric statistics literature. We show that estimators based either on data-splitting or a leave-one-out technique enjoy fast rates of convergence and other favorable theoretical properties. We apply this framework to derive estimators for several popular information theoretic quantities, and via empirical evaluation, show the advantage of this approach over existing estimators.	[Kandasamy, Kirthevasan; Poczos, Barnabas; Wasserman, Larry] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Krishnamurthy, Akshay] Microsoft Res, New York, NY USA; [Robins, James M.] Harvard Univ, Cambridge, MA 02138 USA	Carnegie Mellon University; Microsoft; Harvard University	Kandasamy, K (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	kandasamy@cs.cmu.edu; akshaykr@cs.cmu.edu; bapoczos@cs.cmu.edu; larry@stat.cmu.edu; robins@hsph.harvard.edu			NSF [IIS-1247658]; DOE [DESC0011114]	NSF(National Science Foundation (NSF)); DOE(United States Department of Energy (DOE))	This work is supported in part by NSF Big Data grant IIS-1247658 and DOE grant DESC0011114.	Beirlant J., 1997, INT J MATH STAT SCI; Bickel Peter J., 1988, SANKHYA INDIAN J STA; Birge Lucien, 1995, ANN OF STAT; Carter Kevin M., 2010, IEEE T SIGNAL PROCES; Dhillon Inderjit S., 2003, J MACH LEARN RES; Emery M, 1998, LECT PROB THEORY STA; Goria Mohammed Nawaz, 2005, NONPARAMETRIC STAT; Hero Bing Ma, 2002, IEEE SIGNAL PROCESSI, V19; Kallberg David, 2012, ESTIMATION ENTROPY T; Kerkyacharian Gerard, 1996, ANN OF STAT; Krishnamurthy Akshay, 2015, ARTIFICIAL INTELLIGE; Krishnamurthy Akshay, 2014, ICML; Laurent Beatrice, 1996, ANN OF STAT; Learned-Miller Erik, 2003, MACH LEARN RES; Leibe Bastian, 2003, CVPR; Leonenko Nikolai, 2010, J MULTIVARIATE ANAL; Lewi Jeremy, 2006, NIPS; Liu Han, 2012, NIPS; Luisa Fernholz, 1983, LECT NOTES STAT; Miller Erik G, 2003, ICASSP; Moon Kevin, 2014, NIPS; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Noughabi Havva Alizadeh, 2013, J STAT COMPUTATION S; Pal David, 2010, NIPS; Peng Hanchuan, 2005, IEEE PAMI; Perez-Cruz Fernando, 2008, IEEE ISIT; Poczos Barnabas, 2011, AISTATS; Poczos Barnabas, 2011, UAI; Ramirez David, 2009, EUSIPCO; Robins James, 2009, METRIKA; Schneidman Elad, 2002, NIPS; Singh Shashank, 2014, NIPS; Stowell Dan, 2009, IEEE SIGNAL PROCESS; Szabo Zoltan, 2014, J MACH LEARN RES; Tsybakov A.B, 2008, INTRO NONPARAMETRIC; van der Vaart A.W., 1998, ASYMPTOTIC STAT, P87, DOI [10.1017/cbo9780511802256, DOI 10.1017/CBO9780511802256, 10.1017/CBO9780511802256]; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060	37	51	51	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102060
C	Simoncelli, EP; Schwartz, O		Kearns, MS; Solla, SA; Cohn, DA		Simoncelli, EP; Schwartz, O			Modeling surround suppression in V1 neurons with a statistically-derived normalization model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				CAT STRIATE CORTEX; VISUAL-CORTEX; CELLS; SELECTIVITY; RESPONSES	We examine the statistics of natural monochromatic images decomposed using a multi-scale wavelet basis. Although the coefficients of this representation are nearly decorrelated, they exhibit important higher-order statistical dependencies that cannot be eliminated with purely linear processing. In particular, rectified coefficients corresponding to basis functions at neighboring spatial positions, orientations and scales are highly correlated. A method of removing these dependencies is to divide each coefficient by a weighted combination of its rectified neighbors. Several successful models of the steady-state behavior of neurons in primary visual cortex are based on such "divisive normalization" computations, and thus our analysis provides a theoretical justification for these models. Perhaps more importantly, the statistical measurements explicitly specify the weights that should be used in computing the normalization signal. We demonstrate that this weighting is qualitatively consistent with recent physiological experiments that characterize the suppressive effect of stimuli presented outside of the classical receptive field. Our observations thus provide evidence for the hypothesis that early visual neural processing is well matched to these statistical properties of images.	NYU, Ctr Neural Sci, New York, NY 10012 USA	New York University	Simoncelli, EP (corresponding author), NYU, Ctr Neural Sci, New York, NY 10012 USA.			Simoncelli, Eero/0000-0002-1206-527X				ATTNEAVE F, 1954, PSYCHOL REV, V61, P183, DOI 10.1037/h0054663; Barlow H., 1961, SENSORY COMMUNICATIO, P217; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; BONDS AB, 1989, VISUAL NEUROSCI, V2, P41, DOI 10.1017/S0952523800004314; CARANDINI M, 1994, SCIENCE, V264, P1333, DOI 10.1126/science.8191289; Carandini M, 1997, J NEUROSCI, V17, P8621; Cardoso J.-F., 1989, ICASSP-89: 1989 International Conference on Acoustics, Speech and Signal Processing (IEEE Cat. No.89CH2673-2), P2109, DOI 10.1109/ICASSP.1989.266878; CAVANAUGH JR, 1997, NEUR ABSTR, V23; COMMON P, 1994, SIGNAL PROCESS, V36, P387; FIELD DJ, 1987, J OPT SOC AM A, V4, P2379, DOI 10.1364/JOSAA.4.002379; GEISLER WS, 1992, VISION RES, V32, P1409, DOI 10.1016/0042-6989(92)90196-P; HEEGER DJ, 1993, J NEUROPHYSIOL, V70, P1885, DOI 10.1152/jn.1993.70.5.1885; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; MALLAT SG, 1989, IEEE T PATTERN ANAL, V11, P674, DOI 10.1109/34.192463; MULLER J, 1998, UNPUB; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Olshausen BA, 1996, NETWORK-COMP NEURAL, V7, P333, DOI 10.1088/0954-898X/7/2/014; REICHARDT W, 1979, BIOL CYBERN, V35, P81, DOI 10.1007/BF00337434; SANGER TD, 1989, NEURAL NETWORKS, V2, P459, DOI 10.1016/0893-6080(89)90044-0; Simoncelli E., 1990, SUBBAND IMAGE CODING, P143; Simoncelli E. P., 1998, IOVS, V39, pS424; Simoncelli EP, 1998, VISION RES, V38, P743, DOI 10.1016/S0042-6989(97)00183-1; SIMONCELLI EP, 1997, NAT SCEN STAT M HANC; SIMONCELLI EP, 1997, 31 AS C SIGN SYST CO, P673; WEGMANN B, 1990, P SOC PHOTO-OPT INS, V1360, P909, DOI 10.1117/12.24279; [No title captured]	26	51	52	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						153	159						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700022
C	Thulasidasan, S; Chennupati, G; Bilmes, J; Bhattacharya, T; Michalak, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Thulasidasan, Sunil; Chennupati, Gopinath; Bilmes, Jeff; Bhattacharya, Tanmoy; Michalak, Sarah			On Mixup Training: Improved Calibration and Predictive Uncertainty for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Mixup [40] is a recently proposed method for training deep neural networks where additional samples are generated during training by convexly combining random pairs of images and their associated labels. While simple to implement, it has been shown to be a surprisingly effective method of data augmentation for image classification: DNNs trained with mixup show noticeable gains in classification performance on a number of image classification benchmarks. In this work, we discuss a hitherto untouched aspect of mixup training - the calibration and predictive uncertainty of models trained with mixup. We find that DNNs trained with mixup are significantly better calibrated - i.e., the predicted softmax scores are much better indicators of the actual likelihood of a correct prediction - than DNNs trained in the regular fashion. We conduct experiments on a number of image classification architectures and datasets - including large-scale datasets like ImageNet - and find this to be the case. Additionally, we find that merely mixing features does not result in the same calibration benefit and that the label smoothing in mixup training plays a significant role in improving calibration. Finally, we also observe that mixup-trained DNNs are less prone to over-confident predictions on out-of-distribution and random-noise data. We conclude that the typical overconfidence seen in neural networks, even on in-distribution data is likely a consequence of training with hard labels, suggesting that mixup be employed for classification tasks where predictive uncertainty is a significant concern.	[Thulasidasan, Sunil; Chennupati, Gopinath; Bhattacharya, Tanmoy; Michalak, Sarah] Los Alamos Natl Lab, Los Alamos, NM 87545 USA; [Thulasidasan, Sunil; Bilmes, Jeff] Univ Washington, Dept Elect & Comp Engn, Seattle, WA 98195 USA	United States Department of Energy (DOE); Los Alamos National Laboratory; University of Washington; University of Washington Seattle	Thulasidasan, S (corresponding author), Los Alamos Natl Lab, Los Alamos, NM 87545 USA.; Thulasidasan, S (corresponding author), Univ Washington, Dept Elect & Comp Engn, Seattle, WA 98195 USA.	sunil@lanl.gov	Chennupati, Gopinath/AAD-4505-2020; Bhattacharya, Tanmoy/J-8956-2013	Chennupati, Gopinath/0000-0002-6223-8570; Bhattacharya, Tanmoy/0000-0002-1060-652X	Joint Design of Advanced Computing Solutions for Cancer (JDACS4C) program; U.S. Department of Energy [DE-AC5206NA25396]; CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program; DARPA	Joint Design of Advanced Computing Solutions for Cancer (JDACS4C) program; U.S. Department of Energy(United States Department of Energy (DOE)); CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We would like to thank the anonymous referees for their valuable suggestions for improving the paper. The authors were supported in part by the Joint Design of Advanced Computing Solutions for Cancer (JDACS4C) program established by the U.S. Department of Energy (DOE) and the National Cancer Institute (NCI) of the National Institutes of Health. This work was performed under the auspices of the U.S. Department of Energy by Los Alamos National Laboratory under Contract DE-AC5206NA25396. This work was also supported in part by the CONIX Research Center, one of six centers in JUMP, a Semiconductor Research Corporation (SRC) program sponsored by DARPA.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Berk R, 2017, J EXP CRIMINOL, V13, P193, DOI 10.1007/s11292-017-9286-2; Cer D., 2018, ARXIV180311175 CS; Chapelle O, 2001, ADV NEUR IN, V13, P416; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Gal Y, 2016, PR MACH LEARN RES, V48; Guo Chuan, 2017, ARXIV170604599; Guo H., 2019, ARXIV190508941; Guo Hongyu, 2018, ARXIV180902499; Han X., 2017, ARXIVCSLGCSLG1708077; Hendrycks Dan, 2016, ARXIV 161002136; Howard J, 2018, ARXIV180106146; Inoue H., 2018, ARXIV PREPRINT ARXIV; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kendall A., 2017, WHAT UNCERTAINTIES W, V3, P4; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Kyrola A., 2017, ABS170602677 ARXIV; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lee K., 2017, ARXIV171109325; Levinson J, 2011, IEEE INT VEH SYM, P163, DOI 10.1109/IVS.2011.5940562; Li Xin, 2002, COLING 2002 19 INT C, DOI DOI 10.3115/1072228.1072378; Liang DJ, 2018, IEEE ACCESS, V6, P58774, DOI 10.1109/ACCESS.2018.2872698; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Malkin J, 2009, INT CONF ACOUST SPEE, P4465, DOI 10.1109/ICASSP.2009.4960621; Miotto R, 2016, SCI REP-UK, V6, DOI 10.1038/srep26094; Muller R., 2019, ARXIV190602629; Pang B., 2005, P 43 ANN M ASS COMP, V43, P115, DOI DOI 10.3115/1219840.1219855; Peng Zhou, 2016, ARXIV161106639; Pennington J., 2014, P 2014 C EMPIRICAL M, P1532; Pereyra Gabriel, 2017, ICLRW, P5; Sensoy Murat, 2018, NIPS, P3179; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Thulasidasan S, 2019, PR MACH LEARN RES, V97; Vapnik VN., 2015, MEASURES COMPLEXITY, P11, DOI [10.1007/978-3-319-21852-6_3, DOI 10.1007/978-3-319-21852-6_3]; Verma Vikas, 2018, ARXIV180605236; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yoon K, 2014, CONVOLUTIONAL NEURAL; Zhang Hongyi, 2017, ARXIV171009412, DOI DOI 10.1007/978-3-030-01231-1_31	40	50	50	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905055
C	Zhao, SC; Li, B; Yue, XY; Gu, Y; Xu, PF; Hu, RB; Chai, H; Keutzer, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhao, Sicheng; Li, Bo; Yue, Xiangyu; Gu, Yang; Xu, Pengfei; Hu, Runbo; Chai, Hua; Keutzer, Kurt			Multi-source Domain Adaptation for Semantic Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Simulation-to-real domain adaptation for semantic segmentation has been actively studied for various applications such as autonomous driving. Existing methods mainly focus on a single-source setting, which cannot easily handle a more practical scenario of multiple sources with different distributions. In this paper, we propose to investigate multi-source domain adaptation for semantic segmentation. Specifically, we design a novel framework, termed Multi-source Adversarial Domain Aggregation Network (MADAN), which can be trained in an end-to-end manner. First, we generate an adapted domain for each source with dynamic semantic consistency while aligning at the pixel-level cycle-consistently towards the target. Second, we propose sub-domain aggregation discriminator and cross-domain cycle discriminator to make different adapted domains more closely aggregated. Finally, feature-level alignment is performed between the aggregated domain and target domain while training the segmentation network. Extensive experiments from synthetic GTA and SYNTHIA to real Cityscapes and BDDS datasets demonstrate that the proposed MADAN model outperforms state-of-the-art approaches. Our source code is released at: https://github.com/Luodian/MADAN.	[Zhao, Sicheng; Yue, Xiangyu; Keutzer, Kurt] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Li, Bo; Gu, Yang; Xu, Pengfei; Hu, Runbo; Chai, Hua] Didi Chuxing, Beijing, Peoples R China; [Li, Bo] Harbin Inst Technol, Harbin, Peoples R China	University of California System; University of California Berkeley; Harbin Institute of Technology	Zhao, SC (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	schzhao@gmail.com; drluodian@gmail.com; xyyue@berkeley.edu; guyangdavid@didiglobal.com; xupengfeipf@didiglobal.com; hurunbo@didiglobal.com; chaihua@didiglobal.com; keutzer@berkeley.edu			Berkeley DeepDrive; National Natural Science Foundation of China [61701273]	Berkeley DeepDrive; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by Berkeley DeepDrive and the National Natural Science Foundation of China (No. 61701273).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2007, P 15 ACM INT C MULTI; [Anonymous], 2009, ADV NEURAL INFORM PR; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Becker Carlos J, 2013, ADV NEURAL INFORM PR, V1, P485; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Chattopadhyay R, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2382577.2382582; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YH, 2018, PROC CVPR IEEE, P7892, DOI 10.1109/CVPR.2018.00823; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Duan LX, 2012, IEEE T NEUR NET LEAR, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Duan LX, 2012, PROC CVPR IEEE, P1338, DOI 10.1109/CVPR.2012.6247819; Dundar A., 2018, ARXIV PREPRINT ARXIV; Geiger A., 2012, P IEEE COMP SOC C CO; Ghifary M, 2016, LECT NOTES COMPUT SC, V9908, P597, DOI 10.1007/978-3-319-46493-0_36; Ghifary M, 2015, IEEE I CONF COMP VIS, P2551, DOI 10.1109/ICCV.2015.293; Glorot Xavier, 2011, P 28 INT C MACH LEAR, P513, DOI DOI 10.1177/1753193411430810; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2014, IEEE T PATTERN ANAL, V36, P2288, DOI 10.1109/TPAMI.2013.249; Guo PF, 2018, CHIN CONTR CONF, P8568; Hoffman J, 2018, PR MACH LEARN RES, V80; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Hong ZW, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4912; Hu LQ, 2018, PROC CVPR IEEE, P1498, DOI 10.1109/CVPR.2018.00162; Hua XD, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON SMART CITY AND SYSTEMS ENGINEERING (ICSCSE), P518, DOI 10.1109/ICSCSE.2018.00111; Jaradat S, 2017, PROCEEDINGS OF THE ELEVENTH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'17), P407, DOI 10.1145/3109859.3109861; Jhuo IH, 2012, PROC CVPR IEEE, P2168, DOI 10.1109/CVPR.2012.6247924; Kingma D.P, P 3 INT C LEARNING R; LIN GS, 2016, PROC CVPR IEEE, P3194, DOI DOI 10.1109/CVPR.2016.348; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu ZW, 2015, IEEE I CONF COMP VIS, P1377, DOI 10.1109/ICCV.2015.162; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long M, 2015, AUST HUMANIT REV, P93; Louizos C., 2015, ARXIV PREPRINT ARXIV; Patel VM, 2015, IEEE SIGNAL PROC MAG, V32, P53, DOI 10.1109/MSP.2014.2347059; Peng X., 2018, ARXIV181201754; Peng Xingchao, 2017, VISDA VISUAL DOMAIN; Richter SR, 2016, LECT NOTES COMPUT SC, V9906, P102, DOI 10.1007/978-3-319-46475-6_7; Riemer Matthew, 2019, INT C LEARN REPR, V1; Ronneberger O, 2016, INT C MED IM COMP CO, P424, DOI DOI 10.1007/978-3-319-46723-8_49; ROS G, 2016, PROC CVPR IEEE, P3234, DOI DOI 10.1109/CVPR.2016.352; Russo P, 2018, PROC CVPR IEEE, P8099, DOI 10.1109/CVPR.2018.00845; Sankaranarayanan S, 2018, PROC CVPR IEEE, P8503, DOI 10.1109/CVPR.2018.00887; Sinno J.P., 2009, IEEE T KNOWL DATA EN, V22, P1345, DOI [10.1109/TKDE.2009.191, DOI 10.1109/TKDE.2009.191]; Sun BC, 2017, ADV COMPUT VIS PATT, P153, DOI 10.1007/978-3-319-58347-1_8; Sun BC, 2016, AAAI CONF ARTIF INTE, P2058; SUN Q, 2011, ADV NEURAL INFORM PR, P509, DOI DOI 10.1109/TRUSTCOM.2011.66; Sun SL, 2013, INT CONF MACH LEARN, P24, DOI 10.1109/ICMLC.2013.6890438; Sun SL, 2015, INFORM FUSION, V24, P84, DOI 10.1016/j.inffus.2014.12.003; Tao Y, 2017, CHIN CONTR CONF, P4288, DOI 10.23919/ChiCC.2017.8028032; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tsai YH, 2018, PROC CVPR IEEE, P7472, DOI 10.1109/CVPR.2018.00780; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang PQ, 2018, IEEE WINT CONF APPL, P1451, DOI 10.1109/WACV.2018.00163; Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI 10.1109/ICRA.2019.8793495; Xu RJ, 2018, PROC CVPR IEEE, P3964, DOI 10.1109/CVPR.2018.00417; Xu ZJ, 2012, LECT NOTES COMPUT SC, V7665, P332, DOI 10.1007/978-3-642-34487-9_41; Yu F., 2016, P ICLR 2016; Yu Fisher, 2018, BDD100K DIVERSE DRIV, P6; Yue XY, 2018, ICMR '18: PROCEEDINGS OF THE 2018 ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA RETRIEVAL, P458, DOI 10.1145/3206025.3206080; Yue Xiangyu, 2019, IEEE INT C COMP VIS; Zhang Y, 2017, IEEE I CONF COMP VIS, P2039, DOI 10.1109/ICCV.2017.223; Zhang Y, 2018, PROC CVPR IEEE, P4320, DOI 10.1109/CVPR.2018.00454; Zhao SC, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1319, DOI 10.1145/3240508.3240591; Zhao SC, 2019, AAAI CONF ARTIF INTE, P2620; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179; Zhou BL, 2019, INT J COMPUT VISION, V127, P302, DOI 10.1007/s11263-018-1140-0; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244; Zhu XG, 2018, LECT NOTES COMPUT SC, V11211, P587, DOI 10.1007/978-3-030-01234-2_35; Zhuo JB, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P261, DOI 10.1145/3123266.3123292	74	50	51	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307032
C	Chen, TQ; Zheng, LM; Yan, E; Jiang, ZH; Moreau, T; Ceze, L; Guestrin, C; Krishnamurthy, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Tianqi; Zheng, Lianmin; Yan, Eddie; Jiang, Ziheng; Moreau, Thierry; Ceze, Luis; Guestrin, Carlos; Krishnamurthy, Arvind			Learning to Optimize Tensor Programs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PARALLELISM	We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, current systems rely on manually optimized libraries, e.g., cuDNN, that support only a narrow range of server class GPUs. Such reliance limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search using effective model transfer across workloads. Experimental results show that our framework delivers performance that is competitive with state-of-the-art hand-tuned libraries for low-power CPUs, mobile GPUs, and server-class GPUs.	[Chen, Tianqi; Yan, Eddie; Jiang, Ziheng; Moreau, Thierry; Ceze, Luis; Guestrin, Carlos; Krishnamurthy, Arvind] Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA; [Zheng, Lianmin] Shanghai Jiao Tong Univ, Shanghai, Peoples R China	University of Washington; University of Washington Seattle; Shanghai Jiao Tong University	Chen, TQ (corresponding author), Univ Washington, Paul G Allen Sch Comp Sci & Engn, Seattle, WA 98195 USA.		Jiang, Ziheng/GON-6875-2022		Google PhD Fellowship; ONR [N00014-16-1-2795]; NSF [CCF-1518703, CNS-1614717, CCF-1723352]	Google PhD Fellowship(Google Incorporated); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	We would like to thank members of Sampa, SAMPL and Systems groups at the Allen School for their feedback on the work and manuscript. This work was supported in part by a Google PhD Fellowship for Tianqi Chen, ONR award #N00014-16-1-2795, NSF under grants CCF-1518703, CNS-1614717, and CCF-1723352, and gifts from Intel (under the CAPA program), Oracle, Huawei and anonymous sources.	Abadi M, 2015, P 12 USENIX S OPERAT; Agarwal A, 2014, MSRTR2014112; Allamanis M., 2018, INT C LEARNING REPRE; Bastien F., 2012, DEEP LEARN UNS FEAT; Bondhugula U, 2008, PLDI'08: PROCEEDINGS OF THE 2008 SIGPLAN CONFERENCE ON PROGRAMMING LANGUAGE DESIGN & IMPLEMENTATION, P101, DOI 10.1145/1375581.1375595; Burges Chris, 2005, P 22 INT C MACH LEAR, P89, DOI DOI 10.1145/1102351.1102363; Chen TQ, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P785, DOI 10.1145/2939672.2939785; Chen TQ, 2018, PROCEEDINGS OF THE 13TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P579; Chen Xingyu, 2018, CORR; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; DeVito Zachary, 2018, ARXIV180204730; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Frigo M, 1998, INT CONF ACOUST SPEE, P1381; Golovin D, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1487, DOI 10.1145/3097983.3098043; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Henriksen T, 2017, ACM SIGPLAN NOTICES, V52, P556, DOI 10.1145/3062341.3062354; Howard A.G., 2017, MOBILENETS EFFICIENT; Hutter F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P4197; KIRKPATRICK S, 1983, SCIENCE, V220, P671, DOI 10.1126/science.220.4598.671; Kjolstad F, 2017, P ACM PROGRAM LANG, V1, DOI 10.1145/3133901; Kraska Tim, 2017, CORR; Krause A, 2014, TRACTABILITY, P71; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lavin A, 2016, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2016.435; Li L., 2016, ABS160306560 CORR; Mirhoseini A, 2017, PR MACH LEARN RES, V70; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mullapudi RT, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925952; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Palkar S., 2017, CORR; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ragan-Kelley J, 2013, ACM SIGPLAN NOTICES, V48, P519, DOI 10.1145/2499370.2462176; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Steuwer M, 2017, INT SYM CODE GENER, P74, DOI 10.1109/CGO.2017.7863730; Sujeeth A., 2011, P ANN IEEE ACM INT S, P609; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; VASILACHE N., COMMUNICATION; Verdoolaege S., 2013, ACM T ARCHIT CODE OP, V9, DOI DOI 10.1145/2400682.2400713; Whaley R.C., 1998, P 1998 ACMIEEE C SUP, P1, DOI [DOI 10.1109/SC.1998.10004, 10.5555/509058.509096]; Zaremba Wojciech, 2014, ABS14092329 CORR	44	50	50	0	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303039
C	Hadfield-Menell, D; Milli, S; Abbeel, P; Russell, S; Dragan, AD		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca D.			Inverse Reward Design	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Autonomous agents optimize the reward function we give them. What they don't know is how hard it is for us to design a reward function that actually captures what we want. When designing the reward, we might think of some specific training scenarios, and make sure that the reward will lead to the right behavior in those scenarios. Inevitably, agents encounter new scenarios (e.g., new types of terrain) where optimizing that same reward may lead to undesired behavior. Our insight is that reward functions are merely observations about what the designer actually wants, and that they should be interpreted in the context in which they were designed. We introduce inverse reward design (IRD) as the problem of inferring the true objective based on the designed reward and the training MDP. We introduce approximate methods for solving IRD problems, and use their solution to plan risk-averse behavior in test MDPs. Empirical results suggest that this approach can help alleviate negative side effects of misspecified reward functions and mitigate reward hacking.	[Hadfield-Menell, Dylan; Milli, Smitha; Abbeel, Pieter; Russell, Stuart; Dragan, Anca D.] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94709 USA; [Abbeel, Pieter] Int Comp Sci Inst, OpenAI, Berkeley, CA 94704 USA	University of California System; University of California Berkeley	Hadfield-Menell, D (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94709 USA.	dhm@cs.berkeley.edu; smilli@cs.berkeley.edu; pabbeel@cs.berkeley.edu; russell@cs.berkeley.edu; anca@cs.berkeley.edu	Jeong, Yongwook/N-7413-2016	Dragan, Anca/0000-0001-6312-5466	Center for Human Compatible AI; Open Philanthropy Project; Future of Life Institute; AFOSR; NSF [DGE 1106400]	Center for Human Compatible AI; Open Philanthropy Project; Future of Life Institute; AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); NSF(National Science Foundation (NSF))	This work was supported by the Center for Human Compatible AI and the Open Philanthropy Project, the Future of Life Institute, AFOSR, and NSF Graduate Research Fellowship Grant No. DGE 1106400.	Amodei D, 2016, FAULTY REWARD FUNCTI; Amodei D., 2016, ABS160606565 CORR; Cole P., 1975, LOGIC CONVERSATION S, P43; Duan Yan, 2016, ABS161102779 CORR; Evans O, 2016, AAAI CONF ARTIF INTE, P323; Frank M. C., 2009, P 31 ANN C COGN SCI, P1228; Goodman Noah D, 2014, HDB CONT SEMANTIC TH, V2; Hadfield-Menell Dylan, 2017, P INT JOINT C ART IN; Hadfield-Menell Dylan, 2016, P 30 ANN C NEUR INF; Jain A, 2015, INT J ROBOT RES, V34, P1296, DOI 10.1177/0278364915581193; Javdani S, 2015, ROBOTICS: SCIENCE AND SYSTEMS XI; Murray I., 2006, P 22 C UNC ART INT; Puterman M., 2009, MARKOV DECISION PROC; Russell SP, 2010, ARTIFICIAL INTELLIGE; Singh Satinder, 2010, P INT S AI INSP BIOL, P111; Sorg J., 2010, ADV NEURAL INFORM PR, V23, P2190; Sunnaker M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002803; Syed U., 2007, ADV NEURAL INFORM PR; Ziebart B. D., 2008, AAAI, V8, P1433	20	50	51	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406080
C	Rae, JW; Hunt, JJ; Harley, T; Danihelka, I; Senior, A; Wayne, G; Graves, A; Lillicrap, TP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Rae, Jack W.; Hunt, Jonathan J.; Harley, Tim; Danihelka, Ivo; Senior, Andrew; Wayne, Greg; Graves, Alex; Lillicrap, Timothy P.			Scaling Memory-Augmented Neural Networks with Sparse Reads and Writes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Neural networks augmented with external memory have the ability to learn algorithmic solutions to complex tasks. These models appear promising for applications such as language modeling and machine translation. However, they scale poorly in both space and time as the amount of memory grows - limiting their applicability to real-world domains. Here, we present an end-to-end differentiable memory access scheme, which we call Sparse Access Memory (SAM), that retains the representational power of the original approaches whilst training efficiently with very large memories. We show that SAM achieves asymptotic lower bounds in space and time complexity, and find that an implementation runs 1,000 x faster and with 3,000 x less physical memory than non-sparse models. SAM learns with comparable data efficiency to existing models on a range of synthetic tasks and one-shot Omniglot character recognition, and can scale to tasks requiring 100,000s of time steps and memories. As well, we show how our approach can be adapted for models that maintain temporal associations between memories, as with the recently introduced Differentiable Neural Computer.	[Rae, Jack W.; Hunt, Jonathan J.; Harley, Tim; Danihelka, Ivo; Senior, Andrew; Wayne, Greg; Graves, Alex; Lillicrap, Timothy P.] Google DeepMind, London, England	Google Incorporated	Rae, JW (corresponding author), Google DeepMind, London, England.	jwrae@gmail.com; jjhunt@gmail.com; tharley@gmail.com; danihelka@gmail.com; andrewsenior@gmail.com; gregwayne@gmail.com; gravesa@gmail.com; countzero@gmail.com						Arya S, 1998, J ACM, V45, P891, DOI 10.1145/293347.293348; Bawa Mayank, 2005, P 14 INT C WORLD WID, P651, DOI DOI 10.1145/1060745.1060840; Brady TF, 2008, P NATL ACAD SCI USA, V105, P14325, DOI 10.1073/pnas.0803390105; Collobert R., 2011, NIPS; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Hill Felix, 2015, ARXIV151102301; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lakshminarayanan B., 2014, P ADV NEUR INF PROC, P3140; Mikolov T., 2015, ARXIV150205698, V1502, P05698; Motwani R, 2007, SIAM J DISCRETE MATH, V21, P930, DOI 10.1137/050646858; Muja M, 2014, IEEE T PATTERN ANAL, V36, P2227, DOI 10.1109/TPAMI.2014.2321376; Santoro A, 2016, PR MACH LEARN RES, V48; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Weston J., 2014, ARXIV14103916; Zaremba Wojciech, 2015, ARXIV150500521	22	50	50	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702045
C	Farajtabar, M; Wang, YC; Gomez-Rodriguez, M; Li, S; Zha, HY; Song, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Farajtabar, Mehrdad; Wang, Yichen; Gomez-Rodriguez, Manuel; Li, Shuang; Zha, Hongyuan; Song, Le			COEVOLVE: A Joint Point Process Model for Information Diffusion and Network Co-evolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Information diffusion in online social networks is affected by the underlying network topology, but it also has the power to change it. Online users are constantly creating new links when exposed to new information sources, and in turn these links are alternating the way information spreads. However, these two highly intertwined stochastic processes, information diffusion and network evolution, have been predominantly studied separately, ignoring their co-evolutionary dynamics. We propose a temporal point process model, COEVOLVE, for such joint dynamics, allowing the intensity of one process to be modulated by that of the other. This model allows us to efficiently simulate interleaved diffusion and network events, and generate traces obeying common diffusion and network patterns observed in real-world networks. Furthermore, we also develop a convex optimization framework to learn the parameters of the model from historical diffusion and network evolution traces. We experimented with both synthetic data and data gathered from Twitter, and show that our model provides a good fit to the data as well as more accurate predictions than alternatives.	[Farajtabar, Mehrdad; Wang, Yichen; Li, Shuang; Zha, Hongyuan; Song, Le] Georgia Inst Technol, Atlanta, GA 30332 USA; [Gomez-Rodriguez, Manuel] MPI Software Syst, Kaiserslautern, Germany	University System of Georgia; Georgia Institute of Technology	Farajtabar, M (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	mehrdad@gatech.edu; yichen.wang@gatech.edu; manuelgr@mpi-sws.org; sli370@gatech.edu; zha@cc.gatech.edu; lsong@cc.gatech.edu	Rodriguez, Manuel Gomez/AAB-5005-2021	Gomez Rodriguez, Manuel/0000-0003-3930-1161	NSF/NIH [BIGDATA 1R01GM108341]; ONR [N00014-15-1-2340]; NSF [IIS-1218749, CAREER IIS-1350983]	NSF/NIH(National Science Foundation (NSF)United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSF(National Science Foundation (NSF))	The authors would like to thank Demetris Antoniades and Constantine Dovrolis for providing them with the dataset. The research was supported in part by NSF/NIH BIGDATA 1R01GM108341, ONR N00014-15-1-2340, NSF IIS-1218749, NSF CAREER IIS-1350983.	Aalen OO, 2008, STAT BIOL HEALTH, P1; [Anonymous], 2010, PROC 16 ACM SIGKDD I, DOI DOI 10.1145/1835804.1835933; Antoniades D., 2013, ARXIV13096001; Backstrom L., 2012, WEBSCI; Blundell C, 2012, NIPS; Chakrabarti D, 2004, SIAM PROC S, P442; Cheng J, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P925, DOI 10.1145/2566486.2567997; Du Nan, 2013, Adv Neural Inf Process Syst, V26, P3147; ERDOS P, 1960, B INT STATIST INST, V38, P343; Farajtabar Mehrdad, 2014, Adv Neural Inf Process Syst, V27; Goel S., 2012, EC; Gomez-Rodriguez Manuel, 2011, ICML, P561; GRANOVETTER MS, 1973, AM J SOCIOL, V78, P1360, DOI 10.1086/225469; Gross T., 2008, ADAPTIVE COEVOLUTION; Hunter D., 2011, ICML; Iwata T., 2013, KDD; Kempe D., 2003, ACM SIGKDD INT C KNO, P137, DOI DOI 10.1145/956750.956769; Kwak H., 2010, P 19 INT C WORLD WID, P591, DOI [10.1145/1772690.1772751, DOI 10.1145/1772690.1772751]; Leskovec J., 2008, PROC 14 ACM SIGKDD I, P462, DOI DOI 10.1145/1401890.1401948; Leskovec J., 2010, JMLR; Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893; Linderman SW, 2014, PR MACH LEARN RES, V32, P1413; Liniger T, 2009, THESIS; Myers S, 2014, WWW'14: PROCEEDINGS OF THE 23RD INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P913, DOI 10.1145/2566486.2568043; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Romero D., 2010, P INT AAAI C WEB SOC, P138, DOI DOI 10.1016/J.REGPEP.2010.05.014; Singer Philipp, 2012, Modeling and Mining Ubiquitous Social Media. International Workshops MSM 2011 and MUSE 2011. Revised Selected Papers, P40, DOI 10.1007/978-3-642-33684-3_3; Valera I., 2015, ICDM; Vu D. Q., 2011, NIPS; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; Weng L., 2013, KDD; Zhou K, 2013, ICML; Zhou K., 2013, ARTIF INTELL, P641	34	50	51	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101014
C	Rudi, A; Camoriano, R; Rosasco, L		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Rudi, Alessandro; Camoriano, Raffaello; Rosasco, Lorenzo			Less is More: Nystrom Computational Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				MATRIX; APPROXIMATION; ALGORITHMS	We study Nystrom type subsampling approaches to large scale kernel methods, and prove learning bounds in the statistical learning setting, where random sampling and high probability estimates are considered. In particular, we prove that these approaches can achieve optimal learning bounds, provided the subsampling level is suitably chosen. These results suggest a simple incremental variant of Nystrom Kernel Regularized Least Squares, where the subsampling level implements a form of computational regularization, in the sense that it controls at the same time regularization and computations. Extensive experimental analysis shows that the considered approach achieves state of the art performances on benchmark large scale datasets.	[Rudi, Alessandro; Camoriano, Raffaello; Rosasco, Lorenzo] Univ Genoa, DIBRIS, Via Dodecaneso 35, Genoa, Italy; [Camoriano, Raffaello] Ist Italiano Tecnol, iCub Facil, Genoa, Italy; [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA; [Rosasco, Lorenzo] Ist Italiano Tecnol, Lab Computat & Stat Learning, Cambridge, MA 02139 USA	University of Genoa; Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT)	Rudi, A (corresponding author), Univ Genoa, DIBRIS, Via Dodecaneso 35, Genoa, Italy.	ale_rudi@mit.edu; raffaello.camoriano@iit.it; lrosasco@mit.edu	Camoriano, Raffaello/GWN-0743-2022		Center for Brains, Minds and Machines (CBMM) - NSF STC award [CCF-1231216]; FIRB project - Italian Ministry of Education, University and Research [RBFR12M3AC]	Center for Brains, Minds and Machines (CBMM) - NSF STC award; FIRB project - Italian Ministry of Education, University and Research	The work described in this paper is supported by the Center for Brains, Minds and Machines (CBMM), funded by NSF STC award CCF-1231216; and by FIRB project RBFR12M3AC, funded by the Italian Ministry of Education, University and Research.	Alaoui A., 2014, FAST RANDOMIZED KERN; Bach Francis, 2013, COLT, V30; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Caponnetto A, 2010, ANAL APPL, V8, P161, DOI 10.1142/S0219530510001564; Cohen MB, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P181, DOI 10.1145/2688073.2688113; Cortes C, 2010, P 13 INT C ART INT S, P113; Dai B., 2014, NIPS; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Gittens A., 2013, INT C MACHINE LEARNI, P567; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Jin R, 2013, INFORM THEORY IEEE T, V59; Kumar S., 2009, ADV NEURAL INFORM PR, P1060; Kumar S, 2012, J MACH LEARN RES, V13, P981; Le Q., 2013, ICML; Li M., 2010, P 27 INT C MACH LEAR, P631, DOI DOI 10.5555/3104322.3104403; Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517; Mendelson S, 2010, ANN STAT, V38, P526, DOI 10.1214/09-AOS728; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudi Alessandro, 2013, ADV NEURAL INFORM PR, P2067; Scholkopf B., 2002, LEARNING KERNELS; Si S, 2014, PR MACH LEARN RES, V32; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Steinwart I., 2009, COLT; Wang SS, 2014, JMLR WORKSH CONF PRO, V33, P996; Wang SS, 2013, J MACH LEARN RES, V14, P2729; Yang JY, 2014, PR MACH LEARN RES, V32; Yang T., 2012, ADV NEURAL INFORM PR, P476; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Zhang K., 2008, P 25 INT C MACHINE L, P1232; Zhang Yuchen, 2013, C LEARN THEOR, P592617	34	50	50	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102084
C	Schwartz, O; Chichilnisky, EJ; Simoncelli, EP		Dietterich, TG; Becker, S; Ghahramani, Z		Schwartz, O; Chichilnisky, EJ; Simoncelli, EP			Characterizing neural gain control using spike-triggered covariance	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				RETINAL GANGLION-CELLS; CAT STRIATE CORTEX; TEMPORAL CONTRAST; ADAPTATION; NORMALIZATION; RESPONSES	Spike-triggered averaging techniques are effective for linear characterization of neural responses. But neurons exhibit important nonlinear behaviors, such as gain control, that are not captured by such analyses. We describe a spike-triggered covariance method for retrieving suppressive components of the gain control signal in a neuron. We demonstrate the method in simulation and on retinal ganglion cell data. Analysis of physiological data reveals significant suppressive axes and explains neural nonlinearities. This method should be applicable to other sensory areas and modalities.	NYU, Ctr Neural Sci, New York, NY 10003 USA	New York University	Schwartz, O (corresponding author), NYU, Ctr Neural Sci, 550 1St Ave, New York, NY 10003 USA.	odelia@cns.nyu.edu; ej@salk.edu; eero.simoncelli@nyu.edu		Simoncelli, Eero/0000-0002-1206-527X; Chichilnisky, E.J./0000-0002-5613-0248				ARCAS BAY, 2000, ADV NEURAL INFORMATI, V13, P75; Carandini M, 1997, J NEUROSCI, V17, P8621; Chander D, 2001, J NEUROSCI, V21, P9904, DOI 10.1523/JNEUROSCI.21-24-09904.2001; Chichilnisky EJ, 2001, NETWORK-COMP NEURAL, V12, P199, DOI 10.1088/0954-898X/12/2/306; DEBOER E, 1968, IEEE T BIO-MED ENG, VBM15, P169, DOI 10.1109/TBME.1968.4502561; GEISLER WS, 1992, VISION RES, V32, P1409, DOI 10.1016/0042-6989(92)90196-P; HEEGER DJ, 1992, VISUAL NEUROSCI, V9, P181, DOI 10.1017/S0952523800009640; JONES JP, 1987, J NEUROPHYSIOL, V58, P1187, DOI 10.1152/jn.1987.58.6.1187; Kim KJ, 2001, J NEUROSCI, V21, P287, DOI 10.1523/JNEUROSCI.21-01-00287.2001; LYON RF, 1990, LECT NOTES BIOMATH, V87, P395; Meister M, 1999, NEURON, V22, P435, DOI 10.1016/S0896-6273(00)80700-X; Ringach DL, 1997, VISION RES, V37, P2455, DOI 10.1016/S0042-6989(96)00247-7; Schwartz O, 2001, NAT NEUROSCI, V4, P819, DOI 10.1038/90526; Shapley R., 1984, PROG RETIN RES, V3, P263, DOI [DOI 10.1016/0278-4327(84)90011-7, 10.1016/0278-4327(84)90011-7]; SHAPLEY RM, 1978, J PHYSIOL-LONDON, V285, P275, DOI 10.1113/jphysiol.1978.sp012571; Smirnakis SM, 1997, NATURE, V386, P69, DOI 10.1038/386069a0; VANSTEVENINCK RD, 1988, PROC R SOC SER B-BIO, V234, P379; VICTOR JD, 1987, J PHYSIOL-LONDON, V386, P219, DOI 10.1113/jphysiol.1987.sp016531	18	50	51	0	5	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						269	276						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100034
C	Huh, D; Sejnowski, TJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Huh, Dongsung; Sejnowski, Terrence J.			Gradient Descent for Spiking Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ERROR-BACKPROPAGATION; RULE	Most large-scale network models use neurons with static nonlinearities that produce analog output, despite the fact that information processing in the brain is predominantly carried out by dynamic neurons that produce discrete pulses called spikes. Research in spike-based computation has been impeded by the lack of efficient supervised learning algorithm for spiking neural networks. Here, we present a gradient descent method for optimizing spiking network models by introducing a differentiable formulation of spiking dynamics and deriving the exact gradient calculation. For demonstration, we trained recurrent spiking networks on two dynamic tasks: one that requires optimizing fast (approximate to millisecond) spike-based interactions for efficient encoding of information, and a delayed-memory task over extended duration (approximate to second). The results show that the gradient descent approach indeed optimizes networks dynamics on the time scale of individual spikes as well as on behavioral time scales. In conclusion, our method yields a general purpose supervised learning algorithm for spiking neural networks, which can facilitate further investigations on spike-based computations.	[Huh, Dongsung; Sejnowski, Terrence J.] Salk Inst Biol Studies, La Jolla, CA 92037 USA	Salk Institute	Huh, D (corresponding author), Salk Inst Biol Studies, La Jolla, CA 92037 USA.	huh@salk.edu; terry@salk.edu	Sejnowski, Terrence/AAV-5558-2021					Abbott LF, 2016, NAT NEUROSCI, V19, P350, DOI 10.1038/nn.4241; Boerlin M, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1003258; Bohte SM, 2002, NEUROCOMPUTING, V48, P17, DOI 10.1016/S0925-2312(01)00658-0; Booij O, 2005, INFORM PROCESS LETT, V95, P552, DOI 10.1016/j.ipl.2005.05.023; Brea J, 2013, J NEUROSCI, V33, P9565, DOI 10.1523/JNEUROSCI.4098-12.2013; Brendel W., 2017, ARXIV170303777; Deneve S, 2016, NAT NEUROSCI, V19, P375, DOI 10.1038/nn.4243; DIEHL PU, 2015, IEEE IJCNN; Ermentrout B., 2008, SCHOLARPEDIA, V3, P1398, DOI [10.4249/scholarpedia.1398, DOI 10.4249/SCHOLARPEDIA.1398]; Florian RV, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0040233; Fremaux N, 2016, FRONT NEURAL CIRCUIT, V9, DOI 10.3389/fncir.2015.00085; Gardner B, 2016, PLOS ONE, V11, DOI 10.1371/journal.pone.0161335; Gardner B, 2015, NEURAL COMPUT, V27, P2548, DOI 10.1162/NECO_a_00790; Gutig R, 2016, SCIENCE, V351, DOI 10.1126/science.aab4113; Gutig R, 2006, NAT NEUROSCI, V9, P420, DOI 10.1038/nn1643; Hunsberger Eric, 2015, ARXIV151008829; Izhikevich EM, 2007, CEREB CORTEX, V17, P2443, DOI 10.1093/cercor/bhl152; King DB, 2015, ACS SYM SER, V1214, P1; Lajoie G, 2013, PHYS REV E, V87, DOI 10.1103/PhysRevE.87.052901; Lee JH, 2016, FRONT NEUROSCI-SWITZ, V10, DOI 10.3389/fnins.2016.00508; Legenstein R, 2008, PLOS COMPUT BIOL, V4, DOI 10.1371/journal.pcbi.1000180; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; McKennoch S, 2009, NEURAL COMPUT, V21, P9, DOI 10.1162/neco.2008.09-07-610; Memmesheimer RM, 2014, NEURON, V82, P925, DOI 10.1016/j.neuron.2014.03.026; O'Connor P., 2016, ARXIV160208323; O'Connor P, 2013, FRONT NEUROSCI-SWITZ, V7, DOI 10.3389/fnins.2013.00178; Pfister JP, 2006, NEURAL COMPUT, V18, P1318, DOI 10.1162/neco.2006.18.6.1318; Pontryagin L.S., 1962, MATH THEORY OPTIMAL; Ponulak F, 2010, NEURAL COMPUT, V22, P467, DOI 10.1162/neco.2009.11-08-901; Rezende D., 2011, ADV NEURAL INFORM PR, V24, P136; Rueckauer Bodo, 2016, ARXIV161204052; Schrauwen B, 2004, IEEE IJCNN, P471, DOI 10.1109/IJCNN.2004.1379954; Sengupta A., 2018, ARXIV180202627; Sporea I, 2013, NEURAL COMPUT, V25, P473, DOI 10.1162/NECO_a_00396; Tino P, 2006, NEURAL COMPUT, V18, P591, DOI 10.1162/089976606775623360; Urbanczik R, 2009, NEURAL COMPUT, V21, P340, DOI 10.1162/neco.2008.09-07-605; VanRullen R, 2005, TRENDS NEUROSCI, V28, P1, DOI 10.1016/j.tins.2004.10.010; Xin JG, 2001, IEEE IJCNN, P1772, DOI 10.1109/IJCNN.2001.938430; Zenke F., 2017, ARXIV170511146	39	49	49	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301042
C	Nam, J; Mencia, EL; Kim, HJ; Furnkranz, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nam, Jinseok; Mencia, Eneldo Loza; Kim, Hyunwoo J.; Fuernkranz, Johannes			Maximizing Subset Accuracy with Recurrent Neural Networks in Multi-label Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Multi-label classification is the task of predicting a set of labels for a given input instance. Classifier chains are a state-of-the-art method for tackling such problems, which essentially converts this problem into a sequential prediction problem, where the labels are first ordered in an arbitrary fashion, and the task is to predict a sequence of binary values for these labels. In this paper, we replace classifier chains with recurrent neural networks, a sequence-to-sequence prediction algorithm which has recently been successfully applied to sequential prediction tasks in many domains. The key advantage of this approach is that it allows to focus on the prediction of the positive labels only, a much smaller set than the full set of possible labels. Moreover, parameter sharing across all classifiers allows to better exploit information of previous decisions. As both, classifier chains and recurrent neural networks depend on a fixed ordering of the labels, which is typically not part of a multi-label problem specification, we also compare different ways of ordering the label set, and give some recommendations on suitable ordering strategies.	[Nam, Jinseok; Mencia, Eneldo Loza; Fuernkranz, Johannes] Tech Univ Darmstadt, Knowledge Engn Grp, Darmstadt, Germany; [Kim, Hyunwoo J.] Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA	Technical University of Darmstadt; University of Wisconsin System; University of Wisconsin Madison	Nam, J (corresponding author), Tech Univ Darmstadt, Knowledge Engn Grp, Darmstadt, Germany.		Jeong, Yongwook/N-7413-2016; Fürnkranz, Johannes/AAH-2585-2019	Furnkranz, Johannes/0000-0002-1207-0159; Kim, Hyunwoo/0000-0002-2181-9264	German Institute for Educational Research (DIPF) under the Knowledge Discovery in Scientific Literature (KDSL) program; German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) [GRK 1994/1]	German Institute for Educational Research (DIPF) under the Knowledge Discovery in Scientific Literature (KDSL) program; German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES)	The authors would like to thank anonymous reviewers for their thorough feedback. Computations for this research were conducted on the Lichtenberg high performance computer of the Technische Universitat Darmstadt. The Titan X used for this research was donated by the NVIDIA Corporation. This work has been supported by the German Institute for Educational Research (DIPF) under the Knowledge Discovery in Scientific Literature (KDSL) program, and the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) under grant No. GRK 1994/1.	Bahdanau D., 2015, P 3 INT C LEARN REPR; Cheng W., 2010, P 27 INT C MACH LEAR, P279; Cho K., 2014, P 2014 C EMP METH NA, P1724; Dembczynski K, 2012, FRONT ARTIF INTEL AP, V242, P294, DOI 10.3233/978-1-61499-098-7-294; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Furnkranz J, 2008, MACH LEARN, V73, P133, DOI 10.1007/s10994-008-5064-8; Gehring J, 2017, PR MACH LEARN RES, V70; Ghamrawi N., 2005, ACM INT C INFORM KNO, P195, DOI DOI 10.1145/1099554.1099591; Jasinska K, 2016, PR MACH LEARN RES, V48; Joulin A., 2017, P INT C MACH LEARN I, P1302; Kingma D.P, P 3 INT C LEARNING R; Kumar A, 2013, MACH LEARN, V92, P65, DOI 10.1007/s10994-013-5371-6; Kumar A, 2016, PR MACH LEARN RES, V48; Li C., 2016, P 33 INT C MACH LEAR, P2482; Liu WW, 2015, ADV NEUR IN, V28; Mena D, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3707; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Nam Jinseok, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P437, DOI 10.1007/978-3-662-44851-9_28; Nan Li, 2013, Multiple Classifier Systems. 11th International Workshop, MCS 2013. Proceedings, P146, DOI 10.1007/978-3-642-38067-9_13; Prabhu Y, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P263, DOI 10.1145/2623330.2623651; Read J, 2014, PATTERN RECOGN, V47, P1535, DOI 10.1016/j.patcog.2013.10.006; Read J, 2011, MACH LEARN, V85, P333, DOI 10.1007/s10994-011-5256-5; Senge R, 2014, STUD CLASS DATA ANAL, P163, DOI 10.1007/978-3-319-01595-8_18; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sucar LE, 2014, PATTERN RECOGN LETT, V41, P14, DOI 10.1016/j.patrec.2013.11.007; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tadepalli, 2014, P AAAI C ART INT; Tsoumakas G, 2011, IEEE T KNOWL DATA EN, V23, P1079, DOI 10.1109/TKDE.2010.164; Waegeman W, 2014, J MACH LEARN RES, V15, P3333; Wang J, 2016, PROC CVPR IEEE, P2285, DOI 10.1109/CVPR.2016.251	31	49	51	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405048
C	Zhao, J; Xiong, L; Jayashree, K; Li, JS; Zhao, F; Wang, ZC; Pranata, S; Shen, SM; Yan, SC; Feng, JS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Zhao, Jian; Xiong, Lin; Jayashree, Karlekar; Li, Jianshu; Zhao, Fang; Wang, Zhecan; Pranata, Sugiri; Shen, Shengmei; Yan, Shuicheng; Feng, Jiashi			Dual-Agent GANs for Photorealistic and Identity Preserving Profile Face Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Synthesizing realistic profile faces is promising for more efficiently training deep pose-invariant models for large-scale unconstrained face recognition, by populating samples with extreme poses and avoiding tedious annotations. However, learning from synthetic faces may not achieve the desired performance due to the discrepancy between distributions of the synthetic and real face images. To narrow this gap, we propose a Dual-Agent Generative Adversarial Network (DA-GAN) model, which can improve the realism of a face simulator's output using unlabeled real faces, while preserving the identity information during the realism refinement. The dual agents are specifically designed for distinguishing real v.s. fake and identities simultaneously. In particular, we employ an off-the-shelf 3D face model as a simulator to generate profile face images with varying poses. DA-GAN leverages a fully convolutional network as the generator to generate high-resolution images and an auto-encoder as the discriminator with the dual agents. Besides the novel architecture, we make several key modifications to the standard GAN to preserve pose and texture, preserve identity and stabilize training process: (i) a pose perception loss; (ii) an identity perception loss; (iii) an adversarial loss with a boundary equilibrium regularization term. Experimental results show that DA-GAN not only presents compelling perceptual results but also significantly outperforms state-of-the-arts on the large-scale and challenging NIST IJB-A unconstrained face recognition benchmark. In addition, the proposed DA-GAN is also promising as a new approach for solving generic transfer learning problems more effectively. DA-GAN is the foundation of our submissions to NIST IJB-A 2017 face recognition competitions, where we won the 1st places on the tracks of verification and identification.	[Zhao, Jian; Li, Jianshu; Zhao, Fang; Yan, Shuicheng; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore; [Zhao, Jian] Natl Univ Def Technol, Changsha, Hunan, Peoples R China; [Xiong, Lin; Jayashree, Karlekar; Pranata, Sugiri; Shen, Shengmei] Panasonic R&D Ctr Singapore, Singapore, Singapore; [Wang, Zhecan] Franklin W Olin Coll Engn, Needham, MA USA; [Yan, Shuicheng] Qihoo 360 AI Inst, Beijing, Peoples R China	National University of Singapore; National University of Defense Technology - China; Panasonic; Franklin W. Olin College of Engineering	Zhao, J (corresponding author), Natl Univ Singapore, Singapore, Singapore.; Zhao, J (corresponding author), Natl Univ Def Technol, Changsha, Hunan, Peoples R China.	zhaojian90@u.nus.edu; lin.xiong@sg.panasonic.com; karlekar.jayashree@sg.panasonic.com; jianshu@u.nus.edu; elezhf@u.nus.edu; zhecan.wang@students.olin.edu; sugiri.pranata@sg.panasonic.com; shengmei.shen@sg.panasonic.com; eleyans@u.nus.edu; elefjia@u.nus.edu	Yan, Shuicheng/HCI-1431-2022; Feng, Jiashi/AGX-6209-2022; Jeong, Yongwook/N-7413-2016		China Scholarship Council (CSC) [201503170248]; National University of Singapore [R-263-000-C08-133]; Ministry of Education of Singapore AcRF Tier One grant [R-263-000-C21-112]; NUS IDS grant [R-263-000-C67-646]	China Scholarship Council (CSC)(China Scholarship Council); National University of Singapore(National University of Singapore); Ministry of Education of Singapore AcRF Tier One grant(Ministry of Education, Singapore); NUS IDS grant	The work of Jian Zhao was partially supported by China Scholarship Council (CSC) grant 201503170248.; The work of Jiashi Feng was partially supported by National University of Singapore startup grant R-263-000-C08-133, Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant R-263-000-C67-646.	Abdalmageed W., 2016, CORR, P1, DOI DOI 10.1109/WACV.2016.7477555; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Chen J, 2015, IEEE ICC, P1801, DOI 10.1109/ICC.2015.7248586; Chen JC, 2016, IEEE WINT CONF APPL; Chowdhury Animesh R., 2016, 2016 IEEE International Conference on Plasma Science (ICOPS), DOI 10.1109/PLASMA.2016.7534285; Crosswhite N, 2016, ARXIV160303958; Gong K, 2017, PROC CVPR IEEE, P6757, DOI 10.1109/CVPR.2017.715; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hassner T., 2016, P IEEE C COMP VIS PA, P59; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang R, 2017, IEEE I CONF COMP VIS, P2458, DOI 10.1109/ICCV.2017.267; Kingma D. P., 2013, AUTO ENCODING VARIAT; KLARE BF, 2015, PROC CVPR IEEE, P1931, DOI DOI 10.1109/CVPR.2015.7298803; Li JH, 2016, P IEEE RAS-EMBS INT, P1068, DOI 10.1109/BIOROB.2016.7523773; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Masi I, 2016, PROC CVPR IEEE, P4838, DOI 10.1109/CVPR.2016.523; Masi I, 2016, LECT NOTES COMPUT SC, V9909, P579, DOI 10.1007/978-3-319-46454-1_35; Mirza M., 2014, ARXIV; Odena A., 2016, SEMISUPERVISED LEARN; Parkhi Omkar M., 2015, BRIT MACH VIS C; Ranjan R., 2016, ARXIV161100851; Ranjan R., 2017, ARXIV PREPRINT ARXIV; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Sankaranarayanan S., 2016, P IEEE INT C BIOMETR, P1; Shrivastava A, 2016, PROC CVPR IEEE, P761, DOI 10.1109/CVPR.2016.89; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang D., 2015, ARXIV150707242; Xiangyu Zhu, 2015, 2015 11th IEEE International Conference and Workshops on Automatic Face and Gesture Recognition (FG), P1, DOI 10.1109/FG.2015.7163096; Xiao S., 2016, ACM MM, P691; Xiao ST, 2016, LECT NOTES COMPUT SC, V9905, P57, DOI 10.1007/978-3-319-46448-0_4; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yang J., 2016, ARXIV160305474	34	49	49	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400007
C	d'Avella, A; Tresch, MC		Dietterich, TG; Becker, S; Ghahramani, Z		d'Avella, A; Tresch, MC			Modularity in the motor system: decomposition of muscle patterns as combinations of time-varying synergies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SPINAL-CORD	The question of whether the nervous system produces movement through the combination of a few discrete elements has long been central to the study of motor control. Muscle synergies, i.e. coordinated patterns of muscle activity, have been proposed as possible building blocks. Here we propose a model based on combinations of muscle synergies with a specific amplitude and temporal structure. Time-varying synergies provide a realistic basis for the decomposition of the complex patterns observed in natural behaviors. To extract time-varying synergies from simultaneous recording of EMG activity we developed an algorithm which extends existing non-negative matrix factorization techniques.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	d'Avella, A (corresponding author), MIT, Dept Brain & Cognit Sci, E25-526, Cambridge, MA 02139 USA.		d'Avella, Andrea/K-6856-2018; Tresch, Matthew C/B-7634-2009	d'Avella, Andrea/0000-0002-3393-4956; 				Bizzi E, 1998, Z NATURFORSCH C, V53, P510; DAVELLA A, 2000, THESIS MIT; Ghahramani Z, 1997, NATURE, V386, P392, DOI 10.1038/386392a0; Kargo WJ, 2000, J NEUROSCI, V20, P409, DOI 10.1523/JNEUROSCI.20-01-00409.2000; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; LEWICKI MS, 1999, ADV NEURAL INFORMATI, V11; Mussa-Ivaldi FA, 1999, CURR OPIN NEUROBIOL, V9, P713, DOI 10.1016/S0959-4388(99)00029-X; Saltiel P, 2001, J NEUROPHYSIOL, V85, P605, DOI 10.1152/jn.2001.85.2.605; Tresch MC, 1999, NAT NEUROSCI, V2, P162, DOI 10.1038/5721; Wasserman L, 2000, J MATH PSYCHOL, V44, P92, DOI 10.1006/jmps.1999.1278	10	49	49	0	10	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						141	148						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100018
C	Parmanto, B; Munro, PW; Doyle, HR		Touretzky, DS; Mozer, MC; Hasselmo, ME		Parmanto, B; Munro, PW; Doyle, HR			Improving committee diagnosis with resampling techniques	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV PITTSBURGH,DEPT INFORMAT SCI,PITTSBURGH,PA 15260	Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh									0	49	54	0	3	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						882	888						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00124
C	Wei, BL; Li, G; Xia, X; Fu, ZY; Jin, Z		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wei, Bolin; Li, Ge; Xia, Xin; Fu, Zhiyi; Jin, Zhi			Code Generation as a Dual Task of Code Summarization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Code summarization (CS) and code generation (CG) are two crucial tasks in the field of automatic software development. Various neural network-based approaches are proposed to solve these two tasks separately. However, there exists a specific intuitive correlation between CS and CG, which has not been exploited in previous work. In this paper, we apply the relations between two tasks to improve the performance of both tasks. In other words, exploiting the duality between the two tasks, we propose a dual training framework to train the two tasks simultaneously. In this framework, we consider the dualities on probability and attention weights, and design corresponding regularization terms to constrain the duality. We evaluate our approach on two datasets collected from GitHub, and experimental results show that our dual framework can improve the performance of CS and CG tasks over baselines.	[Wei, Bolin; Li, Ge; Fu, Zhiyi; Jin, Zhi] Peking Univ, Minist Educ, Key Lab High Confidence Software Technol, Beijing, Peoples R China; [Wei, Bolin; Li, Ge; Fu, Zhiyi; Jin, Zhi] Peking Univ, Software Inst, Beijing, Peoples R China; [Xia, Xin] Monash Univ, Fac Informat Technol, Clayton, Vic, Australia	Peking University; Peking University; Monash University	Li, G; Jin, Z (corresponding author), Peking Univ, Minist Educ, Key Lab High Confidence Software Technol, Beijing, Peoples R China.; Li, G; Jin, Z (corresponding author), Peking Univ, Software Inst, Beijing, Peoples R China.	bolin.wbl@gmail.com; lige@pku.edu.cn; xin.xia@monash.edu; ypfzy@pku.edu.cn; zhijin@pku.edu.cn	Xia, Xin/AAD-6217-2022; Jin, Zhi/E-1288-2013		National Key RD Program [2018YFB1003904]; National Natural Science Foundation of China [61832009, 61620106007, 61751210]	National Key RD Program; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank all reviewers for their constructive comments, Fang Liu for discussion on manuscript. This research is supported by the National Key R&D Program under Grant No. 2018YFB1003904, and the National Natural Science Foundation of China under Grant Nos. 61832009, 61620106007 and 61751210.	Allamanis M, 2016, PR MACH LEARN RES, V48; Bahdanau Dzmitry, 2015, ICLR; BALZER R, 1985, IEEE T SOFTWARE ENG, V11, P1257, DOI 10.1109/TSE.1985.231877; Barone Antonio Valerio Miceli, 2017, P 8 INT JOINT C NATU, V2, P314; Biswas P, 2005, I CONF VLSI DESIGN, P651; Cho K., 2014, P 2014 C EMP METH NA, P1724; Cliff Norman, 2014, ORDINAL METHODS BEHA; de Souza Sergio Cozzetti B., 2005, P 23 ANN INT C DESIG, P68, DOI 10.1145/1085313.1085331; Dong L., 2016, ACL; Eriguchi A., 2016, TREE TO SEQUENCE ATT; Fuglede B, 2004, 2004 IEEE INTERNATIONAL SYMPOSIUM ON INFORMATION THEORY, PROCEEDINGS, P31; He Di, 2016, NEURAL INFORM PROCES, P2; Hu X, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2269; Hu X, 2018, INT C PROGRAM COMPRE, P200, DOI 10.1145/3196321.3196334; Iyer Srinivasan, 2016, ACL; Kingma D.P., 2015, INT C LEARN REPR, P1; LI YK, 2018, CVPR, P6116, DOI DOI 10.1109/CVPR.2018.00640; Lin Chin-Yew, 2004, P ANN M ASS COMP LIN; Ling W., 2016, ACL; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Rabinovich M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1139, DOI 10.18653/v1/P17-1105; Sun Zeyu, 2018, ARXIV181106837; Tang Duyu, 2017, ARXIV170602027; Wan Y, 2018, IEEE INT CONF AUTOM, P397, DOI 10.1145/3238147.3238206; WILCOXON F, 1950, ANN NY ACAD SCI, V52, P808, DOI 10.1111/j.1749-6632.1950.tb53974.x; Xia YC, 2017, PR MACH LEARN RES, V70; Xiao H., 2018, ARXIV180901997; Yin PC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P440, DOI 10.18653/v1/P17-1041	28	48	51	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306055
C	Mardani, M; Sun, QY; Vasawanala, S; Papyan, V; Monajemi, H; Pauly, J; Donoho, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mardani, Morteza; Sun, Qingyun; Vasawanala, Shreyas; Papyan, Vardan; Monajemi, Hatef; Pauly, John; Donoho, David			Neural Proximal Gradient Descent for Compressive Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SPARSE	Recovering high-resolution images from limited sensory data typically leads to a serious ill-posed inverse problem, demanding inversion algorithms that effectively capture the prior information. Learning a good inverse mapping from training data faces severe challenges, including: (i) scarcity of training data; (ii) need for plausible reconstructions that are physically feasible; (iii) need for fast reconstruction, especially in real-time applications. We develop a successful system solving all these challenges, using as basic architecture the recurrent application of proximal gradient algorithm. We learn a proximal map that works well with real images based on residual networks. Contraction of the resulting map is analyzed, and incoherence conditions are investigated that drive the convergence of the iterates. Extensive experiments are carried out under different settings: (a) reconstructing abdominal MRI of pediatric patients from highly undersampled Fourier-space data and (b) superresolving natural face images. Our key findings include: 1. a recurrent ResNet with a single residual block unrolled from an iterative algorithm yields an effective proximal which accurately reveals MR image details. 2. Our architecture significantly outperforms conventional non-recurrent deep ResNets by 2dB SNR; it is also trained much more rapidly. 3. It outperforms state-of-the-art compressed-sensing Wavelet-based methods by 4dB SNR, with 100x speedups in reconstruction time.	[Mardani, Morteza; Pauly, John] Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA; [Vasawanala, Shreyas] Stanford Univ, Dept Radiol, Stanford, CA 94305 USA; [Papyan, Vardan; Monajemi, Hatef; Donoho, David] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Sun, Qingyun] Stanford Univ, Dept Math, Stanford, CA 94305 USA	Stanford University; Stanford University; Stanford University; Stanford University	Mardani, M (corresponding author), Stanford Univ, Dept Elect Engn, Stanford, CA 94305 USA.	morteza@stanford.edu; qysun@stanford.edu; vasanawala@stanford.edu; papyan@stanford.edu; monajemi@stanford.edu; pauly@stanford.edu; donoho@stanford.edu	Vasanawala, Shreyas/AAW-4385-2020	Pauly, John/0000-0001-5918-4172				Adler J., 2017, ARXIV170706474; Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bo Zhu, 2017, P 25 ANN M ISMRM HON; Bora A, 2017, PR MACH LEARN RES, V70; Bruna J., 2015, ARXIV151105666; Chen H, 2017, IEEE T MED IMAGING, V36, P2524, DOI 10.1109/TMI.2017.2715284; Diamond S, 2017, ARXIV170508041; Do M., 2016, ARXIV PREPRINT ARXIV; Donoho DL, 2006, IEEE T INFORM THEORY, V52, P1289, DOI 10.1109/TIT.2006.871582; Duarte-Carvajalino JM, 2009, IEEE T IMAGE PROCESS, V18, P1395, DOI 10.1109/TIP.2009.2022459; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greff K, 2016, ARXIV161207771; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Hand P., 2017, ARXIV170507576; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim J, 2016, IEEE CONF COMPUT; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Lee D., 2017, P 25 ANN M ISMRM HON; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Lustig M, 2007, MAGN RESON MED, V58, P1182, DOI 10.1002/mrm.21391; Majumdar A., 2015, ARXIV PREPRINT ARXIV; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Metzler CA, 2017, ADV NEUR IN, V30; Parikh N., 2014, FDN TRENDS OPTIM, V1, P127, DOI DOI 10.1561/2400000003; Ramachandran P., 2018, 6 INT C LEARNING REP; Romano Y, 2017, IEEE T COMPUT IMAG, V3, P110, DOI 10.1109/TCI.2016.2629284; Schlemper J., 2017, P 25 ANN M ISMRM HON; Sonderby C. K., 2016, ARXIV PREPRINT ARXIV; Sprechmann P, 2015, IEEE T PATTERN ANAL, V37, P1821, DOI 10.1109/TPAMI.2015.2392779; Tamir JI., 2016, ISMRM WORKSH DAT SAM; Wang S., 2017, P 25 ANN M ISMRM HON; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xin B, 2016, ADV NEUR IN, V29; Yang Y, 2016, ADV NEUR IN, V29; Zagoruyko S, 2017, ARXIV170600388; Zhao H, 2017, IEEE T COMPUT IMAG, V3, P47, DOI 10.1109/TCI.2016.2644865	37	48	48	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004016
C	Hu, YT; Huang, JB; Schwing, AG		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hu, Yuan-Ting; Huang, Jia-Bin; Schwing, Alexander G.			MaskRNN: Instance Level Video Object Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Instance level video object segmentation is an important technique for video editing and compression. To capture the temporal coherence, in this paper, we develop MaskRNN, a recurrent neural net approach which fuses in each frame the output of two deep nets for each object instance - a binary segmentation net providing a mask and a localization net providing a bounding box. Due to the recurrent component and the localization component, our method is able to take advantage of long-term temporal structures of the video data as well as rejecting outliers. We validate the proposed algorithm on three challenging benchmark datasets, the DAVIS-2016 dataset, the DAVIS-2017 dataset, and the Segtrack v2 dataset, achieving state-of-the-art performance on all of them.	[Hu, Yuan-Ting; Schwing, Alexander G.] UIUC, Champaign, IL 61820 USA; [Huang, Jia-Bin] Virginia Tech, Blacksburg, VA USA	University of Illinois System; University of Illinois Urbana-Champaign; Virginia Polytechnic Institute & State University	Hu, YT (corresponding author), UIUC, Champaign, IL 61820 USA.	ythu2@illinois.edu; jbhuang@vt.edu; aschwing@illinois.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [1718221]; NVIDIA	National Science Foundation(National Science Foundation (NSF)); NVIDIA	This material is based upon work supported in part by the National Science Foundation under Grant No. 1718221. We thank NVIDIA for providing the GPUs used in this research.	[Anonymous], 2015, P CVPR; [Anonymous], 2015, 3 INT C LEARNING REP; Avinash Ramakanth S., 2014, P CVPR; Badrinarayanan V., 2010, P CVPR; Belongie S., 2002, TPAMI; Brendel W., 2009, P ICCV; Brutzer S., 2011, P CVPR; Caelles S., 2017, ARXIV170401926; Caelles S., 2017, P CVPR; Chang J., 2013, P CVPR; Cheng H.T., 2012, P CVPR; Criminisi A., 2006, P CVPR; ELGAMMAL A, 2002, P IEEE; Faktor A., 2014, BMVC; Fan Q., 2015, ACM TOG P SIGGRAPH; Galasso F., 2013, P ICCV; Girshick R., 2015, P CVPR; Grundmann M., 2010, P CVPR; Haymanand E., 2003, P ICCV; He K., 2017, P ICCV; Ilg E., 2017, P CVPR; Irani M., 1994, IJCV; Irani M., 1998, PAMI; Jain S. D., 2014, P ECCV; Jain S.D., 2017, P CVPR; Jampani V., 2017, P CVPR; Khoreva A, 2017, ARXIV170309554; Khoreva A., 2017, P CVPR; Kingma D., 2014, P ICLR; Lee Y. J., 2011, P ICCV; Lezama J., 2011, P CVPR; Li F., 2013, P ICCV; Li W., 2016, ACM TOG P SIGGRAPH; Maerki N., 2016, P CVPR; Nagaraja N., 2015, P ICCV; Ochs P., 2014, TPAMI; Papazoglou A., 2013, P ICCV; Perazzi F., 2015, P ICCV; Perazzi Federico, 2016, P CVPR; Pont-Tuset J., 2017, ARXIV170400675; Price B.L., 2009, P ICCV; Ren Y., 2003, PRL; Schwing A. G., 2015, FULLY CONNECTED DEEP; Torr P. H. S., 1998, P ECCV; Tsai D., 2010, P BMVC; Tsai Y.H., 2016, P CVPR; Vijayanarasimhan S., 2012, P ECCV; Xiao F., 2016, P CVPR; Yuan C., 2007, PAMI; Zhang D., 2013, P CVPR	50	48	48	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400031
C	Gatys, LA; Ecker, AS; Bethge, M		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias			Texture Synthesis Using Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				IMAGE; STATISTICS	Here we introduce a new model of natural textures based on the feature spaces of convolutional neural networks optimised for object recognition. Samples from the model are of high perceptual quality demonstrating the generative power of neural networks trained in a purely discriminative fashion. Within the model, textures are represented by the correlations between feature maps in several layers of the network. We show that across layers the texture representations increasingly capture the statistical properties of natural images while making object information more and more explicit. The model provides a new tool to generate stimuli for neuroscience and might offer insights into the deep representations learned by convolutional neural networks.	[Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias] Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany; [Gatys, Leon A.; Ecker, Alexander S.; Bethge, Matthias] Bernstein Ctr Computat Neurosci, Tubingen, Germany; [Gatys, Leon A.] Univ Tubingen, Grad Sch Neural Informat Proc, Tubingen, Germany; [Ecker, Alexander S.; Bethge, Matthias] Max Planck Inst Biol Cybernet, Tubingen, Germany; [Ecker, Alexander S.] Baylor Coll Med, Houston, TX 77030 USA	Eberhard Karls University of Tubingen; Eberhard Karls University of Tubingen; Max Planck Society; Baylor College of Medicine	Gatys, LA (corresponding author), Univ Tubingen, Ctr Integrat Neurosci, Tubingen, Germany.	leon.gatys@bethgelab.org	Ecker, Alexander S/A-5184-2010	Ecker, Alexander S/0000-0003-2392-5105	German National Academic Foundation; Bernstein Center for Computational Neuroscience [FKZ 01GQ1002]; German Excellency Initiative through the Centre for Integrative Neuroscience Tubingen [EXC307]	German National Academic Foundation; Bernstein Center for Computational Neuroscience; German Excellency Initiative through the Centre for Integrative Neuroscience Tubingen	This work was funded by the German National Academic Foundation (L.A.G.), the Bernstein Center for Computational Neuroscience (FKZ 01GQ1002) and the German Excellency Initiative through the Centre for Integrative Neuroscience Tubingen (EXC307)(M.B., A.S.E, L.A.G.)	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Balas B, 2009, J VISION, V9, DOI [10.1167/9.2.16, 10.1167/9.12.13]; Cadieu CF, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003963; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cimpoi M., 2014, ARXIV14116836CS; Dentinel Zarembaw, 2014, NEURIPS, P1269; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; Freeman J, 2013, NAT NEUROSCI, V16, P974, DOI 10.1038/nn.3402; Freeman J, 2011, NAT NEUROSCI, V14, P1195, DOI 10.1038/nn.2889; He K., 2014, ARXIV14064729; Heeger DJ, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC648; Jaderberg Max, 2014, P BRIT MACH VIS C, P2, DOI DOI 10.5244/C.28.88; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Julesz B., 1962, IRE T INFORM THEORY, V8; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kwatra V, 2003, ACM T GRAPHIC, V22, P277, DOI 10.1145/882262.882264; Lebedev V., 2015, 3 INT C LEARNING REP; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Movshon A. J., 2015, COLD SPRING HARB S Q; Okazawa G, 2015, P NATL ACAD SCI USA, V112, pE351, DOI 10.1073/pnas.1415146112; Portilla J, 2000, INT J COMPUT VISION, V40, P49, DOI 10.1023/A:1026553619983; Rosenholtz R, 2012, J VISION, V12, DOI 10.1167/12.4.14; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Simoncelli EP, 1995, INTERNATIONAL CONFERENCE ON IMAGE PROCESSING - PROCEEDINGS, VOLS I-III, pC444; Wei Li-Yi, 2009, EUROGRAPHICS 2009 ST; Wei LY, 2000, COMP GRAPH, P479, DOI 10.1145/344779.345009; Yamins D. L. K., 2014, P NATL ACAD SCI USA; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	30	48	48	3	21	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100001
C	Gong, BQ; Chao, WL; Grauman, K; Sha, F		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gong, Boqing; Chao, Wei-Lun; Grauman, Kristen; Sha, Fei			Diverse Sequential Subset Selection for Supervised Video Summarization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Video summarization is a challenging problem with great application potential. Whereas prior approaches, largely unsupervised in nature, focus on sampling useful frames and assembling them as summaries, we consider video summarization as a supervised subset selection problem. Our idea is to teach the system to learn from human-created summaries how to select informative and diverse subsets, so as to best meet evaluation metrics derived from human-perceived quality. To this end, we propose the sequential determinantal point process (seqDPP), a probabilistic model for diverse sequential subset selection. Our novel seqDPP heeds the inherent sequential structures in video data, thus overcoming the deficiency of the standard DPP, which treats video frames as randomly permutable items. Meanwhile, seqDPP retains the power of modeling diverse subsets, essential for summarization. Our extensive results of summarizing videos from 3 datasets demonstrate the superior performance of our method, compared to not only existing unsupervised methods but also naive applications of the standard DPP model.	[Gong, Boqing; Chao, Wei-Lun; Sha, Fei] Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA; [Grauman, Kristen] Univ Texas Austin, Dept Comp Sci, Austin, TX 78701 USA	University of Southern California; University of Texas System; University of Texas Austin	Gong, BQ (corresponding author), Univ Southern Calif, Dept Comp Sci, Los Angeles, CA 90089 USA.	boqinggo@usc.edu; weilunc@usc.edu; grauman@cs.utexas.edu; feisha@usc.edu			DARPA [D11-AP00278]; NSF [IIS-1065243]; ARO [W911NF-12-1-0241]; ONR YIP Award [N00014-12-1-0754]; USC Viterbi Doctoral Fellowship; USC Annenberg Graduate Fellowship	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); ARO; ONR YIP Award; USC Viterbi Doctoral Fellowship; USC Annenberg Graduate Fellowship	B. G., W. C. and F. S. are partially supported by DARPA D11-AP00278, NSF IIS-1065243, and ARO #W911NF-12-1-0241. K. G. is supported by ONR YIP Award N00014-12-1-0754 and gifts from Intel and Google. B. G. and W. C. also acknowledge supports from USC Viterbi Doctoral Fellowship and USC Annenberg Graduate Fellowship. We are grateful to Jiebo Luo for providing the Kodak dataset [32].	Affandi R. H., 2012, UAI; Dang H. T, 2005, DOCUMENT UNDERSTANDI, P1; Dumont Emilie, 2009, ICME; Ellouze M, 2010, J VIS COMMUN IMAGE R, V21, P283, DOI 10.1016/j.jvcir.2010.01.007; Feng S., 2012, CVPR; de Avila SEF, 2011, PATTERN RECOGN LETT, V32, P56, DOI 10.1016/j.patrec.2010.08.004; Furini M, 2010, MULTIMED TOOLS APPL, V46, P47, DOI 10.1007/s11042-009-0307-7; Gillenwater J., 2012, NIPS; Gillenwater Jennifer, 2012, EMNLP CNLL; Goldman D., 2006, SIGGRAPH; Hong R., 2009, ACM SIGMM WORKSH SOC; Kang H. W., 2006, CVPR; Khosla A., 2013, CVPR; Kulesza A., 2011, ICML; Kulesza A., 2011, NIPS; Kulesza Alex, 2011, UAI 11; Lee Y. J., 2012, CVPR; Li Yingbo, 2010, ACM MM; Lin H., 2010, NAACL HLT; Liu D, 2010, IEEE T PATTERN ANAL, V32, P2178, DOI 10.1109/TPAMI.2010.31; Liu T., 2002, ECCV; Lowe DG, 2004, INT J COMPUT VISION, V60, P91, DOI 10.1023/B:VISI.0000029664.99615.94; Lu Z., 2013, CVPR; Luo JB, 2009, IEEE T CIRC SYST VID, V19, P289, DOI 10.1109/TCSVT.2008.2009241; Ma Y.F., 2002, ACM MM; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mundur P, 2006, INT J DIGIT LIBRARIE, V6, P219, DOI 10.1007/s00799-005-0129-9; Ngo C. W., 2003, ICCV; Perronnin F., 2007, CVPR; Pritch Yael, 2007, ICCV; Rahtu E., 2010, ECCV; Valdes V, 2012, ACM T MULTIM COMPUT, V8, DOI 10.1145/2240136.2240138; Zhang HJ, 1997, PATTERN RECOGN, V30, P643, DOI 10.1016/S0031-3203(96)00109-4	34	48	48	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101084
C	Dornhege, G; Blankertz, B; Curi, G; Muller, KR		Thrun, S; Saul, K; Scholkopf, B		Dornhege, G; Blankertz, B; Curi, G; Muller, KR			Increase information transfer rates in BCI by CSP extension to multi-class	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				SINGLE-TRIAL EEG; CLASSIFICATION; COMMUNICATION	Brain-Computer Interfaces (BCI) are an interesting emerging technology that is driven by the motivation to develop an effective communication interface translating human intentions into a control signal for devices like computers or neuroprostheses. If this can be done bypassing the usual human output pathways like peripheral nerves and muscles it can ultimately become a valuable tool for paralyzed patients. Most activity in BCI research is devoted to finding suitable features and algorithms to increase information transfer rates (ITRs). The present paper studies the implications of using more classes, e.g., left vs. right hand vs. foot, for operating a BCI. We contribute by (1) a theoretical study showing under some mild assumptions that it is practically not useful to employ more than three or four classes, (2) two extensions of the common spatial pattern (CSP) algorithm, one interestingly based on simultaneous diagonalization, and (3) controlled EEG experiments that underline our theoretical findings and show excellent improved ITRs.	Fraunhofer FIRSTIDA, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Dornhege, G (corresponding author), Fraunhofer FIRSTIDA, Kekulestr 7, D-12489 Berlin, Germany.	dornhege@first.fraunhofer.de; blanker@first.fraunhofer.de; curio@zedat.fu-berlin.de; klaus@first.fraunhofer.de	Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685; Blankertz, Benjamin/0000-0002-2437-4846; Curio, Gabriel/0000-0002-3377-7735				Allwein E. L., 2000, J MACHINE LEARNING R, V1, P113, DOI DOI 10.1162/15324430152733133; Birbaumer N, 1999, NATURE, V398, P297, DOI 10.1038/18581; Blanco A, 2002, TAPPI J, V1, P14; Blankertz B, 2003, IEEE T NEUR SYS REH, V11, P127, DOI 10.1109/TNSRE.2003.814456; Cardoso JF, 1996, SIAM J MATRIX ANAL A, V17, P161, DOI 10.1137/S0895479893259546; Dornhege G, 2003, I IEEE EMBS C NEUR E, P595, DOI 10.1109/CNE.2003.1196898; DORNHEGE G, 2003, ADV NEURAL INF P SYS, V15; Muller-Gerking J, 1999, CLIN NEUROPHYSIOL, V110, P787, DOI 10.1016/S1388-2457(98)00038-8; Obermaier B, 2001, IEEE T NEUR SYS REH, V9, P283, DOI 10.1109/7333.948456; PARRA L, 2002, IN PRESS NEUROIMAGE; Penny WD, 2000, IEEE T REHABIL ENG, V8, P214, DOI 10.1109/86.847820; Peters BO, 2001, IEEE T BIO-MED ENG, V48, P111, DOI 10.1109/10.900270; Pham DT, 2001, SIAM J MATRIX ANAL A, V22, P1136, DOI 10.1137/S089547980035689X; Ramoser H, 2000, IEEE T REHABIL ENG, V8, P441, DOI 10.1109/86.895946; TREJO L, 2003, IN PRESS IEEE T NEUR; Wolpaw JR, 2000, IEEE T REHABIL ENG, V8, P164, DOI 10.1109/TRE.2000.847807; Wolpaw JR, 2000, IEEE T REHABIL ENG, V8, P222, DOI 10.1109/86.847823; Wolpaw JR, 2002, CLIN NEUROPHYSIOL, V113, P767, DOI 10.1016/S1388-2457(02)00057-3; ZIEHE A, 2003, P INT C IND COMP AN, P469	19	48	49	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						733	740						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500092
C	Chen, XH; Liu, JL; Wang, ZY; Yin, WT		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Xiaohan; Liu, Jialin; Wang, Zhangyang; Yin, Wotao			Theoretical Linear Convergence of Unfolded ISTA and its Practical Weights and Thresholds	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In recent years, unfolding iterative algorithms as neural networks has become an empirical success in solving sparse recovery problems. However, its theoretical understanding is still immature, which prevents us from fully utilizing the power of neural networks. In this work, we study unfolded ISTA (Iterative Shrinkage Thresholding Algorithm) for sparse signal recovery. We introduce a weight structure that is necessary for asymptotic convergence to the true sparse signal. With this structure, unfolded ISTA can attain a linear convergence, which is better than the sublinear convergence of ISTA/FISTA in general cases. Furthermore, we propose to incorporate thresholding in the network to perform support selection, which is easy to implement and able to boost the convergence rate both theoretically and empirically. Extensive simulations, including sparse vector recovery and a compressive sensing experiment on real image data, corroborate our theoretical results and demonstrate their practical usefulness. We have made our codes publicly available.(2).	[Chen, Xiaohan; Wang, Zhangyang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA; [Liu, Jialin; Yin, Wotao] Univ Calif Los Angeles, Dept Math, Los Angeles, CA 90095 USA	Texas A&M University System; Texas A&M University College Station; University of California System; University of California Los Angeles	Chen, XH (corresponding author), Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA.	chernxh@tamu.edu; liujl11@math.ucla.edu; atlaswang@tamu.edu; wotaoyin@math.ucla.edu	Liu, Jialin/AAB-3005-2019	Wang, Zhangyang/0000-0002-2050-5693	NSF [RI-1755701, DMS-1720237]; ONR [N0001417121]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research)	The work by X. Chen and Z. Wang is supported in part by NSF RI-1755701. The work by J. Liu and W. Yin is supported in part by NSF DMS-1720237 and ONR N0001417121. We would also like to thank all anonymous reviewers for their tremendously useful comments to help improve our work.	Beck A, 2009, SIAM J IMAGING SCI, V2, P183, DOI 10.1137/080716542; Bertsimas D., 1997, INTRO LINEAR OPTIMIZ; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Blumensath T, 2008, J FOURIER ANAL APPL, V14, P629, DOI 10.1007/s00041-008-9035-z; Borgerding Mark, 2016 IEEE GLOB C SIG; Borgerding Mark, 2017, IEEE T SIGNAL PROCES; Bredies K, 2008, J FOURIER ANAL APPL, V14, P813, DOI 10.1007/s00041-008-9041-1; Giryes Raja, 2018, IEEE T SIGNAL PROCES; Gregor K., 2010, P 27 INT C INT C MAC, P399, DOI DOI 10.5555/3104322.3104374; Hale E., 2008, SIAM J OPTIMIZ, V19, P1; Jian Zhang, 2018, IEEE CVPR; Kulkarni Kuldeep, 2016, P IEEE C COMP VIS PA; Li CB, 2013, COMPUT OPTIM APPL, V56, P507, DOI 10.1007/s10589-013-9576-1; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Metzler CA, 2017, ADV NEUR IN, V30; Moreau Thomas, 2017, ICLR; Osher Stanley, 2010, COMMUNICATIONS MATH; Sprechmann Pablo, 2015, IEEE T PATTERN ANAL; Tao SZ, 2016, SIAM J OPTIMIZ, V26, P313, DOI 10.1137/151004549; Wang ZY, 2016, AAAI CONF ARTIF INTE, P2194; Wang ZY, 2016, PROC CVPR IEEE, P2764, DOI 10.1109/CVPR.2016.302; Wang Zhangyang, 2016, LEARNING DEEP ENCODE, P2174; Wang Zhaowen, SPARSE CODING ITS AP; Wang Z, 2016, INT CONF SOFTW ENG, P369, DOI 10.1109/ICSESS.2016.7883088; Xiao L, 2013, SIAM J OPTIMIZ, V23, P1062, DOI 10.1137/120869997; Xin B, 2016, ADV NEUR IN, V29; Xu YY, 2013, SIAM J IMAGING SCI, V6, P1758, DOI 10.1137/120887795; Zhang LF, 2017, OPTIMIZATION, V66, P1177, DOI 10.1080/02331934.2017.1318133; Zhou JT, 2018, AAAI CONF ARTIF INTE, P4588	29	47	48	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003060
C	Nie, FP; Wang, XQ; Deng, C; Huang, H		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Nie, Feiping; Wang, Xiaoqian; Deng, Cheng; Huang, Heng			Learning A Structured Optimal Bipartite Graph for Co-Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Co-clustering methods have been widely applied to document clustering and gene expression analysis. These methods make use of the duality between features and samples such that the co-occurring structure of sample and feature clusters can be extracted. In graph based co-clustering methods, a bipartite graph is constructed to depict the relation between features and samples. Most existing co-clustering methods conduct clustering on the graph achieved from the original data matrix, which doesn't have explicit cluster structure, thus they require a post-processing step to obtain the clustering results. In this paper, we propose a novel co-clustering method to learn a bipartite graph with exactly k connected components, where k is the number of clusters. The new bipartite graph learned in our model approximates the original graph but maintains an explicit cluster structure, from which we can immediately get the clustering results without post-processing. Extensive empirical results are presented to verify the effectiveness and robustness of our model.	[Nie, Feiping] Northwestern Polytech Univ, Ctr OPTIMAL, Sch Comp Sci, Xian, Shaanxi, Peoples R China; [Wang, Xiaoqian; Huang, Heng] Univ Pittsburgh, Dept Elect & Comp Engn, Pittsburgh, PA 15260 USA; [Deng, Cheng] Xidian Univ, Sch Elect Engn, Xian, Shaanxi, Peoples R China	Northwestern Polytechnical University; Pennsylvania Commonwealth System of Higher Education (PCSHE); University of Pittsburgh; Xidian University	Nie, FP (corresponding author), Northwestern Polytech Univ, Ctr OPTIMAL, Sch Comp Sci, Xian, Shaanxi, Peoples R China.	feipingnie@gmail.com; xqwang1991@gmail.com; chdeng@mail.xidian.edu; heng.huang@pitt.edu	Nie, Feiping/B-3039-2012; Jeong, Yongwook/N-7413-2016	Nie, Feiping/0000-0002-0871-6519; Deng, Cheng/0000-0003-2620-3247	U.S. NSF [IIS 1302675, IIS 1344152, IIS 1619308, IIS 1633753]; NSF [DBI 1356628]; NIH [AG049371]	U.S. NSF(National Science Foundation (NSF)); NSF(National Science Foundation (NSF)); NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	This work was partially supported by U.S. NSF-IIS 1302675, NSF-IIS 1344152, NSF-DBI 1356628, NSF-IIS 1619308, NSF-IIS 1633753, NIH AG049371.	Bhattacharjee A, 2001, P NATL ACAD SCI USA, V98, P13790, DOI 10.1073/pnas.191502998; Chung F.R.K., 1997, CBMS REGIONAL C SERI, V92; Dhillon I. S., 2001, KDD-2001. Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, P269, DOI 10.1145/502512.502550; Ding Chris, 2006, P 12 ACM SIGKDD INT, V2006, P126; Fan K., 1949, THEOREM WEYL EIGEN V, V35, P652; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Gong MG, 2013, IEEE T IMAGE PROCESS, V22, P573, DOI 10.1109/TIP.2012.2219547; Huang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3569; Liao Zhongyue, 2004, BMC Urol, V4, P8, DOI 10.1186/1471-2490-4-8; Mohar B., 1991, GRAPH THEORY COMBINA, V2, P12; Nie FP, 2016, AAAI CONF ARTIF INTE, P1969; Nie FP, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P977, DOI 10.1145/2623330.2623726; Nutzmann HW, 2014, CURR OPIN BIOTECH, V26, P91, DOI 10.1016/j.copbio.2013.10.009; Petricoin EF, 2002, J NATL CANCER I, V94, P1576; Piano F, 2002, CURR BIOL, V12, P1959, DOI 10.1016/S0960-9822(02)01301-5; Potok T. E., 2005, J COMPUT SCI-NETH, V27, P33; Shahnaz F, 2006, INFORM PROCESS MANAG, V42, P373, DOI 10.1016/j.ipm.2004.11.005; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Zelnik-Manor L., 2004, NIPS	19	47	47	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404020
C	Xia, YC; Tian, F; Wu, LJ; Lin, JX; Qin, T; Yu, NH; Liu, TY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xia, Yingce; Tian, Fei; Wu, Lijun; Lin, Jianxin; Qin, Tao; Yu, Nenghai; Liu, Tie-Yan			Deliberation Networks: Sequence Generation Beyond One-Pass Decoding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The encoder-decoder framework has achieved promising progress for many sequence generation tasks, including machine translation, text summarization, dialog system, image captioning, etc. Such a framework adopts an one-pass forward process while decoding and generating a sequence, but lacks the deliberation process: A generated sequence is directly used as final output without further polishing. However, deliberation is a common behavior in human's daily life like reading news and writing papers/articles/books. In this work, we introduce the deliberation process into the encoder-decoder framework and propose deliberation networks for sequence generation. A deliberation network has two levels of decoders, where the first-pass decoder generates a raw sequence and the second-pass decoder polishes and refines the raw sentence with deliberation. Since the second-pass deliberation decoder has global information about what the sequence to be generated might be, it has the potential to generate a better sequence by looking into future words in the raw sentence. Experiments on neural machine translation and text summarization demonstrate the effectiveness of the proposed deliberation networks. On the WMT 2014 English-to-French translation task, our model establishes a new state-of-the-art BLEU score of 41.5.	[Xia, Yingce; Lin, Jianxin; Yu, Nenghai] Univ Sci & Technol China, Hefei, Anhui, Peoples R China; [Tian, Fei; Qin, Tao; Liu, Tie-Yan] Microsoft Res, Beijing, Peoples R China; [Wu, Lijun] Sun Yat Sen Univ, Guangzhou, Guangdong, Peoples R China	Chinese Academy of Sciences; University of Science & Technology of China, CAS; Microsoft; Sun Yat Sen University	Xia, YC (corresponding author), Univ Sci & Technol China, Hefei, Anhui, Peoples R China.	yingce.xia@gmail.com; fetia@microsoft.com; wulijun3@mail2.sysu.edu.cn; linjx@mail.ustc.edu.cn; taoqin@microsoft.com; ynh@ustc.edu.cn; tie-yan.liu@microsoft.com			National Natural Science Foundation of China [61371192]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	The authors would like to thank Yang Fan and Kaitao Song for implementing the deep neural machine translation basic model. This work is partially supported by the National Natural Science Foundation of China (Grant No. 61371192).	Bengio Y., 2014, ARXIV14061078; Chatterjee R., 2016, P 1 C MACH TRANSL SH, V2; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Gehring J., 2017, P ICML; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graff D., 2003, LINGUISTIC DATA CONS, V4, P34; He D., 2017, 31 ANN C NEUR INF PR; He Di, 2016, NEURAL INFORM PROCES, P2; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jean S, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1; Kingma D.P, P 3 INT C LEARNING R; Li Jiwei, 2016, ARXIV161108562; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Niehues J., 2016, COLING; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato MarcAurelio, 2015, ARXIV151106732; Rush Alexander M, 2015, P 2015 C EMP METH NA, P379, DOI DOI 10.18653/V1/D15-1044; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; SHEN S, 2016, P ACL; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; T. D. Team, 2016, ARXIV160502688; Tu ZP, 2017, AAAI CONF ARTIF INTE, P3097; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wiseman S., 2016, ACL; Wu LJ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3098; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xia Y., 2017, EUR C MACH LEARN PRI; Xia YC, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3112; Xia YC, 2017, PR MACH LEARN RES, V70; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z., 2016, ADV NEURAL INF PROCE, V29; Zeiler Matthew D, 2012, ARXIV12125701	36	47	51	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401079
C	Zhang, SZ; Wu, YH; Che, T; Lin, ZH; Memisevic, R; Salakhutdinov, R; Bengio, Y		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Zhang, Saizheng; Wu, Yuhuai; Che, Tong; Lin, Zhouhan; Memisevic, Roland; Salakhutdinov, Ruslan; Bengio, Yoshua			Architectural Complexity Measures of Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				LONG-TERM DEPENDENCIES	In this paper, we systematically analyze the connecting architectures of recurrent neural networks (RNNs). Our main contribution is twofold: first, we present a rigorous graph-theoretic framework describing the connecting architectures of RNNs in general. Second, we propose three architecture complexity measures of RNNs: (a) the recurrent depth, which captures the RNN's over-time nonlinear complexity, (b) the feedforward depth, which captures the local input-output nonlinearity (similar to the "depth" in feedforward neural networks (FNNs)), and (c) the recurrent skip coefficient which captures how rapidly the information propagates over time. We rigorously prove each measure's existence and computability. Our experimental results show that RNNs might benefit from larger recurrent depth and feedforward depth. We further demonstrate that increasing recurrent skip coefficient offers performance boosts on long term dependency problems.	[Zhang, Saizheng; Lin, Zhouhan; Memisevic, Roland; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada; [Wu, Yuhuai] Univ Toronto, Toronto, ON, Canada; [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Che, Tong] Inst Hautes Etud Sci, Bures Sur Yvette, France; [Memisevic, Roland; Salakhutdinov, Ruslan; Bengio, Yoshua] CIFAR, Toronto, ON, Canada	Universite de Montreal; University of Toronto; Carnegie Mellon University; UDICE-French Research Universities; Universite Paris Saclay; Canadian Institute for Advanced Research (CIFAR)	Zhang, SZ (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.				NSERC; Canada Research Chairs; CIFAR; Calcul Quebec; Compute Canada; Samsung; ONR Grant [N000141310721, N000141512791]; IARPA Raytheon BBN Contract [D11PC20071]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada Research Chairs(Canada Research ChairsCGIAR); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Calcul Quebec; Compute Canada; Samsung(Samsung); ONR Grant; IARPA Raytheon BBN Contract	The authors acknowledge the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Samsung, ONR Grant N000141310721, ONR Grant N000141512791 and IARPA Raytheon BBN Contract No. D11PC20071. The authors thank the developers of Theano [28] and Keras [29], and also thank Nicolas Ballas, Tim Cooijmans, Ryan Lowe, Mohammad Pezeshki, Roger Grosse and Alex Schwing for their insightful comments.	Al-Rfou R, 2016, THEANO PYTHON FRAMEW; Arjovsky M., 2015, ARXIV151106464; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bengio Y., 2014, ARXIV14061078; Chollet F., 2015, KERAS; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; ElHihi S, 1996, ADV NEUR IN, V8, P493; Graves A, 2013, ARXIV13080850; Greff K., 2015, ARXIV150304069, DOI 10.1109/TNNLS.2016.2582924; Hermans M., 2013, P ADV NEUR INF PROC, V26, P190; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Jozefowicz R, 2015, PR MACH LEARN RES, V37, P2342; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Koutnik J, 2014, PR MACH LEARN RES, V32, P1863; Krueger David, 2015, ARXIV PREPRINT ARXIV; Le Q.V., 2015, ABS150400941 CORR; Lin TN, 1996, IEEE T NEURAL NETWOR, V7, P1329, DOI 10.1109/72.548162; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Pascanu R., 2013, ARXIV13126026; Pascanu R., 2013, P 30 INT C INT C MAC, P1310; Raiko T, 2012, P INT C ART INT STAT, V22, P924; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Sutskever I., 2011, P 28 INT C MACH LEAR, P1033, DOI DOI 10.1145/346152.346166; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Sutskever I, 2010, NEURAL NETWORKS, V23, P239, DOI 10.1016/j.neunet.2009.10.009	29	47	47	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702050
C	Montufar, G; Pascanu, R; Cho, K; Bengio, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Montufar, Guido; Pascanu, Razvan; Cho, Kyunghyun; Bengio, Yoshua			On the Number of Linear Regions of Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA			Deep learning; neural network; input space partition; rectifier; maxout		We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers.	[Montufar, Guido] Max Planck Inst Math Sci, Leipzig, Germany; [Pascanu, Razvan; Cho, Kyunghyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada	Max Planck Society; Universite de Montreal	Montufar, G (corresponding author), Max Planck Inst Math Sci, Leipzig, Germany.	montufar@mis.mpg.de; pascanur@iro.umontreal.ca; kyunghyun.cho@umontreal.ca; yoshua.bengio@umontreal.ca						[Anonymous], 2013, ARXIV13126098; Anthony M., 1999, NEURAL NETWORK LEARN, V9; Ciresan D, 2012, NEURAL NETWORKS, V32, P333, DOI 10.1016/j.neunet.2012.02.023; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Delalleau O., 2011, NIPS; Glorot X., 2011, AISTATS; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Krause O., 2013, INT C MACH LEARN, P419; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Le Roux N, 2010, NEURAL COMPUT, V22, P2192, DOI 10.1162/neco.2010.08-09-1081; Montufar G., 2014, NEURAL COMPUTATION, V26; Montufar G, 2011, NEURAL COMPUT, V23, P1306, DOI 10.1162/NECO_a_00113; Nair V., 2010, ICML, P807; Pascanu R., 2014, INT C LEARN REPR; Sanjoy D., 2013, INT C MACH LEARN, P1319, DOI DOI 10.5555/3042817.3043084; Stanley R.P, 2004, LECT NOTES; Susskind J, 2010, 2010001 UTML TR; Zaslavsky T., 1975, MEM AM MATH SOC; Zeiler M. D., 2014, EUR C COMP VIS, P818	20	47	47	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647101093
C	Marlin, B		Thrun, S; Saul, K; Scholkopf, B		Marlin, B			Modeling user rating profiles for collaborative filtering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					In this paper we present a generative latent variable model for rating-based collaborative filtering called the User Rating Profile model (URP). The generative process which underlies URP is designed to produce complete user rating profiles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each.	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada	University of Toronto	Marlin, B (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.							Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Boutilier Craig, 2003, P 19 ANN C UNC ART I, P98; Buntine W., 2002, P EUR C MACH LEARN; Claypool M, 1999, P ACM SIGIR WORKSH R, V60; GIROLAMI M, 2003, P 26 ANN INT ACM SIG, P433; Goldberg K, 2001, INFORM RETRIEVAL, V4, P133, DOI 10.1023/A:1011419012209; HOFMANN T, 2001, P EUR C MACH LEARN; MINKA T, 2003, UNPUB ESTIMATING DIR; Resnick P., 1994, P ACM C COMP SUPP CO, P175, DOI DOI 10.1145/192844.192905	9	47	50	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						627	634						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500079
C	Chapelle, O; Weston, J; Bottou, L; Vapnik, V		Leen, TK; Dietterich, TG; Tresp, V		Chapelle, O; Weston, J; Bottou, L; Vapnik, V			Vicinal Risk Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				SUPPORT VECTOR MACHINES	The Vicinal Risk Minimization principle establishes a bridge between generative models and methods derived from the Structural Risk Minimization Principle such as Support Vector Machines or Statistical Regularization. We explain how VRM provides a framework which integrates a number of existing algorithms, such as Parzen windows, Support Vector Machines, Ridge Regression, Constrained Logistic Classifiers and Tangent-Prop. We then show how the approach implies new algorithms for solving problems usually associated with generative models. New algorithms are described for dealing with pattern recognition problems with very different pattern distributions and dealing with unlabeled data. Preliminary empirical results are presented.	AT&T Labs Res, Red Bank, NJ 07701 USA	AT&T	Chapelle, O (corresponding author), AT&T Labs Res, 100 Schultz Dr,POB7033, Red Bank, NJ 07701 USA.							Bennett KP, 1999, ADV NEUR IN, V11, P368; BOTTOU L, 1992, NEURAL COMPUT, V4, P888, DOI 10.1162/neco.1992.4.6.888; BREIMAN L, 1977, TECHNOMETRICS, V19, P135, DOI 10.2307/1268623; Drucker H, 1999, IEEE T NEURAL NETWOR, V10, P1048, DOI 10.1109/72.788645; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; JAAKKOLA T, 2000, ADV NEURAL INFORMATI, V12; LeCun Y., 1998, NEURAL NETWORKS TRIC; LEEN TK, 1995, ADV NEURAL INFORMATI, V7; SCHOLKOPF B, 1998, ADV NEURAL INFORMATI, V10; SCHUURMANS D, 2000, P 17 INT C MACH LEAR; SIMARD P, 1992, ADV NEURAL INFORMATI, V4; TONG S, 2000, P 17 NAT C ART INT; Vapnik V.N., 1999, NATURE STAT LEARNING	13	47	48	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						416	422						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800059
C	Heskes, T		Jordan, MI; Kearns, MJ; Solla, SA		Heskes, T			Selecting weighting factors in logarithmic opinion pools	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					A simple linear averaging of the outputs of several networks as e.g. in bagging [3], seems to follow naturally from a bias/variance decomposition of the sum-squared error. The sum-squared error of the average model is a quadratic function of the weighting factors assigned to the networks in the ensemble [7], suggesting a quadratic programming algorithm for finding the "optimal" weighting factors. If we interpret the output of a network as a probability statement, the sum-squared error corresponds to minus the loglikelihood or the Kullback-Leibler divergence, and linear averaging of the outputs to logarithmic averaging of the probability statements: the logarithmic opinion pool. The crux of this paper is that this whole story about model averaging, bias/variance decompositions, and quadratic programming to find the optimal weighting factors, is not specific for the sum-squared error, but applies to the combination of probability statements of any kind in a logarithmic opinion pool, as long as the Kullback-Leibler divergence plays the role of the error measure. As examples we treat model averaging for classification models under a cross-entropy error measure and models for estimating variances.	Catholic Univ Nijmegen, Fdn Neural Networks, NL-6525 EZ Nijmegen, Netherlands	Radboud University Nijmegen	Heskes, T (corresponding author), Catholic Univ Nijmegen, Fdn Neural Networks, Geert Grooteplein 21, NL-6525 EZ Nijmegen, Netherlands.		Heskes, Tom/A-1443-2010	Heskes, Tom/0000-0002-3398-5235					0	47	47	1	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						266	272						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700038
C	FREUND, Y; HAUSSLER, D		MOODY, JE; HANSON, SJ; LIPPMANN, RP		FREUND, Y; HAUSSLER, D			UNSUPERVISED LEARNING OF DISTRIBUTIONS ON BINARY VECTORS USING 2 LAYER NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	47	48	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						912	919						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00112
C	Fazlyab, M; Robey, A; Hassani, H; Morari, M; Pappas, GJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fazlyab, Mahyar; Robey, Alexander; Hassani, Hamed; Morari, Manfred; Pappas, George J.			Efficient and Accurate Estimation of Lipschitz Constants for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Tight estimation of the Lipschitz constant for deep neural networks (DNNs) is useful in many applications ranging from robustness certification of classifiers to stability analysis of closed-loop systems with reinforcement learning controllers. Existing methods in the literature for estimating the Lipschitz constant suffer from either lack of accuracy or poor scalability. In this paper, we present a convex optimization framework to compute guaranteed upper bounds on the Lipschitz constant of DNNs both accurately and efficiently. Our main idea is to interpret activation functions as gradients of convex potential functions. Hence, they satisfy certain properties that can be described by quadratic constraints. This particular description allows us to pose the Lipschitz constant estimation problem as a semidefinite program (SDP). The resulting SDP can be adapted to increase either the estimation accuracy (by capturing the interaction between activation functions of different layers) or scalability (by decomposition and parallel implementation). We illustrate the utility of our approach with a variety of experiments on randomly generated networks and on classifiers trained on the MNST and Iris datasets. In particular, we experimentally demonstrate that our Lipschitz bounds are the most accurate compared to those in the literature. We also study the impact of adversarial training methods on the Lipschitz bounds of the resulting classifiers and show that our bounds can be used to efficiently provide robustness guarantees.	[Fazlyab, Mahyar; Robey, Alexander; Hassani, Hamed; Morari, Manfred; Pappas, George J.] Univ Penn, ESE Dept, Philadelphia, PA 19104 USA	University of Pennsylvania	Fazlyab, M (corresponding author), Univ Penn, ESE Dept, Philadelphia, PA 19104 USA.	mahyarfa@seas.upenn.edu; arobeyl@seas.upenn.edu; hassani@seas.upenn.edu; morari@seasupenn.edu; pappasg@seasupenn.edu	Pappas, George/J-5774-2016	Pappas, George/0000-0001-9081-0637				Acikmese B, 2011, AUTOMATICA, V47, P1339, DOI 10.1016/j.automatica.2011.02.017; Adusumalli M, 2018, ROUT INT HANDB, P121; Anil C, 2019, PR MACH LEARN RES, V97; [Anonymous], 2018, ARXIV180801415; Aswani A, 2013, AUTOMATICA, V49, P1216, DOI 10.1016/j.automatica.2013.02.003; Balan R, 2017, ARXIV170105217; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Bastani O., 2016, P 30 INT C NEUR INF, P2613; Berkenkamp Felix, 2017, ADV NEURAL INFORM PR, P908; Boyd S, 2004, CONVEX OPTIMIZATION; Chen P.-Y., 2018, NIPS, P4939; Combettes Patrick L., 2019, ARXIV190301014; Dua D., 2017, UCI MACHINE LEARNING; Fazlyab M., 2019, ARXIV190301287; Fazlyab M, 2018, SIAM J OPTIMIZ, V28, P2654, DOI 10.1137/17M1136845; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Goodfellow I. J., 2014, ARXIV14126572; Gouk H., 2018, ARXIV180404368; Grant M., 2013, CVX MATLAB SOFTWARE; Huster T., 2018, JOINT EUR C MACH LEA, P16; Jin M, 2018, ARXIV181011505; Jordan M., 2019, ARXIV190308778; Kurakin A, 2016, INT C LEARN REPR SAN; Madry Aleksander, 2017, ARXIV70606083; MOSEK ApS, 2017, MOSEK OPT TOOLB MATL; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Peck J, 2017, ADV NEURAL INFORM PR, P804; Qian H., 2018, ARXIV180207896; Raghunathan A., 2018, ADV NEURAL INFORM PR, P10900; Raghunathan Aditi, 2018, INT C LEARN REPR; Ruan W., 2018, ARXIV180502242; Ryu EK, 2016, APPL COMPUT MATH-BAK, V15, P3; Singh Gagandeep, 2018, ADV NEURAL INFORM PR, P10802; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Tjeng V., 2017, INT C LEARN REPR; Tsuzuku Y., 2018, ADV NEURAL INFORM PR; Virmaux A., 2018, ADV NEURAL INFORM PR, V31, P3835; Weng Tsui-Wei, 2018, ARXIV180110578; Wong E, 2018, ARXIV180512514; Wong E., 2018, P INT C MACH LEARN, P5286; Yann L., 1998, MNIST DATABASE HANDW, P1; Zhang Huan, 2018, ARXIV180409699; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485	44	46	46	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903010
C	Kynkaanniemi, T; Karras, T; Laine, S; Lehtinen, J; Aila, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kynkaanniemi, Tuomas; Karras, Tero; Laine, Samuli; Lehtinen, Jaakko; Aila, Timo			Improved Precision and Recall Metric for Assessing Generative Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The ability to automatically estimate the quality and coverage of the samples produced by a generative model is a vital requirement for driving algorithm research. We present an evaluation metric that can separately and reliably measure both of these aspects in image generation tasks by forming explicit, non-parametric representations of the manifolds of real and generated data. We demonstrate the effectiveness of our metric in StyleGAN and BigGAN by providing several illustrative examples where existing metrics yield uninformative or contradictory results. Furthermore, we analyze multiple design variants of StyleGAN to better understand the relationships between the model architecture, training methods, and the properties of the resulting sample distribution. In the process, we identify new variants that improve the state-of-the-art. We also perform the first principled analysis of truncation methods and identify an improved method. Finally, we extend our metric to estimate the perceptual quality of individual samples, and use this to study latent space interpolations.	[Kynkaanniemi, Tuomas; Lehtinen, Jaakko] Aalto Univ, Espoo, Finland; [Kynkaanniemi, Tuomas; Karras, Tero; Laine, Samuli; Lehtinen, Jaakko; Aila, Timo] NVIDIA, Santa Clara, CA USA	Aalto University; Nvidia Corporation	Kynkaanniemi, T (corresponding author), Aalto Univ, Espoo, Finland.; Kynkaanniemi, T (corresponding author), NVIDIA, Santa Clara, CA USA.	tuomas.kynkaanniemi@aalto.fi; tkarras@nvidia.com; slaine@nvidia.com; jlehtinen@nvidia.com; taila@nvidia.com	Lehtinen, Jaakko/G-2328-2013					[Anonymous], 2009, P CVPR; Arora S., 2017, ABS170608224 CORR; Binkowski M., 2018, ABS180101401 CORR; Brock A, 2019, P ICLR; Dinh Laurent, 2016, ABS160508803 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grover A., 2018, P AAAI; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Huang X., 2017, ABS170306868 CORR; Karras T., 2019, P CVPR; Karras T., 2017, ABS1710196 CORR; Kingma D. P., 2014, P NIPS; Kingma D. P., 2018, ABS180703039 CORR; Lin Z., 2017, ABS171204086 CORR; Lopez-Paz D., 2017, P ICLR; Marchesi M., 2017, ABS170600082 CORR; Mescheder Lars M., 2018, ABS180104406 CORR; Metz L., 2016, ABS161102163 CORR; Miyato T., 2018, ABS180205637 CORR; Miyato T., 2018, ABS180205957 CORR; Nalisnick E., 2019, P ICLR; Odena A., 2017, ICML; Sajjadi M. S. M., 2018, ABS180600035 CORR; Salimans Tim, 2016, ADV NEURAL INFORM PR; Simon L., 2019, ABS190505441 CORR; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Slowiski R., 2008, MULTIOBJECTIVE OPTIM, V5252; Tolstikhin Ilya, 2018, P ICLR; van den Oord A., 2016, ABS160605328 CORR; van den Oord A, 2016, PR MACH LEARN RES, V48; Zhang H., 2018, ABS180508318 CORR; Zhang Richard, 2018, P CVPR; Zhu J., 2017, ABS170310593 CORR	33	46	46	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303087
C	Zhang, DH; Zhang, TY; Lu, YP; Zhu, ZX; Dong, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Dinghuai; Zhang, Tianyuan; Lu, Yiping; Zhu, Zhanxing; Dong, Bin			You Only Propagate Once: Accelerating Adversarial Training via Maximal Principle	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep learning achieves state-of-the-art results in many tasks in computer vision and natural language processing. However, recent works have shown that deep networks can be vulnerable to adversarial perturbations, which raised a serious robustness issue of deep networks. Adversarial training, typically formulated as a robust optimization problem, is an effective way of improving the robustness of deep networks. A major drawback of existing adversarial training algorithms is the computational overhead of the generation of adversarial examples, typically far greater than that of the network training. This leads to the unbearable overall computational cost of adversarial training. In this paper, we show that adversarial training can be cast as a discrete time differential game. Through analyzing the Pontryagin's Maximum Principle (PMP) of the problem, we observe that the adversary update is only coupled with the parameters of the first layer of the network. This inspires us to restrict most of the forward and back propagation within the first layer of the network during adversary updates. This effectively reduces the total number of full forward and backward propagation to only one for each group of adversary updates. Therefore, we refer to this algorithm YOPO (You Only Propagate Once). Numerical experiments demonstrate that YOPO can achieve comparable defense accuracy with approximately 1/5 similar to 1/4 GPU time of the projected gradient descent (PGD) algorithm.(3)	[Zhang, Dinghuai; Zhang, Tianyuan] Peking Univ, Beijing, Peoples R China; [Lu, Yiping] Stanford Univ, Stanford, CA 94305 USA; [Zhu, Zhanxing] Peking Univ, Sch Math Sci, Ctr Data Sci, Beijing Inst Big Data Res, Beijing, Peoples R China; [Dong, Bin] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing Int Ctr Math Res, Beijing, Peoples R China	Peking University; Stanford University; Peking University; Peking University	Zhu, ZX (corresponding author), Peking Univ, Sch Math Sci, Ctr Data Sci, Beijing Inst Big Data Res, Beijing, Peoples R China.; Dong, B (corresponding author), Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing Int Ctr Math Res, Beijing, Peoples R China.	zhangdinghuai@pku.edu.cn; 1600012888@pku.edu.cn; yplu@stanford.edu; zhanxing.zhu@pku.edu.cn; dongbin@math.pku.edu.cn	Dong, Bin/ABF-1907-2020; Zhu, Zhanxing/GQA-7335-2022		National Natural Science Foundation of China [61806009]; Beijing Natural Science Foundation [Z180001, 4184090]; Beijing Academy of Artificial Intelligence (BAAI); Elite Undergraduate Training Program of Applied Math of the School of Mathematical Sciences at Peking University	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation); Beijing Academy of Artificial Intelligence (BAAI); Elite Undergraduate Training Program of Applied Math of the School of Mathematical Sciences at Peking University	We thank Di He and Long Chen for beneficial discussion. Zhanxing Zhu is supported in part by National Natural Science Foundation of China (No.61806009), Beijing Natural Science Foundation (No. 4184090) and Beijing Academy of Artificial Intelligence (BAAI). Bin Dong is supported in part by Beijing Natural Science Foundation (No. Z180001) and Beijing Academy of Artificial Intelligence (BAAI). Dinghuai Zhang is supported by the Elite Undergraduate Training Program of Applied Math of the School of Mathematical Sciences at Peking University.	[Anonymous], 2017, ARXIV171209196; [Anonymous], 2017, COMMUN MATH STAT; Askari A., 2018, ARXIV180501532; Athalye A., 2018, P 35 INT C MACH LEAR; Boltyanskii VG, 1960, THEORY OPTIMAL PROCE; Cao P., 2019, INT C LEARN REPR; Chen T.Q., 2018, ADV NEURAL INFORM PR; Cisse M, 2017, PR MACH LEARN RES, V70; Evans L. C., 2005, LECT NOTES; Goodfellow I., 2016, DEEP LEARNING; Gu F., 2018, ARXIV181108039; Haber Eldad, 2017, INVERSE PROBL, V34; Huo Z., 2018, ARXIV180410574; Jaderberg M, 2017, PR MACH LEARN RES, V70; Jakubovitz D, 2018, EUR C COMP VIS; Kurakin A, 2016, INT C LEARN REPR SAN; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; LeCun Y., 1988, P 1988 CONNECTIONIST, DOI DOI 10.3168/JDS.S0022-0302(88)79586-7; Li Jia, 2018, ARXIV181101501; LI Q, 2017, J MACH LEARN RES, V18, P5998; Li Qianxiao, 2018, P 35 INT C MACH LEAR, V80, P2985; Li Xuechen, 2019, ARXIV190607868; Lin J., 2019, INT C LEARN REPR; Lin Z, 2019, ARXIV190211029; Liu Y., 2019, INT C LEARNING REPRE; Lu Y., 2017, FINITE LAYER NEURAL; Luo Tiange, 2019, RANDOM MASK ROBUST C; Ma PX, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201334; Madry Aleksander, 2018, ICLR; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Pontryagin LS., 1987, MATH THEORY OPTIMAL, DOI [10.1201/9780203749319, DOI 10.1201/9780203749319]; Qian H., 2018, ARXIV180207896; Shafahi Ali, 2019, ADVERSARIAL TRAINING; Song Y, 2017, ARXIV171010766; Sonoda S, 2019, J MACH LEARN RES, V20; Svoboda Jan, 2019, INT C LEARN REPR; Szegedy Christian, 2013, INTRIGUING PROPERTIE, DOI 10.1364/BOE.8.000579; Taylor G, 2016, PR MACH LEARN RES, V48; Thorpe M, 2018, ARXIV181011741; Wald A, 1939, ANN MATH STAT, V10, P299, DOI 10.1214/aoms/1177732144; Wang B., 2018, ARXIV181110745; Xie C., 2018, ARXIV181203411; Xu W., 2017, P 25 ANN NETW DISTR, DOI DOI 10.14722/NDSS.2018.23198; Xu Yilun, 2019, ARXIV190903388; Ye N., 2018, P 32 INT C NEUR INF, P6892; Zhang H., 2019, ARXIV190108573; Zhang JF, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P4285; Zugner D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6246, DOI 10.1145/3219819.3220078	49	46	47	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300021
C	Bastani, O; Pu, YW; Solar-Lezama, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bastani, Osbert; Pu, Yewen; Solar-Lezama, Armando			Verifiable Reinforcement Learning via Policy Extraction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					While deep reinforcement learning has successfully solved many challenging control tasks, its real-world applicability has been limited by the inability to ensure the safety of learned policies. We propose an approach to verifiable reinforcement learning by training decision tree policies, which can represent complex policies (since they are nonparametric), yet can be efficiently verified using existing techniques (since they are highly structured). The challenge is that decision tree policies are difficult to train. We propose VIPER, an algorithm that combines ideas from model compression and imitation learning to learn decision tree policies guided by a DNN policy (called the oracle) and its Q-function, and show that it substantially outperforms two baselines. We use VIPER to (i) learn a provably robust decision tree policy for a variant of Atari Pong with a symbolic state space, (ii) learn a decision tree policy for a toy game based on Pong that provably never loses, and (iii) learn a provably stable decision tree policy for cart-pole. In each case, the decision tree policy achieves performance equal to that of the original DNN policy.	[Bastani, Osbert; Pu, Yewen; Solar-Lezama, Armando] MIT, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Bastani, O (corresponding author), MIT, Cambridge, MA 02139 USA.	obastani@csail.mit.edu; yewenpu@mit.edu; asolar@csail.mit.edu			Toyota Research Institute; NSF InTrans award [1665282]	Toyota Research Institute; NSF InTrans award	This work was funded by the Toyota Research Institute and NSF InTrans award 1665282.	Abbeel P., 2004, ICML; Akametalu A. K., 2014, CDC; [Anonymous], 2017, CAV; Aswani A., 2013, AUTOMATICA; Ba J., 2014, NIPS; BARTO AG, 1983, IEEE T SYSTEMS MAN C; Bastani Osbert, 2017, FAT ML; Bastani Osbert, 2016, NIPS; Berkenkamp Felix, 2017, NIPS; Bucilua Cristian, 2006, KDD; Collins S., 2005, SCIENCE; de Moura L., 2008, TACAS; Ernst Damien, 2005, JMLR; Garcia Javier, 2015, JMLR; Gehr Timon, 2018, IEEE SECURITY PRIVAC; Goodfellow I. J., 2015, ICLR; Hinton G, 2014, ADV NEURAL INFORM PR, P9; Katz Guy, 2017, CAV; Kuindersma Scott, 2016, AUTONOMOUS ROBOTS; Levine Sergey, 2013, ICML; Mnih V, 2015, NATURE, V518, P529; Moldovan T. M., 2012, ICML; Olshen R., 1984, CLASSIFICATION REGRE; Parrilo PA., 2000, THESIS CALTECH; Ross S., 2011, P INT C ARTIFICIAL I, P627; Sadigh D., 2016, IROS; SCHAAL S, 1999, TRENDS COGNITIVE SCI; Schulman J., 2017, ABS170706347 CORR; Silver D., 2016, NATURE; Szegedy Christian, 2014, ICLR; Tedrake R., 2018, UNDERACTUATED ROBOTI; Tedrake Russ, 2010, IJRR; Turchetta Matteo, 2016, NIPS; Vandewiele Gilles, 2016, NIPS WORKSH INT MACH; Verma A., 2018, ICML; Wu Yifan, 2016, ICML; Ziebart B. D., 2008, AAAI	37	46	46	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302050
C	Hamilton, WL; Bajaj, P; Zitnik, M; Jurafsky, D; Leskovec, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hamilton, William L.; Bajaj, Payal; Zitnik, Marinka; Jurafsky, Dan; Leskovec, Jure			Embedding Logical Queries on Knowledge Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DATABASE; NETWORKS	Learning low-dimensional embeddings of knowledge graphs is a powerful approach used to predict unobserved or missing edges between entities. However, an open challenge in this area is developing techniques that can go beyond simple edge prediction and handle more complex logical queries, which might involve multiple unobserved edges, entities, and variables. For instance, given an incomplete biological knowledge graph, we might want to predict what drugs are likely to target proteins involved with both diseases X and Y?-a query that requires reasoning about all possible proteins that might interact with diseases X and Y. Here we introduce a framework to efficiently make predictions about conjunctive logical queries-a flexible but tractable subset of first-order logic-on incomplete knowledge graphs. In our approach, we embed graph nodes in a low-dimensional space and represent logical operators as learned geometric operations (e.g., translation, rotation) in this embedding space. By performing logical operations within a low-dimensional embedding space, our approach achieves a time complexity that is linear in the number of query variables, compared to the exponential complexity required by a naive enumeration-based approach. We demonstrate the utility of this framework in two application studies on real-world datasets with millions of relations: predicting logical relationships in a network of drug-gene-disease interactions and in a graph-based representation of social interactions derived from a popular web forum.	[Hamilton, William L.; Bajaj, Payal; Zitnik, Marinka; Leskovec, Jure] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA; [Jurafsky, Dan] Stanford Univ, Dept Linguist, Stanford, CA 94305 USA	Stanford University; Stanford University	Hamilton, WL (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.	wleif@stanford.edu; pbajaj@stanford.edu; jure@cs.stanford.edu; jurafsky@stanford.edu; marinka@cs.stanford.edu			NSF [IIS-1149837]; DARPA SIMPLEX; Stanford Data Science Initiative; Huawei; Chan Zuckerberg Biohub; SAP Stanford Graduate Fellowship; NSERC PGS-D grant	NSF(National Science Foundation (NSF)); DARPA SIMPLEX; Stanford Data Science Initiative; Huawei(Huawei Technologies); Chan Zuckerberg Biohub; SAP Stanford Graduate Fellowship; NSERC PGS-D grant	The authors thank Alex Ratner, Stephen Bach, and Michele Catasta for their helpful discussions and comments on early drafts. This research has been supported in part by NSF IIS-1149837, DARPA SIMPLEX, Stanford Data Science Initiative, Huawei, and Chan Zuckerberg Biohub. WLH was also supported by the SAP Stanford Graduate Fellowship and an NSERC PGS-D grant.	Abiteboul S., 1995, FDN DATABASES LOGICA; Ashburner M, 2000, NAT GENET, V25, P25, DOI 10.1038/75556; Bach S., 2017, J MACH LEARN RES, V18, P1; Berant J, 2013, EMNLP; Bordes A., 2014, EMNLP; Bordes A., 2013, ADV NEURAL INF PROCE, V26, P1; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Brown AS, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.29; Cavallo R., 1987, VLDB; Chatr-aryamontri A, 2015, NUCLEIC ACIDS RES, V43, pD470, DOI 10.1093/nar/gku1204; Cohen William W., 2016, ARXIV160506523; Dalvi N., 2007, VLDB; Das R, 2016, ARXIV PREPRINT ARXIV; Das Rajarshi, 2018, INT C LEARN REPR; Demeester Thomas, 2016, EMNLP; Getoor Lise, 2007, INTRO STAT RELATIONA; Gilmer J., 2017, ICML; Guu K., 2015, EMNLP; Hamilton W.L., 2017, IEEE DATA ENG B; Hu Z., 2016, ACL; Indyk P., 1998, ACM S THEOR COMP; KAHN AB, 1962, COMMUN ACM, V5, P558, DOI 10.1145/368996.369025; Krompass D, 2014, LECT NOTES COMPUT SC, V8797, P114, DOI 10.1007/978-3-319-11915-1_8; Kuhn M, 2016, NUCLEIC ACIDS RES, V44, pD1075, DOI 10.1093/nar/gkv1075; Liben-Nowell D, 2007, J AM SOC INF SCI TEC, V58, P1019, DOI 10.1002/asi.20591; Menche J, 2015, SCIENCE, V347, DOI 10.1126/science.1257601; Neelakantan A., 2015, AAAI; NICKEL M, 2011, ICML; Nickel M, 2016, P IEEE, V104, P11, DOI 10.1109/JPROC.2015.2483592; Paszke Adam, 2017, AUTOMATIC DIFFERENTI; Qi C. R., 2017, CVPR; Queralt-Rosinach N., 2015, DATABASE, V2015; Ramanathan G., 2015, AAAI SPRING SERIES; Rocktaschel T., 2017, NIPS; Rocktaschel T., 2017, ARXIV171209687; Rocktaschel Tim, 2014, P ACL 2014 WORKSH SE, P45; Rocktaschel Tim, 2015, P 2015 C N AM CHAPT, P1119, DOI [10.3115/v1/N15-1118, DOI 10.3115/V1/N15-1118]; Rolland T, 2014, CELL, V159, P1212, DOI 10.1016/j.cell.2014.10.050; Szklarczyk D, 2017, NUCLEIC ACIDS RES, V45, pD362, DOI 10.1093/nar/gkw937; Szklarczyk D, 2016, NUCLEIC ACIDS RES, V44, pD380, DOI 10.1093/nar/gkv1277; Tatonetti NP, 2012, SCI TRANSL MED, V4, DOI 10.1126/scitranslmed.3003377; Thulasiraman K., 2011, GRAPHS THEORY ALGORI; Wang M., 2017, NIPS; Wu L., 2017, AAAI; Yang B., 2015, ICLR; Zaheer Manzil, 2017, NIPS; Zhang Y., 2018, AAAI; Zhou T, 2007, PHYS REV E, V76, DOI 10.1103/PhysRevE.76.046115; Zitnik Marinka, 2018, BIOINFORMATICS	49	46	48	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302007
C	Goyal, A; Sordoni, A; Cote, MA; Ke, NR; Bengio, Y		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Goyal, Anirudh; Sordoni, Alessandro; Cote, Marc-Alexandre; Ke, Nan Rosemary; Bengio, Yoshua			Z-Forcing: Training Stochastic Recurrent Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Many efforts have been devoted to training generative latent variable models with autoregressive decoders, such as recurrent neural networks (RNN). Stochastic recurrent models have been successful in capturing the variability observed in natural sequential data such as speech. We unify successful ideas from recently proposed architectures into a stochastic recurrent model: each step in the sequence is associated with a latent variable that is used to condition the recurrent dynamics for future steps. Training is performed with amortised variational inference where the approximate posterior is augmented with a RNN that runs backward through the sequence. In addition to maximizing the variational lower bound, we ease training of the latent variables by adding an auxiliary cost which forces them to reconstruct the state of the backward recurrent network. This provides the latent variables with a task-independent objective that enhances the performance of the overall model. We found this strategy to perform better than alternative approaches such as KL annealing. Although being conceptually simple, our model achieves state-of-the-art results on standard speech benchmarks such as TIMIT and Blizzard and competitive performance on sequential MNIST. Finally, we apply our model to language modeling on the IMDB dataset where the auxiliary cost helps in learning interpretable latent variables.	[Goyal, Anirudh; Ke, Nan Rosemary; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada; [Sordoni, Alessandro; Cote, Marc-Alexandre] Microsoft Maluuba, Montreal, PQ, Canada	Universite de Montreal	Goyal, A (corresponding author), Univ Montreal, MILA, Montreal, PQ, Canada.		Jeong, Yongwook/N-7413-2016		NSERC; CIFAR; Google; Samsung; IBM; Canada Research Chairs	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Google(Google Incorporated); Samsung(Samsung); IBM(International Business Machines (IBM)); Canada Research Chairs(Canada Research ChairsCGIAR)	The authors would like to thank Phil Bachman, Alex Lamb and Adam Trischler for the useful discussions. AG and YB would also like to thank NSERC, CIFAR, Google, Samsung, IBM and Canada Research Chairs for funding, and Compute Canada and NVIDIA for computing resources. The authors would also like to express debt of gratitude towards those who contributed to Theano over the years (as it is no longer maintained), making it such a great tool.	Bachman P., 2015, TRAINING DEEP GENERA; Bachman P, 2016, ADV NEUR IN, V29; Bayer J, 2014, ARXIV14117610; Boulanger-Lewandowski N, 2012, P 29 INT C MACH LEAR, P1159; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Burda Yuri, 2015, ARXIV150900519; Chen X., 2017, P ICLR; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Diao QM, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P193, DOI 10.1145/2623330.2623758; Germain M, 2015, PR MACH LEARN RES, V37, P881; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Gulrajani I., 2016, P INT C LEARN REPR; Gulrajani I., 2017, INT C LEARN REPR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Honkela A, 2004, IEEE T NEURAL NETWOR, V15, P800, DOI 10.1109/TNN.2004.828762; Hu ZT, 2017, PR MACH LEARN RES, V70; Karl M., 2016, INT C LEARN REPR; King DB, 2015, ACS SYM SER, V1214, P1; King S., 2013, 9 ANN BLIZZARD CHALL; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Larochelle H., 2011, INT C ART INT STAT; Louizos C, 2017, PR MACH LEARN RES, V70; Raiko T., 2014, ADV NEURAL INFORM PR; Rezende DJ, 2015, PR MACH LEARN RES, V37, P1530; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Salakhutdinov R., 2008, PROC 25 INT C MACHIN, P872, DOI [10.1145/1390156.1390266, DOI 10.1145/1390156.1390266]; Salimans Tim, 2014, ICML; Semeniuta Stanislau, 2017, P 2017 C EMP METH NA, P627, DOI DOI 10.18653/V1/D17-1066; Serban I. V., 2017, P 2017 C EMP METH NA, P422, DOI DOI 10.18653/V1/D17-1043; Serban IV, 2017, AAAI CONF ARTIF INTE, P3288; Uria B, 2016, J MACH LEARN RES, V17; van den Oord A, 2016, PR MACH LEARN RES, V48; Zhao TC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P654, DOI 10.18653/v1/P17-1061	37	46	46	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406075
C	Hahnloser, RHR; Seung, HS		Leen, TK; Dietterich, TG; Tresp, V		Hahnloser, RHR; Seung, HS			Permitted and forbidden sets in symmetric threshold-linear networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				CORTEX	Ascribing computational principles to neural feedback circuits is an important problem in theoretical neuroscience. We study symmetric threshold-linear networks and derive stability results that go beyond the insights that can be gained from Lyapunov theory or energy functions. By applying linear analysis to subnetworks composed of coactive neurons, we determine the stability of potential steady states. We find that stability depends on two types of eigenmodes. One type determines global stability and the other type determines whether or not multistability is possible. We can prove the equivalence of our stability criteria with criteria taken from quadratic programming. Also, we show that there are permitted sets of neurons that can be coactive at a steady state and forbidden sets that cannot. Permitted sets axe clustered in the sense that subsets of permitted sets are permitted and supersets of forbidden sets are forbidden. By viewing permitted sets as memories stored in the synaptic connections, we can provide a formulation of longterm memory that is more general than the traditional perspective of fixed point attractor networks.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Hahnloser, RHR (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.	rh@ai.mit.edu; seung@mit.edu		Hahnloser, Richard/0000-0002-4039-7773				BENYISHAI R, 1995, P NATL ACAD SCI USA, V92, P3844, DOI 10.1073/pnas.92.9.3844; COHEN M, 1983, IEEE T SYST MAN CYB, V13, P288; DOUGLAS RJ, 1995, SCIENCE, V269, P981, DOI 10.1126/science.7638624; Feng J, 1996, J PHYS A-MATH GEN, V29, P5019, DOI 10.1088/0305-4470/29/16/023; Hahnloser RHR, 2000, NATURE, V405, P947, DOI 10.1038/35016072; HOPFIELD JJ, 1984, P NATL ACAD SCI-BIOL, V81, P3088, DOI 10.1073/pnas.81.10.3088; Horn R. A., 1986, MATRIX ANAL; XIE XH, 2001, P NIPS2001 NEUR INF	8	46	48	0	2	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						217	223						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J		Green Submitted			2022-12-19	WOS:000171891800031
C	Hofmann, T		Solla, SA; Leen, TK; Muller, KR		Hofmann, T			Learning the similarity of documents: An information-geometric approach to document, retrieval and categorization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				CONTINGENCY-TABLES	The project pursued in this paper is to develop from first information-geometric principles a general method for learning the similarity between text documents. Each individual document is modeled as a memoryless information source. Based on a latent class decomposition of the term-document matrix, a low-dimensional (curved) multinomial subfamily is learned. From this model a canonical similarity function - known as the Fisher kernel - is derived. Our approach can be applied for unsupervised and supervised learning problems alike. This in particular covers interesting cases where both, labeled and unlabeled data are available. Experiments in automated indexing and text categorization verify the advantages of the proposed method.	Brown Univ, Dept Comp Sci, Providence, RI 02912 USA	Brown University	Hofmann, T (corresponding author), Brown Univ, Dept Comp Sci, Providence, RI 02912 USA.	hofmann@cs.brown.edu						Amari S.-i., 1985, DIFFERENTIAL GEOMETR, V28; DEERWESTER S, 1990, J AM SOC INFORM SCI, V41, P391, DOI 10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9; EVANS MJ, 1989, BIOMETRIKA, V76, P557; GEIGER D, 1998, MSRTR9831; GILULA Z, 1986, J AM STAT ASSOC, V81, P780, DOI 10.2307/2289010; GOUS A, 1998, THESIS STAT DEPT STA; Hofmann T, 1999, SIGIR'99: PROCEEDINGS OF 22ND INTERNATIONAL CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P50, DOI 10.1145/312624.312649; HOFMANN T, 1999, ADV NEURAL INFORMATI, V11; JAAKKOLA TS, 1999, ADV NEURAL INFORMATI, V11; JOACHIMS T, 1998, INT C MACH LEARN ECM; KASS R., 1997, GEOMETRICAL FDN ASYM; Lee DD, 1999, NATURE, V401, P788, DOI 10.1038/44565; Murray M. K., 1993, MONOGRAPHS STAT APPL, V48; Pereira Fernando, 1993, P 31 ANN M ASS COMP, P183, DOI DOI 10.3115/981574.981598; SAUL L, 1997, P 2 INT C EMP METH N	15	46	55	0	33	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						914	920						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700129
C	Auer, P; Herbster, M; Warmuth, MK		Touretzky, DS; Mozer, MC; Hasselmo, ME		Auer, P; Herbster, M; Warmuth, MK			Exponentially many local minima for single neurons	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						DEPT COMP SCI,SANTA CRUZ,CA				Auer, Peter/AAC-1314-2019	Auer, Peter/0000-0001-8385-9635					0	46	46	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						316	322						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00045
C	Chen, XY; Tian, YD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Chen, Xinyun; Tian, Yuandong			Learning to Perform Local Rewriting for Combinatorial Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Search-based methods for hard combinatorial optimization are often guided by heuristics. Tuning heuristics in various conditions and situations is often time-consuming. In this paper, we propose NeuRewriter that learns a policy to pick heuristics and rewrite the local components of the current solution to iteratively improve it until convergence. The policy factorizes into a region-picking and a rule-picking component, each parameterized by a neural network trained with actor-critic methods in reinforcement learning. NeuRewriter captures the general structure of combinatorial problems and shows strong performance in three versatile tasks: expression simplification, online job scheduling and vehicle routing problems. NeuRewriter outperforms the expression simplification component in Z3 [15]; outperforms DeepRM [33] and Google OR-tools [19] in online job scheduling; and outperforms recent neural baselines [35, 29] and Google OR-tools [19] in vehicle routing problems. (2)	[Chen, Xinyun] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Tian, Yuandong] Facebook AI Res, Menlo Pk, CA USA	University of California System; University of California Berkeley; Facebook Inc	Chen, XY (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	xinyun.chen@berkeley.edu; yuandong@fb.com	Chen, Xinyun/ABZ-9877-2022					Affenzeller M., 2002, Proceedings of the 9th International Conference on Operational Research - KOI 2002, P83; Allamanis M, 2017, PR MACH LEARN RES, V70; Armbrust M, 2010, COMMUN ACM, V53, P50, DOI 10.1145/1721654.1721672; Bachmair L., 1994, Journal of Logic and Computation, V4, P217, DOI 10.1093/logcom/4.3.217; BARYEHUDA R, 1981, J ALGORITHM, V2, P198, DOI 10.1016/0196-6774(81)90020-1; Bay A., 2017, ARXIV170902194; Bello I., 2016, ARXIV PREPRINT ARXIV; Blazewicz J, 1996, EUR J OPER RES, V93, P1, DOI 10.1016/0377-2217(95)00362-2; BRADTKE SJ, 1994, PROCEEDINGS OF THE 1994 AMERICAN CONTROL CONFERENCE, VOLS 1-3, P3475; Bunel R. R., 2016, C NEURAL INFORM PROC, V29, P1444; Cai CH, 2018, BIOL INSPIR COGN ARC, V25, P43, DOI 10.1016/j.bica.2018.07.004; Chen T.Q., 2018, NIPS; Chen W., 2017, ARXIV171107440; Cohn T, 2009, J ARTIF INTELL RES, V34, P637, DOI 10.1613/jair.2655; Deudon M, 2018, LECT NOTES COMPUT SC, V10848, P170, DOI 10.1007/978-3-319-93031-2_12; Evans Richard, 2018, ICLR; Feblowitz D., 2013, P 2 WORKSH PRED IMPR, P1; Grandl R, 2014, ACM SIGCOMM COMP COM, V44, P455, DOI 10.1145/2740070.2626334; Graves A, 2014, NEURAL TURING MACHIN; Haarnoja T, 2017, PR MACH LEARN RES, V70; HSIANG J, 1992, J LOGIC PROGRAM, V14, P71, DOI 10.1016/0743-1066(92)90047-7; Huang D., 2018, ARXIV180600608; Jaderberg M, 2017, PR MACH LEARN RES, V70; Karp RM., 1972, COMPLEXITY COMPUTER, P85; Khalil E., 2017, ADV NEURAL INFORM PR, P6348; Lederman G., 2018, ARXIV180708058; Levine Sergey, 2013, ICML; Mandel T, 2014, AAMAS'14: PROCEEDINGS OF THE 2014 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P1077; Mao HZ, 2016, PROCEEDINGS OF THE 15TH ACM WORKSHOP ON HOT TOPICS IN NETWORKS (HOTNETS '16), P50, DOI 10.1145/3005745.3005750; Mayne D.Q., 1973, CONTROL DYN SYST, V10, P179; Nazari M., 2018, ARXIV180204240, P9861, DOI 10.5555/3327546.3327651; Paetzold G. H., 2013, P 9 BRAZ S INF HUM L; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Ragan-Kelley J, 2013, ACM SIGPLAN NOTICES, V48, P519, DOI 10.1145/2499370.2462176; Reeves C. R., 1995, ADV TOPICS COMPUTER, V15; Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150; Scully Ziv, 2017, ACM SIGMETRICS Performance Evaluation Review, V45, P36, DOI 10.1145/3152042.3152055; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sorensson Niklas, 2005, SAT, V53, P1; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tai K. S., 2015, P ANN M ASS COMP LIN; Tassa Y, 2012, IEEE INT C INT ROBOT, P4906, DOI 10.1109/IROS.2012.6386025; Terekhov D., 2014, SURV OPER RES MANAG, V19, P105, DOI [10.1016/J.SORMS.2014.09.001, DOI 10.1016/J.SORMS.2014.09.001]; Tian YD, 2013, IEEE I CONF COMP VIS, P2288, DOI 10.1109/ICCV.2013.284; van Hoof H., 2019, INT C LEARN REPR; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Vrabie D, 2009, AUTOMATICA, V45, P477, DOI 10.1016/j.automatica.2008.08.017; Zaremba W, 2014, ADV NEUR IN, V27; Zhu X.-D., 2016, P C N AM CHAPT ASS C, P917	50	45	46	7	26	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306030
C	Li, BW; Qi, XJ; Lukasiewicz, T; Torr, PHS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Li, Bowen; Qi, Xiaojuan; Lukasiewicz, Thomas; Torr, Philip H. S.			Controllable Text-to-Image Generation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					In this paper, we propose a novel controllable text-to-image generative adversarial network (ControlGAN), which can effectively synthesise high-quality images and also control parts of the image generation according to natural language descriptions. To achieve this, we introduce a word-level spatial and channel-wise attention-driven generator that can disentangle different visual attributes, and allow the model to focus on generating and manipulating subregions corresponding to the most relevant words. Also, a word-level discriminator is proposed to provide fine-grained supervisory feedback by correlating words with image regions, facilitating training an effective generator which is able to manipulate specific visual attributes without affecting the generation of other content. Furthermore, perceptual loss is adopted to reduce the randomness involved in the image generation, and to encourage the generator to manipulate specific attributes required in the modified text. Extensive experiments on benchmark datasets demonstrate that our method outperforms existing state of the art, and is able to effectively manipulate synthetic images using natural language descriptions. Code is available at https://github.com/mrlibw/ControlGAN.	[Li, Bowen; Qi, Xiaojuan; Lukasiewicz, Thomas; Torr, Philip H. S.] Univ Oxford, Oxford, England	University of Oxford	Li, BW (corresponding author), Univ Oxford, Oxford, England.	bowen.li@cs.ox.ac.uk; xiaojuan.qi@eng.ox.ac.uk; thomas.lukasiewicz@cs.ox.ac.uk; philip.torr@eng.ox.ac.uk		Lukasiewicz, Thomas/0000-0002-7644-1668	Alan Turing Institute under the UK EPSRC [EP/N510129/1]; AXA Research Fund; ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [Seebibyte EP/M013774/1]; EPSRC/MURI [EP/N019474/1]	Alan Turing Institute under the UK EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); AXA Research Fund(AXA Research Fund); ERC(European Research Council (ERC)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by the Alan Turing Institute under the UK EPSRC grant EP/N510129/1, the AXA Research Fund, the ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would also like to acknowledge the Royal Academy of Engineering and FiveAI.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ba J. L, 2015, ARXIV151102793; Brock A., 2016, ARXIV160907093; Cheng MM, 2014, ACM T GRAPHIC, V34, DOI 10.1145/2682628; Denton Emily L, 2015, NEURIPS, V2, P4; Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kheirkhah P, 2017, IEEE IJCNN, P4467, DOI 10.1109/IJCNN.2017.7966422; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Nam S, 2018, ADV NEUR IN, V31; Oliva A, 2003, IEEE IMAGE PROC, P253, DOI 10.1109/icip.2003.1246946; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wah C., 2011, TECH REP; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Xu T, 2018, PROC CVPR IEEE, P1316, DOI 10.1109/CVPR.2018.00143; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Zhang H, 2019, IEEE T PATTERN ANAL, V41, P1947, DOI 10.1109/TPAMI.2018.2856256; Zhang H, 2017, IEEE I CONF COMP VIS, P5908, DOI 10.1109/ICCV.2017.629; Zhang SH, 2017, PROC CVPR IEEE, P4264, DOI 10.1109/CVPR.2017.454; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36	31	45	48	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302010
C	Sun, X; Choi, J; Chen, CY; Wang, NG; Venkataramani, S; Srinivasan, V; Cui, XD; Zhang, W; Gopalakrishnan, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Xiao; Choi, Jungwook; Chen, Chia-Yu; Wang, Naigang; Venkataramani, Swagath; Srinivasan, Vijayalakshmi; Cui, Xiaodong; Zhang, Wei; Gopalakrishnan, Kailash			Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reducing the numerical precision of data and computation is extremely effective in accelerating deep learning training workloads. Towards this end, 8-bit floating point representations (FP8) were recently proposed for DNN training. However, its applicability was only demonstrated on a few selected models and significant degradation is observed when popular networks such as MobileNet and Transformer are trained using FP8. This degradation is due to the inherent precision requirement difference in the forward and backward passes of DNN training. Using theoretical insights, we propose a hybrid FP8 (HFP8) format and DNN end-to-end distributed training procedure. We demonstrate, using HFP8, the successful training of deep learning models across a whole spectrum of applications including Image Classification, Object Detection, Language and Speech without accuracy degradation. Finally, we demonstrate that, by using the new 8 bit format, we can directly quantize a pre-trained model down to 8-bits without losing accuracy by simply fine-tuning batch normalization statistics. These novel techniques enable a new generations of 8-bit hardware that are robust for building and deploying neural network models.	[Sun, Xiao; Choi, Jungwook; Chen, Chia-Yu; Wang, Naigang; Venkataramani, Swagath; Srinivasan, Vijayalakshmi; Cui, Xiaodong; Zhang, Wei; Gopalakrishnan, Kailash] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Choi, Jungwook] IBM Corp, Yorktown Hts, NY USA; [Choi, Jungwook] Hanyang Univ, Elect Engn Dept, Seoul, South Korea	International Business Machines (IBM); International Business Machines (IBM); Hanyang University	Sun, X (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	xsun@us.ibm.com; cchen@us.ibm.com; nwang@us.ibm.com; swagath.venkataramani@us.ibm.com; viji@us.ibm.com; cuix@us.ibm.com; weiz@us.ibm.com; kailash@us.ibm.com	Venkataramani, Swagath/AAA-9473-2022					Alistarh D, 2018, ADV NEUR IN, V31; Amodei D, 2016, PR MACH LEARN RES, V48; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Banner R, 2018, ARXIV180511046; Choi J., 2019, SYSML; Cui Xiaodong, 2017, ARXIV171006937; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; HIGHAM NJ, 1993, SIAM J SCI COMPUT, V14, P783, DOI 10.1137/0914050; Jain Sambhav R, 2019, ARXIV190308066; Jouppi NP, 2017, 44TH ANNUAL INTERNATIONAL SYMPOSIUM ON COMPUTER ARCHITECTURE (ISCA 2017), P1, DOI 10.1145/3079856.3080246; Lecun Y, 2019, ISSCC DIG TECH PAP I, V62, P12, DOI 10.1109/ISSCC.2019.8662396; Li H, 2018, ADV NEUR IN, V31; Li H, 2017, ADV NEUR IN, V30; Lin Po-Chen, 2019, IEEE J EMERGING SELE; Micikevicius Paulius, 2017, ARXIV171003740; Mishra Asit, 2017, ARXIV170901134; Sakr C, 2017, PR MACH LEARN RES, V70; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Santurkar S, 2018, ADV NEUR IN, V31; Sheng T, 2018, 2018 1ST WORKSHOP ON ENERGY EFFICIENT MACHINE LEARNING AND COGNITIVE COMPUTING FOR EMBEDDED APPLICATIONS (EMC2), P14, DOI 10.1109/EMC2.2018.00011; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang N., 2018, PROC C WORKSHOP NEUR, P1; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu SH, 2018, 2018 52ND ANNUAL CONFERENCE ON INFORMATION SCIENCES AND SYSTEMS (CISS), DOI 10.1109/CISS.2018.8362280; Yang GD, 2019, PR MACH LEARN RES, V97; Zhang DQ, 2018, LECT NOTES COMPUT SC, V11212, P373, DOI 10.1007/978-3-030-01237-3_23; Zhao R, 2019, PR MACH LEARN RES, V97	30	45	45	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304086
C	Zhang, HC; Wang, JY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Haichao; Wang, Jianyu			Defense Against Adversarial Attacks Using Feature Scattering-based Adversarial Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a feature scattering-based adversarial training approach for improving model robustness against adversarial attacks. Conventional adversarial training approaches leverage a supervised scheme (either targeted or non-targeted) in generating attacks for training, which typically suffer from issues such as label leaking as noted in recent works. Differently, the proposed approach generates adversarial images for training through feature scattering in the latent space, which is unsupervised in nature and avoids label leaking. More importantly, this new approach generates perturbed images in a collaborative fashion, taking the inter-sample relationships into consideration. We conduct analysis on model robustness and demonstrate the effectiveness of the proposed approach through extensively experiments on different datasets compared with state-of-the-art approaches. Code is available: https://github.com/Haichao-Zhang/FeatureScatter.	[Zhang, Haichao] Horizon Robot, Beijing, Peoples R China; [Zhang, Haichao; Wang, Jianyu] Baidu Res, Beijing, Peoples R China	Baidu	Zhang, HC (corresponding author), Horizon Robot, Beijing, Peoples R China.	hczhang1@gmail.com; wjyouch@gmail.com						Arjovsky M, 2017, PR MACH LEARN RES, V70; Athalye A, 2018, PR MACH LEARN RES, V80; Bard J. F., 1998, PRACTICAL BILEVEL OP; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Biggio Battista, 2017, ABS171203141 CORR; Brendel W., 2018, PROC 6 INT C LEARN R; Brown Tom B, 2017, ARXIV171209665; Carlini N., 2018, IEEE S SEC PRIV WORK; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, ADV NEUR IN, V30; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Dempe S., 2015, BILEVEL PROGRAMMING, DOI [10.1007/978-3-662-45827-3., DOI 10.1007/978-3-662-45827-3]; Draxler F, 2018, PR MACH LEARN RES, V80; Dubey A., 2019, ABS190301612 CORR; Engstrom L, 2019, PR MACH LEARN RES, V97; Etmann C, 2019, PR MACH LEARN RES, V97; Fawzi A, 2018, PROC CVPR IEEE, P3762, DOI 10.1109/CVPR.2018.00396; Genevay A, 2017, ARXIV170601807; Genevay A, 2018, PR MACH LEARN RES, V84; Goldberger J, 2004, ADV NEURAL INF PROCE, V17, P513, DOI DOI 10.5555/2976040.2976105; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo Chuan, 2018, ICLR; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Ilyas A., 2017, ABS171209196 CORR; Ilyas Andrew, 2019, ICLR; Jacobsen Joern-Henrik, 2019, ICLR; Jha S., 2015, ABS151107528 CORR; K. Eykholt, 2018, 12 USENIX WORKSHOP O; Kannan Harini, 2018, ARXIV180306373; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Lindqvist B., 2018, ABS181203405 CORR; Liu XQ, 2018, LECT NOTES COMPUT SC, V11211, P381, DOI 10.1007/978-3-030-01234-2_23; Madry Aleksander, 2017, ARXIV; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Miyato T., 2017, ABS170403976 CORR; Moosavi-Dezfooli SM, 2017, PROC CVPR IEEE, P86, DOI 10.1109/CVPR.2017.17; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Papernot Nicolas, 2017, ABS170403453 CORR; Park S., 2018, IEEE C COMP VIS PATT; Peyre G., 2018, ARXIV180300567; Prakash A, 2018, PROC CVPR IEEE, P8571, DOI 10.1109/CVPR.2018.00894; Rolet A., 2016, INT C ART INT STAT; Salakhutdinov Ruslan, 2007, J MACHINE LEARNING R, P412, DOI DOI 10.1109/ICCV.2017.74; Salimans T., 2018, ARXIV180305573; Samangouei Pouya, 2018, ARXIV180506605; Saul LK, 2004, J MACH LEARN RES, V4, P119, DOI 10.1162/153244304322972667; Schmidt L, 2018, ADV NEUR IN, V31; Song Y, 2017, ARXIV171010766; Su J., 2017, ABS171008864 CORR; Tanay T., 2016, ARXIV PREPRINT ARXIV; Tramer Florian, 2018, INT C LEARN REPR ICL; Villani C., 2008, OPTIMAL TRANSPORT OL; Wang J, 2019, IEEE I CONF COMP VIS, P8200, DOI 10.1109/ICCV.2019.00829; Xiao C., INT JOINT C ART INT; Xie C., 2018, ARXIV181203411; Xie CY, 2018, IEEE PHOTONICS J, V10, DOI 10.1109/JPHOT.2018.2809731; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xie Y., 2018, ARXIV180204307; Yan Z., 2018, ADV NEURAL INFORM PR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhang H., 2019, ABS190710737 CORR; Zhang HC, 2019, IEEE I CONF COMP VIS, P421, DOI 10.1109/ICCV.2019.00051; Zhang Han, 2019, ICLR	72	45	47	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301078
C	Gueguen, L; Sergeev, A; Kadlec, B; Liu, R; Yosinski, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gueguen, Lionel; Sergeev, Alex; Kadlec, Ben; Liu, Rosanne; Yosinski, Jason			Faster Neural Networks Straight from JPEG	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RETRIEVAL; IMAGE	The simple, elegant approach of training convolutional neural networks (CNNs) directly from RGB pixels has enjoyed overwhelming empirical success. But could more performance be squeezed out of networks by using different input representations? In this paper we propose and explore a simple idea: train CNNs directly on the blockwise discrete cosine transform (DCT) coefficients computed and available in the middle of the JPEG codec. Intuitively, when processing JPEG images using CNNs, it seems unnecessary to decompress a blockwise frequency representation to an expanded pixel representation, shuffle it from CPU to GPU, and then process it with a CNN that will learn something similar to a transform back to frequency representation in its first layers. Why not skip both steps and feed the frequency domain into the network directly? In this paper, we modify libjpeg to produce DCT coefficients directly, modify a ResNet-50 network to accommodate the differently sized and strided input, and evaluate performance on ImageNet. We find networks that are both faster and more accurate, as well as networks with about the same accuracy but 1.77x faster than ResNet-50.	[Gueguen, Lionel; Sergeev, Alex; Kadlec, Ben] Uber, San Francisco, CA 94105 USA; [Liu, Rosanne; Yosinski, Jason] Uber AI Labs, San Francisco, CA USA	Uber Technologies, Inc.	Gueguen, L (corresponding author), Uber, San Francisco, CA 94105 USA.	lgueguen@uber.com; asergeev@uber.com; bkadlec@uber.com; rosanne@uber.com; yosinskig@uber.com		Yosinski, Jason/0000-0002-4701-0199				Adler A., 2016, ARXIV161009615; [Anonymous], 2018, JPEG LIB TURB; Brock Andrew, 2017, 5 INT C LEARN REPR T; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dan Fu, 2016, USING COMPRESSION SP; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Feng GC, 2003, PATTERN RECOGN, V36, P977, DOI 10.1016/S0031-3203(02)00114-0; Gueguen L, 2008, IEEE T KNOWL DATA EN, V20, P562, DOI 10.1109/TKDE.2007.190718; Hafed ZM, 2001, INT J COMPUT VISION, V43, P167, DOI 10.1023/A:1011183429707; Han Song, 2015, CORR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hudson G, 2017, IEEE MULTIMEDIA, V24, P96, DOI 10.1109/MMUL.2017.38; Iandola F.N., 2016, ARXIV; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu SZ, 2002, IEEE T CIRC SYST VID, V12, P1139, DOI 10.1109/TCSVT.2002.806819; Mandal MK, 1999, IMAGE VISION COMPUT, V17, P513, DOI 10.1016/S0262-8856(98)00143-7; Mnih V., 2013, ARXIV E PRINTS; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sergeev A, 2017, MEET HOROVOD UBERS O; Shen B, 1996, P SOC PHOTO-OPT INS, V2670, P404, DOI 10.1117/12.234779; Srom Martin, 2018, GPUJP; Torfason R., 2018, PROC INT C LEARN REP, P1; Ulicny R., 2017, IRMACH VIS IMAGE PRO, P44; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Yosinski J., 2015, ICML DEEP LEARN WORK; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	27	45	45	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303089
C	Ono, Y; Trulls, E; Fua, P; Yi, KM		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ono, Yuki; Trulls, Eduard; Fua, Pascal; Yi, Kwang Moo			LF-Net: Learning Local Features from Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present a novel deep architecture and a training strategy to learn a local feature pipeline from scratch, using collections of images without the need for human supervision. To do so we exploit depth and relative camera pose cues to create a virtual target that the network should achieve on one image, provided the outputs of the network for the other image. While this process is inherently non-differentiable, we show that we can optimize the network in a two-branch setup by confining it to one branch, while preserving differentiability in the other. We train our method on both indoor and outdoor datasets, with depth data from 3D sensors for the former, and depth estimates from an off-the-shelf Structure-from-Motion solution for the latter. Our models outperform the state of the art on sparse feature matching on both datasets, while running at 60+ fps for QVGA images.	[Ono, Yuki] Sony Imaging Prod & Solut Inc, Atsugi, Kanagawa, Japan; [Trulls, Eduard; Fua, Pascal] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Yi, Kwang Moo] Univ Victoria, Visual Comp Grp, Victoria, BC, Canada	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; University of Victoria	Ono, Y (corresponding author), Sony Imaging Prod & Solut Inc, Atsugi, Kanagawa, Japan.	yuki.ono@sony.com; eduard.trulls@epfl.ch; pascal.fua@epfl.ch; kyi@uvic.ca	Yi, Kwang Moo/C-2612-2016	Yi, Kwang Moo/0000-0001-9036-3822	Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant "Deep Visual Geometry Machines" [RGPIN-2018-03788]	Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant "Deep Visual Geometry Machines"(Natural Sciences and Engineering Research Council of Canada (NSERC))	This work was partially supported by the Natural Sciences and Engineering Research Council of Canada (NSERC) Discovery Grant "Deep Visual Geometry Machines" (RGPIN-2018-03788), and by systems supplied by Compute Canada.	Alcantarilla P. F., 2012, ECCV; Alcantarilla P. F., 2013, BMVC; Balntas V., 2017, CVPR; Balntas V., 2016, ARXIV PREPRINT; Bay H., 2006, ECCV; Bay H, 2006, LECT NOTES COMPUT SC, V3951, P404, DOI 10.1007/11744023_32; Bian JW, 2017, PROC CVPR IEEE, P2828, DOI 10.1109/CVPR.2017.302; Brown M., 2011, PAMI; Chapelle O, 2010, INFORM RETRIEVAL, V13, P216, DOI 10.1007/s10791-009-9110-3; Dai A., 2017, CVPR; Detone D., 2018, CVPR WORKSH DEEP LEA; Engel J., 2014, ECCV; Fua P., 2016, ECCV; Hartley Richard, 2000, MULTIPLE VIEW GEOMET, V7, P8; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heinly J., 2015, CVPR; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Kavukcuoglu K, 2015, ADV NEURAL INF PROCE, P2017; Keller M., 2018, CVPR; Kingma D.P., 2015, INT C LEARN REPR, P1; Kokkinos I, 2017, CVPR; Lenc Karel, 2016, ECCV; Leung T., 2015, CVPR; Lowe D., 2004, IJCV, V20; Mikolajczyk K, 2005, IEEE T PATTERN ANAL, V27, P1615, DOI 10.1109/TPAMI.2005.188; Mishchuk A., 2017, NIPS; Mishkin Dmytro, 2018, ECCV, V1, P2; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mukherjee D, 2015, MACH VISION APPL, V26, P443, DOI 10.1007/s00138-015-0679-9; Mur-Artal R, 2015, IEEE T ROBOT, V31, P1147, DOI 10.1109/TRO.2015.2463671; Rosten E., 2006, ECCV; Rosten E, 2010, IEEE T PATTERN ANAL, V32, P105, DOI 10.1109/TPAMI.2008.275; Rublee E., 2011, ICCV; Savinov N., 2017, CVPR; Schonberger J. L., 2017, CVPR; Schonberger Johannes Lutz, 2016, CVPR; Simo-Serra E., 2015, ICCV; Simonyan K., 2014, PAMI; Strecha C., 2012, PAMI, V34; Thomee B., 2016, CACM; Tian Y., 2017, PAMI, P661, DOI DOI 10.1109/TPAMI.2009.77; Tian Yurun, 2017, CVPR; Ulyanov D., 2017, CVPR; Ummenhofer B., 2017, CVPR; Verdie Y., 2015, CVPR; Vijayanarasimhan S., 2017, SFM NET LEARNING STR; Wei X., 2018, CVPR; Wu C., 2013, 3DV; Yi K.M., 2018, CVPR; Yi K. M., 2016, CVPR; Zagoruyko S., 2015, CVPR; Zamir A. R., 2016, ECCV; Zhou T., 2017, CVPR	53	45	45	3	13	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000071
C	Wang, XJ; Zhang, R; Sun, Y; Qi, JZ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wang, Xiaojie; Zhang, Rui; Sun, Yu; Qi, Jianzhong			KDGAN: Knowledge Distillation with Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Knowledge distillation (KD) aims to train a lightweight classifier suitable to provide accurate inference with constrained resources in multi-label learning. Instead of directly consuming feature-label pairs, the classifier is trained by a teacher, i.e., a high-capacity model whose training may he resource-hungry. The accuracy of the classifier trained this way is usually suboptimal because it is difficult to learn the true data distribution from the teacher. An alternative method is to adversarially train the classifier against a discriminator in a two-player game akin to generative adversarial networks (GAN), which can ensure the classifier to learn the true data distribution at the equilibrium of this game. However, it may take excessively long time for such a two-player game to reach equilibrium due to high-variance gradient updates. To address these limitations, we propose a three-player game named KDGAN consisting of a classifier, a teacher, and a discriminator. The classifier and the teacher learn from each other via distillation losses and are adversarially trained against the discriminator via adversarial losses. By simultaneously optimizing the distillation and adversarial losses, the classifier will learn the true data distribution at the equilibrium. We approximate the discrete distribution learned by the classifier (or the teacher) with a concrete distribution. From the concrete distribution, we generate continuous samples to obtain low-variance gradient updates, which speed up the training. Extensive experiments using real datasets confirm the superiority of KDGAN in both accuracy and training speed.	[Wang, Xiaojie; Zhang, Rui; Qi, Jianzhong] Univ Melbourne, Melbourne, Vic, Australia; [Sun, Yu] Twitter Inc, San Francisco, CA USA	University of Melbourne; Twitter, Inc.	Zhang, R (corresponding author), Univ Melbourne, Melbourne, Vic, Australia.	xiaojiew94@gmail.com; rui.zhang@unimelb.edu.au; ysun@twitter.com; jianzhong.qi@unimelb.edu.au	QI, JIANZHONG/P-7112-2015	QI, JIANZHONG/0000-0001-6501-9050	Australian Research Council [FT120100832, DP180102050]	Australian Research Council(Australian Research Council)	This work is supported by Australian Research Council Future Fellowship Project FT120100832 and Discovery Project DP180102050. We thank the anonymous reviewers for their feedback on the paper. We have incorporated responses to reviewers' comments in the paper.	Abadi Martin, 2016, arXiv; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arjovsky M., 2017, ARXIV170107875; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Bottou L., 2016, ARXIV160604838; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Chen Guobin, 2017, NEURIPS; Chen L., 2012, IEEE T MULTIMEDIA; CHENG Weiwei, 2010, ICML; Chongxuan L., 2017, NEURIPS; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Feizi S., 2017, ARXIV PREPRINT ARXIV; Gan Z., 2017, NEURIPS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grbovic M., 2013, MACHINE LEARNING; Guillaumin M, 2009, IEEE I CONF COMP VIS, P309, DOI 10.1109/ICCV.2009.5459266; Gumbel E. J., 1954, STAT THEORY EXTREME; Gupta S, 2016, PROC CVPR IEEE, P2827, DOI 10.1109/CVPR.2016.309; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton G., 2015, NIPS WORKSH, P1; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jang E., 2017, ICLR; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li X., 2013, ACMMM; Lopez-Paz David, 2016, INT C LEARN REPR ICL, DOI DOI 10.1109/PAC.2017.13.ARXIV:1511.0; Maddison C. J., 2014, NIPS; Maddison Chris J, 2017, ICLR; Makadia A., 2010, IJCV; Metz Luke, 2017, ICLR; Mikolov T., 2013, ARXIV; Pechyony D., 2010, NEURIPS; Romero Adriana, 2014, ARXIV14126550; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Salimans T, 2016, ADV NEUR IN, V29; Sau BB, 2016, ARXIV PREPRINT ARXIV; Su J.-C., 2016, ARXIV160400433; Sun Y., 2017, TOIS; Sun Y., 2016, WWW; Sun Y., 2016, SIGKDD; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Tucker G., 2017, NEURIPS; Vapnik V, 2015, J MACH LEARN RES, V16, P2023; Vapnik V, 2009, NEURAL NETWORKS, V22, P544, DOI 10.1016/j.neunet.2009.06.042; Wang J, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P515, DOI 10.1145/3077136.3080786; Wang X., 2018, PAKDD, P597, DOI DOI 10.1007/978-3-319-93040-447; Xu Zheng, 2017, ARXIV170900513; Yu LQ, 2017, AAAI CONF ARTIF INTE, P66; Zhang M.-L., 2014, TKDE; Zhang YZ, 2017, PR MACH LEARN RES, V70; Zhang Yizhe, 2016, NIPS WORKSH ADV TRAI; Zhao J., 2017, ICLR	56	45	47	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300072
C	Cisse, M; Adi, Y; Neverova, N; Keshet, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Cisse, Moustapha; Adi, Yossi; Neverova, Natalia; Keshet, Joseph			Houdini: Fooling Deep Structured Visual and Speech Recognition Models with Adversarial Examples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CLASSIFICATION	Generating adversarial examples is a critical step for evaluating and improving the robustness of learning machines. So far, most existing methods only work for classification and are not designed to alter the true performance measure of the problem at hand. We introduce a novel flexible approach named Houdini for generating adversarial examples specifically tailored for the final performance measure of the task considered, be it combinatorial and non-decomposable. We successfully apply Houdini to a range of applications such as speech recognition, pose estimation and semantic segmentation. In all cases, the attacks based on Houdini achieve higher success rate than those based on the traditional surrogates used to train the models while using a less perceptible adversarial perturbation.	[Cisse, Moustapha; Neverova, Natalia] Facebook AI Res, Menlo Pk, CA 94025 USA; [Adi, Yossi; Keshet, Joseph] Bar Ilan Univ, Ramat Gan, Israel	Facebook Inc; Bar Ilan University	Cisse, M (corresponding author), Facebook AI Res, Menlo Pk, CA 94025 USA.	moustaphacisse@fb.com; yossiadidrum@gmail.com; nneverova@fb.com; jkeshet@cs.biu.ac.il						Amodei D, 2016, PR MACH LEARN RES, V48; Amodei Dario, 2015, ARXIV151202595; Andriluka M, 2014, PROC CVPR IEEE, P3686, DOI 10.1109/CVPR.2014.471; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Bulat A, 2016, LECT NOTES COMPUT SC, V9911, P717, DOI 10.1007/978-3-319-46478-7_44; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Cords M., 2016, CVPR; Do C., 2008, ADV NEURAL INFORM PR, V22; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fawzi A., 2015, ARXIV150202590; Fawzi A., 2016, ADV NEURAL INFORM PR, P1632; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Graves A., 2006, P 23 INT C MACH LEAR, P369; Hazan Tamir, 2010, NEURIPS; He K., 2016, ARXIV170306870; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Keshet J., 2011, P 24 INT C NEUR INF, P2205; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kurakin A., 2016, ARXIV PREPRINT ARXIV; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Levenshtein V. I, 1966, SOV PHYS DOKL, V10, P707; McAllester D., 2010, ADV NEURAL INFORM PR, V24; Moosavi-Dezfooli S., 2015, ARXIV151104599; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Papernot N., 2016, ARXIV160202697; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Tabacof Pedro, 2015, ARXIV151005328; Tewari A, 2007, J MACH LEARN RES, V8, P1007; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xie C., 2017, CORR; Yu F., 2016, P ICLR 2016	38	45	48	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407007
C	Donti, PL; Amos, B; Kolter, JZ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Donti, Priya L.; Amos, Brandon; Kolter, J. Zico			Task-based End-to-end Model Learning in Stochastic Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					With the increasing popularity of machine learning techniques, it has become common to see prediction algorithms operating within some larger process. However, the criteria by which we train these algorithms often differ from the ultimate criteria on which we evaluate them. This paper proposes an end-to-end approach for learning probabilistic machine learning models in a manner that directly captures the ultimate task-based objective for which they will be used, within the context of stochastic programming. We present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling task, and a real-world energy storage arbitrage task. We show that the proposed approach can outperform both traditional modeling and purely black-box policy optimization approaches in these applications.	[Donti, Priya L.] Carnegie Mellon Univ, Dept Comp Sci, Dept Engr & Publ Policy, Pittsburgh, PA 15213 USA; [Amos, Brandon; Kolter, J. Zico] Carnegie Mellon Univ, Dept Comp Sci, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Donti, PL (corresponding author), Carnegie Mellon Univ, Dept Comp Sci, Dept Engr & Publ Policy, Pittsburgh, PA 15213 USA.	pdonti@cs.cmu.edu; bamos@cs.cmu.edu; zkolter@cs.cmu.edu		Donti, Priya/0000-0002-8503-7464	National Science Foundation Graduate Research Fellowship Program [DGE1252522]; Department of Energy Computational Science Graduate Fellowship	National Science Foundation Graduate Research Fellowship Program(National Science Foundation (NSF)); Department of Energy Computational Science Graduate Fellowship(United States Department of Energy (DOE))	This material is based upon work supported by the National Science Foundation Graduate Research Fellowship Program under Grant No. DGE1252522, and by the Department of Energy Computational Science Graduate Fellowship.	Amos B., 2016, ARXIV160907152; Amos B., 2017, ARXIV170300443; [Anonymous], 2016, MULTIOBJECTIVE DEEP; [Anonymous], 2005, ADV NEURAL INFO PROC; Bansal S., 2017, ARXIV170309260; Bengio Y, 1997, INT J NEURAL SYST, V8, P433, DOI 10.1142/S0129065797000422; Boggs P., 1995, ACTA NUMER, V4, P1, DOI 10.1017/S0962492900002518; Buzacott J., 1993, PRENTICE HALL INT SE; Catanzaro B., 2015, ARXIV151202595; Elmachtoub A. N., 2017, ARXIV171008005; Finn C, 2017, ARXIV170303400; Gould S., 2016, ARXIV160705447; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Harada K, 2006, GECCO 2006: GENETIC AND EVOLUTIONARY COMPUTATION CONFERENCE, VOL 1 AND 2, P659; Hazan Tamir, 2010, NEURIPS; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kingma D.P, P 3 INT C LEARNING R; Levine S, 2016, J MACH LEARN RES, V17; Linderoth J, 2006, ANN OPER RES, V142, P215, DOI 10.1007/s10479-006-6169-8; ROCKAFELLAR RT, 1991, MATH OPER RES, V16, P119, DOI 10.1287/moor.16.1.119; Shapiro A., 2007, TUTORIAL STOCHASTIC; Song Y, 2016, PR MACH LEARN RES, V48; Stoyanov Veselin, 2011, P AISTATS; Tamar A, 2016, ADV NEURAL INFORM PR, P2146; Thomas RW, 2006, IEEE COMMUN MAG, V44, P51, DOI 10.1109/MCOM.2006.273099; Wallace SW, 2003, HDBK OPER R, V10, P637; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang T, 2012, INT C PATT RECOG, P3304; Wiering MA, 2014, 2014 IEEE SYMPOSIUM ON ADAPTIVE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING (ADPRL), P111; Ziemba William T, 2006, STOCHASTIC OPTIMIZAT, V1	33	45	44	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405055
C	Jiang, Z; Balu, A; Hegde, C; Sarkar, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Jiang, Zhanhong; Balu, Aditya; Hegde, Chinmay; Sarkar, Soumik			Collaborative Deep Learning in Fixed Topology Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DISTRIBUTED OPTIMIZATION	There is significant recent interest to parallelize deep learning algorithms in order to handle the enormous growth in data and model sizes. While most advances focus on model parallelization and engaging multiple computing agents via using a central parameter server, aspect of data parallelization along with decentralized computation has not been explored sufficiently. In this context, this paper presents a new consensus-based distributed SGD (CDSGD) (and its momentum variant, CDMSGD) algorithm for collaborative deep learning over fixed topology networks that enables data parallelization as well as decentralized computation. Such a framework can be extremely useful for learning agents with access to only local/private data in a communication constrained environment. We analyze the convergence properties of the proposed algorithm with strongly convex and nonconvex objective functions with fixed and diminishing step sizes using concepts of Lyapunov function construction. We demonstrate the efficacy of our algorithms in comparison with the baseline centralized SGD and the recently proposed federated averaging algorithm (that also enables data parallelism) based on benchmark datasets such as MNIST, CIFAR-10 and CIFAR-100.	[Jiang, Zhanhong; Balu, Aditya; Sarkar, Soumik] Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA; [Hegde, Chinmay] Iowa State Univ, Dept Elect & Comp Engn, Ames, IA USA	Iowa State University; Iowa State University	Jiang, Z (corresponding author), Iowa State Univ, Dept Mech Engn, Ames, IA 50011 USA.	zhjiang@iastate.edu; baditya@iastate.edu; chinmay@iastate.edu; soumiks@iastate.edu	Balu, Aditya/AAB-6953-2020; SARKAR, SOUMIK/T-9707-2018; Jeong, Yongwook/N-7413-2016	BALU, ADITYA/0000-0003-2005-2548	USDA-NIFA [2017-67021-25965]; National Science Foundation [CNS-1464279, CCF-1566281]	USDA-NIFA(United States Department of Agriculture (USDA)); National Science Foundation(National Science Foundation (NSF))	This paper is based upon research partially supported by the USDA-NIFA under Award no. 2017-67021-25965, the National Science Foundation under Grant No. CNS-1464279 and No. CCF-1566281. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the authors and do not necessarily reflect the views of the funding agencies.	Abadi M, 2015, P 12 USENIX S OPERAT; Blot Michael, 2016, ARXIV161109726; Bottou L., 2016, ARXIV160604838; Brendan McMahan H., 2016, ARXIV160205629; Chilimbi T., 2014, OSDI, P571; Choi HL, 2010, AUTOMATICA, V46, P1266, DOI 10.1016/j.automatica.2010.05.004; De S, 2016, IEEE DATA MINING, P111, DOI [10.1109/ICDM.2016.0022, 10.1109/ICDM.2016.177]; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Gupta S., 2015, ARXIV150904210; Hajinezhad Davood, ZENITH ZEROTH ORDER; Jha DK, 2016, INT J CONTROL, V89, P984, DOI 10.1080/00207179.2015.1110754; Jin P.H., 2016, ARXIV161104581; Lan G., 2017, ARXIV170103961; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liu C, 2017, MEAS SCI TECHNOL, V28, DOI 10.1088/1361-6501/28/1/014011; Mukherjee K, 2011, AUTOMATICA, V47, P185, DOI 10.1016/j.automatica.2010.10.031; Nedic A, 2016, IEEE T AUTOMAT CONTR, V61, P3936, DOI 10.1109/TAC.2016.2529285; Nedic A, 2015, IEEE T AUTOMAT CONTR, V60, P601, DOI 10.1109/TAC.2014.2364096; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Nesterov Y., 2018, APPL OPTIMIZATION; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Ram SS, 2012, OPTIM METHOD SOFTW, V27, P71, DOI 10.1080/10556788.2010.511669; Scaman K, 2017, PR MACH LEARN RES, V70; Strom N., 2015, INTERSPEECH, V7, P10; Su H., 2015, ARXIV150701239; Yuan K., 2013, ARXIV13107063; Zeng Jinshan, 2016, ARXIV160805766; Zhang Chiyuan, 2016, ARXIV161103530; Zhang S., 2015, P ADV NEURAL INFORM, P685, DOI DOI 10.1145/3207677.3277958; Zhang W., 2015, ARXIV PREPRINT ARXIV	30	45	45	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405095
C	Russell, C; Kusner, MJ; Loftus, JR; Silva, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Russell, Chris; Kusner, Matt J.; Loftus, Joshua R.; Silva, Ricardo			When Worlds Collide: Integrating Different Counterfactual Assumptions in Fairness	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RISK	Machine learning is now being used to make crucial decisions about people's lives. For nearly all of these decisions there is a risk that individuals of a certain race, gender, sexual orientation, or any other subpopulation are unfairly discriminated against. Our recent method has demonstrated how to use techniques from counterfactual inference to make predictions fair across different subpopulations. This method requires that one provides the causal model that generated the data at hand. In general, validating all causal implications of the model is not possible without further assumptions. Hence, it is desirable to integrate competing causal models to provide counterfactually fair decisions, regardless of which causal "world" is the correct one. In this paper, we show how it is possible to make predictions that are approximately fair with respect to multiple possible causal models at once, thus mitigating the problem of exact causal specification. We frame the goal of learning a fair classifier as an optimization problem with fairness constraints entailed by competing causal explanations. We show how this optimization problem can be efficiently solved using gradient-based methods. We demonstrate the flexibility of our model on two real-world fair classification problems. We show that our model can seamlessly balance fairness in multiple worlds with prediction accuracy.	[Russell, Chris; Kusner, Matt J.; Loftus, Joshua R.; Silva, Ricardo] Alan Turing Inst, London, England; [Russell, Chris] Univ Surrey, Guildford, Surrey, England; [Kusner, Matt J.] Univ Warwick, Coventry, W Midlands, England; [Loftus, Joshua R.] NYU, New York, NY 10003 USA; [Silva, Ricardo] UCL, London, England	University of Surrey; University of Warwick; New York University; University of London; University College London	Russell, C (corresponding author), Alan Turing Inst, London, England.; Russell, C (corresponding author), Univ Surrey, Guildford, Surrey, England.	crussell@turing.ac.uk; mkusner@turing.ac.uk; loftus@nyu.edu; ricardo@stats.ucl.ac.uk	Jeong, Yongwook/N-7413-2016	Russell, Chris/0000-0003-1665-1759; Loftus, Joshua/0000-0002-2905-1632	Alan Turing Institute under the EPSRC [EP/N510129/1]; EPSRC Platform Grant [EP/P022529/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Platform Grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by The Alan Turing Institute under the EPSRC grant EP/N510129/1. CR acknowledges additional support under the EPSRC Platform Grant EP/P022529/1.	Angwin J., 2016, MACHINE BIAS; [Anonymous], 2016, COMPAS RISK SCALES D; Berk R., 2017, ARXIV PREPRINT ARXIV; Brennan T, 2009, CRIM JUSTICE BEHAV, V36, P21, DOI 10.1177/0093854808326545; Dawid AP, 2000, J AM STAT ASSOC, V95, P407, DOI 10.2307/2669377; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Khandani AE, 2010, J BANK FINANC, V34, P2767, DOI 10.1016/j.jbankfin.2010.06.001; Kirkpatrick K, 2017, COMMUN ACM, V60, P21, DOI 10.1145/3022181; Kleinberg Jon, 2016, P 8 INN THEOR COMP S; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V31; Kutnowski M., 2017, J COMMUN SAF WELL BE, V2, P13, DOI [10.35502/jcswb.36, DOI 10.35502/JCSWB.36]; Larson Jeff, 2016, PROPUBLICA 5 2016; Lopez-Paz David, 2016, ARXIV160703300; Pearl J, 2016, J CAUSAL INFERENCE, V4, DOI 10.1515/jci-2016-0021; Pearsall B, 2010, NIJ J, P16; Richardson T.S., 2013, 128 U WASH CTR STAT, V128; Upchurch P., 2016, ARXIV161105507; Wightman, 1998, LSAC RES REPORT SERI; Zafar Muhammad Bilal, 2016, ARXIV161008452	20	45	45	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406047
C	Xiao, S; Farajtabar, M; Ye, XJ; Yang, JC; Song, L; Zha, HY		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xiao, Shuai; Farajtabar, Mehrdad; Ye, Xiaojing; Yang, Junchi; Song, Le; Zha, Hongyuan			Wasserstein Learning of Deep Generative Point Process Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Point processes are becoming very popular in modeling asynchronous sequential data due to their sound mathematical foundation and strength in modeling a variety of real-world phenomena. Currently, they are often characterized via intensity function which limits model's expressiveness due to unrealistic assumptions on its parametric form used in practice. Furthermore, they are learned via maximum likelihood approach which is prone to failure in multi-modal distributions of sequences. In this paper, we propose an intensity-free approach for point processes modeling that transforms nuisance processes to a target one. Furthermore, we train the model using a likelihood-free leveraging Wasserstein distance between point processes. Experiments on various synthetic and real-world data substantiate the superiority of the proposed point process model over conventional ones.	[Xiao, Shuai] Shanghai Jiao Tong Univ, Shanghai, Peoples R China; [Farajtabar, Mehrdad; Song, Le; Zha, Hongyuan] Georgia Inst Technol, Coll Comp, Atlanta, GA 30332 USA; [Ye, Xiaojing] Georgia State Univ, Sch Math, Atlanta, GA 30303 USA; [Yang, Junchi] IBM Res China, Beijing, Peoples R China; [Song, Le] Ant Financial, Xihu, Peoples R China	Shanghai Jiao Tong University; University System of Georgia; Georgia Institute of Technology; University System of Georgia; Georgia State University; International Business Machines (IBM)	Xiao, S (corresponding author), Shanghai Jiao Tong Univ, Shanghai, Peoples R China.	benjaminforever@sjtu.edu.cn; mehrdad@gatech.edu; xye@gsu.edu; yanjc@cn.ibm.com; lsong@cc.gatech.edu; zha@cc.gatech.edu	Jeong, Yongwook/N-7413-2016; Xiao, Shuai/AAQ-5932-2020		NSF [IIS-1639792, IIS-1218749, IIS-1717916, CMMI-1745382, CNS-1704701]; NIH BIGDATA [1R01GM108341]; ONR [N00014-15-1-2340]; NSFC [61602176]; Intel ISTC; NVIDIA; Amazon AWS; NSF CAREER [IIS-1350983]	NSF(National Science Foundation (NSF)); NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); ONR(Office of Naval Research); NSFC(National Natural Science Foundation of China (NSFC)); Intel ISTC; NVIDIA; Amazon AWS; NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This project was supported in part by NSF (IIS-1639792, IIS-1218749, IIS-1717916, CMMI-1745382), NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF CNS-1704701, ONR N00014-15-1-2340, NSFC 61602176, Intel ISTC, NVIDIA and Amazon AWS.	Aalen OO, 2008, STAT BIOL HEALTH, P1; Arjovsky M., 2017, ARXIV170107875; Arjovsky M., 2016, NIPS 2016 WORKSH ADV, V2017; Bacry E, 2015, MARK MICROSTRUCT LIQ, V1, DOI 10.1142/S2382626615500057; Cuturi M, 2017, PR MACH LEARN RES, V70; Decreusefond L, 2016, ANN PROBAB, V44, P2147, DOI 10.1214/15-AOP1020; Du N, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1555, DOI 10.1145/2939672.2939875; Farajtabar M, 2015, ADV NEUR IN, V28; Farajtabar Mehrdad, 2014, Adv Neural Inf Process Syst, V27; Farajtabar Mehrdad, 2016, NIPS; Ghosh Arnab, 2016, ARXIV160909444; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Gulrajani I, 2017, P NIPS 2017; Hawkes A. G., 1971, BIOMETRIKA; Huszar Ferenc, 2015, ABS151105101 CORR; Isham V., 1979, Stochastic Processes & their Applications, V8, P335, DOI 10.1016/0304-4149(79)90008-5; Kingman J. F. C., 1993, POISSON PROCESSES; Lian WZ, 2015, PR MACH LEARN RES, V37, P2030; Linderman SW, 2014, PR MACH LEARN RES, V32, P1413; Mei HY, 2017, ADV NEUR IN, V30; Mogren Olof, 2016, ARXIV161109904; Nguyen  A., 2016, 161200005 ARXIV; OGATA Y, 1981, IEEE T INFORM THEORY, V27, P23, DOI 10.1109/TIT.1981.1056305; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Schuhmacher D, 2008, ADV APPL PROBAB, V40, P651, DOI 10.1239/aap/1222868180; Theis Lucas, 2015, ARXIV151101844; Vere-Jones, 2003, INTRO THEORY POINT P; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Xiao S., 2017, OINT MODELING EVENT; Xiao Shuai, 2017, AAAI; Xu LZ, 2014, MANAGE SCI, V60, P1392, DOI 10.1287/mnsc.2014.1952; Zhou D., 2005, PROCEEDING ICML 2005, P1028	33	45	46	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403031
C	Wang, P; Shen, XH; Russell, B; Cohen, S; Price, B; Yuille, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Peng; Shen, Xiaohui; Russell, Bryan; Cohen, Scott; Price, Brian; Yuille, Alan			SURGE: Surface Regularized Geometry Estimation from a Single Image	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This paper introduces an approach to regularize 2.5D surface normal and depth predictions at each pixel given a single input image. The approach infers and reasons about the underlying 3D planar surfaces depicted in the image to snap predicted normals and depths to inferred planar surfaces, all while maintaining fine detail within objects. Our approach comprises two components: (i) a four-stream convolutional neural network (CNN) where depths, surface normals, and likelihoods of planar region and planar boundary are predicted at each pixel, followed by (ii) a dense conditional random field (DCRF) that integrates the four predictions such that the normals and depths are compatible with each other and regularized by the planar region and planar boundary information. The DCRF is formulated such that gradients can be passed to the surface normal and depth CNNs via backpropagation. In addition, we propose new planar-wise metrics to evaluate geometry consistency within planar surfaces, which are more tightly related to dependent 3D editing applications. We show that our regularization yields a 30 % relative improvement in planar consistency on the NYU v2 dataset [24].	[Wang, Peng] Univ Calif Los Angeles, Los Angeles, CA 90095 USA; [Shen, Xiaohui; Russell, Bryan; Cohen, Scott; Price, Brian] Adobe Res, San Jose, CA USA; [Yuille, Alan] Johns Hopkins Univ, Baltimore, MD 21218 USA	University of California System; University of California Los Angeles; Adobe Systems Inc.; Johns Hopkins University	Wang, P (corresponding author), Univ Calif Los Angeles, Los Angeles, CA 90095 USA.			Yuille, Alan L./0000-0001-5207-9249	NSF Expedition for Visual Cortex on Silicon NSF [CCF-1317376]; Army Research Office ARO [62250-CS]	NSF Expedition for Visual Cortex on Silicon NSF; Army Research Office ARO	This work is supported by the NSF Expedition for Visual Cortex on Silicon NSF award CCF-1317376 and the Army Research Office ARO 62250-CS.	Adams A, 2010, COMPUT GRAPH FORUM, V29, P753, DOI 10.1111/j.1467-8659.2009.01645.x; Bansal A, 2016, PROC CVPR IEEE, P5965, DOI 10.1109/CVPR.2016.642; Barron J. T., 2015, FAST BILATERAL SOLVE; Barron JT, 2015, PROC CVPR IEEE, P4466, DOI 10.1109/CVPR.2015.7299076; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Eigen D, 2015, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2015.304; Eigen David, 2014, NEURIPS; Guo R., 2013, ICCV; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Hoiem D., 2007, ICCV; Honauer  K., 2015, ICCV; Ikehata S, 2015, IEEE I CONF COMP VIS, P1323, DOI 10.1109/ICCV.2015.156; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kokkinos I., 2016, ICLR; Krahenbuhl P., 2012, NEURAL INFORM PROCES; Krahenbuhl  P., 2012, ECCV; Krahenbuhl P., 2013, ICML; Ladicky  L., 2014, NON TRADITIONAL REF; Li B, 2015, PROC CVPR IEEE, P1119, DOI 10.1109/CVPR.2015.7298715; Liu FY, 2015, PROC CVPR IEEE, P5162, DOI 10.1109/CVPR.2015.7299152; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Schwing AG, 2013, IEEE I CONF COMP VIS, P353, DOI 10.1109/ICCV.2013.51; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Silberman N, 2012, LECT NOTES COMPUT SC, V7576, P746, DOI 10.1007/978-3-642-33715-4_54; Song SR, 2015, PROC CVPR IEEE, P567, DOI 10.1109/CVPR.2015.7298655; Srajer  F., 2014, 2 INT C 3D VIS 3DV 2, V1; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wang P, 2015, PROC CVPR IEEE, P2800, DOI 10.1109/CVPR.2015.7298897; Wang XL, 2015, PROC CVPR IEEE, P539, DOI 10.1109/CVPR.2015.7298652; Xiao JX, 2014, INT J COMPUT VISION, V110, P243, DOI 10.1007/978-3-642-33718-5_48; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Zheng S, 2015, IEEE I CONF COMP VIS, P1529, DOI 10.1109/ICCV.2015.179	32	45	47	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973704027
C	Yu, HF; Rao, N; Dhillon, IS		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Yu, Hsiang-Fu; Rao, Nikhil; Dhillon, Inderjit S.			Temporal Regularized Matrix Factorization for High-dimensional Time Series Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Time series prediction problems are becoming increasingly high-dimensional in modern applications, such as climatology and demand forecasting. For example, in the latter problem, the number of items for which demand needs to be forecast might be as large as 50,000. In addition, the data is generally noisy and full of missing values. Thus, modern applications require methods that are highly scalable, and can deal with noisy data in terms of corruptions or missing values. However, classical time series methods usually fall short of handling these issues. In this paper, we present a temporal regularized matrix factorization (TRMF) framework which supports data-driven temporal learning and forecasting. We develop novel regularization schemes and use scalable matrix factorization methods that are eminently suited for high-dimensional time series data that has many missing values. Our proposed TRMF is highly general, and subsumes many existing approaches for time series analysis. We make interesting connections to graph regularization methods in the context of learning the dependencies in an autoregressive framework. Experimental results show the superiority of TRMF in terms of scalability and prediction quality. In particular, TRMF is two orders of magnitude faster than other methods on a problem of dimension 50,000, and generates better forecasts on real-world datasets such as Wal-mart E-commerce datasets.	[Yu, Hsiang-Fu; Dhillon, Inderjit S.] Univ Texas Austin, Austin, TX 78712 USA; [Rao, Nikhil] Technicolor Res, Issy Les Moulineaux, France	University of Texas System; University of Texas Austin; Technicolor SA	Yu, HF (corresponding author), Univ Texas Austin, Austin, TX 78712 USA.	rofuyu@cs.utexas.edu; nikhilrao86@gmail.com; inderjit@cs.utexas.edu			NSF [CCF-1320746, IIS-1546459, CCF-1564000]	NSF(National Science Foundation (NSF))	This research was supported by NSF grants (CCF-1320746, IIS-1546459, and CCF-1564000) and gifts from Walmart Labs and Adobe. We thank Abhay Jha for the help on Walmart experiments.	Anava O, 2015, PR MACH LEARN RES, V37, P2191; CHEN Z., 2005, 68 RIKEN LAB ADV BRA; Ghahramani Z., 1996, CRGTR962 U TOTR DEP; Han F., 2013, INT C MACH LEARN, P172; Jain P., 2013, ABS13060626 CORR; Kalman RE., 1960, T ASME J BASIC ENG, V82, P35, DOI [10.1115/1.3662552, DOI 10.1115/1.3662552]; Li L, 2011, P 28 INT C MACH LEAR, P185; Li L, 2009, KDD-09: 15TH ACM SIGKDD CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P507; Melnyk I., 2016, P 23 INT C MACH LEAR; Nicholson WB, 2014, TECHNICAL REPORT; Petris G, 2009, USE R, P1, DOI 10.1007/b135794_1; Petris G, 2010, J STAT SOFTW, V36, P1; Rallapalli S, 2010, MOBICOM 10 & MOBIHOC 10: PROCEEDINGS OF THE 16TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING AND THE 11TH ACM INTERNATIONAL SYMPOSIUM ON MOBILE AD HOC NETWORKING AND COMPUTING, P161; Rao N., 2015, ADV NEURAL INFORM PR, V27; Roughan M, 2012, IEEE ACM T NETWORK, V20, P662, DOI 10.1109/TNET.2011.2169424; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; Smola AJ, 2003, LECT NOTES ARTIF INT, V2777, P144, DOI 10.1007/978-3-540-45167-9_12; Sun JZ, 2012, INT CONF ACOUST SPEE, P1897, DOI 10.1109/ICASSP.2012.6288274; Wang HS, 2007, J ROY STAT SOC B, V69, P63; West M., 2013, SPRINGER SERIES STAT; Wilson KW, 2008, INTERSPEECH 2008: 9TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2008, VOLS 1-5, P411; Xiong L., 2010, SIAM INT C DATA MINI, P223; Xu Miao, 2013, ADV NEURAL INFORM PR, P2301, DOI DOI 10.5555/2999792.2999869; Yu H.-F., 2014, INT C MACH LEARN, P593; Zhang Y, 2009, SIGCOMM 2009, P267; Zheng FH, 2012, PROCEEDINGS OF 2012 INTERNATIONAL CONFERENCE ON PUBLIC ADMINISTRATION (8TH), VOL III, P403	27	45	45	5	17	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701017
C	Norouzi, M; Collins, MD; Johnson, M; Fleet, DJ; Kohli, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Norouzi, Mohammad; Collins, Maxwell D.; Johnson, Matthew; Fleet, David J.; Kohli, Pushmeet			Efficient Non-greedy Optimization of Decision Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Decision trees and randomized forests are widely used in computer vision and machine learning. Standard algorithms for decision tree induction optimize the split functions one node at a time according to some splitting criteria. This greedy procedure often leads to suboptimal trees. In this paper, we present an algorithm for optimizing the split functions at all levels of the tree jointly with the leaf parameters, based on a global objective. We show that the problem of finding optimal linear-combination (oblique) splits for decision trees is related to structured prediction with latent variables, and we formulate a convex-concave upper bound on the tree's empirical loss. Computing the gradient of the proposed surrogate objective with respect to each training exemplar is O(d(2)), where d is the tree depth, and thus training deep trees is feasible. The use of stochastic gradient descent for optimization enables effective training with large datasets. Experiments on several classification benchmarks demonstrate that the resulting non-greedy decision trees outperform greedy decision tree baselines.	[Norouzi, Mohammad; Fleet, David J.] Univ Toronto, Dept Comp Sci, Toronto, ON, Canada; [Collins, Maxwell D.] Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA; [Johnson, Matthew; Kohli, Pushmeet] Microsoft Res, New York, NY USA	University of Toronto; University of Wisconsin System; University of Wisconsin Madison; Microsoft	Norouzi, M (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.			/0000-0003-0734-7114	Google fellowship; NSERC Canada; NCAP program of the CIFAR	Google fellowship(Google Incorporated); NSERC Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)); NCAP program of the CIFAR	MN was financially supported in part by a Google fellowship. DF was financially supported in part by NSERC Canada and the NCAP program of the CIFAR.	Bennett K.P., 1994, COMP SCI STAT VOL 26, V26, P156; Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808; BENNETT KP, 1997, 97100 RENSS POL I DE, P2396; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Criminisi A., 2013, DECISION FORESTCOM; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gall J, 2011, IEEE T PATTERN ANAL, V33, P2188, DOI 10.1109/TPAMI.2011.70; Hastie T, 2009, ELEMENTS STAT LEARNI; Hyafil L., 1976, Information Processing Letters, V5, P15, DOI 10.1016/0020-0190(76)90095-8; Jancsary J, 2012, LECT NOTES COMPUT SC, V7578, P112, DOI 10.1007/978-3-642-33786-4_9; JORDAN MI, 1994, NEURAL COMPUT, V6, P181, DOI 10.1162/neco.1994.6.2.181; Konukoglu E, 2012, LECT NOTES COMPUT SC, V7512, P75, DOI 10.1007/978-3-642-33454-2_10; Lakshminarayanan B., 2014, P ADV NEUR INF PROC, P3140; Mingers J., 1989, Machine Learning, V4, P227, DOI 10.1023/A:1022604100933; Murthy S. K., 1995, THESIS CITESEER; Norouzi M., 2011, INT C MACHINE LEARNI, P353; Norouzi M., 2015, ARXIV150606155; Nowozin S., 2012, ICML, P297; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; Shotton J., 2013, IEEE T PAMI; Taskar B., 2003, ADV NEURAL INFORM PR, P659; Tsochantaridis Ioannis, 2004, P 21 INT C MACH LEAR; Yu C.-N. J., 2009, P 26 ANN INT C MACHI, P1169, DOI [10.1145/1553374.1553523, DOI 10.1145/1553374.1553523]	25	45	46	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913102035
C	Lee, S; Kim, JK; Zheng, X; Ho, Q; Gibson, GA; Xing, EP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Lee, Seunghak; Kim, Jin Kyu; Zheng, Xun; Ho, Qirong; Gibson, Garth A.; Xing, Eric P.			On Model Parallelization and Scheduling Strategies for Distributed Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Distributed machine learning has typically been approached from a data parallel perspective, where big data are partitioned to multiple workers and an algorithm is executed concurrently over different data subsets under various synchronization schemes to ensure speed-up and/or correctness. A sibling problem that has received relatively less attention is how to ensure efficient and correct model parallel execution of ML algorithms, where parameters of an ML program are partitioned to different workers and undergone concurrent iterative updates. We argue that model and data parallelisms impose rather different challenges for system design, algorithmic adjustment, and theoretical analysis. In this paper, we develop a system for model-parallelism, STRADS, that provides a programming abstraction for scheduling parameter updates by discovering and leveraging changing structural properties of ML programs. STRADS enables a flexible tradeoff between scheduling efficiency and fidelity to intrinsic dependencies within the models, and improves memory efficiency of distributed ML. We demonstrate the efficacy of model-parallel algorithms implemented on STRADS versus popular implementations for topic modeling, matrix factorization, and Lasso.	[Lee, Seunghak; Kim, Jin Kyu; Zheng, Xun; Gibson, Garth A.; Xing, Eric P.] Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA; [Ho, Qirong] ASTAR, Inst Infocomm Res, Singapore 138632, Singapore	Carnegie Mellon University; Agency for Science Technology & Research (A*STAR); A*STAR - Institute for Infocomm Research (I2R)	Lee, S (corresponding author), Carnegie Mellon Univ, Sch Comp Sci, Pittsburgh, PA 15213 USA.	seunghak@cs.cmu.edu; jinkyuk@cs.cmu.edu; xunzheng@cs.cmu.edu; hoqirong@gmail.com; garth@cs.cmu.edu; epxing@cs.cmu.edu	Ho, Qirong/AAB-8152-2022		NSF [IIS1447676, CNS-1042543 (PRObE [11])]; DARPA [FA87501220324]; Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC)	NSF(National Science Foundation (NSF)); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC)	This work was done under support from NSF IIS1447676, CNS-1042543 (PRObE [11]), DARPA FA87501220324, and support from Intel via the Intel Science and Technology Center for Cloud Computing (ISTC-CC).	[Anonymous], 2012, H ACM INT C WEB SEAR, DOI DOI 10.1145/2124295.2124337; Bennett James, 2007, P KDD CUP WORKSH, V2007, P35; Blei DM, 2003, J MACH LEARN RES, V3, P993, DOI 10.1162/jmlr.2003.3.4-5.993; Bradley J., 2011, ICML; Dai W., 2014, AAAI; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Fan JQ, 2009, J MACH LEARN RES, V10, P2013; Friedman J, 2007, ANN APPL STAT, V1, P302, DOI 10.1214/07-AOAS131; Gemulla R., 2011, SIGKDD; Gibson G., 2013, USENIX LOGIN, V38; Gonzalez J., 2011, AISTATS; Gonzalez J. E., 2012, 10 USENIX S OPERATIN, P17; Griffiths TL, 2004, P NATL ACAD SCI USA, V101, P5228, DOI 10.1073/pnas.0307752101; Ho Qirong, 2013, Adv Neural Inf Process Syst, V2013, P1223; Lau Jey Han, 2013, ACM T SPEECH LANGUAG, V10, P1; Le Q., 2012, INT C MACH LEARN, DOI DOI 10.1109/MSP.2011.940881; Li M., 2014, 11 USENIX S OP SYST, V14, P583, DOI DOI 10.1145/2640087.2644155; Low Y, 2012, PROC VLDB ENDOW, V5, P716, DOI 10.14778/2212351.2212354; Newman D, 2009, J MACH LEARN RES, V10, P1801; Scherrer C., 2012, NIPS; Wang Y., 2014, ARXIV14054402CSIR; Wei J., 2013, ARXIV13127869STATML; Yu HF, 2012, IEEE DATA MINING, P765, DOI 10.1109/ICDM.2012.168; Zaharia M, 2010, HOTCLOUD, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?; Zhou YH, 2008, LECT NOTES COMPUT SC, V5034, P337, DOI 10.1007/978-3-540-68880-8_32; Zinkevich M., 2009, NIPS; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	28	45	47	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103049
C	Bengio, Y; Bengio, S		Solla, SA; Leen, TK; Muller, KR		Bengio, Y; Bengio, S			Modeling high-dimensional discrete data with multi-layer neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					The curse of dimensionality is severe when modeling high-dimensional discrete data: the number of possible combinations of the variables explodes exponentially. In this paper we propose a new architecture for modeling high-dimensional data that requires resources (parameters and computations) that grow only at most as the square of the number of variables, using a multi-layer neural network to represent the joint distribution of the variables as the product of conditional distributions. The neural network can be interpreted as a graphical model without hidden random variables, but in which the conditional distributions are tied through the hidden units. The connectivity of the neural network can be pruned by using dependency tests between the variables. Experiments on modeling the distribution of several discrete data sets show statistically significant improvements over other methods such as naive Bayes and comparable Bayesian networks, and show that significant improvements can be obtained by pruning the network.	Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Bengio, Y (corresponding author), Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada.	bengioy@iro.umontreal.ca; bengio@idiap.ch						Chow C. K., 1962, IRE T ELECTRON COM, VEC-11, P683; LAURITZEN SL, 1995, COMPUT STAT DATA AN, V19, P191, DOI 10.1016/0167-9473(93)E0056-A; [No title captured]	3	45	46	1	5	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						400	406						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700057
C	Seung, HS		Jordan, MI; Kearns, MJ; Solla, SA		Seung, HS			Learning continuous attractors in recurrent networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					One approach to invariant object recognition employs a recurrent neural network as an associative memory. In the standard depiction of the network's state space, memories of objects are stored as attractive fixed points of the dynamics. I argue for a modification of this picture: if an object has a continuous family of instantiations, it should be represented by a continuous attractor. This idea is illustrated with a network that learns to complete patterns. To perform the task of filling ii missing information, the network develops a continuous attractor that models the manifold from which the patterns are drawn. From a statistical viewpoint, the pattern completion task allows a formulation of unsupervised learning in terms of regression rather than density estimation.	AT&T Bell Labs, Lucent Technol, Murray Hill, NJ 07974 USA	Alcatel-Lucent; Lucent Technologies; AT&T; Nokia Corporation; Nokia Bell Labs	Seung, HS (corresponding author), AT&T Bell Labs, Lucent Technol, 600 Mt Ave, Murray Hill, NJ 07974 USA.								0	45	45	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						654	660						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700093
C	MOZER, MC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MOZER, MC			INDUCTION OF MULTISCALE TEMPORAL STRUCTURE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	45	44	0	1	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						275	282						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00034
C	Ding, XH; Ding, GG; Zhou, XX; Guo, YC; Han, JG; Liu, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ding, Xiaohan; Ding, Guiguang; Zhou, Xiangxin; Guo, Yuchen; Han, Jungong; Liu, Ji			Global Sparse Momentum SGD for Pruning Very Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep Neural Network (DNN) is powerful but computationally expensive and memory intensive, thus impeding its practical usage on resource-constrained front-end devices. DNN pruning is an approach for deep model compression, which aims at eliminating some parameters with tolerable performance degradation. In this paper, we propose a novel momentum-SGD-based optimization method to reduce the network complexity by on-the-fly pruning. Concretely, given a global compression ratio, we categorize all the parameters into two parts at each training iteration which are updated using different rules. In this way, we gradually zero out the redundant parameters, as we update them using only the ordinary weight decay but no gradients derived from the objective function. As a departure from prior methods that require heavy human works to tune the layer-wise sparsity ratios, prune by solving complicated non-differentiable problems or finetune the model after pruning, our method is characterized by 1) global compression that automatically finds the appropriate per-layer sparsity ratios; 2) end-to-end training; 3) no need for a time-consuming re-training process after pruning; and 4) superior capability to find better winning tickets which have won the initialization lottery.	[Ding, Xiaohan; Ding, Guiguang; Guo, Yuchen] Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China; [Ding, Xiaohan; Ding, Guiguang; Guo, Yuchen] Tsinghua Univ, Sch Software, Beijing, Peoples R China; [Zhou, Xiangxin] Tsinghua Univ, Dept Elect Engn, Beijing, Peoples R China; [Guo, Yuchen] Tsinghua Univ, Dept Automat, Beijing, Peoples R China; [Guo, Yuchen] Tsinghua Univ, Inst Brain & Cognit Sci, Beijing, Peoples R China; [Han, Jungong] Univ Warwick, WMG Data Sci, Coventry, W Midlands, England; [Liu, Ji] Kwai AI Platform, Kwai Seattle AI Lab, Kwai FeDA Lab, Seattle, WA USA	Tsinghua University; Tsinghua University; Tsinghua University; Tsinghua University; University of Warwick	Ding, XH (corresponding author), Beijing Natl Res Ctr Informat Sci & Technol BNRis, Beijing, Peoples R China.; Ding, XH (corresponding author), Tsinghua Univ, Sch Software, Beijing, Peoples R China.	dxh17@mails.tsinghua.edu.cn; dinggg@tsinghua.edu.cn; xx-zhou16@mails.tsinghua.edu.cn; yuchen.w.guo@gmail.com; jungonghan77@gmail.com; ji.liu.uwisc@gmail.com	Han, Jungong/ABE-6812-2020		National Key RAMP;D Program of China [2018YFC0807500]; National Natural Science Foundation of China [61571269, 61971260]; National Postdoctoral Program for Innovative Talents [BX20180172]; China Postdoctoral Science Foundation [2018M640131]	National Key RAMP;D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Postdoctoral Program for Innovative Talents; China Postdoctoral Science Foundation(China Postdoctoral Science Foundation)	We sincerely thank all the reviewers for their comments. This work was supported by the National Key R&D Program of China (No. 2018YFC0807500), National Natural Science Foundation of China (No. 61571269, No. 61971260), National Postdoctoral Program for Innovative Talents (No. BX20180172), and the China Postdoctoral Science Foundation (No. 2018M640131). Corresponding author: Guiguang Ding, Jungong Han.	Alvarez JM, 2017, ADV NEUR IN, V30; [Anonymous], FITNETS HINTS THIN D; Bengio Yoshua, 2019, 7 INT C LEARN REPR I; Bengio Yoshua, 2015, 3 INT C LEARN REPR I; Rossini MB, 2016, PROP INTELECT, P227; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Carbin M., LOTTERY TICKET HYPOT; Castellano G, 1997, IEEE T NEURAL NETWOR, V8, P519, DOI 10.1109/72.572092; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; diaeresis>el Mathieu Micha<spacing, 2014, 2 INT C LEARN REPR I, P3; Ding X., 2018, 32 AAAI C ART INT; Ding XH, 2019, PROC CVPR IEEE, P4938, DOI 10.1109/CVPR.2019.00508; Ding XH, 2019, PR MACH LEARN RES, V97; Dong X, 2017, ADV NEUR IN, V30; Figurnov Mikhail, 2016, NEURIPS; Ge CY, 2018, INT CONF CLOUD COMPU, P1072, DOI 10.1109/CCIS.2018.8691376; Goh G., 2017, DISTILL, V2, pe6, DOI DOI 10.23915/DISTILL.00006; Gordon A, 2018, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2018.00171; Guo YW, 2016, ADV NEUR IN, V29; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han S., 2016, P 4 INT C LEARN REPR, P1; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; Hinton G., 2015, ARXIV150302531; Hu H., 2016, ARXIV PREPRINT ARXIV; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang XW, 2015, ACTA POLYM SIN, P1133; Kathuria A, 2018, INTRO OPTIMIZATION D; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lin Shaohui, 2019, ARXIV190107827; Liu ZC, 2019, IEEE I CONF COMP VIS, P3295, DOI [10.1109/ICCV.2019.00339, 10.1109/ICCV.2019.00339D\]; Liu Z, 2017, IEEE I CONF COMP VIS, P2755, DOI 10.1109/ICCV.2017.298; Liu Zhuang, RETHINKING VALUE NET; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Molchanov P., 2017, P INT C LEARN REPR I, P1; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Roth Volker, 2008, P 25 INT C MACH LEAR, P848; Rutishauser H., 1959, REFINED ITERATIVE ME, P24, DOI DOI 10.1007/978-3-0348-7224-9_2; Sainath TN, 2013, INT CONF ACOUST SPEE, P6655, DOI 10.1109/ICASSP.2013.6638949; Srinivas S, 2017, IEEE COMPUT SOC CONF, P455, DOI 10.1109/CVPRW.2017.61; Stork D.G., 1993, ADV NEURAL INF PROCE, P164; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Theis Lucas, 2018, ARXIV180105787; Vasilache N, FAST CONVOLUTIONAL N; Wang Huan, 2018, ARXIV181108390; Wang YH, 2016, ADV NEUR IN, V29, P253; Wang YH, 2017, PR MACH LEARN RES, V70; Wen W, 2016, ADV NEUR IN, V29; Yang TJ, 2017, PROC CVPR IEEE, P6071, DOI 10.1109/CVPR.2017.643; Zhang T., 2018, P EUROPEAN C COMPUTE, P184; Zhang XY, 2016, IEEE T PATTERN ANAL, V38, P1943, DOI 10.1109/TPAMI.2015.2502579	57	44	44	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306039
C	Sitzmann, V; Zollhofer, M; Wetzstein, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sitzmann, Vincent; Zollhofer, Michael; Wetzstein, Gordon			Scene Representation Networks: Continuous 3D-Structure-Aware Neural Scene Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Unsupervised learning with generative models has the potential of discovering rich representations of 3D scenes. While geometric deep learning has explored 3D-structure-aware representations of scene geometry, these models typically require explicit 3D supervision. Emerging neural scene representations can be trained only with posed 2D images, but existing methods ignore the three-dimensional structure of scenes. We propose Scene Representation Networks (SRNs), a continuous, 3D-structure-aware scene representation that encodes both geometry and appearance. SRNs represent scenes as continuous functions that map world coordinates to a feature representation of local scene properties. By formulating the image formation as a differentiable ray-marching algorithm, SRNs can be trained end-to-end from only 2D images and their camera poses, without access to depth or shape. This formulation naturally generalizes across scenes, learning powerful geometry and appearance priors in the process. We demonstrate the potential of SRNs by evaluating them for novel view synthesis, few-shot reconstruction, joint shape and appearance interpolation, and unsupervised discovery of a non-rigid face model.(1)	[Sitzmann, Vincent; Zollhofer, Michael; Wetzstein, Gordon] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Sitzmann, V (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	sitzmann@cs.stanford.edu; zollhoefer@cs.stanford.edu; gordon.wetzstein@stanford.edu		Wetzstein, Gordon/0000-0002-9243-6885	Stanford Graduate Fellowship; Max Planck Center for Visual Computing and Communication (MPC-VCC); NSF [IIS 1553333, CMMI 1839974]; Sloan Fellowship; Okawa Research Grant; PECASE	Stanford Graduate Fellowship(Stanford University); Max Planck Center for Visual Computing and Communication (MPC-VCC); NSF(National Science Foundation (NSF)); Sloan Fellowship(Alfred P. Sloan Foundation); Okawa Research Grant; PECASE(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA)	We thank Ludwig Schubert for fruitful discussions. Vincent Sitzmann was supported by a Stanford Graduate Fellowship. Michael Zollhofer was supported by the Max Planck Center for Visual Computing and Communication (MPC-VCC). Gordon Wetzstein was supported by NSF awards (IIS 1553333, CMMI 1839974), by a Sloan Fellowship, by an Okawa Research Grant, and a PECASE.	Achlioptas P, 2018, PR MACH LEARN RES, V80; Alet F., 2019, P ICML; [Anonymous], 2013, P ICLR; Bever TG, 2010, BIOLINGUISTICS, V4, P174; Bottou L, 2017, P ICML; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chintala S, 2016, ICLR; Choy C. B., 2016, P ECCV; Dinh L., 2015, P ICLR WORKSH; Eslami SMA, 2018, SCIENCE, V360, P1204, DOI 10.1126/science.aar6170; Finn C, 2017, PR MACH LEARN RES, V70; Gadelha M, 2017, INT CONF 3D VISION, P402, DOI 10.1109/3DV.2017.00053; Genova K., 2019, P ICCV; Goodfellow I., 2013, ADV NEURAL INF PROCE, V27; Groueix T., 2018, P CVPR; Ha D., 2017, P ICLR; Hane C., 2019, P PAMI, V1, P1; Hart JC, 1996, VISUAL COMPUT, V12, P527, DOI 10.1007/s003710050084; Hartley R., 2003, MULTIPLE VIEW GEOMET; Hinton G., 2011, P ICANN; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton Geoffrey, 2019, ARXIV PREPRINT ARXIV; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Insafutdinov E., 2018, ABS181009381 CORR; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jack D., 2018, LEARNING FREE FORM D; Jaderberg M., 2015, P NIPS; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Kanazawa Angjoo, 2018, ECCV; Kar A., 2017, P NIPS, P365; Karras T., 2018, P ICLR; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kingma D.P., 2018, P 32 INT C NEUR INF, P10236; Kulkarni T., 2015, P CVPR; Kulkarni T. D., 2015, P NIPS; Kumar A., 2018, CONSISTENT JUMPY PRE; Lin CY, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P3, DOI 10.1109/ISMAR-Adjunct.2018.00021; Liu Y., 2019, P ICLR; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mescheder L., 2019, P CVPR; Meshry M., 2019, P CVPR; Mirza M., 2014, ARXIV PREPRINT ARXIV; Mordvintsev Alexander, 2018, DISTILL, V3, pe12, DOI DOI 10.23915/DISTILL.00012; Nguyen-Phuoc T., 2019, P ICCV; Nguyen-Phuoc T. H., 2018, P NIPS; Oord A. v. d., 2016, P ICML; Park Jeong Joon, 2019, ARXIV190105103; Paysan P, 2009, AVSS: 2009 6TH IEEE INTERNATIONAL CONFERENCE ON ADVANCED VIDEO AND SIGNAL BASED SURVEILLANCE, P296, DOI 10.1109/AVSS.2009.58; Qi C. R., 2017, P CVPR; Qi C. R., 2016, P CVPR; Rezende D. Jimenez, 2016, P NIPS; Riegler G., 2017, P CVPR; Shu Z., 2017, P CVPR; Sitzmann Vincent, 2019, P CVPR; Stanley KO, 2007, GENET PROGRAM EVOL M, V8, P131, DOI 10.1007/s10710-007-9028-8; Sun Xingyuan, 2018, P CVPR; Tang C., 2019, P ICLR; Tatarchenko M., 2016, P ECCV; Tatarchenko M, 2017, IEEE I CONF COMP VIS, P2107, DOI 10.1109/ICCV.2017.230; Tulsiani S., P CVPR; Tung H. F., P ICCV; Tung H.-Y. F., 2019, P CVPR; van den Oord Aaron, 2016, ARXIV160605328; van den Oord Aaron, 2016, P NIPS; Worrall D. E., 2017, P ICCV, V4; Yan X., 2016, P NIPS; Yang J., 2015, P NIPS; Yuille A, 2006, TRENDS COGN SCI, V10, P301, DOI 10.1016/j.tics.2006.05.002; Zhou X, 2018, DRUG DES DEV THER, V12, P1, DOI 10.2147/DDDT.S146925; Zhu, 2018, P 32 INT C NEUR INF, P118; Zhu J., 2017, P ICCV; Zhu Jun-Yan, 2016, P ECCV	73	44	44	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301015
C	Vuorio, R; Sun, SH; Hu, HX; Lim, JJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vuorio, Risto; Sun, Shao-Hua; Hu, Hexiang; Lim, Joseph J.			Multimodal Model-Agnostic Meta-Learning via Task-Aware Modulation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Model-agnostic meta-learners aim to acquire meta-learned parameters from similar tasks to adapt to novel tasks from the same distribution with few gradient updates. With the flexibility in the choice of models, those frameworks demonstrate appealing performance on a variety of domains such as few-shot image classification and reinforcement learning. However, one important limitation of such frameworks is that they seek a common initialization shared across the entire task distribution, substantially limiting the diversity of the task distributions that they are able to learn from. In this paper, we augment MAML [5] with the capability to identify the mode of tasks sampled from a multimodal task distribution and adapt quickly through gradient updates. Specifically, we propose a multimodal MAML (MMAML) framework, which is able to modulate its meta-learned prior parameters according to the identified mode, allowing more efficient fast adaptation. We evaluate the proposed model on a diverse set of few-shot learning tasks, including regression, image classification, and reinforcement learning. The results not only demonstrate the effectiveness of our model in modulating the meta-learned prior in response to the characteristics of tasks but also show that training on a multimodal distribution can produce an improvement over unimodal training. The code for this project is publicly available at https://vuoristo.github.io/MMAML.	[Vuorio, Risto] Univ Michigan, Ann Arbor, MI 48109 USA; [Sun, Shao-Hua; Hu, Hexiang; Lim, Joseph J.] Univ Southern Calif, Los Angeles, CA 90007 USA	University of Michigan System; University of Michigan; University of Southern California	Vuorio, R (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.; Sun, SH (corresponding author), Univ Southern Calif, Los Angeles, CA 90007 USA.	vuoristo@gmail.com; shaohuas@usc.edu; hexiangh@usc.edu; limjj@usc.edu	Sun, Shao-Hua/GVS-5566-2022; Sun, Shao-Hua/AAB-6903-2019; Hu, Hexiang/GNW-4536-2022	Sun, Shao-Hua/0000-0001-7579-6734; 	SK T-Brain	SK T-Brain	This work was initiated when Risto Vuorio was at SK T-Brain and was partially supported by SK T-Brain. The authors are grateful for the fruitful discussion with Kuan-Chieh Wang, Max Smith, and Youngwoon Lee. The authors appreciate the anonymous NeurIPS reviewers as well as the anonymous reviewers who rejected this paper but provided constructive feedback for improving this paper in previous submission cycles.	Andrychowicz M, 2016, ADV NEUR IN, V29; Bengio Samy, 1992, C OPT ART BIOL NEUR, P6; Chen Wei-Yu, 2019, INT C LEARN REPR, P12; Devlin J, 2016, INT CONF EUR ENERG; Duan Y., 2016, RL2 FAST REINFORCEME; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, INT C LEARN REPR; Finn Chelsea, 2018, ADV NEURAL INFORM PR, P9516; Grant Erin, 2018, INT C LEARN REPR; Haarnoja T, 2018, PR MACH LEARN RES, V80; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huh Minyoung, 2019, IEEE C COMP VIS PATT; Huynh LN, 2018, PROCEEDINGS OF THE 2018 INTERNATIONAL WORKSHOP ON EMBEDDED AND MOBILE DEEP LEARNING (EMDL '18), P7, DOI 10.1145/3212725.3212730; Kim T., 2018, ADV NEURAL INFORM PR; Koch G., 2015, ICML DEEP LEARNING W; Lake Brenden, 2011, C COGN SCI SOC, P6; Lee Y., 2019, INT C LEARN REPR; Lee Y, 2018, KOREAN J CHEM ENG, V35, P1, DOI 10.1007/s11814-017-0236-5; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Maji Subhransu, 2013, ARXIV13065151; Mishra N., 2018, INT C LEARN REPR, P1; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Munkhdalai T, 2017, PR MACH LEARN RES, V70; Nichol Alex, 2018, ARXIV180302999; Oreshkin Boris N, 2018, ADV NEURAL INFORM PR; Park T, 2019, PROC CVPR IEEE, P2332, DOI 10.1109/CVPR.2019.00244; Perez E, 2018, AAAI CONF ARTIF INTE, P3942; Peters Matthew E., 2018, P 2018 C N AM CHAPT, V1, P2227, DOI DOI 10.18653/V1/N18-1202; Rajeswaran A, 2018, ROBOTICS: SCIENCE AND SYSTEMS XIV; Ravi S., 2017, INT C LEARN REPR, P12; Rothfuss J., 2019, INT C LEARN REPR; Rusu Andrei A, 2019, ICLR; Santoro A, 2016, PR MACH LEARN RES, V48; Schmidhuber J, 1998, LEARNING TO LEARN, P293; Schmidhuber J, 1997, MACH LEARN, V28, P105, DOI 10.1023/A:1007383707642; Schmidhuber J, 1987, THESIS; Shixiang Gu, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P3389, DOI 10.1109/ICRA.2017.7989385; Shyam P, 2017, PR MACH LEARN RES, V70; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Thrun S., 2012, LEARNING LEARN; Todorov T, 2012, SALMAGUNDI, P8; Triantafillou Eleni, 2018, MET WORKSH NEUR INF; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Vuorio R, 2018, ARXIV180606928; Wah C., 2011, TECH REP; Wang Jane X, 2018, NATURE NEUROSCIENCE; Wang JW, 2017, ACTA OPHTHALMOL, V95, pE10, DOI 10.1111/aos.13227; Ye H.-J., 2018, ARXIV PREPRINT ARXIV; Zhu LY, 2018, INT WIREL COMMUN, P30	53	44	44	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300001
C	Bunel, R; Turkaslan, I; Torr, PHS; Kohli, P; Kumar, MP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bunel, Rudy; Turkaslan, Ilker; Torr, Philip H. S.; Kohli, Pushmeet; Kumar, M. Pawan			A Unified View of Piecewise Linear Neural Network Verification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The success of Deep Learning and its potential use in many safety-critical applications has motivated research on formal verification of Neural Network (NN) models. Despite the reputation of learned NN models to behave as black boxes and the theoretical hardness of proving their properties, researchers have been successful in verifying some classes of models by exploiting their piecewise linear structure and taking insights from formal methods such as Satisifiability Modulo Theory. These methods are however still far from scaling to realistic neural networks. To facilitate progress on this crucial area, we make two key contributions. First, we present a unified framework that encompasses previous methods. This analysis results in the identification of new methods that combine the strengths of multiple existing approaches, accomplishing a speedup of two orders of magnitude compared to the previous state of the art. Second, we propose a new data set of benchmarks which includes a collection of previously released testcases. We use the benchmark to provide the first experimental comparison of existing algorithms and identify the factors impacting the hardness of verification problems.	[Bunel, Rudy; Turkaslan, Ilker; Torr, Philip H. S.] Univ Oxford, Oxford, England; [Kohli, Pushmeet] Deepmind, London, England; [Kumar, M. Pawan] Univ Oxford, Alan Turing Inst, Oxford, England	University of Oxford; University of Oxford	Bunel, R (corresponding author), Univ Oxford, Oxford, England.	rudy@robots.ox.ac.uk; ilker.turkaslan@lmh.ox.ac.uk; philip.torr@eng.ox.ac.uk; pushmeet@google.com; pawan@robots.ox.ac.uk			ERC [ERC-2012-AdG 321162-HELIOS]; EPSRC [EP/M013774/1]; EPSRC/MURI [EP/N019474/1]	ERC(European Research Council (ERC)European Commission); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC/MURI(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work was supported by ERC grant ERC-2012-AdG 321162-HELIOS, EPSRC grant Seebibyte EP/M013774/1 and EPSRC/MURI grant EP/N019474/1. We would also like to acknowledge the Royal Academy of Engineering and FiveAI.	Barrett Clark, 2006, INT C LOG PROGR ART; Bastani Osbert, 2016, NIPS; Buxton J. N., 1970, SOFTWARE ENG TECHNIQ; Cheng C., 2017, ARXIV171003107; Dvijotham K., 2018, UAI; Ehlers Ruediger, 2017, PLANET; Ehlers Ruediger, 2017, AUTOMATED TECHNOLOGY; Hein M., 2017, NIPS; Hickey Timothy, 2001, J ACM JACM; Huang Xiaowei, 2017, INT C COMP AID VER; Katz Guy, 2017, RELUPLEX; Katz Guy, 2017, CAV; Kolter Zico, 2017, ARXIV171100851; Marques-Silva Joao P, 1999, IEEE T COMPUTERS; Narodytska N., 2017, ARXIV170906662; Pulina Luca, 2010, CAV; Sherali Hanif D, 1994, DISCRETE APPL MATH; Tjeng V., 2017, INT C LEARN REPR; Xiang Weiming, 2017, ARXIV170803322; Zakrzewski Radosiaw R., 2001, IJCNN	20	44	45	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304077
C	Hanin, B; Rolnick, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hanin, Boris; Rolnick, David			How to Start Training: The Effect of Initialization and Architecture	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We identify and study two common failure modes for early training in deep ReLU nets. For each, we give a rigorous proof of when it occurs and how to avoid it, for fully connected, convolutional, and residual architectures. We show that the first failure mode, exploding or vanishing mean activation length, can be avoided by initializing weights from a symmetric distribution with variance 2/fan-in and, for ResNets, by correctly scaling the residual modules. We prove that the second failure mode, exponentially large variance of activation length, never occurs in residual nets once the first failure mode is avoided. In contrast, for fully connected nets, we prove that this failure mode can happen and is avoided by keeping constant the sum of the reciprocals of layer widths. We demonstrate empirically the effectiveness of our theoretical results in predicting when networks are able to start training In particular, we note that many popular initializations fail our criteria, whereas correct initialization and architecture allows much deeper networks to be trained.	[Hanin, Boris] Texas A&M Univ, Dept Math, College Stn, TX 77843 USA; [Rolnick, David] MIT, Dept Math, Cambridge, MA 02139 USA	Texas A&M University System; Texas A&M University College Station; Massachusetts Institute of Technology (MIT)	Hanin, B (corresponding author), Texas A&M Univ, Dept Math, College Stn, TX 77843 USA.	bhanin@math.tamu.edu; drolnick@mit.edu						[Anonymous], ARXIV150400941; [Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; Arjovsky M, 2016, PR MACH LEARN RES, V48; Arpit D, 2017, ARXIV170605394; Billingsley P., 2008, PROBABILITY MEASURE; Chollet F, 2018, KERAS; Ge R., 2017, ARXIV170400708; Ge Yang, 2017, ADV NEURAL INF PROCE, V30, P7103; Gilmer Justin, 2016, INT C LEARN REPR; Giryes R, 2016, IEEE T SIGNAL PROCES, V64, P3444, DOI 10.1109/TSP.2016.2546221; Hanin B., 2018, ADV NEURAL INFORM PR; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Henaff M., 2016, P 33 INT C MACH LEAR, P2034; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jin Chi, 2017, ARXIV170300887; Jing L., 2017, INT C MACHINE LEARNI, P1733; Kadmon J, 2015, PHYS REV X, V5, DOI 10.1103/PhysRevX.5.041030; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Paszke A., 2017, AUTOMATIC DIFFERENTI, DOI DOI 10.1016/J.COMPAG.2018.04.002; Pennington J., 2017, 31 INT C NEUR INF PR, P4788, DOI DOI 10.5555/3295222.3295232; Poole B, 2016, ADV NEUR IN, V29; Raghu M, 2017, PR MACH LEARN RES, V70; Saxe A., 2014, INT C LEARNING REPRE; Schoenholz S. S., 2017, ARXIV171006570; Shalev-Shwartz S, 2017, PR MACH LEARN RES, V70; Shanmugam R., 2015, STAT SCI ENG; Taki Masato, 2017, ARXIV PREPRINT ARXIV; Wilber M, 2016, P ADV NEUR INF PROC, V30, P550; Wu Yuhuai, 2017, ADV NEURAL INFORM PR, P5285; Yang Greg, 2018, WORKSH INT C LEARN R; Zeng HQ, 2017, PROC INT CONF RECON	34	44	44	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300053
C	Lucic, M; Kurach, K; Michalski, M; Bousquet, O; Gelly, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lucic, Mario; Kurach, Karol; Michalski, Marcin; Bousquet, Olivier; Gelly, Sylvain			Are GANs Created Equal? A Large-Scale Study	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generative adversarial networks (GAN) are a powerful subclass of generative models. Despite a very rich research activity leading to numerous interesting GAN algorithms, it is still very hard to assess which algorithm(s) perform better than others. We conduct a neutral, multi-faceted large-scale empirical study on state-of-the art models and evaluation measures. We find that most models can reach similar scores with enough hyperparameter optimization and random restarts. This suggests that improvements can arise from a higher computational budget and tuning more than fundamental algorithmic changes. To overcome some limitations of the current metrics, we also propose several data sets on which precision and recall can be computed. Our experimental results suggest that future GAN research should be based on more systematic and objective evaluation procedures. Finally, we did not find evidence that any of the tested algorithms consistently outperforms the non-saturating GAN introduced in [9].	[Lucic, Mario; Kurach, Karol; Michalski, Marcin; Bousquet, Olivier; Gelly, Sylvain] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Lucic, M; Kurach, K (corresponding author), Google Brain, Mountain View, CA 94043 USA.	lucic@google.com; kkurach@google.com						Arjovsky M, 2017, PR MACH LEARN RES, V70; Arora Sanjeev, 2018, INT C LEARN REPR ICL; Bachman Philip, 2015, INT C MACH LEARN ICM; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Fedus William, 2018, INT C LEARN REPR; Gerhard HE, 2013, PLOS COMPUT BIOL, V9, DOI 10.1371/journal.pcbi.1002873; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; Heusel M., 2017, ADV NEURAL INFORM PR, P6626, DOI DOI 10.5555/3295222.3295408; Huszar Ferenc, 2015, ABS151105101 CORR; Kingma D.P, P 3 INT C LEARNING R; Kodali Naveen, 2017, ARXIV170507215; Kurach K., 2018, ARXIV180704720; Mahendran A, 2015, PROC CVPR IEEE, P5188, DOI 10.1109/CVPR.2015.7299155; Mao Xudong, 2017, ICCV, DOI [10.1109/ICCV.2017.304, DOI 10.1109/ICCV.2017.304]; Mescheder Lars, 2017, ADV NEURAL INFORM PR, P1825; Mirza M., 2014, ARXIV; Odena A, 2017, PR MACH LEARN RES, V70; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Sajjadi Mehdi SM, 2018, ADV NEURAL INFORM PR, P5234; Salimans T, 2016, ADV NEUR IN, V29; Theis Lucas, 2015, ARXIV151101844; Wu Yuxin, 2017, INT C LEARN REPR ICL; Zhang H., 2017, ICCV	26	44	45	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300065
C	Tulyakov, S; Ivanov, A; Fleuret, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tulyakov, Stepan; Ivanov, Anton; Fleuret, Francois			Practical Deep Stereo (PDS): Toward applications-friendly deep stereo matching.	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					End-to-end deep-learning networks recently demonstrated extremely good performance for stereo matching. However, existing networks are difficult to use for practical applications since (1) they are memory-hungry and unable to process even modest-size images, (2) they have to be trained for a given disparity range. The Practical Deep Stereo (PDS) network that we propose addresses both issues: First, its architecture relies on novel bottleneck modules that drastically reduce the memory footprint in inference, and additional design choices allow to handle greater image size during training. This results in a model that leverages large image context to resolve matching ambiguities. Second, a novel sub-pixel cross-entropy loss combined with a MAP estimator make this network less sensitive to ambiguous matches, and applicable to any disparity range without re-training. We compare PDS to state-of-the-art methods published over the recent months, and demonstrate its superior performance on FlyingThings3D and KITTI sets.	[Tulyakov, Stepan; Ivanov, Anton] Ecole Polytech Fed Lausanne, Space Engn Ctr, Lausanne, Switzerland; [Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Tulyakov, S (corresponding author), Ecole Polytech Fed Lausanne, Space Engn Ctr, Lausanne, Switzerland.	stepan.tulyakov@epfl.ch; anton.ivanov@epfl.ch; francois.fleuret@idiap.ch			NCCR PlanetS; CaSSIS project of the University of Bern through the Swiss Space Office via ESA's PRODEX program	NCCR PlanetS; CaSSIS project of the University of Bern through the Swiss Space Office via ESA's PRODEX program	We gratefully acknowledge support from the NCCR PlanetS and CaSSIS project of the University of Bern funded through the Swiss Space Office via ESA's PRODEX program. We also acknowledge the support of NVIDIA Corporation with the donation of the GeForce GTX TITAN X used for this research.	[Anonymous], 2017, P IEEE C COMP VIS PA; [Anonymous], [No title captured]; Barron J. T., 2015, CVPR; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chen Z., 2015, ICCV; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Galun Meirav, 2015, ICCV; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Guney F., 2015, CVPR; Hadfield Simon, 2015, ICCV; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; JEON HG, 2016, PROC CVPR IEEE, P4086, DOI DOI 10.1109/CVPR.2016.443; Jie Zequn, 2018, CVPR; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kim K. R., 2016, ICIP; KITTY, KITT STER SCOR; Knobelreiter P., 2017, CVPR; Lempitsky V., 2016, ARXIV160708022V3; Li A, 2016, CVPR; Liang Z., 2018, CVPR; Luo W., 2016, IEEE C COMP VIS PATT; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Mei Xing, 2011, IEEE INT C COMP VIS, P467, DOI DOI 10.1109/ICCVW.2011.6130280; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Nam Kyoung Won, 2012, HEALTHCARE INFORM RE, P1; Pang J., 2017, IEEE INT C COMP VIS; Pollefeys, 2017, SGM NETS SEMIGLOBAL; PyTorch, PYT WEB SIT; Queau Yvain, 2017, CVPR; Scharstein Daniel, 2001, P IEEE WORKSH STER M; Seki A., 2016, P BRIT MACH VIS C; Shaked A., 2017, IEEE C COMP VIS PATT IEEE C COMP VIS PATT; Shean DE, 2016, ISPRS J PHOTOGRAMM, V116, P101, DOI 10.1016/j.isprsjprs.2016.03.012; Tulyakov S., 2017, ICCV; Ulusoy Ali Osman, 2017, CVPR; Verleysen Cedric, 2016, CVPR; Wang Ting-Chun, 2016, CVPR; Zagoruyko S., 2015, LEARNING COMP IMAGE; Zbontar J., 2015, CVPR; Zbontar J, 2016, J MACH LEARN RES, V17; Zhang FH, 2018, IEEE T IMAGE PROCESS, V27, P822, DOI 10.1109/TIP.2017.2752370; Zhang H., 2018, 6 INT C LEARNING REP, DOI 10.48550/arXiv.1710.09412; Zhong Y., 2017, SELF SUPERVISED LEAR	43	44	46	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000038
C	Feichtenhofer, C; Pinz, A; Wildes, RP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Feichtenhofer, Christoph; Pinz, Axel; Wildes, Richard P.			Spatiotemporal Residual Networks for Video Action Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Two-stream Convolutional Networks (ConvNets) have shown strong performance for human action recognition in videos. Recently, Residual Networks (ResNets) have arisen as a new technique to train extremely deep architectures. In this paper, we introduce spatiotemporal ResNets as a combination of these two approaches. Our novel architecture generalizes ResNets for the spatiotemporal domain by introducing residual connections in two ways. First, we inject residual connections between the appearance and motion pathways of a two-stream architecture to allow spatiotemporal interaction between the two streams. Second, we transform pretrained image ConvNets into spatiotemporal networks by equipping them with learnable convolutional filters that are initialized as temporal residual connections and operate on adjacent feature maps in time. This approach slowly increases the spatiotemporal receptive field as the depth of the model increases and naturally integrates image ConvNet design principles. The whole model is trained end-to-end to allow hierarchical learning of complex spatiotemporal features. We evaluate our novel spatiotemporal ResNet using two widely used action recognition benchmarks where it exceeds the previous state-of-the-art.	[Feichtenhofer, Christoph; Pinz, Axel] Graz Univ Technol, Graz, Austria; [Wildes, Richard P.] York Univ, Toronto, ON, Canada	Graz University of Technology; York University - Canada	Feichtenhofer, C (corresponding author), Graz Univ Technol, Graz, Austria.	feichtenhofer@tugraz.at; axel.pinz@tugraz.at; wildes@cse.yorku.ca			Austrian Science Fund (FWF) [P27076]; NSERC; DOC Fellowship of the Austrian Academy of Sciences at the Institute of Electrical Measurement and Measurement Signal Processing, Graz University of Technology	Austrian Science Fund (FWF)(Austrian Science Fund (FWF)); NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); DOC Fellowship of the Austrian Academy of Sciences at the Institute of Electrical Measurement and Measurement Signal Processing, Graz University of Technology	This work was supported by the Austrian Science Fund (FWF) under project P27076 and NSERC. The GPUs used for this research were donated by NVIDIA. Christoph Feichtenhofer is a recipient of a DOC Fellowship of the Austrian Academy of Sciences at the Institute of Electrical Measurement and Measurement Signal Processing, Graz University of Technology.	[Anonymous], 2015, INT C LEARN REPR; Ballas N., 2016, P ICLR; Bilen H., 2016, P CVPR; BORN RT, 1992, NATURE, V357, P497, DOI 10.1038/357497a0; Donahue J., 2015, P CVPR; Feichtenhofer Christoph, P CVPR; GOODALE MA, 1992, TRENDS NEUROSCI, V15, P20, DOI 10.1016/0166-2236(92)90344-8; He K., 2016, ARXIV160305027; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioannou Yani, 2016, P ICLR; Ioffe S., 2015, P ICML; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Karpathy A., 2014, P CVPR; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Kuehne H., 2011, P ICCV; Le Q., 2011, P CVPR; Mahasseni B., REGULARIZING LONG SH; Ng J. Y.-H., 2015, P CVPR; Sharma S., 2015, NIPS WORKSHOP TIME S; Simonyan K., 2014, P ICLR; Simonyan Karen, 2014, NIPS; Soomro K., 2012, COMPUT SCI; Szegedy C., 2017, P 31 AAAI C ART INT; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Taylor Graham W, 2010, P ECCV; Tran D., 2015, P ICCV; VANESSEN DC, 1994, NEURON, V13, P1, DOI 10.1016/0896-6273(94)90455-3; Vedaldi A., 2015, P ACM INT C MULT; Wang H., 2013, P ICCV; Wang Limin, 2015, P CVPR; Wang Xiaolong, 2016, P CVPR; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22	32	44	44	0	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703069
C	Gao, YJ; Archer, E; Paninski, L; Cunningham, JP		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gao, Yuanjun; Archer, Evan; Paninski, Liam; Cunningham, John P.			Linear dynamical neural population models through nonlinear embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					A body of recent work in modeling neural activity focuses on recovering low-dimensional latent features that capture the statistical structure of large-scale neural populations. Most such approaches have focused on linear generative models, where inference is computationally tractable. Here, we propose fLDS, a general class of nonlinear generative models that permits the firing rate of each neuron to vary as an arbitrary smooth function of a latent, linear dynamical state. This extra flexibility allows the model to capture a richer set of neural variability than a purely linear model, but retains an easily visualizable low-dimensional latent space. To fit this class of non-conjugate models we propose a variational inference scheme, along with a novel approximate posterior capable of capturing rich temporal correlations across time. We show that our techniques permit inference in a wide class of generative models. We also show in application to two neural datasets that, compared to state-of-the-art neural population models, fLDS captures a much larger proportion of neural variability with a small number of latent dimensions, providing superior predictive performance and interpretability.	[Gao, Yuanjun; Archer, Evan; Paninski, Liam; Cunningham, John P.] Columbia Univ, Dept Stat, New York, NY 10027 USA; [Archer, Evan; Paninski, Liam; Cunningham, John P.] Columbia Univ, Grossman Ctr, New York, NY USA	Columbia University; Columbia University	Gao, YJ (corresponding author), Columbia Univ, Dept Stat, New York, NY 10027 USA.	yg2312@columbia.edu; evan@stat.columbia.edu; liam@stat.columbia.edu; jpc2181@columbia.edu						Archer E., 2015, ARXIV PREPRINT ARXIV; Archer E. W., 2014, ADV NEURAL INFORM PR, V27, P343; Bastien F., 2012, DEEP LEARN UNS FEAT; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Buesing L., 2014, ADV NEURAL INFORM PR, P3500; Burda Yuri, 2015, ARXIV150900519; Churchland MM, 2012, NATURE, V487, P51, DOI 10.1038/nature11129; Ecker AS, 2014, NEURON, V82, P235, DOI 10.1016/j.neuron.2014.02.006; Frigola R., 2014, ADV NEURAL INFORM PR, V27, P3680; Gao Y., 2015, NIPS, P2035; Goris RLT, 2014, NAT NEUROSCI, V17, P858, DOI 10.1038/nn.3711; Graf ABA, 2011, NAT NEUROSCI, V14, P239, DOI 10.1038/nn.2733; Hafting T, 2005, NATURE, V436, P801, DOI 10.1038/nature03721; Johnson Matthew J, 2016, ARXIV160306277; Khan Emtiyaz, 2013, P 30 INT C MACH LEAR, P951; Kingma D. P., 2013, AUTO ENCODING VARIAT; Macke Jakob H, 2012, ADV NEURAL INFORM PR, V24, P1350; Paninski L., 2013, ADV NEURAL INF PROCE, V26, P2391; Ranganath R., 2013, ARXIV14010118; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Sontag D., 2015, ARXIV151105121; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; Yu BM, 2009, J NEUROPHYSIOL, V102, P614, DOI 10.1152/jn.90941.2008; Zeiler Matthew D, 2012, ARXIV12125701; Zhao Y., 2016, ARXIV160403053	27	44	44	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703066
C	Wang, YH; Xu, C; You, S; Tao, DC; Xu, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Yunhe; Xu, Chang; You, Shan; Tao, Dacheng; Xu, Chao			CNNpack: Packing Convolutional Neural Networks in the Frequency Domain	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Deep convolutional neural networks (CNNs) are successfully used in a number of applications. However, their storage and computational requirements have largely prevented their widespread use on mobile devices. Here we present an effective CNN compression approach in the frequency domain, which focuses not only on smaller weights but on all the weights and their underlying connections. By treating convolutional filters as images, we decompose their representations in the frequency domain as common parts (i.e., cluster centers) shared by other similar filters and their individual private parts (i.e., individual residuals). A large number of low-energy frequency coefficients in both parts can be discarded to produce high compression without significantly compromising accuracy. We relax the computational burden of convolution operations in CNNs by linearly combining the convolution responses of discrete cosine transform (DCT) bases. The compression and speed-up ratios of the proposed algorithm are thoroughly analyzed and evaluated on benchmark image datasets to demonstrate its superiority over state-of-the-art methods.	[Wang, Yunhe; You, Shan; Xu, Chao] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China; [Xu, Chang; Tao, Dacheng] Univ Technol Sydney, Sch Software, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW, Australia; [Wang, Yunhe; You, Shan; Xu, Chao] Peking Univ, Cooperat Medianet Innovat Ctr, Beijing, Peoples R China	Peking University; University of Technology Sydney; Peking University	Wang, YH (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.; Wang, YH (corresponding author), Peking Univ, Cooperat Medianet Innovat Ctr, Beijing, Peoples R China.	wangyunhe@pku.edu.cn; Chang.Xu@uts.edu.au; youshan@pku.edu.cn; Dacheng.Tao@uts.edu.au; xuchao@cis.pku.edu.cn		Xu, Chang/0000-0002-4756-0609	National Natural Science Foundation of China [NSFC 61375026, 2015BAF15B00]; Australian Research Council [FT-130101457, DP-140102164, LE-140100061]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Australian Research Council(Australian Research Council)	This work was supported by the National Natural Science Foundation of China under Grant NSFC 61375026 and 2015BAF15B00, and Australian Research Council Projects: FT-130101457, DP-140102164 and LE-140100061.	AHMED N, 1974, IEEE T COMPUT, VC 23, P90, DOI 10.1109/T-C.1974.223784; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2015, COMPRESSING NEURAL N; Arora S, 2014, PR MACH LEARN RES, V32; Chen W., 2015, ARXIV PREPRINT ARXIV; Courbariaux Matthieu, 2016, BINARIZED NEURAL NET; Dentinel Zarembaw, 2014, NEURIPS, P1269; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Gong Yunchao, 2014, ARXIV14126115; Han S., 2016, P 4 INT C LEARN REPR, P1; Han Song, 2015, ARXIV PREPRINT ARXIV, P1135; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jia Y., 2014, P 22 ACM INT C MULT, P675; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sun Y., 2014, ADV NEURAL INFORM PR, P1988; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089	22	44	45	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29						253	261						9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703026
C	Landauer, TK; Laham, D; Foltz, P		Jordan, MI; Kearns, MJ; Solla, SA		Landauer, TK; Laham, D; Foltz, P			Learning human-like knowledge by singular value decomposition: A progress report	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Singular value decomposition (SVD) can be viewed as a method for unsupervised training of a network that associates two classes of events reciprocally by linear connections through a single hidden layer. SVD was used to learn and represent relations among very large numbers of words (20k-60k) and very large numbers of natural text passages (1k-70k) in which they occurred. The result was 100-350 dimensional "semantic spaces" in which any trained or newly added word or passage could be represented as a vector, and similarities were measured by the cosine of the contained angle between vectors. Good accuracy in simulating human judgments and behaviors has been demonstrated by performance on multiple-choice vocabulary and domain knowledge tests, emulation of expert essay evaluations, and in several other ways. Examples are also given of how the kind of knowledge extracted by this method can be applied.	Univ Colorado, Dept Psychol, Boulder, CO 80309 USA	University of Colorado System; University of Colorado Boulder	Landauer, TK (corresponding author), Univ Colorado, Dept Psychol, Boulder, CO 80309 USA.								0	44	47	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						45	51						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700007
C	Amari, S; Murata, N; Muller, KR; Finke, M; Yang, H		Touretzky, DS; Mozer, MC; Hasselmo, ME		Amari, S; Murata, N; Muller, KR; Finke, M; Yang, H			Statistical theory of overtraining - Is cross-validation asymptotically effective?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TOKYO,DEPT MATH ENGN & INFORMAT PHYS,BUNKYO KU,TOKYO 113,JAPAN	University of Tokyo			Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685					0	44	45	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						176	182						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00025
C	Cheng, SY; Dong, YP; Pang, TY; Su, H; Zhu, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Cheng, Shuyu; Dong, Yinpeng; Pang, Tianyu; Su, Hang; Zhu, Jun			Improving Black-box Adversarial Attacks with a Transfer-based Prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We consider the black-box adversarial setting, where the adversary has to generate adversarial perturbations without access to the target models to compute gradients. Previous methods tried to approximate the gradient either by using a transfer gradient of a surrogate white-box model, or based on the query feedback. However, these methods often suffer from low attack success rates or poor query efficiency since it is non-trivial to estimate the gradient in a high-dimensional space with limited information. To address these problems, we propose a prior-guided random gradient-free (P-RGF) method to improve black-box adversarial attacks, which takes the advantage of a transfer-based prior and the query information simultaneously. The transfer-based prior given by the gradient of a surrogate model is appropriately integrated into our algorithm by an optimal coefficient derived by a theoretical analysis. Extensive experiments demonstrate that our method requires much fewer queries to attack black-box models with higher success rates compared with the alternative state-of-the-art methods.	[Cheng, Shuyu; Dong, Yinpeng; Pang, Tianyu; Su, Hang; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, BNRist Ctr, Inst AI,THBI Lab,State Key Lab Intell Tech & Sys, Beijing 100084, Peoples R China	Tsinghua University	Cheng, SY (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, BNRist Ctr, Inst AI,THBI Lab,State Key Lab Intell Tech & Sys, Beijing 100084, Peoples R China.	chengsy18@mai1s.tsinghua.edu.cn; dyp17@mai1s.tsinghua.edu.cn; pty17@mai1s.tsinghua.edu.cn; suhangss@mai1.tsinghua.edu.cn; dcszj@mai1.tsinghua.edu.cn	Tianyu, Pang/AAW-2653-2020		National Key Research and Development Program of China [2017YFA0700904]; NSFC [61620106010, 61621136008, 61571261]; Beijing NSF Project [L172037]; Beijing Academy of Artificial Intelligence (BAAI); Tiangong Institute for Intelligent Computing; JP Morgan Faculty Research Program; NVIDIA NVAIL Program; GPU/DGX Acceleration	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Beijing Academy of Artificial Intelligence (BAAI); Tiangong Institute for Intelligent Computing; JP Morgan Faculty Research Program; NVIDIA NVAIL Program; GPU/DGX Acceleration	This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008, 61571261), Beijing NSF Project (No. L172037), Beijing Academy of Artificial Intelligence (BAAI), Tiangong Institute for Intelligent Computing, the JP Morgan Faculty Research Program and the NVIDIA NVAIL Program with GPU/DGX Acceleration.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Athalye A, 2018, PR MACH LEARN RES, V80; Bhagoji AN, 2018, LECT NOTES COMPUT SC, V11216, P158, DOI 10.1007/978-3-030-01258-8_10; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Brendel Wieland, 2018, P ICLR; Brunner T, 2019, IEEE I CONF COMP VIS, P4957, DOI 10.1109/ICCV.2019.00506; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Dong Yinpeng, 2019, P IEEE C COMP VIS PA; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Guo C., 2018, INT C LEARN REPR ICL; Guo Chuan, 2018, ARXIV180908758; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Ilyas A, 2019, INT C LEARN REPR ICL; Ilyas A, 2018, PR MACH LEARN RES, V80; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lax P. D., 2014, CALCULUS APPL; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Liu Y., 2017, 5 INT C LEARN REPR I; Madry A., 2018, P ICLR VANC BC CAN; Maheswaranathan Niru, 2019, INT C MACH LEARN ICM; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Oh S. J., 2018, P INT C LEARN REPR; Papernot N, 2016, ARXIV160507277, DOI 10.48550/arXiv.1605.07277; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy Christian, 2014, P 2 INT C LEARNING R; Tu Chun-Chen, 2019, AAAI; Uesato Jonathan, 2018, P 35 INT C MACH LEAR; Xie Cihang, 2018, P INT C LEARN REPR I	36	43	44	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866902055
C	Liao, R; Li, Y; Song, Y; Wang, S; Hamilton, WL; Duvenaud, D; Urtasun, R; Zemel, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liao, Renjie; Li, Yujia; Song, Yang; Wang, Shenlong; Hamilton, William L.; Duvenaud, David; Urtasun, Raquel; Zemel, Richard			Efficient Graph Generation with Graph Recurrent Attention Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose a new family of efficient and expressive deep generative models of graphs, called Graph Recurrent Attention Networks (GRANs). Our model generates graphs one block of nodes and associated edges at a time. The block size and sampling stride allow us to trade off sample quality for efficiency. Compared to previous RNN-based graph generative models, our framework better captures the auto-regressive conditioning between the already-generated and to-be-generated parts of the graph using Graph Neural Networks (GNNs) with attention. This not only reduces the dependency on node ordering but also bypasses the long-term bottleneck caused by the sequential nature of RNNs. Moreover, we parameterize the output distribution per block using a mixture of Bernoulli, which captures the correlations among generated edges within the block. Finally, we propose to handle node orderings in generation by marginalizing over a family of canonical orderings. On standard benchmarks, we achieve state-of-the-art time efficiency and sample quality compared to previous models. Additionally, we show our model is capable of generating large graphs of up to 5K nodes with good quality.	[Liao, Renjie; Wang, Shenlong; Duvenaud, David; Urtasun, Raquel; Zemel, Richard] Univ Toronto, Toronto, ON, Canada; [Liao, Renjie; Wang, Shenlong; Urtasun, Raquel] Uber ATG Toronto, Toronto, ON, Canada; [Liao, Renjie; Wang, Shenlong; Duvenaud, David; Urtasun, Raquel; Zemel, Richard] Vector Inst, Toronto, ON, Canada; [Li, Yujia] DeepMind, London, England; [Song, Yang] Stanford Univ, Stanford, CA 94305 USA; [Hamilton, William L.] McGill Univ, Montreal, PQ, Canada; [Hamilton, William L.] Mila Quebec Artificial Intelligence Inst, Montreal, PQ, Canada; [Zemel, Richard] Canadian Inst Adv Res, Toronto, ON, Canada	University of Toronto; Stanford University; McGill University; Canadian Institute for Advanced Research (CIFAR)	Liao, R (corresponding author), Univ Toronto, Toronto, ON, Canada.; Liao, R (corresponding author), Uber ATG Toronto, Toronto, ON, Canada.; Liao, R (corresponding author), Vector Inst, Toronto, ON, Canada.	rjliao@cs.toronto.edu; yujiali@google.com; yangsong@cs.stanford.edu; slwang@cs.toronto.edu; wlh@cs.mcgill.ca; duvenaud@cs.toronto.edu; urtasun@cs.toronto.edu; zemel@cs.toronto.edu			Connaught International Scholarship; RBC Fellowship; Canada CIFAR Chair in Al; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PCO0003]	Connaught International Scholarship; RBC Fellowship; Canada CIFAR Chair in Al; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)	RL was supported by Connaught International Scholarship and RBC Fellowship. WLH was supported by a Canada CIFAR Chair in Al. RL, RU and RZ were supported in part by the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PCO0003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: the views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; [Anonymous], 2019, ARXIV190401569; [Anonymous], 2008, P 7 PYTH SCI C SCIPY; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Batagelj V., 2003, CSDS0310049 CORR; Chu Hang, 2019, ICCV; Dai H., 2018, P 6 INT C LEARN REPR; Dobson PD, 2003, J MOL BIOL, V330, P771, DOI 10.1016/S0022-2836(03)00628-4; ERDOS P, 1960, B INT STATIST INST, V38, P343; Golomb S. W., 1996, POLYOMINOES PUZZLES, V16; Gomez-Bombarelli R, 2018, ACS CENTRAL SCI, V4, P268, DOI 10.1021/acscentsci.7b00572; Grover Aditya, 2018, ARXIV180310459; Gunnemann, 2018, ARXIV180300816; HOLLAND PW, 1983, SOC NETWORKS, V5, P109, DOI 10.1016/0378-8733(83)90021-7; Jin W., 2018, P 35 INT C MACHINE L; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2016, VARIATIONAL GRAPH AU; Kusner M. J., 2017, ICML; Leskovec J, 2010, J MACH LEARN RES, V11, P985; Li Y., 2018, ARXIV; Li YB, 2018, J CHEMINFORMATICS, V10, DOI 10.1186/s13321-018-0287-6; Liu J., 2019, GRAPH NORMALIZING FL; Liu Q, 2018, ADV NEURAL INFORM PR, V31; Ma Tengfei, 2018, ARXIV180902630; Murphy R. L., 2019, ARXIV190302541; Neumann M, 2013, INT WORKSH MIN LEARN; Segler MHS, 2018, ACS CENTRAL SCI, V4, P120, DOI 10.1021/acscentsci.7b00512; SEIDMAN SB, 1983, SOC NETWORKS, V5, P269, DOI 10.1016/0378-8733(83)90028-X; Simonovsky Martin, 2018, ARXIV180203480; Theis Lucas, 2015, ARXIV151101844; van den Oord Aron, 2016, NIPS; Velickovic P., 2017, STAT-US, V1050, P20; Wasserman S, 1996, PSYCHOMETRIKA, V61, P401, DOI 10.1007/BF02294547; Watts DJ, 1998, NATURE, V393, P440, DOI 10.1038/30918; You J., 2018, ICML, P5694; You J., 2018, CORR ABS180602473, P6410; Yule GU, 1925, PHILOS T R SOC LON B, V213, P21, DOI 10.1098/rstb.1925.0002	38	43	44	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304028
C	Aytar, Y; Pfaff, T; Budden, D; Paine, T; Wang, ZY; de Freitas, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Aytar, Yusuf; Pfaff, Tobias; Budden, David; Le Paine, Tom; Wang, Ziyu; de Freitas, Nando			Playing hard exploration games by watching YouTube	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep reinforcement learning methods traditionally struggle with tasks where environment rewards are particularly sparse. One successful method of guiding exploration in these domains is to imitate trajectories provided by a human demonstrator. However, these demonstrations are typically collected under artificial conditions, i.e. with access to the agent's exact environment setup and the demonstrator's action and reward trajectories. Here we propose a two-stage method that overcomes these limitations by relying on noisy, unaligned footage without access to such data. First, we learn to map unaligned videos from multiple sources to a common representation using self-supervised objectives constructed over both time and modality (i.e. vision and sound). Second, we embed a single YouTube video in this representation to construct a reward function that encourages an agent to imitate human gameplay. This method of one-shot imitation allows our agent to convincingly exceed human-level performance on the infamously hard exploration games MONTEZUMA'S REVENGE, PITFALL! and PRIVATE EYE for the first time, even if the agent is not presented with any environment rewards.	[Aytar, Yusuf; Pfaff, Tobias; Budden, David; Le Paine, Tom; Wang, Ziyu; de Freitas, Nando] DeepMind, London, England		Aytar, Y (corresponding author), DeepMind, London, England.	yusufaytar@google.com; tpfaff@google.com; budden@google.com; tpaine@google.com; ziyu@google.com; nandodefreitas@google.com						Abbeel P., 2004, P 21 INT C MACHINE L, P1; [Anonymous], 2014, ABS14123474 CORR; Arandjelovic R, 2017, IEEE I CONF COMP VIS, P609, DOI 10.1109/ICCV.2017.73; Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Aytar Y., 2017, IEEE T PATTERN ANAL; Aytar Yusuf, 2017, ARXIV170600932; Barth-Maron G., 2018, INT C LEARN REPR ICL; Bellemare M., 2016, NEURIPS; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bellemare Marc G, 2017, INT C MACH LEARN ICM; Doersch C, 2017, IEEE I CONF COMP VIS, P2070, DOI 10.1109/ICCV.2017.226; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Espeholt Lasse, 2018, CORR; Finn Chelsea, 2017, C ROB LEARN PMLR, P357; Ganin Y., 2016, JMLR, V17, P2096; Gruslys A., 2017, INT C LEARN REPR ICL; Gupta A., 2017, ARXIV170703374; Hadfield-Menell D., 2017, ADV NEURAL INFORM PR, P6768; Hessel M., 2017, P AAAI C ART INT; Hester Todd, 2017, P AAAI C ART INT; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Horgan D., 2018, INT C LEARN REPR ICL; HOSU I, 2016, CORR; Le Paine Tom, 2018, ARXIV181005017; Mathematicien E.-U, 1958, INTRO MULTIVARIATE S, V2; Osband Ian, 2017, ARXIV170307608; Owens A, 2016, LECT NOTES COMPUT SC, V9905, P801, DOI 10.1007/978-3-319-46448-0_48; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Pathak Deepak, 2018, ARXIV180408606; Pohlen T., 2018, ARXIV PREPRINT ARXIV; Savinov N., 2018, ARXIV180300653; Sermanet P., 2017, P IEEE INT C ROB AUT; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Singh S., 2009, P ANN C COGNITIVE SC, P2601; Stadie Bradly C, 2017, INT C LEARN REPR ICL; Torabi F., 2018, ARXIV180501954; Trigeorgis G, 2018, IEEE T PATTERN ANAL, V40, P1128, DOI 10.1109/TPAMI.2017.2710047; van den Oord Aaron, 2019, REPRESENTATION LEARN, P4; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vecerik Mel, 2017, LEVERAGING DEMONSTRA; Vondrick C., 2015, ARXIV150408023; Wang Ziyu, 2015, INT C MACH LEARN ICM; Yu T., 2018, ARXIV180201557; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou B., 2014, CORR, V1412, P6856; Zhou TH, 2016, PROC CVPR IEEE, P117, DOI 10.1109/CVPR.2016.20; Zhu J.-Y., 2017, ARXIV170310593; Ziebart B. D., 2008, AAAI, V8, P1433	49	43	44	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302091
C	Lin, HZ; Jegelka, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lin, Hongzhou; Jegelka, Stefanie			ResNet with one-neuron hidden layers is a Universal Approximator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORKS; BOUNDS	We demonstrate that a very deep ResNet with stacked modules that have one neuron per hidden layer and ReLU activation functions can uniformly approximate any Lebesgue integrable function in d dimensions, i.e. l(1)(R-d). Due to the identity mapping inherent to ResNets, our network has alternating layers of dimension one and d. This stands in sharp contrast to fully connected networks, which are not universal approximators if their width is the input dimension d [21, 11]. Hence, our result implies an increase in representational power for narrow deep networks by the ResNet architecture.	[Lin, Hongzhou; Jegelka, Stefanie] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Lin, HZ (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	hongzhou@mit.edu; stefje@mit.edu			Defense Advanced Research Projects Agency [YFA17 N66001-17-1-4039]	Defense Advanced Research Projects Agency(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA))	We would like to thank Jeffery Z. HaoChen for useful feedback and suggestions for this paper. This research was supported by The Defense Advanced Research Projects Agency (grant number YFA17 N66001-17-1-4039). The views, opinions, and/or findings contained in this article are those of the author and should not be interpreted as representing the official views or policies, either expressed or implied, of the Defense Advanced Research Projects Agency or the Department of Defense.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arora S, 2018, PR MACH LEARN RES, V80; Beise H., 2018, ARXIV180701194; Brutzkus A., 2018, ICLR; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Cohen N., 2016, C LEARNING THEORY, V49, P698; Cybenko G., 1989, Mathematics of Control, Signals, and Systems, V2, P303, DOI 10.1007/BF02551274; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; FUNAHASHI K, 1989, NEURAL NETWORKS, V2, P183, DOI 10.1016/0893-6080(89)90003-8; Hanin B, 2017, ARXIV171011278; HARDT M., 2017, P 5 INT C LEARN REPR; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Kawaguchi Kenji, 2016, ADV NEURAL INFORM PR, P586; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; KURKOVA V, 1992, NEURAL NETWORKS, V5, P501, DOI 10.1016/0893-6080(92)90012-8; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liang S., 2017, PROC INT C LEARN REP; Lu Z, 2017, ADV NEUR IN, V30; Mhaskar HN, 1996, NEURAL COMPUT, V8, P164, DOI 10.1162/neco.1996.8.1.164; Nguyen Q, 2017, PR MACH LEARN RES, V70; Rolnick D., 2018, 6 INT C LEARN REPR I; SHALEV-SHWARTZ S., 2017, ARXIV170600687; Shamir O., 2018, ARXIV180406739; Singh Aarti, 2017, ARXIV PREPRINT ARXIV; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Telgarsky M., 2016, C LEARN THEOR COLT; YAROTSKY D., 2018, ARXIV180203620; Yarotsky D, 2017, NEURAL NETWORKS, V94, P103, DOI 10.1016/j.neunet.2017.07.002; Yun C., 2018, INT C LEARN REPR ICL; Zhang Chiyuan, 2016, ARXIV161103530	35	43	43	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000065
C	Shamir, O		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Shamir, Ohad			Without-Replacement Sampling for Stochastic Gradient Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Stochastic gradient methods for machine learning and optimization problems are usually analyzed assuming data points are sampled with replacement. In contrast, sampling without replacement is far less understood, yet in practice it is very common, often easier to implement, and usually performs better. In this paper, we provide competitive convergence guarantees for without-replacement sampling under several scenarios, focusing on the natural regime of few passes over the data. Moreover, we describe a useful application of these results in the context of distributed optimization with randomly-partitioned data, yielding a nearly-optimal algorithm for regularized least squares (in terms of both communication complexity and runtime complexity) under broad parameter regimes. Our proof techniques combine ideas from stochastic optimization, adversarial online learning and transductive learning theory, and can potentially be applied to other stochastic optimization and learning problems.	[Shamir, Ohad] Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel	Weizmann Institute of Science	Shamir, O (corresponding author), Weizmann Inst Sci, Dept Comp Sci & Appl Math, Rehovot, Israel.	ohad.shamir@weizmann.ac.il			FP7 Marie Curie CIG grant; ISF [425/13]; Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	FP7 Marie Curie CIG grant; ISF(Israel Science Foundation); Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI)	This research is supported in part by an FP7 Marie Curie CIG grant, an ISF grant 425/13, and by the Intel Collaborative Research Institute for Computational Intelligence (ICRI-CI).	Agarwal A., 2011, CORR; [Anonymous], 2015, ARXIV150707595; Balcan M.-F., 2012, P C LEARN THEOR PRIN, V23; Bottou Leon, 2012, NEURAL NETWORKS TRIC, P421, DOI [10.1007/978-3-642-35289-8_25, DOI 10.1007/978-3-642-35289-8_25]; Cotter A., 2011, P NEURIPS GRAN SPAIN; Dekel O, 2012, J MACH LEARN RES, V13, P165; El-Yaniv R, 2009, J ARTIF INTELL RES, V35, P193, DOI 10.1613/jair.2587; Gross D., 2010, ARXIV10012738; Gurbuzbalaban M., 2015, ARXIV151008560; Hazan E., 2015, BOOK DRAFT; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Lacoste-Julien Simon, 2012, ARXIV12122002; Nedic A, 2001, APPL OPTIMIZAT, V54, P223; Rakhlin Alexander, 2011, ARXIV11095647; Recht B., 2012, COLT; Shalev-Shwartz S, 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir O., 2014, ALLERTON; Shamir O., 2013, P INT C MACH LEARN A, P71; Shamir O, 2014, PR MACH LEARN RES, V32, P1000; Vapnik VN, 1998, STAT LEARNING THEORY, DOI DOI 10.1007/978-1-4419-1428-6_5864; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Zhang, 2013, ADV NEURAL INFORM PR, P315; Zhang YC, 2015, PR MACH LEARN RES, V37, P353; Zhang YC, 2013, J MACH LEARN RES, V14, P3321	25	43	43	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701102
C	Liu, WW; Tsang, IW		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Liu, Weiwei; Tsang, Ivor W.			On the Optimality of Classifier Chain for Multi-label Classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					To capture the interdependencies between labels in multi-label classification problems, classifier chain (CC) tries to take the multiple labels of each instance into account under a deterministic high-order Markov Chain model. Since its performance is sensitive to the choice of label order, the key issue is how to determine the optimal label order for CC. In this work, we first generalize the CC model over a random label order. Then, we present a theoretical analysis of the generalization error for the proposed generalized model. Based on our results, we propose a dynamic programming based classifier chain (CC-DP) algorithm to search the globally optimal label order for CC and a greedy classifier chain (CC-Greedy) algorithm to find a locally optimal CC. Comprehensive experiments on a number of real-world multi-label data sets from various domains demonstrate that our proposed CC-DP algorithm outperforms state-of-the-art approaches and the CC-Greedy algorithm achieves comparable prediction performance with CC-DP.	[Liu, Weiwei; Tsang, Ivor W.] Univ Technol, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW, Australia	University of Technology Sydney	Liu, WW (corresponding author), Univ Technol, Ctr Quantum Computat & Intelligent Syst, Sydney, NSW, Australia.	liuweiwei863@gmail.com; ivor.tsang@uts.edu.au		Tsang, Ivor/0000-0001-8095-4637	Australian Research Council Future Fellowship [FT130100746]	Australian Research Council Future Fellowship(Australian Research Council)	This research was supported by the Australian Research Council Future Fellowship FT130100746.	Bartlett P, 1999, ADVANCES IN KERNEL METHODS, P43; Barutcuoglu Z, 2006, BIOINFORMATICS, V22, P830, DOI 10.1093/bioinformatics/btk048; Bennett KP, 2000, MACH LEARN, V41, P295, DOI 10.1023/A:1007600130808; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Cheng W., 2010, P 27 INT C MACH LEAR, P279; Guo Y., 2011, PROC 22 INT JOINT C, P1300, DOI DOI 10.5591/978-1-57735-516-8/IJCA111-220; Hsu D., 2009, P 22 INT C NEURAL IN, V22, P772; Huang SC, 2012, PROCEEDINGS OF THE 6TH INTERNATIONAL CONFERENCE ON APAC 2011; Kang F., 2006, 2006 IEEE COMPUTER S, P1719, DOI DOI 10.1109/CVPR.2006.90; Kearns M. J., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P382, DOI 10.1109/FSCS.1990.89557; Liu WW, 2015, AAAI CONF ARTIF INTE, P2800; Mao Q, 2013, IEEE T IMAGE PROCESS, V22, P1583, DOI 10.1109/TIP.2012.2233490; Read J, 2009, LECT NOTES ARTIF INT, V5782, P254, DOI 10.1007/978-3-642-04174-7_17; Schapire RE, 2000, MACH LEARN, V39, P135, DOI 10.1023/A:1007649029923; Tai F, 2012, NEURAL COMPUT, V24, P2508, DOI 10.1162/NECO_a_00320; Tan Mingkui, 2015, IEEE C COMP VIS PATT; Tsoumakas G, 2010, DATA MINING AND KNOWLEDGE DISCOVERY HANDBOOK, SECOND EDITION, P667, DOI 10.1007/978-0-387-09823-4_34; Yi Z., 2011, J MACHINE LEARNING R, V15, P873; Zhang M.L., 2010, KDD, P999; Zhang Y., 2012, P 29 INT C MACH LEAR, P1575	22	43	43	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103039
C	Lu, W; Rajapakse, JC		Leen, TK; Dietterich, TG; Tresp, V		Lu, W; Rajapakse, JC			Constrained independent component analysis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO			independent component analysis; constrained independent component analysis; constrained optimization; Lagrange multiplier methods		The paper presents a novel technique of constrained independent component analysis (CICA) to introduce constraints into the classical ICA and solve the constrained optimization problem by using Lagrange multiplier methods. This paper shows that CICA can be used to order the resulted independent components in a specific manner and normalize the demixing matrix in the signal separation procedure. It can systematically eliminate the ICA's indeterminacy on permutation and dilation. The experiments demonstrate the use of CICA in ordering of independent components while providing normalized demixing processes.	Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore	Nanyang Technological University & National Institute of Education (NIE) Singapore; Nanyang Technological University	Lu, W (corresponding author), Nanyang Technol Univ, Sch Comp Engn, Singapore 639798, Singapore.	asjagath@ntu.edu.sg	Rajapakse, Jagath C/B-8485-2008	Rajapakse, Jagath C/0000-0001-7944-1658				AMARI S, 1996, ADV NEURAL INFORMATI, V8; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bertsekas D.P., 2019, REINFORCEMENT LEARNI; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Hyvarinen A, 1996, INT J NEURAL SYST, V7, P671, DOI 10.1142/S0129065796000646; Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719; RAJAPAKSE JC, 2000, 2 INT ICSC S NEUR CO	7	43	43	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						570	576						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800081
C	Burges, CJC; Crisp, DJ		Solla, SA; Leen, TK; Muller, KR		Burges, CJC; Crisp, DJ			Uniqueness of the SVM solution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				SUPPORT VECTOR MACHINES; PATTERN-RECOGNITION	We give necessary and sufficient conditions for uniqueness of the support vector solution for the problems of pattern recognition and regression estimation, for a general class of cost functions. We show that if the solution is not unique, all support vectors are necessarily at bound, and we give some simple examples of non-unique solutions. We note that uniqueness of the primal (dual) solution does not necessarily imply uniqueness of the dual (primal) solution. We show how to compute the threshold b when the solution is unique, but when all support vectors are at bound, in which case the usual method for determining b does not work.	Lucent Technol, Bell Labs, Adv Technol, Holmdel, NJ USA; Univ Adelaide, Dept Elect Engn, Ctr Sensor Signal & Informat Proc, Adelaide, SA, Australia	Alcatel-Lucent; Lucent Technologies; AT&T; University of Adelaide	Burges, CJC (corresponding author), Lucent Technol, Bell Labs, Adv Technol, Holmdel, NJ USA.							BOSER B, 1992, 5 ANN WORKSH COMP LE; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Burges CJC, 1997, ADV NEUR IN, V9, P375; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Fletcher R, 1987, PRACTICAL METHODS OP, V1; Horn R. A., 1986, MATRIX ANAL; MANGARASIAN OL, 1998, 9814 U WISC; SCHOLKOPF B, 1998, NC2TR1998031 NEUROCO; SMOLA A, 1998, IN PRESS STAT COMPUT; Smola AJ, 1998, ALGORITHMICA, V22, P211, DOI 10.1007/PL00013831; Vapnik V, 1997, ADV NEUR IN, V9, P281; Vapnik V.N, 1998, STAT LEARNING THEORY	12	43	44	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						223	229						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700032
C	Aljundi, R; Lin, M; Goujaud, B; Bengio, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Aljundi, Rahaf; Lin, Min; Goujaud, Baptiste; Bengio, Yoshua			Gradient based sample selection for online continual learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A continual learning agent learns online with a non-stationary and never-ending stream of data. The key to such learning process is to overcome the catastrophic forgetting of previously seen data, which is a well known problem of neural networks. To prevent forgetting, a replay buffer is usually employed to store the previous data for the purpose of rehearsal. Previous works often depend on task boundary and i.i.d. assumptions to properly select samples for the replay buffer. In this work, we formulate sample selection as a constraint reduction problem based on the constrained optimization view of continual learning. The goal is to select a fixed subset of constraints that best approximate the feasible region defined by the original constraints. We show that it is equivalent to maximizing the diversity of samples in the replay buffer with parameters gradient as the feature. We further develop a greedy alternative that is cheap and efficient. The advantage of the proposed method is demonstrated by comparing to other alternatives under the continual learning setting. Further comparisons are made against state of the art methods that rely on task boundaries which show comparable or even better results for our method.	[Aljundi, Rahaf] Katholieke Univ Leuven, Leuven, Belgium; [Aljundi, Rahaf; Lin, Min; Goujaud, Baptiste; Bengio, Yoshua] Mila, Montreal, PQ, Canada	KU Leuven	Aljundi, R (corresponding author), Katholieke Univ Leuven, Leuven, Belgium.	rahaf.al.jundi@gmail.com; mavenlin@gmail.com; baptiste.goujaud@gmail.com; yoshua.bengio@mila.quebec			FWO	FWO(FWO)	Rahaf Aljundi is funded by FWO.	Aljundi Rahaf, 2019, ICLR; [Anonymous], 2017, P CVPR; [Anonymous], 1994, NETWORK; Beck Matthias, 2009, ARXIV09064031; Charikar M, 2004, SIAM J COMPUT, V33, P1417, DOI 10.1137/S0097539702418498; Chaudhry A., 2019, CORR; Chaudhry Arslan, 2018, ARXIV181200420, V2, P6; Farquhar Sebastian, 2018, ARXIV180509733; Isele D, 2018, 32 AAAI C ART INT; Lin L.J, 1993, P TECHN REP DTIC DOC, DOI 10.5555/168871; Lopez-Paz D., 2017, ADV NEURAL INF PROCE, P6467; Mallya A, 2018, PROC CVPR IEEE, P7765, DOI 10.1109/CVPR.2018.00810; Mnih V., 2013, ARXIV PREPRINT ARXIV; Nguyen C. V., 2017, ARXIV171010628; Ribando JM, 2006, DISCRETE COMPUT GEOM, V36, P479, DOI 10.1007/s00454-006-1253-4; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; VITTER JS, 1985, ACM T MATH SOFTWARE, V11, P37, DOI 10.1145/3147.3165	17	42	42	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903045
C	Guo, XX; Wu, H; Cheng, Y; Rennie, S; Tesauro, G; Feris, RS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Xiaoxiao; Wu, Hui; Cheng, Yu; Rennie, Steven; Tesauro, Gerald; Feris, Rogerio Schmidt			Dialog-based Interactive Image Retrieval	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				RELEVANCE FEEDBACK	Existing methods for interactive image retrieval have demonstrated the merit of integrating user feedback, improving retrieval results. However, most current systems rely on restricted forms of user feedback, such as binary relevance responses, or feedback based on a fixed set of relative attributes, which limits their impact. In this paper, we introduce a new approach to interactive image search that enables users to provide feedback via natural language, allowing for more natural and effective interaction. We formulate the task of dialog-based interactive image retrieval as a reinforcement learning problem, and reward the dialog system for improving the rank of the target image during each dialog turn. To mitigate the cumbersome and costly process of collecting human-machine conversations as the dialog system learns, we train our system with a user simulator, which is itself trained to describe the differences between target and candidate images. The efficacy of our approach is demonstrated in a footwear retrieval application. Experiments on both simulated and real-world data show that 1) our proposed learning framework achieves better accuracy than other supervised and reinforcement learning baselines and 2) user feedback based on natural language rather than pre-specified attributes leads to more effective retrieval results, and a more natural and expressive communication interface.	[Guo, Xiaoxiao; Wu, Hui; Cheng, Yu; Tesauro, Gerald; Feris, Rogerio Schmidt] IBM Res AI, Yorktown Hts, NY 10598 USA; [Tesauro, Gerald] Fusemachines Inc, New York, NY USA		Guo, XX (corresponding author), IBM Res AI, Yorktown Hts, NY 10598 USA.	xiaoxiao.guo@ibm.com; wuhu@us.ibm.com; chengyu@us.ibm.com; srennie@gmail.com; gtesauro@us.ibm.com; rsferis@us.ibm.com						Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Barrett DP, 2016, IEEE T PATTERN ANAL, V38, P2069, DOI 10.1109/TPAMI.2015.2505297; Berg Tamara L, 2010, ECCV; Bordes Antoine, 2017, ICLR; Das  Abhishek, 2017, CVPR; Das  Abhishek, 2017, ICCV; de Vries Harm, 2017, CVPR; FLICKNER M, 1995, COMPUTER, V28, P23, DOI 10.1109/2.410146; Frome Andrea, 2013, NEURIPS; Gordo A, 2017, INT J COMPUT VISION, V124, P237, DOI 10.1007/s11263-017-1016-8; Gordo A, 2016, LECT NOTES COMPUT SC, V9910, P241, DOI 10.1007/978-3-319-46466-4_15; Guo X, 2014, ADV NEURAL INFORM PR, P3338; Guo Xiaoxiao, 2017, INT C LEARN REPR ICL; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hu R., 2016, CVPR; Huang JS, 2015, IEEE I CONF COMP VIS, P1062, DOI 10.1109/ICCV.2015.127; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kovashka A., 2012, CVPR; Kovashka A., 2013, ICCV; Kovashka A, 2017, ADV COMPUT VIS PATT, P89, DOI 10.1007/978-3-319-50077-5_5; Kulkarni G., 2011, P 24 CVPR CIT; Li C, 2014, SIGIR'14: PROCEEDINGS OF THE 37TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P163, DOI 10.1145/2600428.2609618; Li Chenliang, 2016, CIKM; Li S., 2017, ARXIV170205729; Li Xiujun, 2016, ARXIV161205688; Liu ZW, 2016, PROC CVPR IEEE, P1096, DOI 10.1109/CVPR.2016.124; Maji  Subhransu, 2012, ECCV; Parikh D, 2011, IEEE I CONF COMP VIS, P503, DOI 10.1109/ICCV.2011.6126281; Plummer BA, 2015, IEEE I CONF COMP VIS, P2641, DOI 10.1109/ICCV.2015.303; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; Rohrbach Anna, 2016, ECCV; Rui Y, 1999, J VIS COMMUN IMAGE R, V10, P39, DOI 10.1006/jvci.1999.0413; Rui Y, 1998, IEEE T CIRC SYST VID, V8, P644, DOI 10.1109/76.718510; Serban I.V., 2016, AAAI; Shi Z., 2015, CVPR; Sigal L., 2017, NIPS; Strub Florian, 2017, IJCAI; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tapaswi M., 2016, IEEE C COMP VIS PATT; Tellex Stefanie, 2009, P ACM INT C IM VID R, P1; Thomee B, 2012, INT J MULTIMED INF R, V1, P71, DOI 10.1007/s13735-012-0014-4; Vaquero D. A., 2009, WACV; Vedantam Ramakrishna, 2017, CVPR; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang J, 2014, PROC CVPR IEEE, P1386, DOI 10.1109/CVPR.2014.180; Wang L, 2016, PROC CVPR IEEE, P5005, DOI 10.1109/CVPR.2016.541; Wang Xuanhui, 2018, CIKM; Williams JD, 2007, COMPUT SPEECH LANG, V21, P393, DOI 10.1016/j.csl.2006.06.008; Wu  Hong, 2004, ICPR; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang XS, 2016, IEEE T MULTIMEDIA, V18, P1832, DOI 10.1109/TMM.2016.2582379; Yu A, 2017, ADV COMPUT VIS PATT, P119, DOI 10.1007/978-3-319-50077-5_6; Yu LC, 2016, LECT NOTES COMPUT SC, V9906, P69, DOI 10.1007/978-3-319-46475-6_5; Zhou XS, 2003, MULTIMEDIA SYST, V8, P536, DOI 10.1007/s00530-002-0070-3	55	42	42	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300063
C	Hanin, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hanin, Boris			Which Neural Net Architectures Give Rise to Exploding and Vanishing Gradients?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We give a rigorous analysis of the statistical behavior of gradients in a randomly initialized fully connected network N with ReLU activations. Our results show that the empirical variance of the squares of the entries in the input-output Jacobian of N is exponential in a simple architecture-dependent constant beta, given by the sum of the reciprocals of the hidden layer widths. When beta is large, the gradients computed by N at initialization vary wildly. Our approach complements the mean field theory analysis of random networks. From this point of view, we rigorously compute finite width corrections to the statistics of gradients at the edge of chaos.	[Hanin, Boris] Texas A&M Univ, Dept Math, College Stn, TX 77843 USA	Texas A&M University System; Texas A&M University College Station	Hanin, B (corresponding author), Texas A&M Univ, Dept Math, College Stn, TX 77843 USA.	bhanin@math.tamu.edu						[Anonymous], 1991, SEPP HOCHREITERS FUN; Arjovsky M, 2016, PR MACH LEARN RES, V48; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Ganguli, 2017, ADV NEURAL INFORM PR, P4785; Hanin  Boris, 2018, ADV NEURAL INFORM PR, P32; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Henaff M, 2016, PR MACH LEARN RES, V48; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Kolen J.F., 2001, GRADIENT FLOW RECURR, P237, DOI [10.1109/9780470544037.ch14, DOI 10.1109/9780470544037.CH14]; Mishkin Dmytro, 2015, ICLR; Perez-Cruz F, 2018, INT C ART INT STAT, V84, P1924; Poole B, 2016, ADV NEUR IN, V29; Raghu M, 2017, PR MACH LEARN RES, V70; Schoenholz S. S., 2017, ARXIV171006570; Xie Di, 2017, CVPR	21	42	43	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300054
C	Mania, H; Guy, A; Recht, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Mania, Horia; Guy, Aurelia; Recht, Benjamin			Simple random search of static linear policies is competitive for reinforcement learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Model-free reinforcement learning aims to offer off-the-shelf solutions for controlling dynamical systems without requiring models of the system dynamics. We introduce a model-free random search algorithm for training static, linear policies for continuous control problems. Common evaluation methodology shows that our method matches state-of-the-art sample efficiency on the benchmark MuJoCo locomotion tasks. Nonetheless, more rigorous evaluation reveals that the assessment of performance on these benchmarks is optimistic. We evaluate the performance of our method over hundreds of random seeds and many different hyperparameter configurations for each benchmark task. This extensive evaluation is possible because of the small computational footprint of our method. Our simulations highlight a high variability in performance in these benchmark tasks, indicating that commonly used estimations of sample efficiency do not adequately evaluate the performance of RL algorithms. Our results stress the need for new baselines, benchmarks and evaluation methodology for RL algorithms.	[Mania, Horia; Guy, Aurelia; Recht, Benjamin] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Mania, H (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	hmania@berkeley.edu; lia@berkeley.edu; brecht@berkeley.edu			NSF CISE Expeditions Award [CCF-1730628]; DHS Award [HSHQDC-16-3-00083]; NSF [CCF-1359814]; ONR [N00014-14-1-0024, N00014-17-1-2191]; DARPA Fundamental Limits of Learning (Fun LoL) Program; Amazon AWS AI Research Award	NSF CISE Expeditions Award; DHS Award; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA Fundamental Limits of Learning (Fun LoL) Program; Amazon AWS AI Research Award	We thank Orianna DeMasi, Moritz Hardt, Eric Jonas, Robert Nishihara, Rebecca Roelofs, Esther Rolf, Vaishaal Shankar, Ludwig Schmidt, Nilesh Tripuraneni, Stephen Tu for many helpful comments and suggestions. HM thanks Robert Nishihara and Vaishaal Shankar for sharing their expertise in parallel computing. As part of the RISE lab, HM is generally supported in part by NSF CISE Expeditions Award CCF-1730628, DHS Award HSHQDC-16-3-00083, and gifts from Alibaba, Amazon Web Services, Ant Financial, CapitalOne, Ericsson, GE, Google, Huawei, Intel, IBM, Microsoft, Scotiabank, Splunk and VMware. BR is generously supported in part by NSF award CCF-1359814, ONR awards N00014-14-1-0024 and N00014-17-1-2191, the DARPA Fundamental Limits of Learning (Fun LoL) Program, and an Amazon AWS AI Research Award.	Agarwal A., 2010, OPTIMAL ALGORITHMS O, P28; [Anonymous], 2017, ARXIV170802596; [Anonymous], 2014, P INT C MACH LEARN; Bach F., 2016, C LEARN THEOR; Brockman G., 2016, OPENAI GYM; Dean S., 2017, ARXIV171001688; Duan Y, 2016, INT C MACH LEARN, P1329; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Gu S, 2016, INT C LEARN REPR; Haarnoja T., 2018, P 35 INT C MACH LEAR; Haarnoja T., 2017, P INT C MACH LEARN; Heess N., 2017, ABS170702286 CORR; Henderson P., 2017, ABS170906560 CORR; Islam R., 2017, ARXIV PREPRINT ARXIV; Jamieson K. G., 2012, QUERY COMPLEXITY DER, P2672; MATYAS J, 1965, AUTOMAT REM CONTR+, V26, P244; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Plappert Matthias, 2017, ARXIV170601905; Pritzel Alexander, 2016, INT C LEARNING REPRE; Rajeswaran A., 2017, ADV NEURAL INFORM PR; Salimans T., 2017, ARXIV170303864; Schulman J., 2017, ABS170706347 CORR; Schulman J., 2015, INT C LEARN REPR; Schulman J., 2015, TRUST REGION POLICY, P1889; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Tu Stephen, 2017, ARXIV171208642; Tumanov A., 2017, ABS171205889 CORR; Wang Z., 2016, INT C LEARN REPR; Wu Y., 2017, ADV NEURAL INFORM PR	32	42	42	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301076
C	Tao, GH; Ma, SQ; Liu, YQ; Zhang, XY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tao, Guanhong; Ma, Shiqing; Liu, Yingqi; Zhang, Xiangyu			Attacks Meet Interpretability: Attribute-steered Detection of Adversarial Samples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Adversarial sample attacks perturb benign inputs to induce DNN misbehaviors. Recent research has demonstrated the widespread presence and the devastating consequences of such attacks. Existing defense techniques either assume prior knowledge of specific attacks or may not work well on complex models due to their underlying assumptions. We argue that adversarial sample attacks are deeply entangled with interpretability of DNN models: while classification results on benign inputs can be reasoned based on the human perceptible features/attributes, results on adversarial samples can hardly be explained. Therefore, we propose a novel adversarial sample detection technique for face recognition models, based on interpretability. It features a novel bi-directional correspondence inference between attributes and internal neurons to identify neurons critical for individual attributes. The activation values of critical neurons are enhanced to amplify the reasoning part of the computation and the values of other neurons are weakened to suppress the uninterpretable part. The classification results after such transformation are compared with those of the original model to detect adversaries. Results show that our technique can achieve 94% detection accuracy for 7 different kinds of attacks with 9.91% false positives on benign inputs. In contrast, a state-of-the-art feature squeezing technique can only achieve 55% accuracy with 23.3% false positives.	[Tao, Guanhong; Ma, Shiqing; Liu, Yingqi; Zhang, Xiangyu] Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA	Purdue University System; Purdue University; Purdue University West Lafayette Campus	Tao, GH (corresponding author), Purdue Univ, Dept Comp Sci, W Lafayette, IN 47907 USA.	taog@cs.purdue.edu; ma229@cs.purdue.edu; liu1751@cs.purdue.edu; xyzhang@cs.purdue.edu			DARPA [FA8650-15-C-7562]; NSF [1748764, 1409668]; ONR [N000141410468, N000141712947]; Sandia National Lab [1701331]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Sandia National Lab(United States Department of Energy (DOE))	We thank the anonymous reviewers for their constructive comments. This research was supported, in part, by DARPA under contract FA8650-15-C-7562, NSF under awards 1748764 and 1409668, ONR under contracts N000141410468 and N000141712947, and Sandia National Lab under award 1701331. Any opinions, findings, and conclusions in this paper are those of the authors only and do not necessarily reflect the views of our sponsors.	Bau D, 2017, PROC CVPR IEEE, P3319, DOI 10.1109/CVPR.2017.354; Brown TB, 2017, CSCV171209665 ARXIV; Buades A, 2005, PROC CVPR IEEE, P60, DOI 10.1109/cvpr.2005.38; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Du M, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1285, DOI 10.1145/3133956.3134015; Erhan D, 2009, 1341 U MONTR, V1341, P1, DOI DOI 10.2464/JILM.23.425; Faghri Fartash, 2016, ARXIV PREPRINT ARXIV; Feinman R., 2017, ARXIV PREPRINT ARXIV; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Grosse Kathrin, 2017, ARXIV170206280; Gu S., 2014, ARXIV14125068; Huang Gary B., 2007, 0749 U MASS, P7; Kalchbrenner N, 2014, PROCEEDINGS OF THE 52ND ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P655, DOI 10.3115/v1/p14-1062; King DE, 2009, J MACH LEARN RES, V10, P1755; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kumar N, 2009, IEEE I CONF COMP VIS, P365, DOI 10.1109/ICCV.2009.5459250; Kurakin A., 2016, ARXIV PREPRINT ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu YQ, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23291; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Madry Aleksander, 2017, ARXIV; Mahendran A, 2016, INT J COMPUT VISION, V120, P233, DOI 10.1007/s11263-016-0911-8; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Nasrabadi Nasser M, 2007, J ELECTRON IMAGING, V16; Nguyen A, 2016, ARXIV160203616, V29, P1; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Parkhi Omkar M., 2015, BRIT MACH VIS C; Pei KX, 2017, PROCEEDINGS OF THE TWENTY-SIXTH ACM SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES (SOSP '17), P1, DOI 10.1145/3132747.3132785; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sharif M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1528, DOI 10.1145/2976749.2978392; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Takeda H, 2007, IEEE T IMAGE PROCESS, V16, P349, DOI 10.1109/TIP.2006.888330; Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198; Yosinski J., 2015, ICML DEEP LEARN WORK; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	37	42	44	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002028
C	Yin, Z; Shen, YY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yin, Zi; Shen, Yuanyuan			On the Dimensionality of Word Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we provide a theoretical understanding of word embedding and its dimensionality. Motivated by the unitary-invariance of word embedding, we propose the Pairwise Inner Product (PIP) loss, a novel metric on the dissimilarity between word embeddings. Using techniques from matrix perturbation theory, we reveal a fundamental bias-variance trade-off in dimensionality selection for word embeddings. This bias-variance trade-off sheds light on many empirical observations which were previously unexplained, for example the existence of an optimal dimensionality. Moreover, new insights and discoveries, like when and how word embeddings are robust to over-fitting, are revealed. By optimizing over the biasvariance trade-off of the PIP loss, we can explicitly answer the open question of dimensionality selection for word embedding.	[Yin, Zi; Shen, Yuanyuan] Stanford Univ, Stanford, CA 94305 USA; [Shen, Yuanyuan] Microsoft Corp, Redmond, WA 98052 USA	Stanford University; Microsoft	Yin, Z (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	s0960974@gmail.com; Yuanyuan.Shen@microsoft.com						Arora Sanjeev, 2016, WORD EMBEDDINGS EXPL; Artetxe Mikel, 2016, P 2016 C EMP METH NA, P2289, DOI DOI 10.18653/V1/D16-1250; Bengio Y, 2001, ADV NEUR IN, V13, P932; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Breese J.S., 2013, EMPIRICAL ANAL PREDI; Bullinaria JA, 2012, BEHAV RES METHODS, V44, P890, DOI 10.3758/s13428-011-0183-8; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes EJ, 2009, FOUND COMPUT MATH, V9, P717, DOI 10.1007/s10208-009-9045-5; Caron J, 2001, COMPUTATIONAL INFORMATION RETRIEVAL, P157; Chatterjee S, 2015, ANN STAT, V43, P177, DOI 10.1214/14-AOS1272; DAVIS C, 1970, SIAM J NUMER ANAL, V7, P1, DOI 10.1137/0707001; Dhillon PS, 2015, J MACH LEARN RES, V16, P3035; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Finkelstein Lev, 2001, P 10 INT C WORLD WID, P406, DOI DOI 10.1145/371920.372094; Frome Andrea, 2013, NEURIPS; Halawi Guy, 2012, P 18 ACM SIGKDD INT, P1406, DOI [10.1145/371920.372094, DOI 10.1145/371920.372094, 10.1145/2339530.2339751, DOI 10.1145/2339530.2339751]; Hamilton WL, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1489; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kato T., 1966, PERTURBATION THEORY; Kong WH, 2017, ANN STAT, V45, P2218, DOI 10.1214/16-AOS1525; Lample G., 2016, C N AM CHAPTER ASS C, P260, DOI [10.18653/v1/N16-1030, 10.18653/v1/n16-1030, DOI 10.18653/V1/N16-1030]; Levy O., 2015, T ASSOC COMPUT LING, V3, P211, DOI [10.1162/tacl_a_00134, DOI 10.1162/TACL_A_00134]; Levy O, 2014, ADV NEUR IN, V27; Mahoney Matt, 2011, LARGE TEXT COMPRESSI; Mikolov T., 2013, ARXIV; Mikolov T., 2013, ARXIV PREPRINT ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mirsky L., 1960, Q J MATH, V11, P50, DOI [10.1093/qmath/11.1.50, DOI 10.1093/QMATH/11.1.50]; PAIGE CC, 1994, LINEAR ALGEBRA APPL, V209, P303; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Salton G., 1971, SMART RETRIEVAL SYST; Schnabel Tobias, 2015, P 2015 C EMP METH NA, P298, DOI DOI 10.18653/V1/D15-1036; Smith Samuel L, 2017, ARXIV170203859; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Stewart G., 1990, MATRIX PERTURBATION; STEWART GW, 1990, SIAM REV, V32, P579, DOI 10.1137/1032121; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Turney PD, 2012, J ARTIF INTELL RES, V44, P533, DOI 10.1613/jair.3640; Turney PD, 2010, J ARTIF INTELL RES, V37, P141, DOI 10.1613/jair.2934; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Weyl H, 1912, MATH ANN, V71, P441, DOI 10.1007/BF01456804; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yin Z, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P2131, DOI 10.1145/3097983.3098148; Zi Yin, 2018, ARXIV180300502	50	42	45	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300082
C	Zhang, Z; Wang, MZ; Xiang, YJ; Huang, Y; Nehorai, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Zhen; Wang, Mianzhi; Xiang, Yijian; Huang, Yan; Nehorai, Arye			RetGK: Graph Kernels based on Return Probabilities of Random Walks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Graph-structured data arise in wide applications, such as computer vision, bioinformatics, and social networks. Quantifying similarities among graphs is a fundamental problem. In this paper, we develop a framework for computing graph kernels, based on return probabilities of random walks. The advantages of our proposed kernels are that they can effectively exploit various node attributes, while being scalable to large datasets. We conduct extensive graph classification experiments to evaluate our graph kernels. The experimental results show that our graph kernels significantly outperform existing state-of-the-art approaches in both accuracy and computational efficiency.	[Zhang, Zhen; Wang, Mianzhi; Xiang, Yijian; Huang, Yan; Nehorai, Arye] Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63130 USA	Washington University (WUSTL)	Zhang, Z (corresponding author), Washington Univ, Dept Elect & Syst Engn, St Louis, MO 63130 USA.	zhen.zhang@wustl.edu; mianzhi.wang@wustl.edu; yijian.xiang@wustl.edu; yanhuang640@wustl.edu; nehorai@wustl.edu	Xiang, Yijian/AAQ-3641-2020	Wang, Mianzhi/0000-0002-3317-7035	AFOSR [FA9550-16-1-0386]	AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was supported in part by the AFOSR grant FA9550-16-1-0386.	Borgwardt K. M., 2005, 5 IEEE INT C DAT MIN, DOI DOI 10.1109/ICDM.2005.132; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chung F.R.K., 1997, AM MATH SOC, DOI DOI 10.1090/CBMS/092; Cinlar E., 2013, INTRO STOCHASTIC PRO; Da San Martino  Giovanni, 2017, IEEE T NEURAL NETWOR; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Dobson Paul D, 2003, J Mol Biol, V330, P771; Feragen Aasa, 2013, NIPS, P216; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Gretton A, 2012, J MACH LEARN RES, V13, P723; Haussler D, 1999, CONVOLUTION KERNELS, P95; Helma C, 2001, BIOINFORMATICS, V17, P107, DOI 10.1093/bioinformatics/17.1.107; Kandola J, 2003, LECT NOTES ARTIF INT, V2777, P288, DOI 10.1007/978-3-540-45167-9_22; Kazius J, 2005, J MED CHEM, V48, P312, DOI 10.1021/jm040835a; Kersting Kristian, 2016, BENCHMARK DATA SETS; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Kriege N.M., 2012, INT C MACHINE LEARNI, P291; Morris C, 2016, IEEE DATA MINING, P1095, DOI [10.1109/ICDM.2016.0142, 10.1109/ICDM.2016.114]; Niepert M, 2016, PR MACH LEARN RES, V48; Nikolentzos G, 2017, AAAI CONF ARTIF INTE, P2429; Orsini F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3756; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Schieber TA, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms13928; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Sutherland JJ, 2003, J CHEM INF COMP SCI, V43, P1906, DOI 10.1021/ci034143r; Szabo  Zoltan, 2017, ARXIV170808157; Verma S, 2017, ADV NEUR IN, V30; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Yanardag P., 2015, ADV NEURAL INFORM PR, P2134; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Zhang M., 2018, P AAAI C ART INT	33	42	43	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304001
C	Gal, Y; Hron, J; Kendall, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gal, Yarin; Hron, Jiri; Kendall, Alex			Concrete Dropout	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Dropout is used as a practical tool to obtain uncertainty estimates in large vision models and reinforcement learning (RL) tasks. But to obtain well-calibrated uncertainty estimates, a grid-search over the dropout probabilities is necessary- a prohibitive operation with large models, and an impossible one with RL. We propose a new dropout variant which gives improved performance and better calibrated uncertainties. Relying on recent developments in Bayesian deep learning, we use a continuous relaxation of dropout's discrete masks. Together with a principled optimisation objective, this allows for automatic tuning of the dropout probability in large models, and as a result faster experimentation cycles. In RL this allows the agent to adapt its uncertainty dynamically as more data is observed. We analyse the proposed variant extensively on a range of tasks, and give insights into common practice in the field where larger dropout probabilities are often used in deeper model layers.	[Gal, Yarin; Hron, Jiri; Kendall, Alex] Univ Cambridge, Cambridge, England; [Gal, Yarin] Alan Turing Inst, London, England	University of Cambridge	Gal, Y (corresponding author), Univ Cambridge, Cambridge, England.; Gal, Y (corresponding author), Alan Turing Inst, London, England.	yarin.gal@eng.cam.ac.uk; jh2084@cam.acuk; agk34@cam.ac.uk	Jeong, Yongwook/N-7413-2016					Amodei D., 2016, CONCRETE PROBLEMS AI; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, TECHNICAL REPORT; [Anonymous], 2015, FRANCOIS CHOLLET; Ba J., 2013, ADV NEURAL INFORM PR, P3084; Beal Matthew J., 2003, BAYESIAN STAT; Bui TD, 2016, PR MACH LEARN RES, V48; Burges, 1998, MNIST DATABASE HANDW; Fu MC, 2006, HBK OPERAT RES MANAG, V13, P575, DOI 10.1016/S0927-0507(06)13019-4; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gal Y., 2016, THESIS, V1, P3; Gal Y, 2016, PR MACH LEARN RES, V48; Gal Yarin, 2016, ADV NEURAL INFORM PR, P1019, DOI DOI 10.5555/3157096.3157211; Gao H., 2016, ARXIV160806993; Glasserman P., 2013, MONTE CARLO METHODS, V53; GLYNN PW, 1990, COMMUN ACM, V33, P75, DOI 10.1145/84537.84552; Hernandez-Lobato JM, 2015, PR MACH LEARN RES, V37, P1699; Hinton GE, 2012, IMPROVING NEURAL NET, DOI DOI 10.9774/GLEAF.978-1-909493-38-4_2; Jang Eric, 2016, BAYES DEEP LEARN WOR; Jegou S., 2016, ARXIV161109326; Kahn G, 2017, ARXIV PREPRINT ARXIV; Kampffmeyer M, 2016, IEEE COMPUT SOC CONF, P680, DOI 10.1109/CVPRW.2016.90; Kendall A, 2015, P BRIT MACH VIS C 20; Kendall Alex, 2017, ADV NEURAL INFORM PR, DOI DOI 10.5555/3295222.3295309; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Li Yingzhen, 2017, 170302914 ARXIV; Lichman M, 2013, UCI MACHINE LEARNING; Maddison Chris J., 2016, BAYES DEEP LEARN WOR; Molchanov Dmitry, 2016, BAYES DEEP LEARN WOR; Paisley J., 2012, ARXIV12066430; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Titsias MK, 2014, PR MACH LEARN RES, V32, P1971; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	35	42	42	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403063
C	Rudi, A; Rosasco, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rudi, Alessandro; Rosasco, Lorenzo			Generalization Properties of Learning with Random Features	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				APPROXIMATION	( )We study the generalization properties of ridge regression with random features in the statistical learning framework. We show for the first time that O(1/root n) learning bounds can be achieved with only O(root n log n) random features rather than O(n) as suggested by previous results. Further, we prove faster learning rates and show that they might require more random features, unless they are sampled according to a possibly problem dependent distribution. Our results shed light on the statistical computational trade-offs in large scale kernelized learning, showing the potential effectiveness of random features in reducing the computational complexity while keeping optimal generalization properties.	[Rudi, Alessandro] Ecole Normale Super, INRIA, Sierra Project Team, F-75012 Paris, France; [Rosasco, Lorenzo] Univ Genoa, Ist Italiano Tecnol, Genoa, Italy; [Rosasco, Lorenzo] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); Istituto Italiano di Tecnologia - IIT; University of Genoa; Massachusetts Institute of Technology (MIT)	Rudi, A (corresponding author), Ecole Normale Super, INRIA, Sierra Project Team, F-75012 Paris, France.	alessandro.rudi@inria.fr; lrosasco@mit.edu	Jeong, Yongwook/N-7413-2016; Rudi, Alessandro/J-7323-2013	Rudi, Alessandro/0000-0002-3879-7794	Air Force project (European Office of Aerospace Research and Development) [FA9550-17-1-0390]; FIRB project (Italian Ministry of Education, University and Research) [RBFR12M3AC]	Air Force project (European Office of Aerospace Research and Development); FIRB project (Italian Ministry of Education, University and Research)	The authors gratefully acknowledge the contribution of Raffaello Camoriano who was involved in the initial phase of this project. These preliminary result appeared in the 2016 NIPS workshop "Adaptive and Scalable Nonparametric Methods in ML". This work is funded by the Air Force project FA9550-17-1-0390 (European Office of Aerospace Research and Development) and by the FIRB project RBFR12M3AC (Italian Ministry of Education, University and Research).	Alaoui A., 2015, NIPS; [Anonymous], 1998, STAT LEARNING THEORY; ARONSZAJN N, 1950, T AM MATH SOC, V68, P337, DOI 10.1090/s0002-9947-1950-0051437-7; Bach F., 2015, ARXIV E PRINTS; Bach F., 2013, COLT; Bhatia R., 2013, MATRIX ANAL, V169; Bishop C.M, 2006, PATTERN RECOGN; Boucheron Stephane, 2004, ADV LECT MACHINE LEA; Caponnetto A., 2007, OPTIMAL RATES REGULA; Caponnetto Andrea, 2006, ADAPTATION REGULARIZ; Cho Y., 2009, NIPS, P342; Cortes Corinna, 2010, AISTATS; Cucker F, 2002, B AM MATH SOC, V39, P1; Drineas P, 2012, J MACH LEARN RES, V13, P3475; FUJII J, 1993, P AM MATH SOC, V118, P827, DOI 10.2307/2160128; Halko N, 2011, SIAM REV, V53, P217, DOI 10.1137/090771806; Hamid R, 2014, PR MACH LEARN RES, V32, P19; Kar P., 2012, AISTATS; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Le Q., 2013, ICML; Minsker S., 2011, SOME EXTENSIONS BERN; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Plan Y, 2014, DISCRETE COMPUT GEOM, V51, P438, DOI 10.1007/s00454-013-9561-6; Poggio T., 1990, P IEEE; Raginsky M., 2009, NIPS; Rahimi A., 2007, NIPS; Rahimi A., 2009, NIPS; Rudi A., 2015, NIPS; Scholkopf B, 2001, LECT NOTES ARTIF INT, V2111, P416, DOI 10.1007/3-540-44581-1_27; Scholkopf B., 2001, LEARNING KERNELS SUP; Smale S, 2007, CONSTR APPROX, V26, P153, DOI 10.1007/s00365-006-0659-y; Smale S, 2003, ANAL APPL, V1, P17, DOI 10.1142/S0219530503000089; Smola A. J., 2000, ICML; Sriperumbudur B. K., 2015, ARXIV E PRINTS; Steinwart I., 2008, SUPPORT VECTOR MACHI; Steinwart I, 2006, IEEE T INFORM THEORY, V52, P4635, DOI 10.1109/TIT.2006.881713; Tropp J. A., 2012, USER FRIENDLY TOOLS; Vedaldi A, 2012, IEEE T PATTERN ANAL, V34, P480, DOI 10.1109/TPAMI.2011.153; Wahba G., 1990, SPLINE MODELS OBSERV, V59; Williams C. K. I., 2000, NIPS; Yang JY, 2014, PR MACH LEARN RES, V32; Yang JY, 2014, PROC CVPR IEEE, P971, DOI 10.1109/CVPR.2014.129; Yang Tianbao, 2012, NIPS, P485; Yurinsky, 1995, SUMS GAUSSIAN VECTOR	46	42	42	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403028
C	Meng, Q; Ke, GL; Wang, TF; Chen, W; Ye, QW; Ma, ZM; Liu, TY		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Meng, Qi; Ke, Guolin; Wang, Taifeng; Chen, Wei; Ye, Qiwei; Ma, Zhi-Ming; Liu, Tie-Yan			A Communication-Efficient Parallel Algorithm for Decision Tree	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Decision tree (and its extensions such as Gradient Boosting Decision Trees and Random Forest) is a widely used machine learning algorithm, due to its practical effectiveness and model interpretability. With the emergence of big data, there is an increasing need to parallelize the training process of decision tree. However, most existing attempts along this line suffer from high communication costs. In this paper, we propose a new algorithm, called Parallel Voting Decision Tree (PV-Tree), to tackle this challenge. After partitioning the training data onto a number of (e.g., M) machines, this algorithm performs both local voting and global voting in each iteration. For local voting, the top-k attributes are selected from each machine according to its local data. Then, globally top-2k attributes are determined by a majority voting among these local candidates. Finally, the full-grained histograms of the globally top-2k attributes are collected from local machines in order to identify the best (most informative) attribute and its split point. PV-Tree can achieve a very low communication cost (independent of the total number of attributes) and thus can scale out very well. Furthermore, theoretical analysis shows that this algorithm can learn a near optimal decision tree, since it can find the best attribute with a large probability. Our experiments on real-world datasets show that PV-Tree significantly outperforms the existing parallel decision tree algorithms in the trade-off between accuracy and efficiency.	[Meng, Qi] Peking Univ, Beijing, Peoples R China; [Ke, Guolin; Wang, Taifeng; Chen, Wei; Ye, Qiwei; Liu, Tie-Yan] Microsoft Res, Redmond, WA USA; [Ma, Zhi-Ming] Chinese Acad Math & Syst Sci, Beijing, Peoples R China	Peking University; Microsoft; Chinese Academy of Sciences; Academy of Mathematics & System Sciences, CAS	Meng, Q (corresponding author), Peking Univ, Beijing, Peoples R China.	qimeng13@pku.edu.cn; Guolin.Ke@microsoft.com; taifengw@microsoft.com; wche@microsoft.com; qiwye@microsoft.com; mazm@amt.ac.cn; tie-yan.liu@microsoft.com						Agrawal Rakesh, 2001, US Patent, Patent No. [6,230,151, 6230151]; Alsabti K., 1998, Proceedings Fourth International Conference on Knowledge Discovery and Data Mining, P2; Ben-Haim Y, 2010, J MACH LEARN RES, V11, P849; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Burges C., 2010, LEARNING, V11; Friedman J., 2001, ELEMENTS STAT LEARNI, V1; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gehrke J, 1999, SIGMOD RECORD, VOL 28, NO 2 - JUNE 1999, P169, DOI 10.1145/304181.304197; Jahrer M, 2012, KDDCUP WORKSH; Jin RM, 2003, SIAM PROC S, P119; Joshi MV, 1998, FIRST MERGED INTERNATIONAL PARALLEL PROCESSING SYMPOSIUM & SYMPOSIUM ON PARALLEL AND DISTRIBUTED PROCESSING, P573, DOI 10.1109/IPPS.1998.669983; Kufrin R., 1997, MACH INTELL PATT REC, P279; Mehta M., 1996, Advances in Database Technology - EDBT '96. 5th International Conference on Extending Database Technology. Proceedings, P18, DOI 10.1007/BFb0014141; Panda B, 2009, PROC VLDB ENDOW, V2, P1426, DOI 10.14778/1687553.1687569; Pearson Robert Allan, 1993, COARSE GRAINED PARAL; Quinlan J. R., 1986, Machine Learning, V1, P81, DOI 10.1023/A:1022643204877; SAFAVIAN SR, 1991, IEEE T SYST MAN CYB, V21, P660, DOI 10.1109/21.97458; Shafer J, 1996, PROCEEDINGS OF THE INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P544; Svore Krysta M, 2011, SCALING MACHINE LEAR, V2; Tyree S., 2011, P 20 INT C WORLD WID, P387, DOI DOI 10.1145/1963405.1963461; Yu C. Y., 2001, TECH REP; Zhou ZH., 2012, ENSEMBLE METHODS FDN, DOI [10.1201/b12207, DOI 10.1201/B12207]	23	42	43	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703017
C	Sajjadi, M; Javanmardi, M; Tasdizen, T		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Sajjadi, Mehdi; Javanmardi, Mehran; Tasdizen, Tolga			Regularization With Stochastic Transformations and Perturbations for Deep Semi-Supervised Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Effective convolutional neural networks are trained on large sets of labeled data. However, creating large labeled datasets is a very costly and time-consuming task. Semi-supervised learning uses unlabeled data to train a model with higher accuracy when there is a limited set of labeled data available. In this paper, we consider the problem of semi-supervised learning with convolutional neural networks. Techniques such as randomized data augmentation, dropout and random max-pooling provide better generalization and stability for classifiers that are trained using gradient descent. Multiple passes of an individual sample through the network might lead to different predictions due to the non-deterministic behavior of these techniques. We propose an unsupervised loss function that takes advantage of the stochastic nature of these methods and minimizes the difference between the predictions of multiple passes of a training sample through the network. We evaluate the proposed method on several benchmark datasets.	[Sajjadi, Mehdi; Javanmardi, Mehran; Tasdizen, Tolga] Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA	Utah System of Higher Education; University of Utah	Sajjadi, M (corresponding author), Univ Utah, Dept Elect & Comp Engn, Salt Lake City, UT 84112 USA.	mehdi@sci.utah.edu; mehran@sci.utah.edu; tolga@sci.utah.edu			NSF [IIS-1149299]	NSF(National Science Foundation (NSF))	This work was supported by NSF IIS-1149299.	Agrawal P, 2015, IEEE I CONF COMP VIS, P37, DOI 10.1109/ICCV.2015.13; Bennett KP, 1999, ADV NEUR IN, V11, P368; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Chang J.R., 2015, COMPUTER SCI; Chawla Shuchi, 2001, LEARNING LABELED UNL; Ciresan D, 2012, PROC CVPR IEEE, P3642, DOI 10.1109/CVPR.2012.6248110; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, P766; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; Graham B., 2014, ARXIV; Graham Benjamin, 2014, ARXIV14096070; Hinton G.E., 2012, COMPUT SCI, V3, P212, DOI DOI 10.9774/GLEAF.978-1-909493-38-42; Jarrett K, 2009, IEEE I CONF COMP VIS, P2146, DOI 10.1109/ICCV.2009.5459469; Jayaraman D., 2016, COMPUTER VISION PATT; Jayaraman D, 2015, IEEE I CONF COMP VIS, P1413, DOI 10.1109/ICCV.2015.166; Joachims T, 1999, MACHINE LEARNING, PROCEEDINGS, P200; Johnson Rie, 2015, Adv Neural Inf Process Syst, V28, P919; Kavukcuoglu K., 2010, ARXIV10103467; Krizhevskey A., 2014, CUDA CONVNET; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Le Cun B. B., 1990, ADV NEURAL INFORM PR; LeCun Y, 2004, PROC CVPR IEEE, P97; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2010, IEEE INT SYMP CIRC S, P253, DOI 10.1109/ISCAS.2010.5537907; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Miller DJ, 1997, ADV NEUR IN, V9, P571; Netzer Y., 2011, NIPS DLW; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sajjadi M., 2016, INT C IM PROC; Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239; Sun L, 2014, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2014.336; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Weston Jason, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P639, DOI 10.1007/978-3-642-35289-8_34; Zhang X, 2013, ARXIV PREPRINT ARXIV; Zhu X., 2002, TECH REP; Zhu Xiaojin., 2003, P ICLR, P912	40	42	42	2	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702080
C	Wu, YH; Zhang, SZ; Zhang, Y; Bengio, Y; Salakhutdinov, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wu, Yuhuai; Zhang, Saizheng; Zhang, Ying; Bengio, Yoshua; Salakhutdinov, Ruslan			On Multiplicative Integration with Recurrent Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We introduce a general and simple structural design called "Multiplicative Integration" (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models.	[Wu, Yuhuai] Univ Toronto, Toronto, ON, Canada; [Zhang, Saizheng; Zhang, Ying; Bengio, Yoshua] Univ Montreal, MILA, Montreal, PQ, Canada; [Salakhutdinov, Ruslan] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Bengio, Yoshua; Salakhutdinov, Ruslan] CIFAR, Toronto, ON, Canada	University of Toronto; Universite de Montreal; Carnegie Mellon University; Canadian Institute for Advanced Research (CIFAR)	Wu, YH (corresponding author), Univ Toronto, Toronto, ON, Canada.	ywu@cs.toronto.edu; saizheng.zhang@umontreal.ca; ying.zhang@umontreal.ca; yoshua.bengio@umontreal.ca; rsalakhu@cs.cmu.edu			NSERC; Canada Research Chairs; CIFAR; Calcul Quebec; Compute Canada; Disney research; ONR [N000141310721]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Canada Research Chairs(Canada Research ChairsCGIAR); CIFAR(Canadian Institute for Advanced Research (CIFAR)); Calcul Quebec; Compute Canada; Disney research; ONR(Office of Naval Research)	The authors acknowledge the following agencies for funding and support: NSERC, Canada Research Chairs, CIFAR, Calcul Quebec, Compute Canada, Disney research and ONR Grant N000141310721. The authors thank the developers of Theano [29] and Keras [30], and also thank Jimmy Ba for many thought-provoking discussions.	Al-Rfou R, 2016, THEANO PYTHON FRAMEW; Bahdanau Dzmitry, 2015, ARXIV150804395, P4945; BAUM LE, 1967, B AM MATH SOC, V73, P360, DOI 10.1090/S0002-9904-1967-11751-8; Bengio Y., 2014, ARXIV14061078; Chollet F., 2015, KERAS; Chung JY, 2015, PR MACH LEARN RES, V37, P2067; Cooijmans T., 2016, RECURRENT BATCH NORM; GOUDREAU MW, 1994, IEEE T NEURAL NETWOR, V5, P511, DOI 10.1109/72.286928; Graves A., 2006, P 23 INT C MACH LEAR, P369; Graves A, 2013, ARXIV13080850; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Hannun A.Y., 2014, ARXIV14082873; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hermann K. M., 2015, ADV NEURAL INFORM PR, V28, P1693; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Kalchbrenner N., 2015, COMPUTER SCI; Kingma D.P, P 3 INT C LEARNING R; Kiros R., 2015, ADV NEURAL INFORM PR, V28, P3294; Krueger David, 2015, ARXIV PREPRINT ARXIV; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Miao YJ, 2015, 2015 IEEE WORKSHOP ON AUTOMATIC SPEECH RECOGNITION AND UNDERSTANDING (ASRU), P167, DOI 10.1109/ASRU.2015.7404790; Mohri M, 2002, COMPUT SPEECH LANG, V16, P69, DOI 10.1006/csla.2001.0184; Pachitariu M., 2013, ARXIV13015650; Pezeshki Mohammad, 2015, ARXIV151106430; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; Wessels T, 2000, IEEE IJCNN, P271, DOI 10.1109/IJCNN.2000.857908; Yang Bishan, 2014, ARXIV14126575; Zhang Saizheng, 2016, ARXIV160208210	30	42	42	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701072
C	Punyakanok, V; Roth, D		Leen, TK; Dietterich, TG; Tresp, V		Punyakanok, V; Roth, D			The use of classifiers in sequential inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We study the problem of combining the outcomes of several different classifiers in a way that provides a coherent inference that satisfies some constraints. In particular, we develop two general approaches for an important subproblem - identifying phrase structure. The first is a Markovian approach that extends standard HMMs to allow the use of a rich observation structure and of general classifiers to model state-observation dependencies. The second is an extension of constraint satisfaction formalisms. We develop efficient combination algorithms under both models and study them experimentally in the context of shallow parsing.	Univ Illinois, Dept Comp Sci, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign	Punyakanok, V (corresponding author), Univ Illinois, Dept Comp Sci, 1304 W Springfield Ave, Urbana, IL 61801 USA.	punyakan@cs.uiuc.edu; danr@cs.uiuc.edu						Abney S., 1991, PRINCIPLE BASED PARS, P257; Appelt D., 1993, P IJCAI; ARGAMON S, 1999, J EXPT THEORETHICAL, V10, P1; Burge CB, 1998, CURR OPIN STRUC BIOL, V8, P346, DOI 10.1016/S0959-440X(98)80069-9; Cardie C., 1998, P 36 ANN M ASS COMP, P218; Carlson A, 1999, UIUCDCSR992101; CHURCH KW, 1988, P ACL C APPL NAT LAN; Fickett JW, 1996, COMPUT CHEM, V20, P103, DOI 10.1016/S0097-8485(96)80012-X; Freitag Dayne, 1999, P AAAI 99 WORKSH MAC, P31; Golding AR, 1999, MACH LEARN, V34, P107, DOI 10.1023/A:1007545901558; GREFFENSTETTE G, 1993, ACL 93 WORKSH ACQ LE; Grishman Ralph, 1995, 6 MESS UND C MUC 6 P; GUSFIELD D, 1992, ALGORITHMICA, V8, P103, DOI 10.1007/BF01758838; HARRIS ZS, 1957, LANGUAGE, V33, P283, DOI 10.2307/411155; Haussler D, 1998, TRENDS BIOTECHNOL, P12, DOI 10.1016/S0167-7799(98)00129-2; Khardon R, 1997, J ACM, V44, P697, DOI 10.1145/265910.265918; Mackworth A.K., 1992, ENCY ARTIFICIAL INTE, P285; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; MCCALLUM A, 2000, IN PRESS P ICML 2000; MORGANTI R, 1995, PUBL ASTRON SOC AUST, V12, P3, DOI 10.1017/S1323358000019962; MUNOZ M, 1999, EMNLP VLC 99; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; RAMSHAW LA, 1995, P 3 ANN WORKSH VER L; Roth D, 2000, PROC CVPR IEEE, P724, DOI 10.1109/CVPR.2000.855892; Roth D, 1998, FIFTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE (AAAI-98) AND TENTH CONFERENCE ON INNOVATIVE APPLICATIONS OF ARTIFICAL INTELLIGENCE (IAAI-98) - PROCEEDINGS, P806; Valiant L. G., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P287, DOI 10.1145/279943.279999	26	42	43	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						995	1001						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800140
C	Jung, TP; Makeig, S; Westerfield, M; Townsend, J; Courchesne, E; Sejnowski, TJ		Kearns, MS; Solla, SA; Cohn, DA		Jung, TP; Makeig, S; Westerfield, M; Townsend, J; Courchesne, E; Sejnowski, TJ			Analyzing and visualizing single-trial event-related potentials	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BLIND SEPARATION	Event-related potentials (ERPs), are portions of electroencephalographic (EEG) recordings that are both time- and phase-locked to experimental events. ERPs are usually averaged to increase their signal/noise ratio relative to non-phase locked EEG activity, regardless of the fact that response activity in single epochs may vary widely in time course and scalp distribution. This study applies a linear decomposition tool, Independent Component Analysis (ICA) [1], to multichannel single-trial EEG records to derive spatial filters that decompose single-trial EEG epochs into a sum of temporally independent and spatially fixed components arising from distinct or overlapping brain or extra-brain networks. Our results on normal and autistic subjects show that ICA can separate artifactual, stimulus-locked, response-locked, and non-event related background EEG activities into separate components, allowing (1) removal of pervasive artifacts of all types from single-trial EEG records, and (2) identification of both stimulus- and response-locked EEG components. Second, this study proposes a new visualization tool, the 'ERP image', for investigating variability in latencies and amplitudes of event-evoked responses in spontaneous EEG or MEG records. We show that sorting single-trial ERP epochs in order of reaction time and plotting the potentials in 2-D clearly reveals underlying patterns of response variability linked to performance. These analysis and visualization tools appear broadly applicable to electrophyiological research on both normal and clinical populations.	Salk Inst, Howard Hughes Med Inst, San Diego, CA 92186 USA	Howard Hughes Medical Institute; Salk Institute	Jung, TP (corresponding author), Salk Inst, Howard Hughes Med Inst, POB 85800, San Diego, CA 92186 USA.	jung@salk.edu; scott@salk.edu; terry@salk.edu	Sejnowski, Terrence/AAV-5558-2021					BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Jung TP, 1998, ADV NEUR IN, V10, P894; Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719; Makeig S, 1997, P NATL ACAD SCI USA, V94, P10979, DOI 10.1073/pnas.94.20.10979; MAKEIG S, IN PRESS J NEUROSCIE; YABE H, 1993, ELECTROEN CLIN NEURO, V87, P403, DOI 10.1016/0013-4694(93)90154-N; YABE H, 1995, ELECTROEN CLIN NEURO, V94, P288; [No title captured]	8	42	42	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						118	124						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700017
C	Deprelle, T; Groueix, T; Fisher, M; Kim, VG; Russell, BC; Aubry, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Deprelle, Theo; Groueix, Thibault; Fisher, Matthew; Kim, Vladimir G.; Russell, Bryan C.; Aubry, Mathieu			Learning elementary structures for 3D shape generation and matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We propose to represent shapes as the deformation and combination of learnable elementary 3D structures, which are primitives resulting from training over a collection of shapes. We demonstrate that the learned elementary 3D structures lead to clear improvements in 3D shape generation and matching. More precisely, we present two complementary approaches for learning elementary structures: (i) patch deformation learning and (ii) point translation learning. Both approaches can be extended to abstract structures of higher dimensions for improved results. We evaluate our method on two tasks: reconstructing ShapeNet objects and estimating dense correspondences between human scans (FAUST inter challenge). We show 16% improvement over surface deformation approaches for shape reconstruction and outperform FAUST inter and intra challenge state of the art by 2% and 7%, respectively.	[Deprelle, Theo; Groueix, Thibault; Kim, Vladimir G.; Aubry, Mathieu] UPE, Ecole Ponts, LIGM, UMR 8049, Champs Sur Marne, France; [Fisher, Matthew; Russell, Bryan C.] Adobe Res, San Jose, CA USA	Universite Gustave-Eiffel; ESIEE Paris; Centre National de la Recherche Scientifique (CNRS); Ecole des Ponts ParisTech; Adobe Systems Inc.	Deprelle, T (corresponding author), UPE, Ecole Ponts, LIGM, UMR 8049, Champs Sur Marne, France.				ANR project EnHerit [ANR-17-CE23-0008]; Labex Bezout	ANR project EnHerit(French National Research Agency (ANR)); Labex Bezout	This work was partly supported by ANR project EnHerit ANR-17-CE23-0008, Labex Bezout, and gifts from Adobe to Ecole des Ponts.	Allen B., 2002, SIGGRAPH; Allen B., 2003, SIGGRAPH; BIEDERMAN I, 1987, PSYCHOL REV, V94, P115, DOI 10.1037/0033-295X.94.2.115; Binford I, 1971, IEEE C SYST CONTR; Bogo F., 2014, CVPR; Chang A. X., 2015, ABS151203012 CORR; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Genova K., 2019, ABS190406447 CORR; Golovinskiy A., 2009, COMPUTERS GRAPHICS S; Groueix T., 2018, P IEEE C COMP VIS PA; Groueix Thibault, 2018, ECCV; Huang Q., 2011, ACM T GRAPHICS P SIG; Kaiser Adrien, 2018, COMPUTER GRAPHICS FO; Kanazawa Angjoo, 2018, ECCV; Kim VG, 2013, ACM T GRAPHIC, V32, DOI 10.1145/2461912.2461933; Li J, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3072959.3073637; Li L., 2018, ARXIV181108988; Li YY, 2011, ACM T GRAPHIC, V30, DOI 10.1145/1964921.1964947; Loper M., 2015, SIGGRAPH ASIA; Paschalidou D., 2019, P IEEE C COMP VIS PA; Roberts Lawrence G, 1963, THESIS, P2; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; Schnabel R, 2007, COMPUT GRAPH FORUM, V26, P214, DOI 10.1111/j.1467-8659.2007.01016.x; Schnabel R, 2009, COMPUT GRAPH FORUM, V28, P503, DOI 10.1111/j.1467-8659.2009.01389.x; Sharma G, 2018, PROC CVPR IEEE, P5515, DOI 10.1109/CVPR.2018.00578; Sidi O., 2011, T GRAPHICS P SIGGRAP; Tulsiani S., 2017, COMPUTER VISION PATT; Varol G., 2017, CVPR; Yang Y., 2018, 2018 IEEE CVF C COMP; Zou CH, 2017, IEEE I CONF COMP VIS, P900, DOI 10.1109/ICCV.2017.103; Zuffi S., 2015, P IEEE C COMP VIS PA	32	41	41	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307045
C	Franceschi, JY; Dieuleveut, A; Jaggi, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Franceschi, Jean-Yves; Dieuleveut, Aymeric; Jaggi, Martin			Unsupervised Scalable Representation Learning for Multivariate Time Series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Time series constitute a challenging data type for machine learning algorithms, due to their highly variable lengths and sparse labeling in practice. In this paper, we tackle this challenge by proposing an unsupervised method to learn universal embeddings of time series. Unlike previous works, it is scalable with respect to their length and we demonstrate the quality, transferability and practicability of the learned representations with thorough experiments and comparisons. To this end, we combine an encoder based on causal dilated convolutions with a novel triplet loss employing time-based negative sampling, obtaining general-purpose representations for variable length and multivariate time series.	[Franceschi, Jean-Yves] Sorbonne Univ, CNRS, LIP6, F-75005 Paris, France; [Franceschi, Jean-Yves; Dieuleveut, Aymeric; Jaggi, Martin] Ecole Polytech Fed Lausanne, MLO, CH-1015 Lausanne, Switzerland; [Dieuleveut, Aymeric] Ecole Polytech, CMAP, Palaiseau, France; [Franceschi, Jean-Yves] ENS Lyon, Lyon, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Institut Polytechnique de Paris; Ecole Normale Superieure de Lyon (ENS de LYON)	Franceschi, JY (corresponding author), Sorbonne Univ, CNRS, LIP6, F-75005 Paris, France.	jean-yves.franceschi@lip6.fr; aymeric.dieuleveut@polytechnique.edu; martin.jaggi@epfl.cn		Jaggi, Martin/0000-0003-1579-5558	SFA-AM ETH Board initiative; LOCUST ANR project [ANR-15-CE23-0027]; CLEAR (Center for LEArning and data Retrieval, joint laboratory with Thalesi10)	SFA-AM ETH Board initiative; LOCUST ANR project(French National Research Agency (ANR)); CLEAR (Center for LEArning and data Retrieval, joint laboratory with Thalesi10)(Thales Group)	We would like to acknowledge Patrick Gallinari, Sylvain Lamprier, Mehdi Lamrayah, Etienne Simon, Valentin Guiguet, Clara Gainon de Forsan de Gabriac, Eloi Zablocki, Antoine Saporta, Edouard Delasalles, Sidak Pal Singh, Andreas Hug, Jean-Baptiste Cordonnier, Andreas Loukas and Francois Fleuret for helpful comments and discussions. We thank as well our anonymous reviewers for their constructive suggestions, Liljefors et al. (2019) for their extensive and positive reproducibility report on our work, and all contributors to the datasets and archives we used for this project (Dau et al., 2018; Bagnall et al., 2018; Dheeru & Karra Taniskidou, 2017). We acknowledge financial support from the SFA-AM ETH Board initiative, the LOCUST ANR project (ANR-15-CE23-0027) and CLEAR (Center for LEArning and data Retrieval, joint laboratory with Thalesi<SUP>10</SUP>).	Arora S, 2019, PR MACH LEARN RES, V97; Bagnall A.J, 2018, UEA MULTIVARIATE TIM; Bagnall A, 2017, DATA MIN KNOWL DISC, V31, P606, DOI 10.1007/s10618-016-0483-9; Bai S., 2018, ARXIV PREPRINT ARXIV; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bostrom A, 2015, LECT NOTES COMPUT SC, V9263, P257, DOI 10.1007/978-3-319-22729-0_20; Bredin H, 2017, INT CONF ACOUST SPEE, P5430, DOI 10.1109/ICASSP.2017.7953194; Cui Z, 2016, ARXIV160306995; Dan H. A, 2018, UCR TIME SERIES CLAS; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Fawaz HI, 2019, DATA MIN KNOWL DISC, V33, P917, DOI 10.1007/s10618-019-00619-1; Fortuin V., 2019, INT C LEARN REPR; Goldberg Y., 2014, ARXIV PREPRINT ARXIV; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hua Y, 2003, PARALLEL AND DISTRIBUTED COMPUTING, APPLICATIONS AND TECHNOLOGIES, PDCAT'2003, PROCEEDINGS, P268, DOI 10.1109/PDCAT.2003.1236303; Kingma D.P, P 3 INT C LEARNING R; Lei Qi, 2017, ARXIV170203584; Liljefors F, 2019, UNSUPERVISED SCALABL; Lines J, 2018, ACM T KNOWL DISCOV D, V12, DOI 10.1145/3182382; Lines J, 2015, DATA MIN KNOWL DISC, V29, P565, DOI 10.1007/s10618-014-0361-2; Lingfei Wu, 2018, P INT C ART INT STAT, P793; Logeswaran Lajanugen, 2018, ARXIV180302893; Lu R, 2017, INT CONF ACOUST SPEE, P121, DOI 10.1109/ICASSP.2017.7952130; Malhotra P., 2017, ARXIV170608838; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Oord A. v. d., 2018, ARXIV180703748; Oord A. v. d., 2016, ARXIV160903499; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Schafer P, 2015, DATA MIN KNOWL DISC, V29, P1505, DOI 10.1007/s10618-014-0377-7; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Srivastava N, 2015, PR MACH LEARN RES, V37, P843; Turpault N, 2019, INT CONF ACOUST SPEE, P760, DOI 10.1109/ICASSP.2019.8683774; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Villegas Ruben, 2017, ICLR, DOI DOI 10.48550/ARXIV.1706.08033; WANG ZG, 2017, IEEE IJCNN, P1578, DOI DOI 10.1109/IJCNN.2017.7966039; Wu L, 2018, AAAI CONF ARTIF INTE, P5569; Young T, 2018, IEEE COMPUT INTELL M, V13, P55, DOI 10.1109/MCI.2018.2840738; Yu F., 2016, P ICLR 2016	43	41	41	4	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304063
C	Hsieh, TI; Lo, YC; Chen, HT; Liu, TL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hsieh, Ting-I; Lo, Yi-Chen; Chen, Hwann-Tzong; Liu, Tyng-Luh			One-Shot Object Detection with Co-Attention and Co-Excitation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper aims to tackle the challenging problem of one-shot object detection. Given a query image patch whose class label is not included in the training data, the goal of the task is to detect all instances of the same class in a target image. To this end, we develop a novel co-attention and co-excitation (CoAE) framework that makes contributions in three key technical aspects. First, we propose to use the non-local operation to explore the co-attention embodied in each query-target pair and yield region proposals accounting for the one-shot situation. Second, we formulate a squeeze-and-co-excitation scheme that can adaptively emphasize correlated feature channels to help uncover relevant proposals and eventually the target objects. Third, we design a margin-based ranking loss for implicitly learning a metric to predict the similarity of a region proposal to the underlying query, no matter its class label is seen or unseen in training. The resulting model is therefore a two-stage detector that yields a strong baseline on both VOC and MS-COCO under one-shot setting of detecting objects from both seen and never-seen classes. Codes are available at https://github.com/timy90022/One-Shot-Object-Detection.	[Hsieh, Ting-I; Lo, Yi-Chen; Chen, Hwann-Tzong] Natl Tsing Hua Univ, Hsinchu, Taiwan; [Liu, Tyng-Luh] Acad Sinica, Taipei, Taiwan; [Chen, Hwann-Tzong] Aeolus Robot, San Francisco, CA USA; [Liu, Tyng-Luh] Taiwan AI Labs, Taipei, Taiwan	National Tsing Hua University; Academia Sinica - Taiwan	Hsieh, TI (corresponding author), Natl Tsing Hua Univ, Hsinchu, Taiwan.	tingihsieh.tw@gmail.com; howardyclo@gmail.com; htchen@cs.nthu.edu.tw; liutyng@ailabs.tw			Ministry of Science and Technology (MOST), Taiwan [106-2221-E-007-080-MY3, 107-2218-E-007-047, 107-2634-F-001-002, 108-2634-F-001007]	Ministry of Science and Technology (MOST), Taiwan(Ministry of Science and Technology, Taiwan)	This work was supported in part by the Ministry of Science and Technology (MOST), Taiwan under Grants 106-2221-E-007-080-MY3, 107-2218-E-007-047, 107-2634-F-001-002, and 108-2634-F-001007. We are particularly grateful to the National Center for High-performance Computing (NCHC) for providing computational resources and facilities. The authors also like to thank Songhao Jia for insightful discussions on the implementation.	Cen MB, 2018, IEEE IMAGE PROC, P3718, DOI 10.1109/ICIP.2018.8451102; Chen H, 2018, AAAI CONF ARTIF INTE, P2836; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hu J., 2018, SQUEEZE EXCITATION N; Kang Bingyi, 2018, CORR; Koch G., 2015, ICML DEEP LEARNING W; Kong T., 2019, ARXIV190403797; Law H, 2018, LECT NOTES COMPUT SC, V11218, P765, DOI 10.1007/978-3-030-01264-9_45; Li B, 2018, PROC CVPR IEEE, P8971, DOI 10.1109/CVPR.2018.00935; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2017, IEEE I CONF COMP VIS, P2999, DOI 10.1109/ICCV.2017.324; Michaelis Claudio, 2018, CORR; Palmer S.E., 1999, VISION SCI PHOTONS P; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Schwartz Eli, 2018, CORR; Sermanet P., 2013, ARXIV PREPRINT ARXIV; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Sung F, 2018, PROC CVPR IEEE, P1199, DOI 10.1109/CVPR.2018.00131; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Wang SL, 2018, PROC CVPR IEEE, P2589, DOI 10.1109/CVPR.2018.00274; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Zhang Tianyi, 2019, ARXIV E PRINTS; Zhaowei Cai, 2018, 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition. Proceedings, P6154, DOI 10.1109/CVPR.2018.00644	29	41	41	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302069
C	Hsu, CC; Hsu, KJ; Tsai, CC; Lin, YY; Chuang, YY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hsu, Cheng-Chun; Hsu, Kuang-Jui; Tsai, Chung-Chi; Lin, Yen-Yu; Chuang, Yung-Yu			Weakly Supervised Instance Segmentation using the Bounding Box Tightness Prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper presents a weakly supervised instance segmentation method that consumes training data with tight bounding box annotations. The major difficulty lies in the uncertain figure-ground separation within each bounding box since there is no supervisory signal about it. We address the difficulty by formulating the problem as a multiple instance learning (MIL) task, and generate positive and negative bags based on the sweeping lines of each bounding box. The proposed deep model integrates MIL into a fully supervised instance segmentation network, and can be derived by the objective consisting of two terms, i.e., the unary term and the pairwise term. The former estimates the foreground and background areas of each bounding box while the latter maintains the unity of the estimated object masks. The experimental results show that our method performs favorably against existing weakly supervised methods and even surpasses some fully supervised methods for instance segmentation on the PASCAL VOC dataset. The code is available at https://github.com/chengchunhsu/WSIS_BBTP.	[Hsu, Cheng-Chun; Lin, Yen-Yu; Chuang, Yung-Yu] Acad Sinica, Taipei, Taiwan; [Hsu, Kuang-Jui; Tsai, Chung-Chi] Qualcomm Technol Inc, Taipei, Taiwan; [Lin, Yen-Yu] Natl Chiao Tung Univ, Hsinchu, Taiwan; [Chuang, Yung-Yu] Natl Taiwan Univ, Taipei, Taiwan	Academia Sinica - Taiwan; National Yang Ming Chiao Tung University; National Taiwan University	Hsu, CC (corresponding author), Acad Sinica, Taipei, Taiwan.	hsu06118@citi.sinica.edu.tw; kuangjui@qti.qualcomm.com; chuntsai@qti.qualcomm.com; lin@cs.nctu.edu.tw; cyy@csie.ntu.edu.tw		Lin, Yen-Yu/0000-0002-7183-6070	Qualcomm through a Taiwan University Research Collaboration Project; Ministry of Science and Technology (MOST) [107-2628-E-001-005-MY3, 108-2634-F-007-009]; MOST Joint Research Center for AI Technology; All Vista Healthcare [108-2634-F-002-004]	Qualcomm through a Taiwan University Research Collaboration Project; Ministry of Science and Technology (MOST)(Ministry of Science and Technology, China); MOST Joint Research Center for AI Technology; All Vista Healthcare	This work was funded in part by Qualcomm through a Taiwan University Research Collaboration Project and also supported in part by the Ministry of Science and Technology (MOST) under grants 107-2628-E-001-005-MY3 and 108-2634-F-007-009, and MOST Joint Research Center for AI Technology and All Vista Healthcare under grant 108-2634-F-002-004.	Ahn J, 2018, PROC CVPR IEEE, P4981, DOI 10.1109/CVPR.2018.00523; Bai M, 2017, PROC CVPR IEEE, P2858, DOI 10.1109/CVPR.2017.305; Bearman A, 2016, LECT NOTES COMPUT SC, V9911, P549, DOI 10.1007/978-3-319-46478-7_34; Bellver Bueno M., 2019, CVPR WORKSH, P93; Bin Jin, 2017, PROC CVPR IEEE, P1705, DOI 10.1109/CVPR.2017.185; Briq R., 2018, BMVC; Chang F.-J., 2014, CVPR; Chen LC, 2018, PROC CVPR IEEE, P4013, DOI 10.1109/CVPR.2018.00422; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen YC, 2019, PROC CVPR IEEE, P1791, DOI 10.1109/CVPR.2019.00189; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Dai JF, 2015, PROC CVPR IEEE, P3992, DOI 10.1109/CVPR.2015.7299025; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; De Brabandere B, 2017, IEEE COMPUT SOC CONF, P478, DOI 10.1109/CVPRW.2017.66; Everingham Mark, 2010, IJCV; Fan RC, 2018, LECT NOTES COMPUT SC, V11213, P371, DOI 10.1007/978-3-030-01240-3_23; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Guler RA, 2018, PROC CVPR IEEE, P7297, DOI 10.1109/CVPR.2018.00762; Hariharan B., 2015, CVPR 2015; Hariharan B, 2014, LECT NOTES COMPUT SC, V8695, P297, DOI 10.1007/978-3-319-10584-0_20; Hariharan B, 2011, IEEE I CONF COMP VIS, P991, DOI 10.1109/ICCV.2011.6126343; Hayder Z, 2017, PROC CVPR IEEE, P587, DOI 10.1109/CVPR.2017.70; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hou QB, 2018, ADV NEUR IN, V31; Hsu K.J., 2017, BMVC; Hsu KJ, 2019, PROC CVPR IEEE, P8838, DOI 10.1109/CVPR.2019.00905; Hsu KJ, 2014, IEEE T IMAGE PROCESS, V23, P1722, DOI 10.1109/TIP.2014.2307436; Hsu Y.-C., 2018, IJCNN; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Huang Zilong, 2018, CVPR; Hung WC, 2017, IEEE I CONF COMP VIS, P2650, DOI 10.1109/ICCV.2017.287; Khoreva A, 2017, PROC CVPR IEEE, P1665, DOI 10.1109/CVPR.2017.181; Kingma D.P, P 3 INT C LEARNING R; Kirillov A, 2017, PROC CVPR IEEE, P7322, DOI 10.1109/CVPR.2017.774; Koltun V, 2011, ADV NEURAL INFORM PR, P109, DOI DOI 10.5555/2986459.2986472; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lempitsky Victor, 2009, ICCV; Li Yi, 2017, P IEEE C COMP VIS PA; Liang XD, 2018, IEEE T PATTERN ANAL, V40, P2978, DOI 10.1109/TPAMI.2017.2775623; Lin Di, 2016, CVPR; Lin G., 2019, TPAMI; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2018, PROC CVPR IEEE, P8759, DOI 10.1109/CVPR.2018.00913; Liu Y, 2018, LECT NOTES COMPUT SC, V11209, P449, DOI 10.1007/978-3-030-01228-1_27; Mehta S, 2018, LECT NOTES COMPUT SC, V11214, P561, DOI 10.1007/978-3-030-01249-6_34; Papandreou G, 2018, LECT NOTES COMPUT SC, V11218, P282, DOI 10.1007/978-3-030-01264-9_17; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Pont-Tuset J., 2017, TPAMI; Qian R., 2019, AAAI; Qin XL, 2017, 2017 IEEE INTERNATIONAL CONFERENCE ON PROGNOSTICS AND HEALTH MANAGEMENT (ICPHM), P1, DOI 10.1109/ICPHM.2017.7998297; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rother Carsten, 2004, TOG; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen T., 2018, CVPR 2018; Tang M, 2018, LECT NOTES COMPUT SC, V11220, P524, DOI 10.1007/978-3-030-01270-0_31; Tang M, 2018, PROC CVPR IEEE, P1818, DOI 10.1109/CVPR.2018.00195; Uijlings J. R. R., 2013, SELECTIVE SEARCH OBJ; Vernaza P, 2017, PROC CVPR IEEE, P2953, DOI 10.1109/CVPR.2017.315; Wang LM, 2017, PROC CVPR IEEE, P6402, DOI 10.1109/CVPR.2017.678; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wei YC, 2018, PROC CVPR IEEE, P7268, DOI 10.1109/CVPR.2018.00759; Yang MK, 2018, PROC CVPR IEEE, P3684, DOI 10.1109/CVPR.2018.00388; Zhang H, 2018, PROC CVPR IEEE, P7151, DOI 10.1109/CVPR.2018.00747; Zhang ZY, 2018, LECT NOTES COMPUT SC, V11214, P238, DOI 10.1007/978-3-030-01249-6_15; Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472	73	41	42	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306057
C	Tramer, F; Boneh, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Tramer, Florian; Boneh, Dan			Adversarial Training and Robustness for Multiple Perturbations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Defenses against adversarial examples, such as adversarial training, are typically tailored to a single perturbation type (e.g., small l(infinity)-noise). For other perturbations, these defenses offer no guarantees and, at times, even increase the model's vulnerability. Our aim is to understand the reasons underlying this robustness trade-off, and to train models that are simultaneously robust to multiple perturbation types. We prove that a trade-off in robustness to different types of l(p)-bounded and spatial perturbations must exist in a natural and simple statistical setting. We corroborate our formal analysis by demonstrating similar robustness trade-offs on MNIST and CIFAR10. We propose new multi-perturbation adversarial training schemes, as well as an efficient attack for the l(1)-norm, and use these to show that models trained against multiple attacks fail to achieve robustness competitive with that of models trained on each attack individually. In particular, we find that adversarial training with first-order l(infinity), l(1) and l(2) attacks on MNIST achieves merely 50% robust accuracy, partly because of gradient-masking. Finally, we propose affine attacks that linearly interpolate between perturbation types and further degrade the accuracy of adversarially trained models.	[Tramer, Florian; Boneh, Dan] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Tramer, F (corresponding author), Stanford Univ, Stanford, CA 94305 USA.			Tramer, Florian/0000-0001-8703-8762				Athalye A, 2018, PR MACH LEARN RES, V80; Brendel W., 2018, PROC 6 INT C LEARN R; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Carlini N, 2016, PROCEEDINGS OF THE 25TH USENIX SECURITY SYMPOSIUM, P513; Carlini Nicholas, 2018, PROTOTYPICAL EXAMPLE; Chen J., 2019, ARXIV190402144; Chen PY, 2018, AAAI CONF ARTIF INTE, P10; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Demontis A, 2016, LECT NOTES COMPUT SC, V10029, P322, DOI 10.1007/978-3-319-49055-7_29; Duchi J., 2008, PROC 25 INT C MACH L, P272; Engstrom L., 2017, ARXIV171202779; EricWong Zico, 2018, INT C MACH LEARN, P5286; Fawzi Alhussein, 2018, ARXIV180208686; Geirhos R., 2019, P INT C LEARNING REP, P1; Gilmer Justin, 2018, ARXIV180102774; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Grosse K., 2017, EUR S RES COMP SEC, VProceedings of the ESORICS (2), P62, DOI DOI 10.1007/978-3-319-66399-9_4; Hendrycks D., 2019, ICLR, P1; Ilyas A., 2019, P ADV NEUR INF PROC; Jo J., 2017, ARXIV; Kang Daniel, 2019, ARXIV190501034; Khoury M., 2019, GEOMETRY ADVERSARIAL; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Li B., 2018, ARXIV180903113, P1; Madry A., 2018, TUTORIAL NEURIPS 201; Madry Aleksander, 2017, ARXIV; Mahloujifar S., 2018, ARXIV180903063; Raghunathan Aditi, 2018, ARXIV180109344; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; Schmidt L, 2018, ADV NEUR IN, V31; Schonherr L, 2019, 26TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2019), DOI 10.14722/ndss.2019.23288; Schott L., 2019, 1 ADVERSARIALLY ROBU; Shafahi A., 2019, P 7 INT C LEARN REPR; Shafahi A, 2019, ADV NEUR IN, V32; Sharma Y., 2017, ARXIV171010733; Stock P, 2018, LECT NOTES COMPUT SC, V11210, P504, DOI 10.1007/978-3-030-01231-1_31; TRAMER F, 2018, ARXIV181103194; Tramer F., 2019, NEUR INF PROC SYST N; Tramer Florian, 2018, INT C LEARN REPR ICL; Tsipras Dimitris, 2019, ROBUSTNESS MAY BE OD, V1, P2; Xu H, 2009, J MACH LEARN RES, V10, P1485; Zhang DH, 2019, ADV NEUR IN, V32	44	41	41	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305081
C	Buckman, J; Hafner, D; Tucker, G; Brevdo, E; Lee, H		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Buckman, Jacob; Hafner, Danijar; Tucker, George; Brevdo, Eugene; Lee, Honglak			Sample-Efficient Reinforcement Learning with Stochastic Ensemble Value Expansion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Integrating model-free and model-based approaches in reinforcement learning has the potential to achieve the high performance of model-free algorithms with low sample complexity. However, this is difficult because an imperfect dynamics model can degrade the performance of the learning algorithm, and in sufficiently complex environments, the dynamics model will almost always be imperfect. As a result, a key challenge is to combine model-based approaches with model-free learning in such a way that errors in the model do not degrade performance. We propose stochastic ensemble value expansion (STEVE), a novel model-based technique that addresses this issue. By dynamically interpolating between model rollouts of various horizon lengths for each individual example, SIEVE ensures that the model is only utilized when doing so does not introduce significant errors. Our approach outperforms model-free baselines on challenging continuous control benchmarks with an order-of-magnitude increase in sample efficiency, and in contrast to previous model-based approaches, performance does not degrade in complex environments.	[Buckman, Jacob; Hafner, Danijar; Tucker, George; Brevdo, Eugene; Lee, Honglak] Google Brain, Mountain View, CA 94043 USA; [Buckman, Jacob] Google AI Residency Program, San Francisco, CA 94103 USA	Google Incorporated	Buckman, J (corresponding author), Google Brain, Mountain View, CA 94043 USA.; Buckman, J (corresponding author), Google AI Residency Program, San Francisco, CA 94103 USA.	jacobbuckman@gmail.com; mail@danijar.com; gjt@google.com; ebrevdo@google.com; honglak@google.com						Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; [Anonymous], 2018, SOFT ACTOR CRITIC OF; Barth-Maron G, 2018, INT C LEARN REPR; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Boedecker, 2017, C ROB LEARN, P195; Brockman G., 2016, OPENAI GYM; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Depeweg S., 2016, LEARNING POLICY SEAR; Espeholt L., 2018, P INT C MACH LEARN; Feinberg V., 2018, ARXIV PREPRINT ARXIV; Fleiss J L, 1993, Stat Methods Med Res, V2, P121, DOI 10.1177/096228029300200202; Fujimoto S, 2018, PR MACH LEARN RES, V80; Gal Y, 2016, PR MACH LEARN RES, V48; Gu S., 2017, INT C LEARN REPR; Gu SX, 2016, PR MACH LEARN RES, V48; Heess N., 2015, NIPS; Horgan D., 2018, INT C LEARN REPR; Kingma D.P., 2015, INT C LEARN REPR ICL; Klimov O., ROBOSCHOOL; Kurutach T., 2018, INT C LEARN REPR; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Mnih V, 2013, PLAYING ATARI DEEP R; Munos R, 2016, P 30 INT C NEUR INF; Pritzel Alexander, 2016, INT C LEARNING REPRE; Schulman J., 2017, ABS170706347 CORR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Thomas Philip S, 2015, ADV NEURAL INFORM PR, V28, P334; Weber T., 2017, 31 C NEUR INF PROC S	31	41	42	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002074
C	Nam, S; Kim, Y; Kim, SJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Nam, Seonghyeon; Kim, Yunji; Kim, Seon Joo			Text-Adaptive Generative Adversarial Networks: Manipulating Images with Natural Language	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					This paper addresses the problem of manipulating images using natural language description. Our task aims to semantically modify visual attributes of an object in an image according to the text describing the new visual appearance. Although existing methods synthesize images having new attributes, they do not fully preserve text-irrelevant contents of the original image. In this paper, we propose the text-adaptive generative adversarial network (TAGAN) to generate semantically manipulated images while preserving text-irrelevant contents. The key to our method is the text-adaptive discriminator that creates word-level local discriminators according to input text to classify fine-grained attributes independently. With this discriminator, the generator learns to generate images where only regions that correspond to the given text are modified. Experimental results show that our method outperforms existing methods on CUB and Oxford-102 datasets, and our results were mostly preferred on a user study. Extensive analysis shows that our method is able to effectively disentangle visual attributes and produce pleasing outputs.	[Nam, Seonghyeon; Kim, Yunji; Kim, Seon Joo] Yonsei Univ, Seoul, South Korea	Yonsei University	Nam, S (corresponding author), Yonsei Univ, Seoul, South Korea.	shnnam@yonsei.ac.kr; kim_yunji@yonsei.ac.kr; seonjookim@yonsei.ac.kr			National Research Foundation of Korea (NRF) - Ministry of Education [NRF2015H1A2A1033924]; Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIP) [2018-0-01858]; ICT R&D program of MSIT/IITP [2017-0-01772]	National Research Foundation of Korea (NRF) - Ministry of Education(National Research Foundation of KoreaMinistry of Education (MOE), Republic of Korea); Institute for Information & communications Technology Promotion (IITP) - Korea government (MSIP)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); ICT R&D program of MSIT/IITP(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea)	This work was supported by Global Ph.D. Fellowship Program through the National Research Foundation of Korea (NRF) funded by the Ministry of Education (NRF2015H1A2A1033924), Institute for Information & communications Technology Promotion (IITP) grant funded by the Korea government (MSIP) (2018-0-01858, Video Manipulation and Language-based Image Editing Technique for Detecting Manipulated Image/Video), and the ICT R&D program of MSIT/IITP (2017-0-01772, Development of QA systems for Video Story Understanding to pass the Video Turing Test).	Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Brock A., 2016, ARXIV160907093; Dong H, 2017, IEEE I CONF COMP VIS, pCP1, DOI 10.1109/ICCV.2017.608; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He Zhenliang, 2017, ARXIV171110678; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Lample Guillaume, 2017, ARXIV170600409; Li SW, 2017, COMPLEXITY, DOI 10.1155/2017/9781890; Liang  X., 2017, ARXIV170800315; Luan FJ, 2017, PROC CVPR IEEE, P6997, DOI 10.1109/CVPR.2017.740; Mirza M., 2014, ARXIV; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Reed S. E., 2016, ADV NEURAL INFORM PR, P217; Reed S, 2016, PR MACH LEARN RES, V48; Reed S, 2016, PROC CVPR IEEE, P49, DOI 10.1109/CVPR.2016.13; Salimans T, 2016, ADV NEUR IN, V29; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Wah C., 2011, TECH REP; Yan XC, 2016, LECT NOTES COMPUT SC, V9908, P776, DOI 10.1007/978-3-319-46493-0_47; Zhang H., 2017, ARXIV PREPRINT ARXIV; Zhang H., 2017, ICCV; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang  Z., 2018, CVPR; ZHOU B, 2016, PROC CVPR IEEE, P2921, DOI DOI 10.1109/CVPR.2016.319; Zhu JY, 2016, LECT NOTES COMPUT SC, V9909, P597, DOI 10.1007/978-3-319-46454-1_36; Zhu Jun-Yan, 2017, ICCV	34	41	43	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300005
C	Santoro, A; Faulkner, R; Raposo, D; Rae, J; Chrzanowski, M; Weber, T; Wierstre, D; Vinyals, O; Pascanu, R; Lillicrap, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Santoro, Adam; Faulkner, Ryan; Raposo, David; Rae, Jack; Chrzanowski, Mike; Weber, Theophane; Wierstre, Daan; Vinyals, Oriol; Pascanu, Razvan; Lillicrap, Timothy			Relational recurrent neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SYSTEM	Memory-based neural networks model temporal data by leveraging an ability to remember information for long periods. It is unclear, however, whether they also have an ability to perform complex relational reasoning with the information they remember. Here, we first confirm our intuitions that standard memory architectures may struggle at tasks that heavily involve an understanding of the ways in which entities are connected - i.e., tasks involving relational reasoning. We then improve upon these deficits by using a new memory module - a Relational Memory Core (RMC) - which employs multi-head dot product attention to allow memories to interact. Finally, we test the RMC on a suite of tasks that may profit from more capable relational reasoning across sequential information, and show large gains in RL domains (e.g. Mini PacMan), program evaluation, and language modeling, achieving state-of-the-art results on the WikiText-103, Project Gutenberg, and GigaWord datasets.	[Santoro, Adam; Faulkner, Ryan; Raposo, David; Rae, Jack; Chrzanowski, Mike; Weber, Theophane; Wierstre, Daan; Vinyals, Oriol; Pascanu, Razvan; Lillicrap, Timothy] DeepMind, London, England; [Rae, Jack; Lillicrap, Timothy] UCL, Comp Sci, CoMPLEX, London, England	University of London; University College London	Santoro, A (corresponding author), DeepMind, London, England.	adamsantoro@google.com; rfaulk@google.com; draposo@google.com; jwrae@google.com; chrzanowskim@google.com; theophane@google.com; weirstra@google.com; vinyals@google.com; razp@google.com; countzero@google.com						Bai S., 2018, ICLR; Bandanau D, 2016, INT CONF ACOUST SPEE, P4945, DOI 10.1109/ICASSP.2016.7472618; Battaglia Peter W, 2016, ARXIV161200222; Bengio Y., 2014, ARXIV14061078; Chelba Ciprian, 2013, ARXIV13123005; Dauphin Yann N., 2016, CORR; Ding Liu, 2018, ARXIV180602919; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Espeholt L, 2018, PR MACH LEARN RES, V80; Gers FA, 1999, IEE CONF PUBL, P850, DOI [10.1049/cp:19991218, 10.1162/089976600300015015]; Gilmer J, 2017, PR MACH LEARN RES, V70; Grave Edouard, 2016, ARXIV161204426; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Henaff Mikael, 2017, 5 INT C LEARN REPR I; Hiemstra Djoerd, 2001, USING LANGUAGE MODEL; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hu H, 2018, PROC CVPR IEEE, P3588, DOI 10.1109/CVPR.2018.00378; Jaitly Navdeep, 2015, ADV NEURAL INFORM PR, V28; Jozefowicz Rafal, 2016, ARXIV160202410; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Knowlton BJ, 2012, TRENDS COGN SCI, V16, P373, DOI 10.1016/j.tics.2012.06.002; Li Yujia, 2016, P INT C LEARN REPR I, P2; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Merity Stephen, 2018, SCALABLE LANGUAGE MO; Parker Robert, 2011, ENGLISH GIGAWORD; Pascanu R., 2013, ARXIV13126026; Pavez Juan, 2018, ARXIV180509354; Rae JW, 2018, PR MACH LEARN RES, V80; Santoro A, 2017, ARXIV PREPRINT ARXIV; Santoro A, 2017, ADV NEUR IN, V30; Santoro A, 2016, PR MACH LEARN RES, V48; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schacter D. L., 1994, MEMORY SYSTEMS 1994; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I, 2014, ADV NEUR IN, V27; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Velickovic P., 2018, P INT C LEARN REPR; Waltz JA, 1999, PSYCHOL SCI, V10, P119, DOI 10.1111/1467-9280.00118; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Yang Zhilin, 2017, ARXIV171103953; Zambaldi Vinicius, 2018, ARXIV180601830; Zaremba Wojciech, 2014, ARXIV14104615V3	47	41	46	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001082
C	Montgomery, W; Levine, S		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Montgomery, William; Levine, Sergey			Guided Policy Search via Approximate Mirror Descent	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Guided policy search algorithms can be used to optimize complex nonlinear policies, such as deep neural networks, without directly computing policy gradients in the high-dimensional parameter space. Instead, these methods use supervised learning to train the policy to mimic a "teacher" algorithm, such as a trajectory optimizer or a trajectory-centric reinforcement learning method. Guided policy search methods provide asymptotic local convergence guarantees by construction, but it is not clear how much the policy improves within a small, finite number of iterations. We show that guided policy search algorithms can be interpreted as an approximate variant of mirror descent, where the projection onto the constraint manifold is not exact. We derive a new guided policy search algorithm that is simpler and provides appealing improvement and convergence guarantees in simplified convex and linear settings, and show that in the more general nonlinear setting, the error in the projection step can be bounded. We provide empirical results on several simulated robotic navigation and manipulation tasks that show that our method is stable and achieves similar or better performance when compared to prior guided policy search methods, with a simpler formulation and fewer hyperparameters.	[Montgomery, William; Levine, Sergey] Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA	University of Washington; University of Washington Seattle	Montgomery, W (corresponding author), Univ Washington, Dept Comp Sci & Engn, Seattle, WA 98195 USA.	wmonty@cs.washington.edu; svlevine@cs.washington.edu			ONR Young Investigator Program award	ONR Young Investigator Program award	We thank the anonymous reviewers for their helpful and constructive feedback. This research was supported in part by an ONR Young Investigator Program award.	Beck A, 2003, OPER RES LETT, V31, P167, DOI 10.1016/S0167-6377(02)00231-6; Guo X, 2014, ADV NEURAL INFORM PR, P3338; Levine S., 2013, ADV NEURAL INFORM PR; Levine S, 2016, J MACH LEARN RES, V17; Levine S, 2014, ADV NEUR IN, V27; Levine S, 2015, IEEE INT CONF ROBOT, P156, DOI 10.1109/ICRA.2015.7138994; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Mordatch I., 2014, ROBOTICS SCI SYSTEMS; Mordatch I., 2015, ADV NEURAL INFORM PR; Peters J, 2008, NEURAL NETWORKS, V21, P682, DOI 10.1016/j.neunet.2008.02.003; Peters J, 2010, AAAI CONF ARTIF INTE, P1607; Ross St<prime>ephane, 2011, AISTATS; Ross S, 2013, IEEE INT CONF ROBOT, P1765, DOI 10.1109/ICRA.2013.6630809; Schneider J, 2003, P INT JOINT C ART IN; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Song J, 2004, I C CONT AUTOMAT ROB, P2223; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Zhang T., 2016, INT C ROB AUT ICRA	19	41	41	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700069
C	Lavrenko, V; Manmatha, R; Jeon, J		Thrun, S; Saul, K; Scholkopf, B		Lavrenko, V; Manmatha, R; Jeon, J			A model for learning the semantics of pictures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We propose an approach to learning the semantics of images which allows us to automatically annotate an image with keywords and to retrieve images based on text queries. We do this using a formalism that models the generation of annotated images. We assume that every image is divided into regions, each described by a continuous-valued feature vector. Given a training set of images with annotations, we compute a joint probabilistic model of image features and words which allow us to predict the probability of generating a word given the image regions. This may be used to automatically annotate and retrieve images given a word as a query. Experiments show that our model significantly outperforms the best of the previously reported results on the tasks of automatic image annotation and retrieval.	Univ Massachusetts, Dept Comp Sci, Ctr Intelligent Informat Retrieval, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Lavrenko, V (corresponding author), Univ Massachusetts, Dept Comp Sci, Ctr Intelligent Informat Retrieval, Amherst, MA 01003 USA.	lavrenko@cs.umass.edu; manmatha@cs.umass.edu; jeon@cs.umass.edu						Barnard K, 2003, J MACH LEARN RES, V3, P1107, DOI 10.1162/153244303322533214; BLEI D, 2003, COMMUNICATION; Blei D.M., 2003, P 26 ANN INT ACM SIG, P127, DOI [10.1145/860435.860460, DOI 10.1145/860435.860460]; Duygulu P, 2002, LECT NOTES COMPUT SC, V2353, P97; Jeon J., 2003, P ACM SIGIR C RES DE, P119; Lavrenko V., 2002, Proceedings of SIGIR 2002. Twenty-Fifth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P175; LVRENKO V, 2001, P 24 INT ACM SIGIR C, P120; Mori Y., 1999, MISRM 99 1 INT WORKS; Ponte Jay M, 1998, P 21 ANN INT ACM SIG, P275, DOI DOI 10.1145/290941.291008; SCNEIDERMAN H, P IEEE CVPR 2000, P1746; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688	11	41	49	1	6	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						553	560						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500070
C	Olshausen, BA; Millman, KJ		Solla, SA; Leen, TK; Muller, KR		Olshausen, BA; Millman, KJ			Learning sparse codes with a mixture-of-Gaussians prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We describe a method for learning an overcomplete set of basis functions for the purpose of modeling sparse structure in images. The sparsity of the basis function coefficients is modeled with a mixture-of-Gaussians distribution. One Gaussian captures nonactive coefficients with a small-variance distribution centered at zero, while one or more other Gaussians capture active coefficients with a large-variance distribution. We show that when the prior is in such a form, there exist efficient methods for learning the basis functions as well as the parameters of the prior. The performance of the algorithm is demonstrated on a number of test cases and also on natural images. The basis functions learned on natural images are similar to those obtained with other methods, but the sparse form of the coefficient distribution is much better described. Also, since the parameters of the prior are adapted to the data, no assumption about sparse structure in the images need be made a priori, rather it is learned from the data.	Univ Calif Davis, Dept Psychol, Davis, CA 95616 USA	University of California System; University of California Davis	Olshausen, BA (corresponding author), Univ Calif Davis, Dept Psychol, 1544 Newton Ct, Davis, CA 95616 USA.			Millman, Kenneth/0000-0002-5263-5070				Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Lewicki MS, 1999, J OPT SOC AM A, V16, P1587, DOI 10.1364/JOSAA.16.001587; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303	6	41	41	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						841	847						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700119
C	Abbott, LF; Song, S		Kearns, MS; Solla, SA; Cohn, DA		Abbott, LF; Song, S			Temporally asymmetric Hebbian learning, spike timing and neuronal response variability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				LONG-TERM POTENTIATION; SYNAPTIC PLASTICITY; MODEL; HIPPOCAMPUS; CELLS; RAT	Recent experimental data indicate that the strengthening or weakening of synaptic connections between neurons depends on the relative timing of pre- and postsynaptic action potentials. A Hebbian synaptic modification rule based on these data leads to a stable state in which the excitatory and inhibitory inputs to a neuron are balanced, producing an irregular pattern of firing. It has been proposed that neurons in vivo operate in such a mode.	Brandeis Univ, Volen Ctr, Waltham, MA 02454 USA	Brandeis University	Abbott, LF (corresponding author), Brandeis Univ, Volen Ctr, Waltham, MA 02454 USA.							Abbott LF, 1996, CEREB CORTEX, V6, P406, DOI 10.1093/cercor/6.3.406; Amit DJ, 1997, CEREB CORTEX, V7, P237, DOI 10.1093/cercor/7.3.237; Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0; BI GQ, 1999, IN PRESS J NEUROPHYS; Blum KI, 1996, NEURAL COMPUT, V8, P85, DOI 10.1162/neco.1996.8.1.85; Bugmann G, 1997, NEURAL COMPUT, V9, P985, DOI 10.1162/neco.1997.9.5.985; Debanne D, 1998, J PHYSIOL-LONDON, V507, P237, DOI 10.1111/j.1469-7793.1998.237bu.x; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; Gerstner W, 1997, J COMPUT NEUROSCI, V4, P79, DOI 10.1023/A:1008820728122; GUSTAFSSON B, 1987, J NEUROSCI, V7, P774; Hebb D.O., 1949, ORG BEHAV NEUROPSYCH; Hertz J., 1991, INTRO THEORY NEURAL, DOI DOI 10.1201/9780429499661; KEMPTER R, 1999, UNPUB HEBBIAN LEARNI; LEVY WB, 1983, NEUROSCIENCE, V8, P791, DOI 10.1016/0306-4522(83)90010-6; MALENKA RC, 1993, TRENDS NEUROSCI, V16, P521, DOI 10.1016/0166-2236(93)90197-T; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Minai A. A., 1993, INNS WORLD C NEUR NE, P505; Rumelhart D. E., 1988, PARALLEL DISTRIBUTED; Rumelhart DE, 1986, PARALLEL DISTRIBUTED; Shadlen Michael N., 1994, Current Opinion in Neurobiology, V4, P569, DOI 10.1016/0959-4388(94)90059-0; Shadlen MN, 1998, J NEUROSCI, V18, P3870; SOFTKY WR, 1992, NEURAL COMPUT, V4, P643, DOI 10.1162/neco.1992.4.5.643; SOFTKY WR, 1993, J NEUROSCI, V13, P334; Troyer TW, 1997, NEURAL COMPUT, V9, P971, DOI 10.1162/neco.1997.9.5.971; Troyer TW, 1997, COMPUTATIONAL NEUROSCIENCE: TRENDS IN RESEARCH, 1997, P197; Tsodyks M., 1995, NETWORK, V6, P1; van Vreeswijk C, 1998, NEURAL COMPUT, V10, P1321, DOI 10.1162/089976698300017214; vanVreeswijk C, 1996, SCIENCE, V274, P1724, DOI 10.1126/science.274.5293.1724; Zhang LI, 1998, NATURE, V395, P37, DOI 10.1038/25665	29	41	41	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						69	75						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700010
C	Haruno, M; Wolpert, DM; Kawato, M		Kearns, MS; Solla, SA; Cohn, DA		Haruno, M; Wolpert, DM; Kawato, M			Multiple paired forward-inverse models for human motor learning and control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Humans demonstrate a remarkable ability to generate accurate and appropriate motor behavior under many different and often uncertain environmental conditions. This paper describes a new modular approach to human motor learning and control, based on multiple pairs of inverse (controller) and forward (predictor) models. This architecture simultaneously learns the multiple inverse models necessary for control as well as how to select the inverse models appropriate for a given environment. Simulations of object manipulation demonstrates the ability to learn multiple objects, appropriate generalization to novel objects and the inappropriate activation of motor programs based on visual cues, followed by on-line correction, seen in the "size-weight illusion".	ATR, Human Informat Proc Res Labs, Seika, Kyoto 61902, Japan		Haruno, M (corresponding author), ATR, Human Informat Proc Res Labs, 2-2 Hikaridai, Seika, Kyoto 61902, Japan.			Wolpert, Daniel/0000-0003-2011-2790				Brenner E, 1996, EXP BRAIN RES, V111, P473; Flanagan WP, 1997, IEEE COMPUT GRAPH, V17, P4, DOI 10.1109/MCG.1997.586010; Fraser A., 1993, TIME SERIES PREDICTI, P265; GOMI H, 1993, NEURAL NETWORKS, V6, P485, DOI 10.1016/S0893-6080(05)80053-X; GORDON AM, 1991, EXP BRAIN RES, V83, P477; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; KAWATO M, 1990, ADVANCED NEURAL COMPUTERS, P365; Narendra KS, 1997, IEEE T AUTOMAT CONTR, V42, P171, DOI 10.1109/9.554398; Pawelzik K, 1996, NEURAL COMPUT, V8, P340, DOI 10.1162/neco.1996.8.2.340; Wolpert DM, 1998, NEURAL NETWORKS, V11, P1317, DOI 10.1016/S0893-6080(98)00066-5	10	41	42	0	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						31	37						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700005
C	Sen, R; Yu, HF; Dhillon, I		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sen, Rajat; Yu, Hsiang-Fu; Dhillon, Inderjit			Think Globally, Act Locally: A Deep Neural Network Approach to High-Dimensional Time Series Forecasting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Forecasting high-dimensional time series plays a crucial role in many applications such as demand forecasting and financial predictions. Modern datasets can have millions of correlated time-series that evolve together, i.e they are extremely high dimensional (one dimension for each individual time-series). There is a need for exploiting global patterns and coupling them with local calibration for better prediction. However, most recent deep learning approaches in the literature are one-dimensional, i.e, even though they are trained on the whole dataset, during prediction, the future forecast for a single dimension mainly depends on past values from the same dimension. In this paper, we seek to correct this deficiency and propose DeepGLO, a deep forecasting model which thinks globally and acts locally. In particular, DeepGLO is a hybrid model that combines a global matrix factorization model regularized by a temporal convolution network, along with another temporal network that can capture local properties of each time-series and associated covariates. Our model can be trained effectively on high-dimensional but diverse time series, where different time series can have vastly different scales, without a priori normalization or rescaling. Empirical results demonstrate that DeepGLO can outperform state-of-the-art approaches; for example, we see more than 25% improvement in WAPE over other methods on a public dataset that contains more than 100K-dimensional time series.	[Sen, Rajat; Yu, Hsiang-Fu; Dhillon, Inderjit] Amazon, Seattle, WA 98170 USA; [Dhillon, Inderjit] UT Austin, Austin, TX USA	Amazon.com; University of Texas System; University of Texas Austin	Sen, R (corresponding author), Amazon, Seattle, WA 98170 USA.							Alexandrov Alexander, 2019, ARXIV190605264; [Anonymous], 2017, FBPROPH; Bai S., 2018, ARXIV; Borovykh A, 2017, STAT-US; BOX GEP, 1968, ROY STAT SOC C-APP, V17, P91; Chatfield C., 2000, TIME SERIES FORECAST; Chen CHW, 2001, AIP CONF PROC, V584, P96, DOI 10.1063/1.1405589; Cuturi M., 2011, P 28 INT C MACH LEAR, P929; Flunkert V., 2017, ARXIV170404110; Gers Felix A, 1999, LEARNING FORGET CONT; Hamilton James D., 1994, TIME SERIES ANAL, V2; Hyndman RJ, 2008, SPRINGER SER STAT, P3; Kim KJ, 2003, NEUROCOMPUTING, V55, P307, DOI 10.1016/S0925-2312(03)00372-2; Lai GK, 2018, ACM/SIGIR PROCEEDINGS 2018, P95, DOI 10.1145/3209978.3210006; Larson P. D., 2001, J BUS LOGIST, V22, P259, DOI DOI 10.1002/J.2158-1592.2001.TB00165.X; Lutkepohl Helmut, 2005, NEW INTRO MULTIPLE T; MCKENZIE E, 1984, J FORECASTING, V3, P333, DOI 10.1002/for.3980030312; Rangapuram Syama Sundar, 2018, ADV NEURAL INF PROCE, P7796; Seeger MW., 2016, ADV NEURAL INFORM PR, P4646; Sundermeyer M., 2012, 13 ANN C INT SPEECH; Trindade Artur, 2011, ELECTRICITYLOADDIAGR; van den Oord A., 2016, GENERATIVE MODEL RAW; Wang Y, 2019, INT C MACH LEARN, P6607; Wen R., 2017, TIME SERIES WORKSHOP; Wilson K. W., 2008, 9 ANN C INT SPEECH C; Yu B., 2017, ARXIV170904875; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847	28	40	40	3	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304080
C	Ben-Nun, T; Jakobovits, AS; Hoefler, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ben-Nun, Tal; Jakobovits, Alice Shoshana; Hoefler, Torsten			Neural Code Comprehension: A Learnable Representation of Code Semantics	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				GRAPH; MODEL	With the recent success of embeddings in natural language processing, research has been conducted into applying similar methods to code analysis. Most works attempt to process the code directly or use a syntactic tree representation, treating it like sentences written in a natural language. However, none of the existing methods are sufficient to comprehend program semantics robustly, due to structural features such as function calls, branching, and interchangeable order of statements. In this paper, we propose a novel processing technique to learn code semantics, and apply it to a variety of program analysis tasks. In particular, we stipulate that a robust distributional hypothesis of code applies to both human- and machine-generated programs. Following this hypothesis, we define an embedding space, inst2vec, based on an Intermediate Representation (IR) of the code that is independent of the source programming language. We provide a novel definition of contextual flow for this IR, leveraging both the underlying data- and control-flow of the program. We then analyze the embeddings qualitatively using analogies and clustering, and evaluate the learned representation on three different high-level tasks. We show that even without fine-tuning, a single RNN architecture and fixed inst2vec embeddings outperform specialized approaches for performance prediction (compute device mapping, optimal thread coarsening); and algorithm classification from raw code (104 classes), where we set a new state-of-the-art.	[Ben-Nun, Tal; Jakobovits, Alice Shoshana; Hoefler, Torsten] Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Ben-Nun, T (corresponding author), Swiss Fed Inst Technol, CH-8092 Zurich, Switzerland.	talbn@inf.ethz.ch; alicej@student.ethz.ch; htor@inf.ethz.ch	Hoefler, Torsten/AAB-7478-2022		ETH Postdoctoral Fellowship; Marie Curie Actions for People COFUND program	ETH Postdoctoral Fellowship; Marie Curie Actions for People COFUND program	We wish to thank Theodoros Theodoridis, Kfir Levy, Tobias Grosser, and Yunyan Guo for fruitful discussions. The authors also acknowledge MeteoSwiss, and thank Hussein Harake, Colin McMurtrie, and the whole CSCS team for granting access to the Greina machines, and for their excellent technical support. TBN is supported by the ETH Postdoctoral Fellowship and Marie Curie Actions for People COFUND program.	Abadi M, 2015, P 12 USENIX S OPERAT; Allamanis M, 2017, LEARNING REPRESENT P; Allamanis M., 2017, ARXIV PREPRINT ARXIV; Allamanis M, 2016, PR MACH LEARN RES, V48; Allamanis M, 2015, 2015 10TH JOINT MEETING OF THE EUROPEAN SOFTWARE ENGINEERING CONFERENCE AND THE ACM SIGSOFT SYMPOSIUM ON THE FOUNDATIONS OF SOFTWARE ENGINEERING (ESEC/FSE 2015) PROCEEDINGS, P38, DOI 10.1145/2786805.2786849; Allamanis Miltiadis, 2017, ABS170507867 CORR; Alon Uri, 2018, ABS180309544 CORR; AMD, AMD OPENCL ACC PAR P; Balaprakash P, 2013, IEEE INT C CL COMP; Baldauf M, 2011, MON WEATHER REV, V139, P3887, DOI 10.1175/MWR-D-10-05013.1; Bielik P, 2016, PR MACH LEARN RES, V48; Bunel Rudy, 2017, INT C LEARN REPR; Che SA, 2009, I S WORKL CHAR PROC, P44, DOI 10.1109/IISWC.2009.5306797; CLICK C, 1995, SIGPLAN NOTICES, V30, P35, DOI 10.1145/202530.202534; Click Cliff, 1995, ACM T PROGRAMMING LA, V17; Cummins C, 2017, INT CONFER PARA, P219, DOI 10.1109/PACT.2017.24; CYTRON R, 1991, ACM T PROGR LANG SYS, V13, P451, DOI 10.1145/115372.115320; Dam Hoa Khanh, 2016, ABS160802715 CORR; Danalis A., 2010, PROC GPGPU 3, P63, DOI [10.1145/1735688.1735702, DOI 10.1145/1735688.1735702]; Dongarra Jack, 2002, WELDING RES COUNCIL, P1; dos Santos CN, 2014, PR MACH LEARN RES, V32, P1818; ELMAN JL, 1990, COGNITIVE SCI, V14, P179, DOI 10.1207/s15516709cog1402_1; FERRANTE J, 1987, ACM T PROGR LANG SYS, V9, P319, DOI 10.1145/24039.24041; GitHub, 2017, GITHUB OCT; Glorot Xavier, 2011, AIS, P315; Grauer-Gray Scott, 2012, AUTOTUNING HIGH LEVE; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Grewe D, 2013, INT SYM CODE GENER, P161; Gu XD, 2016, FSE'16: PROCEEDINGS OF THE 2016 24TH ACM SIGSOFT INTERNATIONAL SYMPOSIUM ON FOUNDATIONS OF SOFTWARE ENGINEERING, P631, DOI 10.1145/2950290.2950334; Harris Z, 1981, PAPERS SYNTAX, P3; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hsiao CH, 2014, ACM SIGPLAN NOTICES, V49, P49, DOI [10.1145/2714064.2660226, 10.1145/2660193.2660226]; Itseez, 2015, OP SOURC COMP VIS LI; Kingma D.P, P 3 INT C LEARNING R; LAMPORT L, 1978, COMMUN ACM, V21, P558, DOI 10.1145/359545.359563; Lattner C, 2004, INT SYM CODE GENER, P75, DOI 10.1109/CGO.2004.1281665; Leather H, 2009, INT SYM CODE GENER, P81, DOI 10.1109/CGO.2009.21; Levy D, 2017, PR MACH LEARN RES, V70; Linux, LIN KERN SOURC COD V; LLVM, FLANG FORTRAN COMP F; LLVM, 2018, LLVM LANG REF MAN; LLVM, CLANG C LANG FAM FRO; Magni A, 2014, INT CONFER PARA, P455, DOI 10.1145/2628071.2628087; Mikolov T., 2013, ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mou LL, 2016, AAAI CONF ARTIF INTE, P1287; Namolaru M, 2010, PROCEEDINGS OF THE 2010 INTERNATIONAL CONFERENCE ON COMPILERS, ARCHITECTURES AND SYNTHESIS FOR EMBEDDED SYSTEMS (CASES '10), P197, DOI 10.1145/1878921.1878951; Nobre R, 2016, ACM SIGPLAN NOTICES, V51, P21, DOI [10.1145/2907950.2907959, 10.1145/2980930.2907959]; Pantel, 2005, P 43 ANN M ASS COMP, P125; Park Eunjung, 2012, P 10 INT S CODE GENE, P196, DOI [10.1145/2259016.2259042, DOI 10.1145/2259016.2259042]; Raychev V, 2014, ACM SIGPLAN NOTICES, V49, P419, DOI [10.1145/2666356.2594321, 10.1145/2594291.2594321]; Schkufza E, 2013, ACM SIGPLAN NOTICES, V48, P305, DOI 10.1145/2499368.2451150; Seo S, 2011, I S WORKL CHAR PROC, P137, DOI 10.1109/IISWC.2011.6114174; Sreedhar V. C., 1995, Conference Record of POPL'95: 22nd ACM SIGPLAN-SIGACT Symposium Principles of Programming Languages, P62, DOI 10.1145/199448.199464; Stratton John A., 2012, REVISED BENCHMARK SU; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Vechev M, 2016, FOUND TRENDS PROGRAM, V3, P231, DOI 10.1561/2500000028; Wang B., 2015, DEEP LEARNING ASPECT, P1; White M, 2016, IEEE INT CONF AUTOM, P87, DOI 10.1145/2970276.2970326; Xu Xiaojun, 2017, ABS170806525 CORR; Yahav Eran, 2018, CORR; Yang YX, 2017, IEEE INT CONF AUTOM, P682, DOI 10.1109/ASE.2017.8115678	64	40	43	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303057
C	Chen, LM; Zhang, GX; Zhou, HN		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chen, Laming; Zhang, Guoxin; Zhou, Hanning			Fast Greedy MAP Inference for Determinantal Point Process to Improve Recommendation Diversity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The determinantal point process (DPP) is an elegant probabilistic model of repulsion with applications in various machine learning tasks including summarization and search. However, the maximum a posteriori (MAP) inference for DPP which plays an important role in many applications is NP-hard, and even the popular greedy algorithm can still be too computationally expensive to be used in large-scale real-time scenarios. To overcome the computational challenge, in this paper, we propose a novel algorithm to greatly accelerate the greedy MAP inference for DPP. In addition, our algorithm also adapts to scenarios where the repulsion is only required among nearby few items in the result sequence. We apply the proposed algorithm to generate relevant and diverse recommendations. Experimental results show that our proposed algorithm is significantly faster than state-of-the-art competitors, and provides a better relevance-diversity trade-off on several public datasets, which is also confirmed in an online A/B test.	[Chen, Laming; Zhou, Hanning] Hulu LLC, Beijing, Peoples R China; [Zhang, Guoxin] Kwai Inc, Beijing, Peoples R China		Chen, LM (corresponding author), Hulu LLC, Beijing, Peoples R China.	laming.chen@hulu.com; zhangguoxin@kuaishou.com; ericzhouh@gmail.com						Adomavicius Gediminas, 2011, P 1 INT WORKSH NOV D, P3; Agrawal R., 2009, P WSDM, P5, DOI DOI 10.1145/1498759.1498766; [Anonymous], 2012, H ACM INT C WEB SEAR, DOI DOI 10.1145/2124295.2124337; [Anonymous], 2013, P 7 ACM C RECOMMENDE; Ashkan A, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1742; Aytekin T, 2014, J INTELL INF SYST, V42, P1, DOI 10.1007/s10844-013-0252-9; Bertin-Mahieux T., 2011, P INT SOC MUS INF RE, V2, P10; Boim Rubi, 2011, INT C INF KNOWL MAN, DOI DOI 10.1145/2063576.2063684; Borodin A., 2012, P 31 ACM SIGMOD SIGA, P155; Bradley K., 2001, P 12 IR C ART INT CO, P85; Buchbinder N, 2015, SIAM J COMPUT, V44, P1384, DOI 10.1137/130929205; Carbonell J., 1998, Proceedings of the 21st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, P335, DOI 10.1145/290941.291025; Chandar P, 2013, SIGIR'13: THE PROCEEDINGS OF THE 36TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH & DEVELOPMENT IN INFORMATION RETRIEVAL, P413; Civril A, 2009, THEOR COMPUT SCI, V410, P4801, DOI 10.1016/j.tcs.2009.06.018; Gartrell M, 2017, AAAI CONF ARTIF INTE, P1912; Gartrell M, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P349, DOI 10.1145/2959100.2959178; Gillenwater J., 2012, ADV NEURAL INFORM PR; Gillenwater J., 2014, ADV NEURAL INFORM PR, V2, P3149; Gillenwater J., 2014, APPROXIMATE INFERENC; Gong B., 2014, ADV NEURAL INFORM PR, P2069; Han I., 2017, ICML, P1384; He J, 2012, NIPS, P1142; Hurley N, 2011, ACM T INTERNET TECHN, V10, DOI 10.1145/1944339.1944341; Jarvelin K., 2000, SIGIR Forum, V34, P41; Karypis G., 2001, Proceedings of the 2001 ACM CIKM. Tenth International Conference on Information and Knowledge Management, P247, DOI 10.1145/502585.502627; KO CW, 1995, OPER RES, V43, P684, DOI 10.1287/opre.43.4.684; Koren Y, 2009, COMPUTER, V42, P30, DOI 10.1109/MC.2009.263; Kulesza A., 2010, P NEURIPS, V1411, P1412; Kulesza A., 2011, UAI, P419; Kulesza Alex, 2011, ICML; Lee SC, 2017, WWW'17 COMPANION: PROCEEDINGS OF THE 26TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P809, DOI 10.1145/3041021.3054219; Li CT, 2016, PR MACH LEARN RES, V48; MACCHI O, 1975, ADV APPL PROBAB, V7, P83, DOI 10.2307/1425855; Mariet Z, 2015, PR MACH LEARN RES, V37, P2389; MEHTA ML, 1960, NUCL PHYS, V18, P420, DOI 10.1016/0029-5582(60)90414-4; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Niemann K, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P955; Parambath SAP, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P15, DOI 10.1145/2959100.2959149; Qin Lijing, 2013, IJCAI; Schott J. R., 1999, J AM STAT ASSOC, V94, P1388, DOI [10.2307/2669960, DOI 10.2307/2669960]; Steck H., 2011, P 5 ACM C RECOMMENDE, P125; Teo CH, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P35, DOI 10.1145/2959100.2959171; Vargas S, 2014, PROCEEDINGS OF THE 8TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'14), P209, DOI 10.1145/2645710.2645743; Voorhees E.M., 1999, NAT LANG ENG, V99, P77, DOI DOI 10.1017/S1351324901002789; Wu L, 2016, ACM T INTEL SYST TEC, V7, DOI 10.1145/2700496; Xia L, 2017, SIGIR'17: PROCEEDINGS OF THE 40TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P535, DOI 10.1145/3077136.3080775; Yao JG, 2016, AAAI CONF ARTIF INTE, P3080; Yu C., 2009, EDBT, P368; Zhang M, 2008, RECSYS'08: PROCEEDINGS OF THE 2008 ACM CONFERENCE ON RECOMMENDER SYSTEMS, P123	52	40	41	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000015
C	Guo, DY; Tang, DY; Duan, N; Zhou, M; Yin, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Guo, Daya; Tang, Duyu; Duan, Nan; Zhou, Ming; Yin, Jian			Dialog-to-Action: Conversational Question Answering Over a Large-Scale Knowledge Base	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present an approach to map utterances in conversation to logical forms, which will be executed on a large-scale knowledge base. To handle enormous ellipsis phenomena in conversation, we introduce dialog memory management to manipulate historical entities, predicates, and logical forms when inferring the logical form of current utterances. Dialog memory management is embodied in a generative model, in which a logical form is interpreted in a top-down manner following a small and flexible grammar. We learn the model from denotations without explicit annotation of logical forms, and evaluate it on a large-scale dataset consisting of 200K dialogs over 12.8M entities. Results verify the benefits of modeling dialog memory, and show that our semantic parsing-based approach outperforms a memory network based encoder-decoder model by a huge margin.	[Guo, Daya; Yin, Jian] Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Big Data Anal & Proc, Guangzhou, Guangdong, Peoples R China; [Tang, Duyu; Duan, Nan; Zhou, Ming] Microsoft Res Asia, Beijing, Peoples R China; [Guo, Daya] Microsoft Res, Beijing, Peoples R China	Sun Yat Sen University; Microsoft; Microsoft Research Asia; Microsoft	Guo, DY (corresponding author), Sun Yat Sen Univ, Sch Data & Comp Sci, Guangdong Key Lab Big Data Anal & Proc, Guangzhou, Guangdong, Peoples R China.; Guo, DY (corresponding author), Microsoft Res, Beijing, Peoples R China.	guody5@mail2.sysu.edu.cn; dutang@microsoft.com; nanduan@microsoft.com; mingzhou@microsoft.com; issjyin@mail.sysu.edu.cn			National Natural Science Foundation of China [61472453, U1401256, U1501252, U1611264, U1711261, U1711262]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is supported by the National Natural Science Foundation of China (61472453, U1401256, U1501252, U1611264, U1711261, U1711262). Thanks to the anonymous reviewers and Junwei Bao for their helpful comments and suggestions.	Artzi Y, 2013, T ASSOC COMPUT LING, V1, P49, DOI DOI 10.1162/TACL_A_00209; Bengio Y., 2014, ARXIV14061078; Bordes A., 2013, ADV NEURAL INFORM PR; Dodge Jesse, 2015, ARXIV151106931; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Guu K, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1051, DOI 10.18653/v1/P17-1097; Iyer Srinivasan, 2018, P 2018 C EMP METH NA, P1643; Iyyer M, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P1821, DOI 10.18653/v1/P17-1167; Krishnamurthy Jayant, 2017, P 2017 C EMP METH NA, P1516, DOI DOI 10.18653/V1/D17-1160; Long R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1456; Luo Liangchen, 2019, P 33 AAAI C ART INT; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; Miller A., 2016, ARXIV160603126, P1400; Misra Dipendra Kumar, 2016, P 2016 C EMPIRICAL M, P1775; Saha A., 2018, ARXIV180110314; SHUM HY, 2018, ARXIV PREPRINT ARXIV, V19, P10, DOI DOI 10.1631/FITEE.1700826; Socher R., 2013, ADV NEURAL INFORM PR, V26, P1; Suhr A, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2072; Suhr Alane, 2018, ARXIV180406868; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vlachos A., 2014, T ASS COMPUTATIONAL, V2, P547; Wei Lu, 2008, 2008 C EMPIRICAL MET, P783; Weston Jason E, 2016, ADV NEURAL INFORM PR, P829; Yin PC, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P440, DOI 10.18653/v1/P17-1041; Yin Pengcheng, 2018, ARXIV181013337; Zettlemoyer L. S., 2009, P JOINT C 47 ANN M A, V2, P976, DOI DOI 10.18653/V1/P19-1443	29	40	41	5	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302092
C	Kumar, A; Sattigeri, P; Wadhawan, K; Karlinsky, L; Feris, R; Freeman, WT; Wornell, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kumar, Abhishek; Sattigeri, Prasanna; Wadhawan, Kahini; Karlinsky, Leonid; Feris, Rogerio; Freeman, William T.; Wornell, Gregory			Co-regularized Alignment for Unsupervised Domain Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks, trained with large amount of labeled data, can fail to generalize well when tested with examples from a target domain whose distribution differs from the training data distribution, referred as the source domain. It can be expensive or even infeasible to obtain required amount of labeled data in all possible domains. Unsupervised domain adaptation sets out to address this problem, aiming to learn a good predictive model for the target domain using labeled examples from the source domain but only unlabeled examples from the target domain. Domain alignment approaches this problem by matching the source and target feature distributions, and has been used as a key component in many state-of-the-art domain adaptation methods. However, matching the marginal feature distributions does not guarantee that the corresponding class conditional distributions will be aligned across the two domains. We propose co-regularized domain alignment for unsupervised domain adaptation, which constructs multiple diverse feature spaces and aligns source and target distributions in each of them individually, while encouraging that alignments agree with each other with regard to the class predictions on the unlabeled target examples. The proposed method is generic and can be used to improve any domain adaptation method which uses domain alignment. We instantiate it in the context of a recent state-of-the-art method and observe that it provides significant performance improvements on several domain adaptation benchmarks.	[Kumar, Abhishek; Sattigeri, Prasanna; Wadhawan, Kahini; Karlinsky, Leonid; Feris, Rogerio] MIT, IBM Res, IBM Watson AI Lab, Cambridge, MA 02139 USA; [Freeman, William T.; Wornell, Gregory] MIT, Cambridge, MA 02139 USA	International Business Machines (IBM); Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT)	Kumar, A (corresponding author), MIT, IBM Res, IBM Watson AI Lab, Cambridge, MA 02139 USA.	abhishk@us.ibm.com; psattig@us.ibm.com; kahini.wadhawan@ibm.com; leonidka@il.ibm.com; rsferis@us.ibm.com; billf@mit.edu; gww@mit.edu	Sattigeri, Prasanna/U-4063-2019	Sattigeri, Prasanna/0000-0003-4435-0486; Wadhawan, Kahini/0000-0002-7221-7220				Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Blum A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P92, DOI 10.1145/279943.279962; Bousmalis K, 2017, PROC CVPR IEEE, P95, DOI 10.1109/CVPR.2017.18; Bousmalis Konstantinos, 2016, ADV NEURAL INFORM PR, P343; Bousmalis Konstantinos, 2017, ABS170907857 CORR; Chapelle O, 2005, P 10 INT WORKSH ART, V2005, P57; Chen M., 2011, ADV NEURAL INF PROCE, P2456; Daume III Hal, 2010, NIPS; Dietterich TG, 2000, LECT NOTES COMPUT SC, V1857, P1, DOI 10.1007/3-540-45014-9_1; DRUCKER H, 1994, NEURAL COMPUT, V6, P1289, DOI 10.1162/neco.1994.6.6.1289; Dumoulin V., 2017, P INT C LEARN REPR T; Fernando B, 2013, IEEE I CONF COMP VIS, P2960, DOI 10.1109/ICCV.2013.368; French G., 2018, P ICLR; Ganin Y., 2016, JMLR, V17, P2096; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Ghifary M, 2014, LECT NOTES ARTIF INT, V8862, P898, DOI 10.1007/978-3-319-13560-1_76; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grandvalet Y., 2005, CAP, P529; Kumar A., 2011, P 28 INT C MACH LEAR, P393, DOI DOI 10.5555/3104482.3104532; Kumar Abhishek, 2011, NEURIPS, P2, DOI DOI 10.5555/2986459.2986617; Laine S., 2016, INT C LEARN REPR; Lee S., 2016, ADV NEURAL INFORM PR, V29, P2119; Liu Ming-Yu, 2017, NIPS; Long MS, 2015, PR MACH LEARN RES, V37, P97; Miyato T, 2019, IEEE T PATTERN ANAL, V41, P1979, DOI 10.1109/TPAMI.2018.2858821; Murez Z., 2017, ARXIV171200479; Nguyen XL, 2010, IEEE T INFORM THEORY, V56, P5847, DOI 10.1109/TIT.2010.2068870; Rosen B. E., 1996, Connection Science, V8, P373, DOI 10.1080/095400996116820; Rosenberg D. S., 2007, P 11 INT C ART INT S, P396; SAITO K, 2018, CVPR; Saito K, 2017, PR MACH LEARN RES, V70; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Shu Rui, 2018, P 6 INT C LEARN REPR, P2; Sindhwani V., 2005, P 22 INT C MACH LEAR, P824, DOI DOI 10.1145/1102351.1102455; Sindhwani V., 2008, INT C MACH LEARN, V307, P976, DOI DOI 10.1145/1390156.1390279; Sridharan Karthik, 2008, COLT, P403; Sun B., 2014, BMVC, V1, P3; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Vazquez D, 2014, IEEE T PATTERN ANAL, V36, P797, DOI 10.1109/TPAMI.2013.163; Yan HL, 2017, PROC CVPR IEEE, P945, DOI 10.1109/CVPR.2017.107; Zhou ZH, 2005, IEEE T KNOWL DATA EN, V17, P1529, DOI 10.1109/TKDE.2005.186; Zolna Konrad, 2018, INT C LEARN REPR	46	40	41	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003086
C	Ma, TF; Chen, J; Xiao, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ma, Tengfei; Chen, Jie; Xiao, Cao			Constrained Generation of Semantically Valid Graphs via Regularizing Variational Autoencoders	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep generative models have achieved remarkable success in various data domains, including images, time series, and natural languages. There remain, however, substantial challenges for combinatorial structures, including graphs. One of the key challenges lies in the difficulty of ensuring semantic validity in context. For example, in molecular graphs, the number of bonding-electron pairs must not exceed the valence of an atom; whereas in protein interaction networks, two proteins may be connected only when they belong to the same or correlated gene ontology terms. These constraints are not easy to be incorporated into a generative model. In this work, we propose a regularization framework for variational autoencoders as a step toward semantic validity. We focus on the matrix representation of graphs and formulate penalty terms that regularize the output distribution of the decoder to encourage the satisfaction of validity constraints. Experimental results confirm a much higher likelihood of sampling valid graphs in our approach, compared with others reported in the literature.	[Ma, Tengfei; Chen, Jie; Xiao, Cao] IBM Res, New York, NY 10598 USA	International Business Machines (IBM)	Ma, TF (corresponding author), IBM Res, New York, NY 10598 USA.	Tengfei.Ma1@ibm.com; chenjie@us.ibm.com; cxiao@us.ibm.com	cai, jie/HHS-0606-2022	Ma, Tengfei/0000-0002-1086-529X				[Anonymous], 2016, CORR; [Anonymous], 2017, ICLR; [Anonymous], 2015, CORR; Arjovsky Martin, 2017, ICML 17; Arjovsky Martin, 2017, ICLR 17; Arora Sanjeev, 2017, ICML 17; Barabasi AL, 1999, SCIENCE, V286, P509, DOI 10.1126/science.286.5439.509; Bjerrum Esben Jannik, 2017, CORR; Bojchevski Aleksandar, 2018, ICML; Dai Hanjun, 2018, INT C LEARN REPR; Defferrard M., 2016, CORR; Duvenaud David K, 2015, NIPS 15; ERDOS P, 1960, B INT STATIST INST, V38, P343; Gaunt A. L., 2016, CORR; Gilmer J., 2017, ICML; Gomez-Bombarelli R, 2016, CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo Xiaojie, 2018, ARXIV PREPRINT ARXIV; Hajibagheri A., 2017, INT C SOC COMP BEH C; Hu Z., 2017, P 34 INT C MACH LEAR; Hu Zhiting, 2017, CORR; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Irwin John J., 2012, J CHEM INFORM MODELI, V52; Jaques N, 2016, CORR; Jin Wengong, 2018, CORR; Kingma D.P., 2013, CORR; Kusner MJ, 2017, PR MACH LEARN RES, V70; Li Yujia, 2015, ICML 15; Nowozin Sebastian, 2016, P ADV NEURAL INFORM; Radford A., 2015, CORR; Ramakrishnan R, 2014, SCI DATA, V1, DOI 10.1038/sdata.2014.22; Salimans Tim, 2016, ADV NEURAL INFORM PR; Scarselli F., 2009, IEEE T NEURAL NETWOR, V20; Segler M. H. S., 2017, CORR; Simonovsky Martin, 2018, GRAPHVAE GENERATION; van den Oord A., 2016, SSW; van den Oord Aron, 2016, NIPS; Vondrick C., 2016, CORR; WEININGER D, 1988, J CHEM INF COMP SCI, V28, P31, DOI 10.1021/ci00057a005; Xi Chen, 2016, CORR; Zhu PIsolaJun-Yan, 2018, ARXIV	41	40	41	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001064
C	Moyer, D; Gao, SY; Brekelmans, R; Steeg, GV; Galstyan, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Moyer, Daniel; Gao, Shuyang; Brekelmans, Rob; Steeg, Greg Ver; Galstyan, Aram			Invariant Representations without Adversarial Training	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Representations of data that are invariant to changes in specified factors are useful for a wide range of problems: removing potential biases in prediction problems, controlling the effects of covariates, and disentangling meaningful factors of variation. Unfortunately, learning representations that exhibit invariance to arbitrary nuisance factors yet remain useful for other tasks is challenging. Existing approaches cast the trade-off between task performance and invariance in an adversarial way, using an iterative minimax optimization. We show that adversarial training is unnecessary and sometimes counter-productive; we instead cast invariant representation learning as a single information-theoretic objective that can be directly optimized. We demonstrate that this approach matches or exceeds performance of state-of-the-art adversarial approaches for learning fair representations and for generative modeling with controllable transformations.	[Moyer, Daniel; Gao, Shuyang; Brekelmans, Rob; Steeg, Greg Ver; Galstyan, Aram] Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA	University of Southern California	Moyer, D (corresponding author), Univ Southern Calif, Informat Sci Inst, Los Angeles, CA 90089 USA.	moyerd@usc.edu; gaos@usc.edu; brekelma@usc.edu; gregv@isi.edu; galstyan@isi.edu	Thompson, Paul M/C-4194-2018	Thompson, Paul M/0000-0002-4720-8867; Galstyan, Aram/0000-0003-4215-0886; Ver Steeg, Greg/0000-0002-0793-141X	DARPA [W911NF-16-1-0575, FA8750-17-C-0106]; NSF Graduate Research Fellowship Program [DGE-1418060]	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF Graduate Research Fellowship Program(National Science Foundation (NSF))	This work was supported by DARPA grants W911NF-16-1-0575 and FA8750-17-C-0106, as well as the NSF Graduate Research Fellowship Program Grant Number DGE-1418060. We would like to thank the conference organizers, area chairs, and especially the anonymous reviewers for their work and helpful input. We also would like to thank Ayush Jaiswal for several insightful conversations, and Ishaan Gulrajani for finding and correcting a bug in our evaluation code.	Achille A., 2017, ARXIV170601350; Achille Alessandro, 2018, IEEE T PATTERN ANAL; Cohen TS, 2016, PR MACH LEARN RES, V48; Feis RA, 2015, FRONT NEUROSCI-SWITZ, V9, DOI 10.3389/fnins.2015.00395; Fortin JP, 2017, NEUROIMAGE, V161, P149, DOI 10.1016/j.neuroimage.2017.08.047; Freckleton RP, 2002, J ANIM ECOL, V71, P542, DOI 10.1046/j.1365-2656.2002.00618.x; FREEMAN WT, 1991, IEEE T PATTERN ANAL, V13, P891, DOI 10.1109/34.93808; Gallagher R. J., 2016, ARXIV161110277; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Greenspan  H., 1994, OVERCOMPLETE STEERAB; Kamiran F., 2009, COMPUTER CONTROL COM, P1, DOI 10.1109/IC4.2009.4909197; Kingma DP, 2 INT C LEARN REPR I, P1; Lample Guillaume, 2017, ARXIV170600409; Louizos C., 2015, ARXIV PREPRINT ARXIV; Lowe D. G., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1150, DOI 10.1109/ICCV.1999.790410; Raudenbush S. W., 1994, HDB RES SYNTHESIS, V421; Soatto Stefano, 2014, ARXIV14117676; Tishby  N., 2000, PHYSICS0004057; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Xie Q., 2017, PROC NEURAL INF PROC, P585; Zemel R., 2013, P INT C MACH LEARN, P325	22	40	40	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003062
C	Ramakrishnan, S; Agrawal, A; Lee, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ramakrishnan, Sainandan; Agrawal, Aishwarya; Lee, Stefan			Overcoming Language Priors in Visual Question Answering with Adversarial Regularization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Modern Visual Question Answering (VQA) models have been shown to rely heavily on superficial correlations between question and answer words learned during training - e.g. overwhelmingly reporting the type of room as kitchen or the sport being played as tennis, irrespective of the image. Most alarmingly, this shortcoming is often not well reflected during evaluation because the same strong priors exist in test distributions; however, a VQA system that fails to ground questions in image content would likely perform poorly in real-world settings. In this work, we present a novel regularization scheme for VQA that reduces this effect. We introduce a question-only model that takes as input the question encoding from the VQA model and must leverage language biases in order to succeed. We then pose training as an adversarial game between the VQA model and this question-only adversary - discouraging the VQA model from capturing language biases in its question encoding. Further, we leverage this question-only model to estimate the increase in model confidence after considering the image, which we maximize explicitly to encourage visual grounding. Our approach is a model agnostic training procedure and simple to implement. We show empirically that it can improve performance significantly on a bias-sensitive split of the VQA dataset for multiple base models - achieving state-of-the-art on this task. Further, on standard VQA tasks, our approach shows significantly less drop in accuracy compared to existing bias-reducing VQA models.	[Ramakrishnan, Sainandan; Agrawal, Aishwarya; Lee, Stefan] Georgia Inst Technol, Atlanta, GA 30332 USA	University System of Georgia; Georgia Institute of Technology	Ramakrishnan, S (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	sainandancv@gatech.edu; aishwarya@gatech.edu; steflee@gatech.edu			NSF; AFRL; DARPA; Siemens; Google; Amazon; ONR YIPs; ONR [N00014-16-1-{2713,2793}]	NSF(National Science Foundation (NSF)); AFRL(United States Department of DefenseUS Air Force Research Laboratory); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Siemens(Siemens AG); Google(Google Incorporated); Amazon; ONR YIPs; ONR(Office of Naval Research)	This work was supported in part by NSF, AFRL, DARPA, Siemens, Google, Amazon, ONR YIPs and ONR Grants N00014-16-1-{2713,2793}. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of any sponsor.	Agrawal A, 2017, C VQA COMPOSITIONAL; Agrawal Aishwarya, 2016, ARXIV160607356; Agrawal Aishwarya, 2017, DONT JUST ASSUME LOO; Anderson P., 2017, BOTTOM UP TOP DOWN A; Andreas Jacob, 2015, NEURAL MODULE NETWOR; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S., 2015, INT C COMP VIS ICCV; Bigham, 2018, CVPR; Burns Kaylee, 2018, ECCV; Dai B., 2017, CVPR; Das A, 2017, IEEE INT C ELECTR TA; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gordon J., 2013, P 2013 WORKSH AUT KN, V13, P25, DOI DOI 10.1145/2509558.2509563; Goyal Y., 2016, MAKING VQA MATTER EL; Hu R, 2017, LEARNING REASON END; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Johnson Justin, 2017, INFERRING EXECUTING; Kafle K., 2017, ICCV; Lample G, 2017, ADV NEURAL INFORM PR; Louppe Gilles, 2017, ADV NEURAL INFORM PR, P982; Lu J., 2016, HIERARCHICAL QUESTIO; Mirza M., 2014, ARXIV PREPRINT ARXIV; Perez Ethan, 2017, LEARNING VISUAL REAS; Radford A., 2015, CVPR; Ren S., 2015, CORR ABS150601497; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E., 2017, CVPR; Yang Zichao, 2015, STACKED ATTENTION NE; Zhang H., 2017, ICCV; Zhang P, 2016, PROC CVPR IEEE, P5014, DOI 10.1109/CVPR.2016.542; Zhao J., 2017, MEN ALSO SHOPPING RE; Zhu Y., 2016, CVPR	32	40	40	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301052
C	Soltanayev, S; Chun, SY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Soltanayev, Shakarim; Chun, Se Young			Training deep learning based denoisers without ground truth data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SURE; NETWORK; PARAMETERS	Recently developed deep-learning-based denoisers often outperform state-of-the-art conventional denoisers, such as the BM3D. They are typically trained to minimize the mean squared error (MSE) between the output image of a deep neural network and a ground truth image. In deep learning based denoisers, it is important to use high quality noiseless ground truth data for high performance, but it is often challenging or even infeasible to obtain noiseless images in application areas such as hyperspectral remote sensing and medical imaging. In this article, we propose a method based on Stein's unbiased risk estimator (SURE) for training deep neural network denoisers only based on the use of noisy images. We demonstrate that our SURE-based method, without the use of ground truth data, is able to train deep neural network denoisers to yield performances close to those networks trained with ground truth, and to outperform the state-of-the-art denoiser BM3D. Further improvements were achieved when noisy test images were used for training of denoiser networks using our proposed SURE-based method. Code is available at https://github.com/Shakarim94/Net-SURE.	[Soltanayev, Shakarim; Chun, Se Young] UNIST, Dept Elect Engn, Ulsan, South Korea	Ulsan National Institute of Science & Technology (UNIST)	Soltanayev, S (corresponding author), UNIST, Dept Elect Engn, Ulsan, South Korea.	shakarim@unist.ac.kr; sychun@unist.ac.kr	Chun, Young/B-6653-2013	Chun, Young/0000-0001-8739-8960	Basic Science Research Program through the National Research Foundation of Korea(NRF) - Ministry of Education [NRF-2017R1D1A1B05035810]; Technology Innovation Program or Industrial Strategic Technology Development Program - Ministry of Trade, Industry & Energy (MOTIE, Korea) [10077533]	Basic Science Research Program through the National Research Foundation of Korea(NRF) - Ministry of Education; Technology Innovation Program or Industrial Strategic Technology Development Program - Ministry of Trade, Industry & Energy (MOTIE, Korea)	This work was supported partly by Basic Science Research Program through the National Research Foundation of Korea(NRF) funded by the Ministry of Education(NRF-2017R1D1A1B05035810) and partly by the Technology Innovation Program or Industrial Strategic Technology Development Program (10077533, Development of robotic manipulation algorithm for grasping/assembling with the machine learning using visual and tactile sensing information) funded by the Ministry of Trade, Industry & Energy (MOTIE, Korea).	Abadi M, 2016, PROCEEDINGS OF OSDI'16: 12TH USENIX SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION, P265; Blu T, 2007, IEEE T IMAGE PROCESS, V16, P2778, DOI 10.1109/TIP.2007.906002; Bottou Leon, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Buades A, 2005, MULTISCALE MODEL SIM, V4, P490, DOI 10.1137/040616024; Burger HC, 2012, PROC CVPR IEEE, P2392, DOI 10.1109/CVPR.2012.6247952; Chen H, 2017, IEEE T MED IMAGING, V36, P2524, DOI 10.1109/TMI.2017.2715284; Chen LC, 2015, CORR; Dabov K, 2007, IEEE T IMAGE PROCESS, V16, P2080, DOI 10.1109/TIP.2007.901238; Deledalle CA, 2014, SIAM J IMAGING SCI, V7, P2448, DOI 10.1137/140968045; Donoho DL, 1995, J AM STAT ASSOC, V90, P1200, DOI 10.1080/01621459.1995.10476626; Eldar YC, 2009, IEEE T SIGNAL PROCES, V57, P471, DOI 10.1109/TSP.2008.2008212; Gao RH, 2017, IEEE I CONF COMP VIS, P1095, DOI 10.1109/ICCV.2017.124; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Kang E, 2017, MED PHYS, V44, pe360, DOI 10.1002/mp.12344; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y., 2015, NAT METHODS, V521, P436, DOI [10.1038/nature14539, DOI 10.1038/nmeth.3707, DOI 10.1038/nature14539]; Lefkimmiatis S, 2017, PROC CVPR IEEE, P5882, DOI 10.1109/CVPR.2017.623; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mao XJ, 2016, ADV NEUR IN, V29; Nguyen MP, 2017, IEEE T IMAGE PROCESS, V26, P1637, DOI 10.1109/TIP.2017.2658941; Ramani S, 2008, IEEE T IMAGE PROCESS, V17, P1540, DOI 10.1109/TIP.2008.2001404; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Salmon J, 2010, IEEE SIGNAL PROC LET, V17, P269, DOI 10.1109/LSP.2009.2038954; STEIN CM, 1981, ANN STAT, V9, P1135, DOI 10.1214/aos/1176345632; Tibshirani R. J, 2016, ARXIV161209415; Ulyanov D, 2018, PROC CVPR IEEE, P9446, DOI 10.1109/CVPR.2018.00984; Van De Ville D, 2009, IEEE SIGNAL PROC LET, V16, P973, DOI 10.1109/LSP.2009.2027669; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang YQ, 2014, IEEE SIGNAL PROC LET, V21, P1150, DOI 10.1109/LSP.2014.2314613; Xie J., 2012, ADV NEURAL INFORM PR, P341, DOI DOI 10.1109/AGRO-GEOINFORMATICS.2012.6311605; Ye MC, 2015, IEEE T GEOSCI REMOTE, V53, P2621, DOI 10.1109/TGRS.2014.2363101; Zhang H, 2017, PROC CVPR IEEE, P2925, DOI 10.1109/CVPR.2017.312; Zhang K, 2017, IEEE T IMAGE PROCESS, V26, P3142, DOI 10.1109/TIP.2017.2662206; Zhang XP, 1998, IEEE SIGNAL PROC LET, V5, P265, DOI 10.1109/97.720560; Zhussip Magauiya, 2018, ARXIV180600961	38	40	40	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303027
C	Yao, LY; Li, S; Li, YL; Huai, MD; Gao, J; Zhang, AD		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yao, Liuyi; Li, Sheng; Li, Yaliang; Huai, Mengdi; Gao, Jing; Zhang, Aidong			Representation Learning for Treatment Effect Estimation from Observational Data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				PROPENSITY SCORE; CAUSAL; INFERENCE	Estimating individual treatment effect (ITE) is a challenging problem in causal inference, due to the missing counterfactuals and the selection bias. Existing ITE estimation methods mainly focus on balancing the distributions of control and treated groups, but ignore the local similarity information that provides meaningful constraints on the ITE estimation. In this paper, we propose a local similarity preserved individual treatment effect (SITE) estimation method based on deep representation learning. SITE preserves local similarity and balances data distributions simultaneously, by focusing on several hard samples in each mini-batch. Experimental results on synthetic and three real-world datasets demonstrate the advantages of the proposed SITE method, compared with the state-of-the-art ITE estimation methods.	[Yao, Liuyi; Huai, Mengdi; Gao, Jing; Zhang, Aidong] SUNY Buffalo, Buffalo, NY 14260 USA; [Li, Sheng] Univ Georgia, Athens, GA 30602 USA; [Li, Yaliang] Tencent Med AI Lab, Palo Alto, CA USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo; University System of Georgia; University of Georgia	Yao, LY (corresponding author), SUNY Buffalo, Buffalo, NY 14260 USA.	liuyiyao@buffalo.edu; sheng.li@uga.edu; yaliangli@tencent.com; mengdihu@buffalo.edu; jing@buffalo.edu; azhang@buffalo.edu			US National Science Foundation [NSF IIS-1747614, IIS-1218393, IIS-1514204]; NVIDIA Corporation	US National Science Foundation(National Science Foundation (NSF)); NVIDIA Corporation	This work was supported in part by the US National Science Foundation under grants NSF IIS-1747614, IIS-1218393 and IIS-1514204. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation. Also, we gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan Xp GPU used for this research.	Alaa A.M., 2017, ADV NEURAL INFORM PR, P3427; Almond D, 2005, Q J ECON, V120, P1031, DOI 10.1162/003355305774268228; Athey S., 2016, ARXIV161001271; Athey S, 2016, P NATL ACAD SCI USA, V113, P7353, DOI 10.1073/pnas.1510489113; Chang YL, 2017, AAAI CONF ARTIF INTE, P1770; Chernozhukov V, 2013, ECONOMETRICA, V81, P2205, DOI 10.3982/ECTA10582; Chipman HA, 2010, ANN APPL STAT, V4, P266, DOI 10.1214/09-AOAS285; Crump RK, 2008, REV ECON STAT, V90, P389, DOI 10.1162/rest.90.3.389; DAmour A., 2017, ARXIV171102582; Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Funk MJ, 2011, AM J EPIDEMIOL, V173, P761, DOI 10.1093/aje/kwq439; Glass TA, 2013, ANNU REV PUBL HEALTH, V34, P61, DOI 10.1146/annurev-publhealth-031811-124606; Hill JL, 2011, J COMPUT GRAPH STAT, V20, P217, DOI 10.1198/jcgs.2010.08162; Huang C., 2016, ADV NEURAL INFORM PR, P1262; Iacus SM, 2012, POLIT ANAL, V20, P1, DOI 10.1093/pan/mpr013; Imai K, 2014, J R STAT SOC B, V76, P243, DOI 10.1111/rssb.12027; Imbens GW, 2015, CAUSAL INFERENCE FOR STATISTICS, SOCIAL, AND BIOMEDICAL SCIENCES: AN INTRODUCTION, P1, DOI 10.1017/CBO9781139025751; Johansson FD, 2016, PR MACH LEARN RES, V48; Kingma D.P, P 3 INT C LEARNING R; Kuang K, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P265, DOI 10.1145/3097983.3098032; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Lee BK, 2010, STAT MED, V29, P337, DOI 10.1002/sim.3782; Li S., 2016, IJCAI, P3768; Li S.D., 2017, ADV NEURAL INFORM PR, P930; Louizos C., 2017, P 31 ANN C NEUR INF; Pearl J, 2009, CAUSALITY, DOI 10.1017/CBO9780511803161; ROSENBAUM PR, 1983, BIOMETRIKA, V70, P41, DOI 10.1093/biomet/70.1.41; RUBIN DB, 1974, J EDUC PSYCHOL, V66, P688, DOI 10.1037/h0037350; Shalit U, 2017, PR MACH LEARN RES, V70; Splawa-Neyman J, 1990, STAT SCI, V5, P465, DOI DOI 10.1214/ss/1177012031; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Wager Stefan, 2017, J AM STAT ASS; Wang PY, 2015, WSDM'15: PROCEEDINGS OF THE EIGHTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P67, DOI 10.1145/2684822.2685294; Wang Y., 2017, ADV NEURAL INFORM PR, P5824; Yoon Jaehong, 2018, 6 INT C LEARN REPR I; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150; Zhao S., 2017, P 10 INT C ED DATA M	37	40	40	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302063
C	Baluja, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Baluja, Shumeet			Hiding Images in Plain Sight: Deep Steganography	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				NEURAL-NETWORK	Steganography is the practice of concealing a secret message within another, ordinary, message. Commonly, steganography is used to unobtrusively hide a small message within the noisy regions of a larger image. In this study, we attempt to place a full size color image within another image of the same size. Deep neural networks are simultaneously trained to create the hiding and revealing processes and are designed to specifically work as a pair. The system is trained on images drawn randomly from the ImageNet database, and works well on natural images from a wide variety of sources. Beyond demonstrating the successful application of deep learning to hiding images, we carefully examine how the result is achieved and explore extensions. Unlike many popular steganographic methods that encode the secret message within the least significant bits of the carrier image, our approach compresses and distributes the secret image's representation across all of the available bits.	[Baluja, Shumeet] Google Inc, Google Res, Mountain View, CA 94043 USA	Google Incorporated	Baluja, S (corresponding author), Google Inc, Google Res, Mountain View, CA 94043 USA.	shumeet@google.com	Jeong, Yongwook/N-7413-2016					[Anonymous], P INT S EL IM SAN FR; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Boehm B, 2014, CORR; Brandao AS, 2016, IEEE LAT AM T, V14, P1361, DOI 10.1109/TLA.2016.7459621; Cardoso JF, 1998, P IEEE, V86, P2009, DOI 10.1109/5.720250; darknet.org.uk, 2014, STEG STEG TOOL DET S; Fridrich Jessica, 2011, Information Hiding. 13th International Conference, IH 2011. Revised Selected Papers, P85, DOI 10.1007/978-3-642-24178-9_7; Fridrich J., 2001, IEEE Multimedia, V8, P22, DOI 10.1109/93.959097; FRIDRICH J, 2002, ELECT IMAGING 2002, V4675, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hayes J, 2017, ARXIV170300371; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Husien S, 2015, NEURAL COMPUT APPL, V26, P111, DOI 10.1007/s00521-014-1702-1; Hyvarinen A, 2004, INDEPENDENT COMPONEN, V46; Jarusek R, 2015, ADV INTELL SYST, V378, P317, DOI 10.1007/978-3-319-19824-8_26; Kavitha V, 2004, LECT NOTES ARTIF INT, V3157, P429; Kessler G. C., 2015, OVERVIEW STEGANOGRAP; Kessler GC, 2011, ADV COMPUT, V83, P51, DOI 10.1016/B978-0-12-385510-7.00002-3; Kessler Gary C, 2014, FORENSIC SCI COMMUNI, V6; Khan Imran, 2010, 2010 International Conference on Emerging Trends in Robotics and Communication Technologies (INTERACT 2010), P46, DOI 10.1109/INTERACT.2010.5706192; Kingma D.P., 2015, INT C LEARN REPR, P1; Ozer H, 2003, PROC SPIE, V5020, P55, DOI 10.1117/12.477313; Parikka J., 2017, HIDDEN PLAIN SIGHT S; Pevny T, 2010, LECT NOTES COMPUT SC, V6387, P161, DOI 10.1007/978-3-642-16435-4_13; Pibre Lionel, 2015, ARXIV151104855; Qian Yinlong, 2015, SPIE IS T ELECT IMAG; Russakovsky O., 2014, CORR; Sheisi Hossein, 2012, INT J COMPUTER ELECT, V4, P458; Shen L, 2011, PROC CVPR IEEE, P697, DOI 10.1109/CVPR.2011.5995738; Tamimi Abdelfatah A, 2013, INT J ADV COMPUTER S, V4; Vincent P, 2010, J MACH LEARN RES, V11, P3371; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; WATSON AB, 1993, P SPIE; Yaghmaee F, 2010, EURASIP J ADV SIG PR, DOI 10.1155/2010/851920	34	40	40	7	20	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402012
C	Schulam, P; Saria, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Schulam, Peter; Saria, Suchi			Reliable Decision Support using Counterfactual Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CAUSAL; INFERENCE	Decision-makers are faced with the challenge of estimating what is likely to happen when they take an action. For instance, if I choose not to treat this patient, are they likely to die? Practitioners commonly use supervised learning algorithms to fit predictive models that help decision-makers reason about likely future outcomes, but we show that this approach is unreliable, and sometimes even dangerous. The key issue is that supervised learning algorithms are highly sensitive to the policy used to choose actions in the training data, which causes the model to capture relationships that do not generalize. We propose using a different learning objective that predicts counterfactuals instead of predicting outcomes under an existing action policy as in supervised learning. To support decision-making in temporal settings, we introduce the Counterfactual Gaussian Process (CGP) to predict the counterfactual future progression of continuous-time trajectories under sequences of future actions. We demonstrate the benefits of the CGP on two important decision-support tasks: risk prediction and "what if?" reasoning for individualized treatment planning.	[Schulam, Peter; Saria, Suchi] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21211 USA	Johns Hopkins University	Schulam, P (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21211 USA.	pschulam@cs.jhu.edu; ssaria@cs.jhu.edu	Jeong, Yongwook/N-7413-2016		DARPA YFA [D17AP00014]; NSF SCH [1418590]; NSF Graduate Research Fellowship	DARPA YFA; NSF SCH; NSF Graduate Research Fellowship(National Science Foundation (NSF))	We thank the anonymous reviewers for their insightful feedback. This work was supported by generous funding from DARPA YFA #D17AP00014 and NSF SCH #1418590. PS was also supported by an NSF Graduate Research Fellowship. We thank Katie Henry and Andong Zhan for help with the ICU data set. We also thank Miguel Hernan for pointing us to earlier work by James Robins on treatment-confounder feedback.	Alaa A. M., 2016, ICML WORKSH COMP FRA; Arjas E, 2004, SCAND J STAT, V31, P171, DOI 10.1111/j.1467-9469.2004.02-134.x; Bottou L, 2013, J MACH LEARN RES, V14, P3207; Brodersen KH, 2015, ANN APPL STAT, V9, P247, DOI 10.1214/14-AOAS788; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Cheng L., 2017, ARXIV170309112; Cunningham J, 2012, JMLR P, P255; Daley D., 2007, INTRO THEORY POINT P; Doroudi S, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Dudik Miroslav, 2011, PROC 28 INTERNAT C M; Dyagilev K, 2016, MACH LEARN, V102, P323, DOI 10.1007/s10994-015-5527-7; Gong MM, 2016, PR MACH LEARN RES, V48; HAWKES AG, 1971, BIOMETRIKA, V58, P83, DOI 10.1093/biomet/58.1.83; Jiang N, 2016, PR MACH LEARN RES, V48; Johansson FD, 2016, PR MACH LEARN RES, V48; Lehman LWH, 2015, IEEE J BIOMED HEALTH, V19, P1068, DOI 10.1109/JBHI.2014.2330827; Lok JJ, 2008, ANN STAT, V36, P1464, DOI 10.1214/009053607000000820; Morgan SL, 2015, ANAL METHOD SOC RES, P1; Murphy SA, 2003, J R STAT SOC B, V65, P331, DOI 10.1111/1467-9868.00389; Nahum-Shani I, 2012, PSYCHOL METHODS, V17, P478, DOI 10.1037/a0029373; Neyman J, 1923, STAT SCI, V5, P465, DOI DOI 10.1214/SS/1177012031; Neyman J., 1923, ROCZNIKI NAUK ROLNIC, V10, P1, DOI DOI 10.1214/SS/1177012031; Ng AY, 2006, SPRINGER TRAC ADV RO, V21, P363; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Paduraru C., 2012, WORKSH REINF LEARN, P89; Pearl J., 2009, CAUSALITY MODELS REA; Ribeiro Marco Tulio, 2016, P KDD, P97, DOI [10.18653/v1/n16-3020, DOI 10.1145/2939672.2939778]; ROBINS J, 1986, MATH MODELLING, V7, P1393, DOI 10.1016/0270-0255(86)90088-6; ROBINS J, 1992, BIOMETRIKA, V79, P321, DOI 10.1093/biomet/79.2.321; Robins J.M., 1997, LECT NOTES STAT, P69, DOI DOI 10.1007/978-1-4612-1842-5_4; Robins JM, 2009, CH CRC HANDB MOD STA, P553; Robins James M., 2000, STAT MODELS EPIDEMIO, P1, DOI DOI 10.1007/978-1-4612-1284-3_; RUBIN DB, 1978, ANN STAT, V6, P34, DOI 10.1214/aos/1176344064; Saeed M, 2011, CRIT CARE MED, V39, P952, DOI 10.1097/CCM.0b013e31820a92c6; Sch/lkopf, 2013, ORDINARY DIFFERENTIA; Scharfstein D, 2014, STAT BIOPHARM RES, V6, P338, DOI 10.1080/19466315.2014.966920; Schulam P., 2015, NIPS; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Sokol A, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-2891; Soleimani H, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Swaminathan A, 2015, WWW'15 COMPANION: PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB, P939, DOI 10.1145/2740908.2742564; Taubman SL, 2009, INT J EPIDEMIOL, V38, P1599, DOI 10.1093/ije/dyp192; TAYLOR JMG, 1994, J AM STAT ASSOC, V89, P727; Wiens J, 2016, J MACH LEARN RES, V17; Xu Y, 2016, MACH LEARN HEALTHC C, V56, P282	46	40	41	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401071
C	Wang, ZY; Merel, J; Reed, S; Wayne, G; de Freitas, N; Heess, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Ziyu; Merel, Josh; Reed, Scott; Wayne, Greg; de Freitas, Nando; Heess, Nicolas			Robust Imitation of Diverse Behaviors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep generative models have recently shown great promise in imitation learning for motor control. Given enough data, even supervised approaches can do one-shot imitation learning; however, they are vulnerable to cascading failures when the agent trajectory diverges from the demonstrations. Compared to purely supervised methods, Generative Adversarial Imitation Learning (GAIL) can learn more robust controllers from fewer demonstrations, but is inherently mode-seeking and more difficult to train. In this paper, we show how to combine the favourable aspects of these two approaches. The base of our model is a new type of variational autoencoder on demonstration trajectories that learns semantic policy embeddings. We show that these embeddings can be learned on a 9 DoF Jaco robot arm in reaching tasks, and then smoothly interpolated with a resulting smooth interpolation of reaching behavior. Leveraging these policy representations, we develop a new version of GAIL that (1) is much more robust than the purely-supervised controller, especially with few demonstrations, and (2) avoids mode collapse, capturing many diverse behaviors when GAIL on its own does not. We demonstrate our approach on learning diverse gaits from demonstration on a 2D biped and a 62 DoF 3D humanoid in the MuJoCo physics environment.	[Wang, Ziyu; Merel, Josh; Reed, Scott; Wayne, Greg; de Freitas, Nando; Heess, Nicolas] DeepMind, London, England		Wang, ZY (corresponding author), DeepMind, London, England.	ziyu@google.com; jsmerel@google.com; reedscot@google.com; gregwayne@google.com; nandodefreitas@google.com; heess@google.com	Jeong, Yongwook/N-7413-2016					Argall BD, 2009, ROBOT AUTON SYST, V57, P469, DOI 10.1016/j.robot.2008.10.024; Arjovsky M., 2017, ARXIV170107875; Baram N., 2016, ARXIV161202179; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Graves A., 2005, ARTIFICIAL NEURAL NE, P753; Hausman K., 2017, ARXIV170510479; Hjelm R Devon, 2017, ARXIV PREPRINT ARXIV; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hussein A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3054912; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kuefler A., 2017, ARXIV170106699; Li Yuxi, 2017, ARXIV170107274; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Merel Josh, 2017, ARXIV170702201; Mirza M., 2014, ARXIV; Muico U., 2009, SIGGRAPH; Oord A.V.D., 2016, SSW; Peng X. B., 2017, SIGGRAPH; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Qi Guo-Jun, 2017, ARXIV170106264; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Rosca Mihaela, 2017, ARXIV170604987; Ross S., 2010, AISTATS; Ross St<prime>ephane, 2011, AISTATS; Rusu A. A., 2015, ARXIV151106295; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schuster M, 1997, IEEE T SIGNAL PROCES, V45, P2673, DOI 10.1109/78.650093; Sharon D, 2005, IEEE INT CONF ROBOT, P2387, DOI 10.1109/robot.2005.1570470; Silver D, 2014, PR MACH LEARN RES, V32; Sok K. W., 2007, SIMULATING BIPED BEH; Stadie Bradly C, 2017, INT C LEARN REPR ICL; Theis Lucas, 2015, ARXIV151101844; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; van den Oord Aaron, 2016, ARXIV160605328; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang Ruohan, 2017, ARXIV170403817; Yin K., 2007, SIGGRAPH	42	40	40	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405039
C	Ba, J; Hinton, G; Mnih, V; Leibo, JZ; Ionescu, C		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Ba, Jimmy; Hinton, Geoffrey; Mnih, Volodymyr; Leibo, Joel Z.; Ionescu, Catalin			Using FastWeights to Attend to the Recent Past	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				NEURAL-NETWORKS	Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.	[Ba, Jimmy; Hinton, Geoffrey] Univ Toronto, Toronto, ON, Canada; [Hinton, Geoffrey] Google Brain, Mountain View, CA USA; [Mnih, Volodymyr; Leibo, Joel Z.; Ionescu, Catalin] Google DeepMind, London, England	University of Toronto; Google Incorporated; Google Incorporated	Ba, J (corresponding author), Univ Toronto, Toronto, ON, Canada.	jimmy@psi.toronto.edu; geoffhinton@google.com; vmnih@google.com; jzl@google.com; cdi@google.com		Leibo, Joel/0000-0002-3153-916X				Abbott LF, 2004, NATURE, V431, P796, DOI 10.1038/nature03010; Anderson James A, 1981, PARALLEL MODELS ASS, P9; Ba J., 2015, ICLR 2015 C TRACK P; Barak O, 2007, PLOS COMPUT BIOL, V3, P323, DOI 10.1371/journal.pcbi.0030035; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Danihelka I, 2016, PR MACH LEARN RES, V48; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Graves A., 2014, ARXIV14105401; Grefenstette E, 2015, ADV NEUR IN, V28; Gross R, 2010, IMAGE VISION COMPUT, V28, P807, DOI 10.1016/j.imavis.2009.08.002; Hinton, 2016, ARXIV PREPRINT ARXIV; Hinton G.E., 1987, P 9 ANN C COGNITIVE, P177; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; HOPFIELD JJ, 1982, P NATL ACAD SCI-BIOL, V79, P2554, DOI 10.1073/pnas.79.8.2554; Kingma D.P, P 3 INT C LEARNING R; KOHONEN T, 1972, IEEE T COMPUT, VC 21, P353, DOI 10.1109/TC.1972.5008975; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Mnih V, 2016, PR MACH LEARN RES, V48; Schmidhuber J., 1993, INT C ART NEUR NETW, P460; Tsodyks M, 1998, NEURAL COMPUT, V10, P821, DOI 10.1162/089976698300017502; Weston J., 2014, ARXIV14103916; Willshaw David J, 1969, NATURE; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zucker RS, 2002, ANNU REV PHYSIOL, V64, P355, DOI 10.1146/annurev.physiol.64.092501.114547	26	40	41	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700021
C	Kondor, R; Pan, H		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Kondor, Risi; Pan, Horace			The Multiscale Laplacian Graph Kernel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Many real world graphs, such as the graphs of molecules, exhibit structure at multiple different scales, but most existing kernels between graphs are either purely local or purely global in character. In contrast, by building a hierarchy of nested subgraphs, the Multiscale Laplacian Graph kernels (MLG kernels) that we define in this paper can account for structure at a range of different scales. At the heart of the MLG construction is another new graph kernel, called the Feature Space Laplacian Graph kernel (FLG kernel), which has the property that it can lift a base kernel defined on the vertices of two graphs to a kernel between the graphs. The MLG kernel applies such FLG kernels to subgraphs recursively. To make the MLG kernel computationally feasible, we also introduce a randomized projection procedure, similar to the Nystrom method, but for RKHS operators.	[Kondor, Risi] Univ Chicago, Dept Comp Sci, Dept Stat, Chicago, IL 60637 USA; [Pan, Horace] Univ Chicago, Dept Comp Sci, Chicago, IL 60637 USA	University of Chicago; University of Chicago	Kondor, R (corresponding author), Univ Chicago, Dept Comp Sci, Dept Stat, Chicago, IL 60637 USA.	risi@cs.uchicago.edu; hopan@uchicago.edu			University of Chicago Research Computing Center [DARPA-D16AP00112, NSF-1320344]	University of Chicago Research Computing Center	This work was completed in part with computing resources provided by the University of Chicago Research Computing Center and with the support of DARPA-D16AP00112 and NSF-1320344.	Alexa Marc, 2009, PROC EUR S GEOM PROC, V28; Borgwardt K. M., 2005, P INT SYST MOL BIOL; Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Dai HJ, 2016, PR MACH LEARN RES, V48; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Feragen A., 2013, NIPS; Gartner T., 2002, NIPS 02 WORKSH UNR D; Inokuchi A, 2003, MACH LEARN, V50, P321, DOI 10.1023/A:1021726221443; JEBARA T, 2003, P ANN C COMP LEARN T; Johansson FD, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P467, DOI 10.1145/2783258.2783341; Kondor R., 2008, P 25 INT C MACH LEAR, P496, DOI DOI 10.1145/1390156.1390219; Kondor R, 2003, P INT C MACH LEARN I; Kubinyi H, 2003, NAT REV DRUG DISCOV, V2, P665, DOI 10.1038/nrd1156; Mika S, 1999, ADV NEUR IN, V11, P536; Neumann Marion, 2016, MACHINE LEARNING; Scholkopf B., 2002, LEARNING KERNELS; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Toivonen H, 2003, BIOINFORMATICS, V19, P1183, DOI 10.1093/bioinformatics/btg130; Vishwanathan S. V. N., 2010, J MACHINE LEARNING R, V11; Wale N, 2008, KNOWL INF SYST, V14, P347, DOI 10.1007/s10115-007-0103-5; Williams CKI, 2001, ADV NEUR IN, V13, P682	24	40	40	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700099
C	Zhong, MJ; Goddard, N; Sutton, C		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Zhong, Mingjun; Goddard, Nigel; Sutton, Charles			Signal Aggregate Constraints in Additive Factorial HMMs, with Application to Energy Disaggregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Blind source separation problems are difficult because they are inherently unidentifiable, yet the entire goal is to identify meaningful sources. We introduce a way of incorporating domain knowledge into this problem, called signal aggregate constraints (SACs). SACs encourage the total signal for each of the unknown sources to be close to a specified value. This is based on the observation that the total signal often varies widely across the unknown sources, and we often have a good idea of what total values to expect. We incorporate SACs into an additive factorial hidden Markov model (AFHMM) to formulate the energy disaggregation problems where only one mixture signal is assumed to be observed. A convex quadratic program for approximate inference is employed for recovering those source signals. On a real-world energy disaggregation data set, we show that the use of SACs dramatically improves the original AFHMM, and significantly improves over a recent state-of-the-art approach.	[Zhong, Mingjun; Goddard, Nigel; Sutton, Charles] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland	University of Edinburgh	Zhong, MJ (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	mzhong@inf.ed.ac.uk; nigel.goddard@inf.ed.ac.uk; csutton@inf.ed.ac.uk		Zhong, Mingjun/0000-0002-1525-1270	Engineering and Physical Sciences Research Council [EP/K002732/1]	Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	This work is supported by the Engineering and Physical Sciences Research Council (grant number EP/K002732/1).	Asif HMS, 2011, BIOINFORMATICS, V27, P1277, DOI 10.1093/bioinformatics/btr113; Bach F.R., 2005, ADV NEURAL INFORM PR, V17, P65; Comon P, 2010, HANDBOOK OF BLIND SOURCE SEPARATION: INDEPENDENT COMPONENT ANALYSIS AND APPLICATIONS, P1; Ganchev K, 2010, J MACH LEARN RES, V11, P2001; Ghosh S., 2012, P 26 AAAI C ART INT, V1, P356, DOI 10.5555/2900728.2900780; Grant MC, 2008, LECT NOTES CONTR INF, V371, P95, DOI 10.1007/978-1-84800-155-8_7; HART GW, 1992, P IEEE, V80, P1870, DOI 10.1109/5.192069; Hastie T, 2009, ELEMENTS STAT LEARNI; Johnson MJ, 2013, J MACH LEARN RES, V14, P673; Kim H., 2011, P SIAM C DAT MIN MES, DOI [10.1137/1.9781611972818.64, 10.1137/1, DOI 10.1137/1]; Kolter J.Z., 2012, P ARTIFICIAL INTELLI, P1472; Liang Percy, 2009, P 26 ANN INT C MACH, P641; Mann G., 2008, P ACL, P870; Parson O., 2014, THESIS; Roweis ST, 2001, ADV NEUR IN, V13, P793; Saul LK, 1999, MACH LEARN, V37, P75, DOI 10.1023/A:1007649326333; Titsias M. K., 2013, ARXIV13111189; Yang M., 2011, SIGMOD, P817; Zeifman M, 2011, IEEE T CONSUM ELECTR, V57, P76, DOI 10.1109/TCE.2011.5735484; Zhong M., 2013, NEUR INF PROC SYST W; Zimmermann JP, 2012, HOUSEHOLD ELECT SURV; Zoha A, 2012, SENSORS-BASEL, V12, P16838, DOI 10.3390/s121216838	23	40	41	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102087
C	Poupart, P; Boutiller, C		Thrun, S; Saul, K; Scholkopf, B		Poupart, P; Boutiller, C			Bounded finite state controllers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA					We describe a new approximation algorithm for solving partially observable MDPs. Our bounded policy iteration approach searches through the space of bounded-size, stochastic finite state controllers, combining several advantages of gradient ascent (efficiency, search through restricted controller space) and policy iteration (less vulnerability to local optima).	Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada	University of Toronto	Poupart, P (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON M5S 3H5, Canada.	ppoupart@cs.toronto.edu; cebly@cs.toronto.edu						Aberdeen D., 2002, P 19 INT C MACH LEAR, P3; Boutilier C, 1996, PROCEEDINGS OF THE THIRTEENTH NATIONAL CONFERENCE ON ARTIFICIAL INTELLIGENCE AND THE EIGHTH INNOVATIVE APPLICATIONS OF ARTIFICIAL INTELLIGENCE CONFERENCE, VOLS 1 AND 2, P1168; BRAZIUNAS D, 2003, THESIS U TORONTO TOR; Cassandra A., 1997, P 13 ANN C UNC ART I, P54; Cheng H.-T., 1988, THESIS U BRIT COLUMB; FENG Z, 2001, P ECP 01 TOL SPAIN; HANSEN EA, 1998, P 14 C UNC ART INT, P211; Hauskrecht M, 2000, J ARTIF INTELL RES, V13, P33, DOI 10.1613/jair.678; Kaelbling LP, 1998, ARTIF INTELL, V101, P99, DOI 10.1016/S0004-3702(98)00023-X; Meuleau N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P427; Meuleau N, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P417; PINEAU J, 2003, P IJCAI 03 AC MEX; POUPART P, 2002, P NIPS 02 VANC CAN, P1547; Zhang NL, 2001, J ARTIF INTELL RES, V14, P29, DOI 10.1613/jair.761	14	40	40	2	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						823	830						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500103
C	Torresani, L; Hertzmann, A; Bregler, C		Thrun, S; Saul, K; Scholkopf, B		Torresani, L; Hertzmann, A; Bregler, C			Learning non-rigid 3D shape from 2D motion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				MISSING DATA	This paper presents an algorithm for learning the time-varying shape of a non-rigid 3D object from uncalibrated 2D tracking data. We model shape motion as a rigid component (rotation and translation) combined with a non-rigid deformation. Reconstruction is ill-posed if arbitrary deformations are allowed. We constrain the problem by assuming that the object shape at each time instant is drawn from a Gaussian distribution. Based on this assumption, the algorithm simultaneously estimates 3D shape and motion for each time frame, learns the parameters of the Gaussian, and robustly fills-in missing data points. We then extend the algorithm to model temporal smoothness in object shape, thus allowing it to handle severe cases of missing data.	Stanford Univ, Stanford, CA 94305 USA	Stanford University	Torresani, L (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	ltorresa@cs.stanford.edu; hertzman@dgp.toronto.edu; chris.bregler@nyu.edu		Hertzmann, Aaron/0000-0001-9667-0292				Blake A., 1998, ACTIVE CONTOURS, DOI [10.1007/978-1-4471-1555-7, DOI 10.1007/978-1-4471-1555-7]; Blanz V, 1999, COMP GRAPH, P187, DOI 10.1145/311535.311556; BRAND M, 2001, P CVPR 2001; BREGLER A, 2000, P CVPR 2000; Cootes T. E, 2001, P SPIE MED IMAGING; Ghahramani Zoubin, 1996, CRGTR961 U TOR; GRUBER A, 2003, P NIPS 2003; Jacobs DW, 2001, COMPUT VIS IMAGE UND, V82, P57, DOI 10.1006/cviu.2001.0906; SHUM HY, 1995, IEEE T PATTERN ANAL, V17, P854, DOI 10.1109/34.406651; Shumway R. H., 1982, Journal of Time Series Analysis, V3, P253, DOI 10.1111/j.1467-9892.1982.tb00349.x; SOATTO S, 2002, P ECCV 2002 MAY; TOMASI C, 1992, INT J COMPUT VISION, V9, P137, DOI 10.1007/BF00129684; TORRESANI L, 2001, P CVPR	13	40	40	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1555	1562						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500193
C	Smith, N; Gales, M		Dietterich, TG; Becker, S; Ghahramani, Z		Smith, N; Gales, M			Speech recognition using SVMs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					An important issue in applying SVMs to speech recognition is the ability to classify variable length sequences. This paper presents extensions to a standard scheme for handling this variable length data, the Fisher score. A more useful mapping is introduced based on the likelihood-ratio. The score-space defined by this mapping avoids some limitations of the Fisher score. Class-conditional generative models are directly incorporated into the definition of the score-space. The mapping, and appropriate normalisation schemes, are evaluated on a speaker-independent isolated letter task where the new mapping outperforms both the Fisher score and HMMs trained to maximise likelihood.	Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England	University of Cambridge	Smith, N (corresponding author), Univ Cambridge, Dept Engn, Cambridge CB2 1PZ, England.							FANTY M, 1991, NEURAL INFORMATION P, V3, P220; FINE S, 2001, P INT C AC SPEECH SI, V1; JAAKOLA TS, 1999, ADV NEURAL INFORMATI, V11; JOACHIMS T, 1999, ADV KERNAL METHODS S; Loizou PC, 1996, IEEE T SPEECH AUDI P, V4, P430, DOI 10.1109/89.544528; Oliver N., 2000, ADV LARGE MARGIN CLA; SMITH N, 2001, CUEDFINFENTTR387; SMITH N, 2001, CUEDFINFENGTR412; TSUDA K, 2002, ADV NEURAL INFORMATI, V14; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	10	40	41	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1197	1204						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100149
C	Gordon, GJ		Leen, TK; Dietterich, TG; Tresp, V		Gordon, GJ			Reinforcement learning with function approximation converges to a region	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					Many algorithms for approximate reinforcement learning are not known to converge. In fact, there are counterexamples showing that the adjustable weights in some algorithms may oscillate within a region rather than converging to a point. This paper shows that, for two popular algorithms, such oscillation is the worst that can happen: the weights cannot diverge, but instead must converge to a bounded region. The algorithms are SARSA(0) and V(0); the latter algorithm was used in the well-known TD-Gammon program.				ggordon@cs.cmu.edu						BAIRD LC, 1995, MACH LEARN P 12 INT; de Farias D.P., 2000, J OPTIMIZATION THEOR, V105; Gordon G., 1996, CHATTERING SARSA LAM; Gordon G., 1995, CMUCS95103; Kemeny J. G., 1983, FINITE MARKOV CHAINS; POLYAK BT, 1973, AUTOMAT REM CONTR+, V34, P377; RUMMERY GA, 1994, 166 CAMBR U ENG DEP; Sutton R. S., 1988, Machine Learning, V3, P9, DOI 10.1023/A:1022633531479; Sutton-Tyrrell K, 1999, CIRCULATION, V99, P1105; TESAURO G, 1994, NEURAL COMPUT, V6, P215, DOI 10.1162/neco.1994.6.2.215	11	40	40	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1040	1046						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800146
C	Mangasarian, OL; Musicant, DR		Leen, TK; Dietterich, TG; Tresp, V		Mangasarian, OL; Musicant, DR			Active support vector machine classification	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					An active set strategy is applied to the dual of a simple reformulation of the standard quadratic program of a linear support vector machine. This application generates a fast new dual algorithm that consists of solving a finite number of linear equations, with a typically large dimensionality equal to the number of points to be classified. However, by making novel use of the Sherman-Morrison-Woodbury formula, a much smaller matrix of the order of the original input space is inverted at each step. Thus, a problem with a 32-dimensional input space and 7 million points required inverting positive definite symmetric matrices of size 33 x 33 with a total running time of 96 minutes on a 400 MHz Pentium II. The algorithm requires no specialized quadratic or linear programming code, but merely a linear equation solver which is publicly available.	Univ Wisconsin, Dept Comp Sci, Madison, WI 53706 USA	University of Wisconsin System; University of Wisconsin Madison	Mangasarian, OL (corresponding author), Univ Wisconsin, Dept Comp Sci, 1210 W Dayton St, Madison, WI 53706 USA.	olvi@cs.wise.edu; dmusican@carleton.edu						Anderson E, 1995, LAPACK USERS GUIDE; Bertsekas D. P., 1999, NONLINEAR PROGRAM, V2nd; Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; Burges CJC, 1997, ADV NEUR IN, V9, P375; CHERKASSKY V, 1988, LEARNING DATA CONCEP; Cherkassky V. S., 1998, LEARNING DATA CONCEP, V1st; *CLAPACK, F2CED VERS LAPACK; Cristianini N., 2000, INTRO SUPPORT VECTOR; FERRIS MC, 2000, 0005 U WISC COMP SCI; Golub Gene H., 2013, MATRIX COMPUTATION, V3; *ILOG, 1999, CPLEX 6 5 REF MAN; JOACHIMS T, 1998, SVMLIGHT; LEE YJ, 2000, COMPUTATIONAL OPTIMI; Mangasarian O.L., 1994, NONLINEAR PROGRAMMIN; Mangasarian OL, 1999, IEEE T NEURAL NETWOR, V10, P1032, DOI 10.1109/72.788643; Mangasarian OL, 2000, ADV NEUR IN, P135; MANGASARIAN OL, 1994, MATH PROGRAM, V66, P241, DOI 10.1007/BF01581148; *MATLAB, 1992, US GUID; MORE JJ, 1989, NUMER MATH, V55, P377, DOI 10.1007/BF01396045; Murphy P.M., 1992, UCI REPOSITORY MACHI; Musicant D. R., 1998, NDC NORMALLY DISTRIB; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Scholkopf B., 1998, ADV KERNEL METHODS S; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	24	40	44	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						577	583						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800082
C	Zhang, W; Dietterich, TG		Touretzky, DS; Mozer, MC; Hasselmo, ME		Zhang, W; Dietterich, TG			High-performance job-shop scheduling with a time-delay TD(lambda) network	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	Advances in Neural Information Processing Systems		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						OREGON STATE UNIV, DEPT COMP SCI, CORVALLIS, OR 97331 USA	Oregon State University									0	40	42	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1024	1030						7	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00144
C	Hendrycks, D; Mazeika, M; Kadavath, S; Song, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hendrycks, Dan; Mazeika, Mantas; Kadavath, Saurav; Song, Dawn			Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Self-supervision provides effective representations for downstream tasks without requiring labels. However, existing approaches lag behind fully supervised training and are often not thought beneficial beyond obviating or reducing the need for annotations. We find that self-supervision can benefit robustness in a variety of ways, including robustness to adversarial examples, label corruption, and common input corruptions. Additionally, self-supervision greatly benefits out-of-distribution detection on difficult, near-distribution outliers, so much so that it exceeds the performance of fully supervised methods. These results demonstrate the promise of self-supervision for improving robustness and uncertainty estimation and establish these tasks as new axes of evaluation for future self-supervised learning research.	[Hendrycks, Dan; Kadavath, Saurav; Song, Dawn] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Mazeika, Mantas] UIUC, Champaign, IL USA	University of California System; University of California Berkeley; University of Illinois System; University of Illinois Urbana-Champaign	Hendrycks, D (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	hendrycks@berkeley.edu; mantas3@illinois.edu; sauravkadavath@berkeley.edu; dawnsong@berkeley.edu			National Science Foundation Frontier Grant	National Science Foundation Frontier Grant	This material is in part based upon work supported by the National Science Foundation Frontier Grant. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	Athalye A., 2018, P 35 INT C MACH LEAR; Behrmann Jens, 2018, ABS181100995 ARXIV; Carlini N., 2017, CORR ABS170507263; Charikar M., 2017, STOC; Dan H., 2019, INT C LEARN REPR; Davis J., 2006, INT C MACH LEARN; Deng J., 2009, CVPR; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Dosovitskiy A, 2016, IEEE T PATTERN ANAL, V38, P1734, DOI 10.1109/TPAMI.2015.2496141; Gidaris S., 2018, INT C LEARN REPR; Golan I., 2018, CORR; Henaff O.J., 2019, DATA EFFICIENT IMAGE; Hendrycks Dan, 2018, NEURIPS; Hendrycks Dan, 2019, P INT C MACH LEARN; Hendrycks Dan, 2019, ICLR; Hendrycks Dan, 2017, ICLR; Hjelm R.D., 2019, INT C LEARN REPR; Ji X., 2018, CORR; Kurakin A., 2017, ICLR; Larsson G, 2016, LECT NOTES COMPUT SC, V9908, P577, DOI 10.1007/978-3-319-46493-0_35; Lee Kimin, 2018, ICLR; Lin T., 2017, ICCV; Loshchilov Ilya, 2016, ICLR; Madry Aleksander, 2018, ICLR; Nettleton David F, 2010, ARTIF INTELL REV; Patrini G., 2017, CVPR; Ruff L, 2018, PR MACH LEARN RES, V80; Schmidt L., 2018, NEURIPS; Smola AJ, 1999, ADV NEUR IN, V11, P585; Srivastava N., 2014, J MACHINE LEARNING R; Sukhbaatar Sainbayar, 2014, ICLR WORKSH; Torralba A, 2008, IEEE T PATTERN ANAL, V30, P1958, DOI 10.1109/TPAMI.2008.128; van den Oord Aaron, 2018, ARXIV180205666; van den Oord Aaron, 2018, NEURIPS; Vondrick C, 2016, PROC CVPR IEEE, P98, DOI 10.1109/CVPR.2016.18; Vondrick Carl, 2018, EUR C COMP VIS ECCV; Woo S., 2018, EUR C COMP VIS ECCV; Xie Cihang, 2018, FEATURE DENOISING IM; Zagoruyko Sergey, 2016, BMVC; Zhai Xiaohua, 2019, S SELF SUPERVISED SE; Zhang H., 2019, ARXIV190108573; Zhang Z., 2018, ADV NEURAL INFORM PR, P8778	42	39	39	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907033
C	Javed, K; White, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Javed, Khurram; White, Martha			Meta-Learning Representations for Continual Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					A continual learning agent should be able to build on top of existing knowledge to learn on new data quickly while minimizing forgetting. Current intelligent systems based on neural network function approximators arguably do the opposite-they are highly prone to forgetting and rarely trained to facilitate future learning. One reason for this poor behavior is that they learn from a representation that is not explicitly trained for these two goals. In this paper, we propose OML, an objective that directly minimizes catastrophic interference by learning representations that accelerate future learning and are robust to forgetting under online updates in continual learning. We show that it is possible to learn naturally sparse representations that are more effective for online updating. Moreover, our algorithm is complementary to existing continual learning strategies, such as MER and GEM. Finally, we demonstrate that a basic online updating strategy on representations learned by OML is competitive with rehearsal based methods for continual learning. (1)	[Javed, Khurram; White, Martha] Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 1P8, Canada	University of Alberta	Javed, K (corresponding author), Univ Alberta, Dept Comp Sci, Edmonton, AB T6G 1P8, Canada.	kjaved@ualberta.ca; whitem@ualberta.ca	White, Martha/AAF-7066-2020	White, Martha/0000-0002-5356-2950				Al-Shedivat M., 2018, INT C LEARN REPR, P1; Aljundi R., 2019, ICLR; Aljundi R, 2018, LECT NOTES COMPUT SC, V11207, P144, DOI 10.1007/978-3-030-01219-9_9; Aljundi Rahaf, 2019, ADV NEURAL INFORM PR, P11849; Bengio Yoshua, 2019, ARXIV190110912; Chaudhry Arslan, 2019, CONTINUAL LEARNING T; Chaudhry Arslan, 2019, P INT C LEARN REPR I, P2; Finn C, 2017, PR MACH LEARN RES, V70; Finn Chelsea, 2018, THESIS, P1; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; French Robert M, 1991, ANN COGN SCI SOC C E; Kingma D.P, P 3 INT C LEARNING R; Kirkpatrick J., 2017, NATL ACAD SCI; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Lee SW, 2017, ADV NEUR IN, V30; Li Zhenguo, 2017, METASGD LEARNING LEA; Li Zhizhong, 2018, IEEE T PATTERN ANAL; Lin Long-Ji, 1992, MACHINE LEARNING; Liu V, 2019, AAAI CONF ARTIF INTE, P4384; Liu Xialei, 2018, INT C PATT REC, P3; Lopez-Paz D, 2017, ADV NEUR IN, V30; Metz Luke, 2019, INT C LEARN REPR; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Riemer Matthew, 2019, INT C LEARN REPR, V1; Schmidhuber J, 1987, THESIS; Shin H, 2017, ADV NEUR IN, V30; Sutton R. S., 1990, Machine Learning: Proceedings of the Seventh International Conference (1990), P216; Zenke F, 2017, PR MACH LEARN RES, V70	30	39	39	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301077
C	Singh, A; Joachims, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Singh, Ashudeep; Joachims, Thorsten			Policy Learning for Fairness in Ranking	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Conventional Learning-to-Rank (LTR) methods optimize the utility of the rankings to the users, but they are oblivious to their impact on the ranked items. However, there has been a growing understanding that the latter is important to consider for a wide range of ranking applications (e.g. online marketplaces, job placement, admissions). To address this need, we propose a general LTR framework that can optimize a wide range of utility metrics (e.g. NDCG) while satisfying fairness of exposure constraints with respect to the items. This framework expands the class of learnable ranking functions to stochastic ranking policies, which provides a language for rigorously expressing fairness specifications. Furthermore, we provide a new LTR algorithm called FAIR-PG-RANK for directly searching the space of fair ranking policies via a policy-gradient approach. Beyond the theoretical evidence in deriving the framework and the algorithm, we provide empirical results on simulated and real-world datasets verifying the effectiveness of the approach in individual and group-fairness settings.	[Singh, Ashudeep; Joachims, Thorsten] Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA	Cornell University	Singh, A (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14850 USA.	ashudeep@cs.cornell.edu; tj@cs.cornell.edu			NSF [IIS-1615706, IIS-1513692, IIS-1901168]	NSF(National Science Foundation (NSF))	This work was supported in part by a gift from Workday Inc., as well as NSF awards IIS-1615706, IIS-1513692, and IIS-1901168. We thank Jessica Hong for the interesting discussions that informed the direction of this paper. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.	Agarwal Aman, 2019, INT C WEB SEARCH DAT; [Anonymous], ARXIV171209752; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Biega AJ, 2018, ACM/SIGIR PROCEEDINGS 2018, P405, DOI 10.1145/3209978.3210063; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Cao Z, 2007, LECT NOTES COMPUT SC, V4464, P129; Celis L Elisa, 2017, ARXIV170406840; Chapelle D, 2011, COMPUT FLUID SOLID M, P1, DOI 10.1007/978-3-642-16408-8; Chris Burges T.S., 2005, P 22 INT MACH LEARN, DOI 10.1145/1102351.1102363; Dheeru D., 2019, UCI MACHINE LEARNING; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Edelman B, 2017, AM ECON J-APPL ECON, V9, P1, DOI 10.1257/app.20160213; Fang Zhichong, 2019, ACM C RES DEV INF RE; Grimmelmann James, 2011, NEXT DIGITAL DECADE, P435; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Introna LD, 2000, INFORM SOC, V16, P169, DOI 10.1080/01972240050133634; Joachims T, 2006, PROC 22 ACM SIGKDD I, P217, DOI DOI 10.1145/1150402.1150429; Joachims T, 2007, ACM T INFORM SYST, V25, DOI 10.1145/1229179.1229181; Joachims T, 2017, WSDM'17: PROCEEDINGS OF THE TENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P781, DOI 10.1145/3018661.3018699; Joachims T, 2009, MACH LEARN, V77, P27, DOI [10.1007/S10994-009-5108-8, 10.1007/s10994-009-5108-8]; Kay Matthew, 2015, CHI; Ke Yang, 2017, SSDBM; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Megahed M, 2017, CIKM; Mehrotra R, 2018, CIKM'18: PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON INFORMATION AND KNOWLEDGE MANAGEMENT, P2243, DOI 10.1145/3269206.3272027; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; Plackett R. L., 1975, Applied Statistics, V24, P193, DOI 10.2307/2346567; ROBERTSON SE, 1977, J DOC, V33, P294, DOI 10.1108/eb026647; Scott M, 2017, NY TIMES; Singh A, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P2219, DOI 10.1145/3219819.3220088; Sutton R. S., 1998, INTRO REINFORCEMENT, V135; Taylor M., 2008, P 2008 INT C WEB SEA, P77, DOI DOI 10.1145/1341531.1341544; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Ye J., 2009, P 18 ACM C INFORM KN, P2061, DOI [10.1145/1645953.1646301, DOI 10.1145/1645953.1646301]; Zehlike Meike., 2018, ARXIV PREPRINT ARXIV; Zemel R., 2013, P INT C MACH LEARN, P325; Zhai CX, 2003, P 26 ANN INT ACM SIG, P1017, DOI [DOI 10.1145/860435.860440, 10.1145/860435.860440]; Zliobaite Indre, 2015, FATML WORKSH ICML	41	39	40	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305042
C	Yang, GS; Ramanan, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yang, Gengshan; Ramanan, Deva			Volumetric Correspondence Networks for Optical Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Many classic tasks in vision - such as the estimation of optical flow or stereo disparities - can be cast as dense correspondence matching. Well-known techniques for doing so make use of a cost volume, typically a 4D tensor of match costs between all pixels in a 2D image and their potential matches in a 2D search window. State-of-the-art (SOTA) deep networks for flow/stereo make use of such volumetric representations as internal layers. However, such layers require significant amounts of memory and compute, making them cumbersome to use in practice. As a result, SOTA networks also employ various heuristics designed to limit volumetric processing, leading to limited accuracy and overfitting. Instead, we introduce several simple modifications that dramatically simplify the use of volumetric layers - (1) volumetric encoder-decoder architectures that efficiently capture large receptive fields, (2) multi-channel cost volumes that capture multi-dimensional notions of pixel similarities, and finally, (3) separable volumetric filtering that significantly reduces computation and parameters while preserving accuracy. Our innovations dramatically improve accuracy over SOTA on standard benchmarks while being significantly easier to work with - training converges in 7X fewer iterations, and most importantly, our networks generalize across correspondence tasks. On-the-fly adaptation of search windows allows us to repurpose optical flow networks for stereo (and vice versa), and can also be used to implement adaptive networks that increase search window sizes on-demand.	[Yang, Gengshan; Ramanan, Deva] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Ramanan, Deva] Argo AI, Pittsburgh, PA USA	Carnegie Mellon University	Yang, GS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	gengshay@cs.cmu.edu; deva@cs.cmu.edu			CMU Argo AI Center for Autonomous Vehicle Research	CMU Argo AI Center for Autonomous Vehicle Research	This work was supported by the CMU Argo AI Center for Autonomous Vehicle Research.	[Anonymous], 2017, IEEE T PATTERN ANAL, DOI DOI 10.1109/TPAMI.2016.2644615; Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Campbell NDF, 2008, LECT NOTES COMPUT SC, V5302, P766, DOI 10.1007/978-3-540-88682-2_58; Chang JR, 2018, PROC CVPR IEEE, P5410, DOI 10.1109/CVPR.2018.00567; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Geiger A., 2012, CVPR; Guo X, 2019, ABS190210859 ARXIV; Hirschmuller H, 2008, IEEE T PATTERN ANAL, V30, P328, DOI [10.1109/TPAMI.2007.1166, 10.1109/TPAMl.2007.1166]; Hirschmuller H, 2009, PROC CVPR IEEE, P437, DOI 10.1109/CVPRW.2009.5206493; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Howard A. G., 2017, MOBILENETS EFFICIENT; Hu XY, 2012, IEEE T PATTERN ANAL, V34, P2121, DOI 10.1109/TPAMI.2012.46; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Hui TW, 2018, PROC CVPR IEEE, P8981, DOI 10.1109/CVPR.2018.00936; Hui Tak-Wai, 2019, ARXIV190307414; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M., 2014, P BRIT MACH VIS C; Junhwa H., 2019, CVPR; Kendall A, 2017, IEEE I CONF COMP VIS, P66, DOI 10.1109/ICCV.2017.17; Kolmogorov Vladimir, 2001, TECHNICAL REPORT; Kong Shu, 2019, ARXIV190401693; Lebedev V., 2015, 3 INT C LEARNING REP; LeCun Y, 1995, HDB BRAIN THEORY NEU, V3361, P1995, DOI DOI 10.1007/S13398-014-0173-7.2; Luo W., 2016, CVPR; Mayer N., 2016, CVPR; Menze Moritz, 2015, CVPR; Rocco I., 2018, P ADV NEURAL INF PRO, P1651; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Scharstein D, 2001, IEEE WORKSHOP ON STEREO AND MULTI-BASELINE VISION, PROCEEDINGS, P131, DOI 10.1023/A:1014573219977; Scharstein D., 2014, GERM C PATT REC; SIfre L., 2014, ARXIV14031687; Sun D., 2019, PAMI; Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x; Sun Deqing, 2018, CVPR; Tulyakov S, 2018, ADV NEUR IN, V31; Urtasun R., 2010, ACCV; Wang F, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P1041, DOI 10.1145/3123266.3123359; Wang X., 2017, ARXIV170206890; Xu J., 2017, P C COMP VIS PATT RE, P1289; Yin Z., 2019, CVPR; Zbontar J., 2016, J MACH LEARN RES, V17, P2; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716	46	39	40	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300072
C	Hendrycks, D; Mazeika, M; Wilson, D; Gimpel, K		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hendrycks, Dan; Mazeika, Mantas; Wilson, Duncan; Gimpel, Kevin			Using Trusted Data to Train Deep Networks on Labels Corrupted by Severe Noise	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					The growing importance of massive datasets used for deep learning makes robustness to label noise a critical property for classifiers to have. Sources of label noise include automatic labeling, non-expert labeling, and label corruption by data poisoning adversaries. Numerous previous works assume that no source of labels can be trusted. We relax this assumption and assume that a small subset of the training data is trusted. This enables substantial label corruption robustness performance gains. In addition, particularly severe label noise can be combated by using a set of trusted data with clean labels. We utilize trusted data by proposing a loss correction technique that utilizes trusted examples in a data-efficient manner to mitigate the effects of label noise on deep neural network classifiers. Across vision and natural language processing tasks, we experiment with various label noises at several strengths, and show that our method significantly outperforms existing methods.	[Hendrycks, Dan] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Mazeika, Mantas] Univ Chicago, Chicago, IL 60637 USA; [Wilson, Duncan] Foundat Res Inst, Basel, Switzerland; [Gimpel, Kevin] Toyota Technol Inst, Chicago, IL USA	University of California System; University of California Berkeley; University of Chicago; Toyota Technological Institute - Chicago	Hendrycks, D (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	hendrycks@berkeley.edu; mantas@ttic.edu; duncanw@nevada.unr.edu; kgimpel@ttic.edu						[Anonymous], 2013, P C EMP METH NAT LAN; [Anonymous], 2014, ICLR; Biggio Battista, 2011, ACML; Bo Li, 2016, NIPS; Charikar M., 2017, STOC; Gimpel K., 2011, ACL; Guo C., 2017, ICML; Hendrycks D., 2017, ICLR; Hendrycks  Dan, 2016, 160608415 ARXIV; Larsen  J, 1998, AC SPEECH SIGN PROC; Li Y., 2017, ICCV; Loshchilov I., 2016, ICLR; Maas A L, 2011, P 49 ANN M ASS COMP, V1, P142; Menon Aditya Krishna, 2016, CORR; Mnih V., 2012, ICML; Natarajan N., 2013, ADV NEURAL INFORM PR, V26; Nettleton David F, 2010, ARTIF INTELL REV; Patrini G., 2016, CVPR; Pechenizkiy  M, 2006, 19 IEEE S COMP BAS M; Reed  Scott, 2014, ICLR WORKSH; Ren Thu Mengye, 2018, ICML; Srivastava N., 2014, J MACHINE LEARNING R; Steinhardt J., 2017, NIPS; Sukhbaatar Sainbayar, 2014, ICLR WORKSH; Veit A., 2017, CVPR; Verleysen Michel, 2014, IEEE T NEURAL NETW L; Xiao Tong, 2015, CVPR; Xie S., 2016, CVPR; Zagoruyko S., 2016, BMVC; Zhu  Xingquan, 2004, ARTIFICIAL INTELLIGE	30	39	39	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005007
C	Liu, SJ; Kailkhura, B; Chen, PY; Ting, PS; Chang, SY; Amini, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Sijia; Kailkhura, Bhavya; Chen, Pin-Yu; Ting, Paishun; Chang, Shiyu; Amini, Lisa			Zeroth-Order Stochastic Variance Reduction for Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					As application demands for zeroth-order (gradient-free) optimization accelerate, the need for variance reduced and faster converging approaches is also intensifying. This paper addresses these challenges by presenting: a) a comprehensive theoretical analysis of variance reduced zeroth-order (ZO) optimization, b) a novel variance reduced ZO algorithm, called ZO-SVRG, and c) an experimental evaluation of our approach in the context of two compelling applications, black-box chemical material classification and generation of adversarial examples from black-box deep neural network models. Our theoretical analysis uncovers an essential difficulty in the analysis of ZO-SVRG: the unbiased assumption on gradient estimates no longer holds. We prove that compared to its first-order counterpart, ZO-SVRG with a two-point random gradient estimator could suffer an additional error of order O(1/b), where b is the mini-batch size. To mitigate this error, we propose two accelerated versions of ZO-SVRG utilizing variance reduced gradient estimators, which achieve the best rate known for ZO stochastic optimization (in terms of iterations). Our extensive experimental results show that our approaches outperform other state-of-the-art ZO algorithms, and strike a balance between the convergence rate and the function query complexity.	[Liu, Sijia; Chen, Pin-Yu; Chang, Shiyu; Amini, Lisa] MIT, IBM AI Watson Lab, IBM Res, Cambridge, MA 02139 USA; [Kailkhura, Bhavya] Lawrence Livermore Natl Lab, Livermore, CA USA; [Ting, Paishun] Univ Michigan, Ann Arbor, MI 48109 USA	International Business Machines (IBM); Massachusetts Institute of Technology (MIT); United States Department of Energy (DOE); Lawrence Livermore National Laboratory; University of Michigan System; University of Michigan	Liu, SJ (corresponding author), MIT, IBM AI Watson Lab, IBM Res, Cambridge, MA 02139 USA.		Chen, Pin-Yu/AAA-1059-2020		MIT-IBM Watson AI Lab; U.S. Department of Energy by Lawrence Livermore National Laboratory [DE-AC52-07NA27344 (LLNL-CONF-751658)]	MIT-IBM Watson AI Lab(International Business Machines (IBM)); U.S. Department of Energy by Lawrence Livermore National Laboratory(United States Department of Energy (DOE))	This work was fully supported by the MIT-IBM Watson AI Lab. Bhavya Kailkhura was supported under the auspices of the U.S. Department of Energy by Lawrence Livermore National Laboratory under Contract DE-AC52-07NA27344 (LLNL-CONF-751658). The authors are also grateful to the anonymous reviewers for their helpful comments,	Agarwal A., 2010, P COLT, P28; Allen-Zhu ZY, 2016, PR MACH LEARN RES, V48; Brent R.P., 2013, ALGORITHMS MINIMIZAT; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chatterji N., 2018, ARXIV180205431; Chen P.-Y., 2017, P AISEC CCS 2017, P15, DOI DOI 10.1145/3128572.3140448; Chen T., 2017, ARXIV170709060; Choromanski K. M., 2017, ADV NEURAL INFORM PR, P6524; Duchi JC, 2015, IEEE T INFORM THEORY, V61, P2788, DOI 10.1109/TIT.2015.2409256; Dvurechensky P., 2018, ARXIV180209022; Flaxman AD, 2005, PROCEEDINGS OF THE SIXTEENTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P385; Fu MC, 2002, INFORMS J COMPUT, V14, P192, DOI 10.1287/ijoc.14.3.192.113; Gao  X., 2014, OPTIMIZATION ONLINE, V12; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; Grathwohl W., 2017, ARXIV171100123; Gu B., 2016, ARXIV161201425; Hajinezhad D., 2017, ARXIV171009997; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kirklin S, 2015, NPJ COMPUT MATER, V1, DOI 10.1038/npjcompumats.2015.10; Kresse G, 1996, COMP MATER SCI, V6, P15, DOI 10.1016/0927-0256(96)00008-0; Lei LH, 2017, ADV NEUR IN, V30; Lian Xiangru, 2016, P C NEUR INF PROC SY, P3054; LIU S, 2018, P 21 INT C ART INT S, V84, P288; Madry A., 2018, ARXIV PREPRINT ARXIV; Nesterov Y., 2015, FUNDATIONS COMPUTATI, V2, P527; Nitanda A, 2016, JMLR WORKSH CONF PRO, V51, P195; Reddi SJ, 2016, PR MACH LEARN RES, V48; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Shamir O., 2013, P C LEARN THEOR, P3; Shamir O, 2017, J MACH LEARN RES, V18; Spall J.C, 2005, INTRO STOCHASTIC SEA, V65; Tucker G., 2017, ADV NEURAL INFORM PR, P2624; Wang H., 2014, ARXIV14070107; Wang Y., 2018, INT C ART INT STAT A, P1356; Ward L, 2016, NPJ COMPUT MATER, V2, DOI 10.1038/npjcompumats.2016.28; Xu  Peng, 2017, ARXIV170807827; Yang W., 2003, COMPUTATIONAL MED CH, P103	38	39	39	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303070
C	Gan, Z; Chen, LQ; Wang, WY; Pu, YC; Zhang, YZ; Liu, H; Li, CY; Carin, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gan, Zhe; Chen, Liqun; Wang, Weiyao; Pu, Yunchen; Zhang, Yizhe; Liu, Hao; Li, Chunyuan; Carin, Lawrence			Triangle Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					A Triangle Generative Adversarial Network (Delta-GAN) is developed for semi-supervised cross-domain joint distribution matching, where the training data consists of samples from each domain, and supervision of domain correspondence is provided by only a few paired samples. Delta-GAN consists of four neural networks, two generators and two discriminators. The generators are designed to learn the two-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs. The generators and discriminators are trained together using adversarial learning. Under mild assumptions, in theory the joint distributions characterized by the two generators concentrate to the data distribution. In experiments, three different kinds of domain pairs are considered, image-label, image-image and image-attribute pairs. Experiments on semi-supervised image classification, image-to-image translation and attribute-based image generation demonstrate the superiority of the proposed approach.	[Gan, Zhe; Chen, Liqun; Wang, Weiyao; Pu, Yunchen; Zhang, Yizhe; Liu, Hao; Li, Chunyuan; Carin, Lawrence] Duke Univ, Durham, NC 27706 USA	Duke University	Gan, Z (corresponding author), Duke Univ, Durham, NC 27706 USA.	zhe.gan@duke.edu	Jeong, Yongwook/N-7413-2016; Li, Chunyuan/AAG-1303-2020	Carin, Lawrence/0000-0001-6277-7948	ARO; DARPA; DOE; NGA; ONR	ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); DOE(United States Department of Energy (DOE)); NGA; ONR(Office of Naval Research)	This research was supported in part by ARO, DARPA, DOE, NGA and ONR.	[Anonymous], 2017, ICLR; [Anonymous], 2016, ICML; [Anonymous], 2017, ICCV; Arjovsky M., 2017, P 2017 INT C LEARN R, P1; Bottou L., 2017, ARXIV170107875STATML; Darrell T, 2017, ICLR; Denton E. L., 2015, NIPS; Dumoulin Vincent, 2017, LEARNED REPRESENTATI; Gan Zhe, 2017, CVPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Isola P., 2017, CVPR; Kim T, 2017, ICML; Krizhevsky A., 2009, CITESEER; LeCun Yann, 1998, P IEEE; Ledig C., 2017, CVPR; Li C., 2017, NIPS; Lin T.-Y., 2014, EUR C COMP VIS, P740, DOI 10.1007/978-3-319-10602-1_48; Liu M.Y., 2016, NIPS; Liu M. -Y., 2017, NIPS; Liu Z., 2015, ICCV; Metz L., 2017, ICLR; Mirza M., 2014, CONDITIONAL GENERATI; Perarnau G., 2016, NIPS WORKSH; Pu Y., 2016, NIPS; Pu Y., 2017, NIPS; Radford A., 2016, ICLR; Salimans Tim, 2016, ADV NEURAL INFORM PR; Springenberg Jost Tobias, 2015, ARXIV151106390; Taigman Y., 2017, ICLR; Xia Y., 2017, ICML; Yi Z., 2017, ICCV; Yu L., 2017, AAAI; Zhang H., 2017, ICCV; Zhang Y., 2016, NIPS WORKSH ADV TRAI; Zhang Y., 2017, ICML	35	39	39	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405032
C	Hoffer, E; Hubara, I; Soudry, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hoffer, Elad; Hubara, Itay; Soudry, Daniel			Train longer, generalize better: closing the generalization gap in large batch training of neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ANOMALOUS DIFFUSION; MEDIA	Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem. Contributions: We examine the initial high learning rate training phase. We find that the weight distance from its initialization grows logarithmically with the number of weight updates. We therefore propose a "random walk on a random landscape" statistical model which is known to exhibit similar "ultra-slow" diffusion behavior. Following this hypothesis we conducted experiments to show empirically that the "generalization gap" stems from the relatively small number of updates rather than the batch size, and can be completely eliminated by adapting the training regime used. We further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "Ghost Batch Normalization" which enables significant decrease in the generalization gap without increasing the number of updates. To validate our findings we conduct several additional experiments on MNIST, CIFAR-10, CIFAR-100 and ImageNet. Finally, we reassess common practices and beliefs concerning training of deep models and suggest they may not be optimal to achieve good generalization.	[Hoffer, Elad; Hubara, Itay; Soudry, Daniel] Technion Israel Inst Technol, Haifa, Israel	Technion Israel Institute of Technology	Hoffer, E (corresponding author), Technion Israel Inst Technol, Haifa, Israel.	elad.hoffer@gmail.com; itayhubara@gmail.com; daniel.soudry@gmail.com	Jeong, Yongwook/N-7413-2016		Taub Foundation; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) [D16PC00003]	Taub Foundation; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC)	We wish to thank Nir Ailon, Dar Gilboa, Kfir Levy and Igor Berman for their feedback on the initial manuscript. The research was partially supported by the Taub Foundation, and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/ Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of IARPA, DoI/IBC, or the U.S. Government.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ICLR; Bottou L, 1998, LECT NOTES COMPUTER, V1524; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; BOUCHAUD JP, 1987, J PHYS-PARIS, V48, P1445, DOI 10.1051/jphys:019870048090144500; BOUCHAUD JP, 1990, PHYS REP, V195, P127, DOI 10.1016/0370-1573(90)90099-N; Bray A. J., 2007, PHYS REV LETT, V98, P1; Catanzaro B., 2015, ARXIV151202595; Choromanska A., 2015, AISTATS15, V38; Das D., 2016, ARXIV160206709; Dauphin Y., 2014, IDENTIFYING ATTACKIN, P1; Dauphin Y.N., 2015, ABS150204390 CORR; Dean J., 2012, NIPS 12, V1, P1223; Dinh L., 2017, ARXIV170304933; Duchi J, 2011, J MACH LEARN RES, V12, P2121; DURRETT R, 1986, COMMUN MATH PHYS, V104, P87, DOI 10.1007/BF01210794; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ioffe S, 2015, PR MACH LEARN RES, V37, P448; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A., 2014, ARXIV; Kyrola A., 2017, ABS170602677 ARXIV; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li K., 2009, CVPR09; Li Mu, 2017, THESIS, P5; Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612; Luong M-T, 2015, ARXIV150804025; MARINARI E, 1983, PHYS REV LETT, V50, P1223, DOI 10.1103/PhysRevLett.50.1223; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Montavon G., 2012, NEURAL NETWORKS TRIC; Recht B, 2016, ICML, P1; Ruder S., 2016, ARXIV; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Soudry D, 2017, ARXIV E PRINTS; Soudry Daniel, 2017, ARXIV170205777; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Wan L., 2013, ICML 13; You Y., 2017, LARGE BATCH TRAINING; Zagoruyko K., 2016, BMVC; Zhang C., 2017, ICLR; Zhang S., 2015, NEURAL INFORM PROCES, P685	46	39	39	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401074
C	Karkus, P; Hsu, D; Lee, WS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Karkus, Peter; Hsu, David; Lee, Wee Sun			QMDP-Net: Deep Learning for Planning under Partial Observability	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				VALUE-ITERATION	This paper introduces the QMDP-net, a neural network architecture for planning under partial observability. The QMDP-net combines the strengths of model-free learning and model-based planning. It is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture. The QMDP-net is fully differentiable and allows for end-to-end training. We train a QMDP-net on different tasks so that it can generalize to new ones in the parameterized task set and "transfer" to other similar tasks beyond the set. In preliminary experiments, QMDP-net showed strong performance on several robotic tasks in simulation. Interestingly, while QMDP-net encodes the QMDP algorithm, it sometimes outperforms the QMDP algorithm in the experiments, as a result of end-to-end learning.	[Karkus, Peter; Hsu, David] Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore, Singapore; [Karkus, Peter; Hsu, David; Lee, Wee Sun] Natl Univ Singapore, Sch Comp, Singapore, Singapore	National University of Singapore; National University of Singapore	Karkus, P (corresponding author), Natl Univ Singapore, NUS Grad Sch Integrat Sci & Engn, Singapore, Singapore.; Karkus, P (corresponding author), Natl Univ Singapore, Sch Comp, Singapore, Singapore.	karkus@comp.nus.edu.sg; dyhsu@comp.nus.edu.sg; leews@comp.nus.edu.sg	Jeong, Yongwook/N-7413-2016; Karkus, Peter/AAJ-9778-2021		Singapore Ministry of Education AcRF grant [MOE2016-T2-2-068]; National University of Singapore AcRF grant [R-252-000-587-112]	Singapore Ministry of Education AcRF grant(Ministry of Education, Singapore); National University of Singapore AcRF grant(National University of Singapore)	We thank Leslie Kaelbling and Tomas Lozano-Perez for insightful discussions that helped to improve our understanding of the problem. The work is supported in part by Singapore Ministry of Education AcRF grant MOE2016-T2-2-068 and National University of Singapore AcRF grant R-252-000-587-112.	Abadi M, 2015, P 12 USENIX S OPERAT; Bagnell J. A., 2003, ADV NEURAL INFORM PR, P831; Bai HY, 2010, SPRINGER TRAC ADV RO, V68, P175; Bakker B, 2003, IROS 2003: PROCEEDINGS OF THE 2003 IEEE/RSJ INTERNATIONAL CONFERENCE ON INTELLIGENT ROBOTS AND SYSTEMS, VOLS 1-4, P430; Bengio Y., 2014, ARXIV14061078; Boots B, 2011, INT J ROBOT RES, V30, P954, DOI 10.1177/0278364911404092; Gupta S, 2017, PROC CVPR IEEE, P7272, DOI 10.1109/CVPR.2017.769; Haarnoja T, 2016, ADV NEUR IN, V29; Hanna Kurniawati., 2008, ROBOTICS SCI SYSTEMS, DOI [10.15607/RSS.2008.IV.009, DOI 10.15607/RSS.2008.IV.009]; Hausknecht Matthew, 2015, 2015 AAAI FALL S SER; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Howard A., 2003, ROBOTICS DATA SET RE; Hsiao K, 2007, IEEE INT CONF ROBOT, P4685, DOI 10.1109/ROBOT.2007.364201; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jonschkowski R., 2016, WORKSH DEEP LEARN AC; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Littman M. L., 1995, Machine Learning. Proceedings of the Twelfth International Conference on Machine Learning, P362; Mirowski P., 2016, ARXIV PREPRINT ARXIV, DOI DOI 10.1016/j.neuroscience.2018.04.006; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Ng AY, 1999, MACHINE LEARNING, PROCEEDINGS, P278; Okada M., 2017, ARXIV170609597; PAPADIMITRIOU CH, 1987, MATH OPER RES, V12, P441, DOI 10.1287/moor.12.3.441; Pineau J., 2003, ADV NEURAL INFORM PR; Shani G, 2005, LECT NOTES ARTIF INT, V3720, P353, DOI 10.1007/11564096_35; Shani G, 2013, AUTON AGENT MULTI-AG, V27, P1, DOI 10.1007/s10458-012-9200-2; Shankar T, 2016, INT C PATT RECOG, P2592, DOI 10.1109/ICPR.2016.7900026; Silver D., 2010, NIPS, P2164; Silver D., 2016, PREDICTRON END TO EN; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Spaan MTJ, 2005, J ARTIF INTELL RES, V24, P195, DOI 10.1613/jair.1659; Stachniss C., ROBOTICS 2D LASER DA; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Ye N, 2017, J ARTIF INTELL RES, V58, P231, DOI 10.1613/jair.5328	37	39	39	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404074
C	Rudi, A; Carratino, L; Rosasco, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Rudi, Alessandro; Carratino, Luigi; Rosasco, Lorenzo			FALKON: An Optimal Large Scale Kernel Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ALGORITHMS	Kernel methods provide a principled way to perform non linear, nonparametric learning. They rely on solid functional analytic foundations and enjoy optimal statistical properties. However, at least in their basic form, they have limited applicability in large scale scenarios because of stringent computational requirements in terms of time and especially memory. In this paper, we take a substantial step in scaling up kernel methods, proposing FALKON, a novel algorithm that allows to efficiently process millions of points. FALKON is derived combining several algorithmic principles, namely stochastic subsampling, iterative solvers and preconditioning. Our theoretical analysis shows that optimal statistical accuracy is achieved requiring essentially O(n) memory and O (n root n) time. An extensive experimental analysis on large scale datasets shows that, even with a single machine, FALKON outperforms previous state of the art solutions, which exploit parallel/distributed architectures.	[Rudi, Alessandro] Ecole Normale Super, INRIA, Sierra Project Team, Paris, France; [Carratino, Luigi; Rosasco, Lorenzo] Univ Genoa, Genoa, Italy; [Rudi, Alessandro; Rosasco, Lorenzo] IIT, LCSL, Genoa, Italy; [Rosasco, Lorenzo] MIT, Cambridge, MA 02139 USA	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS); University of Genoa; Istituto Italiano di Tecnologia - IIT; Massachusetts Institute of Technology (MIT)	Rudi, A (corresponding author), Ecole Normale Super, INRIA, Sierra Project Team, Paris, France.	alessandro.rudi@inria.fr	Jeong, Yongwook/N-7413-2016; Rudi, Alessandro/J-7323-2013	Carratino, Luigi/0000-0001-9947-9944; Rudi, Alessandro/0000-0002-3879-7794	Air Force project [FA9550-17-1-0390]; FIRB project [RBFR12M3AC]	Air Force project; FIRB project	The authors would like to thank Mikhail Belkin, Benjamin Recht and Siyuan Ma, Eric Fosler-Lussier, Shivaram Venkataraman, Stephen L. Tu, for providing their features of the TIMIT and YELP datasets, and NVIDIA Corporation for the donation of the Tesla K40c GPU used for this research. This work is funded by the Air Force project FA9550-17-1-0390 (European Office of Aerospace Research and Development) and by the FIRB project RBFR12M3AC (Italian Ministry of Education, University and Research).	Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Alves Alexandre, 2016, ABS161207725 CORR; [Anonymous], 2008, INFORM SCI STAT; Avron Haim, 2016, ARXIV161103220; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Bach Francis, 2013, C LEARNING THEORY, P185; Baldi P, 2014, NAT COMMUN, V5, DOI 10.1038/ncomms5308; Bertin-Mahieux Thierry, 2011, P 12 INT C MUS INF R, DOI DOI 10.7916/D8NZ8J07; Boucheron Stephane, 2004, ADV LECT MACHINE LEA; Camoriano R, 2016, JMLR WORKSH CONF PRO, V51, P1403; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Caponnetto A, 2010, ANAL APPL, V8, P161, DOI 10.1142/S0219530510001564; Chen Jie, 2016, ABS160800860 CORR; Cohen MB, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P181, DOI 10.1145/2688073.2688113; Cutajar K, 2016, PR MACH LEARN RES, V48; Dai B., 2014, NIPS; DIEULEVEUT A., 2014, ARXIV14080361; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Fasshauer GE, 2012, SIAM J SCI COMPUT, V34, pA737, DOI 10.1137/110824784; Gonen Alon, 2016, ARXIV160202350; Lo Gerfo L, 2008, NEURAL COMPUT, V20, P1873, DOI 10.1162/neco.2008.05-07-517; Ma SY, 2017, ADV NEUR IN, V30; May Avner, 2017, ABS170103577 CORR; Peters J, 2017, ADAPT COMPUT MACH LE; Po-Sen Huang, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P205, DOI 10.1109/ICASSP.2014.6853587; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Reed M., 1980, METHODS MODERN MATH, V1; Rudi A, 2015, ADV NEUR IN, V28; Rudi Alessandro, 2016, ARXIV160204474; Rudi Alessandro, 2013, ADV NEURAL INFORM PR, P2067; Saad Y., 2003, ITERATIVE METHODS SP, DOI [10.1137/1.9780898718003, DOI 10.1137/1.9780898718003]; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Tu Stephen, 2016, ARXIV160205310; Yang Y, 2017, ANN STAT, V45, P991, DOI 10.1214/16-AOS1472; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Zhang Yuchen, 2013, C LEARN THEOR, P592617	43	39	39	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403092
C	Bialek, W		Leen, TK; Dietterich, TG; Tresp, V		Bialek, W			Stability and noise in biochemical switches	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				MOLECULAR SWITCH; PROTEIN-KINASE; SIMULATION; MECHANISM; NETWORKS; DYNAMICS; STORAGE; SCALE	Many processes in biology, from the regulation of gene expression in bacteria to memory in the brain, involve switches constructed from networks of biochemical reactions. Crucial molecules are present in small numbers, raising questions about noise and stability. Analysis of noise in simple reaction schemes indicates that switches stable for years and switchable in milliseconds can be built from fewer than one hundred molecules. Prospects for direct tests of this prediction, as well as implications, are discussed.	NEC Res Inst, Princeton, NJ 08540 USA	NEC Corporation	Bialek, W (corresponding author), NEC Res Inst, 4 Independence Way, Princeton, NJ 08540 USA.							Baras F, 1996, J CHEM PHYS, V105, P8257, DOI 10.1063/1.472679; Barkai N, 1997, NATURE, V387, P913, DOI 10.1038/43199; Bhalla US, 1999, SCIENCE, V283, P381, DOI 10.1126/science.283.5400.381; COLEMAN S, 1975, ASPECTS SYMMETRY; DYKMAN MI, 1994, J CHEM PHYS, V100, P5735, DOI 10.1063/1.467139; Giannoni M, 1989, CHAOS QUANTUM PHYS; Ha TJ, 1999, P NATL ACAD SCI USA, V96, P893, DOI 10.1073/pnas.96.3.893; JOHNSON AD, 1981, NATURE, V294, P217, DOI 10.1038/294217a0; KENNEDY MB, 1994, ANNU REV BIOCHEM, V63, P571, DOI 10.1146/annurev.biochem.63.1.571; Lawrence Peter A, 1993, MAKING FLY GENETICS, V6; LISMAN JE, 1985, P NATL ACAD SCI USA, V82, P3055, DOI 10.1073/pnas.82.9.3055; LISMAN JE, 1988, P NATL ACAD SCI USA, V85, P5320, DOI 10.1073/pnas.85.14.5320; Lu HP, 1998, SCIENCE, V282, P1877, DOI 10.1126/science.282.5395.1877; McAdams HH, 1999, TRENDS GENET, V15, P65, DOI 10.1016/S0168-9525(98)01659-X; McAdams HH, 1998, ANNU REV BIOPH BIOM, V27, P199, DOI 10.1146/annurev.biophys.27.1.199; MILLER SG, 1986, CELL, V44, P861, DOI 10.1016/0092-8674(86)90008-5; MURRAY AW, 1992, NATURE, V359, P599, DOI 10.1038/359599a0; Ptashne M, 1992, GENETIC SWITCH PHAGE; Rieke F, 1998, REV MOD PHYS, V70, P1027, DOI 10.1103/RevModPhys.70.1027; Schrodinger E, 1944, WHAT IS LIFE; Shivashankar GV, 2000, APPL PHYS LETT, V76, P3638, DOI 10.1063/1.126732; Slack JMW, 1983, EGG EMBRYO DETERMINA	22	39	39	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						103	109						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800015
C	Chen, SSB; Gopinath, RA		Leen, TK; Dietterich, TG; Tresp, V		Chen, SSB; Gopinath, RA			Gaussianization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				PROJECTION PURSUIT	High dimensional data modeling is difficult mainly because the so-called "curse of dimensionality". We propose a technique called "Gaussianization" for high dimensional density estimation, which alleviates the curse of dimensionality by exploiting the independence structures in the data. Gaussianization is motivated from recent developments in the statistics literature: projection pursuit, independent component analysis and Gaussian mixture models with semi-tied covariances. We propose an iterative Gaussianization procedure which converges weakly: at each iteration, the data is first transformed to the least dependent coordinates and then each coordinate is marginally Gaussianized by univariate techniques. Gaussianization offers density estimation sharper than traditional kernel methods and radial basis function methods. Gaussianization can be viewed as efficient solution of nonlinear independent component analysis and high dimensional projection pursuit.	Renaissance Technol, E Setauket, NY 11733 USA		Chen, SSB (corresponding author), Renaissance Technol, E Setauket, NY 11733 USA.							Attias H, 1999, NEURAL COMPUT, V11, P803, DOI 10.1162/089976699300016458; FRIEDMAN JH, 1987, J AM STAT ASSOC, V82, P249, DOI 10.2307/2289161; Gales MJF, 1999, IEEE T SPEECH AUDI P, V7, P272, DOI 10.1109/89.759034; HUBER PJ, 1985, ANN STAT, V13, P435, DOI 10.1214/aos/1176349519; HWANG JN, 1994, IEEE T SIGNAL PROCES, V42, P2795, DOI 10.1109/78.324744; SCHWARZ G, 1978, ANN STAT, V6, P461, DOI 10.1214/aos/1176344136	6	39	41	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						423	429						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800060
C	Karakoulas, G; Shawe-Taylor, J		Kearns, MS; Solla, SA; Cohn, DA		Karakoulas, G; Shawe-Taylor, J			Optimizing classifiers for imbalanced training sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO			Computational Learning Theory; Generalization; fat-shattering; large margin; pac estimates; unequal loss; imbalanced datasets		Following recent results [9, 8] showing the importance of the fat-shattering dimension in explaining the beneficial effect of a large margin on generalization performance, the current paper investigates the implications of these results for the case of imbalanced datasets and develops two approaches to setting the threshold. The approaches are incorporated into ThetaBoost, a boosting algorithm for dealing with unequal loss functions. The performance of ThetaBoost and the two approaches are tested experimentally.	Canadian Imperial Bank Commerce, Global Analyt Grp, Toronto, ON M5J 2S8, Canada		Karakoulas, G (corresponding author), Canadian Imperial Bank Commerce, Global Analyt Grp, 161 Bay St,BCE 11, Toronto, ON M5J 2S8, Canada.			Shawe-Taylor, John/0000-0002-2030-0073				CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Freund Y, 1996, P 13 INT C MACH LEAR, P148, DOI DOI 10.5555/3091696.3091715; Kearns M. J., 1990, Proceedings. 31st Annual Symposium on Foundations of Computer Science (Cat. No.90CH2925-6), P382, DOI 10.1109/FSCS.1990.89557; Kubat M, 1998, MACH LEARN, V30, P195, DOI 10.1023/A:1007452223027; Merz C.J., 1997, UCI REPOSITORY MACHI; SCHAPIRE R, 1998, P 11 ANN C COMP LEAR; SCHAPIRE RE, 1997, P 14 INT C MACH LEAR, P322; Shawe-Taylor J, 1998, ALGORITHMICA, V22, P157, DOI 10.1007/PL00013827	9	39	39	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						253	259						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700036
C	Bartlett, MS; Sejnowski, TJ		Mozer, MC; Jordan, MI; Petsche, T		Bartlett, MS; Sejnowski, TJ			Viewpoint invariant face recognition using independent component analysis and attractor networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We have explored two approaches to recognizing faces across changes in pose. First, we developed a representation of face images based on independent component analysis (ICA) and compared it to a principal component analysis (PCA) representation for face recognition. The ICA basis vectors for this data set were more spatially local than the PCA basis vectors and the ICA representation had greater invariance to changes in pose. Second, we present a model for the development of viewpoint invariant responses to faces from visual experience in a biological system. The temporal continuity of natural visual experience was incorporated into an attractor network model by Hebbian learning following a lowpass temporal filter on unit activities. When combined with the temporal filter, a basic Hebbian update rule became a generalization of Griniasty et al. (1993), which associates temporally proximal input patterns into basins of attraction. The system acquired representations of faces that were largely independent of pose.			Bartlett, MS (corresponding author), UNIV CALIF SAN DIEGO,SALK INST,LA JOLLA,CA 92037, USA.		Sejnowski, Terrence/AAV-5558-2021						0	39	41	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						817	823						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00115
C	Conneau, A; Lample, G		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Conneau, Alexis; Lample, Guillaume			Cross-lingual Language Model Pretraining	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models are publicly available(1).	[Conneau, Alexis; Lample, Guillaume] Facebook AI Res, Paris, France; [Conneau, Alexis] Univ Le Mans, Le Mans, France; [Lample, Guillaume] Sorbonne Univ, Paris, France	Facebook Inc; Le Mans Universite; UDICE-French Research Universities; Sorbonne Universite	Conneau, A (corresponding author), Facebook AI Res, Paris, France.; Conneau, A (corresponding author), Univ Le Mans, Le Mans, France.	aconneau@fb.com; glample@fb.com						Al-Rfou Rami, 2018, ARXIV180804444; Ammar Waleed, 2016, ARXIV160201925; Anoop Kunchukuttan, 2018, LREC; Artetxe Mikel, 2018, INT C LEARN REPR ICL; Artetxe Mikel, 2018, ARXIV181210464; Bojanowski Piotr., 2017, TACL, V5, P135, DOI [10.1162/tacl_a_00051, DOI 10.1162/TACL_A_00051]; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Bowman S. R., 2015, EMNLP; Camacho-Collados Jose, 2017, P 11 INT WORKSH SEM, P15, DOI DOI 10.18653/V1/S17-2002; Chou YC, 2008, 14TH ISSAT INTERNATIONAL CONFERENCE ON RELIABILITY AND QUALITY IN DESIGN, PROCEEDINGS, P224; Conneau A., 2018, ICLR; Conneau A., 2018, EMNLP; Conneau Alexis, 2018, LREC; Dai Zihang, 2019, TRANSFORMER XL LANGU, P2; Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; Eriguchi Akiko, 2018, ARXIV180904686; Faruqui M., 2014, P EMNLP; Hendrycks D., 2016, GAUSSIAN ERROR LINEA; Hermann Karl Moritz, 2014, ARXIV14044641; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Howard J, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P328; Johnson M., 2017, T ASSOC COMPUT LING, DOI 10.1162/tacl_a_00065; Jozefowicz Rafal, 2016, ARXIV160202410; Kingma D.P, P 3 INT C LEARNING R; Koehn P, 2007, 45 ANN M ASS COMP LI, P177, DOI DOI 10.3115/1557769.1557821; Lample G, 2018, ICLR; Lample Guillaume, 2018, EMNLP; Mikolov T., 2013, COMPUTER SCI; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Ramachandran Prajit, 2016, ARXIV161102683; Sennrich R., 2016, ARXIV PREPRINT ARXIV; Sennrich R, 2016, PROCEEDINGS OF THE 54TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, VOL 1, P1715; Smith Samuel L, 2017, INT C LEARN REPR; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sutskever I., 2018, IMPROVING LANGUAGE U; Taylor WL, 1953, JOURNALISM QUART, V30, P415, DOI 10.1177/107769905303000401; Tiedemann J., 2012, LREC; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Wada Takashi, 2018, ARXIV180902306; Wang A., 2018, P 2018 EMNLP WORKSH; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Williams Adina, 2017, NAACL; Xing Chao, 2015, P NAACL; Ziemski M., 2016, LREC	44	38	38	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307011
C	Liu, SC; Saito, SSK; Chen, WK; Li, H		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Shichen; Saito, Shunsuke; Chen, Weikai; Li, Hao			Learning to Infer Implicit Surfaces without 3D Supervision	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent advances in 3D deep learning have shown that it is possible to train highly effective deep models for 3D shape generation, directly from 2D images. This is particularly interesting since the availability of 3D models is still limited compared to the massive amount of accessible 2D images, which is invaluable for training. The representation of 3D surfaces itself is a key factor for the quality and resolution of the 3D output. While explicit representations, such as point clouds and voxels, can span a wide range of shape variations, their resolutions are often limited. Mesh-based representations are more efficient but are limited by their ability to handle varying topologies. Implicit surfaces, however, can robustly handle complex shapes, topologies, and also provide flexible resolution control. We address the fundamental problem of learning implicit surfaces for shape inference without the need of 3D supervision. Despite their advantages, it remains nontrivial to (1) formulate a differentiable connection between implicit surfaces and their 2D renderings, which is needed for image-based supervision; and (2) ensure precise geometric properties and control, such as local smoothness. In particular, sampling implicit surfaces densely is also known to be a computationally demanding and very slow operation. To this end, we propose a novel ray-based field probing technique for efficient image-to-field supervision, as well as a general geometric regularizer for implicit surfaces, which provides natural shape priors in unconstrained regions. We demonstrate the effectiveness of our framework on the task of single-view image-based 3D shape digitization and show how we outperform state-of-the-art techniques both quantitatively and qualitatively.	[Liu, Shichen; Saito, Shunsuke; Chen, Weikai; Li, Hao] USC Inst Creat Technol, Los Angeles, CA 90094 USA; [Liu, Shichen; Saito, Shunsuke; Li, Hao] Univ Southern Calif, Los Angeles, CA 90007 USA; [Li, Hao] Pinscreen, Los Angeles, CA USA	University of Southern California	Chen, WK (corresponding author), USC Inst Creat Technol, Los Angeles, CA 90094 USA.	liushichen95@gmail.com; shunsuke.saito16@gmail.com; chenwk891@gmail.com; hao@hao-li.com			ONR YIP grant [N00014-17-S-FO14]; CONIX Research Center, a Semiconductor Research Corporation (SRC) program - DARPA; Andrew and Erna Viterbi Early Career Chair; U.S. Army Research Laboratory (ARL) [W911NF-14-D-0005]; Adobe; Sony	ONR YIP grant; CONIX Research Center, a Semiconductor Research Corporation (SRC) program - DARPA; Andrew and Erna Viterbi Early Career Chair; U.S. Army Research Laboratory (ARL)(United States Department of DefenseUS Army Research Laboratory (ARL)); Adobe; Sony	This research was conducted at USC and was funded by in part by the ONR YIP grant N00014-17-S-FO14, the CONIX Research Center, a Semiconductor Research Corporation (SRC) program sponsored by DARPA, the Andrew and Erna Viterbi Early Career Chair, the U.S. Army Research Laboratory (ARL) under contract number W911NF-14-D-0005, Adobe, and Sony.	Achlioptas Panos, 2017, ARXIV170702392; Azinovic D, 2019, PROC CVPR IEEE, P2442, DOI 10.1109/CVPR.2019.00255; Carr JC, 2001, COMP GRAPH, P67, DOI 10.1145/383259.383266; Chang Angel X., 2015, arXiv; Chen ZQ, 2019, PROC CVPR IEEE, P5932, DOI 10.1109/CVPR.2019.00609; Deschaintre V, 2018, ACM T GRAPHIC, V37, DOI 10.1145/3197517.3201378; Fan HQ, 2017, PROC CVPR IEEE, P2463, DOI 10.1109/CVPR.2017.264; Furukawa Y, 2010, IEEE T PATTERN ANAL, V32, P1362, DOI 10.1109/TPAMI.2009.161; Genova K, 2018, PROC CVPR IEEE, P8377, DOI 10.1109/CVPR.2018.00874; Groueix T, 2018, PROC CVPR IEEE, P216, DOI 10.1109/CVPR.2018.00030; Huang Z, 2018, LECT NOTES COMPUT SC, V11220, P351, DOI 10.1007/978-3-030-01270-0_21; Insafutdinov E, 2018, ADV NEUR IN, V31; Kato H, 2018, PROC CVPR IEEE, P3907, DOI 10.1109/CVPR.2018.00411; Kundu A, 2018, PROC CVPR IEEE, P3559, DOI 10.1109/CVPR.2018.00375; Liao YY, 2018, PROC CVPR IEEE, P2916, DOI 10.1109/CVPR.2018.00308; Lin CY, 2018, ADJUNCT PROCEEDINGS OF THE 2018 IEEE INTERNATIONAL SYMPOSIUM ON MIXED AND AUGMENTED REALITY (ISMAR), P3, DOI 10.1109/ISMAR-Adjunct.2018.00021; Liu GL, 2017, IEEE I CONF COMP VIS, P2280, DOI 10.1109/ICCV.2017.248; Liu HT, 2018, STRENGTH FRACT COMP, V11, P1, DOI 10.3233/SFC-180209; Liu SC, 2019, IEEE I CONF COMP VIS, P7707, DOI 10.1109/ICCV.2019.00780; Loper MM, 2014, LECT NOTES COMPUT SC, V8695, P154, DOI 10.1007/978-3-319-10584-0_11; Tran L, 2018, PROC CVPR IEEE, P7346, DOI 10.1109/CVPR.2018.00767; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mescheder L, 2019, PROC CVPR IEEE, P4455, DOI 10.1109/CVPR.2019.00459; Michalkiewicz Mateusz, 2019, ARXIV190106802; Newcombe RA, 2011, INT SYM MIX AUGMENT, P127, DOI 10.1109/ISMAR.2011.6092378; Park JJ, 2019, PROC CVPR IEEE, P165, DOI 10.1109/CVPR.2019.00025; Richardson E, 2017, PROC CVPR IEEE, P5553, DOI 10.1109/CVPR.2017.589; Sengupta S, 2018, PROC CVPR IEEE, P6296, DOI 10.1109/CVPR.2018.00659; Shen Chen, 2005, ACM SIGGRAPH 2005 CO, P204; Sigg Christian, 2006, THESIS; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Tewari A., 2017, IEEE INT C COMP VIS, V2, P5; Tewari A, 2018, PROC CVPR IEEE, P2549, DOI 10.1109/CVPR.2018.00270; Tulsiani S, 2017, IEEE T PATTERN ANAL, V39, P719, DOI 10.1109/TPAMI.2016.2574713; Tulsiani S, 2017, PROC CVPR IEEE, P209, DOI 10.1109/CVPR.2017.30; Wang NY, 2018, LECT NOTES COMPUT SC, V11215, P55, DOI 10.1007/978-3-030-01252-6_4; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029	38	38	38	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424308033
C	Perslev, M; Jensen, MH; Darkner, S; Jennum, PJ; Igel, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Perslev, Mathias; Jensen, Michael Hejselbak; Darkner, Sune; Jennum, Poul Jorgen; Igel, Christian			U-Time: A Fully Convolutional Network for Time Series Segmentation Applied to Sleep Staging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Neural networks are becoming more and more popular for the analysis of physiological time-series. The most successful deep learning systems in this domain combine convolutional and recurrent layers to extract useful features to model temporal relations. Unfortunately, these recurrent models are difficult to tune and optimize. In our experience, they often require task-specific modifications, which makes them challenging to use for non-experts. We propose U-Time, a fully feed-forward deep learning approach to physiological time series segmentation developed for the analysis of sleep data. U-Time is a temporal fully convolutional network based on the U-Net architecture that was originally proposed for image segmentation. U-Time maps sequential inputs of arbitrary length to sequences of class labels on a freely chosen temporal scale. This is done by implicitly classifying every individual time-point of the input signal and aggregating these classifications over fixed intervals to form the final predictions. We evaluated U-Time for sleep stage classification on a large collection of sleep electroencephalography (EEG) datasets. In all cases, we found that U-Time reaches or outperforms current state-of-the-art deep learning models while being much more robust in the training process and without requiring architecture or hyperparameter adaptation across tasks.	[Perslev, Mathias; Jensen, Michael Hejselbak; Darkner, Sune; Igel, Christian] Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark; [Jennum, Poul Jorgen] Rigshosp, Danish Ctr Sleep Med, Copenhagen, Denmark	University of Copenhagen; Rigshospitalet; University of Copenhagen	Perslev, M (corresponding author), Univ Copenhagen, Dept Comp Sci, Copenhagen, Denmark.	map@di.ku.dk; mhejselbak@gmail.com; darkner@di.ku.dk; poul.joergen.jennum@regionh.dk; igel@diku.dk	Perslev, Mathias/AAV-9501-2020	Perslev, Mathias/0000-0002-0358-4692				Abadi M, 2015, P 12 USENIX S OPERAT; Andreotti F, 2018, IEEE ENG MED BIO, P171, DOI 10.1109/EMBC.2018.8512214; Bai S., 2018, ARXIV PREPRINT ARXIV; Biswal Siddharth, 2017, ABS170708262 CORR; Chen Q, 2017, ABS171209662 CORR; Crum WR, 2006, IEEE T MED IMAGING, V25, P1451, DOI 10.1109/TMI.2006.880587; DICE LR, 1945, ECOLOGY, V26, P297, DOI 10.2307/1932409; Faust O, 2019, COMPUT METH PROG BIO, V176, P81, DOI 10.1016/j.cmpb.2019.04.032; Faust O, 2018, COMPUT METH PROG BIO, V161, P1, DOI 10.1016/j.cmpb.2018.04.005; Ghassemi MM, 2018, COMPUT CARDIOL CONF, V45, DOI [10.22489/cinc.2018.049, 10.22489/CinC.2018.049]; Goldberger AL, 2000, CIRCULATION, V101, pE215, DOI 10.1161/01.CIR.101.23.e215; Iber C, 2007, AASM MANUAL SCORING; Kales A., 1968, MANUAL STANDARDIZED; Kemp B, 2000, IEEE T BIO-MED ENG, V47, P1185, DOI 10.1109/10.867928; Khalighi S, 2016, COMPUT METH PROG BIO, V124, P180, DOI 10.1016/j.cmpb.2015.10.013; Kingma D.P, P 3 INT C LEARNING R; Koch H, 2019, J SLEEP RES, V28, DOI 10.1111/jsr.12780; Koch TL, 2019, I S BIOMED IMAGING, P15, DOI 10.1109/ISBI.2019.8759563; Lea C., 2016, ABS160808242 CORR; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Luo W., 2017, ABS170104128 CORR; Odena A, 2016, DISTILL, DOI [10.23915/distill.00003.-URL, 10.23915/distill.00003]; Perslev M, 2019, LECT NOTES COMPUT SC, V11765, P30, DOI 10.1007/978-3-030-32245-8_4; Phan H., 2018, ABS180506546 CORR; Robert C, 1998, J NEUROSCI METH, V79, P187, DOI 10.1016/S0165-0270(97)00178-7; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ronzhina M, 2012, SLEEP MED REV, V16, P251, DOI 10.1016/j.smrv.2011.06.003; Sateia MJ, 2014, CHEST, V146, P1387, DOI 10.1378/chest.14-0970; Schenck CH, 2014, SLEEP MED, V15, P157, DOI 10.1016/j.sleep.2013.11.001; Sorensen T.A., 1948, KONG DANSK VIDENSK S, V5, P4, DOI DOI 10.1234/12345678; Stephansen JB, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-018-07229-3; Sudre CH, 2017, LECT NOTES COMPUT SC, V10553, P240, DOI 10.1007/978-3-319-67558-9_28; Supratak A, 2017, IEEE T NEUR SYS REH, V25, P1998, DOI 10.1109/TNSRE.2017.2721116; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Terzano MG, 2001, SLEEP MED, V2, P537, DOI 10.1016/S1389-9457(01)00149-6; Tsinalis O, 2016, ANN BIOMED ENG, V44, P1587, DOI 10.1007/s10439-015-1444-y; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vilamala A., 2017, ABS171000633 CORR; Virtanen P, 2019, ABS19010121 CORR; Warby SC, 2014, NAT METHODS, V11, P385, DOI [10.1038/NMETH.2855, 10.1038/nmeth.2855]; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999	41	38	38	4	12	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304042
C	Ibarz, B; Leike, J; Pohlen, T; Irving, G; Legg, S; Amodei, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Irving, Geoffrey; Legg, Shane; Amodei, Dario			Reward learning from human preferences and demonstrations in Atari	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NEURAL-NETWORKS	To solve complex real-world problems with reinforcement learning, we cannot rely on manually specified reward functions. Instead, we can have humans communicate an objective to the agent directly. In this work, we combine two approaches to learning from human feedback: expert demonstrations and trajectory preferences. We train a deep neural network to model the reward function and use its predicted reward to train an DQN-based deep reinforcement learning agent on 9 Atari games. Our approach beats the imitation learning baseline in 7 games and achieves strictly superhuman performance on 2 games without using game rewards. Additionally, we investigate the goodness of fit of the reward model, present some reward hacking problems, and study the effects of noise in the human labels.	[Ibarz, Borja; Leike, Jan; Pohlen, Tobias; Legg, Shane] DeepMind, London, England; [Irving, Geoffrey; Amodei, Dario] OpenAI, San Francisco, CA USA		Ibarz, B (corresponding author), DeepMind, London, England.	bibarz@google.com; leike@google.com; pohlen@google.com; irving@openai.com; legg@google.com; damodei@openai.com						Abbeel P., 2004, P 21 INT C MACHINE L, P1; Akrour Riad, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P116, DOI 10.1007/978-3-642-33486-3_8; Amodei D., 2016, CONCRETE PROBLEMS AI; Andrychowicz M., 2017, ADV NEURAL INFORM PR; [Anonymous], 2017, 5 INT C LEARN REPR I; Arpad E, 1978, RATING CHESSPLAYERS; Bellemare M., 2016, NEURIPS; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; BRADLEY RA, 1952, BIOMETRIKA, V39, P324, DOI 10.2307/2334029; Chentanez, 2005, ADV NEURAL INFORM PR, P1281, DOI DOI 10.21236/ADA440280; Christiano Paul F, 2017, P 31 INT C NEURAL IN, P4302; Dabney W., 2017, ARXIV171010044; Daniel C, 2015, AUTON ROBOT, V39, P389, DOI 10.1007/s10514-015-9454-z; El Asri L, 2016, AAMAS'16: PROCEEDINGS OF THE 2016 INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS & MULTIAGENT SYSTEMS, P457; Everitt Tom, 2018, THESIS; Eysenbach B., 2018, INT C LEARN REPR; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Hester T., 2018, AAAI C ART INT; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; Knox WB, 2009, K-CAP'09: PROCEEDINGS OF THE FIFTH INTERNATIONAL CONFERENCE ON KNOWLEDGE CAPTURE, P9; Lehman J., 2018, ARXIV180303453; Lin Zhiyu, 2017, ARXIV170903969; Mathewson KW, 2017, ARXIV170301274; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mohamed S., 2015, NIPS; Nair A, 2018, IEEE INT CONF ROBOT, P6292; Orseau L, 2013, LECT NOTES ARTIF INT, V8139, P158; Pathak D, 2017, P 34 INT C MACH LEAR, P2778, DOI DOI 10.5555/3305890.3305968; Pilarski PM, 2011, INT C REHAB ROBOT; Salge C, 2014, EMERGENCE COMPLEX CO, V9, P67, DOI 10.1007/978-3-642-53734-9_4; Saunders W, 2018, PROCEEDINGS OF THE 17TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS (AAMAS' 18), P2067; Schaul T, 2015, P INT C LEARN REPR; Schmidhuber J, 2006, CONNECT SCI, V18, P173, DOI 10.1080/09540090600768658; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Storck J., 1995, ICANN '95. International Conference on Artificial Neural Networks. Neuronimes '95 Scientific Conference, P159; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tarvainen Antti, 2017, CORR, Vabs/1703; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Vecerik Mel, 2017, LEVERAGING DEMONSTRA; Wang ZY, 2016, PR MACH LEARN RES, V48; Warnell Garrett, 2017, ARXIV170910163; Wierstra D., 2016, ABS161107507 CORR; Wilson A., 2012, ADV NEURAL INFORM PR, V25, P1133; Wirth C., 2013, ECML PKDD WORKSH REI; Wirth C., 2016, AAAI; Wirth Christian, 2017, J MACHINE LEARNING R, V18, P4945; Zhang X., 2018, CORR; Zhu Y., 2018, ROBOTICS SCI SYSTEMS; Ziebart B. D., 2008, AAAI, V8, P1433	57	38	39	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002055
C	Jacot, A; Gabriel, F; Hongler, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jacot, Arthur; Gabriel, Franck; Hongler, Clement			Neural Tangent Kernel: Convergence and Generalization in Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				MULTILAYER FEEDFORWARD NETWORKS	At initialization, artificial neural networks (ANNs) are equivalent to Gaussian processes in the infinite-width limit (12;9), thus connecting them to kernel methods. We prove that the evolution of an ANN during training can also be described by a kernel: during gradient descent on the parameters of an ANN, the network function f(theta) (which maps input vectors to output vectors) follows the kernel gradient of the functional cost (which is convex, in contrast to the parameter cost) w.r.t. a new kernel: the Neural Tangent Kernel (NTK). This kernel is central to describe the generalization features of ANNs. While the NTK is random at initialization and varies during training, in the infinite-width limit it converges to an explicit limiting kernel and it stays constant during training. This makes it possible to study the training of ANNs in function space instead of parameter space. Convergence of the training can then be related to the positive-definiteness of the limiting NTK. We then focus on the setting of least-squares regression and show that in the infinite-width limit, the network function f(theta) follows a linear differential equation during training. The convergence is fastest along the largest kernel principal components of the input data with respect to the NTK, hence suggesting a theoretical motivation for early stopping. Finally we study the NTK numerically, observe its behavior for wide networks, and compare it to the infinite-width limit.	[Jacot, Arthur; Gabriel, Franck; Hongler, Clement] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Gabriel, Franck] Imperial Coll London, London, England	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Imperial College London	Jacot, A (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	arthur.jacot@netopera.net; franckrgabriel@gmail.com; clement.hongler@gmail.com			ERC CG CRITICAL; ERC SG Constamis; NCCR SwissMAP; Blavatnik Family Foundation; Latsis Foundation	ERC CG CRITICAL; ERC SG Constamis; NCCR SwissMAP; Blavatnik Family Foundation; Latsis Foundation	The authors thank K. Kytola for many interesting discussions. The second author was supported by the ERC CG CRITICAL. The last author acknowledges support from the ERC SG Constamis, the NCCR SwissMAP, the Blavatnik Family Foundation and the Latsis Foundation.	BELKIN M., 2018, UNDERSTAND DEEP LEAR; Cho Y., 2009, NIPS, P342; Choromanska A, 2015, JMLR WORKSH CONF PRO, V38, P192; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Dragomir S.S., 2002, SOME GRONWALL TYPE I; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; HORNIK K, 1989, NEURAL NETWORKS, V2, P359, DOI 10.1016/0893-6080(89)90020-8; Karakida R., 2018, UNIVERSAL STAT FISHE; Lee J., 2018, ICLR; LESHNO M, 1993, NEURAL NETWORKS, V6, P861, DOI 10.1016/S0893-6080(05)80131-5; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Pascanu R., 2014, SADDLE POINT PROBLEM; Pennington J, 2017, PR MACH LEARN RES, V70; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Sagun L., 2017, ABS170604454 CORR; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Shawe-Taylor J., 2004, KERNEL METHODS PATTE; Zhang  C., 2017, ICLR 2017 P FEB	19	38	38	3	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003016
C	Olson, M; Wyner, AJ; Berk, R		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Olson, Matthew; Wyner, Abraham J.; Berk, Richard			Modern Neural Networks Generalize on Small Data Sets	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					In this paper, we use a linear program to empirically decompose fitted neural networks into ensembles of low-bias sub-networks. We show that these sub-networks are relatively uncorrelated which leads to an internal regularization process, very much like a random forest, which can explain why a neural network is surprisingly resistant to overfitting. We then demonstrate this in practice by applying large neural networks, with hundreds of parameters per training observation, to a collection of 116 real-world data sets from the UCI Machine Learning Repository. This collection of data sets contains a much smaller number of training examples than the types of image classification tasks generally studied in the deep learning literature, as well as non-trivial label noise. We show that even in this setting deep neural nets are capable of achieving superior classification accuracy without overfitting.	[Olson, Matthew; Wyner, Abraham J.; Berk, Richard] Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA	University of Pennsylvania	Olson, M (corresponding author), Univ Penn, Wharton Sch, Dept Stat, Philadelphia, PA 19104 USA.	maolson@wharton.upenn.edu; ajw@wharton.upenn.edu; berkr@wharton.upenn.edu						[Anonymous], 2016, 2016 IEEE C COMPUTER, DOI [DOI 10.1109/CVPR.2016.90, 10.1109/CVPR.2016.90]; BARTLETT P. L., 2017, ADV NEURAL INFORM PR, V30, P6240; Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Breiman L, 2000, ANN STAT, V28, P374; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Breiman L., 2000, TECHNICAL REPORT; Chollet F, 2018, DEEP LEARNING PYTHON; Dinh L., 2017, P INT C MACH LEARN, P1019; Fernandez-Delgado M, 2014, J MACH LEARN RES, V15, P3133; Golowich N., 2018, PROC C LEARN THEORY, P297; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Kawaguchi K., 2017, ARXIV171005468V4STAT, DOI DOI 10.2196/jmir.5870; Kingma Diederik P., 2015, 3 INT C LEARN REPRES, V3; Liang T, 2017, ABS171101530 CORR; Lin HW, 2017, J STAT PHYS, V168, P1223, DOI 10.1007/s10955-017-1836-5; Neyshabur B., 2017, ADV NEURAL INFORM PR, P5947; Segal M.R., 2004, MACHINE LEARNING BEN; Shavit N., 2017, ABS170510694 CORR; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Unterthiner T, 2015, COMPUTER SCI, DOI DOI 10.48550/ARXIV.1511.07289; Wilber M, 2016, P ADV NEUR INF PROC, V30, P550; Wyner AJ, 2017, J MACH LEARN RES, V18, P1; Zeng HQ, 2017, PROC INT CONF RECON	24	38	38	1	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303060
C	Pang, TY; Du, C; Dong, YP; Zhu, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Pang, Tianyu; Du, Chao; Dong, Yinpeng; Zhu, Jun			Towards Robust Detection of Adversarial Examples	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Although the recent progress is substantial, deep learning methods can be vulnerable to the maliciously generated adversarial examples. In this paper, we present a novel training procedure and a thresholding test strategy, towards robust detection of adversarial examples. In training, we propose to minimize the reverse cross-entropy (RCE), which encourages a deep network to learn latent representations that better distinguish adversarial examples from normal ones. In testing, we propose to use a thresholding strategy as the detector to filter out adversarial examples for reliable predictions. Our method is simple to implement using standard algorithms, with little extra training cost compared to the common cross-entropy minimization We apply our method to defend various attacking methods on the widely used MNIST and CIFAR-10 datasets, and achieve significant improvements on robust predictions under all the threat models in the adversarial setting.	[Pang, Tianyu; Du, Chao; Dong, Yinpeng; Zhu, Jun] Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intell Tech & Syst, THBI Lab,BNRist Ctr, Beijing, Peoples R China	Tsinghua University	Zhu, J (corresponding author), Tsinghua Univ, Dept Comp Sci & Tech, State Key Lab Intell Tech & Syst, THBI Lab,BNRist Ctr, Beijing, Peoples R China.	pty17@mails.tsinghua.edu.cn; du-c14@mails.tsinghua.edu.cn; dyp17@mails.tsinghua.edu.cn; dcszj@mail.tsinghua.edu.cn	Tianyu, Pang/AAW-2653-2020		National Key Research and Development Program of China [2017YFA0700904]; NSFC [61620106010, 61621136008, 61332007]; Beijing NSF Project [L172037]; Tiangong Institute for Intelligent Computing; Siemens; Intel; NVIDIA NVAIL Program	National Key Research and Development Program of China; NSFC(National Natural Science Foundation of China (NSFC)); Beijing NSF Project; Tiangong Institute for Intelligent Computing; Siemens(Siemens AG); Intel(Intel Corporation); NVIDIA NVAIL Program	This work was supported by the National Key Research and Development Program of China (No. 2017YFA0700904), NSFC Projects (Nos. 61620106010, 61621136008, 61332007), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, and the projects from Siemens and Intel.	Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; Bhagoji AN, 2017, ARXIV PREPRINT ARXIV; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Dong YP, 2018, PROC CVPR IEEE, P9185, DOI 10.1109/CVPR.2018.00957; Dvijotham K, 2018, ARXIV180510265; Dvijotham K., 2018, P UNC ART INT UAI, P162; Elsayed GF, 2018, ADV NEUR IN, V31; EricWong Zico, 2018, INT C MACH LEARN, P5286; Feinman R., 2017, ARXIV PREPRINT ARXIV; Gilmer Justin, 2018, ARXIV180102774; Gong Zhitao, 2017, ARXIV170404960; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Grosse Kathrin, 2017, ARXIV170206280; Gu Shixiang, 2014, C NEUR INF PROC SYST; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A, 2018, ICLR, P99, DOI DOI 10.1201/9781351251389-8; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LI X., 2016, ARXIV161207767; Liu Y, 2017, INT C LEARN REPR ICL; Ma X., 2018, 2018 15 INT S WIRELE, P1; Madry Aleksander, 2017, ARXIV; Metzen J. H., 2017, 5 INT C LEARNING REP, DOI DOI 10.1109/ICCV.2017.300; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Papernot N., 2016, ARXIV160202697; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Papernot N, 2016, 1ST IEEE EUROPEAN SYMPOSIUM ON SECURITY AND PRIVACY, P372, DOI 10.1109/EuroSP.2016.36; Rozsa A, 2016, IEEE COMPUT SOC CONF, P410, DOI 10.1109/CVPRW.2016.58; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Zheng S, 2016, PROC CVPR IEEE, P4480, DOI 10.1109/CVPR.2016.485	35	38	41	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304058
C	Shazeer, N; Cheng, YL; Parmar, N; Tran, D; Vaswani, A; Koanantakool, P; Hawkins, P; Lee, H; Hong, MS; Young, C; Sepassi, R; Hechtman, B		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shazeer, Noam; Cheng, Youlong; Parmar, Niki; Tran, Dustin; Vaswani, Ashish; Koanantakool, Penporn; Hawkins, Peter; Lee, HyoukJoong; Hong, Mingsheng; Young, Cliff; Sepassi, Ryan; Hechtman, Blake			Mesh-TensorFlow: Deep Learning for Supercomputers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Batch-splitting (data-parallelism) is the dominant distributed Deep Neural Network (DNN) training strategy, due to its universal applicability and its amenability to Single-Program-Multiple-Data (SPMD) programming. However, batch-splitting suffers from problems including the inability to train very large models (due to memory constraints), high latency, and inefficiency at small batch sizes. All of these can be solved by more general distribution strategies (model-parallelism). Unfortunately, efficient model-parallel algorithms tend to be complicated to discover, describe, and to implement, particularly on large clusters. We introduce Mesh-TensorFlow, a language for specifying a general class of distributed tensor computations. Where data-parallelism can be viewed as splitting tensors and operations along the "batch" dimension, in Mesh-TensorFlow, the user can specify any tensor-dimensions to be split across any dimensions of a multi-dimensional mesh of processors. A Mesh-TensorFlow graph compiles into a SPMD program consisting of parallel operations coupled with collective communication primitives such as Allreduce. We use Mesh-TensorFlow to implement an efficient data-parallel, model-parallel version of the Transformer [16] sequence-to-sequence model. Using TPU meshes of up to 512 cores, we train Transformer models with up to 5 billion parameters, surpassing state of the art results on WMT' 14 English-to-French translation task and the one-billion-word language modeling benchmark. Mesh-Tensorflow is available at https://github.com/tensorflow/mesh.	[Shazeer, Noam; Cheng, Youlong; Parmar, Niki; Tran, Dustin; Vaswani, Ashish; Koanantakool, Penporn; Hawkins, Peter; Lee, HyoukJoong; Hong, Mingsheng; Young, Cliff; Sepassi, Ryan; Hechtman, Blake] Google Brain, Mountain View, CA 94043 USA	Google Incorporated	Shazeer, N (corresponding author), Google Brain, Mountain View, CA 94043 USA.	noam@google.com; ylc@google.com; nikip@google.com; trandustin@google.com; avaswani@google.com; penporn@google.com; phawkins@google.com; hyouklee@google.com; hongm@google.com; cliffy@google.com; rsepassi@google.com; blakehechtman@google.com						Abadi M, 2015, P 12 USENIX S OPERAT; AGGARWAL A, 1990, THEOR COMPUT SCI, V71, P3, DOI 10.1016/0304-3975(90)90188-N; BERNTSEN J, 1989, PARALLEL COMPUT, V12, P335, DOI 10.1016/0167-8191(89)90091-4; Calvin Justus A., 2015, P 5 WORKSH IRR APPL, DOI 10.1145/2833179.2833186; Dean J., 2012, ADV NEURAL INFORM PR, P1223, DOI DOI 10.5555/2999134.2999271; Demmel J, 2013, INT PARALL DISTRIB P, P261, DOI 10.1109/IPDPS.2013.80; Demmel James, 2018, ARXIV180206905; Gholami Amir, 2018, SPAA 18; Jain Nikhil, 2010, P 24 ACM INT C SUP, P27; Jia Z., 2018, DATA MODEL PARALLELI; Jia Zhihao, 2018, ARXIV180204924; Jin P., 2018, SPATIALLY PARALLEL C; Koanantakool P, 2016, INT PARALL DISTRIB P, P842, DOI 10.1109/IPDPS.2016.117; Koanantakool Penporn, 2018, P 21 INT C ART INT S, V84, P1376; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Shazeer N., 2017, ICLR; Solomonik E, 2014, J PARALLEL DISTR COM, V74, P3176, DOI 10.1016/j.jpdc.2014.06.002; Solomonik E, 2011, LECT NOTES COMPUT SC, V6853, P90, DOI 10.1007/978-3-642-23397-5_10; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vinyals Oriol, 2016, CORR; WOLFE M, 1989, PROCEEDINGS : SUPERCOMPUTING 89, P655, DOI 10.1145/76263.76337	21	38	38	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852005003
C	Chen, YP; Li, JN; Xiao, HX; Jin, XJ; Yan, SC; Feng, JS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Chen, Yunpeng; Li, Jianan; Xiao, Huaxin; Jin, Xiaojie; Yan, Shuicheng; Feng, Jiashi			Dual Path Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this work, we present a simple, highly efficient and modularized Dual Path Network (DPN) for image classification which presents a new topology of connection paths internally. By revealing the equivalence of the state-of-the-art Residual Network (ResNet) and Densely Convolutional Network (DenseNet) within the HORNN framework, we find that ResNet enables feature re-usage while DenseNet enables new features exploration which are both important for learning good representations. To enjoy the benefits from both path topologies, our proposed Dual Path Network shares common features while maintaining the flexibility to explore new features through dual path architectures. Extensive experiments on three benchmark datasets, ImagNet-1k, Places365 and PASCAL VOC, clearly demonstrate superior performance of the proposed DPN over state-of-the-arts. In particular, on the ImagNet-1k dataset, a shallow DPN surpasses the best ResNeXt-101(64 x 4 d) with 26% smaller model size, 25% less computational cost and 8% lower memory consumption, and a deeper DPN (DPN-131) further pushes the state-of-the-art single model performance with about 2 times faster training speed. Experiments on the Places365 large-scale scene dataset, PASCAL VOC detection dataset, and PASCAL VOC segmentation dataset also demonstrate its consistently better performance than DenseNet, ResNet and the latest ResNeXt model over various applications.	[Chen, Yunpeng; Li, Jianan; Xiao, Huaxin; Jin, Xiaojie; Yan, Shuicheng; Feng, Jiashi] Natl Univ Singapore, Singapore, Singapore; [Li, Jianan] Beijing Inst Technol, Beijing, Peoples R China; [Xiao, Huaxin] Natl Univ Def Technol, Changsha, Hunan, Peoples R China; [Yan, Shuicheng] Qihoo 360 AI Inst, Beijing, Peoples R China	National University of Singapore; Beijing Institute of Technology; National University of Defense Technology - China	Chen, YP (corresponding author), Natl Univ Singapore, Singapore, Singapore.		Feng, Jiashi/AGX-6209-2022; Yan, Shuicheng/HCI-1431-2022		National University of Singapore [R-263-000-C08-133]; Ministry of Education of Singapore AcRF Tier One grant [R-263-000-C21-112]; NUS IDS grant [R-263-000-C67-646]	National University of Singapore(National University of Singapore); Ministry of Education of Singapore AcRF Tier One grant(Ministry of Education, Singapore); NUS IDS grant	The work of Jiashi Feng was partially supported by National University of Singapore startup grant R-263-000-C08-133, Ministry of Education of Singapore AcRF Tier One grant R-263-000-C21-112 and NUS IDS grant R-263-000-C67-646.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Chen Y., 2017, CORR; Cheng T, 2016, AIDS BEHAV, V20, P377, DOI 10.1007/s10461-015-1101-3; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; He K., 2017, ARXIV170306870, P2980, DOI [10.1109/ICCV.2017.322, DOI 10.1109/ICCV.2017.322]; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kim J, 2016, PROC CVPR IEEE, P1637, DOI [10.1109/CVPR.2016.181, 10.1109/CVPR.2016.182]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langkvist M, 2014, PATTERN RECOGN LETT, V42, P11, DOI 10.1016/j.patrec.2014.01.008; Lee CY, 2016, JMLR WORKSH CONF PRO, V51, P464; Liao Q., 2016, CORR; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Newell A, 2016, LECT NOTES COMPUT SC, V9912, P483, DOI 10.1007/978-3-319-46484-8_29; Pleiss G., 2017, ABS170706990 CORR; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Soltani Rohollah, 2016, ARXIV160500064; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zhang X., 2016, ARXIV161105725; Zhou Bolei, 2016, PLACES IMAGE DATABAS	24	38	38	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404052
C	Ding, BL; Kulkarni, J; Yekhanin, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ding, Bolin; Kulkarni, Janardhan; Yekhanin, Sergey			Collecting Telemetry Data Privately	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The collection and analysis of telemetry data from user's devices is routinely performed by many software companies. Telemetry collection leads to improved user experience but poses significant risks to users' privacy. Locally differentially private (LDP) algorithms have recently emerged as the main tool that allows data collectors to estimate various population statistics, while preserving privacy. The guarantees provided by such algorithms are typically very strong for a single round of telemetry collection, but degrade rapidly when telemetry is collected regularly. In particular, existing LDP algorithms are not suitable for repeated collection of counter data such as daily app usage statistics. In this paper, we develop new LDP mechanisms geared towards repeated collection of counter data, with formal privacy guarantees even after being executed for an arbitrarily long period of time. For two basic analytical tasks, mean estimation and histogram estimation, our LDP mechanisms for repeated data collection provide estimates with comparable or even the same accuracy as existing single-round LDP collection mechanisms. We conduct empirical evaluation on real-world counter datasets to verify our theoretical results. Our mechanisms have been deployed by Microsoft to collect telemetry across millions of devices.	[Ding, Bolin; Kulkarni, Janardhan; Yekhanin, Sergey] Microsoft Res, Redmond, WA 98052 USA	Microsoft	Ding, BL (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	bolind@microsoft.com; jakul@microsoft.com; yekhanin@microsoft.com	Jeong, Yongwook/N-7413-2016					Agrawal S, 2005, PROC INT CONF DATA, P193; [Anonymous], 2017, ARXIV170902753; Bansal N, 2008, SIAM J COMPUT, V38, P1157, DOI 10.1137/060674417; Bassily R., 2017, ADV NEURAL INFORM PR; Bassily R, 2015, ACM S THEORY COMPUT, P127, DOI 10.1145/2746539.2746632; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Ding B., 2017, COLLECTING TELEMETRY; Duchi J., 2013, ADV NEURAL INFORM PR, V26, P1529; Duchi J. C., 2016, ABS160402390 CORR; Duchi JC, 2013, ANN IEEE SYMP FOUND, P429, DOI 10.1109/FOCS.2013.53; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Erlingsson U, 2014, CCS'14: PROCEEDINGS OF THE 21ST ACM CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P1054, DOI 10.1145/2660267.2660348; Evfimievski A., 2003, P 22 ACM SIGACT SIGM, P211, DOI DOI 10.1145/773153.773174; Fanti Giulia, 2016, Proceedings on Privacy Enhancing Technologies, V2016, P41, DOI 10.1515/popets-2016-0015; Goemans MX, 2002, SIAM J DISCRETE MATH, V15, P165, DOI 10.1137/S089548019936223X; HSU J, 2012, INT C AUT LANG PROGR, V7391, P461; Kairouz P., 2016, ICML; WARNER SL, 1965, J AM STAT ASSOC, V60, P63, DOI 10.2307/2283137; Wasserman L, 2010, J AM STAT ASSOC, V105, P375, DOI 10.1198/jasa.2009.tm08651	20	38	38	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403062
C	Li, P; Milenkovic, O		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Pan; Milenkovic, Olgica			Inhomogeneous Hypergraph Clustering with Applications	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				ORGANIZATION; EIGENVALUES	Hypergraph partitioning is an important problem in machine learning, computer vision and network analytics. A widely used method for hypergraph partitioning relies on minimizing a normalized sum of the costs of partitioning hyperedges across clusters. Algorithmic solutions based on this approach assume that different partitions of a hyperedge incur the same cost. However, this assumption fails to leverage the fact that different subsets of vertices within the same hyperedge may have different structural importance. We hence propose a new hypergraph clustering technique, termed inhomogeneous hypergraph partitioning, which assigns different costs to different hyperedge cuts. We prove that inhomogeneous partitioning produces a quadratic approximation to the optimal solution if the inhomogeneous costs satisfy submodularity constraints. Moreover, we demonstrate that inhomogenous partitioning offers significant performance improvements in applications such as structure learning of rankings, subspace segmentation and motif clustering.	[Li, Pan; Milenkovic, Olgica] UIUC, Dept ECE, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Li, P (corresponding author), UIUC, Dept ECE, Champaign, IL 61820 USA.	panli2@illinois.edu; milenkov@illinois.edu	Jeong, Yongwook/N-7413-2016		NSF [CCF 1527636]	NSF(National Science Foundation (NSF))	The authors gratefully acknowledge many useful suggestions by the reviewers. They are also indebted to the reviewers for providing many additional and relevant references. This work was supported in part by the NSF grant CCF 1527636.	Agarwal S, 2005, PROC CVPR IEEE, P838; Agarwal S., 2006, ICML, P17, DOI DOI 10.1145/1143844.1143847; Ailon N, 2008, J ACM, V55, DOI 10.1145/1411509.1411513; Allesina S, 2005, OIKOS, V110, P164, DOI 10.1111/j.0030-1299.2005.13082.x; [Anonymous], [No title captured]; [Anonymous], 2010, P 2010 SIAM INT C DA, DOI [10.1137/1.9781611972801.49, DOI 10.1137/1.9781611972801.49]; Awasthi Pranjal, 2014, NIPS, P2609; Bansal N, 2002, ANN IEEE SYMP FOUND, P238, DOI 10.1109/SFCS.2002.1181947; Benson AR, 2016, SCIENCE, V353, P163, DOI 10.1126/science.aad9029; Benson Austin R, 2015, Proc SIAM Int Conf Data Min, V2015, P118; Bu Y., 2016, ARXIV160702653; Bul`o S. R., 2009, ADV NEURAL INFORM PR, P1571; Chung F., 2007, P ICCM; Devanur N. R., 2013, ARXIV13044948; Gao WH, 2017, IEEE INT SYMP INFO, P1267, DOI 10.1109/ISIT.2017.8006732; Ghoshdastidar D., 2014, ADV NEURAL INFORM PR, P397; Ghoshdastidar D., 2015, ARXIV150501582; Gormley IC, 2007, LECT NOTES COMPUT SC, V4503, P90; Hein M., 2013, P ADV NEUR INF PROC, V26; Huang J, 2012, ELECTRON J STAT, V6, P199, DOI 10.1214/12-EJS670; Jeong H, 2000, NATURE, V407, P651, DOI 10.1038/35036627; Kim Sungwoong, 2011, ADV NEURAL INFORM PR; Knyazev Andrew V, 2017, ARXIV170101394; Leordeanu M., 2012, P 15 INT C ART INT S, P676; Li GY, 2013, NUMER LINEAR ALGEBR, V20, P1001, DOI 10.1002/nla.1877; Li P., 2017, P IEEE INT C COMP CO, P109; Liu H., 2010, ADV NEURAL INFORM PR, P1414; Louis A, 2015, ACM S THEORY COMPUT, P713, DOI 10.1145/2746539.2746555; Meila M., 2014, ADV NEURAL INFORM PR, V27, P631; Milo R, 2002, SCIENCE, V298, P824, DOI 10.1126/science.298.5594.824; Ng AY, 2002, ADV NEUR IN, V14, P849; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Yin HB, 2017, AER ADV ENG RES, V112, P555; Zhang CZ, 2017, PR MACH LEARN RES, V70; Zhou D, 2006, P 2006 C ADV NEURAL, V19, DOI 10.7551/mitpress/7503.003.0205	37	38	38	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402035
C	Musco, C; Musco, C		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Musco, Cameron; Musco, Christopher			Recursive Sampling for the Nystrom Method	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				MATRIX; APPROXIMATION	We give the first algorithm for kernel Nystrom approximation that runs in linear time in the number of training points and is provably accurate for all kernel matrices, without dependence on regularity or incoherence conditions. The algorithm projects the kernel onto a set of s landmark points sampled by their ridge leverage scores, requiring just O(ns) kernel evaluations and O(ns(2)) additional runtime. While leverage score sampling has long been known to give strong theoretical guarantees for Nystrom approximation, by employing a fast recursive sampling scheme, our algorithm is the first to make the approach scalable. Empirically we show that it finds more accurate kernel approximations in less time than popular techniques such as classic Nystrom approximation and the random Fourier features method.	[Musco, Cameron; Musco, Christopher] MIT, EECS, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Musco, C (corresponding author), MIT, EECS, Cambridge, MA 02139 USA.	cnmusco@mit.edu; cpmusco@mit.edu						Achlioptas Dimitris, 2001, ADV NEURAL INFORM PR; Alaoui A., 2015, P 28 INT C NEURAL IN, P775; Avron H, 2014, ADV NEUR IN, V27; BACH FR, 2002, J MACHINE LEARNING R, V3, P1; Bach Francis, 2013, P 26 ANN C COMP LEAR; Balcan MF, 2006, MACH LEARN, V65, P79, DOI 10.1007/s10994-006-7550-1; Belabbas MA, 2009, P NATL ACAD SCI USA, V106, P369, DOI 10.1073/pnas.0810600105; BOUTSIDIS C., 2016, P 48 ANN ACM S THEOR; Calandriello Daniele, 2017, P 20 INT C ART INT S; Calandriello Daniele, 2016, P 32 ANN C UNC ART I, P62; Chen SY, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P201; Clarkson KL, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P2061; Cohen MB, 2015, PROCEEDINGS OF THE 6TH INNOVATIONS IN THEORETICAL COMPUTER SCIENCE (ITCS'15), P181, DOI 10.1145/2688073.2688113; Cohen MB, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P1758; Cohen MB, 2015, ACM S THEORY COMPUT, P163, DOI 10.1145/2746539.2746569; Cortes C, 2010, P 13 INT C ART INT S, P113; Drineas, 2009, ADV NEURAL INFORM PR, P153, DOI DOI 10.1016/J.FUTURE.2017.11.043; Drineas P, 2005, J MACH LEARN RES, V6, P2153; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Drineas P, 2012, J MACH LEARN RES, V13, P3475; Feldman D, 2013, PROCEEDINGS OF THE TWENTY-FOURTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS (SODA 2013), P1434; Fine S, 2002, J MACH LEARN RES, V2, P243, DOI 10.1162/15324430260185619; Gittens A., 2013, INT C MACHINE LEARNI, P567; Gittens A., 2011, ARXIV11105305; Hall M., 2008, WEKA DATA MINING SOF, V11, P10, DOI [10.1145/1656274.1656278, DOI 10.1145/1656274.1656278]; Hastie T., 2002, ELEMENTS STAT LEARNI; Hsu D, 2014, FOUND COMPUT MATH, V14, P569, DOI 10.1007/s10208-014-9192-1; IBM Reseach Division Skylark Team, 2014, LIBSK SKETCH BAS DIS; Kumar S, 2012, J MACH LEARN RES, V13, P981; Le Quoc V., 2013, INT C MACH LEARN, P244; Li Chengtao, 2016, P 33 INT C MACH LEAR; Li M, 2015, IEEE T NEUR NET LEAR, V26, P152, DOI 10.1109/TNNLS.2014.2359798; Lichman M, 2013, UCI MACHINE LEARNING; Mitzenmacher M., 2005, PROBABILITY COMPUTIN; Paul S, 2016, NEURAL COMPUT, V28, P716, DOI 10.1162/NECO_a_00816; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peters J, 2017, ADAPT COMPUT MACH LE; Platt John, 2005, P 8 INT C ART INT ST; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rudi A, 2015, ADV NEUR IN, V28; Scholkopf B, 1999, ADVANCES IN KERNEL METHODS, P327; Silva V.D., 2003, NIPS, P721; Smola A. J., 2000, P 17 INT C MACH LEAR, P911; Tong Zhang, 2006, LEARNING, V17; Tropp JA, 2015, FOUND TRENDS MACH LE, V8, P2, DOI 10.1561/2200000048; Tu Stephen, 2016, ARXIV160205310; Uzilov AV, 2006, BMC BIOINFORMATICS, V7, DOI 10.1186/1471-2105-7-173; Wainwright Martin, 2013, P 26 ANN C COMP LEAR; Wang SS, 2013, J MACH LEARN RES, V14, P2729; Wang Weiran, 2016, ARXIV160202172; Williams CKI, 2001, ADV NEUR IN, V13, P682; Woodruff DP, 2014, FOUND TRENDS THEOR C, V10, P1, DOI 10.1561/0400000060; Yang T., 2012, ADV NEURAL INFORM PR, P476; Yun Yang, 2015, ANN STAT; Zhang K., 2008, P 25 INT C MACHINE L, P1232	56	38	38	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403087
C	Poloczek, M; Wang, JL; Frazier, PI		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Poloczek, Matthias; Wang, Jialei; Frazier, Peter I.			Multi-Information Source Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				KNOWLEDGE-GRADIENT; PROBABILITY	We consider Bayesian methods for multi-information source optimization (MISO), in which we seek to optimize an expensive-to-evaluate black-box objective function while also accessing cheaper but biased and noisy approximations ("information sources"). We present a novel algorithm that outperforms the state of the art for this problem by using a Gaussian process covariance kernel better suited to MISO than those used by previous approaches, and an acquisition function based on a one-step optimality analysis supported by efficient parallelization. We also provide a novel technique to guarantee the asymptotic quality of the solution provided by this algorithm. Experimental evaluations demonstrate that this algorithm consistently finds designs of higher value at less cost than previous approaches.	[Poloczek, Matthias] Univ Arizona, Dept Syst & Ind Engn, Tucson, AZ 85721 USA; [Wang, Jialei] IBM Corp, Analyt Off, Armonk, NY 10504 USA; [Frazier, Peter I.] Cornell Univ, Sch Operat Res & Informat Engn, Ithaca, NY 14853 USA	University of Arizona; International Business Machines (IBM); Cornell University	Poloczek, M (corresponding author), Univ Arizona, Dept Syst & Ind Engn, Tucson, AZ 85721 USA.	poloczek@email.arizona.edu; jw865@cornell.edu; pf98@cornell.edu	Jeong, Yongwook/N-7413-2016	Frazier, Peter/0000-0002-3501-3341	NSF CAREER [CMMI-1254298]; NSF [CMMI-1536895, IIS-1247696]; AFOSR [FA9550-12-1-0200, FA9550-15-1-0038, FA9550-16-1-0046]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR))	This work was partially supported by NSF CAREER CMMI-1254298, NSF CMMI-1536895, NSF IIS-1247696, AFOSR FA9550-12-1-0200, AFOSR FA9550-15-1-0038, and AFOSR FA9550-16-1-0046.	Allaire D, 2014, INT J UNCERTAIN QUAN, V4, P1, DOI 10.1615/Int.J.UncertaintyQuantification.2013004121; Alvarez MA, 2012, FOUND TRENDS MACH LE, V4, P195, DOI 10.1561/2200000036; Brynjarsdottir J, 2014, INVERSE PROBL, V30, DOI 10.1088/0266-5611/30/11/114007; Chick S, 2012, ASSEMBLE ORDER SIMUL; Cinlar E, 2011, GRAD TEXTS MATH, V261, P1, DOI 10.1007/978-0-387-87859-1; Forrester AIJ, 2007, P R SOC A, V463, P3251, DOI 10.1098/rspa.2007.1900; Frazier P, 2009, INFORMS J COMPUT, V21, P599, DOI 10.1287/ijoc.1080.0314; Frazier PI, 2008, SIAM J CONTROL OPTIM, V47, P2410, DOI 10.1137/070693424; Ghosal S, 2006, ANN STAT, V34, P2413, DOI 10.1214/009053606000000795; Goovaerts P., 1997, GEOSTATISTICS NATURA; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Hong LJ, 2006, OPER RES, V54, P115, DOI 10.1287/opre.1050.0237; Huang D, 2006, STRUCT MULTIDISCIP O, V32, P369, DOI 10.1007/s00158-005-0587-0; Kandasamy K., 2016, ADV NEURAL INFORM PR; Kennedy MC, 2000, BIOMETRIKA, V87, P1, DOI 10.1093/biomet/87.1.1; Klein A., 2016, CORR; Lam R, 2015, 56 AIAA ASCE AHS ASC; Le Gratiet L., 2014, INT J UNCERTAINTY QU, V4; Le Gratiet L, 2015, TECHNOMETRICS, V57, P418, DOI 10.1080/00401706.2014.928233; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; Picheny V, 2013, TECHNOMETRICS, V55, P2, DOI 10.1080/00401706.2012.707580; Poloczek M, 2016, WINT SIMUL C PROC, P770, DOI 10.1109/WSC.2016.7822140; Qu HS, 2015, OPER RES, V63, P931, DOI 10.1287/opre.2015.1395; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Scott W, 2011, SIAM J OPTIMIZ, V21, P996, DOI 10.1137/100801275; Shah A, 2015, ADV NEURAL INFORM PR, V12, P3330; Snoek J., 2016, COMMUNICATION; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Srinivas N., 2009, ARXIV09123995; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Teh Y.-W., 2005, ARTIFICIAL INTELLIGE, V10; Theano, 2017, THEAN LOG REGR; Toscano-Palmerin S., 2016, P 12 INT C MONT CARL; Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2; Williams, 2007, NEURAL INFORM PROCES, P153, DOI DOI 10.5555/2981562.2981582; WINKLER RL, 1981, MANAGE SCI, V27, P479, DOI 10.1287/mnsc.27.4.479; Wu Jiajun, 2017, ADV NEURAL INFORM PR	38	38	38	3	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404035
C	Defazio, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Defazio, Aaron			A Simple Practical Accelerated Method for Finite Sums	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We describe a novel optimization method for finite sums (such as empirical risk minimization problems) building on the recently introduced SAGA method. Our method achieves an accelerated convergence rate on strongly convex smooth problems. Our method has only one parameter (a step size), and is radically simpler than other accelerated methods for finite sums. Additionally it can be applied when the terms are non-smooth, yielding a method applicable in many areas where operator splitting methods would traditionally be applied.	[Defazio, Aaron] Ambiata, Sydney, NSW, Australia		Defazio, A (corresponding author), Ambiata, Sydney, NSW, Australia.							Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Defazio A, 2014, ADV NEUR IN, V27; Defazio AJ, 2014, PR MACH LEARN RES, V32, P1125; Hofmann T., 2015, ADV NEURAL INFORM PR, V28, P2305; Konecny Jakub, 2013, ARXIV E PRINTS; Lan G., 2015, ARXIV E PRINTS; Lin H., 2015, ADV NEURAL INFORM PR, P3384; Mairal Julien, 2014, TECHNICAL REPORT; Nesterov, 1998, INTRO LECT CONVEX PR; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; ROCKAFELLAR RT, 1976, SIAM J CONTROL, V14, P877, DOI 10.1137/0314056; Schmidt M. W., 2013, MINIMIZING FINITE SU; Shalev-Shwartz S., 2013, ADV NEURAL INFORM PR, P378; Shalev-Shwartz S, 2011, MATH PROGRAM, V127, P3, DOI 10.1007/s10107-010-0420-4; Shalev- Shwartz Shai, 2013, JMLR; Shalev- Shwartz Shai, 2013, TECHNICAL REPORT; Zhang, 2013, ADV NEURAL INFORM PR, P315	17	38	39	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701011
C	Pachitariu, M; Steinmetz, N; Kadir, S; Carandini, M; Harris, K		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Pachitariu, Marius; Steinmetz, Nick; Kadir, Shabnam; Carandini, Matteo; Harris, Kenneth			Fast and accurate spike sorting of high-channel count probes with KiloSort	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					New silicon technology is enabling large-scale electrophysiological recordings in vivo from hundreds to thousands of channels. Interpreting these recordings requires scalable and accurate automated methods for spike sorting, which should minimize the time required for manual curation of the results. Here we introduce KiloSort, a new integrated spike sorting framework that uses template matching both during spike detection and during spike clustering. KiloSort models the electrical voltage as a sum of template waveforms triggered on the spike times, which allows overlapping spikes to be identified and resolved. Unlike previous algorithms that compress the data with PCA, KiloSort operates on the raw data which allows it to construct a more accurate model of the waveforms. Processing times are faster than in previous algorithms thanks to batch-based optimization on GPUs. We compare KiloSort to an established algorithm and show favorable performance, at much reduced processing times. A novel post-clustering merging step based on the continuity of the templates further reduced substantially the number of manual operations required on this data, for the neurons with near-zero error rates, paving the way for fully automated spike sorting of multichannel electrode recordings.	[Pachitariu, Marius; Steinmetz, Nick; Kadir, Shabnam; Carandini, Matteo; Harris, Kenneth] UCL, London, England	University of London; University College London	Pachitariu, M (corresponding author), UCL, London, England.	ucgtmpa@ucl.ac.uk	Carandini, Matteo/ABD-8296-2021	Carandini, Matteo/0000-0003-4880-7682; Harris, Kenneth/0000-0002-5930-6456				Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Einevoll GT, 2012, CURR OPIN NEUROBIOL, V22, P11, DOI 10.1016/j.conb.2011.10.001; Ekanadham C, 2014, J NEUROSCI METH, V222, P47, DOI 10.1016/j.jneumeth.2013.10.001; Franke F, 2015, J NEUROPHYSIOL, V114, P2535, DOI 10.1152/jn.00993.2014; Harris KD, 2000, J NEUROPHYSIOL, V84, P401, DOI 10.1152/jn.2000.84.1.401; Hill DN, 2011, J NEUROSCI, V31, P8699, DOI 10.1523/JNEUROSCI.0971-11.2011; Neto Joana P, 2016, BIORXIV; Pillow JW, 2013, PLOS ONE, V8, DOI 10.1371/journal.pone.0062123; Quiroga RQ, 2012, CURR BIOL, V22, pR45, DOI 10.1016/j.cub.2011.11.005; Rodriguez A, 2014, SCIENCE, V344, P1492, DOI 10.1126/science.1242072; Rossant C, 2016, NAT NEUROSCI, V19, P634, DOI 10.1038/nn.4268	11	38	38	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702073
C	Bhatia, K; Jain, P; Kar, P		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Bhatia, Kush; Jain, Prateek; Kar, Purushottam			Robust Regression via Hard Thresholding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of Robust Least Squares Regression (RLSR) where several response variables can be adversarially corrupted. More specifically, for a data matrix X is an element of R-pxn and an underlying model w*, the response vector is generated as y = X(T)w* + b where b is an element of R-n is the corruption vector supported over at most C . n coordinates. Existing exact recovery results for RLSR focus solely on L-1-penalty based convex formulations and impose relatively strict model assumptions such as requiring the corruptions b to be selected independently of X. In this work, we study a simple hard-thresholding algorithm called TORRENT which, under mild conditions on X, can recover w* exactly even if b corrupts the response variables in an adversarial manner, i.e. both the support and entries of b are selected adversarially after observing X and w*. Our results hold under deterministic assumptions which are satisfied if X is sampled from any sub-Gaussian distribution. Finally unlike existing results that apply only to a fixed w*, generated independently of X, our results are universal and hold for any w* is an element of R-p. Next, we propose gradient descent-based extensions of TORRENT that can scale efficiently to large scale problems, such as high dimensional sparse recovery. and prove similar recovery guarantees for these extensions. Empirically we find TORRENT, and more so its extensions, offering significantly faster recovery than the state-of-the-art L-1 solvers. For instance, even on moderate-sized datasets (with p = 50K) with around 40% corrupted responses, a variant of our proposed method called TORRENT-HYB is more than 20x faster than the best L-1 solver.	[Bhatia, Kush; Jain, Prateek] Microsoft Res, Bengaluru, Karnataka, India; [Kar, Purushottam] Indian Inst Technol Kanpur, Kanpur, Uttar Pradesh, India	Indian Institute of Technology System (IIT System); Indian Institute of Technology (IIT) - Kanpur	Bhatia, K (corresponding author), Microsoft Res, Bengaluru, Karnataka, India.	t-kushb@microsoft.com; prajain@microsoft.com; purushot@cse.iitk.ac.in	Kar, Purushottam/W-8113-2019	Kar, Purushottam/0000-0003-2096-5267				Blumensath T, 2011, IEEE T INFORM THEORY, V57, P4660, DOI 10.1109/TIT.2011.2146550; Blumensath T, 2009, APPL COMPUT HARMON A, V27, P265, DOI 10.1016/j.acha.2009.04.002; Chen Yudong, 2013, 30 INT C MACH LEARN; Garg Rahul, 2009, 26 INT C MACH LEARN; Jain Prateek, 2014, 28 ANN C NEUR INF PR; Laurent B, 2000, ANN STAT, V28, P1302; Legendre Adrien-Marie, 1805, SOURCE BOOK MATH, P576; McWilliams Brian, 2014, 28 ANN C NEUR INF PR; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2036, DOI 10.1109/TIT.2012.2232347; Rousseeuw P.J., 1987, ROBUST REGRESSION OU; Rousseeuw PJ, 2006, DATA MIN KNOWL DISC, V12, P29, DOI 10.1007/s10618-005-0024-4; ROUSSEEUW PJ, 1984, J AM STAT ASSOC, V79, P871, DOI 10.2307/2288718; She Yiyuan, ARXIV10062592STATME; Studer C, 2012, IEEE T INFORM THEORY, V58, P3115, DOI 10.1109/TIT.2011.2179701; Wright J, 2010, IEEE T INFORM THEORY, V56, P3540, DOI 10.1109/TIT.2010.2048473; Wright J, 2009, IEEE T PATTERN ANAL, V31, P210, DOI 10.1109/TPAMI.2008.79; Yang Allen Y., 2012, CORR	18	38	38	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103048
C	Dauphin, YN; Pascanu, R; Gulcehre, C; Cho, K; Ganguli, S; Bengio, Y		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Dauphin, Yann N.; Pascanu, Razvan; Gulcehre, Caglar; Cho, Kyunghyun; Ganguli, Surya; Bengio, Yoshua			Identifying and attacking the saddle point problem in high-dimensional non-convex optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				GRADIENT DESCENT	A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.	[Dauphin, Yann N.; Pascanu, Razvan; Gulcehre, Caglar; Cho, Kyunghyun; Bengio, Yoshua] Univ Montreal, Montreal, PQ, Canada; [Ganguli, Surya] Stanford Univ, Stanford, CA 94305 USA	Universite de Montreal; Stanford University	Dauphin, YN (corresponding author), Univ Montreal, Montreal, PQ, Canada.	dauphiya@iro.umontreal.ca; r.pascanu@gmail.com; gulcehrc@iro.umontreal.ca; kyunghyun.cho@umontreal.ca; sganguli@standford.edu; yoshua.bengio@umontreal.ca			CIFAR; Canada Research Chairs; DeepMind Google Fellowship; Burroughs Wellcome Foundation; Sloan Foundation	CIFAR(Canadian Institute for Advanced Research (CIFAR)); Canada Research Chairs(Canada Research ChairsCGIAR); DeepMind Google Fellowship(Google Incorporated); Burroughs Wellcome Foundation(Burroughs Wellcome Fund); Sloan Foundation(Alfred P. Sloan Foundation)	We would like to thank the developers of Theano (Bergstra et al., 2010; Bastien et al., 2012). We would also like to thank CIFAR, and Canada Research Chairs for funding, and Compute Canada, and Calcul Quebec for providing computational resources. Razvan Pascanu is supported by a DeepMind Google Fellowship. Surya Ganguli thanks the Burroughs Wellcome and Sloan Foundations for support.	[Anonymous], 2010, PYTH SCI COMP C; [Anonymous], 2014, INT C LEARN REPR; BALDI P, 1989, NEURAL NETWORKS, V2, P53, DOI 10.1016/0893-6080(89)90014-2; Bastien F., 2012, THEANO NEW FEATURES; BENGIO Y, 1994, IEEE T NEURAL NETWOR, V5, P157, DOI 10.1109/72.279181; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bray AJ, 2007, PHYS REV LETT, V98, DOI 10.1103/PhysRevLett.98.150201; Callahan J., 2010, UNDERGRADUATE TEXTS; Fyodorov YV, 2007, J STAT PHYS, V129, P1081, DOI 10.1007/s10955-007-9386-x; Inoue M, 2003, J PHYS SOC JPN, V72, P805, DOI 10.1143/JPSJ.72.805; Martens J., 2010, P 27 INT C MACH LEAR, P735; Mizutani E., 2010, ADV NEURAL INFORM PR, V23, P1669; Murray W., 2010, TECHNICAL REPORT; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Parisi G., 2007, ARXIV07060094; Pascanu R., 2014, 14054604 ARXIV; Pascanu R., 2014, INT C LEARN REPR; Pascanu R., 2014, ICML; Rasmussen CE, 2005, ADAPT COMPUT MACH LE, P1; Rattray M, 1998, PHYS REV LETT, V81, P5461, DOI 10.1103/PhysRevLett.81.5461; Sohl-Dickstein J., 2014, ICML; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Vinyals O., 2012, AISTATS; WIGNER EP, 1958, ANN MATH, V67, P325, DOI 10.2307/1970008	25	38	38	7	16	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102047
C	Michalski, V; Memisevic, R; Konda, K		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Michalski, Vincent; Memisevic, Roland; Konda, Kishore			Modeling Deep Temporal Dependencies with Recurrent "Grammar Cells"	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					We propose modeling time series by representing the transformations that take a frame at time t to a frame at time t+1. To this end we show how a bi-linear model of transformations, such as a gated autoencoder, can be turned into a recurrent network, by training it to predict future frames from the current one and the inferred transformation using backprop-through-time. We also show how stacking multiple layers of gating units in a recurrent pyramid makes it possible to represent the "syntax" of complicated time series, and that it can outperform standard recurrent neural networks in terms of prediction accuracy on a variety of tasks.	[Michalski, Vincent; Konda, Kishore] Goethe Univ Frankfurt, Frankfurt, Germany; [Memisevic, Roland] Univ Montreal, Montreal, PQ, Canada	Goethe University Frankfurt; Universite de Montreal	Michalski, V (corresponding author), Goethe Univ Frankfurt, Frankfurt, Germany.	vmichals@rz.uni-frankfurt.de; roland.memisevic@umontreal.ca; konda.kishorereddy@gmail.com			German Federal Ministry of Education and Research (BMBF) [01GQ0841]; NSERC Discovery grant; Google faculty research award	German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); NSERC Discovery grant(Natural Sciences and Engineering Research Council of Canada (NSERC)); Google faculty research award(Google Incorporated)	This work was supported by the German Federal Ministry of Education and Research (BMBF) in project 01GQ0841 (BFNT Frankfurt), by an NSERC Discovery grant and by a Google faculty research award.	Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Hochreiter S, 1991, UNTERSUCHUNGEN DYNAM, V91; LeCun Y., 2001, P 2001 IEEE C COMP V; Luttinen Jaakko, 2014, Machine Learning and Knowledge Discovery in Databases. European Conference, ECML PKDD 2014. Proceedings: LNCS 8725, P338, DOI 10.1007/978-3-662-44851-9_22; Martin D., 2001, P ICCV, P416, DOI DOI 10.1109/ICCV.2001.937655; Memisevic R., 2013, P 30 INT C MACH LEAR; Memisevic R., 2007, P 2007 IEEE C COMP V; Memisevic R, 2013, IEEE T PATTERN ANAL, V35, P1829, DOI 10.1109/TPAMI.2013.53; Memisevic R, 2011, IEEE I CONF COMP VIS, P1591, DOI 10.1109/ICCV.2011.6126419; Memisevic R, 2010, NEURAL COMPUT, V22, P1473, DOI 10.1162/neco.2010.01-09-953; Michalski V., 2013, THESIS; Olshausen B. A., 2007, BILINEAR MODELS NATU; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P234, DOI 10.1162/neco.1992.4.2.234; Socher Richard, P 2013 C EMP METH NA; Sutskever I, 2011, GENERATING TEXT RECU, P8; Sutskever I., 2008, ADV NEURAL INFORM PR, V21, P1601; Taylor G.W., 2007, ADV NEURAL INFORM PR, P1345; Taylor GW, 2011, J MACH LEARN RES, V12, P1025; WERBOS PJ, 1988, NEURAL NETWORKS, V1, P339, DOI 10.1016/0893-6080(88)90007-X; Wiskott L, 2002, NEURAL COMPUT, V14, P715, DOI 10.1162/089976602317318938	22	38	40	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102110
C	Kohlmorgen, J; Lemm, S		Dietterich, TG; Becker, S; Ghahramani, Z		Kohlmorgen, J; Lemm, S			A dynamic HMM for on-line segmentation of sequential data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				EXPERTS	We propose a novel method for the analysis of sequential data that exhibits an inherent mode switching. In particular, the data might be a non-stationary time series from a dynamical system that switches between multiple operating modes. Unlike other approaches, our method processes the data incrementally and without any training of internal parameters. We use an HMM with a dynamically changing number of states and an on-line variant of the Viterbi algorithm that performs an unsupervised segmentation and classification of the data on-the-fly, i.e. the method is able to process incoming data in real-time. The main idea of the approach is to track and segment changes of the probability density of the data in a sliding window on the incoming data stream. The usefulness of the algorithm is demonstrated by an application to a switching dynamical system.	Fraunhofer FIRST IDA, D-12489 Berlin, Germany	Fraunhofer Gesellschaft; Fraunhofer Institute Center Schloss Birlinghoven	Kohlmorgen, J (corresponding author), Fraunhofer FIRST IDA, Kekulestr 7, D-12489 Berlin, Germany.	jek@first.fraunhofer.de; lemm@first.fraunhofer.de						Bellman RE, 1957, DYNAMIC PROGRAMMING; Bengio Y., 1995, Advances in Neural Information Processing Systems 7, P427; Bengio Y., 1999, Neural Computing Surveys, V2; Bishop, 1995, NEURAL NETWORKS PATT; Husmeier D, 2000, NEURAL NETWORKS, V13, P287, DOI 10.1016/S0893-6080(00)00018-6; Kehagias A, 1997, NEURAL COMPUT, V9, P1691, DOI 10.1162/neco.1997.9.8.1691; Kohlmorgen J, 2000, BIOL CYBERN, V83, P73, DOI 10.1007/s004220000144; KOHLMORGEN J, IN PRESS; Liehr S, 1999, THEOR BIOSCI, V118, P246; MACKEY MC, 1977, SCIENCE, V197, P287, DOI 10.1126/science.267326; PACKARD NH, 1980, PHYS REV LETT, V45, P712, DOI 10.1103/PhysRevLett.45.712; Pawelzik K, 1996, NEURAL COMPUT, V8, P340, DOI 10.1162/neco.1996.8.2.340; RABINER LR, 1989, P IEEE, V77, P257, DOI 10.1109/5.18626; Ramamurti V, 1999, IEEE T NEURAL NETWOR, V10, P152, DOI 10.1109/72.737501	14	38	39	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						793	800						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100099
C	Zemel, RS; Pitassi, T		Leen, TK; Dietterich, TG; Tresp, V		Zemel, RS; Pitassi, T			A gradient-based boosting algorithm for regression problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					In adaptive boosting, several weak learners trained sequentially are combined to boost the overall algorithm performance. Recently adaptive boosting methods for classification problems have been derived as gradient descent algorithms. This formulation justifies key elements and parameters in the methods, all chosen to optimize a single common. objective function. We propose an analogous formulation for adaptive boosting of regression problems, utilizing a novel objective function that leads to a simple boosting algorithm. We prove that this method reduces training error, and compare its performance to other regression methods.	Univ Toronto, Dept Comp Sci, Toronto, ON, Canada	University of Toronto	Zemel, RS (corresponding author), Univ Toronto, Dept Comp Sci, Toronto, ON, Canada.							BORDLEY RF, 1982, MANAGE SCI, V28, P1137, DOI 10.1287/mnsc.28.10.1137; BREIMAN L, 1997, 504 TR UC BERK STAT; DUFFY N, 2000, P COLT, V13; Freund Y, 1997, J COMPUT SYST SCI, V55, P119, DOI 10.1006/jcss.1997.1504; Friedman J. H., 1999, GREEDY FUNCTION APPR; FRIEDMAN JH, 1999, IN PRESS ANN STAT; GEMAN S, 1992, NEURAL COMPUT, V4, P1, DOI 10.1162/neco.1992.4.1.1; Hastie T.J., 1990, GEN ADDITIVE MODELS, V43; Heskes T, 1998, NEURAL COMPUT, V10, P1425, DOI 10.1162/089976698300017232; HINTON GE, 2000, 2000004 NUTR U COLL; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; KARAKOULAS G, 1999, ADV LARGE MARGIN CLA; Krogh A, 1995, NIPS, P7; MASON L, 1999, NIPS, V11; RATSCH G, 2000, P COLT, V13; SCHAPIRE RE, 1990, MACH LEARN, V5, P197, DOI 10.1023/A:1022648800760; SCHAPIRE RE, 1998, P COLT, V11	17	38	41	3	6	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						696	702						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800099
C	TESAURO, G		MOODY, JE; HANSON, SJ; LIPPMANN, RP		TESAURO, G			PRACTICAL ISSUES IN TEMPORAL DIFFERENCE LEARNING	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	38	38	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						259	266						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00032
C	Wu, DX; Xia, ST; Wang, YS		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Wu, Dongxian; Xia, Shu-Tao; Wang, Yisen			Adversarial Weight Perturbation Helps Robust Generalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					The study on improving the robustness of deep neural networks against adversarial examples grows rapidly in recent years. Among them, adversarial training is the most promising one, which flattens the input loss landscape (loss change with respect to input) via training on adversarially perturbed examples. However, how the widely used weight loss landscape (loss change with respect to weight) performs in adversarial training is rarely explored. In this paper, we investigate the weight loss landscape from a new perspective, and identify a clear correlation between the flatness of weight loss landscape and robust generalization gap. Several well-recognized adversarial training improvements, such as early stopping, designing new objective functions, or leveraging unlabeled data, all implicitly flatten the weight loss landscape. Based on these observations, we propose a simple yet effective Adversarial Weight Perturbation (AWP) to explicitly regularize the flatness of weight loss landscape, forming a double-perturbation mechanism in the adversarial training framework that adversarially perturbs both inputs and weights. Extensive experiments demonstrate that AWP indeed brings flatter weight loss landscape and can be easily incorporated into various existing adversarial training methods to further boost their adversarial robustness.	[Wu, Dongxian; Xia, Shu-Tao] Tsinghua Univ, Beijing, Peoples R China; [Wang, Yisen] Peking Univ, Sch EECS, Key Lab Machine Percept MoE, Beijing, Peoples R China; [Wu, Dongxian; Xia, Shu-Tao] PCL Res Ctr Networks & Commun, Peng Cheng Lab, Shenzhen, Guangdong, Peoples R China	Tsinghua University; Peking University; Peng Cheng Laboratory	Wang, YS (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MoE, Beijing, Peoples R China.	wu-dx16@mails.tsinghua.edu.cn; yisen.wang@pku.edu.cn	Wu, Dongxian/AAE-7731-2022	WU, Dongxian/0000-0002-5147-3516	National Natural Science Foundation of China [62006153, 61771273]; CCF-Baidu Open Fund [OF2020002]; National Key Research and Development Program of China [2018YFB1800204]; R&D Program of Shenzhen [JCYJ20180508152204044]; project PCL Future Greater-Bay Area Network Facilities for Large-scale Experiments and Applications [LZC0019]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); CCF-Baidu Open Fund; National Key Research and Development Program of China; R&D Program of Shenzhen; project PCL Future Greater-Bay Area Network Facilities for Large-scale Experiments and Applications	Yisen Wang is partially supported by the National Natural Science Foundation of China under Grant 62006153, and CCF-Baidu Open Fund (OF2020002). Shu-Tao Xia is partially supported by the National Key Research and Development Program of China under Grant 2018YFB1800204, the National Natural Science Foundation of China under Grant 61771273, the R&D Program of Shenzhen under Grant JCYJ20180508152204044, and the project PCL Future Greater-Bay Area Network Facilities for Large-scale Experiments and Applications (LZC0019).	Andriushchenko Maksym, 2019, ARXIV191200049; [Anonymous], 2017, ICLR; Athalye Anish, 2018, ICML; Bai Yang, 2019, ICCV; Bai Yang, 2020, ECCV; Carlini Nicholas, 2017, S P; Carmon Yair, 2019, NEURIPS; Chaudhari P., 2017, ICLR; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Croce F, 2020, P 37 INT C MACH LEAR; Croce F, 2019, 25TH AMERICAS CONFERENCE ON INFORMATION SYSTEMS (AMCIS 2019); Devlin J., 2019, P 2019 C N AM CHAPT, V1, P4171; DeVries T., 2017, P 2017 COMPUTER VISI; Foret Pierre, 2020, ARXIV201001412; Goodfellow I. J., 2015, ICLR; He K., 2016, ECCV; He Zhezhi, 2019, CVPR; Hendrycks Dan, 2019, ICML; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Khan M. E., 2018, ICML; KHIM J, 2018, ARXIV PREPRINT ARXIV; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Liu Chen, 2020, NEURIPS; LopezPaz David, 2018, ICLR; Loshchilov I., 2017, ICLR; Lyu Chunchuan, 2015, ICDM; Ma Xingjun, 2018, ICLR; Madry Aleksander, 2018, ICML; Montasser Omar, 2019, COLT; MOOSAVIDEZFOOLI SM, 2019, CVPR; Najafi Amir, 2019, NEURIPS; Nakkiran P., 2019, ARXIV190100532; Netzer Y., 2011, P NIPS WORKSH DEEP L; Neyshabur Behnam, 2017, NEURIPS; Norton Matthew, 2019, ARXIV191010844; Papernot N., 2017, ASIA CCS; Papernot Nicolas, 2016, EUROS P; Papernot Nicolas, 2016, S P; Prabhu Vinay Uday, 2019, ARXIV190709061; Qin Chongli, 2019, NEURIPS; RICE L, 2020, ICML; Ross Andrew Slavin, 2018, AAAI C ART INT; Schmidt L., 2018, NEURIPS; Simonyan K., 2015, ARXIV PREPRINT ARXIV; Smith Leslie N, 2017, WACV; Taylor G., 2018, NEURIPS; Tramer F., 2018, ICLR; Tsipras Dimitris, 2019, ICLR; Uesato Jonathan, 2019, NEURIPS; Uesato Jonathan, 2018, ICML; Wang X., 2020, PROC INT C LEARN REP, P1; Wang Y., 2017, ARXIV170207793; Wang Yisen, 2019, ICML; Wang Yisen, 2020, ICLR; Wen Y., 2018, ICLR; WONG E, 2020, ICLR; Wu Dongxian, 2020, ICLR; Xu W., 2017, P 25 ANN NETW DISTR, DOI DOI 10.14722/NDSS.2018.23198; Yin D., 2019, ICML; Yu F., 2018, ARXIV181000144; Zagoruyko S, 2016, P BRIT MACH VIS C BM, DOI [10.5244/C.30.87, DOI 10.5244/C.30.87]; Zhai R., 2019, ARXIV190600555; Zhang H., 2019, ICML	63	37	37	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													12	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000071
C	Micaelli, P; Storkey, A		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Micaelli, Paul; Storkey, Amos			Zero-shot Knowledge Transfer via Adversarial Belief Matching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Performing knowledge transfer from a large teacher network to a smaller student is a popular task in modern deep learning applications. However, due to growing dataset sizes and stricter privacy regulations, it is increasingly common not to have access to the data that was used to train the teacher. We propose a novel method which trains a student to match the predictions of its teacher without using any data or metadata. We achieve this by training an adversarial generator to search for images on which the student poorly matches the teacher, and then using them to train the student. Our resulting student closely approximates its teacher for simple datasets like SVHN, and on CIFAR10 we improve on the stateof-the-art for few-shot distillation (with 100 images per class), despite using no data. Finally, we also propose a metric to quantify the degree of belief matching between teacher and student in the vicinity of decision boundaries, and observe a significantly higher match between our zero-shot student and the teacher, than between a student distilled with real data and the teacher.	[Micaelli, Paul; Storkey, Amos] Univ Edinburgh, Edinburgh, Midlothian, Scotland	University of Edinburgh	Micaelli, P (corresponding author), Univ Edinburgh, Edinburgh, Midlothian, Scotland.	paul.micaelli@ed.ac.uk; a.storkey@ed.ac.uk			UK Engineering and Physical Sciences Research Council [EP/L016427/1]; University of Edinburgh; Huawei DDMPLab Innovation Research Grant; EPSRC Centre for Doctoral Training in Data Science	UK Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh; Huawei DDMPLab Innovation Research Grant; EPSRC Centre for Doctoral Training in Data Science(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors would like to thank Benjamin Rhodes, Miguel Jaques, Luke Darlow, Antreas Antoniou, Elliot Crowley, Joseph Mellor and Etienne Toussaint for their useful feedback throughout this project. Our work was supported in part by the EPSRC Centre for Doctoral Training in Data Science, funded by the UK Engineering and Physical Sciences Research Council (grant EP/L016427/1) and the University of Edinburgh as well as a Huawei DDMPLab Innovation Research Grant. The opinions expressed and arguments employed herein do not necessarily reflect the official views of these funding bodies.	Ahn S., 2019, ABS190405835 CORR; Bucila C, 2006, P 12 ACM SIGKDD INT, P535, DOI DOI 10.1145/1150402.1150464; Burton PR, 2015, BIOINFORMATICS, V31, P3241, DOI 10.1093/bioinformatics/btv279; Dehghani M., 2017, ABS171102799 CORR; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; FREDRIKSON M, 2015, P 22 ACM SIGSAC C CO, P1322, DOI DOI 10.1145/2810103.2813677; Fredrikson M, 2014, PROCEEDINGS OF THE 23RD USENIX SECURITY SYMPOSIUM, P17; Geirhos R., 2018, ABS181112231 CORR; Gupta S, 2015, PR MACH LEARN RES, V37, P1737; Han S., 2016, P INT C LEARNING REP; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; Hubara Itay, 2016, QUANTIZED NEURAL NET, P3; Ilyas A, 2019, ARXIV190502175; Kimura A., 2018, ARXIV180203039; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Larochelle H., 2008, AAAI, V1, P3; Li T., 2018, ABS181201839 CORR, Vabs/1812.01839; Lopes Raphael Gontijo, 2017, ABS171007535 CORR; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Oliver A, 2018, ADV NEUR IN, V31; Oord A.V.D., 2016, SSW; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Romero Adriana, 2015, ICLR; RUSU A A, 2016, 4 INT C LEARN REPR I, P1; Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sun Chen, 2017, ABS170702968 CORR; Taigman Y, 2014, PROC CVPR IEEE, P1701, DOI 10.1109/CVPR.2014.220; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Tramer F., 2016, ABS160902943 CORR; Wang Tongzhou, 2018, ARXIV181110959; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	40	37	39	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901021
C	Balle, B; Barthe, G; Gaboardi, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Balle, Borja; Barthe, Gilles; Gaboardi, Marco			Privacy Amplification by Subsampling: Tight Analyses via Couplings and Divergences	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SAMPLE COMPLEXITY; BOUNDS	Differential privacy comes equipped with multiple analytical tools for the design of private data analyses. One important tool is the so-called "privacy amplification by subsampling" principle, which ensures that a differentially private mechanism run on a random subsample of a population provides higher privacy guarantees than when run on the entire population. Several instances of this principle have been studied for different random subsampling methods, each with an ad-hoc analysis. In this paper we present a general method that recovers and improves prior analyses, yields lower bounds and derives new instances of privacy amplification by subsampling. Our method leverages a characterization of differential privacy as a divergence which emerged in the program verification community. Furthermore, it introduces new tools, including advanced joint convexity and privacy profiles, which might be of independent interest.	[Balle, Borja] Amazon Res, Cambridge, England; [Barthe, Gilles] IMDEA Software Inst, Madrid, Spain; [Gaboardi, Marco] SUNY Buffalo, Buffalo, NY USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Balle, B (corresponding author), Amazon Res, Cambridge, England.	pigem@amazon.co.uk; gilles.barthe@imdea.org; gaboardi@buffalo.edu						Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; [Anonymous], P 35 INT C MACH LEAR; Barthe G, 2016, PROCEEDINGS OF THE 31ST ANNUAL ACM-IEEE SYMPOSIUM ON LOGIC IN COMPUTER SCIENCE (LICS 2016), P749, DOI 10.1145/2933575.2934554; Barthe G, 2012, POPL 12: PROCEEDINGS OF THE 39TH ANNUAL ACM SIGPLAN-SIGACT SYMPOSIUM ON PRINCIPLES OF PROGRAMMING LANGUAGES, P97; Bartolomeo G, 2013, IDENTIFICATION AND MANAGEMENT OF DISTRIBUTED DATA: NGN, CONTENT-CENTRIC NETWORKS AND THE WEB, P49; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Beimel A, 2014, MACH LEARN, V94, P401, DOI 10.1007/s10994-013-5404-1; Beimel A, 2010, LECT NOTES COMPUT SC, V5978, P437, DOI 10.1007/978-3-642-11799-2_26; Beimel Amos, 2013, INNOVATIONS THEORETI, P97, DOI [10.1145/2422436.2422450, DOI 10.1145/2422436.2422450]; Bun M, 2016, LECT NOTES COMPUT SC, V9985, P635, DOI 10.1007/978-3-662-53641-4_24; Bun M, 2015, ANN IEEE SYMP FOUND, P634, DOI 10.1109/FOCS.2015.45; Bun Mark, 2018, S THEOR COMP STOC; Chaudhuri K, 2006, LECT NOTES COMPUT SC, V4117, P198; Dwork C., 2016, ARXIV PREPRINT ARXIV; Dwork C, 2013, FOUND TRENDS THEOR C, V9, P211, DOI 10.1561/0400000042; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Jalko J, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Kasiviswanathan SP, 2011, SIAM J COMPUT, V40, P793, DOI 10.1137/090756090; Li N., 2012, ASIA CCS, P32, DOI 10.1145/2414456.2414474; Mironov I, 2017, P IEEE CSFW, P263, DOI 10.1109/CSF.2017.11; Murtagh J, 2016, LECT NOTES COMPUT SC, V9562, P157, DOI 10.1007/978-3-662-49096-9_7; Osterreicher F, 2002, RGMIA RES REP COLL; Park M., 2016, CORR; Sason I, 2016, IEEE T INFORM THEORY, V62, P5973, DOI 10.1109/TIT.2016.2603151; Ullman J., 2017, CS7880 RIGOROUS APPR; Vadhan S, 2017, INFORM SEC CRYPT TEX, P347, DOI 10.1007/978-3-319-57048-8_7; Wang YX, 2015, PR MACH LEARN RES, V37, P2493; Wang Yu-Xiang, 2019, P 22 INT C ART INT S; Wang YJ, 2016, J MACH LEARN RES, V17, P1	30	37	38	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000075
C	Dean, S; Mania, H; Matni, N; Recht, B; Tu, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Dean, Sarah; Mania, Horia; Matni, Nikolai; Recht, Benjamin; Tu, Stephen			Regret Bounds for Robust Adaptive Control of the Linear Quadratic Regulator	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We consider adaptive control of the Linear Quadratic Regulator (LQR), where an unknown linear system is controlled subject to quadratic costs. Leveraging recent developments in the estimation of linear systems and in robust controller synthesis, we present the first provably polynomial time algorithm that provides high probability guarantees of sub-linear regret on this problem. We further study the interplay between regret minimization and parameter estimation by proving a lower bound on the expected regret in terms of the exploration schedule used by any algorithm. Finally, we conduct a numerical study comparing our robust adaptive algorithm to other methods from the adaptive LQR literature, and demonstrate the flexibility of our proposed method by extending it to a demand forecasting problem subject to state constraints.	[Dean, Sarah; Mania, Horia; Matni, Nikolai; Recht, Benjamin; Tu, Stephen] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Dean, S (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.				NSF Graduate Research Fellowship [DGE 1752814]; NSF CISE Expeditions Award [CCF-1730628]; DHS Award [HSHQDC-16-3-00083]; NSF [CCF-1359814]; ONR [N00014-17-1-2191, N00014-17-1-2401, N00014-17-1-2502]; DARPA Fundamental Limits of Learning (Fun LoL) Program; DARPA Lagrange Program; Amazon AWS AI Research Award	NSF Graduate Research Fellowship(National Science Foundation (NSF)); NSF CISE Expeditions Award; DHS Award; NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); DARPA Fundamental Limits of Learning (Fun LoL) Program; DARPA Lagrange Program; Amazon AWS AI Research Award	We thank the anonymous reviewers for their feedback, which improved the clarity of our presentation. SD is supported by an NSF Graduate Research Fellowship under Grant No. DGE 1752814. As part of the RISE lab, HM is generally supported in part by NSF CISE Expeditions Award CCF-1730628, DHS Award HSHQDC-16-3-00083, and gifts from Alibaba, Amazon Web Services, Ant Financial, CapitalOne, Ericsson, GE, Google, Huawei, Intel, IBM, Microsoft, Scotiabank, Splunk and VMware. BR is generously supported in part by NSF award CCF-1359814, ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-17-1-2502, the DARPA Fundamental Limits of Learning (Fun LoL) and Lagrange Programs, and an Amazon AWS AI Research Award.	Abbasi-Yadkori Y., 2011, C LEARN THEOR; Abbasi-Yadkori Yasin, 2015, C UNC ART INT; Abbasi-Yadkori Yasin, 2018, ARXIV180406021; Abeille M., 2017, AISTATS; Bittanti S., 2006, COMMUNICATIONS INFOR, V6; Dean S., 2017, ARXIV171001688; DEAN S, 2018, ARXIV180509388; Faradonbeh M. K. S., 2017, ARXIV171107230; Fazel M., 2018, INT C MACH LEARN; Fiechter C.-N., 1997, C LEARN THEOR; Ibrahimi M., 2012, NEURAL INFORM PROCES; Ioannou P., 1996, ROBUST ADAPTIVE CONT, V1; Krstic M, 1995, NONLINEAR ADAPTIVE C; Matni Nikolai, 2017, IEEE C DEC CONTR; Osband Ian, 2016, ARXIV160802731; Simchowitz M., 2018, C LEARN THEOR; Tu S., 2018, INT C MACH LEARN; Wang Y.-S., 2016, ARXIV161004815; Yi Ouyang, 2017, ARXIV170904047; Zhou K., 1995, ROBUST OPTIMAL CONTR	20	37	37	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304022
C	Fawzi, A; Fawzi, H; Fawzi, O		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Fawzi, Alhussein; Fawzi, Hamza; Fawzi, Omar			Adversarial vulnerability for any classifier	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Despite achieving impressive performance, state-of-the-art classifiers remain highly vulnerable to small, imperceptible, adversarial perturbations. This vulnerability has proven empirically to be very intricate to address. In this paper, we study the phenomenon of adversarial perturbations under the assumption that the data is generated with a smooth generative model. We derive fundamental upper bounds on the robustness to perturbations of any classification function, and prove the existence of adversarial perturbations that transfer well across different classifiers with small risk. Our analysis of the robustness also provides insights onto key properties of generative models, such as their smoothness and dimensionality of latent space. We conclude with numerical experimental results showing that our bounds provide informative baselines to the maximal achievable robustness on several datasets.	[Fawzi, Alhussein] DeepMind, London, England; [Fawzi, Hamza] Univ Cambridge, Dept Appl Math & Theoret Phys, Cambridge, England; [Fawzi, Omar] Univ Lyon, ENS Lyon, CNRS, UCBL,LIP, F-69342 Lyon 07, France	University of Cambridge; Centre National de la Recherche Scientifique (CNRS); Ecole Normale Superieure de Lyon (ENS de LYON)	Fawzi, A (corresponding author), DeepMind, London, England.	afawzi@google.com; h.fawzi@damtp.cam.ac.uk; omar.fawzi@ens-lyon.fr		Fawzi, Omar/0000-0001-8491-0359				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, ARXIV171209196; Arjovsky M, 2017, PR MACH LEARN RES, V70; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Carlini Nicholas, 2017, P 10 ACM WORKSHOP AR, P3, DOI [10.1145/3128572.3140444, DOI 10.1145/3128572.3140444]; Chicco D., 2014, P 5 ACM C BIOINF COM, P533; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Cisse M, 2017, PR MACH LEARN RES, V70; Dvijotham K., 2018, P UNC ART INT UAI, P162; Elsayed GF, 2018, ADV NEUR IN, V31; Fawzi A., 2015, ARXIV150202590; Fawzi Alhussein, 2016, ADV NEURAL INFORM PR; Gilmer Justin, 2018, ARXIV180102774; Goodfellow I.J., 2015, ARXIV PREPRINT ARXIV; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS 2017; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hein M, 2017, NIPS 17; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kos J., 2017, ARXIV170206832; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liu Yanpei, 2016, ARXIV161102770; Madry Aleksander, 2017, ARXIV; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Papernot N., 2015, ARXIV151104508; Peck J, 2017, ADV NEURAL INFORM PR, P804; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Raghunathan Aditi, 2018, ARXIV180109344; Rauber J., 2017, ROBUST VISION BENCHM; Samangouei Pouya, 2018, ARXIV180506605; Sinha A, 2017, ARXIV171010571; Spencer M, 2015, IEEE ACM T COMPUT BI, V12, P103, DOI 10.1109/TCBB.2014.2343960; Tanay T., 2016, ARXIV PREPRINT ARXIV; Uesato J, 2018, PR MACH LEARN RES, V80; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	42	37	37	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301019
C	He, L; Bian, A; Jaggi, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		He, Lie; Bian, An; Jaggi, Martin			COLA: Decentralized Linear Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DISTRIBUTED OPTIMIZATION; CONVERGENCE; ALGORITHM	Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run on-device, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices.	[He, Lie; Jaggi, Martin] Ecole Polytech Fed Lausanne, Lausanne, Switzerland; [Bian, An] Swiss Fed Inst Technol, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne; Swiss Federal Institutes of Technology Domain; ETH Zurich	He, L (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	lie.he@epfl.ch; ybian@inf.ethz.ch; martin.jaggi@epfl.ch		Jaggi, Martin/0000-0003-1579-5558	SNSF [200021_175796]; Microsoft Research JRC project 'Coltrain'; Google Focused Research Award	SNSF(Swiss National Science Foundation (SNSF)); Microsoft Research JRC project 'Coltrain'(Microsoft); Google Focused Research Award(Google Incorporated)	We thank Prof. Bharat K. Bhargava for fruitful discussions. We acknowledge funding from SNSF grant 200021_175796, Microsoft Research JRC project 'Coltrain', as well as a Google Focused Research Award.	Agarwal A, 2011, ADV NEURAL INFORM PR, P873; Bianchi P, 2016, IEEE T AUTOMAT CONTR, V61, P2947, DOI 10.1109/TAC.2015.2512043; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cevher V, 2014, IEEE SIGNAL PROC MAG, V31, P32, DOI 10.1109/MSP.2014.2329397; Duchi JC, 2012, IEEE T AUTOMAT CONTR, V57, P592, DOI 10.1109/TAC.2011.2161027; Dunner C., 2016, ICML, P783; Dunner Celestine, 2018, ICML 2018 P 35 INT C, P1357; Gargiani M., 2017, THESIS; HASTINGS WK, 1970, BIOMETRIKA, V57, P97, DOI 10.2307/2334940; Jaggi M., 2014, ADV NEURAL INF PROCE, V4, P3068; Jakovetic D, 2012, 2012 20TH TELECOMMUNICATIONS FORUM (TELFOR), P867, DOI 10.1109/TELFOR.2012.6419345; Konecn J., 2016, ARXIV161005492; Lee C.-P., 2018, ACM INT C KNOWL DISC; Lee C. P., 2017, ARXIV170903043; Lian XR, 2017, ADV NEUR IN, V30; Lian Xiangru, 2018, ICML 2018 P 35 INT C; Ma CX, 2015, PR MACH LEARN RES, V37, P1973; McMahan Brendan, 2015, ARXIV151103575; McMahan HB, 2017, PR MACH LEARN RES, V54, P1273; Mokhtari A, 2016, J MACH LEARN RES, V17; Nedic A, 2017, SIAM J OPTIMIZ, V27, P2597, DOI 10.1137/16M1084316; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Reddi S.J., 2016, FAST; Rockafellar R. T., 1970, CONVEX ANAL; Scaman K., 2018, ARXIV180600291; Scaman K, 2017, PR MACH LEARN RES, V70; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Shi W, 2014, IEEE T SIGNAL PROCES, V62, P1750, DOI 10.1109/TSP.2014.2304432; Sirb B, 2018, SIAM J OPTIMIZ, V28, P1232, DOI 10.1137/16M1081257; Smith V, 2018, J MACH LEARN RES, V18; Smith V, 2017, ADV NEUR IN, V30; Stich Sebastian U., 2018, NIPS 2018 ADV NEURAL; Tang HL, 2018, PR MACH LEARN RES, V80; Tang Hanlin, 2018, NIPS 2018 ADV NEURAL; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Wang J., 2017, P MACHINE LEARNING R, P1882; Wei Ermin, 2013, O 1 K CONVERGENCE AS; Wu TY, 2018, IEEE T SIGNAL INF PR, V4, P293, DOI 10.1109/TSIPN.2017.2695121; Yang Tianbao, 2013, NIPS 2014 ADV NEUR P, V27; Yuan K, 2016, SIAM J OPTIMIZ, V26, P1835, DOI 10.1137/130943170; Zhang S., 2015, P ADV NEURAL INFORM, P685, DOI DOI 10.1145/3207677.3277958; Zinkevich M., 2010, P ADV NEUR INF PROC, P2595	45	37	37	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304054
C	Jayaraman, B; Wang, LX; Evans, D; Gu, QQ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jayaraman, Bargav; Wang, Lingxiao; Evans, David; Gu, Quanquan			Distributed Learning without Distress: Privacy-Preserving Empirical Risk Minimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Distributed learning allows a group of independent data owners to collaboratively learn a model over their data sets without exposing their private data. We present a distributed learning approach that combines differential privacy with secure multiparty computation. We explore two popular methods of differential privacy, output perturbation and gradient perturbation, and advance the state-of-the-art for both methods in the distributed learning setting. In our output perturbation method, the parties combine local models within a secure computation and then add the required differential privacy noise before revealing the model. In our gradient perturbation method, the data owners collaboratively train a global model via an iterative learning algorithm. At each iteration, the parties aggregate their local gradients within a secure computation, adding sufficient noise to ensure privacy before the gradient updates are revealed. For both methods, we show that the noise can be reduced in the multi-party setting by adding the noise inside the secure computation after aggregation, asymptotically improving upon the best previous results. Experiments on real world data sets demonstrate that our methods provide substantial utility gains for typical privacy requirements.	[Jayaraman, Bargav; Evans, David] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA; [Wang, Lingxiao; Gu, Quanquan] Univ Calif Los Angeles, Dept Comp Sci, Los Angeles, CA 90095 USA	University of Virginia; University of California System; University of California Los Angeles	Jayaraman, B (corresponding author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.	bj4nq@virginia.edu; lingxw@cs.ucla.edu; evans@virginia.edu; qgu@cs.ucla.edu		Wang, Lingxiao/0000-0002-8798-692X	National Science Foundation [1111781, 1717950, 1804603]; Google; Intel; Amazon	National Science Foundation(National Science Foundation (NSF)); Google(Google Incorporated); Intel(Intel Corporation); Amazon	This work was partially supported by the National Science Foundation (Awards #1111781, #1717950, and #1804603) and research awards from Google, Intel, and Amazon.	[Anonymous], ACM SIGSAC C COMP CO; [Anonymous], 2004, USENIX SEC S; [Anonymous], 2006, ANN INT C THEOR APPL; Asuncion A, 2007, UCI MACHINE LEARNING; Bay S. D, 1999, UCI MACHINE LEARNING; Bindschaedler Vincent, 2017, 7 ACM C DAT APPL SEC; Bonawitz Keith, 2017, ACM SIGSAC C COMP CO; BOX GEP, 1958, ANN MATH STAT, V29, P610, DOI 10.1214/aoms/1177706645; Bun Mark, 2016, THEOR CRYPT C; Burkhart Martin, 2010, USENIX SEC S; Carlini N., 2018, 180208232 ARXIV; Chaudhuri K., 2009, ADV NEURAL INFORM PR; Chaudhuri K., 2011, J MACHINE LEARNING R; Chen YR, 2018, INFORM SCIENCES, V451, P34, DOI 10.1016/j.ins.2018.03.061; Damgard Ivan, 2009, INT WORKSH PUBL KEY; Damgard Ivan, 2012, ADV CRYPTOLOGY CRYPT; Doerner Jack, 2017, ABSENTMINDED CRYPTO; Dwork C., 2006, THEOR CRYPT C; Dwork C ., 2008, INT C THEOR APPL MOD; Dwork Cynthia, 2004, ADV CRYPTOLOGY CRYPT; Fredrikson Matthew, 2014, 23 USENIX SEC S; Gascon A., 2017, POPETS, P345; Goldwasser Shafi, 1987, 19 ACM S THEOR COMP; Gupta Trinabh, 2017, C ACM SPEC INT GROUP; Heikkila Mikko, 2017, ARXIV170301106; HOLZER A., 2012, ACM C COMP COMM SEC; Huang Y., 2011, 20 USENIX SEC S; Huang Y, 2012, P IEEE S SECUR PRIV, P272, DOI 10.1109/SP.2012.43; Jain P., 2013, INT C MACH LEARN; Jain Prateek, 2012, 25 ANN C LEARN THEOR; LINDELL Y., 2007, ADV CRYPTOLOGY EUROC; Lindell  Y., 2009, J PRIVACY CONFIDENTI; Lindell Yehuda, 2000, ADV CRYPTOLOGY CRYPT; Ma Xiaobo, 2018, INFORM SCI; Morrison T., 2017, TECHNICAL REPORT; Nikolaenko V, 2013, P IEEE S SECUR PRIV, P334, DOI 10.1109/SP.2013.30; Orlandi Claudio, 2012, ADV CRYPTOLOGY CRYPT; Parsa I., 1998, UCI MACHINE LEARNING; Pathak Manas, 2010, ADV NEURAL INFORM PR; Pinkas Benny, 2009, INT C THEOR APPL CRY; Rajkumar Arun, 2012, ARTIFICIAL INTELLIGE; Ramage Daniel, 2018, 6 INT C LEARN REPR; Rastogi Aseem, 2014, 35 IEEE S SEC PRIV; Shi E, 2017, ACM T ALGORITHMS, V13, DOI 10.1145/3146549; SHOKRI R., 2015, ACM C COMP COMM SEC; Shokri R, 2017, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2017.41; Sridharan K., 2009, ADV NEURAL INFORM PR; Tian L., 2016, NIPS WORKSH PRIV MUL; Vaidya Jaideep, 2008, VLDB J; Wang Di, 2017, ADV NEURAL INFORM PR; Wang Qian, 2018, IEEE T KNOWLEDGE DAT; Wang X., 2016, EMP TOOLKIT EFFICIEN; Wang Xiao, 2017, ACM SIGSAC C COMP CO; Yang Zhiqiang, 2005, SIAM INT C DATA MINI; Yao A. C., 1982, S FDN COMP SCI; Zahur S., 2015, 20151153 CRYPT EPRIN; Zhang Jiaqi, 2017, 26 INT JOINT C ART I	57	37	38	5	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000081
C	Lazic, N; Lu, T; Boutilier, C; Ryu, M; Wong, E; Roy, B; Imwalle, G		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lazic, Nevena; Lu, Tyler; Boutilier, Craig; Ryu, Moonkyung; Wong, Eehern; Roy, Binz; Imwalle, Greg			Data center cooling using model-predictive control	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				SYSTEMS	Despite the impressive recent advances in reinforcement learning (RL) algorithms, their deployment to real-world physical systems is often complicated by unexpected events, limited data, and the potential for expensive failures. In this paper, we describe an application of RL "in the wild" to the task of regulating temperatures and airflow inside a large-scale data center (DC). Adopting a data-driven, model-based approach, we demonstrate that an RL agent with little prior knowledge is able to effectively and safely regulate conditions on a server floor after just a few hours of exploration, while improving operational efficiency relative to existing PID controllers.	[Lazic, Nevena; Lu, Tyler; Boutilier, Craig; Ryu, Moonkyung] Google Res, Mountain View, CA 94043 USA; [Wong, Eehern; Roy, Binz; Imwalle, Greg] Google Cloud, Mountain View, CA USA	Google Incorporated	Lazic, N (corresponding author), Google Res, Mountain View, CA 94043 USA.	nevena@google.com; tylerlu@google.com; cboutilier@google.com; mkryu@google.com; ejwong@google.com; binzroy@google.com; gregi@google.com						Abbasi-Yadkori Y., 2015, UAI, P1; Abbasi-Yadkori Yasin, 2011, COMPUTATIONAL LEARNI; Abeille M., 2017, AISTATS; ASTROM KJ, 1965, J MATH ANAL APPL, V10, P174, DOI 10.1016/0022-247X(65)90154-X; ASTROM KJ, 1973, AUTOMATICA, V9, P185, DOI 10.1016/0005-1098(73)90073-3; Barroso Luiz Andre, 2013, SYNTHESIS LECT COMPU, V8, P3; Bittanti S, 2006, COMMUN INF SYST, V6, P299; Box G. E. P., 2015, TIME SERIES ANAL FOR; Chen J, 2000, CONTROL ORIENTED SYS, V19; Dean S., 2017, ARXIV171001688; Feng JJ, 2015, ENERG BUILDINGS, V87, P199, DOI 10.1016/j.enbuild.2014.11.037; Gao J., 2014, MACHINE LEARNING APP; Hardt M., 2016, ARXIV160905191; Hayes M, 1996, STAT DIGITAL SIGNAL; HELMICKI AJ, 1991, IEEE T AUTOMAT CONTR, V36, P1163, DOI 10.1109/9.90229; Ibrahimi M., 2012, ADV NEURAL INFORM PR, P2636; Kelman A., 2011, P 2011 IFAC WORLD C; Kingma DP, 2015, INT C LEARN REPR ICL; Ljung L., 1999, SYSTEM IDENTIFICATIO, V2nd; Ljung Lennart, 1983, THEORY PRACTICE RECU, V5; Ma YD, 2012, IEEE T CONTR SYST T, V20, P796, DOI 10.1109/TCST.2011.2124461; Ma YD, 2012, IEEE CONTR SYST MAG, V32, P44, DOI 10.1109/MCS.2011.2172532; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pearl J., 1988, PROBABILISTIC REASON, DOI 10.1016/B978-0-08-051489-5.50008-4; Pinto L., 2017, CORR; Shehabi A., 2016, TECHNICAL REPORT; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Yi Ouyang, 2017, ARXIV170904047; Zhou R., 2012, ASME INT MECHN ENG C, P1789	29	37	38	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303078
C	Wai, HT; Yang, ZR; Wang, ZR; Hong, MY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wai, Hoi-To; Yang, Zhuoran; Wang, Zhaoran; Hong, Mingyi			Multi-Agent Reinforcement Learning via Double Averaging Primal-Dual Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				POLICY EVALUATION	Despite the success of single-agent reinforcement learning, multi-agent reinforcement learning (MARL) remains challenging due to complex interactions between agents. Motivated by decentralized applications such as sensor networks, swarm robotics, and power grids, we study policy evaluation in MARL, where agents with jointly observed state-action pairs and private local rewards collaborate to learn the value of a given policy. In this paper, we propose a double averaging scheme, where each agent iteratively performs averaging over both space and time to incorporate neighboring gradient information and local reward information, respectively. We prove that the proposed algorithm converges to the optimal solution at a global geometric rate. In particular, such an algorithm is built upon a primal-dual reformulation of the mean squared projected Bellman error minimization problem, which gives rise to a decentralized convex-concave saddle-point problem. To the best of our knowledge, the proposed double averaging primal-dual optimization algorithm is the first to achieve fast finite-time convergence on decentralized convex-concave saddle-point problems.	[Wai, Hoi-To] Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China; [Yang, Zhuoran] Princeton Univ, Princeton, NJ 08544 USA; [Wang, Zhaoran] Northwestern Univ, Evanston, IL USA; [Hong, Mingyi] Univ Minnesota, Minneapolis, MN USA	Chinese University of Hong Kong; Princeton University; Northwestern University; University of Minnesota System; University of Minnesota Twin Cities	Wai, HT (corresponding author), Chinese Univ Hong Kong, Shatin, Hong Kong, Peoples R China.	htwai@se.cuhk.edu.hk; zy6@princeton.edu; zhaoranwang@gmail.com; mhong@umn.edu	Hong, Mingyi/H-6274-2013; Wang, Zhaoran/P-7113-2018		NSF-CMMI [1727757]; AFOSR [15RT0767];  [NSF CCF-BSF 1714672]	NSF-CMMI(National Science Foundation (NSF)); AFOSR(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)); 	The authors would like to thank for the useful comments from three anonymous reviewers. HTW's work was supported by the grant NSF CCF-BSF 1714672. MH's work has been supported in part by NSF-CMMI 1727757, and AFOSR 15RT0767.	[Anonymous], 2017, ARXIV170602275; Arslan G, 2017, IEEE T AUTOMAT CONTR, V62, P1545, DOI 10.1109/TAC.2016.2598476; Callaway DS, 2011, P IEEE, V99, P184, DOI 10.1109/JPROC.2010.2081652; Chambolle A, 2016, MATH PROGRAM, V159, P253, DOI 10.1007/s10107-015-0957-3; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Chen JS, 2012, IEEE T SIGNAL PROCES, V60, P4289, DOI 10.1109/TSP.2012.2198470; Chen Y., 2016, ARXIV161202516; Corke P, 2005, SPRINGER TRAC ADV RO, V15, P234; Cortes J, 2004, IEEE T ROBOTIC AUTOM, V20, P243, DOI 10.1109/TRA.2004.824698; Dai B., 2016, ARXIV160704579; Dai B., 2017, ARXIV171210285; Dai B., 2017, ARXIV171210282, p2017b; Dall'Anese E, 2013, IEEE T SMART GRID, V4, P1464, DOI 10.1109/TSG.2013.2248175; Defazio A., 2014, ADV NEURAL INFORM PR, P1646, DOI DOI 10.5555/2968826.2969010.[0NLINE].AVAILABLE:HTTP://PAPERS.NIPS.CC/PAPER/5258-SAGA-A-FAST-INCREMENTAL-GRADIENT-METH-0D-WITH-SUPP0RT-F0R-N0N-STR0NGLY-C0NVEX-C0MP0SITE-0BJECTIVES.PDF; Du S., 2017, ARXIV170207944; Foerster J., 2017, ARXIV170208887; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Gupta Jayesh K., 2017, Autonomous Agents and Multiagent Systems, AAMAS 2017: Workshops, Best Papers. Revised Selected Papers: LNAI 10642, P66, DOI 10.1007/978-3-319-71682-4_5; Gurbuzbalaban M, 2017, SIAM J OPTIMIZ, V27, P1035, DOI 10.1137/15M1049695; Hu J., 2003, J MACHINE LEARNING R, V4, P1039, DOI DOI 10.5555/945365.964288; Johansson M., 2014, 2014 IEEE INT WORKSH, P1; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; Kober J, 2012, ADAPT LEARN OPTIM, V12, P579; Lauer M., 2000, INT C MACH LEARN; Lee D., 2018, ARXIV180308031; Lian X., 2016, ARXIV161004674; Lin A., 2014, DECENTRALIZED PRIVAC; Littman M. L., 2001, Cognitive Systems Research, V2, P55, DOI 10.1016/S1389-0417(01)00015-8; Littman ML, 1994, ICML 1994, P157; Liu B, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P504; Macua S. V., 2017, ARXIV171010363; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nedic A, 2003, DISCRETE EVENT DYN S, V13, P79, DOI 10.1023/A:1022192903948; Nedic A, 2009, IEEE T AUTOMAT CONTR, V54, P48, DOI 10.1109/TAC.2008.2009515; Omidshafiei S, 2017, PR MACH LEARN RES, V70; Palaniappan B., 2016, NEURIPS, P1416; Parisotto Emilio, 2015, ARXIV151106342; Pu S., 2018, ARXIV180511454; Qu G., 2017, IEEE T CONTROL NETWO; Rabbat M, 2004, IPSN '04: THIRD INTERNATIONAL SYMPOSIUM ON INFORMATION PROCESSING IN SENSOR NETWORKS, P20; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Shi W, 2015, SIAM J OPTIMIZ, V25, P944, DOI 10.1137/14096668X; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sutton R. S., 2009, ADV NEURAL INFORM PR, V21, P1609; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Teh Y.W., 2017, ARXIV170704175; TSITSIKLIS JN, 1986, IEEE T AUTOMAT CONTR, V31, P803, DOI 10.1109/TAC.1986.1104412; Macua SV, 2015, IEEE T AUTOMAT CONTR, V60, P1260, DOI 10.1109/TAC.2014.2368731; Wang M., 2017, ARXIV PREPRINT ARXIV; Wang X., 2003, P ADV NEURAL INF PRO, P1603; Wilson A., 2007, PROC INT C MACH LEAR, P1015, DOI DOI 10.1145/1273496; Zhang K., 2018, P MACH LEARN RES; Zhu MH, 2010, AUTOMATICA, V46, P322, DOI 10.1016/j.automatica.2009.10.021	54	37	38	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004023
C	Yang, ZC; Hu, ZT; Dyer, C; Xing, EP; Berg-Kirkpatrick, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yang, Zichao; Hu, Zhiting; Dyer, Chris; Xing, Eric P.; Berg-Kirkpatrick, Taylor			Unsupervised Text Style Transfer using Language Models as Discriminators	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Binary classifiers are often employed as discriminators in GAN-based unsupervised style transfer systems to ensure that transferred sentences are similar to sentences in the target domain. One difficulty with this approach is that the error signal provided by the discriminator can be unstable and is sometimes insufficient to train the generator to produce fluent language. In this paper, we propose a new technique that uses a target domain language model as the discriminator, providing richer and more stable token-level feedback during the learning process. We train the generator to minimize the negative log likelihood (NLL) of generated sentences, evaluated by the language model. By using a continuous approximation of discrete sampling under the generator, our model can be trained using back-propagation in an end-to-end fashion. Moreover, our empirical results show that when using a language model as a structured discriminator, it is possible to forgo adversarial steps during training, making the process more stable. We compare our model with previous work that uses convolutional networks (CNNs) as discriminators, as well as a broad set of other approaches. Results show that the proposed method achieves improved performance on three tasks: word substitution decipherment, sentiment modification, and related language translation.	[Yang, Zichao; Hu, Zhiting; Xing, Eric P.; Berg-Kirkpatrick, Taylor] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Dyer, Chris] DeepMind, London, England	Carnegie Mellon University	Yang, ZC (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zichaoy@cs.cmu.edu; zhitingh@cs.cmu.edu; cdyer@google.com; epxing@cs.cmu.edu; tberg@cs.cmu.edu						Abadi M, 2015, P 12 USENIX S OPERAT; [Anonymous], 2017, ABS170207983 CORR; Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Artetxe Mikel, 2017, ARXIV171011041; Bowman Samuel R, 2016, SIGNLL C COMP NAT LA, DOI DOI 10.18653/V1/K16-1002; Brants T., 2007, P 2007 JOINT C EMP M, P858; Chen J., 2016, ARXIV160604646; Chung J., 2014, ARXIV14123555; Denton Emily L, 2015, NEURIPS, V2, P4; Dou Q., 2012, P 2012 JOINT C EMP M, P266; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Fu Zhenxin, 2017, ARXIV171106861; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Gomez Aidan N., 2018, P 2018 IEEE INT S ME, DOI DOI 10.1109/MEMEA.2018.8438744; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulcehre Caglar, 2015, USING MONOLINGUAL CO; He Di, 2016, NEURAL INFORM PROCES, P2; Hu Z., 2017, CORR, Vabs/1706.00550; Hu Z., 2018, ARXIV180609764; Hu ZT, 2018, NLP OPEN SOURCE SOFTWARE (NLP-OSS), P13; Hu ZT, 2017, PR MACH LEARN RES, V70; Huang X, 2017, IEEE I CONF COMP VIS, P1510, DOI 10.1109/ICCV.2017.167; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Jang E., 2016, ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Lample Guillaume, 2017, INT C LEARN REPR; Li Jiwei, 2017, P EMNLP; Lin K, 2017, ADV NEUR IN, V30; Liu Y, 2017, ADV NEUR IN, V30; Pourdamghani N., 2017, P 2017 C EMPIRICAL M, P2513, DOI DOI 10.18653/V1/D17-1266; Prabhumoye S, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P866; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Shen TX, 2017, ADV NEUR IN, V30; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wen Tsung-Hsien, 2016, ARXIV PREPRINT ARXIV, P438; Yang Z., 2017, ARXIV; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Zhu Jun-Yan, 2017, ICCV	46	37	37	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001081
C	Wang, D; Ye, MW; Xu, JH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Di; Ye, Minwei; Xu, Jinhui			Differentially Private Empirical Risk Minimization Revisited: Faster and More General	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In this paper we study the differentially private Empirical Risk Minimization (ERM) problem in different settings. For smooth (strongly) convex loss function with or without (non)-smooth regularization, we give algorithms that achieve either optimal or near optimal utility bounds with less gradient complexity compared with previous work. For ERM with smooth convex loss function in high-dimensional (p >> n) setting, we give an algorithm which achieves the upper bound with less gradient complexity than previous ones. At last, we generalize the expected excess empirical risk from convex loss functions to non-convex ones satisfying the Polyak-Lojasiewicz condition and give a tighter upper bound on the utility than the one in [34].	[Wang, Di; Ye, Minwei; Xu, Jinhui] SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA	State University of New York (SUNY) System; State University of New York (SUNY) Buffalo	Wang, D (corresponding author), SUNY Buffalo, Dept Comp Sci & Engn, Buffalo, NY 14260 USA.	dwang45@buffalo.edu; minweiye@buffalo.edu; jinhui@buffalo.edu			NSF [IIS-1422591, CCF-1422324, CCF-1716400]	NSF(National Science Foundation (NSF))	This research was supported in part by NSF through grants IIS-1422591, CCF-1422324, and CCF-1716400.	Abadi M, 2016, CCS'16: PROCEEDINGS OF THE 2016 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P308, DOI 10.1145/2976749.2978318; Agarwal N, 2017, PR MACH LEARN RES, V70; Allen-Zhu Z., 2017, 8 INN THEOR COMP SCI; Allen-Zhu Z, 2016, PR MACH LEARN RES, V48; Allen-Zhu Z, 2017, ACM S THEORY COMPUT, P1200, DOI 10.1145/3055399.3055448; Bassily R, 2014, ANN IEEE SYMP FOUND, P464, DOI 10.1109/FOCS.2014.56; Chaudhuri K, 2012, ADV NEURAL INFORM PR, P989; Chaudhuri K., 2009, ADV NEURAL INFORM PR, P289; Chaudhuri K, 2011, J MACH LEARN RES, V12, P1069; Dwork C, 2008, LECT NOTES COMPUT SC, V4978, P1, DOI 10.1007/978-3-540-79228-4_1; Dwork C, 2006, LECT NOTES COMPUT SC, V3876, P265, DOI 10.1007/11681878_14; Dwork C, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P11, DOI 10.1145/2591796.2591883; Dwork C, 2010, ANN IEEE SYMP FOUND, P51, DOI 10.1109/FOCS.2010.12; Feldman D, 2009, ACM S THEORY COMPUT, P361; Hardt M, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P331; Hazan E, 2015, ADV NEUR IN, V28; Jain P., 2012, COLT, V23; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Kasiviswanathan S.P., 2016, P 33 INT C MACHINE L, P488; Kifer D., 2012, J MACH LEARN RES WOR, V23; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Nitanda A., 2014, ADV NEURAL INFORM PR, P1574; Prasad S, 2017, PODS'17: PROCEEDINGS OF THE 36TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P167, DOI 10.1145/3034786.3034795; Reddi SJ, 2016, PR MACH LEARN RES, V48; Talwar K., 2015, NIPS 2015 P 28 INT C, V2, P3025; Talwar K., 2014, ARXIV14115417; Thakurta A., 2013, ADV NEURAL INFORM PR, P2733; Wang YQ, 2016, BMC GENET, V17, DOI 10.1186/s12863-016-0364-7; Wu X., 2015, ARXIV151206388; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zhang JQ, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3922	34	37	38	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402075
C	Polito, M; Perona, P		Dietterich, TG; Becker, S; Ghahramani, Z		Polito, M; Perona, P			Grouping and dimensionality reduction by locally linear embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					Locally Linear Embedding (LLE) is an elegant nonlinear dimensionality-reduction technique recently introduced by Roweis and Saul [2]. It fails when the data is divided into separate groups. We study a variant of LLE that can simultaneously group the data and calculate local embedding of each group. An estimate for the upper bound on the intrinsic dimension of the data set is obtained automatically.	CALTECH, Div Phys Math & Astron, Pasadena, CA 91125 USA	California Institute of Technology	Polito, M (corresponding author), CALTECH, Div Phys Math & Astron, Pasadena, CA 91125 USA.	polito@caltech.edu; perona@caltech.edu						BISHOP C, 1995, NEURAL NETWORKS PAT; Roweis ST, 2000, SCIENCE, V290, P2323, DOI 10.1126/science.290.5500.2323; Tenenbaum JB, 2000, SCIENCE, V290, P2319, DOI 10.1126/science.290.5500.2319	3	37	37	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1255	1262						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100156
C	Sollich, P		Solla, SA; Leen, TK; Muller, KR		Sollich, P			Probabilistic methods for Support Vector Machines	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					I describe a framework for interpreting Support Vector Machines (SVMs) as maximum a posteriori (MAP) solutions to inference problems with Gaussian Process priors. This can provide intuitive guidelines for choosing a 'good' SVM kernel. It can also assign (by evidence maximization) optimal values to parameters such as the noise level C which cannot be determined unambiguously from properties of the MAP solution alone (such as cross-validation error). I illustrate this using a simple approximate expression for the SVM evidence. Once C has been determined, error bars on SVM predictions can also be obtained.	Kings Coll London, Dept Math, London WC2R 2LS, England	University of London; King's College London	Sollich, P (corresponding author), Kings Coll London, Dept Math, London WC2R 2LS, England.	peter.sollich@kcl.ac.uk	Sollich, Peter/H-2174-2011; Sollich, Peter/ABC-2993-2020	Sollich, Peter/0000-0003-0169-7893; 				Burges CJC, 1998, DATA MIN KNOWL DISC, V2, P121, DOI 10.1023/A:1009715923555; CRISTIANINI N, NIPS 11; JAAKKOLA TS, P 7 INT WORKSH ART I; OPPER M, IN PRESS ADV LARGE M; SCHOLKOPF B, 1998, ADV KERNAL METHODS S; SCHOLKOPF B, NIPS 11; SEEGER M, UNPUB NIPS 12; Smola A., 1998, TR1998030 NEUROCOLT; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; SOLLICH P, UNPUB ICANN 99; WAHBA G, 1997, 984 U WISC; Williams CKI, 1998, NATO ADV SCI I D-BEH, V89, P599	12	37	39	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						349	355						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700050
C	Szepesvari, C		Jordan, MI; Kearns, MJ; Solla, SA		Szepesvari, C			The asymptotic convergence-rate of Q-learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					In this paper we show that for discounted MDPs with discount factor gamma > 1/2 the asymptotic rate of convergence of Q-learning is O(1/t(R(1-gamma))) if R(1 - gamma) < 1/2 and O(root loglogt/t) otherwise provided that the state-action pairs are sampled from a fixed probability distribution. Here R = p(min)/p(max) is the ratio of the minimum and maximum state-action occupation frequencies. The results extend to convergent on-line learning provided that p(min) > 0, where p(min) and p(max) now become the minimum and maximum state-action occupation frequencies corresponding to the stationary distribution.	Associat Comp Inc, H-1121 Budapest, Hungary		Szepesvari, C (corresponding author), Associat Comp Inc, Konkoly Thege M 29-33, H-1121 Budapest, Hungary.								0	37	39	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1064	1070						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700150
C	Bienenstock, E; Geman, S; Potter, D		Mozer, MC; Jordan, MI; Petsche, T		Bienenstock, E; Geman, S; Potter, D			Compositionality, MDL priors, and object recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Images are ambiguous at each of many levels of a contextual hierarchy. Nevertheless, the high-level interpretation of most scenes is unambiguous, as evidenced by the superior performance of humans. This observation argues for global vision models, such as deformable templates. Unfortunately, such models are computationally intractable for unconstrained problems. We propose a compositional model in which primitives are recursively composed, subject to syntactic restrictions, to form tree-structured objects and object groupings. Ambiguity is propagated up the hierarchy in the form of multiple interpretations, which are later resolved by a Bayesian, equivalently minimum-description-length, cost functional.			Bienenstock, E (corresponding author), BROWN UNIV,DIV APPL MATH,PROVIDENCE,RI 02912, USA.		Bienenstock, Elie/AAE-2146-2019						0	37	37	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						838	844						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00118
C	Mozer, MC; Vidmar, L; Dodier, RH		Mozer, MC; Jordan, MI; Petsche, T		Mozer, MC; Vidmar, L; Dodier, RH			The neurothermostat: Predictive optimal control of residential heating systems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The Neurothermostat is an adaptive controller that regulates indoor air temperature in a residence by switching a furnace on or off. The task is framed as an optimal control problem in which both comfort and energy costs are considered as part of the control objective. Because the consequences of control decisions are delayed in time, the Neurothermostat must anticipate heating demands with predictive models of occupancy patterns and the thermal response of the house and furnace. Occupancy pattern prediction is achieved by a hybrid neural net / look-up table. The Neurothermostat searches, at each discrete time step, for a decision sequence that minimizes the expected cost over a fixed planning horizon. The first decision in this sequence is taken, and this process repeats. Simulations of the Neurothermostat were conducted using artificial occupancy data in which regularity was systematically varied, as well as occupancy data from an actual residence. The Neurothermostat is compared against three conventional policies, and achieves reliably lower costs. This result is robust to the relative weighting of comfort and energy costs and the degree of variability in the occupancy patterns.			Mozer, MC (corresponding author), UNIV COLORADO,DEPT COMP SCI,BOULDER,CO 80309, USA.								0	37	51	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						953	959						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00134
C	Doya, K		Touretzky, DS; Mozer, MC; Hasselmo, ME		Doya, K			Temporal difference learning in continuous time and space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						ATR,HUMAN INFORMAT PROC RES LABS,KYOTO 61902,JAPAN										0	37	38	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						1073	1079						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00151
C	vanSchaik, A; Fragniere, E; Vittoz, E		Touretzky, DS; Mozer, MC; Hasselmo, ME		vanSchaik, A; Fragniere, E; Vittoz, E			Improved silicon cochlea using compatible lateral bipolar transistors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SWISS FED INST TECHNOL,MANTRA,CTR NEUROMIMET SYST,CH-1015 LAUSANNE,SWITZERLAND	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne				van Schaik, Andre/0000-0001-6140-017X					0	37	37	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						671	677						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00095
C	Eriksson, D; Pearce, M; Gardner, JR; Turner, R; Poloczek, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Eriksson, David; Pearce, Michael; Gardner, Jacob R.; Turner, Ryan; Poloczek, Matthias			Scalable Global Optimization via Local Bayesian Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Bayesian optimization has recently emerged as a popular method for the sample-efficient optimization of expensive black-box functions. However, the application to high-dimensional problems with several thousand observations remains challenging, and on difficult problems Bayesian optimization is often not competitive with other paradigms. In this paper we take the view that this is due to the implicit homogeneity of the global probabilistic models and an overemphasized exploration that results from global acquisition. This motivates the design of a local probabilistic approach for global optimization of large-scale high-dimensional problems. We propose the TuRBO algorithm that fits a collection of local models and performs a principled global allocation of samples across these models via an implicit bandit approach. A comprehensive evaluation demonstrates that TuRBO outperforms state-of-the-art methods from machine learning and operations research on problems spanning reinforcement learning, robotics, and the natural sciences.	[Eriksson, David; Gardner, Jacob R.; Turner, Ryan; Poloczek, Matthias] Uber AI, San Francisco, CA 94103 USA; [Pearce, Michael] Univ Warwick, Coventry, W Midlands, England	University of Warwick	Eriksson, D (corresponding author), Uber AI, San Francisco, CA 94103 USA.	eriksson@uber.com; m.a.l.pearce@warwick.ac.uk; jake.gardner@uber.com; ryan.turner@uber.com; poloczek@uber.com		Eriksson, David/0000-0002-3143-0922				Acerbi L, 2017, ADV NEURAL INFORM PR, P1836; Akrour R., 2017, INT C MACHINE LEARNI, P41; [Anonymous], 2017, P ADV NEURAL INFORM; [Anonymous], 2000, ICIAM; [Anonymous], 2007, MATH TODAY B I MATH, DOI DOI 10.1108/13639519910299580; Assael J.-A. M., 2014, ARXIV14107172; Bai Y, 2016, IEEE IJCNN, P3124, DOI 10.1109/IJCNN.2016.7727597; Binois M, 2020, J GLOBAL OPTIM, V76, P69, DOI 10.1007/s10898-019-00839-1; Binois M, 2015, LECT NOTES COMPUT SC, V8994, P281, DOI 10.1007/978-3-319-19084-6_28; Calandra R, 2016, ANN MATH ARTIF INTEL, V76, P5, DOI 10.1007/s10472-015-9463-9; Chen YR, 2012, PROCEEDINGS OF THE 9TH INTERNATIONAL CONFERENCE ON INNOVATION AND MANAGEMENT, P1379; Chevalier Clement, 2013, Learning and Intelligent Optimization. 7th International Conference, LION 7. Revised Selected Papers: LNCS 7997, P59, DOI 10.1007/978-3-642-44973-4_7; Constantine PG, 2015, ACTIVE SUBSPACES EME, V2; Dong N, 2017, WINT SIMUL C PROC, P2206; Eriksson D., 2018, ADV NEURAL INFORM PR, P6867; Frazier P.I., 2018, ARXIV, DOI 10.1287/educ.2018.0188; Gardner JR, 2017, PR MACH LEARN RES, V54, P1311; Garnett R, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P230; Gonzalez J, 2016, JMLR WORKSH CONF PRO, V51, P648; Hansen N, 2006, STUD FUZZ SOFT COMP, V192, P75; Jin Y, 2005, IEEE T EVOLUT COMPUT, V9, P303, DOI 10.1109/TEVC.2005.846356; Kandasamy K, 2018, PR MACH LEARN RES, V84; Kandasamy K, 2015, PR MACH LEARN RES, V37, P295; Krityakierne Tipaluck, 2015, Machine Learning, Optimization and Big Data. First International Workshop, MOD 2015. Revised Selected Papers: LNCS 9432, P185, DOI 10.1007/978-3-319-27926-8_16; Marmin Sebastien, 2015, Machine Learning, Optimization and Big Data. First International Workshop, MOD 2015. Revised Selected Papers: LNCS 9432, P37, DOI 10.1007/978-3-319-27926-8_4; McLeod M., 2018, INT C MACH LEARN, P3440; Hernandez-Lobato JM, 2017, PR MACH LEARN RES, V70; Mutny M., 2018, ADV NEURAL INFORM PR, P9005; Nayebi A., 2019, INT C MACH LEARN, P4752; NELDER JA, 1965, COMPUT J, V7, P308, DOI 10.1093/comjnl/7.4.308; Oh C, 2018, PR MACH LEARN RES, V80; POWELL MJD, 1993, MATH APPL, V275, P51; Rolland Paul, 2018, INT C ART INT STAT, P298; Russo DJ, 2018, FOUND TRENDS MACH LE, V11, P1, DOI 10.1561/2200000070; Shah A, 2015, ADV NEURAL INFORM PR, V12, P3330; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J., 2012, P 25 INT C NEUR INF, V2, P2951, DOI DOI 10.48550/ARXIV.1206.2944; Snoek J, 2014, PR MACH LEARN RES, V32, P1674; Snoek J, 2015, PR MACH LEARN RES, V37, P2171; Springenberg J.T., 2016, P ADV NEURAL INFORM, P4134; Taddy MA, 2009, TECHNOMETRICS, V51, P389, DOI 10.1198/TECH.2009.08007; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Wabersich K. P., 2016, ARXIV161203117; Wang J., 2016, ARXIV160205149; Wang Z., 2018, INT C ARTIFICIAL INT, V84, P745; Wang ZY, 2016, J ARTIF INTELL RES, V55, P361, DOI 10.1613/jair.4806; Williams Christopher KI, 2006, GAUSSIAN PROCESSES M, V2; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	50	36	36	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305048
C	He, FX; Liu, TL; Tao, DC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		He, Fengxiang; Liu, Tongliang; Tao, Dacheng			Control Batch Size and Learning Rate to Generalize Well: Theoretical and Empirical Evidence	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep neural networks have received dramatic success based on the optimization method of stochastic gradient descent (SGD). However, it is still not clear how to tune hyper-parameters, especially batch size and learning rate, to ensure good generalization. This paper reports both theoretical and empirical evidence of a training strategy that we should control the ratio of batch size to learning rate not too large to achieve a good generalization ability. Specifically, we prove a PAC-Bayes generalization bound for neural networks trained by SGD, which has a positive correlation with the ratio of batch size to learning rate. This correlation builds the theoretical foundation of the training strategy. Furthermore, we conduct a large-scale experiment to verify the correlation and training strategy. We trained 1,600 models based on architectures ResNet-110, and VGG-19 with datasets CIFAR-10 and CIFAR-100 while strictly control unrelated variables. Accuracies on the test sets are collected for the evaluation. Spearman's rank-order correlation coefficients and the corresponding p values on 164 groups of the collected data demonstrate that the correlation is statistically significant, which fully supports the training strategy.	[He, Fengxiang; Liu, Tongliang; Tao, Dacheng] Univ Sydney, UBTECH Sydney AI Ctr, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia	University of Sydney	He, FX (corresponding author), Univ Sydney, UBTECH Sydney AI Ctr, Sch Comp Sci, Fac Engn, Darlington, NSW 2008, Australia.	fengxiang.he@sydney.edu.au; tongliang.liu@sydney.edu.au; dacheng.tao@sydney.edu.au	Liu, Tongliang/AAA-1506-2021	Liu, Tongliang/0000-0002-9640-6472; He, Fengxiang/0000-0001-5584-2385	Australian Research Council [FL-170100117, DP180103424, DE190101473]	Australian Research Council(Australian Research Council)	This work was supported in part by Australian Research Council Projects FL-170100117, DP180103424, and DE190101473.	Allen-Zhu Z, 2019, PR MACH LEARN RES, V97; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, COMMUN MATH STAT; Brutzkus A., 2018, ICLR; Chen Y., 2018, ARXIV180401619; Dinh L, 2017, PR MACH LEARN RES, V70; Gardiner C W, 2009, HDB STOCHASTIC METHO, V4th; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Goyal P., 2017, LARGE MINIBATCH SGD; Hardt M., 2015, PREPRINT; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jastrzebski Stanislaw, 2017, ARXIV171104623; Keskar Nitish Shirish, 2017, INT C LEARN REPR, DOI [10.48550/arxiv.1609.04836, DOI 10.48550/ARXIV.1609.04836]; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Li H, 2018, ADV NEUR IN, V31; Lin J, 2016, INT CONF EUR ENERG; Liu TL, 2017, PR MACH LEARN RES, V70; Ljung Lennart, 2012, STOCHASTIC APPROXIMA, V17; London B, 2017, ADV NEUR IN, V30; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; McAllester DA, 1999, MACH LEARN, V37, P355, DOI 10.1023/A:1007618624809; Mou W., 2018, P MACHINE LEARNING R, P605; Pensia A, 2018, IEEE INT SYMP INFO, P546; Saad D, 1999, ON LINE LEARNING NEU, P9, DOI DOI 10.1017/CBO9780511569920.003; Smith S.L., 2018, P ICLR; Spearman C, 1904, AM J PSYCHOL, V15, P72, DOI 10.2307/1412159; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Uhlenbeck GE, 1930, PHYS REV, V36, P0823, DOI 10.1103/PhysRev.36.823	32	36	36	0	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301017
C	Knyazev, B; Taylor, GW; Amer, MR		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Knyazev, Boris; Taylor, Graham W.; Amer, Mohamed R.			Understanding Attention and Generalization in Graph Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We aim to better understand attention over nodes in graph neural networks (GNNs) and identify factors influencing its effectiveness. We particularly focus on the ability of attention GNNs to generalize to larger, more complex or noisy graphs. Motivated by insights from the work on Graph Isomorphism Networks, we design simple graph reasoning tasks that allow us to study attention in a controlled environment. We find that under typical conditions the effect of attention is negligible or even harmful, but under certain conditions it provides an exceptional gain in performance of more than 60% in some of our classification tasks. Satisfying these conditions in practice is challenging and often requires optimal initialization or supervised training of attention. We propose an alternative recipe and train attention in a weakly-supervised fashion that approaches the performance of supervised models, and, compared to unsupervised models, improves results on several synthetic as well as real datasets. Source code and datasets are available at https://github.com/bknyaz/graph-attention-pool.	[Knyazev, Boris] Univ Guelph, Vector Inst, Guelph, ON, Canada; [Taylor, Graham W.] Univ Guelph, Vector Inst, Canada CIFAR AI Chair, Guelph, ON, Canada; [Amer, Mohamed R.] Robust AI, Palo Alto, CA USA; [Amer, Mohamed R.] SRI Int, Menlo Pk, CA USA	University of Guelph; University of Guelph; SRI International	Knyazev, B (corresponding author), Univ Guelph, Vector Inst, Guelph, ON, Canada.	bknyazev@uoguelph.ca; gwtaylor@uoguelph.ca; mohamed@robust.ai			Defense Advanced Research Projects Agency (DARPA); Canadian Institute for Advanced Research; Canada Foundation for Innovation	Defense Advanced Research Projects Agency (DARPA)(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Canadian Institute for Advanced Research(Canadian Institute for Advanced Research (CIFAR)); Canada Foundation for Innovation(Canada Foundation for InnovationCGIAR)	This research was developed with funding from the Defense Advanced Research Projects Agency (DARPA). The views, opinions and/or findings expressed are those of the author and should not be interpreted as representing the official views orpolicies of the Department of Defense or the U.S. Government. The authors also acknowledge support from the Canadian Institute for Advanced Research and the Canada Foundation for Innovation. We are also thankful to Angus Galloway for feedback.	Achanta R., 2012, IEEE T PATTERN ANAL; [Anonymous], 2019, INT C LEARN REPR ICL; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Deac A, 2019, J COMPUT BIOL, V26, P536, DOI 10.1089/cmb.2018.0175; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Dobson PD, 2003, J MOL BIOL, V330, P771, DOI 10.1016/S0022-2836(03)00628-4; Dodge Samuel, 2017, 2017 26 INT C COMP C, P1, DOI DOI 10.1109/ICCCN.2017.8038465; Fey M, 2018, PROC CVPR IEEE, P869, DOI 10.1109/CVPR.2018.00097; Gao Hongyang, 2018, P 36 INT C MACH LEAR; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hendrycks Dan, 2019, INT C LEARN REPR ICL; Kingma DP, 2015, INT C LEARN REPR ICL; Kipf T.N., 2017, INT C LEARN REPR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee J. B., 2018, ARXIV180707984; Lee JB, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P1666, DOI 10.1145/3219819.3219980; Leskovec J, 2007, ACM T WEB, V1, DOI 10.1145/1232722.1232727; Liu Chenxi, 2017, P 31 AAAI C ART INT; Monti F., 2017, CVPR, V1, P3; Park D. H., 2016, ARXIV161204757; Shaham Uri, 2018, INT C LEARN REPR ICL; Shrivastava A, 2014, 2014 PROCEEDINGS OF THE IEEE/ACM INTERNATIONAL CONFERENCE ON ADVANCES IN SOCIAL NETWORKS ANALYSIS AND MINING (ASONAM 2014), P62, DOI 10.1109/ASONAM.2014.6921561; Szegedy Christian, 2014, PROC 2 INT C LEARN R; Vaswani A., 2017, P 31 INT C NEUR INF, P5998, DOI DOI 10.5555/3295222.3295349; Velickovic P, 2018, PROC 7 INT C LEARNIN; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Ying R., 2018, P 2018 ANN C NEUR IN; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang Jiani, 2018, UAI	29	36	38	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304023
C	Yuan, YT; Ma, L; Wang, JW; Liu, W; Zhu, WW		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Yuan, Yitian; Ma, Lin; Wang, Jingwen; Liu, Wei; Zhu, Wenwu			Semantic Conditioned Dynamic Modulation for Temporal Sentence Grounding in Videos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Temporal sentence grounding in videos aims to detect and localize one target video segment, which semantically corresponds to a given sentence. Existing methods mainly tackle this task via matching and aligning semantics between a sentence and candidate video segments, while neglect the fact that the sentence information plays an important role in temporally correlating and composing the described contents in videos. In this paper, we propose a novel semantic conditioned dynamic modulation (SCDM) mechanism, which relies on the sentence semantics to modulate the temporal convolution operations for better correlating and composing the sentence-related video contents over time. More importantly, the proposed SCDM performs dynamically with respect to the diverse video contents so as to establish a more precise matching relationship between sentence and video, thereby improving the temporal grounding accuracy. Extensive experiments on three public datasets demonstrate that our proposed model outperforms the state-of-the-arts with clear margins, illustrating the ability of SCDM to better associate and localize relevant video contents for temporal sentence grounding. Our code for this paper is available at https://github.com/yytzsy/SCDM.	[Yuan, Yitian] Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Beijing, Peoples R China; [Yuan, Yitian; Ma, Lin; Wang, Jingwen; Liu, Wei] Tencent AI Lab, Bellevue, WA USA; [Zhu, Wenwu] Tsinghua Univ, Beijing, Peoples R China	Tsinghua University; Tsinghua University	Yuan, YT (corresponding author), Tsinghua Univ, Tsinghua Berkeley Shenzhen Inst, Beijing, Peoples R China.	yyt18@mails.tsinghua.edu.cn; forest.linma@gmail.com; jaywongjaywong@gmail.com; wl2223@columbia.edu; wwzhu@tsinghua.edu.cn	wang, jing/GVT-8700-2022; wang, jing/GRS-7509-2022	Liu, Wei/0000-0002-3865-8145	National Natural Science Foundation of China; Shenzhen Nanshan District Ling-Hang Team Grant	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shenzhen Nanshan District Ling-Hang Team Grant	This work was supported by National Natural Science Foundation of China Major Project No.U1611461 and Shenzhen Nanshan District Ling-Hang Team Grant under No.LHTD20170005.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen Jingyuan, 2018, P C EMP METH NAT LAN, P162; Chen Jingyuan, 2019, 33 AAAI C ART INT; Chen Shaoxiang, 2019, AAAI C ART INT; Chen Zhenfang, 2019, 57 ANN M ASS COMP LI; de Vries H, 2017, ADV NEUR IN, V30; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Dumoulin Vincent, 2017, ICLR; Feng B, 2018, 2018 6TH INTERNATIONAL CONFERENCE ON MECHANICAL, AUTOMOTIVE AND MATERIALS ENGINEERING (CMAME), P51, DOI 10.1109/CMAME.2018.8592353; Feng Y, 2019, PROC CVPR IEEE, P1288, DOI 10.1109/CVPR.2019.00138; Gao JY, 2017, IEEE I CONF COMP VIS, P5277, DOI 10.1109/ICCV.2017.563; Gavrilyuk K, 2018, PROC CVPR IEEE, P5958, DOI 10.1109/CVPR.2018.00624; Ge RZ, 2019, IEEE WINT CONF APPL, P245, DOI 10.1109/WACV.2019.00032; Girshick R, 2015, IEEE I CONF COMP VIS, P1440, DOI 10.1109/ICCV.2015.169; Hendricks LA, 2017, IEEE I CONF COMP VIS, P5804, DOI 10.1109/ICCV.2017.618; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Krishna R, 2017, IEEE I CONF COMP VIS, P706, DOI 10.1109/ICCV.2017.83; Lin TW, 2017, PROCEEDINGS OF THE 2017 ACM MULTIMEDIA CONFERENCE (MM'17), P988, DOI 10.1145/3123266.3123343; Liu M, 2018, ACM/SIGIR PROCEEDINGS 2018, P15, DOI 10.1145/3209978.3210003; Liu Y, 2019, PROC CVPR IEEE, P3599, DOI 10.1109/CVPR.2019.00372; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Regneri M., 2013, TACL, V1, P25, DOI DOI 10.1162/TACL_A_00207; Singh B, 2016, PROC CVPR IEEE, P1961, DOI 10.1109/CVPR.2016.216; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2017, AAAI CONF ARTIF INTE, P4278; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wang L, 2014, IEEE INT CONF VLSI; Wang Shuohang, 2016, ARXIV160807905; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Wu AM, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P1029; Xu H., 2019, P AAAI, V2, P7; Yuan J, 2016, PROC CVPR IEEE, P3093, DOI 10.1109/CVPR.2016.337; Yuan Yitian, 2019, 27 ACM INT C MULT; Zhang Da, 2018, ARXIV181200087	36	36	36	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300049
C	Gong, CY; He, D; Tan, X; Qin, T; Wang, LW; Liu, TY		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Gong, Chengyue; He, Di; Tan, Xu; Qin, Tao; Wang, Liwei; Liu, Tie-Yan			FRAGE: Frequency-Agnostic Word Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Continuous word representation (aka word embedding) is a basic building block in many neural network-based models used in natural language processing tasks. Although it is widely accepted that words with similar semantics should be close to each other in the embedding space, we find that word embeddings learned in several tasks are biased towards word frequency: the embeddings of high-frequency and low-frequency words lie in different subregions of the embedding space, and the embedding of a rare word and a popular word can be far from each other even if they are semantically similar. This makes learned word embeddings ineffective, especially for rare words, and consequently limits the performance of these neural network models. In this paper, we develop FRequency-AGnostic word Embedding (FRAGE) which is a neat, simple yet effective way to learn word representation using adversarial training We conducted comprehensive studies on ten datasets across four natural language processing tasks, including word similarity, language modeling, machine translation, and text classification. Results show that with FRAGE, we achieve higher performance than the baselines in all tasks.	[Gong, Chengyue] Peking Univ, Beijing, Peoples R China; [He, Di; Wang, Liwei] Peking Univ, Sch EECS, MOE, Key Lab Machine Percept, Beijing, Peoples R China; [Tan, Xu; Qin, Tao; Liu, Tie-Yan] Microsoft Res Asia, Beijing, Peoples R China; [Wang, Liwei] Peking Univ, Beijing Inst Big Data Res, Ctr Data Sci, Beijing, Peoples R China	Peking University; Peking University; Microsoft; Microsoft Research Asia; Peking University	Gong, CY (corresponding author), Peking Univ, Beijing, Peoples R China.	cygong@pku.edu.cn; di_he@pku.edu.cn; xu.tan@microsoft.com; taoqin@microsoft.com; wanglw@cis.pku.edu.cn; tie-yan.liu@microsoft.com		Qin, Tao/0000-0002-9095-0776	National Basic Research Program of China (973 Program) [2015CB352502]; NSFC [61573026]; BJNSF [L172037]; Microsoft Research Asia	National Basic Research Program of China (973 Program)(National Basic Research Program of China); NSFC(National Natural Science Foundation of China (NSFC)); BJNSF; Microsoft Research Asia(Microsoft)	This work is supported by National Basic Research Program of China (973 Program) (grant no. 2015CB352502), NSFC (61573026) and BJNSF (L172037) and a grant from Microsoft Research Asia. We would like to thank the anonymous reviewers for their valuable comments on our paper.	Al-Rfou Rami, 2013, P 17 C COMP NAT LANG, P183, DOI DOI 10.1007/S10479-011-0841-3; Arjovsky M., 2017, ARXIV170107875; Arora S, 2016, SIMPLE TOUGH TO BEAT; Bengio Y., 2014, ARXIV14061078; Conneau Alexis, 2017, ARXIV171004087; Dai Andrew M., 2015, ADV NEURAL INFORM PR; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Edunov Sergey, 2017, ARXIV171104956; Ganin Yaroslav, 2015, ICML; Gehring J., 2017, P ICML; Gehring Jonas, 2016, CONVOLUTIONAL ENCODE; Gong C., 2018, CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grave Edouard, 2016, ARXIV161204426; Jozefowicz Rafal, 2016, ARXIV160202410; Kalchbrenner Nal, 2016, ARXIV161010099; Kim Y, 2016, AAAI CONF ARTIF INTE, P2741; Krause B., 2017, ARXIV170907432; Lai SW, 2015, AAAI CONF ARTIF INTE, P2267; Lample Guillaume, 2017, INT C LEARN REPR; Larson RR, 2010, J AM SOC INF SCI TEC, V61, P852, DOI 10.1002/asi.21234; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Merity Stephen, 2017, ICLR; Mikolov T, 2010, 11TH ANNUAL CONFERENCE OF THE INTERNATIONAL SPEECH COMMUNICATION ASSOCIATION 2010 (INTERSPEECH 2010), VOLS 1-2, P1045; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mu J., 2017, ARXIV170201417; Mu J., 2017, CORR; Ott M, 2018, PR MACH LEARN RES, V80; Pennington Jeffrey., 2014, P 2014 C EMP METH NA, P1532, DOI [10.3115/v1/D14-1162, DOI 10.3115/V1/D14-1162]; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Sennrich Rico, 2015, ARXIV150807909; Soudry D., 2018, ICLR, P1; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Vaswani Ashish, 2018, CORR; Wang Y., DUAL TRANSFER LEARNI; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yang Zhilin, 2017, ICLR; Zhu Jun-Yan, 2017, ICCV	42	36	37	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301033
C	Yu, YL; Ji, Z; Fu, YW; Guo, JC; Pang, YW; Zhang, ZF		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Yu, Yunlong; Ji, Zhong; Fu, Yanwei; Guo, Jichang; Pang, Yanwei; Zhang, Zhongfei (Mark)			Stacked Semantics-Guided Attention Model for Fine-Grained Zero-Shot Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Zero-Shot Learning (ZSL) is generally achieved via aligning the semantic relationships between the visual features and the corresponding class semantic descriptions. However, using the global features to represent fine-grained images may lead to sub-optimal results since they neglect the discriminative differences of local regions. Besides, different regions contain distinct discriminative information. The important regions should contribute more to the prediction. To this end, we propose a novel stacked semantics-guided attention (S(2)GA) model to obtain semantic relevant features by using individual class semantic features to progressively guide the visual features to generate an attention map for weighting the importance of different local regions. Feeding both the integrated visual features and the class semantic features into a multi-class classification architecture, the proposed framework can be trained end-to-end. Extensive experimental results on CUB and NABird datasets show that the proposed approach has a consistent improvement on both fine-grained zero-shot classification and retrieval tasks.	[Yu, Yunlong; Ji, Zhong; Guo, Jichang; Pang, Yanwei] Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China; [Fu, Yanwei] Fudan Univ, Sch Data Sci, AITRICS, Shanghai, Peoples R China; [Zhang, Zhongfei (Mark)] SUNY Binghamton, Dept Comp Sci, Binghamton, NY 13902 USA	Tianjin University; Fudan University; State University of New York (SUNY) System; State University of New York (SUNY) Binghamton	Ji, Z (corresponding author), Tianjin Univ, Sch Elect & Informat Engn, Tianjin, Peoples R China.	yuyunlong@tju.edu.cn; jizhong@tju.edu.cn; yanweifu@fudan.edu.cn; jcguo@tju.edu.cn; pyw@tju.edu.cn; zhongfei@cs.binghamton.edu			National Natural Science Foundation of China [61771329, 61632018]; National Basic Research Program of China [2014CB340403]; China Scholarship Council	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Basic Research Program of China(National Basic Research Program of China); China Scholarship Council(China Scholarship Council)	This work was supported by the National Natural Science Foundation of China under Grant 61771329, the National Basic Research Program of China (Grant No. 2014CB340403), the National Natural Science Foundation of China under Grant 61632018. Yunlong Yu also acknowledges the support of China Scholarship Council. The authors are very grateful for NVIDIA's support in providing GPUs that made this work possible.	Akata Z, 2016, PROC CVPR IEEE, P59, DOI 10.1109/CVPR.2016.14; Akata Z, 2016, IEEE T PATTERN ANAL, V38, DOI 10.1109/TPAMI.2015.2487986; Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Ba J., 2015, ICLR; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Elhoseiny M, 2017, PROC CVPR IEEE, P6288, DOI 10.1109/CVPR.2017.666; Frome Andrea, 2013, NEURIPS; Fu YW, 2015, IEEE T PATTERN ANAL, V37, P2332, DOI 10.1109/TPAMI.2015.2408354; Johnson M., 2017, T ASSOC COMPUT LING, DOI 10.1162/tacl_a_00065; Lampert CH, 2014, IEEE T PATTERN ANAL, V36, P453, DOI 10.1109/TPAMI.2013.140; Mikolov Tomas., 2013, ADV NEURAL INF PROCE, V2, P3111, DOI DOI 10.5555/2999792.2999959; Morgado P., 2017, CVPR; Pedersoli M, 2017, IEEE I CONF COMP VIS, P1251, DOI 10.1109/ICCV.2017.140; Peng H, 2017, CONFERENCE PROCEEDINGS OF 2017 3RD IEEE INTERNATIONAL CONFERENCE ON CONTROL SCIENCE AND SYSTEMS ENGINEERING (ICCSSE), P22, DOI 10.1109/CCSSE.2017.8087887; Qiao RZ, 2016, PROC CVPR IEEE, P2249, DOI 10.1109/CVPR.2016.247; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; SALTON G, 1988, INFORM PROCESS MANAG, V24, P513, DOI 10.1016/0306-4573(88)90021-0; Simonyan K., 2014, ICLR; Socher Richard, 2013, NEURIPS; Sung F., 2018, CVPR; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; Van Horn G, 2015, PROC CVPR IEEE, P595, DOI 10.1109/CVPR.2015.7298658; Wah Catherine, 2011, CALTECH UCSD BIRDS 2; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Yu DF, 2017, PROC CVPR IEEE, P4187, DOI 10.1109/CVPR.2017.446; Yu Y., 2018, IEEE T CYBERNETICS, V49, P1; Zhang H, 2016, PROC CVPR IEEE, P1143, DOI 10.1109/CVPR.2016.129; Zhang L, 2017, PROC CVPR IEEE, P3010, DOI 10.1109/CVPR.2017.321; Zhu Yi, 2018, CVPR	32	36	38	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000049
C	Zahavy, T; Haroush, M; Merlis, N; Mankowitz, DJ; Mannor, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zahavy, Tom; Haroush, Matan; Merlis, Nadav; Mankowitz, Daniel J.; Mannor, Shie			Learn What Not to Learn: Action Elimination with Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Learning how to act when there are many available actions in each state is a challenging task for Reinforcement Learning (RL) agents, especially when many of the actions are redundant or irrelevant. In such cases, it is sometimes easier to learn which actions not to take. In this work, we propose the Action-Elimination Deep Q-Network (AE-DQN) architecture that combines a Deep RL algorithm with an Action Elimination Network (AEN) that eliminates sub-optimal actions. The AEN is trained to predict invalid actions, supervised by an external elimination signal provided by the environment. Simulations demonstrate a considerable speedup and added robustness over vanilla DQN in text-based games with over a thousand discrete actions.	[Zahavy, Tom; Haroush, Matan; Merlis, Nadav; Mannor, Shie] Technion Israel Inst Technol, Haifa, Israel; [Zahavy, Tom] Google Res, Haifa, Israel; [Mankowitz, Daniel J.] Deepmind, London, England	Technion Israel Institute of Technology; Google Incorporated	Zahavy, T; Haroush, M; Merlis, N (corresponding author), Technion Israel Inst Technol, Haifa, Israel.; Zahavy, T (corresponding author), Google Res, Haifa, Israel.	tomzahavy@campus.technion.ac.il; matan.h@campus.technion.ac.il; merlis@campus.technion.ac.il		Mannor, Shie/0000-0003-4439-7647				Abbasi-Yadkori Y., 2011, ADV NEURAL INFORM PR, P2312; Azizzadenesheli K., 2018, EFFICIENT EXPLORATIO; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bertsekas DP, 1995, PROCEEDINGS OF THE 34TH IEEE CONFERENCE ON DECISION AND CONTROL, VOLS 1-4, P560, DOI 10.1109/CDC.1995.478953; Budzianowski P., 2017, SUB DOMAIN MODELLING; Chu W., 2011, P 14 INT C ART INT S, V15, P208; Cote M.A., 2018, TEXTWORLD LEARNING E; Dalal G, 2016, PR MACH LEARN RES, V48; Dhingra B., 2016, P 55 ANN M ASS COMP; Dulac-Arnold, 2015, ARXIV151207679; Dulac-Arnold Gabriel, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P180, DOI 10.1007/978-3-642-33486-3_12; Even-Dar E., 2003, P 20 INT C MACH LEAR; Fulda N., 2017, WHAT CAN YOU ROCK AF; Glavic M, 2017, IFAC PAPERSONLINE, V50, P6918, DOI 10.1016/j.ifacol.2017.08.1217; He J., 2015, CORR; Hester T., 2018, AAAI C ART INT; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; JADERBERG M, 2016, 4 INT C LEARN REPR I, P1; Kakade Sham M., 2003, THESIS; Kim Y., 2014, P 2014 C EMPIRICAL M, DOI [10.3115/v1/D14-1181, DOI 10.3115/V1/D14-1181]; Kostka B., 2017, COMP INT GAM CIG 201; Lagoudakis M. G., 2003, P 20 INT C MACH LEAR; Levine N., 2017, ADV NEURAL INFORM PR, P3138; Li Jiwei, 2016, DEEP REINFORCEMENT L; Li X., 2017, END TO END TASK COMP; Lin L. - J., 1992, MACHINE LEARNING, V8; Lipton, 2016, COMBATING REINFORCEM; Lipton Z. C., 2016, ARXIV160805081; Liu B., 2017, NIPS WORKSH CONV AI; Machado M. C., 2017, REVISITING ARCADE LE; Mannion P, 2016, AUTON SYST, P47, DOI 10.1007/978-3-319-25808-9_4; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Narasimhan K., 2015, ARXIV150608941; Pazis J., 2011, P 28 INT C MACHINE L, P1185; Peng B., 2017, P 2017 C EMP METH NA; Riquelme Carlos, 2018, ARXIV180209127; Serban Iulian Vlad, 2017, DEEP REINFORCEMENT L; Su Pei-Hao, 2016, ARXIV160602689; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; Thrun S., 1993, P 1993 CONNECTIONIST; Van der Pol E., 2016, P NIPS; van Hasselt Hado, 2009, Proceedings 2009 International Joint Conference on Neural Networks (IJCNN 2009 - Atlanta), P1149, DOI 10.1109/IJCNN.2009.5178745; WATKINS CJCH, 1992, MACH LEARN, V8, P279, DOI 10.1007/BF00992698; Wen Z, 2015, IEEE T SMART GRID, V6, P2312, DOI 10.1109/TSG.2015.2396993; Wu Yonghui, 2016, GOOGLES NEURAL MACHI; Yuan Xingdi, 2018, ARXIV180611525; Zahavy T., 2018, 30 C INN APPL ART IN; Zahavy T, 2016, PR MACH LEARN RES, V48; Zelinka M., 2018, USING REINFORCEMENT; Zhao T., 2016, END TO END LEARNING; Zrihem NB, 2016, ARXIV160607112	54	36	36	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303055
C	Zanfir, A; Marinoiu, E; Zanfir, M; Popa, AI; Sminchisescu, C		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zanfir, Andrei; Marinoiu, Elisabeta; Zanfir, Mihai; Popa, Alin-Ionut; Sminchisescu, Cristian			Deep Network for the Integrated 3D Sensing of Multiple People in Natural Images	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present MubyNet - a feed-forward, multitask, bottom up system for the integrated localization, as well as 3d pose and shape estimation, of multiple people in monocular images. The challenge is the formal modeling of the problem that intrinsically requires discrete and continuous computation, e.g. grouping people vs. predicting 3d pose. The model identifies human body structures (joints and limbs) in images, groups them based on 2d and 3d information fused using learned scoring functions, and optimally aggregates such responses into partial or complete 3d human skeleton hypotheses under kinematic tree constraints, but without knowing in advance the number of people in the scene and their visibility relations. We design a multi-task deep neural network with differentiable stages where the person grouping problem is formulated as an integer program based on learned body part scores parameterized by both 2d and 3d information. This avoids suboptimality resulting from separate 2d and 3d reasoning, with grouping performed based on the combined representation. The final stage of 3d pose and shape prediction is based on a learned attention process where information from different human body parts is optimally integrated. State-of-the-art results are obtained in large scale datasets like Human3.6M and Panoptic, and qualitatively by reconstructing the 3d shape and pose of multiple people, under occlusion, in difficult monocular images.	[Sminchisescu, Cristian] Lund Univ, Fac Engn, Dept Math, Lund, Sweden; [Zanfir, Andrei; Marinoiu, Elisabeta; Zanfir, Mihai; Popa, Alin-Ionut; Sminchisescu, Cristian] Romanian Acad, Inst Math, Bucharest, Romania	Lund University; Institute of Mathematics of the Romanian Academy; Romanian Academy of Sciences; University of Bucharest	Zanfir, A (corresponding author), Romanian Acad, Inst Math, Bucharest, Romania.	andrei.zanfir@imar.ro; elisabeta.marinoiu@imar.ro; mihai.zanfir@imar.ro; alin.popa@imar.ro; cristian.sminchisescu@math.lth.se	Zanfir, Mihai/AAG-7817-2021		European Research Council Consolidator grant SEED, CNCS-UEFISCDI [PN-III-P4-ID-PCE-2016-0535, PN-III-P4-ID-PCCF-2016-0180]; EU Horizon 2020 grant DE-ENIGMA [688835]; SSF	European Research Council Consolidator grant SEED, CNCS-UEFISCDI(Consiliul National al Cercetarii Stiintifice (CNCS)Unitatea Executiva pentru Finantarea Invatamantului Superior, a Cercetarii, Dezvoltarii si Inovarii (UEFISCDI)); EU Horizon 2020 grant DE-ENIGMA; SSF(Swedish Foundation for Strategic Research)	This work was supported in part by the European Research Council Consolidator grant SEED, CNCS-UEFISCDI (PN-III-P4-ID-PCE-2016-0535, PN-III-P4-ID-PCCF-2016-0180), the EU Horizon 2020 grant DE-ENIGMA (688835), and SSF.	Bogo F, 2016, LECT NOTES COMPUT SC, V9909, P561, DOI 10.1007/978-3-319-46454-1_34; Brau E, 2016, INT CONF 3D VISION, P582, DOI 10.1109/3DV.2016.84; Cao Z, 2017, PROC CVPR IEEE, P1302, DOI 10.1109/CVPR.2017.143; Chen WZ, 2016, INT CONF 3D VISION, P479, DOI 10.1109/3DV.2016.58; Gkioxari G, 2014, PROC CVPR IEEE, pCP32, DOI 10.1109/CVPR.2014.458; Insafutdinov E, 2016, LECT NOTES COMPUT SC, V9910, P34, DOI 10.1007/978-3-319-46466-4_3; Ionescu C, 2014, PROC CVPR IEEE, P1661, DOI 10.1109/CVPR.2014.215; Ionescu C, 2014, IEEE T PATTERN ANAL, V36, P1325, DOI 10.1109/TPAMI.2013.248; Iqbal U., 2016, ECCV; Jia Y., 2014, P 22 ACM INT C MULT, P675; Joo H, 2015, IEEE I CONF COMP VIS, P3334, DOI 10.1109/ICCV.2015.381; Katircioglu I, 2018, INT J COMPUT VISION, V126, P1326, DOI 10.1007/s11263-018-1066-6; Li SJ, 2015, LECT NOTES COMPUT SC, V9004, P332, DOI 10.1007/978-3-319-16808-1_23; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; LOPER M, 2015, SIGGRAPH, V34; Martinez Julieta, 2017, ICCV; Moreno-Noguer F, 2017, PROC CVPR IEEE, P1561, DOI 10.1109/CVPR.2017.170; Papandreou G, 2017, PROC CVPR IEEE, P3711, DOI 10.1109/CVPR.2017.395; Pavlakos G, 2017, PROC CVPR IEEE, P1253, DOI 10.1109/CVPR.2017.138; Popa Alin-Ionut, 2017, CVPR; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rogez G, 2016, ADV NEUR IN, V29; Rogez G, 2017, PROC CVPR IEEE, P1216, DOI 10.1109/CVPR.2017.134; Tekin B, 2017, IEEE I CONF COMP VIS, P3961, DOI 10.1109/ICCV.2017.425; Tompson J.J., 2014, ADV NEURAL INFORM PR, V27, P1799; Toshev A, 2014, PROC CVPR IEEE, P1653, DOI 10.1109/CVPR.2014.214; Wei SE, 2016, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2016.511; Zanfir A, 2018, PROC CVPR IEEE, P2148, DOI 10.1109/CVPR.2018.00229; Zhou XW, 2016, PROC CVPR IEEE, P4966, DOI 10.1109/CVPR.2016.537; Zhou Xingyi, 2017, IEEE INT C COMP VIS	33	36	36	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003001
C	Zhao, Y; Xiong, YJ; Lin, DH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhao, Yue; Xiong, Yuanjun; Lin, Dahua			Trajectory Convolution for Action Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					How to leverage the temporal dimension is one major question in video analysis. Recent works [47, 36] suggest an efficient approach to video feature learning, i.e., factorizing 3D convolutions into separate components respectively for spatial and temporal convolutions. The temporal convolution, however, comes with an implicit assumption - the feature maps across time steps are well aligned so that the features at the same locations can be aggregated. This assumption can be overly strong in practical applications, especially in action recognition where the motion serves as a crucial cue. In this work, we propose a new CNN architecture TrajectoryNet, which incorporates trajectory convolution, a new operation for integrating features along the temporal dimension, to replace the existing temporal convolution. This operation explicitly takes into account the changes in contents caused by deformation or motion, allowing the visual features to be aggregated along the the motion paths, trajectories. On two large-scale action recognition datasets, Something-Something V1 and Kinetics, the proposed network architecture achieves notable improvement over strong baselines.	[Zhao, Yue; Lin, Dahua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China; [Xiong, Yuanjun] Amazon Rekognit, Seattle, WA USA	Chinese University of Hong Kong	Zhao, Y (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	zy317@ie.cuhk.edu.hk; yuanjx@amazon.com; dhlin@ie.cuhk.edu.hk	Lin, Dahua/W-6576-2019; Zhao, Yue/AGU-0409-2022	Lin, Dahua/0000-0002-8865-7896; Zhao, Yue/0000-0003-2753-5921	Big Data Collaboration Research grant from SenseTime Group (CUHK) [TS1610626]; Early Career Scheme (ECS) of Hong Kong [24204215]	Big Data Collaboration Research grant from SenseTime Group (CUHK); Early Career Scheme (ECS) of Hong Kong	This work is partially supported by the Big Data Collaboration Research grant from SenseTime Group (CUHK Agreement No. TS1610626), and the Early Career Scheme (ECS) of Hong Kong (No. 24204215).	Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dalal N, 2005, PROC CVPR IEEE, P886, DOI 10.1109/cvpr.2005.177; Dalal N, 2006, LECT NOTES COMPUT SC, V3952, P428, DOI 10.1007/11744047_33; De Brabandere B, 2016, ADV NEUR IN, V29; Dosovitskiy A, 2015, IEEE I CONF COMP VIS, P2758, DOI 10.1109/ICCV.2015.316; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Feichtenhofer C, 2017, PROC CVPR IEEE, P7445, DOI 10.1109/CVPR.2017.787; Feichtenhofer C, 2016, PROC CVPR IEEE, P1933, DOI 10.1109/CVPR.2016.213; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Heng Wang, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P3169, DOI 10.1109/CVPR.2011.5995407; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jegou H, 2012, IEEE T PATTERN ANAL, V34, P1704, DOI 10.1109/TPAMI.2011.235; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; JOHANSSON G, 1973, PERCEPT PSYCHOPHYS, V14, P201, DOI 10.3758/BF03212378; Karpathy A, 2014, PROC CVPR IEEE, P1725, DOI 10.1109/CVPR.2014.223; Kay W., 2017, ARXIV PREPRINT ARXIV; Laptev I, 2005, INT J COMPUT VISION, V64, P107, DOI 10.1007/s11263-005-1838-7; Laptev I, 2008, PROC CVPR IEEE, P3222, DOI 10.1109/cvpr.2008.4587756; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Messing R, 2009, IEEE I CONF COMP VIS, P104, DOI 10.1109/ICCV.2009.5459154; Monfort Mathew, 2018, ARXIV180103150; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sevilla-Lara L., 2017, P GERM C PATT REC; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Soomro K., 2012, ARXIV; Sun DQ, 2018, PROC CVPR IEEE, P8934, DOI 10.1109/CVPR.2018.00931; Sun DQ, 2010, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2010.5539939; Sun J, 2009, PROC CVPR IEEE, P2004, DOI 10.1109/CVPRW.2009.5206721; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tran Du, 2017, ARXIV170805038; WANG H, 2013, INT C COMP VIS, DOI DOI 10.1109/ICCV.2013.441; Wang Heng, 2018, IEEE C COMP VIS PATT; Wang HW, 2009, PROCEEDINGS OF THE THIRD INTERNATIONAL CONFERENCE ON MANAGEMENT SCIENCE AND ENGINEERING MANAGEMENT, P124; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang LM, 2015, PROC CVPR IEEE, P4305, DOI 10.1109/CVPR.2015.7299059; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Wang Xingxing, 2012, ASIAN C COMPUTER VIS, P572; Willems G, 2008, LECT NOTES COMPUT SC, V5303, P650, DOI 10.1007/978-3-540-88688-4_48; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Yi Z., 2017, CORR; Zach C, 2007, LECT NOTES COMPUT SC, V4713, P214, DOI 10.1007/978-3-540-74936-3_22; Zhou BL, 2018, LECT NOTES COMPUT SC, V11212, P122, DOI 10.1007/978-3-030-01237-3_8; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43	52	36	36	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302023
C	Czarnecki, WM; Osindero, S; Jaderberg, M; Swirszcz, G; Pascanu, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Czarnecki, Wojciech Marian; Osindero, Simon; Jaderberg, Max; Swirszcz, Grzegorz; Pascanu, Razvan			Sobolev Training for Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					At the heart of deep learning we aim to use neural networks as function approxi-mators - training them to produce outputs from inputs in emulation of a ground truth function or data creation process. In many cases we only have access to input-output pairs from the ground truth, however it is becoming more common to have access to derivatives of the target output with respect to the input - for example when the ground truth function is itself a neural network such as in network compression or distillation. Generally these target derivatives are not computed, or are ignored. This paper introduces Sobolev Training for neural networks, which is a method for incorporating these target derivatives in addition the to target values while training. By optimising neural networks to not only approximate the function's outputs but also the function's derivatives we encode additional information about the target function within the parameters of the neural network. Thereby we can improve the quality of our predictors, as well as the data-efficiency and generalization capabilities of our learned function approximation. We provide theoretical justifications for such an approach as well as examples of empirical evidence on three distinct domains: regression on classical optimisation datasets, distilling policies of an agent playing Atari, and on large-scale applications of synthetic gradients. In all three domains the use of Sobolev Training, employing target derivatives in addition to target values, results in models with higher accuracy and stronger generalisation.	[Czarnecki, Wojciech Marian; Osindero, Simon; Jaderberg, Max; Swirszcz, Grzegorz; Pascanu, Razvan] DeepMind, London, England		Czarnecki, WM (corresponding author), DeepMind, London, England.	lejlot@google.com; osindero@google.com; jaderberg@google.com; swirszcz@google.com; razp@google.com	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Fairbank M, 2012, IEEE IJCNN; Fairbank M, 2012, IEEE T NEUR NET LEAR, V23, P1671, DOI 10.1109/TNNLS.2012.2205268; GALLANT AR, 1992, NEURAL NETWORKS, V5, P129, DOI 10.1016/S0893-6080(05)80011-5; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton G., 2015, ARXIV150302531; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Hyvarinen A, 2005, J MACH LEARN RES, V6, P695; Jaderberg M., 2016, ARXIV160805343; Konda V., 1999, NIPS; Krantz S., 2012, HDB COMPLEX VARIABLE; Maeda Shin- ichi, 2017, ICLR WORKSH P; Miller WT., 1995, NEURAL NETWORKS CONT; Mnih V., 2013, ARXIV13125602CS, DOI DOI 10.1038/NATURE14236; Mnih V, 2016, PR MACH LEARN RES, V48; Oord A.V.D., 2016, SSW; Pascanu Razvan, 2013, ARXIV13013584; Rifai S, 2011, LECT NOTES ARTIF INT, V6912, P645, DOI 10.1007/978-3-642-23783-6_41; Rusu A. A., 2015, ARXIV151106295; Sau BB, 2016, ARXIV PREPRINT ARXIV; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Simard P., 1991, ADV NEURAL INFORM PR, V91, P895; Tassa Y, 2007, IEEE T NEURAL NETWOR, V18, P1031, DOI 10.1109/TNN.2007.899249; Vincent P, 2011, NEURAL COMPUT, V23, P1661, DOI 10.1162/NECO_a_00142; Werbos P, 1992, HDB INTELLIGENT CONT, P493; Wu A., 2017, ARXIV170400060; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	30	36	36	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404034
C	Gao, WH; Kannan, S; Oh, S; Viswanath, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Gao, Weihao; Kannan, Sreeram; Oh, Sewoong; Viswanath, Pramod			Estimating Mutual Information for Discrete-Continuous Mixtures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				DIVERGENCE ESTIMATION; FEATURE-SELECTION; ENTROPY; DISTRIBUTIONS	Estimation of mutual information from observed samples is a basic primitive in machine learning, useful in several learning tasks including correlation mining, information bottleneck, Chow-Liu tree, and conditional independence testing in (causal) graphical models. While mutual information is a quantity well-defined for general probability spaces, estimators have been developed only in the special case of discrete or continuous pairs of random variables. Most of these estimators operate using the 3H-principle, i.e., by calculating the three (differential) entropies of X, Y and the pair (X, Y). However, in general mixture spaces, such individual entropies are not well defined, even though mutual information is. In this paper, we develop a novel estimator for estimating mutual information in discrete-continuous mixtures. We prove the consistency of this estimator theoretically as well as demonstrate its excellent empirical performance. This problem is relevant in a wide-array of applications, where some variables are discrete, some continuous, and others are a mixture between continuous and discrete components.	[Gao, Weihao; Viswanath, Pramod] Univ Illinois, Coordinated Sci Lab, Dept ECE, Urbana, IL 61801 USA; [Kannan, Sreeram] Univ Washington, Dept Elect Engn, Seattle, WA 98195 USA; [Oh, Sewoong] Univ Illinois, Coordinated Sci Lab, Dept IESE, Urbana, IL 61801 USA	University of Illinois System; University of Illinois Urbana-Champaign; University of Washington; University of Washington Seattle; University of Illinois System; University of Illinois Urbana-Champaign	Gao, WH (corresponding author), Univ Illinois, Coordinated Sci Lab, Dept ECE, Urbana, IL 61801 USA.	wgao9@illinois.edu; ksreeram@uw.edu; swoh@illinois.edu; pramodv@illinois.edu	Jeong, Yongwook/N-7413-2016		NSF [CNS-1527754, CCF-1553452, CCF-1705007, CCF-1651236, CCF-1617745, CNS-1718270]; GOOGLE Faculty Research Award	NSF(National Science Foundation (NSF)); GOOGLE Faculty Research Award(Google Incorporated)	This work was partially supported by NSF grants CNS-1527754, CCF-1553452, CCF-1705007, CCF-1651236, CCF-1617745, CNS-1718270 and GOOGLE Faculty Research Award.	Acharya Jayadev, MAXIMUM LIKELIHOOD A; BATTITI R, 1994, IEEE T NEURAL NETWOR, V5, P537, DOI 10.1109/72.298224; Beirlant J., 1997, INT J MATH STAT SCI, V6, P17; BERRETT TB, 2016, ARXIV160600304; Biau G., 2015, LECT NEAREST NEIGHBO; Bishop C. M., 2006, MACH LEARN, V128, P1, DOI DOI 10.1002/9780471740360.EBS0904; Bu YH, 2016, IEEE INT SYMP INFO, P1118, DOI 10.1109/ISIT.2016.7541473; Chan C, 2015, P IEEE, V103, P1883, DOI 10.1109/JPROC.2015.2458316; CHOW CK, 1968, IEEE T INFORM THEORY, V14, P462, DOI 10.1109/TIT.1968.1054142; Cover T., 1991, ELEMENTS INFORM THEO, P279, DOI [DOI 10.1007/978-3-642-04898-2_643, 10.1002/0471200611.ch12]; Finak G, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0844-5; Gao S., 2015, ARXIV150800536; Gao SY, 2015, JMLR WORKSH CONF PRO, V38, P277; Gao WH, 2017, IEEE INT SYMP INFO, P1267, DOI 10.1109/ISIT.2017.8006732; Gao Weihao, 2016, P ADV NEUR INF PROC, P2460; Gelfand I., 1959, CALCULATION AMOUNT I; Han YJ, 2015, IEEE T INFORM THEORY, V61, P6343, DOI 10.1109/TIT.2015.2478816; Han YJ, 2015, IEEE INT SYMP INFO, P1372, DOI 10.1109/ISIT.2015.7282680; Jiao JT, 2015, IEEE T INFORM THEORY, V61, P2835, DOI [10.1109/TIT.2015.2412945, 10.1109/tit.2015.2412945]; Jiao Jiantao, 2014, NONASYMPTOTIC THEORY; Kharchenko PV, 2014, NAT METHODS, V11, P740, DOI [10.1038/NMETH.2967, 10.1038/nmeth.2967]; Kozachenko L. F., 1987, Problems of Information Transmission, V23, P95; Kraskov A, 2004, PHYS REV E, V69, DOI 10.1103/PhysRevE.69.066138; Krishnaswamy S, 2014, SCIENCE, V346, P1079, DOI 10.1126/science.1250689; Li Pan, 2017, ARXIV170901249; Marbach D, 2012, NAT METHODS, V9, P796, DOI [10.1038/NMETH.2016, 10.1038/nmeth.2016]; Moon Kevin R, 2017, ARXIV170108083; Muller A. C., 2012, INFORM THEORETIC CLU; Paninski L, 2003, NEURAL COMPUT, V15, P1191, DOI 10.1162/089976603321780272; Paninski L, 2008, IEEE T INFORM THEORY, V54, P4384, DOI 10.1109/TIT.2008.928251; Peng HC, 2005, IEEE T PATTERN ANAL, V27, P1226, DOI 10.1109/TPAMI.2005.159; Perez A, 1959, THEORY PROBABILITY I, V4; Pierson E, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0805-z; Pinsker M. S., 1960, INFORM INFORM STABIL; Polyanskiy Y, 2017, IMA VOL MATH APPL, V161, P211, DOI 10.1007/978-1-4939-7005-6_7; Reshef DN, 2011, SCIENCE, V334, P1518, DOI 10.1126/science.1205438; Rieke F., 1999, SPIKES EXPLORING NEU; Ross BC, 2014, PLOS ONE, V9, DOI 10.1371/journal.pone.0087357; Singh S, 2017, PR MACH LEARN RES, V70; Steeg G. Ver, 2014, STAT, V1050, P27; Szabo Z, 2014, J MACH LEARN RES, V15, P283; Valiant G, 2011, ACM S THEORY COMPUT, P685; Wang Q, 2005, IEEE T INFORM THEORY, V51, P3064, DOI 10.1109/TIT.2005.853314; Wang Q, 2009, IEEE T INFORM THEORY, V55, P2392, DOI 10.1109/TIT.2009.2016060; Wu AR, 2014, NAT METHODS, V11, P41, DOI 10.1038/nmeth.2694; Wu YH, 2016, IEEE T INFORM THEORY, V62, P3702, DOI 10.1109/TIT.2016.2548468	52	36	36	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406007
C	Lai, WS; Huang, JB; Yang, MH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lai, Wei-Sheng; Huang, Jia-Bin; Yang, Ming-Hsuan			Semi-Supervised Learning for Optical Flow with Generative Adversarial Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Convolutional neural networks (CNNs) have recently been applied to the optical flow estimation problem. As training the CNNs requires sufficiently large amounts of labeled data, existing approaches resort to synthetic, unrealistic datasets. On the other hand, unsupervised methods are capable of leveraging real-world videos for training where the ground truth flow fields are not available. These methods, however, rely on the fundamental assumptions of brightness constancy and spatial smoothness priors that do not hold near motion boundaries. In this paper, we propose to exploit unlabeled videos for semi-supervised learning of optical flow with a Generative Adversarial Network. Our key insight is that the adversarial loss can capture the structural patterns of flow warp errors without making explicit assumptions. Extensive experiments on benchmark datasets demonstrate that the proposed semi-supervised algorithm performs favorably against purely supervised and baseline semi-supervised learning schemes.	[Lai, Wei-Sheng; Yang, Ming-Hsuan] Univ Calif Merced, Merced, CA 95343 USA; [Huang, Jia-Bin] Virginia Tech, Blacksburg, VA USA; [Yang, Ming-Hsuan] Nvidia Res, Santa Clara, CA USA	University of California System; University of California Merced; Virginia Polytechnic Institute & State University	Lai, WS (corresponding author), Univ Calif Merced, Merced, CA 95343 USA.	wlai24@ucmerced.edu; jbhuang@vt.edu; mhyang@ucmerced.edu	Yang, Ming-Hsuan/AAE-7350-2019; Yang, Ming-Hsuan/T-9533-2019; Jeong, Yongwook/N-7413-2016	Yang, Ming-Hsuan/0000-0003-4848-2304; 	NSF CAREER [1149783]	NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD))	This work is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.	Bailer C, 2015, IEEE I CONF COMP VIS, P4015, DOI 10.1109/ICCV.2015.457; Baker Simon, 2007, 2007 11th IEEE International Conference on Computer Vision, P1; Brox T, 2011, IEEE T PATTERN ANAL, V33, P500, DOI 10.1109/TPAMI.2010.143; Butler DJ, 2012, LECT NOTES COMPUT SC, V7577, P611, DOI 10.1007/978-3-642-33783-3_44; Collobert R., 2011, NIPS; Denton E, 2015, DEEP GENERATIVE IMAG, DOI DOI 10.5555/; Fischer Philipp, 2015, ARXIV150406852, P2; Ganin Y, 2016, J MACH LEARN RES, V17; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Geiger A, 2012, PROC CVPR IEEE, P3354, DOI 10.1109/CVPR.2012.6248074; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Hoffman J, 2016, FCNS WILD PIXELLEVEL; HORN BKP, 1981, ARTIF INTELL, V17, P185, DOI 10.1016/0004-3702(81)90024-2; Ilg E, 2017, PROC CVPR IEEE, P1647, DOI 10.1109/CVPR.2017.179; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jaderberg M., 2015, ADV NEURAL INFORM PR, P2017, DOI DOI 10.1038/NBT.3343; Jurie F., 2017, ARXIV PREPRINT ARXIV; Kingma D.P, P 3 INT C LEARNING R; Kuznietsov Yevhen, 2017, CVPR; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Luc P., 2016, P INT C NEUR INF PRO; Lucas BD., 1981, ITERATIVE IMAGE REGI, P674, DOI DOI 10.5555/1623264.1623280; Mayer N, 2016, PROC CVPR IEEE, P4040, DOI 10.1109/CVPR.2016.438; Menze M, 2015, PROC CVPR IEEE, P3061, DOI 10.1109/CVPR.2015.7298925; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Patras I., 2016, ICIP; Perazzi Federico, 2016, P IEEE C COMP VIS PA; Ranjan A, 2017, PROC CVPR IEEE, P2720, DOI 10.1109/CVPR.2017.291; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546; Revaud J., 2015, CVPR; Rosenbaum D., 2013, ADV NEURAL INFORM PR, V26; Rosenbaum D., 2016, BRIGHTNESS CONSTANCY; Soomro K., 2012, ARXIV; Sun D., 2008, ECCV; Sun DQ, 2014, INT J COMPUT VISION, V106, P115, DOI 10.1007/s11263-013-0644-x; Wang X., 2016, ECCV; Weinzaepfel P, 2013, IEEE I CONF COMP VIS, P1385, DOI 10.1109/ICCV.2013.175; Yu J. J., 2016, ECCVW; Zhang YT, 2016, PR MACH LEARN RES, V48; Zhu JY, 2017, IEEE I CONF COMP VIS, P2242, DOI 10.1109/ICCV.2017.244	42	36	37	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400034
C	Shen, W; Zhao, K; Guo, YL; Yuille, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Shen, Wei; Zhao, Kai; Guo, Yilu; Yuille, Alan			Label Distribution Learning Forests	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				FACIAL AGE ESTIMATION	Label distribution learning (LDL) is a general learning framework, which assigns to an instance a distribution over a set of labels rather than a single label or multiple labels. Current LDL methods have either restricted assumptions on the expression form of the label distribution or limitations in representation learning, e.g., to learn deep features in an end-to-end manner. This paper presents label distribution learning forests (LDLFs) - a novel label distribution learning algorithm based on differentiable decision trees, which have several advantages: 1) Decision trees have the potential to model any general form of label distributions by a mixture of leaf node predictions. 2) The learning of differentiable decision trees can be combined with representation learning. We define a distribution-based loss function for a forest, enabling all the trees to be learned jointly, and show that an update function for leaf node predictions, which guarantees a strict decrease of the loss function, can be derived by variational bounding. The effectiveness of the proposed LDLFs is verified on several LDL tasks and a computer vision application, showing significant improvements to the state-of-the-art LDL methods.	[Shen, Wei; Zhao, Kai; Guo, Yilu] Shanghai Univ, Sch Commun & Informat Engn, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China; [Shen, Wei; Yuille, Alan] Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA	Shanghai University; Johns Hopkins University	Shen, W (corresponding author), Shanghai Univ, Sch Commun & Informat Engn, Key Lab Specialty Fiber Opt & Opt Access Networks, Shanghai Inst Adv Commun & Data Sci, Shanghai, Peoples R China.; Shen, W (corresponding author), Johns Hopkins Univ, Dept Comp Sci, Baltimore, MD 21218 USA.	shenwei1231@gmail.com; zhaok1206@gmail.com; gyl.luan0@gmail.com; alan.l.yuille@gmail.com	Jeong, Yongwook/N-7413-2016	Yuille, Alan L./0000-0001-5207-9249	National Natural Science Foundation of China [61672336]; Shanghai Municipal Education Commission; Shanghai Education Development Foundation [15CG43]; ONR [N00014-15-1-2356]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Shanghai Municipal Education Commission(Shanghai Municipal Education Commission (SHMEC)); Shanghai Education Development Foundation; ONR(Office of Naval Research)	This work was supported in part by the National Natural Science Foundation of China No. 61672336, in part by "Chen Guang" project supported by Shanghai Municipal Education Commission and Shanghai Education Development Foundation No. 15CG43 and in part by ONR N00014-15-1-2356.	Amit Y, 1997, NEURAL COMPUT, V9, P1545, DOI 10.1162/neco.1997.9.7.1545; Berger AL, 1996, COMPUT LINGUIST, V22, P39; Breiman L, 1996, MACH LEARN, V24, P123, DOI 10.3390/risks8030083; Criminisi A., 2013, DECISION FORESTCOM; Gao B.-B., 2017, ARXIV161101731; Geng X., 2010, P AAAI; Geng X, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3511; Geng X, 2016, IEEE T KNOWL DATA EN, V28, P1734, DOI 10.1109/TKDE.2016.2545658; Geng X, 2014, PROC CVPR IEEE, P1837, DOI 10.1109/CVPR.2014.237; Geng X, 2014, INT C PATT RECOG, P4465, DOI 10.1109/ICPR.2014.764; Geng X, 2013, IEEE T PATTERN ANAL, V35, P2401, DOI 10.1109/TPAMI.2013.51; Guo G., 2010, P 2010 IEEE COMPUTER, P71, DOI DOI 10.1109/CVPRW.2010.5543609; Guo GD, 2008, IEEE T IMAGE PROCESS, V17, P1178, DOI 10.1109/TIP.2008.924280; Guo GD, 2014, PROC CVPR IEEE, P4257, DOI 10.1109/CVPR.2014.542; He Z., 2017, IEEE T IMAGE PROCESS; Ho TK, 1998, IEEE T PATTERN ANAL, V20, P832, DOI 10.1109/34.709601; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kontschieder P, 2015, IEEE I CONF COMP VIS, P1467, DOI 10.1109/ICCV.2015.172; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lanitis A, 2004, IEEE T SYST MAN CY B, V34, P621, DOI 10.1109/TSMCB.2003.817091; Parkhi Omkar M., 2015, BRIT MACH VIS C; Ricanek K, 2006, PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE ON AUTOMATIC FACE AND GESTURE RECOGNITION - PROCEEDINGS OF THE SEVENTH INTERNATIONAL CONFERENCE, P341; Shotton J, 2011, PROC CVPR IEEE, P1297, DOI 10.1109/CVPR.2011.5995316; Tin Kam Ho, 1995, Proceedings of the Third International Conference on Document Analysis and Recognition, P278, DOI 10.1109/ICDAR.1995.598994; Tsoumakas G., 2007, INT J DATA WAREHOUS, V3, P1; Xing C, 2016, PROC CVPR IEEE, P4489, DOI 10.1109/CVPR.2016.486; Yang X., 2016, INT JOINT C ART INT, P2259; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zhou Y, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P1247, DOI 10.1145/2733373.2806328	30	36	36	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400080
C	Xu, D; Ouyang, WL; Alameda-Pineda, X; Ricci, E; Wang, XG; Sebe, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Xu, Dan; Ouyang, Wanli; Alameda-Pineda, Xavier; Ricci, Elisa; Wang, Xiaogang; Sebe, Nicu			Learning Deep Structured Multi-Scale Features using Attention-Gated CRFs for Contour Prediction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Recent works have shown that exploiting multi-scale representations deeply learned via convolutional neural networks (CNN) is of tremendous importance for accurate contour detection. This paper presents a novel approach for predicting contours which advances the state of the art in two fundamental aspects, i.e. multi-scale feature generation and fusion. Different from previous works directly considering multi-scale feature maps obtained from the inner layers of a primary CNN architecture, we introduce a hierarchical deep model which produces more rich and complementary representations. Furthermore, to refine and robustly fuse the representations learned at different scales, the novel Attention-Gated Conditional Random Fields (AG-CRFs) are proposed. The experiments ran on two publicly available datasets (BSDS500 and NYUDv2) demonstrate the effectiveness of the latent AG-CRF model and of the overall hierarchical framework.	[Xu, Dan; Sebe, Nicu] Univ Trento, Trento, Italy; [Ouyang, Wanli] Univ Sydney, Sydney, NSW, Australia; [Alameda-Pineda, Xavier] INRIA, Percept Grp, Rennes, France; [Ricci, Elisa] Univ Perugia, Perugia, Italy; [Wang, Xiaogang] Chinese Univ Hong Kong, Hong Kong, Peoples R China	University of Trento; University of Sydney; Inria; University of Perugia; Chinese University of Hong Kong	Xu, D (corresponding author), Univ Trento, Trento, Italy.	dan.xu@unitn.it; wanli.ouyang@sydney.edu.au; xavier.alameda-pineda@inria.fr; elisa.ricci@unipg.it; xgwang@ee.cuhk.edu.hk; niculae.sebe@unitn.it	Jeong, Yongwook/N-7413-2016	Sebe, Niculae/0000-0002-6597-7248				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P., 2011, TPAMI, V33; Bertasius G, 2015, PROC CVPR IEEE, P4380, DOI 10.1109/CVPR.2015.7299067; CANNY J, 1986, IEEE T PATTERN ANAL, V8, P679, DOI 10.1109/TPAMI.1986.4767851; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Chu X., 2016, NIPS; Chung J., 2014, ARXIV14123555; Comaniciu D, 2002, IEEE T PATTERN ANAL, V24, P603, DOI 10.1109/34.1000236; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dollar P, 2015, IEEE T PATTERN ANAL, V37, P1558, DOI 10.1109/TPAMI.2014.2377715; Dollar P, 2013, IEEE I CONF COMP VIS, P1841, DOI 10.1109/ICCV.2013.231; Felzenszwalb PF, 2004, INT J COMPUT VISION, V59, P167, DOI 10.1023/B:VISI.0000022288.19776.77; Gers FA, 2003, J MACH LEARN RES, V3, P115, DOI 10.1162/153244303768966139; Gupta S, 2014, LECT NOTES COMPUT SC, V8695, P345, DOI 10.1007/978-3-319-10584-0_23; Gupta S, 2013, PROC CVPR IEEE, P564, DOI 10.1109/CVPR.2013.79; Hallman S., 2015, CVPR; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kokkinos I., 2015, COMPUT VIS PATTERN R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Lim JJ, 2013, PROC CVPR IEEE, P3158, DOI 10.1109/CVPR.2013.406; Liu Y, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P1617; Maninis KK, 2016, LECT NOTES COMPUT SC, V9905, P580, DOI 10.1007/978-3-319-46448-0_35; Martin DR, 2004, IEEE T PATTERN ANAL, V26, P530, DOI 10.1109/TPAMI.2004.1273918; Minka T., 2009, NIPS; Mnih V, 2014, ADV NEUR IN, V27; Pont-Tuset Jordi, 2016, TPAMI; Ren XF, 2008, LECT NOTES COMPUT SC, V5304, P533; Ren Z., 2013, CVPR; Shen W, 2015, PROC CVPR IEEE, P3982, DOI 10.1109/CVPR.2015.7299024; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Silberman Nathan, 2012, EUR C COMP VIS, DOI 10.1007/978-3-642-33715-4_54; Tang Y., 2010, NIPS WORKSH TRANSF L; Winn J., 2012, AISTATS; Xiao T, 2015, PROC CVPR IEEE, P2691, DOI 10.1109/CVPR.2015.7298885; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Xu D, 2017, PROC CVPR IEEE, P161, DOI 10.1109/CVPR.2017.25; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang J., 2016, OBJECT CONTOUR DETEC; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; Yu F., 2016, P ICLR 2016; Zeng X., 2016, ARXIV161002579; Zhang ZY, 2016, PROC CVPR IEEE, P669, DOI 10.1109/CVPR.2016.79; Zhao Q., 2015, BMVC	46	36	37	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404004
C	van Hasselt, H; Guez, A; Hessel, M; Mnih, V; Silver, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		van Hasselt, Hado; Guez, Arthur; Hessel, Matteo; Mnih, Volodymyr; Silver, David			Learning values across many orders of magnitude	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Most learning algorithms are not invariant to the scale of the signal that is being approximated. We propose to adaptively normalize the targets used in the learning updates. This is important in value-based reinforcement learning, where the magnitude of appropriate value approximations can change over time when we update the policy of behavior. Our main motivation is prior work on learning to play Atari games, where the rewards were clipped to a predetermined range. This clipping facilitates learning across many different games with a single learning algorithm, but a clipped reward function can result in qualitatively different behavior. Using adaptive normalization we can remove this domain-specific heuristic without diminishing overall performance.	[van Hasselt, Hado; Guez, Arthur; Hessel, Matteo; Mnih, Volodymyr; Silver, David] Google DeepMind, London, England	Google Incorporated	van Hasselt, H (corresponding author), Google DeepMind, London, England.							Amari S, 1998, NEURAL COMPUT, V10, P251, DOI 10.1162/089976698300017746; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Desjardins G., 2015, ADV NEURAL INFORM PR, P2071; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Hasselt H, 2010, ADV NEURAL INFORM PR, V23, P2613, DOI DOI 10.5555/2997046.2997187; Kingma D.P, P 3 INT C LEARNING R; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Liang Y., 2016, INT C AUT AG MULT SY; Martens J, 2015, PR MACH LEARN RES, V37, P2408; McCulloch W., 1943, B MATH BIOPHYS, V5, P115, DOI [10.1007/BF02478259, DOI 10.1007/BF02478259]; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Munos R, 2016, P 30 AAAI C ART INT; Osband I., 2016, ABS160204621 CORR; Rosenblatt F., 1961, PRINCIPLES NEURODYNA, DOI 10.21236/AD0256582; Ross S., 2013, P 29 C UNC ART INT; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Schaul T., 2016, ABS151105952 CORR; Schmidhuber J, 2015, NEURAL NETWORKS, V61, P85, DOI 10.1016/j.neunet.2014.09.003; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Van Hasselt Hado, 2016, P AAAI C ART INT, V30; Wang ZY, 2016, PR MACH LEARN RES, V48; Watkins C.J.C.H., 1989, THESIS, P9	28	36	35	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973700040
C	Koenig, S; Likhachev, M		Dietterich, TG; Becker, S; Ghahramani, Z		Koenig, S; Likhachev, M			Incremental A*	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				SHORTEST	Incremental search techniques find optimal solutions to series of similar search tasks much faster than is possible by solving each search task from scratch. While researchers have developed incremental versions of uninformed search methods, we develop an incremental version of A*. The first search of Lifelong Planning A* is the same as that of A* but all subsequent searches are much faster because it reuses those parts of the previous search tree that are identical to the new search tree. We then present experimental results that demonstrate the advantages of Lifelong Planning A* for simple route planning tasks.	Georgia Inst Technol, Coll Comp, Atlanta, GA 30312 USA	University System of Georgia; Georgia Institute of Technology	Koenig, S (corresponding author), Georgia Inst Technol, Coll Comp, Atlanta, GA 30312 USA.							Frigioni D, 2000, J ALGORITHMS, V34, P251, DOI 10.1006/jagm.1999.1048; LIKHACHEV M, 2001, LIFELONG PLANNING AA; Pearl J, 1985, HEURISTICS INTELLIGE; Ramalingam G, 1996, J ALGORITHM, V21, P267, DOI 10.1006/jagm.1996.0046; Stentz, 1995, P INT JOINT C ART IN, V95, P1652; Thrun Sebastian, 1998, LEARNING LEARN, P181, DOI 10.1007/978-1-4615-5529-2	6	36	36	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1539	1546						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100191
C	Ormoneit, D; Sidenbladh, H; Black, MJ; Hastie, T		Leen, TK; Dietterich, TG; Tresp, V		Ormoneit, D; Sidenbladh, H; Black, MJ; Hastie, T			Learning and tracking cyclic human motion	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present methods for learning and tracking human motion in video. We estimate a statistical model of typical activities from a large set of 3D periodic human motion data by segmenting these data automatically into "cycles". Then the mean and the principal components of the cycles are computed using a new algorithm that accounts for missing information and enforces smooth transitions between cycles. The learned temporal model provides a prior probability distribution over human motions that can be used in a Bayesian framework for tracking human subjects in complex monocular video sequences and recovering their 3D motion.	Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Stanford University	Ormoneit, D (corresponding author), Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA.			Hastie, Trevor/0000-0002-0164-3142				Bobick A., 1996, ICPR, P95; CHAM TJ, 1999, COMPUTER VISION PATT, P239; Isard M., 1996, EUR C COMP VIS, P343; LEVENTON ME, 1998, TR9806 MITS EL RES L; ORMONEIT D, 2000, UNPUB IEEE WORKSH HU; SEITZ SM, 1994, CVPR, P970; SHERLOCK G, 2000, IMPUTING MISSING DAT; SIDENBLADH H, IN PRESS ECCV 2000 D; Yacoob Y, 1999, COMPUT VIS IMAGE UND, V73, P232, DOI 10.1006/cviu.1998.0726	9	36	36	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						894	900						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800126
C	Jaakkola, T; Meila, M; Jebara, T		Solla, SA; Leen, TK; Muller, KR		Jaakkola, T; Meila, M; Jebara, T			Maximum entropy discrimination	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO					We present a general framework for discriminative estimation based on the maximum entropy principle and its extensions. All calculations involve distributions over structures and/or parameters rather than specific settings and reduce to relative entropy projections. This holds even when the data is not separable within the chosen parametric class, in the context of anomaly detection rather than classification, or when the labels in the training set are uncertain or incomplete. Support vector machines are naturally subsumed under this class and we provide several extensions. We are also able to estimate exactly and efficiently discriminative distributions over tree structures of class-conditional models within this framework. Preliminary experimental results are indicative of the potential in these techniques.	MIT, AI Lab, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Jaakkola, T (corresponding author), MIT, AI Lab, 545 Technol Sq, Cambridge, MA 02139 USA.							COVER, 1991, ELEMENTS INFORMATION; JAAKKOLA T, 1999, AITR1668 MIT; JAAKKOLA T, 1998, NIPS 11; JEBARA T, 1998, NIPS, V11; Joachims T., 1999, INT C MACH LEARN; Kivinen J., 1999, P 12 ANN C COMP LEAR; LEVIN, 1978, P MAX ENTR FORM C MI; MEILA M, 1998, NIPS, V11; Vapnik V.N, 1998, STAT LEARNING THEORY; West DB., 2002, INTRO GRAPH THEORY, V2	10	36	37	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						470	476						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700067
C	Xie, XH; Seung, HS		Solla, SA; Leen, TK; Muller, KR		Xie, XH; Seung, HS			Spike-based learning rules and stabilization of persistent neural activity	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				NEURONS; NETWORK; CORTEX	We analyze the conditions under which synaptic learning rules based on action potential timing can be approximated by learning rules based on firing rates. In particular, we consider a form of plasticity in which synapses depress when a presynaptic spike is followed by a postsynaptic spike, and potentiate with the opposite temporal ordering. Such differential anti-Hebbian plasticity can be approximated under certain conditions by a learning rule that depends on the time derivative of the postsynaptic firing rate. Such a learning rule acts to stabilize persistent neural activity patterns in recurrent neural networks.	MIT, Dept Brain & Cognit Sci, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Xie, XH (corresponding author), MIT, Dept Brain & Cognit Sci, E25-618, Cambridge, MA 02139 USA.			Xie, Xiaohui/0000-0002-5479-6345				ABBOTT LF, 1999, ADV NEURAL INFO P SY, V11; Bell CC, 1997, NATURE, V387, P278, DOI 10.1038/387278a0; Bi GQ, 1998, J NEUROSCI, V18, P10464, DOI 10.1523/jneurosci.18-24-10464.1998; Camperi M, 1998, J COMPUT NEUROSCI, V5, P383, DOI 10.1023/A:1008837311948; CANNON SC, 1983, BIOL CYBERN, V49, P127, DOI 10.1007/BF00320393; ERMENTROUT B, 1994, NEURAL COMPUT, V6, P679, DOI 10.1162/neco.1994.6.4.679; GEORGOPOULOS AP, 1993, SCIENCE, V260, P47, DOI 10.1126/science.8465199; Gerstner W, 1996, NATURE, V383, P76, DOI 10.1038/383076a0; Hebb D., 1949, ORG BEHAV; Kempter R, 1999, PHYS REV E, V59, P4498, DOI 10.1103/PhysRevE.59.4498; Markram H, 1997, SCIENCE, V275, P213, DOI 10.1126/science.275.5297.213; Roberts PD, 1999, J COMPUT NEUROSCI, V7, P235, DOI 10.1023/A:1008910918445; Seung HS, 1996, P NATL ACAD SCI USA, V93, P13339, DOI 10.1073/pnas.93.23.13339; SEUNG HS, 2000, J COMPUT NEUROSCI; SEUNG HS, 2000, NEURON; Shriki O., 1998, Society for Neuroscience Abstracts, V24, P143; Zhang K, 1996, J NEUROSCI, V16, P2112	17	36	38	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						199	205						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700029
C	Simard, PY; Bottou, L; Haffner, P; LeCun, Y		Kearns, MS; Solla, SA; Cohn, DA		Simard, PY; Bottou, L; Haffner, P; LeCun, Y			Boxlets: a fast convolution algorithm for signal processing and neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO					Signal processing and pattern recognition algorithms make extensive use of convolution. In many cases, computational accuracy is not as important as computational speed. In feature extraction, for instance, the features of interest in a signal are usually quite distorted. This form of noise justifies some level of quantization in order to achieve faster feature extraction. Our approach consists of approximating regions of the signal with low degree polynomials, and then differentiating the resulting signals in order to obtain impulse functions (or derivatives of impulse functions). With this representation, convolution becomes extremely simple and can be implemented quite effectively. The true convolution can be recovered by integrating the result of the convolution. This method yields substantial speed up in feature extraction and is applicable to convolutional neural networks.	Microsoft Corp, Redmond, WA 98052 USA	Microsoft	Simard, PY (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.							HECKBERT P, 1986, ACM SIGGRAPH COMPUT, V20, P315; LeCun Y., 1995, HDB BRAIN THEORY NEU	2	36	38	1	9	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						571	577						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700081
C	Lewicki, MS; Sejnowski, TJ		Jordan, MI; Kearns, MJ; Solla, SA		Lewicki, MS; Sejnowski, TJ			Learning nonlinear overcomplete representations for efficient coding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We derive a learning algorithm for inferring an overcomplete basis by viewing it as probabilistic model of the observed data. Overcomplete bases allow for better approximation of the underlying statistical density. Using a Laplacian prior on the basis coefficients removes redundancy and leads to representations that are sparse and are a nonlinear function of the data. This can be viewed as a generalization of the technique of independent component analysis and provides a method for blind source separation of fewer mixtures than sources. We demonstrate the utility of overcomplete representations on natural speech and show that compared to the traditional Fourier basis the inferred representations potentially have much greater coding efficiency.	Salk Inst Biol Studies, Howard Hughes Med Inst, Computat Neurobiol Lab, La Jolla, CA 92037 USA	Howard Hughes Medical Institute; Salk Institute	Lewicki, MS (corresponding author), Salk Inst Biol Studies, Howard Hughes Med Inst, Computat Neurobiol Lab, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.		Sejnowski, Terrence/AAV-5558-2021						0	36	37	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						556	562						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700079
C	Thorpe, SJ; Gautrais, J		Mozer, MC; Jordan, MI; Petsche, T		Thorpe, SJ; Gautrais, J			Rapid visual processing using spike asynchrony	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We have investigated the possibility that rapid processing in the visual system could be achieved by using the order of firing in different neurones as a code, rather than more conventional firing rate schemes. Using SPIKENET, a neural net simulator based on integrate-and-fire neurones and in which neurones in the input layer function as analog-to-delay converters, we have modeled the initial stages of visual processing. Initial results are extremely promising. Even with activity in retinal output cells limited to one spike per neuron per image (effectively ruling out any form of rate coding), sophisticated processing based on asynchronous activation was nonetheless possible.			Thorpe, SJ (corresponding author), CTR RECH CERVEAU & COGNIT,F-31062 TOULOUSE,FRANCE.		Gautrais, Jacques/B-4523-2008; THORPE, Simon J/A-5661-2008	Gautrais, Jacques/0000-0002-7002-9920; THORPE, Simon J/0000-0003-4997-3367					0	36	39	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						901	907						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00127
C	Bartlett, MS; Viola, PA; Sejnowski, TJ; Golomb, BA; Larsen, J; Hager, JC; Ekman, P		Touretzky, DS; Mozer, MC; Hasselmo, ME		Bartlett, MS; Viola, PA; Sejnowski, TJ; Golomb, BA; Larsen, J; Hager, JC; Ekman, P			Classifying facial action	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						SALK INST BIOL STUDIES,HOWARD HUGHES MED INST,LA JOLLA,CA 92037	Howard Hughes Medical Institute; Salk Institute			Sejnowski, Terrence/AAV-5558-2021						0	36	36	0	2	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						823	829						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00116
C	Fu, CY; Wu, X; Hu, YB; Huang, HB; He, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fu, Chaoyou; Wu, Xiang; Hu, Yibo; Huang, Huaibo; He, Ran			Dual Variational Generation for Low Shot Heterogeneous Face Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Heterogeneous Face Recognition (HFR) is a challenging issue because of the large domain discrepancy and a lack of heterogeneous data. This paper considers HFR as a dual generation problem, and proposes a novel Dual Variational Generation (DVG) framework. It generates large-scale new paired heterogeneous images with the same identity from noise, for the sake of reducing the domain gap of HFR. Specifically, we first introduce a dual variational autoencoder to represent a joint distribution of paired heterogeneous images. Then, in order to ensure the identity consistency of the generated paired heterogeneous images, we impose a distribution alignment in the latent space and a pairwise identity preserving in the image space. Moreover, the HFR network reduces the domain discrepancy by constraining the pairwise feature distances between the generated paired heterogeneous images. Extensive experiments on four HFR databases show that our method can significantly improve state-of-the-art results.	[Fu, Chaoyou; Wu, Xiang; Hu, Yibo; Huang, Huaibo; He, Ran] CASIA, NLPR & CRIPAC, Beijing, Peoples R China; [Fu, Chaoyou; He, Ran] Univ Chinese Acad Sci, Beijing, Peoples R China; [He, Ran] Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; Chinese Academy of Sciences; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences	He, R (corresponding author), CASIA, NLPR & CRIPAC, Beijing, Peoples R China.; He, R (corresponding author), Univ Chinese Acad Sci, Beijing, Peoples R China.; He, R (corresponding author), Chinese Acad Sci, Ctr Excellence Brain Sci & Intelligence Technol, Beijing, Peoples R China.	chaoyou.fu@nlpr.ia.ac.cn; alfredxiangwu@gmail.com; yibo.hu@cripac.ia.ac.cn; huaibo.huang@cripac.ia.ac.cn; rhe@nlpr.ia.ac.cn	Wu, Xiang/AAU-4792-2021	Wu, Xiang/0000-0001-5317-1338	National Natural Science Foundation of China [61622310]; Beijing Natural Science Foundation [JQ18017]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	This work is funded by the National Natural Science Foundation of China (Grants No. 61622310) and Beijing Natural Science Foundation (Grants No. JQ18017).	[Anonymous], 2014, ICLR; Bhatt H. S., 2012, TECHNICAL REPORT; Chen J., 2009, CVPR; Deng Jiankang, 2019, CVPR, V1; Deng Zhongying, 2019, AAAI; Deng Zhongying, 2019, TIP; Felix Juefei-Xu, 2015, CVPR WORKSH; Goodfellow Ian, 2014, 27 INT C NEURAL INFO; Goswami D., 2011, ICCV WORKSH; Gross R., 2010, IMAGE VISION COMPUTI; Guo Y., 2016, ECCV; He R., 2018, TPAMI; He R., 2017, AAAI; Heusel M., 2017, NEURIPS; Hu Y., 2018, CVPR; Huang D., 2012, TECHNICAL REPORT; Huang H., 2018, NEURIPS; Huang R., 2017, ICCV; Huo Jing, 2017, IEEE T CYBERNETICS; Karras Tero, 2018, ICLR; Klare B., 2011, TPAMI; Lezama J., 2017, CVPR; Li S. Z., 2013, CVPR WORKSH; Liu Ming-Yu, 2016, NEURIPS; Liu X., 2016, ICB; Mao Qi, 2019, CVPR; Peng Chunlei, 2019, PR; Reale C., 2016, CVPR WORKSH; Saxena S., 2016, ECCV WORKSH; Shu Z., 2018, ECCV; Song Lingxiao, 2018, AAAI; Tran L., 2017, CVPR; Wang G., 2019, ARXIV190312003; Wu Xiang, 2018, TIFS; Wu Xiang, 2019, AAAI; Zhang He, 2019, IJCV; Zhang W., 2011, CVPR; Zhao Jian, 2018, CVPR 2018; Zhao Jian, 2018, IJCAI	39	35	36	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302064
C	Romano, Y; Patterson, E; Candes, EJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Romano, Yaniv; Patterson, Evan; Candes, Emmanuel J.			Conformalized Quantile Regression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICTION; MODELS	Conformal prediction is a technique for constructing prediction intervals that attain valid coverage in finite samples, without making distributional assumptions. Despite this appeal, existing conformal methods can be unnecessarily conservative because they form intervals of constant or weakly varying length across the input space. In this paper we propose a new method that is fully adaptive to heteroscedasticity. It combines conformal prediction with classical quantile regression, inheriting the advantages of both. We establish a theoretical guarantee of valid coverage, supplemented by extensive experiments on popular regression datasets. We compare the efficiency of conformalized quantile regression to other conformal methods, showing that our method tends to produce shorter intervals.	[Romano, Yaniv; Patterson, Evan; Candes, Emmanuel J.] Stanford Univ, Dept Stat, Stanford, CA 94305 USA; [Candes, Emmanuel J.] Stanford Univ, Dept Math, Stanford, CA 94305 USA	Stanford University; Stanford University	Romano, Y (corresponding author), Stanford Univ, Dept Stat, Stanford, CA 94305 USA.				Office of Naval Research (ONR) [N00014-16-1-2712]; Army Research Office (ARO) [W911NF-17-1-0304]; Math + X award from the Simons Foundation; ARO; Math + X award; Zuckerman Institute; ISEF Foundation; Viterbi Fellowship, Technion	Office of Naval Research (ONR)(Office of Naval Research); Army Research Office (ARO); Math + X award from the Simons Foundation; ARO; Math + X award; Zuckerman Institute; ISEF Foundation; Viterbi Fellowship, Technion	E. C. was partially supported by the Office of Naval Research (ONR) under grant N00014-16-1-2712, by the Army Research Office (ARO) under grant W911NF-17-1-0304, by the Math + X award from the Simons Foundation and by a generous gift from TwoSigma. E. P. and Y. R. were partially supported by the ARO grant. Y. R. was also supported by the same Math + X award. Y. R. thanks the Zuckerman Institute, ISEF Foundation and the Viterbi Fellowship, Technion, for providing additional research support. We thank Chiara Sabatti for her insightful comments on a draft of this paper and Ryan Tibshirani for his crucial remarks on our early experimental findings.	BASSETT G, 1982, J AM STAT ASSOC, V77, P407; Chen WY, 2018, STAT-US, V7, DOI 10.1002/sta4.173; Chen Wenyu, 2016, ARXIV161109933; Chernozhukov V, 2010, ECONOMETRICA, V78, P1093, DOI 10.3982/ECTA7880; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gal Y, 2016, PR MACH LEARN RES, V48; Hunter DR, 2000, J COMPUT GRAPH STAT, V9, P60; Johansson U., 2015, IEEE INT JOINT C NEU, P1; Johansson U, 2014, IEEE INT CONF BIG DA, P461, DOI 10.1109/BigData.2014.7004263; Johansson U, 2014, MACH LEARN, V97, P155, DOI 10.1007/s10994-014-5453-0; KOENKER R, 1978, ECONOMETRICA, V46, P33, DOI 10.2307/1913643; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402; Lei J, 2018, J AM STAT ASSOC, V113, P1094, DOI 10.1080/01621459.2017.1307116; Lei J, 2014, J R STAT SOC B, V76, P71, DOI 10.1111/rssb.12021; Lei J, 2013, J AM STAT ASSOC, V108, P278, DOI 10.1080/01621459.2012.751873; Lian C, 2016, IEEE T NEUR NET LEAR, V27, P2683, DOI 10.1109/TNNLS.2015.2512283; Linusson Henrik, 2014, Advances in Knowledge Discovery and Data Mining. 18th Pacific-Asia Conference (PAKDD 2014). Proceedings: LNCS 8443, P224, DOI 10.1007/978-3-319-06608-0_19; Meinshausen N, 2006, J MACH LEARN RES, V7, P983; Neely A, 2018, INT C MACH LEARN, V9, P6473; Papadopoulos H, 2002, LECT NOTES ARTIF INT, V2430, P345; Papadopoulos Harris, 2008, Proceedings of the IASTED International Conference on Artificial Intelligence and Applications, P64; Papadopoulos H, 2008, TOOLS ARTIFICIAL INT; Papadopoulos H, 2011, NEURAL NETWORKS, V24, P842, DOI 10.1016/j.neunet.2011.05.008; Papadopoulos H, 2011, J ARTIF INTELL RES, V40, P815, DOI 10.1613/jair.3198; Steinwart I, 2011, BERNOULLI, V17, P211, DOI 10.3150/10-BEJ267; Tagasovska N., 2018, NEURIPS; Takeuchi I, 2006, J MACH LEARN RES, V7, P1231; Taylor JW, 2000, J FORECASTING, V19, P299, DOI 10.1002/1099-131X(200007)19:4<299::AID-FOR775>3.0.CO;2-V; Vovk V, 1999, MACHINE LEARNING, PROCEEDINGS, P444; Vovk V., 2005, ALGORITHMIC LEARNING, DOI DOI 10.1007/B106715; Vovk V, 2015, ANN MATH ARTIF INTEL, V74, P9, DOI 10.1007/s10472-013-9368-4; Vovk V, 2009, ANN STAT, V37, P1566, DOI 10.1214/08-AOS622; Vovk Vladimir, 2017, MACH LEARN, P1; Vovk Vladimir, 2019, ARXIV190206579; Zhou KQ, 1998, J NONPARAMETR STAT, V9, P239, DOI 10.1080/10485259808832745; Zhou KQ, 1996, ANN STAT, V24, P287	38	35	35	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303052
C	Allen-Zhu, Z		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Allen-Zhu, Zeyuan			Natasha 2: Faster Non-Convex Optimization Than SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				CUBIC REGULARIZATION	We design a stochastic algorithm to find epsilon-approximate local minima of any smooth nonconvex function in rate O(epsilon(-3.25)), with only oracle access to stochastic gradients. The best result before this work was O (epsilon(-4) by stochastic gradient descent (SGD).(2)	[Allen-Zhu, Zeyuan] Microsoft Res AI, Redmond, WA 98052 USA		Allen-Zhu, Z (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	zeyuan@csail.mit.edu						Agarwal N, 2017, STOC; [Anonymous], 2015, ADV NEURAL INFORM PR; [Anonymous], ICML; ARORA S, 2015, COLT; Choromanska A., 2015, AISTATS; Dauphin Y.N., 2014, P 27 INT C NEUR INF, P2933, DOI DOI 10.5555/2969033.2969154; Defazio Aaron, 2014, NEURIPS; Duchi J, 2011, J MACH LEARN RES, V12, P2121; FROSTIG R, 2015, ICML; Garber Dan, 2016, ICML; Ghadimi S., 2015, MATH PROGRAM, V156, P1; Goodfellow I. J., 2015, INT C LEARNING REPRE; Hazan E, 2016, PR MACH LEARN RES, V48; Jin C., 2017, ICML; Johnson R., 2013, ADV NEURAL INF PROCE, V26, P315, DOI DOI 10.5555/2999611.2999647; LI YZ, 2017, NEURIPS; Lin Hongzhou, 2015, NEURIPS; Nesterov Y, 2004, INTRO LECT CONVEX PR, VI; Nesterov Y, 2008, MATH PROGRAM, V112, P159, DOI 10.1007/s10107-006-0089-x; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; OJA E, 1982, J MATH BIOL, V15, P267, DOI 10.1007/BF00275687; Reddi S. J., 2016, ICML; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; SHALEVSHWARTZ S, 2016, ICML; Sun R., 2015, FOCS; Xiao L, 2014, SIAM J OPTIMIZ, V24, P2057, DOI 10.1137/140961791; Zeiler Matthew D., 2012, ABS12125701 ARXIV; Zhang L., 2013, P 26 INT C NEUR INF, P980; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]; [No title captured]	46	35	35	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302067
C	Anand, N; Huang, PS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Anand, Namrata; Huang, Po-Ssu			Generative Modeling for Protein Structures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				COMPUTATIONAL DESIGN; OPTIMIZATION; AFFINITY	Analyzing the structure and function of proteins is a key part of understanding biology at the molecular and cellular level. In addition, a major engineering challenge is to design new proteins in a principled and methodical way. Current computational modeling methods for protein design are slow and often require human oversight and intervention. Here, we apply Generative Adversarial Networks (GANs) to the task of generating protein structures, toward application in fast de novo protein design. We encode protein structures in terms of pairwise distances between alpha-carbons on the protein backbone, which eliminates the need for the generative model to learn translational and rotational symmetries. We then introduce a convex formulation of corruption-robust 3D structure recovery to fold the protein structures from generated pairwise distance maps, and solve these problems using the Alternating Direction Method of Multipliers. We test the effectiveness of our models by predicting completions of corrupted protein structures and show that the method is capable of quickly producing structurally plausible solutions.	[Anand, Namrata; Huang, Po-Ssu] Bioengn Dept, Stanford, CA USA		Anand, N (corresponding author), Bioengn Dept, Stanford, CA USA.	namrataa@stanford.edu; possu@stanford.edu		Anand, Namrata/0000-0002-2766-8954; Huang, Possu/0000-0002-7948-2895				Alipanahi B, 2013, J COMPUT BIOL, V20, P296, DOI 10.1089/cmb.2012.0089; AlQuraishi M., 2018, BIORXIV; Ba J., 2017, P 3 INT C LEARN REPR; Berman J.H.M., 2000, NUCLEIC ACIDS RES, V106, P16972; Boomsma W, 2008, P NATL ACAD SCI USA, V105, P8932, DOI 10.1073/pnas.0801715105; Bottou L., 2017, ARXIV170107875STATML; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Boyd S, 2004, CONVEX OPTIMIZATION; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Canutescu AA, 2003, PROTEIN SCI, V12, P963, DOI 10.1110/ps.0242703; Das R, 2008, ANNU REV BIOCHEM, V77, P363, DOI 10.1146/annurev.biochem.77.062906.171838; Duvenaud David K, 2015, P NIPS; ENGH RA, 1991, ACTA CRYSTALLOGR A, V47, P392, DOI 10.1107/S0108767391001071; Gomez-Bombarelli Rafael, 2016, ACS CENTRAL SCI; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I., 2017, INT C NEURAL INF PRO; Hamelryck T, 2006, PLOS COMPUT BIOL, V2, P1121, DOI 10.1371/journal.pcbi.0020131; Hopf TA, 2014, ELIFE, V3, DOI 10.7554/eLife.03430; Huang PS, 2016, NATURE, V537, P320, DOI 10.1038/nature19946; Huang PS, 2011, PLOS ONE, V6, DOI 10.1371/journal.pone.0024109; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Jiang L, 2008, SCIENCE, V319, P1387, DOI 10.1126/science.1152692; Joosten RP, 2011, NUCLEIC ACIDS RES, V39, pD411, DOI 10.1093/nar/gkq1105; KABSCH W, 1983, BIOPOLYMERS, V22, P2577, DOI 10.1002/bip.360221211; Kamisetty H, 2013, P NATL ACAD SCI USA, V110, P15674, DOI 10.1073/pnas.1314045110; Killoran N., 2017, GENERATING DESIGNING; Kusner M. J., 2017, P INT C MACHINE LEAR; Mandell DJ, 2009, NAT METHODS, V6, P551, DOI 10.1038/nmeth0809-551; Metz L., 2016, ARXIV161102163; Mirza M., 2014, CONDITIONAL GENERATI; O'Donoghue B, 2016, J OPTIMIZ THEORY APP, V169, P1042, DOI 10.1007/s10957-016-0892-3; ODonoghue B., 2017, SCS SPLITTING CONIC; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Pratap JV, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2012.0369; Radford A., 2015, P COMP C; Rothlisberger D, 2008, NATURE, V453, P190, DOI 10.1038/nature06879; Salimans T., 2016, ADV NEUR IN, P2234; Shi WZ, 2016, PROC CVPR IEEE, P1874, DOI 10.1109/CVPR.2016.207; Siegel JB, 2010, SCIENCE, V329, P309, DOI 10.1126/science.1190239; Smart AD, 2017, P NATL ACAD SCI USA, V114, pE8174, DOI 10.1073/pnas.1705064114; Strauch EM, 2017, NAT BIOTECHNOL, V35, P667, DOI 10.1038/nbt.3907; Tinberg CE, 2013, NATURE, V501, P212, DOI 10.1038/nature12443; Wang Jingxue, 2018, ARXIV180107130; Whitehead TA, 2012, NAT BIOTECHNOL, V30, P543, DOI 10.1038/nbt.2214; Wu ZQ, 2018, CHEM SCI, V9, P513, DOI 10.1039/c7sc02664a; Yeh R., 2016, PREPRINT; Zheng F, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0178272	47	35	35	2	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002008
C	Lin, ZA; Khetan, A; Fanti, G; Oh, S		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lin, Zinan; Khetan, Ashish; Fanti, Giulia; Oh, Sewoong			PacGAN: The power of two samples in generative adversarial networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Generative adversarial networks (GANs) are a technique for learning generative models of complex data distributions from samples. Despite remarkable advances in generating realistic images, a major shortcoming of GANs is the fact that they tend to produce samples with little diversity, even when trained on diverse datasets. This phenomenon, known as mode collapse, has been the focus of much recent work. We study a principled approach to handling mode collapse, which we call packing. The main idea is to modify the discriminator to make decisions based on multiple samples from the same class, either real or artificially generated. We draw analysis tools from binary hypothesis testing-in particular the seminal result of Blackwell [4]-to prove a fundamental connection between packing and mode collapse. We show that packing naturally penalizes generators with mode collapse, thereby favoring generator distributions with less mode collapse during the training process. Numerical experiments on benchmark datasets suggest that packing provides significant improvements.	[Lin, Zinan; Fanti, Giulia] Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA; [Khetan, Ashish; Oh, Sewoong] Univ Illinois, IESE Dept, Champaign, IL USA	Carnegie Mellon University; University of Illinois System; University of Illinois Urbana-Champaign	Lin, ZA (corresponding author), Carnegie Mellon Univ, ECE Dept, Pittsburgh, PA 15213 USA.	zinanl@andrew.cmu.edu; ashish.khetan09@gmail.com; gfanti@andrew.cmu.edu; swoh@illinois.edu	Lin, Zinan/T-2375-2019		NSF [CNS-1527754, CCF-1553452, CCF-1705007, RI-1815535, ACI-1445606]; Google Faculty Research Award; National Science Foundation [OCI-1053575]	NSF(National Science Foundation (NSF)); Google Faculty Research Award(Google Incorporated); National Science Foundation(National Science Foundation (NSF))	This work is supported by NSF awards CNS-1527754, CCF-1553452, CCF-1705007, and RI-1815535 and Google Faculty Research Award. This work used the Extreme Science and Engineering Discovery Environment (XSEDE), which is supported by National Science Foundation grant number OCI-1053575. Specifically, it used the Bridges system, which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC). This work is partially supported by the generous research credits on AWS cloud computing resources from Amazon.	Arjovsky M., 2017, ARXIV170107875; Arora S., 2017, ARXIV170608224; BLACKWELL D, 1953, ANN MATH STAT, V24, P265, DOI 10.1214/aoms/1177729032; Che Tong, 2016, ARXIV161202136; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Donahue J., 2016, ARXIV160509782; Dumoulin Vincent, 2016, ARXIV E PRINTS; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Kairouz P, 2017, IEEE T INFORM THEORY, V63, P4037, DOI 10.1109/TIT.2017.2685505; Karras T., 2017, PROGR GROWING GANS I; Kingma D.P, P 3 INT C LEARNING R; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; LeCun Y., 2010, MNIST HANDWRITTEN DI; Li J, 2017, ARXIV170609884; Liu S., 2017, ARXIV170508991; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Metz Luke, 2016, ARXIV161102163; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Reed S, 2016, PR MACH LEARN RES, V48; Salimans T, 2016, ADV NEUR IN, V29; Srivastava Akash, 2017, ADV NEURAL INFORM PR, P3310, DOI DOI 10.5555/3294996.3295090; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thekumparampil K., ATTENTION BASED GRAP, V1803, P1; Tolstikhin Ilya O., 2017, ARXIV170102386; Zaheer Manzil, 2017, NIPS, P3391	27	35	35	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301048
C	Shafahi, A; Huang, WR; Najibi, M; Suciu, O; Studer, C; Dumitras, T; Goldstein, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Shafahi, Ali; Huang, W. Ronny; Najibi, Mahyar; Suciu, Octavian; Studer, Christoph; Dumitras, Tudor; Goldstein, Tom			Poison Frogs! Targeted Clean-Label Poisoning Attacks on Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Data poisoning is an attack on machine learning models wherein the attacker adds examples to the training set to manipulate the behavior of the model at test time. This paper explores poisoning attacks on neural nets. The proposed attacks use "clean-labels"; they don't require the attacker to have any control over the labeling of training data. They are also targeted; they control the behavior of the classifier on a specific test instance without degrading overall classifier performance. For example, an attacker could add a seemingly innocuous image (that is properly labeled) to a training set for a face recognition engine, and control the identity of a chosen person at test time. Because the attacker does not need to control the labeling function, poisons could be entered into the training set simply by leaving them on the web and waiting for them to be scraped by a data collection bot. We present an optimization-based method for crafting poisons, and show that just one single poison image can control classifier behavior when transfer learning is used. For full end-to-end training, we present a "watermarking" strategy that makes poisoning reliable using multiple (approximate to 50) poisoned training instances. We demonstrate our method by generating poisoned frog images from the CIFAR dataset and using them to manipulate image classifiers.	[Shafahi, Ali; Huang, W. Ronny; Najibi, Mahyar; Suciu, Octavian; Dumitras, Tudor; Goldstein, Tom] Univ Maryland, College Pk, MD 20742 USA; [Studer, Christoph] Cornell Univ, Ithaca, NY 14853 USA	University System of Maryland; University of Maryland College Park; Cornell University	Shafahi, A (corresponding author), Univ Maryland, College Pk, MD 20742 USA.	ashafahi@cs.umd.edu; wrhuang@umd.edu; najibi@cs.umd.edu; osuciu@umiacs.umd.edu; studer@cornell.edu; tudor@umiacs.umd.edu; tomg@cs.umd.edu	Suciu, Octavian/AAW-2675-2021		Office of Naval Research [N00014-17-1-2078]; DARPA Lifelong Learning Machines [FA8650-18-2-7833]; DARPA YFA program [D18AP00055]; Sloan Foundation; Xilinx, Inc.; US National Science Foundation (NSF) [ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, ECCS-1824379]; Department of Defense	Office of Naval Research(Office of Naval Research); DARPA Lifelong Learning Machines; DARPA YFA program(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Sloan Foundation(Alfred P. Sloan Foundation); Xilinx, Inc.; US National Science Foundation (NSF)(National Science Foundation (NSF)); Department of Defense(United States Department of Defense)	Goldstein and Shafahi were supported by the Office of Naval Research (N00014-17-1-2078), DARPA Lifelong Learning Machines (FA8650-18-2-7833), the DARPA YFA program (D18AP00055), and the Sloan Foundation. Studer was supported in part by Xilinx, Inc. and by the US National Science Foundation (NSF) under grants ECCS-1408006, CCF-1535897, CCF-1652065, CNS-1717559, and ECCS-1824379. Dumitras and Suciu were supported by the Department of Defense.	Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5; Biggio B., 2012, INT C MACH LEARN; Biggio B., 2013, JOINT EUR C MACH LEA, P387, DOI DOI 10.1007/978-3-642-40994-3_25; Chen X., 2017, ARXIV171205526; Diakonikolas I., 2016, ARXIV160603077; Goldstein T., 2014, FIELD GUIDE FORWARD; Goodfellow I. J., 2015, P ICLR; Gu T., 2017, ARXIV170806733; He K., 2015, CVPR, V7, P171, DOI DOI 10.3389/FPSYG.2013.00124; Koh P. W, 2017, ARXIV170304730; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Liu Yingqi, 2017, TROJANING ATTACK NEU, P3; Mahloujifar S, 2017, 2017950 CRYPT EPRINT; Mahloujifar S., 2017, ABS171103707 CORR; Munoz-Gonzalez L., 2017, P 10 ACM WORKSHOP AR, P27, DOI [10.1145/3128572.3140451, 10.1145/3128572]; Nelson B., 2008, P 1 US WORKSH LARG S; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Steinhardt J., 2017, ARXIV170603691; Suciu O., 2018, ARXIV180306975; Szegedy C, 2013, 2 INT C LEARNING REP; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Szegedy C, 2015, PROC CVPR IEEE, P1, DOI 10.1109/CVPR.2015.7298594; YANG C., 2017, ARXIV170301340	23	35	35	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000059
C	Dai, B; Lin, DH		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Dai, Bo; Lin, Dahua			Contrastive Learning for Image Captioning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Image captioning, a popular topic in computer vision, has achieved substantial progress in recent years. However, the distinctiveness of natural descriptions is often overlooked in previous work. It is closely related to the quality of captions, as distinctive captions are more likely to describe images with their unique aspects. In this work, we propose a new learning method, Contrastive Learning (CL), for image captioning. Specifically, via two constraints formulated on top of a reference model, the proposed method can encourage distinctiveness, while maintaining the overall quality of the generated captions. We tested our method on two challenging datasets, where it improves the baseline model by significant margins. We also showed in our studies that the proposed method is generic and can be used for models with various structures.	[Dai, Bo; Lin, Dahua] Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China	Chinese University of Hong Kong	Dai, B (corresponding author), Chinese Univ Hong Kong, Dept Informat Engn, Hong Kong, Peoples R China.	db014@ie.cuhk.edu.hk; dhlin@ie.cuhk.edu.hk	Lin, Dahua/W-6576-2019; Jeong, Yongwook/N-7413-2016	Lin, Dahua/0000-0002-8865-7896; Dai, Bo/0000-0003-0777-9232	Big Data Collaboration Research grant from SenseTime Group (CUHK) [TS1610626]; General Research Fund (GRF) of Hong Kong [14236516]; Early Career Scheme (ECS) of Hong Kong [24204215]	Big Data Collaboration Research grant from SenseTime Group (CUHK); General Research Fund (GRF) of Hong Kong; Early Career Scheme (ECS) of Hong Kong	This work is partially supported by the Big Data Collaboration Research grant from SenseTime Group (CUHK Agreement No.TS1610626), the General Research Fund (GRF) of Hong Kong (No.14236516) and the Early Career Scheme (ECS) of Hong Kong (No.24204215).	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Devlin J., 2015, ARXIV150504467; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Gutmann MU, 2012, J MACH LEARN RES, V13, P307; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jas M, 2015, PROC CVPR IEEE, P2727, DOI 10.1109/CVPR.2015.7298889; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Kuznetsova Polina, 2014, T ASSOC COMPUT LING, V2, P351, DOI DOI 10.1162/TACL_A_00188; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu S, 2016, ARXIV161200370; Lu Jiasen, 2016, ABS161201887 CORR; Mao JH, 2016, PROC CVPR IEEE, P11, DOI 10.1109/CVPR.2016.9; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Park CC, 2017, PROC CVPR IEEE, P6432, DOI 10.1109/CVPR.2017.681; Rennie SJ, 2017, PROC CVPR IEEE, P1179, DOI 10.1109/CVPR.2017.131; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vedantam R., 2017, ARXIV170102870; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yao T, 2016, ARXIV161101646; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503; Zhou L., 2016, ARXIV160604621	27	35	37	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400086
C	Hofer, C; Kwitt, R; Niethammer, M; Uhl, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Hofer, Christoph; Kwitt, Roland; Niethammer, Marc; Uhl, Andreas			Deep Learning with Topological Signatures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				PERSISTENCE; SHAPES	Inferring topological and geometrical information from data can offer an alternative perspective on machine learning problems. Methods from topological data analysis, e.g., persistent homology, enable us to obtain such information, typically in the form of summary representations of topological features. However, such topological signatures often come with an unusual structure (e.g., multisets of intervals) that is highly impractical for most machine learning techniques. While many strategies have been proposed to map these topological signatures into machine learning compatible representations, they suffer from being agnostic to the target learning task. In contrast, we propose a technique that enables us to input topological signatures to deep neural networks and learn a task-optimal representation during training. Our approach is realized as a novel input layer with favorable theoretical properties. Classification experiments on 2D object shapes and social network graphs demonstrate the versatility of the approach and, in case of the latter, we even outperform the state-of-the-art by a large margin.	[Hofer, Christoph; Kwitt, Roland; Uhl, Andreas] Univ Salzburg, Dept Comp Sci, Salzburg, Austria; [Niethammer, Marc] Univ N Carolina, Chapel Hill, NC USA	Salzburg University; University of North Carolina; University of North Carolina Chapel Hill	Hofer, C (corresponding author), Univ Salzburg, Dept Comp Sci, Salzburg, Austria.	chofer@cosy.sbg.ac.at; roland.kwitt@sbg.ac.at; mn@cs.unc.edu; Uhl@cosy.sbg.ac.at	Jeong, Yongwook/N-7413-2016; Kwitt, Roland/AFS-8639-2022		Austrian Science Fund FWF [00012]; Spinal Cord Injury and Tissue Regeneration Center Salzburg (SCI-TReCS), Paracelsus Medical University, Salzburg	Austrian Science Fund FWF(Austrian Science Fund (FWF)); Spinal Cord Injury and Tissue Regeneration Center Salzburg (SCI-TReCS), Paracelsus Medical University, Salzburg	This work was partially funded by the Austrian Science Fund FWF (KLI project 00012) and the Spinal Cord Injury and Tissue Regeneration Center Salzburg (SCI-TReCS), Paracelsus Medical University, Salzburg.	ADAMS H, 2017, JMLR, V18; Adcock A, 2013, ARXIV PREPRINT ARXIV; Bai X., 2009, ICCV WORKSH NONR SHA; Barnett I., 2016, CORR; Bendich P, 2016, ANN APPL STAT, V10, P198, DOI 10.1214/15-AOAS886; Bubenik P, 2015, J MACH LEARN RES, V16, P77; Carlsson G, 2008, INT J COMPUT VISION, V76, P1, DOI 10.1007/s11263-007-0056-x; Carlsson G, 2009, B AM MATH SOC, V46, P255, DOI 10.1090/S0273-0979-09-01249-X; Chazal F., 2014, JOCG, V6, P140; Chazal F, 2013, J ACM, V60, DOI 10.1145/2535927; Chazal F, 2009, COMPUT GRAPH FORUM, V28, P1393, DOI 10.1111/j.1467-8659.2009.01516.x; Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5; Cohen-Steiner D, 2010, FOUND COMPUT MATH, V10, P127, DOI 10.1007/s10208-010-9060-6; Edelsbrunner H, 2002, DISCRETE COMPUT GEOM, V28, P511, DOI 10.1007/s00454-002-2885-2; Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO; Hatcher A., 2005, ALGEBRAIC TOPOLOGY; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kusano G., 2016, ICML; Kwitt R., 2015, NIPS; Latecki L., 2000, CVPR; Li C., 2014, CVPR; Mischaikow K, 2013, DISCRETE COMPUT GEOM, V50, P330, DOI 10.1007/s00454-013-9529-6; Niepert M, 2016, PR MACH LEARN RES, V48; Qi CR, 2017, PROC CVPR IEEE, P77, DOI 10.1109/CVPR.2017.16; Ravanbakhsh S., 2017, ICLR, P1; Reininghaus R., 2015, CVPR; Singh G, 2008, J VISION, V8, DOI 10.1167/8.8.11; Turner K, 2014, INF INFERENCE, V3, P310, DOI 10.1093/imaiai/iau011; Wang XG, 2014, PATTERN RECOGN, V47, P2116, DOI 10.1016/j.patcog.2013.12.008; Yanardag P., 2015, KDD, P1365	31	35	35	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401065
C	Kumar, A; Sattigeri, P; Fletcher, PT		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kumar, Abhishek; Sattigeri, Prasanna; Fletcher, P. Thomas			Semi-supervised Learning with GANs: Manifold Invariance with Improved Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Semi-supervised learning methods using Generative adversarial networks (GANs) have shown promising empirical success recently. Most of these methods use a shared discriminator/classifier which discriminates real examples from fake while also predicting the class label. Motivated by the ability of the GANs generator to capture the data manifold well, we propose to estimate the tangent space to the data manifold using GANs and employ it to inject invariances into the classifier. In the process, we propose enhancements over existing methods for learning the inverse mapping (i.e., the encoder) which greatly improves in terms of semantic similarity of the reconstructed sample with the input sample. We observe considerable empirical gains in semi-supervised learning over baselines, particularly in the cases when the number of labeled examples is low. We also provide insights into how fake examples influence the semi-supervised learning procedure.	[Kumar, Abhishek; Sattigeri, Prasanna] IBM Res AI, Yorktown Hts, NY 10598 USA; [Fletcher, P. Thomas] Univ Utah, Salt Lake City, UT USA	Utah System of Higher Education; University of Utah	Kumar, A (corresponding author), IBM Res AI, Yorktown Hts, NY 10598 USA.	abhishk@us.ibm.com; psattig@us.ibm.com; fletcher@sci.utah.edu	Sattigeri, Prasanna/U-4063-2019	Sattigeri, Prasanna/0000-0003-4435-0486				Arjovsky M., 2017, ARXIV170107875; Bernstein Alexander V, 2012, ARXIV12126031; Bernstein AV, 2014, ICML 2014 WORKSH TOP, V25, P1; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Canas G., 2012, ADV NEURAL INF PROCE, P2465; Chen GL, 2011, APPL NUMER HARMON AN, P199, DOI 10.1007/978-0-8176-8095-4_10; Djork-Arn, ICLR 2016; Donahue J., 2016, ARXIV160509782; dos Santos Cicero Nogueira, 2017, ARXIV170702198; Dumoulin Vincent, 2016, ARXIV E PRINTS; Friedman J., 2001, ELEMENTS STAT LEARNI, V1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Gulrajani I, 2017, P NIPS 2017; Jia K, 2015, NEUROCOMPUTING, V160, P250, DOI 10.1016/j.neucom.2015.02.023; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Laine Samuli, 2016, ARXIV161002242; Lerman G, 2010, PREPRINT; Maaloe L, 2016, PR MACH LEARN RES, V48; Menon AK, 2016, PR MACH LEARN RES, V48; Miyato T, 2015, ARXIV150700677; Mohamed Shakir, 2016, ARXIV161003483; Mroueh Y, 2017, ADV NEUR IN, V30; Mroueh Y, 2017, PR MACH LEARN RES, V70; Mroueh Youssef, 2015, ADV NEURAL INFORM PR, P1558; Niyogi P, 2011, SIAM J COMPUT, V40, P646, DOI 10.1137/090762932; Nowozin S, 2016, ADV NEUR IN, V29; Odena A., 2016, SEMISUPERVISED LEARN; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Raj Anant, 2017, AISTATS; Rasmus A., 2015, ADV NEURAL INFORM PR, P3546, DOI DOI 10.1186/1477-5956-9-S1-S5; Rifai S., 2011, ADV NEURAL INF PROCE, V24; Salimans T, 2016, ADV NEUR IN, V29; Simard PY, 1998, LECT NOTES COMPUT SC, V1524, P239; Spivak M., 1999, COMPREHENSIVE INTRO, VI; Springenberg Jost Tobias, 2015, ARXIV151106390; Tu Z, 2007, PROC CVPR IEEE, P500; Vidal R, 2005, IEEE T PATTERN ANAL, V27, P1945, DOI 10.1109/TPAMI.2005.244; Zhao J., 2015, ARXIV150602351	41	35	36	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405060
C	Maddison, CJ; Lawson, D; Tucker, G; Heess, N; Norouzi, M; Mnih, A; Doucet, A; Teh, YW		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Maddison, Chris J.; Lawson, Dieterich; Tucker, George; Heess, Nicolas; Norouzi, Mohammad; Mnih, Andriy; Doucet, Arnaud; Teh, Yee Whye			Filtering Variational Objectives	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					When used as a surrogate objective for maximum likelihood estimation in latent variable models, the evidence lower bound (ELBO) produces state-of-the-art results. Inspired by this, we consider the extension of the ELBO to a family of lower bounds defined by a particle filter's estimator of the marginal likelihood, the filtering variational objectives (FIVOs). FIVOs take the same arguments as the ELBO, but can exploit a model's sequential structure to form tighter bounds. We present results that relate the tightness of FIVO's bound to the variance of the particle filter's estimator by considering the generic case of bounds defined as log-transformed likelihood estimators. Experimentally, we show that training with FIVO results in substantial improvements over training the same model architecture with the ELBO on sequential data.	[Maddison, Chris J.; Heess, Nicolas; Mnih, Andriy; Teh, Yee Whye] DeepMind, London, England; [Lawson, Dieterich; Tucker, George; Norouzi, Mohammad] Google Brain, Mountain View, CA USA; [Maddison, Chris J.; Doucet, Arnaud] Univ Oxford, Oxford, England	Google Incorporated; University of Oxford	Maddison, CJ (corresponding author), DeepMind, London, England.; Maddison, CJ (corresponding author), Univ Oxford, Oxford, England.	cmaddis@google.com; dieterichl@google.com; gjt@google.com	Jeong, Yongwook/N-7413-2016	Doucet, Arnaud/0000-0002-7662-419X	EPSRC [EP/K000276/1]; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC [617071]	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC(European Research Council (ERC))	We thank Matt Hoffman, Matt Johnson, Danilo J. Rezende, Jascha Sohl-Dickstein, and Theophane Weber for helpful discussions and support in this project. A. Doucet was partially supported by the EPSRC grant EP/K000276/1. Y. W. Teh's research leading to these results has received funding from the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013) ERC grant agreement no. 617071.	Abadi M., TENSORFLOW LARGE SCA; Andrieu C, 2010, J R STAT SOC B, V72, P269, DOI 10.1111/j.1467-9868.2009.00736.x; Beal Matthew James, 2003, VARIATIONAL ALGORITH; Bengio Y, 2013, NIPS; Berard J, 2014, ELECTRON J PROBAB, V19, P1, DOI 10.1214/EJP.v19-3428; Bornschein J., 2015, ICLR; Boulanger-Lewandowski N., 2012, ICML; Bowman S. R., 2016, ARXIV, P10; Burda Y., 2016, ICLR; Burda Y., 2015, AISTATS; Cerou F, 2011, ANN I H POINCARE-PR, V47, P629, DOI 10.1214/10-AIHP358; Chung J., 2015, NIPS; Del Moral P., 2004, PROB APPL S; DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Dinh L., 2016, ARXIV160508803CSSTAT; Doucet A., 2009, OXFORD HDB NONLINEAR, P656; Gal Y., 2016, U CAMBRIDGE; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grosse R. B., 2015, ARXIV151102543; Gu S., 2015, NIPS; Hoffman MD, 2013, J MACH LEARN RES, V14, P1303; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D. P., 2014, ICML; Kingma D. P., 2016, NIPS; Kucukelbir Alp, 2016, ARXIV160300788; Le T. A., 2017, ARXIV170510306; Mnih Andriy, 2014, INT C MACH LEARN; Mnih Andriy, 2016, P ICML; Mohamed Shakir, 2016, ARXIV161003483; Naesseth C. A., 2017, ARXIV170511140; Nowozin S, 2016, ADV NEUR IN, V29; Pitt MK, 2012, J ECONOMETRICS, V171, P134, DOI 10.1016/j.jeconom.2012.06.004; Ranganath R., 2014, AISTATS; Ranganath R., 2016, NIPS; Rezende D.J., 2014, ICML; REZENDE DJ, 2015, ICML; Salimans T., 2015, ICML; Sonderbyz Soren Kaae, 2016, NIPS; Tran D., 2017, ARXIV170208896; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; WU CFJ, 1983, ANN STAT, V11, P95, DOI 10.1214/aos/1176346060	43	35	35	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406062
C	Wang, YX; Hebert, M		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Wang, Yu-Xiong; Hebert, Martial			Learning from Small Sample Sets by Combining Unsupervised Meta-Training with CNNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					This work explores CNNs for the recognition of novel categories from few examples. Inspired by the transferability properties of CNNs, we introduce an additional unsupervised meta-training stage that exposes multiple top layer units to a large amount of unlabeled real-world images. By encouraging these units to learn diverse sets of low-density separators across the unlabeled data, we capture a more generic, richer description of the visual world, which decouples these units from ties to a specific set of categories. We propose an unsupervised margin maximization that jointly estimates compact high-density regions and infers low-density separators. The low-density separator (LDS) modules can be plugged into any or all of the top layers of a standard CNN architecture. The resulting CNNs significantly improve the performance in scene classification, fine-grained recognition, and action recognition with small training samples.	[Wang, Yu-Xiong; Hebert, Martial] Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA	Carnegie Mellon University	Wang, YX (corresponding author), Carnegie Mellon Univ, Inst Robot, Pittsburgh, PA 15213 USA.	yuxiongw@cs.cmu.edu; hebert@cs.cmu.edu			ONR MURI [N000141612007]; U.S. Army Research Laboratory (ARL) under the Collaborative Technology Alliance Program [W911NF-10-2-0016]	ONR MURI(MURIOffice of Naval Research); U.S. Army Research Laboratory (ARL) under the Collaborative Technology Alliance Program	We thank Liangyan Gui, Carl Doersch, and Deva Ramanan for valuable and insightful discussions. This work was supported in part by ONR MURI N000141612007 and U.S. Army Research Laboratory (ARL) under the Collaborative Technology Alliance Program, Cooperative Agreement W911NF-10-2-0016. We also thank NVIDIA for donating GPUs and AWS Cloud Credits for Research program.	Ahmed A., 2008, ECCV; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Azizpour Hossein, 2015, TPAMI, P1; Ben-david S., 2009, AISTATS; Bergamo A, 2014, IEEE T PATTERN ANAL, V36, P1988, DOI 10.1109/TPAMI.2014.2313111; Bertinetto Luca, 2016, NIPS; Chapelle O, 2005, P 10 INT WORKSH ART, V2005, P57; Choi J, 2013, PROC CVPR IEEE, P875, DOI 10.1109/CVPR.2013.118; Dai D., 2013, ICCV; Dosovitskiy A., 2014, ADV NEURAL INFORM PR, V27, P766, DOI [DOI 10.1109/TPAMI.2015.2496141, 10.48550/arXiv.1406.6909]; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Hariharan B, 2016, ARXIV160602819; Held D., 2016, ICLR; Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50; Jia Y., 2014, P 22 ACM INT C MULT, P675; Joulin A, 2016, LECT NOTES COMPUT SC, V9911, P67, DOI 10.1007/978-3-319-46478-7_5; Koch G., 2015, ICML DEEP LEARNING W; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lake BM, 2015, SCIENCE, V350, P1332, DOI 10.1126/science.aab3050; Li Z, 2016, LECT NOTES COMPUT SC, V9906, P541, DOI 10.1007/978-3-319-46475-6_34; Nilsback ME, 2008, SIXTH INDIAN CONFERENCE ON COMPUTER VISION, GRAPHICS & IMAGE PROCESSING ICVGIP 2008, P722, DOI 10.1109/ICVGIP.2008.47; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Rastegari M, 2012, LECT NOTES COMPUT SC, V7577, P876, DOI 10.1007/978-3-642-33783-3_63; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Thomee B, 2016, COMMUN ACM, V59, P64, DOI 10.1145/2812802; Torralba A., 2009, CVPR; Vinyals Oriol, 2016, ARXIV160604080, P3630; Wang Y.-X., 2016, AAAI; Wang Y.X., 2015, CVPR; Wang YX, 2016, LECT NOTES COMPUT SC, V9910, P616, DOI 10.1007/978-3-319-46466-4_37; Weston J., 2008, P 25 INT C MACHINE L, P1168, DOI [DOI 10.1145/1390156.1390303, 10.1145/1390156.1390303]; Xiao JX, 2016, INT J COMPUT VISION, V119, P3, DOI 10.1007/s11263-014-0748-y; Yang S, 2015, IEEE I CONF COMP VIS, P3676, DOI 10.1109/ICCV.2015.419; Yao BP, 2011, IEEE I CONF COMP VIS, P1331, DOI 10.1109/ICCV.2011.6126386; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhou Bolei, 2014, ADV NEURAL INFORM PR, P7, DOI DOI 10.5555/2968826.2968881	38	35	36	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703044
C	Ndiaye, E; Fercoq, O; Gramfort, A; Salmon, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph			GAP Safe screening rules for sparse multi-task and multi-class models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA				LASSO; REGRESSION; SELECTION	High dimensional regression benefits from sparsity promoting regularizations. Screening rules leverage the known sparsity of the solution by ignoring some variables in the optimization, hence speeding up solvers. When the procedure is proven not to discard features wrongly the rules are said to be safe. In this paper we derive new safe rules for generalized linear models regularized with l(1) and l(1)/l(2) norms. The rules are based on duality gap computations and spherical safe regions whose diameters converge to zero. This allows to discard safely more variables, in particular for low regularization parameters. The GAP Safe rule can cope with any iterative solver and we illustrate its performance on coordinate descent for multi-task Lasso, binary and multinomial logistic regression, demonstrating significant speed ups on all tested datasets with respect to previous safe rules.	[Ndiaye, Eugene; Fercoq, Olivier; Gramfort, Alexandre; Salmon, Joseph] Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France	Centre National de la Recherche Scientifique (CNRS); IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; UDICE-French Research Universities; Universite Paris Cite; Universite Paris Saclay	Ndiaye, E (corresponding author), Univ Paris Saclay, Telecom ParisTech, LTCI, CNRS, F-75013 Paris, France.	eugene.ndiaye@telecom-paristech.fr; olivier.fercoq@telecom-paristech.fr; alexandre.gramfort@telecom-paristech.fr; joseph.salmon@telecom-paristech.fr		Gramfort, Alexandre/0000-0001-9791-4404	Chair Machine Learning for Big Data at Telecom ParisTech; Orange/Telecom ParisTech think tank phi-TAB; FMJH Program Gaspard Monge in optimization and operation research; EDF	Chair Machine Learning for Big Data at Telecom ParisTech; Orange/Telecom ParisTech think tank phi-TAB; FMJH Program Gaspard Monge in optimization and operation research; EDF(Electricite de France (EDF))	We acknowledge the support from Chair Machine Learning for Big Data at Telecom ParisTech and from the Orange/Telecom ParisTech think tank phi-TAB. This work benefited from the support of the "FMJH Program Gaspard Monge in optimization and operation research", and from the support to this program from EDF.	Argyriou A., 2006, ADV NEURAL INFORM PR, P41; Argyriou A, 2008, MACH LEARN, V73, P243, DOI 10.1007/s10994-007-5040-8; Bauschke HH, 2011, CMS BOOKS MATH, P1, DOI 10.1007/978-1-4419-9467-7; Blondel M, 2013, MACH LEARN, V93, P31, DOI 10.1007/s10994-013-5367-2; Bonnefoy A, 2015, IEEE T SIGNAL PROCES, V63, P5121, DOI 10.1109/TSP.2015.2447503; Bonnefoy A, 2014, EUR SIGNAL PR CONF, P6; Buhlmann P, 2011, SPRINGER SER STAT, P1, DOI 10.1007/978-3-642-20192-9; Efron B, 2004, ANN STAT, V32, P407, DOI 10.1214/009053604000000067; El Ghaoui L, 2012, PAC J OPTIM, V8, P667; Fercoq O, 2015, PR MACH LEARN RES, V37, P333; Friedman J, 2010, J STAT SOFTW, V33, P1, DOI 10.18637/jss.v033.i01; Gramfort A, 2012, PHYS MED BIOL, V57, P1937, DOI 10.1088/0031-9155/57/7/1937; Hiriart-Urruty J. B., 1993, ADV THEORY BUNDLE ME, V306; Koh KM, 2007, J MACH LEARN RES, V8, P1519; Manning CD, 1999, FDN STAT NATURAL LAN; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Tibshirani R, 2012, J R STAT SOC B, V74, P245, DOI 10.1111/j.1467-9868.2011.01004.x; Wang J., 2012, ARXIV12113966; Wang J, 2014, ADV NEUR IN, V27; Xiang Zhen James, 2014, ARXIV14054897; Yuan M, 2006, J R STAT SOC B, V68, P49, DOI 10.1111/j.1467-9868.2005.00532.x	24	35	35	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100067
C	d'Alche-Buc, F; Grandvalet, Y; Arnbroise, C		Dietterich, TG; Becker, S; Ghahramani, Z		d'Alche-Buc, F; Grandvalet, Y; Arnbroise, C			Semi-supervised MarginBoost	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					In many discrimination problems a large amount of data is available but only a few of them are labeled. This provides a strong motivation to improve or develop methods for semi-supervised learning. In this paper, boosting is generalized to this task within the optimization framework of MarginBoost. We extend the margin definition to unlabeled data and develop the gradient descent algorithm that corresponds to the resulting margin cost function. This meta-learning scheme can be applied to any base classifier able to benefit from unlabeled data. We propose here to apply it to mixture models trained with an Expectation-Maximization algorithm. Promising results are presented on benchmarks with different rates of labeled data.	Univ Paris 06, CNRS, UMR 7606, LIP6, F-75252 Paris, France	Centre National de la Recherche Scientifique (CNRS); UDICE-French Research Universities; Sorbonne Universite; Universite Paris Cite	d'Alche-Buc, F (corresponding author), Univ Paris 06, CNRS, UMR 7606, LIP6, F-75252 Paris, France.	florence.dAlche@lip6.fr; Yves.Grandvalet@hds.utc.fr; Christophe.Ambroise@hds.utc.fr						AMBROISE C, 2000, IFCS 2000; Aubin, 1984, ANAL NONLINEAIRE SES; Bennett KP, 1999, ADV NEUR IN, V11, P368; Bishop CM, 1998, IEEE T PATTERN ANAL, V20, P281, DOI 10.1109/34.667885; BLUM A, 1998, P 1998 C COMP LEARN; BREIMAN L, 1997, 504 U CAL STAT DEP; Freund Y., 1996, Machine Learning. Proceedings of the Thirteenth International Conference (ICML '96), P148; Friedman J, 2000, ANN STAT, V28, P337, DOI 10.1214/aos/1016218223; GRANDVALET Y, 2001, ICANN 2001       AUG; Mason Llew, 2000, ADV LARGE MARGIN CLA; McLachlan, 1997, EM ALGORITHM EXTENSI; Nigam K, 2000, MACH LEARN, V39, P103, DOI 10.1023/A:1007692713085; Ratsch G, 2001, MACH LEARN, V42, P287, DOI 10.1023/A:1007618119488; RATSCH G, 1998, SOFT MARGINS ADABOOS; Schapire RE, 1998, ANN STAT, V26, P1651; SEEGER M, LEARNING LABELED UNL	16	35	35	0	2	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						553	560						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100069
C	Tenenbaum, JB		Solla, SA; Leen, TK; Muller, KR		Tenenbaum, JB			Rules and similarity in concept learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				DIMENSION	This paper argues that two apparently distinct modes of generalizing concepts - abstracting rules and computing similarity to exemplars - should both be seen as special cases of a more general Bayesian learning framework. Bayes explains the specific workings of these two modes - which rules are abstracted, how similarity is measured - as well as why generalization should appear rule- or similarity-based in different situations. This analysis also suggests why the rules/similarity distinction, even if not computationally fundamental, may still be useful at the algorithmic level as part of a principled approximation to fully Bayesian learning.	Stanford Univ, Dept Psychol, Stanford, CA 94305 USA	Stanford University	Tenenbaum, JB (corresponding author), Stanford Univ, Dept Psychol, Stanford, CA 94305 USA.							Erickson MA, 1998, J EXP PSYCHOL GEN, V127, P107, DOI 10.1037/0096-3445.127.2.107; HAUSSLER D, 1994, MACH LEARN, V14, P83, DOI 10.1007/BF00993163; MITCHELL TOM M., 1997, MACH LEARN, P2; Nosofsky RM, 1998, PSYCHON B REV, V5, P345, DOI 10.3758/BF03208813; SHEPARD RN, 1979, PSYCHOL REV, V86, P87, DOI 10.1037/0033-295X.86.2.87; SHEPARD RN, 1987, SCIENCE, V237, P1317, DOI 10.1126/science.3629243; SLOMAN SA, 1998, SIMILARITY SYMBOLS H; SMITH E, 1979, PSYCH REV, V86, P87; TENENBAUM J, 1996, NIPS 8; TENENBAUM J, 1999, NIPS 11; Tenenbaum J., 1999, THESIS MIT	12	35	35	0	5	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						59	65						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700009
C	Hou, M; Tang, JJ; Zhang, JH; Kong, WZ; Zhao, QB		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hou, Ming; Tang, Jiajia; Zhang, Jianhai; Kong, Wanzeng; Zhao, Qibin			Deep Multimodal Multilinear Fusion with High-order Polynomial Pooling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Tensor-based multimodal fusion techniques have exhibited great predictive performance. However, one limitation is that existing approaches only consider bilinear or trilinear pooling, which fails to unleash the complete expressive power of multilinear fusion with restricted orders of interactions. More importantly, simply fusing features all at once ignores the complex local intercorrelations, leading to the deterioration of prediction. In this work, we first propose a polynomial tensor pooling (PTP) block for integrating multimodal features by considering high-order moments, followed by a tensorized fully connected layer. Treating PTP as a building block, we further establish a hierarchical polynomial fusion network (HPFN) to recursively transmit local correlations into global ones. By stacking multiple PTPs, the expressivity capacity of HPFN enjoys an exponential growth w.r.t. the number of layers, which is shown by the equivalence to a very deep convolutional arithmetic circuits. Various experiments demonstrate that it can achieve the state-of-the-art performance.	[Hou, Ming; Tang, Jiajia; Zhao, Qibin] RIKEN, Tensor Learning Unit, Ctr Adv Intelligence Project, Wako, Saitama, Japan; [Tang, Jiajia; Zhang, Jianhai; Kong, Wanzeng] Hangzhou Dianzi Univ, Coll Comp Sci, Hangzhou, Peoples R China	RIKEN; Hangzhou Dianzi University	Zhao, QB (corresponding author), RIKEN, Tensor Learning Unit, Ctr Adv Intelligence Project, Wako, Saitama, Japan.	ming.hou@riken.jp; hdutangjiajia@163.com; jhzhang@hdu.edu.cn; kongwanzeng@hdu.edu.cn; qibin.zhao@riken.jp			JSPS KAKENHI [17K00326]; national key research and development program intergovernmental international science and technology innovation cooperation project (MOST-RIKEN) [2017YFE0116800]; national natural science foundation of China [61633010]	JSPS KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI)); national key research and development program intergovernmental international science and technology innovation cooperation project (MOST-RIKEN); national natural science foundation of China(National Natural Science Foundation of China (NSFC))	This work was partially supported by JSPS KAKENHI (Grant No. 17K00326), the national key research and development program intergovernmental international science and technology innovation cooperation project (MOST-RIKEN) under Grant 2017YFE0116800 and the national natural science foundation of China (Grant No. 61633010).	Baltrusaitis T, 2019, IEEE T PATTERN ANAL, V41, P423, DOI 10.1109/TPAMI.2018.2798607; Busso C, 2008, LANG RESOUR EVAL, V42, P335, DOI 10.1007/s10579-008-9076-6; CARROLL JD, 1970, PSYCHOMETRIKA, V35, P283, DOI 10.1007/BF02310791; Chen Zadeh M, 2017, EMPIRICAL METHODS NA, P1103, DOI 10.18653/v1/D17-1115; Cichocki A, 2016, FOUND TRENDS MACH LE, V9, pI, DOI 10.1561/2200000059; Cohen N., 2016, C LEARNING THEORY, V49, P698; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; D'Mello SK, 2015, ACM COMPUT SURV, V47, DOI 10.1145/2682899; Fukui Akira, 2016, ARXIV160601847; Hackbusch W, 2009, J FOURIER ANAL APPL, V15, P706, DOI 10.1007/s00041-009-9094-9; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kolda TG, 2009, SIAM REV, V51, P455, DOI 10.1137/07070111X; Liang P. P., 2019, P C N, P2599; Liang PP, 2018, ICMI'18: PROCEEDINGS OF THE 20TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P472, DOI 10.1145/3242969.3243019; Liang Paul Pu., 2018, P 2018 C EMPIRICAL M, P150, DOI [10.18653/v1/d18-1014, DOI 10.18653/V1/D18-1014, 10.18653/v1/D18-1014]; Lin TY, 2015, IEEE I CONF COMP VIS, P1449, DOI 10.1109/ICCV.2015.170; Liu Z, 2018, PROCEEDINGS OF THE 56TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL), VOL 1, P2247; Morency L. -P., 2011, P 13 INT C MULT INT, P169, DOI DOI 10.1145/2070481.2070509; Morvant E, 2014, LECT NOTES COMPUT SC, V8621, P153, DOI 10.1007/978-3-662-44415-3_16; Nojavanasghari B, 2016, ICMI'16: PROCEEDINGS OF THE 18TH ACM INTERNATIONAL CONFERENCE ON MULTIMODAL INTERACTION, P284, DOI 10.1145/2993148.2993176; Oseledets IV, 2011, SIAM J SCI COMPUT, V33, P2295, DOI 10.1137/090752286; Park S, 2014, PROC EUR S-STATE DEV, P50, DOI 10.1109/ESSDERC.2014.6948755; Poria S, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017), VOL 1, P873, DOI 10.18653/v1/P17-1081; Rajagopalan SS, 2016, LECT NOTES COMPUT SC, V9911, P338, DOI 10.1007/978-3-319-46478-7_21; Shutova Ekaterina, 2016, P 2016 C N AM CHAPT, P160, DOI [10.18653/v1/N16-1020, DOI 10.18653/V1/N16-1020]; TUCKER LR, 1966, PSYCHOMETRIKA, V31, P279, DOI 10.1007/BF02289464; Zadeh A, 2016, IEEE INTELL SYST, V31, P82, DOI 10.1109/MIS.2016.94; Zadeh Amir, 2016, ARXIV160606259; Zadeh Amir, 2018, AAAI C ART INT; Zhao QB, 2019, INT CONF ACOUST SPEE, P8608, DOI 10.1109/ICASSP.2019.8682231	31	34	35	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903072
C	Qin, C; You, HX; Wang, LC; Kuo, CCJ; Fu, Y		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qin, Can; You, Haoxuan; Wang, Lichen; Kuo, C. -C. Jay; Fu, Yun			PointDAN: A Multi-Scale 3D Domain Adaption Network for Point Cloud Representation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Domain Adaptation (DA) approaches achieved significant improvements in a wide range of machine learning and computer vision tasks (i.e., classification, detection, and segmentation). However, as far as we are aware, there are few methods yet to achieve domain adaptation directly on 3D point cloud data. The unique challenge of point cloud data lies in its abundant spatial geometric information, and the semantics of the whole object is contributed by including regional geometric structures. Specifically, most general-purpose DA methods that struggle for global feature alignment and ignore local geometric information are not suitable for 3D domain alignment. In this paper, we propose a novel 3D Domain Adaptation Network for point cloud data (PointDAN). PointDAN jointly aligns the global and local features in multi-level. For local alignment, we propose Self-Adaptive (SA) node module with an adjusted receptive field to model the discriminative local structures for aligning domains. To represent hierarchically scaled features, node-attention module is further introduced to weight the relationship of SA nodes across objects and domains. For global alignment, an adversarial-training strategy is employed to learn and align global features across domains. Since there is no common evaluation benchmark for 3D point cloud DA scenario, we build a general benchmark (i.e., PointDA-10) extracted from three popular 3D object/scene datasets (i.e., ModelNet, ShapeNet and ScanNet) for cross-domain 3D objects classification fashion. Extensive experiments on PointDA-10 illustrate the superiority of our model over the state-of-the-art general-purpose DA methods.(1)	[Qin, Can; You, Haoxuan; Fu, Yun] Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA; [You, Haoxuan] Columbia Univ, Dept Comp Sci, New York, NY 10027 USA; [Kuo, C. -C. Jay] Univ Southern Calif, Dept Elect & Comp Engn, Los Angeles, CA 90089 USA; [Fu, Yun] Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA	Northeastern University; Columbia University; University of Southern California; Northeastern University	Qin, C (corresponding author), Northeastern Univ, Dept Elect & Comp Engn, Boston, MA 02115 USA.	qin.ca@husky.neu.edu; haoxuan.you@columbia.edu; wanglichenxj@gmail.com; cckuo@sipi.usc.edu; yunfu@ece.neu.edu	Kuo, C.-C. Jay/A-7110-2011	Kuo, C.-C. Jay/0000-0001-9474-5035				Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Ben-David S, 2010, MACH LEARN, V79, P151, DOI 10.1007/s10994-009-5152-4; Borgwardt KM, 2006, BIOINFORMATICS, V22, pE49, DOI 10.1093/bioinformatics/btl242; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Charles R., 2017, ADV NEURAL INFORM PR; Dai A, 2017, PROC CVPR IEEE, P6545, DOI 10.1109/CVPR.2017.693; Dai A, 2017, PROC CVPR IEEE, P2432, DOI 10.1109/CVPR.2017.261; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Dong JH, 2019, IEEE I CONF COMP VIS, P10711, DOI 10.1109/ICCV.2019.01081; Feng YF, 2018, PROC CVPR IEEE, P264, DOI 10.1109/CVPR.2018.00035; Feng YT, 2019, AAAI CONF ARTIF INTE, P8279; Ganin Y., 2014, ARXIV14097495; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gopalan R, 2011, IEEE I CONF COMP VIS, P999, DOI 10.1109/ICCV.2011.6126344; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li Y, 2018, 2018 IEEE 3RD INTERNATIONAL CONFERENCE ON IMAGE, VISION AND COMPUTING (ICIVC), P828; Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mifdal J, 2017, INT GEOSCI REMOTE SE, P3373; Nair V., 2010, ICML, P807; Pan SJ, 2011, IEEE T NEURAL NETWOR, V22, P199, DOI 10.1109/TNN.2010.2091281; Qin C, 2019, IEEE INT CONF COMP V, P1055, DOI 10.1109/ICCVW.2019.00135; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Saito K, 2018, PROC CVPR IEEE, P3723, DOI 10.1109/CVPR.2018.00392; Saleh K., 2019, ARXIV190508955; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; Sugiyama M., 2008, NIPS, P1433; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Wang LC, 2019, IEEE T IMAGE PROCESS, V28, P1023, DOI 10.1109/TIP.2018.2870945; Wu BC, 2019, IEEE INT CONF ROBOT, P4376, DOI 10.1109/ICRA.2019.8793495; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Xie CC, 2013, INT CONF COMPUTAT, P222, DOI 10.1109/ICCPS.2013.6893597; You HX, 2019, AAAI CONF ARTIF INTE, P9119; You HX, 2018, PROCEEDINGS OF THE 2018 ACM MULTIMEDIA CONFERENCE (MM'18), P1310, DOI 10.1145/3240508.3240702; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Zhang YL, 2018, LECT NOTES COMPUT SC, V11211, P294, DOI [10.1007/978-3-030-01234-2_18, 10.1007/978-3-030-01240-3_22]; Zhou X., 2018, P EUR C COMP VIS ECC; Zhou Y, 2018, PROC CVPR IEEE, P4490, DOI 10.1109/CVPR.2018.00472; Zou Y, 2018, LECT NOTES COMPUT SC, V11207, P297, DOI 10.1007/978-3-030-01219-9_	42	34	34	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307023
C	Ren, MY; Liao, RJ; Fetaya, E; Zemel, RS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ren, Mengye; Liao, Renjie; Fetaya, Ethan; Zemel, Richard S.			Incremental Few-Shot Learning with Attention Attractor Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				BACKPROPAGATION; ALGORITHM	Machine learning classifiers are often trained to recognize a set of pre-defined classes. However, in many applications, it is often desirable to have the flexibility of learning additional concepts, with limited data and without re-training on the full training set. This paper addresses this problem, incremental few-shot learning, where a regular classification network has already been trained to recognize a set of base classes, and several extra novel classes are being considered, each with only a few labeled examples. After learning the novel classes, the model is then evaluated on the overall classification performance on both base and novel classes. To this end, we propose a meta-learning model, the Attention Attractor Network, which regularizes the learning of novel classes. In each episode, we train a set of new weights to recognize novel classes until they converge, and we show that the technique of recurrent back-propagation can back-propagate through the optimization process and facilitate the learning of these parameters. We demonstrate that the learned attractor network can help recognize novel classes while remembering old classes without the need to review the original training set, outperforming various baselines.	[Ren, Mengye; Liao, Renjie; Fetaya, Ethan; Zemel, Richard S.] Univ Toronto, Toronto, ON, Canada; [Ren, Mengye; Liao, Renjie; Fetaya, Ethan; Zemel, Richard S.] Vector Inst, Toronto, ON, Canada; [Ren, Mengye; Liao, Renjie] Uber ATG, San Francisco, CA 94107 USA	University of Toronto	Ren, MY (corresponding author), Univ Toronto, Toronto, ON, Canada.; Ren, MY (corresponding author), Vector Inst, Toronto, ON, Canada.; Ren, MY (corresponding author), Uber ATG, San Francisco, CA 94107 USA.	mren@cs.toronto.edu; rjliao@cs.toronto.edu; ethanf@cs.toronto.edu; zemel@cs.toronto.edu			NSERC; Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) [D16PC00003]	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC)	Supported by NSERC and the Intelligence Advanced Research Projects Activity (IARPA) via Department of Interior/Interior Business Center (DoI/IBC) contract number D16PC00003. The U.S. Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.	Almeida L. B., 1987, IEEE First International Conference on Neural Networks, P609; Andrychowicz M, 2016, ADV NEUR IN, V29; Balaji Yogesh, 2018, ADV NEURAL INFORM PR; Bertinetto L., 2018, ABS180508136 CORR; Castro FM, 2018, LECT NOTES COMPUT SC, V11216, P241, DOI 10.1007/978-3-030-01258-8_15; Finn C, 2017, PR MACH LEARN RES, V70; Garcia V., 2018, ICLR; Gidaris S, 2018, PROC CVPR IEEE, P4367, DOI 10.1109/CVPR.2018.00459; Hariharan B, 2017, IEEE I CONF COMP VIS, P3037, DOI 10.1109/ICCV.2017.328; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Kemker R., 2018, ICLR; Kingma D.P, P 3 INT C LEARNING R; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Koch G., 2015, ICML DEEP LEARNING W; Lake Brenden, 2011, C COGN SCI SOC, P6; Lee JD, 2018, 2018 3RD INTERNATIONAL CONFERENCE ON CONTROL AND ROBOTICS ENGINEERING (ICCRE), P6, DOI 10.1109/ICCRE.2018.8376424; Li XC, 2019, 2019 WORLD ROBOT CONFERENCE SYMPOSIUM ON ADVANCED ROBOTICS AND AUTOMATION (WRC SARA 2019), P7, DOI 10.1109/WRC-SARA.2019.8931909; Liao R., 2018, P 35 INT C MACH LEAR; MCCLELLAND JL, 1995, PSYCHOL REV, V102, P419, DOI 10.1037/0033-295X.102.3.419; Mishra N., 2018, INT C LEARN REPR, P1; Mozer M. C., 2009, OXFORD COMPANION CON, P86; Naik D. K., 1992, P IEEE INT JOINT C N; Pappireddy MR, 2017, 2017 18TH INTERNATIONAL WORKSHOP ON MICROPROCESSOR AND SOC TEST, SECURITY AND VERIFICATION (MTV 2017), P5, DOI 10.1109/MTV.2017.12; Hoang PH, 2018, INT C CONTR AUTOMAT, P1; PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229; Qi H, 2018, PROC CVPR IEEE, P5822, DOI 10.1109/CVPR.2018.00610; Rebuffi Sylvestre-Alvise, 2017, PROC CVPR IEEE, P8, DOI DOI 10.1109/CVPR.2017.587; Ren M., 2018, ICLR; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Santoro A, 2016, PR MACH LEARN RES, V48; Snell J, 2017, ADV NEUR IN, V30; Sprechmann P., 2018, P 6 INT C LEARN REPR; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Thrun S, 1998, LEARNING TO LEARN, P181; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wang Y, 2018, PROC CVPR IEEE, P8042, DOI 10.1109/CVPR.2018.00839; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337; Williams RJ, 1990, NEURAL COMPUT, V2, P490, DOI 10.1162/neco.1990.2.4.490; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Zemel RS, 2001, NEURAL COMPUT, V13, P1045, DOI 10.1162/08997660151134325; Zhu CY, 1997, ACM T MATH SOFTWARE, V23, P550, DOI 10.1145/279232.279236	43	34	34	1	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305029
C	Wang, KA; Pleiss, G; Gardner, JR; Tyree, S; Weinberger, KQ; Wilson, AG		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Wang, Ke Alexander; Pleiss, Geoff; Gardner, Jacob R.; Tyree, Stephen; Weinberger, Kilian Q.; Wilson, Andrew Gordon			Exact Gaussian Processes on a Million Data Points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Gaussian processes (GPs) are flexible non-parametric models, with a capacity that grows with the available data. However, computational constraints with standard inference procedures have limited exact GPs to problems with fewer than about ten thousand training points, necessitating approximations for larger datasets. In this paper, we develop a scalable approach for exact GPs that leverages multi-GPU parallelization and methods like linear conjugate gradients, accessing the kernel matrix only through matrix multiplication. By partitioning and distributing kernel matrix multiplies, we demonstrate that an exact GP can be trained on over a million points, a task previously thought to be impossible with current computing hardware, in less than 2 hours. Moreover, our approach is generally applicable, without constraints to grid data or specific kernel classes. Enabled by this scalability, we perform the first-ever comparison of exact GPs against scalable GP approximations on datasets with 10(4) - 10(6) data points, showing dramatic performance improvements.	[Wang, Ke Alexander; Pleiss, Geoff; Weinberger, Kilian Q.; Wilson, Andrew Gordon] Cornell Univ, Ithaca, NY 14853 USA; [Gardner, Jacob R.] Uber AI Labs, San Francisco, CA USA; [Tyree, Stephen] NVIDIA, Santa Clara, CA USA; [Wilson, Andrew Gordon] NYU, New York, NY 10003 USA	Cornell University; Nvidia Corporation; New York University	Wang, KA (corresponding author), Cornell Univ, Ithaca, NY 14853 USA.				NSF [IIS-1910266, IIS-1563887]; Facebook Research; NSF I-DISRE [1934714]; Amazon Research Award; National Science Foundation [III-1618134, III-1526012, IIS-1149882, IIS-1724282, TRIPODS-1740822]; Bill and Melinda Gates Foundation; Office of Naval Research; SAP America Inc.	NSF(National Science Foundation (NSF)); Facebook Research(Facebook Inc); NSF I-DISRE; Amazon Research Award; National Science Foundation(National Science Foundation (NSF)); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); Office of Naval Research(Office of Naval Research); SAP America Inc.	KAW and AGW are supported by NSF IIS-1910266, NSF IIS-1563887, Facebook Research, NSF I-DISRE 1934714, and an Amazon Research Award. GP and KQW are supported in part by the III-1618134, III-1526012, IIS-1149882, IIS-1724282, and TRIPODS-1740822 grants from the National Science Foundation. In addition, they are supported by the Bill and Melinda Gates Foundation, the Office of Naval Research, and SAP America Inc.	Asuncion A, 2007, UCI MACHINE LEARNING; Cheng CA, 2017, ADV NEUR IN, V30; Cunningham J. P., 2008, P 25 INT C MACH LEAR, P192, DOI DOI 10.1145/1390156.1390181; Cutajar K., 2016, ICML; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; de Garis Matthews A. G., 2016, THESIS; Deisenroth M., 2011, PROC 28 INT C MACH L, P465; Deisenroth MP, 2015, PR MACH LEARN RES, V37, P1481; Deisenroth MP, 2015, IEEE T PATTERN ANAL, V37, P408, DOI 10.1109/TPAMI.2013.218; Dong K., 2017, NEURIPS; Nguyen DT, 2019, SAC '19: PROCEEDINGS OF THE 34TH ACM/SIGAPP SYMPOSIUM ON APPLIED COMPUTING, P1286, DOI 10.1145/3297280.3297409; Evans TW, 2018, PR MACH LEARN RES, V80; Gardner J. R., 2018, AISTATS; Gardner JR, 2018, ADV NEUR IN, V31; Gibbs M., 1997, TECHNICAL REPORT; Golub Gene H., 2013, MATRIX COMPUTATION, V3; Gustavson F. G., 2006, INT WORKSH APPL PAR, P550; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Izmailov P., 2018, INT C ART INT STAT, P726; Kingma D.P, P 3 INT C LEARNING R; Le Q., 2013, ICML; LIU DC, 1989, MATH PROGRAM, V45, P503, DOI 10.1007/BF01589116; Liu H., 2018, ARXIV180701065; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Murray I., 2010, ELLIPTICAL SLICE SAM; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pleiss G., 2018, ICML; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rasmussen C., 2006, GAUSSIAN PROCESSES M, V1; Rasmussen CE, 2001, ADV NEUR IN, V13, P294; Roberts S, 2013, PHILOS T R SOC A, V371, DOI 10.1098/rsta.2011.0550; Saatci Y., 2012, THESIS U CAMBRIDGE C; Salimbeni H, 2018, ADV NEUR IN, V31; Salimbeni Hugh, 2017, ADV NEURAL INFORM PR, V30, P4588; Snelson E., 2006, ADV NEURAL INFORM PR, V18, P1259; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Ubaru S, 2017, SIAM J MATRIX ANAL A, V38, P1075, DOI 10.1137/16M1104974; Wilson A., 2013, INT C MACH LEARN, P1067; Wilson A., 2014, P ADV NEUR INF PROC, V4, P3626; Wilson A. G., 2012, INT C MACH LEARN; Wilson AG, 2016, ADV NEUR IN, V29; Wilson AG, 2016, JMLR WORKSH CONF PRO, V51, P370; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775; Yang ZC, 2015, JMLR WORKSH CONF PRO, V38, P1098	47	34	34	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906032
C	Cao, J; Hu, YB; Zhang, HW; He, R; Sun, ZA		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Cao, Jie; Hu, Yibo; Zhang, Hongwen; He, Ran; Sun, Zhenan			Learning a High Fidelity Pose Invariant Model for High-resolution Face Frontalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Face frontalization refers to the process of synthesizing the frontal view of a face from a given profile. Due to self-occlusion and appearance distortion in the wild, it is extremely challenging to recover faithful results and preserve texture details in a high-resolution. This paper proposes a High Fidelity Pose Invariant Model (HF-PIM) to produce photographic and identity-preserving results. HF-PIM frontalizes the profiles through a novel texture warping procedure and leverages a dense correspondence field to bind the 2D and 3D surface spaces. We decompose the prerequisite of warping into dense correspondence field estimation and facial texture map recovering, which are both well addressed by deep networks. Different from those reconstruction methods relying on 3D data, we also propose Adversarial Residual Dictionary Learning (ARDL) to supervise facial texture map recovering with only monocular images. Exhaustive experiments on both controlled and uncontrolled environments demonstrate that the proposed method not only boosts the performance of pose-invariant face recognition but also dramatically improves high-resolution frontalization appearances.	[He, Ran] Univ Chinese Acad Sci, CASIA, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China; Univ Chinese Acad Sci, CASIA, Ctr Res Intelligent Percept & Comp, Beijing 100049, Peoples R China; Univ Chinese Acad Sci, CASIA, Ctr Excellence Brain Sci & Intelligence Technol, Beijing 100049, Peoples R China	Chinese Academy of Sciences; Institute of Automation, CAS; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; University of Chinese Academy of Sciences, CAS; Chinese Academy of Sciences; Institute of Automation, CAS; University of Chinese Academy of Sciences, CAS	He, R (corresponding author), Univ Chinese Acad Sci, CASIA, Natl Lab Pattern Recognit, Beijing 100049, Peoples R China.	jie.cao@cripac.ia.ac.cn; yibo.hu@cripac.ia.ac.cn; hongwen.zhang@cripac.ia.ac.cn; rhe@nlpr.ia.ac.cn; znsun@nlpr.ia.ac.cn	Zhang, Hongwen/ABI-2791-2020; cai, jie/HHS-0606-2022	Zhang, Hongwen/0000-0001-8633-4551; 	National Key Research and Development Program of China [2017YFC0821602, 2016YFB1001000]; National Natural Science Foundation of China [61427811, 61573360]	National Key Research and Development Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work is funded by the National Key Research and Development Program of China (Grant No. 2017YFC0821602, 2016YFB1001000) and the National Natural Science Foundation of China (Grant No. 61427811, 61573360).	[Anonymous], 2015, PUSHING FRONTIERS UN; [Anonymous], 2017, ICCV; Arjovsky Martin, 2017, ICML; Booth James, 2014, ICIP; CHEN X, 2016, NEURIPS; Cole Forrester, 2017, CVPR; Dana Hang, 2017, CVPR; Deng J., 2018, CVPR; Dovgard R., 2004, ECCV; Ferrari C., 2016, ICPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gross R., 2010, MULTI PIE; Guler Riza Alp, 2018, CVPR; Guler Riza Alp, 2017, CVPR; Hassner Tal, 2015, CVPR; Hassner Tal, 2013, ICCV; He R., 2017, AAAI; Hu Y., 2018, CVPR; Huang Gary B., 2007, 0749 U MASS; Huang H., 2017, ICCV; Huang R., 2017, ICCV; Johnson J, 2016, ECCV; Karras Tero, 2018, ICLR; Liu Z., 2015, ICCV; Mao X., 2017, ICCV; Parkhi O. M., 2015, BMVC; Paszke Adam, 2017, NEURIPS W; Paysan Pascal, 2009, AVSS; Radford A., 2016, ICLR; Shrivastava A., 2017, CVPR; Tian Y., 2018, IJCAI; Tran L., 2017, CVPR; Tran Luan, 2018, CVPR; van Gemert J. C., 2008, ECCV; VOLKER B, 1999, SIGGRAPH; Yin Xi, 2017, ICCV; Zhao Jian, 2018, CVPR; Zhao Jian, 2017, NEURIPS; Zhao Jian, 2018, IJCAI; Zhu Xiangyu, 2015, CVPR; Zhu Z., 2016, CVPR	41	34	35	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302085
C	Hoffman, J; Mohri, M; Zhang, NS		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Hoffman, Judy; Mohri, Mehryar; Zhang, Ningshan			Algorithms and Theory for Multiple-Source Adaptation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				DOMAIN ADAPTATION	We present a number of novel contributions to the multiple-source adaptation problem. We derive new normalized solutions with strong theoretical guarantees for the cross-entropy loss and other similar losses. We also provide new guarantees that hold in the case where the conditional probabilities for the source domains are distinct. Moreover, we give new algorithms for determining the distribution-weighted combination solution for the cross-entropy loss and other losses. We report the results of a series of experiments with real-world datasets. We find that our algorithm outperforms competing approaches by producing a single robust model that performs well on any target mixture distribution. Altogether, our theory, algorithms, and empirical results provide a full solution for the multiple-source adaptation problem with very practical benefits.	[Hoffman, Judy] Univ Calif Berkeley, CS Dept, Berkeley, CA 94720 USA; [Mohri, Mehryar] Courant Inst, New York, NY 10012 USA; [Mohri, Mehryar] Google, New York, NY 10012 USA; [Zhang, Ningshan] NYU, New York, NY 10012 USA	University of California System; University of California Berkeley; Google Incorporated; New York University	Hoffman, J (corresponding author), Univ Calif Berkeley, CS Dept, Berkeley, CA 94720 USA.	jhoffman@eecs.berkeley.edu; mohri@cims.nyu.edu; nzhang@stern.nyu.edu			NSF [IIS-1618662]	NSF(National Science Foundation (NSF))	We thank Cyril Allauzen for comments on a previous draft of this paper. This work was partly funded by NSF CCF-1535987 and NSF IIS-1618662.	[Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; [Anonymous], 2007, P 15 ACM INT C MULTI; Arndt, 2003, INFORM MEASURES INFO; Ben-David S., 2007, ADV NEURAL INFORM PR, V19, P137; Blanchard Gilles, 2011, NIPS, V24, P3; Blitzer J., P 45 ANN M ASS COMP, P440, DOI DOI 10.1109/IRPS.2011.5784441; Cortes C, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P169, DOI 10.1145/2783258.2783368; Cortes C, 2014, THEOR COMPUT SCI, V519, P103, DOI 10.1016/j.tcs.2013.09.027; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Crammer K, 2008, J MACH LEARN RES, V9, P1757; Donahue J, 2014, PR MACH LEARN RES, V32; Dredze M., 2008, P 25 INT C MACHINE L, V307, P264, DOI DOI 10.1145/1390156.1390190; Duan L., 2009, P 26 ANN INT C MACH, P289, DOI DOI 10.1145/1553374.1553411; Duan Lixin, 2012, IEEE Trans Neural Netw Learn Syst, V23, P504, DOI 10.1109/TNNLS.2011.2178556; Ganin Y, 2015, PR MACH LEARN RES, V37, P1180; Gong BQ, 2012, PROC CVPR IEEE, P2066, DOI 10.1109/CVPR.2012.6247911; Gong Boqing, 2013, P ADV NEUR INF PROC, P1286; Hoffman J., 2013, ICLR; Hoffman J, 2012, LECT NOTES COMPUT SC, V7573, P702, DOI 10.1007/978-3-642-33709-3_50; Huang J., 2006, ADV NEURAL INFORM PR, DOI DOI 10.7551/MITPRESS/7503.003.0080; Khosla A, 2012, LECT NOTES COMPUT SC, V7572, P158, DOI 10.1007/978-3-642-33718-5_12; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Liao H, 2013, INT CONF ACOUST SPEE, P7947, DOI 10.1109/ICASSP.2013.6639212; Long M., 2015, P 32 INT C MACH LEAR, V1, P97; Mansour Y., 2009, COLT; Mansour Y., 2009, P 25 C UNC ART INT, P367; Mansour Yishay, 2008, NIPS, P1041; Muandet Krikamol, 2013, ICML; Pan SJ, 2010, IEEE T KNOWL DATA EN, V22, P1345, DOI 10.1109/TKDE.2009.191; Pei ZY, 2018, AAAI CONF ARTIF INTE, P3934; Renyi A., 1961, P 4 BERKELEY S MATH, V1; Roark B., 2012, P ACL 2012 SYSTEM DE, P61; Saenko K, 2010, LECT NOTES COMPUT SC, V6314, P213, DOI 10.1007/978-3-642-15561-1_16; Sha, 2013, P INT C MACH LEARN; Sriperumbudur BK, 2012, NEURAL COMPUT, V24, P1391, DOI 10.1162/NECO_a_00283; Taigman Y., 2017, ICLR; Tao P. D., 1997, ACTA MATH VIETNAM, V22, P289; Tao PD, 1998, SIAM J OPTIMIZ, V8, P476, DOI 10.1137/S1052623494274313; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; Xu Z, 2014, LECT NOTES COMPUT SC, V8691, P628, DOI 10.1007/978-3-319-10578-9_41; Yuille AL, 2003, NEURAL COMPUT, V15, P915, DOI 10.1162/08997660360581958; Zhang K, 2015, AAAI CONF ARTIF INTE, P3150	43	34	34	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002076
C	Lenssen, JE; Fey, M; Libuschewski, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lenssen, Jan Eric; Fey, Matthias; Libuschewski, Pascal			Group Equivariant Capsule Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We present group equivariant capsule networks, a framework to introduce guaranteed equivariance and invariance properties to the capsule network idea. Our work can be divided into two contributions. First, we present a generic routing by agreement algorithm defined on elements of a group and prove that equivariance of output pose vectors, as well as invariance of output activations, hold under certain conditions. Second, we connect the resulting equivariant capsule networks with work from the field of group convolutional networks. Through this connection, we provide intuitions of how both methods relate and are able to combine the strengths of both approaches in one deep neural network architecture. The resulting framework allows sparse evaluation of the group convolution operator, provides control over specific equivariance and invariance properties, and can use routing by agreement instead of pooling operations. In addition, it is able to provide interpretable and equivariant representation vectors as output capsules, which disentangle evidence of object existence from its pose.	[Lenssen, Jan Eric; Fey, Matthias; Libuschewski, Pascal] TU Dortmund Univ, Comp Graph Grp, D-44227 Dortmund, Germany	Dortmund University of Technology	Lenssen, JE (corresponding author), TU Dortmund Univ, Comp Graph Grp, D-44227 Dortmund, Germany.	janeric.lenssen@udo.edu; matthias.fey@udo.edu; pascal.libuschewski@udo.edu			Deutsche Forschungsgemeinschaft (DFG) within the Collaborative Research Center [SFB 876]	Deutsche Forschungsgemeinschaft (DFG) within the Collaborative Research Center(German Research Foundation (DFG))	Part of the work on this paper has been supported by Deutsche Forschungsgemeinschaft (DFG) within the Collaborative Research Center SFB 876 Providing Information by Resource-Constrained Analysis, projects B2 and A6.	Bengio Y., 2007, P 24 INT C MACH LEAR, P473, DOI DOI 10.1145/1273496.1273556; Bhatia, 2012, MATRIX INFORM GEOMET; Cohen T. S., 2018, ARXIV E PRINTS; Cohen T. S., 2017, INT C LEARN REPR ICL; Cohen Taco, 2018, ICLR; Cohen TS, 2016, PR MACH LEARN RES, V48; Dieleman S., 2016, P 33 INT C MACH LEAR, P1889; Fey M., 2018, IEEE C COMP VIS PATT; Gilmer J, 2017, PR MACH LEARN RES, V70; Henriques JF, 2017, PR MACH LEARN RES, V70; Hinton G. E., 2018, INT C LEARN REPR ICL; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Marcos D, 2017, IEEE I CONF COMP VIS, P5058, DOI 10.1109/ICCV.2017.540; Sabour Sara, 2017, PROC 31 INT C NEURAL; Weiler M., 2018, IEEE C COMP VIS PATT; Worrall D. E., 2017, IEEE C COMP VIS PATT; Zhou YZ, 2017, PROC CVPR IEEE, P4961, DOI 10.1109/CVPR.2017.527	18	34	34	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003040
C	Janner, M; Wu, JJ; Kulkarni, TD; Yildirim, I; Tenenbaum, JB		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Janner, Michael; Wu, Jiajun; Kulkarni, Tejas D.; Yildirim, Ilker; Tenenbaum, Joshua B.			Self-Supervised Intrinsic Image Decomposition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				REFLECTANCE	Intrinsic decomposition from a single image is a highly challenging task, due to its inherent ambiguity and the scarcity of training data. In contrast to traditional fully supervised learning approaches, in this paper we propose learning intrinsic image decomposition by explaining the input image. Our model, the Rendered Intrinsics Network (RIN), joins together an image decomposition pipeline, which predicts reflectance, shape, and lighting conditions given a single image, with a recombination function, a learned shading model used to recompose the original input based off of intrinsic image predictions. Our network can then use unsupervised reconstruction error as an additional signal to improve its intermediate representations. This allows large-scale unlabeled data to be useful during training, and also enables transferring learned knowledge to images of unseen object categories, lighting conditions, and shapes. Extensive experiments demonstrate that our method performs well on both intrinsic image decomposition and knowledge transfer.	[Janner, Michael; Wu, Jiajun; Yildirim, Ilker; Tenenbaum, Joshua B.] MIT, Cambridge, MA 02139 USA; [Kulkarni, Tejas D.] DeepMind, London, England	Massachusetts Institute of Technology (MIT)	Janner, M (corresponding author), MIT, Cambridge, MA 02139 USA.	janner@mit.edu; jiajunwu@mit.edu; tejasdkulkarni@gmail.com; ilkery@mit.edu; jbt@mit.edu	Jeong, Yongwook/N-7413-2016; Wu, JiaJun/GQH-7885-2022		ONR MURI [N00014-16-1-2007]; Center for Brain, Minds and Machines (NSF) [1231216]; Toyota Research Institute; Samsung	ONR MURI(MURIOffice of Naval Research); Center for Brain, Minds and Machines (NSF); Toyota Research Institute; Samsung(Samsung)	This work is supported by ONR MURI N00014-16-1-2007, the Center for Brain, Minds and Machines (NSF #1231216), Toyota Research Institute, and Samsung.	Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow H. G., 1978, COMPUTER VISION SYST; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Chang Angel X., 2015, ARXIV151203012CSGR P; Grosse R, 2009, IEEE I CONF COMP VIS, P2335, DOI 10.1109/ICCV.2009.5459428; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hold-Geoffroy Y, 2017, PROC CVPR IEEE, P2373, DOI 10.1109/CVPR.2017.255; Horn B. K., 1974, COMPUT VISION GRAPH, V3, P277, DOI DOI 10.1016/0146-664X(74)90022-7; Innamorati C, 2017, COMPUT GRAPH FORUM, V36, P15, DOI 10.1111/cgf.13220; Kar A, 2015, PROC CVPR IEEE, P1966, DOI 10.1109/CVPR.2015.7298807; Kingma D.P, P 3 INT C LEARNING R; Kulkarni TD, 2015, ADV NEUR IN, V28; LAND EH, 1971, J OPT SOC AM, V61, P1, DOI 10.1364/JOSA.61.000001; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Lombardi S, 2016, IEEE T PATTERN ANAL, V38, P129, DOI 10.1109/TPAMI.2015.2430318; Lombardi Stephen, 2012, CVPR; Nalbach Oliver, 2017, COMPUTER GRAPHICS FO, V36; Narihira T, 2015, IEEE I CONF COMP VIS, P2992, DOI 10.1109/ICCV.2015.342; Narihira T, 2015, PROC CVPR IEEE, P2965, DOI 10.1109/CVPR.2015.7298915; Oxholm G, 2016, IEEE T PATTERN ANAL, V38, P376, DOI 10.1109/TPAMI.2015.2450734; Queau Yvain, 2015, INT C SCAL SPAC VAR; Rematas Konstantinos, 2016, CVPR; Shi J, 2017, PROC CVPR IEEE, P5844, DOI 10.1109/CVPR.2017.619; Shu ZX, 2017, PROC CVPR IEEE, P5444, DOI 10.1109/CVPR.2017.578; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tang Y., 2012, ARXIV12064635, P1419, DOI DOI 10.48550/ARXIV.1206.6445; Weiss Y, 2001, EIGHTH IEEE INTERNATIONAL CONFERENCE ON COMPUTER VISION, VOL II, PROCEEDINGS, P68, DOI 10.1109/ICCV.2001.937606; Wu JJ, 2017, ADV NEUR IN, V30	30	34	34	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406002
C	Liu, Q		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Liu, Qiang			Stein Variational Gradient Descent as Gradient Flow	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Stein variational gradient descent (SVGD) is a deterministic sampling algorithm that iteratively transports a set of particles to approximate given distributions, based on a gradient-based update that guarantees to optimally decrease the KL divergence within a function space. This paper develops the first theoretical analysis on SVGD. We establish that the empirical measures of the SVGD samples weakly converge to the target distribution, and show that the asymptotic behavior of SVGD is characterized by a nonlinear Fokker-Planck equation known as Vlasov equation in physics. We develop a geometric perspective that views SVGD as a gradient flow of the KL divergence functional under a new metric structure on the space of distributions induced by Stein operator.	[Liu, Qiang] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA	Dartmouth College	Liu, Q (corresponding author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.	qiang.liu@dartmouth.edu	Jeong, Yongwook/N-7413-2016		NSF CRII [1565796]	NSF CRII(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE))	This work is supported in part by NSF CRII 1565796. We thank Lester Mackey and the anonymous reviewers for their comments.	Berlinet A., 2011, REPRODUCING KERNEL H; BRAUN W, 1977, COMMUN MATH PHYS, V56, P101, DOI 10.1007/BF01611497; Chen Yutian, 2010, UAI; Chwialkowski K, 2016, PR MACH LEARN RES, V48; Dai B., 2016, 19 INT C ART INT STA; Dick J, 2013, ACTA NUMER, V22, P133, DOI 10.1017/S0962492913000044; Gorham J, 2017, PR MACH LEARN RES, V70; Han J, 2017, CONFERENCE ON UNCERTAINTY IN ARTIFICIAL INTELLIGENCE (UAI2017); Liu Q., 2016, NEURIPS; Liu Q, 2016, PR MACH LEARN RES, V48; OATES C, 2016, ARXIV160303220; Oates CJ, 2017, J R STAT SOC B, V79, P695, DOI 10.1111/rssb.12185; Otto F., 2001, THE GEOMETRY OF DISS; Spohn H., 2012, LARGE SCALE DYNAMICS; Stein C, 1986, LECT NOTES MONOGRAPH, V7, P164; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vlasov A. A., 1938, J EXP THEOR PHYS+, V8, P291; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001	19	34	34	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403018
C	Roeder, G; Wu, YH; Duvenaud, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Roeder, Geoffrey; Wu, Yuhuai; Duvenaud, David			Sticking the Landing: Simple, Lower-Variance Gradient Estimators for Variational Inference	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a simple and general variant of the standard reparameterized gradient estimator for the variational evidence lower bound. Specifically, we remove a part of the total derivative with respect to the variational parameters that corresponds to the score function. Removing this term produces an unbiased gradient estimator whose variance approaches zero as the approximate posterior approaches the exact posterior. We analyze the behavior of this gradient estimator theoretically and empirically, and generalize it to more complex variational distributions such as mixtures and importance-weighted posteriors.	[Roeder, Geoffrey; Wu, Yuhuai; Duvenaud, David] Univ Toronto, Toronto, ON, Canada	University of Toronto	Roeder, G (corresponding author), Univ Toronto, Toronto, ON, Canada.	roeder@cs.toronto.edu; ywu@cs.toronto.edu; duvenaud@cs.toronto.edu	Jeong, Yongwook/N-7413-2016					Abadi M, 2015, P 12 USENIX S OPERAT; Bergstra J., 2010, P PYTH SCI COMP C SC, V4, P1, DOI DOI 10.25080/MAJORA-92BF1922-003; Burda Yuri, 2015, ARXIV150900519; Collobert R, 2002, TECHNICAL REPORT; Dinh L, 2016, ARXIV PREPRINT ARXIV; Glorot X., 2010, PROC MACH LEARN RES, P249; Han SB, 2016, JMLR WORKSH CONF PRO, V51, P829; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Lake B. M., 2014, THESIS; LeCun Y., 1998, MNIST DATASET HANDWR; Maclaurin Dougal, 2015, AUTOGRAD REVERSEMODE; Ranganath R., 2014, ARTIFICIAL INTELLIGE, P814; Rezende DJ, 2014, PR MACH LEARN RES, V32, P1278; Rezende DJ, 2015, 32 INT C MACH LEARN; Ruiz Francisco JR, 2016, ARXIV161002287; Tan Linda SL, 2017, STAT COMPUT, P1, DOI DOI 10.1038/S41598-017-03281-Z; Tran Dustin, 2016, ARXIV161009787; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696	20	34	34	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649407002
C	Verma, S; Zhang, ZL		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Verma, Saurabh; Zhang, Zhi-Li			Hunt For The Unique, Stable, Sparse And Fast Feature Learning On Graphs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					For the purpose of learning on graphs, we hunt for a graph feature representation that exhibit certain uniqueness, stability and sparsity properties while also being amenable to fast computation. This leads to the discovery of family of graph spectral distances (denoted as FGSD) and their based graph feature representations, which we prove to possess most of these desired properties. To both evaluate the quality of graph features produced by FGSD and demonstrate their utility, we apply them to the graph classification problem. Through extensive experiments, we show that a simple SVM based classification algorithm, driven with our powerful FGSD based graph features, significantly outperforms all the more sophisticated state-of-art algorithms on the unlabeled node datasets in terms of both accuracy and speed; it also yields very competitive results on the labeled datasets - despite the fact it does not utilize any node label information.	[Verma, Saurabh; Zhang, Zhi-Li] Univ Minnesota Twin Cities, Dept Comp Sci, Minneapolis, MN 55455 USA	University of Minnesota System; University of Minnesota Twin Cities	Verma, S (corresponding author), Univ Minnesota Twin Cities, Dept Comp Sci, Minneapolis, MN 55455 USA.	verma@cs.umn.edu; zhang@cs.umn.edu	Jeong, Yongwook/N-7413-2016		ARO MURI Award [W911NF-12-1-0385]; DTRA [HDTRA1-14-1-0040]; NSF [CNS-1618339, CNS-1617729]	ARO MURI Award(MURI); DTRA(United States Department of DefenseDefense Threat Reduction Agency); NSF(National Science Foundation (NSF))	This research was supported in part by ARO MURI Award W911NF-12-1-0385, DTRA grant HDTRA1-14-1-0040, and NSF grants CNS-1618339, CNS-1618339 and CNS-1617729.	[Anonymous], 1987, MATH MAG, DOI DOI 10.1080/0025570X.1987.11977274; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Babai L., 2015, CORR; Belkin M, 2003, NEURAL COMPUT, V15, P1373, DOI 10.1162/089976603321780317; Borgwardt K. M., 2005, 5 IEEE INT C DAT MIN, DOI DOI 10.1109/ICDM.2005.132; Botsch M., 2005, Mathematics of Surfaces XI 11th IMA International Conference. Proceedings (Lecture Notes in Computer Science Vol. 3604), P62, DOI 10.1007/11537908_5; Bousquet O, 2002, J MACH LEARN RES, V2, P499, DOI 10.1162/153244302760200704; Bruna J, 2013, PROC INT C LEARN REP; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Costa Fabrizio, 2010, INT C MACHINE LEARNI, P255, DOI DOI 10.1016/J.NEUROPHARM.2007.07.003; Dai HJ, 2016, PR MACH LEARN RES, V48; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Duvenaud David K, 2015, P NIPS; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Henaff M, 2015, ARXIV150605163; Katsikis VN, 2011, APPL MATH COMPUT, V217, P9828, DOI 10.1016/j.amc.2011.04.080; Kipf TN, 2016, P INT C LEARN REPR; Kondor R., 2007, CORR; Kondor R., 2008, P 25 INT C MACH LEAR, P496, DOI DOI 10.1145/1390156.1390219; Kondor R., 2009, P 26 ANN INT C MACH, P529, DOI DOI 10.1145/1553374.1553443; Kondor R, 2016, ADV NEUR IN, V29; Lipman Y., 2010, ACM T GRAPHIC, P1, DOI DOI 10.1145/1778765.1778840; Montavon G., 2012, ADV NEURAL INF PROCE, P440; Nadler B., 2005, ADV NEURAL INFORM PR, P955; Niepert M, 2016, PR MACH LEARN RES, V48; Orsini F, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P3756; ROSENBLATT J, 1982, SIAM J ALGEBRA DISCR, V3, P343, DOI 10.1137/0603035; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Shuman DI, 2013, IEEE SIGNAL PROC MAG, V30, P83, DOI 10.1109/MSP.2012.2235192; Watson G. N., 1927, COURSE MODERN ANAL; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417	34	34	35	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400009
C	Wang, JF; Hu, XL		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Jianfeng; Hu, Xiaolin			Gated Recurrent Convolution Neural Network for OCR	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				RECOGNITION	Optical Character Recognition (OCR) aims to recognize text in natural images. Inspired by a recently proposed model for general image classification, Recurrent Convolution Neural Network (RCNN), we propose a new architecture named Gated RCNN (GRCNN) for solving this problem. Its critical component, Gated Recurrent Convolution Layer (GRCL), is constructed by adding a gate to the Recurrent Convolution Layer (RCL), the critical component of RCNN. The gate controls the context modulation in RCL and balances the feed-forward information and the recurrent information. In addition, an efficient Bidirectional Long Short-Term Memory (BLSTM) is built for sequence modeling. The GRCNN is combined with BLSTM to recognize text in natural images. The entire GRCNN-BLSTM model can be trained end-to-end. Experiments show that the proposed model outperforms existing methods on several benchmark datasets including the IIIT-5K, Street View Text (SVT) and ICDAR.	[Wang, Jianfeng] Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China; [Hu, Xiaolin] Tsinghua Univ, Tsinghua Natl Lab Informat Sci & Technol TNList, Dept Comp Sci & Technol, CBICR, Beijing 100084, Peoples R China	Beijing University of Posts & Telecommunications; Tsinghua University	Wang, JF (corresponding author), Beijing Univ Posts & Telecommun, Beijing 100876, Peoples R China.	jianfengwang1991@gmail.com; xlhu@tsinghua.edu.cn	Jeong, Yongwook/N-7413-2016		National Basic Research Program (973 Program) of China [2013CB329403]; National Natural Science Foundation of China [91420201, 61332007, 61621136008, 61620106010]	National Basic Research Program (973 Program) of China(National Basic Research Program of China); National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	This work was supported in part by the National Basic Research Program (973 Program) of China under grant no. 2013CB329403, the National Natural Science Foundation of China under grant nos. 91420201, 61332007, 61621136008 and 61620106010, and in part by a grant from Sensetime.	Almazan J, 2014, IEEE T PATTERN ANAL, V36, P2552, DOI 10.1109/TPAMI.2014.2339814; Alsharif O., 2013, COMPUTER SCI; Bissacco A, 2013, IEEE I CONF COMP VIS, P785, DOI 10.1109/ICCV.2013.102; Cho Kyunghyun, 2014, ARXIV, DOI 10.3115/v1/w14-4012; Gers FA, 2000, IEEE IJCNN, P189, DOI 10.1109/IJCNN.2000.861302; Goel V, 2013, PROC INT CONF DOC, P398, DOI 10.1109/ICDAR.2013.87; Gordo A., 1998, CVPR, P2956; Graves A., 2006, P 23 INT C MACH LEAR, P369; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; HUBEL DH, 1965, J NEUROPHYSIOL, V28, P229, DOI 10.1152/jn.1965.28.2.229; Jaderberg M, 2014, ABS14062227 CORR; Jaderberg M., 2014, ICLR; Jaderberg M, 2014, LECT NOTES COMPUT SC, V8692, P512, DOI 10.1007/978-3-319-10593-2_34; Jawahar C. V., 2014, CVPR, P2687; Jones HE, 2001, J NEUROPHYSIOL, V86, P2011, DOI 10.1152/jn.2001.86.4.2011; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lee CY, 2016, PROC CVPR IEEE, P2231, DOI 10.1109/CVPR.2016.245; Lee CY, 2014, PROC CVPR IEEE, P4050, DOI 10.1109/CVPR.2014.516; Liang M., 2015, NIPS; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liu XY, 2017, IEEE INT C INTELL TR; Lucas SM, 2003, PROC INT CONF DOC, P682; Mishra A., 2013, BMVC; Neumann L, 2012, PROC CVPR IEEE, P3538, DOI 10.1109/CVPR.2012.6248097; NOVIKOVA T, 2012, ECCV, V7577, P752; Rodriguez-Serrano JA, 2015, INT J COMPUT VISION, V113, P193, DOI 10.1007/s11263-014-0793-6; Schuster M., 1997, BIDIRECTIONAL RECURR; Shi BG, 2017, IEEE T PATTERN ANAL, V39, P2298, DOI 10.1109/TPAMI.2016.2646371; Shi CZ, 2013, PROC CVPR IEEE, P2961, DOI 10.1109/CVPR.2013.381; Socher R, 2010, NIPS; Su BL, 2015, LECT NOTES COMPUT SC, V9003, P35, DOI 10.1007/978-3-319-16865-4_3; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Wang J., 2016, NIPS; Wang K, 2011, IEEE I CONF COMP VIS, P1457, DOI 10.1109/ICCV.2011.6126402; Wang K, 2010, LECT NOTES COMPUT SC, V6311, P591, DOI 10.1007/978-3-642-15549-9_43; Wang T, 2012, INT C PATT RECOG, P3304; Xie SN, 2017, PROC CVPR IEEE, P5987, DOI 10.1109/CVPR.2017.634; Yao C, 2014, PROC CVPR IEEE, P4042, DOI 10.1109/CVPR.2014.515; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zeiler Matthew D, 2012, ARXIV12125701	43	34	34	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400032
C	Atwood, J; Towsley, D		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Atwood, James; Towsley, Don			Diffusion-Convolutional Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We present diffusion-convolutional neural networks (DCNNs), a new model for graph-structured data. Through the introduction of a diffusion-convolution operation, we show how diffusion-based representations can be learned from graphstructured data and used as an effective basis for node classification. DCNNs have several attractive qualities, including a latent representation for graphical data that is invariant under isomorphism, as well as polynomial-time prediction and learning that can be represented as tensor operations and efficiently implemented on a GPU. Through several experiments with real structured datasets, we demonstrate that DCNNs are able to outperform probabilistic relational models and kernel-on-graph methods at relational node classification tasks.	[Atwood, James; Towsley, Don] Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Atwood, J (corresponding author), Univ Massachusetts, Coll Informat & Comp Sci, Amherst, MA 01003 USA.	jatwood@cs.umass.edu; towsley@cs.umass.edu			Army Research Office [W911NF-12-1-0385]; ARL [W911NF-09-2-0053]	Army Research Office; ARL(United States Department of DefenseUS Army Research Laboratory (ARL))	We would like to thank Bruno Ribeiro, Pinar Yanardag, and David Belanger for their feedback on drafts of this paper. This work was supported in part by Army Research Office Contract W911NF-12-1-0385 and ARL Cooperative Agreement W911NF-09-2-0053. This work was also supported by NVIDIA through the donation of equipment used to perform experiments.	[Anonymous], 2010, PYTH SCI COMP C; [Anonymous], 2015, NIPS; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Bruna J, 2014, SPECTRAL NETWORKS LO; Cohn  Trevor, 2006, ECML; DEBNATH AK, 1991, J MED CHEM, V34, P786, DOI 10.1021/jm00106a046; Duchi J., 2011, J MACHINE LEARNING R; Fouss F, 2012, NEURAL NETWORKS, V31, P53, DOI 10.1016/j.neunet.2012.03.001; Henaff M., 2015, COMPUT SCI; Koller D., 2009, PROBABILISTIC GRAPHI; Micheli A., 2009, IEEE T NEURAL NETWOR; Scarselli F., 2009, IEEE T NEURAL NETWOR; Sen  P, 2007, TECHNICAL REPORT; Sen Prithviraj, 2008, AI MAGAZINE; Toivonen H, 2003, BIOINFORMATICS, V19, P1183, DOI 10.1093/bioinformatics/btg130; Verbeek J., 2007, NIPS; Wale  Nikil, 2007, KNOWL INF SYST, V14, P347; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417	18	34	34	7	23	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701069
C	Gruslys, A; Munos, R; Danihelka, I; Lanctot, M; Graves, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Gruslys, Audrunas; Munos, Remi; Danihelka, Ivo; Lanctot, Marc; Graves, Alex			Memory-Efficient Backpropagation Through Time	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose a novel approach to reduce memory consumption of the backpropagation through time (BPTT) algorithm when training recurrent neural networks (RNNs). Our approach uses dynamic programming to balance a trade-off between caching of intermediate results and recomputation. The algorithm is capable of tightly fitting within almost any user-set memory budget while finding an optimal execution policy minimizing the computational cost. Computational devices have limited memory capacity and maximizing a computational performance given a fixed memory budget is a practical use-case. We provide asymptotic computational upper bounds for various regimes. The algorithm is particularly effective for long sequences. For sequences of length 1000, our algorithm saves 95% of memory usage while using only one third more time per iteration than the standard BPTT.	[Gruslys, Audrunas; Munos, Remi; Danihelka, Ivo; Lanctot, Marc; Graves, Alex] Google DeepMind, London, England	Google Incorporated	Gruslys, A (corresponding author), Google DeepMind, London, England.	audrunas@google.com; munos@google.com; danihelka@google.com; lanctot@google.com; gravesa@google.com						Chen Tianqi, 2016, TRAINING DEEP NETS S, V6, P6; Dauvergne B, 2006, LECT NOTES COMPUT SC, V3994, P566; Eck D., 2002, I DALLE MOLLE STUDI, V103, P48; Graves A., 2012, STUDIES COMPUTATIONA; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Grefenstette E, 2015, ADV NEUR IN, V28; Gregor K, 2015, PR MACH LEARN RES, V37, P1462; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Ignateva A., 2015, ARXIV PREPRINT ARXIV; Mnih V, 2016, PR MACH LEARN RES, V48; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Sutskever Ilya, 2011, P 28 INT C MACH LEAR; WERBOS PJ, 1990, P IEEE, V78, P1550, DOI 10.1109/5.58337	14	34	35	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701078
C	Mairal, J		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Mairal, Julien			End-to-End Kernel Learning with Supervised Convolutional Kernel Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					In this paper, we introduce a new image representation based on a multilayer kernel machine. Unlike traditional kernel methods where data representation is decoupled from the prediction task, we learn how to shape the kernel with supervision. We proceed by first proposing improvements of the recently-introduced convolutional kernel networks (CKNs) in the context of unsupervised learning; then, we derive backpropagation rules to take advantage of labeled training data. The resulting model is a new type of convolutional neural network, where optimizing the filters at each layer is equivalent to learning a linear subspace in a reproducing kernel Hilbert space (RKHS). We show that our method achieves reasonably competitive performance for image classification on some standard "deep learning" datasets such as CIFAR-10 and SVHN, and also for image super-resolution, demonstrating the applicability of our approach to a large variety of image-related tasks.	[Mairal, Julien] INRIA, Rennes, France	Inria	Mairal, J (corresponding author), INRIA, Rennes, France.	julien.mairal@inria.fr	Mairal, Julien/AAL-5611-2021		ANR (MACARON project) [ANR-14-CE23-0003-01]	ANR (MACARON project)(French National Research Agency (ANR))	This work was supported by ANR (MACARON project ANR-14-CE23-0003-01).	Absil PA, 2008, OPTIMIZATION ALGORITHMS ON MATRIX MANIFOLDS, P1; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arbelaez P, 2011, IEEE T PATTERN ANAL, V33, P898, DOI 10.1109/TPAMI.2010.161; Bo L., 2011, CVPR; Broomhead D. S., 1988, Complex Systems, V2, P321; Cho Y., 2009, ADV NIPS; Damianou Andreas, 2013, ARTIF INTELL, P207, DOI DOI 10.1002/NME.1296; Dong C, 2016, IEEE T PATTERN ANAL, V38, P295, DOI 10.1109/TPAMI.2015.2439281; Dong C, 2014, LECT NOTES COMPUT SC, V8692, P184, DOI 10.1007/978-3-319-10593-2_13; Goodfellow I. J., 2013, P ICML; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Y, 1998, LECT NOTES COMPUT SC, V1524, P9, DOI 10.1007/3-540-49430-8_2; Lee C. - Y., 2016, P AISTATS; Lee CY, 2015, JMLR WORKSH CONF PRO, V38, P562; Lin H., 2015, ADV NIPS; Lin M., 2013, P ICLR; Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Paulin M., 2015, P ICCV; Peters J, 2017, ADAPT COMPUT MACH LE; Rahimi A., 2007, ADV NIPS; Sonnenburg S, 2006, J MACH LEARN RES, V7, P1531; Sydorov V., 2014, P CVPR; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Timofte R, 2013, IEEE I CONF COMP VIS, P1920, DOI 10.1109/ICCV.2013.241; Wang Z., 2015, P ICCV; Williams C., 2001, ADV NIPS; Zeiler MD, 2013, ARXIV13013557, DOI DOI 10.1007/978-3-319-26532-2_6; Zeyde Roman, 2010, INT C CURV SURF, P711, DOI DOI 10.1007/978-3-642-27413-8_47; Zhang K., 2008, P ICML	31	34	34	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701041
C	Gonen, M; Margolin, AA		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Gonen, Mehmet; Margolin, Adam A.			Localized Data Fusion for Kernel k-Means Clustering with Application to Cancer Biology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					In many modern applications from, for example, bioinformatics and computer vision, samples have multiple feature representations coming from different data sources. Multiview learning algorithms try to exploit all these available information to obtain a better learner in such scenarios. In this paper, we propose a novel multiple kernel learning algorithm that extends kernel k-means clustering to the multiview setting, which combines kernels calculated on the views in a localized way to better capture sample-specific characteristics of the data. We demonstrate the better performance of our localized data fusion approach on a human colon and rectal cancer data set by clustering patients. Our method finds more relevant prognostic patient groups than global data fusion methods when we evaluate the results with respect to three commonly used clinical biomarkers.	[Gonen, Mehmet; Margolin, Adam A.] Oregon Hlth & Sci Univ, Dept Biomed Engn, Portland, OR 97239 USA	Oregon Health & Science University	Gonen, M (corresponding author), Oregon Hlth & Sci Univ, Dept Biomed Engn, Portland, OR 97239 USA.	gonen@ohsu.edu; margolin@ohsu.edu		Gonen, Mehmet/0000-0002-2483-075X	Integrative Cancer Biology Program [1U54CA149237]; Cancer Target Discovery and Development (CTDD) Network of the National Cancer Institute [1U01CA176303]	Integrative Cancer Biology Program(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Cancer Institute (NCI)); Cancer Target Discovery and Development (CTDD) Network of the National Cancer Institute	This study was financially supported by the Integrative Cancer Biology Program (grant no 1U54CA149237) and the Cancer Target Discovery and Development (CTDD) Network (grant no 1U01CA176303) of the National Cancer Institute.	[Anonymous], 2011, ADV NEURAL INFORM PR; Blaschko M.B., 2008, P IEEE C COMP VIS PA; Chaudhuri K., 2009, P 26 INT C MACH LEAR; Chen J., 2007, P 13 ACM SIGKDD INT; Ding C, 2004, P 21 INT C MACHINE L; Girolami M, 2002, IEEE T NEURAL NETWOR, V13, P780, DOI 10.1109/TNN.2002.1000150; Gonen M, 2013, PATTERN RECOGN, V46, P795, DOI 10.1016/j.patcog.2012.09.002; Gonen M, 2011, J MACH LEARN RES, V12, P2211; Hartigan J.A., 1975, CLUSTERING ALGORITHM; Hotelling H, 1936, BIOMETRIKA, V28, P321, DOI 10.1093/biomet/28.3-4.321; Huang H.-C., 2012, P IEEE C COMP VIS PA; Jin R., 2007, ADV NEURAL INFORM PR; Lai P L, 2000, Int J Neural Syst, V10, P365, DOI 10.1016/S0129-0657(00)00034-X; Lange T., 2006, ADV NEURAL INFORM PR, V18; Mo QX, 2013, P NATL ACAD SCI USA, V110, P4245, DOI 10.1073/pnas.1208949110; Mosek, 2014, MOSEK OPT TOOLS MAN; Muzny DM, 2012, NATURE, V487, P330, DOI 10.1038/nature11252; Noble W. S., 2004, KERNEL METHODS COMPU; Pearson K, 1901, PHILOS MAG, V2, P559, DOI 10.1080/14786440109462720; Scholkopf B, 1998, NEURAL COMPUT, V10, P1299, DOI 10.1162/089976698300017467; Shen R, 2012, PLOS ONE, V7, DOI 10.1371/journal.pone.0035236; Strehl A., 2003, Journal of Machine Learning Research, V3, P583, DOI 10.1162/153244303321897735; Tang W., 2009, P 9 IEEE INT C DAT M; Wang B, 2014, NAT METHODS, V11, P333, DOI [10.1038/NMETH.2810, 10.1038/nmeth.2810]; XU L, 2004, ADV NEURAL INFORM PR, V17; Yu S, 2012, IEEE T PATTERN ANAL, V34, P1031, DOI 10.1109/TPAMI.2011.255; Yuan YY, 2011, PLOS COMPUT BIOL, V7, DOI 10.1371/journal.pcbi.1002227; Zha H., 2001, ADV NEURAL INFORM PR, V14	28	34	34	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100015
C	Lewicki, MS; Olshausen, BA		Jordan, MI; Kearns, MJ; Solla, SA		Lewicki, MS; Olshausen, BA			Inferring sparse, overcomplete image codes using an efficient coding framework	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					We apply a general technique for learning overcomplete bases (Lewicki and Sejnowski, 1998) to the problem of finding efficient image codes. The bases learned by the algorithm are localized, oriented, and bandpass, consistent with earlier results obtained using different methods (Olshausen and Field, 1996; Bell and Sejnowski, 1997). We show that the learned bases are Gabor-like in structure and that higher degrees of overcompleteness produce greater sampling density in position, orientation, and scale. The efficient coding framework provides a method for comparing different bases objectively by calculating their probability given the observed data. Compared to complete and overcomplete Fourier and wavelet bases, the learned bases have much better coding efficiency. We demonstrate the improvement in the representation of the learned bases by showing superior noise reduction properties.	Salk Inst Biol Studies, Howard Hughes Med Inst, Computat Neurobiol Lab, La Jolla, CA 92037 USA	Howard Hughes Medical Institute; Salk Institute	Lewicki, MS (corresponding author), Salk Inst Biol Studies, Howard Hughes Med Inst, Computat Neurobiol Lab, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.								0	34	36	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						815	821						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700115
C	Precup, D; Sutton, RS		Jordan, MI; Kearns, MJ; Solla, SA		Precup, D; Sutton, RS			Multi-time models for temporally abstract planning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Planning and learning at multiple levels of temporal abstraction is a key problem for artificial intelligence. In this paper we summarize an approach to this problem based on the mathematical framework of Markov decision processes and reinforcement learning. Current model-based reinforcement learning is based on one-step models that cannot represent common-sense higher-level actions, such as going to lunch, grasping an object, or flying to Denver. This paper generalizes prior work on temporally abstract models [Sutton, 1995] and extends it from the prediction setting to include actions, control, and planning. We introduce a more general form of temporally abstract model, the multi-time model, and establish its suitability for planning and learning by virtue of its relationship to the Bellman equations. This paper summarizes the theoretical framework of multi-time models and illustrates their potential advantages in a gridworld planning task.	Univ Massachusetts, Amherst, MA 01003 USA	University of Massachusetts System; University of Massachusetts Amherst	Precup, D (corresponding author), Univ Massachusetts, Amherst, MA 01003 USA.								0	34	35	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						1050	1056						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700148
C	GILES, CL; MILLER, CB; CHEN, D; SUN, GZ; CHEN, HH; LEE, YC		MOODY, JE; HANSON, SJ; LIPPMANN, RP		GILES, CL; MILLER, CB; CHEN, D; SUN, GZ; CHEN, HH; LEE, YC			EXTRACTING AND LEARNING AN UNKNOWN GRAMMAR WITH RECURRENT NEURAL NETWORKS	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	34	36	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						317	324						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00039
C	Bucher, M; Vu, TH; Cord, M; Perez, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bucher, Maxime; Vu, Tuan-Hung; Cord, Matthieu; Perez, Patrick			Zero-Shot Semantic Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Semantic segmentation models are limited in their ability to scale to large numbers of object classes. In this paper, we introduce the new task of zero-shot semantic segmentation: learning pixel-wise classifiers for never-seen object categories with zero training examples. To this end, we present a novel architecture, ZS3Net, combining a deep visual segmentation model with an approach to generate visual representations from semantic word embeddings. By this way, ZS3Net addresses pixel classification tasks where both seen and unseen categories are faced at test time (so called "generalized" zero-shot classification). Performance is further improved by a self-training step that relies on automatic pseudo-labeling of pixels from unseen classes. On the two standard segmentation datasets, Pascal-VOC and Pascal-Context, we propose zero-shot benchmarks and set competitive baselines. For complex scenes as ones in the Pascal-Context dataset, we extend our approach by using a graph-context encoding to fully leverage spatial context priors coming from class-wise segmentation maps.	[Bucher, Maxime; Vu, Tuan-Hung; Cord, Matthieu; Perez, Patrick] Valeo Ai, Paris, France; [Cord, Matthieu] Sorbonne Univ, Paris, France	UDICE-French Research Universities; Sorbonne Universite	Bucher, M (corresponding author), Valeo Ai, Paris, France.	maxime.bucher@valeo.com; tuan-hung.vu@valeo.com; matthieu.cord@lip6.fr; patrick.perez@valeo.com	VU, Tuan-Hung/AAD-4516-2021					Akata Z, 2015, PROC CVPR IEEE, P2927, DOI 10.1109/CVPR.2015.7298911; Akata Zeynep, 2015, TPAMI; [Anonymous], 2018, PAMI; Badrinarayanan V, 2017, IEEE T PATTERN ANAL, V39, P2481, DOI 10.1109/TPAMI.2016.2644615; Bansal A., 2018, EUR C COMP VIS ECCV; Bottou L, 2010, COMPSTAT'2010: 19TH INTERNATIONAL CONFERENCE ON COMPUTATIONAL STATISTICS, P177, DOI 10.1007/978-3-7908-2604-3_16; Bucher M., 2016, ECCV; Bucher Maxime, 2017, ICCV WORKSH; Changpinyo S, 2016, PROC CVPR IEEE, P5327, DOI 10.1109/CVPR.2016.575; Chao WL, 2016, LECT NOTES COMPUT SC, V9906, P52, DOI 10.1007/978-3-319-46475-6_4; Chen LCE, 2018, LECT NOTES COMPUT SC, V11211, P833, DOI 10.1007/978-3-030-01234-2_49; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Csurka G, 2013, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2013, DOI 10.5244/C.27.32; Dai JF, 2015, IEEE I CONF COMP VIS, P1635, DOI 10.1109/ICCV.2015.191; Demirel B., 2018, BRIT MACH VIS C BMVC; Everingham M, 2015, INT J COMPUT VISION, V111, P98, DOI 10.1007/s11263-014-0733-5; Frome Andrea, 2013, NEURIPS; Girshick R., 2014, CVPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hariharan Bharath, 2011, P IEEE C COMP VIS PA; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; Kingma D. P, 2014, ARXIV13126114; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kodirov E, 2017, PROC CVPR IEEE, P4447, DOI 10.1109/CVPR.2017.473; Li Y., 2017, IEEE C COMP VIS PATT; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Maas Andrew L, 2013, P ICML CIT, V30, P3, DOI DOI 10.21437/INTERSPEECH.2016-1230; Mikolov T., 2013, ARXIV; Mottaghi R., 2014, IEEE C COMP VIS PATT; Papandreou G, 2015, IEEE I CONF COMP VIS, P1742, DOI 10.1109/ICCV.2015.203; Pathak D, 2015, IEEE I CONF COMP VIS, P1796, DOI 10.1109/ICCV.2015.209; Rahman S, 2019, LECT NOTES COMPUT SC, V11361, P547, DOI 10.1007/978-3-030-20887-5_34; Romera-Paredes B, 2015, PR MACH LEARN RES, V37, P2152; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Socher Richard, 2013, NEURIPS; Song J, 2018, PROC CVPR IEEE, P1024, DOI 10.1109/CVPR.2018.00113; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Verma V.K., 2018, CVPR; Verma VK, 2017, LECT NOTES ARTIF INT, V10535, P792, DOI 10.1007/978-3-319-71246-8_48; Xian YQ, 2018, PROC CVPR IEEE, P5542, DOI 10.1109/CVPR.2018.00581; Xian YQ, 2016, PROC CVPR IEEE, P69, DOI 10.1109/CVPR.2016.15; Yu F., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1006/JMBI.1990.9999; Zhang H., 2018, CVPR; Zhang Z., 2016, ECCV; Zhang Z., 2015, ICCV; Zhao HS, 2017, PROC CVPR IEEE, P6230, DOI 10.1109/CVPR.2017.660; Zhu P., 2019, TCSVT; Zhu X.J, 2005, SEMISUPERVISED LEARN	51	33	34	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300043
C	Dombrowski, AK; Alber, M; Anders, CJ; Ackermann, M; Muller, KR; KesselI, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dombrowski, Ann-Kathrin; Alber, Maximilian; Anders, Christopher J.; Ackermann, Marcel; Mueller, Klaus-Robert; Kessel, Pan, I			Explanations can be manipulated and geometry is to blame	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Explanation methods aim to make neural networks more trustworthy and interpretable. In this paper, we demonstrate a property of explanation methods which is disconcerting for both of these purposes. Namely, we show that explanations can be manipulated arbitrarily by applying visually hardly perceptible perturbations to the input that keep the network's output approximately constant. We establish theoretically that this phenomenon can be related to certain geometrical properties of neural networks. This allows us to derive an upper bound on the susceptibility of explanations to manipulations. Based on this result, we propose effective mechanisms to enhance the robustness of explanations. [GRAPHICS] .	[Dombrowski, Ann-Kathrin; Anders, Christopher J.; Mueller, Klaus-Robert; Kessel, Pan, I] Tech Univ Berlin, Machine Learning Grp, Berlin, Germany; [Ackermann, Marcel] Fraunhofer Heinrich Hertz Inst, Dept Video Coding & Analyt, Berlin, Germany; [Mueller, Klaus-Robert] Max Planck Inst Informat, Saarbrucken, Germany; [Mueller, Klaus-Robert] Korea Univ, Dept Brain & Cognit Engn, Seoul, South Korea; [Alber, Maximilian] Charite, Berlin, Germany	Technical University of Berlin; Fraunhofer Gesellschaft; Max Planck Society; Korea University; Free University of Berlin; Humboldt University of Berlin; Charite Universitatsmedizin Berlin	Dombrowski, AK (corresponding author), Tech Univ Berlin, Machine Learning Grp, Berlin, Germany.		Mueller, Klaus-Robert/Y-3547-2019	Mueller, Klaus-Robert/0000-0002-3861-7685	German Ministry for Education and Research as Berlin Big Data Center [01IS18025A]; German Ministry for Education and Research as Berlin Center for Machine Learning [01IS180371]; Information & Communications Technology Planning & Evaluation (IITP) - Korea government [2017-0-001779]; Research Training Group "Differential Equation-and Data-driven Models in Life Sciences and Fluid Dynamics (DAEDALUS)" [GRK 2433]; Grant Math+; German Research Foundation (DFG) [EXC 2046/1, 390685689]	German Ministry for Education and Research as Berlin Big Data Center; German Ministry for Education and Research as Berlin Center for Machine Learning; Information & Communications Technology Planning & Evaluation (IITP) - Korea government(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of Korea); Research Training Group "Differential Equation-and Data-driven Models in Life Sciences and Fluid Dynamics (DAEDALUS)"; Grant Math+; German Research Foundation (DFG)(German Research Foundation (DFG))	We want to thank the anonymous reviewers for their helpful feedback. We also thank Kristof Schutt, Gregoire Montavon and Shinichi Nakajima for useful discussions. This work is supported by the German Ministry for Education and Research as Berlin Big Data Center (01IS18025A) and Berlin Center for Machine Learning (01IS180371). This work is also supported by the Information & Communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (No. 2017-0-001779), as well as by the Research Training Group "Differential Equation-and Data-driven Models in Life Sciences and Fluid Dynamics (DAEDALUS)" (GRK 2433) and Grant Math+, EXC 2046/1, Project ID 390685689 both funded by the German Research Foundation (DFG).	Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Alber M, 2019, J MACH LEARN RES, V20; Alvarez-Melis D, 2018, ADV NEUR IN, V31; Alvarez-Melis David, 2018, ARXIV180608049; Ancona Marco, 2018, ICLR, DOI DOI 10.1109/TNSE.2020.2996738; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bach S, 2015, PLOS ONE, V10, DOI 10.1371/journal.pone.0130140; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Dabkowski P, 2017, ADV NEUR IN, V30; Fong RC, 2017, IEEE I CONF COMP VIS, P3449, DOI 10.1109/ICCV.2017.371; Ghorbani A, 2019, AAAI CONF ARTIF INTE, P3681; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heo J, 2019, ADV NEUR IN, V32; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kindermans P., 2018, P INT C LEARNING REP, P1; Kindermans P-J, 2019, EXPLAINABLE INTERPRE, P267, DOI DOI 10.1007/978-3-030-28954-6_14; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lapuschkin S, 2019, NAT COMMUN, V10, DOI 10.1038/s41467-019-08987-4; Lundberg SM, 2017, ADV NEUR IN, V30; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Samek W., 2019, EXPLAINABLE AI INTER, DOI DOI 10.1007/978-3-030-28954-6; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Smilkov D, 2017, ARXIV; Springenberg Jost Tobias, 2015, 3 INT LEARN REPR ICL; Sundararajan M, 2017, PR MACH LEARN RES, V70; Tu Loring W, 2017, DIFFERENTIAL GEOMETR, V275; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zintgraf Luisa M., 2017, P ICLR	35	33	34	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905026
C	Fedorov, I; Adams, RP; Mattina, M; Whatmough, PN		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fedorov, Igor; Adams, Ryan P.; Mattina, Matthew; Whatmough, Paul N.			SpArSe: Sparse Architecture Search for CNNs on Resource-Constrained Microcontrollers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				INTERNET; THINGS	The vast majority of processors in the world are actually microcontroller units (MCUs), which find widespread use performing simple control tasks in applications ranging from automobiles to medical devices and office equipment. The Internet of Things (IoT) promises to inject machine learning into many of these every-day objects via tiny, cheap MCUs. However, these resource-impoverished hardware platforms severely limit the complexity of machine learning models that can be deployed. For example, although convolutional neural networks (CNNs) achieve state-of-the-art results on many visual recognition tasks, CNN inference on MCUs is challenging due to severe memory limitations. To circumvent the memory challenge associated with CNNs, various alternatives have been proposed that do fit within the memory budget of an MCU, albeit at the cost of prediction accuracy. This paper challenges the idea that CNNs are not suitable for deployment on MCUs. We demonstrate that it is possible to automatically design CNNs which generalize well, while also being small enough to fit onto memory-limited MCUs. Our Sparse Architecture Search method combines neural architecture search with pruning in a single, unified approach, which learns superior models on four popular IoT datasets. The CNNs we find are more accurate and up to 7.4x smaller than previous approaches, while meeting the strict MCU working memory constraint.	[Fedorov, Igor; Mattina, Matthew; Whatmough, Paul N.] Arm ML Res, Adelphi, MD 20783 USA; [Adams, Ryan P.] Princeton Univ, Princeton, NJ 08544 USA	Princeton University	Fedorov, I (corresponding author), Arm ML Res, Adelphi, MD 20783 USA.	igor.fedorov@arm.com; rpa@princeton.edu; matthew.mattina@arm.com; paul.whatmough@arm.com						[Anonymous], 2018, EUR C COMP VIS ECCV; Atzori L, 2010, COMPUT NETW, V54, P2787, DOI 10.1016/j.comnet.2010.05.010; Cai H., 2019, PROC INT C LEARN REP; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Darrell T., 2019, INT C LEARN REPR; de Campos Teofilo Emidio, 2009, CHARACTER RECOGNITIO, V2, P273; Dekel O., 2016, COMMUNICATION; Elsken T., 2018, J MACH LEARN RES; Elsken T, 2019, J MACH LEARN RES, V20; Elsken Thomas, 2019, INT C LEARN REPR; Falkner S, 2018, PR MACH LEARN RES, V80; Frankle J., 2019, TRAINABLE NEURAL NET, P05; Gale Trevor, 2019, CORR; Garrido-Merchan E.., 2018, DEALING CATEGORICAL; Gope Dibakar, 2019, TERNARY MOBILENETS V; Gubbi J, 2013, FUTURE GENER COMP SY, V29, P1645, DOI 10.1016/j.future.2013.01.010; Guo Z., 2019, ARXIV190400420; Gupta C, 2017, PR MACH LEARN RES, V70; Gural A, 2019, PR MACH LEARN RES, V97; Han S, 2016, CONF PROC INT SYMP C, P243, DOI 10.1109/ISCA.2016.30; He YH, 2017, IEEE I CONF COMP VIS, P1398, DOI 10.1109/ICCV.2017.155; He Yihui, 2018, P EUR C COMP VIS ECC, P784; Hernandez-Lobato Jose Miguel, 2016, NIPS WORKSH BAYES OP; Howard A. G., 2017, MOBILENETS EFFICIENT; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; Kingma D., 2014, ADAM METHOD STOCHAST; Kingma Durk P, 2015, ADV NEURAL INFORM PR, P2575; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2017, COMMUN ACM, V60, P84, DOI 10.1145/3065386; Kumar A, 2017, PR MACH LEARN RES, V70; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liu Hanxiao, 2019, INT C LEARNING REPRE; Meunier Francois, 2014, MORGAN STANLEY R MAR, P1; Molchanov D, 2017, PR MACH LEARN RES, V70; Molchanov P., 2016, CORR; Paria B., 2018, CORR; RASMUSSEN CE, 2004, SUMMER SCH MACHINE L, V3176, P00063; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Sifre Laurent, 2014, THESIS; Stamoulis D., 2019, ARXIV190402877; Swersky K., P NIPS WORKSH BAYES; Tan Mingxing, 2018, ARXIV180711626; Thakker Urmish, 2019, ABS191002558 ARXIV; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236; UN, 2010, WORLD POP PROSP 2010; Varma M, 2005, INT J COMPUT VISION, V62, P61, DOI 10.1007/s11263-005-4635-4; Wang Kuan, 2019, P IEEE C COMP VIS PA; Wei T, 2016, PR MACH LEARN RES, V48; Welling M, 2017, P 31 C NEUR INF PROC, P3288; Whatmough Paul N., 2019, P 2 SYSML C PAL ALT; Winther O.:, 2016, ARXIV160202282; Yang T.-J., 2017, P IEEE C COMP VIS PA, P5687; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zoph B., 2016, ICLR; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	55	33	33	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305003
C	Liu, XH; Yin, GJ; Shao, J; Wang, XG; Li, HS		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Liu, Xihui; Yin, Guojun; Shao, Jing; Wang, Xiaogang; Li, Hongsheng			Learning to Predict Layout-to-image Conditional Convolutions for Semantic Image Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Semantic image synthesis aims at generating photorealistic images from semantic layouts. Previous approaches with conditional generative adversarial networks (GAN) show state-of-the-art performance on this task, which either feed the semantic label maps as inputs to the generator, or use them to modulate the activations in normalization layers via affine transformations. We argue that convolutional kernels in the generator should be aware of the distinct semantic labels at different locations when generating images. In order to better exploit the semantic layout for the image generator, we propose to predict convolutional kernels conditioned on the semantic label map to generate the intermediate feature maps from the noise maps and eventually generate the images. Moreover, we propose a feature pyramid semantics-embedding discriminator, which is more effective in enhancing fine details and semantic alignments between the generated images and the input semantic layouts than previous multi-scale discriminators. We achieve state-of-the-art results on both quantitative metrics and subjective evaluation on various semantic segmentation datasets, demonstrating the effectiveness of our approach.(1)	[Liu, Xihui; Wang, Xiaogang; Li, Hongsheng] Chinese Univ Hong Kong, Hong Kong, Peoples R China; [Yin, Guojun] Univ Sci & Technol China, Hefei, Peoples R China; [Shao, Jing] SenseTime Res, Hong Kong, Peoples R China	Chinese University of Hong Kong; Chinese Academy of Sciences; University of Science & Technology of China, CAS	Liu, XH (corresponding author), Chinese Univ Hong Kong, Hong Kong, Peoples R China.	xihuiliu@ee.cuhk.edu.hk; gjyin91@gmail.com; shaojing@sensetime.com; xgwang@ee.cuhk.edu.hk; hsli@ee.cuhk.edu.hk			SenseTime Group Limited; General Research Fund through the Research Grants Council of Hong Kong [CUHK14202217, CUHK14203118, CUHK14207319, CUHK14208417, CUHK14239816]; CUHK Direct Grant	SenseTime Group Limited; General Research Fund through the Research Grants Council of Hong Kong; CUHK Direct Grant(Chinese University of Hong Kong)	This work is supported in part by SenseTime Group Limited, in part by the General Research Fund through the Research Grants Council of Hong Kong under Grants CUHK14202217, CUHK14203118, CUHK14207319, CUHK14208417, CUHK14239816, and in part by CUHK Direct Grant. We thank Lu Sheng for proofreading and helpful suggestions on the paper.	[Anonymous], 2017, ADV NEURAL INFORM PR; Brabandere B.D., 2016, ADV NEURAL INFORM PR, P667; Brock A., 2018, ARXIV; Chollet F, 2017, PROC CVPR IEEE, P1800, DOI 10.1109/CVPR.2017.195; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Ha David, 2016, ARXIV160909106; Harley AW, 2017, IEEE I CONF COMP VIS, P5048, DOI 10.1109/ICCV.2017.539; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; Howard A. G., 2017, MOBILENETS EFFICIENT; Hu Xuecai, 2019, ARXIV190300875; Isola P, 2017, PROC CVPR IEEE, P5967, DOI 10.1109/CVPR.2017.632; Jin DK, 2017, COMPUT VIS PATT REC, P151, DOI 10.1016/B978-0-08-101291-8.00007-9; Jo Y, 2018, PROC CVPR IEEE, P3224, DOI 10.1109/CVPR.2018.00340; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Karras T., 2018, ARXIV181204948; Karras T, 2017, ARXIV171010196; Kingma D.P, P 3 INT C LEARNING R; Koyama M, 2018, P INT C LEARN REPR; Li ZX, 2018, INT CONF WEARAB IMPL, P185; Li Z, 2017, DESTECH TRANS SOC, P495; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Lucic Mario, 2019, ARXIV190302271; Ma NN, 2018, LECT NOTES COMPUT SC, V11218, P122, DOI 10.1007/978-3-030-01264-9_8; Park Taesung, 2019, ARXIV190307291; Shen FL, 2018, PROC CVPR IEEE, P8061, DOI 10.1109/CVPR.2018.00841; Su Hang, 2019, ARXIV190405373; Wang TC, 2018, PROC CVPR IEEE, P8798, DOI 10.1109/CVPR.2018.00917; Xu T., 2018, CVPR; Xue Tianfan, 2016, ADV NEURAL INFORM PR, P2; Zhang H., 2018, ARXIV180508318; Zhang H., 2017, ICCV; Zhao Fang, 2018, P EUR C COMP VIS ECC, P19; Zhou GB, 2017, 2017 INTERNATIONAL CONFERENCE ON COMPUTER SYSTEMS, ELECTRONICS AND CONTROL (ICCSEC), P633, DOI 10.1109/ICCSEC.2017.8446713	36	33	33	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300052
C	Nouiehed, M; Sanjabi, M; Huang, TJ; Lee, JD; Razaviyayn, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Nouiehed, Maher; Sanjabi, Maziar; Huang, Tianjian; Lee, Jason D.; Razaviyayn, Meisam			Solving a Class of Non-Convex Min-Max Games Using Iterative First Order Methods	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recent applications that arise in machine learning have surged significant interest in solving min-max saddle point games. This problem has been extensively studied in the convex-concave regime for which a global equilibrium solution can be computed efficiently. In this paper, we study the problem in the non-convex regime and show that an E-first order stationary point of the game can be computed when one of the player's objective can be optimized to global optimality efficiently. In particular, we first consider the case where the objective of one of the players satisfies the Polyak-Lojasiewicz (PL) condition. For such a game, we show that a simple multi-step gradient descent-ascent algorithm finds an epsilon-first order stationary point of the problem in (O) over tilde(epsilon(-2)) iterations. Then we show that our framework can also be applied to the case where the objective of the "max-player" is concave. In this case, we propose a multi-step gradient descent-ascent algorithm that finds an E-first order stationary point of the game in (O) over tilde(epsilon(-3.5)) iterations, which is the best known rate in the literature. We applied our algorithm to a fair classification problem of Fashion-MNIST dataset and observed that the proposed algorithm results in smoother training and better generalization.	[Nouiehed, Maher; Huang, Tianjian; Razaviyayn, Meisam] Univ Southern Calif, Dept Ind & Syst Engn, Los Angeles, CA 90089 USA; [Sanjabi, Maziar] Univ Southern Calif, Data Sci & Operat Dept, Marshall Sch Business, Los Angeles, CA 90089 USA; [Lee, Jason D.] Princeton Univ, Dept Elect Engn, Princeton, NJ 08544 USA	University of Southern California; University of Southern California; Princeton University	Nouiehed, M (corresponding author), Univ Southern Calif, Dept Ind & Syst Engn, Los Angeles, CA 90089 USA.	nouiehed@usc.edu; sanjabi@usc.edu; tianjian@usc.edu; jasonlee@princeton.edu; razaviya@usc.edu		Lee, Jason/0000-0003-0064-7800					0	33	34	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906057
C	Salman, H; Yang, G; Zhang, H; Hsieh, CJ; Zhang, PC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Salman, Hadi; Yang, Greg; Zhang, Huan; Hsieh, Cho-Jui; Zhang, Pengchuan			A Convex Relaxation Barrier to Tight Robustness Verification of Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Verification of neural networks enables us to gauge their robustness against adversarial attacks. Verification algorithms fall into two categories: exact verifiers that run in exponential time and relaxed verifiers that are efficient but incomplete. In this paper, we unify all existing LP-relaxed verifiers, to the best of our knowledge, under a general convex relaxation framework. This framework works for neural networks with diverse architectures and nonlinearities and covers both primal and dual views of neural network verification. Next, we perform large-scale experiments, amounting to more than 22 CPU-years, to obtain exact solution to the convex-relaxed problem that is optimal within our framework for ReLU networks. We find the exact solution does not significantly improve upon the gap between PGD and existing relaxed verifiers for various networks trained normally or robustly on MNIST and CIFAR datasets. Our results suggest there is an inherent barrier to tight verification for the large class of methods captured by our framework. We discuss possible causes of this barrier and potential future directions for bypassing it.	[Salman, Hadi; Yang, Greg; Zhang, Pengchuan] Microsoft Res AI, Redmond, WA 98052 USA; [Zhang, Huan; Hsieh, Cho-Jui] Univ Calif Los Angeles, Los Angeles, CA 90024 USA; [Salman, Hadi] Microsoft AI Residency Program, Redmond, WA USA	University of California System; University of California Los Angeles	Salman, H (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	hadi.salman@microsoft.com; gregyang@microsoft.com; huan@huan-zhang.com; chohsieh@cs.ucla.edu; penzhan@microsoft.com	Zhang, Pengchuan/AAR-3769-2021					Adusumalli M, 2018, ROUT INT HANDB, P121; Anderson Ross, 2018, ARXIV181101988; Boyd S., 2004, CONVEX OPTIMIZATION, DOI [10.1017/CBO9780511804441, DOI 10.1017/CBO9780511804441.001, 10.1017/cbo97805118044 41]; Bunel R, 2018, ADV NEUR IN, V31; Carlini N., 2017, ARXIV170910207; Cohen J, 2019, PR MACH LEARN RES, V97; Diamond S, 2016, J MACH LEARN RES, V17; Domahidi Alexander, 2013, 2013 European Control Conference (ECC), P3071; Dvijotham K, 2018, ARXIV180510265; Dvijotham Krishnamurthy, 2018, DUAL APPROACH SCALAB; Ehlers R, 2017, LECT NOTES COMPUT SC, V10482, P269, DOI 10.1007/978-3-319-68167-2_19; EricWong Zico, 2018, INT C MACH LEARN, P5286; Fischetti Matteo, 2017, ARXIV171206174; Gehr T, 2018, P IEEE S SECUR PRIV, P3, DOI 10.1109/SP.2018.00058; Gowal Sven, 2018, EFFECTIVENESS INTERV; Hein M, 2017, NIPS 17; Jaisinghani D, 2018, MOBICOM'18: PROCEEDINGS OF THE 24TH ANNUAL INTERNATIONAL CONFERENCE ON MOBILE COMPUTING AND NETWORKING, P823, DOI 10.1145/3241539.3267717; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Lecuyer M., 2018, ARXIV180203471; Li B., 2018, ARXIV180903113, P1; Lomuscio A., 2017, ABS170607351; Madry Aleksander, 2017, ARXIV; Mirman M, 2018, PR MACH LEARN RES, V80; Qin Chongli, 2019, ICLR; Raghunathan A, 2018, ADV NEUR IN, V31; Raghunathan Aditi, 2018, ARXIV180109344; Rockafellar R. T., 1970, CONVEX ANAL; Salman Hadi, 2019, ADV NEURAL INFORM PR; Scheibler K., 2015, MBMV, P30; Singh G., 2019, INT C LEARNING REPRE; Singh G, 2019, P ACM PROGRAM LANG, V3, DOI 10.1145/3290354; Tjeng V., 2019, P INT C LEARN REPR I; Wang S., 2018, ARXIV181102625; Wang SQ, 2018, ADV NEUR IN, V31; Weng TW, 2018, PR MACH LEARN RES, V80; Wong E, 2018, ADV NEUR IN, V31; Xiao K. Y., 2019, INT C LEARN REPR; Zhang H, 2018, ADV NEUR IN, V31; Zhang Huan, 2019, AAAI C ART INT	40	33	34	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866901046
C	Plumb, G; Molitor, D; Talwalkar, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Plumb, Gregory; Molitor, Denali; Talwalkar, Ameet			Model Agnostic Supervised Local Explanations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Model interpretability is an increasingly important component of practical machine learning. Some of the most common forms of interpretability systems are example-based, local, and global explanations. One of the main challenges in interpretability is designing explanation systems that can capture aspects of each of these explanation types, in order to develop a more thorough understanding of the model. We address this challenge in a novel model called MAPLE that uses local linear modeling techniques along with a dual interpretation of random forests (both as a supervised neighborhood approach and as a feature selection method). MAPLE has two fundamental advantages over existing interpretability systems. First, while it is effective as a black-box explanation system, MAPLE itself is a highly accurate predictive model that provides faithful self explanations, and thus sidesteps the typical accuracy-interpretability trade-off. Specifically, we demonstrate, on several UCI datasets, that MAPLE is at least as accurate as random forests and that it produces more faithful local explanations than LIME, a popular interpretability system. Second, MAPLE provides both example-based and local explanations and can detect global patterns, which allows it to diagnose limitations in its local explanations.	[Plumb, Gregory; Talwalkar, Ameet] CMU, Pittsburgh, PA 15213 USA; [Molitor, Denali] Univ Calif Los Angeles, Los Angeles, CA 90024 USA	Carnegie Mellon University; University of California System; University of California Los Angeles	Plumb, G (corresponding author), CMU, Pittsburgh, PA 15213 USA.	gdplumb@andrew.cmu.edu; dmolitor@math.ucla.edu; talwalkar@cmu.edu			DARPA [FA875017C0141]; National Science Foundation [IIS1705121, IIS1838017]; Google Faculty Award; Amazon Web Services Award; Carnegie Bosch Institute Research Award; Okawa Grant	DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); National Science Foundation(National Science Foundation (NSF)); Google Faculty Award(Google Incorporated); Amazon Web Services Award; Carnegie Bosch Institute Research Award; Okawa Grant	This work was supported in part by DARPA FA875017C0141, the National Science Foundation grants IIS1705121 and IIS1838017, an Okawa Grant, a Google Faculty Award, an Amazon Web Services Award, and a Carnegie Bosch Institute Research Award. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of DARPA, the National Science Foundation, or any other funding agency.	Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Bien J, 2011, ANN APPL STAT, V5, P2403, DOI 10.1214/11-AOAS495; Bloniarz A, 2016, JMLR WORKSH CONF PRO, V51, P1450; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Chatterjee S., 1986, STAT SCI, V1, P379, DOI [10.1214/ss/1177013622, DOI 10.1214/SS/1177013622]; COOK RD, 1980, TECHNOMETRICS, V22, P495; Dheeru D., 2019, UCI MACHINE LEARNING; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Kaufman S, 2012, ACM T KNOWL DISCOV D, V6, DOI 10.1145/2382577.2382579; Kazemitabar S Jalil, 2017, P 31 INT C NEUR INF, P425; Kim B, 2017, ARXIV PREPRINT ARXIV; Koh PW, 2017, PR MACH LEARN RES, V70; Lin Y, 2006, J AM STAT ASSOC, V101, P578, DOI 10.1198/016214505000001230; Lipton Zachary C, 2016, INT C MACH LEARN WOR; Lundberg SM, 2017, ADV NEUR IN, V30; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Ribeiro Marco Tulio, 2018, AAAI; Scornet E, 2016, IEEE T INFORM THEORY, V62, P1485, DOI 10.1109/TIT.2016.2514489	19	33	34	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302052
C	Ridgeway, K; Mozer, MC		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ridgeway, Karl; Mozer, Michael C.			Learning Deep Disentangled Embeddings With the F-Statistic Loss	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep-embedding methods aim to discover representations of a domain that make explicit the domain's class structure and thereby support few-shot learning. Disentangling methods aim to make explicit compositional or factorial structure. We combine these two active but independent lines of research and propose a new paradigm suitable for both goals. We propose and evaluate a novel loss function based on the F statistic, which describes the separation of two or more distributions. By ensuring that distinct classes are well separated on a subset of embedding dimensions, we obtain embeddings that are useful for few-shot learning. By not requiring separation on all dimensions, we encourage the discovery of disentangled representations. Our embedding method matches or beats state-of-the-art, as evaluated by performance on recall@ k and few-shot learning tasks. Our method also obtains performance superior to a variety of alternatives on disentangling, as evaluated by two key properties of a disentangled representation: modularity and explicitness. The goal of our work is to obtain more interpretable, manipulable, and generalizable deep representations of concepts and categories.	[Ridgeway, Karl; Mozer, Michael C.] Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA; [Ridgeway, Karl] Sensory Inc, Boulder, CO 80302 USA	University of Colorado System; University of Colorado Boulder	Ridgeway, K (corresponding author), Univ Colorado, Dept Comp Sci, Boulder, CO 80309 USA.; Ridgeway, K (corresponding author), Sensory Inc, Boulder, CO 80302 USA.	karl.ridgeway@colorado.edu; mozer@colorado.edu			National Science Foundation [EHR-1631428, SES-1461535]	National Science Foundation(National Science Foundation (NSF))	This research was supported by the National Science Foundation awards EHR-1631428 and SES-1461535.	Chopra S, 2005, P IEEE C COMP VIS PA, P349, DOI DOI 10.1109/CVPR.2005.202; Eastwood C., 2018, ICLR; Higgins I., 2017, P INT C LEARN REPR T; Karaletsos Theofanis, 2015, INT UROL NEPHROL, P1; Kim Hyunjik, 2017, LEARNING DISENTANGLE; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kulkarni TD, 2015, ADV NEUR IN, V28; LeCun Y, 2004, PROC CVPR IEEE, P97; Li W, 2014, PROC CVPR IEEE, P152, DOI 10.1109/CVPR.2014.27; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Snell Jake, 2017, ADV NEURAL INFORM PR, V30, pxxx; Song HO, 2016, PROC CVPR IEEE, P4004, DOI 10.1109/CVPR.2016.434; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Ustinova E., 2016, ADV NEURAL INFORM PR, V29, P4170; Veit Andreas, 2016, DISENTANGLING NONLIN; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wah C., 2011, TECH REP; Yi D, 2014, INT C PATT RECOG, P34, DOI 10.1109/ICPR.2014.16; Zheng L, 2015, IEEE I CONF COMP VIS, P1116, DOI 10.1109/ICCV.2015.133	24	33	34	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300018
C	Ritter, H; Botev, A; Barber, D		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ritter, Hippolyt; Botev, Aleksandar; Barber, David			Online Structured Laplace Approximations for Overcoming Catastrophic Forgetting	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We introduce the Kronecker factored online Laplace approximation for overcoming catastrophic forgetting in neural networks. The method is grounded in a Bayesian online learning framework, where we recursively approximate the posterior after every task with a Gaussian, leading to a quadratic penalty on changes to the weights. The Laplace approximation requires calculating the Hessian around a mode, which is typically intractable for modern architectures. In order to make our method scalable, we leverage recent block-diagonal Kronecker factored approximations to the curvature. Our algorithm achieves over 90% test accuracy across a sequence of 50 instantiations of the permuted MNIST dataset, substantially outperforming related methods for overcoming catastrophic forgetting.	[Ritter, Hippolyt; Botev, Aleksandar; Barber, David] UCL, London, England; [Barber, David] Alan Turing Inst, London, England; [Barber, David] Reinfer Io, London, England	University of London; University College London	Ritter, H (corresponding author), UCL, London, England.	j.ritter@cs.ucl.ac.uk		Botev, Aleksandar/0000-0001-9021-1124	Alan Turing Institute under the EPSRC [EP/N510129/1]	Alan Turing Institute under the EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	We thank Raza Habib, Harshil Shah and the anonymous reviewers for their feedback. This work was supported by the Alan Turing Institute under the EPSRC grant EP/N510129/1.	[Anonymous], 1998, ON LINE LEARNING NEU; [Anonymous], 1964, COMP MATH MATH PHYS+; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Botev A., 2017, INT C MACHINE LEARNI, P557; Dieleman S., 2015, LASAGNE 1 RELEASE, V1; Eskin E., 2004, ADV NEURAL INFORM PR, P441; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; French RM, 1999, TRENDS COGN SCI, V3, P128, DOI 10.1016/S1364-6613(99)01294-2; Ghahramani Z., 2000, NIPS 2000 WORKSH ONL; Goodfellow I. J., 2013, ARXIV13126211, DOI DOI 10.1109/ISIE.2007; Grant Erin, 2018, INT C LEARN REPR; Grosse R, 2016, PR MACH LEARN RES, V48; Gupta A. K., 1999, MATRIX VARIATE DISTR, V104; He X, 2018, INT C LEARN REPR; Honkela A., 2003, 4 INT S IND COMP AN, P803; Huszar F., 2018, P NATL ACAD SCI; King DB, 2015, ACS SYM SER, V1214, P1; Kirkpatrick J., 2018, P NATL ACAD SCI; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LEE SW, 2017, ADV NEURAL INFORM PR, P4655; Lopez-Paz David, 2017, P 31 INT C NEUR INF, P6467; MACKAY DJC, 1992, NEURAL COMPUT, V4, P448, DOI 10.1162/neco.1992.4.3.448; Martens J, 2015, PR MACH LEARN RES, V37, P2408; Martens James, 2014, NEW INSIGHTS PERSPEC; Maybeck P. S., 1982, STOCHASTIC MODELS ES; Minka T.P., 2001, P 17 C UNC ART INT, P362; Miron M., 2018, ARXIV180101423; Nesterov Y., 1983, SOV MATH DOKL, V27, P372; Netzer Y., 2011, NIPS WORKSH DEEP LEA, V2, P5; Nguyen Cuong V, 2018, INT C LEARN REPR; Ritter H., 2018, INT C LEARN REPR; Rusu A. A., 2016, PROGR NEURAL NETWORK; Sagun Levent, 2017, ARXIV170604454; Shin H., 2017, ARXIV170508690; Theano Development Team, 2016, ARXIV E PRINTS; Xiao H., 2017, ARXIV 170807747; Zenke F, 2017, PR MACH LEARN RES, V70	41	33	33	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303071
C	Su, SP; Zhang, C; Han, K; Tian, YH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Su, Shupeng; Zhang, Chao; Han, Kai; Tian, Yonghong			Greedy Hash: Towards Fast Optimization for Accurate Hash Coding in CNN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				QUANTIZATION	To convert the input into binary code, hashing algorithm has been widely used for approximate nearest neighbor search on large-scale image sets due to its computation and storage efficiency. Deep hashing further improves the retrieval quality by combining the hash coding with deep neural network. However, a major difficulty in deep hashing lies in the discrete constraints imposed on the network output, which generally makes the optimization NP hard. In this work, we adopt the greedy principle to tackle this NP hard problem by iteratively updating the network toward the probable optimal discrete solution in each iteration. A hash coding layer is designed to implement our approach which strictly uses the sign function in forward propagation to maintain the discrete constraints, while in back propagation the gradients are transmitted intactly to the front layer to avoid the vanishing gradients. In addition to the theoretical derivation, we provide a new perspective to visualize and understand the effectiveness and efficiency of our algorithm. Experiments on benchmark datasets show that our scheme outperforms state-of-the-art hashing methods in both supervised and unsupervised tasks.	[Su, Shupeng; Zhang, Chao; Han, Kai; Tian, Yonghong] Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China; [Tian, Yonghong] Peking Univ, Sch EECS, Natl Engn Lab Video Technol, Beijing, Peoples R China; [Han, Kai] Huawei Noahs Ark Lab, Beijing, Peoples R China	Peking University; Peking University; Huawei Technologies	Zhang, C (corresponding author), Peking Univ, Sch EECS, Key Lab Machine Percept MOE, Beijing, Peoples R China.	sushupeng@pku.edu.cn; c.zhang@pku.edu.cn; hankai@pku.edu.cn; yhtian@pku.edu.cn	Han, Kai/AAC-7659-2019	Han, Kai/0000-0002-9761-2702	National Key R&D Program of China [2017YF-B1002400]; National Natural Science Foundation of China [61671027, U1611461]; National Key Basic Research Program of China [2015CB352303]	National Key R&D Program of China; National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); National Key Basic Research Program of China(National Basic Research Program of China)	This work was supported in part by the National Key R&D Program of China under Grant 2017YF-B1002400, the National Natural Science Foundation of China under Grant 61671027, U1611461 and the National Key Basic Research Program of China under Grant 2015CB352303.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2017, P IEEE C COMP VIS PA; Attouch H, 2013, MATH PROGRAM, V137, P91, DOI 10.1007/s10107-011-0484-9; Bengio Yoshua, 2013, ARXIV13083432; Bolte J., 2014, COMPUTER VISION IMAG, V146, P459; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Cao ZJ, 2017, IEEE I CONF COMP VIS, P5609, DOI 10.1109/ICCV.2017.598; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Gionis A, 1999, PROCEEDINGS OF THE TWENTY-FIFTH INTERNATIONAL CONFERENCE ON VERY LARGE DATA BASES, P518; Gong YC, 2013, IEEE T PATTERN ANAL, V35, P2916, DOI 10.1109/TPAMI.2012.193; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; He KM, 2013, PROC CVPR IEEE, P2938, DOI 10.1109/CVPR.2013.378; Heo JP, 2012, PROC CVPR IEEE, P2957, DOI 10.1109/CVPR.2012.6248024; Hu MQ, 2018, IEEE T IMAGE PROCESS, V27, P545, DOI 10.1109/TIP.2017.2749147; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A., 2014, CORR; Lai H., 2015 IEEE C COMP VIS; Li Q, 2017, ADV NEUR IN, V30; Li W., 2016, INT JOINT C ARTIFICI, P1711; Lin KV, 2016, PROC CVPR IEEE, P1183, DOI 10.1109/CVPR.2016.133; Lin K, 2015, IEEE COMPUT SOC CONF, DOI 10.1109/CVPRW.2015.7301269; Liu HM, 2016, PROC CVPR IEEE, P2064, DOI 10.1109/CVPR.2016.227; Liu W, 2012, PROC CVPR IEEE, P2074, DOI 10.1109/CVPR.2012.6247912; Masci J, 2014, IEEE T PATTERN ANAL, V36, P824, DOI 10.1109/TPAMI.2013.225; Raiko Tapani, 2014, ARXIV14062989; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Shen FM, 2016, IEEE T IMAGE PROCESS, V25, P5610, DOI 10.1109/TIP.2016.2612883; Shen FM, 2015, PROC CVPR IEEE, P37, DOI 10.1109/CVPR.2015.7298598; Wang X., 2016, P AS C COMP VIS, P70; Xia RK, 2014, AAAI CONF ARTIF INTE, P2156; Yang HF, 2015, ARXIV150700101; Zhang RM, 2015, IEEE T IMAGE PROCESS, V24, P4766, DOI 10.1109/TIP.2015.2467315; Zhang ZM, 2016, PROC CVPR IEEE, P1487, DOI 10.1109/CVPR.2016.165; Zhao F, 2015, PROC CVPR IEEE, P1556, DOI 10.1109/CVPR.2015.7298763; Zhou Y., 2017, ARXIV171011445; Zhu H, 2016, AAAI CONF ARTIF INTE, P2415	36	33	33	3	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300074
C	Tatbul, N; Lee, TJ; Zdonik, S; Alam, M; Gottschlich, J		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tatbul, Nesime; Lee, Tae Jun; Zdonik, Stan; Alam, Mejbah; Gottschlich, Justin			Precision and Recall for Time Series	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ANOMALY DETECTION TECHNIQUES	Classical anomaly detection is principally concerned with point-based anomalies, those anomalies that occur at a single point in time. Yet, many real-world anomalies are range-based, meaning they occur over a period of time. Motivated by this observation, we present a new mathematical model to evaluate the accuracy of time series classification algorithms. Our model expands the well-known Precision and Recall metrics to measure ranges, while simultaneously enabling customization support for domain-specific preferences.	[Tatbul, Nesime; Alam, Mejbah; Gottschlich, Justin] Intel Labs, Hillsboro, OR 97124 USA; [Tatbul, Nesime] MIT, Cambridge, MA 02139 USA; [Lee, Tae Jun] Microsoft, Redmond, WA USA; [Zdonik, Stan] Brown Univ, Providence, RI 02912 USA	Intel Corporation; Massachusetts Institute of Technology (MIT); Microsoft; Brown University	Tatbul, N (corresponding author), Intel Labs, Hillsboro, OR 97124 USA.; Tatbul, N (corresponding author), MIT, Cambridge, MA 02139 USA.	tatbul@csail.mit.edu; tae_jun_lee@alumni.brown.edu; sbz@cs.brown.edu; mejbah.alam@intel.com; justin.gottschlich@intel.com			Intel; NSF [IIS-1526639]	Intel(Intel Corporation); NSF(National Science Foundation (NSF))	UU We thank Eric Metcalf for his help with experiments. This research has been funded in part by Intel and by NSF grant IIS-1526639.	Aggarwal, 2015, OUTLIER ANAL, DOI [DOI 10.1007/978-1-4614-6396-2, 10.1007/978-1-4614-6396-2]; Ahmad S, 2017, NEUROCOMPUTING, V262, P134, DOI 10.1016/j.neucom.2017.04.070; AlEroud A, 2012, 2012 ASE INTERNATIONAL CONFERENCE ON CYBER SECURITY (CYBERSECURITY), P40, DOI 10.1109/CyberSecurity.2012.12; Aminikhanghahi S, 2017, KNOWL INF SYST, V51, P339, DOI 10.1007/s10115-016-0987-z; Anava O, 2015, PR MACH LEARN RES, V37, P2191; Baeza-Yates Ricardo, 2011, MODERN INFORM RETRIE; Bailis P, 2017, SIGMOD'17: PROCEEDINGS OF THE 2017 ACM INTERNATIONAL CONFERENCE ON MANAGEMENT OF DATA, P541, DOI 10.1145/3035918.3035928; Chandola V, 2008, IEEE DATA MINING, P743, DOI 10.1109/ICDM.2008.151; Chen G.H., 2013, ADV NEURAL INFORM PR, P1088; Cheng H., 2009, P 2009 SIAM INT C DA, V1, P413; Cook DJ, 2015, WILEY SER PARA DIST, P1, DOI 10.1002/9781119010258; Ding H, 2008, PROC VLDB ENDOW, V1, P1542; Formenti S., 2017, LANCET ONCOLOGY, P718; Gottschlich J., 2018, PARANOM PARALLEL ANO; Guha S, 2016, PR MACH LEARN RES, V48; Hermanussen M., 2013, P 26 INT C NEUR INF, P1; Hofmeyr S. A., 1998, Journal of Computer Security, V6, P151; Husken M, 2003, NEUROCOMPUTING, V50, P223, DOI 10.1016/S0925-2312(01)00706-8; Keogh E, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P226, DOI 10.1109/ICDM.2005.79; Keogh E., 2005, UCR TIME SERIES DATA; Kourou K, 2015, COMPUT STRUCT BIOTEC, V13, P8, DOI 10.1016/j.csbj.2014.11.005; Krishnamurthy B., 2003, P 3 ACM SIGCOMM C IN, P234, DOI DOI 10.1145/948205.948236; Laptev N, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1939, DOI 10.1145/2783258.2788611; Lavin A, 2015, 2015 IEEE 14TH INTERNATIONAL CONFERENCE ON MACHINE LEARNING AND APPLICATIONS (ICMLA), P38, DOI 10.1109/ICMLA.2015.141; Lee T. J., 2018, IN C SYST MACH LEARN; Li S. C., 2016, P 30 INT C NEUR INF, P1812; Lipton Z.C., 2015, ICLR, P1; Malhotra P., 2016, ICML AN DET WORKSH; Malhotra P, 2015, LONG SHORT TERM MEMO; Papadimitriou S, 2005, P 31 INT C VER LARG, P697; Patcha A, 2007, COMPUT NETW, V51, P3448, DOI 10.1016/j.comnet.2007.02.001; Prasad NR, 2009, CMC-COMPUT MATER CON, V14, P1, DOI 10.1145/1541880.1541882; Singh N, 2017, IEEE IJCNN, P1570, DOI 10.1109/IJCNN.2017.7966038; Tavallaee M, 2010, IEEE T SYST MAN CY C, V40, P516, DOI 10.1109/TSMCC.2010.2048428; Urmson C, 2008, J FIELD ROBOT, V25, P425, DOI 10.1002/rob.20255; Ward JA, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1889681.1889687; Yu Hsiang-Fu, 2016, ADV NEURAL INFORM PR, P847; Zhai SF, 2016, PR MACH LEARN RES, V48	38	33	34	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301087
C	Zhang, L; Zhu, GM; Mei, L; Shen, PY; Shah, SAA; Bennamoun, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Liang; Zhu, Guangming; Mei, Lin; Shen, Peiyi; Shah, Syed Afaq Ali; Bennamoun, Mohammed			Attention in Convolutional LSTM for Gesture Recognition	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Convolutional long short-term memory (LSTM) networks have been widely used for action/gesture recognition, and different attention mechanisms have also been embedded into the LSTM or the convolutional LSTM (ConvLSTM) networks. Based on the previous gesture recognition architectures which combine the three-dimensional convolution neural network (3DCNN) and ConvLSTM, this paper explores the effects of attention mechanism in ConvLSTM. Several variants of ConvLSTM are evaluated: (a) Removing the convolutional structures of the three gates in ConvLSTM, (b) Applying the attention mechanism on the input of ConvLSTM, (c) Reconstructing the input and (d) output gates respectively with the modified channel-wise attention mechanism. The evaluation results demonstrate that the spatial convolutions in the three gates scarcely contribute to the spatiotemporal feature fusion, and the attention mechanisms embedded into the input and output gates cannot improve the feature fusion. In other words, ConvLSTM mainly contributes to the temporal fusion along with the recurrent steps to learn the long-term spatiotemporal features, when taking as input the spatial or spatiotemporal features. On this basis, a new variant of LSTM is derived, in which the convolutional structures are only embedded into the input-to-state transition of LSTM. The code of the LSTM variants is publicly available(2).	[Zhang, Liang; Zhu, Guangming; Mei, Lin; Shen, Peiyi] Xidian Univ, Xian, Shaanxi, Peoples R China; [Shah, Syed Afaq Ali] Cent Queensland Univ, Rockhampton, Qld, Australia; [Bennamoun, Mohammed] Univ Western Australia, Nedlands, WA, Australia	Xidian University; Central Queensland University; University of Western Australia	Zhang, L (corresponding author), Xidian Univ, Xian, Shaanxi, Peoples R China.	liangzhang@xidian.edu.cn; gmzhu@xidian.edu.cn; l_mei72@hotmail.com; pyshen@xidian.edu.cn; afaq.shah@uwa.edu.au; mohammed.bennamoun@uwa.edu.au	Bennamoun, Mohammed/C-2789-2013	Bennamoun, Mohammed/0000-0002-6603-3257; Shah, Syed Afaq Ali/0000-0003-2181-8445	National Natural Science Foundation of China [61702390]; Fundamental Research Funds for the Central Universities [JB181001]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work is partially supported by the National Natural Science Foundation of China under Grant No.61702390, and the Fundamental Research Funds for the Central Universities under Grant JB181001.	Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Tran D, 2015, IEEE I CONF COMP VIS, P4489, DOI 10.1109/ICCV.2015.510; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Greff K, 2017, IEEE T NEUR NET LEAR, V28, P2222, DOI 10.1109/TNNLS.2016.2582924; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Howard A.G., 2017, MOBILENETS EFFICIENT; Escalante HJ, 2016, INT C PATT RECOG, P67, DOI 10.1109/ICPR.2016.7899609; Kaiser L., 2017, ARXIV PREPRINT ARXIV; Li YN, 2016, INT C PATT RECOG, P25, DOI 10.1109/ICPR.2016.7899602; Luo WX, 2017, IEEE INT CON MULTI, P439, DOI 10.1109/ICME.2017.8019325; Miao QG, 2017, IEEE INT CONF COMP V, P3047, DOI 10.1109/ICCVW.2017.360; Narayana  Pradyumna, 2018, 2018 IEEE C COMP VIS; Pigou L, 2018, INT J COMPUT VISION, V126, P430, DOI 10.1007/s11263-016-0957-7; Sudhakaran S, 2017, IEEE INT CONF COMP V, P2339, DOI 10.1109/ICCVW.2017.276; Tran Du, 2017, ARXIV170805038; Wan J, 2017, IEEE INT CONF COMP V, P3189, DOI 10.1109/ICCVW.2017.377; Wan J, 2016, IEEE COMPUT SOC CONF, P761, DOI 10.1109/CVPRW.2016.100; Wang HG, 2017, IEEE INT CONF COMP V, P3138, DOI 10.1109/ICCVW.2017.371; Wang L, 2018, IEEE ACCESS, V6, P17913, DOI 10.1109/ACCESS.2018.2817253; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang YB, 2017, ADV NEUR IN, V30; Xingjian S., 2015, ADV NEURAL INFORM PR, P802, DOI DOI 10.1007/978-3-319-21233-3_6; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Zhang L, 2017, IEEE INT CONF COMP V, P3120, DOI 10.1109/ICCVW.2017.369; Zhu GM, 2016, INT C PATT RECOG, P19, DOI 10.1109/ICPR.2016.7899601; Zhu GM, 2017, IEEE ACCESS, V5, P4517, DOI 10.1109/ACCESS.2017.2684186; 2017, TWENTYBN JESTER DATA	29	33	36	0	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823301090
C	Du, SS; Jin, C; Lee, JD; Jordan, MI; Poczos, B; Singh, A		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Du, Simon S.; Jin, Chi; Lee, Jason D.; Jordan, Michael, I; Poczos, Barnabas; Singh, Aarti			Gradient Descent Can Take Exponential Time to Escape Saddle Points	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Although gradient descent (GD) almost always escapes saddle points asymptotically [Lee et al., 2016], this paper shows that even with fairly natural random initialization schemes and non-pathological functions, GD can be significantly slowed down by saddle points, taking exponential time to escape. On the other hand, gradient descent with perturbations [Ge et al., 2015, Jin et al., 2017] is not slowed down by saddle points-it can find an approximate local minimizer in polynomial time. This result implies that GD is inherently slower than perturbed GD, and justifies the importance of adding perturbations for efficient non-convex optimization. While our focus is theoretical, we also present experiments that illustrate our theoretical findings.	[Du, Simon S.; Poczos, Barnabas; Singh, Aarti] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Jin, Chi; Jordan, Michael, I] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Lee, Jason D.] Univ Southern Calif, Los Angeles, CA USA	Carnegie Mellon University; University of California System; University of California Berkeley; University of Southern California	Du, SS (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	ssdu@cs.cmu.edu; chijin@berkeley.edu; jasonlee@marshall.usc.edu; jordan@cs.berkeley.edu; bapoczos@cs.cmu.edu; aartisingh@cmu.edu	Jordan, Michael I/C-5253-2013; Jeong, Yongwook/N-7413-2016	Lee, Jason/0000-0003-0064-7800; Jordan, Michael/0000-0001-8935-817X	NSF [IIS1563887]; ARPA-E Terra program; Mathematical Data Science program of the Office of Naval Research [N00014-15-1-2670]; ARO [W911NF-17-1-0304]; DARPA [D17AP00001]; AFRL [FA8750-17-2-0212]; CMU ProSEED/BrainHub Seed Grant	NSF(National Science Foundation (NSF)); ARPA-E Terra program; Mathematical Data Science program of the Office of Naval Research(Office of Naval Research); ARO; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); AFRL(United States Department of DefenseUS Air Force Research Laboratory); CMU ProSEED/BrainHub Seed Grant	S.S.D. and B.P. were supported by NSF grant IIS1563887 and ARPA-E Terra program. C.J. and M.I.J. were supported by the Mathematical Data Science program of the Office of Naval Research under grant number N00014-15-1-2670. J.D.L. was supported by ARO W911NF-17-1-0304. A.S. was supported by DARPA grant D17AP00001, AFRL grant FA8750-17-2-0212 and a CMU ProSEED/BrainHub Seed Grant. The authors thank Rong Ge, Qing Qu, John Wright, Elad Hazan, Sham Kakade, Benjamin Recht, Nathan Srebro, and Lin Xiao for useful discussions. The authors thank Stephen Wright and Michael O'Neill for pointing out calculation errors in the older version.	Agarwal N, 2017, STOC; [Anonymous], 2016, ARXIV161209296; [Anonymous], 2016, ADV NEURAL INFORM PR; Bhojanapalli S., 2016, ADV NEURAL INFORM PR, P3873; Candes EJ, 2015, IEEE T INFORM THEORY, V61, P1985, DOI 10.1109/TIT.2015.2399924; CARMON Y, 2016, ARXIV161200547; Carmon Yair, 2016, ARXIV161100756; Chang Alan, 2015, ARXIV150801779; Curtis FE, 2017, MATH PROGRAM, V162, P1, DOI 10.1007/s10107-016-1026-2; DOUGHERTY RL, 1989, MATH COMPUT, V52, P471, DOI 10.1090/S0025-5718-1989-0962209-1; Du Simon S., 2018, INT C LEARN REPR; Ge R., 2015, P C LEARNING THEORY, P797, DOI DOI 10.1109/ICMTMA.2015.197; Ge R, 2017, PR MACH LEARN RES, V70; Hardt M, 2014, ANN IEEE SYMP FOUND, P651, DOI 10.1109/FOCS.2014.75; Jain P, 2017, PR MACH LEARN RES, V54, P479; Jin C, 2017, PR MACH LEARN RES, V70; Lee J. D., 2016, C LEARN THEOR, P1246; Levy K. Y., 2016, ARXIV161104831; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nesterov Y, 2006, MATH PROGRAM, V108, P177, DOI 10.1007/s10107-006-0706-8; Netrapalli P., 2013, P ADV NEUR INF PROC, P2796; Palis J. J., 2012, GEOMETRIC THEORY DYN; PEMANTLE R, 1990, ANN PROBAB, V18, P698, DOI 10.1214/aop/1176990853; Sun J, 2017, IEEE T INFORM THEORY, V63, P853, DOI 10.1109/TIT.2016.2632162; Sun J, 2016, IEEE INT SYMP INFO, P2379, DOI 10.1109/ISIT.2016.7541725; Sun RY, 2016, IEEE T INFORM THEORY, V62, P6535, DOI 10.1109/TIT.2016.2598574; Whitney H, 1934, T AM MATH SOC, V36, P63, DOI 10.2307/1989708; Yi X., 2016, NIPS, P4152; Yin G George, 2003, STOCHASTIC APPROXIMA, V35; Zhang Xiao, 2017, ARXIV170100481	32	33	34	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401011
C	Ishida, T; Niu, G; Hu, WH; Sugiyama, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ishida, Takashi; Niu, Gang; Hu, Weihua; Sugiyama, Masashi			Learning from Complementary Labels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Collecting labeled data is costly and thus a critical bottleneck in real-world classification tasks. To mitigate this problem, we propose a novel setting, namely learning from complementary labels for multi-class classification. A complementary label specifies a class that a pattern does not belong to. Collecting complementary labels would be less laborious than collecting ordinary labels, since users do not have to carefully choose the correct class from a long list of candidate classes. However, complementary labels are less informative than ordinary labels and thus a suitable approach is needed to better learn from them. In this paper, we show that an unbiased estimator to the classification risk can be obtained only from complementarily labeled data, if a loss function satisfies a particular symmetric condition. We derive estimation error bounds for the proposed method and prove that the optimal parametric convergence rate is achieved. We further show that learning from complementary labels can be easily combined with learning from ordinary labels (i.e., ordinary supervised learning), providing a highly practical implementation of the proposed method. Finally, we experimentally demonstrate the usefulness of the proposed methods.	[Ishida, Takashi] Sumitomo Mitsui Asset Management, Tokyo, Japan; [Ishida, Takashi; Niu, Gang; Hu, Weihua; Sugiyama, Masashi] Univ Tokyo, Tokyo, Japan; [Ishida, Takashi; Niu, Gang; Hu, Weihua; Sugiyama, Masashi] RIKEN, Tokyo, Japan	University of Tokyo; RIKEN	Ishida, T (corresponding author), Sumitomo Mitsui Asset Management, Tokyo, Japan.; Ishida, T (corresponding author), Univ Tokyo, Tokyo, Japan.; Ishida, T (corresponding author), RIKEN, Tokyo, Japan.	ishida@ms.k.u-tokyo.ac.jp; gang@ms.k.u-tokyo.ac.jp; hu@ms.k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022; Jeong, Yongwook/N-7413-2016	Sugiyama, Masashi/0000-0001-6658-6743; 	JST CREST [JPMJCR1403]	JST CREST(Japan Science & Technology Agency (JST)Core Research for Evolutional Science and Technology (CREST))	GN and MS were supported by JST CREST JPMJCR1403. We thank Ikko Yamane for the helpful discussions.	[Anonymous], 2008, CROWDSOURCING WHY PO; [Anonymous], NIPS; Belkin M, 2006, J MACH LEARN RES, V7, P2399; Blanchard G, 2010, J MACH LEARN RES, V11, P2973; Boutell MR, 2004, PATTERN RECOGN, V37, P1757, DOI 10.1016/j.patcog.2004.03.009; Chapelle O., 2006, IEEE T NEURAL NETW, V20, P542; Cour T, 2011, J MACH LEARN RES, V12, P1501; Denis F., 1998, ALT; Dhillon I. S., 2007, ICML; du Plessis M. C., 2015, ICML; Dwork C, 2008, TAMC; ELKAN C, 2008, KDD; Goldberger J., 2004, NIPS; Grandvalet Y, 2004, ADV NEURAL INFORM PR, V17; Kingma D.P., 2015, INT C LEARN REPR, P1; Kipf Thomas N., 2017, INT C LEARNING REPRE; Kiryo R., 2017, NIPS; Laine Samuli, 2017, PROC INT C LEARN REP; Ledoux M., 1991, PROBABILITY BANACH S, DOI [10.1007/978-3-642-20212-4, DOI 10.1007/978-3-642-20212-4]; MANN G, 2007, ICML; McDiarmid C., 1989, SURVEYS COMBINATORIC, V141, P148, DOI DOI 10.1017/CBO9781107359949.008; Mohri M., 2018, FDN MACHINE LEARNING; Nair V., 2010, ICML; Niu  G., 2016, NIPS; Niu G., 2013, ICML; Niu G, 2014, NEURAL COMPUT, V26, P1717, DOI 10.1162/NECO_a_00614; Sakai T., 2017, ICML; Scholkopf B., 2001, LEARNING KERNELS; Tokui S., 2015, P WORKSH MACH LEARN; Vapnik V.N, 1998, STAT LEARNING THEORY; Ward G, 2009, BIOMETRICS, V65, P554, DOI 10.1111/j.1541-0420.2008.01116.x; Weinberger KQ, 2009, J MACH LEARN RES, V10, P207; Xing EP, 2002, ADV NEURAL INFORM PR; Yang Z., 2016, ICML; Zhang T, 2004, J MACH LEARN RES, V5, P1225; Zhou D., 2003, NIPS; Zhu X., 2003, ICML	38	33	33	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405070
C	Li, YZ; Song, JM; Ermon, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Li, Yunzhu; Song, Jiaming; Ermon, Stefano			InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.	[Li, Yunzhu] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Song, Jiaming; Ermon, Stefano] Stanford Univ, Stanford, CA 94305 USA	Massachusetts Institute of Technology (MIT); Stanford University	Li, YZ (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	liyunzhu@mit.edu; tsong@cs.stanford.edu; ermon@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Toyota Research Institute (TRI); Intel Corporation; NSF [1651565, 1522054, 1733686]; FLI	Toyota Research Institute (TRI); Intel Corporation(Intel Corporation); NSF(National Science Foundation (NSF)); FLI	We thank Shengjia Zhao and Neal Jean for their assistance and advice. Toyota Research Institute (TRI) provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. This research was also supported by Intel Corporation, FLI and NSF grants 1651565, 1522054, 1733686.	Abbeel P., 2004, P 21 INT C MACHINE L, P1; Aly M, 2008, IEEE INT VEH SYM, P165, DOI 10.1109/ivs.2008.4621152; Arjovsky M., 2017, ARXIV170107875; Bloem M, 2014, IEEE DECIS CONTR P, P4911, DOI 10.1109/CDC.2014.7040156; Bojarski Mariusz, 2016, arXiv; Chen CY, 2015, IEEE I CONF COMP VIS, P2722, DOI [10.1109/ICCV.2015.104, 10.1109/ICCV.2015.312]; Englert P, 2015, P INT S ROB RES; Ermon S, 2015, AAAI CONF ARTIF INTE, P644; Finn C, 2016, PR MACH LEARN RES, V48; Geiger A, 2013, INT J ROBOT RES, V32, P1231, DOI 10.1177/0278364913491297; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Ho J, 2016, PR MACH LEARN RES, V48; Ho Jonathan, 2016, ADV NEURAL INFORM PR; Kingma D.P, P 3 INT C LEARNING R; Kitani KM, 2012, LECT NOTES COMPUT SC, V7575, P201, DOI 10.1007/978-3-642-33765-9_15; Kuefler A., 2017, ARXIV170106699; Lenz P, 2011, IEEE INT VEH SYM, P926, DOI 10.1109/IVS.2011.5940558; Levine Sergey, 2013, ICML; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pomerleau D.A., 1989, ALVINN AUTONOMOUS LA; Pomerleau DA, 1991, NEURAL COMPUT, V3, P88, DOI 10.1162/neco.1991.3.1.88; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Ross S., 2010, AISTATS, P3; Ross St<prime>ephane, 2011, AISTATS; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salimans T, 2016, ADV NEUR IN, V29; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2015, ARXIV150602438; Sermanet Pierre, 2016, ARXIV161206699; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Stadie B.C., 2017, ICLR; Syed U., 2008, P 25 INT C MACH LEAR, P1032, DOI [DOI 10.1145/1390156.1390286, 10.1145/1390156.1390286]; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Jiakai, 2016, ARXIV160506450; Ziebart B. D., 2008, AAAI, V8, P1433	41	33	33	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403085
C	Avron, H; Nguyen, HL; Woodruff, DP		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Avron, Haim; Nguyen, Huy L.; Woodruff, David P.			Subspace Embeddings for the Polynomial Kernel	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Sketching is a powerful dimensionality reduction tool for accelerating statistical learning algorithms. However, its applicability has been limited to a certain extent since the crucial ingredient, the so-called oblivious subspace embedding, can only be applied to data spaces with an explicit representation as the column span or row span of a matrix, while in many settings learning is done in a high-dimensional space implicitly defined by the data matrix via a kernel transformation. We propose the first fast oblivious subspace embeddings that are able to embed a space induced by a non-linear kernel without explicitly mapping the data to the high-dimensional space. In particular, we propose an embedding for mappings induced by the polynomial kernel. Using the subspace embeddings, we obtain the fastest known algorithms for computing an implicit low rank approximation of the higher-dimension mapping of the data matrix, and for computing an approximate kernel PCA of the data, as well as doing approximate kernel principal component regression.	[Avron, Haim] IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA; [Nguyen, Huy L.] Univ Calif Berkeley, Simons Inst, Berkeley, CA 94720 USA; [Woodruff, David P.] IBM Almaden Res Ctr, San Jose, CA 95120 USA	International Business Machines (IBM); University of California System; University of California Berkeley; International Business Machines (IBM)	Avron, H (corresponding author), IBM TJ Watson Res Ctr, Yorktown Hts, NY 10598 USA.	haimav@us.ibm.com; hlnguyen@cs.princeton.edu; dpwoodru@us.ibm.com	Nguyen, Huy/GWQ-6433-2022; Nguyen, Huy/AFN-7027-2022					Avron H., 2013, ADV NEURAL INFORM PR; CARTER JL, 1979, J COMPUT SYST SCI, V18, P143, DOI 10.1016/0022-0000(79)90044-8; Charikar M, 2004, THEOR COMPUT SCI, V312, P3, DOI 10.1016/S0304-3975(03)00400-6; CLARKSON K, 2009, P 41 ANN ACM S THEOR; Clarkson KL, 2013, STOC'13: PROCEEDINGS OF THE 2013 ACM SYMPOSIUM ON THEORY OF COMPUTING, P81; Drineas P, 2008, SIAM J MATRIX ANAL A, V30, P844, DOI 10.1137/07070471X; Hamid R., 2014, P 31 INT C MACH LEAR; JOLLIFFE IT, 1982, APPL STAT-J ROY ST C, V31, P300, DOI 10.2307/2348005; Kannan R., 2014, P 27 C LEARN THEOR C; Kar P., 2012, ARTIF INTELL, P583; Le Q. V., 2013, P 30 INT C MACH LEAR; Mahoney MW, 2011, FOUND TRENDS MACH LE, V3, P123, DOI 10.1561/2200000035; Nelson J., 2013, 54 IEEE ANN S FDN CO; Pham N, 2013, 19TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'13), P239, DOI 10.1145/2487575.2487591; Pagh R, 2013, ACM T COMPUT THEORY, V5, DOI 10.1145/2493252.2493254; Patrascu M, 2012, J ACM, V59, DOI 10.1145/2220357.2220361; Rahimi A., 2007, ADV NEURAL INFORM PR, P3	17	33	33	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647100019
C	Gentile, C; Warmuth, MK		Kearns, MS; Solla, SA; Cohn, DA		Gentile, C; Warmuth, MK			Linear hinge loss and average margin	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				BOUNDS	We describe a unifying method for proving relative loss bounds for online linear threshold classification algorithms, such as the Perceptron and the Winnow algorithms. For classification problems the discrete loss is used, i.e., the total number of prediction mistakes. We introduce a continuous loss function, called the "linear hinge loss", that can be employed to derive the updates of the algorithms. We first prove bounds w.r.t. the linear hinge loss and then convert them to the discrete loss. We introduce a notion of "average margin" of a set of examples. We show how relative loss bounds based on the linear hinge loss can be converted to relative loss bounds i.t.o. the discrete loss using the average margin.	Univ Milan, DSI, I-20135 Milan, Italy	University of Milan	Gentile, C (corresponding author), Univ Milan, DSI, Via Comelico 39, I-20135 Milan, Italy.	gentile@dsi.unimi.it; manfred@cse.ucsc.edu						AZOURY K, 1998, UNPUB RELATIVE LOSS; Bregman L. M., 1967, COMP MATH MATH PHYS+, V7, P200, DOI DOI 10.1016/0041-5553(67)90040-7; Freund Y., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P209, DOI 10.1145/279943.279985; GROVE AJ, 1997, 10 COLT, P171; HELMBOLD DP, 1995, NIPS 1995, P309; Kivinen J, 1997, ARTIF INTELL, V97, P325, DOI 10.1016/S0004-3702(97)00039-8; Kivinen J, 1997, INFORM COMPUT, V132, P1, DOI 10.1006/inco.1996.2612; Kivinen J, 1998, ADV NEUR IN, V10, P287; Littlestone N., 1988, Machine Learning, V2, P285, DOI 10.1023/A:1022869011914; LITTLESTONE N, 1991, 4 COLT, P147; Littlestone N., 1989, THESIS U CALIFORNIA	11	33	33	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						225	231						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700032
C	Tipping, ME		Kearns, MS; Solla, SA; Cohn, DA		Tipping, ME			Probabilistic visualisation of high-dimensional binary data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS; MODEL	We present a probabilistic latent-variable framework for data visualisation, a key feature of which is its applicability to binary and categorical data types for which few established methods exist. A variational approximation to the likelihood is exploited to derive a fast algorithm for determining the model parameters. Illustrations of application to real and synthetic binary data sets art given.	Microsoft Res, Cambridge CB2 3NH, England	Microsoft	Tipping, ME (corresponding author), Microsoft Res, St George House,1 Guildhall St, Cambridge CB2 3NH, England.							Bartholomew D. J., 1987, LATENT VARIABLE MODE; Bishop CM, 1998, NEURAL COMPUT, V10, P215, DOI 10.1162/089976698300017953; Bishop CM, 1998, IEEE T PATTERN ANAL, V20, P281, DOI 10.1109/34.667885; JAAKKOLA TS, 1997, P 1997 C ART INT STA; Lowe D, 1997, ADV NEUR IN, V9, P543; MACKAY DJC, 1995, NUCL INSTRUM METH A, V354, P73, DOI 10.1016/0168-9002(94)00931-7; Moustaki I, 1996, BRIT J MATH STAT PSY, V49, P313, DOI 10.1111/j.2044-8317.1996.tb01091.x; Sammel MD, 1997, J ROY STAT SOC B MET, V59, P667, DOI 10.1111/1467-9868.00090	8	33	34	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						592	598						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700084
C	Bengio, Y; Gingras, F		Touretzky, DS; Mozer, MC; Hasselmo, ME		Bengio, Y; Gingras, F			Recurrent neural networks for missing or asynchronous data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV MONTREAL,DEPT INFORMAT & RECH OPERAT,MONTREAL,PQ H3C 3J7,CANADA	Universite de Montreal									0	33	34	0	3	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						395	401						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00056
C	Balazevic, I; Allen, C; Hospedales, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Balazevic, Ivana; Allen, Carl; Hospedales, Timothy			Multi-relational Poincare Graph Embeddings	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Hyperbolic embeddings have recently gained attention in machine learning due to their ability to represent hierarchical data more accurately and succinctly than their Euclidean analogues. However, multi-relational knowledge graphs often exhibit multiple simultaneous hierarchies, which current hyperbolic models do not capture. To address this, we propose a model that embeds multi-relational graph data in the Poincare ball model of hyperbolic space. Our Multi-Relational Poincare model (MuRP) learns relation-specific parameters to transform entity embeddings by Mbbius matrix-vector multiplication and Mbbius addition. Experiments on the hierarchical WN 18RR knowledge graph show that our Poincare embeddings outperform their Euclidean counterpart and existing embedding methods on the link prediction task, particularly at lower dimensionality.	[Balazevic, Ivana; Allen, Carl; Hospedales, Timothy] Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland; [Hospedales, Timothy] Samsung AI Ctr, Cambridge, England	University of Edinburgh	Balazevic, I (corresponding author), Univ Edinburgh, Sch Informat, Edinburgh, Midlothian, Scotland.	ivana.balazevic@ed.ac.uk; carl.allen@ed.ac.uk; t.hospedales@ed.ac.uk			Centre for Doctoral Training in Data Science; EPSRC [EP/L016427/1]; University of Edinburgh	Centre for Doctoral Training in Data Science; EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); University of Edinburgh	We thank Rik Sarkar, Ivan Titov, Jonathan Mallinson, Eryk Kopczynski and the anonymous reviewers for helpful comments. Ivana Balazevic and Carl Allen were supported by the Centre for Doctoral Training in Data Science, funded by EPSRC (grant EP/L016427/1) and the University of Edinburgh.	Allen Carl, 2019, INT C MACH LEARN; [Anonymous], 2018, ADV NEURAL INFORM PR; Balazevic Ivana, 2019, EMPIRICAL METHODS NA; Becigneul Gary, 2019, INT C LEARNING REPRE; Bollacker Kurt, 2008, ACM SIGMOD INT C MAN; Bonnabel S., 2013, IEEE T AUTOMATIC CON; Cannon J.W., 1997, FLAVORS GEOMETRY, P59; Carlson Andrew, 2010, AAAI C ART INT; De Sa Christopher, 2018, INT C MACH LEARN; Dettmers Tim, 2018, AAAI C ART INT; Dhuliawala Shehzaad, 2018, INT C LEARN REPR; Feng Jun, 2016, PRINCIPLES KNOWLEDGE; Ganea Octavian, 2018, ADV NEURAL INFORM PR; Ganea Octavian-Eugen, 2018, INT C MACH LEARN; Gu A., 2019, ICLR; Gulcehre Caglar, 2019, INT C LEARN REPR; Krackhardt David, 1994, COMPUTATIONAL ORG TH; Lacroix Timothee, 2018, INT C MACH LEARN; Levy Omer, 2014, COMPUTATIONAL NATURA; Mikolov T., 2013, N AM CHAPTER ASS COM; Mikolov T., 2013, 27 ANN C NEUR INF PR, P3111; Miller George A, 1995, COMMUNICATIONS ACM; Nguyen Dat Quoc, 2016, N AM CHAPTER ASS COM; Nickel Maximilian, 2011, ICML; Nickel Maximillian, 2017, ADV NEURAL INFORM PR; Nickel Maximillian, 2018, INT C MACH LEARN; Pennington Jeffrey, 2014, P 2014 EMNLP; Sarkar Rik, 2011, INT S GRAPH DRAW; Sun Z., 2019, 7 INT C LEARNING REP; Suzuki A., 2019, RIEMANNIAN TRANSE MU; Tifrea Alexandru, 2019, INT C LEARN REPR; Toutanova Kristina, 2015, P 2015 EMNLP; Trouillon Theo, 2016, INT C MACH LEARN; Ungar AA, 2001, COMPUT MATH APPL, V41, P135, DOI 10.1016/S0898-1221(01)85012-4; Xiong Wenhan, 2017, EMPIRICAL METHODS NA; Yakhnenko O., 2013, INT C ADV NEURAL INF; Yang B., 2015, P INT C LEARN REPR I; Yang Fan, 2017, ADV NEURAL INFORM PR	38	32	34	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304046
C	Dehmamy, N; Barabasi, AL; Yu, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Dehmamy, Nima; Barabasi, Albert-Laszlo; Yu, Rose			Understanding the Representation Power of Graph Neural Networks in Learning Graph Topology	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION	To deepen our understanding of graph neural networks, we investigate the representation power of Graph Convolutional Networks (GCN) through the looking glass of graph moments, a key property of graph topology encoding path of various lengths. We find that GCNs are rather restrictive in learning graph moments. Without careful design, GCNs can fail miserably even with multiple layers and nonlinear activation functions. We analyze theoretically the expressiveness of GCNs, concluding that a modular GCN design, using different propagation rules with residual connections could significantly improve the performance of GCN. We demonstrate that such modular designs are capable of distinguishing graphs from different graph generation models for surprisingly small graphs, a notoriously difficult problem in network science. Our investigation suggests that, depth is much more influential than width, with deeper GCNs being more capable of learning higher order graph moments. Additionally, combining GCN modules with different propagation rules is critical to the representation power of GCNs.	[Dehmamy, Nima] Northwestern Univ, Kellogg Sch Management, CSSI, Evanston, IL 60208 USA; [Dehmamy, Nima; Barabasi, Albert-Laszlo] Northeastern Univ, Ctr Complex Network Res, Boston, MA 02115 USA; [Yu, Rose] Northeastern Univ, Khoury Coll Comp Sci, Boston, MA 02115 USA; [Barabasi, Albert-Laszlo] Dana Farber Canc Inst, Ctr Canc Syst Biol, Boston, MA 02115 USA; [Barabasi, Albert-Laszlo] Harvard Med Sch, Brigham & Womens Hosp, Boston, MA 02115 USA; [Barabasi, Albert-Laszlo] Cent European Univ, Ctr Network Sci, Budapest, Hungary	Northwestern University; Northeastern University; Northeastern University; Harvard University; Dana-Farber Cancer Institute; Harvard University; Brigham & Women's Hospital; Harvard Medical School; Central European University	Dehmamy, N (corresponding author), Northwestern Univ, Kellogg Sch Management, CSSI, Evanston, IL 60208 USA.	nimadt@bu.edu; alb@neu.edu; roseyu@northeastern.edu		Dehmamy, Nima/0000-0003-1617-5502	NSF [185034]; ONR-OTA [N00014-18-9-0001]	NSF(National Science Foundation (NSF)); ONR-OTA	This work was supported in part by NSF #185034, ONR-OTA (N00014-18-9-0001).	Albert R, 2002, REV MOD PHYS, V74, P47, DOI 10.1103/RevModPhys.74.47; [Anonymous], 2018, ARXIV181006111; BARRON AR, 1994, MACH LEARN, V14, P115, DOI 10.1023/A:1022650905902; Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Battaglia Peter W, 2018, ARXIV180601261; Bondy JA, 2008, GTM 244; Bonner S, 2016, 2016 IEEE INTERNATIONAL CONFERENCE ON BIG DATA (BIG DATA), P3290, DOI 10.1109/BigData.2016.7840988; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Bruna Joan, 2014, ICLR, DOI DOI 10.1145/3170427.3188467; Canning James P, 2018, ARXIV180502682; Du SS, 2019, ADV NEUR IN, V32; Eldan R., 2016, P 29 C LEARNING THEO, V49, P907; Gilmer J, 2017, PR MACH LEARN RES, V70; Goyal P, 2018, KNOWL-BASED SYST, V151, P78, DOI 10.1016/j.knosys.2018.03.022; Guo T, 2013, PROCEEDINGS OF THE 22ND ACM INTERNATIONAL CONFERENCE ON INFORMATION & KNOWLEDGE MANAGEMENT (CIKM'13), P817; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hamilton WL, 2017, REPRESENTATION LEARN; He K., 2016, PROC IEEE C COMPUTER, P770, DOI DOI 10.1109/CVPR.2016.90; HORNIK K, 1991, NEURAL NETWORKS, V4, P251, DOI 10.1016/0893-6080(91)90009-T; Kipf TN, 2016, P INT C LEARN REPR; Li Y., 2018, ARXIV PREPRINT ARXIV; LIN YL, 1995, SIAM J DISCRETE MATH, V8, P99, DOI 10.1137/S089548019120016X; Morris C, 2019, AAAI CONF ARTIF INTE, P4602; Newman M., 2010, NETWORKS INTRO, DOI [DOI 10.1093/ACPROF:OSO/9780199206650.001.0001, 10.1162/artl_r_00062., 10.1162/artl_r_00062]; Raghu M, 2017, PR MACH LEARN RES, V70; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Telgarsky M.., 2016, C LEARNING THEORY, P1517; Velickovic P., 2017, ARXIV MACHINE LEARNI, P1; Verma Saurabh, 2019, KDD '19: Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, P1539, DOI 10.1145/3292500.3330956; Wu ZH, 2021, IEEE T NEUR NET LEAR, V32, P4, DOI 10.1109/TNNLS.2020.2978386; Xu K., 2018, INT C LEARN REPR; Xu KYL, 2018, PR MACH LEARN RES, V80; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; You JX, 2018, ADV NEUR IN, V31; Zhang Z., 2018, ARXIV181204202	38	32	32	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866907011
C	Fan, QF; Chen, CF; Kuehne, H; Pistoia, M; Cox, D		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Fan, Quanfu; Chen, Chun-Fu (Richard); Kuehne, Hilde; Pistoia, Marco; Cox, David			More Is Less: Learning Efficient Video Representations by Big-Little Network and Depthwise Temporal Aggregation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Current state-of-the-art models for video action recognition are mostly based on expensive 3D ConvNets. This results in a need for large GPU clusters to train and evaluate such architectures. To address this problem, we present an lightweight and memory-friendly architecture for action recognition that performs on par with or better than current architectures by using only a fraction of resources. The proposed architecture is based on a combination of a deep subnet operating on low-resolution frames with a compact subnet operating on high-resolution frames, allowing for high efficiency and accuracy at the same time. We demonstrate that our approach achieves a reduction by 3 similar to 4 times in FLOPs and similar to 2 times in memory usage compared to the baseline. This enables training deeper models with more input frames under the same computational budget. To further obviate the need for large-scale 3D convolutions, a temporal aggregation module is proposed to model temporal dependencies in a video at very small additional computational costs. Our models achieve strong performance on several action recognition benchmarks including Kinetics, Something-Something and Moments-in-time. The code and models are available at https://github.com/IBM/bLVNet-TAM.	[Fan, Quanfu; Kuehne, Hilde; Cox, David] MIT, IBM Waston AI Lab, Cambridge, MA 02142 USA; [Chen, Chun-Fu (Richard); Pistoia, Marco] IBM TJ Waston Res Ctr, Yorktown Hts, NY 10598 USA	Massachusetts Institute of Technology (MIT)	Fan, QF (corresponding author), MIT, IBM Waston AI Lab, Cambridge, MA 02142 USA.	qfan@us.ibm.com; chenrich@us.ibm.com; kuehne@ibm.com; pistoia@us.ibm.com; david.d.cox@ibm.com	Kuehne, Hilde/AAW-6860-2021; Kuehne, Hilde/ABG-5472-2020	Kuehne, Hilde/0000-0003-1079-4441; 				Aggarwal JK, 2011, ACM COMPUT SURV, V43, DOI 10.1145/1922649.1922653; Cao ZC, 2018, IEEE INT CON AUTO SC, P803, DOI 10.1109/COASE.2018.8560578; Carreira J, 2017, PROC CVPR IEEE, P4724, DOI 10.1109/CVPR.2017.502; Chen C, 2019, INT C LEARN REPR; Courtney PG, 2015, IEEE COMP SEMICON; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Feichtenhofer C, 2019, IEEE I CONF COMP VIS, P6201, DOI 10.1109/ICCV.2019.00630; Ghanem Bernard, 2015, CVPR; Goyal R, 2017, IEEE I CONF COMP VIS, P5843, DOI 10.1109/ICCV.2017.622; Hara K, 2017, IEEE INT CONF COMP V, P3154, DOI 10.1109/ICCVW.2017.373; Hussein N, 2019, PROC CVPR IEEE, P254, DOI 10.1109/CVPR.2019.00034; Jiang Y.-G., 2014, THUMOS CHALLENGE ACT; Kay W., 2017, ARXIV PREPRINT ARXIV; Kuehne H., 2011, P INT C COMP VIS, DOI DOI 10.1109/ICCV.2011.6126543; Lin Ji, 2018, ARXIV181108383; Monfort Mathew, 2019, IEEE T PATTERN ANAL; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Soomro K., 2012, ARXIV; Tran D, 2018, PROC CVPR IEEE, P6450, DOI 10.1109/CVPR.2018.00675; Wang LM, 2016, LECT NOTES COMPUT SC, V9912, P20, DOI 10.1007/978-3-319-46484-8_2; Wang LM, 2018, PROC CVPR IEEE, P1430, DOI 10.1109/CVPR.2018.00155; Wang XL, 2018, PROC CVPR IEEE, P7794, DOI 10.1109/CVPR.2018.00813; Wang XP, 2018, IDEAS HIST MOD CHINA, V19, P1, DOI 10.1163/9789004385580_002; Xie SN, 2018, LECT NOTES COMPUT SC, V11219, P318, DOI 10.1007/978-3-030-01267-0_19; Zolfaghari M, 2018, LECT NOTES COMPUT SC, V11206, P713, DOI 10.1007/978-3-030-01216-8_43	26	32	32	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424302028
C	Roelofs, R; Fridovich-Keil, S; Miller, J; Shankar, V; Hardt, M; Recht, B; Schmidt, L		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Roelofs, Rebecca; Fridovich-Keil, Sara; Miller, John; Shankar, Vaishaal; Hardt, Moritz; Recht, Benjamin; Schmidt, Ludwig			A Meta-Analysis of Overfitting in Machine Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We conduct the first large meta-analysis of overfitting due to test set reuse in the machine learning community. Our analysis is based on over one hundred machine learning competitions hosted on the Kaggle platform over the course of several years. In each competition, numerous practitioners repeatedly evaluated their progress against a holdout set that forms the basis of a public ranking available throughout the competition. Performance on a separate test set used only once determined the final ranking. By systematically comparing the public ranking with the final ranking, we assess how much participants adapted to the holdout set over the course of a competition. Our study shows, somewhat surprisingly, little evidence of substantial overfitting. These findings speak to the robustness of the holdout method across different data domains, loss functions, model classes, and human analysts.	[Roelofs, Rebecca; Fridovich-Keil, Sara; Miller, John; Shankar, Vaishaal; Hardt, Moritz; Recht, Benjamin; Schmidt, Ludwig] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Roelofs, R (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	roelofs@berkeley.edu; sfk@berkeley.edu; miller_john@berkeley.edu; vaishaal@berkeley.edu; hardt@berkeley.edu; brecht@berkeley.edu; ludwig@berkeley.edu		Fridovich-Keil, Sara/0000-0002-7661-4987	ONR [N00014-17-1-2191, N00014-17-1-2401, N00014-18-1-2833]; DARPA [FA8750-18-C-0101, W911NF-16-1-0552]; Siemens Futuremakers Fellowship; Amazon AWS AI Research Award	ONR(Office of Naval Research); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Siemens Futuremakers Fellowship; Amazon AWS AI Research Award	This research was generously supported in part by ONR awards N00014-17-1-2191, N00014-17-1-2401, and N00014-18-1-2833, the DARPA Assured Autonomy (FA8750-18-C-0101) and Lagrange (W911NF-16-1-0552) programs, a Siemens Futuremakers Fellowship, and an Amazon AWS AI Research Award.	Blum Avrim, 2015, INT C MACH LEARN ICM; Chen T., 2016, INT C KNOWL DISC DAT; Deng J., 2009, PROC CVPR IEEE, P248, DOI DOI 10.1109/CVPR.2009.5206848; Dwork C., 2015, ACM S THEOR COMP STO; Engstrom L., 2017, NIPS 2017 WORKSH MAC; Feldman V, 2019, PR MACH LEARN RES, V97; Fukushima K., 1980, BIOL CYBERNETICS; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR; Mania H., 2019, ADV NEURAL INFORM PR; Murphy KP, 2012, MACHINE LEARNING: A PROBABILISTIC PERSPECTIVE, P1; Pedregosa F., 2011, J MACH LEARN RES; RECHT B, 2019, ICML; Roth A., 2017, LECT NOTES ALGORITHM; Russakovsky O, 2015, IMAGENET LARGE SCALE, V115, P211; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Torralba A., 2011, C COMP VIS PATT REC; Trotman J., 2019, META KAGGLE COMPETIT; Yadav D., 2019, EVALAI BETTER EVALUA; Zrnic T, 2019, PR MACH LEARN RES, V97	21	32	32	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866900073
C	Xu, JJ; Sun, X; Zhang, ZY; Zhao, GX; Lin, JY		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xu, Jingjing; Sun, Xu; Zhang, Zhiyuan; Zhao, Guangxiang; Lin, Junyang			Understanding and Improving Layer Normalization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Layer normalization (LayerNorm) is a technique to normalize the distributions of intermediate layers. It enables smoother gradients, faster training, and better generalization accuracy. However, it is still unclear where the effectiveness stems from. In this paper, our main contribution is to take a step further in understanding LayerNorm. Many of previous studies believe that the success of LayerNorm comes from forward normalization. Unlike them, we find that the derivatives of the mean and variance are more important than forward normalization by re-centering and re-scaling backward gradients. Furthermore, we find that the parameters of LayerNorm, including the bias and gain, increase the risk of over-fitting and do not work in most cases. Experiments show that a simple version of LayerNorm (LayerNorm-simple) without the bias and gain outperforms LayerNorm on four datasets. It obtains the state-of-the-art performance on En-Vi machine translation. To address the over-fitting problem, we propose a new normalization method, Adaptive Normalization (AdaNorm), by replacing the bias and gain with a new transformation function. Experiments show that AdaNorm demonstrates better results than LayerNorm on seven out of eight datasets.	[Xu, Jingjing; Sun, Xu; Zhang, Zhiyuan; Lin, Junyang] Peking Univ, MOE Key Lab Computat Linguist, Sch EECS, Beijing, Peoples R China; [Sun, Xu; Zhao, Guangxiang] Peking Univ, Ctr Data Sci, Beijing, Peoples R China	Peking University; Peking University	Sun, X (corresponding author), Peking Univ, MOE Key Lab Computat Linguist, Sch EECS, Beijing, Peoples R China.; Sun, X (corresponding author), Peking Univ, Ctr Data Sci, Beijing, Peoples R China.	jingjingxu@pku.edu.cn; xusun@pku.edu.cn; zzy1210@pku.edu.cn; zhaoguangxiang@pku.edu.cn; linjunyang@pku.edu.cn	xu, jing/GRR-8698-2022		National Natural Science Foundation of China [61673028]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC))	We thank all reviewers for providing the thoughtful and constructive suggestions. This work was supported in part by National Natural Science Foundation of China (No. 61673028).	Al-Rfou R., 2018, ABS180804444 CORR; Ba L.J, 2016, P C WORKSH NEUR INF; Bjorck N., 2018, ADV NEURAL INFORM PR; Cettolo  M., 2014, IWSLT 2014, V2014; Cettolo M., 2015, IWSLT 2015; Chen D., 2014, P 2014 C EMPIRICAL M, P740, DOI DOI 10.3115/V1/D14-1082; Cho K., 2015, CORR; Dai Z., 2019, ARXIV190102860; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Klambauer Gnter, 2017, SELF NORMALIZING NEU; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Marcus M.P., 1993, COMPUT LINGUIST, V19, P313, DOI DOI 10.21236/ADA273556; Ott Myle, 2019, NAACL, P48; Pang B., 2005, P 43 ANN M ASS COMP, V43, P115, DOI DOI 10.3115/1219840.1219855; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ranzato M., 2016, P INT C LEARN REPR, P1; Santurkar S., 2018, P ADV NEURAL INFORM, P2483; Socher R., 2013, EMNLP, P1631, DOI DOI 10.1371/JOURNAL.PONE.0073791; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Ulyanov D., 2016, ABS160708022 CORR; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wiseman S., 2016, P 2016 C EMP METH NA, P1296; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Zeng HQ, 2017, PROC INT CONF RECON; Zhang H., 2019, ABS190109321 CORR	27	32	33	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304039
C	Zhang, KQ; Yang, ZR; Basar, T		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Kaiqing; Yang, Zhuoran; Basar, Tamer			Policy Optimization Provably Converges to Nash Equilibria in Zero-Sum Linear Quadratic Games	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We study the global convergence of policy optimization for finding the Nash equilibria (NE) in zero-sum linear quadratic (LQ) games. To this end, we first investigate the landscape of LQ games, viewing it as a nonconvex-nonconcave saddle-point problem in the policy space. Specifically, we show that despite its nonconvexity and nonconcavity, zero-sum LQ games have the property that the stationary point of the objective function with respect to the linear feedback control policies constitutes the NE of the game. Building upon this, we develop three projected nested-gradient methods that are guaranteed to converge to the NE of the game. Moreover, we show that all these algorithms enjoy both globally sublinear and locally linear convergence rates. Simulation results are also provided to illustrate the satisfactory convergence properties of the algorithms. To the best of our knowledge, this work appears to be the first one to investigate the optimization landscape of LQ games, and provably show the convergence of policy optimization methods to the NE. Our work serves as an initial step toward understanding the theoretical aspects of policy-based reinforcement learning algorithms for zero-sum Markov games in general.	[Zhang, Kaiqing; Basar, Tamer] Univ Illinois, ECE & CSL, Champaign, IL 61820 USA; [Yang, Zhuoran] Princeton Univ, ORFE, Princeton, NJ 08544 USA	University of Illinois System; University of Illinois Urbana-Champaign; Princeton University	Zhang, KQ (corresponding author), Univ Illinois, ECE & CSL, Champaign, IL 61820 USA.	kzhang66@illinois.edu; zy6@princeton.edu; basar1@illinois.edu			US Army Research Laboratory (ARL) [W911NF-17-2-0196]; Office of Naval Research (ONR) MURI [N00014-16-1-2710]; Tencent PhD Fellowship	US Army Research Laboratory (ARL)(United States Department of DefenseUS Army Research Laboratory (ARL)); Office of Naval Research (ONR) MURI(MURIOffice of Naval Research); Tencent PhD Fellowship	K. Zhang and T. Basar were supported in part by the US Army Research Laboratory (ARL) Cooperative Agreement W911NF-17-2-0196, and in part by the Office of Naval Research (ONR) MURI Grant N00014-16-1-2710. Z. Yang was supported by Tencent PhD Fellowship.	Adolphs Leonard, 2019, LOCAL SADDLE POINT O; Al-Tamimi A, 2007, AUTOMATICA, V43, P473, DOI 10.1016/j.automatica.2006.09.019; [Anonymous], 1972, LINEAR OPTIMAL CONTR; [Anonymous], 2018, ADV NEURAL INFORM PR; [Anonymous], 2016, ARXIV PREPRINT ARXIV; Balduzzi D, 2018, PR MACH LEARN RES, V80; Banerjee B., 2003, P 2 INT JOINT C AUTO, P686; Basar T., 2008, OPTIMAL CONTROL RELA; Bertsekas D. P., 2005, DYNAMIC PROGRAMMING, V1; Bowling M., 2001, INT JOINT C ARTIFICI, V17, P1021; Cartis Coralia, 2017, ARXIV170907180; Conitzer V, 2007, MACH LEARN, V67, P23, DOI 10.1007/s10994-006-0143-1; Daskalakis Constantinos, 2018, ADV NEURAL INFORM PR, P9236; Graham A, 2018, KRONECKER PRODUCTS M; Grnarova P., 2017, ARXIV170603269; Hernandez-Leal P., 2017, ARXIV170709183; Heusel M., 2017, 31 C NEUR INF PROC S, P6626; JACOBSON DH, 1977, IEEE T AUTOMAT CONTR, V22, P490, DOI 10.1109/TAC.1977.1101515; JACOBSON DH, 1973, IEEE T AUTOMAT CONTR, VAC18, P124, DOI 10.1109/TAC.1973.1100265; Jin C., 2019, ARXIV190200618; Kakade S, 2002, ADV NEUR IN, V14, P1531; Khamaru Koulik, 2018, ARXIV180409629; Konda VR, 2000, ADV NEUR IN, V12, P1008; KONSTANTINOV MM, 1993, KYBERNETIKA, V29, P18; Krantz S.G., 2012, IMPLICIT FUNCTION TH; Lagoudakis M. G., 2002, P C UNC ART INT ALB, V15, P1659; Lin Q., 2018, ARXIV PREPRINT ARXIV; Lin Q., 2018, ARXIV PREPRINT ARXIV; Littman ML, 1994, ICML 1994, P157; Lu S., 2018, P 7 INT C LEARN REPR; MAGNUS JR, 1985, J MATH PSYCHOL, V29, P474, DOI 10.1016/0022-2496(85)90006-9; Malik D., 2018, ARXIV181208305; Mazumdar E., 2018, ARXIV180405464; Mazumdar Eric V, 2019, ARXIV190100838; Mertikopoulos P., 2019, INT C LEARN REPR; Mnih V., 2016, INT C MACH LEARN, DOI DOI 10.5555/3045390.3045594; MURTY KG, 1987, MATH PROGRAM, V39, P117, DOI 10.1007/BF02592948; Nagarajan Vaishnavh, 2017, ADV NEURAL INFORM PR, P5585; Nesterov Y., 2013, INTRO LECT CONVEX OP, V87; Nouiehed Maher, 2019, ARXIV190208297; Papini Matteo, 2018, ARXIV180605618; Perolat Julien, 2018, P 21 INT C ART INT S, V84, P919; Pinto L, 2017, PR MACH LEARN RES, V70; Piot Bilal, 2016, C ART INT STAT; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Singer Yaron, 2017, ADV NEURAL INFORM PR, P4705; STOORVOGEL AA, 1994, IEEE T AUTOMAT CONTR, V39, P686, DOI 10.1109/9.280789; Sun JG, 1998, SIAM J MATRIX ANAL A, V19, P39, DOI 10.1137/S0895479895291303; Sutton Richard S, 2018, NATURE; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Tu S, 2019, C LEARN THEOR COLT, P3036; Tyrtyshnikov Eugene E, 2012, BRIEF INTRO NUMERICA; Vinyals Oriol, 2019, ALPHAS TAR MASTERING; WHITTLE P, 1981, ADV APPL PROBAB, V13, P764, DOI 10.2307/1426972; Zhang K., 2019, ARXIV191009496; Zhang K., 2019, ARXIV190608383; Zhang K., 2018, ARXIV181202783; Zou S., 2019, ARXIV190202234CSSTAT	63	32	32	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903026
C	Bartunov, S; Santoro, A; Richards, BA; Marris, L; Hinton, GE; Lillicrap, TP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Bartunov, Sergey; Santoro, Adam; Richards, Blake A.; Marris, Luke; Hinton, Geoffrey E.; Lillicrap, Timothy P.			Assessing the Scalability of Biologically-Motivated Deep Learning Algorithms and Architectures	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				BACKPROPAGATION	The backpropagation of error algorithm (BP) is impossible to implement in a real brain. The recent success of deep networks in machine learning and AI, however, has inspired proposals for understanding how the brain might learn across multiple layers, and hence how it might approximate BP. As of yet, none of these proposals have been rigorously evaluated on tasks where BP-guided deep learning has proved critical, or in architectures more structured than simple fully-connected networks. Here we present results on scaling up biologically motivated models of deep learning on datasets which need deep networks with appropriate architectures to achieve good performance. We present results on the MNIST, CIFAR-10, and ImageNet datasets, explore variants of target-propagation (TP) and feedback alignment (FA) algorithms, and examine performance in both fully- and locally-connected architectures. We also introduce weight-transport-free variants of difference target propagation (DTP) modified to remove backpropagation from the penultimate layer. Many of these algorithms perform well for MNIST, but for CIFAR and ImageNet we find that TP and FA variants perform significantly worse than BP, especially for networks composed of locally connected units, opening questions about whether new architectures and algorithms are required to scale these approaches. Our results and implementation details help establish baselines for biologically motivated deep learning schemes going forward.	[Bartunov, Sergey; Santoro, Adam; Marris, Luke] DeepMind, London, England; [Richards, Blake A.] Univ Toronto, Toronto, ON, Canada; [Hinton, Geoffrey E.] Google Brain, Mountain View, CA USA; [Lillicrap, Timothy P.] UCL, DeepMind, London, England	University of Toronto; Google Incorporated; University of London; University College London	Bartunov, S (corresponding author), DeepMind, London, England.							ACKLEY DH, 1985, COGNITIVE SCI, V9, P147; Almeida L. B., 1990, ARTIFICIAL NEURAL NE, P102; Ba J., 2017, P 3 INT C LEARN REPR; Bengio, 2014, ARXIV14077906; Bengio Y., 2015, ARXIV151002777; Bengio Y, 2017, NEURAL COMPUTATION; Bengio Y., 2015, STDP BIOLOGICALLY PL; Bengio Yoshua, 2016, ARXIV160601651; CRICK F, 1989, NATURE, V337, P129, DOI 10.1038/337129a0; Dumoulin V., 2016, GUIDE CONVOLUTION AR; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; GROSSBERG S, 1987, COGNITIVE SCI, V11, P23, DOI 10.1111/j.1551-6708.1987.tb00862.x; Hinton G, 2007, NIPS 2007 DEEP LEARN, V656; Hinton G.E., 1988, NEURAL INFORM PROCES, P358; Kording KP, 2001, J COMPUT NEUROSCI, V11, P207, DOI 10.1023/A:1013776130161; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun, 1986, DISORDERED SYSTEMS B, DOI DOI 10.1007/978-3-642-82657-3_24; LeCun Y., 1987, THESIS U P M CURIE P; Lee DH, 2015, LECT NOTES ARTIF INT, V9284, P498, DOI 10.1007/978-3-319-23528-8_31; Lillicrap T. P., 2014, ARXIV14110247; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Movellan J., 1991, CONNECTIONIST MODELS, P10, DOI 10.1016/B978-1-4832-1448-1.50007-X; Nokland, 2016, ADV NEURAL INFORM PR, V29, P1037; O'Reilly Randall C, 1996, NEURAL COMPUT, V8, P895; Ororbia A. G., 2018, ARXIV180301834; Ororbia II A. G., 2018, ARXIV180511703; Parisien C, 2008, NEURAL COMPUT, V20, P1473, DOI 10.1162/neco.2008.07-06-295; Pineda F. J., 1988, Journal of Complexity, V4, P216, DOI 10.1016/0885-064X(88)90021-0; PINEDA FJ, 1987, PHYS REV LETT, V59, P2229, DOI 10.1103/PhysRevLett.59.2229; Roland B, 2016, ELIFE, V5, DOI 10.7554/eLife.16335; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sacramento Joao, 2017, ARXIV180100062; Samadi Arash, 2017, NEURAL COMPUTATION; Scellier B, 2017, FRONT COMPUT NEUROSC, V11, DOI 10.3389/fncom.2017.00024; Springenberg J.T., 2014, ARXIV14126806; Whittington James CR, 2017, NEURAL COMPUTATION; Xie XH, 2003, NEURAL COMPUT, V15, P441, DOI 10.1162/089976603762552988	39	32	31	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003088
C	Liu, WY; Lin, RM; Liu, Z; Liu, LX; Yu, ZD; Dai, B; Song, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Liu, Weiyang; Lin, Rongmei; Liu, Zhen; Liu, Lixin; Yu, Zhiding; Dai, Bo; Song, Le			Learning towards Minimum Hyperspherical Energy	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Neural networks are a powerful class of nonlinear functions that can be trained end-to-end on various applications. While the over-parametrization nature in many neural networks renders the ability to fit complex functions and the strong representation power to handle challenging tasks, it also leads to highly correlated neurons that can hurt the generalization ability and incur unnecessary computation cost. As a result, how to regularize the network to avoid undesired representation redundancy becomes an important issue. To this end, we draw inspiration from a well-known problem in physics - Thomson problem, where one seeks to find a state that distributes N electrons on a unit sphere as evenly as possible with minimum potential energy. In light of this intuition, we reduce the redundancy regularization problem to generic energy minimization, and propose a minimum hyperspherical energy (MHE) objective as generic regularization for neural networks. We also propose a few novel variants of MHE, and provide some insights from a theoretical point of view. Finally, we apply neural networks with MHE regularization to several challenging tasks. Extensive experiments demonstrate the effectiveness of our intuition, by showing the superior performance with MHE regularization.	[Liu, Weiyang; Liu, Zhen; Dai, Bo] Georgia Inst Technol, Atlanta, GA 30332 USA; [Lin, Rongmei] Emory Univ, Atlanta, GA 30322 USA; [Liu, Lixin] South China Univ Technol, Guangzhou, Guangdong, Peoples R China; [Yu, Zhiding] NVIDIA, Santa Clara, CA USA; [Dai, Bo] Google Brain, Mountain View, CA USA; [Song, Le] Ant Financial, Hangzhou, Zhejiang, Peoples R China	University System of Georgia; Georgia Institute of Technology; Emory University; South China University of Technology; Nvidia Corporation; Google Incorporated	Liu, WY (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	wyliu@gatech.edu			NSF [IIS-1218749, IIS-1639792 EAGER, IIS-1841351 EAGER, CCF-1836822, CNS-1704701]; NIH BIGDATA [1R01GM108341]; NSF CAREER [IIS-1350983]; ONR [N00014-15-1-2340]; Intel ISTC; NVIDIA; Amazon AWS; Siemens	NSF(National Science Foundation (NSF)); NIH BIGDATA(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); NSF CAREER(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR(Office of Naval Research); Intel ISTC; NVIDIA; Amazon AWS; Siemens(Siemens AG)	This project was supported in part by NSF IIS-1218749, NIH BIGDATA 1R01GM108341, NSF CAREER IIS-1350983, NSF IIS-1639792 EAGER, NSF IIS-1841351 EAGER, NSF CCF-1836822, NSF CNS-1704701, ONR N00014-15-1-2340, Intel ISTC, NVIDIA, Amazon AWS and Siemens. We would like to thank NVIDIA corporation for donating Titan Xp GPUs to support our research. We also thank Tuo Zhao for the valuable discussions and suggestions.	Aghasi Alireza, 2017, NIPS; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Batra, 2016, ICLR, P1; Bilyk D, 2017, SB MATH+, V208, P744, DOI 10.1070/SM8656; Brock Andrew, 2017, ICLR; Deng Jiankang, 2018, ARXIV180107698; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gotz M, 2001, INT SER NUMER MATH, V137, P159; Gulrajani I, 2017, P NIPS 2017; Han S., 2016, P 4 INT C LEARN REPR, P1; Hardin D.P., 2004, NOTICES AMS, V51, P1186; Hardin DP, 2003, MATHPH0311024 ARXIV; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton, 2016, ARXIV PREPRINT ARXIV; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Huang GB, 2007, 07 UMASS TR; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Kemelmacher-Shlizerman Ira, 2016, CVPR; Kuijlaars ABJ, 1998, T AM MATH SOC, V350, P523, DOI 10.1090/S0002-9947-98-02119-9; Kuncheva LI, 2003, MACH LEARN, V51, P181, DOI 10.1023/A:1022859003006; Landkof N, 1972, FDN MODERN POTENTIAL, V180; LeCun Y., 2016, ICLR, DOI DOI 10.1109/WCNC.2016.7564824; Li Nan, 2012, JOINT EUR C MACH LEA; Liu W., 2017, P IEEE C COMPUTER VI, P212; Liu WY, 2018, PROC CVPR IEEE, P2771, DOI 10.1109/CVPR.2018.00293; Liu WY, 2016, PR MACH LEARN RES, V48; Liu Weiyang, 2017, NIPS; Liu Y., 2017, ARXIV171000870; MAIRAL J., 2009, P 26 ANN INT C MACH, P689, DOI [10.1145/1553374.1553463, DOI 10.1145/1553374.1553463]; Meng Deyu, 2014, NIPS; Ramirez I, 2010, PROC CVPR IEEE, P3501, DOI 10.1109/CVPR.2010.5539964; Rodriguez Pau, 2017, ICLR; Saff EB, 1997, MATH INTELL, V19, P5, DOI 10.1007/BF03024331; Salimans T, 2016, ADV NEUR IN, V29; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Shang WL, 2016, PR MACH LEARN RES, V48; Sharma Prakhar, 2017, NIPS WORKSH DEEP LEA; Smale S, 1998, MATH INTELL, V20, P7, DOI 10.1007/BF03025291; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Sun Y, 2014, PROC CVPR IEEE, P1891, DOI 10.1109/CVPR.2014.244; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Tammes PML, 1930, RECL TRAV BOT NEERL, V27, P1; Thomson JJ, 1904, PHILOS MAG, V7, P237, DOI 10.1080/14786440409463107; Wang Feng, 2017, ACM MULTIMEDIA; Wang H, 2018, PROC CVPR IEEE, P5265, DOI 10.1109/CVPR.2018.00552; Warde-Farley D., 2017, INT C LEARN REPR; Wen YD, 2016, LECT NOTES COMPUT SC, V9911, P499, DOI 10.1007/978-3-319-46478-7_31; Xie B., 2016, ARXIV161103131; Xie Di, 2017, CVPR; Xie PT, 2017, PR MACH LEARN RES, V70; Xie PT, 2018, PR MACH LEARN RES, V80; Xie P, 2016, PR MACH LEARN RES, V48; Yi D., 2014, LEARNING FACE REPRES, V1411, P7923; Zhang X, 2018, PROC CVPR IEEE, P6848, DOI 10.1109/CVPR.2018.00716; Zhang Y, 2016, IEEE T IMAGE PROCESS, V25, DOI 10.1109/TIP.2016.2549360	60	32	32	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000070
C	Lunz, S; Oktem, O; Schonlieb, CB		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lunz, Sebastian; Oktem, Ozan; Schonlieb, Carola-Bibiane			Adversarial Regularizers in Inverse Problems	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Inverse Problems in medical imaging and computer vision are traditionally solved using purely model-based methods. Among those variational regularization models are one of the most popular approaches. We propose a new framework for applying data-driven approaches to inverse problems, using a neural network as a regularization functional. The network learns to discriminate between the distribution of ground truth images and the distribution of unregularized reconstructions. Once trained, the network is applied to the inverse problem by solving the corresponding variational problem. Unlike other data-based approaches for inverse problems, the algorithm can be applied even if only unsupervised training data is available. Experiments demonstrate the potential of the framework for denoising on the BSDS dataset and for computed tomography reconstruction on the LIDC dataset.	[Lunz, Sebastian; Schonlieb, Carola-Bibiane] Univ Cambridge, DAMTP, Cambridge CB3 0WA, England; [Oktem, Ozan] KTH Royal Inst Technol, Dept Math, S-10044 Stockholm, Sweden	University of Cambridge; Royal Institute of Technology	Lunz, S (corresponding author), Univ Cambridge, DAMTP, Cambridge CB3 0WA, England.	lunz@math.cam.ac.uk; ozan@kth.se; cbs31@cam.ac.uk			EPSRC [EP/L016516/1]; Cantab Capital Institute for the Mathematics of Information; Swedish Foundation for Strategic Research grant [AM13-0049]; Leverhulme Trust project on 'Breaking the non-convexity barrier', EPSRC grant [EP/M00483X/1]; EPSRC Centre [EP/N014588/1]; RISE project CHiPS; RISE project NoMADS; Alan Turing Institute	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Cantab Capital Institute for the Mathematics of Information; Swedish Foundation for Strategic Research grant; Leverhulme Trust project on 'Breaking the non-convexity barrier', EPSRC grant(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); EPSRC Centre(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); RISE project CHiPS; RISE project NoMADS; Alan Turing Institute	The authors acknowledge the National Cancer Institute and the Foundation for the National Institutes of Health, and their critical role in the creation of the free publicly available LIDC/IDRI Database used in this study. The work by Sebastian Lunz was supported by the EPSRC grant EP/L016516/1 for the University of Cambridge Centre for Doctoral Training, the Cambridge Centre for Analysis and by the Cantab Capital Institute for the Mathematics of Information. The work by Ozan Oktem was supported by the Swedish Foundation for Strategic Research grant AM13-0049. Carola-Bibiane Schonlieb acknowledges support from the Leverhulme Trust project on 'Breaking the non-convexity barrier', EPSRC grant Nr. EP/M00483X/1, the EPSRC Centre Nr. EP/N014588/1, the RISE projects CHiPS and NoMADS, the Cantab Capital Institute for the Mathematics of Information and the Alan Turing Institute.	Adler J, 2018, IEEE T MED IMAGING, V37, P1322, DOI 10.1109/TMI.2018.2799231; Adler J, 2017, INVERSE PROBL, V33, DOI 10.1088/1361-6420/aa9581; Arbelaez P., IEEE T PATTERN ANAL, V33; Argyrou M, 2012, IEEE NUCL SCI CONF R, P3324; Arjovsky M., 2017, INT C MACH LEARN ICM; Armato S. G., 2011, MED PHYS, V38; Benning Martin, 2017, INT C SCAL SPAC VAR; Bora A., 2017, ARXIV170303208; Calatroni Luca, 2012, RADON BOOK SERIES, V8; Chambolle Antonin, 2016, ACTA NUMERICA, V25; Engl HW, 1996, REGULARIZATION INVER, V375; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gulrajani I, 2017, P NIPS, P1; Hammernik Kerstin, 2018, MAGNETIC RESONANCE M, V79; Jin KH, 2017, IEEE T IMAGE PROCESS, V26, P4509, DOI 10.1109/TIP.2017.2713099; Knoll Florian, 2011, MAGNETIC RESONANCE M, V65; Leary R, 2013, ULTRAMICROSCOPY, V131, P70, DOI 10.1016/j.ultramic.2013.03.019; Meinhardt Tim, 2017, INT C COMP VIS ICCV; Milgrom P, 2002, ECONOMETRICA, V70, P583, DOI 10.1111/1468-0262.00296; Romano Y, 2017, SIAM J IMAGING SCI, V10, P1804, DOI 10.1137/16M1102884; Ronneberger O., 2015, P INT C MED IMAG COM, P234, DOI [DOI 10.1007/978-3-319-24574-4_28, DOI 10.48550/ARXIV.1505.04597]; RUDIN LI, 1992, PHYSICA D, V60, P259, DOI 10.1016/0167-2789(92)90242-F; Scherzer O, 2009, APPL MATH SCI, V167, P3; Schlemper Jo, 2017, INT C INF PROC MED I; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang Z, 2004, IEEE T IMAGE PROCESS, V13, P600, DOI 10.1109/TIP.2003.819861; Xie J., 2012, ADV NEURAL INF PROCE	28	32	32	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003010
C	Wong, C; Houlsby, N; Lu, YF; Gesmundo, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wong, Catherine; Houlsby, Neil; Lu, Yifeng; Gesmundo, Andrea			Transfer Learning with Neural AutoML	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We reduce the computational cost of Neural AutoML with transfer learning. AutoML relieves human effort by automating the design of ML algorithms. Neural AutoML has become popular for the design of deep learning architectures, however, this method has a high computation cost. To address this we propose Transfer Neural AutoML that uses knowledge from prior tasks to speed up network design. We extend RL-based architecture search methods to support parallel training on multiple tasks and then transfer the search strategy to new tasks. On language and image classification tasks, Transfer Neural AutoML reduces convergence time over single-task training by over an order of magnitude on many tasks.	[Wong, Catherine] MIT, Cambridge, MA 02139 USA; [Houlsby, Neil; Lu, Yifeng; Gesmundo, Andrea] Google Brain, Mountain View, CA USA	Massachusetts Institute of Technology (MIT); Google Incorporated	Wong, C (corresponding author), MIT, Cambridge, MA 02139 USA.	catwong@mit.edu; neilhoulsby@google.com; yifenglu@google.com; agesmundo@google.com						Almeida Tiago, 2013, INT J INFORM SECURIT; Baker Bowen, 2017, ICLR; Bardenet Remi, 2013, INT C MACHINE LEARNI, P199; Barnes Jeremy, 2017, P 8 WORKSH COMP APPR, DOI [10.18653/v1/W17-5202, DOI 10.18653/V1/W17-5202]; Bello I, 2017, PR MACH LEARN RES, V70; Bergstra J., 2013, JMLR WORKSHOP C P IC, V28, P115, DOI [10.5555/3042817.3042832, DOI 10.5555/3042817.3042832]; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bergstra James S, 2011, ADV NEURAL INFORM PR, P2546, DOI [10.5555/2986459.2986743, DOI 10.5555/2986459.2986743]; Cai H., 2017, ARXIV170704873; Cheng H., 2016, DLRS 2016PROCEEDINGS, P7, DOI [10.1145/2988450.2988454, DOI 10.1145/2988450.2988454]; Conti E., 2017, ARXIV171206560; Feurer Matthias, 2015, AAAI; Finn C, 2017, PR MACH LEARN RES, V70; Greensmith E, 2004, J MACH LEARN RES, V5, P1471; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Le, 2017, ARXIV PREPRINT ARXIV; Le Quoc, 2014, ICML ICML 14; Li Bofang, 2016, COLING; Liu C., 2017, ARXIV171200559; Liu Hanxiao, 2018, ARXIV180609055; Maas Andrew L., 2011, ACL HUMAN LANGUAGE T; Miikkulainen Risto, 2017, ABS170300548 CORR; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Mishra Nikhil, 2017, NIPS 2017 WORKSH MET; Nachum Ofir, 2017, ICLR; Negrinho R., 2017, ARXIV170408792; Pham H, 2018, PR MACH LEARN RES, V80; Razavian Ali Sharif, 2014, P IEEE C COMP VIS PA, P806, DOI DOI 10.1109/CVPRW.2014.131; Real E, 2017, PR MACH LEARN RES, V70; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Such F. P., 2017, ARXIV171206567; Teh YW, 2017, ADV NEUR IN, V30; Wichrowska O, 2017, PR MACH LEARN RES, V70; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yogatama D, 2014, JMLR WORKSH CONF PRO, V33, P1077; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhan Y., 2015, ARXIV150700436; Zhong ZX, 2018, AAAI CONF ARTIF INTE, P5714; Zoph B., 2017, P1; Zoph B, 2018, PROC CVPR IEEE, P8697, DOI 10.1109/CVPR.2018.00907	43	32	33	1	8	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002086
C	Xu, J; Zhu, ZX		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Xu, Ju; Zhu, Zhanxing			Reinforced Continual Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Most artificial intelligence models are limited in their ability to solve new tasks faster, without forgetting previously acquired knowledge. The recently emerging paradigm of continual learning aims to solve this issue, in which the model learns various tasks in a sequential fashion. In this work, a novel approach for continual learning is proposed, which searches for the best neural architecture for each coming task via sophisticatedly designed reinforcement learning strategies. We name it as Reinforced Continual Learning. Our method not only has good performance on preventing catastrophic forgetting but also fits new tasks well. The experiments on sequential classification tasks for variants of MNIST and CIFAR-100 datasets demonstrate that the proposed approach outperforms existing continual learning alternatives for deep networks.	[Xu, Ju; Zhu, Zhanxing] Peking Univ, Ctr Data Sci, Beijing, Peoples R China; [Zhu, Zhanxing] BIBDR, Beijing, Peoples R China	Peking University	Zhu, ZX (corresponding author), Peking Univ, Ctr Data Sci, Beijing, Peoples R China.; Zhu, ZX (corresponding author), BIBDR, Beijing, Peoples R China.	xuju@pku.edu.cn; zhanxing.zhu@pku.edu.cn	Zhu, Zhanxing/GQA-7335-2022		National Natural Science Foundation of China [61806009]; Beijing Natural Science Foundation [4184090]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Beijing Natural Science Foundation(Beijing Natural Science Foundation)	Supported by National Natural Science Foundation of China (Grant No: 61806009) and Beijing Natural Science Foundation (Grant No: 4184090).	Bello I, 2017, PR MACH LEARN RES, V70; Fernando Chrisantha, 2017, PATHNET EVOLUTION CH; Kemker R., 2017, ARXIV171110563; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LEE SW, 2017, ADV NEURAL INFORM PR, P4655; Lopez-Paz D., 2017, NIPS; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Rebuffi SA, 2017, PROC CVPR IEEE, P5533, DOI 10.1109/CVPR.2017.587; Robins A., 1995, Connection Science, V7, P123, DOI 10.1080/09540099550039318; Rusu A. A., 2016, PROGR NEURAL NETWORK; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Thrun Sebastian, 1995, INT C INT ROB SYST; WILLIAMS RJ, 1992, MACH LEARN, V8, P229, DOI 10.1007/BF00992696; Yoon J., 2017, ARXIV170801547; Yu LT, 2017, AAAI CONF ARTIF INTE, P2852; Zenke Friedemann, 2017, INT C MACH LEARN ICM; Zoph B., 2016, ICLR	20	32	32	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823300083
C	Lu, JS; Kannan, A; Yang, JW; Parikh, D; Batra, D		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Lu, Jiasen; Kannan, Anitha; Yang, Jianwei; Parikh, Devi; Batra, Dhruv			Best of Both Worlds: Transferring Knowledge from Discriminative Learning to a Generative Visual Dialog Model	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a novel training framework for neural sequence models, particularly for grounded dialog generation. The standard training paradigm for these models is maximum likelihood estimation (MLE), or minimizing the cross-entropy of the human responses. Across a variety of domains, a recurring problem with MLE trained generative neural dialog models (G) is that they tend to produce 'safe' and generic responses (I don't know', 'I can't tell'). In contrast, discriminative dialog models (D) that are trained to rank a list of candidate human responses outperform their generative counterparts; in terms of automatic metrics, diversity, and informativeness of the responses. However, D is not useful in practice since it can not be deployed to have real conversations with users. Our work aims to achieve the best of both worlds - the practical usefulness of G and the strong performance of D - via knowledge transfer from D to G. Our primary contribution is an end-to-end trainable generative visual dialog model, where G receives gradients from D as a perceptual (not adversarial) loss of the sequence sampled from G. We leverage the recently proposed Gumbel-Softmax (GS) approximation to the discrete distribution - specifically, a RNN augmented with a sequence of GS samplers, coupled with the straight-through gradient estimator to enable end-to-end differentiability. We also introduce a stronger encoder for visual dialog, and employ a self-attention mechanism for answer encoding along with a metric learning loss to aid D in better capturing semantic similarities in answer responses. Overall, our proposed model outperforms state-of-the-art on the VisDial dataset by a significant margin (2.67% on recall@10).	[Lu, Jiasen; Yang, Jianwei; Parikh, Devi; Batra, Dhruv] Georgia Inst Technol, Atlanta, GA 30332 USA; [Kannan, Anitha] Curai, Palo Alto, CA USA; [Parikh, Devi; Batra, Dhruv] Facebook AI Res, Menlo Pk, CA USA	University System of Georgia; Georgia Institute of Technology; Facebook Inc	Lu, JS (corresponding author), Georgia Inst Technol, Atlanta, GA 30332 USA.	jiasenlu@gatech.edu; jw2yang@gatech.edu; parikh@gatech.edu; dbatra@gatech.edu	Jeong, Yongwook/N-7413-2016					Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Bengio Yoshua, 2013, ARXIV; Bordes Antoine, 2016, ARXIV160507683; Chen Kan, 2015, ARXIV151105960; Chen TH, 2015, DES AUT CON, DOI 10.1145/2744769.2744837; Dai B., 2017, ARXIV170306029; Das A, 2017, IEEE I CONF COMP VIS, P2970, DOI 10.1109/ICCV.2017.321; Das A, 2017, PROC CVPR IEEE, P1080, DOI 10.1109/CVPR.2017.121; de Vries H, 2016, ARXIV161108481; Denton Emily L, 2015, NEURIPS, V2, P4; Donahue J, 2015, PROC CVPR IEEE, P2625, DOI 10.1109/CVPR.2015.7298878; Dosovitskiy Alexey, 2016, NEURIPS; Fang H, 2015, PROC CVPR IEEE, P1473, DOI 10.1109/CVPR.2015.7298754; Gao H., 2015, NIPS; Gatys LA., 2015, PROC CVPR IEEE, V16, P326, DOI [10.1167/16.12.326, DOI 10.1109/CVPR.2016.265]; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hinton G., 2015, ARXIV150302531; Jang E., 2016, ARXIV; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Karpathy A, 2015, PROC CVPR IEEE, P3128, DOI 10.1109/CVPR.2015.7298932; Kusner Matt J, 2016, ARXIV161104051; Ledig C, 2017, PROC CVPR IEEE, P105, DOI 10.1109/CVPR.2017.19; Li Jiwei, 2017, P EMNLP; Lin C.-Y., 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.3115/V1/D14-1020; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Chia-Wei, 2016, P EMP METH NAT LANG; Liu S, 2016, ARXIV161200370; Lu JS, 2016, ADV NEUR IN, V29; Lu Jiasen, 2016, CVPR; Maddison Chris J, 2016, ARXIV161100712; Malinowski M, 2015, IEEE I CONF COMP VIS, P1, DOI 10.1109/ICCV.2015.9; Mei Hongyuan, 2016, ARXIV161106997; Mostafazadeh Nasrin, 2017, P 8 INT JOINT C NAT, V1, P462; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Ranzato MarcAurelio, 2015, ARXIV151106732; Ren M., 2015, ADV NEURAL INFORM PR, V28, P2953; Serban I.V, 2016, ARXIV160506069; Serban Iulian V, 2015, ARXIV150704808, V7; Serban IV, 2017, AAAI CONF ARTIF INTE, P3295; Shetty Rakshith, 2017, ABS170310476 CORR; Sohn Kihyuk, 2016, NEURIPS, DOI DOI 10.5555/3157096.3157304; Sordoni Alessandro, 2015, P 2015 C N AM CHAPT, P196, DOI DOI 10.3115/V1/N15-1020; Strub F, 2017, PROCEEDINGS OF THE TWENTY-SIXTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2765; Sukhbaatar S, 2015, ADV NEUR IN, V28; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yu LQ, 2017, AAAI CONF ARTIF INTE, P66; Zhao J, 2016, 2016 IEEE MTT-S INTERNATIONAL WIRELESS SYMPOSIUM (IWS), DOI 10.1109/ICSSSM.2016.7538614; Zhu Jun-Yan, 2017, ICCV	55	32	32	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400030
C	McCann, B; Bradbury, J; Xiong, CM; Socher, R		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		McCann, Bryan; Bradbury, James; Xiong, Caiming; Socher, Richard			Learned in Translation: Contextualized Word Vectors	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Computer vision has benefited from initializing multiple deep layers with weights pretrained on large supervised training sets like ImageNet. Natural language processing (NLP) typically sees initialization of only the lowest layer of deep models with pretrained word vectors. In this paper, we use a deep LSTM encoder from an attentional sequence-to-sequence model trained for machine translation (MT) to contextualize word vectors. We show that adding these context vectors (CoVe) improves performance over using only unsupervised word and character vectors on a wide variety of common NLP tasks: sentiment analysis (SST, IMDb), question classification (TREC), entailment (SNLI), and question answering (SQuAD). For fine-grained sentiment analysis and entailment, CoVe improves performance of our baseline models to the state of the art.				bmccann@salesforce.com; james.bradbury@salesforce.com; cxiong@salesforce.com; rsocher@salesforce.com	Jeong, Yongwook/N-7413-2016					Agirre Eneko, 2014, SEMEVAL COLING; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; [Anonymous], 2013, EMNLP; [Anonymous], 2014, 2014 IEEE C COMP VIS, P580, DOI [10.1109/CVPR.2014.81, DOI 10.1109/CVPR.2014.81]; [Anonymous], 2015, NIPS; [Anonymous], COLING; Bahdanau D., 2015, ICLR; Bowman S.R., 2015, EMNLP; Bowman Samuel R., 2014, CORR; Cettolo Mauro, 2015, IWSLT; Chen Qian, 2016, CORR; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Conneau A, 2017, EMNLP, DOI 10.18653/v1/D17-1070; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dieng A.B., 2016, CORR; Dong L, 2016, CORR; Fukui A., 2016, EMNLP; Graves A, 2005, NEURAL NETWORKS, V18, P602, DOI 10.1016/j.neunet.2005.06.042; Guo H, 2017, ARXIV170707248; Hashimoto K., 2016, CORR; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hill F., 2016, HLT NAACL; Hill F, 2017, MACH TRANSL, V31, P3, DOI 10.1007/s10590-017-9194-2; Huang ML, 2017, ACM T INFORM SYST, V35, DOI 10.1145/3052770; Ioffe S., 2015, PROC INT C MACH LEAR, P448; Jiang J, 2017, MACHINE COMPREHENSIO; Johnson R., 2016, ICML; Kiros R., 2015, NIPS; Klein G., 2017, ARXIV E PRINTS; Koehn P., 2007, ACL; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kumar A., 2016, ICML; Loni B., 2011, TSD; Looks Moshe, 2017, CORR; Lu Jiasen, 2016, ABS161201887 CORR; Luong T., 2015, EMNLP; Maas A., 2011, P 49 ANN M ASS COMPU, P142; Madabushi H. T., 2016, COLING; Mikolov T., 2013, INT C LEARN REPRESEN, DOI 10.1109/TCBB.2021.3077905; Min S., 2017, QUESTION ANSWERING T; Miyato Takeru, 2017, ADVERSARIAL TRAINING, P1; Mou L., 2015, EMNLP; Munkhdalai Tsendsuren, 2016, CORR; Nair V., 2010, ICML, P807; Nallapati R., 2016, CONLL; Paria B., 2016, CORR; Pennington J., 2014, P 2014 C EMP METH NA, P1532, DOI DOI 10.3115/V1/D14-1162; Qi YK, 2016, PROC CVPR IEEE, P4303, DOI 10.1109/CVPR.2016.466; Radford Alec, 2017, CORR; Rajpurkar Pranav, 2016, P EMNLP; Ramachandran P., 2016, CORR; Saenko K., 2010, ECCV, P2; Seo Minjoon, 2017, ABS161101603 ARXIV; Sha L., 2016, COLING; Silva J, 2011, ARTIF INTELL REV, V35, P137, DOI 10.1007/s10462-010-9188-4; Socher R., 2014, ACL; Specia L., 2016, WMT; Sutskever I., 2014, P ADV INT C NEUR INF, P3104; Tackstrom O., 2016, DECOMPOSABLE ATTENTI; Van-Tu Nguyen, 2016, INDIAN J SCI TECHNOL, V17; Voorhees E. M., 1999, TREC; WANG W, 2017, GATED SELF MATCHING; Warde-Farley D., 2013, ICML; Wieting J., 2016, ICLR; Xin Li, 2006, Natural Language Engineering, V12, P229, DOI 10.1017/S1351324905003955; Xiong C., 2017, ICRL; Xiong CM, 2016, PR MACH LEARN RES, V48; Yu Y., 2017, ICLR; Zhang R., 2016, HLT NAACL; Zhu Y., 2011, P 25 AAAI C ART INT	70	32	33	2	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406036
C	Neyshabur, B; Bhojanapalli, S; McAllester, D; Srebro, N		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Neyshabur, Behnam; Bhojanapalli, Srinadh; McAllester, David; Srebro, Nathan			Exploring Generalization in Deep Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				CLASSIFICATION; NETWORKS	With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.	[Neyshabur, Behnam; Bhojanapalli, Srinadh; McAllester, David; Srebro, Nathan] Toyota Technol Inst Chicago, Chicago, IL 60637 USA	Toyota Technological Institute - Chicago	Neyshabur, B (corresponding author), Toyota Technol Inst Chicago, Chicago, IL 60637 USA.	bneyshabur@ttic.edu; srinadh@ttic.edu; mcallester@ttic.edu; nati@ttic.edu						Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Bartlett P.L., 2017, PREPRINT; Bartlett P. L., 2017, ARXIV170608498; Bartlett PL, 1998, IEEE T INFORM THEORY, V44, P525, DOI 10.1109/18.661502; Bartlett PL, 1998, NEURAL COMPUT, V10, P2159, DOI 10.1162/089976698300017016; Chaudhari P., 2016, ARXIV161101838; Dziugaite Gintare Karolina, 2017, ARXIV170311008; Evgeniou T, 2000, ADV COMPUT MATH, V13, P1, DOI 10.1023/A:1018946025316; Hardt Moritz, 2016, ICML; Harvey N., 2017, ARXIV170302930; Keskar N.S., 2016, ABS160904836; Langford John., 2001, ADV NEURAL INFORM PR, P809; McAllester D, 2003, LECT NOTES ARTIF INT, V2777, P203, DOI 10.1007/978-3-540-45167-9_16; McAllester D. A., 1999, Proceedings of the Twelfth Annual Conference on Computational Learning Theory, P164, DOI 10.1145/307400.307435; McAllester D. A., 1998, Proceedings of the Eleventh Annual Conference on Computational Learning Theory, P230, DOI 10.1145/279943.279989; Neyshabur  B., 2015, P INT C LEARN REPR W; Neyshabur B., 2016, ADV NEURAL INFORM PR; Neyshabur B., 2015, C LEARN THEOR; Neyshabur Behnam, 2015, P NIPS, P2422; Neyshabur  Behnam, 2016, INT C LEARN REPR; Shalev-Shwartz S., 2014, UNDERSTANDING MACHIN, DOI DOI 10.1017/CBO9781107298019; Smola AJ, 1998, NEURAL NETWORKS, V11, P637, DOI 10.1016/S0893-6080(98)00032-X; Sokolic Jure, 2016, ARXIV161004574; Srebro N., 2005, P ADV NEURAL INFORM; von Luxburg U, 2004, J MACH LEARN RES, V5, P669; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1; Zeng HQ, 2017, PROC INT CONF RECON	30	32	32	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406003
C	Singh, R; Lanchantin, J; Sekhon, A; Qi, YJ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Singh, Ritambhara; Lanchantin, Jack; Sekhon, Arshdeep; Qi, Yanjun			Attend and Predict: Understanding Gene Regulation by Selective Attention on Chromatin	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The past decade has seen a revolution in genomic technologies that enabled a flood of genome-wide profiling of chromatin marks. Recent literature tried to understand gene regulation by predicting gene expression from large-scale chromatin measurements. Two fundamental challenges exist for such learning tasks: (1) genome-wide chromatin signals are spatially structured, high-dimensional and highly modular; and (2) the core aim is to understand what the relevant factors are and how they work together. Previous studies either failed to model complex dependencies among input signals or relied on separate feature analysis to explain the decisions. This paper presents an attention-based deep learning approach, AttentiveChrome, that uses a unified architecture to model and to interpret dependencies among chromatin factors for controlling gene regulation. AttentiveChrome uses a hierarchy of multiple Long Short-Term Memory (LSTM) modules to encode the input signals and to model how various chromatin marks cooperate automatically. AttentiveChrome trains two levels of attention jointly with the target prediction, enabling it to attend differentially to relevant marks and to locate important positions per mark. We evaluate the model across 56 different cell types (tasks) in humans. Not only is the proposed architecture more accurate, but its attention scores provide a better interpretation than state-of-the-art feature visualization methods such as saliency maps.(1)	[Singh, Ritambhara; Lanchantin, Jack; Sekhon, Arshdeep; Qi, Yanjun] Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA	University of Virginia	Qi, YJ (corresponding author), Univ Virginia, Dept Comp Sci, Charlottesville, VA 22903 USA.	yanjun@virginia.edu	Jeong, Yongwook/N-7413-2016	Qi, Yanjun/0000-0002-5796-7453				Alipanahi B., 2015, PREDICTING SEQUENCE; Ba Jimmy, MULTIPLE OBJECT RECO; Baehrens D, 2010, J MACH LEARN RES, V11, P1803; Bannister AJ, 2011, CELL RES, V21, P381, DOI 10.1038/cr.2011.22; Boros J, 2014, MOL CELL BIOL, V34, P3662, DOI 10.1128/MCB.00205-14; Cheng C, 2011, GENOME BIOL, V12, DOI 10.1186/gb-2011-12-2-r15; Chorowski I. K., 2015, ADV NEURAL INFORM PR, V28, P577, DOI DOI 10.1016/0167-739X(94)90007-8; Corbetta M, 2002, NAT REV NEUROSCI, V3, P201, DOI 10.1038/nrn755; Dong XJ, 2013, EPIGENOMICS-UK, V5, P113, DOI [10.2217/EPI.13.13, 10.2217/epi.13.13]; Dong XJ, 2012, GENOME BIOL, V13, DOI 10.1186/gb-2012-13-9-r53; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Ho B. H, 2015, SOME CURRENT ADV RES, P123; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Karlic R, 2010, P NATL ACAD SCI USA, V107, P2926, DOI 10.1073/pnas.0909344107; Karpathy A., 2015, ARXIV150602078; Kelley David R, 2016, BASSET LEARNING REGU; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kundaje A, 2015, NATURE, V518, P317, DOI 10.1038/nature14248; Lanchantin J, 2016, ARXIV; Lanchantin J, 2016, ARXIV160803644; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Jiwei, 2015, VISUALIZING UNDERSTA; Li Yao, 2015, COMP VIS ICCV 2015 I; Lin ZM, 2016, AAAI CONF ARTIF INTE, P27; Luong M., 2015, P 2015 C EMP METH NA, P1412, DOI [10.18653/v1/D15-1166, DOI 10.18653/V1/D15-1166]; McManus S, 2011, EMBO J, V30, P2388, DOI 10.1038/emboj.2011.140; Mnih V., 2014, NEURAL INFORM PROCES, DOI DOI 10.48550/ARXIV.1406.6247; Quang D, 2016, NUCLEIC ACIDS RES, V44, DOI 10.1093/nar/gkw226; Simonyan Karen, 2013, DEEP INSIDE CONVOLUT, P2; Singh R, 2016, BIOINFORMATICS, V32, P639, DOI 10.1093/bioinformatics/btw427; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Vinyals O., 2015, ADV NEURAL INFORM PR, P2692; Xu HJ, 2016, LECT NOTES COMPUT SC, V9911, P451, DOI 10.1007/978-3-319-46478-7_28; Xu K, 2015, PR MACH LEARN RES, V37, P2048; Yang Z, 2016, P 2016 C N AM CHAPTE, P1480; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhou J, 2014, ARXIV14031347; Zhou J, 2015, NAT METHODS, V12, P931, DOI [10.1038/nmeth.3547, 10.1038/NMETH.3547]	38	32	33	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST	30147283				2022-12-19	WOS:000452649406082
C	Steinhardt, J; Koh, PW; Liang, P		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Steinhardt, Jacob; Koh, Pang Wei; Liang, Percy			Certified Defenses for Data Poisoning Attacks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA				SECURITY	Machine learning systems trained on user-provided data are susceptible to data poisoning attacks, whereby malicious users inject false training data with the aim of corrupting the learned model. While recent work has proposed a number of attacks and defenses, little is understood about the worst-case loss of a defense in the face of a determined attacker. We address this by constructing approximate upper bounds on the loss across a broad family of attacks, for defenders that first perform outlier removal followed by empirical risk minimization. Our approximation relies on two assumptions: (1) that the dataset is large enough for statistical concentration between train and test error to hold, and (2) that outliers within the clean (nonpoisoned) data do not have a strong effect on the model. Our bound comes paired with a candidate attack that often nearly matches the upper bound, giving us a powerful tool for quickly assessing defenses on a given dataset. Empirically, we find that even under a simple defense, the MNIST-1-7 and Dogfish datasets are resilient to attack, while in contrast the IMDB sentiment dataset can be driven from 12% to 23% test error by adding only 3% poisoned data.	[Steinhardt, Jacob; Koh, Pang Wei; Liang, Percy] Stanford Univ, Stanford, CA 94305 USA	Stanford University	Steinhardt, J (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	jsteinha@stanford.edu; pangwei@cs.stanford.edu; pliang@cs.stanford.edu	Jeong, Yongwook/N-7413-2016		Fannie & John Hertz Foundation Fellowship; NSF Graduate Research Fellowship; Future of Life Institute grant; Open Philanthropy Project	Fannie & John Hertz Foundation Fellowship; NSF Graduate Research Fellowship(National Science Foundation (NSF)); Future of Life Institute grant; Open Philanthropy Project	JS was supported by a Fannie & John Hertz Foundation Fellowship and an NSF Graduate Research Fellowship. This work was also partially supported by a Future of Life Institute grant and a grant from the Open Philanthropy Project. We are grateful to Daniel Selsam, Zhenghao Chen, and Nike Sun, as well as to the anonymous reviewers, for a great deal of helpful feedback.	[Anonymous], 2012, P 29 INT COFERENCE I; [Anonymous], 2016, SCI SECURITY PRIVACY; [Anonymous], 2016, TRANSFERABILITY MACH; [Anonymous], 2016, ADVERSARIAL EXAMPLES; Awasthi P, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P449, DOI 10.1145/2591796.2591839; Barreno M, 2010, MACH LEARN, V81, P121, DOI 10.1007/s10994-010-5188-5; Behzadan V, 2017, VULNERABILITY DEEP R; Bhatia K., 2015, ADV NEURAL INF PROCE, P721; Biggio B., 2014, WORKSH ART INT SEC A; Biggio B., 2014, WORKSH STRUCT SYNT S; Biggio B., 2013, WORKSH ART INT SEC A; Biggio B, 2014, IEEE T KNOWL DATA EN, V26, P984, DOI 10.1109/TKDE.2013.57; Bishop M. A., 2002, ART SCI COMPUTER SEC; Bruckner M., 2011, P 17 ACM SIGKDD INT, P547, DOI DOI 10.1145/2020408.2020495; Bruckner M, 2012, J MACH LEARN RES, V13, P2617; Burkard C., 2017, INT WORKSH SEC PRIV; Charikar M., 2017, S THEOR COMP STOC; Chen Y., 2013, ROBUST HIGH DIMENSIO; Cretu GF, 2008, P IEEE S SECUR PRIV, P81, DOI 10.1109/SP.2008.11; Diakonikolas I., 2016, FDN COMPUTER SCI FOC; Diamond S, 2016, J MACH LEARN RES, V17; Gardiner J, 2016, ACM COMPUT SURV, V49, DOI 10.1145/3003816; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goodfellow Ian J., 2015, INT C LEARNING REPRE; Gurobi Optimization LLC, 2020, GUROBI OPTIMIZER REF; Huang S., 2017, ADVERSARIAL ATTACKS; Kakade S.M., 2009, ADV NEURAL INFORM PR; Kerckhoffs A., 1883, J SCI MILITAIRES, VIX, P161; Koh P. W., 2017, INT C MACH LEARN ICM; Laishram R., 2016, CURIE METHOD PROTECT; Lakhina A, 2004, ACM SIGCOMM COMP COM, V34, P219, DOI 10.1145/1030194.1015492; Laskov P., 2014, S SEC PRIV; Li B., 2016, ADV NEURAL INFORM PR; Liao Y.-H., 2017, TACTICS ADVERSARIAL; Liu J, 2016, J MACH LEARN RES, V17; Lofberg J., 2004, CACSD; Maas A.L., 2011, 49 ANN M ASS COMP LI, P142; Mei S., 2015, ASS ADVANCEMENT ARTI; Metsis V., 2006, CEAS, V17, P28; Nasrabadi N. M., 2011, ADV NEURAL INFORM PR; Newell A., 2014, P 2014 WORKSHOP ARTI, P83, DOI 10.1145/2666652.2666661; Newsome J., 2006, INT WORKSH REC ADV I; Nguyen NH, 2013, IEEE T INFORM THEORY, V59, P2017, DOI 10.1109/TIT.2013.2240435; Park S, 2017, ACM IEEE INT CONF CY, P155, DOI 10.1145/3055004.3055006; Rubinstein B., 2009, ACM SIGCOMM C INT ME; Shalev-Shwartz S, 2012, FOUND TRENDS MACH LE, V4, P107, DOI 10.1561/2200000018; Steinhardt J., 2014, ARXIV14124182; Steinhardt Jacob, 2016, ADV NEURAL INFORM PR, P4439; Sturm JF, 1999, OPTIM METHOD SOFTW, V11-2, P625, DOI 10.1080/10556789908805766; Szegedy Christian, 2014, PROC 2 INT C LEARN R; Tramer F., 2016, USENIX SECURITY; Vuurens J., 2011, ACM SIGIR WORKSH CRO; Wang G, 2016, THESIS; Xiao H., 2012, EUR C ART INT; Xiao H., 2015, INT C MACH LEARN ICM; Xiao L, 2010, J MACH LEARN RES, V11, P2543; Yang C, 2017, GENERATIVE POISONING; Zhou Y., 2016, PAC AS C KNOWL DISC; Zhu X, 2015, ARTIFICIAL INTELLIGE	65	32	32	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403057
C	Lian, XR; Huang, YJ; Li, YC; Liu, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Lian, Xiangru; Huang, Yijun; Li, Yuncheng; Liu, Ji			Asynchronous Parallel Stochastic Gradient for Nonconvex Optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Asynchronous parallel implementations of stochastic gradient (SG) have been broadly used in solving deep neural network and received many successes in practice recently. However, existing theories cannot explain their convergence and speedup properties, mainly due to the nonconvexity of most deep learning formulations and the asynchronous parallel mechanism. To fill the gaps in theory and provide theoretical supports, this paper studies two asynchronous parallel implementations of SG: one is over a computer network and the other is on a shared memory system. We establish an ergodic convergence rate O (1/root K) for both algorithms and prove that the linear speedup is achievable if the number of workers is bounded by root K (K is the total number of iterations). Our results generalize and improve existing analysis for convex minimization.	[Lian, Xiangru; Huang, Yijun; Li, Yuncheng; Liu, Ji] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA	University of Rochester	Lian, XR (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.	lianxiangru@gmail.com; huangyj0@gmail.com; raingomm@gmail.com; ji.liu.uwisc@gmail.com			NSF [CNS-1548078]; NEC fellowship; University of Rochester	NSF(National Science Foundation (NSF)); NEC fellowship; University of Rochester	This project is supported by the NSF grant CNS-1548078, the NEC fellowship, and the startup funding at University of Rochester. We thank Professor Daniel Gildea and Professor Sandhya Dwarkadas at University of Rochester, Professor Stephen J. Wright at University of Wisconsin-Madison, and anonymous (meta-) reviewers for their constructive comments and helpful advices.	Agarwal A, 2011, NIPS; Avron H., 2014, IPDPS; Bengio Y, 2003, J MACH LEARN RES, V3, P1137, DOI 10.1162/153244303322533223; Bertsekas D.P., 1989, PARALLEL DISTRIBUTED, V23; Dean J., 2012, ADV NEURAL INFORM PR, V25; Dekel O, 2012, J MACH LEARN RES, V13, P165; Fercoq O., 2013, ARXIV13125799; Feyzmahdavian H. R., 2015, ARXIV E PRINTS; Ghadimi S, 2013, SIAM J OPTIMIZ, V23, P2341, DOI 10.1137/120880811; HONG M., 2014, ARXIV14126058; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Li M., 2014, NIPS; Li M., 2013, BIG LEARN NIPS WORKS; Li Mu, 2014, P 11 USENIX C OP SYS, P583; Liu J., 2014, ARXIV14033862; Liu J., 2014, ICML; Mania H., 2015, ARXIV150706970; Marecek J., 2014, ARXIV14060238; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Niu F., 2011, NIPS; Paine T., 2013, NIPS; Petroni F., 2014, ACM C REC SYST; Sridhar, 2014, ARXIV14014780; Tappenden Rachael, 2015, ARXIV150303033; Tran K., 2015, ICML; Yun H., 2013, ARXIV13120193; Zhang R., 2014, ICML; Zhang S., 2014, CORR	31	32	32	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101011
C	Merolla, P; Boahen, K		Thrun, S; Saul, K; Scholkopf, B		Merolla, P; Boahen, K			A recurrent model of orientation maps with simple and complex cells	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA				VISUAL-CORTEX; SELECTIVITY	We describe a neuromorphic chip that utilizes transistor heterogeneity, introduced by the fabrication process, to generate orientation maps similar to those imaged in vivo. Our model consists of a recurrent network of excitatory and inhibitory cells in parallel with a push-pull stage. Similar to a previous model the recurrent network displays hotspots of activity that give rise to visual feature maps. Unlike previous work, however, the map for orientation does not depend on the sign of contrast. Instead, sign-independent cells driven by both ON and OFF channels anchor the map, while push-pull interactions give rise to sign-preserving cells. These two groups of orientation-selective cells are similar to complex and simple cells observed in VI.	Univ Penn, Dept Bioengn, Philadelphia, PA 19104 USA	University of Pennsylvania	Merolla, P (corresponding author), Univ Penn, Dept Bioengn, Philadelphia, PA 19104 USA.							BLASDEL GG, 1992, J NEUROSCI, V12, P3139, DOI 10.1523/JNEUROSCI.12-08-03139.1992; BOAHEN K, 1992, NIPS91; Boahen KA, 2000, IEEE T CIRCUITS-II, V47, P416, DOI 10.1109/82.842110; Crair MC, 1998, SCIENCE, V279, P566, DOI 10.1126/science.279.5350.566; Culurciello E, 2003, IEEE J SOLID-ST CIRC, V38, P281, DOI 10.1109/JSSC.2002.807412; Ernst UA, 2001, NAT NEUROSCI, V4, P431, DOI 10.1038/86089; HUBEL DH, 1962, J PHYSIOL-LONDON, V160, P106, DOI 10.1113/jphysiol.1962.sp006837; Ringach DL, 2002, J NEUROSCI, V22, P5639; ZAGHLOUL K, 2002, NEUROSCIENCE	9	32	32	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						995	1002						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500124
C	Heskes, T		Mozer, MC; Jordan, MI; Petsche, T		Heskes, T			Balancing between bagging and bumping	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				We compare different methods to combine predictions from neural networks trained on different bootstrap samples of a regression problem. One of these methods, introduced in [6] and which we here call balancing, is based on the analysis of the ensemble generalization error into an ambiguity term and a term incorporating generalization performances of individual networks. We show how to estimate these individual errors from the residuals on validation patterns. Weighting factors for the different networks follow from a quadratic programming problem. On a real-world problem concerning the prediction of sales figures and an the well-known Boston housing data set, balancing clearly outperforms other recently proposed alternatives as bagging [1] and bumping [8].			Heskes, T (corresponding author), UNIV NIJMEGEN,RWCP NOVEL FUNCT SNN LAB,GEERT GROOTEPLEIN 21,NL-6525 EZ NIJMEGEN,NETHERLANDS.		Heskes, Tom/A-1443-2010	Heskes, Tom/0000-0002-3398-5235					0	32	33	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						466	472						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00066
C	Murata, N; Muller, KR; Ziehe, A; Amari, SI		Mozer, MC; Jordan, MI; Petsche, T		Murata, N; Muller, KR; Ziehe, A; Amari, SI			Adaptive on-line learning in changing environments	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				An adaptive on-line algorithm extending the learning of learning idea is proposed and theoretically motivated. Relying only on gradient flow information it can be applied, to learning continuous functions or distributions, even when no explicit loss function is given and the Hessian is not available. Its efficiency is demonstrated for a non-stationary blind separation task of acoustic signals.			Murata, N (corresponding author), GMD FIRST,RUDOWER CHAUSSEE 5,D-12489 BERLIN,GERMANY.		Mueller, Klaus-Robert/Y-3547-2019; Muller, Klaus R/C-3196-2013; Murata, Noboru/J-3345-2012	Mueller, Klaus-Robert/0000-0002-3861-7685; Murata, Noboru/0000-0002-4258-6877					0	32	35	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						599	605						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00085
C	Hua, WZ; Zhou, Y; De Sa, C; Zhang, ZR; Suh, GE		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hua, Weizhe; Zhou, Yuan; De Sa, Christopher; Zhang, Zhiru; Suh, G. Edward			Channel Gating Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper introduces channel gating, a dynamic, fine-grained, and hardware-efficient pruning scheme to reduce the computation cost for convolutional neural networks (CNNs). Channel gating identifies regions in the features that contribute less to the classification result, and skips the computation on a subset of the input channels for these ineffective regions. Unlike static network pruning, channel gating optimizes CNN inference at run-time by exploiting input-specific characteristics, which allows substantially reducing the compute cost with almost no accuracy loss. We experimentally show that applying channel gating in state-of-the-art networks achieves 2.7-8.0x reduction in floating-point operations (FLOPs) and 2.0-4.4x reduction in off-chip memory accesses with a minimal accuracy loss on CIFAR-10. Combining our method with knowledge distillation reduces the compute cost of ResNet-18 by 2.6x without accuracy drop on ImageNet. We further demonstrate that channel gating can be realized in hardware efficiently. Our approach exhibits sparsity patterns that are well-suited to dense systolic arrays with minimal additional hardware. We have designed an accelerator for channel gating networks, which can be implemented using either FPGAs or ASICs. Running a quantized ResNet-18 model for ImageNet, our accelerator achieves an encouraging speedup of 2.4x on average, with a theoretical FLOP reduction of 2.8x.				wh399@cornell.edu; yz882@cornell.edu; cdesa@cornell.edu; zhiruz@cornell.edu; gs272@cornell.edu		Zhang, Zhiru/0000-0002-0778-0308	Semiconductor Research Corporation; DARPA; NSF [1453378, 1618275]	Semiconductor Research Corporation; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF))	This work was partially sponsored by Semiconductor Research Corporation and DARPA, NSF Awards #1453378 and #1618275, a research gift from Xilinx, Inc., and a GPU donation from NVIDIA Corporation. The authors would like to thank the Batten Research Group, especially Christopher Torng (Cornell Univ.), for sharing their Modular VLSI Build System. The authors also thank Zhaoliang Zhang and Kaifeng Xu (Tsinghua Univ.) for the C++ implementation of channel gating and Ritchie Zhao and Oscar Castaneda (Cornell Univ.) for insightful discussions.	[Anonymous], 2016, ABS160305279 CORR; [Anonymous], ABS160507678 CORR; Bengio Y., 2016, ABS160202830 CORR; Chen T., 2015, ABS151201274 CORR; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dong Xuanyi, 2017, ABS170308651 CORR; Figurnov Michael, 2016, P 30 INT C NEUR INF, P955; Figurnov Michael, 2016, ABS161202297 CORR; Gao X., 2018, DYNAMIC CHANNEL PRUN; Han S., 2015, 4 INT C LEARN REPR; He K., 2015, ABS151203385 CORR; He Y., 2017, ABS170706168 CORR; He Y., 2018, IJCAI, P2234; Hinton G., 2015, COMPUTER SCI; Howard A. G., 2017, MOBILENETS EFFICIENT; Ioffe S, 2015, ARXIV 1502 03167, V32, P448; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Li H., 2016, ABS160808710 CORR; Lin J, 2017, ADV NEUR IN, V30; Lin X., 2017, ADV NEURAL INFORM PR, P345; Liu Zhuang, 2017, ABS170806519 CORR; Lu Zongqing, 2017, ABS170909503 CORR; McGill M, 2017, PR MACH LEARN RES, V70; Mullapudi Ravi Teja, 2018, IEEE C COMP VIS PATT; Nisa I, 2018, INT C HIGH PERFORM, P32, DOI 10.1109/HiPC.2018.00013; Patterson David, 2017, ABS170404760 CORR; Shazeer Noam, 2017, ABS170106538 CORR; Wu Zuxuan, 2017, ABS171108393 CORR; Zhang Xiangyu, 2017, ABS170701083 CORR; Zhuang Z, 2018, DISCRIMINATION AWARE	30	31	31	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301083
C	Ivkin, N; Rothchild, D; Ullah, E; Braverman, V; Stoica, I; Arora, R		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Ivkin, Nikita; Rothchild, Daniel; Ullah, Enayat; Braverman, Vladimir; Stoica, Ion; Arora, Raman			Communication-efficient Distributed SGD with Sketching	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Large-scale distributed training of neural networks is often limited by network bandwidth, wherein the communication time overwhelms the local computation time. Motivated by the success of sketching methods in sub-linear/streaming algorithms, we introduce SKETCHED-SGD(4), an algorithm for carrying out distributed SGD by communicating sketches instead of full gradients. We show that SKETCHED-SGD has favorable convergence rates on several classes of functions. When considering all communication - both of gradients and of updated model weights - SKETCHED-SGD reduces the amount of communication required compared to other gradient compression methods from O(d) or O(W) to O(log d), where d is the number of model parameters and W is the number of workers participating in training. We run experiments on a transformer model, an LSTM, and a residual network, demonstrating up to a 40x reduction in total communication cost with no loss in final model performance. We also show experimentally that SKETCHED-SGD scales to at least 256 workers without increasing communication cost or degrading model performance.	[Ivkin, Nikita] Amazon, Seattle, WA 98108 USA; [Rothchild, Daniel; Stoica, Ion] Univ Calif Berkeley, Berkeley, CA USA; [Ivkin, Nikita; Ullah, Enayat; Braverman, Vladimir; Arora, Raman] Johns Hopkins Univ, Baltimore, MD 21218 USA	Amazon.com; University of California System; University of California Berkeley; Johns Hopkins University	Ivkin, N (corresponding author), Amazon, Seattle, WA 98108 USA.	ivkin@amazon.com; drothchild@berkeley.edu; enayat@jhu.edu; vova@cs.jhu.edu; istoica@berkeley.edu; arora@cs.jhu.edu	Ivkin, Nikita/GYE-7564-2022		NSF BIGDATA [IIS-1546482, IIS-1838139]; NSF CAREER grant [1652257]; ONR Award [N00014-18-1-2364]; Lifelong Learning Machines program from DARPA/MTO; National Science Foundation Graduate Research Fellowship [DGE 1752814]	NSF BIGDATA; NSF CAREER grant(National Science Foundation (NSF)NSF - Office of the Director (OD)); ONR Award; Lifelong Learning Machines program from DARPA/MTO; National Science Foundation Graduate Research Fellowship(National Science Foundation (NSF))	This research was supported, in part, by NSF BIGDATA grants IIS-1546482 and IIS-1838139, NSF CAREER grant 1652257, ONR Award N00014-18-1-2364 and the Lifelong Learning Machines program from DARPA/MTO. This material is based upon work supported by the National Science Foundation Graduate Research Fellowship under Grant No. DGE 1752814.	Agarwal PK, 2013, ACM T DATABASE SYST, V38, DOI 10.1145/2500128; Aghazadeh Amirali, 2018, P MACHINE LEARNING R, V80, P80; Alistarh D., 2017, ADV NEURAL INF PROCE, P1709; Alistarh D., 2018, NEURIPS, P5977; [Anonymous], 2012, PROC INT C MACH LEAR; [Anonymous], 2017, ADV NEURAL INFORM PR; Bernstein J., 2018, ARXIV180204434; Braverman V, 2017, PODS'17: PROCEEDINGS OF THE 36TH ACM SIGMOD-SIGACT-SIGAI SYMPOSIUM ON PRINCIPLES OF DATABASE SYSTEMS, P361, DOI 10.1145/3034786.3034798; Charikar M., INT C AUT LANG PROGR, P693; Coleman C., 2017, TRAINING, V100, P102; Cormode G, 2005, J ALGORITHMS, V55, P58, DOI 10.1016/j.jalgor.2003.12.001; Golmant N., 2018, ARXIV181112941; Ivkin N, 2018, ASTRON COMPUT, V23, P166, DOI 10.1016/j.ascom.2018.04.003; Jaggi M., 2019, ARXIV190109847; Keskar N.S., 2016, ABS160904836; Klein G, 2017, PROCEEDINGS OF THE 55TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL 2017): SYSTEM DEMONSTRATIONS, P67, DOI 10.18653/v1/P17-4012; Koloskova A., 2019, ARXIV190200340; Konecny J., 2016, ARXIV161005492; Krizhevsky A., 2009, LEARNING MULTIPLE LA; Kyrola A., 2017, ABS170602677 ARXIV; Li M, 2014, PROCEEDINGS OF THE 20TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING (KDD'14), P661, DOI 10.1145/2623330.2623612; Lin Y., 2017, ARXIV171201887; Ma S., 1688, P INT C MACH LEARN S, V80; Mania H., 2015, ARXIV150706970; Muthukrishnan S, 2005, FOUND TRENDS THEOR C, V1, P1, DOI 10.1561/0400000002; Paszke Adam, 2017, AUTOMATIC DIFFERENTI, P5; Radford A., 2019, OPENAI BLOG, V1, P9, DOI DOI 10.18653/V1/P19-1195; Seide F, 2014, INTERSPEECH, P1058; Shallue Christopher J, 2018, ARXIV181103600; Stich S. U., 2018, P 32 INT C NEUR INF, P4452; Strom N., 2015, 16 ANN C INT SPEECH; Tai KS, 2018, INT CONF MANAGE DATA, P757, DOI 10.1145/3183713.3196930; Wen W., 2017, P NIPS, P1509; You Y., 2017, LARGE BATCH TRAINING	37	31	31	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904076
C	Kull, M; Perello-Nieto, M; Kangsepp, M; Silva, T; Song, H; Flach, P		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kull, Meelis; Perello-Nieto, Miquel; Kangsepp, Markus; Silva Filho, Telmo; Song, Hao; Flach, Peter			Beyond temperature scaling: Obtaining well-calibrated multiclass probabilities with Dirichlet calibration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CLASSIFIERS	Class probabilities predicted by most multiclass classifiers are uncalibrated, often tending towards over-confidence. With neural networks, calibration can be improved by temperature scaling, a method to learn a single corrective multiplicative factor for inputs to the last softmax layer. On non-neural models the existing methods apply binary calibration in a pairwise or one-vs-rest fashion. We propose a natively multiclass calibration method applicable to classifiers from any model class, derived from Dirichlet distributions and generalising the beta calibration method from binary classification. It is easily implemented with neural nets since it is equivalent to log-transforming the uncalibrated probabilities, followed by one linear layer and softmax. Experiments demonstrate improved probabilistic predictions according to multiple measures (confidence-ECE, classwise-ECE, log-loss, Brier score) across a wide range of datasets and classifiers. Parameters of the learned Dirichlet calibration map provide insights to the biases in the uncalibrated model.	[Kull, Meelis; Kangsepp, Markus] Univ Tartu, Dept Comp Sci, Tartu, Estonia; [Perello-Nieto, Miquel; Song, Hao; Flach, Peter] Univ Bristol, Dept Comp Sci, Bristol, Avon, England; [Silva Filho, Telmo] Univ Fed Paratba, Dept Stat, Joao Pessoa, Paraiba, Brazil; [Flach, Peter] Alan Turing Inst, London, England	University of Tartu; University of Bristol	Kull, M (corresponding author), Univ Tartu, Dept Comp Sci, Tartu, Estonia.	meelis.kull@ut.ee; miquel.perellonieto@bris.ac.uk; markus.kangsepp@ut.ee; telmo@de.ufpb.br; hao.song@bristol.ac.uk; peter.flach@bristol.ac.uk		Silva Filho, Telmo/0000-0003-0826-6885; Flach, Peter/0000-0001-6857-5810	Estonian Research Council [PUT1458]; UK Engineering and Physical Sciences Research Council (EPSRC) [EP/R005273/1]; Alan Turing Institute under EPSRC [EP/N510129/1]	Estonian Research Council(Estonian Research Council); UK Engineering and Physical Sciences Research Council (EPSRC)(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Alan Turing Institute under EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The work of MKu and MK5i was supported by the Estonian Research Council under grant PUT1458. The work of MPN and HS was supported by the SPHERE Next Steps Project funded by the UK Engineering and Physical Sciences Research Council (EPSRC), Grant EP/R005273/1. The work of PF and HS was supported by The Alan Turing Institute under EPSRC, Grant EP/N510129/1.	Allikivi M.-L., 2019, MACHINE LEARNING KNO, P68; Bradbury J., 2018, JAX COMPOSABLE TRANS; Brier G. W., 1950, MON WEATHER REV, V78, P1, DOI [10.1175/1520-0493(1950)0782.0.co;2, DOI 10.1175/1520-0493(1950)078<0001:VOFEIT>2.0.CO;2]; Cheni A., 2017, BASE PRETRAINED MODE; Chollet E, 2015, KERAS; DEGROOT MH, 1983, J ROY STAT SOC D-STA, V32, P12; Demsar J, 2006, J MACH LEARN RES, V7, P1; Ferri U, 2003, LECT NOTES ARTIF INT, V2837, P121; Guo C., 2017, 34 INT C MACH LEARN; He K., 2015, ABS151203385 CORR; Huang G, 2016, ARXIV160806993; Huang G., 2016, ABS160309382 CORR; Kuleshov V., 2018, ARXIV180700263; Kull M, 2015, LECT NOTES ARTIF INT, V9284, P68, DOI 10.1007/978-3-319-23528-8_5; Kumar A, 2018, IEEE ENER CONV, P2805, DOI 10.1109/ECCE.2018.8557649; Kumar A., 2019, ADV NEURAL INFORM PR; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Maddox W., 2019, ABS190202476 CORR; Milios D., 2018, ADV NEURAL INFORM PR, P6005; Murphy Allan H, 1977, J ROYAL STAT SOC C, V26, P41, DOI [DOI 10.2307/2346866, 10.2307/2346866]; Naeini Mahdi Pakdaman., 2015, AAAI C ART INT; Naeini MP, 2016, IEEE DATA MINING, P360, DOI [10.1109/ICDM.2016.0047, 10.1109/ICDM.2016.96]; Platt JC, 2000, ADV NEUR IN, P61; Zadrozny Bianca, 2001, ICML; Zagoruyko S., 2016, BRIT MACHINE VISION	28	31	31	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904001
C	Kumar, A; Fu, J; Tucker, G; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kumar, Aviral; Fu, Justin; Tucker, George; Levine, Sergey			Stabilizing Off-Policy Q-Learning via Bootstrapping Error Reduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Off-policy reinforcement learning aims to leverage experience collected from prior policies for sample-efficient learning. However, in practice, commonly used off-policy approximate dynamic programming methods based on Q-learning and actor-critic methods are highly sensitive to the data distribution, and can make only limited progress without collecting additional on-policy data. As a step towards more robust off-policy algorithms, we study the setting where the off-policy experience is fixed and there is no further interaction with the environment. We identify bootstrapping error as a key source of instability in current methods. Bootstrapping error is due to bootstrapping from actions that lie outside of the training data distribution, and it accumulates via the Bellman backup operator. We theoretically analyze bootstrapping error, and demonstrate how carefully constraining action selection in the backup can mitigate it. Based on our analysis, we propose a practical algorithm, bootstrapping error accumulation reduction (BEAR). We demonstrate that BEAR is able to learn robustly from different off-policy distributions, including random and suboptimal demonstrations, on a range of continuous control tasks.	[Kumar, Aviral; Fu, Justin; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Tucker, George; Levine, Sergey] Google Brain, Mountain View, CA USA	University of California System; University of California Berkeley; Google Incorporated	Kumar, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	aviralk@berkeley.edu; justinjfu@eecs.berkeley.edu; gjt@google.com; svlevine@eecs.berkeley.edu			Berkeley DeepDrive; JPMorgan Chase Co.; NSF [IIS-1651843, IIS-1614653]; DARPA Assured Autonomy program; ARL DCIST CRA [W911NF-17-2-0181]	Berkeley DeepDrive; JPMorgan Chase Co.; NSF(National Science Foundation (NSF)); DARPA Assured Autonomy program; ARL DCIST CRA	We thank Kristian Hartikainen for sharing implementations of RL algorithms and for help in debugging certain issues. We thank Matthew Soh for help in setting up environments. We thank Aurick Zhou, Chelsea Finn, Abhishek Gupta and Kelvin Xu for informative discussions. We thank Ofir Nachum for comments on an earlier draft of this paper. We thank Google, NVIDIA, and Amazon for providing computational resources. This research was supported by Berkeley DeepDrive, JPMorgan Chase & Co., NSF IIS-1651843 and IIS-1614653, the DARPA Assured Autonomy program, and ARL DCIST CRA W911NF-17-2-0181.	Antos A., 2008, ADV NEURAL INFORM PR, P9; Antos A, 2007, 2007 IEEE INTERNATIONAL SYMPOSIUM ON APPROXIMATE DYNAMIC PROGRAMMING AND REINFORCEMENT LEARNING, P330, DOI 10.1109/ADPRL.2007.368207; Bennett J, 2007, THE NETFLIX PRIZE; Byrd Jonathon, 2019, ICML; Culotta A., 2010, ADV NEURAL INFORM PR, V23, P568; de Bruin Tim, 2015, IMPORTANCE EXPERIENC; Devlin J., 2018, P 2019 C N AM CHAPTE, P4171, DOI DOI 10.18653/V1/N19-1423DIEZPF; Espeholt Lasse, 2018, P INT C MACH LEARN I; Fu Justin, 2019, ARXIV190210250; Fujimoto S, 2018, PR MACH LEARN RES, V80; Fujimoto Scott, 2018, ARXIV181202900; Gao Yang, 2018, ICLR WORKSH; Gelada Carles, 2019, ABS190109455 CORR; Grau-Moya Jordi, 2019, INT C LEARN REPR; Gretton A, 2012, J MACH LEARN RES, V13, P723; Haarnoja T., 2018, P 35 INT C MACH LEAR; Jaques N., 2019, CORR; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kakade Sham, 2002, P 19 INT C MACH LEAR, P267; Kalashnikov D, 2018, P C ROB LEARN, P651; Laroche R., 2019, ICML; Levine S., 2018, CORR; Li K., 2009, CVPR09; Mahmood A. R., 2015, ARXIV150701569; Munos R., 2003, P 20 INT C MACH LEAR, V3, P560; Munos R, 2016, P 30 INT C NEUR INF; Munos R~mi, 2005, P NAT C ART INT; Precup Doina, 2001, INT C MACH LEARN ICM; Schaal Stefan, 1999, IS IMITATION LEARNIN; Schaul Tom, 2016, CORR; Scherrer B, 2015, J MACH LEARN RES, V16, P1629; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Sutton RS, 2018, ADAPT COMPUT MACH LE, P1; Todorov E, 2012, IEEE INT C INT ROBOT, P5026, DOI 10.1109/IROS.2012.6386109; Wu Yifan, 2019, ICML 2019; Yu Fisher, 2018, BDD100K DIVERSE DRIV, P6	38	31	31	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866903040
C	Vu, T; Jang, H; Pham, TX; Yoo, CD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vu, Thang; Jang, Hyunjun; Pham, Trung X.; Yoo, Chang D.			Cascade RPN: Delving into High-Quality Region Proposal Network with Adaptive Convolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					This paper considers an architecture referred to as Cascade Region Proposal Network (Cascade RPN) for improving the region-proposal quality and detection performance by systematically addressing the limitation of the conventional RPN that heuristically defines the anchors and aligns the features to the anchors. First, instead of using multiple anchors with predefined scales and aspect ratios, Cascade RPN relies on a single anchor per location and performs multi-stage refinement. Each stage is progressively more stringent in defining positive samples by starting out with an anchor-free metric followed by anchor-based metrics in the ensuing stages. Second, to attain alignment between the features and the anchors throughout the stages, adaptive convolution is proposed that takes the anchors in addition to the image features as its input and learns the sampled features guided by the anchors. A simple implementation of a two-stage Cascade RPN achieves AR 13.4 points higher than that of the conventional RPN, surpassing any existing region proposal methods. When adopting to Fast R-CNN and Faster R-CNN, Cascade RPN can improve the detection mAP by 3.1 and 3.5 points, respectively. The code is made publicly available at https://github.com/thangvubk/Cascade-RPN.	[Vu, Thang; Jang, Hyunjun; Pham, Trung X.; Yoo, Chang D.] Korea Adv Inst Sci & Technol, Dept Elect Engn, Daejeon, South Korea	Korea Advanced Institute of Science & Technology (KAIST)	Vu, T (corresponding author), Korea Adv Inst Sci & Technol, Dept Elect Engn, Daejeon, South Korea.	thangvubk@kaist.ac.kr; wiseholi@kaist.ac.kr; trungpx@kaist.ac.kr; cd_yoo@kaist.ac.kr		Pham Xuan, Trung/0000-0003-4177-7054	Institute for Information & communications Technology Planning & Evaluation(IITP) - Korea government (MSIT) [2017-0-01780, 2019-0-01396]	Institute for Information & communications Technology Planning & Evaluation(IITP) - Korea government (MSIT)(Institute for Information & Communication Technology Planning & Evaluation (IITP), Republic of KoreaMinistry of Science & ICT (MSIT), Republic of Korea)	This work was supported by Institute for Information & communications Technology Planning & Evaluation(IITP) grant funded by the Korea government (MSIT) (2017-0-01780, The technology development for event recognition/relational reasoning and learning knowledge-based system for video understanding) and (No. 2019-0-01396, Development of framework for analyzing, detecting, mitigating of bias in AI model and training data)	Alexe B., 2012, TPAMI; Ali Farhadi, 2018, Arxiv, DOI arXiv:1804.02767; Arbelaez P, 2014, PROC CVPR IEEE, P328, DOI 10.1109/CVPR.2014.49; Astua Carlos, 2014, SENSORS; Cai ZW, 2018, PROC CVPR IEEE, P6154, DOI 10.1109/CVPR.2018.00644; Carreira Joao, 2011, TPAMI; Chavali N, 2016, PROC CVPR IEEE, P835, DOI 10.1109/CVPR.2016.97; Chen K., 2019, ARXIV PREPRINT ARXIV; Chen K, 2019, PROC CVPR IEEE, P4969, DOI 10.1109/CVPR.2019.00511; Conte Donatello, 2005, INT C PATT REC IM AN; Dai JF, 2017, IEEE I CONF COMP VIS, P764, DOI 10.1109/ICCV.2017.89; Danielczuk M., 2018, ARXIV180905825; Fan H, 2019, P IEEECVF C COMPUTER, P5374; Furgale Paul, 2013, IEEE INT VEH S IV; Gidaris Spyros, 2016, ARXIV160604446; Girshick R., 2015, ICCV; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Hanene C, 2015, I C SCI TECH AUTO CO, P1, DOI 10.1109/STA.2015.7505095; He KM, 2017, IEEE I CONF COMP VIS, P2980, DOI [10.1109/ICCV.2017.322, 10.1109/TPAMI.2018.2844175]; Hosang J., 2014, BMVC; Hosang J., 2016, TPAMI; Li Hongyang, 2019, IJCV; Liao Wentong, 2017, ISPRS ANN PHOTOGRAMM; Lin T.-Y., 2017, PROC CVPR IEEE, P936, DOI [10.1109/CVPR.2017.106, DOI 10.1109/CVPR.2017.106]; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Lu Hsueh-Fu, 2018, ECCV; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pinheiro PO, 2016, LECT NOTES COMPUT SC, V9905, P75, DOI 10.1007/978-3-319-46448-0_5; Redmon J., 2016, IEEE C COMPUTER VISI, DOI [10.1109/CVPR.2017.690, DOI 10.1109/CVPR.2017.690]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Tian Z, 2019, IEEE I CONF COMP VIS, P9626, DOI 10.1109/ICCV.2019.00972; Wang JQ, 2019, PROC CVPR IEEE, P2960, DOI 10.1109/CVPR.2019.00308; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Yang B., 2016, CVPR; Yu F., 2016, P ICLR 2016; Yu Jiahui, 2016, 24 ACM INT C MULT; Zhong Q, 2017, ARXIV171010749; Zitnick CL, 2014, LECT NOTES COMPUT SC, V8693, P391, DOI 10.1007/978-3-319-10602-1_26	41	31	32	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301042
C	Zhang, M; Li, JJ; Ji, W; Piao, YR; Lu, HC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Zhang, Miao; Li, Jingjing; Ji, Wei; Piao, Yongri; Lu, Huchuan			Memory-oriented Decoder for Light Field Salient Object Detection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Light field data have been demonstrated in favor of many tasks in computer vision, but existing works about light field saliency detection still rely on hand-crafted features. In this paper, we present a deep-learning-based method where a novel memory-oriented decoder is tailored for light field saliency detection. Our goal is to deeply explore and comprehensively exploit internal correlation of focal slices for accurate prediction by designing feature fusion and integration mechanisms. The success of our method is demonstrated by achieving the state of the art on three datasets. We present this problem in a way that is accessible to members of the community and provide a large-scale light field dataset that facilitates comparisons across algorithms. The code and dataset are made publicly available at https://github.com/OIPLab-DUT/MoLF.	[Zhang, Miao; Li, Jingjing; Ji, Wei; Piao, Yongri; Lu, Huchuan] Dalian Univ Technol, Dalian, Peoples R China	Dalian University of Technology	Piao, YR (corresponding author), Dalian Univ Technol, Dalian, Peoples R China.	miaozhang@dlut.edu.cn; lijingjing@mail.dlut.edu.cn; jiwei521@mail.dlut.edu.cn; yrpiao@dlut.edu.cn; lhchuan@dlut.edu.cn	Li, Jing/GYU-5036-2022; Ji, Wei/AAK-2370-2021	Ji, Wei/0000-0003-4059-5902	National Natural Science Foundation of China [61605022, 61976035]; Fundamental Research Funds for the Central Universities [DUT19JC58]	National Natural Science Foundation of China(National Natural Science Foundation of China (NSFC)); Fundamental Research Funds for the Central Universities(Fundamental Research Funds for the Central Universities)	This work was supported by the National Natural Science Foundation of China (61605022 and 61976035) and the Fundamental Research Funds for the Central Universities (DUT19JC58). The authors are grateful to the reviewers for their suggestions in improving the quality of the paper.	Achanta R, 2009, PROC CVPR IEEE, P1597, DOI 10.1109/CVPRW.2009.5206596; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Borji A., 2014, ARXIV14115878; Borji A, 2012, LECT NOTES COMPUT SC, V7573, P414, DOI 10.1007/978-3-642-33709-3_30; Chen H, 2018, PROC CVPR IEEE, P3051, DOI 10.1109/CVPR.2018.00322; Chen H, 2019, IEEE T IMAGE PROCESS, V28, P2825, DOI 10.1109/TIP.2019.2891104; Chen H, 2019, PATTERN RECOGN, V86, P376, DOI 10.1016/j.patcog.2018.08.007; Craye C, 2016, IEEE INT CONF ROBOT, P2303, DOI 10.1109/ICRA.2016.7487379; Dai J, 2016, PROCEEDINGS 2016 IEEE INTERNATIONAL CONFERENCE ON INDUSTRIAL TECHNOLOGY (ICIT), P1796, DOI 10.1109/ICIT.2016.7475036; Deng ZJ, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P684; Fan DP, 2017, IEEE I CONF COMP VIS, P4558, DOI 10.1109/ICCV.2017.487; Fan DP, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P698; FENG D, 2016, PROC CVPR IEEE, P2343, DOI DOI 10.1109/CVPR.2016.257; Gao D, 2007, INT C NEUR INF PROC; Guo X., 2017, ARXIV171110729; Han JW, 2018, IEEE T CYBERNETICS, V48, P3171, DOI 10.1109/TCYB.2017.2761775; Harel J., 2006, PAPER PRESENTED INT, P545, DOI DOI 10.7551/MITPRESS/7503.003.0073; Hongyi Su, 2015, Intelligent Computing Theories and Methodologies. 11th International Conference, ICIC 2015. Proceedings: LNCS 9225, P1, DOI 10.1007/978-3-319-22180-9_1; Hou QB, 2019, IEEE T PATTERN ANAL, V41, P815, DOI 10.1109/TPAMI.2018.2815688; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Itti L, 1998, IEEE T PATTERN ANAL, V20, P1254, DOI 10.1109/34.730558; Ju R, 2014, IEEE IMAGE PROC, P1115, DOI 10.1109/ICIP.2014.7025222; Lee GY, 2016, PROC CVPR IEEE, P660, DOI 10.1109/CVPR.2016.78; Li GB, 2015, PROC CVPR IEEE, P5455, DOI 10.1109/CVPR.2015.7299184; Li NY, 2017, IEEE T PATTERN ANAL, V39, P1605, DOI 10.1109/TPAMI.2016.2610425; Li NY, 2015, PROC CVPR IEEE, P5216, DOI 10.1109/CVPR.2015.7299158; Li NY, 2014, PROC CVPR IEEE, P2806, DOI 10.1109/CVPR.2014.359; Li X, 2016, IEEE T IMAGE PROCESS, V25, P3919, DOI 10.1109/TIP.2016.2579306; Li X, 2018, LECT NOTES COMPUT SC, V11206, P287, DOI [10.1007/978-3-030-01216-8_18, 10.1007/978-3-030-01267-0_22]; Li XH, 2013, IEEE I CONF COMP VIS, P2976, DOI 10.1109/ICCV.2013.370; Li Y, 2014, PROC CVPR IEEE, P280, DOI 10.1109/CVPR.2014.43; Lin GS, 2017, PROC CVPR IEEE, P5168, DOI 10.1109/CVPR.2017.549; Liu NA, 2018, PROC CVPR IEEE, P3089, DOI 10.1109/CVPR.2018.00326; Liu NA, 2016, PROC CVPR IEEE, P678, DOI 10.1109/CVPR.2016.80; Luo ZM, 2017, PROC CVPR IEEE, P6593, DOI 10.1109/CVPR.2017.698; Min Chen, 2011, IEEE INFOCOM 2011 - IEEE Conference on Computer Communications. Workshops, P409, DOI 10.1109/INFCOMW.2011.5928847; Niu YZ, 2012, PROC CVPR IEEE, P454, DOI 10.1109/CVPR.2012.6247708; Peng HW, 2014, LECT NOTES COMPUT SC, V8691, P92, DOI 10.1007/978-3-319-10578-9_7; Perazzi F, 2012, PROC CVPR IEEE, P733, DOI 10.1109/CVPR.2012.6247743; Qin Y, 2015, PROC CVPR IEEE, P110, DOI 10.1109/CVPR.2015.7298606; Qu LQ, 2017, IEEE T IMAGE PROCESS, V26, P2274, DOI 10.1109/TIP.2017.2682981; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shao L, 2006, PATTERN RECOGN, V39, P1932, DOI 10.1016/j.patcog.2006.04.010; Shi XS, 2015, 2015 IEEE ADVANCED INFORMATION TECHNOLOGY, ELECTRONIC AND AUTOMATION CONTROL CONFERENCE (IAEAC), P802, DOI 10.1109/IAEAC.2015.7428667; Smeulders AWM, 2014, IEEE T PATTERN ANAL, V36, P1442, DOI 10.1109/TPAMI.2013.230; Song G, 2018, IEEE IMAGE PROC, P1563, DOI 10.1109/ICIP.2018.8451201; Tu WC, 2016, PROC CVPR IEEE, P2334, DOI 10.1109/CVPR.2016.256; Wang LJ, 2015, PROC CVPR IEEE, P3183, DOI 10.1109/CVPR.2015.7298938; Wang TC, 2016, LECT NOTES COMPUT SC, V9907, P121, DOI 10.1007/978-3-319-46487-9_8; Wang W, 2019, ARXIV190409146; Woo SH, 2018, LECT NOTES COMPUT SC, V11211, P3, DOI 10.1007/978-3-030-01234-2_1; Xie SN, 2015, IEEE I CONF COMP VIS, P1395, DOI 10.1109/ICCV.2015.164; Yeung HWF, 2019, IEEE T IMAGE PROCESS, V28, P2319, DOI 10.1109/TIP.2018.2885236; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang J, 2017, ACM T MULTIM COMPUT, V13, DOI 10.1145/3107956; Zhang J, 2015, PROCEEDINGS OF THE TWENTY-FOURTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE (IJCAI), P2212; Zhang PP, 2017, IEEE I CONF COMP VIS, P202, DOI 10.1109/ICCV.2017.31; Zhang XN, 2018, PROC CVPR IEEE, P714, DOI 10.1109/CVPR.2018.00081; Zhao R, 2015, PROC CVPR IEEE, P1265, DOI 10.1109/CVPR.2015.7298731; Zhou WH, 2018, INT C PATT RECOG, P2362, DOI 10.1109/ICPR.2018.8545490; Zhu CX, 2018, IEEE ICC; Zhu CB, 2017, IEEE INT CONF COMP V, P1509, DOI 10.1109/ICCVW.2017.178; Zhu H, 2019, PROC CVPR IEEE, P4486, DOI 10.1109/CVPR.2019.00462; Zhu WJ, 2014, PROC CVPR IEEE, P2814, DOI 10.1109/CVPR.2014.360	67	31	32	0	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300081
C	Lopez, R; Regier, J; Jordan, MI; Yosef, N		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lopez, Romain; Regier, Jeffrey; Jordan, Michael I.; Yosef, Nir			Information Constraints on Auto-Encoding Variational Bayes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Parameterizing the approximate posterior of a generative model with neural networks has become a common theme in recent machine learning research. While providing appealing flexibility, this approach makes it difficult to impose or assess structural constraints such as conditional independence. We propose a framework for learning representations that relies on auto-encoding variational Bayes, in which the search space is constrained via kernel-based measures of independence. In particular, our method employs the d-variable Hilbert-Schmidt Independence Criterion (dHSIC) to enforce independence between the latent representations and arbitrary nuisance factors. We show how this method can be applied to a range of problems, including problems that involve learning invariant and conditionally independent representations. We also present a full-fledged application to single-cell RNA sequencing (scRNA-seq). In this setting the biological signal is mixed in complex ways with sequencing errors and sampling effects. We show that our method outperforms the state-of-the-art approach in this domain.	[Lopez, Romain; Regier, Jeffrey; Jordan, Michael I.; Yosef, Nir] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA; [Jordan, Michael I.] Univ Calif Berkeley, Dept Stat, Berkeley, CA 94720 USA; [Yosef, Nir] Ragon Inst MGH MIT & Harvard, Cambridge, MA USA; [Yosef, Nir] Chan Zuckerberg Biohub, San Francisco, CA USA	University of California System; University of California Berkeley; University of California System; University of California Berkeley; Harvard University; Harvard Medical School; Massachusetts General Hospital; Massachusetts Institute of Technology (MIT); Ragon Institute	Lopez, R (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	romain_lopez@berkeley.edu; regier@berkeley.edu; jordan@cs.berkeley.edu; niryosef@berkeley.edu	Jordan, Michael I/C-5253-2013	Regier, Jeffrey/0000-0002-1472-5235; Jordan, Michael/0000-0001-8935-817X	NIH-NIAID [U19 AI090023]	NIH-NIAID(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Institute of Allergy & Infectious Diseases (NIAID))	NY and RL were supported by grant U19 AI090023 from NIH-NIAID.	[Anonymous], 2010, P 13 INT C ART INT S; Ba J., 2017, P 3 INT C LEARN REPR; Buettner F, 2015, NAT BIOTECHNOL, V33, P155, DOI 10.1038/nbt.3102; Burda Yuri, 2016, INT C LEARN REPR; Burgess Christopher P, 2017, LEARNING DISENTANGLE; Chen JB, 2018, PR MACH LEARN RES, V80; Chen Tian Qi, 2018, INT C LEARN REPR WOR; Chen X, 2016, ADV NEUR IN, V29; Cole Michael B, 2017, BIORXIV; DURRIEU JL, 2012, IEEE INT C AC SPEECH, P4833; Finak G, 2015, GENOME BIOL, V16, DOI 10.1186/s13059-015-0844-5; Flaxman  Seth, 2016, P 32 C UNC ART INT; FUKUMIZU K, 2008, ADV NEURAL INFORM PR, V20, P489; Gelman A, 2007, DATA ANAL USING REGR; Georghiades AS, 2001, IEEE T PATTERN ANAL, V23, P643, DOI 10.1109/34.927464; Gretton A, 2005, LECT NOTES ARTIF INT, V3734, P63; Gretton A., 2008, ADV NEURAL INFORM PR, P585; Gretton A, 2012, J MACH LEARN RES, V13, P723; Grun D, 2014, NAT METHODS, V11, P637, DOI [10.1038/NMETH.2930, 10.1038/nmeth.2930]; Higgins I., 2017, ICLR, P1; Hoffman Matthew D, 2016, ADV APPROXIMATE BAYE; Jitkrittum W., 2017, INT C MACH LEARN ICM, V70, P1742; Kim Hyunjik, 2017, LEARNING DISENTANGLE; Kingma DP, 2014, ADV NEUR IN, P3581, DOI DOI 10.5555/2969033.2969226; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Klein AM, 2015, CELL, V161, P1187, DOI 10.1016/j.cell.2015.04.044; Lee KC, 2005, IEEE T PATTERN ANAL, V27, P684, DOI 10.1109/TPAMI.2005.92; Li QH, 2011, ANN APPL STAT, V5, P1752, DOI 10.1214/11-AOAS466; Lopez  R., 2018, BIORXIV; Louizos Christos, 2016, INT C LEARN REPR; Love MI, 2014, GENOME BIOL, V15, DOI 10.1186/s13059-014-0550-8; Macosko  Evan, 2017, CELL, V161, P1202; Mairal J., 2009, ADV NEURAL INFORM PR, P1033; Makhzani Alireza, 2016, INT C LEARN REPR WOR; Nakaya HI, 2011, NAT IMMUNOL, V12, P786, DOI 10.1038/ni.2067; Perez-<prime>Suay A., 2018, APPL SOFT COMPUTING; Pfister N, 2018, J R STAT SOC B, V80, P5, DOI 10.1111/rssb.12235; Risso D, 2018, NAT COMMUN, V9, DOI 10.1038/s41467-017-02554-5; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Szabo  Zoltan, 2018, J MACHINE LEARNING R, V18, P1; Tanay A, 2017, NATURE, V541, P331, DOI 10.1038/nature21350; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wagner A, 2016, NAT BIOTECHNOL, V34, P1145, DOI 10.1038/nbt.3711; Wang B, 2017, NAT METHODS, V14, P414, DOI 10.1038/nmeth.4207; WATANABE S, 1960, IBM J RES DEV, V4, P66, DOI 10.1147/rd.41.0066; Zheng GXY, 2017, NAT COMMUN, V8, DOI 10.1038/ncomms14049	47	31	32	6	29	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852000060
C	Trask, A; Hill, F; Reed, S; Rae, J; Dyer, C; Blunsom, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Trask, Andrew; Hill, Felix; Reed, Scott; Rae, Jack; Dyer, Chris; Blunsom, Phil			Neural Arithmetic Logic Units	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				NETWORKS	Neural networks can learn to represent and manipulate numerical information, but they seldom generalize well outside of the range of numerical values encountered during training. To encourage more systematic numerical extrapolation, we propose an architecture that represents numerical quantities as linear activations which are manipulated using primitive arithmetic operators, controlled by learned gates. We call this module a neural arithmetic logic unit (NALU), by analogy to the arithmetic logic unit in traditional processors. Experiments show that NALU-enhanced neural networks can learn to track time, perform arithmetic over images of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in images In contrast to conventional architectures, we obtain substantially better generalization both inside and outside of the range of numerical values encountered during training, often extrapolating orders of magnitude beyond trained numerical ranges.	[Trask, Andrew; Hill, Felix; Reed, Scott; Rae, Jack; Dyer, Chris; Blunsom, Phil] DeepMind, London, England; [Trask, Andrew; Blunsom, Phil] Univ Oxford, Oxford, England; [Rae, Jack] UCL, London, England	University of Oxford; University of London; University College London	Trask, A (corresponding author), DeepMind, London, England.; Trask, A (corresponding author), Univ Oxford, Oxford, England.	atrask@google.com; felixhill@google.com; reedscot@google.com; jwrae@google.com; cdyer@google.com; pblunsom@google.com						Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Arteta C, 2014, LECT NOTES COMPUT SC, V8691, P504, DOI 10.1007/978-3-319-10578-9_33; Brunton SL, 2016, P NATL ACAD SCI USA, V113, P3932, DOI 10.1073/pnas.1517384113; Chan AB, 2008, PROC CVPR IEEE, P1766, DOI 10.1109/cvpr.2008.4587569; Dehaene S., 2011, SPACE TIME NUMBER BR, DOI 10.1016/C2010-0-66570-9; FODOR JA, 1988, COGNITION, V28, P3, DOI 10.1016/0010-0277(88)90031-5; Gallistel C. Randy, 2017, PHILOS T ROYAL SOC B, V373; Gelman R., 1978, CHILDS UNDERSTANDING; Gers FA, 2001, IEEE T NEURAL NETWOR, V12, P1333, DOI 10.1109/72.963769; Graves A., 2014, ARXIV14105401; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Grefenstette Edward, 2015, P NIPS; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang Gao, 2016, P ECCV; Jozefowicz Rafal, 2016, ARXIV160202410; Krizhevsky A., 2010, UNPUB; Kumar A, 2016, PR MACH LEARN RES, V48; Lake Brendan, 2018, P ICML; Marcus Gary, 2003, ALGEBRAIC MIND INTEG; Mikolov T., 2015, ARXIV150205698, V1502, P05698; Mnih V, 2016, PR MACH LEARN RES, V48; Piazza M, 2004, NEURON, V44, P547, DOI 10.1016/j.neuron.2004.10.014; Reed S., 2016, INT C LEARN REPR; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Segui Santi, 2015, CORR; Srivatsan R, 2015, SEVA, SAVIOUR AND STATE: CASTE POLITICS, TRIBAL WELFARE AND CAPITALIST DEVELOPMENT, P1; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Weiss Gail, 2018, P ACL; Weston Jason, 2015, P ICLR; Xie WD, 2018, COMP M BIO BIO E-IV, V6, P283, DOI 10.1080/21681163.2016.1149104; Zaremba W, 2014, CORR; Zhang C, 2015, PROC CVPR IEEE, P833, DOI 10.1109/CVPR.2015.7298684	33	31	31	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852002057
C	Alvarez, JM; Salzmann, M		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Alvarez, Jose M.; Salzmann, Mathieu			Compression-aware Training of Deep Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					In recent years, great progress has been made in a variety of application domains thanks to the development of increasingly deeper neural networks. Unfortunately, the huge number of units of these networks makes them expensive both computationally and memory-wise. To overcome this, exploiting the fact that deep networks are over-parametrized, several compression strategies have been proposed. These methods, however, typically start from a network that has been trained in a standard manner, without considering such a future compression. In this paper, we propose to explicitly account for compression in the training process. To this end, we introduce a regularizer that encourages the parameter matrix of each layer to have low rank during training We show that accounting for compression during training allows us to learn much more compact, yet at least as effective, models than state-of-the-art compression techniques.	[Alvarez, Jose M.] Toyota Res Inst, Los Altos, CA 94022 USA; [Salzmann, Mathieu] Ecole Polytech Fed Lausanne, CVLab, Lausanne, Switzerland	Toyota Motor Corporation; Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Alvarez, JM (corresponding author), Toyota Res Inst, Los Altos, CA 94022 USA.	jose.alvarez@tri.global; mathieu.salzmann@epfl.ch	Jeong, Yongwook/N-7413-2016	Salzmann, Mathieu/0000-0002-8347-8637				Alvarez J. M., 2016, CORR; [Anonymous], 2016, NIPS; Ba L. J., 2016, CORRABS160706450; Babaeizadeh M, 2016, EMDNN NIPS WORKSH; Bengio Y., 2009, ADV NEURAL INFORM PR, V22, P99; Cai JF, 2010, SIAM J OPTIMIZ, V20, P1956, DOI 10.1137/080738970; Candes E. J., 2008, CORR; Clevert Djork-Arne, 2015, CORR; Cogswell M., 2016, ICLR; Collins M. D., 2014, CORR; Collobert R., 2011, BIGLEARN NIPS WORKSH; Courbariaux M., 2016, ABS160202830 CORR; Denil Misha, 2013, ADV NEURAL INFORM PR; Denton E., 2014, NIPS; Duchi John, 2010, UCBEECS201024; Graves Alex, 2011, NIPS; Gupta S, 2015, CORR; Han S., 2016, ICLR; Han S., 2015, NIPS; Harandi Mehrtash, 2016, CORR; Hassibi B., 1993, ICNN; He K., 2015, CORR; Hinton V. O., 2014, DISTILLING KNOWLEDGE; Ioannou Y., 2015, CORR; Ioffe S., 2015, P 32 INT C MACH LEAR, P448; Jaderberg M., 2014, ECCV; Jaderberg M., 2014, BRIT MACH VIS C; Ji CY, 1990, NEURAL COMPUT, V2, P188, DOI 10.1162/neco.1990.2.2.188; Kingma D.P., 2015, 3 INT C LEARN REPR I, P1, DOI DOI 10.1007/S11390-017-1754-7; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Lebedev Vadim, 2014, CORR; LeCun Y., 1990, NIPS; Liu B., 2015, CVPR; Molchanov P., 2016, CORR; Mozer M., 1988, NIPS; Pan H., 2016, CORR; Richard E., 2012, ICML; Rodriguez Pau, 2017, ICLR; Russakovsky O., 2014, CORR; Salimans T., 2016, CORR; Simon Noah, 2013, J COMPUTATIONAL GRAP; Simonyan K., 2014, 3 INT C LEARN REPR I; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Szegedy C., 2015, P IEEE C COMP VIS PA, P1, DOI DOI 10.1109/CVPR.2015.7298594; Tai Cheng, 2015, CORR; Ullrich K., 2016, CORR; Warde-Farley D., 2013, ICML; Weigend A. S., 1991, NIPS; Wen W., 2017, CORR; Wen W., 2016, NIPS; Xiong W, 2016, IEEE DATA MINING, P519, DOI [10.1109/ICDM.2016.0063, 10.1109/ICDM.2016.66]; Zeiler Matthew D., 2012, CORR; ZHANG S, 2015, CORR; Zhou H., 2006, ECCV	55	31	31	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400082
C	Fu, J; Co-Reyes, JD; Levine, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Fu, Justin; Co-Reyes, John D.; Levine, Sergey			EX2: Exploration with Exemplar Models for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Deep reinforcement learning algorithms have been shown to learn complex tasks using highly general policy classes. However, sparse reward problems remain a significant challenge. Exploration methods based on novelty detection have been particularly successful in such settings but typically require generative or predictive models of the observations, which can be difficult to train when the observations are very high-dimensional and complex, as in the case of raw images. We propose a novelty detection algorithm for exploration that is based entirely on discriminatively trained exemplar models, where classifiers are trained to discriminate each visited state against all others. Intuitively, novel states are easier to distinguish against other states seen during training. We show that this kind of discriminative modeling corresponds to implicit density estimation, and that it can be combined with count-based exploration to produce competitive results on a range of popular benchmark tasks, including state-of-the-art results on challenging egocentric observations in the vizDoom benchmark.	[Fu, Justin; Co-Reyes, John D.; Levine, Sergey] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Fu, J (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	justinfu@eecs.berkeley.edu; jcoreyes@eecs.berkeley.edu; sylevine@eecs.berkeley.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1700696, IIS-1614653]; ONR Young Investigator Program award; Berkeley DeepDrive	NSF(National Science Foundation (NSF)); ONR Young Investigator Program award; Berkeley DeepDrive	We would like to thank Adam Stooke, Sandy Huang, and Haoran Tang for providing efficient and parallelizable policy search code. We thank Joshua Achiam for help with setting up benchmark tasks. This research was supported by NSF IIS-1614653, NSF IIS-1700696, an ONR Young Investigator Program award, and Berkeley DeepDrive.	Abel David, 2016, ADV NEURAL INFORM PR; Achiam J., 2017, ARXIV170301732; Barto Andrew G., 2003, DISCRETE EVENT DYNAM, V13; Bellemare M., 2016, NEURIPS; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Brafman RI, 2003, J MACH LEARN RES, V3, P213, DOI 10.1162/153244303765208377; Bubeck S, 2012, FOUND TRENDS MACH LE, V5, P1, DOI 10.1561/2200000024; Chentanez N., 2005, ADV NEURAL INFORM PR; Duan Y, 2016, PR MACH LEARN RES, V48; Florensa C., 2017, PROC INT C MACH LEAR, P1; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Heess N., 2016, LEARNING TRANSFER MO; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Kakade Sham M., 2003, INT C MACH LEARN ICM; Kearns M, 2002, MACH LEARN, V49, P209, DOI 10.1023/A:1017984413808; Kolter J. Z., 2009, INT C MACH LEARN ICM; Kulkarni T. D., 2016, PROC 30 INT C NEURAL, P3675; Li L., 2011, ADV NEURAL INFORM PR, P2249, DOI DOI 10.5555/2986459.2986710; Lillicrap T.P., 2015, CONTINUOUS CONTROL D, DOI DOI 10.1561/2200000006; Malisiewicz T., 2011, INT C COMP VIS ICCV; Mathieu Michael, 2015, ARXIV151105440; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Pathak D, 2017, PR MACH LEARN RES, V70; Pazis Jason, 2013, AAAI C ART INT AAAI; Salimans T, 2016, ADV NEUR IN, V29; Schmidhuber Jurgen, 1990, P 1 INT C SIM AD BEH; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Stadie B. C., 2015, ARXIV150700814; Stolle Martin, 2002, LEARNING OPTIONS REI, DOI [10.1007/3-540-45622-8_16, DOI 10.1007/3-540-45622-8_16]; Strehl Alexander L., 2009, J COMPUTER SYSTEM SC; Tang HL, 2017, INT CONF MEAS, P1, DOI [10.1109/ICMTMA.2017.0009, 10.1109/ICMTMA.2017.8]	32	31	33	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402061
C	Bastani, O; Ioannou, Y; Lampropoulos, L; Vytiniotis, D; Nori, AV; Criminisi, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bastani, Osbert; Ioannou, Yani; Lampropoulos, Leonidas; Vytiniotis, Dimitrios; Nori, Aditya, V; Criminisi, Antonio			Measuring Neural Net Robustness with Constraints	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Despite having high accuracy, neural nets have been shown to be susceptible to adversarial examples, where a small perturbation to an input can cause it to become mislabeled. We propose metrics for measuring the robustness of a neural net and devise a novel algorithm for approximating these metrics based on an encoding of robustness as a linear program. We show how our metrics can be used to evaluate the robustness of deep neural nets with experiments on the MNIST and CIFAR-10 datasets. Our algorithm generates more informative estimates of robustness metrics compared to estimates based on existing algorithms. Furthermore, we show how existing approaches to improving robustness "overfit" to adversarial examples generated using a specific algorithm. Finally, we show that our techniques can be used to additionally improve neural net robustness both according to the metrics that we propose, but also according to previously proposed metrics.	[Bastani, Osbert] Stanford Univ, Stanford, CA 94305 USA; [Ioannou, Yani] Univ Cambridge, Cambridge, England; [Lampropoulos, Leonidas] Univ Penn, Philadelphia, PA 19104 USA; [Vytiniotis, Dimitrios; Nori, Aditya, V; Criminisi, Antonio] Microsoft Res, Redmond, WA USA	Stanford University; University of Cambridge; University of Pennsylvania; Microsoft	Bastani, O (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	obastani@cs.stanford.edu; yai20@cam.ac.uk; llamp@seas.upenn.edu; dimitris@microsoft.com; adityan@microsoft.com; antcrim@microsoft.com	Ioannou, Yani Andrew/AAL-6029-2021	Ioannou, Yani Andrew/0000-0002-9797-5888				Nguyen A, 2015, PROC CVPR IEEE, P427, DOI 10.1109/CVPR.2015.7298640; [Anonymous], 2014, DEEP NEURAL NETWORK; Chalupka K., 2015, VISUAL CAUSAL FEATUR; Fawzi A., 2015, ARXIV E PRINTS; Feng J., 2016, ARXIV160202389; Globerson Amir, 2006, P 23 INT C MACH LEAR, P353, DOI DOI 10.1145/1143844.1143889; Goodfellow I. J., 2015, ICLR; Huang R., 2015, CORR; Jia YQ, 2014, PROCEEDINGS OF THE 2014 ACM CONFERENCE ON MULTIMEDIA (MM'14), P675, DOI 10.1145/2647868.2654889; Krizhevsky A., 2009, TR2009 U TOR DEP COM, P32; Krizhevsky A., 2012, PROC INT C NEURAL IN, P1106; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; LeCun Y., 2001, INTELLIGENT SIGNAL P, P306; Lin M., 2013, CORR; Miyato Takeru, 2015, STAT, V1050, P25; Montufar G. F., 2014, ADV NEURAL INFORM PR, P2924; Moosavi Dezfooli S. M., 2016, P 2016 IEEE C COMP V; Negahban Sahand, 2015, ARXIV PREPRINT ARXIV; Papernot N., 2016, ARXIV160202697; Sabour S., 2015, ARXIV151105122; Szegedy C, 2014, INTRIGUING PROPERTIE, P6; Tabacof Pedro, 2015, CORR; Xu H, 2012, MACH LEARN, V86, P391, DOI 10.1007/s10994-011-5268-1	23	31	31	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973702086
C	Orabona, F		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Orabona, Francesco			Simultaneous Model Selection and Optimization through Parameter-free Stochastic Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA				ONLINE; REGULARIZATION; CLASSIFICATION; CLASSIFIERS	Stochastic gradient descent algorithms for training linear and kernel predictors are gaining more and more importance, thanks to their scalability. While various methods have been proposed to speed up their convergence, the model selection phase is often ignored. In fact, in theoretical works most of the time assumptions are made, for example, on the prior knowledge of the norm of the optimal solution, while in the practical world validation methods remain the only viable approach. In this paper, we propose a new kernel-based stochastic gradient descent algorithm that performs model selection while training, with no parameters to tune, nor any form of cross-validation. The algorithm builds on recent advancement in online learning theory for unconstrained settings, to estimate over time the right regularization in a data-dependent way. Optimal rates of convergence are proved under standard smoothness assumptions on the target function as well as preliminary empirical results.	[Orabona, Francesco] Yahoo Labs, New York, NY 10036 USA		Orabona, F (corresponding author), Yahoo Labs, New York, NY 10036 USA.	francesco@orabona.com						Auer P, 2002, J COMPUT SYST SCI, V64, P48, DOI 10.1006/jcss.2001.1795; Bach F., 2013, ADV NEURAL INFORM PR, V26, P773; Bartlett PL, 2006, J AM STAT ASSOC, V101, P138, DOI 10.1198/016214505000000907; Blanchard G, 2008, ANN STAT, V36, P489, DOI 10.1214/009053607000000839; Bousquet O., 2008, ADV NEURAL INFORM PR, P161, DOI DOI 10.7751/mitpress/8996.003.0015; Caponnetto A, 2007, FOUND COMPUT MATH, V7, P331, DOI [10.1007/s10208-006-0196-8, 10.1007/S10208-006-0196-8]; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Cesa-Bianchi Nicolo, 2006, PREDICTION LEARNING; Chang CC, 2011, ACM T INTEL SYST TEC, V2, DOI 10.1145/1961189.1961199; Chaudhuri K., 2009, ADV NEURAL INFORM PR; Chen DR, 2004, J MACH LEARN RES, V5, P1143; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Luo H., 2014, ADV NEURAL INFORM PR; McMahan H. B., 2014, COLT; Mendelson S, 2010, ANN STAT, V38, P526, DOI 10.1214/09-AOS728; Orabona F., 2014, ARXIV14063816; Orabona F., 2013, ADV NEURAL INFORM PR, P1806; ROBBINS H, 1951, ANN MATH STAT, V22, P400, DOI 10.1214/aoms/1177729586; ROSASCO L., 2014, ARXIV14050042; ROSENBLATT F, 1958, PSYCHOL REV, V65, P386, DOI 10.1037/h0042519; SHALEV- SHWARTZ S., 2007, P 24 INT C MACH LEAR, P807, DOI [DOI 10.1145/1273496.1273598, 10.1145/1273496.1273598]; Smale S, 2006, FOUND COMPUT MATH, V6, P145, DOI 10.1007/s10208-004-0160-z; Srebro N., 2010, ADV NEURAL INFORM PR, P2199; Streeter M., 2012, ADV NEURAL INFORM PR; Tarres P., ARXIV11035538; Tsybakov AB, 2004, ANN STAT, V32, P135; Yao Y, 2007, CONSTR APPROX, V26, P289, DOI 10.1007/s00365-006-0663-2; Yao YA, 2010, IEEE T INFORM THEORY, V56, P6470, DOI 10.1109/TIT.2010.2079010; Ying YM, 2008, FOUND COMPUT MATH, V8, P561, DOI 10.1007/s10208-006-0237-y; Ying YM, 2006, IEEE T INFORM THEORY, V52, P4775, DOI 10.1109/TIT.2006.883632; Zhang T., 2004, P 21 INT C MACH LEAR, P116, DOI 10.1145/1015330.1015332; Zinkevich, 2003, P 20 INT C MACH LEAR, P928	36	31	31	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647102064
C	Wipf, D; Palmer, J; Rao, B		Thrun, S; Saul, K; Scholkopf, B		Wipf, D; Palmer, J; Rao, B			Perspectives on sparse Bayesian learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					Recently, relevance vector machines (RVM) have been fashioned from a sparse Bayesian learning (SBL) framework to perform supervised learning using a weight prior that encourages sparsity of representation. The methodology incorporates an additional set of hyperparameters governing the prior, one for each weight, and then adopts a specific approximation to the full marginalization over all weights and hyperparameters. Despite its empirical success however, no rigorous motivation for this particular approximation is currently available. To address this issue, we demonstrate that SBL can be recast as the application of a rigorous variational approximation to the full model by expressing the prior in a dual form. This formulation obviates the necessity of assuming any hyperpriors and leads to natural, intuitive explanations of why sparsity is achieved in practice.	Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92092 USA	University of California System; University of California San Diego	Wipf, D (corresponding author), Univ Calif San Diego, Dept Elect & Comp Engn, San Diego, CA 92092 USA.			Wipf, David/0000-0002-2768-4540				Bishop C.M., 2000, 16 C UNCERTAINTY ART, P46; Duda R.O., 2001, PATTERN CLASSIFICATI, V20; Faul AC, 2002, ADV NEUR IN, V14, P383; Girolami M, 2001, NEURAL COMPUT, V13, P2517, DOI 10.1162/089976601753196003; Jordan MI, 1999, MACH LEARN, V37, P183, DOI 10.1023/A:1007665907178; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Tipping ME, 2001, J MACH LEARN RES, V1, P211, DOI 10.1162/15324430152748236	7	31	31	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						249	256						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500032
C	Figueiredo, MAT		Dietterich, TG; Becker, S; Ghahramani, Z		Figueiredo, MAT			Adaptive sparseness using Jeffreys Prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				GAUSSIAN-PROCESSES; APPROXIMATION; SELECTION	In this paper we introduce a new sparseness inducing prior which does not involve any (hyper)parameters that need to be adjusted or estimated. Although other applications are possible, we focus here on supervised learning problems: regression and classification. Experiments with several publicly available benchmark data sets show that the proposed approach yields state-of-the-art performance. In particular, our method outperforms support vector machines and performs competitively with the best alternative techniques, both in terms of error rates and sparseness, although it involves no tuning or adjusting of sparseness-controlling hyper-parameters.	Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal	Instituto de Telecomunicacoes; Universidade de Lisboa; Instituto Superior Tecnico	Figueiredo, MAT (corresponding author), Univ Tecn Lisboa, Inst Telecommun, Inst Super Tecn, P-1049001 Lisbon, Portugal.	mtf@lx.it.pt	Figueiredo, Mario/C-5428-2008	Figueiredo, Mario/0000-0002-0970-7745				ALBERT JH, 1993, J AM STAT ASSOC, V88, P669, DOI 10.2307/2290350; Berger J.O., 1980, STAT DECISION THEORY; Bishop, 1995, NEURAL NETWORKS PATT; Bishop C.M., 2000, 16 C UNCERTAINTY ART, P46; Chen SSB, 1998, SIAM J SCI COMPUT, V20, P33, DOI 10.1137/S1064827596304010; Cherkassky V. S., 1998, LEARNING DATA CONCEP, V1st; Cristianini N., 2000, INTRO SUPPORT VECTOR, DOI [10.1017/CBO9780511801389, DOI 10.1017/CBO9780511801389]; DONOHO DL, 1994, BIOMETRIKA, V81, P425, DOI 10.1093/biomet/81.3.425; Figueiredo MAT, 2001, IEEE T IMAGE PROCESS, V10, P1322, DOI 10.1109/83.941856; HOERL AE, 1970, TECHNOMETRICS, V12, P55, DOI 10.1080/00401706.1970.10488634; KIMELDOR.GS, 1970, ANN MATH STAT, V41, P495, DOI 10.1214/aoms/1177697089; Lange K., 1993, J COMPUT GRAPH STAT, V2, P175, DOI DOI 10.2307/1390698; MacKay DJC, 1996, FUND THEOR, V62, P221; McCullagh P., 1989, GENERALIZED LINEAR M, DOI DOI 10.1007/978-1-4899-3242-6; Neal RM., 1996, BAYESIAN LEARNING NE, P29; Osborne MR, 2000, IMA J NUMER ANAL, V20, P389, DOI 10.1093/imanum/20.3.389; POGGIO T, 1990, P IEEE, V78, P1481, DOI 10.1109/5.58326; Poggio T, 1998, NEURAL COMPUT, V10, P1445, DOI 10.1162/089976698300017250; Ripley BD., 1996; Seeger M, 2000, ADV NEUR IN, V12, P603; Tibshirani R., 2018, J R STAT SOC B, V58, P267; Tipping ME, 2000, ADV NEUR IN, V12, P652; Vapnik V.N, 1998, STAT LEARNING THEORY; WILLIAMS C, 1998, LEARNING INFERENCE G; Williams CKI, 1998, IEEE T PATTERN ANAL, V20, P1342, DOI 10.1109/34.735807; WILLIAMS CKI, 2001, NIPS, V13; WILLIAMS PM, 1995, NEURAL COMPUT, V7, P117, DOI 10.1162/neco.1995.7.1.117	27	31	31	0	4	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						697	704						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100087
C	Heisele, B; Serre, T; Pontil, M; Vetter, T; Poggio, T		Dietterich, TG; Becker, S; Ghahramani, Z		Heisele, B; Serre, T; Pontil, M; Vetter, T; Poggio, T			Categorization by learning and combining object parts	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					We describe an algorithm for automatically learning discriminative components of objects with SVM classifiers. It is based on growing image parts by minimizing theoretical bounds on the error probability of an SVM. Component-based face classifiers are then combined in a second stage to yield a hierarchical SVM classifier. Experimental results in face classification show considerable robustness against rotations in depth and suggest performance at significantly better level than other face detection systems. Novel aspects of our approach are: a) an algorithm to learn component-based classification experts and their combination, b) the use of 3-D morphable models for training, and c) a maximum operation on the output of each component classifier which may be relevant for biological models of visual recognition.	MIT, Ctr Biol & Computat Learning, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT)	Heisele, B (corresponding author), MIT, Ctr Biol & Computat Learning, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	heisele@ai.mit.edu; serre@ai.mit.edu; pontil@ing.unisi.it; vetter@informatik.uni-freiburg.de; tp@ai.mit.edu						Heisele B., 2001, P IEEE C COMP VIS PA; Heisele B., 2000, 1687 AI MIT CTR BIOL; HEISELE B, 2001, P 8 INT C COMP VIS V; LEUNG TK, 1995, FIFTH INTERNATIONAL CONFERENCE ON COMPUTER VISION, PROCEEDINGS, P637, DOI 10.1109/ICCV.1995.466878; Mohan A, 2001, IEEE T PATTERN ANAL, V23, P349, DOI 10.1109/34.917571; Papageorgiou C, 2000, INT J COMPUT VISION, V38, P15, DOI 10.1023/A:1008162616689; POGGIO T, 1990, NATURE, V343, P263, DOI 10.1038/343263a0; Riesenhuber M, 1999, NAT NEUROSCI, V2, P1019, DOI 10.1038/14819; Rikert T. D., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1046, DOI 10.1109/ICCV.1999.790386; ROWLEY H, 1997, CMUCS97201; Schneiderman H, 2000, PROC CVPR IEEE, P746, DOI 10.1109/CVPR.2000.855895; SIM T, 2001, 0102 CMU; SUNG KK, 1996, THESIS MIT CTR BIOL; VAILLANT R, 1993, IEE CONF PUBL, V372, P26; Vapnik V.N, 1998, STAT LEARNING THEORY; Vetter T, 1998, INT J COMPUT VISION, V28, P103, DOI 10.1023/A:1008058932445	16	31	32	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						1239	1245						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100154
C	Hutter, M		Dietterich, TG; Becker, S; Ghahramani, Z		Hutter, M			Distribution of mutual information	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	Advances in Neural Information Processing Systems		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA					The mutual information of two random variables and 3 with joint probabilities {pi(ij)} is commonly used in learning Bayesian nets as well as in many other fields. The chances pi(ij) are usually estimated by the empirical sampling frequency n(ij)/n leading to a point estimate I(n(ij)/n) for the mutual information. To answer questions like "is I(n(ij)/n) consistent with zero?" or "what is the probability that the true mutual information is much larger than the point estimate?" one has to go beyond the point estimate. In the Bayesian framework one can answer these questions by utilizing a (second order) prior distribution p(pi) comprising prior information about pi. From the prior p(pi) one can compute the posterior p(pi\n), from which the distribution p(I\n) of the mutual information can be calculated. We derive reliable and quickly computable approximations for p(I\n). We concentrate on the mean, variance, skewness, and kurtosis, and non-informative priors. For the mean we also give an exact expression. Numerical issues and the range of validity are discussed.	IDSIA, CH-6928 Lugano, Switzerland	Universita della Svizzera Italiana	Hutter, M (corresponding author), IDSIA, Galleria 2, CH-6928 Lugano, Switzerland.	marcus@idsia.ch		Hutter, Marcus/0000-0002-3263-4097				[Anonymous], 1974, HDB MATH FUNCTIONS; Brand M, 1999, NEURAL COMPUT, V11, P1155, DOI 10.1162/089976699300016395; Buntine W, 1996, IEEE T KNOWL DATA EN, V8, P195, DOI 10.1109/69.494161; Cover T.M., 2006, ELEMENTS INFORM THEO, DOI [10.1002/047174882X, DOI 10.1002/047174882X]; Gelman A, 2013, BAYESIAN DATA ANAL, P16; Heckerman D, 1998, NATO ADV SCI I D-BEH, V89, P301; Kleiter G. D., 1999, Soft Computing, V3, P162, DOI 10.1007/s005000050065; KLEITER GD, 1996, P 6 INT C INF PROC M, P985; Press W., 1992, NUMERICAL RECIPES C, VSecond edition.; Soofi ES, 2000, J AM STAT ASSOC, V95, P1349, DOI 10.2307/2669786; WOLF DR, 1993, LANLLAUR93833	11	31	31	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						399	406						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100050
C	Attias, H; Platt, JC; Acero, A; Deng, L		Leen, TK; Dietterich, TG; Tresp, V		Attias, H; Platt, JC; Acero, A; Deng, L			Speech denoising and dereverberation using probabilistic models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					This paper presents a unified probabilistic framework for denoising and dereverberation of speech signals. The framework transforms the denoising and dereverberation problems into Bayes-optimal signal estimation. The key idea is to use a strong speech model that is pre-trained on a large data set of clean speech. Computational efficiency is achieved by using variational EM, working in the frequency domain, and employing conjugate priors. The framework covers both single and multiple microphones. We apply this approach to noisy reverberant speech signals and get results substantially better than standard methods.	Microsoft Corp, Redmond, WA 98052 USA	Microsoft	Attias, H (corresponding author), Microsoft Corp, 1 Microsoft Way, Redmond, WA 98052 USA.	hagaia@microsoft.com; jplatt@microsoft.com; alexac@microsoft.com; deng@microsoft.com	Platt, John/GOH-2678-2022					Attias H, 2000, ADV NEUR IN, V12, P209; ATTIAS H, 2001, MSRTR200102; Brandstein MS, 1998, INT CONF ACOUST SPEE, P3613, DOI 10.1109/ICASSP.1998.679662; Cardoso JF, 1997, IEEE SIGNAL PROC LET, V4, P112, DOI 10.1109/97.566704; DEMBO A, 1988, IEEE T ACOUST SPEECH, V36, P471, DOI 10.1109/29.1551; Deng L., 2000, 6 INT C SPOK LANG PR; EPHRAIM Y, 1992, P IEEE, V80, P1526, DOI 10.1109/5.168664; Platt J. C., 1992, ADV NEUR IN, P730; Saul LK, 1996, J ARTIF INTELL RES, V4, P61, DOI 10.1613/jair.251	9	31	33	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						758	764						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800107
C	Dayan, P; Kakade, S		Leen, TK; Dietterich, TG; Tresp, V		Dayan, P; Kakade, S			Explaining away in weight space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO				BACKWARD BLOCKING; JUDGMENT	Explaining away has mostly been considered in terms of inference of states in belief networks. We show how it can also arise in a Bayesian context in inference about the weights governing relationships such as those between stimuli and reinforcers in conditioning experiments such as backward blocking. We show how explaining away in weight space can be accounted for using an extension of a Kalman filter model; provide a new approximate way of looking at the Kalman gain matrix as a whitener for the correlation matrix of the observation process; suggest a network implementation of this whitener using an architecture due to Goodall; and show that the resulting model exhibits backward blocking.	Univ Coll London, Gatsby Computat Neurosci Unit, London WC1N 3AR, England	University of London; University College London	Dayan, P (corresponding author), Univ Coll London, Gatsby Computat Neurosci Unit, 17 Queen Sq, London WC1N 3AR, England.							ATICK JJ, 1993, NEURAL COMPUT, V5, P45, DOI 10.1162/neco.1993.5.1.45; GOODALL MC, 1960, NATURE, V185, P557, DOI 10.1038/185557a0; JORDAN M, 1998, LEARNING GRAPHICAL M; KAKADE S, 2000, ADV NEURAL INFORMATI, V12; Miller RR, 1996, J EXP PSYCHOL GEN, V125, P370, DOI 10.1037/0096-3445.125.4.370; Rescorla RA., 1972, CLASSICAL CONDITION, pp. 64, DOI DOI 10.1101/GR.110528.110; SHANKS DR, 1985, Q J EXP PSYCHOL-B, V37, P1, DOI 10.1080/14640748508402082; SUTTON RS, 1992, P 7 YAL WORKSH AD LE; Wagner A. R., 1989, CONT LEARNING THEORI, P149; Widrow B., 1985, ADAPTIVE SIGNAL PROC	10	31	32	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						451	457						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800064
C	Olshausen, BA; Sallee, P; Lewicki, MS		Leen, TK; Dietterich, TG; Tresp, V		Olshausen, BA; Sallee, P; Lewicki, MS			Learning sparse image codes using a wavelet pyramid architecture	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We show how a wavelet basis may be adapted to best represent natural images in terms of sparse coefficients. The wavelet basis, which may be either complete or overcomplete, is specified by a small number of spatial functions which are repeated across space and combined in a recursive fashion so as to be self-similar across scale. These functions are adapted to minimize the estimated code length under a model that assumes images are composed of a linear superposition of sparse, independent components. When adapted to natural images, the wavelet bases take on different orientations and they evenly tile the orientation domain, in stark contrast to the standard, non-oriented wavelet bases used in image compression. When the basis set is allowed to be overcomplete, it also yields higher coding efficiency than standard wavelet bases.	Univ Calif Davis, Dept Psychol, Davis, CA 95616 USA	University of California System; University of California Davis	Olshausen, BA (corresponding author), Univ Calif Davis, Dept Psychol, 1544 Newton Ct, Davis, CA 95616 USA.							Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Lewicki MS, 1999, J OPT SOC AM A, V16, P1587, DOI 10.1364/JOSAA.16.001587; Mallat S., 1999, WAVELET TOUR SIGNAL, DOI 10.1016/B978-012466606-1/50008-8; Olshausen BA, 1997, VISION RES, V37, P3311, DOI 10.1016/S0042-6989(97)00169-7; Olshausen BA, 2000, ADV NEUR IN, V12, P841; SHAPIRO JM, 1993, IEEE T SIGNAL PROCES, V41, P3445, DOI 10.1109/78.258085; SIMONCELLI EP, 1992, IEEE T INFORM THEORY, V38, P587, DOI 10.1109/18.119725; SIMONCELLI EP, MATLAB PYRAMID TOOLB; van Hateren JH, 1998, P ROY SOC B-BIOL SCI, V265, P359, DOI 10.1098/rspb.1998.0303	9	31	35	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						887	893						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800125
C	Flexer, A		Mozer, MC; Jordan, MI; Petsche, T		Flexer, A			Limitations of self-organizing maps for vector quantization and multidimensional scaling	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The limitations of using self-organizing maps (SOM) for either clustering/vector quantization (VQ) or multidimensional scaling (MDS) are being discussed by reviewing recent empirical findings and the relevant theory. SOM's remaining ability of doing both VQ and MDS at the same time is challenged by a new combined technique of online K-means clustering plus Sammon mapping of the cluster centroids. SOM are shown to perform significantly worse in terms of quantization error, in recovering the structure of the clusters and in preserving the topology in a comprehensive empirical study using a series of multivariate normal clustering problems.			Flexer, A (corresponding author), AUSTRIAN RES INST ARTIFICIAL INTELLIGENCE,SCHOTTENGASSE 3,A-1010 VIENNA,AUSTRIA.			Flexer, Arthur/0000-0002-1691-737X					0	31	33	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						445	451						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00063
C	MATAN, O; BURGES, CJC; LECUN, Y; DENKER, JS		MOODY, JE; HANSON, SJ; LIPPMANN, RP		MATAN, O; BURGES, CJC; LECUN, Y; DENKER, JS			MULTIDIGIT RECOGNITION USING A SPACE DISPLACEMENT NEURAL NETWORK	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 4	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	5TH CONF ON NEURAL INFORMATION PROCESSING SYSTEMS - NATURAL AND SYNTHETIC ( NIPS-91 )	DEC 02-05, 1991	DENVER, CO																0	31	34	0	0	MORGAN KAUFMANN PUB INC	SAN MATEO	SAN MATEO			1-55860-222-4	ADV NEUR IN			1992	4						488	495						8	Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Neurosciences & Neurology	BW71Y					2022-12-19	WOS:A1992BW71Y00060
C	Taleb, A; Loetzsch, W; Danz, N; Severin, J; Gaertner, T; Bergner, B; Lippert, C		Larochelle, H; Ranzato, M; Hadsell, R; Balcan, MF; Lin, H		Taleb, Aiham; Loetzsch, Winfried; Danz, Noel; Severin, Julius; Gaertner, Thomas; Bergner, Benjamin; Lippert, Christoph			3D Self-Supervised Methods for Medical Imaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS (NEURIPS 2020)	Advances in Neural Information Processing Systems		English	Proceedings Paper	34th Conference on Neural Information Processing Systems (NeurIPS)	DEC 06-12, 2020	ELECTR NETWORK					Self-supervised learning methods have witnessed a recent surge of interest after proving successful in multiple application fields. In this work, we leverage these techniques, and we propose 3D versions for five different self-supervised methods, in the form of proxy tasks. Our methods facilitate neural network feature learning from unlabeled 3D images, aiming to reduce the required cost for expert annotation. The developed algorithms are 3D Contrastive Predictive Coding, 3D Rotation prediction, 3D Jigsaw puzzles, Relative 3D patch location, and 3D Exemplar networks. Our experiments show that pretraining models with our 3D tasks yields more powerful semantic representations, and enables solving downstream tasks more accurately and efficiently, compared to training the models from scratch and to pretraining them on 2D slices. We demonstrate the effectiveness of our methods on three downstream tasks from the medical imaging domain: i) Brain Tumor Segmentation from 3D MRI, ii) Pancreas Tumor Segmentation from 3D CT, and iii) Diabetic Retinopathy Detection from 2D Fundus images. In each task, we assess the gains in data-efficiency, performance, and speed of convergence. Interestingly, we also find gains when transferring the learned representations, by our methods, from a large unlabeled 3D corpus to a small downstream-specific dataset. We achieve results competitive to state-of-the-art solutions at a fraction of the computational expense. We publish our implementations1 for the developed algorithms (both 3D and 2D versions) as an open-source library, in an effort to allow other researchers to apply and extend our methods on their datasets.Y	[Taleb, Aiham; Loetzsch, Winfried; Danz, Noel; Severin, Julius; Gaertner, Thomas; Bergner, Benjamin; Lippert, Christoph] Potsdam Univ, Hasso Plattner Inst, Digital Hlth & Machine Learning, Potsdam, Germany	University of Potsdam	Taleb, A (corresponding author), Potsdam Univ, Hasso Plattner Inst, Digital Hlth & Machine Learning, Potsdam, Germany.	aiham.taleb@hpi.de; winfried.loetzsch@student.hpi.uni-potsdam.de; noel.danz@student.hpi.uni-potsdam.de; julius.severin@hpi.de; thomas.gaertner@student.hpi.uni-potsdam.de; benjamin.bergner@hpi.de; christoph.lippert@student.hpi.uni-potsdam.de		Taleb, Aiham/0000-0002-3743-1299	German Federal Ministry of Education and Research (BMBF) in the project KI-LAB-ITSE [01|S19066]	German Federal Ministry of Education and Research (BMBF) in the project KI-LAB-ITSE(Federal Ministry of Education & Research (BMBF))	This research has been supported by funding from the German Federal Ministry of Education and Research (BMBF) in the project KI-LAB-ITSE (project number 01|S19066). This research has been conducted using the UK Biobank Resource.	Bai WJ, 2019, LECT NOTES COMPUT SC, V11765, P541, DOI 10.1007/978-3-030-32245-8_60; Bakas S, 2017, SCI DATA, V4, DOI 10.1038/sdata.2017.117; Bengio S, 2019, ADV NEURAL INFORM PR, P3347; Blendowski M, 2019, LECT NOTES COMPUT SC, V11769, P649, DOI 10.1007/978-3-030-32226-7_72; Caron Mathilde, 2018, EUR C COMP VIS ECCV; Chaitanya Krishna, 2020, CONTRASTIVE LEARNING; Chandra Siddhartha, 2018, INT MICCAI BRAINLESI, P74; Chen L, 2019, MED IMAGE ANAL, V58, DOI 10.1016/j.media.2019.101539; Chen T, 2020, PR MACH LEARN RES, V119; Cho K., 2014, P 2014 C EMP METH NA, P1724; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Eisenberg RL, 2011, PATIENTS GUIDE MED I; Gidaris Spyros, 2018, ABS180307728 CORR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Goyal P, 2019, IEEE I CONF COMP VIS, P6400, DOI 10.1109/ICCV.2019.00649; Griffiths David, 2019, ABS190704444 CORR; Grunberg Katharina, 2017, ANNOTATING MEDICAL I, P45; Gutmann M., 2010, AISTATS, V9, P297, DOI DOI 10.1145/3292500.3330651; He Kaiming, 2020, P IEEE CVF C COMP VI; Henaff Olivier J., 2019, ABS190509272 CORR; Ioannidou A, 2017, ACM COMPUT SURV, V50, DOI 10.1145/3042064; Isensee F, 2019, LECT NOTES COMPUT SC, V11384, P234, DOI 10.1007/978-3-030-11726-9_21; Islam Sheikh Muhammad Saiful, 2018, ABS181210595 CORR; Jamaludin A, 2017, LECT NOTES COMPUT SC, V10553, P294, DOI 10.1007/978-3-319-67558-9_34; Ji SW, 2013, IEEE T PATTERN ANAL, V35, P221, DOI 10.1109/TPAMI.2012.59; Jiao JB, 2020, I S BIOMED IMAGING, P1847, DOI 10.1109/ISBI45749.2020.9098666; Jing Longlong, 2018, ABS181111387 CORR; Jing Longlong, 2019, ABS190206162 CORR; Johns Edward, 2017, PROC HAMLYN S MED RO, P27, DOI [DOI 10.31256/HSMR2017.14, 10.31256/HSMR2017.14]; Kim D, 2019, AAAI CONF ARTIF INTE, P8545; Li HM, 2018, I S BIOMED IMAGING, P1075, DOI 10.1109/ISBI.2018.8363757; Liu Xingtong, 2018, ABS180609521 CORR; Luxburg U. V., 2016, ADV NEURAL INFORM PR, V29, P4790; Mahajan A., 2018, MICCAI BRATS CHALLEN; Manna Sayan, 2020, Radiol Cardiothorac Imaging, V2, pe200210, DOI 10.1148/ryct.2020200210; Menze BH, 2015, IEEE T MED IMAGING, V34, P1993, DOI 10.1109/TMI.2014.2377694; Mikolov T., 2013, 9 INT C LEARN REPR, P1; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Popli Anmol, 2018, PREC P 7 MICCAI BRAT, P374; Purushwalkam Senthil, 2016, ABS160905420 CORR; Rajpurkar P., 2017, CHEXNET RADIOLOGIST; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Ross T, 2018, INT J COMPUT ASS RAD, V13, P925, DOI 10.1007/s11548-018-1772-0; Sahlsten J, 2019, SCI REP-UK, V9, DOI 10.1038/s41598-019-47181-w; Schroff F, 2015, PROC CVPR IEEE, P815, DOI 10.1109/CVPR.2015.7298682; Simpson Amber L., 2019, ABS190209063 CORR; Spitzer H, 2018, LECT NOTES COMPUT SC, V11072, P663, DOI 10.1007/978-3-030-00931-1_76; Stollenga M., 2015, ABS150607452 CORR; Su Hao, 2017, 3D DEEP LEARNING; Sudlow C, 2015, PLOS MED, V12, DOI 10.1371/journal.pmed.1001779; Tajbakhsh N, 2019, I S BIOMED IMAGING, P1251, DOI 10.1109/ISBI.2019.8759553; Taleb Aiham, 2019, MULTIMODAL SELF SUPE; Torralba A, 2011, PROC CVPR IEEE, P1521, DOI 10.1109/CVPR.2011.5995347; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2018, ABS180703748 CORR; Vondrick Carl, 2018, P EUR C COMP VIS ECC; Vondrick Carl, 2015, ABS150408023 CORR; Walker J, 2015, IEEE I CONF COMP VIS, P2443, DOI 10.1109/ICCV.2015.281; Wang JW, 2019, PROCEEDINGS OF THE 27TH ACM INTERNATIONAL CONFERENCE ON MULTIMEDIA (MM'19), P2548, DOI 10.1145/3343031.3356059; Wang XL, 2015, IEEE I CONF COMP VIS, P2794, DOI 10.1109/ICCV.2015.320; Wang XS, 2017, PROC CVPR IEEE, P3462, DOI 10.1109/CVPR.2017.369; Woolrich MW, 2009, NEUROIMAGE, V45, pS173, DOI 10.1016/j.neuroimage.2008.10.055; Yan K, 2019, ADV COMPUT VIS PATT, P413, DOI 10.1007/978-3-030-13969-8_20; Zhang PY, 2017, I S BIOMED IMAGING, P578, DOI 10.1109/ISBI.2017.7950587; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhou ZW, 2019, LECT NOTES COMPUT SC, V11767, P384, DOI 10.1007/978-3-030-32251-9_42; Zhu JW, 2020, MED IMAGE ANAL, V64, DOI 10.1016/j.media.2020.101746; Zhuang XR, 2019, LECT NOTES COMPUT SC, V11767, P420, DOI 10.1007/978-3-030-32251-9_46	70	30	30	4	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2020	33													15	Computer Science, Artificial Intelligence; Computer Science, Information Systems	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BR0BE					2022-12-19	WOS:000627697000002
C	Gui, SP; Wang, HT; Yang, HC; Yu, C; Wang, ZY; Liu, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Gui, Shupeng; Wang, Haotao; Yang, Haichuan; Yu, Chen; Wang, Zhangyang; Liu, Ji			Model Compression with Adversarial Robustness: A Unified Optimization Framework	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Deep model compression has been extensively studied, and state-of-the-art methods can now achieve high compression ratios with minimal accuracy loss. This paper studies model compression through a different lens: could we compress models without hurting their robustness to adversarial attacks, in addition to maintaining accuracy? Previous literature suggested that the goals of robustness and compactness might sometimes contradict. We propose a novel Adversarially Trained Model Compression (ATMC) framework. ATMC constructs a unified constrained optimization formulation, where existing compression means (pruning, factorization, quantization) are all integrated into the constraints. An efficient algorithm is then developed. An extensive group of experiments are presented, demonstrating that ATMC obtains remarkably more favorable trade-off among model size, accuracy and robustness, over currently available alternatives in various settings. The codes are publicly available at: https://github.com/shupenggui/ATMC.	[Gui, Shupeng; Yang, Haichuan; Yu, Chen] Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA; [Wang, Haotao; Wang, Zhangyang] Texas A&M Univ, Dept Comp Sci & Engn, College Stn, TX 77843 USA; [Liu, Ji] Kwai Inc, Ytech Seattle AI Lab, FeDA Lab, AI Platform, Beijing, Peoples R China	University of Rochester; Texas A&M University System; Texas A&M University College Station	Gui, SP (corresponding author), Univ Rochester, Dept Comp Sci, Rochester, NY 14627 USA.	sgui2@ur.rochester.edu; htwang@tamu.edu; hyang36@ur.rochester.edu; cyu28@ur.rochester.edu; atlaswang@tamu.edu; ji.liu.uwisc@gmail.com	Gui, Shupeng/ABG-2776-2020					Athalye A, 2018, PR MACH LEARN RES, V80; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Candes EJ, 2011, J ACM, V58, DOI 10.1145/1970392.1970395; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Chen WJ, 2019, PROC CVPR IEEE, P7234, DOI 10.1109/CVPR.2019.00741; Cheng Yu, 2017, ARXIV171009282; Courbariaux M., 2015, ADV NEURAL INFORM PR, P3123, DOI DOI 10.1109/TWC.2016.2633262; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Dentinel Zarembaw, 2014, NEURIPS, P1269; Dhillon G.S., 2018, ARXIV180301442; Gong Yunchao, 2014, ARXIV14126115; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Han Song, 2015, ARXIV151000149, DOI DOI 10.1145/2351676.2351678; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hosseini H, 2017, ABS170304318 CORR; Huang XW, 2015, ACTA POLYM SIN, P1133; Hubara I, 2018, J MACH LEARN RES, V18; Jin J., 2014, ARXIV PREPRINT ARXIV; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Kurakin A, 2016, INT C LEARN REPR SAN; Lebedev V., 2015, 3 INT C LEARNING REP; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Liao FZ, 2018, PROC CVPR IEEE, P1778, DOI 10.1109/CVPR.2018.00191; Lin JY, 2019, IEEE INT C ELECTR TA; LLOYD SP, 1982, IEEE T INFORM THEORY, V28, P129, DOI 10.1109/TIT.1982.1056489; Lu HY, 2015, PROC CVPR IEEE, P806, DOI 10.1109/CVPR.2015.7298681; Luo B, 2018, AAAI CONF ARTIF INTE, P1652; Madry Aleksander, 2017, ARXIV; Meng DY, 2017, CCS'17: PROCEEDINGS OF THE 2017 ACM SIGSAC CONFERENCE ON COMPUTER AND COMMUNICATIONS SECURITY, P135, DOI 10.1145/3133956.3134057; Metzen JH, 2017, IEEE I CONF COMP VIS, P2774, DOI 10.1109/ICCV.2017.300; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nakkiran P., 2019, ARXIV190100532; Nakkiran Preetum, 2015, ANN C INT SPEECH COM; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Neyshabur B., 2013, ARXIV13113315; Papernot N., 2017, ARXIV170505264; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Rastegari M, 2016, LECT NOTES COMPUT SC, V9908, P525, DOI 10.1007/978-3-319-46493-0_32; Sinha A, 2017, ARXIV171010571; Tai C., 4 INT C LEARN REPR I; Tramer F., 2017, ARXIV; Tsipras Dimitris, 2018, ARXIV180512152; Wang Yue, 2018, ADV NEUR INF PROC SY; Wang Yue, 2019, ARXIV190704523; Wang YX, 2018, AAAI CONF ARTIF INTE, P5561; Wen W, 2016, ADV NEUR IN, V29; Wu JX, 2016, PROC CVPR IEEE, P4820, DOI 10.1109/CVPR.2016.521; Wu JR, 2018, PR MACH LEARN RES, V80; Xie CH, 2017, IEEE I CONF COMP VIS, P1378, DOI 10.1109/ICCV.2017.153; Xu H, 2012, IEEE T PATTERN ANAL, V34, P187, DOI 10.1109/TPAMI.2011.177; Xu WL, 2018, 25TH ANNUAL NETWORK AND DISTRIBUTED SYSTEM SECURITY SYMPOSIUM (NDSS 2018), DOI 10.14722/ndss.2018.23198; Yang H., 2018, ARXIV180604321; Yang H., 2019, INT C LEARN REPR; Yang H, 2019, EURASIP J WIREL COMM, DOI 10.1186/s13638-019-1442-0; Ye S., 2018, CORR; Ye Shaokai, 2019, ARXIV190312561; Yu XY, 2017, PROC CVPR IEEE, P67, DOI 10.1109/CVPR.2017.15; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87; Zemel R. S., 1994, MINIMUM DESCRIPTION; Zhao Y., 2018, ARXIV181000208; Zhou H, 2016, LECT NOTES COMPUT SC, V9908, P662, DOI 10.1007/978-3-319-46493-0_40; Zugner D, 2019, PROCEEDINGS OF THE TWENTY-EIGHTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P6246, DOI 10.1145/3219819.3220078	64	30	31	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301029
C	Hu, XL; Li, FX; Samaras, D; Chen, C		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hu, Xiaoling; Li Fuxin; Samaras, Dimitris; Chen, Chao			Topology-Preserving Deep Image Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				ACTIVE CONTOURS; PERSISTENCE	Segmentation algorithms are prone to topological errors on fine-scale structures, e.g., broken connections. We propose a novel method that learns to segment with correct topology. In particular, we design a continuous-valued loss function that enforces a segmentation to have the same topology as the ground truth, i.e., having the same Betti number. The proposed topology-preserving loss function is differentiable and we incorporate it into end-to-end training of a deep neural network. Our method achieves much better performance on the Betti number error, which directly accounts for the topological correctness. It also performs superiorly on other topology-relevant metrics, e.g., the Adjusted Rand Index and the Variation of Information. We illustrate the effectiveness of the proposed method on a broad spectrum of natural and biomedical datasets.	[Hu, Xiaoling; Samaras, Dimitris; Chen, Chao] SUNY Stony Brook, Stony Brook, NY 11794 USA; [Li Fuxin] Oregon State Univ, Corvallis, OR 97331 USA	State University of New York (SUNY) System; State University of New York (SUNY) Stony Brook; Oregon State University	Hu, XL (corresponding author), SUNY Stony Brook, Stony Brook, NY 11794 USA.	xiaolhu@cs.stonybrook.edu			NSF [IIS-1911232]	NSF(National Science Foundation (NSF))	The research of Xiaoling Hu and Chao Chen is partially supported by NSF IIS-1909038. The research of Li Fuxin is partially supported by NSF IIS-1911232.	Adams H, 2017, J MACH LEARN RES, V18; Andres B, 2011, IEEE I CONF COMP VIS, P2611, DOI 10.1109/ICCV.2011.6126550; Arganda-Carreras I, 2013, 3D SEGMENTATION NEUR; Arganda-Carreras I, 2015, FRONT NEUROANAT, V9, DOI 10.3389/fnana.2015.00142; Carriere M, 2017, PR MACH LEARN RES, V70; Chao Chen, 2011, 2011 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P2089, DOI 10.1109/CVPR.2011.5995503; CHEN C., 2011, P 27 EUR WORKSH COMP, V11, P197; Chen L.-C., 2014, ARXIV14127062CSCV, P1; Chen L.-C., 2017, RETHINKING ATROUS CO; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Cohen-Steiner D, 2007, DISCRETE COMPUT GEOM, V37, P103, DOI 10.1007/s00454-006-1276-5; Cohen-Steiner D, 2010, FOUND COMPUT MATH, V10, P127, DOI 10.1007/s10208-010-9060-6; Edelsbrunner H, 2000, ANN IEEE SYMP FOUND, P454; Edelsbrunner H., 2010, COMPUTATIONAL TOPOLO; Estrada R, 2015, IEEE T PATTERN ANAL, V37, P1688, DOI 10.1109/TPAMI.2014.2382116; Fakhry A, 2016, BIOINFORMATICS, V32, P2352, DOI 10.1093/bioinformatics/btw165; Funke J., 2017, ARXIV170902974; Gao Mingchen, 2013, Inf Process Med Imaging, V23, P184, DOI 10.1007/978-3-642-38868-2_16; Han X, 2003, IEEE T PATTERN ANAL, V25, P755, DOI 10.1109/TPAMI.2003.1201824; He KM, 2020, IEEE T PATTERN ANAL, V42, P386, DOI 10.1109/TPAMI.2018.2844175; Hofer C, 2017, ADV NEUR IN, V30; Kerber M., 2017, ACM J EXP ALGORITHMI, V22, DOI 10.1145/3064175; Kusano G, 2016, PR MACH LEARN RES, V48; Le Guyader C, 2008, IEEE T IMAGE PROCESS, V17, P767, DOI 10.1109/TIP.2008.919951; Liu T, 2019, IEEE INT C INTELL TR, P3573, DOI 10.1109/ITSC.2019.8916909; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Mnih V, 2013, MACHINE LEARNING AER; Mosinska A, 2018, PROC CVPR IEEE, P3136, DOI 10.1109/CVPR.2018.00331; Munkres J.R., 2018, ELEMENTS ALGEBRAIC T; Ni XY, 2017, PR MACH LEARN RES, V70; Noh H, 2015, IEEE I CONF COMP VIS, P1520, DOI 10.1109/ICCV.2015.178; Nowozin S, 2009, PROC CVPR IEEE, P818, DOI 10.1109/CVPRW.2009.5206567; Oswald Martin Ralf, 2014, Computer Vision - ECCV 2014. 13th European Conference. Proceedings: LNCS 8692, P32, DOI 10.1007/978-3-319-10593-2_3; Poulenard A, 2018, COMPUT GRAPH FORUM, V37, P13, DOI 10.1111/cgf.13487; Quadriato N., 2016, JMLR, P2732, DOI DOI 10.1016/J.CSDA.2013.01.026; Reininghaus J, 2015, PROC CVPR IEEE, P4741, DOI 10.1109/CVPR.2015.7299106; Ronneberger O, 2015, LECT NOTES COMPUT SC, V9351, P234, DOI 10.1007/978-3-319-24574-4_28; Segonne F, 2008, INT J COMPUT VISION, V79, P107, DOI 10.1007/s11263-007-0102-8; Staal J, 2004, IEEE T MED IMAGING, V23, P501, DOI 10.1109/TMI.2004.825627; Stuhmer J, 2013, IEEE I CONF COMP VIS, P2336, DOI 10.1109/ICCV.2013.290; Sundaramoorthi G, 2007, IEEE T IMAGE PROCESS, V16, P803, DOI 10.1109/TIP.2007.891071; Turaga S.C., 2009, ARXIV09115372; Vicente S., 2008, P 2008 IEEE C COMP V, P1, DOI DOI 10.1109/CVPR.2008.4587440; Wagner H., 2012, TOPOLOGICAL METHODS, P91, DOI DOI 10.1007/978-3-642-23175-9_7; Wu PX, 2017, LECT NOTES COMPUT SC, V10265, P80, DOI 10.1007/978-3-319-59050-9_7; Zeng Y, 2008, COMPUT VIS IMAGE UND, V112, P81, DOI 10.1016/j.cviu.2008.07.008; Zomorodian A, 2005, DISCRETE COMPUT GEOM, V33, P249, DOI 10.1007/s00454-004-1146-y; Zou Q, 2012, PATTERN RECOGN LETT, V33, P227, DOI 10.1016/j.patrec.2011.11.004	48	30	30	3	6	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305063
C	Klicpera, J; Weissenberger, S; Gunnemann, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Klicpera, Johannes; Weissenberger, Stefan; Guennemann, Stephan			Diffusion Improves Graph Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				CLASSIFICATION	Graph convolution is the core of most Graph Neural Networks (GNNs) and usually approximated by message passing between direct (one-hop) neighbors. In this work, we remove the restriction of using only the direct neighbors by introducing a powerful, yet spatially localized graph convolution: Graph diffusion convolution (GDC). GDC leverages generalized graph diffusion, examples of which are the heat kernel and personalized PageRank. It alleviates the problem of noisy and often arbitrarily defined edges in real graphs. We show that GDC is closely related to spectral-based models and thus combines the strengths of both spatial (message passing) and spectral methods. We demonstrate that replacing message passing with graph diffusion convolution consistently leads to significant performance improvements across a wide range of models on both supervised and unsupervised tasks and a variety of datasets. Furthermore, GDC is not limited to GNNs but can trivially be combined with any graph-based model or algorithm (e.g. spectral clustering) without requiring any changes to the latter or affecting its computational complexity. Our implementation is available online.(1)	[Klicpera, Johannes; Weissenberger, Stefan; Guennemann, Stephan] Tech Univ Munich, Munich, Germany	Technical University of Munich	Klicpera, J (corresponding author), Tech Univ Munich, Munich, Germany.	klicpera@in.tum.de; stefan.weissenberger@in.tum.de; guennemann@in.tum.de			German Federal Ministry of Education and Research (BMBF) [01IS18036B]; Deutsche Forschungsgemeinschaft (DFG) through the Emmy Noether grant [GU 1409/2-1]; TUM International Graduate School of Science and Engineering (IGSSE) [GSC 81]	German Federal Ministry of Education and Research (BMBF)(Federal Ministry of Education & Research (BMBF)); Deutsche Forschungsgemeinschaft (DFG) through the Emmy Noether grant(German Research Foundation (DFG)); TUM International Graduate School of Science and Engineering (IGSSE)	This research was supported by the German Federal Ministry of Education and Research (BMBF), grant no. 01IS18036B, and by the Deutsche Forschungsgemeinschaft (DFG) through the Emmy Noether grant GU 1409/2-1 and the TUM International Graduate School of Science and Engineering (IGSSE), GSC 81. The authors of this work take full responsibilities for its content.	Abu-El-Haija S, 2018, ADV NEUR IN, V31; Abu-El-Haija Sami, 2019, P ICML; Andersen R, 2006, ANN IEEE SYMP FOUND, P475; [Anonymous], 2017, AAAI; Atwood J., 2016, ADV NEURAL INFORM PR, P1993, DOI DOI 10.5555/3157096.3157320; Backstrom Lars, 2012, ACM WEB SCI C; Baskin II, 1997, J CHEM INF COMP SCI, V37, P715, DOI 10.1021/ci940128y; Berberidis D, 2019, IEEE T SIGNAL PROCES, V67, P1307, DOI 10.1109/TSP.2018.2889984; Berberidis Dimitris, 2018, 181110797 CORR; Bianchi Filippo Maria, 2019, 19010343 CORR; Bojchevski Aleksandar, 2018, INT C LEARN REPR; Bruna J, 2013, PROC INT C LEARN REP; Chen Siheng, 2013, IEEE GLOB C SIGN INF; Chung F, 2007, P NATL ACAD SCI USA, V104, P19735, DOI 10.1073/pnas.0708838104; Decelle A, 2011, PHYS REV LETT, V107, DOI 10.1103/PhysRevLett.107.065701; Derr T, 2018, IEEE DATA MINING, P929, DOI 10.1109/ICDM.2018.00113; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dobson Paul D, 2003, J Mol Biol, V330, P771; Duvenaud David K, 2015, P NIPS; Eliav B, 2018, P ACM MEAS ANAL COMP, V2, DOI 10.1145/3179413; Fey M., 2019, ICLR WORKSH REPR LEA; Fouss F, 2012, NEURAL NETWORKS, V31, P53, DOI 10.1016/j.neunet.2012.03.001; Gilmer J, 2017, PR MACH LEARN RES, V70; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; Hammond DK, 2011, APPL COMPUT HARMON A, V30, P129, DOI 10.1016/j.acha.2010.04.005; Henaff Mikael, 2015, 150605163 CORR; Jones E., 2001, SCIPY OPEN SOURCE SC; Karrer B, 2011, PHYS REV E, V83, DOI 10.1103/PhysRevE.83.016107; Kipf T.N., 2017, 5 INT C LEARN REPRES, P1; Kipf Thomas N., 2016, ARXIV161107308, V2, P1; Klicpera J., 2019, ICLR; Kloster Kyle, 2014, KDD; Kloumann IM, 2017, P NATL ACAD SCI USA, V114, P33, DOI 10.1073/pnas.1611275114; Kondor R.I., 2002, P 19 INT C MACHINE L, P315; Lafon S, 2006, IEEE T PATTERN ANAL, V28, P1393, DOI 10.1109/TPAMI.2006.184; Landau E., 1895, DEUTSCHESWOCHENSCHAC, V11, P366; Leskovec J., 2005, P 11 ACM SIGKDD INT, P177, DOI DOI 10.1145/1081870.1081893; Li Pan, 2019, NEURIPS; Li Y., 2018, ARXIV PREPRINT ARXIV; Li Yujia, 2016, P INT C LEARN REPR I, P2; Ma Jeremy, 2016, ICASSP; Ma Zheng, 2019, ICML WORKSH; Masuda N, 2017, PHYS REP, V716, P1, DOI 10.1016/j.physrep.2017.07.007; McAuley J, 2015, SIGIR 2015: PROCEEDINGS OF THE 38TH INTERNATIONAL ACM SIGIR CONFERENCE ON RESEARCH AND DEVELOPMENT IN INFORMATION RETRIEVAL, P43, DOI 10.1145/2766462.2767755; McCallum AK, 2000, INFORM RETRIEVAL, V3, P127, DOI 10.1023/A:1009953814988; McPherson M, 2001, ANNU REV SOCIOL, V27, P415, DOI 10.1146/annurev.soc.27.1.415; Menche J, 2015, SCIENCE, V347, DOI 10.1126/science.1257601; Mikolov T., 2013, ARXIV; Monti F, 2017, PROC CVPR IEEE, P5425, DOI 10.1109/CVPR.2017.576; Namata Galileo, 2012, P 10 INT WORKSHOP MI, V8, P1; Nassar Huda, 2015, INT WORKSH ALG MOD W; Ng Andrew Y, 2002, NIPS; Niepert M, 2016, PR MACH LEARN RES, V48; Page L., 1999, PAGERANK CITATION RA; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Peixoto T.P., 2014, GRAPH TOOL PYTHON LI, DOI [10.6084/m9.figshare.1164194, DOI 10.6084/M9.FIGSHARE.1164194]; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Qiu JZ, 2018, WSDM'18: PROCEEDINGS OF THE ELEVENTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING, P459, DOI 10.1145/3159652.3159706; Ragain Stephen, 2017, COMMUNITY DETECTION; Rehiek Radim, 2010, LREC 2010 WORKSH NEW; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Sen P, 2008, AI MAG, V29, P93, DOI 10.1609/aimag.v29i3.2157; Shchur Oleksandr, 2018, NIPS WORKSH; Sperduti A, 1997, IEEE T NEURAL NETWOR, V8, P714, DOI 10.1109/72.572108; Stewart G., 1990, MATRIX PERTURBATION; Tang YH, 2018, J CHEM PHYS, V148, DOI 10.1063/1.5008630; Tsitsulin A, 2018, WEB CONFERENCE 2018: PROCEEDINGS OF THE WORLD WIDE WEB CONFERENCE (WWW2018), P539, DOI 10.1145/3178876.3186120; van der Walt S, 2011, COMPUT SCI ENG, V13, P22, DOI 10.1109/MCSE.2011.37; Velickovic P., 2019, ICLR; Velickovic P., 2018, ICLR; Vigna S, 2016, NETW SCI, V4, P433, DOI 10.1017/nws.2016.21; von Luxburg U, 2007, STAT COMPUT, V17, P395, DOI 10.1007/s11222-007-9033-z; Wei Zhewei, 2018, SIGMOD; Wu F, 2019, PR MACH LEARN RES, V97; Xu Bingbing, 2019, IJCAL; Xu K., 2019, ICLR, P1, DOI DOI 10.1109/VTCFALL.2019.8891597; Xu KYL, 2018, PR MACH LEARN RES, V80; Ying R, 2018, KDD'18: PROCEEDINGS OF THE 24TH ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY & DATA MINING, P974, DOI 10.1145/3219819.3219890	82	30	30	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905005
C	Locatello, F; Abbati, G; Rainforth, T; Bauer, S; Scholkopf, B; Bachem, O		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Locatello, Francesco; Abbati, Gabriele; Rainforth, Tom; Bauer, Stefan; Scholkopf, Bernhard; Bachem, Olivier			On the Fairness of Disentangled Representations	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Recently there has been a significant interest in learning disentangled representations, as they promise increased interpretability, generalization to unseen scenarios and faster learning on downstream tasks. In this paper, we investigate the usefulness of different notions of disentanglement for improving the fairness of downstream prediction tasks based on representations. We consider the setting where the goal is to predict a target variable based on the learned representation of high-dimensional observations (such as images) that depend on both the target variable and an unobserved sensitive variable. We show that in this setting both the optimal and empirical predictions can be unfair, even if the target variable and the sensitive variable are independent. Analyzing the representations of more than 12 600 trained state-of-the-art disentangled models, we observe that several disentanglement scores are consistently correlated with increased fairness, suggesting that disentanglement may be a useful property to encourage fairness when sensitive variables are not observed.	[Bachem, Olivier] Google Res, Brain Team, Mountain View, CA USA; [Locatello, Francesco] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland; [Abbati, Gabriele] Univ Oxford, Dept Engn Sci, Oxford, England; [Rainforth, Tom] Univ Oxford, Dept Stat, Oxford, England; [Locatello, Francesco; Bauer, Stefan; Scholkopf, Bernhard] Max Planck Inst Intelligent Syst, Stuttgart, Germany	Google Incorporated; Swiss Federal Institutes of Technology Domain; ETH Zurich; University of Oxford; University of Oxford; Max Planck Society	Locatello, F (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.; Locatello, F (corresponding author), Max Planck Inst Intelligent Syst, Stuttgart, Germany.		Locatello, Francesco/GQY-6025-2022	Locatello, Francesco/0000-0002-4850-0683; Rainforth, Tom/0000-0001-7939-4230	Max Planck ETH Center for Learning Systems; ETH core grant; Google Ph.D. Fellowship; Google Deepmind; University of Oxford; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC [617071]; EPSRC [EP/P026753/1]	Max Planck ETH Center for Learning Systems; ETH core grant; Google Ph.D. Fellowship(Google Incorporated); Google Deepmind(Google Incorporated); University of Oxford; European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC(European Research Council (ERC)); EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC))	The authors thank Sylvain Gelly and Niki Kilbertus for helpful discussions and comments. Francesco Locatello is supported by the Max Planck ETH Center for Learning Systems, by an ETH core grant (to Gunnar Ratsch), and by a Google Ph.D. Fellowship. This work was partially done while Francesco Locatello was at Google Research Zurich. Gabriele Abbati acknowledges funding from Google Deepmind and the University of Oxford. Tom Rainforth is supported in part by the European Research Council under the European Union's Seventh Framework Programme (FP7/2007-2013)/ERC grant agreement no. 617071 and in part by EPSRC funding under grant EP/P026753/1.	Bach FR, 2003, J MACH LEARN RES, V3, P1, DOI 10.1162/153244303768966085; Barocas S., 2019, FAIRNESS MACHINE LEA; Barocas S, 2016, CALIF LAW REV, V104, P671, DOI 10.15779/Z38BG31; Bengio Y., 2007, LARGE SCALE KERNEL M, V34, P1, DOI [DOI 10.1038/NATURE14539, 10.1016/j.asoc.2014.05.028, DOI 10.1016/J.ASOC.2014.05.028]; Bengio Y, 2013, IEEE T PATTERN ANAL, V35, P1798, DOI 10.1109/TPAMI.2013.50; Bouchacourt D, 2018, AAAI CONF ARTIF INTE, P2095; Burgess CP, 2018, ARXIV180403599; Calders T, 2009, INT CONF DAT MIN WOR, P13, DOI 10.1109/ICDMW.2009.83; Calders Toon, 2013, DISCRIMINATION PRIVA, V3, P43, DOI [DOI 10.1007/978-3-642-30487-3_3, 10.1007/978-3-642-30487-3_3]; Calmon Flavio, 2017, NEURAL INFORM PROCES, P1, DOI DOI 10.5555/3294996.3295155; Chen Tian Qi, 2018, ADV NEURAL INFORM PR; Cheung B., 2014, P INT C LEARN REPR W; Cohen T, 2014, PR MACH LEARN RES, V32, P1755; COMON P, 1994, SIGNAL PROCESS, V36, P287, DOI 10.1016/0165-1684(94)90029-9; Deng Zhiwei, 2017, IEEE C COMP VIS PATT; Dumoulin Vincent, 2016, ARXIV E PRINTS; Eastwood Cian, 2018, INT C LEARN REPR; Edwards Harrison, 2016, INT C LEARN REPR ICL, P3; Esmaeili B., 2019, PROC INT C ARTIF INT; Fortuin V., 2019, INT C LEARN REPR; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gondal MW, 2019, ADV NEUR IN, V32; Goodfellow I., 2009, ADV NEURAL INFORM PR, V22, P646, DOI DOI 10.5555/2984093.2984166; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gresele Luigi, 2019, C UNC ART INT UAI; Hardt M., 2016, EQUALITY OPPORTUNITY, P3315, DOI 10.1109/ICCV.2015.169; Higgins I., 2018, ICLR; Higgins I., 2017, P INT C LEARN REPR T; Higgins I., 2018, ARXIV PREPRINT ARXIV; Higgins I, 2017, PR MACH LEARN RES, V70; Hinton GE, 2011, LECT NOTES COMPUT SC, V6791, P44, DOI 10.1007/978-3-642-21735-7_6; Hsieh Jun-Ting, 2018, ADV NEURAL INFORM PR; Hsu Wei-Ning, 2017, ADV NEURAL INFORM PR; Hyvarinen Aapo, 2019, INT C ART INT STAT; Hyvdinen Aapo, 1999, NEURAL NETWORKS; Johansson FD, 2016, PR MACH LEARN RES, V48; Jutten C., 2003, P 4 INT S IND COMP A, P245; Karaletsos Theofanis, 2015, ARXIV150605011; Kilbertus Niki, 2017, ADV NEURAL INFORM PR, P656; Kim H, 2018, PR MACH LEARN RES, V80; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P, 2014, ARXIV13126114; Klys Jack, 2018, ADV NEURAL INFORM PR; Kulkarni TD, 2015, ADV NEUR IN, V28; Kumar A, 2018, PROCEEDINGS OF THE 1ST INTERNATIONAL WORKSHOP ON FUTURE INDUSTRIAL COMMUNICATION NETWORKS (FICN'18), P1, DOI 10.1145/3243318.3243327; Kusner Matt J, 2017, ADV NEURAL INFORM PR, V30, P4066, DOI DOI 10.5555/3294996.3295162; Lake BM, 2017, BEHAV BRAIN SCI, V40, DOI 10.1017/S0140525X16001837; Laversanne-Finot A., 2018, MACHINE LEARNING RES, V87, P487; LeCun Y, 2004, PROC CVPR IEEE, P97; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Locatello F., 2018, ARXIV180411130; Locatello F, 2019, PR MACH LEARN RES, V97; Louizos C., 2015, ARXIV PREPRINT ARXIV; Madras D, 2018, PR MACH LEARN RES, V80; Mathieu E., 2018, ARXIV181202833; McNamara D., 2017, ARXIV171004394; Megan D. P. C., 2016, BIG DATA REPORT ALGO; Pearl J., 2009, CAUSALITY, DOI DOI 10.1017/CBO9780511803161; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Pedreshi D, 2008, P 14 ACM SIGKDD INT, P560, DOI DOI 10.1145/1401890.1401959; Peters J, 2017, ADAPT COMPUT MACH LE; Podesta J., 2014, BIG DATA SEIZING OPP; Pong V., 2018, PROC 2 WORKSHOP LIFE; Poole B, 2019, PR MACH LEARN RES, V97; Reed S, 2015, ADV NEURAL INFORM PR, P1252; Reed S, 2014, PR MACH LEARN RES, V32, P1431; Ridgeway K, 2018, ADV NEUR IN, V31; Rolinek Michal, 2019, P IEEE C COMP VIS PA; Ruiz A., 2019, ARXIV190108534; SCHMIDHUBER J, 1992, NEURAL COMPUT, V4, P863, DOI 10.1162/neco.1992.4.6.863; Schreurs W., 2008, PROFILING EUROPEAN C, P241; Smola, 2007, ADV NEURAL INFORM PR, P513, DOI DOI 10.5555/2188385.2188410; Song, 2018, ARXIV PREPRINT ARXIV; Stdhmer Jan, 2019, ISA VAE INDEPENDENT; Steenbrugge Xander, 2018, P 32 C NEUR INF PROC; Suter Raphael, 2019, INT C MACH LEARN; Thomas Valentin, 2017, LEARN DIS REPR WORKS; Tschannen Michael, 2018, ARXIV181205069; van Steenkiste Sjoerd, 2019, ARXIV190512506; Whitney W. F., 2016, ARXIV160206822; Yang Jimei, 2015, ADV NEURAL INFORM PR; Zemel R., 2013, P INT C MACH LEARN, P325; Zhang BH, 2018, PROCEEDINGS OF THE 2018 AAAI/ACM CONFERENCE ON AI, ETHICS, AND SOCIETY (AIES'18), P335, DOI 10.1145/3278721.3278779; Zliobaite I., 2015, 2 WORKSH FAIRN ACC T	97	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866906029
C	Morcos, AS; Yu, HN; Paganini, M; Tian, YD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Morcos, Ari S.; Yu, Haonan; Paganini, Michela; Tian, Yuandong			One ticket to win them all: generalizing lottery ticket initializations across datasets and optimizers	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					The success of lottery ticket initializations [7] suggests that small, sparsified networks can be trained so long as the network is initialized appropriately. Unfortunately, finding these "winning ticket" initializations is computationally expensive. One potential solution is to reuse the same winning tickets across a variety of datasets and optimizers. However, the generality of winning ticket initializations remains unclear. Here, we attempt to answer this question by generating winning tickets for one training configuration (optimizer and dataset) and evaluating their performance on another configuration. Perhaps surprisingly, we found that, within the natural images domain, winning ticket initializations generalized across a variety of datasets, including Fashion MNIST, SVHN, CIFAR-10/100, ImageNet, and Places365, often achieving performance close to that of winning tickets generated on the same dataset. Moreover, winning tickets generated using larger datasets consistently transferred better than those generated using smaller datasets. We also found that winning ticket initializations generalize across optimizers with high performance. These results suggest that winning ticket initializations generated by sufficiently large datasets contain inductive biases generic to neural networks more broadly which improve training across many settings and provide hope for the development of better initialization methods.	[Morcos, Ari S.; Yu, Haonan; Paganini, Michela; Tian, Yuandong] Facebook AI Res, New York, NY 10003 USA	Facebook Inc	Morcos, AS (corresponding author), Facebook AI Res, New York, NY 10003 USA.	arimorcos@fb.com; haonanu@gmail.com; michela@fb.com; yuandong@fb.com						Aila T., 2016, ARXIV PREPRINT ARXIV; Allen-Zhu Zeyuan, 2018, LEARNING GEN OVER PA; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Arvin AM, 2009, LIVE VARIOLA VIRUS: CONSIDERATIONS FOR CONTINUING RESEARCH, P9; Ayinde BO, 2019, NEURAL NETWORKS, V118, P148, DOI 10.1016/j.neunet.2019.04.021; Du S. S., 2018, GRADIENT DESCENT PRO; Du Simon S, 2018, POWER PARAMETRIZATIO; Gale T., 2019, STATE SPARSITY DEEP; Gilmer Justin, 2016, INT C LEARN REPR; Glorot X., 2010, PROC MACH LEARN RES, P249; Guo YW, 2016, ADV NEUR IN, V29; Hanin B, 2018, ADV NEUR IN, V31; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Huang XW, 2015, ACTA POLYM SIN, P1133; Kingma D.P, P 3 INT C LEARNING R; Kornblith Simon, 2018, DO BETTER IMAGENET M; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Liu Z, 2019, PATTERN ANAL APPL, V22, P1527, DOI 10.1007/s10044-019-00792-5; Molchanov D, 2017, PR MACH LEARN RES, V70; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Neyshabur B., 2019, INT C LEARN REPR; Neyshabur Behnam, 2014, ARXIV14126614; Paszke A., 2017, AUTOMATIC DIFFERENTI; Pretorius Arnu, 2018, ADV NEURAL INFORM PR, V31, P5717; Qin Zhuwei, 2019, INTERPRETABLE CONVOL; SONG Z., 2018, CONVERGENCE THEORY D; Tan Chuanqi, 2018, ICANN; Xiao H., 2017, FASHION MNIST NOVEL; Yang Greg, 2018, INT C LEARN REPR WOR; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Han, 2019, ICLR; Zhou BL, 2018, IEEE T PATTERN ANAL, V40, P1452, DOI 10.1109/TPAMI.2017.2723009; Zoph B., 2017, LEARNING TRANSFERABL	37	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304089
C	Qu, M; Tang, J		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Qu, Meng; Tang, Jian			Probabilistic Logic Neural Networks for Reasoning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Knowledge graph reasoning, which aims at predicting the missing facts through reasoning with the observed facts, is critical to many applications. Such a problem has been widely explored by traditional logic rule-based approaches and recent knowledge graph embedding methods. A principled logic rule-based approach is the Markov Logic Network (MLN), which is able to leverage domain knowledge with first-order logic and meanwhile handle the uncertainty. However, the inference in MLNs is usually very difficult due to the complicated graph structures. Different from MLNs, knowledge graph embedding methods (e.g. TransE, DistMult) learn effective entity and relation embeddings for reasoning, which are much more effective and efficient. However, they are unable to leverage domain knowledge. In this paper, we propose the probabilistic Logic Neural Network (pLogicNet), which combines the advantages of both methods. A pLogicNet defines the joint distribution of all possible triplets by using a Markov logic network with first-order logic, which can be efficiently optimized with the variational EM algorithm. In the E-step, a knowledge graph embedding model is used for inferring the missing triplets, while in the M-step, the weights of logic rules are updated based on both the observed and predicted triplets. Experiments on multiple knowledge graphs prove the effectiveness of pLogicNet over many competitive baselines.	[Qu, Meng; Tang, Jian] Mila Quebec AI Inst, Montreal, PQ, Canada; [Qu, Meng] Univ Montreal, Montreal, PQ, Canada; [Tang, Jian] HEC Montreal, Montreal, PQ, Canada; [Tang, Jian] CIFAR AI Res Chair, Montreal, PQ, Canada	Universite de Montreal; Universite de Montreal; HEC Montreal	Qu, M (corresponding author), Mila Quebec AI Inst, Montreal, PQ, Canada.; Qu, M (corresponding author), Univ Montreal, Montreal, PQ, Canada.				Natural Sciences and Engineering Research Council of Canada; Canada CIFAR AI Chair Program	Natural Sciences and Engineering Research Council of Canada(Natural Sciences and Engineering Research Council of Canada (NSERC)CGIAR); Canada CIFAR AI Chair Program	We would like to thank all the reviewers for the insightful comments. We also thank Prof. Guillaume Rabusseau and Weiping Song for their valuable feedback. Jian Tang is supported by the Natural Sciences and Engineering Research Council of Canada, and the Canada CIFAR AI Chair Program.	[Anonymous], 1999, UAI; [Anonymous], 2013, NEURIPS; [Anonymous], 2014, ICLR; Besag J., 1975, STATISTICIAN; Bollacker K., 2008, SIGMOD; Burke R., 2000, ENCY LIB INFORM SYST; Costa V. S., 2002, UAI; Das Rajarshi, 2018, INT C LEARN REPR; Dettmers T., 2018, AAAI; Ding B., 2018, ACL; Galarraga LA, 2013, WWW; Gershman Samuel, 2014, COGSCI; Giarratano J. C., 1998, EXPERT SYSTEMS; Gilks W. R., 1995, MARKOV CHAIN MONTE C, DOI 10.1201/b14835; Google, FREEB DAT DUMPS; Guo Shu, 2018, AAAI; Hoffman M. D., 2013, J MACHINE LEARNING R; Jackson P., 1998, INTRO EXPERT SYSTEMS, V3rd ed.; Kadlec Rudolf, 2017, WORKSH REPR LEARN NL; Kersting K., 2001, ICILP; Kersting Kristian, 2008, PROBABILISTIC INDUCT; Kok S., 2005, ICML; Lin X. V., 2018, EMNLP; Maasch M, 2016, IEEE MTT S INT MICR; Miller George A, 1995, COMMUNICATIONS ACM; Neal R. M., 1998, LEARNING GRAPHICAL M; Nickel M., 2016, AAAI; Opper M., 2001, ADV MEAN FI ELD METH; Poole D., 1993, ARTIFICIAL INTELLIGE; Popescul A., 2003, KDD WORKSH MULT DAT, P133; Qi Siyuan, 2018, ECCV; Qu Meng, 2019, ICML; Qu Meng, 2018, WWW; Richardson M., 2006, MACHINE LEARNING; Salakhutdinov R., 2010, AISTATS; Schlichtkrull M., 2018, EUROPEAN SEMANTIC WE; Shen Y., 2018, NEURIPS; Singla P., 2005, AAAI; Sun Z., 2019, ICLR; Taskar B., 2002, UAI; Taskar B, 2007, INTRO STAT RELATIONA; Toutanova K., 2015, CVSC; Trouillon T., 2016, ICML; Wang Z., 2014, AAAI; Wellman M. P., 1992, KNOWLEDGE ENG REV; Xiong Wenhan, 2017, EMNLP; Yang B., 2015, ICLR; Yao X., 2014, ACL; Zhang Y., 2019, ARXIV190602111; Zheng Zhedong, 2019, CVPR	50	30	30	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307070
C	Rabanser, S; Gunnemann, S; Lipton, ZC		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Rabanser, Stephan; Guennemann, Stephan; Lipton, Zachary C.			Failing Loudly: An Empirical Study of Methods for Detecting Dataset Shift	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We might hope that when faced with unexpected inputs, well-designed software systems would fire off warnings. Machine learning (ML) systems, however, which depend strongly on properties of their inputs (e.g. the i.i.d. assumption), tend to fail silently. This paper explores the problem of building ML systems that fail loudly, investigating methods for detecting dataset shift, identifying exemplars that most typify the shift, and quantifying shift malignancy. We focus on several datasets and various perturbations to both covariates and label distributions with varying magnitudes and fractions of data affected. Interestingly, we show that across the dataset shifts that we explore, a two-sample-testing-based approach, using pre-trained classifiers for dimensionality reduction, performs best. Moreover, we demonstrate that domain-discriminating approaches tend to be helpful for characterizing shifts qualitatively and determining if they are harmful.	[Rabanser, Stephan] AWS AI Labs, New York, NY USA; [Guennemann, Stephan] Tech Univ Munich, Munich, Germany; [Rabanser, Stephan; Lipton, Zachary C.] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA	Technical University of Munich; Carnegie Mellon University	Rabanser, S (corresponding author), AWS AI Labs, New York, NY USA.; Rabanser, S (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	rabans@amazon.com; guennemann@in.tum.de; zlipton@cmu.edu						Achlioptas D., 2003, J COMPUTER SYSTEM SC, V66; Alemi A. A., 2018, UNCERTAINTY VARIATIO; Ben-David S., 2010, INT C ART INT STAT A; BLAND JM, 1995, BRIT MED J, V310, P170, DOI 10.1136/bmj.310.6973.170; Bojarski Mariusz, 2016, arXiv; Breunig M. M., 2000, SIGMOD Record, V29, P93, DOI 10.1145/335191.335388; Chandola V, 2009, ACM COMPUT SURV, V41, DOI 10.1145/1541880.1541882; Cheng H, 2016, ROCK MECH ROCK ENG, V49, P3481, DOI 10.1007/s00603-016-0998-9; Choi Hyunsun, 2018, WAIC WHY GENERATIVE; Chollet F., 2015, KERAS; Christian Szegedy, 2014, Arxiv, DOI arXiv:1312.6199; Covington P, 2016, PROCEEDINGS OF THE 10TH ACM CONFERENCE ON RECOMMENDER SYSTEMS (RECSYS'16), P191, DOI 10.1145/2959100.2959190; Fei Tony Liu, 2008, INT C DAT MIN ICDM; Goodfellow IJ, 2014, 3 INT C LEARNING REP; Graves A., 2013, P 2013 IEEE INT C AC, P6645, DOI [10.1109/ICASSP.2013.6638947, DOI 10.1109/ICASSP.2013.6638947]; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gretton A, 2009, NEURAL INF PROCESS S, P131; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Heard Nicholas A, 2018, BIOMETRIKA; Hendrycks D., 2019, ICLR, P1; Hinton G, 2012, IEEE SIGNAL PROC MAG, V29, P82, DOI 10.1109/MSP.2012.2205597; Howard Steven R, 2018, ARXIV181008240; John Simes R, 1986, BIOMETRIKA; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Lakhani P, 2017, RADIOLOGY, V284, P574, DOI 10.1148/radiol.2017162326; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Lee Kimin, 2018, INT C LEARN REPR; Liang Shiyu, 2018, INT C LEARN REPR ICL, P1; Lipton Zachary, 2018, ARXIV180203916, P3122; Long MS, 2013, IEEE I CONF COMP VIS, P2200, DOI 10.1109/ICCV.2013.274; Loughin T., 2004, COMPUTATIONAL STAT D; Markou Markos, 2003, SIGNAL PROCESSING; Nene Sameer A, 1996, COIL100; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Ping Li, 2006, P 12 ACM SIGKDD INT; Ramdas Aaditya, 2015, DECREASING POWER KER; Saerens Marco, 2002, NEURAL COMPUTATION; Sch_olkopf B., 2012, P ICML, P1255; Schlegl T, 2017, LECT NOTES COMPUT SC, V10265, P146, DOI 10.1007/978-3-319-59050-9_12; Scholkopf B, 2000, ADV NEUR IN, V12, P582; Sculley D., 2014, SE4ML SOFTW ENG MACH; Shafaei A., 2018, ARXIV180904729; Shalev G, 2018, ADV NEUR IN, V31; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Stone Zak, 2008, IEEE COMP SOC C COMP; Storkey A, 2009, NEURAL INF PROCESS S, P3; Sugiyama M., 2008, NIPS, P1433; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Truong C., 2018, ARXIV180100718; Vovk Vladimir, 2018, ARXIV12124966; Xiao H., 2017, FASHION MNIST NOVEL; Yee Seng Chan, 2005, INT JOINT C ART INT; Zaykin Dmitri V, 2002, GENETIC EPIDEMIOLOGY; Zhang L, 2013, PROCEEDINGS OF 2013 CHINA INTERNATIONAL CONFERENCE ON INSURANCE AND RISK MANAGEMENT, P819; Zugner D, 2018, INT C KNOWL DISC DAT	57	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424301039
C	Sun, ZQ; Li, ZH; Wang, HQ; He, D; Lin, Z; Deng, ZH		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sun, Zhiqing; Li, Zhuohan; Wang, Haoqing; He, Di; Lin, Zi; Deng, Zhi-Hong			Fast Structured Decoding for Sequence Models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Autoregressive sequence models achieve state-of-the-art performance in domains like machine translation. However, due to the autoregressive factorization nature, these models suffer from heavy latency during inference. Recently, non-autoregressive sequence models were proposed to reduce the inference time. However, these models assume that the decoding process of each token is conditionally independent of others. Such a generation process sometimes makes the output sentence inconsistent, and thus the learned non-autoregressive models could only achieve inferior accuracy compared to their autoregressive counterparts. To improve the decoding consistency and reduce the inference cost at the same time, we propose to incorporate a structured inference module into the non-autoregressive models. Specifically, we design an efficient approximation for Conditional Random Fields (CRF) for non-autoregressive sequence models, and further propose a dynamic transition technique to model positional contexts in the CRF. Experiments in machine translation show that while increasing little latency (8 similar to 14ms), our model could achieve significantly better translation performance than previous non-autoregressive models on different translation datasets. In particular, for the WMT14 En-De dataset, our model obtains a BLEU score of 26.80, which largely outperforms the previous non-autoregressive baselines and is only 0.61 lower in BLEU than purely autoregressive models.(1)	[Sun, Zhiqing] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Li, Zhuohan] Univ Calif Berkeley, Berkeley, CA 94720 USA; [Wang, Haoqing; He, Di; Lin, Zi; Deng, Zhi-Hong] Peking Univ, Beijing, Peoples R China; [Sun, Zhiqing; Li, Zhuohan] Microsoft Res Asia, Beijing, Peoples R China	Carnegie Mellon University; University of California System; University of California Berkeley; Peking University; Microsoft; Microsoft Research Asia	Sun, ZQ (corresponding author), Carnegie Mellon Univ, Pittsburgh, PA 15213 USA.	zhiqings@cs.cmu.edu; zhuohan@cs.berkeley.edu; wanghaoqing@pku.edu.cn; di_he@pku.edu.cn; zi.lin@pku.edu.cn; zhdeng@pku.edu.cn						Andor Daniel, 2016, ARXIV160306042; Bahdanau Dzmitry, 2016, ARXIV160707086; Bengio Y., 2014, ARXIV14061078; Brown P. F., 1993, Computational Linguistics, V19, P263; Collins Michael, 2013, FORWARD BACKWARD ALG; Collobert R, 2011, J MACH LEARN RES, V12, P2493; Dzmitry Bahdanau, 2016, Arxiv, DOI arXiv:1409.0473; Edunov Sergey, 2017, ARXIV171104956; Gehring J., 2017, P ICML; Gu Jiatao, 2017, ARXIV171102281; Guo Junliang, 2018, ARXIV181209664; Hinton G., 2015, ARXIV150302531; Kaiser Lukasz, 2018, ARXIV180303382, P2390; Kim Y., 2017, ABS170200887 CORR; Kim Yoon, 2016, ARXIV160607947, DOI [10.18653/v1/D16-1139, DOI 10.18653/V1/D16-1139]; Kim Yoon, 2018, ARXIV181206834; Kingma D.P, P 3 INT C LEARNING R; Lavergne Thomas, 2011, WORKSH STAT MACH TRA, P542; Li ZH, 2019, 2019 CONFERENCE ON EMPIRICAL METHODS IN NATURAL LANGUAGE PROCESSING AND THE 9TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (EMNLP-IJCNLP 2019), P5708; Libovick Jind.rich, 2018, P 2018 C EMP METH NA, P3016, DOI DOI 10.18653/V1/D18-1336; McCallum A., 2000, P 17 INT C MACH LEAR, P591; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Riloff Ellen, 2018, P 2018 C EMP METH NA; Roy A.G., 2018, ARXIV PREPRINT ARXIV; Sutton C, 2012, FOUND TRENDS MACH LE, V4, P267, DOI 10.1561/2200000013; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Tran Ke, 2016, ARXIV160909007; Vaswani A., 2018, MT RES TRACK; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Wang Yiren, 2019, ARXIV190210245; Wu Yonghui, 2016, GOOGLES NEURAL MACHI	32	30	32	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303005
C	Togninalli, M; Ghisu, E; Llinares-Lopez, F; Rieck, B; Borgwardt, K		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Togninalli, Matteo; Ghisu, Elisabetta; Llinares-Lopez, Felipe; Rieck, Bastian; Borgwardt, Karsten			Wasserstein Weisfeiler-Lehman Graph Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Most graph kernels are an instance of the class of R-Convolution kernels, which measure the similarity of objects by comparing their substructures. Despite their empirical success, most graph kernels use a naive aggregation of the final set of substructures, usually a sum or average, thereby potentially discarding valuable information about the distribution of individual components. Furthermore, only a limited instance of these approaches can be extended to continuously attributed graphs. We propose a novel method that relies on the Wasserstein distance between the node feature vector distributions of two graphs, which allows finding subtler differences in data sets by considering graphs as high-dimensional objects rather than simple means. We further propose a Weisfeiler-Lehman-inspired embedding scheme for graphs with continuous node attributes and weighted edges, enhance it with the computed Wasserstein distance, and thereby improve the state-of-the-art prediction performance on several graph classification tasks.	[Togninalli, Matteo; Ghisu, Elisabetta; Llinares-Lopez, Felipe; Rieck, Bastian; Borgwardt, Karsten] Swiss Fed Inst Technol, Dept BIosyst Sci & Engn, Zurich, Switzerland; [Togninalli, Matteo; Ghisu, Elisabetta; Llinares-Lopez, Felipe; Rieck, Bastian; Borgwardt, Karsten] SIB, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich; Swiss Institute of Bioinformatics	Togninalli, M (corresponding author), Swiss Fed Inst Technol, Dept BIosyst Sci & Engn, Zurich, Switzerland.; Togninalli, M (corresponding author), SIB, Lausanne, Switzerland.	matteo.togninalli@bsse.ethz.ch; elisabetta.ghisu@bsse.ethz.ch; felipe.llinares@bsse.ethz.ch; bastian.rieck@bsse.ethz.ch; karsten.borgwardt@bsse.ethz.ch	Rieck, Bastian/J-7507-2019	Rieck, Bastian/0000-0003-4335-0302	Horizon 2020 project CDS-QUAMRI [634541]; Alfried Krupp Prize for Young University Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung; SNSF Starting Grant "Significant Pattern Mining"	Horizon 2020 project CDS-QUAMRI; Alfried Krupp Prize for Young University Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung; SNSF Starting Grant "Significant Pattern Mining"	This work was funded in part by the Horizon 2020 project CDS-QUAMRI, Grant No. 634541 (E.G., K.B.), the Alfried Krupp Prize for Young University Teachers of the Alfried Krupp von Bohlen und Halbach-Stiftung (B.R., K.B.), and the SNSF Starting Grant "Significant Pattern Mining" (F.L., K.B.).	Altschuler J., 2017, ADV NEURAL INFORM PR, P1964; Araya M., 2015, P ADV NEUR INF PROC, P2053; Arjovsky M., 2017, ARXIV170107875; Balcan MF, 2008, MACH LEARN, V72, P89, DOI 10.1007/s10994-008-5059-5; Berg C., 1984, HARMONIC ANAL SEMIGR; Borgwardt KM, 2005, Fifth IEEE International Conference on Data Mining, Proceedings, P74, DOI 10.1109/ICDM.2005.132; Borgwardt KM, 2005, BIOINFORMATICS, V21, pI47, DOI 10.1093/bioinformatics/bti1007; Bridson MR., 2013, METRIC SPACES NONPOS; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Duvenaud David K, 2015, P NIPS; Feragen A, 2015, PROC CVPR IEEE, P3032, DOI 10.1109/CVPR.2015.7298922; Feragen Aasa, 2013, NIPS, P216; Figalli A, 2011, LECT NOTES MATH, V2028, P171, DOI 10.1007/978-3-642-21861-3_4; Flamary R<prime>emi, 2017, POT PYTHON OPTIMAL T; Frohlich H, 2005, IEEE IJCNN, P250; Gaertner Thomas, 2018, INT C MACH LEARN, P3859; Gardner A., 2017, IEEE T CYBERNETICS; Haasdonk B., 2004, DAGM S; Haussler D, 1999, TECHNICAL REPORT; Inokuchi A., 2003, INT C MACHINE LEARNI, P321; Kersting Kristian, 2016, BENCHMARK DATA SETS; Kipf T. N., 2017, 5 INT C LEARN REPR; Klicpera J., 2019, INT C LEARN REPR ICL; Kolouri S, 2016, PROC CVPR IEEE, P5258, DOI 10.1109/CVPR.2016.568; Kriege N.M., 2012, INT C MACHINE LEARNI, P291; Kriege Nils M., 2016, ADV NEURAL INFORM PR; Loosli G, 2016, IEEE T PATTERN ANAL, V38, P1204, DOI 10.1109/TPAMI.2015.2477830; Morris C, 2016, IEEE DATA MINING, P1095, DOI [10.1109/ICDM.2016.0142, 10.1109/ICDM.2016.114]; Ohta S, 2012, ADV GEOM, V12, P571, DOI 10.1515/advgeom-2011-058; ONG C. S., 2004, P 21 INT C MACH LEAR; Peyre G, 2019, FOUND TRENDS MACH LE, V11, P355, DOI 10.1561/2200000073; Rabin J, 2012, LECT NOTES COMPUT SC, V6667, P435, DOI 10.1007/978-3-642-24785-9_37; Rieck B, 2019, PR MACH LEARN RES, V97; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Scholkopf B., 2001, LEARNING KERNELS SUP; Shervashidze N., 2009, NEURIPS, P1660; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; STEVENS SS, 1946, SCIENCE, V103, P677, DOI 10.1126/science.103.2684.677; Sugiyama M, 2018, BIOINFORMATICS, V34, P530, DOI 10.1093/bioinformatics/btx602; Turner K, 2014, DISCRETE COMPUT GEOM, V52, P44, DOI 10.1007/s00454-014-9604-7; Vert J-P, 2008, ARXIV08014061; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Xu H., 2019, ICML, V97, P6932; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417	47	30	31	0	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424306044
C	Xiao, X; Wang, ZG; Rajasekaran, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Xiao, Xia; Wang, Zigeng; Rajasekaran, Sanguthevar			AutoPrune: Automatic Network Pruning by Regularizing Auxiliary Parameters	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Reducing the model redundancy is an important task to deploy complex deep learning models to resource-limited or time-sensitive devices. Directly regularizing or modifying weight values makes pruning procedure less robust and sensitive to the choice of hyperparameters, and it also requires prior knowledge to tune different hyperparameters for different models. To build a better generalized and easy-to-use pruning method, we propose AutoPrune, which prunes the network through optimizing a set of trainable auxiliary parameters instead of original weights. The instability and noise during training on auxiliary parameters will not directly affect weight values, which makes pruning process more robust to noise and less sensitive to hyperparameters. Moreover, we design gradient update rules for auxiliary parameters to keep them consistent with pruning tasks. Our method can automatically eliminate network redundancy with recoverability, relieving the complicated prior knowledge required to design thresholding functions, and reducing the time for trial and error. We evaluate our method with LeNet and VGG-like on MNIST and CIFAR-10 datasets, and with AlexNet, ResNet and MobileNet on ImageNet to establish the scalability of our work. Results show that our model achieves state-of-the-art sparsity, e.g. 7%, 23% FLOPs and 310x, 75x compression ratio for LeNet5 and VGG-like structure without accuracy drop, and 200M and 100M FLOPs for MobileNet V2 with accuracy 73.32% and 66.83% respectively.	[Xiao, Xia; Wang, Zigeng; Rajasekaran, Sanguthevar] Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT 06269 USA	University of Connecticut	Rajasekaran, S (corresponding author), Univ Connecticut, Dept Comp Sci & Engn, Storrs, CT 06269 USA.	xia.xiao@uconn.edu; zigeng.wang@uconn.edu; sanguthevar.rajasekaran@uconn.edu			NSF [1447711, 1514357, 1743418, 1843025]	NSF(National Science Foundation (NSF))	This work has been supported in part by the following NSF grants: 1447711, 1514357, 1743418, and 1843025.	Aghasi A., 2017, NEURIPS; Carreira-Perpinan MA, 2018, PROC CVPR IEEE, P8532, DOI 10.1109/CVPR.2018.00890; Cun YL., 1990, ADV NEURAL INF PROCE, P598, DOI DOI 10.5555/109230.109298; Dong X, 2017, ADV NEUR IN, V30; Gomez A. N., 2019, ARXIV190513678; Gordon A, 2018, PROC CVPR IEEE, P1586, DOI 10.1109/CVPR.2018.00171; Guo YW, 2016, ADV NEUR IN, V29; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Hu H., 2016, ARXIV PREPRINT ARXIV; Huang XW, 2015, ACTA POLYM SIN, P1133; Hubara I, 2016, ADV NEUR IN, V29; Li GY, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P2383; Li H., 2017, P INT C LEARN REPR I, P1; Liu Hanxiao, 2019, INTERNATIONAL CONFER; Liu Z, 2019, PATTERN ANAL APPL, V22, P1527, DOI 10.1007/s10044-019-00792-5; Louizos C, 2017, ADV NEUR IN, V30; Louizos Christos, 2018, INT C LEARN REPR; Maqueda AI, 2018, PROC CVPR IEEE, P5419, DOI 10.1109/CVPR.2018.00568; Molchanov D, 2017, PR MACH LEARN RES, V70; Neklyudov Kirill, 2017, ADV NEURAL INFORM PR, V30; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Sandler M, 2018, PROC CVPR IEEE, P4510, DOI 10.1109/CVPR.2018.00474; Srinivas S, 2017, IEEE COMPUT SOC CONF, P455, DOI 10.1109/CVPRW.2017.61; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Tan MX, 2019, PROC CVPR IEEE, P2815, DOI 10.1109/CVPR.2019.00293; Tartaglione E, 2018, ADV NEUR IN, V31; Ullrich Karen, 2017, ICLR; Wen W, 2016, ADV NEUR IN, V29; Yin Pengcheng, 2019, ICLR; Yu J., 2019, AUTOSLIM ONE SHOT AR; Yu JH, 2019, IEEE I CONF COMP VIS, P1803, DOI 10.1109/ICCV.2019.00189; Yu Jiahui, 2018, ARXIV181208928; Zhu XT, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P3264; Zhuang ZW, 2018, ADV NEUR IN, V31	35	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866905036
C	Kurutach, T; Tamar, A; Yang, G; Russell, S; Abbeel, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Kurutach, Thanard; Tamar, Aviv; Yang, Ge; Russell, Stuart; Abbeel, Pieter			Learning Plannable Representations with Causal InfoGAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				FRAMEWORK	In recent years, deep generative models have been shown to 'imagine' convincing high-dimensional observations such as images, audio, and even video, learning directly from raw data. In this work, we ask how to imagine goal-directed visual plans - a plausible sequence of observations that transition a dynamical system from its current configuration to a desired goal state, which can later be used as a reference trajectory for control. We focus on systems with high-dimensional observations, such as images, and propose an approach that naturally combines representation learning and planning. Our framework learns a generative model of sequential observations, where the generative process is induced by a transition in a low-dimensional planning model, and an additional noise. By maximizing the mutual information between the generated observations and the transition in the planning model, we obtain a low-dimensional representation that best explains the causal nature of the data. We structure the planning model to be compatible with efficient planning algorithms, and we propose several such models based on either discrete or continuous states. Finally, to generate a visual plan, we project the current and goal observations onto their respective states in the planning model, plan a trajectory, and then use the generative model to transform the trajectory to a sequence of observations. We demonstrate our method on imagining plausible visual plans of rope manipulation(3).	[Kurutach, Thanard; Tamar, Aviv; Russell, Stuart; Abbeel, Pieter] Univ Calif Berkeley, Berkeley AI Res, Berkeley, CA 94720 USA; [Yang, Ge] Univ Chicago, Dept Phys, Chicago, IL 60637 USA	University of California System; University of California Berkeley; University of Chicago	Kurutach, T (corresponding author), Univ Calif Berkeley, Berkeley AI Res, Berkeley, CA 94720 USA.				ONR PECASE [N000141612723]; Siemens; Technion Viterbi scholarship	ONR PECASE(Office of Naval Research); Siemens(Siemens AG); Technion Viterbi scholarship	This work was funded in part by ONR PECASE N000141612723 and Siemens. Thanard Kurutach and Aviv Tamar were supported by ONR PECASE N000141612723. Aviv Tamar was partially supported by the Technion Viterbi scholarship. The authors wish to thank Tom Zahavy and Aravind Srinivas for sharing their code for our comparisons with MDP state aggregation baselines, and Elinor Tamar for timely assistance in preprocessing the rope data.	Agrawal P., 2016, ADV NEURAL INFORM PR, P5074; Andrychowicz M., 2017, ADV NEURAL INFORM PR; Baram  N., 2016, SPATIO TEMPORAL ABST; Battaglia PW, 2013, P NATL ACAD SCI USA, V110, P18327, DOI 10.1073/pnas.1306572110; Chung J., 2015, ADV NEURAL INFORM PR, V28, P2980; Corneil D, 2018, PR MACH LEARN RES, V80; Cover T. M., 1991, Elements of information theory, DOI 10.1002/0471200611; Donahue J., 2016, ARXIV160509782; Fikes R. E., 1981, READINGS ARTIFICIAL, P231; Finn, 2018, ARXIV180400645, P4732; Finn Chelsea, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2786, DOI 10.1109/ICRA.2017.7989324; Finn C, 2016, PR MACH LEARN RES, V48; Finn C, 2016, IEEE INT CONF ROBOT, P512, DOI 10.1109/ICRA.2016.7487173; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Jang E., 2016, ARXIV; Kansky K, 2017, PR MACH LEARN RES, V70; Karras T., 2017, PROGR GROWING GANS I; Kingma D. P., 2013, AUTO ENCODING VARIAT; Klein U., 2004, P 21 INT C MACH LEAR, P71, DOI DOI 10.1145/1015330.1015355; Koltun, 2018, ICLR, P1; Konidaris G, 2018, J ARTIF INTELL RES, V61, P215, DOI 10.1613/jair.5575; Lerer A, 2016, PR MACH LEARN RES, V48; Levine S., 2016, JMLR, V17; Liu M, 2017, ARXIV171204065; Machado MC, 2017, PR MACH LEARN RES, V70; Mahadevan S, 2007, J MACH LEARN RES, V8, P2169; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Nair Ashvin, 2017, 2017 IEEE International Conference on Robotics and Automation (ICRA), P2146, DOI 10.1109/ICRA.2017.7989247; Oh J, 2017, ADV NEUR IN, V30; Pong V., 2018, INT C LEARN REPR ICL; Ravindran B., 2016, ARXIV160505359; Riedmiller M, 2018, PR MACH LEARN RES, V80; Russell S., 2010, ARTIF INTELL, DOI DOI 10.1136/gutjnl-2018-317500; Salimans T, 2016, ADV NEUR IN, V29; Santoro A, 2017, ADV NEUR IN, V30; Simester DI, 2006, MANAGE SCI, V52, P683, DOI 10.1287/mnsc.1050.0504; Sutton R. S., 1998, REINFORCEMENT LEARNI, V1; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tamar Aviv, 2016, ADV NEURAL INFORM PR, P2154, DOI DOI 10.5555/3171837.3171991; Vallati M, 2015, AI MAG, V36, P90, DOI 10.1609/aimag.v36i3.2571; Wang  W., 2017, ARXIV171108534; Watter Manuel, 2015, ADV NEURAL INFORM PR, P2746; Watters N, 2017, ARXIV170601433; Wu JJ, 2017, ADV NEUR IN, V30	49	30	30	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003030
C	Scaman, K; Virmaux, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Scaman, Kevin; Virmaux, Aladin			Lipschitz regularity of deep neural networks: analysis and efficient estimation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Deep neural networks are notorious for being sensitive to small well-chosen perturbations, and estimating the regularity of such architectures is of utmost importance for safe and robust practical applications. In this paper, we investigate one of the key characteristics to assess the regularity of such methods: the Lipschitz constant of deep learning architectures. First, we show that, even for two layer neural networks, the exact computation of this quantity is NP-hard and state-of-art methods may significantly overestimate it. Then, we both extend and improve previous estimation methods by providing AutoLip, the first generic algorithm for upper bounding the Lipschitz constant of any automatically differentiable function. We provide a power method algorithm working with automatic differentiation, allowing efficient computations even on large convolutions. Second, for sequential neural networks, we propose an improved algorithm named SeqLip that takes advantage of the linear computation graph to split the computation per pair of consecutive layers. Third we propose heuristics on SeqLip in order to tackle very large networks. Our experiments show that SeqLip can significantly improve on the existing upper bounds. Finally, we provide an implementation of AutoLip in the PyTorch environment that may be used to better estimate the robustness of a given neural network to small perturbations or regularize it using more precise Lipschitz estimations.	[Scaman, Kevin; Virmaux, Aladin] Huawei Noahs Ark Lab, Hong Kong, Peoples R China	Huawei Technologies	Scaman, K (corresponding author), Huawei Noahs Ark Lab, Hong Kong, Peoples R China.	kevin.scaman@huawei.com; aladin.virmaux@huawei.com						Abadi M, 2015, P 12 USENIX S OPERAT; Arjovsky M, 2017, PR MACH LEARN RES, V70; Balan R., 2018, CONT MATH; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); FEDERER H., 1969, CLASSICS MATH; Goodfellow I.J., 2015, STATISTICAL, DOI DOI 10.48550/ARXIV.1412.6572; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Graves A, 2014, PR MACH LEARN RES, V32, P1764; Griewank A, 2008, OTHER TITL APPL MATH, V105, P1, DOI 10.1137/1.9780898717761; Gulrajani I, 2017, P NIPS 2017; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; LeCun Yann, MNIST DATABASE HANDW; Linnainmaa S., 1970, REPRESENTATION CUMUL; Magnus JR., 1985, ECONOMET THEOR, V1, P179, DOI [10.1017/S0266466600011129, DOI 10.1017/S0266466600011129]; Mikolov T., 2013, P 2013 C N AM CHAPTE, P746, DOI DOI 10.3109/10826089109058901; Mises RV, 1929, ZAMM J APPL MATH MEC, V9, P152, DOI [10.1002/zamm.19290090206, DOI 10.1002/ZAMM.19290090105, DOI 10.1002/ZAMM.19290090206]; Oord A.V.D., 2016, SSW; Rall L. B., 1981, LECT NOTES COMPUTER, V120; Szegedy C., 2016, P IEEE C COMP VIS PA, P2818, DOI DOI 10.1109/CVPR.2016.308; Szegedy Christian, 2014, P 2 INT C LEARNING R; Vaswani A., 2017, ADV NEURAL INFORM PR, V30; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; von Luxburg U, 2004, J MACH LEARN RES, V5, P669; Weng Tsui-Wei, 2018, P INT C LEARN REPR I	29	30	30	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303080
C	Wilson, JT; Hutter, F; Deisenroth, MP		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Wilson, James T.; Hutter, Frank; Deisenroth, Marc Peter			Maximizing acquisition functions for Bayesian optimization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Bayesian optimization is a sample-efficient approach to global optimization that relies on theoretically motivated value heuristics (acquisition functions) to guide its search process. Fully maximizing acquisition functions produces the Bayes' decision rule, but this ideal is difficult to achieve since these functions are frequently non-trivial to optimize This statement is especially true when evaluating queries in parallel, where acquisition functions are routinely non-convex, high-dimensional, and intractable. We first show that acquisition functions estimated via Monte Carlo integration are consistently amenable to gradient-based optimization. Subsequently, we identify a common family of acquisition functions, including EI and UCB, whose properties not only facilitate but justify use of greedy approaches for their maximization.	[Wilson, James T.; Deisenroth, Marc Peter] Imperial Coll London, London, England; [Hutter, Frank] Univ Freiburg, Freiburg, Germany; [Deisenroth, Marc Peter] PROWLER Io, Cambridge, England	Imperial College London; University of Freiburg	Wilson, JT (corresponding author), Imperial Coll London, London, England.	j.wilson17@imperial.ac.uk			EPSRC Centre for Doctoral Training in High Performance Embedded and Distributed Systems [EP/L016796/1]; European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme [716721]	EPSRC Centre for Doctoral Training in High Performance Embedded and Distributed Systems(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC))	The authors thank David Ginsbourger, Dario Azzimonti and Henry Wynn for initial discussions regarding the submodularity of various integrals. The support of the EPSRC Centre for Doctoral Training in High Performance Embedded and Distributed Systems (reference EP/L016796/1) is gratefully acknowledged. This work has partly been supported by the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme under grant no. 716721.	[Anonymous], 2017, P ADV NEURAL INFORM; Azimi J, 2010, ADV NEURAL INFORM PR, V23, P109; Bach F., 2013, FDN TRENDS MACHINE L, V6; Bansal S., 2017, ARXIV170309260; Bergstra J, 2012, J MACH LEARN RES, V13, P281; Bochner Salomon, 1959, LECT FOURIER INTEGRA, V42; Bousquet O., 2008, ADV NEURAL INFORM PR; Calandra R., 2016, ANN MATH ARTIFICIAL, V76; Cao X., 1985, IEEE T AUTOMATIC CON, V30; Chen Y., NEAR OPTIMAL BATCH M; Chevalier Clement, 2013, Learning and Intelligent Optimization. 7th International Conference, LION 7. Revised Selected Papers: LNCS 7997, P59, DOI 10.1007/978-3-642-44973-4_7; Christian R., 2007, BAYESIAN CHOICE DECI; Cunningham, 2011, ARXIV11116832; DeGroot M.H., 2005, OPTIMAL STAT DECISIO; Desautels T, 2014, J MACH LEARN RES, V15, P3873; Falkner S, 2018, PR MACH LEARN RES, V80; Frazier PI, 2016, SPRINGER SER MATER S, V225, P45, DOI 10.1007/978-3-319-23871-5_3; Gassmann H. I., 2002, J COMPUTATIONAL GRAP, V11; Gelbart MA, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P250; Genz A., 1992, J COMPUT GRAPH STAT, V1, P141, DOI [DOI 10.1080/10618600.1992.10477010, 10.2307/1390838]; Genz A., 2004, STAT COMPUTING, V14; Ginsbourger D., 2011, INT C ERCIM WG COMP; Ginsbourger D., 2010, KRIGING IS WELL SUIT; Glasserman P., 1988, SIM C P 1988 WINT IE; Gradshteyn I. S., 2014, TABLE INTEGRALS SERI; Hansen N., 2016, ARXIV160400772; Hennig P., 2012, J MACHINE LEARNING R; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Jamieson K., 2016, ARTIFICIAL INTELLIGE; Jang E., 2016, ARXIV; Jones Donald R., 1993, J OPTIMIZATION THEOR; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Karnin Zohar, 2013, P 30 INT C INT C MAC, P1238; Kathuria T., 2016, ADV NEURAL INFORM PR, P4206; Kingma D.P, P 3 INT C LEARNING R; Kotz S., 2004, MULTIVARIATE T DISTR; Krause Andreas, 2014, SUBMODULAR FUNCTION, P5; Kushner H. J., 1964, J BASIC ENG, V86; Maddison Chris J, 2016, ARXIV161100712; Martinez-Cantin R., 2014, J MACHINE LEARNING R, V15; Minoux M., 1978, Proceedings of the 8th IFIP Conference on Optimization Techniques, P234, DOI 10.1007/BFb0006528; Mockus J., 1994, J GLOBAL OPTIMIZATIO, V4; Mokus J., 1975, LECT NOTES COMPUTER, P400, DOI [10.1007/3-540-07165-2_55, DOI 10.1007/3-540-07165-2_55, DOI 10.1007/978-3-662-38527-2_55, 10.1007/3-540-07165-2, DOI 10.1007/3-540-07165-2]; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Osborne M, 2009, LEARNING INTELLIGENT, P1; Rahimi A, 2007, PROC 20 INT C NEURAL, P1177, DOI DOI 10.5555/2981562.2981710; Rezende Danilo Jimenez, 2014, P 31 INT C INT C MAC; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Shah A, 2015, ADV NEURAL INFORM PR, V12, P3330; Shahriari B, 2016, P IEEE, V104, P148, DOI 10.1109/JPROC.2015.2494218; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Springenberg J., 2016, ADV NEURAL INFORM PR; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Swersky K., 2013, ADV NEURAL INFORM PR, P2004, DOI DOI 10.1038/S41598-021-83582-6; Ueno T., 2016, MAT DISCOVERY, V4; Viana F., 2010, AIAA ISSMO MULT AN O; Wang J., 2016, ARXIV160205149; Wang Z, 2017, PR MACH LEARN RES, V70; Wu Jiajun, 2016, ADV NEURAL INFORM PR	61	30	30	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852004044
C	Ghodsi, Z; Gu, TY; Garg, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Ghodsi, Zahra; Gu, Tianyu; Garg, Siddharth			SafetyNets: Verifiable Execution of Deep Neural Networks on an Untrusted Cloud	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Inference using deep neural networks is often outsourced to the cloud since it is a computationally demanding task. However, this raises a fundamental issue of trust. How can a client be sure that the cloud has performed inference correctly? A lazy cloud provider might use a simpler but less accurate model to reduce its own computational load, or worse, maliciously modify the inference results sent to the client. We propose SafetyNets, a framework that enables an untrusted server (the cloud) to provide a client with a short mathematical proof of the correctness of inference tasks that they perform on behalf of the client. Specifically, SafetyNets develops and implements a specialized interactive proof (IP) protocol for verifiable execution of a class of deep neural networks, i.e., those that can be represented as arithmetic circuits. Our empirical results on three-and four-layer deep neural networks demonstrate the run-time costs of SafetyNets for both the client and server are low. SafetyNets detects any incorrect computations of the neural network by the untrusted server with high probability, while achieving state-of-the-art accuracy on the MNIST digit recognition (99:4%) and TIMIT speech recognition tasks (75:22%).	[Ghodsi, Zahra; Gu, Tianyu; Garg, Siddharth] NYU, New York, NY 10003 USA	New York University	Ghodsi, Z (corresponding author), NYU, New York, NY 10003 USA.	zg451@nyu.edu; tg1553@nyu.edu; sg175@nyu.edu	Jeong, Yongwook/N-7413-2016					Arora S, 2009, COMPUTATIONAL COMPLEXITY: A MODERN APPROACH, P1, DOI 10.1017/CBO9780511804090; Coates Adam, 2011, AISTATS, V6, DOI DOI 10.1177/1753193410390845; Cormode G, 2011, PROC VLDB ENDOW, V5, P25, DOI 10.14778/2047485.2047488; Dowlin N, 2016, PR MACH LEARN RES, V48; Gautier A, 2016, ADV NEUR IN, V29; Gennaro R, 2010, LECT NOTES COMPUT SC, V6223, P465, DOI 10.1007/978-3-642-14623-7_25; Goldwasser S, 2008, ACM S THEORY COMPUT, P113; Goodfellow I. J., 2013, ARXIV13024389; LEE KF, 1989, IEEE T ACOUST SPEECH, V37, P1641, DOI 10.1109/29.46546; Livni R, 2014, ADV NEUR IN, V27; LUND C, 1992, J ACM, V39, P859, DOI 10.1145/146585.146605; Mohassel P., 2017, IACR CRYPTOLOGY EPRI; Monrose F., 1999, NDSS, P3; Papernot Nicolas, 2016, ARXIV161103814; Parno B, 2013, P IEEE S SECUR PRIV, P238, DOI 10.1109/SP.2013.47; Thaler J, 2013, LECT NOTES COMPUT SC, V8043, P71, DOI 10.1007/978-3-642-40084-1_5; Vu V, 2013, P IEEE S SECUR PRIV, P223, DOI 10.1109/SP.2013.48; Wahby RS, 2016, P IEEE S SECUR PRIV, P759, DOI 10.1109/SP.2016.51; Walfish M, 2015, COMMUN ACM, V58, P74, DOI 10.1145/2641562; Zeiler MD, 2013, ARXIV13013557, DOI DOI 10.1007/978-3-319-26532-2_6; Zhang YT, 2016, PR MACH LEARN RES, V48	22	30	31	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649404072
C	Kar, A; Hane, C; Malik, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Kar, Abhishek; Hane, Christian; Malik, Jitendra			Learning a Multi-View Stereo Machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a learnt system for multi-view stereopsis. In contrast to recent learning based methods for 3D reconstruction, we leverage the underlying 3D geometry of the problem through feature projection and unprojection along viewing rays. By formulating these operations in a differentiable manner, we are able to learn the system end-to-end for the task of metric 3D reconstruction. End-to-end learning allows us to jointly reason about shape priors while conforming to geometric constraints, enabling reconstruction from much fewer images (even a single image) than required by classical approaches as well as completion of unseen surfaces. We thoroughly evaluate our approach on the ShapeNet dataset and demonstrate the benefits over classical approaches and recent learning based methods.	[Kar, Abhishek; Hane, Christian; Malik, Jitendra] Univ Calif Berkeley, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Kar, A (corresponding author), Univ Calif Berkeley, Berkeley, CA 94720 USA.	akar@berkeley.edu; chaene@berkeley.edu; malik@berkeley.edu	Jeong, Yongwook/N-7413-2016		NSF [IIS-1212798]; ONR [MURI-N00014-10-1-0933]; Swiss National Science Foundation [165245]	NSF(National Science Foundation (NSF)); ONR(Office of Naval Research); Swiss National Science Foundation(Swiss National Science Foundation (SNSF)European Commission)	This work was supported in part by NSF Award IIS-1212798 and ONR MURI-N00014-10-1-0933. Christian Hane is supported by an "Early Postdoc.Mobility" fellowship No. 165245 from the Swiss National Science Foundation. The authors would like to thank David Fouhey, Saurabh Gupta and Shubham Tulsiani for valuable discussions and Fyusion Inc. for providing GPU hours for the work.	Bao S. Yingze, 2013, C COMP VIS PATT REC; Blanz V., 1999, C COMP GRAPH INT TEC; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Cho K., 2014, C EMP METH NAT LANG; Choy C. B, 2016, EUR C COMP VIS ECCV; Collins R. T., 1996, C COMP VIS PATT REC; Cremers D, 2011, IEEE T PATTERN ANAL, V33, P1161, DOI 10.1109/TPAMI.2010.174; Curless Brian, 1996, C COMP GRAPH INT TEC; Dame A., 2013, C COMP VIS PATT REC; Eigen D., 2014, NEURAL INFORM PROCES; Fan H., 2017, C COMP VIS PATT REC; Flynn J., 2016, C COMP VIS PATT REC; Fu MY, 2016, NEURAL PLAST, V2016, DOI 10.1155/2016/3512098; Furukawa Y., 2010, T PATTERN ANAL MACHI; Garg R., 2016, EUR C COMP VIS ECCV; Gargallo P, 2007, IEEE I CONF COMP VIS, P1364; Girdhar Rohit, 2016, EUR C COMP VIS ECCV; Haene C., 2016, T PATTERN ANAL MACHI; Han Xufeng, 2015, C COMP VIS PATT REC; Hane C., 2014, C COMP VIS PATT REC; Hane C., 2014, INT C 3D VIS 3DV; Hane C., 2017, INT C 3D VIS 3DV; Hane C., 2013, C COMP VIS PATT REC; Hartmann W., 2017, INT C COMP VIS ICCV; Hochreiter S, 1997, NEURAL COMPUTATION; Jaderberg M., 2015, NEURAL INFORM PROCES; Kanade T., 1995, INT C INT ROB SYST I; Kar A., 2015, C COMP VIS PATT REC; Kendall A., 2017, INT C COMP VIS ICCV; Kutulakos KN, 2000, INT J COMPUT VISION, V38, P199, DOI 10.1023/A:1008191222954; Labatut Patrick, 2007, INT C COMP VIS ICCV; Ladicky L., 2014, C COMP VIS PATT REC; LAURENTINI A, 1994, IEEE T PATTERN ANAL, V16, P150, DOI 10.1109/34.273735; Lempitsky V. S., 2007, C COMP VIS PATT REC; Lhuillier M., 2005, T PATTERN ANAL MACHI; Liu SB, 2014, IEEE T PATTERN ANAL, V36, P2074, DOI 10.1109/TPAMI.2014.2315820; Marr D., 1976, RETINA NEOCORTEX, P239, DOI [10.1007/978-1-4684-6775-8, DOI 10.1007/978-1-4684-6775-8_9]; Mayer N., 2016, C COMP VIS PATT REC; Merrell P., 2007, INT C COMP VIS ICCV; Pollard T., 2007, C COMP VIS PATT REC; Pollefeys M, 2008, INT J COMPUT VISION, V78, P143, DOI 10.1007/s11263-007-0086-4; Pollefeys M, 2004, INT J COMPUT VISION, V59, P207, DOI 10.1023/B:VISI.0000025798.50602.3a; Rezende D. J., 2016, NEURAL INFORM PROCES; Riegler G., 2017, INT C 3D VIS 3DV, P2; Ronneberger Olaf, 2015, CORR ABS150504597, DOI [DOI 10.48550/ARXIV.1505.04597, DOI 10.1007/978-3-319-24574-4_28], Patent No. [ArXiv150504597Cs, 150504597]; Savinov N., 2016, C COMP VIS PATT REC; Saxena A., 2005, NEURAL INFORM PROCES; Scharstein D, 2002, INT J COMPUT VISION, V47, P7, DOI 10.1023/A:1014573219977; Seitz S.M., 2006, C COMP VIS PATT REC; Sinha A., 2016, EUR C COMP VIS ECCV; SINHA S, 2007, INT C COMP VIS ICCV; Tatarchenko Maxim, 2016, EUR C COMP VIS ECCV; Tulsiani S., 2017, C COMP VIS PATT REC; Tulsiani S., 2016, T PATTERN ANAL MACHI; Ulusoy A. O., 2015, INT C 3D VIS 3DV; Vogiatzis G., 2005, C COMP VIS PATT REC; Yang R., 2003, COMPUTER GRAPHICS FO; Zach C., 2007, INT C COMP VIS ICCV; Zbontar J., 2016, J MACH LEARN RES, V17, P2; Zhou T., 2017, C COMP VIS PATT REC	61	30	30	2	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400035
C	Mroueh, Y; Sercu, T		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Mroueh, Youssef; Sercu, Tom			Fisher GAN	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Generative Adversarial Networks (GANs) are powerful models for learning complex distributions. Stable training of GANs has been addressed in many recent works which explore different metrics between distributions. In this paper we introduce Fisher GAN which fits within the Integral Probability Metrics (IPM) framework for training GANs. Fisher GAN defines a critic with a data dependent constraint on its second order moments. We show in this paper that Fisher GAN allows for stable and time efficient training that does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping. We analyze our Fisher IPM theoretically and provide an algorithm based on Augmented Lagrangian for Fisher GAN. We validate our claims on both image sample generation and semi-supervised classification using Fisher GAN.	[Mroueh, Youssef; Sercu, Tom] IBM TJ Watson Res Ctr, IBM Res AI, AI Fdn, Yorktown Hts, NY 10598 USA	International Business Machines (IBM)	Mroueh, Y (corresponding author), IBM TJ Watson Res Ctr, IBM Res AI, AI Fdn, Yorktown Hts, NY 10598 USA.	mroueh@us.ibm.com; tom.sercul@ibm.com						Arjovsky Mart<prime>in, 2017, P 5 INT C LEARN REPR; Arjovsky M, 2017, PR MACH LEARN RES, V70; Bartlett Peter L., 2005, ANN STAT; Berthelot D., 2017, BEGAN BOUNDARY EQUIL, DOI DOI 10.48550/ARXIV.1703.10717; Dai Z., 2017, ARXIV170201691; Dumoulin Vincent, 2017, ICLR; Dziugaite GK, 2015, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P258; Ekeland I., 1983, INFINITE DIMENSIONAL; Fukumizu K., 2009, INTEGRAL PROBABILITY; Glorot X., 2010, PROC MACH LEARN RES, P249; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Gretton A, 2012, J MACH LEARN RES, V13, P723; Gulrajani I, 2017, P NIPS 2017; Harchaoui Z., 2008, ADV NEURAL INFORM PR, P609; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Hinton, 2016, ARXIV PREPRINT ARXIV; Huang X, 2017, PROC CVPR IEEE, P1866, DOI 10.1109/CVPR.2017.202; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Li YJ, 2015, PR MACH LEARN RES, V37, P1718; Liu ZW, 2015, IEEE I CONF COMP VIS, P3730, DOI 10.1109/ICCV.2015.425; Mao XD, 2017, IEEE I CONF COMP VIS, P2813, DOI 10.1109/ICCV.2017.304; Mohamed Shakir, 2016, ARXIV161003483; Mroueh Y, 2017, PR MACH LEARN RES, V70; Muller Alfred, 1997, ADV APPL PROBABILITY; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Nowozin S, 2016, ADV NEUR IN, V29; Odena A., 2016, SEMISUPERVISED LEARN; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Salimans T, 2016, ADV NEUR IN, V29; Sonderby C.K., 2017, ICLR, P1; Springenberg Jost Tobias, 2015, ARXIV151106390; Sriperumbudur BK, 2012, ELECTRON J STAT, V6, P1550, DOI 10.1214/12-EJS722; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Theis Lucas, 2016, ICLR; Tosi Alessandra, 2014, METRICS PROBABILISTI; Wang D., 2016, ARXIV161101722; Warde-Farley D, 2017, ICLR SUBMISSIONS, V8; Yu F., 2015, LSUN CONSTRUCTION LA, V2, P7	39	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402055
C	Pleiss, G; Raghavan, M; Wu, FL; Kleinberg, J; Weinberger, KQ		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Pleiss, Geoff; Raghavan, Manish; Wu, Felix; Kleinberg, Jon; Weinberger, Kilian Q.			On Fairness and Calibration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					The machine learning community has become increasingly concerned with the potential for bias and discrimination in predictive models. This has motivated a growing line of work on what it means for a classification procedure to be "fair." In this paper, we investigate the tension between minimizing error disparity across different population groups while maintaining calibrated probability estimates. We show that calibration is compatible only with a single error constraint (i.e. equal false-negatives rates across groups), and show that any algorithm that satisfies this relaxation is no better than randomizing a percentage of predictions for an existing classifier. These unsettling findings, which extend and generalize existing results, are empirically confirmed on several datasets.	[Pleiss, Geoff; Raghavan, Manish; Wu, Felix; Kleinberg, Jon; Weinberger, Kilian Q.] Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA	Cornell University	Pleiss, G (corresponding author), Cornell Univ, Dept Comp Sci, Ithaca, NY 14853 USA.	geoff@cs.cornell.edu; manish@cs.cornell.edu; fw245@cornell.edu; kleinber@cs.cornell.edu; kwq4@cornell.edu			National Science Foundation [III-1149882, III-1525919, III-1550179, III-1618134, III-1740822]; Office of Naval Research DOD [N00014-17-1-2175]; Bill and Melinda Gates Foundation; NSF Graduate Research Fellowship [DGE-1650441]; Simons Investigator Award; ARO MURI grant; Facebook Faculty Research Grant; Google Research Grant	National Science Foundation(National Science Foundation (NSF)); Office of Naval Research DOD(Office of Naval ResearchUnited States Department of Defense); Bill and Melinda Gates Foundation(Bill & Melinda Gates Foundation); NSF Graduate Research Fellowship(National Science Foundation (NSF)); Simons Investigator Award; ARO MURI grant(MURI); Facebook Faculty Research Grant(Facebook Inc); Google Research Grant(Google Incorporated)	GP, FW, and KQW are supported in part by grants from the National Science Foundation (III-1149882, III-1525919, III-1550179, III-1618134, and III-1740822), the Office of Naval Research DOD (N00014-17-1-2175), and the Bill and Melinda Gates Foundation. MR is supported by an NSF Graduate Research Fellowship (DGE-1650441). JK is supported in part by a Simons Investigator Award, an ARO MURI grant, a Google Research Grant, and a Facebook Faculty Research Grant.	Angwin J., 2016, PROPUBLICA, P254; [Anonymous], ARXIV170306856; Barocas S., 2016, CALIFORNIA LAW REV, V104; Berk R., 2017, ARXIV PREPRINT ARXIV; Berk R.A., 2016, CRIMINOLOGIST, V41, P6; Bolukbasi T, 2016, ADV NEUR IN, V29; Calders T., 2009, ICDM WORKSH; Calders T., 2012, KDD; Chouldechova A, 2017, ARXIV170300056; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Crowson CS, 2016, STAT METHODS MED RES, V25, P1692, DOI 10.1177/0962280213497434; DAWID AP, 1982, J AM STAT ASSOC, V77, P605, DOI 10.2307/2287720; Dieterich W., 2016, TECHNICAL REPORT; Dwork Cynthia, 2012, P 3 INN THEOR COMP S, P214; Edwards H., 2016, ICLR; Flores A., 2016, TECHNICAL REPORT; Goh G, 2016, ADV NEUR IN, V29; Guo C., 2017, ICML; Hardt M., 2016, ADV NEURAL INFORM PR; Joseph M., 2016, NIPS; Kamiran F., 2009, INT C COMP CONTR COM; Kamishima T., 2011, ICDM WORKSH; Kearns M, 2017, PR MACH LEARN RES, V70; Kilbertus Niki, 2017, NIPS; Kleinberg J. M., 2017, P 8 INN THEOR COMP S, V67, P43; Lichman M., 2013, UCI MACHINE LEARNING; Louizos C., 2016, ICLR; Lum K, 2017, ALGORITHM REMOVING S; NICULESCUMIZIL A, 2005, ICML; Platt JC, 2000, ADV NEUR IN, P61; Romei A, 2014, KNOWL ENG REV, V29, P582, DOI 10.1017/S0269888913000039; White-House, 2016, TECHNICAL REPORT; Woodworth B., 2017, ARXIV PREPRINT ARXIV; Zadrozny Bianca, 2001, ICML; Zafar M. G. R. M. B., 2017, WORLD WID WEB C; Zafar Muhammad Bilal, 2015, ARXIV150705259; Zemel Rich, 2013, ICML; Zliobaite I., 2015, ICML WORKSH FAIRN AC	39	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405074
C	Yang, G; Schoenholz, SS		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Yang, Greg; Schoenholz, Samuel S.			Mean Field Residual Networks: On the Edge of Chaos	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study randomly initialized residual networks using mean field theory and the theory of difference equations. Classical feedforward neural networks, such as those with tanh activations, exhibit exponential behavior on the average when propagating inputs forward or gradients backward. The exponential forward dynamics causes rapid collapsing of the input space geometry, while the exponential backward dynamics causes drastic vanishing or exploding gradients. We show, in contrast, that by adding skip connections, the network will, depending on the nonlinearity, adopt subexponential forward and backward dynamics, and in many cases in fact polynomial. The exponents of these polynomials are obtained through analytic methods and proved and verified empirically to be correct. In terms of the "edge of chaos" hypothesis, these subexponential and polynomial laws allow residual networks to "hover over the boundary between stability and chaos," thus preserving the geometry of the input space and the gradient information flow. In our experiments, for each activation function we study here, we initialize residual networks with different hyperparameters and train them on MNIST. Remarkably, our initialization time theory can accurately predict test time performance of these networks, by tracking either the expected amount of gradient explosion or the expected squared distance between the images of two input vectors. Importantly, we show, theoretically as well as empirically, that common initializations such as the Xavier or the He schemes are not optimal for residual networks, because the optimal initialization variances depend on the depth. Finally, we have made mathematical contributions by deriving several new identities for the kernels of powers of ReLU functions by relating them to the zeroth Bessel function of the second kind.	[Yang, Greg] Microsoft Res AI, Redmond, WA 98052 USA; [Schoenholz, Samuel S.] Google Brain, Mountain View, CA USA; [Yang, Greg] Harvard Univ, Cambridge, MA 02138 USA	Google Incorporated; Harvard University	Yang, G (corresponding author), Microsoft Res AI, Redmond, WA 98052 USA.	gregyan@microsoft.com; schsam@google.com						Bertschinger N, 2004, NEURAL COMPUT, V16, P1413, DOI 10.1162/089976604323057443; Cho Y., 2009, NIPS, P342; Glorot X., 2010, P 13 INT C ART INT S, P249, DOI DOI 10.1.1/207.2059; He KM, 2016, LECT NOTES COMPUT SC, V9908, P630, DOI 10.1007/978-3-319-46493-0_38; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Lillicrap TP, 2016, NAT COMMUN, V7, DOI 10.1038/ncomms13276; Poole B, 2016, ADV NEUR IN, V29; Raghu Maithra, 2016, ARXIV160605336CSSTAT; Schoenholz Samuel S., 2017, DEEP INFORM PROPAGAT	11	30	30	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402089
C	Bellemare, MG; Srinivasan, S; Ostrovski, G; Schaul, T; Saxton, D; Munos, R		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bellemare, Marc G.; Srinivasan, Sriram; Ostrovski, Georg; Schaul, Tom; Saxton, David; Munos, Remi			Unifying Count-Based Exploration and Intrinsic Motivation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We consider an agent's uncertainty about its environment and the problem of generalizing this uncertainty across states. Specifically, we focus on the problem of exploration in non-tabular reinforcement learning. Drawing inspiration from the intrinsic motivation literature, we use density models to measure uncertainty, and propose a novel algorithm for deriving a pseudo-count from an arbitrary density model. This technique enables us to generalize count-based exploration algorithms to the non-tabular case. We apply our ideas to Atari 2600 games, providing sensible pseudo-counts from raw pixels. We transform these pseudo-counts into exploration bonuses and obtain significantly improved exploration in a number of hard games, including the infamously difficult MONTEZUMA'S REVENGE.	[Bellemare, Marc G.; Srinivasan, Sriram; Ostrovski, Georg; Schaul, Tom; Saxton, David; Munos, Remi] Google DeepMind, London, England	Google Incorporated	Bellemare, MG (corresponding author), Google DeepMind, London, England.	bellemare@google.com; srsrinivasan@google.com; ostrovski@google.com; schaul@google.com; saxton@google.com; munos@google.com						[Anonymous], 2016, P INT C MACH LEARN; [Anonymous], 2016, VARIATIONAL INFORM M; Barto A.G., 2013, INTRINSICALLY MOTIVA, P17, DOI [10.1007/978-3-642-32375-1_2, DOI 10.1007/978-3-642-32375-1_2]; Bellemare MG, 2014, PR MACH LEARN RES, V32, P1458; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Cover T. M., 2006, ELEMENTS INFORM THEO, V2; Hutter M., 2013, P C ONL LEARN THEOR; Kolter Z. J., 2009, P 26 INT C MACH LEAR; Leike J., 2016, P C UNC ART INT; Lopes M., 2012, ADV NEURAL INFORM PR, V25; Machado M. C., 2015, AAAI WORKSH LEARN GE; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Mohamed S, 2015, ADV NEUR IN, V28; Ollivier Y., 2015, ARXIV150304304; Orseau L., 2013, P C ALG LEARN THEOR; Oudeyer PY, 2007, IEEE T EVOLUT COMPUT, V11, P265, DOI 10.1109/TEVC.2006.890271; Pazis J., 2016, P 30 AAAI C ART INT; Schmidhuber J, 1991, AN AN P 1 INT C SIM; Singh S., 2004, ADV NEURAL INFORM PR, V16; Stadie B. C., 2015, ARXIV150700814; Strehl AL, 2008, J COMPUT SYST SCI, V74, P1309, DOI 10.1016/j.jcss.2007.08.009; Van den Oord A., 2016, P 33 INT C MACH LEAR; Van Hasselt H., 2016, P 30 AAAI C ART INT	24	30	30	2	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703019
C	Nan, F; Wang, J; Saligrama, V		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Nan, Feng; Wang, Joseph; Saligrama, Venkatesh			Pruning Random Forests for Prediction on a Budget	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					We propose to prune a random forest (RF) for resource-constrained prediction. We first construct a RF and then prune it to optimize expected feature cost & accuracy. We pose pruning RFs as a novel 0-1 integer program with linear constraints that encourages feature re-use. We establish total unimodularity of the constraint set to prove that the corresponding LP relaxation solves the original integer program. We then exploit connections to combinatorial optimization and develop an efficient primal-dual algorithm, scalable to large datasets. In contrast to our bottom-up approach, which benefits from good RF initialization, conventional methods are top-down acquiring features based on their utility value and is generally intractable, requiring heuristics. Empirically, our pruning algorithm outperforms existing state-of-the-art resource-constrained algorithms.	[Nan, Feng] Boston Univ, Syst Engn, Boston, MA 02215 USA; [Wang, Joseph; Saligrama, Venkatesh] Boston Univ, Elect Engn, Boston, MA 02215 USA	Boston University; Boston University	Nan, F (corresponding author), Boston Univ, Syst Engn, Boston, MA 02215 USA.	fnan@bu.edu; joewang@bu.edu; srv@bu.edu		Saligrama, Venkatesh/0000-0002-0675-2268	NSF [CCF: 1320566, CNS: 1330008, CCF: 1527618]; DHS [2013-ST-061-ED0001]; ONR [50202168]; US AF [FA8650-14-C-1728]	NSF(National Science Foundation (NSF)); DHS(United States Department of Homeland Security (DHS)); ONR(Office of Naval Research); US AF	We thank Dr Kilian Weinberger for helpful discussions and Dr David Castanon for the insights on the primal dual algorithm. This material is based upon work supported in part by NSF Grants CCF: 1320566, CNS: 1330008, CCF: 1527618, DHS 2013-ST-061-ED0001, ONR Grant 50202168 and US AF contract FA8650-14-C-1728.	Benbouzid Djalel, 2014, THESIS; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Breiman L., 2017, CLASSIFICATION REGRE; Chakaravarthy VT, 2011, ACM T ALGORITHMS, V7, DOI 10.1145/1921659.1921661; Chapelle O., 2011, P YAH LEARN RANK CHA; Frank A., 2010, UCI MACHINE LEARNING; Friedman JH, 2001, ANN STAT, V29, P1189, DOI 10.1214/aos/1013203451; Gao T., 2011, ADV NEURAL INFORM PR, V24, P1062; Gurobi Optimization Inc, 2015, GUROBI OPTIMIZER REF; jia Li L., 2010, NIPS, DOI [10.1184/R1/6475985.v1, DOI 10.1184/R1/6475985.V1]; Kulkarni V. Y., 2012, Proceedings of the 2012 International Conference on Data Science & Engineering (ICDSE 2012), P64, DOI 10.1109/ICDSE.2012.6282329; Kusner MJ, 2014, AAAI CONF ARTIF INTE, P1939; Lazebnik S, 2006, IEEE CVPR; Li XB, 2001, INFORMS J COMPUT, V13, P332, DOI 10.1287/ijoc.13.4.332.9732; Nan F., 2015, P 32 INT C MACH LEAR; NEMHAUSER GL, 1978, MATH PROGRAM, V14, P265, DOI 10.1007/BF01588971; Sherali HD, 2009, INFORMS J COMPUT, V21, P49, DOI 10.1287/ijoc.1080.0278; Trapeznikov K, 2013, ARTIF INTELL, P581; Wang J., 2015, ADV NEURAL INFORM PR; Xu Z., 2013, ICML, P133; Xu Z. E., 2012, P INT C MACH LEARN I; Yi Zhang, 2005, WORKING PAPER	22	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701107
C	Stollenga, MF; Byeon, W; Liwicki, M; Schmidhuber, J		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Stollenga, Marijn F.; Byeon, Wonmin; Liwicki, Marcus; Schmidhuber, Juergen			Parallel Multi-Dimensional LSTM, With Application to Fast Biomedical Volumetric Image Segmentation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					Convolutional Neural Networks (CNNs) can be shifted across 2D images or 3D videos to segment them. They have a fixed input size and typically perceive only small local contexts of the pixels to be classified as foreground or background. In contrast, Multi-Dimensional Recurrent NNs (MD-RNNs) can perceive the entire spatio-temporal context of each pixel in a few sweeps through all pixels, especially when the RNN is a Long Short-Term Memory (LSTM). Despite these theoretical advantages, however, unlike CNNs, previous MD-LSTM variants were hard to parallelise on GPUs. Here we re-arrange the traditional cuboid order of computations in MD-LSTM in pyramidal fashion. The resulting PyraMiD-LSTM is easy to parallelise, especially for 3D data such as stacks of brain slice images. PyraMiD-LSTM achieved best known pixel-wise brain image segmentation results on MRBrainS13 (and competitive results on EM-ISBI12).	[Stollenga, Marijn F.; Byeon, Wonmin; Schmidhuber, Juergen] Ist Dalle Molle Sull Intelligenza Artificiale, Swiss AI Lab IDSIA, Lugano, Switzerland; [Stollenga, Marijn F.; Byeon, Wonmin; Schmidhuber, Juergen] SUPSI, Manno, Switzerland; [Stollenga, Marijn F.; Schmidhuber, Juergen] USI, Lugano, Switzerland; [Byeon, Wonmin; Liwicki, Marcus] Univ Kaiserslautern, Kaiserslautern, Germany; [Byeon, Wonmin] German Res Ctr Artificial Intelligence DFKI, Saarbrucken, Germany	Universita della Svizzera Italiana; University of Kaiserslautern; German Research Center for Artificial Intelligence (DFKI)	Stollenga, MF (corresponding author), Ist Dalle Molle Sull Intelligenza Artificiale, Swiss AI Lab IDSIA, Lugano, Switzerland.	marijn@idsia.ch; wonmin.byeon@dfki.de	Peters, Jan/P-6027-2019	Peters, Jan/0000-0002-5266-8091	NASCENCE EU project [EU/FP7-ICT-317662]	NASCENCE EU project	We would like to thank Klaus Greff and Alessandro Giusti for their valuable discussions, and Jan Koutnik and Dan Ciresan for their useful feedback. We also thank the ISBI NEATBrain15 organisers [13] and the ISBI 2012 organisers, in particular Adrienne Mendrik and Ignacio Arganda-Carreras. Lastly we thank NVIDIA for generously providing us with hardware to perform our research. This research was funded by the NASCENCE EU project (EU/FP7-ICT-317662).	[Anonymous], 2014, NIPS; [Anonymous], IEEE INT S BIOM IM I; Byeon W., 2015, CVPR; Cardona A, 2010, PLOS BIOL, V8, DOI 10.1371/journal.pbio.1000502; Chetlur Sharan, 2014, CUDNN EFFICIENT PRIM, V0, P1; Ciresan D., 2012, NIPS; Ciresan D., 2011, IJCNN; Gers F. A., 1999, ICANN; Graves A., 2009, PAMI, V31; Graves Alex, 2007, ICANN; Graves Alex, 2009, NIPS; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Hochreiter S., 1991, THESIS; Kass Michael, 1988, INT J COMPUTER VISIO, V2; Krizhevsky A., 2012, ADV NEURAL INF PROCE; Liu T., 2014, J NEUROSCIENCE METHO; Mendrik AM, 2015, COMPUT INTEL NEUROSC, V2015, DOI 10.1155/2015/813696; Pham V., 2014, ICFHR; Pizer S. M., 1987, COMPUT VISION GRAPH; Sak H., 2014, P INTERSPEECH; Srivastava N., 2014, J MACHINE LEARNING R; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Wang L., 2015, NEUROIMAGE; Weber O, 2008, ACM T GRAPHIC, V27, DOI 10.1145/1409625.1409626; Zeiler M. D., 2013, ARXIV13112901CSCV NY	25	30	32	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913100010
C	Wang, XY; Guo, FJ; Heller, KA; Dunson, DB		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Wang, Xiangyu; Guo, Fangjian; Heller, Katherine A.; Dunson, David B.			Parallelizing MCMC with Random Partition Trees	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					The modern scale of data has brought new challenges to Bayesian inference. In particular, conventional MCMC algorithms are computationally very expensive for large data sets. A promising approach to solve this problem is embarrassingly parallel MCMC (EP-MCMC), which first partitions the data into multiple subsets and runs independent sampling algorithms on each subset. The subset posterior draws are then aggregated via some combining rules to obtain the final approximation. Existing EP-MCMC algorithms are limited by approximation accuracy and difficulty in resampling. In this article, we propose a new EP-MCMC algorithm PART that solves these problems. The new algorithm applies random partition trees to combine the subset posterior draws, which is distribution-free, easy to re-sample from and can adapt to multiple scales. We provide theoretical justification and extensive experiments illustrating empirical performance.	[Wang, Xiangyu; Heller, Katherine A.; Dunson, David B.] Duke Univ, Dept Stat Sci, Durham, NC 27706 USA; [Guo, Fangjian] Duke Univ, Dept Comp Sci, Durham, NC 27706 USA	Duke University; Duke University	Wang, XY (corresponding author), Duke Univ, Dept Stat Sci, Durham, NC 27706 USA.	xw56@stat.duke.edu; guo@cs.duke.edu; kheller@stat.duke.edu; dunson@stat.duke.edu						BENTLEY JL, 1975, COMMUN ACM, V18, P509, DOI 10.1145/361002.361007; Blackard Jock A, 1998, P 2 SO FOR GIS C, P189; Blum M., 1973, Journal of Computer and System Sciences, V7, P448, DOI 10.1016/S0022-0000(73)80033-9; Breiman L., 2001, Machine Learning, V45, P5, DOI 10.1023/A:1010933404324; Haario H, 2001, BERNOULLI, V7, P223, DOI 10.2307/3318737; Haario H, 2006, STAT COMPUT, V16, P339, DOI 10.1007/s11222-006-9438-0; HJORT NL, 1995, ANN STAT, V23, P882, DOI 10.1214/aos/1176324627; Lichman M, 2013, UCI MACHINE LEARNING; Liu Linxi, 2014, ARXIV14012597; Maclaurin Dougal, 2014, P C UNC ART INT UAI; Minsker S., 2014, P 31 INT C MACH LEAR; Neiswanger W, 2014, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, P623; Roe BP, 2005, NUCL INSTRUM METH A, V543, P577, DOI 10.1016/j.nima.2004.12.018; Scott Steven L, 2013, EFABBAYES 250 C, V16; SHEN XT, 1994, ANN STAT, V22, P580, DOI 10.1214/aos/1176325486; Srivastava Sanvesh, 2015, P 18 INT C ART INT S, V38; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Wang X., 2013, ARXIV PREPRINT ARXIV; Welling M., 2011, P 28 INT C INT C MAC, P681, DOI DOI 10.4310/CIS.2012.V12.N3.A3	19	30	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913103024
C	Mjolsness, E; Mann, T; Castano, R; Wold, B		Solla, SA; Leen, TK; Muller, KR		Mjolsness, E; Mann, T; Castano, R; Wold, B			From coexpression to coregulation: An approach to inferring transcriptional regulation among gene classes from large-scale expression data	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 12	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	13th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 29-DEC 04, 1999	CO				MODEL	We provide preliminary evidence that existing algorithms for inferring small-scale gene regulation networks from gene expression data can be adapted to large-scale gene expression data coming from hybridization microarrays. The essential steps are (1) clustering many genes by their expression time-course data into a minimal set of clusters of co-expressed genes, (2) theoretically modeling the various conditions under which the time-courses are measured using a continious-time analog recurrent neural network for the cluster mean time-courses, (3) fitting such a regulatory model to the cluster mean time courses by simulated annealing with weight decay, and (4) analysing several such fits for commonalities in the circuit parameter sets including the connection matrices. This procedure can be used to assess the adequacy of existing and future gene expression time-course data sets for determining transcriptional regulatory relationships such as coregulation.	CALTECH, Jet Prop Lab, Pasadena, CA 91109 USA	California Institute of Technology; National Aeronautics & Space Administration (NASA); NASA Jet Propulsion Laboratory (JPL)	Mjolsness, E (corresponding author), CALTECH, Jet Prop Lab, 4800 Oak Grove Dr, Pasadena, CA 91109 USA.							DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; DeRisi JL, 1997, SCIENCE, V278, P680, DOI 10.1126/science.278.5338.680; Eisen MB, 1998, P NATL ACAD SCI USA, V95, P14863, DOI 10.1073/pnas.95.25.14863; HERTZ J, 1998, LECT KROGERRUP DENMA; KOSMAN D, 1998, PAC S BIOC 98; LAM J, 1988, 8817 YAL U EL ENG DE; LAM J, 1988, 8816 YAL U EL ENG DE; MJOLSNESS E, 1991, J THEOR BIOL, V152, P429, DOI 10.1016/S0022-5193(05)80391-1; REINITZ J, 1995, MECH DEVELOP, V49, P133, DOI 10.1016/0925-4773(94)00310-J; REINITZ J, 1995, J EXP ZOOL, V271, P47, DOI 10.1002/jez.1402710106; Smyth P., 1996, P 2 INT C KNOWL DISC; Spellman PT, 1998, MOL BIOL CELL, V9, P3273, DOI 10.1091/mbc.9.12.3273; Wen XL, 1998, P NATL ACAD SCI USA, V95, P334, DOI 10.1073/pnas.95.1.334	13	30	31	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-19450-3	ADV NEUR IN			2000	12						928	934						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BQ93F					2022-12-19	WOS:000165048700131
C	Wolpert, DH; Tumer, K; Frank, J		Kearns, MS; Solla, SA; Cohn, DA		Wolpert, DH; Tumer, K; Frank, J			Using COllective INtelligence to route internet traffic	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NETWORKS	A COllective INtelligence (COIN) is a set of interacting reinforcement learning (RL) algorithms designed in an automated fashion so that their collective behavior optimizes a global utility function. We summarize the theory of COINs, then present experiments using that theory to design COINs to control internet traffic routing. These experiments indicate that COINs outperform all previously investigated RL-based, shortest path routing algorithms.	NASA, Ames Res Ctr, Moffett Field, CA 94035 USA	National Aeronautics & Space Administration (NASA); NASA Ames Research Center	Wolpert, DH (corresponding author), NASA, Ames Res Ctr, Moffett Field, CA 94035 USA.	dhw@ptolemy.arc.nasa.gov; kagan@ptolemy.arc.nasa.gov; frank@ptolemy.arc.nasa.gov						ATKENSON CG, 1996, UNPUB ARTIFICIAL INT; BAUM E, 1998, NEURAL NETWORKS MACH; Bertsekas D. P., 1992, DATA NETWORKS; Boyan J. A., 1994, P INT C ADV NEURAL I, P671; Choi SPM, 1996, ADV NEUR IN, V8, P945; Fudenberg D., 1991, GAME THEORY; HARDIN G, 1968, SCIENCE, V162, P1243, DOI 10.1126/science.162.3859.1243; Korilis YA, 1997, IEEE ACM T NETWORK, V5, P161, DOI 10.1109/90.554730; Marbach P, 1998, ADV NEUR IN, V10, P922; Subramanian D, 1997, INT JOINT CONF ARTIF, P832; WOLPERT D, 1999, IN PRESS P 3 INT C A; WOLPERT D, 1999, IN PRESS HDB AGENT T; WOLPERT DH, 1999, COLLECTIVE INTELLIGE	13	30	30	1	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						952	958						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700134
C	Barber, D; Bishop, CM		Jordan, MI; Kearns, MJ; Solla, SA		Barber, D; Bishop, CM			Ensemble learning for multi-layer networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					Bayesian treatments of learning in neural networks are typically based either on local Gaussian approximations to a mode of the posterior weight distribution, or on Markov chain Monte Carlo simulations. A third approach, called ensemble learning, was introduced by Hinton and van Camp (1993). It aims to approximate the posterior distribution by minimizing the Kullback-Leibler divergence between the true posterior and a parametric approximating distribution. However, the derivation of a deterministic algorithm relied on the use of a Gaussian approximating distribution with a diagonal covariance matrix and so was unable to capture the posterior correlations between parameters. In this paper, we show how the ensemble learning approach can be extended to full-covariance Gaussian distributions while remaining computationally tractable. We also extend the framework to deal with hyperparameters, leading to a simple re-estimation procedure. Initial results from a standard benchmark problem are encouraging.	Univ Nijmegen, SNN, Nijmegen, Netherlands	Radboud University Nijmegen	Barber, D (corresponding author), Univ Nijmegen, SNN, Geert Grooteplein 21, Nijmegen, Netherlands.								0	30	30	0	4	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						395	401						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700056
C	Barber, D; Williams, CKI		Mozer, MC; Jordan, MI; Petsche, T		Barber, D; Williams, CKI			Gaussian processes for Bayesian classification via hybrid Monte Carlo	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The full Bayesian method for applying neural networks to a prediction problem is to set up the prior/hyperprior structure for the net and then perform the necessary integrals. However, these integrals are not tractable analytically, and Markov Chain Monte Carlo (MCMC) methods are slow, especially if the parameter space is high-dimensional. Using Gaussian processes we can approximate the weight space integral analytically, so that only a small number of hyperparameters need be integrated over by MCMC methods. We have applied this idea to classification problems, obtaining excellent results on the real-world problems investigated so far.			Barber, D (corresponding author), ASTON UNIV,DEPT COMP SCI & APPL MATH,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.								0	30	33	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						340	346						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00048
C	Bishop, CM; Svensen, M; Williams, CKI		Mozer, MC; Jordan, MI; Petsche, T		Bishop, CM; Svensen, M; Williams, CKI			GTM: A principled alternative to the self-organizing map	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				The Self-Organizing Map (SOM) algorithm has been extensively studied and has been applied with considerable success to a wide variety of problems. However, the algorithm is derived from heuristic ideas and this leads to a number of significant limitations. In this paper, we consider the problem of modelling the probability density of data in a space of several dimensions in terms of a smaller number of latent, or hidden, variables. We introduce a novel form of latent variable model, which we call the GTM algorithm (for Generative Topographic Mapping), which allows general non-linear transformations from latent space to data space, and which is trained using the EM (expectation-maximization) algorithm. Our approach overcomes the limitations of the SOM, while introducing no significant disadvantages. We demonstrate the performance of the GTM algorithm on simulated data from flow diagnostics for a multi-phase oil pipeline.			Bishop, CM (corresponding author), ASTON UNIV,NEURAL COMP RES GRP,BIRMINGHAM B4 7ET,W MIDLANDS,ENGLAND.		WU, TAKLON/A-2827-2010						0	30	30	0	3	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						354	360						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00050
C	Ghosn, J; Bengio, Y		Mozer, MC; Jordan, MI; Petsche, T		Ghosn, J; Bengio, Y			Multi-task learning for stock selection	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 9: PROCEEDINGS OF THE 1996 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	10th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 02-05, 1996	DENVER, CO	NIPS Fdn				Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions. Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared. When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning. In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks.			Ghosn, J (corresponding author), UNIV MONTREAL,DEPT INFORMAT & RECH OPERAT,MONTREAL,PQ H3C 3J7,CANADA.								0	30	30	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142	1049-5258		0-262-10065-7	ADV NEUR IN			1997	9						946	952						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BH93C					2022-12-19	WOS:A1997BH93C00133
C	Du, SS; Hou, KC; Poczos, B; Salakhutdinov, R; Wang, RS; Xu, KYL		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Du, Simon S.; Hou, Kangcheng; Poczos, Barnabas; Salakhutdinov, Ruslan; Wang, Ruosong; Xu, Keyulu			Graph Neural Tangent Kernel: Fusing Graph Neural Networks with Graph Kernels	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					While graph kernels (GKs) are easy to train and enjoy provable theoretical guarantees, their practical performances are limited by their expressive power, as the kernel function often depends on hand-crafted combinatorial features of graphs. Compared to graph kernels, graph neural networks (GNNs) usually achieve better practical performance, as GNNs use multi-layer architectures and non-linear activation functions to extract high-order information of graphs as features. However, due to the large number of hyper-parameters and the non-convex nature of the training procedure, GNNs are harder to train. Theoretical guarantees of GNNs are also not well-understood. Furthermore, the expressive power of GNNs scales with the number of parameters, and thus it is hard to exploit the full power of GNNs when computing resources are limited. The current paper presents a new class of graph kernels, Graph Neural Tangent Kernels (GNTKs), which correspond to infinitely wide multi-layer GNNs trained by gradient descent. GNTKs enjoy the full expressive power of GNNs and inherit advantages of GKs. Theoretically, we show GNTKs provably learn a class of smooth functions on graphs. Empirically, we test GNTKs on graph classification datasets and show they achieve strong performance.	[Du, Simon S.] Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA; [Hou, Kangcheng] Zhejiang Univ, Hangzhou, Peoples R China; [Poczos, Barnabas; Salakhutdinov, Ruslan; Wang, Ruosong] Carnegie Mellon Univ, Pittsburgh, PA 15213 USA; [Xu, Keyulu] MIT, Cambridge, MA 02139 USA	Institute for Advanced Study - USA; Zhejiang University; Carnegie Mellon University; Massachusetts Institute of Technology (MIT)	Du, SS (corresponding author), Inst Adv Study, Olden Lane, Princeton, NJ 08540 USA.	ssdu@ias.edu; kangchenghou@gmail.com; bapoczos@cs.cmu.edu; rsalakhu@cs.cmu.edu; ruosongw@andrew.cmu.edu; keyulu@mit.edu	Hou, Kangcheng/GRS-6097-2022		AFRL grant [FA8750-17-2-0212]; DARPA [D17AP0000]; NSF [IIS-1763562]; Office of Naval Research [N000141812861]; Nvidia NVAIL award; NSF CAREER award [1553284]; Chevron-MIT Energy Fellowship	AFRL grant; DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); NSF(National Science Foundation (NSF)); Office of Naval Research(Office of Naval Research); Nvidia NVAIL award; NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); Chevron-MIT Energy Fellowship	S. S. Du and B. Poczos acknowledge support from AFRL grant FA8750-17-2-0212 and DARPA D17AP0000. R. Salakhutdinov and R. Wang are supported in part by NSF IIS-1763562, Office of Naval Research grant N000141812861, and Nvidia NVAIL award. K. Xu is supported by NSF CAREER award 1553284 and a Chevron-MIT Energy Fellowship. This work was performed while S. S. Du was a Ph.D. student at Carnegie Mellon University and K. Hou was visiting Carnegie Mellon University.	Arora Sanjeev, 2019, ARXIV190108584; Arora Sanjeev, 2019, ADV NEURAL INFORM PR; Bartlett P. L., 2003, Journal of Machine Learning Research, V3, P463, DOI 10.1162/153244303321897690; Battaglia Peter W, 2016, ARXIV161200222; Defferrard M., 2016, P ADV NEURAL INFORM, P3844; Du S.S., 2019, ARXIV191003016; Du Simon S, 2018, GRADIENT DESCENT FIN; Duvenaud David K, 2015, P NIPS; Fey Matthias, 2019, P ICLR WORKSH REP LE; Gartner T, 2003, LECT NOTES ARTIF INT, V2777, P129, DOI 10.1007/978-3-540-45167-9_11; Gilmer Justin, 2017, INT C MACH LEARN ICM, P1273; Hamilton W., 2017, P ADV NEUR INF PROC, P1024; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Ivanov Sergey, 2018, ICML; Jacot A., 2018, ARXIV180607572; Kawarabayashi K., 2018, P 35 INT C MACH LEAR, P5449; Kipf TN, 2016, P INT C LEARN REPR; Li Y., 2016, INT C LEARN REPR; Niepert M, 2016, PR MACH LEARN RES, V48; Santoro A., 2017, ADV NEURAL INFORM PR, P4967; Santoro A., 2018, INT C MACH LEARN, P4477; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Shervashidze N., 2009, P 12 INT C ART INT S, P488; Shervashidze N, 2011, J MACH LEARN RES, V12, P2539; Velickovic P., 2018, P INT C LEARN REPR, DOI DOI 10.17863/CAM.48429; Vishwanathan SVN, 2010, J MACH LEARN RES, V11, P1201; Xu Keyulu, 2019, INT C LEARN REPR; Xu Keyulu, 2019, ARXIV190513211; Yanardag P, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1365, DOI 10.1145/2783258.2783417; Yang G., 2019, ARXIV190204760; Ying Rex, 2018, ADV NEURAL INFORM PR; Zhang M., 2018, 32 AAAI C ART INT; Zhang Z., 2018, NEURIPS	34	29	30	1	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305069
C	Kubilius, J; Schrimpf, M; Kar, K; Rajalingham, R; Hong, H; Majaj, NJ; Issa, EB; Bashivan, P; Prescott-Roy, J; Schmidt, K; Nayebi, A; Bear, D; Yamins, DLK; DiCarlo, JJ		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kubilius, Jonas; Schrimpf, Martin; Kar, Kohitij; Rajalingham, Rishi; Hong, Ha; Majaj, Najib J.; Issa, Elias B.; Bashivan, Pouya; Prescott-Roy, Jonathan; Schmidt, Kailyn; Nayebi, Aran; Bear, Daniel; Yamins, Daniel L. K.; DiCarlo, James J.			Brain-Like Object Recognition with High-Performing Shallow Recurrent ANNs	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				PREDICT; MODELS	Deep convolutional artificial neural networks (ANNs) are the leading class of candidate models of the mechanisms of visual processing in the primate ventral stream. While initially inspired by brain anatomy, over the past years, these ANNs have evolved from a simple eight-layer architecture in AlexNet to extremely deep and branching architectures, demonstrating increasingly better object categorization performance, yet bringing into question how brain-like they still are. In particular, typical deep models from the machine learning community are often hard to map onto the brain's anatomy due to their vast number of layers and missing biologically-important connections, such as recurrence. Here we demonstrate that better anatomical alignment to the brain and high performance on machine learning as well as neuroscience measures do not have to be in contradiction. We developed CORnet-S, a shallow ANN with four anatomically mapped areas and recurrent connectivity, guided by Brain-Score, a new large-scale composite of neural and behavioral benchmarks for quantifying the functional fidelity of models of the primate ventral visual stream. Despite being significantly shallower than most models, CORnet-S is the top model on Brain-Score and outperforms similarly compact models on ImageNet. Moreover, our extensive analyses of CORnet-S circuitry variants reveal that recurrence is the main predictive factor of both Brain-Score and ImageNet top-I performance. Finally, we report that the temporal evolution of the CORnet-S "IT" neural population resembles the actual monkey IT population dynamics. Taken together, these results establish CORnet-S, a compact, recurrent ANN, as the current best model of the primate ventral visual stream. [GRAPHICS] .	[Kubilius, Jonas; Schrimpf, Martin; Kar, Kohitij; Rajalingham, Rishi; Bashivan, Pouya; Prescott-Roy, Jonathan; Schmidt, Kailyn; DiCarlo, James J.] MIT, McGovern Inst Brain Res, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Kubilius, Jonas] Katholieke Univ Leuven, Brain & Cognit, Leuven, Belgium; [Schrimpf, Martin; Kar, Kohitij; Bashivan, Pouya; DiCarlo, James J.] MIT, Dept Brain & Cognit Sci, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Schrimpf, Martin; Kar, Kohitij; DiCarlo, James J.] MIT, Ctr Brains Minds & Machines, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Hong, Ha] Bay Labs Inc, San Francisco, CA 94102 USA; [Majaj, Najib J.] NYU, Ctr Neural Sci, New York, NY 10003 USA; [Issa, Elias B.] Columbia Univ, Dept Neurosci, Zuckerman Mind Brain Behav Inst, New York, NY 10027 USA; [Nayebi, Aran] Stanford Univ, Neurosci PhD Program, Stanford, CA 94305 USA; [Bear, Daniel; Yamins, Daniel L. K.] Stanford Univ, Dept Psychol, Stanford, CA 94305 USA; [Yamins, Daniel L. K.] Stanford Univ, Dept Comp Sci, Stanford, CA 94305 USA	Massachusetts Institute of Technology (MIT); KU Leuven; Massachusetts Institute of Technology (MIT); Massachusetts Institute of Technology (MIT); New York University; Columbia University; Stanford University; Stanford University; Stanford University	Kubilius, J (corresponding author), MIT, McGovern Inst Brain Res, 77 Massachusetts Ave, Cambridge, MA 02139 USA.; Kubilius, J (corresponding author), Katholieke Univ Leuven, Brain & Cognit, Leuven, Belgium.		Kar, Kohitij/J-8264-2015	Kar, Kohitij/0000-0002-4283-9256	European Union's Horizon 2020 research and innovation programme [705498]; US National Eye Institute [RO1-EY014970]; Office of Naval Research [MURI-114407]; Simons Foundation [325500, 542965, 543061]; James S. McDonnell foundation [220020469]; US National Science Foundation [iis-ril703161]; Semiconductor Research Corporation (SRC); DARPA; Research Foundation -Flanders (FWO); Flemish Government -department EWI	European Union's Horizon 2020 research and innovation programme; US National Eye Institute(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USANIH National Eye Institute (NEI)); Office of Naval Research(Office of Naval Research); Simons Foundation; James S. McDonnell foundation; US National Science Foundation(National Science Foundation (NSF)); Semiconductor Research Corporation (SRC); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); Research Foundation -Flanders (FWO)(FWO); Flemish Government -department EWI	This project has received funding from the European Union's Horizon 2020 research and innovation programme under grant agreement No 705498 (J.K.), US National Eye Institute (RO1-EY014970, J.J.D.), Office of Naval Research (MURI-114407, J.J.D), the Simons Foundation (SCGB [325500, 542965], J.J.D; 543061, D.L.K.Y), the James S. McDonnell foundation (220020469, D.L.K.Y.) and the US National Science Foundation (iis-ril703161, D.L.K.Y.). This work was also supported in part by the Semiconductor Research Corporation (SRC) and DARPA. The computational resources and services used in this work were provided in part by the VSC (Flemish Supercomputer Center), funded by the Research Foundation -Flanders (FWO) and the Flemish Government -department EWI.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Bar M, 2006, P NATL ACAD SCI USA, V103, P449, DOI 10.1073/pnas.0507062103; Bashivan P, 2019, SCIENCE, V364, P453, DOI 10.1126/science.aav9436; Boski M, 2017, 2017 10TH INTERNATIONAL WORKSHOP ON MULTIDIMENSIONAL (ND) SYSTEMS (NDS); Cichy R. M, 2016, ARXIV160102970; Clarke A, 2018, J COGNITIVE NEUROSCI, V30, P1590, DOI 10.1162/jocn_a_01325; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; DiCarlo JJ, 2012, NEURON, V73, P415, DOI 10.1016/j.neuron.2012.01.010; Glorot X., 2010, PROC MACH LEARN RES, P249; Guclu U, 2015, J NEUROSCI, V35, P10005, DOI 10.1523/JNEUROSCI.5023-14.2015; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Howard A.G., 2017, MOBILENETS EFFICIENT; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Iandola Forrest N., 2016, SQUEEZENET ALEXNET L; Jastrzebski S., 2017, ARXIV PREPRINT ARXIV; Kar Kohitij, 2019, NAT NEUROSCI; Khaligh-Razavi SM, 2014, PLOS COMPUT BIOL, V10, DOI 10.1371/journal.pcbi.1003915; Kornblith Simon, 2018, ARXIV180508974; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Kubilius J, 2018, NEUROIMAGE, V180, P110, DOI 10.1016/j.neuroimage.2017.12.006; Kubilius J, 2016, PLOS COMPUT BIOL, V12, DOI 10.1371/journal.pcbi.1004896; Lamme VAF, 2000, TRENDS NEUROSCI, V23, P571, DOI 10.1016/S0166-2236(00)01657-X; Leroux Sam, 2018, ICLR WORKSH; Liang M, 2015, PROC CVPR IEEE, P3367, DOI 10.1109/CVPR.2015.7298958; Liao Q., 2016, CORR; Lin TY, 2014, LECT NOTES COMPUT SC, V8693, P740, DOI 10.1007/978-3-319-10602-1_48; Liu Chenxi, 2017, PROGRESSIVE NEURAL A; Majaj NJ, 2015, J NEUROSCI, V35, P13402, DOI 10.1523/JNEUROSCI.5181-14.2015; Nayebi A., 2018, ADV NEURAL INFORM PR, P5290; Pedregosa F, 2011, J MACH LEARN RES, V12, P2825; Rajaei Karim, 2018, BIORXIV; Rajalingham R, 2018, J NEUROSCI, V38, P7255, DOI 10.1523/JNEUROSCI.0388-18.2018; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Schrimpf Martin, 2017, THESIS; Silberman N., 2016, TENSORFLOW SLIM IMAG; SINGH PK, 2019, NAT NEUROSCI, P19, DOI DOI 10.1145/3345837.3355955; Spoerer CJ, 2017, FRONT PSYCHOL, V8, DOI 10.3389/fpsyg.2017.01551; Szegedy C., 2017, AAAI, V4, P12, DOI DOI 10.1016/J.PATREC.2014.01.008; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Szegedy C, 2015, P IEEE C COMP VIS PA, P1, DOI [10.1109/cvpr.2015.7298594, 10.1109/CVPR.2015.7298594]; Szegedy C, 2016, PROC CVPR IEEE, P2818, DOI 10.1109/CVPR.2016.308; Tang HL, 2018, P NATL ACAD SCI USA, V115, P8835, DOI 10.1073/pnas.1719397115; TOVEE MJ, 1994, CURR BIOL, V4, P1125, DOI 10.1016/S0960-9822(00)00253-0; Wagemans J, 2012, PSYCHOL BULL, V138, P1172, DOI 10.1037/a0029333; Wu YX, 2018, LECT NOTES COMPUT SC, V11217, P3, DOI 10.1007/978-3-030-01261-8_1; Yamins D.L., 2013, ADV NEURAL INFORM PR, V26, P3093, DOI [DOI 10.5555/2999792.2999957, 10.5555/2999792.2999957]; Yamins DLK, 2016, NAT NEUROSCI, V19, P356, DOI 10.1038/nn.4244; Yamins DLK, 2014, P NATL ACAD SCI USA, V111, P8619, DOI 10.1073/pnas.1403112111; Zamir AR, 2017, PROC CVPR IEEE, P1808, DOI 10.1109/CVPR.2017.196; Zoph B., 2017, LEARNING TRANSFERABL	50	29	29	0	9	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904089
C	Mahajan, A; Rashid, T; Samvelyan, M; Whiteson, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Mahajan, Anuj; Rashid, Tabish; Samvelyan, Mikayel; Whiteson, Shimon			MAVEN: Multi-Agent Variational Exploration	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Centralised training with decentralised execution is an important setting for cooperative deep multi-agent reinforcement learning due to communication constraints during execution and computational tractability in training. In this paper, we analyse value-based methods that are known to have superior performance in complex environments [43]. We specifically focus on QMIX [40], the current state-of-the-art in this domain. We show that the representational constraints on the joint action-values introduced by QMIX and similar methods lead to provably poor exploration and suboptimality. Furthermore, we propose a novel approach called MAVEN that hybridises value and policy-based methods by introducing a latent space for hierarchical control. The value-based agents condition their behaviour on the shared latent variable controlled by a hierarchical policy. This allows MAVEN to achieve committed, temporally extended exploration, which is key to solving complex multi-agent tasks. Our experimental results show that MAVEN achieves significant performance improvements on the challenging SMAC domain [43].	[Mahajan, Anuj; Rashid, Tabish; Whiteson, Shimon] Univ Oxford, Dept Comp Sci, Oxford, England; [Samvelyan, Mikayel] Russian Armenian Univ, Yerevan, Armenia	University of Oxford; Russian Armenian Slavonic University	Mahajan, A (corresponding author), Univ Oxford, Dept Comp Sci, Oxford, England.	anuj.mahajan@cs.ox.ac.uk			Oxford-Google DeepMind Graduate Scholarship; Drapers Scholarship; Engineering and Physical Sciences Research Council [EP/M508111/1, EP/N509711/1]; European Research Council under the European Union's Horizon 2020 research and innovation programme [637713]; NVIDIA; Oracle Cloud Innovation Accelerator	Oxford-Google DeepMind Graduate Scholarship(Google Incorporated); Drapers Scholarship; Engineering and Physical Sciences Research Council(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); European Research Council under the European Union's Horizon 2020 research and innovation programme(European Research Council (ERC)); NVIDIA; Oracle Cloud Innovation Accelerator	AM is generously funded by the Oxford-Google DeepMind Graduate Scholarship and Drapers Scholarship. TR is funded by the Engineering and Physical Sciences Research Council (EP/M508111/1, EP/N509711/1). This project has received funding from the European Research Council under the European Union's Horizon 2020 research and innovation programme (grant agreement number 637713). The experiments were made possible by generous equipment grant by NVIDIA and cloud credit grant from Oracle Cloud Innovation Accelerator.	Aumann Robert, 1974, J MATH ECON, V1, P67, DOI 10.1016/0304-4068(74)90037-8; Barber D., 2011, BAYESIAN TIME SERIES; Bishop C.M, 2006, PATTERN RECOGN; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Boyd S, 2004, CONVEX OPTIMIZATION; Busoniu L, 2008, IEEE T SYST MAN CY C, V38, P156, DOI 10.1109/TSMCC.2007.913919; Cao YC, 2013, IEEE T IND INFORM, V9, P427, DOI 10.1109/TII.2012.2219061; EYSENBACH B, 2018, 6 INT C LEARN REPR I, P1; Fellows M., 2018, ARXIV181101132; Florensa Carlos, 2017, ARXIV170403012; Foerster J. N., 2016, P ADV NEUR INF PROC, P2137; Foerster J, 2017, PR MACH LEARN RES, V70; Foerster JN, 2018, AAAI CONF ARTIF INTE, P2974; Gregor Karol, 2016, ARXIV161107507; Guckelsberger C., 2018, P C CIG; Guestrin C, 2002, ADV NEUR IN, V14, P1523; Guestrin C, 2003, J ARTIF INTELL RES, V19, P399, DOI 10.1613/jair.1000; Hausknecht M., 2015, AAAI FALL S SEQ DEC; Hochreiter S., 1997, STUD COMPUT INTELL, V9, P1735, DOI DOI 10.1007/978-3-642-24797-2; Houthooft R., 2016, ADV NEURAL INFORM PR, P1109; Huttenrauch Maximilian, 2017, AAMAS 2017 AUT ROB M; Kingma D. P., 2014, P INT C LEARN REPR; Kingma DP., 2016, ADV NEURAL INFORM PR, V29, P4743; Kraemer L, 2016, NEUROCOMPUTING, V190, P82, DOI 10.1016/j.neucom.2016.01.031; Levine Sergey, 2018, ARXIV180402808; Lin Alex Tong, 2019, ARXIV190202311; Lowe R., 2017, P INT C NEUR INF PRO, P6379; Ma JY, 2018, IEEE INT CONF ROBOT, P7254; Mahajan A., 2017, ARXIV170602999; Mahajan A, 2017, AAMAS'17: PROCEEDINGS OF THE 16TH INTERNATIONAL CONFERENCE ON AUTONOMOUS AGENTS AND MULTIAGENT SYSTEMS, P1619; Mnih Andriy, 2014, INT C MACH LEARN; Mnih V., 2013, ARXIV PREPRINT ARXIV; Oliehoek F. A., 2016, CONCISE INTRO DECENT, V1st; Omidshafiei S, 2017, PR MACH LEARN RES, V70; Osband Ian, 2017, ARXIV170307608; Pathak D., 2017, ICML; Peng P., 2017, ARXIV170310069; Rashid Tabish, 2018, INT C MACH LEARN, P2; Rezende D.J., 2014, PROC INT CONFER ENCE; Rezende Danilo Jimenez, 2015, ARXIV150505770; Samvelyan M., 2019, P 18 INT C AUT AG MU; Son Kyunghwan, 2019, ARXIV190505408; Sukhbaatar S., 2016, ADV NEURAL INF PROCE, V29, P2244, DOI DOI 10.5555/3157096.3157348; Sunehag P., 2017, ARXIV170605296; Tampuu A, 2017, PLOS ONE, V12, DOI 10.1371/journal.pone.0172395; Tan M., 1993, P 10 INT C MACHINE L, P330, DOI DOI 10.1016/B978-1-55860-307-3.50049-6; Todorov E., 2007, ADV NEURAL INFORM PR, V19, DOI 10.7551/mitpress/7503.003.0176; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Van Roy Benjamin., 2018, P 35 INT C MACH LEAR, P1270; Wainwright MJ, 2008, FOUND TRENDS MACH LE, V1, P1, DOI 10.1561/2200000001; Yang E., 2004, TECHNICAL REPORT; Zheng Stephan, 2018, OPENREVIEW; Ziebart B. D., 2008, AAAI, V8, P1433	53	29	30	0	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424307061
C	Benaim, S; Wolf, L		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Benaim, Sagie; Wolf, Lior			One-Shot Unsupervised Cross Domain Translation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Given a single image x from domain A and a set of images from domain B, our task is to generate the analogous of x in B. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain B is trained. Then, given the new sample x, we create a variational autoencoder for domain A by adapting the layers that are close to the image in order to directly fit x, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample x, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain A. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation.	[Benaim, Sagie; Wolf, Lior] Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel; [Wolf, Lior] Facebook AI Res, Tel Aviv, Israel	Tel Aviv University; Facebook Inc	Benaim, S (corresponding author), Tel Aviv Univ, Sch Comp Sci, Tel Aviv, Israel.				European Research Council (ERC) under the European Union [ERC CoG 725974]	European Research Council (ERC) under the European Union(European Research Council (ERC))	This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant ERC CoG 725974). The contribution of Sagie Benaim is part of Ph.D. thesis research conducted at Tel Aviv University.	Benaim S, 2017, ADV NEUR IN, V30; Choi Y, 2018, PROC CVPR IEEE, P8789, DOI 10.1109/CVPR.2018.00916; Conneau A, 2017, INT C LEARN REPR ICL; Cordts M, 2016, PROC CVPR IEEE, P3213, DOI 10.1109/CVPR.2016.350; Fauconnier G., 2002, WAY WE THINK CONCEPT; Fung P., 1998, P 17 INT C COMP LING, V1, P414; Galanti T, 2018, INT C LEARN REPR; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Hoshen Y, 2018, C EMP METH NAT LANG; Hoshen Y, 2018, INT C LEARN REPR ICL; Isola P., 2017, IMAGE TO IMAGE TRANS, P1125; Johnson J, 2016, LECT NOTES COMPUT SC, V9906, P694, DOI 10.1007/978-3-319-46475-6_43; Kim T, 2017, PR MACH LEARN RES, V70; Koehn P, 2002, P ACL WORKSH UNS LEX, V9, P9, DOI [DOI 10.3115/1118627.1118629, 10.3115/1118627.1118629]; Lample Guillaume, 2018, ICLR; LeCun Y., 2010, MNIST HANDWRITTEN DI; Liu MY, 2017, ADV NEUR IN, V30; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; LIU ZW, 2017, PROCEEDINGS OF THE 5, V1, P1959, DOI DOI 10.1109/TPAMI.2017.2737535; Mao X, 2016, MULTI CLASS GENERATI; Mikolov T., 2013, ARXIV; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Radford A., 2015, ARXIV PREPRINT ARXIV, DOI DOI 10.1051/0004-6361/201527329; Rapp R, 1999, P 37 ANN M ASS COMP; Schafer C., 2002, P 6 C NAT LANG LEARN, P1; Taigman Yaniv, 2017, 5 INT C LEARN REPR I; Tylecek R, 2013, LECT NOTES COMPUT SC, V8142, P364, DOI 10.1007/978-3-642-40602-7_39; YOSINSKI J, 2014, ADV NEURAL INFORM PR, P3320, DOI DOI 10.1109/IJCNN.2016.7727519; Zhang Meng, 2017, P 2017 C EMP METH NA, P1934; Zhu Jun-Yan, 2017, ICCV	31	29	29	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302014
C	Chizat, L; Bach, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Chizat, Lenaic; Bach, Francis			On the Global Convergence of Gradient Descent for Over-parameterized Models using Optimal Transport	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Many tasks in machine learning and signal processing can be solved by minimizing a convex function of a measure. This includes sparse spikes deconvolution or training a neural network with a single hidden layer. For these problems, we study a simple minimization method: the unknown measure is discretized into a mixture of particles and a continuous-time gradient descent is performed on their weights and positions. This is an idealization of the usual way to train neural networks with a large hidden layer. We show that, when initialized correctly and in the many-particle limit, this gradient flow, although non-convex, converges to global minimizers. The proof involves Wasserstein gradient flows, a by-product of optimal transport theory. Numerical experiments show that this asymptotic behavior is already at play for a reasonable number of particles, even in high dimension.	[Chizat, Lenaic; Bach, Francis] PSL Res Univ, ENS, INRIA, Paris, France	Inria; UDICE-French Research Universities; PSL Research University Paris; Ecole Normale Superieure (ENS)	Chizat, L (corresponding author), PSL Res Univ, ENS, INRIA, Paris, France.	lenaic.chizat@inria.fr; francis.bach@inria.fr			Region Ile-de-France; European Research Council [SEQUOIA 724063]	Region Ile-de-France(Region Ile-de-France); European Research Council(European Research Council (ERC)European Commission)	We acknowledge supports from grants from Region Ile-de-France and the European Research Council (grant SEQUOIA 724063).	ABRAHAM R, 1967, TRANSVERSAL MAPPINGS; Absil PA, 2005, SIAM J OPTIMIZ, V16, P531, DOI 10.1137/040605266; Ambrosio L., 2008, LECT MATH ETH ZURICH, VSecond; Bach F., 2017, J MACH LEARN RES, V18, p1 629 681; Blanchet A, 2018, J FUNCT ANAL, V275, P1650, DOI 10.1016/j.jfa.2018.06.014; Boyd N, 2017, SIAM J OPTIMIZ, V27, P616, DOI 10.1137/15M1035793; Bredies K, 2013, ESAIM CONTR OPTIM CA, V19, P190, DOI 10.1051/cocv/2011205; BROWDER FE, 1983, P SYMP PURE MATH, V39, P49; Catala P, 2017, J PHYS CONF SER, V904, DOI 10.1088/1742-6596/904/1/012015; Chu Wang, 2015, ARXIV151002558; Cohn Donald L., 1980, MEASURE THEORY, V165; Combettes PL, 2011, SPRINGER SER OPTIM A, V49, P185, DOI 10.1007/978-1-4419-9569-8_10; de Castro Y, 2012, J MATH ANAL APPL, V395, P336, DOI 10.1016/j.jmaa.2012.05.011; Duval V, 2015, FOUND COMPUT MATH, V15, P1315, DOI 10.1007/s10208-014-9228-6; Goodfellow I, 2016, ADAPT COMPUT MACH LE, P1; Gunasekar S, 2017, ADV NEUR IN, V30; Haeffele B, 2017, PROC IEEE C COMPUT V, P7331; Hauer Daniel, 2017, ARXIV170703129; Haykin S, 1998, NEURAL NETWORKS COMP, V2nd; Jaggi M., 2013, P 30 INT C MACH LEAR, V28; Journee M, 2010, SIAM J OPTIMIZ, V20, P2327, DOI 10.1137/080731359; Kushner Harold, 2003, STOCHASTIC APPROXIMA, V35; Lasserre J.-B., 2010, MOMENTS POSITIVE POL, V1; Li Y., 2017, ADV NEURAL INFORM PR, P597; Mei S, 2018, P NATL ACAD SCI USA, V115, pE7665, DOI 10.1073/pnas.1806579115; NITANDA A., 2017, ARXIV171205438; Poon C., 2018, ARXIV180208464; Rockafellar R. T., 1997, CONVEX ANAL; Rotskoff Grant M, 2018, STAT-US; Santambrogio F, 2017, B MATH SCI, V7, P87, DOI 10.1007/s13373-017-0101-1; Scieur D., 2017, ADV NEURAL INFORM PR, P1109; Sirignano J., 2018, MEAN FIELD ANAL NEUR; Soltanolkotabi M., 2018, IEEE T INFORM THEORY; Soudry Daniel, 2017, ARXIV170205777; Venturi L., 2018, ARXIV PREPRINT ARXIV; Whitney H., 1935, DUKE MATH J, V1, P514	37	29	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823303007
C	Ganea, OE; Becigneul, G; Hofmann, T		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Ganea, Octavian-Eugen; Becigneul, Gary; Hofmann, Thomas			Hyperbolic Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				POINCARE BALL MODEL	Hyperbolic spaces have recently gained momentum in the context of machine learning due to their high capacity and tree-likeliness properties. However, the representational power of hyperbolic geometry is not yet on par with Euclidean geometry, mostly because of the absence of corresponding hyperbolic neural network layers. This makes it hard to use hyperbolic embeddings in downstream tasks. Here, we bridge this gap in a principled manner by combining the formalism of Mobius gyrovector spaces with the Riemannian geometry of the Poincare model of hyperbolic spaces. As a result, we derive hyperbolic versions of important deep learning tools: multinomial logistic regression, feed-forward and recurrent neural networks such as gated recurrent units. This allows to embed sequential data and perform classification in the hyperbolic space. Empirically, we show that, even if hyperbolic optimization tools are limited, hyperbolic sentence embeddings either outperform or are on par with their Euclidean variants on textual entailment and noisy-prefix recognition tasks.	[Ganea, Octavian-Eugen; Becigneul, Gary; Hofmann, Thomas] Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland	Swiss Federal Institutes of Technology Domain; ETH Zurich	Ganea, OE; Becigneul, G (corresponding author), Swiss Fed Inst Technol, Dept Comp Sci, Zurich, Switzerland.	octavian.ganea@inf.ethz.ch; gary.becigneul@inf.ethz.ch			Swiss National Science Foundation (SNSF) [167176]; Max Planck ETH Center for Learning Systems	Swiss National Science Foundation (SNSF)(Swiss National Science Foundation (SNSF)); Max Planck ETH Center for Learning Systems	This research is funded by the Swiss National Science Foundation (SNSF) under grant agreement number 167176. Gary Becigneul is also funded by the Max Planck ETH Center for Learning Systems.	Albert Ungar Abraham, 2008, ANAL HYPERBOLIC GEOM; [Anonymous], 1979, COMPREHENSIVE INTRO; Bahdanau D., 2015, INT C LEARN REPR ICL; Birman GS, 2001, J MATH ANAL APPL, V254, P321, DOI 10.1006/jmaa.2000.7280; Bonnabel S, 2013, IEEE T AUTOMAT CONTR, V58, P2217, DOI 10.1109/TAC.2013.2254619; Bordes A., 2013, ADV NEURAL INFORM PR; Bowman SR., 2015, EMNLP, P632, DOI DOI 10.18653/V1/D15-1075; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Cannon J.W., 1997, FLAVORS GEOMETRY, P59; Chen Z., 2016, P 12 USENIX S OP SYS, P265, DOI 10.5555/ 3026877.3026899; Ganea Octavian-Eugen, 2018, P 35 INT C MACH LEAR; Gromov Mikhael, 1987, MATH SCI RES I PUBL, V8, P75, DOI 10.1007/978-1-4613-9586-7_3; Hamasaki M, 2017, MAR BIOTECHNOL, V19, P579, DOI 10.1007/s10126-017-9777-1; Hopper C., 2010, RICCI FLOW RIEMANNIA; Kim Y., 2014, P 2014 C EMP METH NA; Kingma DP., 2015, INT C LEARN REPR ICL; Krioukov D, 2010, PHYS REV E, V82, DOI 10.1103/PhysRevE.82.036106; Lamping J., 1995, Human Factors in Computing Systems. CHI'95 Conference Proceedings, P401; Lebanon G., 2004, P 21 INT C MACH LEAR, P66; Nickel M., 2011, INT C INT C MACH LEA, P809, DOI DOI 10.5555/3104482.3104584; Nickel M, 2017, ADV NEUR IN, V30; Rockt<spacing diaeresis>aschel T., 2015, P INT C LEARN REPR I; Sala F., 2018, ICML; Tallec Corentin, 2018, P INT C LEARN REPR I; Ungar AA, 2001, COMPUT MATH APPL, V41, P135, DOI 10.1016/S0898-1221(01)85012-4; Ungar Abraham Albert, 2014, ANAL HYPERBOLIC GEOM; Ungar Abraham Albert, 2008, SYNTHESIS LECT MATH, V1, P2; Vendrov Ivan, 2016, P INT C LEARN REPR I; Vermeer J, 2005, TOPOL APPL, V152, P226, DOI 10.1016/j.topol.2004.10.012	29	29	29	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305037
C	Houthooft, R; Chen, RY; Isola, P; Stadie, BC; Wolski, F; Ho, J; Abbeel, P		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Houthooft, Rein; Chen, Richard Y.; Isola, Phillip; Stadie, Bradly C.; Wolski, Filip; Ho, Jonathan; Abbeel, Pieter			Evolved Policy Gradients	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We propose a metalearning approach for learning gradient-based reinforcement learning (RL) algorithms. The idea is to evolve a differentiable loss function, such that an agent, which optimizes its policy to minimize this loss, will achieve high rewards. The loss is parametrized via temporal convolutions over the agent's experience. Because this loss is highly flexible in its ability to take into account the agent's history, it enables fast task learning. Empirical results show that our evolved policy gradient algorithm (EPG) achieves faster learning on several randomized environments compared to an off-the-shelf policy gradient method. We also demonstrate that EPG's learned loss can generalize to out-of-distribution test time tasks, and exhibits qualitatively different behavior from other popular metalearning algorithms.	[Houthooft, Rein; Chen, Richard Y.; Isola, Phillip; Stadie, Bradly C.; Wolski, Filip; Ho, Jonathan] OpenAI, San Francisco, CA 94110 USA; [Isola, Phillip; Stadie, Bradly C.; Ho, Jonathan; Abbeel, Pieter] Univ Calif Berkeley, Berkeley, CA USA; [Isola, Phillip] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA	University of California System; University of California Berkeley; Massachusetts Institute of Technology (MIT)	Houthooft, R (corresponding author), OpenAI, San Francisco, CA 94110 USA.							[Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2018, ARXIV180209464; Brockman G., 2016, OPENAI GYM; Dearden R, 1999, UNCERTAINTY IN ARTIFICIAL INTELLIGENCE, PROCEEDINGS, P150; Duan Y., 2016, RL2 FAST REINFORCEME; Duan Y, 2016, INT C MACH LEARN, P1329; Finn C., 2017, ARXIV171011622; Finn C, 2017, ARXIV170303400; Gershman S.J., 2017, BEHAV BRAIN SCI, V40; Haarnoja T., 2017, ARXIV170208165; Hansen N, 2001, EVOL COMPUT, V9, P159, DOI 10.1162/106365601750190398; Kolter J. Z., 2009, P 26 ANN INT C MACHI, P513, DOI DOI 10.1145/1553374.1553441; Konda VR, 2000, ADV NEUR IN, V12, P1008; Mishra Nikhil, 2017, ARXIV170703141; Nesterov Y, 2017, FOUND COMPUT MATH, V17, P527, DOI 10.1007/s10208-015-9296-2; Ostrovski G., 2017, ARXIV170301310; Pathak D., 2017, INT C MACH LEARN ICM, V2017; Rechenberg I., 1973, EVOLUTIONSSTRATEGIE; Salimans T., 2017, ARXIV170303864; SCHMIDHUBER J, 2003, NAT COMP SER, P579; Schulman, 2017, ARXIV170601502; Schulman J., 2015, ARXIV PREPRINT ARXIV; Schulman J., 2017, ABS170706347 CORR; Schulman J, 2015, PR MACH LEARN RES, V37, P1889; Schulman John, 2017, EQUIVALENCE POLICY; Schwefel H.P., 1977, EVOLUTIONSSTRATEGIEN, P123, DOI [10.1007/978-3-0348-5927-15, DOI 10.1007/978-3-0348-5927-15]; Sehnke F, 2010, NEURAL NETWORKS, V23, P551, DOI 10.1016/j.neunet.2009.12.004; SPALL JC, 1992, IEEE T AUTOMAT CONTR, V37, P332, DOI 10.1109/9.119632; Sutton RS, 2000, ADV NEUR IN, V12, P1057; Sutton RS, 1998, REINFORCEMENT LEARNI, V1; Tang H., 2017, ADV NEURAL INFORM PR; Wang J.X., 2016, ARXIV161105763; Williams R.J., 1992, REINFORCEMENT LEARNI, V173, P5, DOI [10.1007/978-1-4615-3618-5, DOI 10.1007/978-1-4615-3618-5]	34	29	29	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823305042
C	Lage, I; Ross, AS; Kim, B; Gershman, SJ; Doshi-Velez, F		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Lage, Isaac; Ross, Andrew Slavin; Kim, Been; Gershman, Samuel J.; Doshi-Velez, Finale			Human-in-the-Loop Interpretability Prior	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We often desire our models to be interpretable as well as accurate. Prior work on optimizing models for interpretability has relied on easy-to-quantify proxies for interpretability, such as sparsity or the number of operations required. In this work, we optimize for interpretability by directly including humans in the optimization loop. We develop an algorithm that minimizes the number of user studies to find models that are both predictive and interpretable and demonstrate our approach on several data sets. Our human subjects results show trends towards different proxy notions of interpretability on different datasets, which suggests that different proxies are preferred on different tasks.	[Lage, Isaac; Ross, Andrew Slavin; Doshi-Velez, Finale] Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA; [Kim, Been] Google Brain, Mountain View, CA USA; [Gershman, Samuel J.] Harvard Univ, Dept Psychol, Cambridge, MA 02138 USA	Harvard University; Google Incorporated; Harvard University	Lage, I (corresponding author), Harvard Univ, Dept Comp Sci, Cambridge, MA 02138 USA.	isaaclage@g.harvard.edu; andrew_ross@g.harvard.edu; beenkim@google.com; gershman@fas.harvard.edu; finale@seas.harvard.edu			NIH [5T32LM012411-02]; Google Faculty Research Award; Harvard Dean's Competitive Fund	NIH(United States Department of Health & Human ServicesNational Institutes of Health (NIH) - USA); Google Faculty Research Award(Google Incorporated); Harvard Dean's Competitive Fund	IL acknowledges support from NIH 5T32LM012411-02. All authors acknowledge support from the Google Faculty Research Award and the Harvard Dean's Competitive Fund. All authors thank Emily Chen and Jeffrey He for their support with the experimental interface, and Weiwei Pan and the Harvard DTaK group for many helpful discussions and insights.	Altendorf Eric, 2005, P 21 C UNC ART INT U, P18; Bach F., 2010, ADV NEURAL INFORM PR, P118; Caruana R, 2015, KDD'15: PROCEEDINGS OF THE 21ST ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1721, DOI 10.1145/2783258.2788613; Chaloner K, 1995, STAT SCI, V10, P273, DOI 10.1214/ss/1177009939; Christiano Paul F, 2017, P 31 INT C NEURAL IN, P4302; Chu W, 2004, IEEE T NEURAL NETWOR, V15, P29, DOI 10.1109/TNN.2003.820830; Dheeru D., 2019, UCI MACHINE LEARNING; Freitas A, 2014, ACM SIGKDD EXPLORATI, V15, P1, DOI DOI 10.1145/2594473.2594475; Hinton G.E., 2012, NEURAL NETWORKS TRIC, P599, DOI 10.1007/978-3-642-35289-8_32; Kim B., 2014, ADV NEURAL INFORM PR, P1952; Kim B, 2017, ARXIV PREPRINT ARXIV; Kosorukoff Alex, 2001, P IEEE INT C SYST MA, V5; Lavrac N, 1999, ARTIF INTELL MED, V16, P3, DOI 10.1016/S0933-3657(98)00062-1; Lipton Zachary C, 2016, INT C MACH LEARN WOR; Little Greg, 2010, P 23 ANN ACM S US IN, P57, DOI DOI 10.1145/1866029.1866040; Ma Yifei, 2012, ABS12093694 CORR; Masood Muhammad A., 2018, PARTICLE BASED VARIA; Menaka Narayanan Emily, 2018, DO HUMANS UNDERSTAND; Poursabzi-Sangdeh Forough, 2018, ABS180207810 CORR; Ribeiro MT, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1135, DOI 10.1145/2939672.2939778; Rokach Lior, 2014, INTRO DECISION TREES, P1; Ross A., 2018, 2018 ICML WORKSH HUM; Ross A, 2017, WORKSH TRANSP INT MA; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas N., 2009, P 27 INT C MACHINE L, P1015; Tamuz Omer, 2011, ABS11051033 CORR; Tibshirani R, 1996, J ROY STAT SOC B MET, V58, P267, DOI 10.1111/j.2517-6161.1996.tb02080.x; Ustun B, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P1125, DOI 10.1145/3097983.3098161; Wilson A.G., 2015, ADV NEURAL INFORM PR, V28, P2854; Wirth C, 2017, J MACH LEARN RES, V18; Wu MK, 2018, AAAI CONF ARTIF INTE, P1670; Zhu X., 2003, INT C MACH LEARN	34	29	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA	33623354				2022-12-19	WOS:000461852004069
C	Tsuzuku, Y; Sato, I; Sugiyama, M		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Tsuzuku, Yusuke; Sato, Issei; Sugiyama, Masashi			Lipschitz-Margin Training: Scalable Certification of Perturbation Invariance for Deep Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA				ROBUSTNESS	High sensitivity of neural networks against malicious perturbations on inputs causes security concerns. To take a steady step towards robust classifiers, we aim to create neural network models provably defended from perturbations. Prior certification work requires strong assumptions on network structures and massive computational costs, and thus the range of their applications was limited. From the relationship between the Lipschitz constants and prediction margins, we present a computationally efficient calculation technique to lower-bound the size of adversarial perturbations that can deceive networks, and that is widely applicable to various complicated networks. Moreover, we propose an efficient training procedure that robustifies networks and significantly improves the provably guarded areas around data points. In experimental evaluations, our method showed its ability to provide a non-trivial guarantee and enhance robustness for even large networks.	[Tsuzuku, Yusuke; Sato, Issei; Sugiyama, Masashi] Univ Tokyo, RIKEN, Tokyo, Japan	RIKEN; University of Tokyo	Tsuzuku, Y (corresponding author), Univ Tokyo, RIKEN, Tokyo, Japan.	tsuzuku@ms.k.u-tokyo.ac.jp; sato@k.u-tokyo.ac.jp; sugi@k.u-tokyo.ac.jp	Sugiyama, Masashi/AEO-1176-2022	Sugiyama, Masashi/0000-0001-6658-6743	Toyota/Dwango AI scholarship; KAKENHI [17H00757]	Toyota/Dwango AI scholarship; KAKENHI(Ministry of Education, Culture, Sports, Science and Technology, Japan (MEXT)Japan Society for the Promotion of ScienceGrants-in-Aid for Scientific Research (KAKENHI))	Authors appreciate Takeru Miyato for valuable feedback. YT was supported by Toyota/Dwango AI scholarship. IS was supported by KAKENHI 17H04693. MS was supported by KAKENHI 17H00757.	Akhtar N., 2018, CORR; [Anonymous], 2018, INT C LEARN REPR; Athalye A, 2018, PR MACH LEARN RES, V80; BARTLETT P., 2017, SPECTRALLY NORMALIZE; Bastani O., 2015, ADV NEURAL INFORM PR, V28, P2613; Carlini N, 2017, P IEEE S SECUR PRIV, P39, DOI 10.1109/SP.2017.49; Cisse M, 2017, PR MACH LEARN RES, V70; Friedman J, 1998, LINEAR ALGEBRA APPL, V280, P199, DOI 10.1016/S0024-3795(98)10020-4; Goodfellow I. J., 2015, P ICLR; Goodfellow I. J., 2018, CORR; Guo Chuan, 2018, COUNTERING ADVERSARI, V2, P5; Hein M, 2017, NIPS 17; Katz G, 2017, ELECTRON P THEOR COM, P19, DOI 10.4204/EPTCS.257.3; Katz G, 2017, LECT NOTES COMPUT SC, V10426, P97, DOI 10.1007/978-3-319-63387-9_5; Kurakin Alexey, 2017, INT C LEARNING REPRE; Kwiatkowska Marta, 2018, P 27 INT JOINT C ART, P2651; Langford J., 2002, ADV NEURAL INFORM PR, P439; Madry Aleksander, 2018, ICLR; Moosavi-Dezfooli SM, 2016, PROC CVPR IEEE, P2574, DOI 10.1109/CVPR.2016.282; Nair V., 2010, P 27 INT C MACHINE L, P807, DOI DOI 10.5555/3104322.3104425; Netzer Y., 2011, NEUR INF PROC SYST W; Neyshabur Behnam, 2018, INT C LEARN REPR; Papernot N, 2016, P IEEE S SECUR PRIV, P582, DOI 10.1109/SP.2016.41; Peck J, 2017, ADV NEURAL INFORM PR, P804; Sinha A., 2018, ICLR; Su J., 2017, CORR; Szegedy C., 2014, 2014 INT C LEARNING, DOI DOI 10.48550/ARXIV.1312.6199; Tramer F., 2018, ICLR 18; Weng T.-W., 2018, INT C LEARN REPR; Wong E., 2018, P INT C MACH LEARN, P5286; Xu W., 2018, NETW DISTR SYST SEC; Yoshida Y., 2017, CORR; Zagoruyko S, 2016, 5 INT C LEARN REPRES, DOI DOI 10.5244/C.30.87	34	29	30	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852001011
C	Zhang, XM; Zhang, ZT; Zhang, CK; Tenenbaum, JB; Freeman, WT; Wu, JJ		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Zhang, Xiuming; Zhang, Zhoutong; Zhang, Chengkai; Tenenbaum, Joshua B.; Freeman, William T.; Wu, Jiajun			Learning to Reconstruct Shapes from Unseen Classes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					From a single image, humans are able to perceive the full 3D shape of an object by exploiting learned shape priors from everyday life. Contemporary single-image 3D reconstruction algorithms aim to solve this task in a similar fashion, but often end up with priors that are highly biased by training classes. Here we present an algorithm, Generalizable Reconstruction (GenRe), designed to capture more generic, class-agnostic shape priors. We achieve this with an inference network and training procedure that combine 2.5D representations of visible surfaces (depth and silhouette), spherical shape representations of both visible and non-visible surfaces, and 3D voxel-based representations, in a principled manner that exploits the causal structure of how 3D shapes give rise to 2D images. Experiments demonstrate that GenRe performs well on single-view shape reconstruction, and generalizes to diverse novel objects from categories not seen during training.	[Zhang, Xiuming; Zhang, Zhoutong; Zhang, Chengkai; Tenenbaum, Joshua B.; Wu, Jiajun] MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Freeman, William T.] MIT, CSAIL, Google Res, 77 Massachusetts Ave, Cambridge, MA 02139 USA	Massachusetts Institute of Technology (MIT); Google Incorporated; Massachusetts Institute of Technology (MIT)	Zhang, XM (corresponding author), MIT, CSAIL, 77 Massachusetts Ave, Cambridge, MA 02139 USA.		Wu, JiaJun/GQH-7885-2022; Zhang, Xiuming/AAQ-9850-2021	Zhang, Xiuming/0000-0002-4326-727X	NSF [1231216, 1447476]; ONR MURI [N00014-16-1-2007]; Toyota Research Institute; Shell; Facebook	NSF(National Science Foundation (NSF)); ONR MURI(MURIOffice of Naval Research); Toyota Research Institute; Shell(Royal Dutch Shell); Facebook(Facebook Inc)	We thank the anonymous reviewers for their constructive comments. This work is supported by NSF #1231216, NSF #1447476, ONR MURI N00014-16-1-2007, Toyota Research Institute, Shell, and Facebook.	Akata Z., 2016, CVPR; Antol S., 2014, ECCV; Bansal A., 2016, P CVPR; Barron JT, 2015, IEEE T PATTERN ANAL, V37, P1670, DOI 10.1109/TPAMI.2014.2377712; Barrow H., 1978, COMPUTER VISION SYST; Barrow Harry G, 1977, IJCAI; Bart E., 2005, CVPR; Bell S, 2014, ACM T GRAPHIC, V33, DOI 10.1145/2601097.2601206; Bronstein AM, 2008, MONOGR COMPUT SCI, P1, DOI 10.1007/978-0-387-73301-2_1; Cao Z., 2017, 3DV; Chang A. X., 2015, ARXIV PREPRINT ARXIV; Chen Weifeng, 2016, NEURIPS; Choy C.B., 2016, ECCV; Cohen Taco, 2017, ICML; Cohen Taco S, 2018, ICLR; Eigen D., 2015, ICCV; Esteves Carlos, 2018, ECCV; Fan H., 2017, CVPR; Farhadi A., 2009, CVPR; Funk Christopher, 2017, ICCV; Girdhar R., 2016, ECCV; Groueix Thibault, 2018, COMPUTER VISION PATT; Hane Christian, 2017, 3DV; Hariharan B., 2017, ICCV; Horn B.K.P., 1989, SHAPE SHADING; Izadi S., 2011, UIST; Jakob Wenzel, 2010, MITSUBA RENDERER; JANNER M, 2017, NEURIPS; Jayaraman Dinesh, 2018, ECCV; Kar A., 2015, CVPR; Kazhdan Michael, 2002, ECCV; Kazhdan Michael, 2004, SGP; Lampert C. H., 2009, CVPR; Lewiner T., 2003, Journal of Graphics Tools, V8, P1, DOI 10.1080/10867651.2003.10487582; Marr D., 1982, VISION; McCormac J., 2017, ICCV; Novotny D., 2017, ICCV; Peng X., 2015, ICCV; Proesmans Marc, 1996, ICPR; Riegler G., 2017, CVPR; Rock J., 2015, P CVPR; Ronneberger O., 2015, INT C MED IM COMP CO, P234, DOI [10.1007/978-3-319-24574-4_28, DOI 10.1007/978-3-319-24574-4_28]; Sagawa R., 2011, ICCV; Shi J., 2017, CVPR; Shin D., 2018, CVPR; Silberman N., 2012, ECCV; Soltani A. A., 2017, COMPUTER VISION PATT; Song S., 2017, CVPR; Sun J, 2015, CVPR; Sun Xingyuan, 2018, P IEEE C COMP VIS PA; Tappen Marshall F, 2003, NEURIPS; Tatarchenko M., 2016, ECCV; Tatarchenko M., 2017, ICCV; Torralba Antonio, 2007, IEEE TPAMI, V29; Tulsiani S., 2017, IEEE C COMP VIS PATT; Wang P., 2017, CVPR; Wang X., 2015, CVPR; Wang Y., 2016, ECCV; Weiss Y., 2001, ICCV; Wu J., 2017, NEURIPS; Wu Jiajun, 2018, EUR C COMP VIS 2018; Wu Jiajun, 2016, NEURIPS; Xian Y., 2017, CVPR; Yan Xinchen, 2016, NEURIPS; Yao Shunyu, 2018, NEURIPS; Zhang R, 1999, IEEE T PATTERN ANAL, V21, P690, DOI 10.1109/34.784284; Zhu Jun-Yan, 2018, NEURIPS	69	29	29	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823302028
C	Bateni, M; Behnezhad, S; Derakhshan, M; Hajiaghayi, M; Kiveris, R; Lattanzi, S; Mirrokni, V		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Bateni, MohammadHossein; Behnezhad, Soheil; Derakhshan, Mahsa; Hajiaghayi, MohammadTaghi; Kiveris, Raimondas; Lattanzi, Silvio; Mirrokni, Vahab			Affinity Clustering: Hierarchical Clustering at Scale	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					Graph clustering is a fundamental task in many data-mining and machine-learning pipelines. In particular, identifying a good hierarchical structure is at the same time a fundamental and challenging problem for several applications. The amount of data to analyze is increasing at an astonishing rate each day. Hence there is a need for new solutions to efficiently compute effective hierarchical clusterings on such huge data. The main focus of this paper is on minimum spanning tree (MST) based clusterings. In particular, we propose affinity, a novel hierarchical clustering based on Boruvka's MST algorithm. We prove certain theoretical guarantees for affinity (as well as some other classic algorithms) and show that in practice it is superior to several other state-of-the-art clustering algorithms. Furthermore, we present two MapReduce implementations for affinity. The first one works for the case where the input graph is dense and takes constant rounds. It is based on a Massively Parallel MST algorithm for dense graphs that improves upon the state-of-the-art algorithm of Lattanzi et al. [34]. Our second algorithm has no assumption on the density of the input graph and finds the affinity clustering in O (log n) rounds using Distributed Hash Tables (DHTs). We show experimentally that our algorithms are scalable for huge data sets, e.g., for graphs with trillions of edges.	[Bateni, MohammadHossein; Kiveris, Raimondas; Lattanzi, Silvio; Mirrokni, Vahab] Google Res, Mountain View, CA 94043 USA; [Behnezhad, Soheil; Derakhshan, Mahsa; Hajiaghayi, MohammadTaghi] Univ Maryland, College Pk, MD 20742 USA	Google Incorporated; University System of Maryland; University of Maryland College Park	Bateni, M (corresponding author), Google Res, Mountain View, CA 94043 USA.	bateni@google.com; soheil@cs.umd.edu; mahsaa@cs.umd.edu; hajiagha@cs.umd.edu; rkiveris@google.com; silviol@google.com; mirrokni@google.com	Jeong, Yongwook/N-7413-2016		NSF CAREER award [CCF-1053605]; NSF BIGDATA [IIS-1546108]; NSF AF [CCF-1161365]; DARPA GRAPHS/AFOSR [FA9550-12-1-0423]; DARPA SIMPLEX grant	NSF CAREER award(National Science Foundation (NSF)NSF - Office of the Director (OD)); NSF BIGDATA; NSF AF(National Science Foundation (NSF)NSF - Directorate for Computer & Information Science & Engineering (CISE)); DARPA GRAPHS/AFOSR; DARPA SIMPLEX grant	Supported in part by NSF CAREER award CCF-1053605, NSF BIGDATA grant IIS-1546108, NSF AF: Medium grant CCF-1161365, DARPA GRAPHS/AFOSR grant FA9550-12-1-0423, and another DARPA SIMPLEX grant.	Ackerman M., 2010, COLT, P270; AGRAWAL A, 1995, SIAM J COMPUT, V24, P440, DOI 10.1137/S0097539792236237; Ahn Kook Jin, 2012, P 33 ANN ACM SIAM S, P17; Andoni A, 2014, STOC'14: PROCEEDINGS OF THE 46TH ANNUAL 2014 ACM SYMPOSIUM ON THEORY OF COMPUTING, P574, DOI 10.1145/2591796.2591805; Bahmani B, 2012, PROC VLDB ENDOW, V5, P622, DOI 10.14778/2180912.2180915; Balcan M.F., 2013, P 26 INT C NEURAL IN, P1995; Balcan MF, 2008, ACM S THEORY COMPUT, P671; Bateni M., 2014, NIPS, P2591; Bateni M, 2011, J ACM, V58, DOI 10.1145/2027216.2027219; Beame P., 2013, P 32 ACM SIGMOD SIGA, P273, DOI DOI 10.1145/2463664.2465224; Bor uvka O., 1926, JISTEM PROBLEMU MINI; Chang F, 2008, ACM T COMPUT SYST, V26, DOI 10.1145/1365815.1365816; Charikar M, 2017, PROCEEDINGS OF THE TWENTY-EIGHTH ANNUAL ACM-SIAM SYMPOSIUM ON DISCRETE ALGORITHMS, P841; Chen Jin, 2013, P 4 INT SC WORKSH DA; Chitnis R, 2016, P 27 ANN ACM SIAM S, P1326; Chitnis RH, 2015, P 26 ANN ACM SIAM S, P1234; Dasgupta S, 2016, ACM S THEORY COMPUT, P118, DOI 10.1145/2897518.2897527; Dean J, 2004, USENIX ASSOCIATION PROCEEDINGS OF THE SIXTH SYMPOSIUM ON OPERATING SYSTEMS DESIGN AND IMPLEMENTATION (OSDE '04), P137; Ene A., 2011, SIGKDD, DOI DOI 10.1145/2020408.2020515; Esfandiari H, 2015, P 26 ANN ACM SIAM S, P1217; Glazer Assaf, 2014, ADV NEURAL INFORM PR, P999; GOEMANS MX, 1995, SIAM J COMPUT, V24, P296, DOI 10.1137/S0097539793242618; Goldberger J., 2004, P INT C NEURAL INFOR, P505; Goodrich MT, 2011, LECT NOTES COMPUT SC, V7074, P374, DOI 10.1007/978-3-642-25591-5_39; Gower J. C., 1969, APPL STAT, P54, DOI [DOI 10.2307/2346439, 10.2307/2346439]; Haggstrom O, 1996, RANDOM STRUCT ALGOR, V9, P295, DOI 10.1002/(SICI)1098-2418(199610)9:3<295::AID-RSA3>3.0.CO;2-S; Hajiaghayi M, 2013, ANN IEEE SYMP FOUND, P558, DOI 10.1109/FOCS.2013.66; Im SJ, 2017, ACM S THEORY COMPUT, P798, DOI 10.1145/3055399.3055460; Jin C, 2015, 2015 IEEE FIRST INTERNATIONAL CONFERENCE ON BIG DATA COMPUTING SERVICE AND APPLICATIONS (BIGDATASERVICE 2015), P418, DOI 10.1109/BigDataService.2015.67; JUNGER M, 1995, ALGORITHMICA, V13, P357, DOI 10.1007/BF01293485; Karloff H, 2010, PROC APPL MATH, V135, P938; Kiveris R., 2014, ACM S CLOUD COMP; Krishnamurthy A., 2012, ICML, P887; Kruskal J. B., 1956, P AM MATH SOC, V7, P48, DOI [DOI 10.1090/S0002-9939-1956-0078686-7, 10.2307/2033241]; Lattanzi S, 2011, SPAA 11: PROCEEDINGS OF THE TWENTY-THIRD ANNUAL SYMPOSIUM ON PARALLELISM IN ALGORITHMS AND ARCHITECTURES, P85; Leskovec J, 2014, MINING OF MASSIVE DATASETS, 2ND EDITION, P1; Leskovec J, 2014, SNAP DATASETS STANFO; Lichman M, 2013, UCI MACHINE LEARNING; Newman M., 2010, NETWORKS INTRO, DOI [DOI 10.1093/ACPROF:OSO/9780199206650.001.0001, 10.1162/artl_r_00062., 10.1162/artl_r_00062]; RAND WM, 1971, J AM STAT ASSOC, V66, P846, DOI 10.2307/2284239; Roy A., 2016, ADV NEURAL INFORM PR, P2316; Sollin M., 1965, PROGRAMMING GAMES TR; White T., 2012, HADOOP DEFINITIVE GU; Zadeh R.B., 2009, P 25 C UNCERTAINTY A, P639; Zaharia M, 2010, HOTCLOUD, P10, DOI DOI 10.HTTP://DL.ACM.0RG/CITATI0N.CFM?	45	29	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649406089
C	van der Wilk, M; Rasmussen, CE; Liensman, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		van der Wilk, Mark; Rasmussen, Carl Edward; Liensman, James			Convolutional Gaussian Processes	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We present a practical way of introducing convolutional structure into Gaussian processes, making them more suited to high-dimensional inputs like images. The main contribution of our work is the construction of an inter-domain inducing point approximation that is well-tailored to the convolutional kernel. This allows us to gain the generalisation benefit of a convolutional kernel, together with fast but accurate posterior inference. We investigate several variations of the convolutional kernel, and apply it to MNIST and CIFAR-10, where we obtain significant improvements over existing Gaussian process models. We also show how the marginal likelihood can be used to find an optimal weighting between convolutional and RBI kernels to further improve performance. This illustration of the usefulness of the marginal likelihood may help automate discovering architectures in larger models.	[van der Wilk, Mark; Rasmussen, Carl Edward] Univ Cambridge, Dept Engn, Cambridge, England; [Liensman, James] Prowler Io, Cambridge, England	University of Cambridge	van der Wilk, M (corresponding author), Univ Cambridge, Dept Engn, Cambridge, England.	mv310@cam.ac.uk; cer54@cam.ac.uk; james@prowler.io			EPSRC [EP/J012300]; Qualcomm Innovation Fellowship	EPSRC(UK Research & Innovation (UKRI)Engineering & Physical Sciences Research Council (EPSRC)); Qualcomm Innovation Fellowship	CER gratefully acknowledges support from EPSRC grant EP/J012300. MvdW is generously supported by a Qualcomm Innovation Fellowship.	Bauer M., 2016, ADV NEURAL INFORM PR, V29, P1533, DOI DOI 10.5555/3157096.3157268; Bengio Y, 2009, FOUND TRENDS MACH LE, V2, P1, DOI 10.1561/2200000006; Bui T. D., 2016, ARXIV160507066; Calandra R, 2016, IEEE IJCNN, P3338, DOI 10.1109/IJCNN.2016.7727626; de Garis Matthews A. G., 2016, THESIS; Durrande N., 2012, ANN FAC SCI TOULOUSE, V21, P481, DOI [10.5802/afst.1342, DOI 10.5802/AFST.1342]; Duvenaud D., 2013, ARXIV13024922V4; Duvenaud D.K., 2011, ADV NEURAL INFORM PR, P226; Hensman J., 2016, ARXIV161106740; Hensman J., 2013, P 20 9 C UNCERTAINTY, P282, DOI DOI 10.1093/IMAIAI/IAX023; Hensman J, 2015, JMLR WORKSH CONF PRO, V38, P351; Hensman James, 2015, ADV NEURAL INFORM PR, P1639; Hernandez-Lobato D, 2016, JMLR WORKSH CONF PRO, V51, P168; Kingma D.P, P 3 INT C LEARNING R; Krauth Karl, 2016, AUTOGP EXPLORING CAP; Krizhevsky A, 2009, LEARNING MULTIPLE LA; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lazaro-Gredilla M., 2009, ADV NEURAL INFORM PR, V22, P1087; Lazaro-Gredilla M, 2010, J MACH LEARN RES, V11, P1865; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Mairal J., 2014, ADV NEURAL INFORM PR, V27, P2627; Matthews AGD, 2016, JMLR WORKSH CONF PRO, V51, P231; Matthews AGD, 2017, J MACH LEARN RES, V18, P1; Neal R. M., 1996, LECT NOTES STAT; Opper M, 2009, NEURAL COMPUT, V21, P786, DOI 10.1162/neco.2008.08-07-592; Pandey G, 2014, PR MACH LEARN RES, V32, P1719; Quinonero-Candela JQ, 2005, J MACH LEARN RES, V6, P1939; Seeger M.W., 2003, INT WORKSHOP ARTIFIC, VR4, P254; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Snelson E., 2005, ADV NEURAL INFORM PR, V18, P1257; Sriperumbudur BK, 2011, J MACH LEARN RES, V12, P2389; Steinwart I, 2002, J MACH LEARN RES, V2, P67, DOI 10.1162/153244302760185252; Titsias M. K., 2009, ARTIF INTELL STAT, V3; Villacampa-Calvo C, 2017, PR MACH LEARN RES, V70; Wilson AG, 2016, ADV NEUR IN, V29; Wilson AG, 2015, PR MACH LEARN RES, V37, P1775	36	29	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402087
C	Wang, MZ; Tang, YH; Wang, J; Deng, J		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Mingzhe; Tang, Yihe; Wang, Jian; Deng, Jia			Premise Selection for Theorem Proving by Deep Graph Embedding	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a deep learning-based approach to the problem of premise selection: selecting mathematical statements relevant for proving a given conjecture. We represent a higher-order logic formula as a graph that is invariant to variable renaming but still fully preserves syntactic and semantic information. We then embed the graph into a vector via a novel embedding method that preserves the information of edge ordering. Our approach achieves state-of-the-art results on the HolStep dataset, improving the classification accuracy from 83% to 90.3%.	[Wang, Mingzhe; Tang, Yihe; Wang, Jian; Deng, Jia] Univ Michigan, Ann Arbor, MI 48109 USA	University of Michigan System; University of Michigan	Wang, MZ (corresponding author), Univ Michigan, Ann Arbor, MI 48109 USA.		Jeong, Yongwook/N-7413-2016		National Science Foundation [1633157]	National Science Foundation(National Science Foundation (NSF))	This work is partially supported by the National Science Foundation under Grant No. 1633157.	Alama J, 2014, J AUTOM REASONING, V52, P191, DOI 10.1007/s10817-013-9286-5; Alemi AA, 2016, ADV NEUR IN, V29; Bridge JP, 2014, J AUTOM REASONING, V53, P141, DOI 10.1007/s10817-014-9301-5; Church Alonzo, 1940, J SYMBOLIC LOGIC, V5, P56, DOI [DOI 10.2307/2266170, 10.2307/2266170]; Denzinger Jorg, 1999, LEARNING PREVIOUS PR; diaeresis>el Defferrard Micha<spacing, 2016, NEURIPS, DOI DOI 10.5555/3157382.3157527; Dowek Gilles, 1992, COQ PROOF ASSISTANT; Duvenaud David K, 2015, P NIPS; Farber M, 2016, LECT NOTES ARTIF INT, V9706, P349, DOI 10.1007/978-3-319-40229-1_24; Goller C, 1996, IEEE IJCNN, P347, DOI 10.1109/ICNN.1996.548916; Gonthier Georges, 2013, MACHINE CHECKED PROO, P163; GORI M, 2005, IEEE IJCNN, P729, DOI DOI 10.1109/IJCNN.2005.1555942; Grover A, 2016, KDD'16: PROCEEDINGS OF THE 22ND ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P855, DOI 10.1145/2939672.2939754; Hales T, 2017, FORUM MATH PI, V5, DOI 10.1017/fmp.2017.1; Harrington J, 2013, J TROP MED-US, V2013, DOI 10.1155/2013/756832; Harrison J., 2014, COMPUTATIONAL LOGIC, V9, P135, DOI [10.1016/B978-0-444-51624-4.50004-6, DOI 10.1016/B978-0-444-51624-4.50004-6]; Harrison John, 2009, HOL LIGHT OVERVIEW T, P60; Henaff M, 2015, ARXIV150605163; Hinton G., 2012, NEURAL NETWORKS MACH, V264, P1; Hoder K, 2011, LECT NOTES ARTIF INT, V6803, P299, DOI 10.1007/978-3-642-22438-6_23; JAIN A, 2016, PROC CVPR IEEE, P5308, DOI DOI 10.1109/CVPR.2016.573; Jakubuv Jan, 2017, ARXIV170106532; Kaliszyk C, 2015, LECT NOTES COMPUT SC, V9450, P88, DOI 10.1007/978-3-662-48899-7_7; Kaliszyk Cezary, 2017, ARXIV170300426; Kern C., 1999, ACM Transactions on Design Automation of Electronic Systems, V4, P123, DOI 10.1145/307988.307989; Kipf TN, 2016, P INT C LEARN REPR; Klein G, 2009, SOSP'09: PROCEEDINGS OF THE TWENTY-SECOND ACM SIGOPS SYMPOSIUM ON OPERATING SYSTEMS PRINCIPLES, P207; Kovacs Laura, 2013, Computer Aided Verification. 25th International Conference, CAV 2013. Proceedings. LNCS 8044, P1, DOI 10.1007/978-3-642-39799-8_1; Kuhlwein D, 2015, J AUTOM REASONING, V55, P91, DOI 10.1007/s10817-015-9329-1; Leroy X, 2009, COMMUN ACM, V52, P107, DOI 10.1145/1538788.1538814; Li Yujia, 2015, ARXIV151105493; Loos S. M., 2017, 21 INT C LOGIC PROGR, P85; Mikolov T., 2013, ARXIV; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Misra Dipendra Kumar, 2016, P 2016 C EMPIRICAL M, P1775; Naumowicz Adam, 2009, BRIEF OVERVIEW MIZAR, P67; Niepert M, 2016, PR MACH LEARN RES, V48; Perozzi B, 2014, KDD, V20, P701, DOI DOI 10.1145/2623330.2623732; Robinson A., 2001, HDB AUTOMATED REASON, VI; Scarselli F, 2009, IEEE T NEURAL NETWOR, V20, P61, DOI 10.1109/TNN.2008.2005605; Schulz S, 2002, AI COMMUN, V15, P111; Schulz S., 2000, LEARNING SEARCH CONT, V230; Socher R., 2011, P 28 INT C INT C MAC, P129; Socher R., 2011, ADV NEURAL INF PROCE, V24, P1; SUTTNER C, 1990, LECT NOTES ARTIF INT, V449, P470; Szegedy C., 2015, ARXIV 1502 03167, P448, DOI DOI 10.1007/S13398-014-0173-7.2; Tai KS, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING, VOL 1, P1556; Tang J, 2015, PROCEEDINGS OF THE 24TH INTERNATIONAL CONFERENCE ON WORLD WIDE WEB (WWW 2015), P1067, DOI 10.1145/2736277.2741093; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; Wenzel M, 2008, LECT NOTES COMPUT SC, V5170, P33, DOI 10.1007/978-3-540-71067-7_7; Whalen D, 2016, ARXIV160802644	51	29	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649402081
C	Bilen, H; Vedaldi, A		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Bilen, Hakan; Vedaldi, Andrea			Integrated Perception with Recurrent Multi-Task Neural Networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN					Modern discriminative predictors have been shown to match natural intelligences in specific perceptual tasks in image classification, object and part detection, boundary extraction, etc. However, a major advantage that natural intelligences still have is that they work well for all perceptual problems together, solving them efficiently and coherently in an integrated manner. In order to capture some of these advantages in machine perception, we ask two questions: whether deep neural networks can learn universal image representations, useful not only for a single task but for all of them, and how the solutions to the different tasks can be integrated in this framework. We answer by proposing a new architecture, which we call multinet, in which not only deep image features are shared between tasks, but where tasks can interact in a recurrent manner by encoding the results of their analysis in a common shared representation of the data. In this manner, we show that the performance of individual tasks in standard benchmarks can be improved first by sharing features between them and then, more significantly, by integrating their solutions in the common representation.	[Bilen, Hakan; Vedaldi, Andrea] Univ Oxford, Visual Geometry Grp, Oxford, England	University of Oxford	Bilen, H (corresponding author), Univ Oxford, Visual Geometry Grp, Oxford, England.	hbilen@robots.ox.ac.uk; vedaldi@robots.ox.ac.uk	Bilen, Hakan/ACY-3128-2022; Bilen, Hakan/AAG-3202-2022; Bilen, Hakan/H-9130-2016	Bilen, Hakan/0000-0002-6947-6918	ERC Starting Grant Integrated and Detailed Image Understanding [EP/L024683/1]	ERC Starting Grant Integrated and Detailed Image Understanding	This work acknowledges the support of the ERC Starting Grant Integrated and Detailed Image Understanding (EP/L024683/1).	Baxter J, 2000, J ARTIF INTELL RES, V12, P149, DOI 10.1613/jair.731; Belagiannis V., 2016, ARXIV160502914; Breiman L, 1997, J ROY STAT SOC B MET, V59, P3, DOI 10.1111/1467-9868.00054; Carreira J, 2016, PROC CVPR IEEE, P4733, DOI 10.1109/CVPR.2016.512; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Chatfield K, 2011, PROCEEDINGS OF THE BRITISH MACHINE VISION CONFERENCE 2011, DOI 10.5244/C.25.76; Chen XJ, 2014, PROC CVPR IEEE, P1979, DOI 10.1109/CVPR.2014.254; Dai Jifeng, 2016, ADV NEURAL INFORM PR, P379, DOI DOI 10.1016/J.JPOWSOUR.2007.02.075; Donahue J, 2013, P 31 INT C MACH LEAR; Everingham M, 2010, INT J COMPUT VISION, V88, P303, DOI 10.1007/s11263-009-0275-4; Everingham Mark, 2010, IJCV; Girshick R., 2015, ICCV; Graves A, 2013, INT CONF ACOUST SPEE, P6645, DOI 10.1109/ICASSP.2013.6638947; Graves A, 2009, IEEE T PATTERN ANAL, V31, P855, DOI 10.1109/TPAMI.2008.137; He KM, 2014, LECT NOTES COMPUT SC, V8691, P346, DOI 10.1007/978-3-319-10578-9_23; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Mikolov T., 2012, GOOGLE; Mitchell T.M., 1993, ADV NEURAL INFORM PR, P287; Najibi Mahyar, 2016, CVPR; Pinheiro P. H., 2013, ARXIV13062795; Ranzato Marc'Aurelio., 2007, PROC CVPR IEEE, P1, DOI [10.1109/CVPR.2007.383157, DOI 10.1109/CVPR.2007.383157]; RUMELHART DE, 1986, NATURE, V323, P533, DOI 10.1038/323533a0; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Sutskever I., 2014, ARXIV14093215, DOI DOI 10.1007/S10107-014-0839-0; Thrun S, 1998, LEARNING TO LEARN, P181; van de Sande Koen E. A., 2011, ICCV; Vedaldi A, 2015, MM'15: PROCEEDINGS OF THE 2015 ACM MULTIMEDIA CONFERENCE, P689, DOI 10.1145/2733373.2807412; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53; Zhang TZ, 2013, INT J COMPUT VISION, V101, P367, DOI 10.1007/s11263-012-0582-z; Zhang ZP, 2014, LECT NOTES COMPUT SC, V8694, P94, DOI 10.1007/978-3-319-10599-4_7	32	29	29	2	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973703029
C	Lam, RR; Willcox, KE; Wolpert, DH		Lee, DD; Sugiyama, M; Luxburg, UV; Guyon, I; Garnett, R		Lam, Remi R.; Willcox, Karen E.; Wolpert, David H.			Bayesian Optimization with a Finite Budget: An Approximate Dynamic Programming Approach	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 29 (NIPS 2016)	Advances in Neural Information Processing Systems		English	Proceedings Paper	30th Conference on Neural Information Processing Systems (NIPS)	2016	Barcelona, SPAIN				GLOBAL OPTIMIZATION	We consider the problem of optimizing an expensive objective function when a finite budget of total evaluations is prescribed. In that context, the optimal solution strategy for Bayesian optimization can be formulated as a dynamic programming instance. This results in a complex problem with uncountable, dimension-increasing state space and an uncountable control space. We show how to approximate the solution of this dynamic programming problem using rollout, and propose rollout heuristics specifically designed for the Bayesian optimization setting. We present numerical experiments showing that the resulting algorithm for optimization with a finite budget outperforms several popular Bayesian optimization algorithms.	[Lam, Remi R.; Willcox, Karen E.] MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA; [Wolpert, David H.] Santa Fe Inst, Santa Fe, NM 87501 USA	Massachusetts Institute of Technology (MIT); The Santa Fe Institute	Lam, RR (corresponding author), MIT, 77 Massachusetts Ave, Cambridge, MA 02139 USA.	rlam@mit.edu; kwillcox@mit.edu; dhw@santafe.edu	Willcox, Karen E/AAH-4519-2021		AFOSR MURI [FA9550-15-1-0038]	AFOSR MURI(United States Department of DefenseAir Force Office of Scientific Research (AFOSR)MURI)	This work was supported in part by the AFOSR MURI on multi-information sources of multi-physics systems under Award Number FA9550-15-1-0038, program manager Dr. Jean-Luc Cambier.	Bertsekas D. P., 1995, DYNAMIC PROGRAMMING, V1; Brochu E., 2010, TUTORIAL BAYESIAN OP; Cashore J. M., WORKING PAPER; Ginsburg DH, 2010, COMPET POLICY INT, V6, P89; Gonzalez-Hernandez Jessica, 2016, J Pediatr Surg, V51, P790, DOI 10.1016/j.jpedsurg.2016.02.024; Hennig P, 2012, J MACH LEARN RES, V13, P1809; Hernandez-Lobato JM., 2014, P ADV NEUR INF PROC, V27, P918; Jones DR, 1998, J GLOBAL OPTIM, V13, P455, DOI 10.1023/A:1008306431147; Ling CK, 2016, AAAI CONF ARTIF INTE, P1860; Lizotte DJ, 2008, PRACTICAL BAYESIAN O; Marchant R., 2015, SEQUENTIAL BAYESIAN; Osborne M, 2009, LEARNING INTELLIGENT, P1; Powell Warren B., 2011, APPROXIMATE DYNAMIC, V842; Seeger Matthias, 2004, Int J Neural Syst, V14, P69, DOI 10.1142/S0129065704001899; Snoek J, 2012, ADV NEURAL INF PROCE, V25, P2951; Srinivas Niranjan, 2010, P 27 INT C MACHINE L, P1015, DOI DOI 10.1109/TIT.2011.2182033; Villemonteix J, 2009, J GLOBAL OPTIM, V44, P509, DOI 10.1007/s10898-008-9354-2	19	29	29	1	2	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2016	29													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM0PG					2022-12-19	WOS:000458973701045
C	Zhang, SX; Choromanska, A; LeCun, Y		Cortes, C; Lawrence, ND; Lee, DD; Sugiyama, M; Garnett, R		Zhang, Sixin; Choromanska, Anna; LeCun, Yann			Deep learning with Elastic Averaging SGD	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 28 (NIPS 2015)	Advances in Neural Information Processing Systems		English	Proceedings Paper	29th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 07-12, 2015	Montreal, CANADA					We study the problem of stochastic optimization for deep learning in the parallel computing environment under communication constraints. A new algorithm is proposed in this setting where the communication and coordination of work among concurrent processes (local workers), is based on an elastic force which links the parameters they compute with a center variable stored by the parameter server (master). The algorithm enables the local workers to perform more exploration, i.e. the algorithm allows the local variables to fluctuate further from the center variable by reducing the amount of communication between local workers and the master. We empirically demonstrate that in the deep learning setting, due to the existence of many local optima, allowing more exploration can lead to the improved performance. We propose synchronous and asynchronous variants of the new algorithm. We provide the stability analysis of the asynchronous variant in the round-robin scheme and compare it with the more common parallelized method ADMM. We show that the stability of EASGD is guaranteed when a simple stability condition is satisfied, which is not the case for ADMM. We additionally propose the momentum-based version of our algorithm that can be applied in both synchronous and asynchronous settings. Asynchronous variant of the algorithm is applied to train convolutional neural networks for image classification on the CIFAR and ImageNet datasets. Experiments demonstrate that the new algorithm accelerates the training of deep architectures compared to DOWNPOUR and other common baseline approaches and furthermore is very communication efficient.	[Zhang, Sixin; Choromanska, Anna] NYU, Courant Inst, New York, NY 10012 USA; [LeCun, Yann] NYU, Ctr Data Sci, New York, NY USA; [LeCun, Yann] Facebook AI Res, New York, NY USA	New York University; New York University; Facebook Inc	Zhang, SX (corresponding author), NYU, Courant Inst, New York, NY 10012 USA.	zsx@cims.nyu.edu; achoroma@cims.nyu.edu; yann@cims.nyu.edu						Agarwal A, 2011, NIPS; [Anonymous], 2001, STUDIES COMPUTATIONA; [Anonymous], 2013, ICML; Azadi Samaneh, 2014, ICML; Bertsekas D. P., 1997, PARALLEL DISTRIBUTED; Bottou L., 1998, ONLINE ALGORITHMS ST; Boyd Stephen, 2010, Foundations and Trends in Machine Learning, V3, P1, DOI 10.1561/2200000016; Cesa-Bianchi N, 2004, IEEE T INFORM THEORY, V50, P2050, DOI 10.1109/TIT.2004.833339; Choromanska A., 2015, AISTATS; Dean J., 2012, ADV NEURAL INFORM PR, V25; Hestenes M., 1975, OPTIMIZATION THEORY; Ho Q., 2013, NIPS; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Langford J., 2009, NIPS; Nesterov Y, 2005, MATH PROGRAM, V103, P127, DOI 10.1007/s10107-004-0552-5; Nesterov Y., 2004, INTRO LECT CONVEX OP, V87; Niu F., 2011, NIPS 11; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Ouyang Hua, 2013, P 30 INT C MACH LEAR, P80; Paine T., 2013, ARXIV; POLYAK BT, 1992, SIAM J CONTROL OPTIM, V30, P838, DOI 10.1137/0330046; Ranzato M., 2013, ARXIV; Seide F, 2014, INT CONF ACOUST SPEE; Sermanet P., 2013, ARXIV; Shamir O., 2014, NIPS; Sutskever I., 2013, P 30 INT C MACH LEAR; Zhang R., 2014, ICML; Zinkevich M., 2010, NIPS, V4, P4	31	29	29	2	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2015	28													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL4WJ					2022-12-19	WOS:000450913101021
C	Pickup, LC; Roberts, SJ; Zisserman, A		Thrun, S; Saul, K; Scholkopf, B		Pickup, LC; Roberts, SJ; Zisserman, A			A sampled texture prior for image super-resolution	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	Advances in Neural Information Processing Systems		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 08, 2003	CANADA				RESOLUTION	Super-resolution aims to produce a high-resolution image from a set of one or more low-resolution images by recovering or inventing plausible high-frequency image content. Typical approaches try to reconstruct a high-resolution image using the sub-pixel displacements of several low-resolution images, usually regularized by a generic smoothness prior over the high-resolution image space. Other methods use training data to learn low-to-high-resolution matches, and have been highly successful even in the single-input-image case. Here we present a domain-specific image prior in the form of a p.d.f. based upon sampled images, and show that for certain types of super-resolution problems, this sample-based prior gives a significant improvement over other common multiple-image super-resolution techniques.	Univ Oxford, Dept Engn Sci, Robot Res Grp, Oxford OX1 3PJ, England	University of Oxford	Pickup, LC (corresponding author), Univ Oxford, Dept Engn Sci, Robot Res Grp, Parks Rd, Oxford OX1 3PJ, England.	elle@robots.ox.ac.uk; sjrob@robots.ox.ac.uk; az@robots.ox.ac.uk	xuan, bo/H-4351-2011					Baker S, 2002, IEEE T PATTERN ANAL, V24, P1167, DOI 10.1109/TPAMI.2002.1033210; Black MJ, 1998, IEEE T IMAGE PROCESS, V7, P421, DOI 10.1109/83.661192; Capel D., 2001, THESIS U OXFORD; Cheeseman P, 1996, FUND THEOR, V62, P293; Efros A. A., 1999, Proceedings of the Seventh IEEE International Conference on Computer Vision, P1033, DOI 10.1109/ICCV.1999.790383; FITZGIBBON A, 2003, P INT C COMP VIS OCT; Freeman WT, 2002, IEEE COMPUT GRAPH, V22, P56, DOI 10.1109/38.988747; IRANI M, 1991, CVGIP-GRAPH MODEL IM, V53, P231, DOI 10.1016/1049-9652(91)90045-L; Schultz RR, 1996, IEEE T IMAGE PROCESS, V5, P996, DOI 10.1109/83.503915; STORKEY AJ, 2003, ADV NEURAL INFORMATI, V15, P1295; Tipping M. E., 2003, ADV NEURAL INFORMATI, V15, P1279	12	29	30	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						1587	1594						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500197
C	Lee, TW; Lewicki, MS; Sejnowski, T		Kearns, MS; Solla, SA; Cohn, DA		Lee, TW; Lewicki, MS; Sejnowski, T			Unsupervised classification with non-Gaussian mixture models using ICA	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				SPARSE	We present an unsupervised classification algorithm based on an ICA mixture model. The ICA mixture model assumes tl lat the observed data can be categorized into several mutually exclusive data classes in which the components in each class are generated by a linear mixture of independent sources. The algorithm finds the independent sources, the mixing matrix for each class and also computes the class membership probability for each data point. This approach extends the Gaussian mixture model so that the classes can have non-Gaussian structure. We demonstrate that this method can learn efficient codes to represent images of natural scenes and text. The learned classes of basis functions yield a better approximation of the underlying distributions of the data, and thus can provide greater coding efficiency. We believe that this method is well suited to modeling structure in high-dimensional data and has many potential applications.	Salk Inst Biol Studies, Howard Hughes Med Inst, Comp Neurobiol Lab, La Jolla, CA 92037 USA	Howard Hughes Medical Institute; Salk Institute	Lee, TW (corresponding author), Salk Inst Biol Studies, Howard Hughes Med Inst, Comp Neurobiol Lab, 10010 N Torrey Pines Rd, La Jolla, CA 92037 USA.	tewon@salk.edu; lewicki@salk.edu; terry@salk.edu	Sejnowski, Terrence/AAV-5558-2021					ATTIAS H, 1999, IN PRESS NEURAL COMP; BELL AJ, 1995, NEURAL COMPUT, V7, P1129, DOI 10.1162/neco.1995.7.6.1129; Bell AJ, 1997, VISION RES, V37, P3327, DOI 10.1016/S0042-6989(97)00121-1; Belouchrani A, 1997, IEEE T SIGNAL PROCES, V45, P434, DOI 10.1109/78.554307; Bishop C.M., 1994, NCRG4288; Duda R.O., 1973, J ROYAL STAT SOC SER; GHAHRAMANI Z, 1994, PROCEEDINGS OF THE 1993 CONNECTIONIST MODELS SUMMER SCHOOL, P316; Girolami M, 1998, NEURAL COMPUT, V10, P2103, DOI 10.1162/089976698300016981; Lee TW, 1999, NEURAL COMPUT, V11, P417, DOI 10.1162/089976699300016719; LEE TW, 1999, IN PRESS INT WORKSH; LEE TW, 1999, IN PRESS INT J MATH; Lewicki MS, 1998, ADV NEUR IN, V10, P556; Lewicki MS, 1998, ADV NEUR IN, V10, P815; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; STUTZ J, 1994, MAXIMUM ENTROPY BAYE	15	29	30	0	1	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						508	514						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700072
C	Rao, RPN; Ruderman, DL		Kearns, MS; Solla, SA; Cohn, DA		Rao, RPN; Ruderman, DL			Learning Lie groups for invariant visual perception	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	Advances in Neural Information Processing Systems		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				NATURAL IMAGES; MODEL; POSITION; CODE	One of the most important problems in visual perception is that of visual invariance: how are objects perceived to be the same despite undergoing transformations such as translations, rotations or scaling? In this paper, we describe a Bayesian method for learning invariances based on Lie group theory. We show that previous approaches based on first-order Taylor series expansions of inputs can be regarded as special cases of the Lie group approach, the latter being capable of handling in principle arbitrarily large transformations. Using a matrix-exponential based generative model of images, we derive an unsupervised algorithm for learning Lie group operators from input data containing infinitesimal transformations. The on-line unsupervised learning algorithm maximizes the posterior probability of generating the training data. We provide experimental results suggesting that the proposed method can learn Lie group operators for handling reasonably large 1-D translations and 2-D rotations.	Sloan Ctr Theoret Neurobiol, Salk Inst, La Jolla, CA 92037 USA	Salk Institute	Rao, RPN (corresponding author), Sloan Ctr Theoret Neurobiol, Salk Inst, La Jolla, CA 92037 USA.	rao@salk.edu; ruderman@salk.edu		Rao, Rajesh P. N./0000-0003-0682-8952				BLACK MJ, 1996, P EUR C COMP VIS, P329; DODWELL PC, 1983, PERCEPT PSYCHOPHYS, V34, P1, DOI 10.3758/BF03205890; Felleman DJ, 1991, CEREB CORTEX, V1, P1, DOI 10.1093/cercor/1.1.1; Foldiak P, 1991, NEURAL COMPUT, V3, P194, DOI 10.1162/neco.1991.3.2.194; FUKUSHIMA K, 1980, BIOL CYBERN, V36, P193, DOI 10.1007/BF00344251; Gibson JJ., 1966, SENSES CONSIDERED PE; LeCun Y, 1989, NEURAL COMPUT, V1, P541, DOI 10.1162/neco.1989.1.4.541; Marks II RJ, 1991, INTRO SHANNON SAMPLI; NORDBERG K, 1994, THESIS LINKOPING U; OLSHAUSEN BA, 1995, J COMPUT NEUROSCI, V2, P45, DOI 10.1007/BF00962707; Olshausen BA, 1996, NATURE, V381, P607, DOI 10.1038/381607a0; Pitts Walter, 1947, BULL MATH BIOPHYS, V9, P127, DOI 10.1007/BF02478291; Rao RPN, 1998, NETWORK-COMP NEURAL, V9, P219, DOI 10.1088/0954-898X/9/2/005; Rao RPN, 1997, NEURAL COMPUT, V9, P721, DOI 10.1162/neco.1997.9.4.721; Simard P., 1993, ADV NEURAL INFORMATI, V5, P50; VANGOOL L, 1995, IMAGE VISION COMPUT, V13, P259, DOI 10.1016/0262-8856(95)99715-D	16	29	30	0	0	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						810	816						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700114
C	Ueda, N; Nakano, R; Ghahramani, Z; Hinton, GE		Kearns, MS; Solla, SA; Cohn, DA		Ueda, N; Nakano, R; Ghahramani, Z; Hinton, GE			SMEM algorithm for mixture models	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 11	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	12th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 30-DEC 05, 1998	DENVER, CO				EM ALGORITHM	We present a split and merge EM (SMEM) algorithm to overcome the local maximum problem in parameter estimation of finite mixture models. In the case of mixture models, non-global maxima often involve having too many components of a mixture model in one part of the space and too few in another, widely separated part of the space. To escape from such configurations we repeatedly perform simultaneous split and merge operations using a new criterion for efficiently selecting the split and merge candidates. We apply the proposed algorithm to the training of Gaussian mixtures and mixtures of factor analyzers using synthetic and real data and show the effectiveness of using the split and merge operations to improve the likelihood of both the training data and of held-out test data.	NTT, Commun Sci Labs, Seika, Kyoto 6190237, Japan	Nippon Telegraph & Telephone Corporation	Ueda, N (corresponding author), NTT, Commun Sci Labs, Seika, Kyoto 6190237, Japan.							DEMPSTER AP, 1977, J ROY STAT SOC B MET, V39, P1, DOI 10.1111/j.2517-6161.1977.tb01600.x; Ghahramani Z, 1997, CRGTR961; Hinton GE, 1997, IEEE T NEURAL NETWOR, V8, P65, DOI 10.1109/72.554192; ISHII K, 1989, SYSTEMS COMPUTERS JA, V21, P669; MACLACHLAN GJ, 1988, MIXTURE MODELS INFER; TIPPING ME, 1997, NCRG973 AST U; Ueda N, 1998, NEURAL NETWORKS, V11, P271, DOI 10.1016/S0893-6080(97)00133-0; UEDA N, 1994, NEURAL NETWORKS, V7, P1211, DOI 10.1016/0893-6080(94)90003-5	8	29	29	0	1	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-11245-0	ADV NEUR IN			1999	11						599	605						7	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP76H					2022-12-19	WOS:000086093700085
C	Schwenk, H; Bengio, Y		Jordan, MI; Kearns, MJ; Solla, SA		Schwenk, H; Bengio, Y			Training methods for Adaptive Boosting of neural networks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 10	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	11th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 01-06, 1997	DENVER, CO					"Boosting" is a general method for improving the performance of any learning algorithm that consistently generates classifiers which need to perform only slightly better than random guessing. A recently proposed and very promising boosting algorithm is AdaBoost [5]. It has been applied with great success to several benchmark machine learning problems using rather simple learning algorithms [4], and decision trees [1, 2, 6]. In this paper we use AdaBoost to improve the performances of neural networks. We compare training methods based on sampling the training set and weighting the cost function. Our system achieves about 1.4% error on a data base of online handwritten digits from more than 200 writers. Adaptive boosting of a multi-layer network achieved 1.5% error on the UCI Letters and 8.1% error on the UCI satellite data set.	Univ Montreal, Dept IRO, Montreal, PQ H3C 3J7, Canada	Universite de Montreal	Schwenk, H (corresponding author), Univ Montreal, Dept IRO, 2920 Chemin de la Tour, Montreal, PQ H3C 3J7, Canada.	schwenk@iro.umontreal.ca; bengioy@iro.umontreal.ca							0	29	30	1	5	MIT PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-10076-2	ADV NEUR IN			1998	10						647	653						7	Behavioral Sciences; Computer Science, Artificial Intelligence; Computer Science, Interdisciplinary Applications; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Behavioral Sciences; Computer Science; Neurosciences & Neurology	BL32J					2022-12-19	WOS:000075130700092
C	Frey, BJ; Hinton, GE; Dayan, P		Touretzky, DS; Mozer, MC; Hasselmo, ME		Frey, BJ; Hinton, GE; Dayan, P			Does the wake-sleep algorithm produce good density estimators?	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						UNIV TORONTO,DEPT COMP SCI,TORONTO,ON M5S 1A4,CANADA	University of Toronto									0	29	29	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						661	667						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00094
C	Kearns, M		Touretzky, DS; Mozer, MC; Hasselmo, ME		Kearns, M			A bound on the error of cross validation using the approximation and estimation rates, with consequences for the training-test split	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO																0	29	29	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						183	189						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00026
C	Ormoneit, D; Tresp, V		Touretzky, DS; Mozer, MC; Hasselmo, ME		Ormoneit, D; Tresp, V			Improved gaussian mixture density estimates using Bayesian penalty terms and network averaging	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 8: PROCEEDINGS OF THE 1995 CONFERENCE	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	9th Annual Conference on Neural Information Processing Systems (NIPS)	NOV 27-30, 1995	DENVER, CO						TECH UNIV MUNICH,INST INFORMAT H2,D-80290 MUNICH,GERMANY	Technical University of Munich									0	29	30	0	0	M I T PRESS	CAMBRIDGE	55 HAYWARD ST, CAMBRIDGE, MA 02142	1049-5258		0-262-20107-0	ADV NEUR IN			1996	8						542	548						3	Computer Science, Information Systems; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BG45M					2022-12-19	WOS:A1996BG45M00077
C	Bera, SK; Chakrabarty, D; Flores, NJ; Negahbani, M		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Bera, Suman K.; Chakrabarty, Deeparnab; Flores, Nicolas J.; Negahbani, Maryam			Fair Algorithms for Clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				APPROXIMATION ALGORITHM; FACILITY LOCATION	We study the problem of finding low-cost fair clusterings in data where each data point may belong to many protected groups. Our work significantly generalizes the seminal work of Chierichetti et al. (NIPS 2017) as follows. We allow the user to specify the parameters that define fair representation. More precisely, these parameters define the maximum over- and minimum under-representation of any group in any cluster. Our clustering algorithm works on any l(p)-norm objective (e.g. k-means, k-median, and k-center). Indeed, our algorithm transforms any vanilla clustering solution into a fair one incurring only a slight loss in quality. Our algorithm also allows individuals to lie in multiple protected groups. In other words, we do not need the protected groups to partition the data and we can maintain fairness across different groups simultaneously. Our experiments show that on established data sets, our algorithm performs much better in practice than what our theoretical results suggest.	[Bera, Suman K.] UC Santa Cruz, Santa Cruz, CA 95064 USA; [Chakrabarty, Deeparnab; Flores, Nicolas J.; Negahbani, Maryam] Dartmouth Coll, Hanover, NH 03755 USA	University of California System; University of California Santa Cruz; Dartmouth College	Bera, SK (corresponding author), UC Santa Cruz, Santa Cruz, CA 95064 USA.	sbera@ucsc.edu; deeparnab@dartmouth.edu; nicolasflores.19@dartmouth.edu; maryam@cs.dartmouth.edu						Aggarwal CC, 2014, CH CRC DATA MIN KNOW, P1; Aguinis Herman, 2005, APPL PSYCHOL HUMAN R; Ahmadian S, 2019, KDD'19: PROCEEDINGS OF THE 25TH ACM SIGKDD INTERNATIONAL CONFERENCCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P267, DOI 10.1145/3292500.3330987; Ahmadian Sara, 2017, ANN IEE S FDN COMP S; Angwin J., 2016, PROPUBLICA, P254; Arthur D., 2007, P 18 ANN ACM SIAM S; Arya V, 2004, SIAM J COMPUT, V33, P544, DOI 10.1137/S0097539702416402; Backurs A, 2019, PR MACH LEARN RES, V97; Bercea Ioana O., 2018, ABS181110319 CORR; Byrka Jaroslaw, 2014, ANN ACM SIAM S DISCR; Calders T, 2010, DATA MIN KNOWL DISC, V21, P277, DOI 10.1007/s10618-010-0190-x; Celis LE, 2018, PROCEEDINGS OF THE TWENTY-SEVENTH INTERNATIONAL JOINT CONFERENCE ON ARTIFICIAL INTELLIGENCE, P144; Celis L Elisa, 2018, 45 INT C AUTOMATA LA, V107; Celis L. Elisa, 2019, PC FAIRN ACC TRANSP; Charikar M, 2002, J COMPUT SYST SCI, V65, P129, DOI 10.1006/jcss.2002.1882; Charikar Moses, 1999, ANN IEEE S FDN COMP; Chen Xingyu, 2019, P 36 P INT C MACH LE; CHIERICHETTI F, 2017, NIPS, P5029; Chierichetti F, 2019, PR MACH LEARN RES, V89; Chouldechova A, 2017, BIG DATA, V5, P153, DOI 10.1089/big.2016.0047; Corbett-Davies S, 2017, KDD'17: PROCEEDINGS OF THE 23RD ACM SIGKDD INTERNATIONAL CONFERENCE ON KNOWLEDGE DISCOVERY AND DATA MINING, P797, DOI 10.1145/3097983.3098095; Dua D., 2017, UCI MACHINE LEARNING; Dwork C., 2012, P 3 INN THEOR COMP S, P214; Friedler Sorelle A., 2016, CORR; GONZALEZ TF, 1985, THEOR COMPUT SCI, V38, P293, DOI 10.1016/0304-3975(85)90224-5; HOCHBAUM DS, 1985, MATH OPER RES, V10, P180, DOI 10.1287/moor.10.2.180; HSU WL, 1979, DISCRETE APPL MATH, V1, P209, DOI 10.1016/0166-218X(79)90044-1; IBM, 2019, IBM IL CPL 12 9; Jain K, 2001, J ACM, V48, P274, DOI 10.1145/375827.375845; Joseph Matthew, 2016, NIPS, P325; Kamishima Toshihiro, 2012, Machine Learning and Knowledge Discovery in Databases. Proceedings of the European Conference (ECML PKDD 2012), P35, DOI 10.1007/978-3-642-33486-3_3; Khandani Amir E, 2010, J BANKING FINANCE, V34; Kiraly T, 2012, COMBINATORICA, V32, P703, DOI 10.1007/s00493-012-2760-6; Kleinberg J.M., 2017, 8 INNOVATIONS THEORE, DOI [10.4230/LIPIcs.ITCS.2017.43, DOI 10.4230/LIPICS.ITCS.2017.43]; Kleindessner Matthaus, 2019, P 36 P INT C MACH LE; Kohavi Ron, 1996, ANN SIGKDD INT C KNO; Li S, 2016, SIAM J COMPUT, V45, P530, DOI 10.1137/130938645; Luong Binh Thanh, 2011, P 17 ACM SIGKDD INT, P502; Malhotra Rashmi, 2003, OMEGA, V31; Meek C, 2002, J MACH LEARN RES, V2, P397, DOI 10.1162/153244302760200678; Moro Paulo Rita Sergio, 2014, DECISION SUPPORT SYS; Pedregosa F., 2011, J MACH LEARN RES; Perlich Claudia, 2014, MACHINE LEARNING, V95; Rosner Clemens, 2018, P 45 INT C AUT LANG; Schmidt M., 2018, ABS181210854 CORR; SHMOYS DB, 1993, MATH PROGRAM, V62, P461, DOI 10.1007/BF01585178; Strack B, 2014, BIOMED RES INT, V2014, DOI 10.1155/2014/781670; The U. S. Equal Employment Opportunity Commission (EEOC), 1979, UN GUID EMPL SEL PRO; Yang K., 2017, P 29 INT C SCI STAT, P22; Yeh C.C., 2009, EXPERT SYSTEMS APPL; Zemel R., 2013, P INT C MACH LEARN, P325	54	28	28	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305001
C	Hudson, DA; Manning, CD		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Hudson, Drew A.; Manning, Christopher D.			Learning by Abstraction: The Neural State Machine	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				LANGUAGE; NETWORKS	We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.	[Hudson, Drew A.; Manning, Christopher D.] Stanford Univ, 353 Serra Mall, Stanford, CA 94305 USA	Stanford University	Hudson, DA (corresponding author), Stanford Univ, 353 Serra Mall, Stanford, CA 94305 USA.	dorarad@cs.stanford.edu; manning@cs.stanford.edu	Manning, Christopher/AAM-9535-2020	Manning, Christopher/0000-0001-6155-649X				Agrawal A, 2018, PROC CVPR IEEE, P4971, DOI 10.1109/CVPR.2018.00522; Agrawal Aishwarya, 2016, ARXIV160607356; Aleksander I., 1994, ICANN '94. Proceedings of the International Conference on Artificial Neural Networks, P212; Anderson P., 2017, ABS170707998 CORR; Andreas J, 2016, PROC CVPR IEEE, P39, DOI 10.1109/CVPR.2016.12; Andreas Jacob, 2019, ICLR; [Anonymous], 2016, ARXIV PREPRINT ARXIV; [Anonymous], 2017, IEEE I CONF COMP VIS, DOI DOI 10.1109/ICCV.2017.322; [Anonymous], 2017, CVPR; [Anonymous], P IEEE C COMP VIS PA; Antol S, 2015, IEEE I CONF COMP VIS, P2425, DOI 10.1109/ICCV.2015.279; Barsalou LW, 2003, TRENDS COGN SCI, V7, P84, DOI 10.1016/S1364-6613(02)00029-3; Battaglia Peter W, 2018, ARXIV 180601261; Bealer G., 1998, PHILOS ISSUES, V9, P261; Bengio Yoshua, 2017, CORR; Boroditsky L, 2011, SCI AM, V304, P62, DOI 10.1038/scientificamerican0211-62; Bottou L, 2014, MACH LEARN, V94, P133, DOI 10.1007/s10994-013-5335-x; Cadene Remi, 2019, ARXIV190209487; Chen Tianshui, 2019, ARXIV190303326; Choi E., 2018, ARXIV180402341; Chomsky N, 2017, PSYCHON B REV, V24, P200, DOI 10.3758/s13423-016-1078-6; Chomsky  Noam, 2014, ASPECTS THEORY SYNTA, V11; Das A, 2017, COMPUT VIS IMAGE UND, V163, P90, DOI 10.1016/j.cviu.2017.10.001; Farhadi A, 2009, PROC CVPR IEEE, P1778, DOI 10.1109/CVPRW.2009.5206772; Fodor J. A., 1975, LANGUAGE THOUGHT, V5; Forcada ML, 2001, LECT NOTES ARTIF INT, V2036, P480; Garnelo M., 2016, P ADV NEUR INF PROC; Gershman S.J., 2017, BEHAV BRAIN SCI, V40; Goodfellow I. J., 2014, ARXIV14126572; Goyal Y, 2017, PROC CVPR IEEE, P6325, DOI 10.1109/CVPR.2017.670; Graves A, 2014, NEURAL TURING MACHIN; Graves A, 2016, NATURE, V538, P471, DOI 10.1038/nature20101; GREENO JG, 1989, AM PSYCHOL, V44, P134, DOI 10.1037/0003-066X.44.2.134; Gupta A, 2017, ARXIV PREPRINT ARXIV; Harman G., 1986, CHANGE VIEW; Higgins  Irina, 2017, ARXIV170703389; Hopcroft J.E., 2008, INTRO AUTOMATA THEOR; Hu R, 2018, PROC CVPR IEEE, P4233, DOI 10.1109/CVPR.2018.00445; Hu RH, 2017, IEEE I CONF COMP VIS, P804, DOI 10.1109/ICCV.2017.93; Hudson Drew A, 2019, C COMP VIS PATT REC; Hudson Drew A, 2018, INT C LEARN REPR ICL; Jia R., 2017, ADVERSARIAL EXAMPLES; Johnson J, 2018, PROC CVPR IEEE, P1219, DOI 10.1109/CVPR.2018.00133; Johnson J, 2017, PROC CVPR IEEE, P1988, DOI 10.1109/CVPR.2017.215; Johnson J, 2015, PROC CVPR IEEE, P3668, DOI 10.1109/CVPR.2015.7298990; Kaiming He, 2016, 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), P770, DOI 10.1109/CVPR.2016.90; Kim J.-H., 2018, ADV NEURAL INFORM PR, P1564; Kingma D.P, P 3 INT C LEARNING R; Kipf TN, 2016, P INT C LEARN REPR; Krishna R, 2017, INT J COMPUT VISION, V123, P32, DOI 10.1007/s11263-016-0981-7; Lake B. M., 2017, ARXIV171100350; Li Linjie, 2019, ARXIV190312314; Li YB, 2017, INT CON DISTR COMP S, P1261, DOI 10.1109/ICDCS.2017.54; Li YL, 2018, CONTROL ENG SER BIRK, P335, DOI 10.1007/978-3-319-64246-8_9; Lin TY, 2017, PROC CVPR IEEE, P936, DOI 10.1109/CVPR.2017.106; Lu JS, 2018, PROC CVPR IEEE, P7219, DOI 10.1109/CVPR.2018.00754; Lu JS, 2017, PROC CVPR IEEE, P3242, DOI 10.1109/CVPR.2017.345; Malinowski M, 2018, LECT NOTES COMPUT SC, V11210, P3, DOI 10.1007/978-3-030-01231-1_1; Mao J. Y., 2019, ARXIV190412584; Mascharka D, 2018, PROC CVPR IEEE, P4942, DOI 10.1109/CVPR.2018.00519; Miller A., 2016, ARXIV160603126, P1400; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; NAVON D, 1977, COGNITIVE PSYCHOL, V9, P353, DOI 10.1016/0010-0285(77)90012-3; NEWELL A, 1980, COGNITIVE SCI, V4, P135, DOI 10.1016/S0364-0213(80)80015-2; Norcliffe-Brown W., 2018, ADV NEURAL INFORM PR, P8334; Pecher D., 2005, GROUNDING COGNITION; Pennington J., 2014, P 2014 C EMPIRICAL M, P1532; Redmon J, 2017, PROC CVPR IEEE, P6517, DOI 10.1109/CVPR.2017.690; Rogers T. T., 2004, SEMANTIC COGNITION P; Santoro A., 2018, INT C MACH LEARN, P4477; Schneider S., 2011, LANGUAGE THOUGHT NEW; Shrestha Robik, 2019, ARXIV190300366; Smolensky P., 1987, Artificial Intelligence Review, V1, P95, DOI 10.1007/BF00130011; Teney Damien, 2017, GRAPH STRUCTURED REP; Velickovic P., 2017, STAT-US, V1050, P20; Vogel J, 2007, INT J COMPUT VISION, V72, P133, DOI 10.1007/s11263-006-8614-1; Vygotsky L. S., 1964, B ORTON SOC, V14, P97, DOI [10.1007/BF02928399, DOI 10.1007/BF02928399]; Wang Peng, 2017, P IEEE C COMP VIS PA, V4; Weston J., 2014, ARXIV14103916; Wu Q, 2016, PROC CVPR IEEE, P203, DOI 10.1109/CVPR.2016.29; Wu Y., 2016, ADV NEURAL INFORM PR, V1609, DOI 10.48550/arXiv.1609.08144; Xiong CM, 2016, PR MACH LEARN RES, V48; Yang Jianwei, 2018, ARXIV180800191; Yang ZC, 2016, PROC CVPR IEEE, P21, DOI 10.1109/CVPR.2016.10; Yi K., 2018, ADV NEURAL INFORM PR, V31, P1031; Zellers R, 2018, PROC CVPR IEEE, P5831, DOI 10.1109/CVPR.2018.00611	88	28	30	0	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424305085
C	Kolouri, S; Nadjahi, K; Simsekli, U; Badeau, R; Rohde, GK		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Kolouri, Soheil; Nadjahi, Kimia; Simsekli, Umut; Badeau, Roland; Rohde, Gustavo K.			Generalized Sliced Wasserstein Distances	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMAL TRANSPORT	The Wasserstein distance and its variations, e.g., the sliced-Wasserstein (SW) distance, have recently drawn attention from the machine learning community. The SW distance, specifically, was shown to have similar properties to the Wasserstein distance, while being much simpler to compute, and is therefore used in various applications including generative modeling and general supervised/unsupervised learning. In this paper, we first clarify the mathematical connection between the SW distance and the Radon transform. We then utilize the generalized Radon transform to define a new family of distances for probability measures, which we call generalized sliced-Wasserstein (GSW) distances. We further show that, similar to the SW distance, the GSW distance can be extended to a maximum GSW (max-GSW) distance. We then provide the conditions under which GSW and max-GSW distances are indeed proper metrics. Finally, we compare the numerical performance of the proposed distances on the generative modeling task of SW flows and report favorable results.	[Kolouri, Soheil] HRL Labs LLC, Malibu, CA 90265 USA; [Nadjahi, Kimia; Simsekli, Umut; Badeau, Roland] Telecom Paris, Inst Polytech Paris, LTCI, Paris, France; [Simsekli, Umut] Univ Oxford, Dept Stat, Oxford, England; [Rohde, Gustavo K.] Univ Virginia, Charlottesville, VA 22904 USA	HRL Laboratories; IMT - Institut Mines-Telecom; Institut Polytechnique de Paris; University of Oxford; University of Virginia	Kolouri, S (corresponding author), HRL Labs LLC, Malibu, CA 90265 USA.	skolouri@hrl.com; kimia.nadjahi@telecom-paris.fr; umut.simsekli@telecom-paris.fr; roland.badeau@telecom-paris.fr; gustavo@virginia.edu			United States Air Force; DARPA [FA8750-18-C-0103]; French National Research Agency (ANR) [ANR-16-CE23-0014]; industrial chair Machine Learning for Big Data from Telecom ParisTech	United States Air Force(United States Department of Defense); DARPA(United States Department of DefenseDefense Advanced Research Projects Agency (DARPA)); French National Research Agency (ANR)(French National Research Agency (ANR)); industrial chair Machine Learning for Big Data from Telecom ParisTech	This work was partially supported by the United States Air Force and DARPA under Contract No. FA8750-18-C-0103. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the United States Air Force and DARPA. This work is also partly supported by the French National Research Agency (ANR) as a part of the FBIMATRIX project (ANR-16-CE23-0014) and by the industrial chair Machine Learning for Big Data from Telecom ParisTech.	[Anonymous], 2014, ICML; Araya M., 2015, P ADV NEUR INF PROC, P2053; Arjovsky M., 2017, ARXIV170107875; BEYLKIN G, 1984, COMMUN PUR APPL MATH, V37, P579, DOI 10.1002/cpa.3160370503; Bonneel N, 2015, J MATH IMAGING VIS, V51, P22, DOI 10.1007/s10851-014-0506-3; Bonnotte N., 2013, UNIDIMENSIONAL EVOLU; Bousquet O., 2017, TECH REP; BRENIER Y, 1991, COMMUN PUR APPL MATH, V44, P375, DOI 10.1002/cpa.3160440402; Carrick M, 2017, IEEE SARNOFF SYMPOS, P60; Courty N, 2017, IEEE T PATTERN ANAL, V39, P1853, DOI 10.1109/TPAMI.2016.2615921; CUTURI M., 2013, P INT C ADV NEURAL I, V26; Cuturi Marco, 2015, SIAM J IMAGING SCI; Denisyuk AS, 1994, AM MATH SOC T, V162, P19; Deshpande I., 2019, IEEE C COMP VIS PATT; Deshpande I, 2018, PROC CVPR IEEE, P3483, DOI 10.1109/CVPR.2018.00367; EHRENPREIS L, 2003, UNIVERSALITY RADON T; Gulrajani I, 2017, ADV NEURAL INFORM PR, V2017, P5767; Helgason S, 2011, INTEGRAL GEOMETRY AND RADON TRANSFORMS, P1, DOI 10.1007/978-1-4419-6055-9; Homan A, 2017, J GEOM ANAL, V27, P1515, DOI 10.1007/s12220-016-9729-4; Karras T, 2017, ARXIV171010196; Kingma D.P, P 3 INT C LEARNING R; Kitagawa J., 2016, ARXIV160305579; Kolouri S, 2019, INT C LEARN REPR; Kolouri S, 2017, IEEE SIGNAL PROC MAG, V34, P43, DOI 10.1109/MSP.2017.2695801; Kolouri S, 2016, PATTERN RECOGN, V51, P453, DOI 10.1016/j.patcog.2015.09.019; Kolouri Soheil, 2018, IEEE C COMP VIS PATT, P1; Kolouri Soheil, 2016, P IEEE C COMP VIS PA, P4876; Kuchment P, 2006, PROC SYM AP, V63, P67; Levy B, 2015, ESAIM-MATH MODEL NUM, V49, P1693, DOI 10.1051/m2an/2015055; Liu Ziwei, 2015, P INT C COMP VIS ICC; Liutkus Antoine, 2019, INT C MACH LEARN; Montavon G., 2016, ADV NEURAL INF PROCE, P3718; Nadjahi K., 2019, ADV NEURAL INFORM PR; Natterer Frank, 1986, MATH COMPUTERIZED TO, V32, P11; Oberman Adam M, 2015, EFFICIENT LINEAR PRO; Paty Francois-Pierre, 2019, INT C MACH LEARN; Peyre G., 2018, ARXIV180300567; Radon J., 1986, IEEE Transactions on Medical Imaging, VMI-5, P170, DOI 10.1109/TMI.1986.4307775; Rouviere Francois, 2015, NONLINEAR RADON FOUR; ROWLAND M, 2019, ARTIF INTELL, V89, P186; Schmitz MA, 2018, SIAM J IMAGING SCI, V11, P643, DOI 10.1137/17M1140431; Schmitzer B, 2016, J MATH IMAGING VIS, V56, P238, DOI 10.1007/s10851-016-0653-9; Solomon J, 2015, ACM T GRAPHIC, V34, DOI 10.1145/2766963; Tolstikhin Ilya, 2018, ICLR 2018; Uhlmann Gunther, 2003, INSIDE OUT INVERSE P, V47; Villani C, 2009, GRUNDLEHR MATH WISS, V338, P5; Wang W, 2013, INT J COMPUT VISION, V101, P254, DOI 10.1007/s11263-012-0566-z; Wang W, 2011, PATTERN RECOGN LETT, V32, P2128, DOI 10.1016/j.patrec.2011.08.010; [No title captured], DOI 10.1007/BF01674015	49	28	28	1	4	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424300024
C	Peng, XB; Chang, M; Zhang, G; Abbeel, P; Levine, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Peng, Xue Bin; Chang, Michael; Zhang, Grace; Abbeel, Pieter; Levine, Sergey			MCP: Learning Composable Hierarchical Control with Multiplicative Compositional Policies	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Humans are able to perform a myriad of sophisticated tasks by drawing upon skills acquired through prior experience. For autonomous agents to have this capability, they must be able to extract reusable skills from past experience that can be recombined in new ways for subsequent tasks. Furthermore, when controlling complex high-dimensional morphologies, such as humanoid bodies, tasks often require coordination of multiple skills simultaneously. Learning discrete primitives for every combination of skills quickly becomes prohibitive. Composable primitives that can be recombined to create a large variety of behaviors can be more suitable for modeling this combinatorial explosion. In this work, we propose multiplicative compositional policies (MCP), a method for learning reusable motor skills that can be composed to produce a range of complex behaviors. Our method factorizes an agent's skills into a collection of primitives, where multiple primitives can be activated simultaneously via multiplicative composition. This flexibility allows the primitives to be transferred and recombined to elicit new behaviors as necessary for novel tasks. We demonstrate that MCP is able to extract composable skills for highly complex simulated characters from pre-training tasks, such as motion imitation, and then reuse these skills to solve challenging continuous control tasks, such as dribbling a soccer ball to a goal, and picking up an object and transporting it to a target location. (Video(1))	[Peng, Xue Bin; Chang, Michael; Zhang, Grace; Abbeel, Pieter; Levine, Sergey] Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA	University of California System; University of California Berkeley	Peng, XB (corresponding author), Univ Calif Berkeley, Dept Elect Engn & Comp Sci, Berkeley, CA 94720 USA.	xbpeng@berkeley.edu; mbchang@berkeley.edu; grace.zhang@berkeley.edu; pabbeel@cs.berkeley.edu; svlevine@eecs.berkeley.edu			NSERC; Berkeley Fellowship for Graduate Study; NSF Graduate Research Fellowship; Berkeley DeepDrive; Honda; ARL DCIST CRA [W911NF-17-2-0181]; Intel; Sony Interactive Entertainment America	NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC)); Berkeley Fellowship for Graduate Study; NSF Graduate Research Fellowship(National Science Foundation (NSF)); Berkeley DeepDrive; Honda; ARL DCIST CRA; Intel(Intel Corporation); Sony Interactive Entertainment America	We would like to thank AWS, Google, and NVIDIA for providing computational resources. This research was funded by an NSERC Postgraduate Scholarship, a Berkeley Fellowship for Graduate Study, an NSF Graduate Research Fellowship, Berkeley DeepDrive, Honda, ARL DCIST CRA W911NF-17-2-0181, Intel, and Sony Interactive Entertainment America.	[Anonymous], [No title captured]; Bacon Pierre-Luc, 2016, AAAI; Bin Peng X, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2897824.2925881; Brockman G., 2016, OPENAI GYM; Caruana R, 1997, MACH LEARN, V28, P41, DOI 10.1023/A:1007379606734; Coros S, 2009, ACM T GRAPHIC, V28, DOI 10.1145/1618452.1618516; Donahue TF, 2014, J CLIN ONCOL, V32, DOI 10.1200/jco.2014.32.4_suppl.168; Evgeniou A., 2007, ADV NEURAL INF PROCE, V19, P41, DOI DOI 10.2139/SSRN.1031158; Eysenbach B., 2019, INT C LEARNING REPRE; Faloutsos P, 2001, COMP GRAPH, P251, DOI 10.1145/383259.383287; Florensa C., 2017, PROC INT C MACH LEAR, P1; Frans K., 2017, ABS171009767 CORR; Haarnoja T, 2018, PR MACH LEARN RES, V80; Hausknecht M. J., 2016, P INT C LEARNING REP, P1; Hausman K., 2018, INT C LEARN REPR; Heess N., 2016, LEARNING TRANSFER MO; Hinton, 1993, ADV NEURAL INFORM PR, V5, P271; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Jacobs RA, 1991, NEURAL COMPUT, V3, P79, DOI 10.1162/neco.1991.3.1.79; Kingma D. P., 2013, AUTO ENCODING VARIAT; Kirkpatricka J, 2017, P NATL ACAD SCI USA, V114, P3521, DOI 10.1073/pnas.1611835114; Krishnan Sanjay, 2017, ABS171005421 CORR; Lillicrap Timothy P., 2016, ARXIV161000633; Liu LB, 2017, ACM T GRAPHIC, V36, DOI 10.1145/3083723; Merel J., 2018, INT C REPR LEARN; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neumann G., 2009, PROC INT C MACH LEAR, P753, DOI [10.1145/1553374.1553471, DOI 10.1145/1553374.1553471]; OpenAI, 2018, ABS180800177 CORR; Paraschos A., 2013, ADV NEURAL INFORM PR; Peng Xue Bin, 2018, ACM T GRAPHIC, V37, DOI DOI 10.1145/3197517.3201311; Ratliff Nathan D., 2018, ABS180102854 CORR; Rusu A. A., 2017, P C ROB LEARN, P262; Rusu A. A., 2016, ARXIV160604671; Schulman J., 2017, ARXIV PREPRINT ARXIV, DOI 10.48550/arXiv.1707.06347; Schulman J., 2016, 4 INT C LEARN REPR; Silver D, 2017, NATURE, V550, P354, DOI 10.1038/nature24270; Sutton R.S., 1998, INTRO REINFORCEMENT, DOI [10.1109/TNN.1998.712192, DOI 10.1109/TNN.1998.712192]; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1; Tessler Chen, 2016, AAAI; Vezhnevets AS, 2017, PR MACH LEARN RES, V70; Yin KK, 2007, ACM T GRAPHIC, V26, DOI 10.1145/1239451.1239556; Zico Kolter J., 2007, P ROB SCI SYST	43	28	28	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													12	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303065
C	Sauder, J; Sievers, B		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Sauder, Jonathan; Sievers, Bjarne			Self-Supervised Deep Learning on Point Clouds by Reconstructing Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					Point clouds provide a flexible and natural representation usable in countless applications such as robotics or self-driving cars. Recently, deep neural networks operating on raw point cloud data have shown promising results on supervised learning tasks such as object classification and semantic segmentation. While massive point cloud datasets can be captured using modern scanning technology, manually labelling such large 3D point clouds for supervised learning tasks is a cumbersome process. This necessitates methods that can learn from unlabelled data to significantly reduce the number of annotated samples needed in supervised learning. We propose a self-supervised learning task for deep learning on raw point cloud data in which a neural network is trained to reconstruct point clouds whose parts have been randomly rearranged. While solving this task, representations that capture semantic properties of the point cloud are learned. Our method is agnostic of network architecture and outperforms current unsupervised learning approaches in downstream object classification tasks. We show experimentally, that pre-training with our method before supervised training improves the performance of state-of-the-art models and significantly improves sample efficiency.	[Sauder, Jonathan; Sievers, Bjarne] Hasso Plattner Inst, Potsdam, Germany	University of Potsdam	Sauder, J (corresponding author), Hasso Plattner Inst, Potsdam, Germany.	jonathan.sauder@student.hpi.de; bjarne.sievers@student.hpi.de		Sauder, Jonathan/0000-0002-3876-1521				Achlioptas Panos, 2017, ABS170702392 CORR; Armeni I, 2016, PROC CVPR IEEE, P1534, DOI 10.1109/CVPR.2016.170; Aubry M, 2011, IEEE I CONF COMP VIS, P1411, DOI 10.1109/ICCV.2011.6126396; Bronstein MM, 2017, IEEE SIGNAL PROC MAG, V34, P18, DOI 10.1109/MSP.2017.2693418; Chang Angel X., 2015, ARXIV151203012CSGR P; CORTES C, 1995, MACH LEARN, V20, P273, DOI 10.1023/A:1022627411411; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Erhan D, 2010, J MACH LEARN RES, V11, P625; Gidaris Spyros, 2018, ARXIV180307728; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Guo YL, 2014, IEEE T PATTERN ANAL, V36, P2270, DOI 10.1109/TPAMI.2014.2316828; Han Zhizhong, 2019, ABS181102744 AAAI; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; James Melville, 2020, Arxiv, DOI arXiv:1802.03426; JiajunWu Chengkai Zhang, 2016, ADV NEURAL INFORM PR, V29, DOI DOI 10.5555/3157096.3157106; Lee H, 2009, P 26 ANN INT C MACH, V26, P609, DOI [10.1145/1553374.1553453, DOI 10.1145/1553374.1553453]; Li JX, 2018, PROC CVPR IEEE, P9397, DOI 10.1109/CVPR.2018.00979; Maturana D, 2015, IEEE INT C INT ROBOT, P922, DOI 10.1109/IROS.2015.7353481; Mikolov Tomas., 2013, ADV NEURAL INFORM PR, P3111, DOI DOI 10.1162/JMLR.2003.3.4-5.951; Noroozi M, 2016, LECT NOTES COMPUT SC, V9910, P69, DOI 10.1007/978-3-319-46466-4_5; Qi CR, 2016, PROC CVPR IEEE, P5648, DOI 10.1109/CVPR.2016.609; Qi Charles R, 2017, ARXIV170602413; Qi Charles Ruizhongtai, 2017, ABS161200593 CVPR; Ravanbakhsh S, 2016, ARXIV PREPRINT ARXIV; Rubner Y, 2000, INT J COMPUT VISION, V40, P99, DOI 10.1023/A:1026543900054; Rusu RB, 2008, 2008 IEEE/RSJ INTERNATIONAL CONFERENCE ON ROBOTS AND INTELLIGENT SYSTEMS, VOLS 1-3, CONFERENCE PROCEEDINGS, P3384, DOI 10.1109/IROS.2008.4650967; Savva M., 2017, P WORKSHOP 3D OBJECT, P39; Sharma A, 2016, LECT NOTES COMPUT SC, V9915, P236, DOI 10.1007/978-3-319-49409-8_20; Su H, 2015, IEEE I CONF COMP VIS, P945, DOI 10.1109/ICCV.2015.114; van der Maaten L, 2008, J MACH LEARN RES, V9, P2579; van Kaick O, 2011, COMPUT GRAPH FORUM, V30, P1681, DOI 10.1111/j.1467-8659.2011.01884.x; Vinyals Oriol, 2015, ARXIV151106391; Wang Yue, 2018, ABS180107829 CORR; WU ZR, 2015, PROC CVPR IEEE, P1912, DOI DOI 10.1109/CVPR.2015.7298801; Yang YQ, 2018, PROC CVPR IEEE, P206, DOI 10.1109/CVPR.2018.00029; Yi L, 2016, ACM T GRAPHIC, V35, DOI 10.1145/2980179.2980238; Zaheer M., 2017, ADV NEURAL INFORM PR, P3391; ZhirongWu Shuran Song, 2014, ABS14065670 CORR	39	28	28	1	3	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0QE					2022-12-19	WOS:000535866904058
C	Srinivas, S; Fleuret, F		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Srinivas, Suraj; Fleuret, Francois			Full-Gradient Representation for Neural Network Visualization	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA					We introduce a new tool for interpreting neural net responses, namely full-gradients, which decomposes the neural net response into input sensitivity and per-neuron sensitivity components. This is the first proposed representation which satisfies two key properties: completeness and weak dependence, which provably cannot be satisfied by any saliency map-based interpretability method. For convolutional nets, we also propose an approximate saliency map representation, called FullGrad, obtained by aggregating the full-gradient components. We experimentally evaluate the usefulness of FullGradin explaining model behaviour with two quantitative tests: pixel perturbation and remove-and-retrain. Our experiments reveal that our method explains model behavior correctly, and more comprehensively, than other methods in the literature. Visual inspection also reveals that our saliency maps are sharper and more tightly confined to object regions than other methods.	[Srinivas, Suraj; Fleuret, Francois] Idiap Res Inst, Martigny, Switzerland; [Srinivas, Suraj; Fleuret, Francois] Ecole Polytech Fed Lausanne, Lausanne, Switzerland	Swiss Federal Institutes of Technology Domain; Ecole Polytechnique Federale de Lausanne	Srinivas, S (corresponding author), Idiap Res Inst, Martigny, Switzerland.; Srinivas, S (corresponding author), Ecole Polytech Fed Lausanne, Lausanne, Switzerland.	suraj.srinivas@idiap.ch; francois.fleuret@idiap.ch			Swiss National Science Foundation under the ISUL [FNS-30209]	Swiss National Science Foundation under the ISUL	This work was supported by the Swiss National Science Foundation under the ISUL grant FNS-30209.	Adebayo Julius, 2018, ADV NEURAL INFORM PR, V1, P7; Ancona Marco, 2018, ICLR, DOI DOI 10.1109/TNSE.2020.2996738; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Karen Simonyan, 2014, ARXIV13126034CS, DOI DOI 10.1038/S41591-018-0335-9; Kim B., 2018, ARXIV180610758; Kindermans Pieter-Jan, 2016, ABS161107270 ARXIV; Lundberg SM, 2017, ADV NEUR IN, V30; Montavon G, 2017, PATTERN RECOGN, V65, P211, DOI 10.1016/j.patcog.2016.11.008; Nie WL, 2018, PR MACH LEARN RES, V80; Samek W, 2017, IEEE T NEUR NET LEAR, V28, P2660, DOI 10.1109/TNNLS.2016.2599820; Selvaraju RR, 2020, INT J COMPUT VISION, V128, P336, DOI 10.1007/s11263-019-01228-7; Shrikumar Avanti, 2017, PMLR, P3145, DOI DOI 10.1145/3292500.3330701; Smilkov D, 2017, ARXIV; Springenberg J.T., 2014, ARXIV14126806; Sundararajan M, 2017, PR MACH LEARN RES, V70; Zeiler MD, 2014, LECT NOTES COMPUT SC, V8689, P818, DOI 10.1007/978-3-319-10590-1_53	16	28	29	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424304016
C	Vaswani, S; Mishkin, A; Laradji, I; Schmidt, M; Gidel, G; Lacoste-Julien, S		Wallach, H; Larochelle, H; Beygelzimer, A; d'Alche-Buc, F; Fox, E; Garnett, R		Vaswani, Sharan; Mishkin, Aaron; Laradji, Issam; Schmidt, Mark; Gidel, Gauthier; Lacoste-Julien, Simon			Painless Stochastic Gradient: Interpolation, Line-Search, and Convergence Rates	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 32 (NIPS 2019)	Advances in Neural Information Processing Systems		English	Proceedings Paper	33rd Conference on Neural Information Processing Systems (NeurIPS)	DEC 08-14, 2019	Vancouver, CANADA				OPTIMIZATION	Recent works have shown that stochastic gradient descent (SGD) achieves the fast convergence rates of full-batch gradient descent for over-parameterized models satisfying certain interpolation conditions. However, the step-size used in these works depends on unknown quantities and SGD's practical performance heavily relies on the choice of this step-size. We propose to use line-search techniques to automatically set the step-size when training models that can interpolate the data. In the interpolation setting, we prove that SGD with a stochastic variant of the classic Armijo line-search attains the deterministic convergence rates for both convex and strongly-convex functions. Under additional assumptions, SGD with Armijo line-search is shown to achieve fast convergence for non-convex functions. Furthermore, we show that stochastic extra-gradient with a Lipschitz line-search attains linear convergence for an important class of non-convex functions and saddle-point problems satisfying interpolation. To improve the proposed methods' practical performance, we give heuristics to use larger step-sizes and acceleration. We compare the proposed algorithms against numerous optimization methods on standard classification tasks using both kernel methods and deep networks. The proposed methods result in competitive performance across all models and datasets, while being robust to the precise choices of hyper-parameters. For multi-class classification using deep networks, SGD with Armijo line-search results in both faster convergence and better generalization.	[Vaswani, Sharan; Lacoste-Julien, Simon] Univ Montreal, Mila, Montreal, PQ, Canada; [Mishkin, Aaron] Univ British Columbia, Vancouver, BC, Canada; [Laradji, Issam] Univ British Columbia, Element AI, Vancouver, BC, Canada; [Schmidt, Mark] Univ British Columbia, 1QBit, CCAI Affiliate Chair Amii, Vancouver, BC, Canada; [Gidel, Gauthier] Univ Montreal, Mila, Element AI, Montreal, PQ, Canada	Universite de Montreal; University of British Columbia; University of British Columbia; University of British Columbia; Universite de Montreal	Vaswani, S (corresponding author), Univ Montreal, Mila, Montreal, PQ, Canada.				NSERC CGS M award; UBC Four-Year Doctoral Fellowships (4YF); Canada CIFAR AI Chair Program; CIFAR LMB Program; Google Focused Research award; IVADO postdoctoral scholarship; Borealis AI fellowship; Canada Excellence Research Chair in "Data Science for Realtime Decision-making"; NSERC [RGPIN-2017-06936, 2015-06068]	NSERC CGS M award; UBC Four-Year Doctoral Fellowships (4YF); Canada CIFAR AI Chair Program; CIFAR LMB Program; Google Focused Research award(Google Incorporated); IVADO postdoctoral scholarship; Borealis AI fellowship; Canada Excellence Research Chair in "Data Science for Realtime Decision-making"; NSERC(Natural Sciences and Engineering Research Council of Canada (NSERC))	We would like to thank Yifan Sun and Nicolas Le Roux for insightful discussions. AM is supported by the NSERC CGS M award. IL is funded by the UBC Four-Year Doctoral Fellowships (4YF), This research was also partially supported by the Canada CIFAR AI Chair Program, the CIFAR LMB Program, by a Google Focused Research award, by an IVADO postdoctoral scholarship (for SV), by a Borealis AI fellowship (for GG), by the Canada Excellence Research Chair in "Data Science for Realtime Decision-making" and by the NSERC Discovery Grants RGPIN-2017-06936 and 2015-06068.	Allen-Zhu Zeyuan, 2017, ACM SIGACT S THEOR C; Almeida Luis, 1998, ON LINE LEARNING NEU; [Anonymous], ICML; Armijo Larry, 1966, PACIFIC J MATH; Bach Francis, 2012, FDN TRENDS MACHINE L; Bassily R., 2018, ARXIV181102564; Baydin AG, 2017, ICLR; Bengio Yoshua, 2012, Neural Networks: Tricks of the Trade. Second Edition: LNCS 7700, P437, DOI 10.1007/978-3-642-35289-8_26; BenTal A, 2009, PRINC SER APPL MATH, P1; Berrada Leonard, 2019, ARXIV190605661; Blanchet Jose, 2019, INFORMS J OPTIMIZATI; Byrd Richard H, 2012, MATH PROGRAMMING; Cevher Volkan, 2018, OPTIMIZATION LETT; Chang C.-C., 2011, ACM T INTELLIGENT SY; Chavdarova T., 2019, ADV NEURAL INFORM PR, V32, P393; Chen Yuxin, 2015, NEURIPS; De S., 2016, ARXIV PREPRINT ARXIV; Defazio Aaron, 2014, NEURIPS; Delyon Bernard, 1993, SIAM J OPTIMIZATION; Duchi J, 2011, J MACH LEARN RES, V12, P2121; Friedlander Michael P, 2012, SIAM J SCI COMPUTING; Frostig R, 2015, PR MACH LEARN RES, V37, P2540; Gidel G., 2019, INT C LEARNING REPRE; Goodfellow Ian, 2016, NIPS 2016 TUTORIAL G; Gratton S, 2018, IMA J NUMER ANAL, V38, P1579, DOI 10.1093/imanum/drx043; Harker Patrick T, 1990, MATH PROGRAMMING; He K, 2016, P 2016 IEEE C COMPUT, P770, DOI [10.1109/CVPR.2016.90, DOI 10.1109/CVPR.2016.90]; Huang G, 2017, PROC CVPR IEEE, P2261, DOI 10.1109/CVPR.2017.243; Iusem Alfredo N, 2019, SIAM J OPTIMIZATION; Iusem AN, 1997, OPTIMIZATION; Iusem AN, 2017, SIAM J OPTIMIZATION; Jain Prateek, 2018, COLT; Joachims Thorsten, 2005, ICML, DOI DOI 10.1145/1102351.1102399; Johnson Rie, 2013, NEURIPS; Juditsky Anatoli, 2011, STOCHASTIC SYSTEMS; Karimi H., 2016, JOINT EUROPEAN C MAC, P795; Khobotov Evgenii Nikolaevich, 1987, USSR COMPUTATIONAL M; Kingma D.P, P 3 INT C LEARNING R; Kleinberg Robert, 2018, ICML; Koepelevich, 1976, EKONOMIKA MATEMATICH, V12, P747; Krejic Natasa, 2013, J COMPUTATIONAL APPL; Kushner Harold J, 1995, IEEE T AUTOMATIC CON; Le Roux Nicolas, 2013, ARXIV13086370; Li YJ, 2017, ADV NEUR IN, V30; Liang T, 2018, ARXIV180800387; Lin Hongzhou, 2015, NEURIPS; Liu Chaoyue, 2019, ARXIV181013395; Loizou N., 2017, ARXIV171209677; Loizou Nicolas, 2017, ARXIV171010737; Loshchilov I., 2016, ARXIV; Luo Liangchen, 2019, INT C LEARN REPR; Ma SY, 2018, PR MACH LEARN RES, V80; Mahsereci Maren, 2017, JMLR; Martius G., 2018, ADV NEURAL INFORM PR; Mescheder L., 2017, NEURIPS; Nemirovski A, 2009, SIAM J OPTIMIZ, V19, P1574, DOI 10.1137/070704277; Nemirovski Arkadi, 2004, SIAM J OPTIMIZATION; Nesterov Y., 2004, APPL OPTIM; Nesterov Y, 2013, MATH PROGRAM, V140, P125, DOI 10.1007/s10107-012-0629-5; Nocedal J, 2006, SPRINGER SER OPER RE, P1, DOI 10.1007/978-0-387-40065-5; Orabona Francesco, 2017, NEURIPS; Palaniappan Balamurugan, 2016, NEURIPS; Paquette C, 2018, STOCHASTIC LINE SEAR; Plagianakos VP, 2001, ADV CONVEX ANAL GLOB; Polyak, 1963, ZH VYCHISLITELNOI MA; Polyak B. T., 1964, COMP MATH MATH PHYS+, V4, P1, DOI [DOI 10.1016/0041-5553(64)90137-5, 10.1016/0041-5553(64)90137-5]; Rahimi A., 2017, REFLECTIONS RANDOM K; Reddi Sashank J, 2019, ICLR; Schapire RE, 1998, ANN STAT, V26, P1651; Schmidt M, 2017, MATH PROGRAM, V162, P83, DOI 10.1007/s10107-016-1030-6; Schmidt Mark, 2015, AISTATS; Schoenauer-Sebag Alice, 2017, ARXIV170901427; Schraudolph Nicol, 1999, LOCAL GAIN ADAPTATIO; Shang Fanhua, 2018, AISTATS; Shao S, 2000, J MATH ANAL APPL; Soltanolkotabi M., 2018, IEEE T INFORM THEORY; Sun Ruoyu, 2016, IEEE T INFORM THEORY; Sutskever I., 2013, P 30 INT C MACH LEAR, P1139, DOI DOI 10.1007/S00287-015-0911-Z; Tan C., 2016, PROC 30 ANN C NEURAL, P685; Tieleman T., 2012, COURSERA NEURAL NETW, V4, P26; Vaswani S., 2019, AISTATS; Wen Junfeng, 2014, ICML; Yadav A, 2017, ARXIV PREPRINT ARXIV; Yu Jin, 2006, NEURIPS; Zeiler Matthew D, 2012, ARXIV12125701; Zhang CX, 2018, PROTEINS, V86, P136, DOI 10.1002/prot.25414; Zhang J., 2017, CORR	89	28	28	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2019	32													14	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BP0ID					2022-12-19	WOS:000534424303069
C	Jia, Y; Zhang, Y; Weiss, RJ; Wang, Q; Shen, J; Ren, F; Chen, ZF; Nguyen, P; Pang, RM; Moreno, IL; Wu, YH		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Jia, Ye; Zhang, Yu; Weiss, Ron J.; Wang, Quan; Shen, Jonathan; Ren, Fei; Chen, Zhifeng; Nguyen, Patrick; Pang, Ruoming; Moreno, Ignacio Lopez; Wu, Yonghui			Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					We describe a neural network-based system for text-to-speech (TTS) synthesis that is able to generate speech audio in the voice of different speakers, including those unseen during training. Our system consists of three independently trained components: (1) a speaker encoder network, trained on a speaker verification task using an independent dataset of noisy speech without transcripts from thousands of speakers, to generate a fixed-dimensional embedding vector from only seconds of reference speech from a target speaker; (2) a sequence-to-sequence synthesis network based on Tacotron 2 that generates a mel spectrogram from text, conditioned on the speaker embedding; (3) an auto-regressive WaveNet-based vocoder network that converts the mel spectrogram into time domain waveform samples. We demonstrate that the proposed model is able to transfer the knowledge of speaker variability learned by the discriminatively-trained speaker encoder to the multispeaker TTS task, and is able to synthesize natural speech from speakers unseen during training. We quantify the importance of training the speaker encoder on a large and diverse speaker set in order to obtain the best generalization performance. Finally, we show that randomly sampled speaker embeddings can be used to synthesize speech in the voice of novel speakers dissimilar from those used in training, indicating that the model has learned a high quality speaker representation.	[Jia, Ye; Zhang, Yu; Weiss, Ron J.; Wang, Quan; Shen, Jonathan; Ren, Fei; Chen, Zhifeng; Nguyen, Patrick; Pang, Ruoming; Moreno, Ignacio Lopez; Wu, Yonghui] Google Inc, Mountain View, CA 94043 USA	Google Incorporated	Jia, Y (corresponding author), Google Inc, Mountain View, CA 94043 USA.	jiaye@google.com; ronw@google.com; ngyuzh@google.com	Jia, Ye/AFN-9975-2022	Jia, Ye/0000-0001-6283-3102				[Anonymous], 1996, ITUT REC P 800 METH; [Anonymous], 2018, ARXIV180309017; Arik S. O., 2018, P 32 INT C NEUR INF, p10 040; Bahdanau D., 2015, 3 INT C LEARN REPR I; BOLL SF, 1979, IEEE T ACOUST SPEECH, V27, P113, DOI 10.1109/TASSP.1979.1163209; Chen Y., 2018, ARXIV180910460; Chung JS, 2018, INTERSPEECH, P1086; Doddipatla R, 2017, INTERSPEECH, P3404, DOI 10.21437/Interspeech.2017-1038; Heigold G, 2016, INT CONF ACOUST SPEE, P5115, DOI 10.1109/ICASSP.2016.7472652; Kyle K. K. J. F. S., 2017, INT C LEARNING REPRE; Nachmani E, 2018, PR MACH LEARN RES, V80; Nagrani Arsha, 2017, ARXIV170608612; Panayotov V, 2015, INT CONF ACOUST SPEE, P5206, DOI 10.1109/ICASSP.2015.7178964; Ping W, 2018, INT C LEARNING REPRE; Shen J., 2018, P IEEE INT C AC SPEE; Skerry-Ryan RJ, 2018, PR MACH LEARN RES, V80; Taigman Y., 2018, P INT C LEARN REPR I; van den Oord A., 2016, GENERATIVE MODEL RAW; Variani Ehsan, 2014, 2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), P4052, DOI 10.1109/ICASSP.2014.6854363; Veaux C., 2017, CSTR VCTK CORPUS ENG; Wan L., 2018, P IEEE INT C AC SPEE; Wang YX, 2017, INTERSPEECH, P4006, DOI 10.21437/Interspeech.2017-1452	23	28	28	0	5	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3BF					2022-12-19	WOS:000461823304049
C	Osband, I; Aslanides, J; Cassirer, A		Bengio, S; Wallach, H; Larochelle, H; Grauman, K; CesaBianchi, N; Garnett, R		Osband, Ian; Aslanides, John; Cassirer, Albin			Randomized Prior Functions for Deep Reinforcement Learning	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 31 (NIPS 2018)	Advances in Neural Information Processing Systems		English	Proceedings Paper	32nd Conference on Neural Information Processing Systems (NIPS)	DEC 02-08, 2018	Montreal, CANADA					Dealing with uncertainty is essential for efficient reinforcement learning. There is a growing literature on uncertainty estimation for deep learning from fixed datasets, but many of the most popular approaches are poorly-suited to sequential decision problems. Other methods, such as bootstrap sampling, have no mechanism for uncertainty that does not come from the observed data. We highlight why this can be a crucial shortcoming and propose a simple remedy through addition of a randomized untrainable 'prior' network to each ensemble member. We prove that this approach is efficient with linear representations, provide simple illustrations of its efficacy with nonlinear representations and show that this approach scales to large-scale problems far better than previous attempts.	[Osband, Ian; Aslanides, John; Cassirer, Albin] DeepMind, London, England		Osband, I (corresponding author), DeepMind, London, England.	iosband@google.com; jaslanides@google.com; cassirer@google.com						Agrawal S., 2012, COLT, V39, P1; Agrawal S., 2013, ARTIF INTELL, P99; Azizzadenesheli  Kamyar, 2018, ARXIV180204412; Barth-Maron G., 2018, PROC INT C LEARN REP, P1; Bellemare M., 2016, NEURIPS; Bellemare MG, 2017, PR MACH LEARN RES, V70; Bellemare MG, 2013, J ARTIF INTELL RES, V47, P253, DOI 10.1613/jair.3912; Blundell A., 2016, ADV NEURAL INFORM PR, P4026; Blundell C, 2015, PR MACH LEARN RES, V37, P1613; Cox D.R., 1979, THEORETICAL STAT; Dabney W, 2018, AAAI CONF ARTIF INTE, P2892; de Finetti Bruno, 1937, ANN LINSTITUT HENRI, VPoincare7, P1; Efron B., 1994, INTRO BOOTSTRAP; Efron Bradley, 1982, JACKKNIFE BOOTSTRAP, V38, DOI [10.1137/1.9781611970319, DOI 10.1137/1.9781611970319]; Fortunato M., 2018, INT C LEARN REPR, P1; Fushiki T, 2005, BERNOULLI, V11, P747, DOI 10.3150/bj/1126126768; Fushiki T, 2005, BERNOULLI, V11, P293, DOI 10.3150/bj/1116340296; Gal Y., 2016, P DAT EFF MACH LEARN, P1; Gal Y, 2017, PROC 31 INT C NEURAL, P3584; Gal Y, 2016, PR MACH LEARN RES, V48; Glorot X., 2010, PROC MACH LEARN RES, P249; Horgan  Daniel, 2018, 6 INT C LEARN REPR; Jaksch T, 2010, J MACH LEARN RES, V11, P1563; Kearns  M., 2002, MACHINE LEARNING, V49; Kingma D.P, P 3 INT C LEARNING R; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lakshminarayanan B, 2017, ADV NEURAL INFORM PR, P6402, DOI DOI 10.5555/3295222.3295387; LeCun Y, 2015, NATURE, V521, P436, DOI 10.1038/nature14539; Legg S, 2007, ADV ARTIFICIAL GEN I, V157, P17; Leike  Jan, 2016, UNCERTAINTY ARTIFICI; Lipton Z. C., 2016, ARXIV160805081; Lu XY, 2017, ADV NEUR IN, V30; MACKAY DJC, 1992, NEURAL COMPUT, V4, P415, DOI 10.1162/neco.1992.4.3.448; Maennel  Hartmut, 2018, ARXIV180308367; Mania H., 2018, ARXIV180307055; Mihatsch O, 2002, MACH LEARN, V49, P267, DOI 10.1023/A:1017940631555; Mnih V, 2016, PR MACH LEARN RES, V48; Mnih V, 2015, NATURE, V518, P529, DOI 10.1038/nature14236; Neal RM, 1996, LECT NOTES STAT, V118; O'Donoghue  Brendan, 2017, ARXIV170905380; Oord A.V.D., 2016, SSW; Osband I., 2015, ARXIV150700300; Osband I., 2016, WORKSH BAYES DEEP LE; Osband I, 2017, PR MACH LEARN RES, V70; Osband I, 2016, PR MACH LEARN RES, V48; Osband Ian, 2017, ARXIV170307608; Osband  Ian, 2016, THESIS; Ostrovski G, 2017, PR MACH LEARN RES, V70; Plappert Matthias, 2017, ARXIV170601905; Rumelhart DE, 1985, TECHNICAL REPORT, DOI 10.1016/b978-1-4832-1446-7.50035-2; Rusmevichientong P, 2010, MATH OPER RES, V35, P395, DOI 10.1287/moor.1100.0446; Russo D, 2014, MATH OPER RES, V39, P1221, DOI 10.1287/moor.2014.0650; Russo Daniel, 2017, ARXIV170702038; Schaul T, 2015, P INT C LEARN REPR; Silver D, 2016, NATURE, V529, P484, DOI 10.1038/nature16961; Srivastava N, 2014, J MACH LEARN RES, V15, P1929; Strens, 2000, P 17 INT C MACH LEAR, P943; SUTTON R S, 2011, 10 INT C AUTONOMOUS, P761, DOI DOI 10.5555/2031678.2031726; Sutton Richard S, 1998, INTRO REINFORCEMENT, V2; Tang  Yunhao, 2017, ARXIV171111225; Tassa Y., 2018, ARXIV180100690; TESAURO G, 1995, COMMUN ACM, V38, P58, DOI 10.1145/203330.203343; Thompson WR, 1933, BIOMETRIKA, V25, P285, DOI 10.1093/biomet/25.3-4.285; Touati A., 2018, ARXIV180602315; van Hasselt H, 2016, AAAI CONF ARTIF INTE, P2094; Van Roy, 2013, ADV NEURAL INFORM PR, P3003; Wald  Abraham, 1992, BREAKTHROUGHS STAT, P342; Wang Ziyu, 2015, ARXIV151106581; Zhang Chiyuan, 2016, ARXIV161103530	71	28	29	3	11	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2018	31													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BM3CA					2022-12-19	WOS:000461852003020
C	Baig, MH; Koltun, V; Torresani, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Baig, Mohammad Harts; Koltun, Vladlen; Torresani, Lorenzo			Learning to Inpaint for Image Compression	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We study the design of deep architectures for lossy image compression. We present two architectural recipes in the context of multi-stage progressive encoders and empirically demonstrate their importance on compression performance. Specifically, we show that: (a) predicting the original image data from residuals in a multi-stage progressive architecture facilitates learning and leads to improved performance at approximating the original content and (b) learning to inpaint (from neighboring image pixels) before performing compression reduces the amount of information that must be stored to achieve a high-quality approximation. Incorporating these design choices in a baseline progressive encoder yields an average reduction of over 60% in file size with similar quality compared to the original residual encoder.	[Baig, Mohammad Harts] Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA; [Koltun, Vladlen] Intel Labs, Santa Clara, CA USA; [Torresani, Lorenzo] Dartmouth Coll, Hanover, NH 03755 USA	Dartmouth College; Intel Corporation; Dartmouth College	Baig, MH (corresponding author), Dartmouth Coll, Dept Comp Sci, Hanover, NH 03755 USA.		Jeong, Yongwook/N-7413-2016		Intel Labs; NSF [CNS-120552]	Intel Labs; NSF(National Science Foundation (NSF))	This work was funded in part by Intel Labs and NSF award CNS-120552. We gratefully acknowledge NVIDIA and Facebook for the donation of GPUs used for portions of this work. We would like to thank George Toderici, Nick Johnston, Johannes Balle for providing us with information needed for accurate assessment. We are grateful to members of the Visual Computing Lab at Intel Labs, and members of the Visual Learning Group at Dartmouth College for their feedback.	Balle Johannes, 2017, 5 INT C LEARN REPR I; Bertalmio M, 2000, COMP GRAPH, P417, DOI 10.1145/344779.344972; Bjontegaard Gisle, 2008, ITU T SC16 Q6 35 VCE; Chen LC, 2018, IEEE T PATTERN ANAL, V40, P834, DOI 10.1109/TPAMI.2017.2699184; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Eastman Kodak Company, 1999, KODAK LOSSLESS TRUE; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; He KM, 2015, IEEE I CONF COMP VIS, P1026, DOI 10.1109/ICCV.2015.123; Jia Y., 2014, P 22 ACM INT C MULT, P675; Kingma D.P, P 3 INT C LEARNING R; Meeker Mary, 2016, INTERNET TRENDS REPO; Pathak D, 2016, PROC CVPR IEEE, P2536, DOI 10.1109/CVPR.2016.278; Raiko T., 2015, ICLR, P1; Rippel O, 2017, PR MACH LEARN RES, V70; Skodras A, 2001, IEEE SIGNAL PROC MAG, V18, P36, DOI 10.1109/79.952804; Theis Lucas, 2017, INT C LEARN REPR; Toderici G., 2016, ARXIV160805148; Toderici G., 2016, 4 INT C LEARN REPR I, P2; Wallace G. K., 1991, Communications of the ACM, V34, P30, DOI 10.1145/103085.103089; Wang Z., 2004, IEEE AS C SIGN SYST, V2, P1398; Yu F., 2016, P ICLR 2016	22	28	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													10	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649401028
C	Luo, ZL; Zou, YL; Hoffman, J; Fei-Fei, L		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Luo, Zelun; Zou, Yuliang; Hoffman, Judy; Fei-Fei, Li			Label Efficient Learning of Transferable Representations across Domains and Tasks	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					We propose a framework that learns a representation transferable across different domains and tasks in a label efficient manner. Our approach battles domain shift with a domain adversarial loss, and generalizes the embedding to novel task using a metric learning-based approach. Our model is simultaneously optimized on labeled source data and unlabeled or sparsely labeled data in the target domain. Our method shows compelling results on novel classes within a new domain even when only a few labeled examples per class are available, outperforming the prevalent fine-tuning approach. In addition, we demonstrate the effectiveness of our framework on the transfer learning task from image object recognition to video action recognition.	[Luo, Zelun; Fei-Fei, Li] Stanford Univ, Stanford, CA 94305 USA; [Zou, Yuliang] Virginia Tech, Blacksburg, VA USA; [Hoffman, Judy] Univ Calif Berkeley, Berkeley, CA 94720 USA	Stanford University; Virginia Polytechnic Institute & State University; University of California System; University of California Berkeley	Luo, ZL (corresponding author), Stanford Univ, Stanford, CA 94305 USA.	zelunluo@stanford.edu; ylzou@vt.edu; jhoffman@eecs.berkeley.edu; feifeili@cs.stanford.edu	Jeong, Yongwook/N-7413-2016; Luo, Zelun/AAO-3655-2021	Luo, Zelun/0000-0003-3597-5046	Stanford Computer Science Department; Stanford Program in AI-assisted Care (PAC)	Stanford Computer Science Department; Stanford Program in AI-assisted Care (PAC)	We would like to start by thanking our sponsors: Stanford Computer Science Department and Stanford Program in AI-assisted Care (PAC). Next, we specially thank De-An Huang, Kenji Hata, Serena Yeung, Ozan Sener and all the members of Stanford Vision and Learning Lab for their insightful discussion and feedback. Lastly, we thank all the anonymous reviewers for their valuable comments.	Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Aytar Y, 2011, IEEE I CONF COMP VIS, P2252, DOI 10.1109/ICCV.2011.6126504; Bousmalis K., 2016, ARXIV161205424; Castrejon L, 2016, PROC CVPR IEEE, P2940, DOI 10.1109/CVPR.2016.321; Csurka Gabriela, 2017, ARXIV170205374; Deng J, 2009, PROC CVPR IEEE, P248, DOI 10.1109/CVPRW.2009.5206848; Doersch C, 2015, IEEE I CONF COMP VIS, P1422, DOI 10.1109/ICCV.2015.167; Donahue J., 2016, ARXIV160509782; Donahue J, 2014, PR MACH LEARN RES, V32; Dumoulin Vincent, 2016, ARXIV E PRINTS; Finn C, 2017, PR MACH LEARN RES, V70; Ganin Y., 2014, ARXIV14097495; Ganin Y, 2016, J MACH LEARN RES, V17; Girshick  R., 2014, COMPUTER VISION PATT, DOI DOI 10.1109/CVPR.2014.81; Goodfellow I, 2020, COMMUN ACM, V63, P139, DOI 10.1145/3422622; Grandvalet Yves, 2004, NIPS, P529; Gretton A, 2009, NEURAL INF PROCESS S, P131; He KM, 2016, PROC CVPR IEEE, P770, DOI 10.1109/CVPR.2016.90; Hinton GE, 2006, SCIENCE, V313, P504, DOI 10.1126/science.1127647; Hinton GE, 1986, PARALLEL DISTRILMTED, V1; Hoffman J, 2016, PROC CVPR IEEE, P826, DOI 10.1109/CVPR.2016.96; Hoffman J, 2016, IEEE INT CONF ROBOT, P5032, DOI 10.1109/ICRA.2016.7487708; Hoffmann Johannes, 2016, 2016 Conference on Precision Electromagnetic Measurements (CPEM), P1, DOI 10.1109/CPEM.2016.7540615; Kalogeiton V, 2016, IEEE T PATTERN ANAL, V38, P2327, DOI 10.1109/TPAMI.2016.2551239; Kingma D.P, P 3 INT C LEARNING R; Kingma D. P., 2013, AUTO ENCODING VARIAT; Koch G., 2015, ICML DEEP LEARNING W; Krizhevsky A, 2012, ADV NEURAL INFORM PR, V25, P1097, DOI 10.1145/3065386; Lecun Y, 1998, P IEEE, V86, P2278, DOI 10.1109/5.726791; Li Yanghao, 2016, ARXIV160304779; Lim J.J., 2011, ADV NEURAL INFORM PR, P118; Liu Ming-Yu, 2016, ADV NEURAL INFORM PR, P2; Liu Ming-Yu, 2017, NIPS; Long J, 2015, PROC CVPR IEEE, P3431, DOI 10.1109/CVPR.2015.7298965; Long MS, 2015, PR MACH LEARN RES, V37, P97; Long MS, 2016, ADV NEUR IN, V29; Long MS., 2017, 34 INT C MACH LEARN; Luo ZL, 2017, PROC CVPR IEEE, P7101, DOI 10.1109/CVPR.2017.751; Misra I, 2016, LECT NOTES COMPUT SC, V9905, P527, DOI 10.1007/978-3-319-46448-0_32; Netzer Y, 2011, NIPS WORKSH DEEP LEA, P2011, DOI DOI 10.2118/18761-MS; Oquab M, 2014, PROC CVPR IEEE, P1717, DOI 10.1109/CVPR.2014.222; Palubinskas Gintautas, 1999, AAAI; Pathak D., 2016, ARXIV161206370; Ravi S., 2017, INT C LEARN REPR, P12; Razavian AS, 2014, IEEE COMPUT SOC CONF, P512, DOI 10.1109/CVPRW.2014.131; Redmon J, 2016, YOU ONLY LOOK ONCE U, DOI [DOI 10.1109/CVPR.2016.91, 10.1109/CVPR.2016.91]; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Rozantsev A., 2016, ARXIV160306432; Russakovsky O, 2015, INT J COMPUT VISION, V115, P211, DOI 10.1007/s11263-015-0816-y; Salakhutdinov Ruslan, 2009, ARTIF INTELL, P448, DOI DOI 10.1109/CVPR.2009.5206577; Simonyan Karen, 2014, ARXIV14062199, DOI DOI 10.1002/14651858.CD001941.PUB3; Sinno J.P., 2009, IEEE T KNOWL DATA EN, V22, P1345, DOI [10.1109/TKDE.2009.191, DOI 10.1109/TKDE.2009.191]; Snell J., 2017, ADV NEURAL INFORM PR, P4077; Soomro K., 2012, ARXIV; Sun BC, 2016, LECT NOTES COMPUT SC, V9915, P443, DOI 10.1007/978-3-319-49409-8_35; Taigman Y., 2016, ARXIV161102200; Tang K., 2012, ADV NEURAL INFORM PR; Tommasi T, 2010, PROC CVPR IEEE, P3081, DOI 10.1109/CVPR.2010.5540064; Tzeng E., 2014, ARXIV PREPRINT ARXIV; Tzeng E, 2017, PROC CVPR IEEE, P2962, DOI 10.1109/CVPR.2017.316; Tzeng E, 2015, IEEE I CONF COMP VIS, P4068, DOI 10.1109/ICCV.2015.463; van den Oord A, 2016, PR MACH LEARN RES, V48; van den Oord Aaron, 2016, ARXIV160605328; van der Maaten L, 2014, J MACH LEARN RES, V15, P3221; van der Maaten L, 2012, MACH LEARN, V87, P33, DOI 10.1007/s10994-011-5273-4; Vincent P., 2008, P 25 INT C MACH LEAR, P1096, DOI 10.1145/1390156.1390294; Vinyals O., 2016, ADV NEURAL INFORM PR, P3637, DOI [10.48550/arXiv.1606.04080, DOI 10.5555/3157382.3157504]; Wei Liu, 2016, Computer Vision - ECCV 2016. 14th European Conference. Proceedings: LNCS 9905, P21, DOI 10.1007/978-3-319-46448-0_2; Weiss Karl, 2016, Journal of Big Data, V3, DOI 10.1186/s40537-016-0043-6; Zhang R, 2016, LECT NOTES COMPUT SC, V9907, P649, DOI 10.1007/978-3-319-46487-9_40; Zhang X., 2015, ARXIV150300591	72	28	31	1	7	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													13	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649400016
C	Swaminathan, A; Krishnamurthy, A; Agarwal, A; Dudik, M; Langford, J; Jose, D; Zitouni, I		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Swaminathan, Adith; Krishnamurthy, Akshay; Agarwal, Alekh; Dudik, Miroslav; Langford, John; Jose, Damien; Zitouni, Imed			Off-policy evaluation for slate recommendation	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper studies the evaluation of policies that recommend an ordered set of items (e.g., a ranking) based on some context-a common scenario in web search, ads, and recommendation. We build on techniques from combinatorial bandits to introduce a new practical estimator that uses logged data to estimate a policy's performance. A thorough empirical evaluation on real-world data reveals that our estimator is accurate in a variety of settings, including as a subroutine in a learningto-rank task, where it achieves competitive performance. We derive conditions under which our estimator is unbiased-these conditions are weaker than prior heuristics for slate evaluation-and experimentally demonstrate a smaller bias than parametric approaches, even when these conditions are violated. Finally, our theory and experiments also show exponential savings in the amount of required data compared with general unbiased estimators.	[Swaminathan, Adith] Microsoft Res, Redmond, WA 98052 USA; [Krishnamurthy, Akshay] Univ Massachusetts, Amherst, MA 01003 USA; [Agarwal, Alekh; Dudik, Miroslav; Langford, John] Microsoft Res, New York, NY USA; [Jose, Damien; Zitouni, Imed] Microsoft, Redmond, WA USA	Microsoft; University of Massachusetts System; University of Massachusetts Amherst; Microsoft; Microsoft	Swaminathan, A (corresponding author), Microsoft Res, Redmond, WA 98052 USA.	adswamin@microsoft.com; akshay@cs.umass.edu; alekha@microsoft.com; mdudik@microsoft.com; jcl@microsoft.com; dajose@microsoft.com; izitouni@microsoft.com	Jeong, Yongwook/N-7413-2016					[Anonymous], INT C MACH LEARN; Asadi Nima, 2013, EUR C ADV INF RETR; Auer P., 2002, J MACHINE LEARNING R; Auer P., 2002, SIAM J COMPUTING; Bottou Leon, 2013, J MACHINE LEARNING R; Bubeck S., 2012, FDN TRENDS MACHINE L; Burges Chris, 2005, INT C MACH LEARN; Cesa-Bianchi Nicolo, 2012, J COMPUTER SYSTEM SC; Chapelle Olivier, 2009, C INF KNOWL MAN; Chapelle Olivier, 2009, INT C WORLD WID WEB; Chu Wei, 2011, JMLR WORKSHOP C P, P208; Dani Varsha, 2008, ADV NEURAL INFORM PR; Dudik M., 2014, STAT SCI; Dudik Miroslav, 2011, INT C MACH LEARN; Dupret Georges E., 2008, SIGIR C RES DEV INF; Filippi Sarah, 2010, ADV NEURAL INFORM PR; Guo Fan, 2009, INT C WORLD WID WEB; Hofmann Katja, 2016, FDN TRENDS INFORM RE; Horvitz Daniel G, 1952, J AM STAT ASS; Kale Satyen, 2010, ADV NEURAL INFORM PR; Kohavi Ron, 2009, KNOWLEDGE DISCOVERY; Krishnamurthy A., 2016, ADV NEURAL INFORM PR; Kveton Branislav, 2015, ARTIFICIAL INTELLIGE; Langford John, 2008, INT C MACH LEARN; Langford John, 2008, ADV NEURAL INFORM PR; Li L., 2015, WSDM; Li Lihong, 2011, INT C WEB SEARCH DAT; Li Lihong, 2010, INT C WORLD WID WEB; Petersen K. B., 2012, MATRIX COOKBOOK; Qin Lijing, 2014, INT C DAT MIN; Qin T., 2013, ARXIV13062597; Rusmevichientong Paat, 2010, MATH OPERATIONS RES; Swaminathan Adith, 2015, INT C MACH LEARN; Tax Niek, 2015, INFORM PROCESSING MA; Wang Y, 2016, PROCEEDINGS OF THE NINTH ACM INTERNATIONAL CONFERENCE ON WEB SEARCH AND DATA MINING (WSDM'16), P103, DOI 10.1145/2835776.2835824	35	28	28	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649403068
C	Wang, LW; Schwing, AG; Lazebnik, S		Guyon, I; Luxburg, UV; Bengio, S; Wallach, H; Fergus, R; Vishwanathan, S; Garnett, R		Wang, Liwei; Schwing, Alexander G.; Lazebnik, Svetlana			Diverse and Accurate Image Description Using a Variational Auto-Encoder with an Additive Gaussian Encoding Space	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 30 (NIPS 2017)	Advances in Neural Information Processing Systems		English	Proceedings Paper	31st Annual Conference on Neural Information Processing Systems (NIPS)	DEC 04-09, 2017	Long Beach, CA					This paper explores image caption generation using conditional variational auto-encoders (CVAEs). Standard CVAEs with a fixed Gaussian prior yield descriptions with too little variability. Instead, we propose two models that explicitly structure the latent space around K components corresponding to different types of image content, and combine components to create priors for images that contain multiple types of content simultaneously (e.g., several kinds of objects). Our first model uses a Gaussian Mixture model (GMM) prior, while the second one defines a novel Additive Gaussian (AG) prior that linearly combines component means. We show that both models produce captions that are more diverse and more accurate than a strong LSTM baseline or a "vanilla" CVAE with a fixed Gaussian prior, with AG-CVAE showing particular promise.	[Wang, Liwei; Schwing, Alexander G.; Lazebnik, Svetlana] Univ Illinois, Champaign, IL 61820 USA	University of Illinois System; University of Illinois Urbana-Champaign	Wang, LW (corresponding author), Univ Illinois, Champaign, IL 61820 USA.	lwang97@illinois.edu; aschwing@illinois.edu; slazebni@illinois.edu	Jeong, Yongwook/N-7413-2016		National Science Foundation [1563727, 1718221]; Sloan Foundation	National Science Foundation(National Science Foundation (NSF)); Sloan Foundation(Alfred P. Sloan Foundation)	This material is based upon work supported in part by the National Science Foundation under Grants No. 1563727 and 1718221, and by the Sloan Foundation. We would like to thank Jian Peng and Yang Liu for helpful discussions.	Anderson P, 2016, LECT NOTES COMPUT SC, V9909, P382, DOI 10.1007/978-3-319-46454-1_24; Andrew Zisserman, 2015, Arxiv, DOI arXiv:1409.1556; Batra D., 2012, ECCV; Bishop C.M., 1994, MIXTURE DENSITY NETW; Chen X, 2015, CORR, V1504, P325; Dai B, 2017, IEEE I CONF COMP VIS, P2989, DOI 10.1109/ICCV.2017.323; Denkowski Michael, 2014, P 9 WORKSH STAT MACH, P376, DOI DOI 10.3115/V1/W14-3348; Deshpande A, 2017, PROC CVPR IEEE, P2877, DOI 10.1109/CVPR.2017.307; Devlin J., 2015, ARXIV150504467; Devlin J, 2015, PROCEEDINGS OF THE 53RD ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS (ACL) AND THE 7TH INTERNATIONAL JOINT CONFERENCE ON NATURAL LANGUAGE PROCESSING (IJCNLP), VOL 2, P100; Farhadi A, 2010, LECT NOTES COMPUT SC, V6314, P15, DOI 10.1007/978-3-642-15561-1_2; Hershey JR, 2007, INT CONF ACOUST SPEE, P317, DOI 10.1109/icassp.2007.366913; Hochreiter S, 1997, NEURAL COMPUT, V9, P1735, DOI [10.1162/neco.1997.9.8.1735, 10.1007/978-3-642-24797-2, 10.1162/neco.1997.9.1.1]; Jain U., 2017, CVPR; Jang E., 2017, ICLR; Jingna Mao, 2015, 2015 IEEE Biomedical Circuits and Systems Conference (BioCAS), P1, DOI 10.1109/BioCAS.2015.7348279; Kingma D.P, P 3 INT C LEARNING R; Kiros R, 2014, PR MACH LEARN RES, V32, P595; Kulkarni G, 2013, IEEE T PATTERN ANAL, V35, P2891, DOI 10.1109/TPAMI.2012.162; Kuznetsova P, 2013, ACL; Lin Chin-Yew, 2004, TEXT SUMMARIZATION B, P74, DOI DOI 10.2307/3105454; Liu S, 2017, IEEE I CONF COMP VIS, P3516, DOI 10.1109/ICCV.2017.378; Mitchell Margaret, 2012, EACL; Papineni K, 2002, 40TH ANNUAL MEETING OF THE ASSOCIATION FOR COMPUTATIONAL LINGUISTICS, PROCEEDINGS OF THE CONFERENCE, P311, DOI 10.3115/1073083.1073135; Ren SQ, 2015, ADV NEUR IN, V28, DOI 10.1109/TPAMI.2016.2577031; Shetty R, 2017, IEEE I CONF COMP VIS, P4155, DOI 10.1109/ICCV.2017.445; VEDANTAM R, 2015, PROC CVPR IEEE, P4566, DOI DOI 10.1109/CVPR.2015.7299087; Vijayakumar Ashwin K, 2016, ARXIV161002424; Vinyals O, 2015, PROC CVPR IEEE, P3156, DOI 10.1109/CVPR.2015.7298935; Wang LM, 2016, PROC CVPR IEEE, P2708, DOI 10.1109/CVPR.2016.296; Wang Zhigang, 2016, IJCAI; Xu K, 2015, PR MACH LEARN RES, V37, P2048; You QZ, 2016, PROC CVPR IEEE, P4651, DOI 10.1109/CVPR.2016.503	36	28	29	0	1	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2017	30													11	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5ST					2022-12-19	WOS:000452649405081
C	Wang, XZ; Schneider, J		Ghahramani, Z; Welling, M; Cortes, C; Lawrence, ND; Weinberger, KQ		Wang, Xuezhi; Schneider, Jeff			Flexible Transfer Learning under Support and Model Shift	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 27 (NIPS 2014)	Advances in Neural Information Processing Systems		English	Proceedings Paper	28th Conference on Neural Information Processing Systems (NIPS)	DEC 08-13, 2014	Montreal, CANADA					Transfer learning algorithms are used when one has sufficient training data for one supervised learning task (the source/training domain) but only very limited training data for a second task (the target/test domain) that is similar but not identical to the first. Previous work on transfer learning has focused on relatively restricted settings, where specific parts of the model are considered to be carried over between tasks. Recent work on covariate shift focuses on matching the marginal distributions on observations X across domains. Similarly, work on target/conditional shift focuses on matching marginal distributions on labels Y and adjusting conditional distributions P(X vertical bar Y), such that P(X) can be matched across domains. However, covariate shift assumes that the support of test P(X) is contained in the support of training P(X), i.e., the training set is richer than the test set. Target/conditional shift makes a similar assumption for P(Y). Moreover, not much work on transfer learning has considered the case when a few labels in the test domain are available. Also little work has been done when all marginal and conditional distributions are allowed to change while the changes are smooth. In this paper, we consider a general case where both the support and the model change across domains. We transform both X and Y by a location-scale shift to achieve transfer between tasks. Since we allow more flexible transformations, the proposed method yields better results on both synthetic data and real-world data.	[Wang, Xuezhi] Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA; [Schneider, Jeff] Carnegie Mellon Univ, Robot Inst, Pittsburgh, PA 15213 USA	Carnegie Mellon University; Carnegie Mellon University	Wang, XZ (corresponding author), Carnegie Mellon Univ, Comp Sci Dept, Pittsburgh, PA 15213 USA.	xuezhiw@cs.cmu.edu; schneide@cs.cmu.edu			US Department of Agriculture [20126702119958]	US Department of Agriculture(United States Department of Agriculture (USDA))	This work is supported in part by the US Department of Agriculture under grant number 20126702119958.	Chattopadhyay R., 2013, ICML; Do C. B., 2005, NEURAL INFORM PROCES; Garnett Roman, 2012, ICML; Gretton Arthur, 2007, NIPS 2007; Huang Jiayuan, 2007, NIPS 2007; Ji Ming, 2012, AISTATS; Liao X., 2005, P 21 INT C MACH LEAR; Ma Yifei, 2013, NIPS; Mihalkova L, 2007, P 22 AAAI C ART INT; Niculescu-Mizil Alexandru, 2007, P 11 INT C ART INT S; Oliva Junier B., 2014, AISTATS; Pan Sinno Jialin, 2009, TKDE 2009; Rahimi A., 2007, ADV NEURAL INFORM PR; Rai Piyush, 2010, ACT LEARN NLP ALNLP; Raina R, 2006, P 23 INT C MACH LEAR; Saha Avishek, 2011, ECML; Seo Sambu, 2000, IJCNN; Shi X., 2008, ECML; Shimodaira H, 2000, J STAT PLAN INFER, V90, P227, DOI 10.1016/S0378-3758(00)00115-4; Song Le, 2009, ICML 2009; SUN Q., 2011, ADV NEURAL INFORM PR, P24; Wang Xuezhi, 2014, ACTIVE TRANSFER LEAR; Zhai, 2007, ANN M ASS COMP LING, P264, DOI [DOI 10.1145/1273496.1273558, DOI 10.1039/B610011B]; Zhang Kun, 2013, ICML 2013; [No title captured]	25	28	29	0	0	NEURAL INFORMATION PROCESSING SYSTEMS (NIPS)	LA JOLLA	10010 NORTH TORREY PINES RD, LA JOLLA, CALIFORNIA 92037 USA	1049-5258			ADV NEUR IN			2014	27													9	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BL5SR					2022-12-19	WOS:000452647103083
C	De Bie, T; Cristianini, N		Thrun, S; Saul, K; Scholkopf, B		De Bie, T; Cristianini, N			Convex methods for transduction	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 16	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	17th Annual Conference on Neural Information Processing Systems (NIPS)	DEC   08, 2003	CANADA					The 2-class transduction problem, as formulated by Vapnik [1], involves finding a separating hyperplane for a labelled data set that is also maximally distant from a given set of unlabelled test points. In this form, the problem has exponential computational complexity in the size of the working set. So far it has been attacked by means of integer programming techniques [2] that do not scale to reasonable problem sizes, or by local search procedures [3]. In this paper we present a relaxation of this task based on semi-definite programming (SDP), resulting in a convex optimization problem that has polynomial complexity in the size of the data set. The results are very encouraging for mid sized data sets, however the cost is still too high for large scale problems, due to the high dimensional search space. To this end, we restrict the feasible region by introducing an approximation based on solving an eigenproblem. With this approximation, the computational cost of the algorithm is such that problems with more than 1000 points can be treated.	Katholieke Univ Leuven, SISTA, ESAT, SCD, B-3001 Heverlee, Belgium	KU Leuven	De Bie, T (corresponding author), Katholieke Univ Leuven, SISTA, ESAT, SCD, Kasteelpk Arenberg 10, B-3001 Heverlee, Belgium.		De Bie, Tijl/B-2920-2013	De Bie, Tijl/0000-0002-2692-7504				BENNETT K, 1999, ADV NEURAL INFORMATI, V11; CHAPELLE O, 2003, ADV NEURAL INFORMATI, V15; CRISTIANINI N, 2003, UNPUB OPTIMIZING KER; HELMBERG C, 2000, ZR0034 TU BERL ZIB K; Horn R. A., 1986, MATRIX ANAL; JOCHIMS T, 1999, P INT C MACH LEARN I; KAMVAR S, 2003, P INT JOINT C ART IN; Poggio T., 2001, P C UNC GEOM COMP; Shi JB, 2000, IEEE T PATTERN ANAL, V22, P888, DOI 10.1109/34.868688; Vapnik V.N, 1998, STAT LEARNING THEORY	11	28	29	0	2	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-20152-6	ADV NEUR IN			2004	16						73	80						8	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BBF99					2022-12-19	WOS:000225309500010
C	Gao, Y; Black, MJ; Bienenstock, E; Shoham, S; Donoghue, JP		Dietterich, TG; Becker, S; Ghahramani, Z		Gao, Y; Black, MJ; Bienenstock, E; Shoham, S; Donoghue, JP			Probabilistic inference of hand motion from neural activity in motor cortex	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 14, VOLS 1 AND 2	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	15th Annual Conference on Neural Information Processing Systems (NIPS)	DEC 03-08, 2001	VANCOUVER, CANADA				MOVEMENT DIRECTION; CORTICAL-NEURONS; PREDICTION	Statistical learning and probabilistic inference techniques are used to infer the hand position of a subject from multi-electrode recordings of neural activity in motor cortex. First, an array of electrodes provides training data of neural firing conditioned on hand kinematics. We learn a non-parametric representation of this firing activity using a Bayesian model and rigorously compare it with previous models using cross-validation. Second, we infer a posterior probability distribution over hand motion conditioned on a sequence of neural test data using Bayesian inference. The learned firing models of multiple cells are used to define a non-Gaussian likelihood term which is combined with a prior probability for the kinematics. A particle filtering method is used to represent, update, and propagate the posterior distribution over time. The approach is compared with traditional linear filtering methods; the results suggest that it may be appropriate for neural prosthetic applications.	Brown Univ, Div Appl Math, Providence, RI 02912 USA	Brown University	Gao, Y (corresponding author), Brown Univ, Div Appl Math, Providence, RI 02912 USA.		Bienenstock, Elie/AAE-2146-2019	Shoham, Shy/0000-0003-0376-3495; Black, Michael/0000-0001-6077-4540				Black MJ, 1996, INT J COMPUT VISION, V19, P57, DOI 10.1007/BF00131148; Brown EN, 1998, J NEUROSCI, V18, P7411; FU QG, 1995, J NEUROPHYSIOL, V73, P836, DOI 10.1152/jn.1995.73.2.836; GEMAN S, 1984, IEEE T PATTERN ANAL, V6, P721, DOI 10.1109/TPAMI.1984.4767596; GEMAN S, 1987, B INT STAT I, P5; GEORGOPOULOS AP, 1986, SCIENCE, V233, P1416, DOI 10.1126/science.3749885; Hatsopoulos NG, 1998, P NATL ACAD SCI USA, V95, P15706, DOI 10.1073/pnas.95.26.15706; Isard M, 1998, INT J COMPUT VISION, V29, P5, DOI 10.1023/A:1008078328650; Maynard EM, 1999, J NEUROSCI, V19, P8083; Moran DW, 1999, J NEUROPHYSIOL, V82, P2676, DOI 10.1152/jn.1999.82.5.2676; Nowak RD, 2000, IEEE T INFORM THEORY, V46, P1811, DOI 10.1109/18.857793; PANINSKI L, 2001, UNPUB J NEUROPHYSIOL; TERZOPOULOS D, 1986, IEEE T PATTERN ANAL, V8, P413, DOI 10.1109/TPAMI.1986.4767807; Wessberg J, 2000, NATURE, V408, P361, DOI 10.1038/35042582	14	28	28	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-04208-8	ADV NEUR IN			2002	14						213	220						8	Computer Science, Artificial Intelligence	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science	BV95T					2022-12-19	WOS:000180520100027
C	Andre, D; Russell, SJ		Leen, TK; Dietterich, TG; Tresp, V		Andre, D; Russell, SJ			Programmable reinforcement learning agents	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present an expressive agent design language for reinforcement learning that allows the user to constrain the policies considered by the learning process.The language includes standard features such as parameterized subroutines, temporary interrupts, aborts, and memory variables, but also allows for unspecified choices in the agent program. For learning that which isn't specified, we present provably convergent learning algorithms. We demonstrate by example that agent programs written in the language are concise as well as modular. This facilitates state abstraction and the transferability of learned skills.	Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94702 USA	University of California System; University of California Berkeley	Andre, D (corresponding author), Univ Calif Berkeley, Div Comp Sci, Berkeley, CA 94702 USA.							ANDRE D, 2000, PROGRAMMABLE HAMS; [Anonymous], 1998, HIERARCHICAL CONTROL; BENSON S, 1995, MACHINE INTELLIGENCE, V14; BERRY G, 1992, SCI COMPUT PROGRAM, V19, P87, DOI 10.1016/0167-6423(92)90005-V; DIETTERICH TG, 2000, NIPS, V12; FIRBY RJ, 1996, AIPS 96, P78; Kaelbling LP, 1996, J ARTIF INTELL RES, V4, P237, DOI 10.1613/jair.301; Nilsson NJ, 1993, J ARTIF INTELL RES, V1, P139, DOI 10.1613/jair.30; PARR R, 1998, NIPS, V10; PESHKIN L, 1999, ICML; Sutton Richard S., 1995, ICML; Sutton RS, 1999, ARTIF INTELL, V112, P181, DOI 10.1016/S0004-3702(99)00052-1	12	28	29	0	0	M I T PRESS	CAMBRIDGE	FIVE CAMBRIDGE CENTER, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						1019	1025						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800143
C	Ben-Hur, A; Horn, D; Siegelmann, HT; Vapnik, V		Leen, TK; Dietterich, TG; Tresp, V		Ben-Hur, A; Horn, D; Siegelmann, HT; Vapnik, V			A support vector method for clustering	ADVANCES IN NEURAL INFORMATION PROCESSING SYSTEMS 13	Advances in Neural Information Processing Systems		English	Proceedings Paper	14th Annual Neural Information Processing Systems Conference (NIPS)	NOV 27-DEC 02, 2000	DENVER, CO					We present a novel method for clustering using the support vector machine approach. Data points are mapped to a high dimensional feature space, where support vectors are used to define a sphere enclosing them. The boundary of the sphere forms in data space a set of closed contours containing the data. Data points enclosed by each contour are defined as a cluster. As the width parameter of the Gaussian kernel is decreased, these contours fit the data more tightly and splitting of contours occurs. The algorithm works by separating clusters according to valleys in the underlying probability distribution, and thus clusters can take on arbitrary geometrical shapes. As in other SV algorithms, outliers can be dealt with by introducing a soft margin constant leading to smoother cluster boundaries. The structure of the data is explored by varying the two parameters. We investigate the dependence of our method on these parameters and apply it to several data sets.	Technion Israel Inst Technol, Fac Ind Engn & Management, IL-32000 Haifa, Israel	Technion Israel Institute of Technology	Ben-Hur, A (corresponding author), Technion Israel Inst Technol, Fac Ind Engn & Management, IL-32000 Haifa, Israel.			Horn, David/0000-0003-2708-186X				Ben-Hur A, 2000, INT C PATT RECOG, P724, DOI 10.1109/ICPR.2000.906177; Blatt M, 1997, NEURAL COMPUT, V9, P1805, DOI 10.1162/neco.1997.9.8.1805; DUBNOV S, UNPUB MACHINE LEARNI; Fisher RA, 1936, ANN EUGENIC, V7, P179, DOI 10.1111/j.1469-1809.1936.tb02137.x; Fletcher R, 1987, PRACTICAL METHODS OP, V1; Fukunaga Keinosuke, 2013, INTRO STAT PATTERN R, P4; Jain A. K., 1988, ALGORITHMS CLUSTERIN, V6; Newman C. B. D., 1998, UCI REPOSITORY MACHI; Platt JC, 1999, ADVANCES IN KERNEL METHODS, P185; Roberts SJ, 1997, PATTERN RECOGN, V30, P261, DOI 10.1016/S0031-3203(96)00079-9; SCHOLKOPF B, 2000, NEURAL INFORMATION P; Tax DMJ, 1999, PATTERN RECOGN LETT, V20, P1191, DOI 10.1016/S0167-8655(99)00087-2; TISHBY N, 2000, NEURAL INFORMATION P; Vapnik V.N, 2000, NATURE STAT LEARNING, V2nd	14	28	34	0	3	MIT PRESS	CAMBRIDGE	ONE ROGERS ST, CAMBRIDGE, MA 02142 USA	1049-5258		0-262-12241-3	ADV NEUR IN			2001	13						367	373						7	Computer Science, Artificial Intelligence; Neurosciences	Conference Proceedings Citation Index - Science (CPCI-S)	Computer Science; Neurosciences & Neurology	BT08J					2022-12-19	WOS:000171891800052
